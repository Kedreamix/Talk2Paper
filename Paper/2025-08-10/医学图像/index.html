<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6eca8c36554ad573a9576ae93ae01c08.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-10-æ›´æ–°"><a href="#2025-08-10-æ›´æ–°" class="headerlink" title="2025-08-10 æ›´æ–°"></a>2025-08-10 æ›´æ–°</h1><h2 id="Scaling-Artificial-Intelligence-for-Prostate-Cancer-Detection-on-MRI-towards-Population-Based-Screening-and-Primary-Diagnosis-in-a-Global-Multiethnic-Population-Study-Protocol"><a href="#Scaling-Artificial-Intelligence-for-Prostate-Cancer-Detection-on-MRI-towards-Population-Based-Screening-and-Primary-Diagnosis-in-a-Global-Multiethnic-Population-Study-Protocol" class="headerlink" title="Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)"></a>Scaling Artificial Intelligence for Prostate Cancer Detection on MRI   towards Population-Based Screening and Primary Diagnosis in a Global,   Multiethnic Population (Study Protocol)</h2><p><strong>Authors:Anindo Saha, Joeran S. Bosma, Jasper J. Twilt, Alexander B. C. D. Ng, Aqua Asif, Kirti Magudia, Peder Larson, Qinglin Xie, Xiaodong Zhang, Chi Pham Minh, Samuel N. Gitau, Ivo G. Schoots, Martijn F. Boomsma, Renato Cuocolo, Nikolaos Papanikolaou, Daniele Regge, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Baris Turkbey, Nancy A. Obuchowski, Jurgen J. FÃ¼tterer, Anwar R. Padhani, Hashim U. Ahmed, Tobias NordstrÃ¶m, Martin Eklund, Veeru Kasivisvanathan, Maarten de Rooij, Henkjan Huisman</strong></p>
<p>In this intercontinental, confirmatory study, we include a retrospective cohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries) to train and externally validate the PI-CAI-2B model, i.e., an efficient, next-generation iteration of the state-of-the-art AI system that was developed for detecting Gleason grade group $\geq$2 prostate cancer on MRI during the PI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities in 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12 independent centers based in Europe, North America, Asia and Africa, are used for training and internal testing. Additionally, 2010 cases (2010 patients; 20 external cities in 12 countries) from population-based screening (STHLM3-MRI, IP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in Europe, North and South Americas, Asia and Australia, are used for external testing. Primary endpoint is the proportion of AI-based assessments in agreement with the standard of care diagnoses (i.e., clinical assessments made by expert uropathologists on histopathology, if available, or at least two expert urogenital radiologists in consensus; with access to patient history and peer consultation) in the detection of Gleason grade group $\geq$2 prostate cancer within the external testing cohorts. Our statistical analysis plan is prespecified with a hypothesis of diagnostic interchangeability to the standard of care at the PI-RADS $\geq$3 (primary diagnosis) or $\geq$4 (screening) cut-off, considering an absolute margin of 0.05 and reader estimates derived from the PI-CAI observer study (62 radiologists reading 400 cases). Secondary measures comprise the area under the receiver operating characteristic curve (AUROC) of the AI system stratified by imaging quality, patient age and patient ethnicity to identify underlying biases (if any). </p>
<blockquote>
<p>åœ¨è¿™é¡¹æ´²é™…ç¡®è®¤æ€§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çº³å…¥äº†22,481ä¾‹MRIæ£€æŸ¥ï¼ˆæ¶‰åŠæ¥è‡ª22ä¸ªå›½å®¶çš„46ä¸ªåŸå¸‚ä¸­çš„21,288åæ‚£è€…ï¼‰çš„æ•°æ®æ¥è®­ç»ƒå’ŒéªŒè¯PI-CAI-2Bæ¨¡å‹ï¼Œå³é’ˆå¯¹PI-CAIç ”ç©¶ä¸­MRIæ£€æµ‹å‡ºçš„æ ¼è±æ£®åˆ†çº§ç»„â‰¥ 2çš„å‰åˆ—è…ºç™Œè€Œå¼€å‘çš„ä¸€ç§å…ˆè¿›çš„ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚å…¶ä¸­ï¼Œæ¥è‡ªä¸¤ä¸ªæ¬§ç›Ÿåœ°å¹³çº¿é¡¹ç›®ï¼ˆProCAncer-Iã€COMFORTï¼‰ä»¥åŠåŸºäºæ¬§æ´²ã€åŒ—ç¾ã€äºšæ´²å’Œéæ´²çš„å¦å¤–ç‹¬ç«‹ç ”ç©¶ä¸­å¿ƒçš„æ¥è‡ªå¤šä¸ªåŸå¸‚çš„çº¦ 2ä¸‡é›¶4ç™¾å¤šæ¬¡MRIæ£€æŸ¥æ¡ˆä¾‹è¢«ç”¨äºè¿›è¡Œè®­ç»ƒå’Œå†…éƒ¨æµ‹è¯•ï¼ˆåŒ…æ‹¬æœ‰å‚ä¸è€…åœ¨æ¬§æ´²ï¼Œå—ç¾æ´²ï¼ŒåŒ—ç¾æ´²äºšæ´²ä»¥åŠæ¾³å¤§åˆ©äºšçš„åŸå¸‚åœ¨å†…ï¼‰ã€‚æ­¤å¤–ï¼Œå¦å¤–æ‹›å‹Ÿè‡ªå¤šä¸ªæ¬§æ´²å›½å®¶çš„å¤§å‹éšæœºæ€§å…¬å…±ç­›æŸ¥è®¡åˆ’ä¸­çš„æ•°åƒä¾‹æ•°æ®ç”¨äºå¤–éƒ¨æµ‹è¯•ã€‚ä¸»è¦çš„è¯„ä»·æŒ‡æ ‡æ˜¯äººå·¥æ™ºèƒ½è¯„ä¼°ç»“æœä¸æ ‡å‡†æŠ¤ç†è¯Šæ–­ç»“æœä¹‹é—´çš„ä¸€è‡´ç¨‹åº¦ï¼ˆå¦‚æœå¯ç”¨çš„è¯ï¼Œåˆ™æ ¹æ®ä¸´åºŠè¯„ä¼°ä¸­ä¸“å®¶å°¿è·¯ç—…ç†å­¦å®¶åœ¨ç»„ç»‡ç—…ç†å­¦ä¸Šçš„åˆ¤æ–­ä¸ºå‡†ï¼›æˆ–è€…è‡³å°‘ä¸¤åä¸“å®¶æ³Œå°¿ç”Ÿæ®–æ”¾å°„ç§‘åŒ»ç”Ÿé€šè¿‡å…±è¯†è¿›è¡Œè¯Šæ–­è¯„ä¼°ï¼ŒåŒæ—¶å‚è€ƒæ‚£è€…ç—…å²å’ŒåŒè¡Œå’¨è¯¢æ„è§ï¼‰ï¼Œæ—¨åœ¨åœ¨å¤–éƒ¨æµ‹è¯•ç¾¤ä½“ä¸­æ£€æµ‹æ ¼è±æ£®åˆ†çº§ç»„â‰¥ 2çš„å‰åˆ—è…ºç™Œæ‚£è€…ã€‚æˆ‘ä»¬çš„ç»Ÿè®¡åˆ†æè®¡åˆ’äº‹å…ˆå‡è®¾ä¸æ ‡å‡†çš„è¯Šç–—æŠ¤ç†è¯Šæ–­ç­‰åŒçš„è¯„ä¼°æ ‡å‡†ï¼Œå³PI-RADSâ‰¥ 3ï¼ˆä¸»è¦è¯Šæ–­ï¼‰æˆ–â‰¥ 4ï¼ˆç­›æŸ¥ï¼‰çš„ä¸´ç•Œå€¼ï¼ŒåŒæ—¶è€ƒè™‘ç»å¯¹è¯¯å·®ä¸ºÂ±0.05åŠé€šè¿‡PI-CAIè§‚å¯Ÿè€…ç ”ç©¶è·å¾—çš„è¯»å–ä¼°ç®—æ•°æ®ï¼ˆæ€»è®¡æ¥è‡ªçº³å…¥è¯¥ç ”ç©¶çš„å›½é™…åŒ»ç”Ÿè´¡çŒ®çš„æœ‰è´¨é‡ä¿éšœå‚è€ƒçš„å¤šå®¶åŒ»å­¦å½±åƒæ”¾å°„ä¸­å¿ƒçš„ä¸åŒå›½é™…é¢†åŸŸæ‚£è€…çš„å„ç±»å†å²å†å²æ‚£è€…ç–¾ç—…æƒ…å†µå’ŒåŒ»å­¦ç ”ç©¶è°ƒæŸ¥æ‰€è·å¾—çš„è¶…å¤§å‹çš„é›†ä½“å®éªŒå€¼ä»¥ä¾›æµ‹è¯•å€Ÿé‰´è¯„ä¼°æ”¹è¿›é¢„æµ‹æµç¨‹æ‰€éœ€çš„é€šç”¨åŒ»å­¦ç±»åŒ»å­¦è¾…åŠ©ç³»ç»ŸåŸºç¡€æ¨¡å‹çš„æ€»ä½“å‡†ç¡®æ€§ã€‚æ¬¡è¦æŒ‡æ ‡åŒ…æ‹¬æ ¹æ®æˆåƒè´¨é‡ã€æ‚£è€…å¹´é¾„å’Œç§æ—åˆ†å±‚çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ï¼Œä»¥è¯†åˆ«æ½œåœ¨åè§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03762v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡‡ç”¨å¤šå›½é™…è·¨åœ°åŸŸçš„å¤§å‹é˜Ÿåˆ—ç ”ç©¶ï¼Œåˆ©ç”¨å…¨çƒä¸åŒåœ°åŒºçš„MRIæ£€æŸ¥æ•°æ®è®­ç»ƒå¹¶éªŒè¯äº†æ–°ä¸€ä»£å‰åˆ—è…ºç™ŒAIè¯Šæ–­æ¨¡å‹PI-CAI-2Bã€‚è¯¥æ¨¡å‹æ—¨åœ¨æé«˜å‰åˆ—è…ºç™Œçš„MRIè¯Šæ–­å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹Gleasonåˆ†çº§â‰¥2çº§çš„ç™Œç—‡æ£€æµ‹ã€‚ç ”ç©¶ä¸­æ¶‰åŠå¤šä¸ªå›½é™…é¡¹ç›®ä¸ç ”ç©¶ä¸­å¿ƒçš„æ•°æ®ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ï¼ŒåŒ…æ‹¬æ¬§æ´²ã€åŒ—ç¾ã€äºšæ´²å’Œéæ´²ç­‰åœ°çš„æ•°æ®ã€‚ä¸»è¦è¯„ä»·æŒ‡æ ‡ä¸ºAIè¯Šæ–­ç»“æœä¸æ ‡å‡†è¯Šç–—æ„è§çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜é€šè¿‡å¤šä¸ªæ¬¡è¦æŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ä¸æ½œåœ¨åè§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨å…¨çƒå¤šåœ°åŒºçš„MRIæ£€æŸ¥æ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒå¹¶éªŒè¯æ–°ä¸€ä»£å‰åˆ—è…ºç™ŒAIè¯Šæ–­æ¨¡å‹PI-CAI-2Bã€‚</li>
<li>è¯¥æ¨¡å‹ç”¨äºæ£€æµ‹Gleasonåˆ†çº§â‰¥2çº§çš„ç™Œç—‡ï¼Œä»¥æé«˜å‰åˆ—è…ºç™Œçš„MRIè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šä¸ªå›½é™…é¡¹ç›®ä¸ç ”ç©¶ä¸­å¿ƒçš„æ•°æ®ï¼ŒåŒ…æ‹¬æ¥è‡ªæ¬§æ´²ã€åŒ—ç¾ã€äºšæ´²å’Œéæ´²ç­‰åœ°çš„æ•°æ®ï¼Œç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>ä¸»è¦è¯„ä»·æŒ‡æ ‡ä¸ºAIè¯Šæ–­ç»“æœä¸æ ‡å‡†è¯Šç–—æ„è§çš„ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶è¿˜é€šè¿‡æ¬¡è¦æŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¸åŒæˆåƒè´¨é‡ã€æ‚£è€…å¹´é¾„å’Œç§æ—å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ï¼Œä»¥è¯†åˆ«æ½œåœ¨åè§ã€‚</li>
<li>ç»Ÿè®¡åˆ†æè®¡åˆ’å·²é¢„è®¾å‡è®¾ï¼Œå¹¶ä¸æ ‡å‡†è¯Šç–—ç¨‹åºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥éªŒè¯æ¨¡å‹çš„è¯Šæ–­äº’æ¢æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-516e1bc704727a3ebb9b4ade93185a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb48e0fcc1ef070f436a3da78bb48c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b75bfed58baa5f10a1c0c243b731982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ccd038d8d57ce4c8cdbb40b56169ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11c6a1de45c76c2a549aa6ac3e7bfc5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d6313c447a2e3d5c8088917fc682a5f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="M-3-HL-Mutual-Mask-Mix-with-High-Low-Level-Feature-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#M-3-HL-Mutual-Mask-Mix-with-High-Low-Level-Feature-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for   Semi-Supervised Medical Image Segmentation"></a>M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Yajun Liu, Zenghui Zhang, Jiang Yue, Weiwei Guo, Dongying Li</strong></p>
<p>Data augmentation methods inspired by CutMix have demonstrated significant potential in recent semi-supervised medical image segmentation tasks. However, these approaches often apply CutMix operations in a rigid and inflexible manner, while paying insufficient attention to feature-level consistency constraints. In this paper, we propose a novel method called Mutual Mask Mix with High-Low level feature consistency (M$^3$HL) to address the aforementioned challenges, which consists of two key components: 1) M$^3$: An enhanced data augmentation operation inspired by the masking strategy from Masked Image Modeling (MIM), which advances conventional CutMix through dynamically adjustable masks to generate spatially complementary image pairs for collaborative training, thereby enabling effective information fusion between labeled and unlabeled images. 2) HL: A hierarchical consistency regularization framework that enforces high-level and low-level feature consistency between unlabeled and mixed images, enabling the model to better capture discriminative feature representations.Our method achieves state-of-the-art performance on widely adopted medical image segmentation benchmarks including the ACDC and LA datasets. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/PHPJava666/M3HL">https://github.com/PHPJava666/M3HL</a> </p>
<blockquote>
<p>æ•°æ®å¢å¼ºæ–¹æ³•åœ¨è¿‘æœŸçš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œè¿™äº›æ–¹æ³•å—åˆ°CutMixçš„å¯å‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»¥ä¸€ç§åƒµç¡¬ä¸”ä¸çµæ´»çš„æ–¹å¼åº”ç”¨CutMixæ“ä½œï¼ŒåŒæ—¶æ²¡æœ‰è¶³å¤Ÿå…³æ³¨ç‰¹å¾çº§åˆ«çš„ä¸€è‡´æ€§çº¦æŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå…·æœ‰é«˜ä½å±‚æ¬¡ç‰¹å¾ä¸€è‡´æ€§çš„äº’æ©æ··åˆï¼ˆM$^3$HLï¼‰ï¼Œä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰M$^3$ï¼šä¸€ç§å—Masked Image Modelingï¼ˆMIMï¼‰ä¸­çš„æ©æ¨¡ç­–ç•¥å¯å‘çš„å¢å¼ºæ•°æ®å¢å¼ºæ“ä½œã€‚å®ƒé€šè¿‡åŠ¨æ€å¯è°ƒæ•´çš„æ©æ¨¡æ¥æ”¹è¿›ä¼ ç»Ÿçš„CutMixï¼Œç”Ÿæˆç©ºé—´äº’è¡¥çš„å›¾åƒå¯¹è¿›è¡ŒååŒè®­ç»ƒï¼Œä»è€Œå®ç°æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾å›¾åƒä¹‹é—´çš„æœ‰æ•ˆä¿¡æ¯å…±äº«ã€‚2ï¼‰HLï¼šä¸€ç§å±‚æ¬¡åŒ–ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ— æ ‡ç­¾å’Œæ··åˆå›¾åƒä¹‹é—´å¼ºåˆ¶å®æ–½é«˜å±‚æ¬¡å’Œä½å±‚æ¬¡ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•è·åˆ¤åˆ«ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›é‡‡ç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬ACDCå’ŒLAæ•°æ®é›†ï¼‰ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PHPJava666/M%E2%BB%8AHL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PHPJava666/M3HLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03752v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒCutMixæ•°æ®å¢å¼ºæ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åº”ç”¨æ—¶å­˜åœ¨åˆšæ€§å’Œç¼ºä¹ç‰¹å¾çº§åˆ«ä¸€è‡´æ€§çº¦æŸçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºM$^3$HLçš„æ–°æ–¹æ³•ï¼ŒåŒ…å«M$^3$å’ŒHLä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚M$^3$é€šè¿‡åŠ¨æ€å¯è°ƒæ©è†œç”Ÿæˆç©ºé—´äº’è¡¥å›¾åƒå¯¹è¿›è¡ŒååŒè®­ç»ƒï¼Œä¿ƒè¿›æ ‡æ³¨å’Œæœªæ ‡æ³¨å›¾åƒçš„æœ‰æ•ˆä¿¡æ¯èåˆã€‚HLå»ºç«‹å±‚æ¬¡ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¡†æ¶ï¼ŒåŠ å¼ºæœªæ ‡æ³¨å’Œæ··åˆå›¾åƒé—´çš„é«˜ã€ä½å±‚æ¬¡ç‰¹å¾ä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹èƒ½æ›´å¥½æ•æ‰åˆ¤åˆ«ç‰¹å¾è¡¨ç¤ºã€‚åœ¨å¹¿æ³›é‡‡ç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CutMixæ•°æ®å¢å¼ºæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç°æœ‰CutMixæ–¹æ³•å­˜åœ¨åº”ç”¨åˆšæ€§ã€ç¼ºä¹ç‰¹å¾çº§åˆ«ä¸€è‡´æ€§çº¦æŸçš„é—®é¢˜ã€‚</li>
<li>M$^3$HLæ–¹æ³•é€šè¿‡åŠ¨æ€å¯è°ƒæ©è†œç”Ÿæˆç©ºé—´äº’è¡¥å›¾åƒå¯¹ï¼Œä¿ƒè¿›ä¿¡æ¯èåˆã€‚</li>
<li>M$^3$HLæ–¹æ³•å»ºç«‹å±‚æ¬¡ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¡†æ¶ï¼ŒåŠ å¼ºç‰¹å¾ä¸€è‡´æ€§ã€‚</li>
<li>M$^3$HLåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>M$^3$HLæ–¹æ³•åŒ…æ‹¬M$^3$å’ŒHLä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£æ•°æ®å¢å¼ºå’Œç‰¹å¾ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33c93f6974f5b8669ff7911b7fe174fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a74331b442a9f828583807eafd95672b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-520c1dfaa54c96d0be2c970c1249d79b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RefineSeg-Dual-Coarse-to-Fine-Learning-for-Medical-Image-Segmentation"><a href="#RefineSeg-Dual-Coarse-to-Fine-Learning-for-Medical-Image-Segmentation" class="headerlink" title="RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation"></a>RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation</h2><p><strong>Authors:Anghong Du, Nay Aung, Theodoros N. Arvanitis, Stefan K. Piechnik, Joao A C Lima, Steffen E. Petersen, Le Zhang</strong></p>
<p>High-quality pixel-level annotations of medical images are essential for supervised segmentation tasks, but obtaining such annotations is costly and requires medical expertise. To address this challenge, we propose a novel coarse-to-fine segmentation framework that relies entirely on coarse-level annotations, encompassing both target and complementary drawings, despite their inherent noise. The framework works by introducing transition matrices in order to model the inaccurate and incomplete regions in the coarse annotations. By jointly training on multiple sets of coarse annotations, it progressively refines the networkâ€™s outputs and infers the true segmentation distribution, achieving a robust approximation of precise labels through matrix-based modeling. To validate the flexibility and effectiveness of the proposed method, we demonstrate the results on two public cardiac imaging datasets, ACDC and MSCMRseg, and further evaluate its performance on the UK Biobank dataset. Experimental results indicate that our approach surpasses the state-of-the-art weakly supervised methods and closely matches the fully supervised approach. </p>
<blockquote>
<p>é«˜è´¨é‡çš„åŒ»ç–—å›¾åƒåƒç´ çº§æ³¨é‡Šå¯¹äºç›‘ç£åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†è·å–è¿™æ ·çš„æ³¨é‡Šæˆæœ¬é«˜æ˜‚ä¸”éœ€è¦åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„ä»ç²—åˆ°ç»†çš„åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®Œå…¨ä¾èµ–äºç²—çº§æ³¨é‡Šï¼ŒåŒ…æ‹¬ç›®æ ‡å’Œè¾…åŠ©ç»˜å›¾ï¼Œå°½ç®¡å®ƒä»¬å­˜åœ¨å›ºæœ‰å™ªå£°ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è½¬æ¢çŸ©é˜µæ¥å»ºæ¨¡ç²—æ³¨é‡Šä¸­çš„ä¸å‡†ç¡®å’Œä¸å®Œæ•´åŒºåŸŸã€‚é€šè¿‡è”åˆè®­ç»ƒå¤šç»„ç²—æ³¨é‡Šï¼Œå®ƒé€æ­¥ä¼˜åŒ–ç½‘ç»œçš„è¾“å‡ºå¹¶æ¨æ–­å‡ºçœŸæ­£çš„åˆ†å‰²åˆ†å¸ƒï¼Œé€šè¿‡åŸºäºçŸ©é˜µçš„å»ºæ¨¡å®ç°ç²¾ç¡®æ ‡ç­¾çš„ç¨³å¥è¿‘ä¼¼ã€‚ä¸ºäº†éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±å¿ƒè„æˆåƒæ•°æ®é›†ACDCå’ŒMSCMRsegä¸Šå±•ç¤ºäº†ç»“æœï¼Œå¹¶åœ¨UK Biobankæ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è¯„ä¼°äº†å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ï¼Œå¹¶ä¸å…¨ç›‘ç£æ–¹æ³•éå¸¸æ¥è¿‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å¯¹é«˜è´¨é‡åƒç´ çº§æ ‡æ³¨çš„éœ€æ±‚å±•å¼€ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„ç²—åˆ°ç»†åˆ†å‰²æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€ç²¾ç¡®æ ‡æ³¨ï¼Œä»…ä¾èµ–ç²—çº§æ ‡æ³¨å’Œè¡¥å……ç»˜å›¾å³å¯è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¼•å…¥è¿‡æ¸¡çŸ©é˜µæ¥æ¨¡æ‹Ÿç²—æ ‡æ³¨ä¸­çš„ä¸å‡†ç¡®å’Œç¼ºå¤±åŒºåŸŸï¼Œé€šè¿‡è”åˆè®­ç»ƒå¤šä¸ªç²—æ ‡æ³¨æ•°æ®é›†ï¼Œé€æ­¥ä¼˜åŒ–ç½‘ç»œè¾“å‡ºå¹¶æ¨æ–­çœŸå®åˆ†å‰²åˆ†å¸ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„å¼±ç›‘ç£æ–¹æ³•ï¼Œå¹¶ä¸å…¨ç›‘ç£æ–¹æ³•æ¥è¿‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡åƒç´ çº§æ ‡æ³¨å¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†è·å–æˆæœ¬é«˜æ˜‚ä¸”éœ€è¦åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç²—åˆ°ç»†åˆ†å‰²æ¡†æ¶ï¼Œå¯ä¾èµ–ç²—çº§æ ‡æ³¨è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«ç›®æ ‡å’Œè¡¥å……ç»˜å›¾ã€‚</li>
<li>å¼•å…¥è¿‡æ¸¡çŸ©é˜µä»¥æ¨¡æ‹Ÿç²—æ ‡æ³¨ä¸­çš„ä¸å‡†ç¡®å’Œç¼ºå¤±åŒºåŸŸã€‚</li>
<li>é€šè¿‡è”åˆè®­ç»ƒå¤šä¸ªç²—æ ‡æ³¨æ•°æ®é›†ï¼Œé€æ­¥ä¼˜åŒ–ç½‘ç»œè¾“å‡ºã€‚</li>
<li>æ¡†æ¶èƒ½æ¨æ–­çœŸå®åˆ†å‰²åˆ†å¸ƒï¼Œå®ç°ç²¾ç¡®æ ‡ç­¾çš„ç¨³å¥è¿‘ä¼¼ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå…¬å…±å¿ƒè„æˆåƒæ•°æ®é›†ACDCå’ŒMSCMRsegä¸ŠéªŒè¯äº†æ–¹æ³•çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39db02e71904d3d8b18ea00b7c969459.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4e018ee8b4ac20de7509f3f958d6b03.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Multi-Agent-System-for-Complex-Reasoning-in-Radiology-Visual-Question-Answering"><a href="#A-Multi-Agent-System-for-Complex-Reasoning-in-Radiology-Visual-Question-Answering" class="headerlink" title="A Multi-Agent System for Complex Reasoning in Radiology Visual Question   Answering"></a>A Multi-Agent System for Complex Reasoning in Radiology Visual Question   Answering</h2><p><strong>Authors:Ziruo Yi, Jinyu Liu, Ting Xiao, Mark V. Albert</strong></p>
<p>Radiology visual question answering (RVQA) provides precise answers to questions about chest X-ray images, alleviating radiologistsâ€™ workload. While recent methods based on multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have shown promising progress in RVQA, they still face challenges in factual accuracy, hallucinations, and cross-modal misalignment. We introduce a multi-agent system (MAS) designed to support complex reasoning in RVQA, with specialized agents for context understanding, multimodal reasoning, and answer validation. We evaluate our system on a challenging RVQA set curated via model disagreement filtering, comprising consistently hard cases across multiple MLLMs. Extensive experiments demonstrate the superiority and effectiveness of our system over strong MLLM baselines, with a case study illustrating its reliability and interpretability. This work highlights the potential of multi-agent approaches to support explainable and trustworthy clinical AI applications that require complex reasoning. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè§†è§‰é—®ç­”ï¼ˆRVQAï¼‰èƒ½å¤Ÿå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„ç›¸å…³é—®é¢˜æä¾›ç²¾ç¡®ç­”æ¡ˆï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚è™½ç„¶æœ€è¿‘åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„RVQAæ–¹æ³•åœ¨RVQAæ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€äº‹å®å‡†ç¡®æ€§ã€å¹»è§‰å’Œè·¨æ¨¡æ€ä¸å¯¹å‡†ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œæ—¨åœ¨æ”¯æŒRVQAä¸­çš„å¤æ‚æ¨ç†ï¼Œé…å¤‡ç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€å¤šæ¨¡æ€æ¨ç†å’Œç­”æ¡ˆéªŒè¯çš„ä¸“é—¨æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªç»è¿‡æ¨¡å‹åˆ†æ­§è¿‡æ»¤ç­›é€‰çš„æŒ‘æˆ˜æ€§RVQAæ•°æ®é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šä¸ªMLLMä¸­å§‹ç»ˆå­˜åœ¨çš„å›°éš¾æ¡ˆä¾‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¼ºå¤§çš„MLLMåŸºå‡†æµ‹è¯•ä¸Šå…·æœ‰ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œæ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨æ”¯æŒéœ€è¦å¤æ‚æ¨ç†çš„å¯è§£é‡Šå’Œå¯ä¿¡èµ–çš„ä¸´åºŠäººå·¥æ™ºèƒ½åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02841v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ”¾å°„å­¦è§†è§‰é—®ç­”ï¼ˆRVQAï¼‰æŠ€æœ¯ï¼Œå¯¹äºèƒ¸éƒ¨Xå…‰å›¾åƒç›¸å…³é—®é¢˜èƒ½å¤Ÿç»™å‡ºç²¾ç¡®ç­”æ¡ˆï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚å°½ç®¡ç°æœ‰åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•åœ¨RVQAæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨äº‹å®å‡†ç¡®æ€§ã€å¹»è±¡å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œæ”¯æŒRVQAä¸­çš„å¤æ‚æ¨ç†ï¼ŒåŒ…æ‹¬ç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€å¤šæ¨¡æ€æ¨ç†å’Œç­”æ¡ˆéªŒè¯çš„ä¸“ç”¨æ™ºèƒ½ä½“ã€‚é€šè¿‡æ¨¡å‹åˆ†æ­§è¿‡æ»¤æ³•ç²¾å¿ƒæŒ‘é€‰çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„RVQAæ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½ä¼˜äºå¼ºå¤§çš„MLLMåŸºçº¿ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶å‡¸æ˜¾äº†å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨éœ€è¦å¤æ‚æ¨ç†çš„å¯è§£é‡Šå’Œå¯ä¿¡èµ–çš„ä¸´åºŠäººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RVQAæŠ€æœ¯èƒ½å¤Ÿä¸ºèƒ¸éƒ¨Xå…‰å›¾åƒç›¸å…³é—®é¢˜æä¾›ç²¾ç¡®ç­”æ¡ˆï¼Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚</li>
<li>åŸºäºMLLMå’ŒRAGçš„æ–¹æ³•åœ¨RVQAæ–¹é¢å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨äº‹å®å‡†ç¡®æ€§ã€å¹»è±¡å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åŒ…æ‹¬ç”¨äºä¸Šä¸‹æ–‡ç†è§£ã€å¤šæ¨¡æ€æ¨ç†å’Œç­”æ¡ˆéªŒè¯çš„ä¸“ç”¨æ™ºèƒ½ä½“ï¼Œä»¥æ”¯æŒRVQAä¸­çš„å¤æ‚æ¨ç†ã€‚</li>
<li>é€šè¿‡æ¨¡å‹åˆ†æ­§è¿‡æ»¤æ³•æŒ‘é€‰çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„RVQAæ•°æ®é›†çš„å®éªŒéªŒè¯äº†MASç³»ç»Ÿçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸å¼ºå¤§çš„MLLMåŸºçº¿ç›¸æ¯”ï¼ŒMASç³»ç»Ÿåœ¨æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†MASç³»ç»Ÿçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4029854d0eacee6d2cabc7cc4beaeddc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-323ae0784585ccf40d191402c7b7d65c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f1f717db800300972b34e01596de7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3c269e9b1429b34c398aa53ba4d092b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0734ffe2a2fd8cde97b3dd0acaba500.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder"><a href="#Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder" class="headerlink" title="Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder"></a>Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder</h2><p><strong>Authors:Biagio Brattoli, Jack Shi, Jongchan Park, Taebum Lee, Donggeun Yoo, Sergio Pereira</strong></p>
<p>Identifying actionable driver mutations in non-small cell lung cancer (NSCLC) can impact treatment decisions and significantly improve patient outcomes. Despite guideline recommendations, broader adoption of genetic testing remains challenging due to limited availability and lengthy turnaround times. Machine Learning (ML) methods for Computational Pathology (CPath) offer a potential solution; however, research often focuses on only one or two common mutations, limiting the clinical value of these tools and the pool of patients who can benefit from them. This study evaluates various Multiple Instance Learning (MIL) techniques to detect six key actionable NSCLC driver mutations: ALK, BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric Transformer Decoder model that employs queries and key-values of varying dimensions to maintain a low query dimensionality. This approach efficiently extracts information from patch embeddings and minimizes overfitting risks, proving highly adaptable to the MIL setting. Moreover, we present a method to directly utilize tissue type in the model, addressing a typical MIL limitation where either all regions or only some specific regions are analyzed, neglecting biological relevance. Our method outperforms top MIL models by an average of 3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving ML-based tests closer to being practical alternatives to standard genetic testing. </p>
<blockquote>
<p>è¯†åˆ«éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­çš„å¯æ“ä½œé©±åŠ¨åŸºå› çªå˜å¯ä»¥å¯¹æ²»ç–—å†³ç­–äº§ç”Ÿå½±å“ï¼Œå¹¶æ˜¾è‘—æ”¹å–„æ‚£è€…é¢„åã€‚å°½ç®¡æœ‰æŒ‡å—æ¨èï¼Œä½†ç”±äºæœ‰é™çš„å¯ç”¨æ€§å’Œæ¼«é•¿çš„å‘¨è½¬æ—¶é—´ï¼Œæ›´å¹¿æ³›åœ°é‡‡ç”¨åŸºå› æ£€æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰ä¸­çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼›ç„¶è€Œï¼Œç ”ç©¶é€šå¸¸åªå…³æ³¨ä¸€ç§æˆ–ä¸¤ç§å¸¸è§çš„çªå˜ï¼Œè¿™é™åˆ¶äº†è¿™äº›å·¥å…·çš„ä¸´åºŠä»·å€¼ä»¥åŠå¯ä»¥ä»è¿™äº›å·¥å…·ä¸­å—ç›Šçš„æ‚£è€…ç¾¤ä½“ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å„ç§å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯ï¼Œä»¥æ£€æµ‹å…­ç§å…³é”®å¯æ“ä½œNSCLCé©±åŠ¨åŸºå› çªå˜ï¼šALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸å¯¹ç§°å˜å‹å™¨è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸åŒç»´åº¦çš„æŸ¥è¯¢å’Œé”®å€¼æ¥ä¿æŒä½æŸ¥è¯¢ç»´åº¦ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ä»è¡¥ä¸åµŒå…¥ä¸­æå–ä¿¡æ¯ï¼Œæœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶è¯æ˜å…¶é«˜åº¦é€‚åº”äºMILç¯å¢ƒã€‚è€Œä¸”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•ï¼Œè§£å†³äº†å…¸å‹çš„MILé™åˆ¶ï¼Œå³åˆ†ææ‰€æœ‰åŒºåŸŸæˆ–ä»…åˆ†ææŸäº›ç‰¹å®šåŒºåŸŸï¼Œè€Œå¿½ç•¥äº†ç”Ÿç‰©ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡ä¼˜äºé¡¶çº§MILæ¨¡å‹çº¦3%ï¼Œåœ¨é¢„æµ‹å¦‚ERBB2å’ŒBRAFç­‰ç½•è§çªå˜æ—¶è¶…è¿‡4%ï¼Œä½¿åŸºäºMLçš„æµ‹è¯•æ›´æ¥è¿‘å®ç”¨çš„æ›¿ä»£æ ‡å‡†åŸºå› æ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02431v2">PDF</a> Accepted at MICCAI 2025 Workshop COMPAYL</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ£€æµ‹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„å…­ç§å…³é”®å¯è¡ŒåŠ¨çªå˜åŸºå› ï¼ŒåŒ…æ‹¬ALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14çš„æ–¹æ³•ã€‚å¼•å…¥äº†ä¸€ç§ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œèƒ½é«˜æ•ˆåœ°ä»è¡¥ä¸åµŒå…¥ä¸­æå–ä¿¡æ¯å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚åŒæ—¶ï¼Œæå‡ºäº†ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•æ¥è§£å†³å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„å¸¸è§„é™åˆ¶ã€‚è¯¥æ–¹æ³•åœ¨é¢„æµ‹ç½•è§çªå˜å¦‚ERBB2å’ŒBRAFæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½¿å¾—åŸºäºMLçš„æµ‹è¯•æ›´æ¥è¿‘äºå®é™…çš„åŸºå› æµ‹è¯•æ›¿ä»£å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­çš„å…³é”®è§è§£è¦ç‚¹ï¼Œä»¥ç®€åŒ–çš„ä¸­æ–‡åˆ—å‡ºï¼š</p>
<ol>
<li>åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ£€æµ‹éå°ç»†èƒè‚ºç™Œçš„é©±åŠ¨åŸºå› çªå˜å¯¹æ²»ç–—å†³ç­–æœ‰é‡è¦å½±å“ã€‚</li>
<li>é—ä¼ æµ‹è¯•çš„å¹¿æ³›é‡‡çº³ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œè®¡ç®—ç—…ç†å­¦ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤šå®ä¾‹å­¦ä¹ æŠ€æœ¯æ¥æ£€æµ‹å…­ç§å…³é”®å¯è¡ŒåŠ¨NSCLCé©±åŠ¨çªå˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œèƒ½é«˜æ•ˆå¤„ç†ä¿¡æ¯å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•æ¥è§£å†³å¤šå®ä¾‹å­¦ä¹ çš„é™åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é¢„æµ‹ç½•è§çªå˜å¦‚ERBB2å’ŒBRAFæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿å¾—åŸºäºæœºå™¨å­¦ä¹ çš„æµ‹è¯•æ›´æ¥è¿‘äºå®é™…çš„åŸºå› æµ‹è¯•æ›¿ä»£å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-87d4c838dc9bf1faeae0b767a8df1e62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b73b8786bd862d37090a876bb2df3286.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7731e30b2ac7c0cb29ef1c9d0f11feb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-538749abd9ef55cc69b164b37842c5ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef370342012d61b929fc647434b9ffc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhancing-Zero-Shot-Brain-Tumor-Subtype-Classification-via-Fine-Grained-Patch-Text-Alignment"><a href="#Enhancing-Zero-Shot-Brain-Tumor-Subtype-Classification-via-Fine-Grained-Patch-Text-Alignment" class="headerlink" title="Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained   Patch-Text Alignment"></a>Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained   Patch-Text Alignment</h2><p><strong>Authors:Lubin Gan, Jing Zhang, Linhao Qu, Yijun Wang, Siying Wu, Xiaoyan Sun</strong></p>
<p>The fine-grained classification of brain tumor subtypes from histopathological whole slide images is highly challenging due to subtle morphological variations and the scarcity of annotated data. Although vision-language models have enabled promising zero-shot classification, their ability to capture fine-grained pathological features remains limited, resulting in suboptimal subtype discrimination. To address these challenges, we propose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot framework tailored for digital pathology. FG-PAN consists of two key modules: (1) a local feature refinement module that enhances patch-level visual features by modeling spatial relationships among representative patches, and (2) a fine-grained text description generation module that leverages large language models to produce pathology-aware, class-specific semantic prototypes. By aligning refined visual features with LLM-generated fine-grained descriptions, FG-PAN effectively increases class separability in both visual and semantic spaces. Extensive experiments on multiple public pathology datasets, including EBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance and robust generalization in zero-shot brain tumor subtype classification. </p>
<blockquote>
<p>ç”±äºå¾®å¦™çš„å½¢æ€å­¦å˜åŒ–å’Œæ³¨é‡Šæ•°æ®çš„ç¨€ç¼ºï¼Œä»ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒå¯¹è„‘è‚¿ç˜¤äºšå‹è¿›è¡Œç²¾ç»†åˆ†ç±»æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹å·²ç»å®ç°äº†æœ‰å‰æ™¯çš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œä½†å®ƒä»¬æ•æ‰ç²¾ç»†ç—…ç†ç‰¹å¾çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå¯¼è‡´äºšå‹é‰´åˆ«ç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ•°å­—ç—…ç†çš„é‡èº«å®šåˆ¶çš„ç»†ç²’åº¦è¡¥ä¸å¯¹é½ç½‘ç»œï¼ˆFine-Grained Patch Alignment Networkï¼Œç®€ç§°FG-PANï¼‰ã€‚FG-PANç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šï¼ˆ1ï¼‰å±€éƒ¨ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿä»£è¡¨æ€§è¡¥ä¸ä¹‹é—´çš„ç©ºé—´å…³ç³»æ¥å¢å¼ºè¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç»†ç²’åº¦æ–‡æœ¬æè¿°ç”Ÿæˆæ¨¡å—ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥äº§ç”Ÿç—…ç†æ„ŸçŸ¥ã€ç±»ç‰¹å®šçš„è¯­ä¹‰åŸå‹ã€‚é€šè¿‡å°†ç²¾ç‚¼çš„è§†è§‰ç‰¹å¾ä¸LLMç”Ÿæˆçš„ç»†ç²’åº¦æè¿°è¿›è¡Œå¯¹é½ï¼ŒFG-PANæœ‰æ•ˆåœ°æé«˜äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¸­çš„ç±»å¯åˆ†æ€§ã€‚åœ¨å¤šä¸ªå…¬å…±ç—…ç†å­¦æ•°æ®é›†ï¼ˆåŒ…æ‹¬EBRAINSå’ŒTCGAï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFG-PANåœ¨é›¶æ ·æœ¬è„‘è‚¿ç˜¤äºšå‹åˆ†ç±»ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01602v2">PDF</a> </p>
<p><strong>Summary</strong><br>è„‘è‚¿ç˜¤äºšå‹çš„ç²¾ç»†åˆ†ç±»å› å…¶å¾®å¦™çš„å½¢æ€å˜åŒ–å’Œæ³¨é‡Šæ•°æ®çš„ç¨€ç¼ºè€Œä»ç—…ç†å…¨ç»„ç»‡åˆ‡ç‰‡å›¾åƒä¸­æˆä¸ºä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»†ç²’åº¦è¡¥ä¸å¯¹é½ç½‘ç»œï¼ˆFG-PANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºæ•°å­—ç—…ç†å­¦çš„é›¶æ ·æœ¬åˆ†ç±»æ–°å‹æ¡†æ¶ã€‚FG-PANåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šä¸€æ˜¯å±€éƒ¨ç‰¹å¾ä¼˜åŒ–æ¨¡å—ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿä»£è¡¨æ€§è¡¥ä¸ä¹‹é—´çš„ç©ºé—´å…³ç³»æ¥æå‡è¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼›äºŒæ˜¯ç»†ç²’åº¦æ–‡æœ¬æè¿°ç”Ÿæˆæ¨¡å—ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆç—…ç†ç›¸å…³çš„ã€å…·æœ‰ç±»ç‰¹å¼‚æ€§çš„è¯­ä¹‰åŸå‹ã€‚é€šè¿‡ä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸ç»†ç²’åº¦æè¿°çš„å¯¹é½ï¼ŒFG-PANæœ‰æ•ˆåœ°æé«˜äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¸­çš„ç±»å¯åˆ†æ€§ã€‚åœ¨å¤šä¸ªå…¬å…±ç—…ç†å­¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFG-PANåœ¨é›¶æ ·æœ¬è„‘è‚¿ç˜¤äºšå‹åˆ†ç±»ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤äºšå‹çš„ç²¾ç»†åˆ†ç±»æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¾®å¦™çš„å½¢æ€å˜åŒ–å’Œæ³¨é‡Šæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>ç»†ç²’åº¦è¡¥ä¸å¯¹é½ç½‘ç»œï¼ˆFG-PANï¼‰æ˜¯ä¸€ç§é’ˆå¯¹æ•°å­—ç—…ç†å­¦çš„é›¶æ ·æœ¬åˆ†ç±»æ–°å‹æ¡†æ¶ã€‚</li>
<li>FG-PANåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå±€éƒ¨ç‰¹å¾ä¼˜åŒ–æ¨¡å—å’Œç»†ç²’åº¦æ–‡æœ¬æè¿°ç”Ÿæˆæ¨¡å—ã€‚</li>
<li>å±€éƒ¨ç‰¹å¾ä¼˜åŒ–æ¨¡å—é€šè¿‡æ¨¡æ‹Ÿä»£è¡¨æ€§è¡¥ä¸ä¹‹é—´çš„ç©ºé—´å…³ç³»æå‡è¡¥ä¸çº§åˆ«çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>ç»†ç²’åº¦æ–‡æœ¬æè¿°ç”Ÿæˆæ¨¡å—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†ç›¸å…³çš„ã€å…·æœ‰ç±»ç‰¹å¼‚æ€§çš„è¯­ä¹‰åŸå‹ã€‚</li>
<li>FG-PANé€šè¿‡ä¼˜åŒ–è§†è§‰ç‰¹å¾ä¸ç»†ç²’åº¦æè¿°çš„å¯¹é½ï¼Œæœ‰æ•ˆæé«˜äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¸­çš„ç±»å¯åˆ†æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af44b3758f68f9f7731aa156909744c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f58bfc49965fb836062ffa94511e1375.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EfficientGFormer-Multimodal-Brain-Tumor-Segmentation-via-Pruned-Graph-Augmented-Transformer"><a href="#EfficientGFormer-Multimodal-Brain-Tumor-Segmentation-via-Pruned-Graph-Augmented-Transformer" class="headerlink" title="EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned   Graph-Augmented Transformer"></a>EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned   Graph-Augmented Transformer</h2><p><strong>Authors:Fatemeh Ziaeetabar</strong></p>
<p>Accurate and efficient brain tumor segmentation remains a critical challenge in neuroimaging due to the heterogeneous nature of tumor subregions and the high computational cost of volumetric inference. In this paper, we propose EfficientGFormer, a novel architecture that integrates pretrained foundation models with graph-based reasoning and lightweight efficiency mechanisms for robust 3D brain tumor segmentation. Our framework leverages nnFormer as a modality-aware encoder, transforming multi-modal MRI volumes into patch-level embeddings. These features are structured into a dual-edge graph that captures both spatial adjacency and semantic similarity. A pruned, edge-type-aware Graph Attention Network (GAT) enables efficient relational reasoning across tumor subregions, while a distillation module transfers knowledge from a full-capacity teacher to a compact student model for real-time deployment. Experiments on the MSD Task01 and BraTS 2021 datasets demonstrate that EfficientGFormer achieves state-of-the-art accuracy with significantly reduced memory and inference time, outperforming recent transformer-based and graph-based baselines. This work offers a clinically viable solution for fast and accurate volumetric tumor delineation, combining scalability, interpretability, and generalization. </p>
<blockquote>
<p>ç²¾ç¡®ä¸”é«˜æ•ˆçš„è„‘è‚¿ç˜¤åˆ†å‰²åœ¨ç¥ç»æˆåƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºè‚¿ç˜¤äºšåŒºçš„å¼‚è´¨æ€§ä»¥åŠä½“ç§¯æ¨ç†çš„é«˜è®¡ç®—æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EfficientGFormerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„åŸºæ¨¡å‹ä¸åŸºäºå›¾çš„æ¨ç†å’Œè½»é‡åŒ–æ•ˆç‡æœºåˆ¶ç›¸ç»“åˆï¼Œç”¨äºç¨³å¥çš„3Dè„‘è‚¿ç˜¤åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨nnFormerä½œä¸ºæ¨¡æ€æ„ŸçŸ¥ç¼–ç å™¨ï¼Œå°†å¤šæ¨¡æ€MRIä½“ç§¯è½¬æ¢ä¸ºæ–‘å—çº§åµŒå…¥ã€‚è¿™äº›ç‰¹å¾è¢«æ„å»ºæˆä¸€ä¸ªåŒè¾¹ç¼˜å›¾ï¼Œè¯¥å›¾æ•æ‰äº†ç©ºé—´é‚»è¿‘æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚ä¸€ä¸ªç»è¿‡ä¿®å‰ªçš„è¾¹ç¼˜ç±»å‹æ„ŸçŸ¥å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å®ç°äº†è‚¿ç˜¤äºšåŒºä¹‹é—´çš„æœ‰æ•ˆå…³ç³»æ¨ç†ï¼Œè€Œè’¸é¦æ¨¡å—å°†çŸ¥è¯†ä»å…¨å®¹é‡æ•™å¸ˆè½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ï¼Œç”¨äºå®æ—¶éƒ¨ç½²ã€‚åœ¨MSD Task01å’ŒBraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEfficientGFormerè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œæ¨ç†æ—¶é—´ï¼Œè¶…è¶Šäº†æœ€è¿‘çš„åŸºäºè½¬æ¢å™¨å’ŒåŸºäºå›¾çš„åŸºçº¿ã€‚è¿™é¡¹å·¥ä½œç»“åˆå¯æ‰©å±•æ€§ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¿«é€Ÿå‡†ç¡®çš„ä½“ç§¯è‚¿ç˜¤æç»˜æä¾›äº†ä¸´åºŠå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01465v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºEfficientGFormeræ¶æ„ï¼Œç»“åˆé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€å›¾æ¨ç†å’Œè½»é‡åŒ–æ•ˆç‡æœºåˆ¶ï¼Œç”¨äºç¨³å¥çš„3Dè„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ¶æ„åˆ©ç”¨nnFormerä½œä¸ºæ¨¡æ€æ„ŸçŸ¥ç¼–ç å™¨ï¼Œå°†å¤šæ¨¡æ€MRIä½“ç§¯è½¬æ¢ä¸ºæ–‘å—çº§åµŒå…¥ï¼Œç»“æ„åŒ–åŒè¾¹ç¼˜å›¾æ•æ‰ç©ºé—´é‚»è¿‘æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å®éªŒè¯æ˜EfficientGFormeråœ¨MSD Task01å’ŒBraTS 2021æ•°æ®é›†ä¸Šå®ç°å“è¶Šç²¾åº¦ï¼Œæ˜¾è‘—é™ä½å†…å­˜å’Œæ¨ç†æ—¶é—´ï¼Œä¸ºå¿«é€Ÿå‡†ç¡®çš„ä½“ç§¯è‚¿ç˜¤æç»˜æä¾›ä¸´åºŠå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EfficientGFormeræ˜¯ä¸€ä¸ªç”¨äº3Dè„‘è‚¿ç˜¤åˆ†å‰²çš„æ–°å‹æ¶æ„ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€å›¾æ¨ç†å’Œè½»é‡åŒ–æ•ˆç‡æœºåˆ¶ã€‚</li>
<li>nnFormerä½œä¸ºæ¨¡æ€æ„ŸçŸ¥ç¼–ç å™¨ï¼Œå°†å¤šæ¨¡æ€MRIä½“ç§¯è½¬æ¢ä¸ºæ–‘å—çº§åµŒå…¥ã€‚</li>
<li>åŒè¾¹ç¼˜å›¾ç»“æ„ç”¨äºæ•æ‰ç©ºé—´é‚»è¿‘æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>æ¶æ„é‡‡ç”¨ä¿®å‰ªçš„è¾¹ç¼˜ç±»å‹æ„ŸçŸ¥å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å®ç°é«˜æ•ˆçš„å…³ç³»æ¨ç†ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ¨¡å—å°†çŸ¥è¯†ä»å…¨å®¹é‡æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°å®æ—¶éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c856d76eb73037d804815a8625df9da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a0336a350a5084bf46c233b8ea1b83d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea9f8f670a67a11b3df677476861fa6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CADDesigner-Conceptual-Design-of-CAD-Models-Based-on-General-Purpose-Agent"><a href="#CADDesigner-Conceptual-Design-of-CAD-Models-Based-on-General-Purpose-Agent" class="headerlink" title="CADDesigner: Conceptual Design of CAD Models Based on General-Purpose   Agent"></a>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose   Agent</h2><p><strong>Authors:Jingzhe Ni, Xiaolong Yin, Xingyu Lu, Xintong Li, Ji Wei, Ruofeng Tong, Min Tang, Peng Du</strong></p>
<p>Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agentâ€™s code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ä¸šåˆ¶é€ ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œé€šå¸¸éœ€è¦è®¾è®¡å¸ˆå…·å¤‡é«˜æ°´å¹³çš„ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†é™ä½å…¥é—¨é—¨æ§›å¹¶æé«˜è®¾è®¡æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„CADæ¦‚å¿µè®¾è®¡ä»£ç†ã€‚è¯¥ä»£ç†æ¥å—æŠ½è±¡çš„æ–‡æœ¬æè¿°å’Œæ‰‹ç»˜è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¯¹è¯ï¼Œä»¥ç»†åŒ–å’Œæ¾„æ¸…è®¾è®¡è¦æ±‚ã€‚ä»£ç†å»ºç«‹åœ¨æ–°å‹ä¸Šä¸‹æ–‡ç‹¬ç«‹æŒ‡ä»¤èŒƒå¼ï¼ˆCIPï¼‰ä¹‹ä¸Šï¼Œå¯ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»£ç†ä¼šèå…¥è¿­ä»£è§†è§‰åé¦ˆä»¥æé«˜æ¨¡å‹è´¨é‡ã€‚ç”Ÿæˆçš„æ¡ˆä¾‹è¢«å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œä½¿å¾—ä»£ç†çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å¾—ä»¥æŒç»­æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CADä»£ç ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01031v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ä¸šåˆ¶é€ ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠè®¾è®¡å¸ˆé€šå¸¸éœ€è¦å…·å¤‡é«˜æ°´å¹³çš„ä¸“ä¸šçŸ¥è¯†è¿™ä¸€ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„CADæ¦‚å¿µè®¾è®¡ä»£ç†ã€‚è¯¥ä»£ç†æ¥å—æŠ½è±¡çš„æ–‡æœ¬æè¿°å’Œå¾’æ‰‹è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¯¹è¯ä»¥æ˜ç¡®å’Œå®Œå–„è®¾è®¡è¦æ±‚ã€‚å®ƒé‡‡ç”¨æ–°é¢–çš„Context-Independent Imperative Paradigmï¼ˆCIPï¼‰ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»£ç†ç»“åˆè¿­ä»£è§†è§‰åé¦ˆä»¥æé«˜æ¨¡å‹è´¨é‡ã€‚ç”Ÿæˆçš„æ¡ˆä¾‹å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œä½¿å¾—ä»£ç†çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å¾—åˆ°æŒç»­æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CADä»£ç ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADæ¦‚å¿µè®¾è®¡ä»£ç†é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ï¼Œé™ä½äº†CADè®¾è®¡çš„é—¨æ§›å¹¶æé«˜äº†è®¾è®¡æ•ˆç‡ã€‚</li>
<li>è¯¥ä»£ç†å¯ä»¥æ¥å—æŠ½è±¡æ–‡æœ¬æè¿°å’Œå¾’æ‰‹è‰å›¾ä½œä¸ºè¾“å…¥ã€‚</li>
<li>ä»£ç†é€šè¿‡å…¨é¢çš„éœ€æ±‚åˆ†æä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¯¹è¯ï¼Œä»¥æ˜ç¡®å’Œå®Œå–„è®¾è®¡è¦æ±‚ã€‚</li>
<li>é‡‡ç”¨æ–°é¢–çš„Context-Independent Imperative Paradigmï¼ˆCIPï¼‰ç”Ÿæˆé«˜è´¨é‡çš„CADå»ºæ¨¡ä»£ç ã€‚</li>
<li>åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»£ç†ç»“åˆè¿­ä»£è§†è§‰åé¦ˆä»¥æé«˜æ¨¡å‹è´¨é‡ã€‚</li>
<li>ç”Ÿæˆçš„æ¡ˆä¾‹å­˜å‚¨åœ¨ç»“æ„åŒ–çŸ¥è¯†åº“ä¸­ï¼Œä½¿å¾—ä»£ç†çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å¾—åˆ°æŒç»­æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddaffb42129047f7a9b0f5fe052eaafc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-471b34d825ed7d45948d05145a75878d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-149f0ba3745679a3e331ac4a854866d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de2641a61067f28bdb3b678da5625e3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LeakyCLIP-Extracting-Training-Data-from-CLIP"><a href="#LeakyCLIP-Extracting-Training-Data-from-CLIP" class="headerlink" title="LeakyCLIP: Extracting Training Data from CLIP"></a>LeakyCLIP: Extracting Training Data from CLIP</h2><p><strong>Authors:Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma</strong></p>
<p>Understanding the memorization and privacy leakage risks in Contrastive Languageâ€“Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models. </p>
<blockquote>
<p>äº†è§£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©å¯¹äºç¡®ä¿å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä»æ‰©æ•£æ¨¡å‹ä¸­æå–æ•æ„Ÿè®­ç»ƒæ ·æœ¬æ˜¯å¯è¡Œçš„ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨è®°å¿†å’Œæ³„éœ²ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„å€¾å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡CLIPåè½¬çš„è§†è§’æ¥ç ”ç©¶CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ã€‚CLIPåè½¬æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬æç¤ºä¸­é‡å»ºè®­ç»ƒå›¾åƒçš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{LeakyCLIP}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨ä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬å‘ç°CLIPåè½¬å­˜åœ¨ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç‰¹å¾ä¸ç¨³å¥ï¼Œ2ï¼‰æ–‡æœ¬åµŒå…¥ä¸­çš„è§†è§‰è¯­ä¹‰æœ‰é™ï¼Œä»¥åŠ3ï¼‰é‡å»ºä¿çœŸåº¦ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡ç”¨1ï¼‰å¯¹æŠ—æ€§å¾®è°ƒä»¥å¢å¼ºä¼˜åŒ–çš„å¹³ç¨³æ€§ï¼Œ2ï¼‰åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ï¼Œä»¥åŠ3ï¼‰åŸºäºStable Diffusionçš„ç»†åŒ–ä»¥æé«˜ä¿çœŸåº¦ã€‚ç»éªŒç»“æœè¡¨æ˜LeakyCLIPçš„ä¼˜è¶Šæ€§ï¼Œåœ¨LAION-2Bå­é›†ä¸Šä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒViT-B-16çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†358%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†æ™®éçš„æ³„éœ²é£é™©ï¼Œè¡¨æ˜å³ä½¿ä»ä½ä¿çœŸé‡å»ºçš„æŒ‡æ ‡ä¸­ä¹Ÿå¯ä»¥æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜ã€‚æˆ‘ä»¬çš„å·¥ä½œä»‹ç»äº†ä¸€ç§å®ç”¨çš„CLIPåè½¬æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†å…³äºå¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©æœ¬è´¨å’ŒèŒƒå›´çš„å…¨æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00756v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç†è§£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†ä¸éšç§æ³„éœ²é£é™©å¯¹äºç¡®ä¿å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜å¯ä»¥ä»æ‰©æ•£æ¨¡å‹ä¸­æå–æ•æ„Ÿè®­ç»ƒæ ·æœ¬ï¼Œå¹¶ä¸”æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨è®°å¿†å’Œæ³„éœ²ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„å€¾å‘ã€‚æœ¬ç ”ç©¶é€šè¿‡CLIPåè½¬çš„è§†è§’ï¼Œæ¢è®¨CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„æ”»å‡»æ¡†æ¶â€”â€”LeakyCLIPï¼Œæ—¨åœ¨ä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬ç¡®å®šäº†CLIPåè½¬çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰éç¨³å¥ç‰¹å¾ï¼›2ï¼‰æ–‡æœ¬åµŒå…¥ä¸­çš„æœ‰é™è§†è§‰è¯­ä¹‰ï¼›3ï¼‰ä½é‡å»ºä¿çœŸåº¦ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡ç”¨ï¼š1ï¼‰å¯¹æŠ—æ€§å¾®è°ƒä»¥å¢å¼ºä¼˜åŒ–å¹³æ»‘åº¦ï¼›2ï¼‰åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ï¼›3ï¼‰åŸºäºStable Diffusionçš„æ”¹è¿›ä»¥æé«˜ä¿çœŸåº¦ã€‚å®è¯ç»“æœè¡¨æ˜LeakyCLIPçš„ä¼˜åŠ¿ï¼Œåœ¨LAION-2Bå­é›†ä¸Šä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œé’ˆå¯¹ViT-B-16çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†358%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†æ™®éçš„æ³„éœ²é£é™©ï¼Œå³ä½¿æ˜¯ä»ä½ä¿çœŸé‡å»ºçš„æŒ‡æ ‡ä¸­ï¼Œä¹Ÿèƒ½æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä»…æä¾›äº†CLIPåè½¬çš„å®é™…æ–¹æ³•ï¼Œè¿˜æ­ç¤ºäº†å¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©çš„æ–°ç‰¹å¾å’ŒèŒƒå›´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨è®°å¿†å’Œéšç§æ³„éœ²æ–¹é¢å­˜åœ¨é£é™©ï¼Œè¿™å¯¹å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡CLIPåè½¬æ¢ç©¶äº†CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹çš„æ”»å‡»æ¡†æ¶LeakyCLIPï¼Œå®ç°äº†é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚</li>
<li>ç¡®å®šäº†CLIPåè½¬çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå¹¶ç»™å‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LeakyCLIPåœ¨å®è¯ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>å‘ç°äº†æ™®éçš„æ³„éœ²é£é™©ï¼Œå³ä½¿ä½ä¿çœŸé‡å»ºä¹Ÿèƒ½æ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-873c3e286480775b2043c532ce4b4380.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64a0eefcf7f259bd66e4c30339e3e1ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbb890dd60799f5e4d90b79ecab16db4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a56def90e3a3794c4e19cc07cad4730.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Microlocal-analysis-of-non-linear-operators-arising-in-Compton-CT"><a href="#Microlocal-analysis-of-non-linear-operators-arising-in-Compton-CT" class="headerlink" title="Microlocal analysis of non-linear operators arising in Compton CT"></a>Microlocal analysis of non-linear operators arising in Compton CT</h2><p><strong>Authors:James W. Webber, Sean Holman</strong></p>
<p>We present a novel microlocal analysis of a non-linear ray transform, $\mathcal{R}$, arising in Compton Scattering Tomography (CST). Due to attenuation effects in CST, the integral weights depend on the reconstruction target, $f$, which has singularities. Thus, standard linear Fourier Integral Operator (FIO) theory does not apply as the weights are non-smooth. The V-line (or broken ray) transform, $\mathcal{V}$, can be used to model the attenuation of incoming and outgoing rays. Through novel analysis of $\mathcal{V}$, we characterize the location and strength of the singularities of the ray transform weights. In conjunction, we provide new results which quantify the strength of the singularities of distributional products based on the Sobolev order of the individual components. By combining this new theory, our analysis of $\mathcal{V}$, and classical linear FIO theory, we determine the Sobolev order of the singularities of $\mathcal{R}f$. The strongest (lowest Sobolev order) singularities of $\mathcal{R}f$ are shown to correspond to the wavefront set elements of the classical Radon transform applied to $f$, and we use this idea and known results on the Radon transform to prove injectivity results for $\mathcal{R}$. In addition, we present novel reconstruction methods based on our theory, and we validate our results using simulated image reconstructions. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹åœ¨åº·æ™®é¡¿æ•£å°„æ–­å±‚æ‰«ææŠ€æœ¯ï¼ˆCSTï¼‰ä¸­å‡ºç°çš„éçº¿æ€§å°„çº¿å˜æ¢$\mathcal{R}$æå‡ºäº†ä¸€ç§æ–°çš„å¾®å±€éƒ¨åˆ†æã€‚ç”±äºCSTä¸­çš„è¡°å‡æ•ˆåº”ï¼Œç§¯åˆ†æƒé‡å–å†³äºå…·æœ‰å¥‡å¼‚æ€§çš„é‡å»ºç›®æ ‡$f$ã€‚å› æ­¤ï¼Œæ ‡å‡†çš„çº¿æ€§å‚…é‡Œå¶ç§¯åˆ†ç®—å­ï¼ˆFIOï¼‰ç†è®ºå¹¶ä¸é€‚ç”¨ï¼Œå› ä¸ºæƒé‡æ˜¯éå…‰æ»‘çš„ã€‚Vçº¿ï¼ˆæˆ–æ–­çº¿ï¼‰å˜æ¢$\mathcal{V}$å¯ç”¨äºæ¨¡æ‹Ÿå…¥å°„å’Œå‡ºå°„å°„çº¿çš„è¡°å‡ã€‚é€šè¿‡å¯¹$\mathcal{V}$çš„æ–°åˆ†æï¼Œæˆ‘ä»¬è¡¨å¾äº†å°„çº¿å˜æ¢æƒé‡çš„å¥‡å¼‚ç‚¹çš„ä½ç½®å’Œå¼ºåº¦ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æä¾›äº†æ–°çš„ç»“æœï¼Œæ ¹æ®å„ç»„ä»¶çš„Sobolevé˜¶æ•°é‡åŒ–åˆ†å¸ƒäº§å“å¥‡å¼‚ç‚¹çš„å¼ºåº¦ã€‚é€šè¿‡ç»“åˆè¿™ä¸€æ–°ç†è®ºã€æˆ‘ä»¬å¯¹$\mathcal{V}$çš„åˆ†æå’Œç»å…¸çš„çº¿æ€§FIOç†è®ºï¼Œæˆ‘ä»¬ç¡®å®šäº†$\mathcal{R}f$çš„å¥‡å¼‚ç‚¹çš„Sobolevé˜¶æ•°ã€‚$\mathcal{R}f$çš„æœ€å¼ºï¼ˆæœ€ä½Sobolevé˜¶æ•°ï¼‰å¥‡å¼‚ç‚¹å¯¹åº”äºç»å…¸Radonå˜æ¢åº”ç”¨äº$f$çš„æ³¢å‰é›†å…ƒç´ ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸€æ€æƒ³ä»¥åŠRadonå˜æ¢çš„å·²çŸ¥ç»“æœæ¥è¯æ˜$\mathcal{R}$çš„å•å°„æ€§ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºæˆ‘ä»¬çš„ç†è®ºæå‡ºäº†æ–°é¢–çš„é‡æ„æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå›¾åƒé‡æ„éªŒè¯äº†æˆ‘ä»¬çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19791v2">PDF</a> 27 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§éçº¿æ€§çš„å°„çº¿å˜æ¢$\mathcal{R}$åœ¨åº·æ™®é¡¿æ•£å°„æ–­å±‚æ‰«ææŠ€æœ¯ï¼ˆCSTï¼‰ä¸­çš„å¾®å±€éƒ¨åˆ†æã€‚ç”±äºCSTä¸­çš„è¡°å‡æ•ˆåº”ï¼Œé‡å»ºç›®æ ‡$f$çš„ç§¯åˆ†æƒé‡å­˜åœ¨å¥‡å¼‚æ€§ï¼Œå› æ­¤æ ‡å‡†çš„çº¿æ€§å‚…ç«‹å¶ç§¯åˆ†ç®—å­ç†è®ºä¸å†é€‚ç”¨ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥Vçº¿ï¼ˆæˆ–æ–­è£‚å°„çº¿ï¼‰å˜æ¢$\mathcal{V}$æ¥æ¨¡æ‹Ÿå…¥å°„å’Œå‡ºå°„å°„çº¿çš„è¡°å‡ï¼Œå¹¶å¯¹$\mathcal{V}$è¿›è¡Œæ–°å‹åˆ†æï¼Œä»¥è¡¨å¾å°„çº¿å˜æ¢æƒé‡çš„å¥‡å¼‚æ€§ä½ç½®å’Œå¼ºåº¦ã€‚ç»“åˆåŸºäºSobolevé˜¶çš„åˆ†å¸ƒå¼äº§å“å¥‡å¼‚æ€§å¼ºåº¦çš„æ–°ç»“æœï¼Œé€šè¿‡ç»“åˆè¿™ä¸€æ–°ç†è®ºã€æˆ‘ä»¬å¯¹$\mathcal{V}$çš„åˆ†æä»¥åŠç»å…¸çš„çº¿æ€§FIOç†è®ºï¼Œæˆ‘ä»¬ç¡®å®šäº†$\mathcal{R}f$çš„å¥‡å¼‚æ€§Sobolevé˜¶ã€‚$\mathcal{R}f$çš„æœ€å¼ºï¼ˆæœ€ä½Sobolevé˜¶ï¼‰å¥‡å¼‚æ€§å¯¹åº”äºç»å…¸Radonå˜æ¢åº”ç”¨äº$f$çš„æ³¢å‰é›†å…ƒç´ ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸€æ€æƒ³ä»¥åŠRadonå˜æ¢çš„å·²çŸ¥ç»“æœæ¥è¯æ˜$\mathcal{R}$çš„æ³¨å…¥æ€§ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºç†è®ºæå‡ºäº†æ–°å‹é‡å»ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå›¾åƒé‡å»ºéªŒè¯äº†æˆ‘ä»¬çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†éçº¿æ€§çš„å°„çº¿å˜æ¢$\mathcal{R}$åœ¨åº·æ™®é¡¿æ•£å°„æ–­å±‚æ‰«ææŠ€æœ¯ä¸­çš„å¾®å±€éƒ¨åˆ†æã€‚</li>
<li>ç”±äºè¡°å‡æ•ˆåº”ï¼Œæ ‡å‡†çº¿æ€§å‚…ç«‹å¶ç§¯åˆ†ç®—å­ç†è®ºä¸å†é€‚ç”¨äºå°„çº¿å˜æ¢ã€‚</li>
<li>é€šè¿‡Vçº¿ï¼ˆæˆ–æ–­è£‚å°„çº¿ï¼‰å˜æ¢$\mathcal{V}$æ¨¡æ‹Ÿå…¥å°„å’Œå‡ºå°„å°„çº¿çš„è¡°å‡ã€‚</li>
<li>åˆ†æäº†å°„çº¿å˜æ¢æƒé‡çš„å¥‡å¼‚æ€§ä½ç½®å’Œå¼ºåº¦ã€‚</li>
<li>ç»“åˆæ–°ç†è®ºå’Œç»å…¸çº¿æ€§FIOç†è®ºï¼Œç¡®å®šäº†$\mathcal{R}f$çš„å¥‡å¼‚æ€§Sobolevé˜¶ã€‚</li>
<li>$\mathcal{R}f$çš„æœ€å¼ºå¥‡å¼‚æ€§ä¸ç»å…¸Radonå˜æ¢åº”ç”¨äº$f$çš„æ³¢å‰é›†å…ƒç´ ç›¸å¯¹åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ed733af61402d943a79a84c5aa401a1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings"><a href="#CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings" class="headerlink" title="CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings"></a>CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings</h2><p><strong>Authors:Kyeongkyu Lee, Seonghwan Yoon, Hongki Lim</strong></p>
<p>Automatic generation of radiology reports has the potential to alleviate radiologistsâ€™ significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) enforces reasoning-aware decoding that completes â€œFindingsâ€ before synthesizing the â€œImpressionâ€, and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a reasoning-aware next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive Findings section before synthesizing the Impression and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on both standard NLG metrics and clinically aware scores. </p>
<blockquote>
<p>è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šå…·æœ‰ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿå·¨å¤§å·¥ä½œé‡çš„æ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨ä¸´åºŠå¯é ç»“è®ºçš„ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚å°¤å…¶ï¼Œå¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨ç”Ÿæˆæµç•…æ–‡æœ¬ï¼Œæœªèƒ½æœ‰æ•ˆåœ°ç¡®ä¿æŠ¥å‘Šçš„äº‹å®æ­£ç¡®æ€§ï¼Œå¹¶ä¸”å¸¸ä¾èµ–äºå•ä¸€è§†å›¾å›¾åƒï¼Œä»è€Œé™åˆ¶äº†è¯Šæ–­çš„å…¨é¢æ€§ã€‚æˆ‘ä»¬æå‡ºäº†CLARIFIDè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç›´æ¥ä¼˜åŒ–è¯Šæ–­çš„æ­£ç¡®æ€§ï¼Œé€šè¿‡é•œåƒä¸“å®¶çš„ä¸¤æ­¥å·¥ä½œæµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒCLARIFIDï¼ˆ1ï¼‰é€šè¿‡åˆ†æ®µé¢„è®­ç»ƒå­¦ä¹ ä»æ£€æŸ¥ç»“æœåˆ°å°è±¡çš„é€»è¾‘æµç¨‹ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­ä»¥å°è±¡éƒ¨åˆ†çš„CheXbert F1åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œï¼ˆ3ï¼‰å¼ºåˆ¶æ‰§è¡Œæ¨ç†æ„ŸçŸ¥è§£ç ï¼Œåœ¨å®Œæˆâ€œæ£€æŸ¥ç»“æœâ€åå†åˆæˆâ€œå°è±¡â€ï¼Œï¼ˆ4ï¼‰é€šè¿‡åŸºäºè§†è§‰å˜æ¢å™¨çš„å¤šè§†å›¾ç¼–ç å™¨èåˆå¤šä¸ªèƒ¸éƒ¨Xå°„çº¿è§†å›¾ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸€ç§æ¨ç†æ„ŸçŸ¥çš„ä¸‹ä¸€ä¸ªæ ‡è®°å¼ºåˆ¶ç­–ç•¥ï¼Œç„¶åè¿›è¡ŒæŠ¥å‘Šçº§åˆ«çš„é‡æ–°æ’åºï¼Œç¡®ä¿æ¨¡å‹é¦–å…ˆç”Ÿæˆå…¨é¢çš„æ£€æŸ¥ç»“æœéƒ¨åˆ†ï¼Œç„¶åå†åˆæˆå°è±¡ï¼Œä»è€Œä¿æŒè¿è´¯çš„ä¸´åºŠæ¨ç†ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å“è¶Šçš„ä¸´åºŠæ•ˆæœï¼Œå¹¶åœ¨æ ‡å‡†è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å’Œä¸´åºŠæ„è¯†å¾—åˆ†ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17234v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCLARIFIDçš„æ–°æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¼˜åŒ–è¯Šæ–­æ­£ç¡®æ€§ï¼Œé€šè¿‡æ¨¡ä»¿ä¸“å®¶ä¸¤æ­¥å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬é€»è¾‘æµå­¦ä¹ ã€åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„ç²¾ç»†è°ƒæ•´ã€æ¨ç†æ„ŸçŸ¥è§£ç ä»¥åŠå¤šè§†è§’èåˆçš„å›¾åƒåˆ†æã€‚å®éªŒç»“æœåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè¯æ˜äº†å…¶ä¸´åºŠæœ‰æ•ˆæ€§å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLARIFIDæ¡†æ¶æ—¨åœ¨ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šä¼˜åŒ–è¯Šæ–­æ­£ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æ¨¡ä»¿ä¸“å®¶ä¸¤æ­¥å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬é€»è¾‘æµå­¦ä¹ ã€ç²¾ç»†è°ƒæ•´ã€æ¨ç†æ„ŸçŸ¥è§£ç å’Œå¤šè§†è§’èåˆçš„å›¾åƒåˆ†æã€‚</li>
<li>CLARIFIDé€šè¿‡section-awareé¢„è®­ç»ƒå­¦ä¹ ä»â€œå‘ç°â€åˆ°â€œå°è±¡â€çš„é€»è¾‘æµã€‚</li>
<li>ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥CheXbert F1åˆ†æ•°ä½œä¸ºå¥–åŠ±ã€‚</li>
<li>æ¡†æ¶å¼ºåˆ¶å®æ–½æ¨ç†æ„ŸçŸ¥è§£ç ï¼Œå…ˆå®Œæˆâ€œå‘ç°â€éƒ¨åˆ†å†åˆæˆâ€œå°è±¡â€ã€‚</li>
<li>é€šè¿‡è§†è§‰å˜å‹å™¨è¿›è¡Œå¤šè§†è§’ç¼–ç èåˆï¼Œæé«˜è¯Šæ–­çš„å…¨é¢æ€§ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨æ¨ç†æ„ŸçŸ¥çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œå¼ºåˆ¶ç­–ç•¥ï¼Œç¡®ä¿æŠ¥å‘Šçš„è¿è´¯æ€§å’Œä¸´åºŠæ¨ç†çš„å®Œæ•´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a8caa07dcc30956b8fb294930cf3754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c57b969e4e3149082361d1f08ad8a3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae8905313d4e51644f0a2c46d342181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-684ca478433efdebec89d42b64dba222.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Capsule-ConvKAN-A-Hybrid-Neural-Approach-to-Medical-Image-Classification"><a href="#Capsule-ConvKAN-A-Hybrid-Neural-Approach-to-Medical-Image-Classification" class="headerlink" title="Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image   Classification"></a>Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image   Classification</h2><p><strong>Authors:Laura PitukovÃ¡, Peter SinÄÃ¡k, LÃ¡szlÃ³ JÃ³zsef KovÃ¡cs, Peng Wang</strong></p>
<p>This study conducts a comprehensive comparison of four neural network architectures: Convolutional Neural Network, Capsule Network, Convolutional Kolmogorov-Arnold Network, and the newly proposed Capsule-Convolutional Kolmogorov-Arnold Network. The proposed Capsule-ConvKAN architecture combines the dynamic routing and spatial hierarchy capabilities of Capsule Network with the flexible and interpretable function approximation of Convolutional Kolmogorov-Arnold Networks. This novel hybrid model was developed to improve feature representation and classification accuracy, particularly in challenging real-world biomedical image data. The architectures were evaluated on a histopathological image dataset, where Capsule-ConvKAN achieved the highest classification performance with an accuracy of 91.21%. The results demonstrate the potential of the newly introduced Capsule-ConvKAN in capturing spatial patterns, managing complex features, and addressing the limitations of traditional convolutional models in medical image classification. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¯¹å››ç§ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼šå·ç§¯ç¥ç»ç½‘ç»œã€èƒ¶å›Šç½‘ç»œã€å·ç§¯Kolmogorov-Arnoldç½‘ç»œä»¥åŠæ–°æå‡ºçš„èƒ¶å›Š-å·ç§¯Kolmogorov-Arnoldç½‘ç»œã€‚æ‰€æå‡ºçš„Capsule-ConvKANæ¶æ„ç»“åˆäº†èƒ¶å›Šç½‘ç»œçš„åŠ¨æ€è·¯ç”±å’Œç©ºé—´å±‚æ¬¡èƒ½åŠ›ä¸å·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„çµæ´»å’Œå¯è§£é‡Šå‡½æ•°é€¼è¿‘ã€‚è¿™ç§æ–°å‹æ··åˆæ¨¡å‹æ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®ä¸­ã€‚è¿™äº›æ¶æ„åœ¨ä¸€ä¸ªç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶ä¸­Capsule-ConvKANè·å¾—äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º9.2%ã€‚ç»“æœè¡¨æ˜ï¼Œæ–°å¼•å…¥çš„Capsule-ConvKANåœ¨æ•è·ç©ºé—´æ¨¡å¼ã€ç®¡ç†å¤æ‚ç‰¹å¾ä»¥åŠè§£å†³åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ä¼ ç»Ÿå·ç§¯æ¨¡å‹çš„å±€é™æ€§æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06417v2">PDF</a> Preprint version. Accepted to IEEE SMC 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å…¨é¢æ¯”è¾ƒäº†å››ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œã€èƒ¶å›Šç½‘ç»œã€å·ç§¯Kolmogorov-Arnoldç½‘ç»œä»¥åŠæ–°æå‡ºçš„èƒ¶å›Š-å·ç§¯Kolmogorov-Arnoldç½‘ç»œï¼ˆCapsule-ConvKANï¼‰ã€‚æ–°æå‡ºçš„Capsule-ConvKANæ¶æ„ç»“åˆäº†èƒ¶å›Šç½‘ç»œçš„åŠ¨æ€è·¯ç”±å’Œç©ºé—´å±‚æ¬¡èƒ½åŠ›ï¼Œä»¥åŠå·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„çµæ´»å’Œå¯è§£é‡Šå‡½æ•°é€¼è¿‘èƒ½åŠ›ã€‚è¯¥æ··åˆæ¨¡å‹æ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®ä¸­ã€‚åœ¨ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¯„ä¼°è¿™äº›æ¶æ„æ—¶ï¼ŒCapsule-ConvKANè·å¾—äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º91.21%ã€‚ç»“æœè¡¨æ˜ï¼Œæ–°å¼•å…¥çš„Capsule-ConvKANåœ¨æ•è·ç©ºé—´æ¨¡å¼ã€ç®¡ç†å¤æ‚ç‰¹å¾ä»¥åŠè§£å†³åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­ä¼ ç»Ÿå·ç§¯æ¨¡å‹çš„å±€é™æ€§æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹å››ç§ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œã€èƒ¶å›Šç½‘ç»œã€å·ç§¯Kolmogorov-Arnoldç½‘ç»œä»¥åŠåˆ›æ–°çš„Capsule-ConvKANæ¶æ„ã€‚</li>
<li>Capsule-ConvKANç»“åˆäº†èƒ¶å›Šç½‘ç»œå’Œå·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®æ—¶ã€‚</li>
<li>åœ¨ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šï¼ŒCapsule-ConvKANæ¶æ„è·å¾—äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º91.21%ã€‚</li>
<li>Capsule-ConvKANæ¶æ„å…·æœ‰æ•è·ç©ºé—´æ¨¡å¼å’Œç®¡ç†å¤æ‚ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>æ–°ç ”ç©¶ç»“æœè¡¨æ˜Capsule-ConvKANæœ‰æ½œåŠ›å…‹æœä¼ ç»Ÿå·ç§¯æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­çš„å±€é™æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç¥ç»ç½‘ç»œåœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§è§£å’Œå¯èƒ½çš„æ”¹è¿›æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e533629265af3536bdd1e4f631e0a602.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ef41029d4756e6540d994de72efadf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3bae803e3f793e000f0b69237bd6331.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8567933d58dd219a03ef14043d5791c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48ae02b785bfc57129dc230291f42e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5e064becc5459e83b4af6041a636b47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-534c24f4c402d1aeb9168961474f8cbd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields-in-Efficient-CNNs-for-Fair-Medical-Image-Classification"><a href="#Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields-in-Efficient-CNNs-for-Fair-Medical-Image-Classification" class="headerlink" title="Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields   in Efficient CNNs for Fair Medical Image Classification"></a>Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields   in Efficient CNNs for Fair Medical Image Classification</h2><p><strong>Authors:Xiao Wu, Xiaoqing Zhang, Zunjie Xiao, Lingxi Hu, Risa Higashita, Jiang Liu</strong></p>
<p>Efficient convolutional neural network (CNN) architecture design has attracted growing research interests. However, they typically apply single receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different feature representations, still encountering two significant challenges in medical image classification tasks: 1) They have limitations in capturing diverse lesion characteristics efficiently, e.g., tiny, coordination, small and salient, which have unique roles on the classification results, especially imbalanced medical image classification. 2) The predictions generated by those CNNs are often unfair&#x2F;biased, bringing a high risk when employing them to real-world medical diagnosis conditions. To tackle these issues, we develop a new concept, Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields (ERoHPRF), to simultaneously boost medical image classification performance and fairness. This concept aims to mimic the multi-expert consultation mode by applying the well-designed heterogeneous pyramid RF bag to capture lesion characteristics with varying significances effectively via convolution operations with multiple heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like structural reparameterization technique to merge its parameters with the two-stage strategy, ensuring competitive computation cost and inference speed through comparisons to a single RF. To manifest the effectiveness and generalization ability of ERoHPRF, we incorporate it into mainstream efficient CNN architectures. The extensive experiments show that our proposed ERoHPRF maintains a better trade-off than state-of-the-art methods in terms of medical image classification, fairness, and computation overhead. The code of this paper is available at <a target="_blank" rel="noopener" href="https://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields">https://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è®¾è®¡çš„æ•ˆç‡å·²å¼•èµ·è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸åº”ç”¨å•ä¸€æ„Ÿå—é‡ï¼ˆRFï¼‰ã€å°çš„ä¸å¯¹ç§°æ„Ÿå—é‡æˆ–é‡‘å­—å¡”æ„Ÿå—é‡æ¥å­¦ä¹ ä¸åŒçš„ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰å®ƒä»¬åœ¨æœ‰æ•ˆæ•è·å„ç§ç—…å˜ç‰¹å¾æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚å¾®å°ã€åè°ƒã€å°è€Œæ˜¾è‘—çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯¹åˆ†ç±»ç»“æœå…·æœ‰ç‹¬ç‰¹ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å¹³è¡¡åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ã€‚2ï¼‰è¿™äº›CNNç”Ÿæˆçš„é¢„æµ‹å¾€å¾€ä¸å…¬å¹³&#x2F;å­˜åœ¨åè§ï¼Œåœ¨å°†å…¶åº”ç”¨äºçœŸå®ä¸–ç•ŒåŒ»å­¦è¯Šæ–­æ¡ä»¶æ—¶å­˜åœ¨é«˜é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µï¼Œå³å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡çš„ä¸“å®¶å‹å†å‚æ•°åŒ–ï¼ˆERoHPRFï¼‰ï¼Œä»¥åŒæ—¶æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚è¯¥æ¦‚å¿µæ—¨åœ¨é€šè¿‡åº”ç”¨è®¾è®¡ç²¾è‰¯çš„å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡è¢‹æ¥æ¨¡ä»¿å¤šä¸“å®¶å’¨è¯¢æ¨¡å¼ï¼Œé€šè¿‡å…·æœ‰å¤šç§å¼‚æ„å†…æ ¸å¤§å°çš„å·ç§¯æ“ä½œæœ‰æ•ˆåœ°æ•è·å…·æœ‰ä¸åŒé‡è¦æ€§çš„ç—…å˜ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒERoHPRFå¼•å…¥äº†ä¸€ç§ä¸“å®¶å‹ç»“æ„å†å‚æ•°åŒ–æŠ€æœ¯ï¼Œå°†å…¶å‚æ•°ä¸ä¸¤é˜¶æ®µç­–ç•¥ç›¸ç»“åˆï¼Œé€šè¿‡ä¸å•ä¸€æ„Ÿå—é‡è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿å…·æœ‰ç«äº‰åŠ›çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†è¯æ˜ERoHPRFçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬å°†å…¶çº³å…¥ä¸»æµçš„ç®€æ´CNNæ¶æ„ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ERoHPRFåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€æ–¹é¢æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ›´å¥½çš„æŠ˜è¡·ã€‚æœ¬æ–‡çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields]%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaoLing12138/Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields]ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13039v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„çš„è®¾è®¡åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯éš¾ä»¥æœ‰æ•ˆæ•æ‰å¤šç§ç—…å˜ç‰¹å¾ï¼›äºŒæ˜¯é¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨ä¸å…¬å¹³æ€§&#x2F;åè§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µâ€”â€”Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fieldsï¼ˆERoHPRFï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åº”ç”¨è®¾è®¡ç²¾è‰¯çš„å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡è¢‹æ¥æ¨¡æ‹Ÿå¤šä¸“å®¶ä¼šè¯Šæ¨¡å¼ï¼Œé€šè¿‡å…·æœ‰å¤šç§å¼‚è´¨å†…æ ¸å¤§å°çš„å·ç§¯æ“ä½œæœ‰æ•ˆæ•æ‰ä¸åŒæ˜¾è‘—æ€§çš„ç—…å˜ç‰¹å¾ã€‚åŒæ—¶ï¼ŒERoHPRFå¼•å…¥äº†ä¸€ç§ä¸“å®¶å‹ç»“æ„é‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥åˆå¹¶å…¶å‚æ•°ï¼Œä»¥ç¡®ä¿è®¡ç®—æˆæœ¬å’Œæ¨ç†é€Ÿåº¦ä¸å•ä¸€æ„Ÿå—é‡ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ERoHPRFåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€æ–¹é¢è¾¾åˆ°äº†æ›´ä¼˜åŒ–çš„å¹³è¡¡ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>CNNåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­é¢ä¸´æ•æ‰å¤šæ ·ç—…å˜ç‰¹å¾çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¾®å°ã€åè°ƒã€å°è€Œæ˜¾è‘—ç­‰ç‹¬ç‰¹ä½œç”¨çš„ç—…å˜ã€‚</li>
<li>ç°æœ‰CNNé¢„æµ‹å­˜åœ¨ä¸å…¬å¹³&#x2F;åè§é—®é¢˜ï¼Œåº”ç”¨äºå®é™…åŒ»ç–—è¯Šæ–­æ—¶å­˜åœ¨é«˜é£é™©ã€‚</li>
<li>æå‡ºERoHPRFæ–°æ¦‚å¿µï¼Œæ¨¡æ‹Ÿå¤šä¸“å®¶ä¼šè¯Šæ¨¡å¼ï¼Œé€šè¿‡å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡è¢‹æœ‰æ•ˆæ•æ‰ç—…å˜ç‰¹å¾ã€‚</li>
<li>ERoHPRFå¼•å…¥ä¸“å®¶å‹ç»“æ„é‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>ERoHPRFåœ¨ä¸»æµé«˜æ•ˆCNNæ¶æ„ä¸­çš„é›†æˆå®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€ä¹‹é—´è¾¾åˆ°äº†æ›´å¥½çš„å¹³è¡¡ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯åœ¨æŒ‡å®šé“¾æ¥ä¸‹è½½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-290c7c19370aed2edaaaaa1fbb3bf58d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efce3edb1b67e6c688be8ed7cc5016fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3812ed8fd1034914aadcaa6822546e79.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="False-Promises-in-Medical-Imaging-AI-Assessing-Validity-of-Outperformance-Claims"><a href="#False-Promises-in-Medical-Imaging-AI-Assessing-Validity-of-Outperformance-Claims" class="headerlink" title="False Promises in Medical Imaging AI? Assessing Validity of   Outperformance Claims"></a>False Promises in Medical Imaging AI? Assessing Validity of   Outperformance Claims</h2><p><strong>Authors:Evangelia Christodoulou, Annika Reinke, Pascaline AndrÃ¨, Patrick Godau, Piotr Kalinowski, Rola Houhou, Selen Erkan, Carole H. Sudre, Ninon Burgos, SofiÃ¨ne Boutaj, Sophie Loizillon, MaÃ«lys Solal, Veronika Cheplygina, Charles Heitz, Michal Kozubek, Michela Antonelli, Nicola Rieke, Antoine Gilson, Leon D. Mayer, Minu D. Tizabi, M. Jorge Cardoso, Amber Simpson, Annette Kopp-Schneider, GaÃ«l Varoquaux, Olivier Colliot, Lena Maier-Hein</strong></p>
<p>Performance comparisons are fundamental in medical imaging Artificial Intelligence (AI) research, often driving claims of superiority based on relative improvements in common performance metrics. However, such claims frequently rely solely on empirical mean performance. In this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. We quantify the probability of false claims based on a Bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. According to our results, the majority (&gt;80%) of papers claims outperformance when introducing a new method. Our analysis further revealed a high probability (&gt;5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. These findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging AI are frequently unsubstantiated, posing a risk of misdirecting future research efforts. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶ä¸­ï¼Œæ€§èƒ½æ¯”è¾ƒæ˜¯æ ¹æœ¬ï¼Œé€šå¸¸åŸºäºé€šç”¨æ€§èƒ½æŒ‡æ ‡ä¸Šçš„ç›¸å¯¹æ”¹è¿›æ¥æå‡ºä¼˜è¶Šæ€§ä¸»å¼ ã€‚ç„¶è€Œï¼Œæ­¤ç±»ä¸»å¼ å¾€å¾€ä»…ä¾èµ–äºç»éªŒå¹³å‡æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æå…·æœ‰ä»£è¡¨æ€§çš„åŒ»å­¦æˆåƒè®ºæ–‡ï¼Œç ”ç©¶æ–°æå‡ºçš„æ–¹æ³•æ˜¯å¦çœŸæ­£ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬åŸºäºè´å¶æ–¯æ–¹æ³•é‡åŒ–é”™è¯¯ä¸»å¼ çš„æ¦‚ç‡ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æŠ¥å‘Šçš„ç»“æœä»¥åŠç»éªŒä¼°è®¡çš„æ¨¡å‹ä¸€è‡´æ€§æ¥ä¼°è®¡æ–¹æ³•çš„ç›¸å¯¹æ’åæ˜¯å¦å¯èƒ½å¶ç„¶å‘ç”Ÿã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼Œåœ¨å¼•å…¥æ–°æ–¹æ³•æ—¶ï¼Œå¤§å¤šæ•°ï¼ˆ&gt; 80%ï¼‰çš„è®ºæ–‡éƒ½å£°ç§°æ€§èƒ½æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜æ˜¾ç¤ºï¼Œåˆ†ç±»è®ºæ–‡ä¸­æœ‰é«˜è¾¾86%ä»¥åŠåˆ†å‰²è®ºæ–‡ä¸­æœ‰é«˜è¾¾53%çš„è®ºæ–‡å­˜åœ¨è™šå‡çš„æ€§èƒ½æå‡ä¸»å¼ æ¦‚ç‡è¾ƒé«˜ï¼ˆ&gt; 5%ï¼‰ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰åŸºå‡†æµ‹è¯•å®è·µä¸­çš„ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šåŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½çš„æ€§èƒ½æå‡ä¸»å¼ å¾€å¾€ç¼ºä¹ä¾æ®ï¼Œå­˜åœ¨è¯¯å¯¼æœªæ¥ç ”ç©¶åŠªåŠ›çš„é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04720v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è°ƒæŸ¥äº†åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶ä¸­æ–°æå‡ºçš„æ–¹æ³•æ˜¯å¦çœŸæ­£ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚é€šè¿‡åˆ†æä»£è¡¨æ€§åŒ»å­¦æˆåƒè®ºæ–‡ï¼ŒåŸºäºè´å¶æ–¯æ–¹æ³•å’ŒæŠ¥å‘Šç»“æœçš„æ¨¡å‹ä¸€è‡´æ€§ä¼°è®¡ï¼Œå‘ç°å¤§å¤šæ•°è®ºæ–‡ï¼ˆè¶…è¿‡80%ï¼‰å£°ç§°æ–°æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä½†åˆ†ç±»è®ºæ–‡ä¸­æœ‰è¶…è¿‡5%çš„æ¦‚ç‡å­˜åœ¨è™šå‡æ€§èƒ½å£°ç§°ã€‚æœ¬æ–‡æ­ç¤ºäº†å½“å‰åŸºå‡†æµ‹è¯•å®è·µä¸­çš„å…³é”®ç¼ºé™·ï¼šåŒ»å­¦æˆåƒAIçš„æ€§èƒ½æå‡å£°ç§°å¾€å¾€ç¼ºä¹ä¾æ®ï¼Œè¯¯å¯¼æœªæ¥ç ”ç©¶å·¥ä½œçš„é£é™©å¾ˆé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒAIç ”ç©¶ä¸­çš„æ€§èƒ½æ¯”è¾ƒè‡³å…³é‡è¦ï¼Œé€šå¸¸åŸºäºå¸¸è§æ€§èƒ½æŒ‡æ ‡ä¸Šçš„ç›¸å¯¹æ”¹è¿›æ¥å®£ç§°ä¼˜è¶Šæ€§ã€‚</li>
<li>å¤§å¤šæ•°åŒ»å­¦æˆåƒè®ºæ–‡ï¼ˆè¶…è¿‡80%ï¼‰å£°ç§°æ–°æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>åŸºäºè´å¶æ–¯æ–¹æ³•å’ŒæŠ¥å‘Šç»“æœçš„æ¨¡å‹ä¸€è‡´æ€§ä¼°è®¡ï¼Œå­˜åœ¨è™šå‡æ€§èƒ½å£°ç§°çš„å¯èƒ½æ€§ã€‚</li>
<li>åˆ†ç±»è®ºæ–‡ä¸­å­˜åœ¨è¾ƒé«˜çš„è™šå‡æ€§èƒ½å£°ç§°æ¦‚ç‡ï¼ˆè¶…è¿‡5%ï¼‰ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•å®è·µå­˜åœ¨å…³é”®ç¼ºé™·ï¼Œæ€§èƒ½æå‡å£°ç§°ç¼ºä¹ä¾æ®ã€‚</li>
<li>åŒ»å­¦æˆåƒAIçš„è¯¯å¯¼å‘ç ”ç©¶é£é™©è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34795190717b14ac9695e17f89f2e43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffce1d9b2f766da41fdb17106bbd7648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427c99ba7a4a0f0ca81b05d4447955b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f6f77ba990d2f1d8e157babf6d54e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1690ec30bfe428b2d45d7f6a2b72b27b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-927c2f85705ad1786cd676578edc30d8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CMT-A-Cascade-MAR-with-Topology-Predictor-for-Multimodal-Conditional-CAD-Generation"><a href="#CMT-A-Cascade-MAR-with-Topology-Predictor-for-Multimodal-Conditional-CAD-Generation" class="headerlink" title="CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional   CAD Generation"></a>CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional   CAD Generation</h2><p><strong>Authors:Jianyu Wu, Yizhou Wang, Xiangyu Yue, Xinzhu Ma, Jingyang Guo, Dongzhan Zhou, Wanli Ouyang, Shixiang Tang</strong></p>
<p>While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the &#96;&#96;edge-counters-surfaceâ€™â€™ priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰é¢†åŸŸï¼Œå‡†ç¡®ä¸”ç”¨æˆ·å‹å¥½çš„CADå¯¹äºå·¥ä¸šè®¾è®¡å’Œåˆ¶é€ è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰æ–¹æ³•çš„è¡¨ç¤ºè¿‡äºç®€åŒ–æˆ–æ¶æ„ä¸æ”¯æŒå¤šæ¨¡å¼è®¾è®¡éœ€æ±‚ï¼Œå› æ­¤ä»éš¾ä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°è¯•ä»æ–¹æ³•å’Œæ•°æ®é›†ä¸¤ä¸ªæ–¹é¢æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰çš„çº§è”MARä¸æ‹“æ‰‘é¢„æµ‹å™¨ï¼ˆCMTï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºCADç”Ÿæˆçš„å¤šæ¨¡å¼æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œçº§è”MARå¯ä»¥æœ‰æ•ˆåœ°æ•è·B-Repä¸­å¿…ä¸å¯å°‘çš„â€œè¾¹ç¼˜å¯¹è¡¨é¢â€çš„å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ‹“æ‰‘é¢„æµ‹å™¨åˆ™ç›´æ¥ä»MARçš„ç´§å‡‘æ ‡è®°ä¸­ä¼°è®¡B-Repçš„æ‹“æ‰‘ç»“æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†ä¿ƒè¿›å¤§è§„æ¨¡è®­ç»ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼CADæ•°æ®é›†mmABCï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡130ä¸‡ä¸ªå¸¦æœ‰å¤šæ¨¡å¼æ³¨é‡Šçš„B-Repæ¨¡å‹ï¼ŒåŒ…æ‹¬ç‚¹äº‘ã€æ–‡æœ¬æè¿°å’Œå¤šä¸ªå›¾åƒè§†å›¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCMTåœ¨æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„CADç”Ÿæˆä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ— æ¡ä»¶ç”Ÿæˆä¸­ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ABCä¸Šçš„è¦†ç›–ç‡å’Œæœ‰æ•ˆæ¯”ç‡åˆ†åˆ«æé«˜äº†+10.68%å’Œ+10.3%ã€‚åœ¨mmABCçš„å›¾åƒæ¡ä»¶CADç”Ÿæˆä¸­ï¼ŒCMTçš„Chamferå€¼æé«˜äº†+4.01ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20830v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰çš„å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…æ‹¬çº§è”çš„å¤šæ¨¡æ€æ¶æ„ï¼ˆCMTï¼‰å’Œæ‹“æ‰‘é¢„æµ‹å™¨ã€‚CMTèƒ½æœ‰æ•ˆæ•æ‰B-Repä¸­çš„â€œè¾¹ç¼˜è®¡æ•°è¡¨é¢â€å…ˆéªŒä¿¡æ¯ï¼Œè€Œæ‹“æ‰‘é¢„æµ‹å™¨åˆ™ç›´æ¥ä»CMTçš„ç´§å‡‘æ ‡è®°ä¸­ä¼°è®¡æ‹“æ‰‘ç»“æ„ã€‚æ­¤å¤–ï¼Œä¸ºæ”¯æŒå¤§è§„æ¨¡è®­ç»ƒï¼Œå¼€å‘äº†ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€CADæ•°æ®é›†mmABCï¼ŒåŒ…å«è¶…è¿‡130ä¸‡å¸¦æœ‰å¤šæ¨¡æ€æ³¨é‡Šçš„B-Repæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCMTåœ¨æ¡ä»¶å’Œæ— æ¡ä»¶CADç”Ÿæˆä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•è¿‡äºç®€åŒ–æˆ–æ— æ³•æ”¯æŒå¤šæ¨¡æ€è®¾è®¡çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«çº§è”çš„å¤šæ¨¡æ€æ¶æ„ï¼ˆCMTï¼‰å’Œæ‹“æ‰‘é¢„æµ‹å™¨ï¼Œèƒ½æœ‰æ•ˆæ•æ‰B-Repä¸­çš„å…³é”®å…ˆéªŒä¿¡æ¯å¹¶ä¼°è®¡æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>ä¸ºæ”¯æŒå¤§è§„æ¨¡è®­ç»ƒï¼Œå¼€å‘äº†ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€CADæ•°æ®é›†mmABCï¼ŒåŒ…å«ä¸°å¯Œçš„B-Repæ¨¡å‹å’Œå¤šç§æ¨¡æ€çš„æ³¨é‡Šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCMTåœ¨CADç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ¡ä»¶ç”Ÿæˆå’Œå›¾åƒæ¡ä»¶ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒCMTåœ¨è¦†ç›–ç‡ã€æœ‰æ•ˆç‡å’ŒChamferç­‰æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå·¥ä¸šè®¾è®¡å’Œåˆ¶é€ é¢†åŸŸæä¾›äº†æ›´å‡†ç¡®ã€ç”¨æˆ·å‹å¥½çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f7a9d209c5aaf79652c585157bec440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2936671fa0355c5c627dcddb597a9b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebabf71c915f3a2d002000ac57078350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529e224112a58816d56628dae200be6e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Aligning-Constraint-Generation-with-Design-Intent-in-Parametric-CAD"><a href="#Aligning-Constraint-Generation-with-Design-Intent-in-Parametric-CAD" class="headerlink" title="Aligning Constraint Generation with Design Intent in Parametric CAD"></a>Aligning Constraint Generation with Design Intent in Parametric CAD</h2><p><strong>Authors:Evan Casey, Tianyu Zhang, Shu Ishida, William P. McCarthy, John Roger Thompson, Amir Khasahmadi, Joseph George Lambourne, Pradeep Kumar Jayaraman, Karl D. D. Willis</strong></p>
<p>We adapt alignment techniques from reasoning LLMs to the task of generating engineering sketch constraints found in computer-aided design (CAD) models. Engineering sketches consist of geometric primitives (e.g. points, lines) connected by constraints (e.g. perpendicular, tangent) that define the relationships between them. For a design to be easily editable, the constraints must effectively capture design intent, ensuring the geometry updates predictably when parameters change. Although current approaches can generate CAD designs, an open challenge remains to align model outputs with design intent, we label this problem â€˜design alignmentâ€™. A critical first step towards aligning generative CAD models is to generate constraints which fully-constrain all geometric primitives, without over-constraining or distorting sketch geometry. Using alignment techniques to train an existing constraint generation model with feedback from a constraint solver, we are able to fully-constrain 93% of sketches compared to 34% when using a naive supervised fine-tuning (SFT) baseline and only 8.9% without SFT. Our approach can be applied to any existing constraint generation model and sets the stage for further research bridging alignment strategies between the language and design domains. Additional results can be found at <a target="_blank" rel="noopener" href="https://autodeskailab.github.io/aligning-constraint-generation/">https://autodeskailab.github.io/aligning-constraint-generation/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å°†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹é½æŠ€æœ¯åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹ä¸­å·¥ç¨‹è‰å›¾çº¦æŸçš„ç”Ÿæˆä»»åŠ¡ã€‚å·¥ç¨‹è‰å›¾ç”±å‡ ä½•å…ƒç´ ï¼ˆå¦‚ç‚¹ã€çº¿ï¼‰ç»„æˆï¼Œè¿™äº›å‡ ä½•å…ƒç´ é€šè¿‡çº¦æŸï¼ˆå¦‚å‚ç›´ã€ç›¸åˆ‡ï¼‰è¿æ¥ï¼Œå®šä¹‰äº†å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†ä½¿è®¾è®¡æ˜“äºç¼–è¾‘ï¼Œçº¦æŸå¿…é¡»æœ‰æ•ˆåœ°æ•æ‰è®¾è®¡æ„å›¾ï¼Œç¡®ä¿å‚æ•°æ›´æ”¹æ—¶å‡ ä½•æ›´æ–°èƒ½å¤Ÿé¢„æµ‹ã€‚å°½ç®¡å½“å‰çš„æ–¹æ³•å¯ä»¥ç”ŸæˆCADè®¾è®¡ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ï¼Œå³å°†æ¨¡å‹è¾“å‡ºä¸è®¾è®¡æ„å›¾å¯¹é½ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜ç§°ä¸ºâ€œè®¾è®¡å¯¹é½â€ã€‚å¯¹é½ç”Ÿæˆå¼CADæ¨¡å‹çš„å…³é”®ç¬¬ä¸€æ­¥æ˜¯ç”Ÿæˆçº¦æŸï¼Œè¿™äº›çº¦æŸå¯ä»¥å®Œå…¨çº¦æŸæ‰€æœ‰å‡ ä½•å…ƒç´ ï¼Œè€Œä¸ä¼šè¿‡åº¦çº¦æŸæˆ–æ‰­æ›²è‰å›¾çš„å‡ ä½•å½¢çŠ¶ã€‚ä½¿ç”¨å¯¹é½æŠ€æœ¯æ¥è®­ç»ƒç°æœ‰çš„çº¦æŸç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆæ¥è‡ªçº¦æŸæ±‚è§£å™¨çš„åé¦ˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¸93%çš„è‰å›¾è¿›è¡Œå¯¹é½çº¦æŸå»ºæ¨¡ã€‚ä¸ä½¿ç”¨åŸºæœ¬çš„ç›‘ç£å¾®è°ƒåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ˆå³ä½¿æœªç»ä»»ä½•ä¿®æ”¹ä¹Ÿæœªå®ç°æ‰€éœ€çš„è¡Œä¸ºç›®æ ‡ï¼‰ã€‚èƒ½å¤Ÿåœ¨å¤§éƒ¨åˆ†å¸¸è§„æ¡†æ¶çš„åŸºç¡€ä¸Šè¿›è¡Œåˆ›é€ ä¸”é’ˆå¯¹æ€§æä¾›å¾®æ”¹åŠå®æ“æ“ä½œæˆ–ä¸“é—¨è¾¾åˆ°æ‰€éœ€çš„ä¸šåŠ¡èƒ½åŠ›æ–¹é¢ä¼˜åŠ¿æ˜æ˜¾ç­‰å–å¾—äº†æ›´ä¼˜å¼‚çš„æˆç»©å æ¯”ç™¾åˆ†æ¯”å¹¶å……åˆ†å‘æŒ¥å¼ºåŒ–æŠ€æœ¯åœ¨è°ƒå‚è°ƒæ•´ç­‰é¢†åŸŸçš„ç»å¯¹æ€§ç²¾å‡†ç‰¹ç‚¹åœ¨é«˜è¾¾é«˜è¾¾ç™¾åˆ†ä¹‹ç™¾çš„æ•°æ®ä¸‹è¶…è¶Šä»¥å¾€çš„æ¨¡å‹åŸºå‡†çº¿è¡¨ç°å®ç°äº†æ–°çš„çªç ´ä»¥åŠæ›´åŠ ä¼˜åŒ–çš„æ¨¡å‹è¡¨ç°èƒ½åŠ›å¹¶åŒæ—¶å¤§å¹…æå‡äº†åœ¨è§£å†³ä¸åŒçº¦æŸæ¡ä»¶æ–¹é¢çš„é«˜åº¦é€‚é…èƒ½åŠ›åŒæ—¶æ›´å¤šçš„ç»“æœå¯ä»¥é€šè¿‡è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™é“¾æ¥è¿›è¡ŒæŸ¥çœ‹æ›´å¤šè¯¦ç»†ä¿¡æ¯<a target="_blank" rel="noopener" href="https://autodeskailab.github.io/aligning-constraint-generation/%E3%80%82">https://autodeskailab.github.io/aligning-constraint-generation/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13178v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¯¹é½æŠ€æœ¯åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å·¥ç¨‹è‰å›¾çº¦æŸç”Ÿæˆä»»åŠ¡ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå·¥ç¨‹è‰å›¾ç”±å‡ ä½•åŸºæœ¬å…ƒç´ å’Œå®šä¹‰å®ƒä»¬ä¹‹é—´å…³ç³»çš„çº¦æŸç»„æˆã€‚ä¸ºäº†è®©è®¾è®¡æ˜“äºç¼–è¾‘ï¼Œçº¦æŸå¿…é¡»æœ‰æ•ˆåœ°æ•æ‰è®¾è®¡æ„å›¾ï¼Œä»¥ç¡®ä¿å½“å‚æ•°å˜åŒ–æ—¶ï¼Œå‡ ä½•å½¢çŠ¶èƒ½å¤Ÿé¢„æµ‹æ›´æ–°ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•èƒ½å¤Ÿç”ŸæˆCADè®¾è®¡ï¼Œä½†å¦‚ä½•è®©æ¨¡å‹è¾“å‡ºä¸è®¾è®¡æ„å›¾å¯¹é½ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ–‡ç« é‡‡ç”¨å¯¹é½æŠ€æœ¯è®­ç»ƒç°æœ‰çš„çº¦æŸç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶ä¸çº¦æŸæ±‚è§£å™¨æä¾›çš„åé¦ˆå¯¹é½ï¼Œä»è€Œèƒ½å¤Ÿå…¨é¢çº¦æŸå¤§éƒ¨åˆ†è‰å›¾ï¼Œç›¸æ¯”ä½¿ç”¨ç®€å•çš„æœ‰ç›‘ç£å¾®è°ƒåŸºçº¿æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ–¹æ³•å¯åº”ç”¨äºä»»ä½•ç°æœ‰çš„çº¦æŸç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä¸ºè¯­è¨€å’Œè®¾è®¡é¢†åŸŸä¹‹é—´çš„å¯¹é½ç­–ç•¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ç¨‹è‰å›¾ç”±å‡ ä½•åŸºæœ¬å…ƒç´ å’Œçº¦æŸç»„æˆï¼Œçº¦æŸéœ€æœ‰æ•ˆæ•æ‰è®¾è®¡æ„å›¾ä»¥ç¡®ä¿è®¾è®¡çš„å¯ç¼–è¾‘æ€§å’Œé¢„æµ‹æ€§ã€‚</li>
<li>å½“å‰CADè®¾è®¡é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯æ¨¡å‹è¾“å‡ºä¸è®¾è®¡æ„å›¾çš„å¯¹é½é—®é¢˜ï¼Œç§°ä¸ºâ€œè®¾è®¡å¯¹é½â€ã€‚</li>
<li>é‡‡ç”¨å¯¹é½æŠ€æœ¯è®­ç»ƒç°æœ‰çš„çº¦æŸç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå…¨é¢çº¦æŸè‰å›¾ï¼Œæé«˜çº¦æŸç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ç®€å•çš„æœ‰ç›‘ç£å¾®è°ƒåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å®Œæˆçº¦æŸç”Ÿæˆä»»åŠ¡ï¼Œå…¨é¢çº¦æŸ93%çš„è‰å›¾ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•å¯åº”ç”¨äºä»»ä½•ç°æœ‰çš„çº¦æŸç”Ÿæˆæ¨¡å‹ï¼Œä¸ºè¯­è¨€å’Œè®¾è®¡é¢†åŸŸçš„å¯¹é½ç­–ç•¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</li>
<li>é€šè¿‡ä¸çº¦æŸæ±‚è§£å™¨çš„åé¦ˆç»“åˆï¼Œèƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶æ›´å¥½åœ°æ•æ‰è®¾è®¡æ„å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da61a4e2075098d4f69ba629d86f9417.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48986fd5824557e25105f2d338476197.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a58a757158400bbedfe3f3f39b0fa264.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1509b1bce45553f04386f050cbf65a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Nonlocal-Retinex-Based-Variational-Model-and-its-Deep-Unfolding-Twin-for-Low-Light-Image-Enhancement"><a href="#Nonlocal-Retinex-Based-Variational-Model-and-its-Deep-Unfolding-Twin-for-Low-Light-Image-Enhancement" class="headerlink" title="Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for   Low-Light Image Enhancement"></a>Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for   Low-Light Image Enhancement</h2><p><strong>Authors:Daniel Torres, Joan Duran, Julia Navarro, Catalina Sbert</strong></p>
<p>Images captured under low-light conditions present significant limitations in many applications, as poor lighting can obscure details, reduce contrast, and hide noise. Removing the illumination effects and enhancing the quality of such images is crucial for many tasks, such as image segmentation and object detection. In this paper, we propose a variational method for low-light image enhancement based on the Retinex decomposition into illumination, reflectance, and noise components. A color correction pre-processing step is applied to the low-light image, which is then used as the observed input in the decomposition. Moreover, our model integrates a novel nonlocal gradient-type fidelity term designed to preserve structural details. Additionally, we propose an automatic gamma correction module. Building on the proposed variational approach, we extend the model by introducing its deep unfolding counterpart, in which the proximal operators are replaced with learnable networks. We propose cross-attention mechanisms to capture long-range dependencies in both the nonlocal prior of the reflectance and the nonlocal gradient-based constraint. Experimental results demonstrate that both methods compare favorably with several recent and state-of-the-art techniques across different datasets. In particular, despite not relying on learning strategies, the variational model outperforms most deep learning approaches both visually and in terms of quality metrics. </p>
<blockquote>
<p>åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ•è·çš„å›¾åƒåœ¨è®¸å¤šåº”ç”¨ä¸­å­˜åœ¨é‡å¤§å±€é™ï¼Œå› ä¸ºå…‰çº¿ä¸ä½³å¯èƒ½ä¼šæ©ç›–ç»†èŠ‚ã€é™ä½å¯¹æ¯”åº¦å’Œéšè—å™ªå£°ã€‚æ¶ˆé™¤ç…§æ˜æ•ˆæœå¹¶æå‡æ­¤ç±»å›¾åƒçš„è´¨é‡å¯¹äºè®¸å¤šä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ï¼‰è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºRetinexåˆ†è§£ï¼ˆåˆ†è§£ä¸ºç…§æ˜ã€åå°„å’Œå™ªå£°æˆåˆ†ï¼‰çš„ä½å…‰ç…§å›¾åƒå¢å¼ºæ–¹æ³•ã€‚å¯¹ä½å…‰ç…§å›¾åƒåº”ç”¨äº†é¢œè‰²æ ¡æ­£é¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åå°†å…¶ä½œä¸ºè§‚å¯Ÿè¾“å…¥ç”¨äºåˆ†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é›†æˆäº†ä¸€ç§æ–°å‹çš„éå±€éƒ¨æ¢¯åº¦å‹ä¿çœŸåº¦é¡¹ï¼Œæ—¨åœ¨ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—ã€‚åŸºäºæ‰€æå‡ºçš„å˜åˆ†æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å…¶æ·±åº¦å±•å¼€å¯¹åº”ç‰©æ¥æ‰©å±•æ¨¡å‹ï¼Œå…¶ä¸­è¿‘ç«¯è¿ç®—ç¬¦è¢«å¯å­¦ä¹ çš„ç½‘ç»œæ‰€æ›¿ä»£ã€‚æˆ‘ä»¬æå‡ºäº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ•è·åå°„çš„éå±€éƒ¨å…ˆéªŒå’ŒåŸºäºéæ¢¯åº¦çš„çº¦æŸä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•å‡ä¼˜äºä¸åŒæ•°æ®é›†ä¸Šçš„æœ€æ–°å…ˆè¿›æŠ€æœ¯ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå°½ç®¡ä¸ä¾èµ–å­¦ä¹ ç­–ç•¥ï¼Œå˜åˆ†æ¨¡å‹åœ¨è§†è§‰å’Œè´¨é‡æŒ‡æ ‡æ–¹é¢å‡ä¼˜äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07810v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºRetinexåˆ†è§£çš„ä½å…‰ç…§å›¾åƒå¢å¼ºæ–¹æ³•ï¼ŒåŒ…æ‹¬å…‰ç…§ã€åå°„å’Œå™ªå£°æˆåˆ†ã€‚é‡‡ç”¨è‰²å½©æ ¡æ­£é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶ç»“åˆæ–°å‹éå±€éƒ¨æ¢¯åº¦ä¿çœŸé¡¹ï¼Œä»¥ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚åŒæ—¶å¼•å…¥è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ·±åº¦å±•å¼€æ¨¡å‹ï¼Œå°†è¿‘ç«¯ç®—å­æ›¿æ¢ä¸ºå¯å­¦ä¹ ç½‘ç»œï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•è·åå°„éå±€éƒ¨å…ˆéªŒå’ŒåŸºäºæ¢¯åº¦çš„çº¦æŸçš„é•¿æœŸä¾èµ–æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå¤šç§æœ€æ–°å…ˆè¿›æŠ€æœ¯åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å…‰ç…§å›¾åƒåœ¨å¤šç§åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºRetinexåˆ†è§£çš„ä½å…‰ç…§å›¾åƒå¢å¼ºæ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨è‰²å½©æ ¡æ­£é¢„å¤„ç†æ­¥éª¤ï¼Œä¸ºåç»­çš„å›¾åƒåˆ†è§£æä¾›åŸºç¡€ã€‚</li>
<li>æ–°å‹éå±€éƒ¨æ¢¯åº¦ä¿çœŸé¡¹è®¾è®¡ç”¨äºä¿ç•™ç»“æ„ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥è‡ªåŠ¨ä¼½é©¬æ ¡æ­£æ¨¡å—ä»¥å¢å¼ºå›¾åƒè´¨é‡ã€‚</li>
<li>é€šè¿‡æ·±åº¦å±•å¼€æ¨¡å‹å’Œéå­¦ä¹ ç­–ç•¥çš„å¼•å…¥ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cfab28bc8cc2462f36685dbcd169c91.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GEMA-Score-Granular-Explainable-Multi-Agent-Scoring-Framework-for-Radiology-Report-Evaluation"><a href="#GEMA-Score-Granular-Explainable-Multi-Agent-Scoring-Framework-for-Radiology-Report-Evaluation" class="headerlink" title="GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for   Radiology Report Evaluation"></a>GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for   Radiology Report Evaluation</h2><p><strong>Authors:Zhenxuan Zhang, Kinhei Lee, Peiyuan Jing, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C Marshall, Yingying Fang, Guang Yang</strong></p>
<p>Automatic medical report generation has the potential to support clinical diagnosis, reduce the workload of radiologists, and demonstrate potential for enhancing diagnostic consistency. However, current evaluation metrics often fail to reflect the clinical reliability of generated reports. Early overlap-based methods focus on textual matches between predicted and ground-truth entities but miss fine-grained clinical details (e.g., anatomical location, severity). Some diagnostic metrics are limited by fixed vocabularies or templates, reducing their ability to capture diverse clinical expressions. LLM-based approaches further lack interpretable reasoning steps, making it hard to assess or trust their behavior in safety-critical settings. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs stable calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient &#x3D; $0.69$ for ReXVal dataset and Kendall coefficient &#x3D; $0.45$ for RadEvalX dataset). The anonymous project demo is available at: <a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/GEMA_score">https://github.com/Zhenxuan-Zhang/GEMA_score</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå…·æœ‰æ”¯æŒä¸´åºŠè¯Šæ–­ã€å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œé‡å’Œæé«˜è¯Šæ–­ä¸€è‡´æ€§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€ä¸èƒ½åæ˜ ç”ŸæˆæŠ¥å‘Šçš„ä¸´åºŠå¯é æ€§ã€‚æ—©æœŸåŸºäºé‡å çš„æ–¹æ³•ä¸“æ³¨äºé¢„æµ‹å’ŒçœŸå®å®ä½“ä¹‹é—´çš„æ–‡æœ¬åŒ¹é…ï¼Œä½†å¿½ç•¥äº†ç»†å¾®çš„ä¸´åºŠç»†èŠ‚ï¼ˆå¦‚è§£å‰–ä½ç½®ã€ä¸¥é‡ç¨‹åº¦ï¼‰ã€‚ä¸€äº›è¯Šæ–­æŒ‡æ ‡å—é™äºå›ºå®šçš„è¯æ±‡æˆ–æ¨¡æ¿ï¼Œé™ä½äº†å®ƒä»¬æ•æ‰å„ç§ä¸´åºŠè¡¨è¾¾çš„èƒ½åŠ›ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•è¿˜ç¼ºä¹å¯è§£é‡Šçš„æ¨ç†æ­¥éª¤ï¼Œä½¿å¾—åœ¨å…³é”®å®‰å…¨ç¯å¢ƒä¸­éš¾ä»¥è¯„ä¼°æˆ–ä¿¡ä»»å®ƒä»¬çš„è¡Œä¸ºã€‚è¿™äº›å±€é™æ€§é˜»ç¢äº†å…¨é¢è¯„ä¼°ç”ŸæˆæŠ¥å‘Šçš„å¯ä¿¡åº¦ï¼Œå¹¶åœ¨å…¶ä¸´åºŠä½¿ç”¨é€‰æ‹©ä¸­æ„æˆé£é™©ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Granular Explainable Multi-Agent Scoreï¼ˆGEMA-Scoreï¼‰ï¼Œå®ƒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹è¿›è¡Œå®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·ã€‚æˆ‘ä»¬çš„GEMA-Scoreè§£æç»“æ„åŒ–æŠ¥å‘Šï¼Œå¹¶é€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡æ¯äº¤äº’äº¤æµè¿›è¡Œç¨³å®šè®¡ç®—ï¼Œä»¥è¯„ä¼°ç–¾ç—…è¯Šæ–­ã€ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„è¯„åˆ†æ™ºèƒ½ä½“è¯„ä¼°å®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­ï¼ŒåŒæ—¶æä¾›è§£é‡Šæ€§åé¦ˆã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒGEMA-Scoreåœ¨å…¬å…±æ•°æ®é›†ä¸Šä¸äººç±»ä¸“å®¶è¯„ä»·çš„å…³è”æ€§æœ€é«˜ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠè¯„åˆ†ä¸­çš„æœ‰æ•ˆæ€§ï¼ˆReXValæ•°æ®é›†çš„Kendallç³»æ•°ä¸º0.69ï¼ŒRadEvalXæ•°æ®é›†çš„Kendallç³»æ•°ä¸º0.45ï¼‰ã€‚åŒ¿åé¡¹ç›®æ¼”ç¤ºç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/GEMA_score%E3%80%82">https://github.com/Zhenxuan-Zhang/GEMA_scoreã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05347v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGranular Explainable Multi-Agent Scoreï¼ˆGEMA-Scoreï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„è¯„ä»·ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·ï¼Œé€šè¿‡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹æ¥è¯„ä¼°ç–¾ç—…è¯Šæ–­ã€ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ã€‚åŒæ—¶ï¼Œå¯¹æŠ¥å‘Šå®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æä¾›è§£é‡Šæ€§åé¦ˆã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒGEMA-Scoreåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä¸“å®¶è¯„ä»·é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰åŒ»å­¦æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆçš„è¯„ä»·æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å…¨é¢åæ˜ æŠ¥å‘Šçš„å¯é æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶GEMA-Scoreï¼Œç»“åˆå®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·æ¥å…¨é¢è¯„ä¼°æŠ¥å‘Šè´¨é‡ã€‚</li>
<li>åˆ©ç”¨å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹æ¥è§£æç»“æ„åŒ–æŠ¥å‘Šå¹¶è¯„ä¼°è¯Šæ–­ã€ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ç­‰æ–¹é¢ã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†å’Œåé¦ˆï¼Œæé«˜æŠ¥å‘Šçš„å®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­çš„ä½¿ç”¨ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºGEMA-Scoreä¸ä¸“å®¶è¯„ä»·é«˜åº¦ä¸€è‡´ï¼Œä¸”å…¬å¼€é¡¹ç›®æ¼”ç¤ºå¯ä¾›å‚è€ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5cec7857e44c3e06c744ea6a320f6307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4acc7f27ca26cfa6e13aecec4060ed7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f7e3cc3674af626017e5066a3ad6c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b4c367807dbf99047643dfb1dfd90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc56595190a6051b3adbef8b7148cc03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f01ca8b223579eee6057c1a68753cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b68d8c8d0d0adf5104d1978644f5faeb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Style-Content-Decomposition-based-Data-Augmentation-for-Domain-Generalizable-Medical-Image-Segmentation"><a href="#Style-Content-Decomposition-based-Data-Augmentation-for-Domain-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Style Content Decomposition-based Data Augmentation for Domain   Generalizable Medical Image Segmentation"></a>Style Content Decomposition-based Data Augmentation for Domain   Generalizable Medical Image Segmentation</h2><p><strong>Authors:Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane, Zhaolin Chen</strong></p>
<p>Due to domain shifts across diverse medical imaging modalities, learned segmentation models often suffer significant performance degradation during deployment. These domain shifts, typically caused by variations in imaging systems, generally comprise two principal components: 1) \textbf{â€œstyleâ€ shifts}, referring to global disparities in image properties such as illumination, contrast, and color; and 2) \textbf{â€œcontentâ€ shifts}, which involve local discrepancies in anatomical structures. To address domain shifts in medical image segmentation, a core challenge arises: how can we decouple the factors within images that determine their â€œstyleâ€ and â€œcontentâ€ components? To this end, we first propose a linear style-content decomposition method that factorizes an image into style codes and content maps, explicitly modeling the â€œstyleâ€ and â€œcontentâ€ components. Building on this, we introduce a \textbf{Sty}le-\textbf{Con}tent decomposition-based data \textbf{a}ugmentation algorithm (StyCona), which leverages this decomposition strategy to guide augmentation of both the global style and local content of source-domain images, enabling the training of a well-generalized model for domain-generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to segmentation model architectures. Experiments on cardiac magnetic resonance imaging and fundus photography segmentation tasks, with single and multiple target domains respectively, demonstrate the effectiveness of StyCona and its superiority over state-of-the-art domain generalization methods. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/Senyh/StyCona">https://github.com/Senyh/StyCona</a>. </p>
<blockquote>
<p>ç”±äºè·¨ä¸åŒåŒ»å­¦å½±åƒæ¨¡å¼ï¼ˆmedical imaging modalitiesï¼‰çš„åŸŸè¿ç§»ï¼ˆdomain shiftsï¼‰ï¼Œå·²è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹åœ¨éƒ¨ç½²æœŸé—´é€šå¸¸ä¼šé­å—æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚è¿™äº›åŸŸè¿ç§»é€šå¸¸ç”±æˆåƒç³»ç»Ÿçš„å˜åŒ–æ‰€å¯¼è‡´ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æˆåˆ†ï¼š1ï¼‰<strong>â€œé£æ ¼â€ï¼ˆstyleï¼‰è½¬å˜</strong>ï¼ŒæŒ‡çš„æ˜¯å›¾åƒå±æ€§å¦‚å…‰ç…§ã€å¯¹æ¯”åº¦å’Œé¢œè‰²çš„å…¨å±€å·®å¼‚ï¼›å’Œ2ï¼‰<strong>â€œå†…å®¹â€ï¼ˆcontentï¼‰è½¬å˜</strong>ï¼Œæ¶‰åŠè§£å‰–ç»“æ„çš„å±€éƒ¨å·®å¼‚ã€‚ä¸ºè§£å†³åŒ»å­¦å½±åƒåˆ†å‰²ä¸­çš„åŸŸè¿ç§»é—®é¢˜ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå¦‚ä½•å‰¥ç¦»å›¾åƒä¸­çš„å› ç´ æ¥åˆ†ç¦»å…¶â€œé£æ ¼â€å’Œâ€œå†…å®¹â€æˆåˆ†ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§çº¿æ€§é£æ ¼-å†…å®¹åˆ†è§£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å›¾åƒåˆ†è§£ä¸ºé£æ ¼ä»£ç å’Œå†…å®¹å›¾ï¼Œæ˜ç¡®åœ°å»ºæ¨¡â€œé£æ ¼â€å’Œâ€œå†…å®¹â€æˆåˆ†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé£æ ¼-å†…å®¹åˆ†è§£çš„æ•°æ®å¢å¼ºç®—æ³•ï¼ˆStyConaï¼‰ï¼Œè¯¥ç®—æ³•åˆ©ç”¨è¿™ç§åˆ†è§£ç­–ç•¥æ¥æŒ‡å¯¼æºåŸŸå›¾åƒçš„å…¨å±€é£æ ¼å’Œå±€éƒ¨å†…å®¹çš„å¢å¼ºï¼Œèƒ½å¤Ÿè®­ç»ƒå‡ºå¯¹åŒ»å­¦å½±åƒåˆ†å‰²è¿›è¡Œé¢†åŸŸä¸€èˆ¬åŒ–çš„é€šç”¨æ¨¡å‹ã€‚StyConaæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œå¯åœ¨æ— éœ€é¢å¤–çš„è®­ç»ƒå‚æ•°æˆ–å¯¹åˆ†å‰²æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œå¤§å¹…æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å¿ƒè„ç£å…±æŒ¯æˆåƒå’Œçœ¼åº•æ‘„å½±åˆ†å‰²ä»»åŠ¡è¿›è¡Œå®éªŒéªŒè¯ï¼Œæ¶‰åŠå•ç›®æ ‡å’Œå¤šç›®æ ‡åŸŸï¼Œè¯æ˜äº†StyConaçš„æœ‰æ•ˆæ€§åŠå…¶ç›¸è¾ƒäºæœ€æ–°é¢†åŸŸæ³›åŒ–æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Senyh/StyCona%E3%80%82">https://github.com/Senyh/StyConaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20619v2">PDF</a> </p>
<p><strong>Summary</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨éƒ¨ç½²æ—¶ï¼Œå› ä¸åŒæˆåƒæ¨¡æ€çš„åŸŸå˜è¿å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ–‡ç« æå‡ºä¸€ç§çº¿æ€§é£æ ¼-å†…å®¹åˆ†è§£æ–¹æ³•ï¼Œå°†å›¾åƒåˆ†è§£ä¸ºé£æ ¼ç å’Œå†…å®¹å›¾ï¼Œä»¥å»ºæ¨¡â€œé£æ ¼â€å’Œâ€œå†…å®¹â€ç»„ä»¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥åŸºäºé£æ ¼-å†…å®¹åˆ†è§£çš„æ•°æ®å¢å¼ºç®—æ³•ï¼ˆStyConaï¼‰ï¼Œé€šè¿‡å¼•å¯¼æºåŸŸå›¾åƒå…¨å±€é£æ ¼å’Œå±€éƒ¨å†…å®¹çš„å¢å¼ºï¼Œè®­ç»ƒé€šç”¨æ€§å¼ºçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚StyConaæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å³æ’å³ç”¨æ¨¡å—ï¼Œå¯æ˜¾è‘—æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€å¢åŠ è®­ç»ƒå‚æ•°æˆ–å¯¹åˆ†å‰²æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨éƒ¨ç½²æ—¶é¢ä¸´åŸŸå˜è¿é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>åŸŸå˜è¿ä¸»è¦åŒ…æ‹¬â€œé£æ ¼â€å’Œâ€œå†…å®¹â€ä¸¤ä¸ªä¸»è¦ç»„ä»¶ã€‚</li>
<li>çº¿æ€§é£æ ¼-å†…å®¹åˆ†è§£æ–¹æ³•è¢«æå‡ºï¼Œä»¥å»ºæ¨¡å›¾åƒçš„â€œé£æ ¼â€å’Œâ€œå†…å®¹â€ç»„ä»¶ã€‚</li>
<li>åŸºäºé£æ ¼-å†…å®¹åˆ†è§£çš„æ•°æ®å¢å¼ºç®—æ³•ï¼ˆStyConaï¼‰è¢«ä»‹ç»ï¼Œç”¨äºå¢å¼ºæºåŸŸå›¾åƒçš„å…¨å±€é£æ ¼å’Œå±€éƒ¨å†…å®¹ã€‚</li>
<li>StyConaæ¨¡å—æé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ˜“äºå®æ–½ã€‚</li>
<li>StyConaåœ¨å¿ƒè„ç£å…±æŒ¯æˆåƒå’Œçœ¼åº•æ‘„å½±åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ccac1ba72d0fe8ebfdcfdc1ed539f5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-146114a8178d3855ac9dfa9a538a63f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b335ba1fbaf045f184dae3f6833491c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35d66b67065a84bec8061d4fa768b78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2660bb93cb15eeef0f879e4add29ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c88e70c727d7b6c22c8a02467a830ba.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Vision-without-Images-End-to-End-Computer-Vision-from-Single-Compressive-Measurements"><a href="#Vision-without-Images-End-to-End-Computer-Vision-from-Single-Compressive-Measurements" class="headerlink" title="Vision without Images: End-to-End Computer Vision from Single   Compressive Measurements"></a>Vision without Images: End-to-End Computer Vision from Single   Compressive Measurements</h2><p><strong>Authors:Fengpu Pan, Heting Gao, Jiangtao Wen, Yuxing Han</strong></p>
<p>Snapshot Compressed Imaging (SCI) offers high-speed, low-bandwidth, and energy-efficient image acquisition, but remains challenged by low-light and low signal-to-noise ratio (SNR) conditions. Moreover, practical hardware constraints in high-resolution sensors limit the use of large frame-sized masks, necessitating smaller, hardware-friendly designs. In this work, we present a novel SCI-based computer vision framework using pseudo-random binary masks of only 8$\times$8 in size for physically feasible implementations. At its core is CompDAE, a Compressive Denoising Autoencoder built on the STFormer architecture, designed to perform downstream tasksâ€“such as edge detection and depth estimationâ€“directly from noisy compressive raw pixel measurements without image reconstruction. CompDAE incorporates a rate-constrained training strategy inspired by BackSlash to promote compact, compressible models. A shared encoder paired with lightweight task-specific decoders enables a unified multi-task platform. Extensive experiments across multiple datasets demonstrate that CompDAE achieves state-of-the-art performance with significantly lower complexity, especially under ultra-low-light conditions where traditional CMOS and SCI pipelines fail. </p>
<blockquote>
<p>å¿«ç…§å‹ç¼©æˆåƒï¼ˆSCIï¼‰æä¾›é«˜é€Ÿã€ä½å¸¦å®½å’ŒèŠ‚èƒ½çš„å›¾åƒé‡‡é›†ï¼Œä½†ä»é¢ä¸´ä½å…‰ç…§å’Œä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œé«˜åˆ†è¾¨ç‡ä¼ æ„Ÿå™¨çš„å®é™…ç¡¬ä»¶çº¦æŸé™åˆ¶äº†å¤§å‹å¸§å°ºå¯¸æ©è†œçš„ä½¿ç”¨ï¼Œéœ€è¦æ›´å°ã€æ›´å‹å¥½çš„ç¡¬ä»¶è®¾è®¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSCIçš„æ–°å‹è®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œä»…ä½¿ç”¨8x8å¤§å°çš„ä¼ªéšæœºäºŒè¿›åˆ¶æ©è†œï¼Œç”¨äºå®ç°ç‰©ç†å¯è¡Œçš„å®ç°ã€‚å…¶æ ¸å¿ƒæ˜¯CompDAEï¼Œä¸€ç§åŸºäºSTFormeræ¶æ„çš„å‹ç¼©å»å™ªè‡ªç¼–ç å™¨ï¼Œæ—¨åœ¨ç›´æ¥ä»å™ªå£°å‹ç¼©çš„åŸå§‹åƒç´ æµ‹é‡å€¼æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚è¾¹ç¼˜æ£€æµ‹å’Œæ·±åº¦ä¼°è®¡ï¼Œè€Œæ— éœ€è¿›è¡Œå›¾åƒé‡å»ºã€‚CompDAEé‡‡ç”¨äº†ä¸€ç§å—BackSlashå¯å‘çš„é€Ÿç‡çº¦æŸè®­ç»ƒç­–ç•¥ï¼Œä»¥ä¿ƒè¿›ç´§å‡‘ã€å¯å‹ç¼©çš„æ¨¡å‹ã€‚ä¸€ä¸ªä¸è½»é‡çº§ä»»åŠ¡ç‰¹å®šè§£ç å™¨é…å¯¹å…±äº«ç¼–ç å™¨ï¼Œå®ç°äº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡å¹³å°ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCompDAEåœ¨å¤æ‚åº¦æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ ç»ŸCMOSå’ŒSCIç®¡é“å¤±è´¥çš„è¶…ä½å…‰ç…§æ¡ä»¶ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15122v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSnapshot Compressed Imagingï¼ˆSCIï¼‰çš„æ–°å‹è®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œä½¿ç”¨å¤§å°ä¸º8Ã—8çš„ä¼ªéšæœºäºŒè¿›åˆ¶æ©è†œï¼Œé€‚ç”¨äºç‰©ç†å¯è¡Œå®ç°ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒä¸ºCompDAEï¼Œä¸€ç§åŸºäºSTFormeræ¶æ„çš„å‹ç¼©å»å™ªè‡ªç¼–ç å™¨ï¼Œå¯ç›´æ¥ä»å¸¦å™ªå£°çš„å‹ç¼©åŸå§‹åƒç´ æµ‹é‡å€¼æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚è¾¹ç¼˜æ£€æµ‹å’Œæ·±åº¦ä¼°è®¡ï¼Œè€Œæ— éœ€å›¾åƒé‡å»ºã€‚CompDAEé‡‡ç”¨å—BackSlashå¯å‘çš„é€Ÿç‡çº¦æŸè®­ç»ƒç­–ç•¥ï¼Œä¿ƒè¿›ç´§å‡‘ã€å¯å‹ç¼©çš„æ¨¡å‹ã€‚é€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œè¯æ˜CompDAEåœ¨è¶…ä½å…‰æ¡ä»¶ä¸‹å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯ä¼ ç»ŸCMOSå’ŒSCIç®¡é“å¤±æ•ˆçš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Snapshot Compressed Imaging (SCI) æä¾›é«˜é€Ÿã€ä½å¸¦å®½å’ŒèŠ‚èƒ½çš„å›¾åƒé‡‡é›†ã€‚</li>
<li>SCIé¢ä¸´ä½å…‰å’Œä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºSCIçš„è®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œä½¿ç”¨å°å°ºå¯¸çš„ä¼ªéšæœºäºŒè¿›åˆ¶æ©è†œï¼ˆ8Ã—8ï¼‰ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒä¸ºCompDAEï¼Œä¸€ç§å‹ç¼©å»å™ªè‡ªç¼–ç å™¨ï¼Œå¯ç›´æ¥ä»å¸¦å™ªå£°çš„å‹ç¼©åƒç´ æµ‹é‡å€¼æ‰§è¡Œä»»åŠ¡ã€‚</li>
<li>CompDAEé‡‡ç”¨å—BackSlashå¯å‘çš„é€Ÿç‡çº¦æŸè®­ç»ƒç­–ç•¥ã€‚</li>
<li>è·¨å¤šä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜CompDAEæ€§èƒ½å“è¶Šï¼Œå°¤å…¶åœ¨è¶…ä½å…‰æ¡ä»¶ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a27ac03491a22b5da5d8862c268013e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4316485a37686bfba0c06b7bc72d6367.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e50352279a95e3f963f3c217b1c61a5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6eca8c36554ad573a9576ae93ae01c08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcbefaf53352d50fa043539faa942dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6118d9a9b753f030e3eefd4fb4529fdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82e51914c41cbfe2bde482334c7e0678.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-213da45fdb14acf2222377610d962c58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cce3e8a806481f3cbc5715fd0a8484d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-11  CX-Mind A Pioneering Multimodal Large Language Model for Interleaved   Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9acde00b43fc0b1fe920d46825989a98.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-10  Scaling LLM Planning NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
