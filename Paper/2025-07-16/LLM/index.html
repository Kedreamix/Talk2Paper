<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Streaming 4D Visual Geometry Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-ee0423bb158a1a1596661f062511dc8e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="Streaming-4D-Visual-Geometry-Transformer"><a href="#Streaming-4D-Visual-Geometry-Transformer" class="headerlink" title="Streaming 4D Visual Geometry Transformer"></a>Streaming 4D Visual Geometry Transformer</h2><p><strong>Authors:Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, Jiwen Lu</strong></p>
<p>Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/wzzheng/StreamVGGT">https://github.com/wzzheng/StreamVGGT</a>. </p>
<blockquote>
<p>ä»è§†é¢‘ä¸­æ„ŸçŸ¥å¹¶é‡å»º4Dæ—¶ç©ºå‡ ä½•ä½“æ˜¯ä¸€é¡¹åŸºç¡€ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ä¸ºäº†ä¿ƒè¿›äº¤äº’å’Œå®æ—¶åº”ç”¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æµå¼4Dè§†è§‰å‡ ä½•è½¬æ¢å™¨ï¼Œå…¶ç†å¿µä¸å¤§å‹è‡ªå›å½’è¯­è¨€æ¨¡å‹ç›¸ä¼¼ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„è®¾è®¡ï¼Œå¹¶é‡‡ç”¨å› æœè½¬æ¢å™¨æ¶æ„ä»¥åœ¨çº¿æ–¹å¼å¤„ç†è¾“å…¥åºåˆ—ã€‚æˆ‘ä»¬ä½¿ç”¨æ—¶é—´å› æœæ³¨æ„åŠ›ï¼Œå¹¶ç¼“å­˜å†å²å¯†é’¥å’Œå€¼ä½œä¸ºéšå¼å†…å­˜ï¼Œä»¥å®ç°é«˜æ•ˆçš„æµå¼ä¼ è¾“é•¿æœŸ4Dé‡å»ºã€‚è¿™ç§è®¾è®¡å¯ä»¥é€šè¿‡å¢é‡æ•´åˆå†å²ä¿¡æ¯æ¥å¤„ç†å®æ—¶4Dé‡å»ºï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç©ºé—´ä¸€è‡´æ€§ã€‚ä¸ºäº†é«˜æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºä»å¯†é›†çš„åŒå‘è§†è§‰å‡ ä½•æ¥åœ°è½¬æ¢å™¨ï¼ˆVGGTï¼‰ä¸­æç‚¼çŸ¥è¯†åˆ°æˆ‘ä»¬çš„å› æœæ¨¡å‹ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒå°†ä¼˜åŒ–åçš„é«˜æ•ˆæ³¨æ„åŠ›è¿ç®—ç¬¦ï¼ˆä¾‹å¦‚FlashAttentionï¼‰ä»å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸè¿ç§»åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚åœ¨å„ç§4Då‡ ä½•æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æé«˜äº†åœ¨çº¿åœºæ™¯ä¸­çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›ï¼Œä¸ºå¯æ‰©å±•å’Œäº¤äº’å¼çš„4Dè§†è§‰ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/wzzheng/StreamVGGT%E3%80%82">https://github.com/wzzheng/StreamVGGTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11539v1">PDF</a> Code is available at: <a target="_blank" rel="noopener" href="https://github.com/wzzheng/StreamVGGT">https://github.com/wzzheng/StreamVGGT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æµå¼4Dè§†è§‰å‡ ä½•è½¬æ¢å™¨ï¼Œé‡‡ç”¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç±»ä¼¼çš„å“²å­¦ç†å¿µï¼Œç”¨äºä»è§†é¢‘ä¸­æ„ŸçŸ¥å’Œé‡å»º4Dæ—¶ç©ºå‡ ä½•ã€‚é€šè¿‡é‡‡ç”¨å› æœè½¬æ¢å™¨æ¶æ„è¿›è¡Œåœ¨çº¿å¤„ç†è¾“å…¥åºåˆ—ï¼Œç»“åˆæ—¶é—´å› æœæ³¨æ„åŠ›å’Œç¼“å­˜å†å²é”®å€¼å¯¹éšå¼å†…å­˜ï¼Œå®ç°é«˜æ•ˆçš„æµå¼é•¿æœŸ4Dé‡å»ºã€‚é€šè¿‡å¢é‡æ•´åˆå†å²ä¿¡æ¯ï¼Œæ¨¡å‹èƒ½åœ¨ä¿æŒé«˜è´¨é‡ç©ºé—´ä¸€è‡´æ€§çš„åŒæ—¶è¿›è¡Œå®æ—¶4Dé‡å»ºã€‚é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œä»å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸè¿ç§»ä¼˜åŒ–åçš„é«˜æ•ˆæ³¨æ„ç®—å­ï¼Œæ¨¡å‹åœ¨åœ¨çº¿åœºæ™¯ä¸­çš„æ¨ç†é€Ÿåº¦å¾—åˆ°æå‡ï¼ŒåŒæ—¶åœ¨å„ç§4Då‡ ä½•æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ï¼Œä¸ºå¯æ‰©å±•å’Œäº¤äº’å¼çš„4Dè§†è§‰ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°çš„æµå¼4Dè§†è§‰å‡ ä½•è½¬æ¢å™¨ï¼Œå¯ä»¥å¤„ç†è§†é¢‘ä¸­çš„4Dæ—¶ç©ºå‡ ä½•ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨å› æœè½¬æ¢å™¨æ¶æ„è¿›è¡Œåœ¨çº¿å¤„ç†è¾“å…¥åºåˆ—ï¼Œå®ç°å®æ—¶å¤„ç†ã€‚</li>
<li>é€šè¿‡æ—¶é—´å› æœæ³¨æ„åŠ›å’Œéšå¼å†…å­˜å®ç°é«˜æ•ˆæµå¼é•¿æœŸ4Dé‡å»ºã€‚</li>
<li>æ¨¡å‹èƒ½åœ¨ä¿æŒé«˜è´¨é‡ç©ºé—´ä¸€è‡´æ€§çš„åŒæ—¶è¿›è¡Œå®æ—¶4Dé‡å»ºï¼Œå¢é‡æ•´åˆå†å²ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹æ”¯æŒä»å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸè¿ç§»é«˜æ•ˆæ³¨æ„ç®—å­ï¼Œæé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-319c66fc290b6eae39567e0adfcb9db8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f04805298f2f05c2b9fba1ba75d428fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb06b342e4c60aae9f4206ae308a02d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00d52be7d9b7b2b801507384285f0c54.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="How-Many-Instructions-Can-LLMs-Follow-at-Once"><a href="#How-Many-Instructions-Can-LLMs-Follow-at-Once" class="headerlink" title="How Many Instructions Can LLMs Follow at Once?"></a>How Many Instructions Can LLMs Follow at Once?</h2><p><strong>Authors:Daniel Jaroslawicz, Brendan Whiting, Parth Shah, Karime Maamari</strong></p>
<p>Production-grade LLM systems require robust adherence to dozens or even hundreds of instructions simultaneously. However, the instruction-following capabilities of LLMs at high instruction densities have not yet been characterized, as existing benchmarks only evaluate models on tasks with a single or few instructions. We introduce IFScale, a simple benchmark of 500 keyword-inclusion instructions for a business report writing task to measure how instruction-following performance degrades as instruction density increases. We evaluate 20 state-of-the-art models across seven major providers and find that even the best frontier models only achieve 68% accuracy at the max density of 500 instructions. Our analysis reveals model size and reasoning capability to correlate with 3 distinct performance degradation patterns, bias towards earlier instructions, and distinct categories of instruction-following errors. Our insights can help inform design of instruction-dense prompts in real-world applications and highlight important performance-latency tradeoffs. We open-source the benchmark and all results for further analysis at <a target="_blank" rel="noopener" href="https://distylai.github.io/IFScale">https://distylai.github.io/IFScale</a>. </p>
<blockquote>
<p>ç”Ÿäº§çº§çš„LLMç³»ç»Ÿéœ€è¦åŒæ—¶ä¸¥æ ¼éµå®ˆå‡ åä¸ªç”šè‡³ä¸Šç™¾ä¸ªæŒ‡ä»¤ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•åªåœ¨å…·æœ‰å•ä¸ªæˆ–å°‘æ•°æŒ‡ä»¤çš„ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå°šæœªè¡¨å¾LLMåœ¨é«˜æŒ‡ä»¤å¯†åº¦ä¸‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†IFScaleåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«500ä¸ªå…³é”®è¯æŒ‡ä»¤ï¼Œç”¨äºè¡¡é‡å•†ä¸šæŠ¥å‘Šå†™ä½œä»»åŠ¡çš„æŒ‡ä»¤éµå¾ªæ€§èƒ½ï¼Œéšç€æŒ‡ä»¤å¯†åº¦çš„å¢åŠ ï¼Œå¦‚ä½•è¯„ä¼°æŒ‡ä»¤éµå¾ªæ€§èƒ½çš„ä¸‹é™ã€‚æˆ‘ä»¬å¯¹ä¸ƒå®¶ä¸»è¦æä¾›å•†çš„20ä¸ªæœ€æ–°æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å³ä½¿åœ¨æœ€é«˜å¯†åº¦500æ¡æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å‡†ç¡®ç‡ä¹Ÿåªæœ‰68%ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å¤§å°ä¸æ¨ç†èƒ½åŠ›ä¸æ€§èƒ½ä¸‹é™çš„ä¸‰ç§ä¸åŒæ¨¡å¼ç›¸å…³ï¼Œåå‘äºæ—©æœŸæŒ‡ä»¤ä»¥åŠä¸åŒçš„æŒ‡ä»¤éµå¾ªé”™è¯¯ç±»åˆ«ã€‚æˆ‘ä»¬çš„è§è§£æœ‰åŠ©äºä¸ºç°å®ä¸–ç•Œåº”ç”¨ç¨‹åºä¸­çš„å¯†é›†æŒ‡ä»¤æç¤ºè®¾è®¡æä¾›å‚è€ƒï¼Œå¹¶å¼ºè°ƒæ€§èƒ½å»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://distylai.github.io/IFScale%E4%B8%8A%E5%85%AC%E5%BC%80%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E6%89%80%E6%9C%89%E7%BB%93%E6%9E%9C%E4%BB%A5%E4%BE%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E3%80%82">https://distylai.github.io/IFScaleä¸Šå…¬å¼€åŸºå‡†æµ‹è¯•å’Œæ‰€æœ‰ç»“æœä»¥ä¾›è¿›ä¸€æ­¥åˆ†æã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦åŒæ—¶éµå®ˆæ•°åç”šè‡³æ•°ç™¾ä¸ªæŒ‡ä»¤ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä»…å¯¹å…·æœ‰å•ä¸€æˆ–å°‘æ•°æŒ‡ä»¤çš„ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œå°šæœªå¯¹é«˜æŒ‡ä»¤å¯†åº¦ä¸‹LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¿›è¡Œè¡¨å¾ã€‚æœ¬æ–‡å¼•å…¥äº†IFScaleåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åŒ…å«å…³é”®è¯çš„500æ¡æŒ‡ä»¤è¿›è¡Œå•†ä¸šæŠ¥å‘Šå†™ä½œä»»åŠ¡æµ‹è¯•ï¼Œä»¥è¡¡é‡æŒ‡ä»¤å¯†åº¦å¢åŠ æ—¶æŒ‡ä»¤éµå¾ªæ€§èƒ½çš„å˜åŒ–ã€‚è¯„ä¼°äº†æ¥è‡ªä¸ƒå®¶ä¸»è¦æä¾›å•†çš„20ä¸ªæœ€æ–°æ¨¡å‹ï¼Œå‘ç°å³ä½¿åœ¨æœ€é«˜æŒ‡ä»¤å¯†åº¦ä¸‹ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º68%ã€‚åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å¤§å°ä¸æ¨ç†èƒ½åŠ›æ˜¯å½±å“æ€§èƒ½çš„ä¸‰ä¸ªç‹¬ç‰¹å› ç´ ç›¸å…³è”çš„å› ç´ ã€å‡ºç°äº†é¢å‘æ—©æœŸæŒ‡ä»¤çš„åè§ä»¥åŠå„ç§ç±»åˆ«çš„æŒ‡ä»¤éµå¾ªé”™è¯¯ã€‚æœ¬æ–‡çš„è§è§£æœ‰åŠ©äºä¸ºçœŸå®ä¸–ç•Œåº”ç”¨ä¸­é«˜å¯†åº¦æŒ‡ä»¤æç¤ºçš„è®¾è®¡æä¾›ä¿¡æ¯ï¼Œå¹¶å¼ºè°ƒæ€§èƒ½å»¶è¿Ÿæƒè¡¡çš„é‡è¦æ€§ã€‚è¯¦æƒ…å¯è§ï¼š<a target="_blank" rel="noopener" href="https://distylai.github.io/IFScale%E3%80%82">https://distylai.github.io/IFScaleã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMç³»ç»Ÿéœ€è¦åŒæ—¶éµå®ˆå¤šä¸ªæŒ‡ä»¤ä»¥é€‚åº”ç”Ÿäº§ç¯å¢ƒã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°é«˜æŒ‡ä»¤å¯†åº¦ä¸‹LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>IFScaleåŸºå‡†æµ‹è¯•é€šè¿‡åŒ…å«å…³é”®è¯çš„æŒ‡ä»¤è¯„ä¼°LLMåœ¨æŒ‡ä»¤å¯†åº¦å¢åŠ æ—¶çš„æ€§èƒ½å˜åŒ–ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æœ€é«˜æŒ‡ä»¤å¯†åº¦ä¸‹çš„å‡†ç¡®ç‡ä»…ä¸º68%ã€‚</li>
<li>æ¨¡å‹å¤§å°ä¸æ¨ç†èƒ½åŠ›ä¸æ€§èƒ½ä¸‹é™çš„ä¸‰ç§æ¨¡å¼ç›¸å…³è”ã€‚</li>
<li>å­˜åœ¨é¢å‘æ—©æœŸæŒ‡ä»¤çš„åè§ä»¥åŠä¸åŒç±»å‹çš„æŒ‡ä»¤éµå¾ªé”™è¯¯ã€‚</li>
<li>è§è§£æœ‰åŠ©äºè®¾è®¡çœŸå®ä¸–ç•Œåº”ç”¨ä¸­é«˜å¯†åº¦æŒ‡ä»¤æç¤ºå¹¶å¼ºè°ƒæ€§èƒ½å»¶è¿Ÿæƒè¡¡çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cc76b2110cd2d0c3b4adbf86366cbd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-349cb85e1c13c4507344eb58ed284b81.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DrafterBench-Benchmarking-Large-Language-Models-for-Tasks-Automation-in-Civil-Engineering"><a href="#DrafterBench-Benchmarking-Large-Language-Models-for-Tasks-Automation-in-Civil-Engineering" class="headerlink" title="DrafterBench: Benchmarking Large Language Models for Tasks Automation in   Civil Engineering"></a>DrafterBench: Benchmarking Large Language Models for Tasks Automation in   Civil Engineering</h2><p><strong>Authors:Yinsheng Li, Zhen Dong, Yi Shao</strong></p>
<p>Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions&#x2F;tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agentsâ€™ proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/Eason-Li-AIS/DrafterBench">https://github.com/Eason-Li-AIS/DrafterBench</a>, with the test set hosted at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Eason666/DrafterBench">https://huggingface.co/datasets/Eason666/DrafterBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸Šè¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¶æœ‰æœ›æˆä¸ºå·¥ä¸šè‡ªåŠ¨åŒ–ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œéœ€è¦ä»å·¥ä¸šè§’åº¦ç³»ç»Ÿåœ°è¯„ä¼°è‡ªåŠ¨åŒ–ä»£ç†ï¼Œä¾‹å¦‚åœ¨åœŸæœ¨å·¥ç¨‹ç­‰é¢†åŸŸéœ€è¦æ›´å¤šçš„åŸºå‡†æµ‹è¯•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DraftBenchæŠ€æœ¯ï¼Œç”¨äºå…¨é¢è¯„ä¼°åœ¨å›¾çº¸å¤æ ¸æŠ€æœ¯ç¯å¢ƒä¸‹çš„LLMä»£ç†è¡¨ç°ã€‚DraftBenchåŒ…å«äº†ä»å®é™…å›¾çº¸æ–‡ä»¶ä¸­æ€»ç»“å‡ºçš„åäºŒç±»ä»»åŠ¡ï¼Œå…±åŒ…å«46ä¸ªè‡ªå®šä¹‰åŠŸèƒ½&#x2F;å·¥å…·ä»¥åŠæ€»è®¡1920ä¸ªä»»åŠ¡ã€‚DraftBenchæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç†è§£å¤æ‚ä¸”è¯­å¢ƒè¾ƒé•¿çš„æŒ‡ä»¤ã€åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ä»¥åŠé€‚åº”åŠ¨æ€æŒ‡ä»¤è´¨é‡é€šè¿‡éšæ€§ç­–ç•¥æ„è¯†ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥å·¥å…·ç®±å…¨é¢è¯„ä¼°äº†ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œã€éµå¾ªæŒ‡ä»¤ä»¥åŠæ‰¹åˆ¤æ€§æ¨ç†ç­‰æ–¹é¢çš„ä¸åŒèƒ½åŠ›ã€‚DraftBenchæä¾›äº†è¯¦ç»†çš„ä»»åŠ¡å‡†ç¡®æ€§å’Œé”™è¯¯ç»Ÿè®¡æ•°æ®åˆ†æï¼Œæ—¨åœ¨æ·±å…¥äº†è§£ä»£ç†çš„èƒ½åŠ›ï¼Œå¹¶ç¡®å®šåœ¨æ•´åˆå·¥ç¨‹åº”ç”¨ä¸­çš„LLMæ—¶çš„æ”¹è¿›ç›®æ ‡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¹³å°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Eason-Li-AIS/DrafterBench%E4%B8%8A%E8%AE%BF%E9%97%AE%EF%BC%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%89%98%E7%AE%A1%E5%9C%A8https://huggingface.co/datasets/Eason666/DrafterBench%E3%80%82">https://github.com/Eason-Li-AIS/DrafterBenchä¸Šè®¿é—®ï¼Œæµ‹è¯•é›†æ‰˜ç®¡åœ¨https://huggingface.co/datasets/Eason666/DrafterBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11527v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/Eason-Li-AIS/DrafterBench">https://github.com/Eason-Li-AIS/DrafterBench</a></p>
<p><strong>Summary</strong></p>
<p>LLMè‡ªåŠ¨åŒ–ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæœ‰æœ›ä¸ºå·¥ä¸šä»»åŠ¡è‡ªåŠ¨åŒ–æä¾›è§£å†³æ–¹æ¡ˆã€‚ä¸ºå…¨é¢è¯„ä¼°LLMä»£ç†åœ¨åœŸæœ¨å·¥ç¨‹é¢†åŸŸçš„æŠ€æœ¯ç»˜å›¾å¤æ ¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæå‡ºDrafterBenchåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«ä»çœŸå®ç»˜å›¾æ–‡ä»¶ä¸­æ€»ç»“çš„12ç±»ä»»åŠ¡ï¼Œå…±46ä¸ªè‡ªå®šä¹‰åŠŸèƒ½&#x2F;å·¥å…·ï¼Œæ€»è®¡1920ä¸ªä»»åŠ¡ã€‚DrafterBenchæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†æµ‹è¯•å·¥å…·ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•AIä»£ç†åœ¨è§£é‡Šå¤æ‚ã€é•¿ä¸Šä¸‹æ–‡æŒ‡ä»¤ã€åˆ©ç”¨å…ˆéªŒçŸ¥è¯†å’Œé€‚åº”åŠ¨æ€æŒ‡ä»¤è´¨é‡æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒå…¨é¢è¯„ä¼°äº†ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œã€éµå¾ªæŒ‡ä»¤å’Œæ‰¹åˆ¤æ€§æ¨ç†ç­‰ä¸åŒèƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æä¾›äº†ä»»åŠ¡å‡†ç¡®æ€§å’Œé”™è¯¯ç»Ÿè®¡çš„è¯¦ç»†åˆ†æï¼Œæ—¨åœ¨æ·±å…¥äº†è§£ä»£ç†èƒ½åŠ›ï¼Œå¹¶ç¡®å®šåœ¨å·¥ç¨‹åº”ç”¨ä¸­æ•´åˆLLMçš„æ”¹è¿›ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè‡ªåŠ¨åŒ–ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶åœ¨å·¥ä¸šä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢ã€‚</li>
<li>DrafterBenchåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°LLMä»£ç†åœ¨åœŸæœ¨å·¥ç¨‹æŠ€æœ¯ç»˜å›¾å¤æ ¸ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>DrafterBenchåŒ…å«ä»çœŸå®ç»˜å›¾æ–‡ä»¶ä¸­æ€»ç»“çš„å¤šç§ä»»åŠ¡ç±»å‹å’Œè‡ªå®šä¹‰åŠŸèƒ½&#x2F;å·¥å…·ã€‚</li>
<li>DrafterBenchæ˜¯ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œå¯æµ‹è¯•AIä»£ç†åœ¨ä¸åŒæ–¹é¢çš„èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤è§£é‡Šã€ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œç­‰ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æä¾›äº†è¯¦ç»†çš„é”™è¯¯ç»Ÿè®¡å’Œä»»åŠ¡å‡†ç¡®æ€§åˆ†æï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£ä»£ç†çš„èƒ½åŠ›å’Œå¼±ç‚¹ã€‚</li>
<li>é€šè¿‡DrafterBenchåŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥è¯†åˆ«å‡ºåœ¨å·¥ç¨‹åº”ç”¨ä¸­æ•´åˆLLMéœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-84fa803f9b86899eb9c773f8a36a0cfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1018827d3328e8408a7666dbc89e1a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5b10e40e1726f1b91653b45a36be013.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1ef6d89d503b892e8c9515ba5392e44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a685fe9787348f4fe84e87f14f21312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d59eb2dff4f18778f4538006951484f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MIRAGE-KV-Cache-Optimization-through-Parameter-Remapping-for-Multi-tenant-LLM-Serving"><a href="#MIRAGE-KV-Cache-Optimization-through-Parameter-Remapping-for-Multi-tenant-LLM-Serving" class="headerlink" title="MIRAGE: KV Cache Optimization through Parameter Remapping for   Multi-tenant LLM Serving"></a>MIRAGE: KV Cache Optimization through Parameter Remapping for   Multi-tenant LLM Serving</h2><p><strong>Authors:Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar</strong></p>
<p>KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. </p>
<blockquote>
<p>KVç¼“å­˜é€šè¿‡é¿å…å†—ä½™è®¡ç®—æ¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ï¼Œä½†è¿™éœ€è¦æ¶ˆè€—æ›´å¤šå†…å­˜ã€‚ä¸ºäº†æ”¯æŒæ›´å¤§çš„KVç¼“å­˜ï¼Œå…ˆå‰çš„ç ”ç©¶é€šè¿‡CPUå¸è½½æŠ€æœ¯æ‰©å±•GPUå†…å­˜ï¼Œæ¶‰åŠåœ¨GPUå’ŒCPUå†…å­˜ä¹‹é—´äº¤æ¢KVç¼“å­˜ã€‚ç„¶è€Œï¼Œç”±äºç¼“å­˜æ˜¯åŠ¨æ€æ›´æ–°çš„ï¼Œè¿™ç§äº¤æ¢ä¼šå¯¼è‡´CPUå†…å­˜æµé‡å¢åŠ ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªå…³é”®ç°è±¡ï¼Œå³ä¸åŠ¨æ€æ›´æ–°çš„KVç¼“å­˜ä¸åŒï¼Œæ¨¡å‹å‚æ•°åœ¨è¿è¡Œæ—¶ä¿æŒä¸å˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MIRAGEï¼Œå®ƒé€šè¿‡é‡æ˜ å°„é¿å…KVç¼“å­˜äº¤æ¢ï¼Œå¹¶é‡æ–°åˆ©ç”¨åˆ†é…ç»™æ¨¡å‹å‚æ•°çš„å†…å­˜ä½œä¸ºKVç¼“å­˜ã€‚è¿™ç§å‚æ•°é‡æ˜ å°„åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­å°¤å…¶æœ‰ç›Šï¼Œå…¶ä¸­å¯ä»¥æ›´åŠ ç§¯æåœ°å›æ”¶ä¸æ´»è·ƒæ¨¡å‹çš„å‚æ•°æ‰€ä½¿ç”¨çš„å†…å­˜ã€‚åˆ©ç”¨ç°ä»£ç¡¬ä»¶ï¼ˆå¦‚NVIDIA Hopperè¶…çº§èŠ¯ç‰‡ï¼‰æä¾›çš„é«˜CPU-GPUå¸¦å®½ï¼Œæˆ‘ä»¬è¯æ˜äº†MIRAGEæ˜¾è‘—ä¼˜äºæœ€æ–°è§£å†³æ–¹æ¡ˆï¼Œå®ç°äº†å°¾ç«¯ä»¤ç‰Œé—´å»¶è¿Ÿå‡å°‘44.8%-82.5%ï¼Œé¦–ä»¤ç‰Œå»¶è¿Ÿå‡å°‘20.7%-99.3%ï¼Œååé‡æé«˜6.6%-86.7%ï¼Œç›¸è¾ƒäºvLLMæœ‰æ˜æ˜¾æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11507v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé”®å€¼ç¼“å­˜ï¼ˆKV cacheï¼‰èƒ½å¤ŸåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ï¼Œä½†ä¼šå ç”¨è¾ƒå¤šå†…å­˜ã€‚å…ˆå‰çš„ç ”ç©¶é€šè¿‡CPUå¸è½½æŠ€æœ¯æ‰©å±•GPUå†…å­˜ä»¥æ”¯æŒæ›´å¤§çš„KVç¼“å­˜ï¼Œè¿™éœ€è¦åœ¨GPUå’ŒCPUå†…å­˜ä¹‹é—´äº¤æ¢KVç¼“å­˜ã€‚ç„¶è€Œï¼Œç”±äºç¼“å­˜ä¼šåŠ¨æ€æ›´æ–°ï¼Œè¿™ç§äº¤æ¢ä¼šå¯¼è‡´CPUå†…å­˜æµé‡å¢å¤§ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°æ¨¡å‹å‚æ•°åœ¨è¿è¡Œæ—¶ä¿æŒæ’å®šï¼Œä¸åŒäºåŠ¨æ€æ›´æ–°çš„KVç¼“å­˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMIRAGEæ–¹æ¡ˆï¼Œé€šè¿‡å†…å­˜é‡æ–°æ˜ å°„é¿å…KVç¼“å­˜äº¤æ¢ï¼Œå°†åŸæœ¬ç”¨äºæ¨¡å‹å‚æ•°çš„å†…å­˜ç”¨äºKVç¼“å­˜ã€‚è¿™ç§å‚æ•°é‡æ–°æ˜ å°„åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­å°¤ä¸ºæœ‰ç›Šï¼Œå¯ä»¥æ›´åŠ ç§¯æåœ°å›æ”¶ä¸ä½¿ç”¨æ¨¡å‹çš„å†…å­˜ã€‚åˆ©ç”¨ç°ä»£ç¡¬ä»¶ï¼ˆå¦‚NVIDIA Grace Hopperè¶…çº§èŠ¯ç‰‡ï¼‰æä¾›çš„é«˜CPU-GPUå¸¦å®½ï¼ŒMIRAGEæ˜¾è‘—ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆï¼Œåœ¨å°¾è¯é—´å»¶è¿Ÿã€å°¾è¯é¦–å­—å»¶è¿Ÿæ–¹é¢å–å¾—äº†æœ€é«˜è¾¾82.5%å’Œ99.3%çš„å‡å°‘ï¼Œå¹¶åœ¨ååé‡ä¸Šæé«˜äº†æœ€é«˜è¾¾86.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KVç¼“å­˜åŠ é€ŸLLMæ¨ç†ï¼Œä½†éœ€å ç”¨å¤§é‡å†…å­˜ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶é€šè¿‡CPUå¸è½½æŠ€æœ¯åˆ©ç”¨CPUå†…å­˜æ¥æ”¯æŒæ›´å¤§çš„KVç¼“å­˜ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨ç¼“å­˜åŠ¨æ€æ›´æ–°æ—¶ä¼šå¯¼è‡´é«˜CPUå†…å­˜æµé‡ã€‚</li>
<li>MIRAGEé€šè¿‡å†…å­˜é‡æ–°æ˜ å°„é¿å…KVç¼“å­˜äº¤æ¢ï¼Œåˆ©ç”¨åŸæœ¬ç”¨äºæ¨¡å‹å‚æ•°çš„å†…å­˜ã€‚</li>
<li>MIRAGEåœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­å°¤å…¶æœ‰ç›Šï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°å›æ”¶ä¸æ´»è·ƒæ¨¡å‹çš„å†…å­˜ã€‚</li>
<li>MIRAGEåˆ©ç”¨ç°ä»£ç¡¬ä»¶çš„é«˜CPU-GPUå¸¦å®½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚</li>
<li>MIRAGEé™ä½äº†å°¾è¯é—´å»¶è¿Ÿã€å°¾è¯é¦–å­—å»¶è¿Ÿï¼Œå¹¶æé«˜äº†ååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb58797373f2944da33198d8ea142a94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cef3a2ea4534ec4015131fee59f8c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a88fa387beffda4cc5eec06a51668ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c77cf1a135d1565e603e45192f583406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9fa359134ee115631c69c88b92446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66d7cbbfd897cbe1d8eae0d5e764481d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Seq-vs-Seq-An-Open-Suite-of-Paired-Encoders-and-Decoders"><a href="#Seq-vs-Seq-An-Open-Suite-of-Paired-Encoders-and-Decoders" class="headerlink" title="Seq vs Seq: An Open Suite of Paired Encoders and Decoders"></a>Seq vs Seq: An Open Suite of Paired Encoders and Decoders</h2><p><strong>Authors:Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, Benjamin Van Durme</strong></p>
<p>The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¤¾åŒºå‡ ä¹ä¸“æ³¨äºä½¿ç”¨çº¯è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬æ›´æ˜“äºç”¨äºæ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œç¤¾åŒºä¸­ä»æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†ä½¿ç”¨çº¯ç¼–ç å™¨æ¨¡å‹æ¥å®Œæˆåˆ†ç±»æˆ–æ£€ç´¢ç­‰ä»»åŠ¡ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»å°è¯•æ¯”è¾ƒè¿™äº›æ¶æ„ï¼Œä½†è¢«è¿«ä¸å…·æœ‰ä¸åŒå‚æ•°æ•°é‡ã€è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®é›†çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬æ¨å‡ºäº†SOTAå¼€æ”¾æ•°æ®Ettinç³»åˆ—æ¨¡å‹ï¼šä»1700ä¸‡å‚æ•°åˆ°1äº¿å‚æ•°çš„çº¯ç¼–ç å™¨æ¨¡å‹å’Œçº¯è§£ç å™¨æ¨¡å‹é…å¯¹ï¼Œåœ¨é«˜è¾¾2ä¸‡äº¿æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºçº¯ç¼–ç å™¨æ¨¡å‹å’Œçº¯è§£ç å™¨æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„é…æ–¹ï¼Œäº§ç”Ÿäº†å„è‡ªç±»åˆ«ä¸­å„è‡ªå¤§å°çš„SOTAé…æ–¹ï¼Œå‡»è´¥äº†ä½œä¸ºç¼–ç å™¨çš„ModernBERTå’Œä½œä¸ºè§£ç å™¨çš„Llama 3.2å’ŒSmolLM2ã€‚ä¸ä¹‹å‰çš„ç ”ç©¶ä¸€æ ·ï¼Œæˆ‘ä»¬å‘ç°çº¯ç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œè§£ç å™¨åˆ™åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æŒç»­è®­ç»ƒå°†è§£ç å™¨æ¨¡å‹é€‚åº”ç¼–ç å™¨ä»»åŠ¡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ä¸å¦‚åªä½¿ç”¨åå‘ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œåœ¨MNLIä¸Šï¼Œ4äº¿å‚æ•°çš„ç¼–ç å™¨è¡¨ç°ä¼˜äº10äº¿å‚æ•°çš„è§£ç å™¨ï¼Œåä¹‹äº¦ç„¶é€‚ç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰ã€‚æˆ‘ä»¬å¼€æºè¿™é¡¹ç ”ç©¶ä¸­çš„æ‰€æœ‰äº§ç‰©ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ã€æŒ‰æ£€æŸ¥ç‚¹åˆ†å‰²çš„è®­ç»ƒé¡ºåºå’Œ200å¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿æœªæ¥çš„ç ”ç©¶èƒ½å¤Ÿåˆ†ææˆ–æ‰©å±•è®­ç»ƒçš„æ‰€æœ‰æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11412v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¤¾åŒºä¸»è¦å…³æ³¨è§£ç å™¨æ¨¡å‹ï¼Œå› å…¶ä¾¿äºæ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œä»æœ‰å¤§é‡ç¤¾åŒºä½¿ç”¨ç¼–ç å™¨æ¨¡å‹è¿›è¡Œå¦‚åˆ†ç±»æˆ–æ£€ç´¢ç­‰ä»»åŠ¡ã€‚å…ˆå‰çš„ç ”ç©¶è¯•å›¾æ¯”è¾ƒè¿™ä¸¤ç§æ¶æ„ï¼Œä½†æ¯”è¾ƒæ¨¡å‹åœ¨å‚æ•°æ•°é‡ã€è®­ç»ƒæŠ€æœ¯å’Œæ•°æ®é›†æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚æœ¬æ–‡ä»‹ç»äº†SOTAå¼€æ”¾æ•°æ®Ettinå¥—ä»¶æ¨¡å‹ï¼ŒåŒ…æ‹¬é…å¯¹ç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»17ç™¾ä¸‡åˆ°1äº¿ï¼Œåœ¨é«˜è¾¾2ä¸‡äº¿æ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚å¯¹ç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹é‡‡ç”¨ç›¸åŒçš„é…æ–¹ï¼Œåœ¨å…¶ç›¸åº”å¤§å°ç±»åˆ«ä¸­äº§ç”Ÿäº†SOTAç»“æœï¼Œä¼˜äºModernBERTä½œä¸ºç¼–ç å™¨å’ŒLlama 3.2å’ŒSmolLM2ä½œä¸ºè§£ç å™¨ã€‚æˆ‘ä»¬å‘ç°ç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œè§£ç å™¨åˆ™åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ˜¾ç¤ºé€šè¿‡æŒç»­è®­ç»ƒå°†è§£ç å™¨æ¨¡å‹é€‚åº”ç¼–ç å™¨ä»»åŠ¡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ä¸å¦‚ä»…ä½¿ç”¨åå‘ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œ4äº¿ç¼–ç å™¨åœ¨MNLIä¸Šçš„æ€§èƒ½ä¼˜äº10äº¿è§£ç å™¨ï¼Œåä¹‹äº¦ç„¶å¯¹äºç”Ÿæˆä»»åŠ¡ï¼‰ã€‚æˆ‘ä»¬å¼€æºè¿™é¡¹ç ”ç©¶ä¸­çš„æ‰€æœ‰åˆ¶å“ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ã€æŒ‰æ£€æŸ¥ç‚¹åˆ†å‰²çš„è®­ç»ƒé¡ºåºå’Œ200å¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿æœªæ¥ç ”ç©¶åˆ†ææˆ–æ‰©å±•åŸ¹è®­çš„å„ä¸ªæ–¹é¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMç¤¾åŒºä¸»è¦å…³æ³¨è§£ç å™¨æ¨¡å‹ï¼Œä½†ä¹Ÿå­˜åœ¨ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹çš„å¹¿æ³›ç¾¤ä½“ã€‚</li>
<li>ç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§£ç å™¨åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>Ettinå¥—ä»¶æ¨¡å‹åŒ…æ‹¬é…å¯¹ç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ç›¸åº”å¤§å°ç±»åˆ«ä¸­äº§ç”ŸSOTAç»“æœã€‚</li>
<li>åŒä¸€é…æ–¹çš„ç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹åœ¨å„è‡ªç±»åˆ«ä¸­å‡è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>é€‚åº”ä»»åŠ¡ï¼ˆå¦‚å°†è§£ç å™¨æ¨¡å‹ç”¨äºç¼–ç å™¨ä»»åŠ¡ï¼‰é€šè¿‡æŒç»­è®­ç»ƒå¯èƒ½ä¸å¦‚ä½¿ç”¨åå‘ç›®æ ‡æœ‰æ•ˆã€‚</li>
<li>ç ”ç©¶ä¸­çš„æ‰€æœ‰æ•°æ®ã€è®­ç»ƒé¡ºåºå’Œæ£€æŸ¥ç‚¹å‡å·²å¼€æºï¼Œä¾¿äºæœªæ¥åˆ†ææˆ–æ‰©å±•ç›¸å…³ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc394cdde9934d7956efddcf74ce9b27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07bb3d738caab635715f1c3d57062bc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-629b97625d5a1aa4b99a14ec4375f07c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92b5766fe2d0379a586b9fe64f8dda8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="What-is-the-Best-Process-Model-Representation-A-Comparative-Analysis-for-Process-Modeling-with-Large-Language-Models"><a href="#What-is-the-Best-Process-Model-Representation-A-Comparative-Analysis-for-Process-Modeling-with-Large-Language-Models" class="headerlink" title="What is the Best Process Model Representation? A Comparative Analysis   for Process Modeling with Large Language Models"></a>What is the Best Process Model Representation? A Comparative Analysis   for Process Modeling with Large Language Models</h2><p><strong>Authors:Alexis Brissard, FrÃ©dÃ©ric Cuppens, Amal Zouaq</strong></p>
<p>Large Language Models (LLMs) are increasingly applied for Process Modeling (PMo) tasks such as Process Model Generation (PMG). To support these tasks, researchers have introduced a variety of Process Model Representations (PMRs) that serve as model abstractions or generation targets. However, these PMRs differ widely in structure, complexity, and usability, and have never been systematically compared. Moreover, recent PMG approaches rely on distinct evaluation strategies and generation techniques, making comparison difficult. This paper presents the first empirical study that evaluates multiple PMRs in the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset containing 55 process descriptions paired with models in nine different PMRs. We evaluate PMRs along two dimensions: suitability for LLM-based PMo and performance on PMG. \textit{Mermaid} achieves the highest overall score across six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in terms of process element similarity. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šè¢«åº”ç”¨äºè¿‡ç¨‹å»ºæ¨¡ï¼ˆPMoï¼‰ä»»åŠ¡ï¼Œå¦‚è¿‡ç¨‹æ¨¡å‹ç”Ÿæˆï¼ˆPMGï¼‰ã€‚ä¸ºäº†æ”¯æŒè¿™äº›ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜å·²ç»å¼•å…¥äº†å„ç§è¿‡ç¨‹æ¨¡å‹è¡¨ç¤ºï¼ˆPMRsï¼‰ï¼Œä½œä¸ºæ¨¡å‹æŠ½è±¡æˆ–ç”Ÿæˆç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›PMRsåœ¨ç»“æ„ã€å¤æ‚æ€§å’Œå¯ç”¨æ€§æ–¹é¢å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œå¹¶ä¸”ä»æœªè¿›è¡Œè¿‡ç³»ç»Ÿæ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæœ€è¿‘çš„PMGæ–¹æ³•ä¾èµ–äºç‹¬ç‰¹çš„è¯„ä¼°ç­–ç•¥å’Œç”ŸæˆæŠ€æœ¯ï¼Œä½¿å¾—æ¯”è¾ƒå˜å¾—å›°éš¾ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹LLMç¯å¢ƒä¸‹çš„PMRsè¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬ä»‹ç»äº†PMoæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«55ä¸ªæµç¨‹æè¿°ä¸ä¹ç§ä¸åŒPMRsä¸­çš„æ¨¡å‹é…å¯¹ã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªç»´åº¦è¯„ä¼°PMRsï¼šé€‚åˆåŸºäºLLMçš„PMoå’ŒPMGæ€§èƒ½ã€‚\textit{Mermaid}åœ¨å…­ä¸ªPMoæ ‡å‡†ä¸­è·å¾—äº†æœ€é«˜æ€»ä½“å¾—åˆ†ï¼Œè€Œ\textit{BPMNæ–‡æœ¬}åœ¨æµç¨‹å…ƒç´ ç›¸ä¼¼æ€§æ–¹é¢å–å¾—äº†æœ€ä½³çš„PMGç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11356v1">PDF</a> 12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµç¨‹å»ºæ¨¡ï¼ˆPMoï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¦‚æµç¨‹æ¨¡å‹ç”Ÿæˆï¼ˆPMGï¼‰ã€‚ä¸ºæ”¯æŒè¿™äº›ä»»åŠ¡ï¼Œç ”ç©¶è€…å·²å¼•å…¥å¤šç§æµç¨‹æ¨¡å‹è¡¨ç¤ºï¼ˆPMRsï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›PMRsåœ¨ç»“æ„ã€å¤æ‚åº¦å’Œå¯ç”¨æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”ä»æœªè¿›è¡Œè¿‡ç³»ç»Ÿæ¯”è¾ƒã€‚æ­¤å¤–ï¼Œå½“å‰çš„PMGæ–¹æ³•ä¾èµ–äºç‹¬ç‰¹çš„è¯„ä¼°ç­–ç•¥å’ŒæŠ€æœ¯ï¼Œä½¿å¾—æ¯”è¾ƒå˜å¾—å›°éš¾ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹LLMèƒŒæ™¯ä¸‹çš„PMRsè¿›è¡Œå®è¯ç ”ç©¶ã€‚æˆ‘ä»¬å¼•å…¥äº†PMoæ•°æ®é›†ï¼ŒåŒ…å«55ä¸ªæµç¨‹æè¿°ä¸ä¹ç§ä¸åŒPMRsçš„æ¨¡å‹é…å¯¹ã€‚æˆ‘ä»¬æ²¿ä¸¤ä¸ªç»´åº¦è¯„ä¼°PMRsï¼šé€‚åˆLLMåŸºç¡€çš„PMoå’ŒPMGæ€§èƒ½ã€‚\textit{Mermaid}åœ¨å…­ä¸ªPMoæ ‡å‡†ä¸­å–å¾—æœ€é«˜æ€»ä½“å¾—åˆ†ï¼Œè€Œ\textit{BPMNæ–‡æœ¬}åœ¨æµç¨‹å…ƒç´ ç›¸ä¼¼æ€§æ–¹é¢å–å¾—æœ€ä½³PMGç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµç¨‹å»ºæ¨¡ï¼ˆPMoï¼‰ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œå¦‚æµç¨‹æ¨¡å‹ç”Ÿæˆï¼ˆPMGï¼‰ã€‚</li>
<li>å­˜åœ¨å¤šç§æµç¨‹æ¨¡å‹è¡¨ç¤ºï¼ˆPMRsï¼‰ï¼Œä½†å®ƒä»¬åœ¨ç»“æ„ã€å¤æ‚åº¦å’Œå¯ç”¨æ€§æ–¹é¢å·®å¼‚æ˜¾è‘—ï¼Œç¼ºä¹ç³»ç»Ÿæ¯”è¾ƒã€‚</li>
<li>å½“å‰çš„PMGæ–¹æ³•ä½¿å¾—æ¯”è¾ƒå˜å¾—å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºç‹¬ç‰¹çš„è¯„ä¼°ç­–ç•¥å’ŒæŠ€æœ¯ã€‚</li>
<li>è®ºæ–‡é¦–æ¬¡å¯¹LLMèƒŒæ™¯ä¸‹çš„PMRsè¿›è¡Œå®è¯ç ”ç©¶ã€‚</li>
<li>å¼•å…¥PMoæ•°æ®é›†ï¼ŒåŒ…å«æµç¨‹æè¿°ä¸ä¸åŒPMRsçš„æ¨¡å‹é…å¯¹ã€‚</li>
<li>\textit{Mermaid}åœ¨å¤šä¸ªPMoæ ‡å‡†ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œ\textit{BPMNæ–‡æœ¬}åœ¨æµç¨‹å…ƒç´ ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0af17b75d5a327ffeb7c243585badd03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b63082945ce30f901d31941d933eb023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28b51e146278dbd09e11082b9e89f590.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc00fa558aab3d11b460f9988f315d39.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Social-Media-Sentiments-Analysis-on-the-July-Revolution-in-Bangladesh-A-Hybrid-Transformer-Based-Machine-Learning-Approach"><a href="#Social-Media-Sentiments-Analysis-on-the-July-Revolution-in-Bangladesh-A-Hybrid-Transformer-Based-Machine-Learning-Approach" class="headerlink" title="Social Media Sentiments Analysis on the July Revolution in Bangladesh: A   Hybrid Transformer Based Machine Learning Approach"></a>Social Media Sentiments Analysis on the July Revolution in Bangladesh: A   Hybrid Transformer Based Machine Learning Approach</h2><p><strong>Authors:Md. Sabbir Hossen, Md. Saiduzzaman, Pabon Shaha</strong></p>
<p>The July Revolution in Bangladesh marked a significant student-led mass uprising, uniting people across the nation to demand justice, accountability, and systemic reform. Social media platforms played a pivotal role in amplifying public sentiment and shaping discourse during this historic mass uprising. In this study, we present a hybrid transformer-based sentiment analysis framework to decode public opinion expressed in social media comments during and after the revolution. We used a brand new dataset of 4,200 Bangla comments collected from social media. The framework employs advanced transformer-based feature extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the proposed hybrid XMB-BERT, to capture nuanced patterns in textual data. Principle Component Analysis (PCA) were utilized for dimensionality reduction to enhance computational efficiency. We explored eleven traditional and advanced machine learning classifiers for identifying sentiments. The proposed hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of 83.7% and outperform other model classifier combinations. This study underscores the potential of machine learning techniques to analyze social sentiment in low-resource languages like Bangla. </p>
<blockquote>
<p>å­ŸåŠ æ‹‰å›½çš„ä¸ƒæœˆé©å‘½æ ‡å¿—ç€ç”±å­¦ç”Ÿé¢†å¯¼çš„å¤§è§„æ¨¡èµ·ä¹‰ï¼Œå…¨å›½äººæ°‘å›¢ç»“èµ·æ¥è¦æ±‚æ­£ä¹‰ã€é—®è´£å’Œåˆ¶åº¦æ€§æ”¹é©ã€‚åœ¨è¿™åœºå†å²æ€§å¤§è§„æ¨¡èµ·ä¹‰ä¸­ï¼Œç¤¾äº¤åª’ä½“å¹³å°åœ¨æ”¾å¤§å…¬ä¼—æƒ…ç»ªå’Œå¡‘é€ è¨€è®ºæ–¹é¢å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ··åˆå˜å‹å™¨çš„æƒ…æ„Ÿåˆ†ææ¡†æ¶ï¼Œä»¥è§£ç é©å‘½æœŸé—´å’Œä¹‹åç¤¾äº¤åª’ä½“è¯„è®ºä¸­è¡¨è¾¾çš„å…¬ä¼—æ„è§ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä»ç¤¾äº¤åª’ä½“æ”¶é›†çš„4200æ¡å…¨æ–°å­ŸåŠ æ‹‰è¯­è¯„è®ºæ•°æ®é›†ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å…ˆè¿›çš„åŸºäºå˜å‹å™¨çš„ç‰¹å¾æå–æŠ€æœ¯ï¼ŒåŒ…æ‹¬BanglaBERTã€mBERTã€XLM-RoBERTaå’Œæå‡ºçš„æ··åˆXMB-BERTï¼Œä»¥æ•æ‰æ–‡æœ¬æ•°æ®ä¸­çš„å¾®å¦™æ¨¡å¼ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¿›è¡Œé™ç»´ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬æ¢ç´¢äº†11ç§ä¼ ç»Ÿå’Œå…ˆè¿›çš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æ¥è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ã€‚æ‰€æå‡ºçš„æ··åˆXMB-BERTä¸æŠ•ç¥¨åˆ†ç±»å™¨ç›¸ç»“åˆï¼Œå–å¾—äº†83.7%çš„æƒŠäººå‡†ç¡®ç‡ï¼Œå¹¶ä¼˜äºå…¶ä»–æ¨¡å‹åˆ†ç±»å™¨ç»„åˆã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨åˆ†æå¦‚å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€çš„ç¤¾ä¼šæƒ…æ„Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11084v1">PDF</a> This paper has been accepted and presented at the IEEE ECAI 2025. The   final version will be available in the IEEE Xplore Digital Library</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†å­ŸåŠ æ‹‰å›½ä¸ƒæœˆé©å‘½æœŸé—´ç¤¾äº¤åª’ä½“ä¸Šçš„å…¬ä¼—æƒ…æ„Ÿã€‚åˆ©ç”¨åŸºäºæ··åˆå˜å‹å™¨çš„æƒ…æ„Ÿåˆ†ææ¡†æ¶ï¼Œç»“åˆBanglaBERTã€mBERTç­‰è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œå¯¹ç¤¾äº¤åª’ä½“è¯„è®ºä¸­çš„å…¬ä¼—æ„è§è¿›è¡Œè§£ç ã€‚ç ”ç©¶é‡‡ç”¨PCAè¿›è¡Œé™ç»´ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå¹¶é€šè¿‡å¤šç§æœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ã€‚æœ€ç»ˆï¼Œä½¿ç”¨æ··åˆXMB-BERTä¸æŠ•ç¥¨åˆ†ç±»å™¨å–å¾—äº†83.7%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºæœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨åˆ†æå­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€çš„ç¤¾ä¼šæƒ…æ„Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰å›½ä¸ƒæœˆé©å‘½æœŸé—´ï¼Œç¤¾äº¤åª’ä½“åœ¨æ”¾å¤§å…¬ä¼—æƒ…ç»ªã€å¡‘é€ èˆ†è®ºæ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ··åˆå˜å‹å™¨æƒ…æ„Ÿåˆ†ææ¡†æ¶è§£ç ç¤¾äº¤åª’ä½“è¯„è®ºä¸­çš„å…¬ä¼—æ„è§ã€‚</li>
<li>ä½¿ç”¨äº†åŒ…æ‹¬BanglaBERTåœ¨å†…çš„å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>é€šè¿‡PCAè¿›è¡Œé™ç»´ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§æœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>æ··åˆXMB-BERTä¸æŠ•ç¥¨åˆ†ç±»å™¨å–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24e70cd6b7566f318c80d403267af043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138aec4d85af7e0b52ac1e1f66ef9c6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f0a5ee214c5d0a959ba3bc692bdcfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a8cc2d2b2eab9b3a2fb9477d80e43b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69a3374b154ee416a4b6455c97c2d72b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc3d6e088b7b9fb7890a928ce1918d71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f05160325f7a35ba9403d832ce331cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04bbc8fb0885c4ff8cc72a991b50fa83.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Universal-Approximation-Theorem-for-a-Single-Layer-Transformer"><a href="#Universal-Approximation-Theorem-for-a-Single-Layer-Transformer" class="headerlink" title="Universal Approximation Theorem for a Single-Layer Transformer"></a>Universal Approximation Theorem for a Single-Layer Transformer</h2><p><strong>Authors:Esmail Gumaan</strong></p>
<p>Deep learning employs multi-layer neural networks trained via the backpropagation algorithm. This approach has achieved success across many domains and relies on adaptive gradient methods such as the Adam optimizer. Sequence modeling evolved from recurrent neural networks to attention-based models, culminating in the Transformer architecture. Transformers have achieved state-of-the-art performance in natural language processing (for example, BERT and GPT-3) and have been applied in computer vision and computational biology. However, theoretical understanding of these models remains limited. In this paper, we examine the mathematical foundations of deep learning and Transformers and present a novel theoretical result. We review key concepts from linear algebra, probability, and optimization that underpin deep learning, and we analyze the multi-head self-attention mechanism and the backpropagation algorithm in detail. Our main contribution is a universal approximation theorem for Transformers: we prove that a single-layer Transformer, comprising one self-attention layer followed by a position-wise feed-forward network with ReLU activation, can approximate any continuous sequence-to-sequence mapping on a compact domain to arbitrary precision. We provide a formal statement and a complete proof. Finally, we present case studies that demonstrate the practical implications of this result. Our findings advance the theoretical understanding of Transformer models and help bridge the gap between theory and practice. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ é‡‡ç”¨é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®­ç»ƒçš„å¤šå±‚ç¥ç»ç½‘ç»œã€‚è¿™ç§æ–¹æ³•åœ¨è®¸å¤šé¢†åŸŸéƒ½å–å¾—äº†æˆåŠŸï¼Œå¹¶ä¾èµ–äºè‡ªé€‚åº”æ¢¯åº¦æ–¹æ³•ï¼Œå¦‚Adamä¼˜åŒ–å™¨ã€‚åºåˆ—å»ºæ¨¡ä»å¾ªç¯ç¥ç»ç½‘ç»œå‘å±•åˆ°åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œæœ€ç»ˆå½¢æˆäº†Transformeræ¶æ„ã€‚Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆä¾‹å¦‚BERTå’ŒGPT-3ï¼‰ï¼Œå¹¶åº”ç”¨äºè®¡ç®—æœºè§†è§‰å’Œè®¡ç®—ç”Ÿç‰©å­¦ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›æ¨¡å‹çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ·±åº¦å­¦ä¹ å’ŒTransformerçš„æ•°å­¦åŸºç¡€ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç†è®ºç»“æœã€‚æˆ‘ä»¬å›é¡¾äº†æ”¯æ’‘æ·±åº¦å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬çº¿æ€§ä»£æ•°ã€æ¦‚ç‡å’Œä¼˜åŒ–çš„å…³é”®æ¦‚å¿µï¼Œå¹¶è¯¦ç»†åˆ†æäº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œåå‘ä¼ æ’­ç®—æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºä¸€ä¸ªå…³äºTransformerçš„é€šç”¨é€¼è¿‘å®šç†ï¼šæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªç”±è‡ªæ³¨æ„åŠ›å±‚ç»„æˆçš„å•å±‚Transformerï¼Œéšåæ˜¯ä¸€ä¸ªä½ç½®æ„ŸçŸ¥å‰é¦ˆç½‘ç»œï¼Œå…·æœ‰ReLUæ¿€æ´»åŠŸèƒ½ï¼Œå¯ä»¥åœ¨ç´§å‡‘åŸŸä¸Šè¿‘ä¼¼ä»»ä½•è¿ç»­åºåˆ—åˆ°åºåˆ—æ˜ å°„ï¼Œè¾¾åˆ°ä»»æ„ç²¾åº¦ã€‚æˆ‘ä»¬ç»™å‡ºäº†æ­£å¼çš„é™ˆè¿°å’Œå®Œæ•´çš„è¯æ˜ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¿™ä¸€ç»“æœçš„å®è·µæ„ä¹‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¨åŠ¨äº†Transformeræ¨¡å‹çš„ç†è®ºç†è§£ï¼Œå¹¶æœ‰åŠ©äºå¼¥åˆç†è®ºä¸å®è·µä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10581v1">PDF</a> 7 pages, 2 figures, 1 theorem, 10 formulas</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ é€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œå·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æ·±åº¦å­¦ä¹ å’ŒTransformerçš„æ•°å­¦åŸºç¡€ï¼Œå¹¶è¯¦ç»†åˆ†æäº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œåå‘ä¼ æ’­ç®—æ³•ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯è¯æ˜äº†ä¸€ç§æ–°å‹çš„å•å±‚Transformeræ¶æ„èƒ½ç²¾ç¡®åœ°è¿‘ä¼¼ä»»ä½•è¿ç»­åºåˆ—åˆ°åºåˆ—æ˜ å°„ï¼Œæ¨åŠ¨äº†Transformeræ¨¡å‹çš„ç†è®ºç†è§£ï¼Œæœ‰åŠ©äºå¼¥åˆäº†ç†è®ºå’Œå®è·µä¹‹é—´çš„é¸¿æ²Ÿã€‚</p>
<p><strong>Key Takeaways</strong><br> æ·±åº¦å­¦ä¹ è¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œä¸åå‘ä¼ æ’­ç®—æ³•ï¼Œä¸”å·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚<br> Transformeræ¶æ„åŸºäºå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å“è¶Šã€‚<br> å½“å‰å¯¹Transformerçš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚<br> æœ¬æ–‡è¯¦ç»†æ¢è®¨äº†æ·±åº¦å­¦ä¹ å’ŒTransformerçš„æ•°å­¦åŸºç¡€ã€‚<br> ç ”ç©¶è¯æ˜äº†ä¸€ç§æ–°å‹çš„å•å±‚Transformeræ¶æ„èƒ½è¿‘ä¼¼ä»»ä½•è¿ç»­åºåˆ—åˆ°åºåˆ—æ˜ å°„ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d157348c5d15bc0abaa13d47bef86d8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a24e82f869c0cd7202dfd4c95feeb3c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Sequence-to-Structure-Uncovering-Substructure-Reasoning-in-Transformers"><a href="#From-Sequence-to-Structure-Uncovering-Substructure-Reasoning-in-Transformers" class="headerlink" title="From Sequence to Structure: Uncovering Substructure Reasoning in   Transformers"></a>From Sequence to Structure: Uncovering Substructure Reasoning in   Transformers</h2><p><strong>Authors:Xinnan Dai, Kai Yang, Jay Revolinsky, Kai Guo, Aoran Wang, Bohang Zhang, Jiliang Tang</strong></p>
<p>Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, we start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡è§£å†³å›¾å½¢æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨æ–‡æœ¬æè¿°ä¸­åµŒå…¥å›¾å½¢ç»“æ„ï¼ŒLLMä»ç„¶å¯ä»¥æœ‰æ•ˆåœ°å›ç­”ç›¸å…³é—®é¢˜ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šä»…è§£ç çš„Transformeræ¶æ„å¦‚ä½•ç†è§£æ½œåœ¨çš„å›¾å½¢ç»“æ„ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»å­ç»“æ„æå–ä»»åŠ¡å¼€å§‹ï¼Œè§£è¯»Transformerå†…éƒ¨çš„æœºåˆ¶å¹¶åˆ†æè¾“å…¥æŸ¥è¯¢çš„å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç»“æœå’Œç†è®ºåˆ†æï¼Œæå‡ºäº†è¯±å¯¼å­ç»“æ„è¿‡æ»¤ï¼ˆISFï¼‰çš„è§‚ç‚¹ï¼Œè¯¥è§‚ç‚¹æ•æ‰äº†å¤šå±‚Transformerä¸­çš„å­ç»“æ„è¯†åˆ«ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯äº†LLMä¸­çš„ISFè¿‡ç¨‹ï¼Œæ­ç¤ºäº†è·¨å±‚çš„å†…éƒ¨åŠ¨æ€ä¸€è‡´æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æ¢ç´¢äº†Transformeråœ¨å¤„ç†å„ç§å›¾å½¢ç±»å‹æ–¹é¢çš„æ›´å¹¿æ³›èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­ç»“æ„æ€ç»´çš„æ¦‚å¿µï¼Œä»¥æœ‰æ•ˆåœ°æå–å¤æ‚çš„ç»„åˆæ¨¡å¼ï¼Œå¹¶è¯æ˜ä»…è§£ç çš„Transformerå¯ä»¥æˆåŠŸåœ°ä»å±æ€§å›¾ä¸­æå–å­ç»“æ„ï¼Œä¾‹å¦‚åˆ†å­å›¾ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºåºåˆ—åŸºç¡€çš„Transformerå¦‚ä½•åœ¨å›¾å½¢æ•°æ®ä¸Šæ‰§è¡Œå­ç»“æ„æå–ä»»åŠ¡æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10435v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡è§£å†³å›¾æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå³ä½¿å›¾ç»“æ„åµŒå…¥æ–‡æœ¬æè¿°ä¸­ï¼Œä¹Ÿèƒ½æœ‰æ•ˆå›ç­”é—®é¢˜ã€‚æœ¬ç ”ç©¶ä»å­ç»“æ„æå–ä»»åŠ¡å…¥æ‰‹ï¼Œè§£æäº†transformerçš„å†…éƒ¨æœºåˆ¶ï¼Œå¹¶æå‡ºäº†Induced Substructure Filtrationï¼ˆISFï¼‰è§†è§’ï¼Œæ­ç¤ºäº†LLMsåœ¨å¤šå±‚transformerä¸­çš„å­ç»“æ„è¯†åˆ«è¿‡ç¨‹ã€‚åŒæ—¶ï¼Œæ¢ç´¢äº†transformerå¤„ç†å¤šç§å›¾ç±»å‹çš„èƒ½åŠ›ï¼Œè¯æ˜å…¶èƒ½æˆåŠŸæå–å±æ€§å›¾çš„å­ç»“æ„ï¼Œå¦‚åˆ†å­å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡è§£å†³å›¾æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå³ä½¿é¢å¯¹åµŒå…¥æ–‡æœ¬æè¿°ä¸­çš„å›¾ç»“æ„ä¹Ÿèƒ½æœ‰æ•ˆåº”å¯¹ã€‚</li>
<li>é€šè¿‡å­ç»“æ„æå–ä»»åŠ¡ï¼Œè§£æäº†transformerçš„å†…éƒ¨æœºåˆ¶ã€‚</li>
<li>æå‡ºäº†Induced Substructure Filtrationï¼ˆISFï¼‰è§†è§’ï¼Œæ­ç¤ºäº†å¤šå±‚transformerä¸­çš„å­ç»“æ„è¯†åˆ«è¿‡ç¨‹ã€‚</li>
<li>LLMsåœ¨å¤„ç†å¤šç§å›¾ç±»å‹æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥å­ç»“æ„æ¦‚å¿µï¼ŒæˆåŠŸæå–äº†å±æ€§å›¾çš„å­ç»“æ„ã€‚</li>
<li>LLMsåœ¨è¿›è¡Œå­ç»“æ„æå–ä»»åŠ¡æ—¶å±•ç°äº†åºåˆ—å¤„ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2869597b645420aa27d88c60bb3e10ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ca0f253ba1f948588c88f80de1fe5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3802f0b5d1e1414fbdcad072507faf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee0423bb158a1a1596661f062511dc8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c58d74c91d0703f5e9c28b0b7ccba740.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Extracting-Important-Tokens-in-E-Commerce-Queries-with-a-Tag-Interaction-Aware-Transformer-Model"><a href="#Extracting-Important-Tokens-in-E-Commerce-Queries-with-a-Tag-Interaction-Aware-Transformer-Model" class="headerlink" title="Extracting Important Tokens in E-Commerce Queries with a Tag   Interaction-Aware Transformer Model"></a>Extracting Important Tokens in E-Commerce Queries with a Tag   Interaction-Aware Transformer Model</h2><p><strong>Authors:Md. Ahsanul Kabir, Mohammad Al Hasan, Aritra Mandal, Liyang Hao, Ishita Khan, Daniel Tunkelang, Zhe Wu</strong></p>
<p>The major task of any e-commerce search engine is to retrieve the most relevant inventory items, which best match the user intent reflected in a query. This task is non-trivial due to many reasons, including ambiguous queries, misaligned vocabulary between buyers, and sellers, over- or under-constrained queries by the presence of too many or too few tokens. To address these challenges, query reformulation is used, which modifies a user query through token dropping, replacement or expansion, with the objective to bridge semantic gap between query tokens and usersâ€™ search intent. Early methods of query reformulation mostly used statistical measures derived from token co-occurrence frequencies from selective user sessions having clicks or purchases. In recent years, supervised deep learning approaches, specifically transformer-based neural language models, or sequence-to-sequence models are being used for query reformulation task. However, these models do not utilize the semantic tags of a query token, which are significant for capturing user intent of an e-commerce query. In this work, we pose query reformulation as a token classification task, and solve this task by designing a dependency-aware transformer-based language model, TagBERT, which makes use of semantic tags of a token for learning superior query phrase embedding. Experiments on large, real-life e-commerce datasets show that TagBERT exhibits superior performance than plethora of competing models, including BERT, eBERT, and Sequence-to-Sequence transformer model for important token classification task. </p>
<blockquote>
<p>ä»»ä½•ç”µå­å•†åŠ¡æœç´¢å¼•æ“çš„ä¸»è¦ä»»åŠ¡éƒ½æ˜¯æ£€ç´¢ä¸ç”¨æˆ·æŸ¥è¯¢ä¸­åæ˜ çš„æ„å›¾æœ€åŒ¹é…çš„ç›¸å…³åº“å­˜é¡¹ç›®ã€‚ç”±äºè®¸å¤šåŸå› ï¼ŒåŒ…æ‹¬æŸ¥è¯¢æ¨¡ç³Šã€ä¹°å®¶å’Œå–å®¶è¯æ±‡ä¸åŒ¹é…ã€æŸ¥è¯¢è¿‡äºçº¦æŸæˆ–è¿‡äºå®½æ¾ç­‰ï¼Œæ­¤ä»»åŠ¡å¹¶ä¸å®¹æ˜“å®Œæˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œé‡‡ç”¨äº†æŸ¥è¯¢é‡æ„çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ é™¤ã€æ›¿æ¢æˆ–æ‰©å±•ç”¨æˆ·æŸ¥è¯¢ä¸­çš„æ ‡è®°æ¥ä¿®æ”¹æŸ¥è¯¢ï¼Œæ—¨åœ¨å¼¥åˆæŸ¥è¯¢æ ‡è®°ä¸ç”¨æˆ·æœç´¢æ„å›¾ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚æ—©æœŸçš„æŸ¥è¯¢é‡æ„æ–¹æ³•å¤§å¤šä½¿ç”¨ä»å…·æœ‰ç‚¹å‡»æˆ–è´­ä¹°è¡Œä¸ºçš„é€‰å®šç”¨æˆ·ä¼šè¯ä¸­æ´¾ç”Ÿçš„æ ‡è®°å…±ç°é¢‘ç‡çš„ç»Ÿè®¡é‡ã€‚è¿‘å¹´æ¥ï¼Œç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºè½¬æ¢å™¨çš„ç¥ç»è¯­è¨€æ¨¡å‹æˆ–åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œè¢«ç”¨äºæŸ¥è¯¢é‡æ„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¹¶æ²¡æœ‰åˆ©ç”¨æŸ¥è¯¢æ ‡è®°çš„è¯­ä¹‰æ ‡ç­¾ï¼Œè¿™å¯¹äºæ•è·ç”µå­å•†åŠ¡æŸ¥è¯¢çš„ç”¨æˆ·æ„å›¾éå¸¸é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢é‡æ„ä½œä¸ºæ ‡è®°åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è®¾è®¡åŸºäºä¾èµ–æ„è¯†çš„è½¬æ¢è¯­è¨€æ¨¡å‹TagBERTæ¥è§£å†³æ­¤ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ ‡è®°çš„è¯­ä¹‰æ ‡ç­¾æ¥å­¦ä¹ æ›´ä¼˜è´¨çš„æŸ¥è¯¢çŸ­è¯­åµŒå…¥ã€‚åœ¨å¤§å‹çœŸå®ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼—å¤šç«äº‰æ¨¡å‹ç›¸æ¯”ï¼ŒTagBERTåœ¨é‡è¦çš„æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬BERTã€eBERTå’Œåºåˆ—åˆ°åºåˆ—è½¬æ¢å™¨æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10385v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µå•†æœç´¢å¼•æ“åœ¨æ£€ç´¢ä¸ç”¨æˆ·æ„å›¾åŒ¹é…çš„åº“å­˜å•†å“æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡ç³ŠæŸ¥è¯¢ã€ä¹°å–åŒæ–¹åœ¨è¯æ±‡ä¸Šçš„ä¸åŒ¹é…ä»¥åŠæŸ¥è¯¢çš„è¿‡åº¦æˆ–ä¸è¶³ç­‰é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä½¿ç”¨æŸ¥è¯¢æ”¹å†™çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ é™¤ã€æ›¿æ¢æˆ–æ‰©å±•ä»¤ç‰Œæ¥ç¼©å°æŸ¥è¯¢ä»¤ç‰Œä¸ç”¨æˆ·æœç´¢æ„å›¾ä¹‹é—´çš„è¯­ä¹‰å·®è·ã€‚æ–‡ç« æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä½¿ç”¨ç»Ÿè®¡åº¦é‡æªæ–½ï¼Œè€Œè¿‘å¹´æ¥åˆ™é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ²¡æœ‰å……åˆ†åˆ©ç”¨æŸ¥è¯¢ä»¤ç‰Œçš„è¯­ä¹‰æ ‡ç­¾æ¥æ•æ‰ç”¨æˆ·æ„å›¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†æŸ¥è¯¢æ”¹å†™ä½œä¸ºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºä¾èµ–æ„è¯†çš„Transformerè¯­è¨€æ¨¡å‹TagBERTæ¥è§£å†³æ­¤é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒTagBERTåœ¨å¤§å‹çœŸå®ç”µå•†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå•†æœç´¢å¼•æ“çš„ä¸»è¦ä»»åŠ¡æ˜¯æ£€ç´¢ä¸ç”¨æˆ·æ„å›¾æœ€åŒ¹é…çš„åº“å­˜å•†å“ã€‚</li>
<li>æŸ¥è¯¢æ”¹å†™æ˜¯è§£å†³ç”µå•†æœç´¢å¼•æ“é¢ä¸´æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ä¼ ç»ŸæŸ¥è¯¢æ”¹å†™æ–¹æ³•ä¸»è¦ä¾èµ–ç»Ÿè®¡åº¦é‡æªæ–½ï¼Œè€Œç°ä»£æ–¹æ³•åˆ™ä½¿ç”¨æ·±åº¦å­¦ä¹ å’Œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚</li>
<li>ç°æœ‰æ¨¡å‹æœªå……åˆ†åˆ©ç”¨æŸ¥è¯¢ä»¤ç‰Œçš„è¯­ä¹‰æ ‡ç­¾æ¥æ•æ‰ç”¨æˆ·æ„å›¾ã€‚</li>
<li>æœ¬æ–‡å°†æŸ¥è¯¢æ”¹å†™ä½œä¸ºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡æ¥è§£å†³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¾èµ–æ„è¯†çš„Transformerè¯­è¨€æ¨¡å‹TagBERTæ¥è§£å†³æŸ¥è¯¢æ”¹å†™é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de43758f319207e52936be3d82435691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-865a173825c14f53265c152da678ee39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5447f512e5d43d677d049d94377a0e7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Abusive-text-transformation-using-LLMs"><a href="#Abusive-text-transformation-using-LLMs" class="headerlink" title="Abusive text transformation using LLMs"></a>Abusive text transformation using LLMs</h2><p><strong>Authors:Rohitash Chandra, Jiyong Choi</strong></p>
<p>Although Large Language Models (LLMs) have demonstrated significant advancements in natural language processing tasks, their effectiveness in the classification and transformation of abusive text into non-abusive versions remains an area for exploration. In this study, we aim to use LLMs to transform abusive text (tweets and reviews) featuring hate speech and swear words into non-abusive text, while retaining the intent of the text. We evaluate the performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and Groq, on their ability to identify abusive text. We them to transform and obtain a text that is clean from abusive and inappropriate content but maintains a similar level of sentiment and semantics, i.e. the transformed text needs to maintain its message. Afterwards, we evaluate the raw and transformed datasets with sentiment analysis and semantic analysis. Our results show Groq provides vastly different results when compared with other LLMs. We have identified similarities between GPT-4o and DeepSeek-V3. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨å°†è¾±éª‚æ€§æ–‡æœ¬åˆ†ç±»å¹¶è½¬æ¢ä¸ºéè¾±éª‚æ€§ç‰ˆæœ¬æ–¹é¢çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæœ‰å¾…æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨LLMå°†åŒ…å«ä»‡æ¨è¨€è®ºå’Œè„è¯çš„è¾±éª‚æ€§æ–‡æœ¬ï¼ˆå¦‚æ¨ç‰¹å’Œè¯„è®ºï¼‰è½¬æ¢ä¸ºéè¾±éª‚æ€§æ–‡æœ¬ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬çš„æ„å›¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤æ¬¾å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬Geminiã€GPT-4oã€DeekSeekå’ŒGroqåœ¨è¯†åˆ«è¾±éª‚æ€§æ–‡æœ¬æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›ä»–ä»¬èƒ½å¤Ÿå°†æ–‡æœ¬è½¬æ¢ä¸ºä¸å«è¾±éª‚å’Œä¸é€‚å½“å†…å®¹çš„æ–‡æœ¬ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æƒ…æ„Ÿæ°´å¹³å’Œè¯­ä¹‰ï¼Œå³è½¬æ¢åçš„æ–‡æœ¬éœ€è¦ä¿ç•™å…¶ä¿¡æ¯ã€‚ä¹‹åï¼Œæˆ‘ä»¬å¯¹åŸå§‹å’Œè½¬æ¢åçš„æ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿåˆ†æå’Œè¯­ä¹‰åˆ†æè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGroqæä¾›çš„ç»“æœä¸å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬å‘ç°äº†GPT-4oå’ŒDeepSeek-V3ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10177v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨æ»¥ç”¨æ–‡æœ¬ï¼ˆå¦‚æ¨ç‰¹å’Œè¯„è®ºï¼‰çš„è½¬æ¢æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œå¯å°†å¸¦æœ‰ä»‡æ¨è¨€è®ºå’Œè„è¯çš„æ–‡æœ¬è½¬æ¢ä¸ºéæ»¥ç”¨æ–‡æœ¬ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬æ„å›¾ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šä¸ªå…ˆè¿›LLMåœ¨è¯†åˆ«æ»¥ç”¨æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶åœ¨æƒ…æ„Ÿåˆ†æå’Œè¯­ä¹‰åˆ†ææ–¹é¢å¯¹åŸå§‹å’Œè½¬æ¢åçš„æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºGroqä¸å…¶ä»–LLMsç›¸æ¯”è¡¨ç°ä¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ»¥ç”¨æ–‡æœ¬åˆ†ç±»å’Œè½¬æ¢æ–¹é¢å­˜åœ¨æ¢ç´¢ç©ºé—´ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å°†å¸¦æœ‰ä»‡æ¨è¨€è®ºå’Œè„è¯çš„æ»¥ç”¨æ–‡æœ¬è½¬æ¢ä¸ºéæ»¥ç”¨æ–‡æœ¬ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬æ„å›¾ã€‚</li>
<li>è¯„ä¼°äº†Geminiã€GPT-4oã€DeekSeekå’ŒGroqç­‰å…ˆè¿›LLMsåœ¨è¯†åˆ«æ»¥ç”¨æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>è½¬æ¢åçš„æ–‡æœ¬éœ€è¦æ¸…é™¤æ»¥ç”¨å’Œä¸é€‚å½“çš„å†…å®¹ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æƒ…æ„Ÿå’Œè¯­ä¹‰æ°´å¹³ã€‚</li>
<li>é€šè¿‡æƒ…æ„Ÿåˆ†æå’Œè¯­ä¹‰åˆ†æè¯„ä¼°äº†åŸå§‹å’Œè½¬æ¢åçš„æ•°æ®é›†ã€‚</li>
<li>Groqåœ¨ä¸å…¶ä»–LLMsçš„å¯¹æ¯”ä¸­è¡¨ç°ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b57cafeb11d06c1de24e657067124e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7399e4aa2089aa4ba8bdc80be2ff1cfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ffc3910d6e9401fdce1fbbcef04630c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e4566c24619d1b311fa87e82672a31e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6ca565041c5151355f6f6aee06f0ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e18641a52a86ec0d53fa5d05b6e748c4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GeLaCo-An-Evolutionary-Approach-to-Layer-Compression"><a href="#GeLaCo-An-Evolutionary-Approach-to-Layer-Compression" class="headerlink" title="GeLaCo: An Evolutionary Approach to Layer Compression"></a>GeLaCo: An Evolutionary Approach to Layer Compression</h2><p><strong>Authors:David Ponce, Thierry Etchegoyhen, Javier Del Ser</strong></p>
<p>Large Language Models (LLM) have achieved remarkable performance across a large number of tasks, but face critical deployment and usage barriers due to substantial computational requirements. Model compression methods, which aim to reduce model size while preserving its capacity, are an important means to mitigate these issues. Promising approaches along these lines, such as structured pruning, typically require costly empirical search for optimal variants and may run the risk of ignoring better solutions. In this work we introduce GeLaCo, an evolutionary approach to LLM compression via layer collapse. Our approach supports an efficient exploration of the compression solution space via population-based search and a module-wise similarity fitness function capturing attention, feed-forward, and hidden state representations. GeLaCo also supports both single and multi-objective evolutionary compression search, establishing the first Pareto frontier along compression and quality axes. We evaluate GeLaCo solutions via both perplexity-based and generative evaluations over foundational and instruction-tuned models, outperforming state-of-the-art alternatives. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤§é‡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚ï¼Œé¢ä¸´ç€å…³é”®çš„éƒ¨ç½²å’Œä½¿ç”¨éšœç¢ã€‚æ¨¡å‹å‹ç¼©æ–¹æ³•æ—¨åœ¨å‡å°æ¨¡å‹å¤§å°çš„åŒæ—¶ä¿ç•™å…¶å®¹é‡ï¼Œæ˜¯ç¼“è§£è¿™äº›é—®é¢˜çš„é‡è¦æ‰‹æ®µã€‚ä¸€äº›æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¦‚ç»“æ„åŒ–ä¿®å‰ªç­‰ï¼Œé€šå¸¸éœ€è¦æ˜‚è´µçš„ç»éªŒæœç´¢æ¥å¯»æ‰¾æœ€ä½³å˜ä½“ï¼Œå¹¶ä¸”å¯èƒ½å­˜åœ¨å¿½è§†æ›´å¥½è§£å†³æ–¹æ¡ˆçš„é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GeLaCoï¼Œä¸€ç§åŸºäºå±‚å´©æºƒçš„LLMå‹ç¼©è¿›åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒé€šè¿‡åŸºäºç§ç¾¤æœç´¢å’Œæ¨¡å—çº§ç›¸ä¼¼åº¦é€‚åº”åº¦å‡½æ•°æ¥æœ‰æ•ˆåœ°æ¢ç´¢å‹ç¼©è§£ç©ºé—´ï¼Œè¯¥å‡½æ•°æ•æ‰æ³¨æ„åŠ›ã€å‰é¦ˆå’Œéšè—çŠ¶æ€è¡¨ç¤ºã€‚GeLaCoè¿˜æ”¯æŒå•ç›®æ ‡å’Œå¤šç›®æ ‡è¿›åŒ–å‹ç¼©æœç´¢ï¼Œåœ¨å‹ç¼©å’Œè´¨é‡è½´æ–¹å‘ä¸Šå»ºç«‹é¦–ä¸ªå¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºå›°æƒ‘åº¦å’Œç”Ÿæˆè¯„ä¼°çš„åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ¥è¯„ä¼°GeLaCoè§£å†³æ–¹æ¡ˆï¼Œå…¶æ€§èƒ½ä¼˜äºæœ€æ–°æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10059v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç”±äºè®¡ç®—éœ€æ±‚å·¨å¤§ï¼Œå…¶åœ¨éƒ¨ç½²å’Œä½¿ç”¨ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æ¨¡å‹å‹ç¼©æ–¹æ³•æ—¨åœ¨å‡å°æ¨¡å‹å¤§å°åŒæ—¶ä¿ç•™å…¶æ€§èƒ½ï¼Œæ˜¯ç¼“è§£è¿™äº›é—®é¢˜çš„é‡è¦æ‰‹æ®µã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå±‚å´©å¡Œçš„LLMå‹ç¼©è¿›åŒ–æ–¹æ³•GeLaCoï¼Œå®ƒé€šè¿‡åŸºäºç§ç¾¤æœç´¢å’Œæ¨¡å—çº§ç›¸ä¼¼åº¦é€‚åº”åº¦å‡½æ•°æœ‰æ•ˆæ¢ç´¢å‹ç¼©è§£ç©ºé—´ï¼Œæ”¯æŒå•ç›®æ ‡å’Œå¤šç›®æ ‡è¿›åŒ–å‹ç¼©æœç´¢ï¼Œåœ¨å‹ç¼©å’Œè´¨é‡æ–¹é¢å»ºç«‹é¦–ä¸ªå¸•ç´¯æ‰˜å‰æ²¿ã€‚é€šè¿‡åŸºäºå›°æƒ‘åº¦å’Œç”Ÿæˆæ€§è¯„ä¼°ï¼Œåœ¨åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œéƒ¨ç½²å’Œä½¿ç”¨å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹å‹ç¼©æ˜¯ç¼“è§£è¿™äº›é—®é¢˜çš„é‡è¦æ‰‹æ®µï¼Œæ—¨åœ¨å‡å°æ¨¡å‹å¤§å°åŒæ—¶ä¿ç•™æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±‚å´©å¡Œçš„LLMå‹ç¼©è¿›åŒ–æ–¹æ³•GeLaCoï¼Œè¯¥æ–¹æ³•é€šè¿‡ç§ç¾¤æœç´¢å’Œæ¨¡å—çº§ç›¸ä¼¼åº¦é€‚åº”åº¦å‡½æ•°æœ‰æ•ˆæ¢ç´¢å‹ç¼©è§£ç©ºé—´ã€‚</li>
<li>GeLaCoæ”¯æŒå•ç›®æ ‡å’Œå¤šç›®æ ‡è¿›åŒ–å‹ç¼©æœç´¢ï¼Œå»ºç«‹å¸•ç´¯æ‰˜å‰æ²¿ï¼Œå³åœ¨å‹ç¼©å’Œè´¨é‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>GeLaCoåœ¨å›°æƒ‘åº¦å’Œç”Ÿæˆæ€§è¯„ä¼°æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†GeLaCoåœ¨åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bed2bc58441a97364a9d428cf035b99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5992081b42d82d083263d10d50c2e60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8306cedd3661e3204e7a205f4b28ecfb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CoSMo-A-Multimodal-Transformer-for-Page-Stream-Segmentation-in-Comic-Books"><a href="#CoSMo-A-Multimodal-Transformer-for-Page-Stream-Segmentation-in-Comic-Books" class="headerlink" title="CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic   Books"></a>CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic   Books</h2><p><strong>Authors:Marc Serra Ortega, Emanuele Vivoli, Artemis LlabrÃ©s, Dimosthenis Karatzas</strong></p>
<p>This paper introduces CoSMo, a novel multimodal Transformer for Page Stream Segmentation (PSS) in comic books, a critical task for automated content understanding, as it is a necessary first stage for many downstream tasks like character analysis, story indexing, or metadata enrichment. We formalize PSS for this unique medium and curate a new 20,800-page annotated dataset. CoSMo, developed in vision-only and multimodal variants, consistently outperforms traditional baselines and significantly larger general-purpose vision-language models across F1-Macro, Panoptic Quality, and stream-level metrics. Our findings highlight the dominance of visual features for comic PSS macro-structure, yet demonstrate multimodal benefits in resolving challenging ambiguities. CoSMo establishes a new state-of-the-art, paving the way for scalable comic book analysis. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†CoSMoï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€Transformerï¼Œç”¨äºæ¼«ç”»ä¹¦ä¸­çš„é¡µé¢æµåˆ†æ®µï¼ˆPSSï¼‰ï¼Œè¿™æ˜¯è‡ªåŠ¨å†…å®¹ç†è§£ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œå› ä¸ºå®ƒæ˜¯è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è§’è‰²åˆ†æã€æ•…äº‹ç´¢å¼•æˆ–å…ƒæ•°æ®ä¸°å¯Œï¼‰æ‰€å¿…éœ€çš„ç¬¬ä¸€é˜¶æ®µã€‚æˆ‘ä»¬å¯¹è¿™ä¸€ç‹¬ç‰¹åª’ä»‹çš„PSSè¿›è¡Œäº†å½¢å¼åŒ–æè¿°ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŒ…å«20,800é¡µçš„æ³¨é‡Šæ•°æ®é›†ã€‚CoSMoæœ‰ä»…é¢å‘è§†è§‰å’Œå¤šæ¨¡æ€ä¸¤ç§å˜ä½“ï¼Œåœ¨å„ç§æŒ‡æ ‡ï¼ˆå¦‚F1-å®ã€å…¨æ™¯è´¨é‡å’Œæµçº§æŒ‡æ ‡ï¼‰ä¸Šå‡è¡¨ç°ä¸€è‡´ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹å’Œè§„æ¨¡æ›´å¤§çš„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜è§†è§‰ç‰¹å¾åœ¨æ¼«ç”»PSSå®è§‚ç»“æ„ä¸­çš„ä¸»å¯¼åœ°ä½ï¼Œä½†åŒæ—¶ä¹Ÿè¯æ˜äº†å¤šæ¨¡æ€åœ¨è§£å†³å¤æ‚æ­§ä¹‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚CoSMoå»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œä¸ºå¯æ‰©å±•çš„æ¼«ç”»ä¹¦åˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoSMoçš„æ–°å‹å¤šæ¨¡æ€Transformeræ¨¡å‹ï¼Œç”¨äºæ¼«ç”»ä¹¦çš„é¡µé¢æµåˆ†å‰²ï¼ˆPSSï¼‰ã€‚è¯¥æ¨¡å‹å¯¹äºè‡ªåŠ¨åŒ–å†…å®¹ç†è§£è‡³å…³é‡è¦ï¼Œå¹¶ä¸”æ˜¯è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è§’è‰²åˆ†æã€æ•…äº‹ç´¢å¼•æˆ–å…ƒæ•°æ®ä¸°å¯Œï¼‰çš„å¿…è¦ç¬¬ä¸€é˜¶æ®µã€‚æœ¬æ–‡æ­£å¼ä¸ºè¿™ä¸€ç‹¬ç‰¹åª’ä»‹åˆ¶å®šPSSæ ‡å‡†ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„20,800é¡µæ³¨é‡Šæ•°æ®é›†ã€‚CoSMoåœ¨è§†è§‰å’Œå¤šåª’ä½“æ¨¡å¼ä¸‹å¼€å‘ï¼Œåœ¨F1-Macroã€å…¨æ™¯è´¨é‡å’Œæµçº§æŒ‡æ ‡æ–¹é¢å‡ä¼˜äºä¼ ç»ŸåŸºå‡†çº¿å’Œæ›´å¤§çš„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è§†è§‰ç‰¹å¾åœ¨æ¼«ç”»PSSå®è§‚ç»“æ„ä¸­çš„ä¸»å¯¼åœ°ä½ï¼Œä½†åŒæ—¶ä¹Ÿå±•ç¤ºäº†è§£å†³å¤æ‚æ­§ä¹‰çš„å¤šæ¨¡æ€ä¼˜åŠ¿ã€‚CoSMoä¸ºå¯æ‰©å±•çš„æ¼«ç”»ä¹¦åˆ†æå¥ å®šäº†æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoSMoæ˜¯ä¸€ç§ç”¨äºæ¼«ç”»ä¹¦çš„é¡µé¢æµåˆ†å‰²ï¼ˆPSSï¼‰çš„å¤šæ¨¡æ€Transformeræ¨¡å‹ï¼Œå¯¹è‡ªåŠ¨åŒ–å†…å®¹ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>æ¼«ç”»ä¹¦çš„PSSæ˜¯è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è§’è‰²åˆ†æã€æ•…äº‹ç´¢å¼•å’Œå…ƒæ•°æ®ä¸°å¯Œï¼‰çš„å¿…è¦ç¬¬ä¸€é˜¶æ®µã€‚</li>
<li>æœ¬æ–‡ä¸ºæ¼«ç”»è¿™ä¸€ç‹¬ç‰¹åª’ä»‹åˆ¶å®šäº†PSSæ ‡å‡†ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ³¨é‡Šæ•°æ®é›†ã€‚</li>
<li>CoSMoåœ¨è§†è§‰å’Œå¤šåª’ä½“æ¨¡å¼ä¸‹å¼€å‘ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¼ ç»ŸåŸºå‡†çº¿å’Œå¤§å‹é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†è§†è§‰ç‰¹å¾åœ¨æ¼«ç”»PSSå®è§‚ç»“æ„ä¸­çš„é‡è¦æ€§ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†å¤šæ¨¡æ€åœ¨è§£å†³å¤æ‚æ­§ä¹‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>CoSMoä¸ºå¤šæ¨¡æ€Transformeræ¨¡å‹åœ¨æ¼«ç”»åˆ†æä¸­çš„åº”ç”¨å»ºç«‹äº†æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7abff8d731149cd1639bcead3dda79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1661cae40a23d9d7686b2fd853d9cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a065e9e1da9a5f109ce205071e16ee98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c0ce12f2ae7ad28cb1cef99a44300a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Can-GPT-4o-mini-and-Gemini-2-0-Flash-Predict-Fine-Grained-Fashion-Product-Attributes-A-Zero-Shot-Analysis"><a href="#Can-GPT-4o-mini-and-Gemini-2-0-Flash-Predict-Fine-Grained-Fashion-Product-Attributes-A-Zero-Shot-Analysis" class="headerlink" title="Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion   Product Attributes? A Zero-Shot Analysis"></a>Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion   Product Attributes? A Zero-Shot Analysis</h2><p><strong>Authors:Shubham Shukla, Kunal Sonalkar</strong></p>
<p>The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the â€˜discovery experienceâ€™ of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (<a target="_blank" rel="noopener" href="https://github.com/yumingj/DeepFashion-MultiModal">https://github.com/yumingj/DeepFashion-MultiModal</a>) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction. </p>
<blockquote>
<p>æ—¶å°šé›¶å”®ä¸šåŠ¡çš„æ ¸å¿ƒåœ¨äºç†è§£äº§å“çš„èƒ½åŠ›ã€‚äº§å“å±æ€§æœ‰åŠ©äºæ ¹æ®ä¸šåŠ¡æµç¨‹ç†è§£äº§å“ã€‚è´¨é‡å±æ€§å¯ä»¥æé«˜å®¢æˆ·åœ¨æµè§ˆé›¶å”®ç½‘ç«™æä¾›çš„æ•°ç™¾ä¸‡äº§å“æ—¶çš„ä½“éªŒï¼Œä»è€Œåˆ›å»ºç»„ç»‡è‰¯å¥½çš„äº§å“ç›®å½•ã€‚æœ€ç»ˆï¼Œäº§å“å±æ€§ç›´æ¥å½±å“å®¢æˆ·çš„â€œå‘ç°ä½“éªŒâ€ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å¤šæ¨¡å¼æ•°æ®æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†çš„æ—¶å°šå±æ€§è¯†åˆ«æ–¹é¢ï¼Œå…¶æ€§èƒ½ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡å¯¹æ‰€æå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½å’Œé€Ÿåº¦ä»¥åŠæˆæœ¬æ•ˆç›Šä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œä¸»è¦åŒ…æ‹¬GPT-4o-miniå’ŒGemini 2.0 Flashã€‚æˆ‘ä»¬ä½¿ç”¨DeepFashion-MultiModalæ•°æ®é›†ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/yumingj/DeepFashion-MultiModal%EF%BC%89%E6%9D%A5%E8%AF%84%E4%BC%B0%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%97%B6%E5%B0%9A%E4%BA%A7%E5%93%81%E5%B1%9E%E6%80%A7%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E8%A1%A8%E7%8E%B0%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E5%AF%B918%E7%B1%BB%E6%97%B6%E5%B0%9A%E5%B1%9E%E6%80%A7%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%AF%84%E4%BC%B0%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E7%82%B9%E6%8F%90%E4%BE%9B%E4%BA%86%E8%A7%81%E8%A7%A3%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AA%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%83%8F%E4%BD%9C%E4%B8%BA%E4%BA%A7%E5%93%81%E4%BF%A1%E6%81%AF%E8%BE%93%E5%85%A5%E7%9A%84%E5%94%AF%E4%B8%80%E6%9D%A5%E6%BA%90%EF%BC%8C%E4%BB%A5%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8F%97%E9%99%90%E7%9A%84%E7%8E%AF%E5%A2%83%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E5%88%86%E6%9E%90%E8%A1%A8%E6%98%8E%EF%BC%8CGemini">https://github.com/yumingj/DeepFashion-MultiModalï¼‰æ¥è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æ—¶å°šäº§å“å±æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹18ç±»æ—¶å°šå±æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ºè¿™äº›æ¨¡å‹çš„ä¼˜ç‚¹æä¾›äº†è§è§£ã€‚æˆ‘ä»¬åªä½¿ç”¨å›¾åƒä½œä¸ºäº§å“ä¿¡æ¯è¾“å…¥çš„å”¯ä¸€æ¥æºï¼Œä»¥åˆ›å»ºä¸€ä¸ªå—é™çš„ç¯å¢ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒGemini</a> 2.0 Flashåœ¨æ‰€æœ‰å±æ€§ä¸Šçš„å®è§‚F1åˆ†æ•°è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°56.79%ï¼Œè€ŒGPT-4o-miniçš„å®è§‚F1åˆ†æ•°ä¸º43.28%ã€‚é€šè¿‡è¯¦ç»†çš„é”™è¯¯åˆ†æï¼Œæˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†å°†è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç”µå­å•†åŠ¡äº§å“å±æ€§ç›¸å…³ä»»åŠ¡çš„å®ç”¨è§è§£ï¼Œå¹¶å¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ–¹æ³•çš„å¿…è¦æ€§ã€‚è¿™é¡¹å·¥ä½œä¹Ÿä¸ºæœªæ¥çš„æ—¶å°šäººå·¥æ™ºèƒ½å’Œå¤šæ¨¡å¼å±æ€§æå–ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09950v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶å°šäº§å“å±æ€§è¯†åˆ«æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡å¯¹GPT-4o-miniå’ŒGemini 2.0 Flashç­‰å…ˆè¿›LLMsè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨DeepFashion-MultiModalæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒGemini 2.0 Flashæ€»ä½“è¡¨ç°æœ€ä½³ï¼ŒGPT-4o-miniæ¬¡ä¹‹ã€‚æ­¤ç ”ç©¶å¯¹äºåœ¨ç”µå­å•†åŠ¡ç”Ÿäº§ä¸­éƒ¨ç½²è¿™äº›LLMsè¿›è¡Œäº§å“å±æ€§è¯†åˆ«ä»»åŠ¡æä¾›äº†å®é™…è§è§£ï¼Œå¹¶å¼ºè°ƒäº†é¢†åŸŸç‰¹å®šå¾®è°ƒæ–¹æ³•çš„éœ€æ±‚ã€‚æ­¤ç ”ç©¶ä¸ºæ—¶å°šAIå’Œå¤šæ¨¡æ€å±æ€§æå–çš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶å°šäº§å“å±æ€§è¯†åˆ«æ–¹é¢çš„åº”ç”¨å—åˆ°å…³æ³¨ã€‚</li>
<li>GPT-4o-miniå’ŒGemini 2.0 Flashç­‰LLMsåœ¨DeepFashion-MultiModalæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>Gemini 2.0 Flashåœ¨æ—¶å°šå±æ€§è¯†åˆ«æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>LLMsåœ¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒè¿›è¡Œäº§å“å±æ€§è¯†åˆ«ä»»åŠ¡æ—¶ï¼Œéœ€è¦é¢†åŸŸç‰¹å®šçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>æ­¤ç ”ç©¶å¼ºè°ƒäº†é”™è¯¯åˆ†æçš„é‡è¦æ€§ï¼Œä¸ºæ”¹è¿›LLMsæä¾›äº†æ–¹å‘ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹äºæœªæ¥æ—¶å°šAIå’Œå¤šæ¨¡æ€å±æ€§æå–çš„ç ”ç©¶å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86422953c2cdf12473a8e7b1eaf342a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716ae70c11d1da2f858848749b5b08ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52b13a3e15104cef5ae1174dc5aed043.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Is-Human-Written-Data-Enough-The-Challenge-of-Teaching-Reasoning-to-LLMs-Without-RL-or-Distillation"><a href="#Is-Human-Written-Data-Enough-The-Challenge-of-Teaching-Reasoning-to-LLMs-Without-RL-or-Distillation" class="headerlink" title="Is Human-Written Data Enough? The Challenge of Teaching Reasoning to   LLMs Without RL or Distillation"></a>Is Human-Written Data Enough? The Challenge of Teaching Reasoning to   LLMs Without RL or Distillation</h2><p><strong>Authors:Wei Du, Branislav Kisacanin, George Armstrong, Shubham Toshniwal, Ivan Moshkov, Alexan Ayrapetyan, Sadegh Mahdavi, Dan Zhao, Shizhe Diao, Dragan Masulovic, Marius Stanean, Advaith Avadhanam, Max Wang, Ashmit Dutta, Shitij Govil, Sri Yanamandara, Mihir Tandon, Sriram Ananthakrishnan, Vedant Rathi, David Zhang, Joonseok Kang, Leon Luo, Titu Andreescu, Boris Ginsburg, Igor Gitman</strong></p>
<p>Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective. </p>
<blockquote>
<p>å…·å¤‡æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆé•¿æœŸã€æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹ï¼Œåœ¨ä¸åŒå¤æ‚çš„ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ æˆ–ä»æ›´å¼ºçš„æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰ä¸­è’¸é¦æ¥è·å¾—è¿™ç§æ¨ç†è½¨è¿¹ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ²¡æœ‰å¾®è°ƒï¼Œç®€å•çš„CoTæç¤ºä¹Ÿèƒ½æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æƒ³çŸ¥é“ï¼Œæ˜¯å¦å¯ä»¥ä½¿ç”¨æç¤ºæˆ–æœ€å°é™åº¦çš„è°ƒæ•´åœ¨åŸºç¡€æ¨¡å‹ä¸­è¯±å‘é•¿æœŸçš„CoTã€‚æˆ‘ä»¬ä»…ä½¿ç”¨æ¥è‡ªæ¨ç†æ¨¡å‹QwQ-32B-Previewçš„20ä¸ªé•¿æœŸCoTç¤ºä¾‹ï¼Œå¯¹åŸºç¡€æ¨¡å‹Qwen2.5-32Bè¿›è¡Œè½»åº¦å¾®è°ƒã€‚ç»“æœæ¨¡å‹çš„è¡¨ç°ä¼˜äºæ›´å¤§çš„Qwen2.5-Math-72B-Instructï¼Œè¡¨æ˜å°‘é‡é«˜è´¨é‡çš„ä¾‹å­å¯ä»¥è§£é”å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢ä½¿ç”¨éæ¨ç†æ¨¡å‹å’Œäººç±»æ³¨é‡Šè€…ç”Ÿæˆçš„CoTæ•°æ®ï¼Œå¹¶ç»“åˆæç¤ºå·¥ç¨‹ã€å¤šè½®ç¼–è¾‘å’Œç»“æ„æŒ‡å¯¼è¿›è¡Œå¢å¼ºã€‚ç„¶è€Œï¼Œå®ƒä»¬éƒ½æ— æ³•åŒ¹é…æ¨ç†æ¨¡å‹è½¨è¿¹çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜ä¸“å®¶CoTçš„æŸäº›æ½œåœ¨ç‰¹è´¨éš¾ä»¥å¤åˆ¶ã€‚æˆ‘ä»¬åˆ†æäº†å½±å“æ¨ç†è’¸é¦çš„å…³é”®å±æ€§ï¼Œå¦‚é—®é¢˜éš¾åº¦ã€å¤šæ ·æ€§å’Œç­”æ¡ˆé•¿åº¦ã€‚è™½ç„¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡ç²¾å¿ƒæŒ‘é€‰çš„äººç±»ç¼–å†™çš„CoTï¼Œå³ä½¿æ•°é‡å¾ˆå°ï¼Œä¹Ÿèƒ½æ¿€æ´»åŸºç¡€æ¨¡å‹ä¸­çš„æ¨ç†è¡Œä¸ºã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬äººç±»ä½œè€…åœ¨å„ä¸ªç²¾ç‚¼é˜¶æ®µçš„æ•°æ®é›†ï¼Œå¹¶é‚€è¯·è¿›ä¸€æ­¥è°ƒæŸ¥å°è§„æ¨¡æ¨ç†ç›‘ç£å¦‚æ­¤æœ‰æ•ˆçš„åŸå› ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09850v2">PDF</a> Accepted at the Second AI for Math Workshop at the 42nd International   Conference on Machine Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºèƒ½è¿›è¡Œæ¨ç†çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆé•¿çš„ã€æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹ï¼Œåœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡æç¤ºå·¥ç¨‹ã€å¤šéç¼–è¾‘å’Œç»“æ„æŒ‡å¯¼ï¼Œä½¿ç”¨æ¥è‡ªéæ¨ç†æ¨¡å‹å’Œäººç±»æ³¨é‡Šè€…çš„æ€ç»´é“¾æ•°æ®ï¼Œä½†éƒ½æ— æ³•åŒ¹é…æ¨ç†æ¨¡å‹è½¨è¿¹çš„æ€§èƒ½ã€‚åˆ†æè¡¨æ˜ï¼ŒæŸäº›ä¸“å®¶æ€ç»´é“¾çš„æ½œåœ¨ç‰¹è´¨éš¾ä»¥å¤åˆ¶ã€‚å°½ç®¡å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†ç²¾å¿ƒç¼–å†™çš„äººç±»æ€ç»´é“¾ï¼Œå³ä½¿æ•°é‡å°‘ï¼Œä¹Ÿèƒ½æ¿€æ´»åŸºç¡€æ¨¡å‹çš„æ¨ç†è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†èƒ½åŠ›å¼ºçš„è¯­è¨€æ¨¡å‹èƒ½é€šè¿‡ç”Ÿæˆé•¿çš„ã€æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹åœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>ä»…é€šè¿‡æç¤ºæˆ–è½»å¾®è°ƒæ•´ï¼Œå°±å¯ä»¥åœ¨åŸºç¡€æ¨¡å‹ä¸­å¼•å‘é•¿çš„CoTã€‚</li>
<li>ä½¿ç”¨æ¥è‡ªéæ¨ç†æ¨¡å‹å’Œäººç±»æ³¨é‡Šè€…çš„CoTæ•°æ®ï¼Œé€šè¿‡å¢å¼ºæç¤ºå·¥ç¨‹ã€å¤šéç¼–è¾‘å’Œç»“æ„æŒ‡å¯¼ï¼Œä½†æ— æ³•åŒ¹é…æ¨ç†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æŸäº›ä¸“å®¶CoTçš„æ½œåœ¨ç‰¹è´¨éš¾ä»¥å¤åˆ¶ã€‚</li>
<li>ç²¾å¿ƒç¼–åˆ¶çš„äººç±»æ€ç»´é“¾ï¼Œå³ä½¿æ•°é‡å°‘ï¼Œä¹Ÿèƒ½æ¿€æ´»æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é—®é¢˜éš¾åº¦ã€å¤šæ ·æ€§å’Œç­”æ¡ˆé•¿åº¦ç­‰å…³é”®å› ç´ å½±å“æ¨ç†è’¸é¦çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b3dbfd602dc5f3fc4ebca7d227485fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d4ba0baebef531ab0f7af930e2b5f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f42dc98c1dc4246e3da28878bf6b7e10.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MENTOR-Efficient-Multimodal-Conditioned-Tuning-for-Autoregressive-Vision-Generation-Models"><a href="#MENTOR-Efficient-Multimodal-Conditioned-Tuning-for-Autoregressive-Vision-Generation-Models" class="headerlink" title="MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive   Vision Generation Models"></a>MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive   Vision Generation Models</h2><p><strong>Authors:Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Jiuxiang Gu, Wen Xiao, Junjie Hu</strong></p>
<p>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: <a target="_blank" rel="noopener" href="https://github.com/HaozheZhao/MENTOR">https://github.com/HaozheZhao/MENTOR</a> </p>
<blockquote>
<p>æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹äº§ç”Ÿäº†é«˜è´¨é‡çš„ç»“æœï¼Œä½†åœ¨ç²¾ç¡®è§†è§‰æ§åˆ¶ã€å¹³è¡¡å¤šæ¨¡å¼è¾“å…¥å’Œå¤æ‚å¤šæ¨¡å¼å›¾åƒç”Ÿæˆçš„è®­ç»ƒéœ€æ±‚æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MENTORï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤šæ¨¡å¼æ¡ä»¶è°ƒæ•´çš„è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶ï¼Œç”¨äºè‡ªå›å½’å¤šæ¨¡å¼å›¾åƒç”Ÿæˆã€‚MENTORç»“åˆARå›¾åƒç”Ÿæˆå™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è¾…åŠ©é€‚é…å™¨æˆ–äº¤å‰æ³¨æ„æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šæ¨¡å¼è¾“å…¥å’Œå›¾åƒè¾“å‡ºä¹‹é—´çš„ç²¾ç»†é¢—ç²’åº¦ã€ä»¤ç‰Œçº§å¯¹é½ã€‚ä¸¤é˜¶æ®µè®­ç»ƒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å¤šæ¨¡å¼å¯¹é½é˜¶æ®µï¼Œå»ºç«‹ç¨³å¥çš„åƒç´ å’Œè¯­ä¹‰çº§å¯¹é½ï¼Œç„¶åæ˜¯ï¼ˆ2ï¼‰å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œå¹³è¡¡å¤šæ¨¡å¼è¾“å…¥çš„é›†æˆï¼Œå¢å¼ºç”Ÿæˆçš„å¯æ§æ€§ã€‚å°½ç®¡æ¨¡å‹è§„æ¨¡é€‚ä¸­ï¼ŒåŸºç¡€ç»„ä»¶ä¸å¤Ÿç†æƒ³ï¼Œè®­ç»ƒèµ„æºæœ‰é™ï¼Œä½†MENTORåœ¨DreamBench++åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨æ¦‚å¿µä¿ç•™å’Œæç¤ºéµå¾ªæ–¹é¢è¶…è¿‡äº†ç«äº‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ã€å¹¿æ³›çš„ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaozheZhao/MENTOR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HaozheZhao/MENTORæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09574v1">PDF</a> 24 pages,12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒçš„æŠ€æœ¯å–å¾—äº†é«˜è´¨é‡çš„ç»“æœï¼Œä½†åœ¨ç²¾ç¡®è§†è§‰æ§åˆ¶ã€å¹³è¡¡å¤šæ¨¡å¼è¾“å…¥å’Œå¤æ‚å¤šæ¨¡å¼å›¾åƒç”Ÿæˆçš„è®­ç»ƒæ•ˆç‡æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†MENTORè¿™ä¸€æ–°å‹çš„è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å®ç°å¤šæ¨¡å¼æ¡ä»¶ä¸‹çš„è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚MENTORç»“åˆäº†ARå›¾åƒç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è¾…åŠ©é€‚é…å™¨æˆ–è·¨æ³¨æ„æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šæ¨¡å¼è¾“å…¥ä¸å›¾åƒè¾“å‡ºä¹‹é—´çš„ç²¾ç»†é¢—ç²’åº¦ã€æ ‡è®°çº§åˆ«çš„å¯¹é½ã€‚ä¸¤é˜¶æ®µè®­ç»ƒåŒ…æ‹¬å»ºç«‹åƒç´ å’Œè¯­ä¹‰çº§åˆ«çš„å¯¹é½çš„å¤šæ¨¡å¼å¯¹é½é˜¶æ®µï¼Œä»¥åŠå¹³è¡¡å¤šæ¨¡å¼è¾“å…¥é›†æˆå¹¶å¢å¼ºç”Ÿæˆå¯æ§æ€§çš„å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚å³ä½¿åœ¨æ¨¡å‹è§„æ¨¡é€‚ä¸­ã€åŸºç¡€ç»„ä»¶æ¬¡ä¼˜å’Œè®­ç»ƒèµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼ŒMENTORåœ¨DreamBench++åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨æ¦‚å¿µä¿ç•™å’Œæç¤ºéµå¾ªæ–¹é¢è¶…è¶Šäº†ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ã€å¹¿æ³›çš„ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¹Ÿæ¯”æ‰©æ•£æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MENTORæ˜¯ä¸€ä¸ªé’ˆå¯¹è‡ªå›å½’å¤šæ¨¡å¼å›¾åƒç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç²¾ç¡®è§†è§‰æ§åˆ¶ã€å¤šæ¨¡å¼è¾“å…¥å¹³è¡¡å’Œå¤æ‚å›¾åƒç”Ÿæˆè®­ç»ƒæ•ˆç‡çš„é—®é¢˜ã€‚</li>
<li>MENTORç»“åˆARå›¾åƒç”Ÿæˆå™¨ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒå®ç°å¤šæ¨¡å¼è¾“å…¥ä¸å›¾åƒè¾“å‡ºçš„ç²¾ç»†å¯¹é½ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µå»ºç«‹åƒç´ å’Œè¯­ä¹‰çº§åˆ«çš„å¯¹é½ï¼Œç¬¬äºŒé˜¶æ®µå¹³è¡¡å¤šæ¨¡å¼è¾“å…¥é›†æˆå¹¶å¢å¼ºç”Ÿæˆçš„å¯æ§æ€§ã€‚</li>
<li>MENTORåœ¨DreamBench++åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ¦‚å¿µä¿ç•™å’Œæç¤ºéµå¾ªæ–¹é¢ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒMENTORåœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ã€ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>MENTORçš„æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-258f0efbb7e914dddded9081e53cf895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cca8e535883a7753dc2b38552284b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a1768a0c75ef6c0209965c30036217.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4ad4fdf8ff88ba7ddff5b097b6b6072.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="On-the-Effect-of-Instruction-Tuning-Loss-on-Generalization"><a href="#On-the-Effect-of-Instruction-Tuning-Loss-on-Generalization" class="headerlink" title="On the Effect of Instruction Tuning Loss on Generalization"></a>On the Effect of Instruction Tuning Loss on Generalization</h2><p><strong>Authors:Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty</strong></p>
<p>Instruction Tuning has emerged as a pivotal post-training paradigm that enables pre-trained language models to better follow user instructions. Despite its significance, little attention has been given to optimizing the loss function used. A fundamental, yet often overlooked, question is whether the conventional auto-regressive objective - where loss is computed only on response tokens, excluding prompt tokens - is truly optimal for instruction tuning. In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning. Through extensive experiments on five language models of different families and scale, three finetuning datasets of different sizes, and five diverse evaluation benchmarks, we show that the standard instruction tuning loss often yields suboptimal performance and limited robustness to input prompt variations. We find that a low-to-moderate weight for prompt tokens coupled with a moderate-to-high weight for response tokens yields the best-performing models across settings and also serve as better starting points for the subsequent preference alignment training. These findings highlight the need to reconsider instruction tuning loss and offer actionable insights for developing more robust and generalizable models. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/kowndinya-renduchintala/WIT">https://github.com/kowndinya-renduchintala/WIT</a>. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²ç»æˆä¸ºä¸€ä¸ªé‡è¦çš„åè®­ç»ƒæ¨¡å¼ï¼Œå®ƒä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚å°½ç®¡å®ƒçš„æ„ä¹‰é‡å¤§ï¼Œä½†å¾ˆå°‘æœ‰äººå…³æ³¨ä¼˜åŒ–æ‰€ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚ä¸€ä¸ªåŸºæœ¬è€Œå¸¸è¢«å¿½è§†çš„é—®é¢˜æ˜¯ï¼Œä¼ ç»Ÿçš„è‡ªå›å½’ç›®æ ‡ï¼ˆæŸå¤±ä»…è®¡ç®—å“åº”æ ‡è®°ï¼Œä¸åŒ…æ‹¬æç¤ºæ ‡è®°ï¼‰æ˜¯å¦çœŸçš„æ˜¯æŒ‡ä»¤è°ƒä¼˜çš„æœ€ä¼˜é€‰æ‹©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æŒ‡ä»¤å¾®è°ƒæŸå¤±ä¸­æç¤ºæ ‡è®°å’Œå“åº”æ ‡è®°å·®å¼‚åŠ æƒçš„å½±å“ï¼Œå¹¶æå‡ºäº†åŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰ä½œä¸ºä¼ ç»ŸæŒ‡ä»¤è°ƒä¼˜çš„æ›´å¥½æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å¯¹äº”ä¸ªä¸åŒå®¶æ—å’Œè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€ä¸‰ä¸ªä¸åŒå¤§å°çš„å¾®è°ƒæ•°æ®é›†å’Œäº”ä¸ªä¸åŒçš„è¯„ä¼°åŸºå‡†è¿›è¡Œå¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ ‡å‡†æŒ‡ä»¤è°ƒä¼˜æŸå¤±é€šå¸¸ä¼šå¯¼è‡´æ¬¡ä¼˜æ€§èƒ½å’Œæœ‰é™çš„åº”å¯¹è¾“å…¥æç¤ºå˜åŒ–çš„é²æ£’æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹æç¤ºæ ‡è®°èµ‹äºˆè¾ƒä½è‡³ä¸­ç­‰çš„æƒé‡ï¼ŒåŒæ—¶å¯¹å“åº”æ ‡è®°èµ‹äºˆä¸­ç­‰è‡³è¾ƒé«˜çš„æƒé‡ï¼Œèƒ½åœ¨å„ç§è®¾ç½®ä¸­å–å¾—æœ€ä½³æ€§èƒ½çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä½œä¸ºåç»­åå¥½å¯¹é½è®­ç»ƒçš„æ›´å¥½èµ·ç‚¹ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é‡æ–°è€ƒè™‘æŒ‡ä»¤è°ƒä¼˜æŸå¤±çš„å¿…è¦æ€§å’Œå¼€å‘æ›´ç¨³å¥å’Œé€šç”¨æ¨¡å‹çš„å®ç”¨è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/kowndinya-renduchintala/WIT%E3%80%82">https://github.com/kowndinya-renduchintala/WITã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07817v2">PDF</a> To appear in Transactions of the Association for Computational   Linguistics (TACL)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æŒ‡ä»¤å¾®è°ƒä¸­çš„æŸå¤±å‡½æ•°ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†åŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰æ–¹æ³•ï¼Œé€šè¿‡å·®å¼‚åŒ–åœ°ç»™æŒ‡ä»¤å’Œå“åº”ä»¤ç‰ŒåŠ æƒæ¥æ”¹è¿›æŸå¤±å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œæ ‡å‡†æŒ‡ä»¤å¾®è°ƒæŸå¤±æ€§èƒ½ä¸ä½³ï¼Œä¸”å¯¹è¾“å…¥æŒ‡ä»¤å˜åŒ–çš„é²æ£’æ€§æœ‰é™ã€‚æœ€ä½³æ€§èƒ½æ¨¡å‹æ˜¯åœ¨æŒ‡ä»¤ä»¤ç‰Œèµ‹äºˆè¾ƒä½è‡³ä¸­ç­‰æƒé‡ï¼Œå“åº”ä»¤ç‰Œèµ‹äºˆä¸­ç­‰è‡³é«˜æƒé‡çš„æƒ…å†µä¸‹è·å¾—çš„ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´ç¨³å¥å’Œé€šç”¨çš„æ¨¡å‹æä¾›äº†é‡æ–°è€ƒè™‘æŒ‡ä»¤å¾®è°ƒæŸå¤±å’Œå¯è¡Œçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒæ˜¯ä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ›´å¥½åœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„å…³é”®åè®­ç»ƒèŒƒå¼ã€‚</li>
<li>å¸¸è§„çš„è‡ªå›å½’ç›®æ ‡åœ¨æŒ‡ä»¤å¾®è°ƒä¸­å¯èƒ½å¹¶éæœ€ä½³ã€‚</li>
<li>åŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰æ–¹æ³•é€šè¿‡å·®å¼‚åŒ–åœ°ç»™æŒ‡ä»¤å’Œå“åº”ä»¤ç‰ŒåŠ æƒæ¥æ”¹è¿›æŸå¤±å‡½æ•°ã€‚</li>
<li>æ ‡å‡†æŒ‡ä»¤å¾®è°ƒæŸå¤±æ€§èƒ½ä¸ä½³ï¼Œä¸”å¯¹è¾“å…¥æŒ‡ä»¤å˜åŒ–çš„é²æ£’æ€§æœ‰é™ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹æ˜¯åœ¨ç»™æŒ‡ä»¤ä»¤ç‰Œèµ‹äºˆè¾ƒä½è‡³ä¸­ç­‰æƒé‡ï¼Œè€Œç»™å“åº”ä»¤ç‰Œèµ‹äºˆä¸­ç­‰è‡³é«˜æƒé‡çš„æƒ…å†µä¸‹è·å¾—çš„ã€‚</li>
<li>WITæ–¹æ³•èƒ½æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a80646de61ca0df3d97d29092a7e442c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c21fccfef3ce9b7cf640d635de8c8811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82d9b80b68df3ec2a9b811acfedbbe1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9c54606002520719f19cfcf7778e9be.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching"><a href="#Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching" class="headerlink" title="Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching"></a>Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching</h2><p><strong>Authors:Thomas Savage</strong></p>
<p>Fine-tuning methods such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) have demonstrated success in training large language models (LLMs) for single-turn tasks. However, these methods fall short in multi-turn applications, such as diagnostic patient interviewing, where understanding how early conversational turns influence downstream completions and outcomes is essential. In medicine, a multi-turn perspective is critical for learning diagnostic schemas and better understanding conversation dynamics. To address this gap, I introduce Savage Conversation Forests (SCF), a reinforcement learning framework that leverages a branched conversation architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple possible conversation continuations at each turn, enabling the model to learn how different early responses affect downstream interactions and diagnostic outcomes. In experiments simulating doctor-patient conversations, SCF with branching outperforms linear conversation architectures on diagnostic accuracy. I hypothesize that SCFâ€™s improvements stem from its ability to provide richer, interdependent training signals across conversation turns. These results suggest that a branched training architecture is an important strategy for fine tuning LLMs in complex multi-turn conversational tasks. </p>
<blockquote>
<p>é’ˆå¯¹å•ä¸€ä»»åŠ¡çš„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰å¾®è°ƒæ–¹æ³•å·²ç»å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¤šä»»åŠ¡å¯¹è¯åœºæ™¯ï¼Œå¦‚åœ¨è¯Šæ–­ç—…äººè®¿è°ˆä¸­ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°å‡ºå±€é™æ€§ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œç†è§£æ—©æœŸå¯¹è¯å›åˆå¦‚ä½•å½±å“åç»­å¯¹è¯å†…å®¹å’Œç»“æœè‡³å…³é‡è¦ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œé‡‡ç”¨å¤šä»»åŠ¡å¯¹è¯è§†è§’å¯¹äºå­¦ä¹ è¯Šæ–­æ¨¡å¼å’Œæ›´å¥½åœ°æŠŠæ¡å¯¹è¯åŠ¨æ€è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸è¶³ï¼Œæˆ‘å¼•å…¥äº†åä¸ºâ€œé‡è›®å¯¹è¯æ£®æ—â€ï¼ˆSCFï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„æ¥å¾®è°ƒLLMï¼Œä»¥åº”å¯¹å¤šä»»åŠ¡å¯¹è¯åœºæ™¯ã€‚SCFèƒ½å¤Ÿåœ¨æ¯ä¸€å¯¹è¯å›åˆç”Ÿæˆå¤šç§å¯èƒ½çš„å»¶ç»­å¯¹è¯å†…å®¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸åŒçš„æ—©æœŸå›åº”å¦‚ä½•å½±å“åç»­çš„äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚åœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­ï¼Œä½¿ç”¨åˆ†æ”¯ç­–ç•¥çš„SCFåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢è¶…è¿‡äº†çº¿æ€§å¯¹è¯æ¶æ„ã€‚æˆ‘è®¤ä¸ºSCFçš„æ”¹è¿›æºäºå…¶åœ¨ä¸åŒå¯¹è¯å›åˆæä¾›ä¸°å¯Œä¸”ç›¸äº’ä¾èµ–çš„è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ†æ”¯è®­ç»ƒæ¶æ„æ˜¯åœ¨å¤æ‚çš„å¤šä»»åŠ¡å¯¹è¯åœºæ™¯ä¸­å¾®è°ƒLLMçš„é‡è¦ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04099v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é€šè¿‡Direct Preference Optimization (DPO)å’ŒGroup Relative Policy Optimization (GRPO)ç­‰å¾®è°ƒæ–¹æ³•ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•å›åˆä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚ä½†åœ¨å¤šå›åˆåº”ç”¨ä¸­ï¼Œå¦‚è¯Šæ–­ç—…äººè®¿è°ˆç­‰ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°æ¬ ä½³ã€‚ç†è§£æ—©æœŸå¯¹è¯å›åˆå¦‚ä½•å½±å“ä¸‹æ¸¸å®Œæˆå’Œç»“æœå¯¹äºå­¦ä¹ è¯Šæ–­æ¨¡å¼å’Œæ›´å¥½åœ°ç†è§£å¯¹è¯åŠ¨æ€è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†Savage Conversation Forests (SCF)è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œç”¨äºå¤šå›åˆå¯¹è¯ã€‚SCFåœ¨æ¯ä¸ªå›åˆç”Ÿæˆå¤šç§å¯èƒ½çš„å¯¹è¯å»¶ç»­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸åŒçš„æ—©æœŸå›åº”å¦‚ä½•å½±å“ä¸‹æ¸¸äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚åœ¨æ¨¡æ‹ŸåŒ»ç”Ÿç—…äººå¯¹è¯çš„å®éªŒä¸­ï¼Œå¸¦æœ‰åˆ†æ”¯çš„SCFåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢ä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚å‡è®¾SCFçš„æ”¹è¿›æºäºå…¶åœ¨å¯¹è¯å›åˆä¸­æä¾›ä¸°å¯Œã€ç›¸äº’ä¾å­˜è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œåˆ†æ”¯è®­ç»ƒæ¶æ„æ˜¯å¤æ‚å¤šå›åˆå¯¹è¯ä»»åŠ¡ä¸­å¾®è°ƒLLMçš„é‡è¦ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DPOå’ŒGRPOç­‰å¾®è°ƒæ–¹æ³•åœ¨å•å›åˆä»»åŠ¡ä¸­æˆåŠŸè®­ç»ƒLLMã€‚</li>
<li>åœ¨å¤šå›åˆå¯¹è¯åº”ç”¨ä¸­ï¼Œç†è§£æ—©æœŸå¯¹è¯å›åˆçš„å½±å“è‡³å…³é‡è¦ã€‚</li>
<li>SCFæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„å¾®è°ƒLLMï¼Œç”¨äºå¤šå›åˆå¯¹è¯ã€‚</li>
<li>SCFç”Ÿæˆå¤šç§å¯èƒ½çš„å¯¹è¯å»¶ç»­ï¼Œä»¥å­¦ä¹ ä¸åŒçš„æ—©æœŸå›åº”å¦‚ä½•å½±å“ä¸‹æ¸¸äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚</li>
<li>åœ¨æ¨¡æ‹ŸåŒ»ç”Ÿç—…äººå¯¹è¯çš„å®éªŒä¸­ï¼ŒSCFçš„åˆ†æ”¯ç»“æ„åœ¨è¯Šæ–­å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>SCFçš„æ”¹è¿›æºäºå…¶æä¾›ä¸°å¯Œã€ç›¸äº’ä¾å­˜çš„è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f98b4223d0644d3fe9a70c330ceae3cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cd4e15ec373b13b3a016e880e007bbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb7560e5ff18f9e64988bc0ea54c738f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34e43544546ec15afd965f333d88e791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b558fd94589aa274609ce601c4667f4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ProtocolLLM-RTL-Benchmark-for-SystemVerilog-Generation-of-Communication-Protocols"><a href="#ProtocolLLM-RTL-Benchmark-for-SystemVerilog-Generation-of-Communication-Protocols" class="headerlink" title="ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication   Protocols"></a>ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication   Protocols</h2><p><strong>Authors:Arnav Sheth, Ivaxi Sheth, Mario Fritz</strong></p>
<p>Recent advances in large language models (LLMs) have demonstrated strong performance in generating code for general-purpose programming languages. However, their potential for hardware description languages (HDLs), such as SystemVerilog, remains largely unexplored. HDL code generation poses unique challenges due to strict timing semantics, concurrency, and synthesizability constraints essential for correct hardware functionality. Further, HDL-based design flows encompass a broad set of tasks beyond structural code generation, including testbench development, assertion-based verification, timing closure, and protocol-level integration for on-chip communication. In this work, we evaluate the capabilities of both open-source and state-of-the-art LLMs in generating synthesizable and functionally accurate SystemVerilog implementations of widely used communication protocols that are critical components of embedded and System-on-Chip (SoC) systems. We introduce ProtocolLLM, the first benchmark suite specifically targeting these protocols with tasks spanning multiple design abstraction levels and varying prompt specificity. Our evaluation method also focuses on timing correctness in addition to synthesizability and syntactic correctness. We observe that most of the models fail to generate SystemVerilog code for communication protocols that follow timing constrains. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨ç”Ÿæˆé€šç”¨ç¼–ç¨‹è¯­è¨€ä»£ç æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆå¦‚SystemVerilogï¼‰æ–¹é¢çš„æ½œåŠ›ä»å¾…å¤§é‡æ¢ç´¢ã€‚ç”±äºä¸¥æ ¼çš„æ—¶åºè¯­ä¹‰ã€å¹¶å‘æ€§å’Œåˆæˆæ€§çº¦æŸå¯¹äºç¡¬ä»¶åŠŸèƒ½æ­£ç¡®æ€§è‡³å…³é‡è¦ï¼ŒHDLä»£ç ç”Ÿæˆå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒåŸºäºHDLçš„è®¾è®¡æµç¨‹æ¶µç›–äº†è¶…å‡ºç»“æ„ä»£ç ç”Ÿæˆçš„ä¸€ç³»åˆ—ä»»åŠ¡ï¼ŒåŒ…æ‹¬æµ‹è¯•å¹³å°å¼€å‘ã€åŸºäºæ–­è¨€çš„éªŒè¯ã€æ—¶åºé—­åˆå’ŒèŠ¯ç‰‡ä¸Šé€šä¿¡çš„åè®®çº§é›†æˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¼€æºå’Œæœ€æ–°LLMåœ¨ç”Ÿæˆåˆæˆå’ŒåŠŸèƒ½å‡†ç¡®çš„SystemVerilogå®ç°æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™äº›å®ç°æ˜¯å¹¿æ³›ä½¿ç”¨çš„é€šä¿¡åè®®çš„å…³é”®ç»„ä»¶ï¼Œä¹Ÿæ˜¯åµŒå…¥å¼å’Œç‰‡ä¸Šç³»ç»Ÿï¼ˆSoCï¼‰ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†ProtocolLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è¿™äº›åè®®çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œä»»åŠ¡æ¶µç›–å¤šä¸ªè®¾è®¡æŠ½è±¡å±‚æ¬¡å’Œä¸åŒçš„æç¤ºç‰¹å¼‚æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ–¹æ³•è¿˜é‡ç‚¹å…³æ³¨æ—¶åºæ­£ç¡®æ€§ï¼Œä»¥åŠåˆæˆèƒ½åŠ›å’Œè¯­æ³•æ­£ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°å¤§å¤šæ•°æ¨¡å‹éƒ½æ— æ³•ç”Ÿæˆéµå¾ªæ—¶åºçº¦æŸçš„é€šä¿¡åè®®çš„SystemVerilogä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07945v2">PDF</a> Accepted at MLSysArch@ISCA 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé€šç”¨ç¼–ç¨‹è¯­è¨€ä»£ç æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLsï¼‰å¦‚SystemVerilogæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç”Ÿæˆç¬¦åˆè¦æ±‚çš„SystemVerilogä»£ç é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚ä¸¥æ ¼çš„æ—¶åºè¯­ä¹‰ã€å¹¶å‘æ€§å’Œåˆæˆæ€§çº¦æŸç­‰ã€‚æœ¬æ–‡è¯„ä¼°äº†å¼€æºå’Œæœ€æ–°LLMåœ¨ç”Ÿæˆåˆæˆå’ŒåŠŸèƒ½å‡†ç¡®çš„SystemVerilogå®ç°æ–¹é¢çš„èƒ½åŠ›ï¼Œé’ˆå¯¹åµŒå…¥å¼å’Œç‰‡ä¸Šç³»ç»Ÿï¼ˆSoCï¼‰ç³»ç»Ÿä¸­å¹¿æ³›ä½¿ç”¨çš„é€šä¿¡åè®®è¿›è¡Œã€‚æˆ‘ä»¬å¼•å…¥äº†ProtocolLLMï¼Œè¯¥å¥—ä»¶ä¸“é—¨é’ˆå¯¹è¿™äº›åè®®è¿›è¡Œæµ‹è¯•ï¼Œä»»åŠ¡æ¶µç›–å¤šä¸ªè®¾è®¡æŠ½è±¡å±‚æ¬¡å’Œä¸åŒæç¤ºç‰¹å¼‚æ€§ã€‚è¯„ä¼°æ–¹æ³•è¿˜å…³æ³¨æ—¶åºæ­£ç¡®æ€§ã€åˆæˆæ€§å’Œè¯­æ³•æ­£ç¡®æ€§ã€‚è§‚å¯Ÿåˆ°å¤§å¤šæ•°æ¨¡å‹åœ¨ç”Ÿæˆéµå¾ªæ—¶åºçº¦æŸçš„é€šä¿¡åè®®SystemVerilogä»£ç æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLsï¼‰å¦‚SystemVerilogæ–¹é¢çš„æ½œåŠ›å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>ç”ŸæˆHDLä»£ç é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸¥æ ¼çš„æ—¶åºè¯­ä¹‰ã€å¹¶å‘æ€§å’Œåˆæˆæ€§çº¦æŸã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†LLMåœ¨ç”Ÿæˆåˆæˆå’ŒåŠŸèƒ½å‡†ç¡®çš„SystemVerilogå®ç°æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ProtocolLLMæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹é€šä¿¡åè®®è¿›è¡Œæµ‹è¯•çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>ProtocolLLMçš„ä»»åŠ¡æ¶µç›–å¤šä¸ªè®¾è®¡æŠ½è±¡å±‚æ¬¡å’Œä¸åŒæç¤ºç‰¹å¼‚æ€§ã€‚</li>
<li>è¯„ä¼°æ–¹æ³•åŒ…æ‹¬æ£€æŸ¥åˆæˆæ€§ã€è¯­æ³•æ­£ç¡®æ€§å’Œæ—¶åºæ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc6f07059d6955a948f4e1b5b5e49af5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bb148321e984b59bb6a4f1f345d5119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420998106f1cbc8b075f96ca5c9f95f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82b097580e8c9a1c42bd352282f281b3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-608a5f1350d91cee04b2f71d28334b6f.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  ViTCoT Video-Text Interleaved Chain-of-Thought for Boosting Video   Understanding in Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
