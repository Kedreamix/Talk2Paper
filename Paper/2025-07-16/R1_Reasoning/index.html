<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-16  ARMOR Aligning Secure and Safe Large Language Models via Meticulous   Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c344a8b0018e5f5d067bd1cc640872a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-16-æ›´æ–°"><a href="#2025-07-16-æ›´æ–°" class="headerlink" title="2025-07-16 æ›´æ–°"></a>2025-07-16 æ›´æ–°</h1><h2 id="ARMOR-Aligning-Secure-and-Safe-Large-Language-Models-via-Meticulous-Reasoning"><a href="#ARMOR-Aligning-Secure-and-Safe-Large-Language-Models-via-Meticulous-Reasoning" class="headerlink" title="ARMOR: Aligning Secure and Safe Large Language Models via Meticulous   Reasoning"></a>ARMOR: Aligning Secure and Safe Large Language Models via Meticulous   Reasoning</h2><p><strong>Authors:Zhengyue Zhao, Yingzi Ma, Somesh Jha, Marco Pavone, Chaowei Xiao</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable generative capabilities. However, their susceptibility to misuse has raised significant safety concerns. While post-training safety alignment methods have been widely adopted, LLMs remain vulnerable to malicious instructions that can bypass safety constraints. Recent efforts have introduced inference-time safety reasoning (system-2 alignment), where LLMs conduct a reasoning process to perform safety verification before final response. We show, however, that these checks are driven by ad-hoc reasoning that diverges from the structured human process, where they first discern a userâ€™s true intent, then evaluate the associated risk based on the true intent. Consequently, these defenses remain vulnerable to sophisticated jailbreak prompts that cloak harmful goals in seemingly benign language. To build secure and safe LLMs, we propose a reasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc chains of thought reasoning process with human-aligned, structured one. At inference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the userâ€™s core intent while discarding deceptive instructions, and (3) applies a policy-grounded safety analysis to the purified request. ARMOR is evaluated on adaptive jailbreak attacks and multiple safety benchmarks, and a test-time scaling is conducted to further improve its performance. Results demonstrate that ARMOR significantly enhances the robustness against state-of-the-art adaptive jailbreak attacks and outperforms recent reasoning-based aligned models across various safety benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæƒŠäººçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ˜“äºè¢«è¯¯ç”¨ï¼Œå¼•å‘äº†é‡å¤§çš„å®‰å…¨æ‹…å¿§ã€‚è™½ç„¶è®­ç»ƒåçš„å®‰å…¨å¯¹é½æ–¹æ³•å·²ç»è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†LLMä»ç„¶å®¹æ˜“å—åˆ°å¯ä»¥ç»•è¿‡å®‰å…¨çº¦æŸçš„æ¶æ„æŒ‡ä»¤çš„å¨èƒã€‚è¿‘æœŸçš„ç ”ç©¶å¼•å…¥äº†æ¨ç†æ—¶é—´å®‰å…¨æ¨ç†ï¼ˆç³»ç»Ÿ2å¯¹é½ï¼‰ï¼Œå…¶ä¸­LLMåœ¨è¿›è¡Œæœ€ç»ˆå“åº”ä¹‹å‰è¿›è¡Œå®‰å…¨éªŒè¯çš„æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ£€æŸ¥æ˜¯ç”±éå¸¸è§„æ¨ç†é©±åŠ¨çš„ï¼Œè¿™ç§æ¨ç†ä¸ç»“æ„åŒ–çš„äººç±»è¿‡ç¨‹å­˜åœ¨åå·®ï¼Œäººç±»ä¼šé¦–å…ˆè¾¨åˆ«ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œç„¶åæ ¹æ®çœŸå®æ„å›¾è¯„ä¼°ç›¸å…³é£é™©ã€‚å› æ­¤ï¼Œè¿™äº›é˜²å¾¡æªæ–½ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªè£…æœ‰å®³ç›®æ ‡çš„é«˜çº§è¶Šç‹±æç¤ºçš„å¨èƒï¼Œè¿™äº›æç¤ºçœ‹ä¼¼æ— å®³ã€‚ä¸ºäº†æ„å»ºå®‰å…¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¨ç†çš„å®‰å…¨å¯¹é½æ¡†æ¶ARMORï¼Œå®ƒç”¨ä¸äººç±»å¯¹é½çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹æ›¿æ¢éå¸¸è§„çš„æ€ç»´é“¾ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒARMORï¼ˆ1ï¼‰æ£€æµ‹å¯èƒ½çš„è¶Šç‹±ç­–ç•¥ï¼Œï¼ˆ2ï¼‰æå–ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾å¹¶ä¸¢å¼ƒæ¬ºéª—æ€§æŒ‡ä»¤ï¼Œå¹¶å¯¹å‡€åŒ–çš„è¯·æ±‚è¿›è¡Œï¼ˆ3ï¼‰åŸºäºç­–ç•¥çš„å®‰å…¨åˆ†æã€‚ARMORåœ¨è‡ªé€‚åº”è¶Šç‹±æ”»å‡»å’Œå¤šä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒARMORæ˜¾è‘—æé«˜äº†å¯¹æŠ—æœ€æ–°è‡ªé€‚åº”è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨å„ç§å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°çš„åŸºäºæ¨ç†çš„å¯¹é½æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11500v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶æ˜“å—æ»¥ç”¨çš„é—®é¢˜å¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ã€‚å°½ç®¡å¹¿æ³›é‡‡ç”¨äº†è®­ç»ƒåçš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œä½†LLMsä»ç„¶é¢ä¸´å¯ä»¥ç»•è¿‡å®‰å…¨çº¦æŸçš„æ¶æ„æŒ‡ä»¤å¨èƒã€‚æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†æ¨ç†æ—¶çš„å®‰å…¨æ¨ç†ï¼ˆç³»ç»Ÿ-2å¯¹é½ï¼‰ï¼Œä½†ç ”ç©¶è¡¨æ˜è¿™äº›æ£€æŸ¥æ˜¯åŸºäºç‰¹æ®Šçš„ã€ä¸ç»“æ„åŒ–çš„äººç±»è¿‡ç¨‹ä¸åŒçš„æ¨ç†è¿‡ç¨‹ã€‚å› æ­¤ï¼Œè¿™äº›é˜²å¾¡æªæ–½ä»å¯èƒ½å—åˆ°ä¼ªè£…æœ‰å®³ç›®æ ‡çš„å…ˆè¿›è¶Šç‹±æç¤ºçš„å¨èƒã€‚ä¸ºäº†æ„å»ºå®‰å…¨å’Œå¯é çš„LLMsï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¨ç†çš„å®‰å…¨å¯¹é½æ¡†æ¶ARMORï¼Œå®ƒç”¨äººç±»å¯¹é½çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹å–ä»£ç‰¹æ®Šçš„æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒARMORèƒ½å¤Ÿæ£€æµ‹å¯èƒ½çš„è¶Šç‹±ç­–ç•¥ï¼Œæå–ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾å¹¶ä¸¢å¼ƒæ¬ºéª—æ€§æŒ‡ä»¤ï¼Œå¹¶å¯¹å‡€åŒ–åçš„è¯·æ±‚è¿›è¡ŒåŸºäºç­–ç•¥çš„å®‰å…¨åˆ†æã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒARMORæ˜¾è‘—æé«˜äº†å¯¹æœ€å…ˆè¿›çš„è‡ªé€‚åº”è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼Œå¹¶åœ¨å„ç§å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€è¿‘çš„æ¨ç†å¯¹é½æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†é¢ä¸´æ»¥ç”¨é£é™©ï¼Œå¼•å‘å®‰å…¨æ‹…å¿§ã€‚</li>
<li>å°½ç®¡æœ‰è®­ç»ƒåçš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œä½†LLMsä»æ˜“å—æ¶æ„æŒ‡ä»¤å¨èƒï¼Œå¯èƒ½ç»•è¿‡å®‰å…¨çº¦æŸã€‚</li>
<li>å½“å‰çš„å®‰å…¨æ¨ç†æ–¹æ³•åŸºäºç‰¹æ®Šçš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ç»“æ„åŒ–çš„äººç±»è¿‡ç¨‹ä¸åŒã€‚</li>
<li>æå‡ºçš„ARMORæ¡†æ¶æ—¨åœ¨é€šè¿‡é‡‡ç”¨äººç±»å¯¹é½çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹æ¥å¢å¼ºLLMsçš„å®‰å…¨æ€§ã€‚</li>
<li>ARMORèƒ½æ£€æµ‹è¶Šç‹±ç­–ç•¥ï¼Œæå–ç”¨æˆ·æ ¸å¿ƒæ„å›¾ï¼Œä¸¢å¼ƒæ¬ºéª—æ€§æŒ‡ä»¤ã€‚</li>
<li>ARMORå¯¹å‡€åŒ–åçš„è¯·æ±‚è¿›è¡ŒåŸºäºç­–ç•¥çš„å®‰å…¨åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4d9a2c5561e6b952c0d6e8ff6091e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6caea04e9850082339085bdd928e71c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e18141174f32a29998e4ad57686f896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dfe09760a1b2b593db231dafee3e918.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EXAONE-4-0-Unified-Large-Language-Models-Integrating-Non-reasoning-and-Reasoning-Modes"><a href="#EXAONE-4-0-Unified-Large-Language-Models-Integrating-Non-reasoning-and-Reasoning-Modes" class="headerlink" title="EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and   Reasoning Modes"></a>EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and   Reasoning Modes</h2><p><strong>Authors:LG AI Research,  :, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun</strong></p>
<p>This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via <a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE">https://huggingface.co/LGAI-EXAONE</a>. </p>
<blockquote>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†EXAONE 4.0ï¼Œå®ƒèåˆäº†éç†æ€§æ¨¡å¼å’Œæ¨ç†æ¨¡å¼ï¼Œå®ç°äº†EXAONE 3.5çš„å“è¶Šå¯ç”¨æ€§å’ŒEXAONE Deepçš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¿æ¥æ™ºèƒ½ä»£ç†æ—¶ä»£ï¼ŒEXAONE 4.0å¢åŠ äº†ä½¿ç”¨æ™ºèƒ½å·¥å…·ç­‰åŸºæœ¬åŠŸèƒ½ï¼Œé™¤äº†è‹±è¯­å’ŒéŸ©è¯­ä¹‹å¤–ï¼Œè¿˜æ”¯æŒè¥¿ç­ç‰™è¯­ã€‚EXAONE 4.0æ¨¡å‹ç³»åˆ—æœ‰ä¸¤ç§å°ºå¯¸ï¼šä¸€ç§æ˜¯é’ˆå¯¹é«˜æ€§èƒ½ä¼˜åŒ–çš„ä¸­å‹32Bæ¨¡å‹ï¼Œå¦ä¸€ç§æ˜¯ä¸“ä¸ºè®¾å¤‡ç«¯åº”ç”¨è®¾è®¡çš„å°å‹1.2Bæ¨¡å‹ã€‚EXAONE 4.0åœ¨å…¶åŒç±»å…¬å¼€æƒé‡æ¨¡å‹ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå³ä½¿åœ¨å°–ç«¯æ¨¡å‹ä¸­ä¹Ÿèƒ½ä¿æŒç«äº‰åŠ›ã€‚è¿™äº›æ¨¡å‹å¯ç”¨äºç ”ç©¶ç›®çš„ï¼Œå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE%E8%BD%BB%E6%9D%BE%E4%B8%8B%E8%BD%BD%E3%80%82">https://huggingface.co/LGAI-EXAONEè½»æ¾ä¸‹è½½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11407v1">PDF</a> Technical Report, 30 Pages</p>
<p><strong>Summary</strong></p>
<p>EXAONE 4.0æ˜¯ä¸€æ¬¾æ•´åˆäº†éæ¨ç†æ¨¡å¼å’Œæ¨ç†æ¨¡å¼çš„æ™ºèƒ½å·¥å…·ï¼Œæ—¨åœ¨å®ç°EXAONE 3.5çš„å“è¶Šå¯ç”¨æ€§å’ŒEXAONE Deepçš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚å®ƒèå…¥äº†ä»£ç†æ™ºèƒ½å·¥å…·çš„ä½¿ç”¨ï¼Œå¹¶æ‰©å±•äº†å¤šè¯­è¨€èƒ½åŠ›ä»¥æ”¯æŒè¥¿ç­ç‰™è¯­ã€è‹±è¯­å’ŒéŸ©è¯­ã€‚è¯¥æ¨¡å‹ç³»åˆ—åŒ…æ‹¬é’ˆå¯¹é«˜æ€§èƒ½ä¼˜åŒ–çš„ä¸­å‹32Bæ¨¡å‹å’Œé€‚ç”¨äºè®¾å¤‡ç«¯åº”ç”¨çš„å°å‹1.2Bæ¨¡å‹ã€‚ç›¸è¾ƒäºåŒç±»å¼€æ”¾æƒé‡æ¨¡å‹ï¼ŒEXAONE 4.0å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³ä¸å‰æ²¿æ¨¡å‹ç«äº‰ã€‚æ¨¡å‹å¯ä¾›ç ”ç©¶ä½¿ç”¨ï¼Œå¯é€šè¿‡huggingface.co&#x2F;LGAI-EXAONEè½»æ¾ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EXAONE 4.0é›†æˆäº†éæ¨ç†æ¨¡å¼å’Œæ¨ç†æ¨¡å¼ï¼Œæå‡äº†æ™ºèƒ½å·¥å…·çš„å¯ç”¨æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥å·¥å…·èå…¥äº†ä»£ç†æ™ºèƒ½å·¥å…·çš„ä½¿ç”¨ï¼Œå¢å¼ºäº†æ™ºèƒ½äº¤äº’ä½“éªŒã€‚</li>
<li>EXAONE 4.0æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬è¥¿ç­ç‰™è¯­ã€è‹±è¯­å’ŒéŸ©è¯­ã€‚</li>
<li>æ¨¡å‹ç³»åˆ—åŒ…æ‹¬é’ˆå¯¹é«˜æ€§èƒ½ä¼˜åŒ–çš„ä¸­å‹32Bæ¨¡å‹å’Œé€‚ç”¨äºè®¾å¤‡ç«¯åº”ç”¨çš„å°å‹1.2Bæ¨¡å‹ã€‚</li>
<li>EXAONE 4.0ç›¸è¾ƒäºåŒç±»æ¨¡å‹å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³ä¸å‰æ²¿æ¨¡å‹ä¿æŒç«äº‰ã€‚</li>
<li>æ¨¡å‹å¯ä¾›ç ”ç©¶ä½¿ç”¨ï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œç ”ç©¶ä¸å¼€å‘ç”Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77691b6bd7ed305dd46dfad4b689d805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a3ea9030dc7a521a7668b98f31159ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3855a734e48195aec5bd7e71e5b176a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c2539b27bed60e569bf03b4a677593b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad58077d5a6770b5a6d33b483a3b0636.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LRCTI-A-Large-Language-Model-Based-Framework-for-Multi-Step-Evidence-Retrieval-and-Reasoning-in-Cyber-Threat-Intelligence-Credibility-Verification"><a href="#LRCTI-A-Large-Language-Model-Based-Framework-for-Multi-Step-Evidence-Retrieval-and-Reasoning-in-Cyber-Threat-Intelligence-Credibility-Verification" class="headerlink" title="LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence   Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification"></a>LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence   Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification</h2><p><strong>Authors:Fengxiao Tang, Huan Li, Ming Zhao, Zongzong Wu, Shisong Peng, Tao Yin</strong></p>
<p>Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for reliable cybersecurity defense. However, traditional approaches typically treat this task as a static classification problem, relying on handcrafted features or isolated deep learning models. These methods often lack the robustness needed to handle incomplete, heterogeneous, or noisy intelligence, and they provide limited transparency in decision-making-factors that reduce their effectiveness in real-world threat environments. To address these limitations, we propose LRCTI, a Large Language Model (LLM)-based framework designed for multi-step CTI credibility verification. The framework first employs a text summarization module to distill complex intelligence reports into concise and actionable threat claims. It then uses an adaptive multi-step evidence retrieval mechanism that iteratively identifies and refines supporting information from a CTI-specific corpus, guided by LLM feedback. Finally, a prompt-based Natural Language Inference (NLI) module is applied to evaluate the credibility of each claim while generating interpretable justifications for the classification outcome. Experiments conducted on two benchmark datasets, CTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by over 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art baselines. These results demonstrate that LRCTI effectively addresses the core limitations of prior methods, offering a scalable, accurate, and explainable solution for automated CTI credibility verification </p>
<blockquote>
<p>éªŒè¯ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰çš„å¯ä¿¡åº¦å¯¹äºå¯é çš„ç½‘ç»œå®‰å…¨é˜²å¾¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸å°†æ­¤ä»»åŠ¡è§†ä¸ºé™æ€çš„åˆ†ç±»é—®é¢˜ï¼Œä¾èµ–äºæ‰‹å·¥ç‰¹å¾æˆ–å­¤ç«‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹å¤„ç†ä¸å®Œæ•´ã€å¼‚æ„æˆ–å˜ˆæ‚æƒ…æŠ¥æ‰€éœ€çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”åœ¨å†³ç­–å› ç´ ä¸­æä¾›çš„é€æ˜åº¦æœ‰é™ï¼Œè¿™äº›å› ç´ é™ä½äº†å®ƒä»¬åœ¨ç°å®å¨èƒç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LRCTIï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è¿›è¡Œå¤šæ­¥CTIå¯ä¿¡åº¦éªŒè¯ã€‚è¯¥æ¡†æ¶é¦–å…ˆé‡‡ç”¨æ–‡æœ¬æ‘˜è¦æ¨¡å—ï¼Œå°†å¤æ‚çš„æƒ…æŠ¥æŠ¥å‘Šç®€åŒ–ä¸ºç®€æ´å¯è¡Œçš„å¨èƒå£°æ˜ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªè‡ªé€‚åº”çš„å¤šæ­¥è¯æ®æ£€ç´¢æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¼šè¿­ä»£åœ°ä»ç‰¹å®šçš„CTIè¯­æ–™åº“ä¸­è¯†åˆ«å’Œç»†åŒ–æ”¯æŒä¿¡æ¯ï¼Œå¹¶ç”±LLMåé¦ˆæŒ‡å¯¼ã€‚æœ€åï¼Œåº”ç”¨åŸºäºæç¤ºçš„è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ¨¡å—æ¥è¯„ä¼°æ¯ä¸ªå£°æ˜çš„å¯ä¿¡åº¦ï¼ŒåŒæ—¶ä¸ºåˆ†ç±»ç»“æœç”Ÿæˆå¯è§£é‡Šçš„ä¾æ®ã€‚åœ¨CTI-200å’ŒPolitiFactä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒLRCTIçš„F1-Macroå’ŒF1-Microå¾—åˆ†æé«˜äº†è¶…è¿‡5%ï¼Œåˆ†åˆ«è¾¾åˆ°äº†90.9%å’Œ93.6%ã€‚è¿™äº›ç»“æœè¯æ˜LRCTIæœ‰æ•ˆåœ°è§£å†³äº†å…ˆå‰æ–¹æ³•çš„æ ¸å¿ƒå±€é™æ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–CTIå¯ä¿¡åº¦éªŒè¯æä¾›äº†å¯æ‰©å±•ã€å‡†ç¡®å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11310v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>CTIçš„å¯ä¿¡åº¦éªŒè¯å¯¹äºå¯é çš„ç½‘ç»œå®‰å…¨é˜²å¾¡è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„éªŒè¯æ–¹æ³•å¸¸å°†å…¶è§†ä¸ºé™æ€åˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨æ‰‹å·¥ç‰¹å¾æˆ–å­¤ç«‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†ä¸å®Œæ•´ã€å¼‚è´¨æˆ–å˜ˆæ‚çš„æƒ…æŠ¥ï¼Œä¸”å†³ç­–å› ç´ é€æ˜åº¦æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„LRCTIæ¡†æ¶ï¼Œç”¨äºå¤šæ­¥éª¤çš„CTIå¯ä¿¡åº¦éªŒè¯ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬æ‘˜è¦æ¨¡å—å°†å¤æ‚çš„æƒ…æŠ¥æŠ¥å‘Šç®€åŒ–ä¸ºç®€æ´çš„è¡ŒåŠ¨å¨èƒå£°æ˜ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”çš„å¤šæ­¥è¯æ®æ£€ç´¢æœºåˆ¶ï¼Œè¿­ä»£åœ°ä»CTIç‰¹å®šè¯­æ–™åº“ä¸­è¯†åˆ«å’Œæ”¯æŒä¿¡æ¯ï¼Œç”±LLMåé¦ˆæŒ‡å¯¼ã€‚æœ€åï¼Œåº”ç”¨åŸºäºæç¤ºçš„è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ¨¡å—æ¥è¯„ä¼°æ¯ä¸ªå£°æ˜çš„å¯ä¿¡åº¦ï¼ŒåŒæ—¶ç”Ÿæˆåˆ†ç±»ç»“æœçš„è§£é‡Šæ€§ä¾æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒLRCTIåœ¨CTI-200å’ŒPolitiFactä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„F1-Macroå’ŒF1-Microå¾—åˆ†æé«˜äº†5%ä»¥ä¸Šï¼Œåˆ†åˆ«è¾¾åˆ°90.9%å’Œ93.6%ï¼Œè¯æ˜äº†LRCTIæœ‰æ•ˆåœ°è§£å†³äº†å…ˆå‰æ–¹æ³•çš„æ ¸å¿ƒé™åˆ¶ï¼Œä¸ºè‡ªåŠ¨CTIå¯ä¿¡åº¦éªŒè¯æä¾›äº†å¯æ‰©å±•ã€å‡†ç¡®å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CTIçš„å¯ä¿¡åº¦éªŒè¯æ˜¯ç½‘ç»œå®‰å…¨é˜²å¾¡çš„å…³é”®ç¯èŠ‚ã€‚</li>
<li>ä¼ ç»ŸéªŒè¯æ–¹æ³•åœ¨å¤„ç†å¤æ‚æƒ…æŠ¥æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹ç¨³å¥æ€§å’Œé€æ˜åº¦ã€‚</li>
<li>LRCTIæ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¤šæ­¥éª¤çš„CTIå¯ä¿¡åº¦éªŒè¯ã€‚</li>
<li>LRCTIé€šè¿‡æ–‡æœ¬æ‘˜è¦æ¨¡å—ç®€åŒ–æƒ…æŠ¥æŠ¥å‘Šï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚</li>
<li>è‡ªé€‚åº”å¤šæ­¥è¯æ®æ£€ç´¢æœºåˆ¶èƒ½å¤Ÿè¿­ä»£åœ°ä»ç‰¹å®šè¯­æ–™åº“ä¸­æŸ¥æ‰¾æ”¯æŒä¿¡æ¯ã€‚</li>
<li>åŸºäºæç¤ºçš„NLIæ¨¡å—ç”¨äºè¯„ä¼°å£°æ˜çš„å¯ä¿¡åº¦ï¼Œå¹¶ç”Ÿæˆè§£é‡Šæ€§ä¾æ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜LRCTIåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºCTIå¯ä¿¡åº¦éªŒè¯æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-752f0dbfaca04ccb0a82a315499a7513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf8deed7a9d7d71dc8c668082ec247d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ade684ae96f59c18ae7f28afb1b6adf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52ffb373d7778d180df857e5da888331.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c64d3811485e7c5657b5af7529220d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c909094b628074fd23956064a2319e9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DuetGraph-Coarse-to-Fine-Knowledge-Graph-Reasoning-with-Dual-Pathway-Global-Local-Fusion"><a href="#DuetGraph-Coarse-to-Fine-Knowledge-Graph-Reasoning-with-Dual-Pathway-Global-Local-Fusion" class="headerlink" title="DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway   Global-Local Fusion"></a>DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway   Global-Local Fusion</h2><p><strong>Authors:Jin Li, Zezhong Ding, Xike Xie</strong></p>
<p>Knowledge graphs (KGs) are vital for enabling knowledge reasoning across various domains. Recent KG reasoning methods that integrate both global and local information have achieved promising results. However, existing methods often suffer from score over-smoothing, which blurs the distinction between correct and incorrect answers and hinders reasoning effectiveness. To address this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with dual-pathway global-local fusion. DuetGraph tackles over-smoothing by segregating â€“ rather than stacking â€“ the processing of local (via message passing) and global (via attention) information into two distinct pathways, preventing mutual interference and preserving representational discrimination. In addition, DuetGraph introduces a coarse-to-fine optimization, which partitions entities into high- and low-score subsets. This strategy narrows the candidate space and sharpens the score gap between the two subsets, which alleviates over-smoothing and enhances inference quality. Extensive experiments on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA) performance, with up to an 8.7% improvement in reasoning quality and a 1.8$\times$ acceleration in training efficiency. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰å¯¹äºè·¨åŸŸçŸ¥è¯†æ¨ç†è‡³å…³é‡è¦ã€‚æœ€è¿‘èåˆäº†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯çš„KGæ¨ç†æ–¹æ³•å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°è¯„åˆ†è¿‡åº¦å¹³æ»‘çš„å½±å“ï¼Œè¿™æ¨¡ç³Šäº†æ­£ç¡®å’Œé”™è¯¯ç­”æ¡ˆä¹‹é—´çš„åŒºåˆ«ï¼Œé˜»ç¢äº†æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DuetGraphï¼Œä¸€ç§å…·æœ‰åŒè·¯å¾„å…¨å±€-å±€éƒ¨èåˆçš„ç²—ç»†ç²’åº¦KGæ¨ç†æœºåˆ¶ã€‚DuetGraphé€šè¿‡åˆ†ç¦»è€Œä¸æ˜¯å †å å±€éƒ¨ï¼ˆé€šè¿‡æ¶ˆæ¯ä¼ é€’ï¼‰å’Œå…¨å±€ï¼ˆé€šè¿‡æ³¨æ„åŠ›ï¼‰ä¿¡æ¯çš„å¤„ç†ï¼Œå°†ä¸¤è€…åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹è·¯å¾„ï¼Œé˜²æ­¢ç›¸äº’å¹²æ‰°ï¼Œä¿æŒè¡¨å¾çš„è¾¨åˆ«åŠ›ï¼Œä»è€Œè§£å†³è¿‡åº¦å¹³æ»‘é—®é¢˜ã€‚æ­¤å¤–ï¼ŒDuetGraphè¿˜å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå°†å®ä½“åˆ’åˆ†ä¸ºé«˜å¾—åˆ†å’Œä½å¾—åˆ†å­é›†ã€‚è¿™ç§ç­–ç•¥ç¼©å°äº†å€™é€‰ç©ºé—´ï¼Œæ‹‰å¤§äº†ä¸¤ä¸ªå­é›†ä¹‹é—´çš„å¾—åˆ†å·®è·ï¼Œç¼“è§£äº†è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œæé«˜äº†æ¨ç†è´¨é‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDuetGraphè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ¨ç†è´¨é‡æé«˜äº†é«˜è¾¾8.7%ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†1.8å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11229v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰åœ¨è·¨åŸŸçŸ¥è¯†æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰çŸ¥è¯†å›¾è°±æ¨ç†æ–¹æ³•å­˜åœ¨çš„è¯„åˆ†è¿‡å¹³æ»‘é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç²—åˆ°ç»†çŸ¥è¯†å›¾è°±æ¨ç†æœºåˆ¶â€”â€”DuetGraphï¼Œè¯¥æœºåˆ¶é€šè¿‡åŒè·¯å¾„å…¨å±€-å±€éƒ¨èåˆçš„æ–¹å¼å¤„ç†ä¿¡æ¯ã€‚DuetGraphé€šè¿‡å°†å±€éƒ¨ï¼ˆé€šè¿‡æ¶ˆæ¯ä¼ é€’ï¼‰å’Œå…¨å±€ï¼ˆé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼‰ä¿¡æ¯çš„å¤„ç†åˆ†ç¦»åˆ°ä¸¤ä¸ªç‹¬ç«‹è·¯å¾„ï¼Œè§£å†³äº†è¯„åˆ†è¿‡å¹³æ»‘é—®é¢˜ï¼Œä¿ç•™äº†è¡¨ç¤ºåˆ¤åˆ«æ€§ã€‚æ­¤å¤–ï¼ŒDuetGraphè¿˜å¼•å…¥äº†ç²—åˆ°ç»†ä¼˜åŒ–ç­–ç•¥ï¼Œå°†å®ä½“åˆ†ä¸ºé«˜ã€ä½åˆ†ä¸¤ç»„ï¼Œç¼©å°å€™é€‰ç©ºé—´ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒDuetGraphåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ¨ç†è´¨é‡æé«˜äº†8.7%ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†1.8å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±(KGs)åœ¨è·¨åŸŸçŸ¥è¯†æ¨ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ç°æœ‰çŸ¥è¯†å›¾è°±æ¨ç†æ–¹æ³•å­˜åœ¨è¯„åˆ†è¿‡å¹³æ»‘é—®é¢˜ï¼Œå½±å“æ¨ç†æ•ˆæœã€‚</li>
<li>DuetGraphæ˜¯ä¸€ç§æ–°å‹çš„ç²—åˆ°ç»†çŸ¥è¯†å›¾è°±æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡åŒè·¯å¾„å…¨å±€-å±€éƒ¨èåˆæ–¹å¼å¤„ç†ä¿¡æ¯ã€‚</li>
<li>DuetGraphè§£å†³äº†è¯„åˆ†è¿‡å¹³æ»‘é—®é¢˜ï¼Œä¿ç•™äº†è¡¨ç¤ºåˆ¤åˆ«æ€§ã€‚</li>
<li>DuetGraphé€šè¿‡å®ä½“åˆ†ç»„ç­–ç•¥ç¼©å°å€™é€‰ç©ºé—´ï¼Œæé«˜æ¨ç†è´¨é‡ã€‚</li>
<li>DuetGraphåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ¨ç†è´¨é‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d69f1ffd8dff0edf8c339336161cc3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02f4eab49d645712203ad9078baab20e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14db9bc56a83fae78785af9cb1d33aea.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-in-Vision-Language-Models-in-Identifying-Unsafe-Concepts-Across-Modalities"><a href="#Bridging-the-Gap-in-Vision-Language-Models-in-Identifying-Unsafe-Concepts-Across-Modalities" class="headerlink" title="Bridging the Gap in Vision Language Models in Identifying Unsafe   Concepts Across Modalities"></a>Bridging the Gap in Vision Language Models in Identifying Unsafe   Concepts Across Modalities</h2><p><strong>Authors:Yiting Qu, Michael Backes, Yang Zhang</strong></p>
<p>Vision-language models (VLMs) are increasingly applied to identify unsafe or inappropriate images due to their internal ethical standards and powerful reasoning abilities. However, it is still unclear whether they can recognize various unsafe concepts when presented in different modalities, such as text and images. To address this, we first compile the UnsafeConcepts dataset, featuring 75 unsafe concepts, i.e., <code>Swastika,&#39;&#39; </code>Sexual Harassment,â€™â€™ and &#96;&#96;Assaults,â€™â€™ along with associated 1.5K images. We then conduct a systematic evaluation of VLMsâ€™ perception (concept recognition) and alignment (ethical reasoning) capabilities. We assess eight popular VLMs and find that, although most VLMs accurately perceive unsafe concepts, they sometimes mistakenly classify these concepts as safe. We also identify a consistent modality gap among open-source VLMs in distinguishing between visual and textual unsafe concepts. To bridge this gap, we introduce a simplified reinforcement learning (RL)-based approach using proximal policy optimization (PPO) to strengthen the ability to identify unsafe concepts from images. Our approach uses reward scores based directly on VLM responses, bypassing the need for collecting human-annotated preference data to train a new reward model. Experimental results show that our approach effectively enhances VLM alignment on images while preserving general capabilities. It outperforms baselines such as supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope our dataset, evaluation findings, and proposed alignment solution contribute to the communityâ€™s efforts in advancing safe VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”±äºå…¶å†…éƒ¨é“å¾·æ ‡å‡†å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºè¯†åˆ«ä¸å®‰å…¨æˆ–ä¸å½“çš„å›¾åƒã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦èƒ½åœ¨æ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡æ€ä¸‹è¯†åˆ«å„ç§ä¸å®‰å…¨æ¦‚å¿µã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆç¼–è¯‘äº†UnsafeConceptsæ•°æ®é›†ï¼ŒåŒ…å«äº†75ä¸ªä¸å®‰å…¨æ¦‚å¿µï¼Œå¦‚â€œä¸‡å­—æ ‡å¿—â€ã€â€œæ€§éªšæ‰°â€å’Œâ€œè¢­å‡»â€ï¼Œä»¥åŠç›¸å…³çš„1500å¼ å›¾åƒã€‚ç„¶åæˆ‘ä»¬å¯¹VLMsçš„æ„ŸçŸ¥ï¼ˆæ¦‚å¿µè¯†åˆ«ï¼‰å’Œå¯¹é½ï¼ˆé“å¾·æ¨ç†ï¼‰èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†å…«ç§æµè¡Œçš„VLMsï¼Œå‘ç°å°½ç®¡å¤§å¤šæ•°VLMsèƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥ä¸å®‰å…¨æ¦‚å¿µï¼Œä½†æœ‰æ—¶ä¼šå°†å®ƒä»¬é”™è¯¯åœ°å½’ç±»ä¸ºå®‰å…¨æ¦‚å¿µã€‚æˆ‘ä»¬è¿˜å‘ç°å¼€æºVLMsåœ¨åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µæ—¶å­˜åœ¨æŒç»­çš„æ¨¡æ€å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºç®€åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥åŠ å¼ºä»å›¾åƒä¸­è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥ä½¿ç”¨VLMå“åº”æ¥ç”Ÿæˆå¥–åŠ±åˆ†æ•°ï¼Œä»è€Œæ— éœ€æ”¶é›†äººç±»æ³¨é‡Šçš„åå¥½æ•°æ®æ¥è®­ç»ƒæ–°çš„å¥–åŠ±æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†å›¾åƒä¸ŠVLMçš„å¯¹é½æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å…¶ä¸€èˆ¬èƒ½åŠ›ã€‚å®ƒä¼˜äºå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®é›†ã€è¯„ä¼°ç»“æœå’Œæå‡ºçš„å¯¹é½è§£å†³æ–¹æ¡ˆèƒ½ä¸ºç¤¾åŒºæ¨è¿›å®‰å…¨VLMsçš„åŠªåŠ›åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11155v1">PDF</a> To Appear in the 34th USENIX Security Symposium, August 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¯†åˆ«ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰ä¸‹çš„ä¸å®‰å…¨æ¦‚å¿µæ—¶çš„è¡¨ç°ã€‚ä¸ºäº†è¯„ä¼°VLMsçš„æ„ŸçŸ¥å’Œä¼¦ç†æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬ç¼–è¯‘äº†UnsafeConceptsæ•°æ®é›†ï¼Œå¹¶å¯¹å…«ç§æµè¡Œçš„VLMsè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å‘ç°è™½ç„¶å¤§å¤šæ•°VLMsèƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥ä¸å®‰å…¨æ¦‚å¿µï¼Œä½†æœ‰æ—¶ä¼šé”™è¯¯åœ°å°†è¿™äº›æ¦‚å¿µå½’ç±»ä¸ºå®‰å…¨ã€‚è¿˜å­˜åœ¨ä¸€ä¸ªæŒç»­çš„æ¨¡æ€å·®è·ï¼Œå³åœ¨åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µæ–¹é¢çš„èƒ½åŠ›å·®å¼‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥åŠ å¼ºä»å›¾åƒä¸­è¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µçš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç›´æ¥ä½¿ç”¨VLMå“åº”æ¥åˆ¶å®šå¥–åŠ±åˆ†æ•°ï¼Œæ— éœ€æ”¶é›†äººç±»åå¥½æ•°æ®æ¥è®­ç»ƒæ–°çš„å¥–åŠ±æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æé«˜äº†VLMåœ¨å›¾åƒä¸Šçš„å¯¹é½æ€§èƒ½ï¼Œä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsè¢«åº”ç”¨äºè¯†åˆ«ä¸å®‰å…¨æˆ–ä¸é€‚çš„å›¾åƒï¼Œä½†å®ƒä»¬åœ¨è¯†åˆ«ä¸åŒæ¨¡æ€ä¸‹çš„ä¸å®‰å…¨æ¦‚å¿µæ—¶ä»å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>UnsafeConceptsæ•°æ®é›†è¢«ç¼–è¯‘ï¼ŒåŒ…å«75ä¸ªä¸å®‰å…¨æ¦‚å¿µå’Œç›¸å…³çš„1.5Kå›¾åƒï¼Œç”¨äºè¯„ä¼°VLMsçš„æ„ŸçŸ¥å’Œä¼¦ç†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°½ç®¡å¤§å¤šæ•°VLMsèƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥ä¸å®‰å…¨æ¦‚å¿µï¼Œä½†å®ƒä»¬æœ‰æ—¶ä¼šé”™è¯¯åœ°å°†è¿™äº›æ¦‚å¿µå½’ç±»ä¸ºå®‰å…¨ã€‚</li>
<li>åœ¨åŒºåˆ†è§†è§‰å’Œæ–‡æœ¬ä¸å®‰å…¨æ¦‚å¿µæ–¹é¢ï¼Œå¼€æºVLMsä¹‹é—´å­˜åœ¨ä¸€è‡´çš„æ¨¡æ€å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç®€åŒ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥æé«˜VLMsåœ¨å›¾åƒä¸Šè¯†åˆ«ä¸å®‰å…¨æ¦‚å¿µçš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ç›´æ¥ä½¿ç”¨VLMå“åº”åˆ¶å®šå¥–åŠ±åˆ†æ•°ï¼Œæ— éœ€äººç±»åå¥½æ•°æ®è®­ç»ƒæ–°çš„å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac1153a6db86660dc8fbda30ee1b9ffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c7cc47e65ec5bcc805bc2ebc57ea78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fcfd7565e3bdbf1fb2bbfbe999ce6e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models"><a href="#MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models" class="headerlink" title="MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models"></a>MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models</h2><p><strong>Authors:Seif Ahmed, Mohamed T. Younes, Abdelrahman Moustafa, Abdelrahman Allam, Hamza Moustafa</strong></p>
<p>We present a robust ensemble-based system for multilingual multimodal reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which handles final answer selection, all coordinated through carefully engineered few-shot and zero-shot prompts. We conducted an extensive ablation study, training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3, Mistral) on an English dataset and its multilingual augmented version. Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for comparison and found it to substantially outperform the trained models. Prompt design also proved critical: enforcing concise, language-normalized formats and prohibiting explanatory text boosted model accuracy on the English validation set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA) achieved first place overall in the multilingual track with 81.4% accuracy, and led 11 out of 13 individual language tracks, with top results such as 95.07% for Croatian and 92.12% for Italian. These findings highlight that lightweight OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual augmentation, can outperform heavier end-to-end models in high-stakes, multilingual educational settings. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºImageCLEF 2025 EXAMS VæŒ‘æˆ˜è®¾è®¡äº†ä¸€ä¸ªç¨³å¥çš„åŸºäºé›†æˆç³»ç»Ÿçš„å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†Gemini 2.5 Flashè¿›è¡Œè§†è§‰æè¿°ã€Gemini 1.5 Proè¿›è¡Œå­—å¹•ä¼˜åŒ–å’Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»¥åŠGemini 2.5 Proä½œä¸ºæ¨ç†å™¨å¤„ç†æœ€ç»ˆç­”æ¡ˆçš„é€‰æ‹©ï¼Œæ‰€æœ‰è¿™äº›éƒ½é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘é‡å’Œé›¶æ ·æœ¬æç¤ºè¿›è¡Œåè°ƒã€‚æˆ‘ä»¬å¯¹å‡ ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGemini 2.5 Flashã€Phi 4ã€Gemma 3ã€Mistralï¼‰è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶è®­ç»ƒï¼Œè®­ç»ƒæ•°æ®é›†åŒ…æ‹¬è‹±è¯­åŠå…¶å¤šè¯­è¨€å¢å¼ºç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹Gemini 2.5 Flashè¿›è¡Œäº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ¯”è¾ƒè¯„ä¼°ï¼Œå‘ç°å…¶æ€§èƒ½å¤§å¤§ä¼˜äºè®­ç»ƒæ¨¡å‹ã€‚æç¤ºè®¾è®¡ä¹Ÿè¢«è¯æ˜æ˜¯å…³é”®çš„ï¼šé€šè¿‡æ‰§è¡Œç®€æ´ã€è¯­è¨€è§„èŒƒåŒ–çš„æ ¼å¼å¹¶ç¦æ­¢è§£é‡Šæ€§æ–‡æœ¬ï¼Œæ¨¡å‹åœ¨è‹±è¯­éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä»55.9%æé«˜åˆ°äº†61.7%ã€‚åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿï¼ˆMSAå›¢é˜Ÿï¼‰åœ¨å¤šè¯­è¨€è½¨é“ä¸Šä»¥81.4%çš„å‡†ç¡®ç‡è·å¾—ç¬¬ä¸€åï¼Œå¹¶åœ¨13ä¸ªå•ä¸€è¯­è¨€è½¨é“ä¸­çš„11ä¸ªè½¨é“ä¸Šååˆ—å‰èŒ…ï¼Œå¦‚åœ¨å…‹ç½—åœ°äºšè¯­çš„95.07%å’Œæ„å¤§åˆ©è¯­çš„92.12%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“è½»é‡çº§çš„OCR-VLMé›†æˆä¸ç²¾ç¡®çš„æç¤ºç­–ç•¥å’Œè·¨è¯­è¨€å¢å¼ºç›¸ç»“åˆæ—¶ï¼Œå¯ä»¥åœ¨é«˜é£é™©çš„å¤šè¯­è¨€æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºæ›´é‡çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦åŸºäºå›¢é˜ŸMSAæ‰€è®¾è®¡çš„ç¨³å¥çš„é›†æˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨å¤šè¯­è¨€å¤šåª’ä½“æ¨¡å¼æ¨ç†ï¼Œé€‚ç”¨äºImageCLEF 2025 EXAMS VæŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿèåˆäº†Gemini 2.5 Flashè§†è§‰æè¿°ã€Gemini 1.5 Proè¿›è¡Œæè¿°ç²¾ç‚¼å’Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»¥åŠGemini 2.5 Proä½œä¸ºæ¨ç†å™¨è¿›è¡Œæœ€ç»ˆç­”æ¡ˆé€‰æ‹©ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘é‡å’Œé›¶æ ·æœ¬æç¤ºè¿›è¡Œåè°ƒã€‚é€šè¿‡å¤§é‡å®éªŒå’Œè¯„ä¼°ï¼Œå›¢é˜Ÿåœ¨è‹±æ–‡æ•°æ®é›†åŠå…¶å¤šè¯­è¨€å¢å¼ºç‰ˆæœ¬ä¸Šè®­ç»ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šå–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“é…ä»¥ç²¾ç¡®æç¤ºç­–ç•¥å’Œè·¨è¯­è¨€å¢å¼ºæ—¶ï¼Œè½»é‡çº§çš„OCR-VLMé›†æˆç³»ç»Ÿå¯ä»¥åœ¨é«˜é£é™©çš„å¤šå…ƒè¯­è¨€æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°å‡ºè¶…è¶Šé‡å‹ç«¯åˆ°ç«¯æ¨¡å‹çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶çš„æœ€å¤§äº®ç‚¹åœ¨äºå¯¹äºç‰¹å®šåœºæ™¯ä¸‹ä½¿ç”¨æ··åˆè¯­è¨€æŠ€æœ¯è·¯çº¿çš„æ¨¡å‹çš„æ–°å‘ç°å’Œæ¨å¹¿ï¼Œå¹¶é€šè¿‡ä¸æ–­çš„è®­ç»ƒå’Œä¿®æ­£æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æ•´ä½“è¡¨ç°å“è¶Šï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æœ‰åŠ›çš„å‚è€ƒä¾æ®ã€‚æ­¤å¤–ï¼Œå›¢é˜Ÿçš„å…‹ç½—åœ°äºšå’Œæ„å¤§åˆ©è¯­è¨€çš„å‡†ç¡®ç‡è¾¾åˆ°äº†æƒŠäººçš„é«˜æ°´å¹³ã€‚æ­¤ç ”ç©¶å°†ä¸ºå¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è§†è§‰æè¿°ä¸æ¨ç†å¸¦æ¥é‡å¤§å½±å“ã€‚è¿™ä¸€ç ”ç©¶å±•ç°äº†èåˆå„ç§æŠ€æœ¯çš„å¼ºå¤§é›†æˆç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºåˆ›æ–°æ€§çš„ç³»ç»Ÿè®¾è®¡å’Œç²¾å‡†çš„æç¤ºç­–ç•¥ã€‚å®ƒåœ¨æ•™è‚²ç¯å¢ƒä¸­çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§éå¸¸å¼•äººæ³¨ç›®ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†è½»é‡çº§OCR-VLMé›†æˆç³»ç»Ÿåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å“è¶Šæ€§èƒ½ï¼Œä»¥åŠå…¶å¦‚ä½•åœ¨å¤–è¯­ç†è§£å’Œåˆ†æä¸Šå±•ç°å‡ºé«˜åº¦æ™ºèƒ½åŒ–å’Œè‡ªä¸»æ€§ï¼Œä½¿å…¶æˆä¸ºå­¦æœ¯ç ”ç©¶åŠè¡Œä¸šå®è·µé¢†åŸŸçš„ä¸€ç§å…¨æ–°è€Œå¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚ç»¼åˆæ¥çœ‹æœ¬è®ºæ–‡çš„æ–¹æ³•å…·æœ‰é«˜åº¦çš„åˆ›æ–°æ€§ã€å®ç”¨æ€§ä»¥åŠå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚è¯¥æ‘˜è¦ç®€æ´æ˜äº†åœ°æ€»ç»“äº†è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹å’Œä¸»è¦å‘ç°ã€‚å®ƒå¼ºè°ƒäº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€å‡†ç¡®æ€§å’Œåœ¨å¤šè¯­ç§ç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ä¸ªç¨³å¥çš„é›†æˆç³»ç»Ÿï¼Œä¸“ä¸ºImageCLEF 2025 EXAMS VæŒ‘æˆ˜è®¾è®¡ï¼Œç”¨äºå¤šè¯­è¨€å¤šåª’ä½“æ¨¡å¼æ¨ç†ã€‚</li>
<li>ç³»ç»Ÿèåˆäº†å¤šç§æŠ€æœ¯æ¨¡å—åŒ…æ‹¬è§†è§‰æè¿°ã€æè¿°ç²¾ç‚¼å’Œä¸€è‡´æ€§æ£€æŸ¥ä»¥åŠæ¨ç†å¤„ç†ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒå’Œè¯„ä¼°ï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šå–å¾—äº†ç¬¬ä¸€åçš„å¥½æˆç»©ã€‚</li>
<li>ç ”ç©¶å‘ç°è½»é‡çº§OCR-VLMé›†æˆç³»ç»Ÿåœ¨é«˜é£é™©çš„å¤šå…ƒè¯­è¨€æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°å‡ºè¶…è¶Šé‡å‹ç«¯åˆ°ç«¯æ¨¡å‹çš„èƒ½åŠ›ã€‚ç²¾å‡†æç¤ºç­–ç•¥å’Œè·¨è¯­è¨€å¢å¼ºæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0d9efb238edd3ca208a135196be004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c344a8b0018e5f5d067bd1cc640872a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd12f8f53a91ab2d8889f73aae6a5472.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-028d93bd57c1f26272b8c3342f232bf1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Learning-to-Tune-Like-an-Expert-Interpretable-and-Scene-Aware-Navigation-via-MLLM-Reasoning-and-CVAE-Based-Adaptation"><a href="#Learning-to-Tune-Like-an-Expert-Interpretable-and-Scene-Aware-Navigation-via-MLLM-Reasoning-and-CVAE-Based-Adaptation" class="headerlink" title="Learning to Tune Like an Expert: Interpretable and Scene-Aware   Navigation via MLLM Reasoning and CVAE-Based Adaptation"></a>Learning to Tune Like an Expert: Interpretable and Scene-Aware   Navigation via MLLM Reasoning and CVAE-Based Adaptation</h2><p><strong>Authors:Yanbo Wang, Zipeng Fang, Lei Zhao, Weidong Chen</strong></p>
<p>Service robots are increasingly deployed in diverse and dynamic environments, where both physical layouts and social contexts change over time and across locations. In these unstructured settings, conventional navigation systems that rely on fixed parameters often fail to generalize across scenarios, resulting in degraded performance and reduced social acceptance. Although recent approaches have leveraged reinforcement learning to enhance traditional planners, these methods often fail in real-world deployments due to poor generalization and limited simulation diversity, which hampers effective sim-to-real transfer. To tackle these issues, we present LE-Nav, an interpretable and scene-aware navigation framework that leverages multi-modal large language model reasoning and conditional variational autoencoders to adaptively tune planner hyperparameters. To achieve zero-shot scene understanding, we utilize one-shot exemplars and chain-of-thought prompting strategies. Additionally, a conditional variational autoencoder captures the mapping between natural language instructions and navigation hyperparameters, enabling expert-level tuning. Experiments show that LE-Nav can generate hyperparameters achieving human-level tuning across diverse planners and scenarios. Real-world navigation trials and a user study on a smart wheelchair platform demonstrate that it outperforms state-of-the-art methods on quantitative metrics such as success rate, efficiency, safety, and comfort, while receiving higher subjective scores for perceived safety and social acceptance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Cavendish518/LE-Nav">https://github.com/Cavendish518/LE-Nav</a>. </p>
<blockquote>
<p>æœåŠ¡æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²åœ¨å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­ï¼Œè¿™äº›ç¯å¢ƒçš„ç‰©ç†å¸ƒå±€å’Œç¤¾ä¼šèƒŒæ™¯ä¼šéšæ—¶é—´å’Œåœ°ç‚¹çš„å˜åŒ–è€Œå˜åŒ–ã€‚åœ¨è¿™äº›éç»“æ„åŒ–è®¾ç½®ä¸­ï¼Œä¾èµ–å›ºå®šå‚æ•°çš„ä¼ ç»Ÿå¯¼èˆªç³»ç»Ÿå¾€å¾€æ— æ³•åœ¨ä¸åŒåœºæ™¯ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œç¤¾ä¼šæ¥å—åº¦é™ä½ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•å·²ç»åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºä¼ ç»Ÿè§„åˆ’å™¨ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ç»å¸¸å¤±è´¥ï¼Œå› ä¸ºæ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œæ¨¡æ‹Ÿå¤šæ ·æ€§æœ‰é™ï¼Œè¿™é˜»ç¢äº†æœ‰æ•ˆçš„æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬ç§»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LE-Navï¼Œè¿™æ˜¯ä¸€ä¸ªå¯è§£é‡Šçš„ã€åœºæ™¯æ„ŸçŸ¥çš„å¯¼èˆªæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œæ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¥é€‚åº”æ€§åœ°è°ƒæ•´è§„åˆ’å™¨è¶…å‚æ•°ã€‚ä¸ºäº†å®ç°é›¶å°„å‡»åœºæ™¯ç†è§£ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€æ¬¡å°„å‡»çš„èŒƒä¾‹å’Œæ€ç»´é“¾æç¤ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œæ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ•æ‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå¯¼èˆªè¶…å‚æ•°ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå®ç°ä¸“å®¶çº§è°ƒæ•´ã€‚å®éªŒè¡¨æ˜ï¼ŒLE-Navå¯ä»¥ç”Ÿæˆåœ¨ä¸åŒè§„åˆ’å™¨å’Œåœºæ™¯ä¸Šè¾¾åˆ°äººç±»çº§è°ƒæ•´çš„è¶…å‚æ•°ã€‚åœ¨æ™ºèƒ½è½®æ¤…å¹³å°ä¸Šçš„ç°å®å¯¼èˆªè¯•éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜ï¼Œå®ƒåœ¨æˆåŠŸç‡ã€æ•ˆç‡ã€å®‰å…¨æ€§å’Œèˆ’é€‚æ€§ç­‰å®šé‡æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ„ŸçŸ¥å®‰å…¨æ€§å’Œç¤¾ä¼šæ¥å—åº¦æ–¹é¢è·å¾—æ›´é«˜çš„ä¸»è§‚è¯„åˆ†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cavendish518/LE-Nav">https://github.com/Cavendish518/LE-Nav</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœåŠ¡æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­éƒ¨ç½²æ—¥ç›Šæ™®éï¼Œé¢ä¸´åœºæ™¯å¤šå˜ã€ç¤¾ä¼šèƒŒæ™¯å¤æ‚ç­‰æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¯¼èˆªç³»ç»Ÿä¾èµ–å›ºå®šå‚æ•°ï¼Œéš¾ä»¥é€‚åº”ä¸åŒåœºæ™¯ï¼Œæ€§èƒ½å’Œç¤¾ä¼šæ¥å—åº¦å—é™ã€‚å¼ºåŒ–å­¦ä¹ è™½è¢«ç”¨äºæ”¹è¿›ä¼ ç»Ÿè§„åˆ’å™¨ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­å› ç¼ºä¹é€šç”¨æ€§å’Œä»¿çœŸå¤šæ ·æ€§è€Œå¤±æ•ˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºLE-Navæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼Œè‡ªé€‚åº”è°ƒæ•´è§„åˆ’å™¨è¶…å‚æ•°ã€‚åˆ©ç”¨ä¸€èŒƒä¾‹å’Œæ€ç»´é“¾æç¤ºç­–ç•¥å®ç°é›¶é•œå¤´åœºæ™¯ç†è§£ã€‚å®éªŒæ˜¾ç¤ºLE-Navå¯ç”Ÿæˆäººç±»çº§åˆ«è°ƒæ•´çš„è¶…å‚æ•°ï¼Œåœ¨ä¸åŒè§„åˆ’å’Œåœºæ™¯ä¸‹è¡¨ç°ä¼˜è¶Šã€‚åœ¨æ™ºèƒ½è½®æ¤…å¹³å°ä¸Šçš„çœŸå®ä¸–ç•Œå¯¼èˆªè¯•éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜ï¼Œå…¶åœ¨æˆåŠŸç‡ã€æ•ˆç‡ã€å®‰å…¨æ€§å’Œèˆ’é€‚æ€§ç­‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸»è§‚æ„ŸçŸ¥å®‰å…¨æ€§å’Œç¤¾ä¼šæ¥å—åº¦æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœåŠ¡æœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­éƒ¨ç½²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœºæ™¯å˜åŒ–å’Œå¤æ‚çš„ç¤¾ä¼šèƒŒæ™¯ã€‚</li>
<li>ä¼ ç»Ÿå¯¼èˆªç³»ç»Ÿä¾èµ–å›ºå®šå‚æ•°ï¼Œéš¾ä»¥é€‚åº”ä¸åŒåœºæ™¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œç¤¾ä¼šæ¥å—åº¦é™ä½ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æœºå™¨äººå¯¼èˆªä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ï¼Œä¸»è¦ç”±äºç¼ºä¹é€šç”¨æ€§å’Œä»¿çœŸå¤šæ ·æ€§ã€‚</li>
<li>LE-Navæ¡†æ¶ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´è§„åˆ’å™¨è¶…å‚æ•°ã€‚</li>
<li>LE-Navåˆ©ç”¨ä¸€èŒƒä¾‹å’Œæ€ç»´é“¾æç¤ºç­–ç•¥å®ç°é›¶é•œå¤´åœºæ™¯ç†è§£ï¼Œæé«˜å¯¼èˆªé€‚åº”æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLE-Navåœ¨ç”Ÿæˆäººç±»çº§åˆ«è°ƒæ•´çš„è¶…å‚æ•°æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œé€‚ç”¨äºå¤šç§è§„åˆ’å’Œåœºæ™¯ã€‚</li>
<li>LE-Navåœ¨çœŸå®ä¸–ç•Œå¯¼èˆªè¯•éªŒå’Œç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€æ•ˆç‡ã€å®‰å…¨æ€§å’Œèˆ’é€‚æ€§ï¼Œä¸»è§‚æ„ŸçŸ¥å®‰å…¨æ€§å’Œç¤¾ä¼šæ¥å—åº¦æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41a3cf3921c67277e1779952f9f34f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfcbdd7cb8c25799b311e6d155024b5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-550fda4c0a1b777ac0e2b69db5838f8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6c3941b66ad7987bde5f96e53a4e872.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20c5ad2e22a0439850248c1c9d64d53e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Teach-Me-Sign-Stepwise-Prompting-LLM-for-Sign-Language-Production"><a href="#Teach-Me-Sign-Stepwise-Prompting-LLM-for-Sign-Language-Production" class="headerlink" title="Teach Me Sign: Stepwise Prompting LLM for Sign Language Production"></a>Teach Me Sign: Stepwise Prompting LLM for Sign Language Production</h2><p><strong>Authors:Zhaoyi An, Rei Kawakami</strong></p>
<p>Large language models, with their strong reasoning ability and rich knowledge, have brought revolution to many tasks of AI, but their impact on sign language generation remains limited due to its complexity and unique rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign language as another natural language. By fine-tuning an LLM, we enable it to learn the correspondence between text and sign language, and facilitate generation. Considering the differences between sign and spoken language, we employ a stepwise prompting strategy to extract the inherent sign language knowledge within the LLM, thereby supporting the learning and generation process. Experimental results on How2Sign and Phoenix14T datasets demonstrate that our approach effectively leverages both the sign language knowledge and reasoning capabilities of LLM to align the different distribution and grammatical rules between sign and spoken language. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å‡­å€Ÿå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œä¸°å¯Œçš„çŸ¥è¯†ï¼Œåœ¨AIçš„è®¸å¤šä»»åŠ¡ä¸­å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†ç”±äºæ‰‹è¯­æœ¬èº«çš„å¤æ‚æ€§å’Œç‹¬ç‰¹è§„åˆ™ï¼Œå…¶åœ¨æ‰‹è¯­ç”Ÿæˆæ–¹é¢çš„å½±å“ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºâ€œæ•™å¯¼æˆ‘æ‰‹è¯­â€ï¼ˆTEAch Me Signï¼Œç®€ç§°TEAM-Signï¼‰çš„æ–¹æ³•ï¼Œå°†æ‰‹è¯­è§†ä¸ºå¦ä¸€ç§è‡ªç„¶è¯­è¨€ã€‚é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶å­¦ä¹ æ–‡æœ¬å’Œæ‰‹è¯­ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶ä¿ƒè¿›ç”Ÿæˆã€‚è€ƒè™‘åˆ°æ‰‹è¯­å’Œå£è¯­ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†æ­¥æç¤ºç­–ç•¥æ¥æå–LLMä¸­çš„å†…åœ¨æ‰‹è¯­çŸ¥è¯†ï¼Œä»è€Œæ”¯æŒå­¦ä¹ å’Œç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨How2Signå’ŒPhoenix14Tæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†LLMçš„æ‰‹è¯­çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯¹é½äº†æ‰‹è¯­å’Œå£è¯­ä¹‹é—´ä¸åŒçš„åˆ†å¸ƒå’Œè¯­æ³•è§„åˆ™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10972v1">PDF</a> Accepted by IEEE ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å› å…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œä¸°å¯Œçš„çŸ¥è¯†è€Œåœ¨äººå·¥æ™ºèƒ½çš„è®¸å¤šä»»åŠ¡ä¸­å¼•å‘äº†é©å‘½ï¼Œä½†å…¶åœ¨æ‰‹è¯­ç”Ÿæˆæ–¹é¢çš„å½±å“ç”±äºæ‰‹è¯­çš„å¤æ‚æ€§å’Œç‹¬ç‰¹è§„åˆ™è€Œå—é™ã€‚æœ¬æ–‡æå‡ºTEAch Me Signï¼ˆTEAM-Signï¼‰æ–¹æ³•ï¼Œå°†æ‰‹è¯­è§†ä¸ºå¦ä¸€ç§è‡ªç„¶è¯­è¨€ã€‚é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶å­¦ä¹ æ–‡æœ¬å’Œæ‰‹è¯­ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶ä¿ƒè¿›ç”Ÿæˆã€‚è€ƒè™‘åˆ°æ‰‹è¯­å’Œå£è¯­ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†æ­¥æç¤ºç­–ç•¥ï¼Œä»LLMä¸­æå–å†…åœ¨çš„æ‰‹è¯­è¨€çŸ¥è¯†ï¼Œä»è€Œæ”¯æŒå­¦ä¹ å’Œç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨How2Signå’ŒPhoenix14Tæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†LLMçš„æ‰‹è¯­çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯¹é½äº†æ‰‹è¯­å’Œå£è¯­ä¹‹é—´ä¸åŒçš„åˆ†å¸ƒå’Œè¯­æ³•è§„åˆ™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨AIçš„å¤šä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¯¹æ‰‹è¯­ç”Ÿæˆçš„å½±å“æœ‰é™ã€‚</li>
<li>TEAM-Signæ–¹æ³•æ—¨åœ¨é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ–‡æœ¬å’Œæ‰‹è¯­ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>æ‰‹è¯­ä¸å£è¯­ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œå› æ­¤é‡‡ç”¨åˆ†æ­¥æç¤ºç­–ç•¥æ¥æå–LLMä¸­çš„æ‰‹è¯­è¨€çŸ¥è¯†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¯¹é½æ‰‹è¯­å’Œå£è¯­ä¹‹é—´çš„ä¸åŒåˆ†å¸ƒå’Œè¯­æ³•è§„åˆ™ã€‚</li>
<li>TEAM-Signæ–¹æ³•åˆ©ç”¨LLMçš„æ‰‹è¯­çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†æ‰‹è¯­ç”Ÿæˆçš„æ•ˆæœã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨How2Signå’ŒPhoenix14Tæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a31010151c92cd35507657ea6df0e5e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32268295c942aae93470e7afd97d2d0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b33811acbb83eb777f89b76cc2953609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17a49b550f3baf3212a474ada82cb058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a307b16054bb36a7511be8d2342af0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Modeling-Understanding-of-Story-Based-Analogies-Using-Large-Language-Models"><a href="#Modeling-Understanding-of-Story-Based-Analogies-Using-Large-Language-Models" class="headerlink" title="Modeling Understanding of Story-Based Analogies Using Large Language   Models"></a>Modeling Understanding of Story-Based Analogies Using Large Language   Models</h2><p><strong>Authors:Kalit Inani, Keshav Kabra, Vijay Marupudi, Sashank Varma</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have brought them closer to matching human cognition across a variety of tasks. How well do these models align with human performance in detecting and mapping analogies? Prior research has shown that LLMs can extract similarities from analogy problems but lack robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the current study focused on a story-based analogical mapping task and conducted a fine-grained evaluation of LLM reasoning abilities compared to human performance. First, it explored the semantic representation of analogies in LLMs, using sentence embeddings to assess whether they capture the similarity between the source and target texts of an analogy, and the dissimilarity between the source and distractor texts. Second, it investigated the effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we examine whether LLMs exhibit similar performance profiles to those observed in humans by evaluating their reasoning at the level of individual analogies, and not just at the level of overall accuracy (as prior studies have done). Our experiments include evaluating the impact of model size (8B vs. 70B parameters) and performance variation across state-of-the-art model architectures such as GPT-4 and LLaMA3. This work advances our understanding of the analogical reasoning abilities of LLMs and their potential as models of human reasoning. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è®©å®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­ä¸äººç±»è®¤çŸ¥çš„åŒ¹é…ç¨‹åº¦æ›´é«˜ã€‚è¿™äº›æ¨¡å‹åœ¨æ£€æµ‹å’Œæ˜ å°„ç±»æ¯”æ–¹é¢ä¸äººç±»è¡¨ç°çš„å¯¹é½ç¨‹åº¦å¦‚ä½•ï¼Ÿå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMå¯ä»¥ä»ç±»æ¯”é—®é¢˜ä¸­æå–ç›¸ä¼¼æ€§ï¼Œä½†ç¼ºä¹åƒäººç±»ä¸€æ ·çš„ç¨³å¥æ¨ç†ã€‚æœ¬ç ”ç©¶ä»¥Webbã€Holyoakå’ŒLuï¼ˆ2023ï¼‰çš„ç ”ç©¶ä¸ºåŸºç¡€ï¼Œé‡ç‚¹å…³æ³¨åŸºäºæ•…äº‹çš„ç±»æ¯”æ˜ å°„ä»»åŠ¡ï¼Œå¯¹LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œç²¾ç»†è¯„ä¼°ï¼Œå¹¶ä¸äººç±»è¡¨ç°è¿›è¡Œæ¯”è¾ƒã€‚é¦–å…ˆï¼Œå®ƒæ¢ç´¢äº†LLMä¸­ç±»æ¯”çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä½¿ç”¨å¥å­åµŒå…¥æ¥è¯„ä¼°å®ƒä»¬æ˜¯å¦æ•è·äº†ç±»æ¯”ä¸­æºæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠæºæ–‡æœ¬å’Œå¹²æ‰°æ–‡æœ¬ä¹‹é—´çš„ä¸ç›¸ä¼¼æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒè°ƒæŸ¥äº†æ˜ç¡®æç¤ºLLMè§£é‡Šç±»æ¯”çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°å•ä¸ªç±»æ¯”å±‚é¢çš„æ¨ç†ï¼Œè€Œéä»…ä»…è¯„ä¼°æ•´ä½“å‡†ç¡®æ€§ï¼ˆå¦‚å…ˆå‰ç ”ç©¶æ‰€åšï¼‰ï¼Œæ¥æ£€æŸ¥LLMæ˜¯å¦è¡¨ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ€§èƒ½ç‰¹å¾ã€‚æˆ‘ä»¬çš„å®éªŒåŒ…æ‹¬è¯„ä¼°æ¨¡å‹å¤§å°ï¼ˆ8Bä¸70Bå‚æ•°ï¼‰çš„å½±å“ï¼Œä»¥åŠåœ¨æœ€å…ˆè¿›æŠ€æœ¯æ¶æ„ï¼ˆå¦‚GPT-4å’ŒLLaMA3ï¼‰ä¹‹é—´æ€§èƒ½çš„å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†æˆ‘ä»¬å¯¹äºLLMçš„ç±»æ¯”æ¨ç†èƒ½åŠ›åŠå…¶ä½œä¸ºäººç±»æ¨ç†æ¨¡å‹æ½œåŠ›çš„ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10957v1">PDF</a> To appear at CogSci 2025</p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç±»æ¯”æ£€æµ‹ä¸æ˜ å°„ä»»åŠ¡ä¸Šçš„è¡¨ç°é€æ¸æ¥è¿‘äººç±»è®¤çŸ¥ã€‚ç ”ç©¶å…³æ³¨æ•…äº‹ç±»ç±»æ¯”æ˜ å°„ä»»åŠ¡ï¼Œç»†è‡´è¯„ä¼°äº†LLMæ¨ç†èƒ½åŠ›ä¸äººç±»è¡¨ç°çš„å·®å¼‚ã€‚ç ”ç©¶é¦–å…ˆæ¢ç´¢äº†LLMä¸­ç±»æ¯”çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä½¿ç”¨å¥å­åµŒå…¥è¯„ä¼°å…¶æ•æ‰æºæ–‡æœ¬ä¸ç›®æ ‡æ–‡æœ¬é—´ç›¸ä¼¼æ€§ä»¥åŠæºæ–‡æœ¬ä¸å¹²æ‰°æ–‡æœ¬é—´å·®å¼‚çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œè°ƒæŸ¥äº†æ˜ç¡®æç¤ºLLMè§£é‡Šç±»æ¯”çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†LLMåœ¨å•ä¸ªç±»æ¯”å±‚é¢çš„æ¨ç†è¡¨ç°ï¼Œè€Œéä»…å…³æ³¨æ€»ä½“å‡†ç¡®ç‡ã€‚å®éªŒè¿˜åŒ…æ‹¬è¯„ä¼°æ¨¡å‹è§„æ¨¡ï¼ˆ8Bä¸70Bå‚æ•°ï¼‰çš„å½±å“ä»¥åŠä¸åŒå…ˆè¿›æ¨¡å‹æ¶æ„ï¼ˆå¦‚GPT-4å’ŒLLaMA3ï¼‰çš„è¡¨ç°å·®å¼‚ã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºå¢è¿›å¯¹LLMç±»æ¯”æ¨ç†èƒ½åŠ›çš„ç†è§£ï¼Œå¹¶æ¨åŠ¨å…¶ä½œä¸ºäººç±»æ¨ç†æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç±»æ¯”æ£€æµ‹ä¸æ˜ å°„ä»»åŠ¡ä¸Šçš„è¡¨ç°é€æ¸æ¥è¿‘äººç±»è®¤çŸ¥ã€‚</li>
<li>ç ”ç©¶å…³æ³¨æ•…äº‹ç±»ç±»æ¯”æ˜ å°„ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œäº†ç»†è‡´çš„è¯„ä¼°ã€‚</li>
<li>LLMçš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›å¯¹äºæ•æ‰ç±»æ¯”ä¸­çš„æºæ–‡æœ¬ä¸ç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯å…³é”®ã€‚</li>
<li>æ˜ç¡®æç¤ºLLMè§£é‡Šç±»æ¯”çš„æœ‰æ•ˆæ€§å¾—åˆ°è°ƒæŸ¥ã€‚</li>
<li>è¯„ä¼°äº†LLMåœ¨å•ä¸ªç±»æ¯”å±‚é¢çš„æ¨ç†è¡¨ç°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºæ¨¡å‹è§„æ¨¡ä¸æ¶æ„å¯¹LLMçš„ç±»æ¯”æ¨ç†èƒ½åŠ›æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6443e17b58c09f286bcc4613ba8dce19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac48101d74ef693193ed219a563f5f14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b4c6129a42b2bc8d14b027d6cf8f7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c305d348fcfecbed842d4ebab1df4562.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70d4468b4d3bba1cb9db7fb09369f82d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MalCodeAI-Autonomous-Vulnerability-Detection-and-Remediation-via-Language-Agnostic-Code-Reasoning"><a href="#MalCodeAI-Autonomous-Vulnerability-Detection-and-Remediation-via-Language-Agnostic-Code-Reasoning" class="headerlink" title="MalCodeAI: Autonomous Vulnerability Detection and Remediation via   Language Agnostic Code Reasoning"></a>MalCodeAI: Autonomous Vulnerability Detection and Remediation via   Language Agnostic Code Reasoning</h2><p><strong>Authors:Jugal Gajjar, Kamalasankari Subramaniakuppusamy, Noha El Kachach</strong></p>
<p>The growing complexity of cyber threats and the limitations of traditional vulnerability detection tools necessitate novel approaches for securing software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI pipeline for autonomous code security analysis and remediation. MalCodeAI combines code decomposition and semantic reasoning using fine-tuned Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA) within the MLX framework, and delivers scalable, accurate results across 14 programming languages. In Phase 1, the model achieved a validation loss as low as 0.397 for functional decomposition and summarization of code segments after 200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In Phase 2, for vulnerability detection and remediation, it achieved a best validation loss of 0.199 using the same number of iterations and trainable layers but with an increased learning rate of 4 x 10^(-5), effectively identifying security flaws and suggesting actionable fixes. MalCodeAI supports red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot generalization to detect complex, zero-day vulnerabilities. In a qualitative evaluation involving 15 developers, the system received high scores in usefulness (mean 8.06&#x2F;10), interpretability (mean 7.40&#x2F;10), and readability of outputs (mean 7.53&#x2F;10), confirming its practical value in real-world development workflows. This work marks a significant advancement toward intelligent, explainable, and developer-centric software security solutions. </p>
<blockquote>
<p>éšç€ç½‘ç»œå¨èƒçš„æ—¥ç›Šå¤æ‚å’Œä¼ ç»Ÿæ¼æ´æ£€æµ‹å·¥å…·çš„å±€é™æ€§ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨æ–°å‹æ–¹æ³•æ¥ä¿éšœè½¯ä»¶ç³»ç»Ÿçš„å®‰å…¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†MalCodeAIï¼Œè¿™æ˜¯ä¸€æ¬¾è¯­è¨€æ— å…³ã€å¤šé˜¶æ®µçš„AIç®¡é“ï¼Œç”¨äºè‡ªä¸»ä»£ç å®‰å…¨åˆ†æå’Œä¿®å¤ã€‚MalCodeAIç»“åˆäº†ä»£ç åˆ†è§£å’Œè¯­ä¹‰æ¨ç†ï¼Œä½¿ç”¨åœ¨MLXæ¡†æ¶å†…é€šè¿‡Low-Rank Adaptationï¼ˆLoRAï¼‰ä¼˜åŒ–çš„fine-tuned Qwen2.5-Coder-3B-Instructæ¨¡å‹ï¼Œå¹¶åœ¨14ç§ç¼–ç¨‹è¯­è¨€ä¸­æä¾›å¯æ‰©å±•ã€å‡†ç¡®çš„ç»“æœã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç»è¿‡200æ¬¡è¿­ä»£ã€6å±‚å¯è®­ç»ƒå±‚å’Œ2x10^ï¼ˆ-5ï¼‰çš„å­¦ä¹ ç‡ï¼Œè¯¥æ¨¡å‹åœ¨åŠŸèƒ½åˆ†è§£å’Œä»£ç æ®µæ‘˜è¦æ–¹é¢çš„éªŒè¯æŸå¤±é™ä½åˆ°äº†0.397ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œç”¨äºæ¼æ´æ£€æµ‹å’Œä¿®å¤ï¼Œä½¿ç”¨ç›¸åŒçš„è¿­ä»£æ¬¡æ•°å’Œå¯è®­ç»ƒå±‚æ•°ï¼Œä½†å°†å­¦ä¹ ç‡æé«˜åˆ°4x10^ï¼ˆ-5ï¼‰ï¼Œå–å¾—äº†æœ€ä½³çš„0.199éªŒè¯æŸå¤±ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«äº†å®‰å…¨æ¼æ´å¹¶æå‡ºäº†å¯è¡Œçš„ä¿®å¤å»ºè®®ã€‚MalCodeAIæ”¯æŒçº¢å¸½å¼æ¼æ´è¿½è¸ªã€åŸºäºCVSSçš„é£é™©è¯„åˆ†ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯æ£€æµ‹å¤æ‚ã€é›¶æ—¥æ¼æ´ã€‚åœ¨æ¶‰åŠ15åå¼€å‘è€…çš„å®šæ€§è¯„ä¼°ä¸­ï¼Œç³»ç»Ÿåœ¨å®ç”¨æ€§ï¼ˆå¹³å‡åˆ†8.06&#x2F;10ï¼‰ã€å¯è§£é‡Šæ€§ï¼ˆå¹³å‡åˆ†7.40&#x2F;10ï¼‰å’Œè¾“å‡ºå¯è¯»æ€§ï¼ˆå¹³å‡åˆ†7.53&#x2F;10ï¼‰æ–¹é¢è·å¾—äº†é«˜åˆ†ï¼Œè¿™è¯å®äº†å®ƒåœ¨ç°å®å¼€å‘æµç¨‹ä¸­çš„å®ç”¨ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€æœç€æ™ºèƒ½ã€å¯è§£é‡Šå’Œé¢å‘å¼€å‘è€…çš„è½¯ä»¶å®‰å…¨è§£å†³æ–¹æ¡ˆè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10898v1">PDF</a> 6 pages, 4 figures, accepted for publication in IEEE 26th   International Conference on Information Reuse and Integration (IRI 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MalCodeAIï¼Œä¸€ç§ç”¨äºè‡ªä¸»ä»£ç å®‰å…¨åˆ†æå’Œä¿®å¤çš„å¤šé˜¶æ®µäººå·¥æ™ºèƒ½ç®¡é“ã€‚å®ƒç»“åˆäº†ä»£ç åˆ†è§£å’Œè¯­ä¹‰æ¨ç†ï¼Œä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„Qwen2.5-Coder-3B-Instructæ¨¡å‹ï¼Œå¹¶é€šè¿‡MLXæ¡†æ¶ä¸­çš„ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œå¯åœ¨14ç§ç¼–ç¨‹è¯­è¨€ä¸­æä¾›å¯æ‰©å±•å’Œå‡†ç¡®çš„ç»“æœã€‚åœ¨é˜¶æ®µ1å’Œé˜¶æ®µ2çš„éªŒè¯ä¸­ï¼Œè¯¥æ¨¡å‹åˆ†åˆ«å®ç°äº†ä½è‡³0.397å’Œ0.199çš„éªŒè¯æŸå¤±ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ä»£ç æ®µåŠŸèƒ½åˆ†è§£ã€æ‘˜è¦ç¼–å†™ã€æ¼æ´æ£€æµ‹å’Œä¿®å¤èƒ½åŠ›ã€‚MalCodeAIæ”¯æŒçº¢å¸½é£æ ¼æ¼æ´åˆ©ç”¨è¿½è¸ªã€åŸºäºCVSSçš„é£é™©è¯„åˆ†ï¼Œå¹¶èƒ½é›¶æ—¥é€šç”¨åŒ–æ£€æµ‹å¤æ‚ã€é›¶æ—¥æ¼æ´ã€‚å¼€å‘è€…è¯„ä»·ä¸­ï¼Œç³»ç»Ÿåœ¨å®ç”¨æ€§ã€è§£é‡Šæ€§å’Œè¾“å‡ºå¯è¯»æ€§æ–¹é¢å‡è·å¾—è¾ƒé«˜è¯„åˆ†ï¼Œè¯æ˜å…¶åœ¨å®é™…å¼€å‘æµç¨‹ä¸­çš„å®ç”¨ä»·å€¼ã€‚æ ‡å¿—ç€è½¯ä»¶å®‰å…¨è§£å†³æ–¹æ¡ˆå‘æ™ºèƒ½åŒ–ã€å¯è§£é‡Šæ€§å’Œå¼€å‘è€…ä¸ºä¸­å¿ƒçš„æ–¹å‘è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MalCodeAIæ˜¯ä¸€ä¸ªå¤šé˜¶æ®µçš„äººå·¥æ™ºèƒ½ç®¡é“ï¼Œç”¨äºè‡ªä¸»ä»£ç å®‰å…¨åˆ†æå’Œä¿®å¤ã€‚</li>
<li>ç»“åˆä»£ç åˆ†è§£å’Œè¯­ä¹‰æ¨ç†ï¼Œä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„Qwen2.5-Coder-3B-Instructæ¨¡å‹ã€‚</li>
<li>åœ¨MLXæ¡†æ¶ä¸­é€šè¿‡Low-Rank Adaptation (LoRA)ä¼˜åŒ–ï¼Œæ”¯æŒ14ç§ç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>é˜¶æ®µ1å’Œé˜¶æ®µ2çš„æ¨¡å‹éªŒè¯æŸå¤±åˆ†åˆ«ä½è‡³0.397å’Œ0.199ã€‚</li>
<li>MalCodeAIå…·å¤‡å¼ºå¤§çš„åŠŸèƒ½åˆ†è§£ã€æ‘˜è¦ç¼–å†™ã€æ¼æ´æ£€æµ‹å’Œä¿®å¤èƒ½åŠ›ã€‚</li>
<li>æ”¯æŒçº¢å¸½é£æ ¼æ¼æ´åˆ©ç”¨è¿½è¸ªã€åŸºäºCVSSçš„é£é™©è¯„åˆ†ï¼Œå¹¶èƒ½é›¶æ—¥é€šç”¨åŒ–æ£€æµ‹å¤æ‚ã€é›¶æ—¥æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e57cd1f1f6ba56fd1b1b7ac69d78c5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0b93784f41ed0c90b97aa01cd143cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1aac71597fafc84adf9ad5b7ba699e02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4262768b2f431dfa4b4bb66e16a911d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06f3abdaa220fc218b4a4d5367b8f670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eb6308855a5c3b0fcba025f38f5c33e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SQLord-A-Robust-Enterprise-Text-to-SQL-Solution-via-Reverse-Data-Generation-and-Workflow-Decomposition"><a href="#SQLord-A-Robust-Enterprise-Text-to-SQL-Solution-via-Reverse-Data-Generation-and-Workflow-Decomposition" class="headerlink" title="SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data   Generation and Workflow Decomposition"></a>SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data   Generation and Workflow Decomposition</h2><p><strong>Authors:Song Cheng, Qiannan Cheng, Linbo Jin, Lei Yi, Guannan Zhang</strong></p>
<p>Transforming natural language into SQL queries (NL2SQL) is crucial for data-driven business applications. Existing frameworks, trained on open-source datasets, struggle with complex business logic and lack domain-specific data for fine-tuning. Additionally, evaluation methods often require annotated data and executable database environments, which are scarce in real-world scenarios. To address these challenges, we propose SQLord, an enterprise-level NL2SQL framework. First, SQLord introduces a data reverse generation approach to convert raw SQL statements into annotated data for supervised fine-tuning (SFT). Second, it proposes a decomposition method for complex queries using an automated workflow generator. Additionally, SQLord features a comprehensive GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios. Offline tests significantly outperform state of the art baselines, and online accuracy consistently exceeds 90, highlighting SQLordâ€™s advantages and effectiveness in complex real world scenarios. SQLord has been successfully applied across multiple scenarios on the worldâ€™s largest B2B e-commerce platform. </p>
<blockquote>
<p>å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢ï¼ˆNL2SQLï¼‰å¯¹äºæ•°æ®é©±åŠ¨çš„ä¸šåŠ¡åº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚ç°æœ‰æ¡†æ¶åœ¨å¼€æºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„ä¸šåŠ¡é€»è¾‘ï¼Œå¹¶ä¸”ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œè¯„ä¼°æ–¹æ³•é€šå¸¸éœ€è¦æ³¨é‡Šæ•°æ®å’Œå¯æ‰§è¡Œçš„æ•°æ®åº“ç¯å¢ƒï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­éå¸¸ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¼ä¸šçº§NL2SQLæ¡†æ¶SQLordã€‚é¦–å…ˆï¼ŒSQLordå¼•å…¥äº†ä¸€ç§æ•°æ®åå‘ç”Ÿæˆæ–¹æ³•ï¼Œå°†åŸå§‹SQLè¯­å¥è½¬æ¢ä¸ºæ³¨é‡Šæ•°æ®ï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å…¶æ¬¡ï¼Œå®ƒæå‡ºäº†ä¸€ç§ä½¿ç”¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆå™¨å¯¹å¤æ‚æŸ¥è¯¢è¿›è¡Œåˆ†è§£çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSQLordè¿˜åŒ…å«ä¸€ä¸ªå…¨é¢çš„GPT-Judgeè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ‰§è¡Œè¯„ä¼°ï¼ˆEXEï¼‰ã€æŸ¥è¯¢SQLè¯„ä¼°ï¼ˆQSEï¼‰å’ŒSQL-SQLè¯„ä¼°ï¼ˆSSEï¼‰ï¼Œé€‚ç”¨äºå„ç§åœºæ™¯ã€‚ç¦»çº¿æµ‹è¯•æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåœ¨çº¿å‡†ç¡®ç‡æŒç»­è¶…è¿‡90%ï¼Œçªæ˜¾äº†SQLordåœ¨å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„ä¼˜åŠ¿å’Œæœ‰æ•ˆæ€§ã€‚SQLordå·²åœ¨å…¨çƒæœ€å¤§çš„B2Bç”µå­å•†åŠ¡å¹³å°ä¸Šçš„å¤šä¸ªåœºæ™¯ä¸­å¾—åˆ°æˆåŠŸåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10629v1">PDF</a> WWW â€˜25: Companion Proceedings of the ACM on Web Conference 2025   Pages 919 - 923 <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3701716.3715541">https://doi.org/10.1145/3701716.3715541</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€åˆ°SQLæŸ¥è¯¢ï¼ˆNL2SQLï¼‰è½¬æ¢çš„é‡è¦æ€§ï¼Œé’ˆå¯¹ç°æœ‰æ¡†æ¶åœ¨å¤„ç†å¤æ‚ä¸šåŠ¡é€»è¾‘å’Œç¼ºä¹ç‰¹å®šé¢†åŸŸæ•°æ®å¾®è°ƒæ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SQLordè¿™ä¸€ä¼ä¸šçº§NL2SQLæ¡†æ¶ã€‚å®ƒé‡‡ç”¨æ•°æ®åå‘ç”Ÿæˆæ–¹æ³•å°†åŸå§‹SQLè¯­å¥è½¬æ¢ä¸ºæ³¨é‡Šæ•°æ®ï¼Œæ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼ŒSQLordè¿˜æå‡ºäº†å¤æ‚æŸ¥è¯¢çš„åˆ†è§£æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆå™¨å®ç°ã€‚å…¶å…¨é¢çš„GPT-Judgeè¯„ä¼°æ¡†æ¶åŒ…æ‹¬æ‰§è¡Œè¯„ä¼°ï¼ˆEXEï¼‰ã€æŸ¥è¯¢SQLè¯„ä¼°ï¼ˆQSEï¼‰å’ŒSQL-SQLè¯„ä¼°ï¼ˆSSEï¼‰ï¼Œé€‚åº”ä¸åŒåœºæ™¯ã€‚ç¦»çº¿æµ‹è¯•è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œåœ¨çº¿å‡†ç¡®ç‡è¶…è¿‡90%ï¼Œè¡¨æ˜SQLordåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ä¼˜åŠ¿å’Œæœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶å·²åœ¨ä¸–ç•Œæœ€å¤§çš„B2Bç”µå­å•†åŠ¡å¹³å°ä¸Šçš„å¤šä¸ªåœºæ™¯æˆåŠŸåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SQLordæ˜¯ä¸€ä¸ªé’ˆå¯¹è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢çš„ä¼ä¸šçº§æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¡†æ¶åœ¨å¤„ç†å¤æ‚ä¸šåŠ¡é€»è¾‘å’Œç‰¹å®šé¢†åŸŸæ•°æ®å¾®è°ƒæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>SQLordé€šè¿‡æ•°æ®åå‘ç”Ÿæˆæ–¹æ³•ï¼Œå°†åŸå§‹SQLè¯­å¥è½¬æ¢ä¸ºæ³¨é‡Šæ•°æ®ï¼Œæ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>æ¡†æ¶å…·æœ‰åˆ†è§£å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆå™¨å®ç°ã€‚</li>
<li>SQLordé…å¤‡äº†å…¨é¢çš„GPT-Judgeè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ‰§è¡Œè¯„ä¼°ã€æŸ¥è¯¢SQLè¯„ä¼°å’ŒSQL-SQLè¯„ä¼°ï¼Œä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>ç¦»çº¿æµ‹è¯•è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œåœ¨çº¿å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</li>
<li>SQLordå·²æˆåŠŸåº”ç”¨äºä¸–ç•Œæœ€å¤§çš„B2Bç”µå­å•†åŠ¡å¹³å°ä¸Šçš„å¤šä¸ªåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaf5374bd35c3d6c7d4361e8999cf302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3437b74805877d0f9fbf226f449d4a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ec9fe81cb1845886f3805b686e97c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1063999bd887f6b99a111cfb059919bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74241b735197f2a7427232fb9c6443bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4581e4b34bc9b5d20841007eb3b45093.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EmbRACE-3K-Embodied-Reasoning-and-Action-in-Complex-Environments"><a href="#EmbRACE-3K-Embodied-Reasoning-and-Action-in-Complex-Environments" class="headerlink" title="EmbRACE-3K: Embodied Reasoning and Action in Complex Environments"></a>EmbRACE-3K: Embodied Reasoning and Action in Complex Environments</h2><p><strong>Authors:Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi</strong></p>
<p>Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agentâ€™s intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the datasetâ€™s effectiveness in enabling the development of embodied reasoning capabilities. </p>
<blockquote>
<p>æœ€æ–°çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¢«åŠ¨ã€ç¦»çº¿å›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éœ€è¦åœ¨çº¿äº¤äº’å’Œä¸»åŠ¨åœºæ™¯ç†è§£çš„åµŒå…¥è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ã€‚åœ¨æ­¤ç±»åœºæ™¯ä¸­ï¼Œä»£ç†ä»ç¬¬ä¸€äººç§°è§†è§’æ„ŸçŸ¥ç¯å¢ƒï¼Œæ¯ä¸ªè¡ŒåŠ¨éƒ½ä¼šåŠ¨æ€åœ°å½±å“éšåçš„è§‚å¯Ÿã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4oã€Claude 3.5 Sonnetå’ŒGemini 2.5 Proåœ¨å¼€æ”¾ç¯å¢ƒäº¤äº’ä¸­ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œåœ¨ç©ºé—´æ¨ç†å’Œé•¿æœŸè§„åˆ’æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EmRACE-3Kï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è¶…è¿‡3000ä¸ªè¯­è¨€æŒ‡å¯¼ä»»åŠ¡ç»„æˆçš„æ•°æ®é›†ï¼Œè¿™äº›ä»»åŠ¡ä½äºä½¿ç”¨Unreal Engineå’ŒUnrealCV-Zooæ¡†æ¶æ„å»ºçš„å¤šå§¿å¤šå½©ã€é€¼çœŸçš„ç¯å¢ƒä¸­ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†å¹¿æ³›çš„åµŒå…¥æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¼èˆªã€å¯¹è±¡æ“ä½œå’Œå¤šé˜¶æ®µç›®æ ‡æ‰§è¡Œã€‚æ¯ä¸ªä»»åŠ¡éƒ½å±•å¼€ä¸ºå¤šæ­¥è½¨è¿¹ï¼Œå°†ç¬¬ä¸€äººç§°è§†è§‰è§‚å¯Ÿä¸é«˜çº§æŒ‡ä»¤ã€å®é™…è¡ŒåŠ¨å’Œè‡ªç„¶è¯­è¨€ç†æ€§ç›¸ç»“åˆï¼Œè¡¨è¾¾ä»£ç†æ¯ä¸€æ­¥çš„æ„å›¾ã€‚ä½¿ç”¨EmRACE-3Kï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°VLMsåœ¨ä¸‰ä¸ªå…³é”®ç»´åº¦çš„åµŒå…¥æ¨ç†èƒ½åŠ›ï¼šæ¢ç´¢ã€åŠ¨æ€ç©ºé—´è¯­ä¹‰æ¨ç†å’Œå¤šé˜¶æ®µç›®æ ‡æ‰§è¡Œã€‚åœ¨é›¶å°„å‡»è®¾ç½®ä¸­ï¼Œæ‰€æœ‰æ¨¡å‹çš„æˆåŠŸç‡éƒ½ä½äº20%ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ‰€æ„æˆçš„æŒ‘æˆ˜ä»¥åŠVLMsåœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„å½“å‰å±€é™æ€§ã€‚ä¸ºäº†è¯æ˜EmRACE-3Kçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¯¹Qwen2.5-VL-7Bè¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•åœ¨ä¸‰ä¸ªæŒ‘æˆ˜ç±»åˆ«ä¸­éƒ½å–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ï¼Œçªæ˜¾äº†æ•°æ®é›†åœ¨åŸ¹å…»åµŒå…¥æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10548v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mxllc.github.io/EmbRACE-3K/">https://mxllc.github.io/EmbRACE-3K/</a></p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¢«åŠ¨ã€ç¦»çº¿å›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡æ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†åœ¨éœ€è¦åœ¨çº¿äº’åŠ¨å’Œä¸»åŠ¨åœºæ™¯ç†è§£çš„å®ä½“åœºæ™¯ä¸­ï¼Œå…¶æ•ˆæœæœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæ¨å‡ºEmRACE-3Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡3000ä¸ªè¯­è¨€æŒ‡å¯¼ä»»åŠ¡ï¼Œæ¶µç›–å¹¿æ³›çš„å®ä½“æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¼èˆªã€å¯¹è±¡æ“ä½œå’Œåˆ†é˜¶æ®µç›®æ ‡æ‰§è¡Œç­‰ã€‚åˆ©ç”¨æ­¤æ•°æ®é›†è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å®ä½“æ¨ç†èƒ½åŠ›ï¼Œå¹¶å»ºç«‹åŸºå‡†æµ‹è¯•ã€‚å½“å‰æ¨¡å‹åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹çš„æˆåŠŸç‡ä½äº20%ï¼Œæ˜¾ç¤ºå…¶å¯¹æŒ‘æˆ˜æ€§å’Œå½“å‰æ¨¡å‹åœ¨äº’åŠ¨ç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚ç»è¿‡ç²¾ç»†åŒ–è®­ç»ƒåçš„æ¨¡å‹æ€§èƒ½æœ‰æ‰€æå‡ï¼Œå‡¸æ˜¾æ•°æ®é›†åœ¨åŸ¹å…»å®ä½“æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¢«åŠ¨ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®ä½“åœºæ™¯ä¸­çš„åœ¨çº¿äº’åŠ¨å’Œä¸»åŠ¨åœºæ™¯ç†è§£æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>EmRACE-3Kæ•°æ®é›†åŒ…å«å¤šæ ·åŒ–çš„è¯­è¨€æŒ‡å¯¼ä»»åŠ¡ï¼Œæ¶µç›–å¯¼èˆªã€å¯¹è±¡æ“ä½œå’Œå¤šé˜¶æ®µç›®æ ‡æ‰§è¡Œç­‰å®ä½“æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨EmRACE-3Kæ•°æ®é›†è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å®ä½“æ¨ç†èƒ½åŠ›ï¼Œå»ºç«‹åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹çš„æˆåŠŸç‡è¾ƒä½ï¼Œæ˜¾ç¤ºå…¶é¢ä¸´çš„æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚</li>
<li>é€šè¿‡ç²¾ç»†åŒ–è®­ç»ƒå¯ä»¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œå‡¸æ˜¾æ•°æ®é›†åœ¨åŸ¹å…»å®ä½“æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>EmRACE-3Kæ•°æ®é›†å¯¹äºæ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®ä½“åœºæ™¯ä¸­çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-44189f935af993549d9cc073a9ae7df8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06a94586280cd08d72c208fe0cc53eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ab42a1b6c589b7579969572111a8693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9d3a030ddcdb08dcd73a94439cab75e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e65b4c5487fb56206ffa0325138adc1c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="REST-Stress-Testing-Large-Reasoning-Models-by-Asking-Multiple-Problems-at-Once"><a href="#REST-Stress-Testing-Large-Reasoning-Models-by-Asking-Multiple-Problems-at-Once" class="headerlink" title="REST: Stress Testing Large Reasoning Models by Asking Multiple Problems   at Once"></a>REST: Stress Testing Large Reasoning Models by Asking Multiple Problems   at Once</h2><p><strong>Authors:Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu</strong></p>
<p>Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key insights emerge from our analysis: (1) the â€œoverthinking trapâ€ is a critical factor contributing to the performance degradation; (2) the models trained with â€œlong2shortâ€ technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation. Code and results are available at <a target="_blank" rel="noopener" href="https://opendatalab.github.io/REST">https://opendatalab.github.io/REST</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„æ¨ç†å¤§æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç‰¹å®šä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°æ–¹æ³•ä»ç„¶å—åˆ°å­¤ç«‹çš„é—®é¢˜è§£å†³æ¨¡å¼çš„é™åˆ¶ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡åºåˆ—æµ‹è¯•è¯„ä¼°å•ä¸€é—®é¢˜çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´å…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“ï¼Œä¸”ç¼ºä¹æŒ‘æˆ˜æ€§ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1åœ¨MATH500ä¸Šè¾¾åˆ°97.0%ï¼‰ï¼Œéœ€è¦èŠ±è´¹é«˜æ˜‚æˆæœ¬åˆ›å»ºæ–°çš„é—®é¢˜éœ€è¦å¤§é‡çš„äººåŠ›æŠ•å…¥ï¼›ï¼ˆ2ï¼‰æ— æ³•åœ¨å¤šä¸Šä¸‹æ–‡å‹åŠ›ä¸‹è¯„ä¼°æ¨¡å‹ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œéƒ¨ç½²çš„å…³é”®è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RESTï¼ˆé€šè¿‡åŒæ—¶æµ‹è¯•è¿›è¡Œæ¨ç†è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å‹åŠ›æµ‹è¯•æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶æš´éœ²å¤šä¸ªé—®é¢˜ç»™LRMsã€‚é™¤äº†åŸºæœ¬çš„æ¨ç†èƒ½åŠ›å¤–ï¼ŒRESTè¿˜è¯„ä¼°äº†å‡ ç§æœªç»å……åˆ†æµ‹è¯•çš„èƒ½åŠ›ï¼šä¸Šä¸‹æ–‡ä¼˜å…ˆçº§åˆ†é…ã€è·¨é—®é¢˜å¹²æ‰°æŠµæŠ—å’ŒåŠ¨æ€è®¤çŸ¥è´Ÿè·ç®¡ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†ä¸€äº›æƒŠäººçš„å‘ç°ï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚DeepSeek-R1ï¼Œåœ¨å‹åŠ›æµ‹è¯•ä¸‹ä¹Ÿä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚é‡è¦çš„æ˜¯ï¼ŒRESTæ˜¾ç¤ºå‡ºæ¯”ç°æœ‰åŸºå‡†æµ‹è¯•æ›´å¼ºçš„è¾¨åˆ«åŠ›ï¼Œæ­ç¤ºäº†å•é—®é¢˜è¯„ä¼°ä¸‹è¡¨ç°ç›¸ä¼¼ã€æ¥è¿‘ä¸Šé™çš„æ¨¡å‹ä¹‹é—´çš„æ˜æ˜¾æ€§èƒ½å·®å¼‚ã€‚ä»æˆ‘ä»¬çš„åˆ†æä¸­å¾—å‡ºäº†ä¸€äº›å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰â€œè¿‡åº¦æ€è€ƒé™·é˜±â€æ˜¯å¯¼è‡´æ€§èƒ½ä¸‹é™çš„å…³é”®å› ç´ ä¹‹ä¸€ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨â€œé•¿è½¬çŸ­â€æŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹åœ¨RESTä¸‹ä¿æŒäº†æ›´é«˜çš„å•é—®é¢˜æ€§èƒ½å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†æ ‡å‡†è®­ç»ƒçš„å¯¹ç­‰æ¨¡å‹ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†RESTä½œä¸ºä¸€ç§ç»æµé«˜æ•ˆã€é¢å‘æœªæ¥çš„è¯„ä¼°æ¨¡å¼ï¼Œæ›´èƒ½åæ˜ ç°å®ä¸–ç•Œçš„æ¨ç†éœ€æ±‚ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹æŒç»­äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚ä»£ç å’Œç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://opendatalab.github.io/REST%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://opendatalab.github.io/RESTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10541v2">PDF</a> REST (Reasoning Evaluation through Simultaneous Testing), a   stress-testing framework that concurrently exposes LRMs to multiple problems   simultaneously</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å½“å‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ä»»åŠ¡ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†å…¶è¯„ä¼°æ–¹æ³•ä»å—é™äºå­¤ç«‹çš„é—®é¢˜è§£å†³æ¨¡å¼ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é€šè¿‡ä¸²è¡Œæµ‹è¯•è¯„ä¼°å•ä¸€é—®é¢˜çš„æ¨ç†èƒ½åŠ›ï¼Œå­˜åœ¨å…³é”®å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RESTï¼ˆé€šè¿‡åŒæ—¶æµ‹è¯•è¿›è¡Œæ¨ç†è¯„ä¼°ï¼‰è¿™ä¸€å‹åŠ›æµ‹è¯•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿LRMsæš´éœ²äºå¤šä¸ªé—®é¢˜ä¹‹ä¸­ï¼Œå¹¶è¯„ä¼°äº†æ¨¡å‹åœ¨åŸºæœ¬æ¨ç†ä¹‹å¤–çš„å‡ é¡¹èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨å‹åŠ›æµ‹è¯•ä¸‹ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚RESTå±•ç°å‡ºæ¯”ç°æœ‰åŸºå‡†æµ‹è¯•æ›´å¼ºçš„é‰´åˆ«åŠ›ï¼Œæ­ç¤ºäº†æ¨¡å‹é—´æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚æœ¬æ–‡åˆ†ææ­ç¤ºäº†ä¸€äº›å…³é”®è§è§£ï¼Œå¦‚â€œè¿‡åº¦æ€è€ƒé™·é˜±â€æ˜¯æ€§èƒ½ä¸‹é™çš„å…³é”®å› ç´ ä¹‹ä¸€ï¼Œâ€œé•¿è½¬çŸ­â€æŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹åœ¨RESTä¸‹ä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒRESTä½œä¸ºä¸€ç§é«˜æ•ˆã€é¢å‘æœªæ¥çš„è¯„ä¼°æ¨¡å¼ï¼Œæ›´å¥½åœ°åæ˜ äº†ç°å®ä¸–ç•Œçš„æ¨ç†éœ€æ±‚ï¼Œå¹¶é™ä½äº†å¯¹äººç±»æŒç»­æ ‡æ³¨çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ä»»åŠ¡ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—è¿›æ­¥ï¼Œä½†è¯„ä¼°æ–¹æ³•å—é™ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°å•ä¸€é—®é¢˜çš„æ¨ç†èƒ½åŠ›ï¼Œå­˜åœ¨æ•°æ®æ±¡æŸ“å’Œç¼ºä¹å¤šä¸Šä¸‹æ–‡å‹åŠ›æµ‹è¯•çš„é—®é¢˜ã€‚</li>
<li>RESTæ¡†æ¶æ˜¯ä¸€ç§å‹åŠ›æµ‹è¯•æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶æš´éœ²æ¨¡å‹é¢å¯¹å¤šä¸ªé—®é¢˜ï¼Œè¯„ä¼°æ¨¡å‹çš„å¤šç§èƒ½åŠ›ã€‚</li>
<li>å…ˆè¿›æ¨¡å‹åœ¨å‹åŠ›æµ‹è¯•ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>RESTå±•ç°å‡ºæ›´å¼ºçš„é‰´åˆ«åŠ›ï¼Œèƒ½æ­ç¤ºæ¨¡å‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>â€œè¿‡åº¦æ€è€ƒé™·é˜±â€æ˜¯æ€§èƒ½ä¸‹é™çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
<li>ä½¿ç”¨â€œé•¿è½¬çŸ­â€æŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹åœ¨RESTè¯„ä¼°ä¸‹è¡¨ç°è¾ƒå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5b6d407fcec4d2c4f4f68826089d623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6034007d15de3b928e7b8e6401a7d6ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4110280705e1af8a4fdbc92efec6b0fa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reasoning-or-Memorization-Unreliable-Results-of-Reinforcement-Learning-Due-to-Data-Contamination"><a href="#Reasoning-or-Memorization-Unreliable-Results-of-Reinforcement-Learning-Due-to-Data-Contamination" class="headerlink" title="Reasoning or Memorization? Unreliable Results of Reinforcement Learning   Due to Data Contamination"></a>Reasoning or Memorization? Unreliable Results of Reinforcement Learning   Due to Data Contamination</h2><p><strong>Authors:Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang</strong></p>
<p>The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸€ç›´æ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œé€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºäº†è¿™äº›èƒ½åŠ›ï¼Œè®¸å¤šæ–°æ–¹æ³•å£°ç§°åœ¨å‡ ä¹æ²¡æœ‰æˆ–æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸€äº›ç ”ç©¶ç”šè‡³è¡¨æ˜ï¼Œéšæœºæˆ–é”™è¯¯çš„å¥–åŠ±ä¿¡å·å¯ä»¥å¢å¼ºæ¨ç†æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›çªç ´å¤§å¤šæ˜¯åœ¨Qwen2.5æ¨¡å‹å®¶æ—ä¸ŠæŠ¥é“çš„ï¼Œå¹¶åœ¨MATH-500ã€AMCå’ŒAIMEç­‰è‘—ååŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè€Œåœ¨å…¶ä»–æ¨¡å‹ï¼ˆå¦‚Llamaï¼‰ä¸Šæœªèƒ½å®ç°ç±»ä¼¼çš„æ”¶ç›Šï¼Œè¿™éœ€è¦è¿›è¡Œè¿›ä¸€æ­¥çš„è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå°½ç®¡Qwen2.5åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨å¤§è§„æ¨¡ç½‘ç»œè¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒä½¿å…¶å®¹æ˜“å—åˆ°æµè¡ŒåŸºå‡†æµ‹è¯•ä¸­çš„æ•°æ®æ±¡æŸ“ã€‚å› æ­¤ï¼Œä»è¿™äº›åŸºå‡†æµ‹è¯•ä¸­å¾—å‡ºçš„ç»“æœå¯èƒ½ä¸å¯é ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨å¯ä»¥ç”Ÿæˆä»»æ„é•¿åº¦å’Œéš¾åº¦çš„å®Œå…¨åˆæˆç®—æœ¯é—®é¢˜ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæˆ‘ä»¬ç§°ä¸ºRandomCalculationçš„å¹²å‡€æ•°æ®é›†ã€‚ä½¿ç”¨è¿™äº›æ— æ³„æ¼çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜åªæœ‰å‡†ç¡®çš„å¥–åŠ±ä¿¡å·æ‰èƒ½æŒç»­æé«˜æ€§èƒ½ï¼Œè€Œå˜ˆæ‚æˆ–é”™è¯¯çš„ä¿¡å·åˆ™ä¸èƒ½ã€‚æˆ‘ä»¬ä¸»å¼ åœ¨æœªè¢«æ±¡æŸ“çš„åŸºå‡†æµ‹è¯•å’Œå„ç§æ¨¡å‹å®¶æ—ä¸­è¯„ä¼°RLæ–¹æ³•ï¼Œä»¥ç¡®ä¿å¾—å‡ºå¯é çš„ç»“è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10532v1">PDF</a> 26 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸€ç›´æ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚æœ€è¿‘çš„ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¢å¼ºäº†è¿™äº›èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸€äº›åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç”šè‡³è¡¨æ˜éšæœºæˆ–é”™è¯¯çš„å¥–åŠ±ä¿¡å·å¯èƒ½å¢å¼ºæ¨ç†æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æˆæœä¸»è¦é›†ä¸­åœ¨Qwen2.5æ¨¡å‹å®¶æ—ä¸Šï¼Œå¯¹å…¶ä»–æ¨¡å‹å¦‚Llamaçš„é€‚ç”¨æ€§å°šå¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚åˆ†æè¡¨æ˜ï¼ŒQwen2.5è™½ç„¶åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶åœ¨å¤§è§„æ¨¡ç½‘ç»œè¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒï¼Œä½¿å…¶å®¹æ˜“å—åˆ°æµè¡ŒåŸºå‡†æµ‹è¯•ä¸­çš„æ•°æ®æ±¡æŸ“å½±å“ï¼Œå¯¼è‡´ç»“æœå¯èƒ½ä¸å¯é ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ç”Ÿæˆå™¨ï¼Œç”Ÿæˆæ— æ³„æ¼çš„éšæœºè®¡ç®—æ•°æ®é›†ã€‚ç ”ç©¶è¯å®ï¼Œåªæœ‰å‡†ç¡®çš„å¥–åŠ±ä¿¡å·æ‰èƒ½æŒç»­æé«˜æ€§èƒ½ï¼Œè€Œå™ªå£°æˆ–é”™è¯¯çš„ä¿¡å·åˆ™ä¸èƒ½ã€‚å»ºè®®å¯¹æœªå—æ±¡æŸ“çš„æ•°æ®é›†å’Œä¸åŒæ¨¡å‹å®¶æ—ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æŒç»­å—åˆ°å…³æ³¨ï¼Œå¼ºåŒ–å­¦ä¹ ç”¨äºå¢å¼ºå…¶æ€§èƒ½ã€‚</li>
<li>Qwen2.5æ¨¡å‹å®¶æ—åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºï¼Œä½†å…¶ä»–æ¨¡å‹é€‚ç”¨æ€§å°šå¾…ç ”ç©¶ã€‚</li>
<li>Qwen2.5åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é¢„è®­ç»ƒä½¿å…¶å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“å½±å“ã€‚</li>
<li>æ•°æ®æ±¡æŸ“å¯èƒ½å¯¼è‡´åŸºå‡†æµ‹è¯•ç»“æœä¸å¯é ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆå™¨åˆ›å»ºæ— æ³„æ¼çš„éšæœºè®¡ç®—æ•°æ®é›†ä»¥åº”å¯¹æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚</li>
<li>å‡†ç¡®çš„å¥–åŠ±ä¿¡å·å¯¹æé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œå™ªå£°æˆ–é”™è¯¯ä¿¡å·æ— æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c8679e3e6127beace56bc8ba85030f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd0de94e645b4a090de80b1a4c08ebac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc84d451464df4e6c21f702f9344f339.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbd62d9f94c4d860688549559d77e9ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Grammar-Guided-Evolutionary-Search-for-Discrete-Prompt-Optimisation"><a href="#Grammar-Guided-Evolutionary-Search-for-Discrete-Prompt-Optimisation" class="headerlink" title="Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation"></a>Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation</h2><p><strong>Authors:Muzhaffar Hazman, Minh-Khoi Pham, Shweta Soundararajan, Goncalo Mordido, Leonardo Custode, David Lynch, Giorgio Cruciata, Yucheng Shi, Hongmeng Song, Wang Chao, Pan Yue, Aleksandar Milenovic, Alexandros Agapitos</strong></p>
<p>Prompt engineering has proven to be a crucial step in leveraging pretrained large language models (LLMs) in solving various real-world tasks. Numerous solutions have been proposed that seek to automate prompt engineering by using the model itself to edit prompts. However, the majority of state-of-the-art approaches are evaluated on tasks that require minimal prompt templates and on very large and highly capable LLMs. In contrast, solving complex tasks that require detailed information to be included in the prompt increases the amount of text that needs to be optimised. Furthermore, smaller models have been shown to be more sensitive to prompt design. To address these challenges, we propose an evolutionary search approach to automated discrete prompt optimisation consisting of two phases. In the first phase, grammar-guided genetic programming is invoked to synthesise prompt-creating programmes by searching the space of programmes populated by function compositions of syntactic, dictionary-based and LLM-based prompt-editing functions. In the second phase, local search is applied to explore the neighbourhoods of best-performing programmes in an attempt to further fine-tune their performance. Our approach outperforms three state-of-the-art prompt optimisation approaches, PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose LLMs in four domain-specific challenging tasks. We also illustrate several examples where these benchmark methods suffer relatively severe performance degradation, while our approach improves performance in almost all task-model combinations, only incurring minimal degradation when it does not. </p>
<blockquote>
<p>æç¤ºå·¥ç¨‹å·²è¢«è¯æ˜æ˜¯å‘æŒ¥é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å„ç§ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨çš„å…³é”®æ­¥éª¤ã€‚å·²ç»æå‡ºäº†è®¸å¤šè§£å†³æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆè¯•å›¾ä½¿ç”¨æ¨¡å‹æœ¬èº«æ¥ç¼–è¾‘æç¤ºï¼Œä»è€Œå®ç°æç¤ºå·¥ç¨‹çš„è‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æœ€å…ˆè¿›çš„è¯„ä¼°æ–¹æ³•éƒ½æ˜¯åœ¨éœ€è¦å¾ˆå°‘æç¤ºæ¨¡æ¿çš„ä»»åŠ¡ä»¥åŠå¯¹éå¸¸å¤§ä¸”åŠŸèƒ½å¼ºå¤§çš„LLMä¸Šè¿›è¡Œçš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéœ€è¦è¯¦ç»†ä¿¡æ¯çš„å¤æ‚ä»»åŠ¡å¢åŠ äº†éœ€è¦ä¼˜åŒ–çš„æ–‡æœ¬é‡ã€‚æ­¤å¤–ï¼Œè¾ƒå°çš„æ¨¡å‹å·²è¢«è¯æ˜å¯¹æç¤ºè®¾è®¡æ›´ä¸ºæ•æ„Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è¿›åŒ–æœç´¢è‡ªåŠ¨åŒ–ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡è¯­æ³•å¼•å¯¼é—ä¼ ç¼–ç¨‹ï¼Œé€šè¿‡æœç´¢ç”±åŸºäºè¯­æ³•çš„å‡½æ•°ç»„åˆã€åŸºäºå­—å…¸å’ŒåŸºäºLLMçš„æç¤ºç¼–è¾‘å‡½æ•°ç»„æˆçš„ç¨‹åºç©ºé—´æ¥åˆæˆåˆ›å»ºæç¤ºçš„ç¨‹åºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå¯¹è¡¨ç°æœ€ä½³çš„ç¨‹åºçš„é‚»è¿‘åŒºåŸŸè¿›è¡Œå±€éƒ¨æœç´¢ï¼Œä»¥è¿›ä¸€æ­¥å¾®è°ƒå…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªç›¸å¯¹è¾ƒå°çš„é€šç”¨LLMä¸Šï¼Œåœ¨å››ä¸ªç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ä»»åŠ¡ä¸­ï¼Œè¶…è¶Šäº†ä¸‰ç§å…ˆè¿›çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼šPromptWizardã€OPROå’ŒRL-Promptã€‚æˆ‘ä»¬è¿˜ä¸¾äº†å‡ ä¸ªä¾‹å­æ¥è¯´æ˜è¿™äº›åŸºå‡†æ–¹æ³•åœ¨ç›¸å¯¹è¾ƒå¤§çš„æ€§èƒ½ä¸‹é™ä¸­é­å—çš„æŸå¤±ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰çš„ä»»åŠ¡æ¨¡å‹ç»„åˆä¸­éƒ½æé«˜äº†æ€§èƒ½ï¼Œåªåœ¨æå°‘æ•°æƒ…å†µä¸‹ä¼šå‡ºç°è½»å¾®çš„æ€§èƒ½ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10326v1">PDF</a> Accepted for Publication at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¿›åŒ–æœç´¢çš„è‡ªåŠ¨åŒ–ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…ä»»åŠ¡ä¸­çš„æç¤ºå·¥ç¨‹é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨è¯­æ³•å¼•å¯¼çš„é—ä¼ ç¼–ç¨‹æ¥åˆæˆæç¤ºåˆ›å»ºç¨‹åºï¼Œç¬¬äºŒé˜¶æ®µå¯¹è¡¨ç°æœ€ä½³çš„ç¨‹åºè¿›è¡Œå±€éƒ¨æœç´¢ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨å››ä¸ªç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¸‰ç§å…ˆè¿›çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶åœ¨å¤§å¤šæ•°ä»»åŠ¡æ¨¡å‹ç»„åˆä¸­æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºå·¥ç¨‹æ˜¯é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å„ç§å®é™…ä»»åŠ¡çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ–¹æ³•æ—¨åœ¨ä½¿ç”¨æ¨¡å‹æœ¬èº«ç¼–è¾‘æç¤ºï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦åœ¨ä»»åŠ¡éœ€è¦å°‘é‡æç¤ºæ¨¡æ¿å’Œé«˜åº¦åŠŸèƒ½å¼ºå¤§çš„LLMsä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å¯¹äºéœ€è¦è¯¦ç»†ä¿¡æ¯çš„å¤æ‚ä»»åŠ¡ï¼Œæç¤ºè®¾è®¡æ›´åŠ é‡è¦ï¼Œä¸”å°å‹æ¨¡å‹å¯¹æç¤ºè®¾è®¡æ›´ä¸ºæ•æ„Ÿã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¿›åŒ–æœç´¢çš„è‡ªåŠ¨åŒ–ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬è¯­æ³•å¼•å¯¼çš„é—ä¼ ç¼–ç¨‹å’Œå±€éƒ¨æœç´¢ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å››ä¸ªç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”ä¸‰ç§å…ˆè¿›çš„æç¤ºä¼˜åŒ–æ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤§å¤šæ•°ä»»åŠ¡æ¨¡å‹ç»„åˆä¸­ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ€§èƒ½ï¼Œä»…åœ¨å°‘æ•°æƒ…å†µä¸‹å‡ºç°è½»å¾®çš„æ€§èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e4071365774b15b77fc3ed286b5607d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df44544f5e6b45ca87a926808c2463a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6f866312a4492b480ec7d901e71da7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53946722cb5199344146e9b0df362550.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aae6a30f7ada40c618c354df7b328d27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fafd65b64a284a7c9ac18609ea579ac4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b08b6c7b46eccae6fc2f56552d9ac9a3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Prompt-Informed-Reinforcement-Learning-for-Visual-Coverage-Path-Planning"><a href="#Prompt-Informed-Reinforcement-Learning-for-Visual-Coverage-Path-Planning" class="headerlink" title="Prompt Informed Reinforcement Learning for Visual Coverage Path Planning"></a>Prompt Informed Reinforcement Learning for Visual Coverage Path Planning</h2><p><strong>Authors:Venkat Margapuri</strong></p>
<p>Visual coverage path planning with unmanned aerial vehicles (UAVs) requires agents to strategically coordinate UAV motion and camera control to maximize coverage, minimize redundancy, and maintain battery efficiency. Traditional reinforcement learning (RL) methods rely on environment-specific reward formulations that lack semantic adaptability. This study proposes Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates the zero-shot reasoning ability and in-context learning capability of large language models with curiosity-driven RL. PIRL leverages semantic feedback from an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal Policy Optimization (PPO) RL policy guiding the agent in position and camera adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI Gym and evaluated in various environments. Furthermore, the sim-to-real-like ability and zero-shot generalization of the agent are tested by operating the agent in Webots simulator which introduces realistic physical dynamics. Results show that PIRL outperforms multiple learning-based baselines such as PPO with static rewards, PPO with exploratory weight initialization, imitation learning, and an LLM-only controller. Across different environments, PIRL outperforms the best-performing baseline by achieving up to 14% higher visual coverage in OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and up to 18% lower redundancy, depending on the environment. The results highlight the effectiveness of LLM-guided reward shaping in complex spatial exploration tasks and suggest a promising direction for integrating natural language priors into RL for robotics. </p>
<blockquote>
<p>æ— äººæœºï¼ˆUAVï¼‰çš„è§†è§‰è¦†ç›–è·¯å¾„è§„åˆ’è¦æ±‚ä»£ç†æˆ˜ç•¥æ€§åœ°åè°ƒæ— äººæœºçš„è¿åŠ¨å’Œç›¸æœºæ§åˆ¶ï¼Œä»¥æœ€å¤§åŒ–è¦†ç›–ï¼Œæœ€å°åŒ–å†—ä½™å¹¶ç»´æŒç”µæ± æ•ˆç‡ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¾èµ–äºç‰¹å®šç¯å¢ƒçš„å¥–åŠ±å…¬å¼ï¼Œç¼ºä¹è¯­ä¹‰é€‚åº”æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†æç¤ºå¼ºåŒ–å­¦ä¹ ï¼ˆPIRLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶é•œå¤´æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¸åŸºäºå¥½å¥‡å¿ƒçš„RLç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚PIRLåˆ©ç”¨æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹GPT-3..çš„è¯­ä¹‰åé¦ˆï¼ŒåŠ¨æ€åœ°å½¢æˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰RLç­–ç•¥çš„å¥–åŠ±åŠŸèƒ½ï¼Œä»¥æŒ‡å¯¼ä»£ç†è¿›è¡Œä½ç½®å’Œç›¸æœºè°ƒæ•´ä»¥å®ç°æœ€ä½³è§†è§‰è¦†ç›–ã€‚PIRLä»£ç†ä½¿ç”¨OpenAI Gymè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å„ç§ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œé€šè¿‡Webotsæ¨¡æ‹Ÿå™¨æ“ä½œä»£ç†æµ‹è¯•äº†å…¶ä»¿çœŸåˆ°ç°å®çš„èƒ½åŠ›å’Œé›¶é•œå¤´æ³›åŒ–èƒ½åŠ›ï¼Œè¯¥æ¨¡æ‹Ÿå™¨å¼•å…¥äº†é€¼çœŸçš„ç‰©ç†åŠ¨æ€ã€‚ç»“æœè¡¨æ˜ï¼ŒPIRLåœ¨å¤šä¸ªå­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºäºé™æ€å¥–åŠ±çš„PPOã€å…·æœ‰æ¢ç´¢æƒé‡åˆå§‹åŒ–çš„PPOã€æ¨¡ä»¿å­¦ä¹ å’Œä»…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ§åˆ¶å™¨ã€‚åœ¨ä¸åŒçš„ç¯å¢ƒä¸­ï¼ŒPIRLåœ¨OpenAI Gymä¸Šå®ç°äº†é«˜è¾¾14ï¼…çš„è§†è§‰è¦†ç›–å’ŒWebotsä¸Šçš„é«˜è¾¾27ï¼…çš„è§†è§‰è¦†ç›–è¶…è¶Šè¡¨ç°æœ€ä½³çš„åŸºå‡†æµ‹è¯•ï¼Œç”µæ± æ•ˆç‡æé«˜äº†é«˜è¾¾25ï¼…ï¼Œå†—ä½™åº¦é™ä½äº†é«˜è¾¾18ï¼…ï¼Œè¿™å–å†³äºç¯å¢ƒã€‚ç»“æœçªå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼å¥–åŠ±å½¢çŠ¶åœ¨å¤æ‚çš„ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå°†è‡ªç„¶è¯­è¨€å…ˆéªŒçŸ¥è¯†èå…¥æœºå™¨äººå¼ºåŒ–å­¦ä¹ æå‡ºäº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10284v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ— äººæœºè§†è§‰è¦†ç›–è·¯å¾„è§„åˆ’éœ€è¦æ™ºèƒ½ä½“åè°ƒæ— äººæœºè¿åŠ¨å’Œç›¸æœºæ§åˆ¶ï¼Œä»¥æœ€å¤§åŒ–è¦†ç›–é¢ç§¯ã€æœ€å°åŒ–å†—ä½™å¹¶ç»´æŒç”µæ± æ•ˆç‡ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ¬¡æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„Prompt-Informedå¼ºåŒ–å­¦ä¹ ï¼ˆPIRLï¼‰ï¼Œä¸å¥½å¥‡å¿ƒé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆã€‚PIRLåˆ©ç”¨GPT-3.5çš„è¯­è¨€æ¨¡å‹è¯­ä¹‰åé¦ˆæ¥åŠ¨æ€å¡‘é€ å¥–åŠ±å‡½æ•°ï¼ŒæŒ‡å¯¼æ— äººæœºä½ç½®å’Œç›¸æœºè°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä½³è§†è§‰è¦†ç›–æ•ˆæœã€‚PIRLæ™ºèƒ½ä½“åœ¨OpenAI Gymå’ŒWebotsæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒPIRLåœ¨å¤šç§ç¯å¢ƒä¸­ä¼˜äºå…¶ä»–å­¦ä¹ åŸºçº¿æ–¹æ³•ï¼Œå¦‚é™æ€å¥–åŠ±çš„PPOç­‰ã€‚åœ¨OpenAI Gymä¸­ï¼ŒPIRLçš„è¦†ç›–é¢ç§¯æ¯”æœ€ä½³åŸºçº¿é«˜å‡ºé«˜è¾¾14%ï¼Œåœ¨Webotsä¸­é«˜å‡º27%ï¼Œç”µæ± æ•ˆç‡æé«˜é«˜è¾¾25%ï¼Œä¸”å†—ä½™åº¦é™ä½é«˜è¾¾18%ã€‚è¿™çªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼å¥–åŠ±å¡‘é€ åœ¨å¤æ‚ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæ•´åˆè‡ªç„¶è¯­è¨€å…ˆéªŒä¿¡æ¯è¿›å…¥æœºå™¨äººå¼ºåŒ–å­¦ä¹ æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>UAVè§†è§‰è¦†ç›–è·¯å¾„è§„åˆ’éœ€è¦åè°ƒæ— äººæœºè¿åŠ¨å’Œç›¸æœºæ§åˆ¶ï¼Œä»¥æé«˜è¦†ç›–æ•ˆç‡ã€‚</li>
<li>PIRLæ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ¬¡æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>GPT-3.5è¯­ä¹‰åé¦ˆç”¨äºåŠ¨æ€å¡‘é€ å¥–åŠ±å‡½æ•°ï¼ŒæŒ‡å¯¼æ— äººæœºä½ç½®å’Œç›¸æœºè°ƒæ•´ã€‚</li>
<li>PIRLæ™ºèƒ½ä½“åœ¨OpenAI Gymå’ŒWebotsæ¨¡æ‹Ÿå™¨ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç§ç¯å¢ƒä¸­ï¼ŒPIRLæ˜¾è‘—ä¼˜äºå…¶ä»–å­¦ä¹ åŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç»“æœçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¡‘é€ æ–¹é¢çš„ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c94893fd4c249297199871a8293545eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-345d5838ac5bc9aa61e8f2eeb25a5f95.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cross-Timeslot-Optimization-for-Distributed-GPU-Inference-Using-Reinforcement-Learning"><a href="#Cross-Timeslot-Optimization-for-Distributed-GPU-Inference-Using-Reinforcement-Learning" class="headerlink" title="Cross-Timeslot Optimization for Distributed GPU Inference Using   Reinforcement Learning"></a>Cross-Timeslot Optimization for Distributed GPU Inference Using   Reinforcement Learning</h2><p><strong>Authors:Chengze Du, Zhiwei Yu, Heng Xu, Haojie Wang, Bo liu, Jialong Li</strong></p>
<p>The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems rely on the current system state to make decisions, without considering how task demand and resource availability evolve over time. This lack of temporal awareness leads to inefficient GPU utilization, high task migration overhead, and poor system responsiveness under dynamic workloads. In this work, we identify the fundamental limitations of these instantaneous-state-only scheduling approaches and propose Temporal Optimal Resource scheduling via Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling framework that captures both long-term workload patterns and short-term execution constraints. It adopts a two-layer design: a macro-level scheduler leverages reinforcement learning and optimal transport to coordinate inter-region task distribution, while a micro-level allocator refines task-to-server assignments within each region to reduce latency and switching costs. Experimental results across multiple network topologies show that TORTA reduces average inference response time by up to 15%, improves load balance by approximately 4-5%, and cuts total operational cost by 10-20% compared to state-of-the-art baseline methods. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„å¿«é€Ÿå¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚å¤§å¤šæ•°ç°æœ‰çš„è°ƒåº¦ç³»ç»Ÿä»…ä¾èµ–äºå½“å‰ç³»ç»ŸçŠ¶æ€æ¥åšå‡ºå†³ç­–ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°ä»»åŠ¡éœ€æ±‚å’Œèµ„æºå¯ç”¨æ€§çš„æ—¶é—´æ¼”å˜ã€‚è¿™ç§ç¼ºä¹æ—¶é—´æ„è¯†çš„è°ƒåº¦æ–¹æ³•å¯¼è‡´äº†GPUåˆ©ç”¨ç‡ä½ä¸‹ã€ä»»åŠ¡è¿ç§»å¼€é”€é«˜ä»¥åŠåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹ç³»ç»Ÿå“åº”æ€§å·®çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è¿™äº›ä»…åŸºäºç¬æ—¶çŠ¶æ€çš„è°ƒåº¦æ–¹æ³•çš„æ ¹æœ¬å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é€šè¿‡ä¸¤å±‚æ¶æ„è¿›è¡Œæ—¶é—´æœ€ä¼˜èµ„æºè°ƒåº¦ï¼ˆTORTAï¼‰ã€‚TORTAå¼•å…¥äº†ä¸€ä¸ªæ—¶ç©ºè°ƒåº¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰é•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼å’ŒçŸ­æœŸæ‰§è¡Œçº¦æŸã€‚å®ƒé‡‡ç”¨äº†ä¸¤å±‚è®¾è®¡ï¼šå®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“æ¥åè°ƒåŒºåŸŸé—´çš„ä»»åŠ¡åˆ†å¸ƒï¼Œè€Œå¾®è§‚åˆ†é…å™¨åˆ™åœ¨æ¯ä¸ªåŒºåŸŸå†…å¯¹ä»»åŠ¡åˆ°æœåŠ¡å™¨çš„åˆ†é…è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘å»¶è¿Ÿå’Œåˆ‡æ¢æˆæœ¬ã€‚è·¨å¤šç§ç½‘ç»œæ‹“æ‰‘çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒTORTAå°†å¹³å‡æ¨ç†å“åº”æ—¶é—´å‡å°‘äº†é«˜è¾¾15%ï¼Œè´Ÿè½½å¹³è¡¡æé«˜äº†çº¦4-5%ï¼Œæ€»è¿è¥æˆæœ¬é™ä½äº†10-20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10259v1">PDF</a> 17 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„å¿«é€Ÿå¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚ç°æœ‰çš„è°ƒåº¦ç³»ç»Ÿç¼ºä¹æ—¶é—´æ„è¯†ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€ä»»åŠ¡è¿ç§»å¼€é”€é«˜ä»¥åŠåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„ç³»ç»Ÿå“åº”æ€§å·®ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ—¶ç©ºæœ€ä¼˜èµ„æºè°ƒåº¦ä¸¤å±‚æ¶æ„ï¼ˆTORTAï¼‰ï¼Œè¯¥æ¶æ„ç»“åˆäº†é•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼å’ŒçŸ­æœŸæ‰§è¡Œçº¦æŸã€‚å®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“æ¥åè°ƒåŒºåŸŸé—´çš„ä»»åŠ¡åˆ†é…ï¼Œè€Œå¾®è§‚åˆ†é…å™¨åˆ™ç»†åŒ–åŒºåŸŸå†…ä»»åŠ¡åˆ°æœåŠ¡å™¨çš„åˆ†é…ä»¥å‡å°‘å»¶è¿Ÿå’Œåˆ‡æ¢æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTORTAå¹³å‡æ¨ç†å“åº”æ—¶é—´å‡å°‘15%ï¼Œè´Ÿè½½å¹³è¡¡æé«˜çº¦4-5%ï¼Œæ€»è¿è¥æˆæœ¬é™ä½10-20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæœåŠ¡çš„å¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚</li>
<li>ç°æœ‰è°ƒåº¦ç³»ç»Ÿç¼ºä¹æ—¶é—´æ„è¯†ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€‚</li>
<li>TORTAæ¶æ„æå‡ºä¸€ç§ç»“åˆé•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼å’ŒçŸ­æœŸæ‰§è¡Œçº¦æŸçš„æ—¶ç©ºæœ€ä¼˜èµ„æºè°ƒåº¦æ–¹æ³•ã€‚</li>
<li>TORTAé‡‡ç”¨ä¸¤å±‚è®¾è®¡ï¼Œå®è§‚è°ƒåº¦å™¨å’Œå¾®è§‚åˆ†é…å™¨åˆ†åˆ«è´Ÿè´£åè°ƒä»»åŠ¡åˆ†é…å’Œç»†åŒ–ä»»åŠ¡åˆ°æœåŠ¡å™¨çš„åˆ†é…ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTORTAç›¸æ¯”ç°æœ‰æ–¹æ³•èƒ½æé«˜è´Ÿè½½å¹³è¡¡ã€é™ä½å“åº”æ—¶é—´å’Œè¿è¥æˆæœ¬ã€‚</li>
<li>TORTAçš„å®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“æ¥è¿›è¡ŒåŒºåŸŸé—´çš„ä»»åŠ¡åè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-badf1127da892f9dae689634206fab3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-473ba346d87c20f8d8a66c4d4221dcd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce71dbc000e051270dbd364a348e595a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dbbf371a09516509677d741565b38d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e70be100258a1c81a87dc67f8aa087c7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="T-GRAB-A-Synthetic-Diagnostic-Benchmark-for-Learning-on-Temporal-Graphs"><a href="#T-GRAB-A-Synthetic-Diagnostic-Benchmark-for-Learning-on-Temporal-Graphs" class="headerlink" title="T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs"></a>T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs</h2><p><strong>Authors:Alireza Dizaji, Benedict Aaron Tjandra, Mehrab Hamidi, Shenyang Huang, Guillaume Rabusseau</strong></p>
<p>Dynamic graph learning methods have recently emerged as powerful tools for modelling relational data evolving through time. However, despite extensive benchmarking efforts, it remains unclear whether current Temporal Graph Neural Networks (TGNNs) effectively capture core temporal patterns such as periodicity, cause-and-effect, and long-range dependencies. In this work, we introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set of synthetic tasks designed to systematically probe the capabilities of TGNNs to reason across time. T-GRAB provides controlled, interpretable tasks that isolate key temporal skills: counting&#x2F;memorizing periodic repetitions, inferring delayed causal effects, and capturing long-range dependencies over both spatial and temporal dimensions. We evaluate 11 temporal graph learning methods on these tasks, revealing fundamental shortcomings in their ability to generalize temporal patterns. Our findings offer actionable insights into the limitations of current models, highlight challenges hidden by traditional real-world benchmarks, and motivate the development of architectures with stronger temporal reasoning abilities. The code for T-GRAB can be found at: <a target="_blank" rel="noopener" href="https://github.com/alirezadizaji/T-GRAB">https://github.com/alirezadizaji/T-GRAB</a>. </p>
<blockquote>
<p>åŠ¨æ€å›¾å­¦ä¹ æ–¹æ³•æœ€è¿‘æ¶Œç°ä¸ºå»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„å…³ç³»æ•°æ®çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿›è¡Œäº†å¤§é‡çš„åŸºå‡†æµ‹è¯•ï¼Œä½†ç›®å‰ä»ä¸æ¸…æ¥šå½“å‰çš„æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼ˆTGNNsï¼‰æ˜¯å¦èƒ½æœ‰æ•ˆåœ°æ•æ‰å‘¨æœŸæ€§ã€å› æœå…³ç³»å’Œé•¿æœŸä¾èµ–ç­‰æ ¸å¿ƒæ—¶é—´æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶åºå›¾æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆT-GRABï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„ç»¼åˆçš„åˆæˆä»»åŠ¡ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ£€æµ‹TGNNsçš„è·¨æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚T-GRABæä¾›å¯æ§ã€å¯è§£é‡Šçš„ä»»åŠ¡ï¼Œå­¤ç«‹å‡ºå…³é”®çš„æ—¶é—´æŠ€èƒ½ï¼šè®¡ç®—&#x2F;è®°å¿†å‘¨æœŸæ€§çš„é‡å¤ã€æ¨æ–­å»¶è¿Ÿçš„å› æœå…³ç³»ã€æ•æ‰æ—¶ç©ºçš„é•¿æœŸä¾èµ–ã€‚æˆ‘ä»¬åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¯„ä¼°äº†11ç§æ—¶åºå›¾å­¦ä¹ æ–¹æ³•ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨æ¨å¹¿æ—¶é—´æ¨¡å¼æ–¹é¢çš„åŸºæœ¬ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå½“å‰æ¨¡å‹çš„å±€é™æ€§æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œçªå‡ºäº†ä¼ ç»Ÿç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•æ‰€éšè—çš„æŒ‘æˆ˜ï¼Œå¹¶æ¨åŠ¨äº†å…·æœ‰æ›´å¼ºæ—¶é—´æ¨ç†èƒ½åŠ›çš„æ¶æ„çš„å‘å±•ã€‚T-GRABçš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/alirezadizaji/T-GRAB">https://github.com/alirezadizaji/T-GRAB</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10183v1">PDF</a> Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>åŠ¨æ€å›¾å­¦ä¹ æ–¹æ³•å·²æˆä¸ºå»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„å…³ç³»æ•°æ®çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿›è¡Œäº†å¤§é‡çš„åŸºå‡†æµ‹è¯•ï¼Œä½†ç›®å‰ä»ä¸æ¸…æ¥šå½“å‰çš„æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼ˆTGNNsï¼‰æ˜¯å¦èƒ½æœ‰æ•ˆåœ°æ•æ‰å‘¨æœŸæ€§ã€å› æœæ€§å’Œé•¿èŒƒå›´ä¾èµ–ç­‰æ ¸å¿ƒæ—¶é—´æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶åºå›¾æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆT-GRABï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„ç»¼åˆçš„åˆæˆä»»åŠ¡ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ£€æµ‹TGNNsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚T-GRABæä¾›å¯æ§ã€å¯è§£é‡Šçš„ä»»åŠ¡ï¼Œéš”ç¦»å…³é”®çš„æ—¶é—´æŠ€èƒ½ï¼ŒåŒ…æ‹¬è®¡æ•°&#x2F;è®°å¿†å‘¨æœŸæ€§é‡å¤ã€æ¨æ–­å»¶è¿Ÿçš„å› æœå…³ç³»å’Œæ•æ‰ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šçš„é•¿æœŸä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å¯¹11ç§æ—¶åºå›¾å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†è¿™äº›ä»»åŠ¡çš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨æ¨å¹¿æ—¶é—´æ¨¡å¼æ–¹é¢çš„åŸºæœ¬ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå½“å‰æ¨¡å‹çš„å±€é™æ€§æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œçªå‡ºäº†ä¼ ç»Ÿç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•æ‰€éšè—çš„æŒ‘æˆ˜ï¼Œå¹¶æ¨åŠ¨äº†å…·æœ‰æ›´å¼ºæ—¶é—´æ¨ç†èƒ½åŠ›çš„æ¶æ„çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€å›¾å­¦ä¹ æ–¹æ³•ç”¨äºå»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„å…³ç³»æ•°æ®ã€‚</li>
<li>æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼ˆTGNNsï¼‰åœ¨æ•æ‰æ ¸å¿ƒæ—¶é—´æ¨¡å¼æ–¹é¢ä»å­˜åœ¨ä¸æ˜ç¡®æ€§ã€‚</li>
<li>å¼•å…¥æ—¶åºå›¾æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆT-GRABï¼‰ï¼Œä»¥è¯„ä¼°TGNNsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>T-GRABåŒ…æ‹¬å¯æ§ã€å¯è§£é‡Šçš„ä»»åŠ¡ï¼Œä¸“æ³¨äºéš”ç¦»å…³é”®çš„æ—¶é—´æŠ€èƒ½ã€‚</li>
<li>é€šè¿‡T-GRABå¯¹11ç§æ—¶åºå›¾å­¦ä¹ æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å…¶åœ¨æ¨å¹¿æ—¶é—´æ¨¡å¼æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å½“å‰æ¨¡å‹çš„å±€é™æ€§éœ€è¦æ”¹è¿›ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ—¶é—´æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00647ee20c4bdfcdee06898e7950817b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f4fff53e475bd2de7646e83261f1e66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ace61e047ad9f114612a1357bda48322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4cfb848508ef0bd4dc4f626382416f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea1eb40c0e1e0af44a683bef94d1e24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5457eba61fe0578f5fe774d56b52492b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EAT-QoS-Aware-Edge-Collaborative-AIGC-Task-Scheduling-via-Attention-Guided-Diffusion-Reinforcement-Learning"><a href="#EAT-QoS-Aware-Edge-Collaborative-AIGC-Task-Scheduling-via-Attention-Guided-Diffusion-Reinforcement-Learning" class="headerlink" title="EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via   Attention-Guided Diffusion Reinforcement Learning"></a>EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via   Attention-Guided Diffusion Reinforcement Learning</h2><p><strong>Authors:Zhifei Xu, Zhiqing Tang, Jiong Lou, Zhi Yao, Xuan Xie, Tian Wang, Yinglong Wang, Weijia Jia</strong></p>
<p>The growth of Artificial Intelligence (AI) and large language models has enabled the use of Generative AI (GenAI) in cloud data centers for diverse AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce unavoidable delays and substantial resource overhead, which are unsuitable for users at the network edge with high QoS demands. Deploying AIGC services on edge servers reduces transmission times but often leads to underutilized resources and fails to optimally balance inference latency and quality. To address these issues, this paper introduces a QoS-aware \underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling (EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to various edge servers, formulating it as a gang scheduling problem that balances inference latency and quality while considering server heterogeneity, such as differing model distributions and cold start issues. 2) We propose a reinforcement learning-based EAT algorithm that uses an attention layer to extract load and task queue information from edge servers and employs a diffusion-based policy network for scheduling, efficiently enabling model reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm to divide tasks and distribute them across multiple edge servers for processing. Experimental results based on our system and large-scale simulations show that our EAT algorithm can reduce inference latency by up to 56% compared to baselines. We release our open-source code at <a target="_blank" rel="noopener" href="https://github.com/zzf1955/EAT">https://github.com/zzf1955/EAT</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¢é•¿ï¼Œç”Ÿæˆå¼AIï¼ˆGenAIï¼‰åœ¨äº‘æ•°æ®ä¸­å¿ƒè¢«å¹¿æ³›åº”ç”¨äºå„ç§AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä»»åŠ¡ã€‚åƒStable Diffusionè¿™æ ·çš„æ¨¡å‹ä¼šå¸¦æ¥ä¸å¯é¿å…çš„å»¶è¿Ÿå’Œå¤§é‡èµ„æºå¼€é”€ï¼Œå¯¹äºç½‘ç»œè¾¹ç¼˜æœ‰é«˜è´¨é‡æœåŠ¡ï¼ˆQoSï¼‰éœ€æ±‚çš„ç”¨æˆ·æ¥è¯´å¹¶ä¸é€‚åˆã€‚åœ¨è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²AIGCæœåŠ¡è™½ç„¶å¯ä»¥å‡å°‘ä¼ è¾“æ—¶é—´ï¼Œä½†å¾€å¾€å¯¼è‡´èµ„æºåˆ©ç”¨ä¸è¶³ï¼Œæ— æ³•å¾ˆå¥½åœ°å¹³è¡¡æ¨ç†å»¶è¿Ÿå’Œè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºQoS çš„è¾¹ç¼˜ååŒ AIGC ä»»åŠ¡è°ƒåº¦ï¼ˆEATï¼‰ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼š1ï¼‰æˆ‘ä»¬å¯¹AIGCä»»åŠ¡è¿›è¡Œåˆ†æ®µï¼Œå¹¶å°†è¡¥ä¸è°ƒåº¦åˆ°ä¸åŒçš„è¾¹ç¼˜æœåŠ¡å™¨ï¼Œå°†å…¶åˆ¶å®šä¸ºä¸€ä¸ªé›†ç¾¤è°ƒåº¦é—®é¢˜ï¼Œåœ¨å¹³è¡¡æ¨ç†å»¶è¿Ÿå’Œè´¨é‡çš„åŒæ—¶è€ƒè™‘æœåŠ¡å™¨å¼‚æ„æ€§ï¼Œå¦‚ä¸åŒçš„æ¨¡å‹åˆ†å¸ƒå’Œå†·å¯åŠ¨é—®é¢˜ã€‚2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„EATç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨æ³¨æ„åŠ›å±‚ä»è¾¹ç¼˜æœåŠ¡å™¨æå–è´Ÿè½½å’Œä»»åŠ¡é˜Ÿåˆ—ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç­–ç•¥ç½‘ç»œè¿›è¡Œè°ƒåº¦ï¼Œä»è€Œæœ‰æ•ˆåœ°å®ç°æ¨¡å‹é‡ç”¨ã€‚3ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªAIGCä»»åŠ¡è°ƒåº¦ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨æˆ‘ä»¬çš„EATç®—æ³•æ¥åˆ†å‰²ä»»åŠ¡å¹¶å°†å®ƒä»¬åˆ†å¸ƒåœ¨å¤šä¸ªè¾¹ç¼˜æœåŠ¡å™¨ä¸Šè¿›è¡Œå¤„ç†ã€‚åŸºäºæˆ‘ä»¬çš„ç³»ç»Ÿå’Œå¤§è§„æ¨¡æ¨¡æ‹Ÿçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„EATç®—æ³•å¯ä»¥å°†æ¨ç†å»¶è¿Ÿå‡å°‘é«˜è¾¾56%ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/zzf1955/EAT%E4%B8%8A%E5%BC%95%E5%8F%9B%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E9%9D%BE%E5%BC%BA%E5%BC%8F-%C3%AD%E3%AF%BABBcode%E3%80%82">https://github.com/zzf1955/EATä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„å¼€æºä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10026v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åœ¨äº‘æ•°æ®ä¸­å¿ƒè¢«å¹¿æ³›åº”ç”¨äºå„ç§AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ¨¡å‹å¦‚Stable Diffusionå¸¦æ¥çš„å»¶è¿Ÿå’Œèµ„æºæ¶ˆè€—é—®é¢˜å¯¹äºç½‘ç»œè¾¹ç¼˜çš„é«˜QoS éœ€æ±‚ç”¨æˆ·æ¥è¯´æ˜¯ä¸å¯æ¥å—çš„ã€‚è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²AIGCæœåŠ¡è™½ç„¶å‡å°‘äº†ä¼ è¾“æ—¶é—´ï¼Œä½†å¾€å¾€å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä¸è¶³ï¼Œæ— æ³•å¾ˆå¥½åœ°å¹³è¡¡æ¨ç†å»¶è¿Ÿå’Œè´¨é‡ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘QoS çš„è¾¹ç¼˜ååŒ AIGC ä»»åŠ¡è°ƒåº¦ï¼ˆEATï¼‰ç®—æ³•ã€‚é€šè¿‡åˆ†å‰²AIGCä»»åŠ¡å¹¶è°ƒåº¦åˆ°ä¸åŒçš„è¾¹ç¼˜æœåŠ¡å™¨ï¼Œå¹³è¡¡æ¨ç†å»¶è¿Ÿå’Œè´¨é‡ï¼ŒåŒæ—¶è€ƒè™‘æœåŠ¡å™¨å·®å¼‚æ€§ã€‚ç®—æ³•é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„EATç®—æ³•ï¼Œä½¿ç”¨æ³¨æ„åŠ›å±‚æå–è¾¹ç¼˜æœåŠ¡å™¨çš„è´Ÿè½½å’Œä»»åŠ¡é˜Ÿåˆ—ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£ç­–ç•¥è¿›è¡Œä»»åŠ¡è°ƒåº¦ï¼Œå®ç°æ¨¡å‹å¤ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒEATç®—æ³•å¯é™ä½æ¨ç†å»¶è¿Ÿè¾¾56%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åœ¨äº‘æ•°æ®ä¸­å¿ƒå¹¿æ³›åº”ç”¨äºå¤šç§ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¦‚Stable Diffusionåœ¨ç½‘ç»œè¾¹ç¼˜å¸¦æ¥å»¶è¿Ÿå’Œèµ„æºæ¶ˆè€—é—®é¢˜ã€‚</li>
<li>è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²AIGCæœåŠ¡è™½å‡å°‘ä¼ è¾“æ—¶é—´ä½†èµ„æºåˆ©ç”¨ç‡ä¸è¶³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é¢å‘QoS çš„è¾¹ç¼˜ååŒ AIGC ä»»åŠ¡è°ƒåº¦ï¼ˆEATï¼‰ç®—æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>EATç®—æ³•é€šè¿‡åˆ†å‰²ä»»åŠ¡å¹¶è°ƒåº¦åˆ°ä¸åŒè¾¹ç¼˜æœåŠ¡å™¨æ¥å¹³è¡¡æ¨ç†å»¶è¿Ÿå’Œè´¨é‡ï¼Œè€ƒè™‘æœåŠ¡å™¨å·®å¼‚æ€§ã€‚</li>
<li>EATç®—æ³•é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä»»åŠ¡è°ƒåº¦ï¼Œå®ç°æ¨¡å‹å¤ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ba6beb016e12767f14b66605d90a9fa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6df777abb3158fd29767db13014260ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e72d1b974108491083da466cf860127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f8d441b9cba39a714d234813ba1782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25dbadce6d92667b4e449b6385600caf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning"><a href="#Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning" class="headerlink" title="Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning"></a>Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning</h2><p><strong>Authors:Zijun Chen, Wenbo Hu, Richang Hong</strong></p>
<p>Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning capabilities in both large language models (LLMs) and multimodal large language models (MLLMs). However, its reliability is often undermined by the accumulation of errors in intermediate steps. This paper introduces an novel approach to calibrate the CoT reasoning accuracy by leveraging the modelâ€™s intrinsic veracity encoding. We discover that specific attention head activations reliably reflect the truthfulness of reasoning steps in CoT. Based on this insight, we train a confidence predictor to evaluate the correctness of each reasoning step using these truthfulness-sensitive activations, dynamically selecting the most plausible reasoning path via beam search. Experimental results demonstrate that our method significantly outperforms the state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and commonsense reasoning tasks, exhibiting superior accuracy and reliability in both unimodal and multimodal settings. We further validate the approach on large reasoning models, confirming its applicability to specialized reasoning models. Additionally, we explore the role of the modelâ€™s self-correction ability in CoT reasoning. This work provides a novel reliability improvement path for CoT reasoning with broad application potential. </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­éƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶å¯é æ€§å¸¸å¸¸å—åˆ°ä¸­é—´æ­¥éª¤ä¸­ç´¯ç§¯é”™è¯¯çš„å½±å“ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æ¥æ ¡å‡†CoTæ¨ç†ç²¾åº¦çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ç‰¹å®šçš„æ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½å¤Ÿå¯é åœ°åæ˜ CoTä¸­æ¨ç†æ­¥éª¤çš„çœŸå®æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç½®ä¿¡åº¦é¢„æµ‹å™¨ï¼Œåˆ©ç”¨è¿™äº›çœŸå®æ€§æ•æ„Ÿçš„æ¿€æ´»æ¥è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶é€šè¿‡é›†æŸæœç´¢åŠ¨æ€é€‰æ‹©æœ€åˆç†çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼ˆå¦‚å°‘æ ·æœ¬CoTã€è‡ªæˆ‘ä¸€è‡´æ€§ã€è‡ªæˆ‘è¯„ä¼°å¼•å¯¼é›†æŸæœç´¢ç­‰ï¼‰ï¼Œåœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­éƒ½æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸Šè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¯å®äº†å®ƒåœ¨ä¸“ç”¨æ¨ç†æ¨¡å‹ä¸­çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†æ¨¡å‹ä¸­è‡ªæˆ‘çº æ­£èƒ½åŠ›åœ¨CoTæ¨ç†ä¸­çš„ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºCoTæ¨ç†æä¾›äº†ä¸€ç§æ–°çš„å¯é æ€§æå‡è·¯å¾„ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10007v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æ¥æ ¡å‡†é“¾å¼æ€ç»´ï¼ˆChain of Thoughtï¼ŒCoTï¼‰æ¨ç†å‡†ç¡®æ€§çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ç‰¹å®šæ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½å¯é åæ˜ CoTæ¨ç†æ­¥éª¤çš„çœŸå®æ€§ï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†ä¿¡å¿ƒé¢„æµ‹å™¨æ¥è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æŸæœç´¢åŠ¨æ€é€‰æ‹©æœ€åˆç†çš„æ¨ç†è·¯å¾„ï¼Œåœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€è®¾ç½®ä¸­éƒ½è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†æ¨¡å‹åœ¨CoTæ¨ç†ä¸­çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æ¥æ ¡å‡†CoTæ¨ç†çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç‰¹å®šæ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½å¯é åæ˜ CoTæ¨ç†æ­¥éª¤çš„çœŸå®æ€§ã€‚</li>
<li>è®­ç»ƒäº†ä¿¡å¿ƒé¢„æµ‹å™¨è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æŸæœç´¢åŠ¨æ€é€‰æ‹©æœ€åˆç†çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>åœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€è®¾ç½®ä¸­éƒ½è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1545fd35875226550fb557f65be080a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d946da7c9c2b363089e0e5b9b7141f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-677b408c1a7ca9c66ce9f7a62ad815e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47989b3b81d7934f27fa4f0411cbc085.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Comprehension Without Competence Architectural Limits of LLMs in   Symbolic Computation and Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a0dc7e9fd0eafb931e3d66e084aa4f3.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  M2DAO-Talker Harmonizing Multi-granular Motion Decoupling and   Alternating Optimization for Talking-head Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
