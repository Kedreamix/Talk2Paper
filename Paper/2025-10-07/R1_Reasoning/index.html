<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Self-Anchor Large Language Model Reasoning via Step-by-step Attention   Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-ae2ebde980e7552d108d63e3a41d704f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941177&auth_key=1759941177-0-0-24735fb633f4732c73b466185ff34858&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-07-æ›´æ–°"><a href="#2025-10-07-æ›´æ–°" class="headerlink" title="2025-10-07 æ›´æ–°"></a>2025-10-07 æ›´æ–°</h1><h2 id="Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment"><a href="#Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment" class="headerlink" title="Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment"></a>Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment</h2><p><strong>Authors:Hongxiang Zhang, Yuan Tian, Tianyi Zhang</strong></p>
<p>To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the modelâ€™s attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between &#96;&#96;non-reasoningâ€™â€™ models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining. </p>
<blockquote>
<p>ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•ä¸ºå¾®è°ƒå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§è½»é‡çº§çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œéšç€æ¨ç†é“¾çš„å»¶ä¼¸ï¼Œå…³é”®çš„ä¸­é—´æ­¥éª¤å’ŒåŸå§‹æç¤ºä¼šè¢«ä¸Šä¸‹æ–‡æ‰€æ·¹æ²¡ï¼Œå¾—ä¸åˆ°è¶³å¤Ÿçš„å…³æ³¨ï¼Œä»è€Œå¯¼è‡´é”™è¯¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Self-Anchorï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ¨ç†çš„å†…åœ¨ç»“æ„æ¥å¼•å¯¼LLMæ³¨æ„çš„æ–°å‹ç®¡é“ã€‚Self-Anchorå°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œå¹¶è‡ªåŠ¨å°†æ¨¡å‹æ³¨æ„åŠ›ä¸æœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤å¯¹é½ï¼Œä½¿æ¨¡å‹åœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå…³æ³¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSelf-Anchoråœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºSOTAæç¤ºæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSelf-Anchoræ˜¾è‘—å‡å°‘äº†â€œéæ¨ç†â€æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæœ‰å¯èƒ½ä½¿å¤§å¤šæ•°LLMèƒ½å¤Ÿè§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03223v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•ä¸ºå¾®è°ƒæä¾›äº†è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚ä½†éšç€æ¨ç†é“¾çš„å»¶ä¼¸ï¼Œå…³é”®çš„ä¸­é—´æ­¥éª¤å’ŒåŸå§‹æç¤ºä¼šåœ¨è¯­å¢ƒä¸­è¢«å¿½ç•¥ï¼Œå¯¼è‡´æ³¨æ„åŠ›ä¸è¶³å’Œé”™è¯¯ã€‚æœ¬æ–‡æå‡ºSelf-Anchorï¼Œä¸€ç§åˆ©ç”¨æ¨ç†çš„å†…åœ¨ç»“æ„æ¥å¼•å¯¼LLMæ³¨æ„åŠ›çš„æ–°æ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ†è§£æ¨ç†è½¨è¿¹ä¸ºç»“æ„åŒ–è®¡åˆ’å¹¶è‡ªåŠ¨å¯¹é½æ¨¡å‹æ³¨æ„åŠ›åˆ°æœ€å…³é”®çš„æ¨ç†æ­¥éª¤ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå…³æ³¨é‡ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒSelf-Anchoråœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°æç¤ºæ–¹æ³•ã€‚å°¤å…¶å€¼å¾—å…³æ³¨çš„æ˜¯ï¼ŒSelf-Anchoræ˜¾è‘—ç¼©å°äº†â€œéæ¨ç†â€æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå…·æœ‰ä½¿å¤§å¤šæ•°LLMæ— éœ€é‡æ–°è®­ç»ƒå°±èƒ½å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æç¤ºæ–¹æ³•åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªè½»é‡çº§çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>éšç€æ¨ç†é“¾çš„å»¶é•¿ï¼Œå…³é”®ä¸­é—´æ­¥éª¤å’ŒåŸå§‹æç¤ºå®¹æ˜“è¢«å¿½è§†ï¼Œå¯¼è‡´é”™è¯¯ã€‚</li>
<li>Self-Anchoråˆ©ç”¨æ¨ç†çš„å†…åœ¨ç»“æ„æ¥å¼•å¯¼LLMçš„æ³¨æ„åŠ›ã€‚</li>
<li>Self-Anchoré€šè¿‡åˆ†è§£æ¨ç†è½¨è¿¹ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œä¿æŒæ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³æ³¨é‡ç‚¹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSelf-Anchoråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>Self-Anchorç¼©å°äº†éæ¨ç†æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fcf24557400a1d7e4547b0de3d609453~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941184&auth_key=1759941184-0-0-039e95d8f76738feccc9c40eb3606e3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-09f2edf6b829d103931b0d7c19fa2d2b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-67073c8199e5318b258734fb99e06fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941198&auth_key=1759941198-0-0-7ff016c3cf9bdef82385fe763d6890c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae2ebde980e7552d108d63e3a41d704f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941205&auth_key=1759941205-0-0-3535e8fdb462b98ef650357d64282d91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward"><a href="#Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward" class="headerlink" title="Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward"></a>Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward</h2><p><strong>Authors:Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17%$ average accuracy on five math benchmarks, an improvement of $2.66%$ over prior methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ é€šè¿‡éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤æ‚æ¨ç†ä¸­æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œä½†å…¶å¯æ‰©å±•æ€§ç»å¸¸å—åˆ°è®­ç»ƒç“¶é¢ˆçš„é˜»ç¢ï¼Œå½“ç­–ç•¥ç†µå´©æºƒæ—¶ï¼Œæ€§èƒ½ä¼šè¾¾åˆ°é¥±å’ŒçŠ¶æ€ï¼Œè¿™è¡¨æ˜æ¢ç´¢çš„ä¸§å¤±ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡ä¿æŒé«˜ç­–ç•¥ç†µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ§åˆ¶æœ‰æ„ä¹‰æ¢ç´¢çš„ç²¾ç¡®æœºåˆ¶ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯¹ç†µçš„ä¸é€‰æ‹©æ€§å…³æ³¨å¯èƒ½æ”¾å¤§æ— å…³æ ‡è®°å¹¶ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†RLVRä¸­çš„æ¢ç´¢åŠ¨æ€ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ¢ç´¢æ ‡è®°ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ¨ç†ç«èŠ±â€ï¼‰é€æ¸è¢«æ·˜æ±°ã€‚æˆ‘ä»¬å‘ç°è™½ç„¶è¿™äº›ç«èŠ±åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­éå¸¸ä¸°å¯Œï¼Œä½†ç”±äºè¿‡åº¦æƒ©ç½šï¼Œå®ƒä»¬åœ¨RLVRæœŸé—´è¢«ç³»ç»Ÿåœ°æ·˜æ±°ï¼Œå¯¼è‡´æ¢ç´¢é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰ã€‚å®ƒçš„æ ¸å¿ƒæœºåˆ¶æ˜¯å°†ç­–ç•¥æ­£åˆ™åŒ–æœå‘å¯å‘å¼ä»£ç†åˆ†å¸ƒã€‚è¯¥ä»£ç†æ˜¯é€šè¿‡è¿‡æ»¤æ‰å‡å®šå™ªå£°æ ‡è®°å¹¶é‡æ–°å½’ä¸€åŒ–å‰©ä½™å€™é€‰æ ‡è®°çš„åˆ†å¸ƒæ¥æ„å»ºçš„ã€‚ç»“æœæ˜¯ä¸€ä¸ªå™ªå£°è¾ƒå°‘çš„ä»£ç†ï¼Œå…¶ä¸­æ¨ç†ç«èŠ±çš„å¯èƒ½æ€§å¾—åˆ°æ”¾å¤§ï¼Œç„¶åä½œä¸ºè½¯æ­£åˆ™åŒ–ç›®æ ‡æ¥å±è”½è¿™äº›æœ‰ä»·å€¼çš„æ ‡è®°å…äºé€šè¿‡KLæ•£åº¦æ¶ˆé™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿåœ¨å¤§çº¦1000æ­¥å†…å®ç°ç¨³å®šçš„åœ¨çº¿ç­–ç•¥è®­ç»ƒï¼Œè¿™æ˜¯åŸºçº¿ç†µæ§åˆ¶æ–¹æ³•å´©æºƒçš„é¢†åŸŸã€‚è¿™ç§æŒç»­çš„æ¢ç´¢å¯¼è‡´äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡60.17%ï¼Œæ¯”å…ˆå‰çš„æ–¹æ³•æé«˜äº†2.66%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03222v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ¡†æ¶ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°è®­ç»ƒç“¶é¢ˆçš„é™åˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™ä¸€ç“¶é¢ˆé—®é¢˜æºè‡ªè®­ç»ƒè¿‡ç¨‹ä¸­æœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ¢ç´¢æ€§æ ‡è®°ï¼ˆâ€œæ¨ç†ç«èŠ±â€ï¼‰è¢«é€æ¸æ·˜æ±°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰ï¼Œå®ƒé€šè¿‡æ­£åˆ™åŒ–ç­–ç•¥æ¥æ¨¡æ‹Ÿä¸€ä¸ªå»å™ªåˆ†å¸ƒï¼Œæ”¾å¤§â€œæ¨ç†ç«èŠ±â€çš„æ¦‚ç‡ï¼Œä»è€Œä¿æŠ¤è¿™äº›æœ‰ä»·å€¼çš„æ ‡è®°ä¸è¢«æ¶ˆé™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿåœ¨çº¦1000æ­¥çš„åœ¨çº¿ç­–ç•¥è®­ç»ƒä¸­ä¿æŒç¨³å®šçš„æ¢ç´¢ï¼Œå®ç°äº†åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†60.17%ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†2.66%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†é¢ä¸´è®­ç»ƒç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç»´æŒç­–ç•¥ç†µçš„é«˜ä½æ¥è§£å†³è®­ç»ƒç“¶é¢ˆï¼Œä½†å¯¹æœ‰æ„ä¹‰æ¢ç´¢çš„ç²¾ç¡®æœºåˆ¶äº†è§£ä¸è¶³ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œæœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ¢ç´¢æ€§æ ‡è®°ï¼ˆâ€œæ¨ç†ç«èŠ±â€ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«é€æ¸æ·˜æ±°æ˜¯ç“¶é¢ˆé—®é¢˜çš„å…³é”®ã€‚</li>
<li>æå‡ºä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ¨¡æ‹Ÿå»å™ªåˆ†å¸ƒä¿æŠ¤å¹¶æ”¾å¤§â€œæ¨ç†ç«èŠ±â€ã€‚</li>
<li>Lp-Regåœ¨åœ¨çº¿ç­–ç•¥è®­ç»ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ç¨³å®šæ€§ï¼Œæ˜¾è‘—æé«˜äº†äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>Lp-Regæ–¹æ³•é€šè¿‡KLæ•£åº¦ä¿æŠ¤ä½æ¦‚ç‡æ ‡è®°ä¸è¢«æ¶ˆé™¤ï¼Œå®ç°äº†æ›´æŒä¹…çš„æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2289f2071c6a3d853493a987799c9b0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2298cc1ee3eb4d89a92e5f76a36b629f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85257ac4774e6fae767b379593e2f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a6a6faec6f1c58b0db4bdbfadfe523.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PRISM-Physics-Causal-DAG-Based-Process-Evaluation-for-Physics-Reasoning"><a href="#PRISM-Physics-Causal-DAG-Based-Process-Evaluation-for-Physics-Reasoning" class="headerlink" title="PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning"></a>PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning</h2><p><strong>Authors:Wanjia Zhao, Qinwei Ma, Jingzhe Shi, Shirley Wu, Jiaqi Han, Yijia Xiao, Si-Yuan Chen, Xiao Luo, Ludwig Schmidt, James Zou</strong></p>
<p>Benchmarks for competition-style reasoning have advanced evaluation in mathematics and programming, yet physics remains comparatively explored. Most existing physics benchmarks evaluate only final answers, which fail to capture reasoning processes, while recent stepwise methods rely on heuristic LLM-as-judge scoring or restrictive linear assumptions, limiting reliability and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. We prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human expertsâ€™ scoring. Experiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities. </p>
<blockquote>
<p>ç«èµ›å‹æ¨ç†çš„åŸºå‡†æµ‹è¯•å·²åœ¨æ•°å­¦å’Œç¼–ç¨‹è¯„ä¼°ä¸­å–å¾—è¿›å±•ï¼Œä½†ç‰©ç†å­¦é¢†åŸŸä»ç›¸å¯¹ç¼ºä¹ç ”ç©¶ã€‚ç°æœ‰çš„å¤§å¤šæ•°ç‰©ç†åŸºå‡†æµ‹è¯•åªè¯„ä¼°æœ€ç»ˆç­”æ¡ˆï¼Œæ— æ³•æ•æ‰æ¨ç†è¿‡ç¨‹ï¼Œè€Œæœ€è¿‘çš„é€æ­¥æ³•åˆ™ä¾èµ–äºå¯å‘å¼çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„åˆ†æˆ–é™åˆ¶æ€§çº¿æ€§å‡è®¾ï¼Œè¿™é™åˆ¶äº†å…¶å¯é æ€§å’Œè¯Šæ–­æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†PRISM-Physicsï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤æ‚ç‰©ç†æ¨ç†é—®é¢˜çš„è¿‡ç¨‹çº§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚è§£å†³æ–¹æ¡ˆä»¥å…¬å¼æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰çš„å½¢å¼è¡¨ç¤ºï¼Œæ˜ç¡®ç¼–ç äº†ä¸­é—´æ­¥éª¤ä¹‹é—´çš„å› æœä¾èµ–å…³ç³»ï¼Œä»¥å®ç°ç²¾ç»†ã€å¯è§£é‡Šå’Œç†è®ºä¸Šçš„è¯„åˆ†ã€‚æˆ‘ä»¬è¯æ˜äº†DAGè¡¨ç¤ºå’Œç›¸åº”è¯„åˆ†ç­–ç•¥çš„æœ€ä¼˜æ€§ã€‚ç»“åˆæˆ‘ä»¬å¼€å‘çš„åŸºäºå®Œå…¨è§„åˆ™çš„æ–¹æ³•ï¼Œç”¨äºç¬¦å·å…¬å¼ç­‰ä»·åŒ¹é…ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å…¬å¼è¡¨ç¤ºä¸­ç¡®ä¿ä¸€è‡´çš„éªŒè¯ï¼Œè€Œæ— éœ€å¯å‘å¼åˆ¤æ–­ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶ä¸äººç±»ä¸“å®¶çš„è¯„åˆ†æ›´åŠ ä¸€è‡´ã€‚åœ¨æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç‰©ç†å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒç»­çš„å¤±è´¥ï¼Œè€Œæ­¥éª¤çº§è¯„åˆ†æ—¢æä¾›äº†è¯Šæ–­æ´å¯ŸåŠ›ï¼Œä¹Ÿä¸ºåç»­è®­ç»ƒæä¾›äº†ä¸°å¯Œçš„ä¿¡å·ã€‚é€šè¿‡ç»“åˆç»“æ„æ€§ä¸¥è°¨ã€ç†è®ºä¿è¯å’Œç¬¦å·éªŒè¯ï¼ŒPRISM-Physicsä¸ºæ¨è¿›è¿‡ç¨‹çº§è¯„ä¼°å’Œå¼•å¯¼æ¨¡å‹å‘æ›´æ·±çš„ç§‘å­¦æ¨ç†èƒ½åŠ›å‘å±•æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03185v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç‰©ç†å­¦é¢†åŸŸçš„è¿‡ç¨‹çº§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•å¯¹äºæ¨åŠ¨å¤æ‚ç‰©ç†æ¨ç†é—®é¢˜çš„è¯„ä»·è‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç‰©ç†åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹ã€‚PRISM-Physicsä½œä¸ºæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨å…¬å¼æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è¡¨ç¤ºè§£å†³æ–¹æ¡ˆï¼Œæ˜ç¡®ç¼–ç ä¸­é—´æ­¥éª¤ä¹‹é—´çš„å› æœä¾èµ–å…³ç³»ï¼Œå®ç°äº†ç²¾ç»†ã€å¯è§£é‡Šå’Œç†è®ºä¸Šçš„è¯„åˆ†ã€‚è¯¥æ¡†æ¶ä¸ä¸“å®¶è¯„åˆ†æ›´ä¸€è‡´ï¼Œèƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†å­¦é¢†åŸŸçš„è¿‡ç¨‹çº§è¯„ä¼°å°šå¾…æ¢ç´¢ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>PRISM-Physicså¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤æ‚ç‰©ç†æ¨ç†é—®é¢˜ã€‚</li>
<li>è§£å†³æ–¹æ¡ˆé‡‡ç”¨å…¬å¼æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è¡¨ç¤ºï¼Œæ˜ç¡®ç¼–ç ä¸­é—´æ­¥éª¤çš„å› æœä¾èµ–å…³ç³»ã€‚</li>
<li>PRISM-Physicså®ç°äº†ç²¾ç»†ã€å¯è§£é‡Šå’Œç†è®ºä¸Šçš„è¯„åˆ†ï¼Œä¸ä¸“å®¶è¯„åˆ†æ›´ä¸€è‡´ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰©ç†æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æŒç»­å¤±è´¥çš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d622b0d509f24e940d40f32fa6788ef.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d91a11045b9d731f135d96be6f47f81~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941252&auth_key=1759941252-0-0-3ce21820cbb5055ab5c3ddb2cfdfdd60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-92f751ce60ccac5e649b30e97b56d35a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus"><a href="#SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus" class="headerlink" title="SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the   SpineMed-450k Corpus"></a>SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the   SpineMed-450k Corpus</h2><p><strong>Authors:Ming Zhao, Wenhui Dong, Yang Zhang, Xiang Zheng, Zhonghao Zhang, Zian Zhou, Yunzhi Guan, Liukun Xu, Wei Peng, Zhaoyang Gong, Zhicheng Zhang, Dachuan Li, Xiaosheng Ma, Yuli Ma, Jianing Ni, Changjiang Jiang, Lixia Tian, Qixin Chen, Kaishun Xia, Pingping Liu, Tongshun Zhang, Zhiqiang Liu, Zhongan Bi, Chenyang Si, Tiansheng Sun, Caifeng Shan</strong></p>
<p>Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our modelâ€™s outputs. </p>
<blockquote>
<p>è„Šæ¤ç–¾ç—…å½±å“å…¨çƒ6.19äº¿äººï¼Œæ˜¯å¯¼è‡´æ®‹ç–¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ„ŸçŸ¥çº§åˆ«çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ä»ç„¶å—åˆ°é™åˆ¶ã€‚è„Šæ¤ç–¾ç—…çš„ä¸´åºŠå†³ç­–éœ€è¦åœ¨ç‰¹å®šçš„æ¤ä½“å±‚é¢è¿›è¡Œå¤æ‚çš„Xå…‰ã€CTå’ŒMRIæ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯è¿½æº¯çš„ã€ä»¥ä¸´åºŠä¸ºåŸºç¡€çš„æŒ‡ä»¤æ•°æ®ä»¥åŠæ ‡å‡†åŒ–çš„è„Šæ¤ç‰¹å®šåŸºå‡†æµ‹è¯•ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpineMedï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ‰§ä¸šè„Šæ¤å¤–ç§‘åŒ»ç”Ÿå…±åŒè®¾è®¡çš„ç”Ÿæ€ç³»ç»Ÿã€‚å®ƒåŒ…æ‹¬SpineMed-45ä¸‡ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºæ¤ä½“çº§åˆ«çš„è·¨æˆåƒæ¨¡æ€æ¨ç†è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ä¸‡ä¸ªæŒ‡ä»¤å®ä¾‹ï¼Œä»¥åŠSpineBenchï¼Œä¸€ä¸ªä»¥ä¸´åºŠä¸ºåŸºç¡€çš„è¯„ä»·æ¡†æ¶ã€‚SpineMed-45ä¸‡æ˜¯ä»å¤šç§æ¥æºç²¾å¿ƒæŒ‘é€‰å’Œæ•´ç†çš„ï¼ŒåŒ…æ‹¬æ•™ç§‘ä¹¦ã€æŒ‡å—ã€å…¬å¼€æ•°æ®é›†ä»¥åŠçº¦1000ä¸ªåŒ¿ååŒ»é™¢ç—…ä¾‹ï¼Œä½¿ç”¨é—­ç¯ç®¡é“çš„ä¸¤é˜¶æ®µå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼ˆåˆç¨¿å’Œä¿®è®¢ç¨¿ï¼‰ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ã€å¯è¿½æº¯çš„æ•°æ®å¯ç”¨äºé—®ç­”ã€å¤šè½®å’¨è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆã€‚SpineBenchå¯¹æ¨¡å‹çš„ä¸´åºŠæ˜¾è‘—è½´è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬æ°´å¹³è¯†åˆ«ã€ç—…ç†è¯„ä¼°å’Œæ‰‹æœ¯è§„åˆ’ã€‚æˆ‘ä»¬å¯¹å‡ ä¸ªæœ€è¿‘å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨SpineBenchä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç²¾ç»†ç²’åº¦çš„æ°´å¹³ç‰¹å®šæ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨SpineMed-45ä¸‡ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºä¸€è‡´ä¸”æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸´åºŠåŒ»ç”Ÿçš„è¯„ä¼°è¯å®äº†æˆ‘ä»¬çš„æ¨¡å‹è¾“å‡ºçš„è¯Šæ–­æ¸…æ™°åº¦å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03160v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­è„Šæ¤ç–¾ç—…å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹æ„ŸçŸ¥çº§åˆ«çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ¨å‡ºSpineMedç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«é¦–æ¬¾ä¸“ä¸ºæ¤ä½“çº§åˆ«æ¨ç†è®¾è®¡çš„SpineMed-450kæ•°æ®é›†å’Œä¸´åºŠåŸºç¡€è¯„ä¼°æ¡†æ¶SpineBenchã€‚SpineMed-450kæ•°æ®é›†ç”±å¤šç§æ¥æºç»„æˆï¼ŒåŒ…æ‹¬æ•™ç§‘ä¹¦ã€æŒ‡å—ã€å¼€æ”¾æ•°æ®é›†å’Œçº¦ä¸€åƒå®¶åŒ¿ååŒ»é™¢ç—…ä¾‹ï¼Œä½¿ç”¨ä¸´åºŠåŒ»ç”Ÿå‚ä¸çš„ä¸¤é˜¶æ®µLLMç”Ÿæˆæ–¹æ³•ç¡®ä¿é«˜è´¨é‡ã€å¯è¿½æº¯çš„æ•°æ®ç”¨äºé—®ç­”ã€å¤šè½®å’¨è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆã€‚åœ¨ä¸´åºŠåŒ»å­¦çš„è½´ä¸Šè¯„ä¼°æ¨¡å‹æ—¶ï¼Œå‘ç°å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨ç²¾ç»†åŒ–ã€ç‰¹å®šçº§åˆ«çš„æ¨ç†å¼±ç‚¹ã€‚ç›¸åï¼Œåœ¨SpineMed-450kä¸Šè°ƒä¼˜çš„æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶å¾—åˆ°ä¸´åºŠåŒ»ç”Ÿçš„ç¡®è®¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„Šæ¤ç–¾ç—…å½±å“å…¨çƒ6.19äº¿äººå¹¶å¯¼è‡´æ®‹ç–¾ï¼Œä½†äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç¼ºä¹æ„ŸçŸ¥çº§åˆ«ã€å¤šæ¨¡æ€æ•°æ®é›†é™åˆ¶äº†AIåœ¨è„Šæ¤ç–¾ç—…è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚</li>
<li>SpineMedç”Ÿæ€ç³»ç»ŸåŒ…æ‹¬ä¸“ä¸ºæ¤ä½“çº§åˆ«æ¨ç†è®¾è®¡çš„SpineMed-450kæ•°æ®é›†å’Œä¸´åºŠåŸºç¡€è¯„ä¼°æ¡†æ¶SpineBenchã€‚</li>
<li>SpineMed-450kæ•°æ®é›†ç»“åˆäº†å¤šç§æ¥æºçš„æ•°æ®ï¼Œå¹¶é‡‡ç”¨äº†ä¸´åºŠå‚ä¸çš„æ•°æ®ç”Ÿæˆæµç¨‹æ¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠåŒ»å­¦è¯„ä¼°ä¸­å­˜åœ¨å¼±ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†åŒ–å’Œç‰¹å®šçº§åˆ«çš„æ¨ç†æ–¹é¢ã€‚</li>
<li>åœ¨SpineMed-450kä¸Šè°ƒä¼˜çš„æ¨¡å‹åœ¨ä¸´åºŠåŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-25573149b6424227361c1112676ba1aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941268&auth_key=1759941268-0-0-8bda0ae8f4c3d257574f987dc1fdd522&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7230dd6ad766a06547c8a0502b0f1e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941275&auth_key=1759941275-0-0-086e232185d21c62d5e162abb80169f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-4117ab9e87b9107d07702fbe61696b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93fd4768e85510523a5989600cf6c169.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8a40ffa6e15fb8b1068c38b04c7f5b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941295&auth_key=1759941295-0-0-217a1116610b6529b99181afc4017e18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c6f6812c8981f4f4132664c7e891d8f6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning"><a href="#MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning" class="headerlink" title="MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning"></a>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning</h2><p><strong>Authors:Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang</strong></p>
<p>Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method. </p>
<blockquote>
<p>è§†è§‰å¯¼èˆªç­–ç•¥è¢«å¹¿æ³›è®¤ä¸ºæ˜¯ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ï¼Œå› ä¸ºå®ƒé€šè¿‡é‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰è§‚å¯Ÿè¿›è¡Œå¯¼èˆªï¼Œä»è€Œæ¨¡ä»¿äººç±»çš„è¡Œä¸ºã€‚ç„¶è€Œï¼Œè§†è§‰è§‚å¯Ÿçš„å…‰å­¦ä¿¡æ¯éš¾ä»¥åƒæ¿€å…‰é›·è¾¾ç‚¹äº‘æˆ–æ·±åº¦å›¾é‚£æ ·è¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œè¿™éœ€è¦ä½¿ç”¨æ™ºèƒ½æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ™ºèƒ½ï¼Œä»¥å¸ˆå¾’æ–¹å¼ä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å„ç§å¯¼èˆªèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å®ç°äº†VLAæ¨¡å‹MM-Navï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„å¤šè§†è§’VLAï¼ˆå…·æœ‰360åº¦è§‚å¯Ÿï¼‰ã€‚å¯¹äºå¤§è§„æ¨¡å¯¼èˆªæ•°æ®ï¼Œæˆ‘ä»¬ä»ä¸‰ä½å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸“å®¶æ”¶é›†ä¸“å®¶æ•°æ®ï¼Œè¿™äº›ä¸“å®¶åœ¨å…·æœ‰ä¸åŒå¯¼èˆªèƒ½åŠ›çš„ä¸‰ä¸ªæŒ‘æˆ˜æ€§å®šåˆ¶ç¯å¢ƒä¸­æ¥å—è®­ç»ƒï¼ŒåŒ…æ‹¬åˆ°è¾¾ã€æŒ¤å‹å’Œé¿å…ã€‚æˆ‘ä»¬è¿­ä»£åœ°ä½¿ç”¨æ¥è‡ªRLä¸“å®¶çš„åœ¨çº¿æ”¶é›†æ•°æ®è®­ç»ƒæˆ‘ä»¬çš„VLAæ¨¡å‹ï¼Œå…¶ä¸­è®­ç»ƒæ¯”ä¾‹æ˜¯æ ¹æ®ä¸ªäººèƒ½åŠ›çš„è¡¨ç°åŠ¨æ€å¹³è¡¡çš„ã€‚åœ¨åˆæˆç¯å¢ƒä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å­¦ç”ŸVLAæ¨¡å‹è¡¨ç°è¶…è¿‡äº†RLæ•™å¸ˆï¼Œè¿™è¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒä½œç”¨ã€‚ç°å®ä¸–ç•Œçš„å¤§é‡å®éªŒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03142v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://pku-epic.github.io/MM-Nav-Web/">https://pku-epic.github.io/MM-Nav-Web/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰å¯¼èˆªç­–ç•¥ä½¿ç”¨ä¸ªäººè§†è§‰è§‚å¯Ÿæ¨¡ä»¿äººç±»å¯¼èˆªï¼Œè§†è§‰è§‚å¯Ÿå…‰å­¦ä¿¡æ¯éš¾ä»¥æ˜¾æ€§å»ºæ¨¡ã€‚æœ¬æ–‡æè®®é‡‡ç”¨Vision-Language-Actionæ¨¡å‹ä»¥ä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å¤šæ ·çš„å¯¼èˆªèƒ½åŠ›ã€‚è®¾è®¡å¤šè§†è§’VLAæ¨¡å‹MM-Navï¼ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹å®ç°ã€‚æ”¶é›†å¼ºåŒ–å­¦ä¹ ä¸“å®¶çš„å¯¼èˆªæ•°æ®ï¼Œä¸ºä¸åŒå¯¼èˆªèƒ½åŠ›å®šåˆ¶ä¸‰ç§æŒ‘æˆ˜ç¯å¢ƒï¼šåˆ°è¾¾ã€å‹ç¼©å’Œé¿å…ã€‚é€šè¿‡å¤§é‡åˆæˆç¯å¢ƒå®éªŒéªŒè¯æ¨¡å‹å¯¹å¤šç§èƒ½åŠ›çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œå­¦ç”ŸVLAæ¨¡å‹è¡¨ç°ä¼˜äºRLæ•™å¸ˆï¼Œè¯æ˜æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒä½œç”¨æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¯¼èˆªç­–ç•¥æ¨¡æ‹Ÿäººç±»é€šè¿‡ä¸ªäººè§†è§‰è§‚å¯Ÿè¿›è¡Œå¯¼èˆªï¼Œå…‰å­¦ä¿¡æ¯çš„å»ºæ¨¡æ˜¯è¿™ä¸€é¢†åŸŸçš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨Vision-Language-Actionæ¨¡å‹å­¦ä¹ å¤šæ ·åŒ–çš„å¯¼èˆªèƒ½åŠ›ï¼Œèƒ½å¤Ÿä»åˆæˆä¸“å®¶æ•°æ®ä¸­å—ç›Šã€‚</li>
<li>æå‡ºçš„MM-Navæ¨¡å‹æ˜¯ä¸€ç§å¤šè§†è§’VLAæ¨¡å‹ï¼Œèåˆäº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸“å®¶æ•°æ®çš„æ”¶é›†æ¶µç›–äº†å¤šç§å¯¼èˆªèƒ½åŠ›ï¼Œå¦‚åˆ°è¾¾ã€å‹ç¼©å’Œé¿å…ç­‰ã€‚</li>
<li>æ¨¡å‹é€šè¿‡åˆæˆç¯å¢ƒå®éªŒéªŒè¯äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒå¯¼èˆªèƒ½åŠ›ä¹‹é—´å¹³è¡¡è®­ç»ƒã€‚</li>
<li>å­¦ç”ŸVLAæ¨¡å‹çš„è¡¨ç°è¶…è¶Šäº†RLæ•™å¸ˆï¼Œè¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒä½œç”¨çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96cd04118467124c38a9a5b7da6d6ff7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b9a3fee8905f84ecc88ece04e69effd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941317&auth_key=1759941317-0-0-fad25c822e1aa5e77ff9811589716c58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-196afbdc75a5fb50763f34351da8d5ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941323&auth_key=1759941323-0-0-390d0d08df1c3fc039a2127b6d8d4bd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a911de169e5d64cc5b31e0556734134c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941330&auth_key=1759941330-0-0-2f2e43e69436f29818a9e51fbe5a6803&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bc59b9dfb0f76dc6d772656d7646701~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941336&auth_key=1759941336-0-0-b54da649ab7248d7a18b27666eb07d6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Unified-Deep-Reinforcement-Learning-Approach-for-Close-Enough-Traveling-Salesman-Problem"><a href="#A-Unified-Deep-Reinforcement-Learning-Approach-for-Close-Enough-Traveling-Salesman-Problem" class="headerlink" title="A Unified Deep Reinforcement Learning Approach for Close Enough   Traveling Salesman Problem"></a>A Unified Deep Reinforcement Learning Approach for Close Enough   Traveling Salesman Problem</h2><p><strong>Authors:Mingfeng Fan, Jiaqi Cheng, Yaoxin Wu, Yifeng Zhang, Yibin Yang, Guohua Wu, Guillaume Sartoretti</strong></p>
<p>In recent years, deep reinforcement learning (DRL) has gained traction for solving the NP-hard traveling salesman problem (TSP). However, limited attention has been given to the close-enough TSP (CETSP), primarily due to the challenge introduced by its neighborhood-based visitation criterion, wherein a node is considered visited if the agent enters a compact neighborhood around it. In this work, we formulate a Markov decision process (MDP) for CETSP using a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination. Specifically, an adapted encoder is employed for effective feature extraction, followed by a node-decoder and a loc-decoder to handle the two sub-tasks, respectively. A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a unified model capable of generalizing across different problem sizes and varying neighborhood radius types (i.e., constant and random radii). Experimental results show that UD3RL outperforms conventional methods in both solution quality and runtime, while exhibiting strong generalization across problem scales, spatial distributions, and radius ranges, as well as robustness to dynamic environments. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åœ¨è§£å†³NPéš¾çš„æ—…è¡Œæ¨é”€å‘˜é—®é¢˜ï¼ˆTSPï¼‰ä¸Šå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºåŸºäºé‚»åŸŸè®¿é—®å‡†åˆ™æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¯¹è¿‘ä¼¼æ—…è¡Œæ¨é”€å‘˜é—®é¢˜ï¼ˆCETSPï¼‰çš„å…³æ³¨åº¦æœ‰é™ã€‚åœ¨è¯¥é—®é¢˜ä¸­ï¼Œå¦‚æœä¸€ä¸ªä»£ç†è¿›å…¥èŠ‚ç‚¹çš„ç´§å‡‘é‚»åŸŸï¼Œåˆ™è¯¥èŠ‚ç‚¹è¢«è§†ä¸ºå·²è®¿é—®ã€‚åœ¨æœ¬å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¦»æ•£åŒ–æ–¹æ¡ˆä¸ºCETSPåˆ¶å®šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€åŒé‡è§£ç DRLï¼ˆUD3RLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å†³ç­–åˆ¶å®šåˆ†ä¸ºèŠ‚ç‚¹é€‰æ‹©å’Œèˆªç‚¹ç¡®å®šã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨é€‚åº”æ€§ç¼–ç å™¨è¿›è¡Œæœ‰æ•ˆç‰¹å¾æå–ï¼Œç„¶ååˆ†åˆ«ç”±èŠ‚ç‚¹è§£ç å™¨å’Œä½ç½®è§£ç å™¨å¤„ç†è¿™ä¸¤ä¸ªå­ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†kæœ€è¿‘é‚»å­å›¾äº¤äº’ç­–ç•¥ï¼Œä»¥å¢å¼ºä½ç½®è§£ç è¿‡ç¨‹ä¸­çš„ç©ºé—´æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®šåˆ¶äº†REINFORCEç®—æ³•æ¥è®­ç»ƒUD3RLï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„é—®é¢˜è§„æ¨¡å’Œå„ç§é‚»åŸŸåŠå¾„ç±»å‹ï¼ˆå³æ’å®šåŠå¾„å’ŒéšæœºåŠå¾„ï¼‰ä¸Šè¿›è¡Œæ¨å¹¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUD3RLåœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œè¿è¡Œæ—¶é—´æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒæ—¶åœ¨é—®é¢˜è§„æ¨¡ã€ç©ºé—´åˆ†å¸ƒå’ŒåŠå¾„èŒƒå›´æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”å¯¹åŠ¨æ€ç¯å¢ƒå…·æœ‰é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03065v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è¢«ç”¨äºè§£å†³NPéš¾çš„æ—…è¡Œæ¨é”€å‘˜é—®é¢˜ï¼ˆTSPï¼‰ã€‚ä½†ç›¸å¯¹è¾ƒå°‘çš„å…³æ³¨è¢«ç»™äºˆæ¥è¿‘å¼TSPï¼ˆCETSPï¼‰ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶åŸºäºé‚»è¿‘çš„è®¿é—®æ ‡å‡†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é‡‡ç”¨ç¦»æ•£åŒ–æ–¹æ¡ˆä¸ºCETSPåˆ¶å®šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶æå‡ºæ–°å‹ç»Ÿä¸€åŒè§£ç å™¨DRLï¼ˆUD3RLï¼‰æ¡†æ¶ï¼Œå°†å†³ç­–åˆ¶å®šåˆ†ä¸ºèŠ‚ç‚¹é€‰æ‹©å’Œèˆªç‚¹ç¡®å®šä¸¤éƒ¨åˆ†ã€‚é€šè¿‡è‡ªé€‚åº”ç¼–ç å™¨è¿›è¡Œæœ‰æ•ˆç‰¹å¾æå–ï¼Œéšåç”±èŠ‚ç‚¹è§£ç å™¨å’Œä½ç½®è§£ç å™¨åˆ†åˆ«å¤„ç†è¿™ä¸¤ä¸ªå­ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥kè¿‘é‚»å­å›¾äº¤äº’ç­–ç•¥ä»¥å¢å¼ºä½ç½®è§£ç è¿‡ç¨‹ä¸­çš„ç©ºé—´æ¨ç†ã€‚å®šåˆ¶REINFORCEç®—æ³•è®­ç»ƒUD3RLï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¸åŒé—®é¢˜è§„æ¨¡å’Œå„ç§åŠå¾„ç±»å‹ï¼ˆå¦‚æ’å®šå’ŒéšæœºåŠå¾„ï¼‰ä¸Šé€šç”¨çš„æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒUD3RLåœ¨è§£å†³æ–¹æ¡ˆè´¨é‡å’Œè¿è¡Œæ—¶é—´æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨é—®é¢˜è§„æ¨¡ã€ç©ºé—´åˆ†å¸ƒå’ŒåŠå¾„èŒƒå›´ä»¥åŠåŠ¨æ€ç¯å¢ƒçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DRLè¢«ç”¨äºè§£å†³TSPé—®é¢˜ï¼Œå…¶ä¸­æ¶‰åŠæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>CETSPé—®é¢˜ç”±äºå…¶åŸºäºé‚»è¿‘çš„è®¿é—®æ ‡å‡†è€Œå—åˆ°è¾ƒå°‘çš„å…³æ³¨ã€‚</li>
<li>æœ¬æ–‡ä¸ºCETSPåˆ¶å®šäº†MDPï¼Œå¹¶æå‡ºUD3RLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»å†³ç­–è¿‡ç¨‹æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>UD3RLä½¿ç”¨è‡ªé€‚åº”ç¼–ç å™¨è¿›è¡Œæœ‰æ•ˆç‰¹å¾æå–ï¼Œå¹¶å¼•å…¥èŠ‚ç‚¹è§£ç å™¨å’Œä½ç½®è§£ç å™¨å¤„ç†ä¸åŒå­ä»»åŠ¡ã€‚</li>
<li>kè¿‘é‚»å­å›¾äº¤äº’ç­–ç•¥ç”¨äºå¢å¼ºç©ºé—´æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®šåˆ¶REINFORCEç®—æ³•è®­ç»ƒUD3RLï¼Œä½¿å…¶å…·æœ‰åœ¨ä¸åŒé—®é¢˜è§„æ¨¡å’ŒåŠå¾„ç±»å‹ä¸Šçš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fd9487bfbe859f596442eba3616c1351~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941344&auth_key=1759941344-0-0-f4ebf5e9d13447a68d9407b5964d8943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d3ac4d0a2d16ae7e57327d47f612c66~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941351&auth_key=1759941351-0-0-d7e27174f2eacc638b6edcf9d8c94763&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04840d9b65592c70482845f9296e809d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941358&auth_key=1759941358-0-0-a731a86df6a7f2c78ce09de2c90aba16&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FR-LUX-Friction-Aware-Regime-Conditioned-Policy-Optimization-for-Implementable-Portfolio-Management"><a href="#FR-LUX-Friction-Aware-Regime-Conditioned-Policy-Optimization-for-Implementable-Portfolio-Management" class="headerlink" title="FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for   Implementable Portfolio Management"></a>FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for   Implementable Portfolio Management</h2><p><strong>Authors:Jianâ€™an Zhang</strong></p>
<p>Transaction costs and regime shifts are major reasons why paper portfolios fail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned Learning under eXecution costs), a reinforcement learning framework that learns after-cost trading policies and remains robust across volatility-liquidity regimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent execution model combining proportional and impact costs, directly embedded in the reward; (ii) a trade-space trust region that constrains changes in inventory flow rather than logits, yielding stable low-turnover updates; and (iii) explicit regime conditioning so the policy specializes to LL&#x2F;LH&#x2F;HL&#x2F;HH states without fragmenting the data. On a 4 x 5 grid of regimes and cost levels with multiple random seeds, FR-LUX achieves the top average Sharpe ratio with narrow bootstrap confidence intervals, maintains a flatter cost-performance slope than strong baselines, and attains superior risk-return efficiency for a given turnover budget. Pairwise scenario-level improvements are strictly positive and remain statistically significant after multiple-testing corrections. We provide formal guarantees on optimality under convex frictions, monotonic improvement under a KL trust region, long-run turnover bounds and induced inaction bands due to proportional costs, positive value advantage for regime-conditioned policies, and robustness to cost misspecification. The methodology is implementable: costs are calibrated from standard liquidity proxies, scenario-level inference avoids pseudo-replication, and all figures and tables are reproducible from released artifacts. </p>
<blockquote>
<p>äº¤æ˜“æˆæœ¬ä¸åˆ¶åº¦è½¬æ¢æ˜¯çº¸ä¸ŠæŠ•èµ„ç»„åˆåœ¨å®æ—¶äº¤æ˜“ä¸­å¤±è´¥çš„ä¸»è¦åŸå› ã€‚æˆ‘ä»¬å¼•å…¥äº†FR-LUXï¼ˆæ‰§è¡Œæˆæœ¬æ„è¯†ä¸‹çš„æ‘©æ“¦æ„ŸçŸ¥åˆ¶åº¦åŒ–å­¦ä¹ ï¼Œè‹±æ–‡å…¨ç§°ä¸ºFriction-aware, Regime-conditioned Learning under eXecution costsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨äº¤æ˜“è¿‡ç¨‹ä¸­å­¦ä¹ æ‰£é™¤æˆæœ¬åçš„ç­–ç•¥ï¼Œå¹¶åœ¨æ³¢åŠ¨ç‡-æµåŠ¨æ€§åˆ¶åº¦ä¸­ä¿æŒç¨³å¥ã€‚FR-LUXé›†æˆäº†ä¸‰ä¸ªè¦ç´ ï¼šï¼ˆiï¼‰ç»“åˆæ¯”ä¾‹å’Œå†²å‡»æˆæœ¬çš„å¾®è§‚ç»“æ„ä¸€è‡´æ‰§è¡Œæ¨¡å‹ï¼Œç›´æ¥åµŒå…¥å¥–åŠ±ä¸­ï¼›ï¼ˆiiï¼‰äº¤æ˜“ç©ºé—´ä¿¡ä»»åŒºåŸŸçº¦æŸåº“å­˜æµé‡å˜åŒ–è€Œéé€»è¾‘ï¼Œäº§ç”Ÿç¨³å®šä¸”ä½å‘¨è½¬ç‡çš„æ›´æ–°ï¼›ï¼ˆiiiï¼‰æ˜ç¡®çš„åˆ¶åº¦æ¡ä»¶ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿé€‚åº”LL&#x2F;LH&#x2F;HL&#x2F;HHçŠ¶æ€ï¼Œè€Œä¸ä¼šç ´åæ•°æ®å®Œæ•´æ€§ã€‚åœ¨åŒ…å«å¤šä¸ªéšæœºç§å­çš„4x5åˆ¶åº¦ä¸æˆæœ¬æ°´å¹³ç½‘æ ¼ä¸Šï¼ŒFR-LUXå®ç°äº†æœ€é«˜çš„å¹³å‡å¤æ™®æ¯”ç‡ï¼Œå…·æœ‰ç‹­çª„çš„è‡ªåŠ©æ³•ç½®ä¿¡åŒºé—´ï¼Œåœ¨ä¿æŒå¹³å¦çš„æˆæœ¬æ€§èƒ½æ–œç‡æ–¹é¢ç›¸å¯¹äºå¼ºå¤§çš„åŸºå‡†çº¿å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨ç»™å®šçš„å‘¨è½¬ç‡é¢„ç®—ä¸‹å®ç°äº†å“è¶Šçš„é£é™©å›æŠ¥æ•ˆç‡ã€‚æˆå¯¹åœºæ™¯çº§åˆ«çš„æ”¹è¿›ä¸¥æ ¼ä¸ºæ­£ï¼Œå¹¶ä¸”åœ¨å¤šæ¬¡æµ‹è¯•æ ¡æ­£åä»ç„¶å…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ€§ã€‚åœ¨å‡¸æ‘©æ“¦æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬æä¾›äº†æœ€ä¼˜æ€§çš„æ­£å¼ä¿è¯ï¼Œåœ¨KLä¿¡ä»»åŒºåŸŸä¸‹å•è°ƒæ”¹è¿›ï¼Œé•¿æœŸå‘¨è½¬ç‡çš„ç•Œé™ä»¥åŠç”±äºæ¯”ä¾‹æˆæœ¬å¼•å‘çš„ä¸è¡ŒåŠ¨åŒºé—´ï¼Œåˆ¶åº¦æ¡ä»¶ä¸‹æ”¿ç­–çš„æ­£ä»·å€¼ä¼˜åŠ¿ä»¥åŠå¯¹æˆæœ¬è¯¯æŒ‡å®šçš„ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•å…·æœ‰å¯è¡Œæ€§ï¼šæˆæœ¬æ˜¯æ ¹æ®æ ‡å‡†æµåŠ¨æ€§ä»£ç†è¿›è¡Œæ ¡å‡†çš„ï¼Œåœºæ™¯çº§æ¨ç†é¿å…äº†ä¼ªå¤åˆ¶ï¼Œæ‰€æœ‰å›¾è¡¨å‡å¯ä»å‘å¸ƒçš„åˆ¶å“ä¸­å¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02986v1">PDF</a> 19 pages, 7 figures, includes theoretical guarantees and empirical   evaluation, submitted to AI&#x2F;ML in Finance track</p>
<p><strong>Summary</strong><br>åœ¨å®æ—¶äº¤æ˜“ä¸­ï¼Œäº¤æ˜“æˆæœ¬ä¸åˆ¶åº¦å˜è¿æ˜¯æŠ•èµ„ç»„åˆå¤±è´¥çš„ä¸»è¦åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FR-LUXï¼ˆè€ƒè™‘æ‘©æ“¦ã€é€‚åº”åˆ¶åº¦å˜åŒ–çš„æ‰§è¡Œæˆæœ¬å­¦ä¹ æ¡†æ¶ï¼‰ï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ äº¤æ˜“åçš„æ”¿ç­–å¹¶ä¿æŒå¯¹æ³¢åŠ¨æ€§å’ŒæµåŠ¨æ€§çš„ç¨³å¥æ€§ã€‚FR-LUXç»“åˆäº†ä¸‰ç§è¦ç´ ï¼šä¸€æ˜¯ç»“åˆæ¯”ä¾‹å’Œå†²å‡»æˆæœ¬çš„å¾®è§‚ç»“æ„ä¸€è‡´æ‰§è¡Œæ¨¡å‹ï¼Œç›´æ¥åµŒå…¥å¥–åŠ±ä¸­ï¼›äºŒæ˜¯è´¸æ˜“ç©ºé—´ä¿¡ä»»åŒºåŸŸï¼Œçº¦æŸåº“å­˜æµåŠ¨çš„å˜åŒ–è€Œä¸æ˜¯å¯¹æ•°ï¼Œäº§ç”Ÿç¨³å®šçš„ä½å‘¨è½¬æ›´æ–°ï¼›ä¸‰æ˜¯æ˜ç¡®çš„åˆ¶åº¦æ¡ä»¶ï¼Œä½¿æ”¿ç­–èƒ½å¤Ÿé’ˆå¯¹LL&#x2F;LH&#x2F;HL&#x2F;HHçŠ¶æ€ä¸“ä¸šåŒ–ï¼Œè€Œä¸ä¼šä½¿æ•°æ®ç¢ç‰‡åŒ–ã€‚åœ¨å¤šä¸ªæˆæœ¬å’Œåˆ¶åº¦æ°´å¹³çš„ç½‘æ ¼ä¸Šï¼ŒFR-LUXå®ç°äº†æœ€é«˜çš„å¹³å‡å¤æ™®æ¯”ç‡ï¼Œç»´æŒäº†ä¸€ä¸ªæ¯”å¼ºåŸºçº¿æ›´å¹³å¦çš„æˆæœ¬æ€§èƒ½æ–œç‡ï¼Œå¹¶åœ¨ç»™å®šçš„å‘¨è½¬é¢„ç®—å†…è¾¾åˆ°äº†å“è¶Šçš„é£é™©å›æŠ¥æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç³»åˆ—å½¢å¼ä¸Šçš„ä¿è¯ã€‚è¯¥æ–¹æ³•æ˜¯å¯è¡Œçš„ï¼šæˆæœ¬æ˜¯é€šè¿‡æ ‡å‡†æµåŠ¨æ€§ä»£ç†æ ¡å‡†çš„ï¼Œåœºæ™¯çº§åˆ«çš„æ¨ç†é¿å…äº†ä¼ªå¤åˆ¶ï¼Œæ‰€æœ‰å›¾è¡¨å’Œè¡¨æ ¼éƒ½å¯ä»¥ä»å‘å¸ƒçš„æ–‡ç‰©ä¸­å¤åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤æ˜“æˆæœ¬å’Œåˆ¶åº¦å˜è¿æ˜¯å¯¼è‡´æŠ•èµ„ç»„åˆåœ¨å®æ—¶äº¤æ˜“ä¸­å¤±è´¥çš„ä¸»è¦åŸå› ã€‚</li>
<li>FR-LUXæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåº”å¯¹äº¤æ˜“æˆæœ¬å’Œä¸åŒå¸‚åœºåˆ¶åº¦çš„å½±å“ã€‚</li>
<li>FR-LUXç»“åˆäº†æ¯”ä¾‹å’Œå†²å‡»æˆæœ¬çš„å¾®è§‚ç»“æ„æ‰§è¡Œæ¨¡å‹ï¼Œå¹¶å°†å…¶ç›´æ¥åµŒå…¥å¥–åŠ±ä¸­ã€‚</li>
<li>é€šè¿‡è´¸æ˜“ç©ºé—´ä¿¡ä»»åŒºåŸŸï¼ŒFR-LUXèƒ½å¤Ÿçº¦æŸåº“å­˜æµåŠ¨çš„å˜åŒ–ï¼Œäº§ç”Ÿç¨³å®šçš„ä½å‘¨è½¬æ›´æ–°ã€‚</li>
<li>FR-LUXå…·æœ‰æ˜ç¡®çš„åˆ¶åº¦æ¡ä»¶ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„å¸‚åœºçŠ¶æ€ï¼ˆLL&#x2F;LH&#x2F;HL&#x2F;HHï¼‰ã€‚</li>
<li>åœ¨å¤šç§æˆæœ¬å’Œåˆ¶åº¦æ°´å¹³çš„æµ‹è¯•ä¸­ï¼ŒFR-LUXå–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼ŒåŒ…æ‹¬é«˜å¤æ™®æ¯”ç‡ã€å¹³ç¨³çš„æˆæœ¬æ€§èƒ½æ–œç‡å’Œä¼˜ç§€çš„é£é™©å›æŠ¥æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6e054b8dc426575e574bf6fe81a20d64.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RoiRL-Efficient-Self-Supervised-Reasoning-with-Offline-Iterative-Reinforcement-Learning"><a href="#RoiRL-Efficient-Self-Supervised-Reasoning-with-Offline-Iterative-Reinforcement-Learning" class="headerlink" title="RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative   Reinforcement Learning"></a>RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative   Reinforcement Learning</h2><p><strong>Authors:Aleksei Arzhantsev, Otmane Sakhi, Flavian Vasile</strong></p>
<p>Reinforcement learning (RL) is central to improving reasoning in large language models (LLMs) but typically requires ground-truth rewards. Test-Time Reinforcement Learning (TTRL) removes this need by using majority-vote rewards, but relies on heavy online RL and incurs substantial computational cost. We propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a family of lightweight offline learning alternatives that can target the same regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to maintain a reference model and instead optimizes weighted log-likelihood objectives, enabling stable training with significantly lower memory and compute requirements. Experimental results show that RoiRL trains to 2.5x faster and consistently outperforms TTRL on reasoning benchmarks, establishing a scalable path to self-improving LLMs without labels. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å æ®æ ¸å¿ƒåœ°ä½ï¼Œä½†é€šå¸¸éœ€è¦çœŸå®å¥–åŠ±ã€‚æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰é€šè¿‡é‡‡ç”¨å¤šæ•°æŠ•ç¥¨å¥–åŠ±æ¥æ¶ˆé™¤è¿™ä¸€éœ€æ±‚ï¼Œä½†ä¾èµ–äºç¹é‡çš„åœ¨çº¿RLï¼Œå¹¶äº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºRoiRLï¼šä»¥ç¦»çº¿è¿­ä»£å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è½»é‡çº§çš„ç¦»çº¿å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°åŒæ ·çš„æ­£åˆ™åŒ–æœ€ä¼˜ç­–ç•¥ã€‚ä¸åŒäºTTRLï¼ŒRoiRLæ— éœ€ç»´æŠ¤å‚è€ƒæ¨¡å‹ï¼Œè€Œæ˜¯ä¼˜åŒ–åŠ æƒå¯¹æ•°ä¼¼ç„¶ç›®æ ‡ï¼Œä»¥è¾ƒä½çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚å®ç°ç¨³å®šè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoiRLçš„è®­ç»ƒé€Ÿåº¦æ˜¯TTRLçš„2.5å€ï¼Œå¹¶ä¸”åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æŒç»­è¡¨ç°ä¼˜äºTTRLï¼Œä¸ºæ— æ ‡ç­¾çš„è‡ªæˆ‘æ”¹è¿›LLMé“ºè®¾äº†å¯æ‰©å±•çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02892v1">PDF</a> Accepted to the Efficient Reasoning Workshop at NeuRIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†é€šå¸¸éœ€è¦çœŸå®å¥–åŠ±ã€‚æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰é€šè¿‡é‡‡ç”¨å¤šæ•°æŠ•ç¥¨å¥–åŠ±æ¶ˆé™¤äº†è¿™ä¸€éœ€æ±‚ï¼Œä½†ä¾èµ–äºç¹é‡çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¹¶äº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºRoiRLï¼šåŸºäºç¦»çº¿è¿­ä»£å¼ºåŒ–å­¦ä¹ çš„æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è½»é‡çº§çš„ç¦»çº¿å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé’ˆå¯¹ç›¸åŒçš„æ­£åˆ™åŒ–æœ€ä¼˜ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚ä¸åŒäºTTRLï¼ŒRoiRLæ— éœ€ç»´æŠ¤å‚è€ƒæ¨¡å‹ï¼Œè€Œæ˜¯ä¼˜åŒ–åŠ æƒå¯¹æ•°ä¼¼ç„¶ç›®æ ‡ï¼Œä»è€Œå®ç°ç¨³å®šçš„è®­ç»ƒå¹¶æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—è¦æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoiRLçš„è®­ç»ƒé€Ÿåº¦æ˜¯TTRLçš„2.5å€ï¼Œä¸”åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æŒç»­è¡¨ç°ä¼˜äºTTRLï¼Œä¸ºæ— æ ‡ç­¾çš„è‡ªæˆ‘æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹å»ºç«‹äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸­å¾ˆé‡è¦ï¼Œä½†éœ€çœŸå®å¥–åŠ±ã€‚</li>
<li>æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰åˆ©ç”¨å¤šæ•°æŠ•ç¥¨å¥–åŠ±ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>RoiRLæ˜¯ä¸€ç§è½»é‡çº§çš„ç¦»çº¿å­¦ä¹ æ–¹æ³•ï¼Œèƒ½ä¼˜åŒ–æ­£åˆ™åŒ–æœ€ä¼˜ç­–ç•¥ã€‚</li>
<li>RoiRLæ— éœ€ç»´æŠ¤å‚è€ƒæ¨¡å‹ï¼Œè€Œæ˜¯ä¼˜åŒ–åŠ æƒå¯¹æ•°ä¼¼ç„¶ç›®æ ‡ã€‚</li>
<li>RoiRLå¯å®ç°ç¨³å®šè®­ç»ƒå¹¶é™ä½å†…å­˜å’Œè®¡ç®—è¦æ±‚ã€‚</li>
<li>RoiRLçš„è®­ç»ƒé€Ÿåº¦æ¯”TTRLå¿«2.5å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-227794aa9f13557daf555221fb6e411d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d712a7b366c66b1600c8716be8c4fc3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941381&auth_key=1759941381-0-0-d382f93eb49337f188269a3103bc855a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reward-Model-Routing-in-Alignment"><a href="#Reward-Model-Routing-in-Alignment" class="headerlink" title="Reward Model Routing in Alignment"></a>Reward Model Routing in Alignment</h2><p><strong>Authors:Xinle Wu, Yao Lu</strong></p>
<p>Reinforcement learning from human or AI feedback (RLHF &#x2F; RLAIF) has become the standard paradigm for aligning large language models (LLMs). However, most pipelines rely on a single reward model (RM), limiting alignment quality and risking overfitting. Recent work explores RM routingâ€“dynamically selecting an RM from a candidate pool to exploit complementary strengths while maintaining $O(1)$ RM callsâ€“but existing methods suffer from cold-start and insufficient exploration. We propose BayesianRouter, a hybrid routing framework that combines offline RM strengths learning with online Bayesian selection. In the offline stage, a multi-task router is trained on preference data to estimate per-RM reliability. In the online stage, a Bayesian Thompson sampling router performs per-query RM selection, initializing RM-specific weight vectors with offline embeddings as Gaussian priors and adaptively updating their posteriors with online rewards to adapt to the evolving policy distribution. Extensive experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently outperforms individual RMs, RM ensembling, and existing routing methods. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»æˆ–AIåé¦ˆï¼ˆRLHF&#x2F;RLAIFï¼‰å·²æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç®¡é“ä¾èµ–äºå•ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œè¿™é™åˆ¶äº†å¯¹é½è´¨é‡å¹¶å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†RMè·¯ç”±â€”â€”ä»å€™é€‰æ± ä¸­åŠ¨æ€é€‰æ‹©ä¸€ä¸ªRMï¼Œä»¥åˆ©ç”¨äº’è¡¥ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒO(1)çš„RMè°ƒç”¨â€”â€”ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å†·å¯åŠ¨å’Œå‹˜æ¢ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†BayesianRouterï¼Œè¿™æ˜¯ä¸€ç§æ··åˆè·¯ç”±æ¡†æ¶ï¼Œç»“åˆäº†ç¦»çº¿RMå¼ºåº¦å­¦ä¹ ä¸åœ¨çº¿è´å¶æ–¯é€‰æ‹©ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œå¤šä»»åŠ¡è·¯ç”±å™¨åœ¨åå¥½æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼°è®¡æ¯ä¸ªRMçš„å¯é æ€§ã€‚åœ¨çº¿é˜¶æ®µï¼Œè´å¶æ–¯Thompsoné‡‡æ ·è·¯ç”±å™¨æ‰§è¡Œæ¯ä¸ªæŸ¥è¯¢çš„RMé€‰æ‹©ï¼Œä½¿ç”¨ç¦»çº¿åµŒå…¥åˆå§‹åŒ–RMç‰¹å®šæƒé‡å‘é‡ä½œä¸ºé«˜æ–¯å…ˆéªŒï¼Œå¹¶è‡ªé€‚åº”åœ°æ ¹æ®å…¶åœ¨çº¿å¥–åŠ±æ›´æ–°åéªŒå€¼ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„ç­–ç•¥åˆ†å¸ƒã€‚åœ¨æŒ‡ä»¤éµå¾ªï¼ˆAlpacaEval-2ã€Arena-Hardã€MT-Benchï¼‰å’Œæ¨ç†ï¼ˆGSM8Kã€MMLUï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBayesianRouteræŒç»­ä¼˜äºå•ä¸ªRMã€RMé›†åˆå’Œç°æœ‰è·¯ç”±æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02850v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼ºåŒ–å­¦ä¹ ä»äººç±»æˆ–AIåé¦ˆï¼ˆRLHF&#x2F;RLAIFï¼‰å·²æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç®¡é“ä¾èµ–äºå•ä¸€å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œé™åˆ¶äº†å¯¹é½è´¨é‡å¹¶å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ã€‚è¿‘æœŸå·¥ä½œæ¢ç´¢äº†RMè·¯ç”±ï¼Œæ—¨åœ¨ä»å€™é€‰æ± ä¸­åŠ¨æ€é€‰æ‹©RMï¼Œåˆ©ç”¨äº’è¡¥ä¼˜åŠ¿å¹¶ä¿æŒO(1)çš„RMè°ƒç”¨æ¬¡æ•°ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å†·å¯åŠ¨å’Œæ¢ç©¶ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆç¦»çº¿RMä¼˜åŠ¿å­¦ä¹ ä¸åœ¨çº¿è´å¶æ–¯é€‰æ‹©çš„æ··åˆè·¯ç”±æ¡†æ¶â€”â€”è´å¶æ–¯è·¯ç”±å™¨ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œå¤šä»»åŠ¡è·¯ç”±å™¨åœ¨åå¥½æ•°æ®ä¸Šè®­ç»ƒä»¥ä¼°è®¡æ¯ä¸ªRMçš„å¯é æ€§ã€‚åœ¨çº¿é˜¶æ®µï¼Œè´å¶æ–¯æ±¤æ™®æ£®é‡‡æ ·è·¯ç”±å™¨æ‰§è¡Œæ¯æŸ¥è¯¢RMé€‰æ‹©ï¼Œä»¥ç¦»çº¿åµŒå…¥åˆå§‹åŒ–RMç‰¹å®šæƒé‡å‘é‡ä½œä¸ºé«˜æ–¯å…ˆéªŒï¼Œå¹¶é€‚åº”æ€§åœ°ç”¨åœ¨çº¿å¥–åŠ±æ›´æ–°å…¶åéªŒæ¦‚ç‡ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„ç­–ç•¥åˆ†å¸ƒã€‚åœ¨æŒ‡ä»¤éµå¾ªï¼ˆAlpacaEval-2ã€Arena-Hardã€MT-Benchï¼‰å’Œæ¨ç†ï¼ˆGSM8Kã€MMLUï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè´å¶æ–¯è·¯ç”±å™¨æŒç»­ä¼˜äºå•ä¸ªRMã€RMé›†æˆå’Œç°æœ‰è·¯ç”±æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»æˆ–AIåé¦ˆï¼ˆRLHF&#x2F;RLAIFï¼‰å·²æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†åšæ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œå­˜åœ¨å¯¹é½è´¨é‡é™åˆ¶åŠè¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>RMè·¯ç”±èƒ½å¤ŸåŠ¨æ€é€‰æ‹©RMï¼Œæ—¨åœ¨åˆ©ç”¨å¤šä¸ªRMçš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰RMè·¯ç”±æ–¹æ³•é¢ä¸´å†·å¯åŠ¨å’Œæ¢ç©¶ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„è´å¶æ–¯è·¯ç”±å™¨ç»“åˆç¦»çº¿RMä¼˜åŠ¿å­¦ä¹ ä¸åœ¨çº¿è´å¶æ–¯é€‰æ‹©ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è´å¶æ–¯è·¯ç”±å™¨åœ¨æŒ‡ä»¤éµå¾ªå’Œæ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-033ce251870601f822eb4f58a48c0b1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941389&auth_key=1759941389-0-0-59944456b5c546fc44d87f9b68392760&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73dc564c8e2b17a2fe11a56df384b289~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941397&auth_key=1759941397-0-0-33b15528aefd5f031a3fce461d2a9a47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="StepChain-GraphRAG-Reasoning-Over-Knowledge-Graphs-for-Multi-Hop-Question-Answering"><a href="#StepChain-GraphRAG-Reasoning-Over-Knowledge-Graphs-for-Multi-Hop-Question-Answering" class="headerlink" title="StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop   Question Answering"></a>StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop   Question Answering</h2><p><strong>Authors:Tengjun Ni, Xin Yuan, Shenghong Li, Kai Wu, Ren Ping Liu, Wei Ni, Wenjie Zhang</strong></p>
<p>Recent progress in retrieval-augmented generation (RAG) has led to more accurate and interpretable multi-hop question answering (QA). Yet, challenges persist in integrating iterative reasoning steps with external knowledge retrieval. To address this, we introduce StepChain GraphRAG, a framework that unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow for enhanced multi-hop QA. Our approach first builds a global index over the corpus; at inference time, only retrieved passages are parsed on-the-fly into a knowledge graph, and the complex query is split into sub-questions. For each sub-question, a BFS-based traversal dynamically expands along relevant edges, assembling explicit evidence chains without overwhelming the language model with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1 scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1). StepChain GraphRAG also fosters enhanced explainability by preserving the chain-of-thought across intermediate retrieval steps. We conclude by discussing how future work can mitigate the computational overhead and address potential hallucinations from large language models to refine efficiency and reliability in multi-hop QA. </p>
<blockquote>
<p>æœ€æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿›å±•ä¸ºå¤šè·³é—®ç­”ï¼ˆQAï¼‰å¸¦æ¥äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå°†è¿­ä»£æ¨ç†æ­¥éª¤ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ç›¸ç»“åˆä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†StepChain GraphRAGæ¡†æ¶ï¼Œå®ƒå°†é—®é¢˜åˆ†è§£ä¸å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æ¨ç†æµç¨‹ç›¸ç»“åˆï¼Œä»¥æ”¹å–„å¤šè·³é—®ç­”ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåœ¨è¯­æ–™åº“ä¸Šæ„å»ºå…¨å±€ç´¢å¼•ï¼›åœ¨æ¨ç†æ—¶ï¼Œä»…å°†æ£€ç´¢åˆ°çš„æ®µè½å³æ—¶è§£æä¸ºçŸ¥è¯†å›¾è°±ï¼Œå¹¶å°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†ä¸ºå­é—®é¢˜ã€‚å¯¹äºæ¯ä¸ªå­é—®é¢˜ï¼ŒåŸºäºBFSçš„éå†ä¼šæ²¿ç€ç›¸å…³è¾¹åŠ¨æ€æ‰©å±•ï¼Œç»„è£…æ˜ç¡®çš„è¯æ®é“¾ï¼Œè€Œä¸ä¼šä½¿è¯­è¨€æ¨¡å‹å—åˆ°å†—ä½™ä¸Šä¸‹æ–‡çš„å›°æ‰°ã€‚åœ¨MuSiQueã€2WikiMultiHopQAå’ŒHotpotQAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒStepChain GraphRAGè¾¾åˆ°äº†æœ€æ–°çš„ç²¾ç¡®åŒ¹é…å’ŒF1åˆ†æ•°ã€‚ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯é¡¶å°–æ–¹æ³•ï¼ŒStepChain GraphRAGå¹³å‡æé«˜äº†2.57%çš„ç²¾ç¡®åŒ¹é…ç‡å’Œ2.13%çš„F1åˆ†æ•°ï¼Œä¸”åœ¨HotpotQAä¸Šçš„æå‡æœ€ä¸ºæ˜¾è‘—ï¼ˆç²¾ç¡®åŒ¹é…ç‡æé«˜4.70%ï¼ŒF1åˆ†æ•°æé«˜3.44%ï¼‰ã€‚StepChain GraphRAGè¿˜é€šè¿‡ä¿ç•™ä¸­é—´æ£€ç´¢æ­¥éª¤çš„æ€ç»´é“¾ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœªæ¥å·¥ä½œå¦‚ä½•å‡è½»è®¡ç®—å¼€é”€å¹¶è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œä»¥æé«˜å¤šè·³é—®ç­”çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02827v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šè·³é—®ç­”ï¼ˆMulti-hop Question Answeringï¼‰çš„æ–°æ¡†æ¶StepChain GraphRAGã€‚è¯¥æ¡†æ¶ç»“åˆäº†é—®é¢˜åˆ†è§£å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æ¨ç†æµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤šè·³é—®ç­”çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ„å»ºå…¨å±€ç´¢å¼•å’Œå®æ—¶è§£ææ£€ç´¢æ®µè½ï¼ŒStepChain GraphRAGèƒ½å¤ŸåŠ¨æ€åœ°æ²¿ç›¸å…³è¾¹ç¼˜æ‰©å±•ï¼Œå½¢æˆæ˜ç¡®çš„è¯æ®é“¾ï¼ŒåŒæ—¶é¿å…è¯­è¨€æ¨¡å‹å—åˆ°è¿‡å¤šæ— å…³å†…å®¹çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStepChain GraphRAGåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ°´å¹³çš„ç²¾ç¡®åŒ¹é…å’ŒF1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>StepChain GraphRAGæ˜¯ä¸€ä¸ªç”¨äºå¤šè·³é—®ç­”çš„æ–°æ¡†æ¶ï¼Œç»“åˆäº†é—®é¢˜åˆ†è§£å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢æ¨ç†æµç¨‹ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå…¨å±€ç´¢å¼•å’Œå®æ—¶è§£ææ£€ç´¢æ®µè½æ¥æé«˜å¤šè·³é—®ç­”çš„å‡†ç¡®ç‡å’Œè§£é‡Šæ€§ã€‚</li>
<li>StepChain GraphRAGå®ç°äº†æœ€æ–°æ°´å¹³çš„ç²¾ç¡®åŒ¹é…å’ŒF1åˆ†æ•°ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„æ•ˆæœæå‡æ˜¾è‘—ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ²¿ç›¸å…³è¾¹ç¼˜åŠ¨æ€æ‰©å±•ï¼Œå½¢æˆæ˜ç¡®çš„è¯æ®é“¾ï¼Œå¢å¼ºäº†é—®ç­”çš„å¯é æ€§ã€‚</li>
<li>StepChain GraphRAGä¹Ÿæé«˜äº†æ•ˆç‡ï¼Œé¿å…äº†è¯­è¨€æ¨¡å‹å—åˆ°è¿‡å¤šæ— å…³å†…å®¹çš„å¹²æ‰°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e06c13cfff28d2c55ac212a1e18c7077~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941404&auth_key=1759941404-0-0-58224683620041250318623d104c90c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d98c54265a9c383955d58dacb2714f8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941411&auth_key=1759941411-0-0-c38098f4bd7e8e4750de4ce511a55fe7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f78736410db2d295e2cfe4c9f32e179b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941418&auth_key=1759941418-0-0-9c416d913d9bef29178ff1701573e417&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Retrv-R1-A-Reasoning-Driven-MLLM-Framework-for-Universal-and-Efficient-Multimodal-Retrieval"><a href="#Retrv-R1-A-Reasoning-Driven-MLLM-Framework-for-Universal-and-Efficient-Multimodal-Retrieval" class="headerlink" title="Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient   Multimodal Retrieval"></a>Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient   Multimodal Retrieval</h2><p><strong>Authors:Lanyun Zhu, Deyi Ji, Tianrun Chen, Haiyang Wu, Shiqi Wang</strong></p>
<p>The success of DeepSeek-R1 demonstrates the immense potential of using reinforcement learning (RL) to enhance LLMsâ€™ reasoning capabilities. This paper introduces Retrv-R1, the first R1-style MLLM specifically designed for multimodal universal retrieval, achieving higher performance by employing step-by-step reasoning to produce more accurate retrieval results. We find that directly applying the methods of DeepSeek-R1 to retrieval tasks is not feasible, mainly due to (1) the high computational cost caused by the large token consumption required for multiple candidates with reasoning processes, and (2) the instability and suboptimal results when directly applying RL to train for retrieval tasks. To address these issues, Retrv-R1 introduces an information compression module with a details inspection mechanism, which enhances computational efficiency by reducing the number of tokens while ensuring that critical information for challenging candidates is preserved. Furthermore, a new training paradigm is proposed, including an activation stage using a retrieval-tailored synthetic CoT dataset for more effective optimization, followed by RL with a novel curriculum reward to improve both performance and efficiency. Incorporating these novel designs, Retrv-R1 achieves SOTA performance, high efficiency, and strong generalization ability, as demonstrated by experiments across multiple benchmarks and tasks. </p>
<blockquote>
<p>DeepSeek-R1çš„æˆåŠŸå±•ç¤ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†Retrv-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºå¤šæ¨¡æ€é€šç”¨æ£€ç´¢è®¾è®¡çš„R1é£æ ¼MLLMï¼Œé€šè¿‡é‡‡ç”¨é€æ­¥æ¨ç†äº§ç”Ÿæ›´å‡†ç¡®çš„æ£€ç´¢ç»“æœï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ç›´æ¥å°†DeepSeek-R1çš„æ–¹æ³•åº”ç”¨äºæ£€ç´¢ä»»åŠ¡å¹¶ä¸å¯è¡Œï¼Œä¸»è¦æ˜¯ç”±äºï¼ˆ1ï¼‰æ¨ç†è¿‡ç¨‹ä¸­å¤šä¸ªå€™é€‰è€…éœ€è¦å¤§é‡ä»¤ç‰Œæ¶ˆè€—ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼›ï¼ˆ2ï¼‰ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ£€ç´¢ä»»åŠ¡è®­ç»ƒæ—¶çš„ä¸ç¨³å®šæ€§å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼ŒRetrv-R1å¼•å…¥äº†ä¸€ä¸ªä¿¡æ¯å‹ç¼©æ¨¡å—å’Œç»†èŠ‚æ£€æŸ¥æœºåˆ¶ï¼Œé€šè¿‡å‡å°‘ä»¤ç‰Œæ•°é‡æé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å€™é€‰è€…çš„å…³é”®ä¿¡æ¯å¾—ä»¥ä¿ç•™ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬ä½¿ç”¨é’ˆå¯¹æ£€ç´¢çš„åˆæˆCoTæ•°æ®é›†è¿›è¡Œæ¿€æ´»é˜¶æ®µï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œä¼˜åŒ–ï¼Œéšåæ˜¯å¼ºåŒ–å­¦ä¹ é…åˆæ–°è¯¾ç¨‹å¥–åŠ±ä»¥æé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡èå…¥è¿™äº›æ–°é¢–è®¾è®¡ï¼ŒRetrv-R1å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€é«˜æ•ˆç‡åŠå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œä»»åŠ¡ä¸­çš„å®éªŒä¸­å¾—åˆ°è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02745v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›å·¨å¤§ã€‚Retrv-R1é€šè¿‡é‡‡ç”¨é€æ­¥æ¨ç†å’Œä¿¡æ¯å‹ç¼©æ¨¡å—ï¼Œå®ç°äº†æ›´é«˜çš„æ£€ç´¢æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç›´æ¥åº”ç”¨DeepSeek-R1æ–¹æ³•å¸¦æ¥çš„é«˜è®¡ç®—æˆæœ¬å’Œä¸ç¨³å®šé—®é¢˜ï¼Œæå‡ºäº†æ–°å‹è®­ç»ƒèŒƒå¼å’Œå¥–åŠ±æœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Retrv-R1æ˜¯ä¸“é—¨ä¸ºå¤šæ¨¡æ€é€šç”¨æ£€ç´¢è®¾è®¡çš„R1é£æ ¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡é€æ­¥æ¨ç†æé«˜æ£€ç´¢å‡†ç¡®æ€§ã€‚</li>
<li>ç›´æ¥åº”ç”¨DeepSeek-R1æ–¹æ³•åœ¨æ£€ç´¢ä»»åŠ¡ä¸­ä¸å¯è¡Œï¼Œä¸»è¦åŸå› æ˜¯é«˜è®¡ç®—æˆæœ¬å’Œä¸ç¨³å®šçš„ç»“æœã€‚</li>
<li>Retrv-R1å¼•å…¥äº†ä¿¡æ¯å‹ç¼©æ¨¡å—ï¼Œä»¥å‡å°‘è®¡ç®—è¿‡ç¨‹ä¸­æ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹æå‡ºäº†ä¸€ä¸ªåŒ…å«æ¿€æ´»é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ çš„æ–°è®­ç»ƒèŒƒå¼ï¼Œé‡‡ç”¨é’ˆå¯¹æ£€ç´¢ä»»åŠ¡çš„åˆæˆCoTæ•°æ®é›†è¿›è¡Œæ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d955e5b53e67bff60e2311b1520edcc.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ec6dac1572f58626849e84dee6ea9ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941433&auth_key=1759941433-0-0-f331ed976dfe05cc52deff62fe2db72f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="IndiCASA-A-Dataset-and-Bias-Evaluation-Framework-in-LLMs-Using-Contrastive-Embedding-Similarity-in-the-Indian-Context"><a href="#IndiCASA-A-Dataset-and-Bias-Evaluation-Framework-in-LLMs-Using-Contrastive-Embedding-Similarity-in-the-Indian-Context" class="headerlink" title="IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using   Contrastive Embedding Similarity in the Indian Context"></a>IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using   Contrastive Embedding Similarity in the Indian Context</h2><p><strong>Authors:Santhosh G S, Akshay Govind S, Gokul S Krishnan, Balaraman Ravindran, Sriraam Natarajan</strong></p>
<p>Large Language Models (LLMs) have gained significant traction across critical domains owing to their impressive contextual understanding and generative capabilities. However, their increasing deployment in high stakes applications necessitates rigorous evaluation of embedded biases, particularly in culturally diverse contexts like India where existing embedding-based bias assessment methods often fall short in capturing nuanced stereotypes. We propose an evaluation framework based on a encoder trained using contrastive learning that captures fine-grained bias through embedding similarity. We also introduce a novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and Anti-stereotypes) comprising 2,575 human-validated sentences spanning five demographic axes: caste, gender, religion, disability, and socioeconomic status. Our evaluation of multiple open-weight LLMs reveals that all models exhibit some degree of stereotypical bias, with disability related biases being notably persistent, and religion bias generally lower likely due to global debiasing efforts demonstrating the need for fairer model development. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå…¶ä»¤äººå°è±¡æ·±åˆ»çš„ä¸Šä¸‹æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨å…³é”®é¢†åŸŸè·å¾—äº†å·¨å¤§çš„å¸å¼•åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„ä¸æ–­éƒ¨ç½²ï¼Œè¦æ±‚å¯¹åµŒå…¥çš„åè§è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦ç­‰æ–‡åŒ–å¤šå…ƒçš„èƒŒæ™¯ä¸‹ï¼Œç°æœ‰çš„åŸºäºåµŒå…¥çš„åè§è¯„ä¼°æ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰å¾®å¦™çš„åˆ»æ¿å°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„ç¼–ç å™¨çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§æ¥æ•æ‰ç»†å¾®çš„åè§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†â€”â€”IndiCASAï¼ˆåŸºäºå°åº¦åè§çš„ä¸Šä¸‹æ–‡å¯¹é½çš„åˆ»æ¿å°è±¡å’Œååˆ»æ¿å°è±¡æ•°æ®é›†ï¼‰ï¼ŒåŒ…å«2575ä¸ªç»äººç±»éªŒè¯çš„å¥å­ï¼Œè·¨è¶Šäº”ä¸ªäººå£ç»Ÿè®¡è½´ï¼šç§å§“ã€æ€§åˆ«ã€å®—æ•™ã€æ®‹ç–¾å’Œç¤¾ä¼šç»æµåœ°ä½ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªå¼€æ”¾æƒé‡LLMçš„è¯„ä»·æ˜¾ç¤ºï¼Œæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºä¸€å®šç¨‹åº¦çš„åˆ»æ¿åè§ï¼Œä¸æ®‹ç–¾æœ‰å…³çš„åè§å°¤ä¸ºæ˜æ˜¾ï¼Œè€Œç”±äºå…¨çƒæ¶ˆé™¤åè§å·¥ä½œçš„åŠªåŠ›ï¼Œå®—æ•™åè§ä¸€èˆ¬è¾ƒä½ï¼Œè¿™æ˜¾ç¤ºäº†æ›´å…¬å¹³æ¨¡å‹å¼€å‘çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02742v1">PDF</a> Accepted at 8th AAAI&#x2F;ACM Conference on AI, Ethics, and Society (AIES)   2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åœ¨å…³é”®é¢†åŸŸçš„æ˜¾è‘—ä½œç”¨è€Œå¤‡å—å…³æ³¨ï¼Œå…¶åœ¨è¯­å¢ƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„éƒ¨ç½²è¦æ±‚å¯¹åµŒå…¥çš„åè§è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦ç­‰æ–‡åŒ–å¤šå…ƒçš„ç¯å¢ƒä¸­ã€‚ç°æœ‰åŸºäºåµŒå…¥çš„åè§è¯„ä¼°æ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰å¾®å¦™çš„åˆ»æ¿å°è±¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„ç¼–ç å™¨è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§æ•æ‰ç²¾ç»†çš„åè§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†â€”â€”IndiCASAï¼ŒåŒ…å«ç»è¿‡äººå·¥éªŒè¯çš„æ¶µç›–äº”ä¸ªç§æ—è½´åˆ«çš„å¥å­ã€‚å¯¹å¤šä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰€æœ‰æ¨¡å‹å‡è¡¨ç°å‡ºä¸€å®šç¨‹åº¦çš„åˆ»æ¿åè§ï¼Œå…¶ä¸­ä¸æ®‹ç–¾ç›¸å…³çš„åè§å°¤ä¸ºæ˜æ˜¾ï¼Œå®—æ•™åè§åˆ™å› å…¨çƒå»åè§åŠªåŠ›è€Œè¾ƒä½ã€‚è¿™å‡¸æ˜¾äº†å¯¹å…¬å¹³æ¨¡å‹å‘å±•çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„å‡ºè‰²è¡¨ç°è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-49d621116f9f715f08712521338851f9.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebc9ec3c8f99d42d9df050280fe779a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941447&auth_key=1759941447-0-0-67fa084228b35c235167b4952a5b4e23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-905623af9713f08b0134bc2d8e99ed45~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941454&auth_key=1759941454-0-0-eb6a6b376dd18938cb0df8fd121d4c0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65847e5ea0d2b09a41ac3ed2e2cdeeab~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941461&auth_key=1759941461-0-0-ccf7b583bf00d38fbda24d4fd5b4fa7d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59ea8c154f9fc6e270dd552d6ea17086~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941467&auth_key=1759941467-0-0-53093b4cdff53ca65c52b922bacbd26d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Time-To-Inconsistency-A-Survival-Analysis-of-Large-Language-Model-Robustness-to-Adversarial-Attacks"><a href="#Time-To-Inconsistency-A-Survival-Analysis-of-Large-Language-Model-Robustness-to-Adversarial-Attacks" class="headerlink" title="Time-To-Inconsistency: A Survival Analysis of Large Language Model   Robustness to Adversarial Attacks"></a>Time-To-Inconsistency: A Survival Analysis of Large Language Model   Robustness to Adversarial Attacks</h2><p><strong>Authors:Yubo Li, Ramayya Krishnan, Rema Padman</strong></p>
<p>Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present the first comprehensive survival analysis of conversational AI robustness, analyzing 36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a time-to-event process. Our survival modeling framework-employing Cox proportional hazards, Accelerated Failure Time, and Random Survival Forest approaches-reveals extraordinary temporal dynamics. We find that abrupt, prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing the hazard of conversational failure. In stark contrast, gradual, cumulative drift is highly protective, vastly reducing the failure hazard and enabling significantly longer dialogues. AFT models with interactions demonstrate superior performance, achieving excellent discrimination and exceptional calibration. These findings establish survival analysis as a powerful paradigm for evaluating LLM robustness, offer concrete insights for designing resilient conversational agents, and challenge prevailing assumptions about the necessity of semantic consistency in conversational AI Systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„é¢è²Œï¼Œç„¶è€Œå®ƒä»¬åœ¨æ‰©å±•çš„å¤šè½®å¯¹è¯ä¸­çš„ç¨³å¥æ€§ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¾§é‡äºé™æ€åŸºå‡†å’Œå•è½®è¯„ä¼°ï¼Œæ— æ³•æ•æ‰å¯¹è¯é€€åŒ–çš„æ—¶é—´åŠ¨æ€ï¼Œè¿™å¹¶ä¸èƒ½çœŸå®åæ˜ ç°å®ä¸–ç•Œä¸­çš„äº’åŠ¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„ç¨³å¥æ€§è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„ç”Ÿå­˜åˆ†æï¼Œåˆ†æäº†è·¨è¶Š9ä¸ªæœ€æ–°LLMçš„36,951ä¸ªå¯¹è¯è½®æ¬¡ï¼Œå¹¶å°†å¤±è´¥å»ºæ¨¡ä¸ºæ—¶é—´åˆ°äº‹ä»¶çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç”Ÿå­˜å»ºæ¨¡æ¡†æ¶é‡‡ç”¨Coxæ¯”ä¾‹é£é™©æ¨¡å‹ã€åŠ é€Ÿå¤±æ•ˆæ—¶é—´æ¨¡å‹å’Œéšæœºç”Ÿå­˜æ£®æ—æ–¹æ³•ï¼Œæ­ç¤ºäº†å¯¹è¯å¤±è´¥çš„æ˜¾è‘—æ—¶é—´åŠ¨æ€æ€§ã€‚æˆ‘ä»¬å‘ç°çªç„¶å‘ç”Ÿçš„å³æ—¶å¯¹å³æ—¶ï¼ˆP2Pï¼‰è¯­ä¹‰æ¼‚ç§»æ˜¯ç¾éš¾æ€§çš„ï¼Œæå¤§åœ°å¢åŠ äº†å¯¹è¯å¤±è´¥çš„é£é™©ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€æ¸ç´¯ç§¯çš„æ¼‚ç§»å…·æœ‰å¾ˆå¼ºçš„ä¿æŠ¤æ€§ä½œç”¨ï¼Œå¤§å¤§é™ä½äº†å¤±è´¥çš„é£é™©ï¼Œå¹¶å®ç°äº†æ›´é•¿çš„å¯¹è¯ã€‚å…·æœ‰äº¤äº’ä½œç”¨çš„åŠ é€Ÿå¤±æ•ˆæ—¶é—´æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†è‰¯å¥½çš„é‰´åˆ«åŠ›å’Œæé«˜çš„æ ¡å‡†åº¦ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ç”Ÿå­˜åˆ†æåœ¨è¯„ä¼°LLMç¨³å¥æ€§æ–¹é¢çš„å¼ºå¤§èŒƒå¼åœ°ä½ï¼Œä¸ºè®¾è®¡å…·æœ‰å¼¹æ€§çš„å¯¹è¯ä»£ç†æä¾›äº†å…·ä½“è§è§£ï¼Œå¹¶æŒ‘æˆ˜äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­è¯­ä¹‰ä¸€è‡´æ€§çš„å¿…è¦æ€§è¿™ä¸€æ™®éå‡è®¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02712v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯AIé¢†åŸŸæ€èµ·é©å‘½ï¼Œä½†åœ¨å¤æ‚å¤šè½®å¯¹è¯ä¸­çš„ç¨³å¥æ€§å°šå¾…æ·±å…¥äº†è§£ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¾§é‡äºé™æ€åŸºå‡†æµ‹è¯•å’Œå•è½®è¯„ä¼°ï¼Œæ— æ³•æ•æ‰çœŸå®å¯¹è¯äº’åŠ¨ä¸­çš„æ—¶åºåŠ¨æ€æ€§å¯¹è¯é€€å˜è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶é¦–æ¬¡é‡‡ç”¨ç”Ÿå­˜åˆ†æå…¨é¢è¯„ä¼°å¯¹è¯AIçš„ç¨³å¥æ€§ï¼Œé€šè¿‡å¯¹è·¨ä¹ç§æœ€å…ˆè¿›çš„LLMçš„36951è½®å¯¹è¯è¿›è¡Œåˆ†æï¼Œå¹¶å°†æ¨¡å‹å¤±è´¥è§†ä¸ºä¸€ä¸ªæ—¶é—´åˆ°äº‹ä»¶çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç”Ÿå­˜å»ºæ¨¡æ¡†æ¶ï¼ˆé‡‡ç”¨Coxæ¯”ä¾‹é£é™©ã€åŠ é€Ÿå¤±æ•ˆæ—¶é—´å’Œéšæœºç”Ÿå­˜æ£®æ—ç­‰æ–¹æ³•ï¼‰æ­ç¤ºäº†æƒŠäººçš„æ—¶åºåŠ¨æ€æ€§ã€‚æˆ‘ä»¬å‘ç°çªç„¶å‡ºç°çš„å³æ—¶è¯­ä¹‰æ¼‚ç§»å…·æœ‰ç¾éš¾æ€§å½±å“ï¼Œæ˜¾è‘—å¢åŠ äº†å¯¹è¯å¤±è´¥çš„é£é™©ã€‚ç›¸åï¼Œé€æ­¥ç´¯ç§¯çš„æ¼‚ç§»æä¾›äº†é«˜åº¦ä¿æŠ¤ï¼Œæå¤§åœ°é™ä½äº†å¤±è´¥é£é™©å¹¶ä½¿å¾—èƒ½å¤Ÿè¿›è¡Œæ›´é•¿çš„å¯¹è¯ã€‚å…·æœ‰äº¤äº’çš„AFTæ¨¡å‹å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œå…·æœ‰å¾ˆé«˜çš„åŒºåˆ†åº¦å’Œå‡ºè‰²çš„æ ¡å‡†èƒ½åŠ›ã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†ç”Ÿå­˜åˆ†æåœ¨è¯„ä¼°LLMç¨³å¥æ€§æ–¹é¢çš„å¼ºå¤§ä½œç”¨ï¼Œä¸ºè®¾è®¡å…·æœ‰å¼¹æ€§çš„å¯¹è¯ä»£ç†æä¾›äº†æ·±åˆ»è§è§£ï¼Œå¹¶æŒ‘æˆ˜äº†å…³äºå¯¹è¯AIç³»ç»Ÿä¸­è¯­ä¹‰ä¸€è‡´æ€§å¿…è¦æ€§çš„æ™®éå‡è®¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å¯¹è¯ä¸­çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶æ— æ³•æ•æ‰çœŸå®å¯¹è¯ä¸­çš„æ—¶åºåŠ¨æ€æ€§å¯¹è¯é€€å˜è¿‡ç¨‹ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡é‡‡ç”¨ç”Ÿå­˜åˆ†æè¯„ä¼°å¯¹è¯AIç¨³å¥æ€§ï¼Œæ­ç¤ºäº†æƒŠäººçš„æ—¶åºåŠ¨æ€æ€§ã€‚</li>
<li>ç”Ÿå­˜å»ºæ¨¡æ¡†æ¶èƒ½å¤Ÿæ•æ‰åˆ°å³æ—¶è¯­ä¹‰æ¼‚ç§»å’Œé€æ­¥ç´¯ç§¯çš„æ¼‚ç§»å¯¹å¯¹è¯ç¨³å¥æ€§çš„å½±å“ã€‚</li>
<li>ç”Ÿå­˜åˆ†ææ¡†æ¶æ­ç¤ºäº†ä¸åŒè¯­ä¹‰æ¼‚ç§»ç±»å‹å¯¹å¯¹è¯å¤±è´¥é£é™©çš„æ˜¾è‘—å½±å“ã€‚</li>
<li>å…·æœ‰äº¤äº’çš„AFTæ¨¡å‹å±•ç°äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°å’Œé«˜åº¦çš„æ ¡å‡†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6bffe2c391eb683a357107d2df5a2169~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941475&auth_key=1759941475-0-0-0f8c09d579166b2872332bf3b2756b4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SoT-Structured-of-Thought-Prompting-Guides-Multilingual-Reasoning-in-Large-Language-Models"><a href="#SoT-Structured-of-Thought-Prompting-Guides-Multilingual-Reasoning-in-Large-Language-Models" class="headerlink" title="SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in   Large Language Models"></a>SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in   Large Language Models</h2><p><strong>Authors:Rui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan Xu, Kaiyu Huang</strong></p>
<p>Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Cherry-qwq/SoT">https://github.com/Cherry-qwq/SoT</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å‘å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡æ·±å…¥æ€è€ƒå®Œæˆå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºèµ„æºé™åˆ¶ï¼Œæ¨ç†èƒ½åŠ›å°šæœªæˆåŠŸæ¨å¹¿åˆ°éé«˜èµ„æºè¯­è¨€ï¼Œè¿™å¯¼è‡´åœ¨å¤šå…ƒè¯­è¨€æ¨ç†ä»»åŠ¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è®­ç»ƒçš„æ–¹æ³•â€œæ€ç»´ç»“æ„â€ï¼ˆStructured-of-Thoughtï¼ŒSoTï¼‰ï¼Œé€šè¿‡å¤šæ­¥è½¬æ¢æé«˜å¤šå…ƒè¯­è¨€æ¨ç†çš„æ€§èƒ½ï¼šè¯­è¨€æ€ç»´è½¬æ¢å’Œç»“æ„çŸ¥è¯†è½¬æ¢ã€‚SoTæ–¹æ³•å°†ç‰¹å®šè¯­è¨€çš„è¯­ä¹‰ä¿¡æ¯è½¬æ¢ä¸ºç‹¬ç«‹äºè¯­è¨€çš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£ä¸åŒè¯­è¨€ä¸­çš„æŸ¥è¯¢ã€‚æ­¤å¤–ï¼ŒSoTæœ‰æ•ˆåœ°å¼•å¯¼LLMè¿›è¡Œæ›´é›†ä¸­çš„æ¨ç†ï¼Œåœ¨å¤„ç†è·¨è¯­è¨€è¡¨è¾¾å·®å¼‚æ—¶ä¿æŒä¸€è‡´çš„åº•å±‚æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹çš„å„ç§ä¸»å¹²æ—¶ï¼ŒSoTåœ¨å¤šä¸ªå¤šå…ƒè¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå‡ ä¸ªå¼ºå¤§çš„åŸºçº¿ã€‚å®ƒè¿˜å¯ä»¥ä¸å…¶ä»–æ— è®­ç»ƒç­–ç•¥é›†æˆä»¥è¿›ä¸€æ­¥æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cherry-qwq/SoT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Cherry-qwq/SoTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02648v1">PDF</a> EMNLP 2025 (findings)</p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘å‘å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡æ·±åº¦æ€è€ƒå®Œæˆå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œä½†åœ¨éé«˜èµ„æºè¯­è¨€çš„æ¨ç†èƒ½åŠ›è½¬ç§»æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•â€”â€”ç»“æ„åŒ–æ€ç»´ï¼ˆStructured-of-Thoughtï¼ŒSoTï¼‰ï¼Œé€šè¿‡è¯­è¨€æ€ç»´è½¬æ¢å’Œç»“æ„åŒ–çŸ¥è¯†è½¬æ¢çš„å¤šæ­¥è½¬æ¢ï¼Œæé«˜å¤šè¯­è¨€æ¨ç†çš„æ€§èƒ½ã€‚SoTæ–¹æ³•å°†è¯­è¨€ç‰¹å®šçš„è¯­ä¹‰ä¿¡æ¯è½¬æ¢ä¸ºè¯­è¨€æ— å…³çš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£ä¸åŒè¯­è¨€çš„æŸ¥è¯¢ã€‚æ­¤å¤–ï¼ŒSoTè¿˜èƒ½æœ‰æ•ˆå¼•å¯¼LLMè¿›è¡Œæ›´é›†ä¸­çš„æ¨ç†ï¼Œåœ¨å¤„ç†è·¨è¯­è¨€è¡¨è¾¾å·®å¼‚æ—¶ä¿æŒä¸€è‡´çš„æ¨ç†é€”å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoTåœ¨å¤šä¸ªå¤šè¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå¤šä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶å¯åœ¨é€‚åº”å„ç§LLMä¸»å¹²æ—¶ä¸å…¶ä»–æ— éœ€è®­ç»ƒçš„ç­–ç•¥ç›¸ç»“åˆä»¥å®ç°è¿›ä¸€æ­¥æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å…·å¤‡é€šè¿‡æ·±åº¦æ€è€ƒå®Œæˆå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>éé«˜èµ„æºè¯­è¨€åœ¨LLMçš„æ¨ç†èƒ½åŠ›è½¬ç§»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Structured-of-Thoughtï¼ˆSoTï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šè¯­è¨€æ¨ç†æ€§èƒ½ã€‚</li>
<li>SoTé€šè¿‡è½¬æ¢ä¸ºè¯­è¨€æ— å…³çš„ç»“æ„åŒ–è¡¨ç¤ºæ¥å¤„ç†ä¸åŒè¯­è¨€çš„æŸ¥è¯¢ã€‚</li>
<li>SoTæœ‰æ•ˆå¼•å¯¼LLMè¿›è¡Œæ›´é›†ä¸­çš„æ¨ç†ï¼Œä¿æŒä¸€è‡´çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>å®éªŒè¯æ˜SoTåœ¨å¤šç§å¤šè¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>SoTå¯ä¸å…¶ä»–æ— éœ€è®­ç»ƒçš„ç­–ç•¥ç»“åˆï¼Œå®ç°è¿›ä¸€æ­¥çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9fa768ba75b7dfd99828f78683e888dc.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-34eb5573b56ae1c2f28d3a13b874e660~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941490&auth_key=1759941490-0-0-43c355f69bc2f9a2114304defb070ba8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d16da59073be523da4f1231f599728e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941497&auth_key=1759941497-0-0-0dcf9a581ed068a5850dd4e9fe171f8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd241617d2e76c5418df10bded0b0acb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941503&auth_key=1759941503-0-0-47521ffcec7fb17b8af59022e0db007e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-dcabee6add9d6ba803cf62458385f56d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ab05dd70cb10573e024bd87a83c458.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Evaluation-Framework-for-Highlight-Explanations-of-Context-Utilisation-in-Language-Models"><a href="#Evaluation-Framework-for-Highlight-Explanations-of-Context-Utilisation-in-Language-Models" class="headerlink" title="Evaluation Framework for Highlight Explanations of Context Utilisation   in Language Models"></a>Evaluation Framework for Highlight Explanations of Context Utilisation   in Language Models</h2><p><strong>Authors:Jingyi Sun, Pepa Atanasova, Sagnik Ray Choudhury, Sekh Mainul Islam, Isabelle Augenstein</strong></p>
<p>Context utilisation, the ability of Language Models (LMs) to incorporate relevant information from the provided context when generating responses, remains largely opaque to users, who cannot determine whether models draw from parametric memory or provided context, nor identify which specific context pieces inform the response. Highlight explanations (HEs) offer a natural solution as they can point the exact context pieces and tokens that influenced model outputs. However, no existing work evaluates their effectiveness in accurately explaining context utilisation. We address this gap by introducing the first gold standard HE evaluation framework for context attribution, using controlled test cases with known ground-truth context usage, which avoids the limitations of existing indirect proxy evaluations. To demonstrate the frameworkâ€™s broad applicability, we evaluate four HE methods â€“ three established techniques and MechLight, a mechanistic interpretability approach we adapt for this task â€“ across four context scenarios, four datasets, and five LMs. Overall, we find that MechLight performs best across all context scenarios. However, all methods struggle with longer contexts and exhibit positional biases, pointing to fundamental challenges in explanation accuracy that require new approaches to deliver reliable context utilisation explanations at scale. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡åˆ©ç”¨æ˜¯æŒ‡è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨ç”Ÿæˆå“åº”æ—¶ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­èå…¥ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¯¹äºç”¨æˆ·æ¥è¯´ä»ç„¶æ˜¯ç›¸å½“æ¨¡ç³Šã€‚ç”¨æˆ·æ— æ³•ç¡®å®šæ¨¡å‹æ˜¯ä»å‚æ•°åŒ–è®°å¿†è¿˜æ˜¯æä¾›çš„ä¸Šä¸‹æ–‡ä¸­è·å–ä¿¡æ¯ï¼Œä¹Ÿæ— æ³•è¯†åˆ«å“ªäº›ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å½±å“äº†å“åº”ã€‚é«˜äº®è§£é‡Šï¼ˆHEsï¼‰æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æŒ‡å‡ºå½±å“æ¨¡å‹è¾“å‡ºçš„ç¡®åˆ‡ä¸Šä¸‹æ–‡ç‰‡æ®µå’Œæ ‡è®°ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œå¹¶æ²¡æœ‰è¯„ä¼°å®ƒä»¬åœ¨å‡†ç¡®è§£é‡Šä¸Šä¸‹æ–‡åˆ©ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç¬¬ä¸€ä¸ªé»„é‡‘æ ‡å‡†çš„HEè¯„ä¼°æ¡†æ¶æ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œè¯¥æ¡†æ¶ç”¨äºä¸Šä¸‹æ–‡å½’å±ï¼Œä½¿ç”¨å¸¦æœ‰å·²çŸ¥åœ°é¢çœŸå®ä¸Šä¸‹æ–‡ä½¿ç”¨çš„å—æ§æµ‹è¯•ç”¨ä¾‹ï¼Œé¿å…äº†ç°æœ‰é—´æ¥ä»£ç†è¯„ä¼°çš„å±€é™æ€§ã€‚ä¸ºäº†å±•ç¤ºè¯¥æ¡†æ¶çš„å¹¿æ³›åº”ç”¨æ€§ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ç§HEæ–¹æ³•â€”â€”ä¸‰ç§æ—¢å®šæŠ€æœ¯å’Œæˆ‘ä»¬ä¸ºæ­¤ä»»åŠ¡æ”¹ç¼–çš„æœºåˆ¶æ€§è§£é‡Šæ–¹æ³•MechLightâ€”â€”è·¨è¶Šå››ç§ä¸Šä¸‹æ–‡åœºæ™¯ã€å››ä¸ªæ•°æ®é›†å’Œäº”ç§è¯­è¨€æ¨¡å‹ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°MechLightåœ¨æ‰€æœ‰ä¸Šä¸‹æ–‡åœºæ™¯ä¸­è¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ–¹æ³•åœ¨è¾ƒé•¿çš„ä¸Šä¸‹æ–‡ä¸Šéƒ½é‡åˆ°äº†å›°éš¾ï¼Œå¹¶è¡¨ç°å‡ºä½ç½®åè§ï¼Œè¿™æŒ‡å‡ºäº†åœ¨è§£é‡Šå‡†ç¡®æ€§æ–¹é¢çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥å®ç°å¤§è§„æ¨¡å¯é çš„ä¸Šä¸‹æ–‡åˆ©ç”¨è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå“åº”æ—¶å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é—®é¢˜ã€‚å°½ç®¡é«˜äº®è§£é‡Šï¼ˆHEsï¼‰å¯ä»¥ä½œä¸ºæŒ‡å‡ºå½±å“æ¨¡å‹è¾“å‡ºçš„å…·ä½“ä¸Šä¸‹æ–‡ç‰‡æ®µå’Œæ ‡è®°çš„è‡ªç„¶è§£å†³æ–¹æ¡ˆï¼Œä½†ç›®å‰å°šæ— å·¥ä½œè¯„ä¼°å…¶åœ¨è§£é‡Šä¸Šä¸‹æ–‡åˆ©ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥ç¬¬ä¸€ä¸ªé»„é‡‘æ ‡å‡†HEè¯„ä¼°æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¸¦æœ‰å·²çŸ¥åœ°é¢çœŸå®ä¸Šä¸‹æ–‡ä½¿ç”¨çš„å—æ§æµ‹è¯•ç”¨ä¾‹ï¼Œé¿å…äº†ç°æœ‰é—´æ¥ä»£ç†è¯„ä¼°çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹å››ç§HEæ–¹æ³•å’Œå››ç§ä¸Šä¸‹æ–‡åœºæ™¯ã€å››ä¸ªæ•°æ®é›†å’Œäº”ä¸ªè¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°MechLightåœ¨æ‰€æœ‰ä¸Šä¸‹æ–‡åœºæ™¯ä¸­è¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿ä¸Šä¸‹æ–‡æ—¶éƒ½å­˜åœ¨å›°éš¾ï¼Œå¹¶è¡¨ç°å‡ºä½ç½®åè§ï¼Œè¿™è¡¨æ˜åœ¨è§£é‡Šå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æ ¹æœ¬æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥æä¾›å¯é çš„ä¸Šä¸‹æ–‡åˆ©ç”¨è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå“åº”æ—¶çš„ä¸Šä¸‹æ–‡åˆ©ç”¨å¯¹ç”¨æˆ·æ¥è¯´ä»ç„¶ä¸é€æ˜ã€‚</li>
<li>é«˜äº®è§£é‡Šï¼ˆHEsï¼‰æ˜¯è§£é‡Šè¯­è¨€æ¨¡å‹å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¸€ä¸ªè‡ªç„¶è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°HEæ–¹æ³•åœ¨è§£é‡Šä¸Šä¸‹æ–‡åˆ©ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§çš„ç ”ç©¶ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªé»„é‡‘æ ‡å‡†HEè¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨å—æ§æµ‹è¯•ç”¨ä¾‹æ¥è¯„ä¼°HEæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MechLightåœ¨è¯„ä¼°çš„å››ç§HEæ–¹æ³•ä¸­è¢«è®¤ä¸ºæ˜¯è¡¨ç°æœ€ä½³çš„ã€‚</li>
<li>æ‰€æœ‰HEæ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿ä¸Šä¸‹æ–‡æ—¶éƒ½å­˜åœ¨å›°éš¾ï¼Œè¡¨ç°å‡ºä½ç½®åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5002e44efdd8fbf74a5508a3bef93e88~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941524&auth_key=1759941524-0-0-c51cf4c91d3db23a94e9354a74a5e8ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="On-the-Role-of-Temperature-Sampling-in-Test-Time-Scaling"><a href="#On-the-Role-of-Temperature-Sampling-in-Test-Time-Scaling" class="headerlink" title="On the Role of Temperature Sampling in Test-Time Scaling"></a>On the Role of Temperature Sampling in Test-Time Scaling</h2><p><strong>Authors:Yuheng Wu, Azalia Mirhoseini, Thierry Tambe</strong></p>
<p>Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a modelâ€™s potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024&#x2F;2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æ¨ç†æ—¶é—´æé«˜æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ä¼šç”Ÿæˆå¤šä¸ªæ¨ç†è½¨è¿¹å¹¶é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¢åŠ æ ·æœ¬æ•°é‡Kå¯ä»¥ç¨³æ­¥æé«˜å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜è¿™ä¸€è¶‹åŠ¿å¹¶éæ— é™æŒç»­ï¼šåœ¨è¾ƒå¤§çš„Kå€¼ä¸‹ï¼Œè¿›ä¸€æ­¥çš„ç¼©æ”¾å¹¶ä¸ä¼šå¸¦æ¥æ”¶ç›Šï¼Œå¹¶ä¸”æ— è®ºè½¨è¿¹æ•°é‡å¦‚ä½•ï¼ŒæŸäº›éš¾é¢˜ä»ç„¶æ— æ³•è§£å†³ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒçš„é‡‡æ ·æ¸©åº¦å¯ä»¥è§£å†³ä¸åŒçš„é—®é¢˜å­é›†ï¼Œè¿™æ„å‘³ç€å•ä¸€æ¸©åº¦çš„ç¼©æ”¾åªèƒ½æ¢ç´¢æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ½œåŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºæ²¿æ¸©åº¦ç»´åº¦è¿›è¡Œç¼©æ”¾ï¼Œè¿™æ‰©å¤§äº†LLMçš„æ¨ç†è¾¹ç•Œã€‚åœ¨Qwen3ï¼ˆ0.6Bã€1.7Bã€4Bã€8Bï¼‰å’Œäº”ä¸ªä»£è¡¨æ€§æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆAIME 2024&#x2F;2025ã€MATH500ã€LiveCodeBenchã€Hi-ToMï¼‰ä¸Šè¿›è¡Œå¹³å‡ï¼Œæ¸©åº¦ç¼©æ”¾æ¯”å•ä¸€æ¸©åº¦TTSé¢å¤–æé«˜äº†7.3ç‚¹ã€‚æ¸©åº¦ç¼©æ”¾è¿˜ä½¿åŸºç¡€æ¨¡å‹çš„æ€§èƒ½èƒ½å¤Ÿè¾¾åˆ°ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¿‡çš„æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œè€Œæ— éœ€è¿›è¡Œé¢å¤–çš„åè®­ç»ƒã€‚æˆ‘ä»¬å¯¹è¿™ä¸€ç°è±¡è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¤šæ¸©åº¦æŠ•ç¥¨æ–¹æ³•ï¼Œä»¥å‡å°‘æ¸©åº¦ç¼©æ”¾çš„å¼€é”€ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTTSçš„å¨åŠ›æ¯”ä»¥å¾€è®¤ä¸ºçš„è¦å¼ºå¤§å¾—å¤šï¼Œè€Œæ¸©åº¦ç¼©æ”¾æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥è§£é”åŸºç¡€æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02611v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶é—´é€šè¿‡æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å¯ä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å‘ç°ï¼Œéšç€æ ·æœ¬æ•°é‡Kçš„å¢åŠ ï¼Œå‡†ç¡®ç‡ç¨³æ­¥æé«˜çš„è¶‹åŠ¿å¹¶éæ— é™æŒç»­ã€‚åœ¨å¤§Kå€¼ä¸‹ï¼Œè¿›ä¸€æ­¥çš„ç¼©æ”¾å¹¶ä¸ä¼šå¸¦æ¥æ”¶ç›Šï¼Œä¸”æŸäº›éš¾é¢˜ä»ç„¶æ— æ³•è§£å†³ã€‚ä¸åŒé‡‡æ ·æ¸©åº¦å¯ä»¥è§£å†³ä¸åŒçš„é—®é¢˜å­é›†ï¼Œå› æ­¤å•æ¸©åº¦ç¼©æ”¾åªæ¢ç´¢äº†æ¨¡å‹çš„éƒ¨åˆ†æ½œåŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºæ²¿æ¸©åº¦ç»´åº¦è¿›è¡Œç¼©æ”¾ï¼Œä»¥æ‰©å¤§LLMçš„æ¨ç†è¾¹ç•Œã€‚åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šï¼Œæ¸©åº¦ç¼©æ”¾æ¯”å•ä¸€æ¸©åº¦TTSé¢å¤–æé«˜äº†7.3ä¸ªç‚¹çš„æ€§èƒ½ï¼Œå¹¶ä½¿å¾—åŸºç¡€æ¨¡å‹æ€§èƒ½æ¥è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éšç€æ ·æœ¬æ•°é‡Kçš„å¢åŠ ï¼ŒTTSçš„æ•ˆç›Šå¹¶éä¸€ç›´å¢é•¿ã€‚</li>
<li>åœ¨å¤§Kå€¼æ—¶ï¼Œè¿›ä¸€æ­¥çš„ç¼©æ”¾ä¸ä¼šå¸¦æ¥å‡†ç¡®ç‡çš„æå‡ã€‚</li>
<li>ä¸åŒé‡‡æ ·æ¸©åº¦å¯ä»¥è§£å†³ä¸åŒçš„é—®é¢˜å­é›†ã€‚</li>
<li>å•æ¸©åº¦ç¼©æ”¾åªæ¢ç´¢äº†æ¨¡å‹çš„éƒ¨åˆ†æ½œåŠ›ã€‚</li>
<li>æ¸©åº¦ç»´åº¦ç¼©æ”¾å¯ä»¥æ‰©å¤§LLMçš„æ¨ç†è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-932f9d56cdb351fa0b7ce076b634138c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c283b73c954c1e5366dbf0b5891cc9b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941539&auth_key=1759941539-0-0-9dc3c94fa60685c50c2d8743c383dfb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4439b891848107dde7992622046f3d93~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941546&auth_key=1759941546-0-0-12f3bbf70a6f6144790c17cb7f4fc9d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff70336f49c5e536ef29a5e80c571cee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941552&auth_key=1759941552-0-0-5f208cc1870b8631c997d65b649ac04b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ef1a869ae37903560b1d9153e829292~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941559&auth_key=1759941559-0-0-e292f0aefefb2a45601badf4e90fa921&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42366b44bcae6fa1247b72acfc2b099d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941566&auth_key=1759941566-0-0-248517f32fe44f226381ab892b10659b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Oracle-RLAIF-An-Improved-Fine-Tuning-Framework-for-Multi-modal-Video-Models-through-Reinforcement-Learning-from-Ranking-Feedback"><a href="#Oracle-RLAIF-An-Improved-Fine-Tuning-Framework-for-Multi-modal-Video-Models-through-Reinforcement-Learning-from-Ranking-Feedback" class="headerlink" title="Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video   Models through Reinforcement Learning from Ranking Feedback"></a>Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video   Models through Reinforcement Learning from Ranking Feedback</h2><p><strong>Authors:Derek Shi, Ruben Glatt, Christine Klymko, Shubham Mohole, Hongjun Choi, Shashank Kushwaha, Sam Sakla, Felipe Leno da Silva</strong></p>
<p>Recent advances in large video-language models (VLMs) rely on extensive fine-tuning techniques that strengthen alignment between textual and visual comprehension. Leading pipelines typically pair supervised fine-tuning (SFT) with reinforcement learning from preference data to enhance video comprehension. However, as VLMs scale in parameter size, so does the cost of gathering enough human feedback. To make fine-tuning more cost-effective, recent frameworks explore reinforcement learning with AI feedback (RLAIF), which replace human preference with AI as a judge. Current RLAIF frameworks rely on a specialized reward model trained with video narratives to create calibrated scalar rewards â€“ an expensive and restrictive pipeline. We propose Oracle-RLAIF, a novel framework that replaces the trained reward model with a more general Oracle ranker which acts as a drop-in model ranking candidate model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce $GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware advantages. Empirically, we demonstrate that Oracle-RLAIF consistently outperforms leading VLMs using existing fine-tuning methods when evaluated across various video comprehension benchmarks. Oracle-RLAIF paves the path to creating flexible and data-efficient frameworks for aligning large multi-modal video models with reinforcement learning from rank rather than score. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€æ–°è¿›å±•ä¾èµ–äºå¹¿æ³›çš„å¾®è°ƒæŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯åŠ å¼ºäº†æ–‡æœ¬å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å¯¹é½ã€‚é¢†å…ˆçš„ç®¡é“é€šå¸¸å°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ä»åå¥½æ•°æ®ä¸­å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥å¢å¼ºè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œéšç€VLMsçš„å‚æ•°è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œæ”¶é›†è¶³å¤Ÿçš„äººç±»åé¦ˆçš„æˆæœ¬ä¹Ÿåœ¨å¢åŠ ã€‚ä¸ºäº†ä½¿å¾®è°ƒæ›´å…·æˆæœ¬æ•ˆç›Šï¼Œæœ€è¿‘çš„æ¡†æ¶æ¢ç´¢äº†ä½¿ç”¨AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ï¼Œç”¨AIä½œä¸ºè£åˆ¤å–ä»£äººç±»åå¥½ã€‚å½“å‰çš„RLAIFæ¡†æ¶ä¾èµ–äºç”¨è§†é¢‘å™äº‹è®­ç»ƒçš„ä¸“ç”¨å¥–åŠ±æ¨¡å‹æ¥åˆ›å»ºæ ¡å‡†æ ‡é‡å¥–åŠ±ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜‚è´µä¸”å—é™çš„ç®¡é“ã€‚æˆ‘ä»¬æå‡ºäº†Oracle-RLAIFè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç”¨æ›´é€šç”¨çš„Oracleæ’åè€…å–ä»£è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å‹æ¥æ’åå€™é€‰æ¨¡å‹å“åº”è€Œéå¯¹å…¶æ‰“åˆ†ã€‚é™¤äº†Oracle-RLAIFï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰çš„$GRPO_{rank}$æ–°å‹åŸºäºæ’åçš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç›´æ¥ä¼˜åŒ–åŸºäºæ’åçš„ä¼˜åŠ¿åºæ•°åé¦ˆã€‚ä»å®è¯ä¸Šçœ‹ï¼Œåœ¨å¤šç§è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOracle-RLAIFçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„é¢†å…ˆVLMsä½¿ç”¨çš„ä¸»è¦å¾®è°ƒæ–¹æ³•ã€‚Oracle-RLAIFä¸ºåˆ›å»ºçµæ´»ä¸”æ•°æ®é«˜æ•ˆçš„å¤§å‹å¤šæ¨¡å¼è§†é¢‘æ¨¡å‹æ¡†æ¶é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›æ¡†æ¶å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»æ’åè€Œéåˆ†æ•°ä¸­è¿›è¡Œå¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02561v1">PDF</a> Proceedings of the 39th Annual Conference on Neural Information   Processing Systems, ARLET Workshop (Aligning Reinforcement Learning   Experimentalists and Theorists)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€æ–°è¿›å±•ä¾èµ–äºå¹¿æ³›çš„å¾®è°ƒæŠ€æœ¯ï¼Œä»¥åŠ å¼ºæ–‡æœ¬å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å¯¹é½ã€‚ä¸»æµç®¡é“é€šå¸¸ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œåå¥½æ•°æ®çš„å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œéšç€VLMsçš„å‚æ•°è§„æ¨¡æ‰©å¤§ï¼Œæ”¶é›†è¶³å¤Ÿäººç±»åé¦ˆçš„æˆæœ¬ä¹Ÿåœ¨ä¸Šå‡ã€‚ä¸ºäº†é™ä½å¾®è°ƒçš„æˆæœ¬ï¼Œæœ€è¿‘çš„æ¡†æ¶æ¢ç´¢äº†ä½¿ç”¨AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ï¼Œç”¨AIä½œä¸ºè¯„åˆ¤å‘˜ä»£æ›¿äººç±»åå¥½ã€‚æˆ‘ä»¬æå‡ºäº†Oracle-RLAIFè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œç”¨æ›´é€šç”¨çš„Oracleæ’åå™¨å–ä»£è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œå¯¹å€™é€‰æ¨¡å‹å“åº”è¿›è¡Œæ’åè€Œéè¯„åˆ†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºé›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„$GRPO_{rank}$æ–°å‹æ’åæŸå¤±å‡½æ•°ï¼Œç›´æ¥ä¼˜åŒ–åºæ•°åé¦ˆå’Œæ’åä¼˜åŠ¿ã€‚ç»éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOracle-RLAIFçš„è¡¨ç°å§‹ç»ˆä¼˜äºé‡‡ç”¨ç°æœ‰å¾®è°ƒæ–¹æ³•çš„å¤§å‹VLMsã€‚Oracle-RLAIFä¸ºåˆ›å»ºçµæ´»ä¸”æ•°æ®é«˜æ•ˆçš„æ¡†æ¶é“ºå¹³äº†é“è·¯ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨æ’åè€Œéè¯„åˆ†æ¥å¼ºåŒ–å¤§å‹å¤šæ¨¡å¼è§†é¢‘æ¨¡å‹çš„å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ æé«˜è§†é¢‘ç†è§£æ€§èƒ½ã€‚</li>
<li>éšç€VLMsè§„æ¨¡å¢é•¿ï¼Œäººç±»åé¦ˆæˆæœ¬ä¸Šå‡ï¼Œéœ€è¦æ›´ç»æµçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ¡†æ¶é€šè¿‡AIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰é™ä½å¾®è°ƒæˆæœ¬ï¼Œä½†ä¾èµ–ç‰¹å®šå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>Oracle-RLAIFæå‡ºç”¨é€šç”¨Oracleæ’åå™¨å–ä»£è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿›è¡Œæ¨¡å‹å“åº”æ’åã€‚</li>
<li>$GRPO_{rank}$æ˜¯ä¸€ç§æ–°å‹æ’åæŸå¤±å‡½æ•°ï¼ŒåŸºäºé›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä¼˜åŒ–åºæ•°åé¦ˆå’Œæ’åä¼˜åŠ¿ã€‚</li>
<li>Oracle-RLAIFåœ¨å„ç§è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3607494e6164a942b40d1135fc767f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941573&auth_key=1759941573-0-0-90c008d876b3774c7dff3e7a540ca33b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c9932413d1840ddafcc65ca89f74fd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941580&auth_key=1759941580-0-0-4b98aa9312da7fdc89943b384e7d4928&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fd4f5490d048374dae14ba265424e76~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941587&auth_key=1759941587-0-0-3cffb83daab9a96b459ba3a5575be296&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Orchestrating-Human-AI-Teams-The-Manager-Agent-as-a-Unifying-Research-Challenge"><a href="#Orchestrating-Human-AI-Teams-The-Manager-Agent-as-a-Unifying-Research-Challenge" class="headerlink" title="Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research   Challenge"></a>Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research   Challenge</h2><p><strong>Authors:Charlie Masters, Advaith Vellanki, Jiangbo Shangguan, Bart Kultys, Jonathan Gilmore, Alastair Moore, Stefano V. Albrecht</strong></p>
<p>While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime - underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems. </p>
<blockquote>
<p>å…³äºä»£ç†äººå·¥æ™ºèƒ½ï¼ˆagentic AIï¼‰å·²ç»å®ç°äº†è‡ªåŠ¨åŒ–å•ä¸ªä»»åŠ¡æ–¹é¢çš„è¿›æ­¥ï¼Œä½†ç®¡ç†å¤æ‚çš„è·¨ä»£ç†å·¥ä½œæµç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†è‡ªä¸»ä»£ç†ç³»ç»Ÿçš„ç ”ç©¶æ„¿æ™¯ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨åŠ¨æ€çš„äººæœºå›¢é˜Ÿä¸­åè°ƒåˆä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šè‡ªä¸»ç®¡ç†ä»£ç†ï¼ˆAgentï¼‰ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿå°†å¤æ‚çš„ç›®æ ‡åˆ†è§£ä¸ºä»»åŠ¡å›¾ï¼Œåˆ†é…ç»™äººç±»å’Œäººå·¥æ™ºèƒ½å·¥ä½œè€…ï¼Œç›‘æ§è¿›åº¦ï¼Œé€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒï¼Œå¹¶ä¿æŒä¸åˆ©ç›Šç›¸å…³è€…çš„é€æ˜æ²Ÿé€šã€‚æˆ‘ä»¬å°†å·¥ä½œæµç¨‹ç®¡ç†å½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚å¯Ÿéšæœºåšå¼ˆï¼Œå¹¶ç¡®å®šäº†å››ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å±‚æ¬¡åˆ†è§£çš„ç»„åˆæ¨ç†ï¼Œï¼ˆ2ï¼‰å¤šå˜åå¥½ä¸‹çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼Œï¼ˆ3ï¼‰éç‰¹å®šå›¢é˜Ÿçš„åè°ƒä¸è§„åˆ’ï¼Œï¼ˆ4ï¼‰è®¾è®¡ä¸éµå¾ªæ³•è§„æ”¿ç­–ã€‚ä¸ºäº†æ¨è¿›è¿™ä¸€è®®ç¨‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MA-Gymè¿™ä¸€å¼€æºæ¨¡æ‹Ÿå’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¤šä»£ç†å·¥ä½œæµç¨‹ç¼–æ’ã€‚é€šè¿‡å¯¹GPT-5å‹ç®¡ç†ä»£ç†åœ¨20ä¸ªå·¥ä½œæµç¨‹ä¸­çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ä»–ä»¬åœ¨å…±åŒä¼˜åŒ–ç›®æ ‡å®Œæˆã€çº¦æŸéµå®ˆå’Œå·¥ä½œæµç¨‹è¿è¡Œæ—¶é—´æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè¿™å‡¸æ˜¾äº†å·¥ä½œæµç¨‹ç®¡ç†çš„å›°éš¾æ€§å’Œå¼€æ”¾æ€§æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†è‡ªä¸»ç®¡ç†ç³»ç»Ÿåœ¨ç»„ç»‡ä¼¦ç†æ–¹é¢çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02557v1">PDF</a> Accepted as an oral paper for the conference for Distributed   Artificial Intelligence (DAI 2025). 8 pages, 2 figures</p>
<p><strong>Summary</strong><br>æ™ºèƒ½è‡ªä¸»ç®¡ç†ç³»ç»Ÿæ˜¯å½“ä»Šäººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŠ¨æ€äººæœºåä½œä¸­çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç®¡ç†ç³»ç»Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¦‚å¿µâ€”â€”â€œè‡ªä¸»ç®¡ç†å™¨ä»£ç†â€ã€‚å®ƒè´Ÿè´£åˆ†è§£å¤æ‚çš„ä»»åŠ¡ç›®æ ‡ã€åˆ†é…ä»»åŠ¡ã€ç›‘æ§è¿›åº¦ã€é€‚åº”ç¯å¢ƒå˜åŒ–ä»¥åŠä¿æŒé€æ˜çš„åˆ©ç›Šç›¸å…³è€…æ²Ÿé€šã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å°†å…¶å½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚å¯Ÿçš„éšæœºåšå¼ˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†å››å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šå±‚æ¬¡ç»“æ„çš„ç»„åˆæ¨ç†ã€å˜åŒ–çš„åå¥½ä¸‹çš„å¤šç›®æ ‡ä¼˜åŒ–ã€ä¸´æ—¶å›¢é˜Ÿçš„åè°ƒå’Œè§„åˆ’ã€è®¾è®¡åŒ–çš„æ²»ç†å’Œåˆè§„æ€§ã€‚æœ¬æ–‡æœ€ç»ˆé€šè¿‡å®éªŒæ¢è®¨äº†è‡ªä¸»ç®¡ç†ç³»ç»Ÿçš„ç»„ç»‡å’Œç¤¾ä¼šå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½è‡ªä¸»ç®¡ç†ç³»ç»Ÿæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€é¡¹é‡è¦æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³å¤æ‚çš„å¤šä»»åŠ¡å·¥ä½œæµç®¡ç†é—®é¢˜ã€‚</li>
<li>è‡ªä¸»ç®¡ç†å™¨ä»£ç†æ˜¯è¯¥é¢†åŸŸçš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿåˆ†è§£ä»»åŠ¡ã€åˆ†é…å·¥ä½œã€ç›‘æ§è¿›åº¦ç­‰ã€‚</li>
<li>å·¥ä½œæµç®¡ç†è¢«å½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚å¯Ÿçš„éšæœºåšå¼ˆé—®é¢˜ï¼Œéœ€è¦è§£å†³å››å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>å±‚æ¬¡ç»“æ„çš„ç»„åˆæ¨ç†æ˜¯å…¶ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„ä»»åŠ¡ç»“æ„ã€‚</li>
<li>åœ¨å˜åŒ–çš„åå¥½ä¸‹å®ç°å¤šç›®æ ‡ä¼˜åŒ–ä¹Ÿæ˜¯ä¸€å¤§éš¾é¢˜ï¼Œéœ€è¦æ»¡è¶³ä¸åŒçš„éœ€æ±‚å’Œä¼˜å…ˆçº§ã€‚</li>
<li>è‡ªä¸»ç®¡ç†ç³»ç»Ÿçš„åè°ƒä¸è§„åˆ’æ¶‰åŠåˆ°ä¸´æ—¶å›¢é˜Ÿçš„åˆä½œé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6991e8c01b1db0b15b8f14e1555758e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941595&auth_key=1759941595-0-0-7af60f493695c10db8630897f4b24e2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-99ceae1cc6a1f0ab929bfffcf527c056.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Do-AI-Models-Perform-Human-like-Abstract-Reasoning-Across-Modalities"><a href="#Do-AI-Models-Perform-Human-like-Abstract-Reasoning-Across-Modalities" class="headerlink" title="Do AI Models Perform Human-like Abstract Reasoning Across Modalities?"></a>Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</h2><p><strong>Authors:Claas Beger, Ryan Yi, Shuhao Fu, Arseny Moskvichev, Sarah W. Tsai, Sivasankaran Rajamanickam, Melanie Mitchell</strong></p>
<p>OpenAIâ€™s o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate modelsâ€™ abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best modelsâ€™ rules are often based on surface-level &#96;&#96;shortcutsâ€™â€™ and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI modelsâ€™ output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal modelsâ€™ abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence. </p>
<blockquote>
<p>OpenAIçš„o3-previewæ¨ç†æ¨¡å‹åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†äººç±»çš„å‡†ç¡®ç‡ï¼Œä½†è¿™æ˜¯å¦æ„å‘³ç€æœ€å…ˆè¿›çš„æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶æ¨ç†ä»»åŠ¡åˆ›å»ºè€…æ‰€æ„å›¾çš„æŠ½è±¡æ¦‚å¿µå‘¢ï¼Ÿæˆ‘ä»¬åœ¨ConceptARCä¸Šè°ƒæŸ¥äº†æ¨¡å‹çš„æŠ½è±¡èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬è¾“å…¥æ¨¡å¼ï¼ˆæ–‡æœ¬ä¸è§†è§‰ï¼‰ã€æ¨¡å‹æ˜¯å¦å¯ä»¥ä½¿ç”¨å¤–éƒ¨Pythonå·¥å…·ï¼Œä»¥åŠå¯¹äºæ¨ç†æ¨¡å‹æ¥è¯´ï¼Œæ¨ç†åŠªåŠ›çš„ç¨‹åº¦ã€‚é™¤äº†æµ‹é‡è¾“å‡ºå‡†ç¡®ç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ¨¡å‹ç”Ÿæˆçš„ç”¨äºè§£é‡Šå…¶è§£å†³æ–¹æ¡ˆçš„è‡ªç„¶è¯­è¨€è§„åˆ™è¿›è¡Œäº†ç²¾ç»†è¯„ä¼°ã€‚è¿™ç§åŒé‡è¯„ä¼°ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹æ˜¯å¦ä½¿ç”¨ConceptARCè®¾è®¡çš„æŠ½è±¡æ¦‚å¿µæ¥è§£å†³é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¾èµ–äºè¡¨é¢æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºçš„æ¨¡å‹ä¸äººç±»è¾“å‡ºå‡†ç¡®ç‡ç›¸åŒ¹é…ï¼Œä½†æœ€ä½³æ¨¡å‹çš„è§„åˆ™é€šå¸¸åŸºäºè¡¨é¢â€œæ·å¾„â€ï¼Œå¹¶ä¸”æ•è·çš„æ„å›¾æŠ½è±¡æ¦‚å¿µè¿œä¸åŠäººç±»ã€‚å› æ­¤ï¼Œä»…é€šè¿‡å‡†ç¡®æ€§è¯„ä¼°ï¼Œå®ƒä»¬çš„ä¸€èˆ¬æŠ½è±¡æ¨ç†èƒ½åŠ›å¯èƒ½è¢«é«˜ä¼°äº†ã€‚åœ¨è§†è§‰æ¨¡å¼ä¸‹ï¼ŒAIæ¨¡å‹çš„è¾“å‡ºå‡†ç¡®ç‡æ€¥å‰§ä¸‹é™ï¼Œä½†æˆ‘ä»¬çš„è§„åˆ™å±‚é¢åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å¯èƒ½è¢«ä½ä¼°äº†ï¼Œå› ä¸ºå®ƒä»¬ä»ç„¶è¡¨ç°å‡ºå¤§é‡èƒ½å¤Ÿæ•è·æ„å›¾æŠ½è±¡çš„è§„åˆ™ï¼Œä½†å¾€å¾€æ— æ³•æ­£ç¡®åº”ç”¨è¿™äº›è§„åˆ™ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨æŠ½è±¡æ¨ç†æ–¹é¢ä»ç„¶è½åäºäººç±»ï¼Œå¹¶ä¸”å•ç‹¬ä½¿ç”¨å‡†ç¡®æ€§æ¥è¯„ä¼°ARCç±»ä»»åŠ¡ä¸Šçš„æŠ½è±¡æ¨ç†å¯èƒ½ä¼šé«˜ä¼°æ–‡æœ¬æ¨¡å¼ä¸­çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½ä¼°è§†è§‰æ¨¡å¼ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶èƒ½å¤Ÿæ›´çœŸå®åœ°åæ˜ å¤šæ¨¡å¼æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ï¼Œå¹¶æä¾›ä¸€ç§æ›´åŸåˆ™æ€§çš„æ–¹æ³•æ¥è·Ÿè¸ªå‘äººç±»æŠ½è±¡ä¸ºä¸­å¿ƒçš„æ™ºèƒ½çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02125v2">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºOpenAIçš„o3-previewæ¨ç†æ¨¡å‹åœ¨ARC-AGIåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°çš„æŠ¥å‘Šã€‚æŠ¥å‘ŠæŒ‡å‡ºï¼Œå°½ç®¡è¯¥æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†äººç±»ï¼Œä½†åœ¨å¤„ç†æŠ½è±¡æ¦‚å¿µæ—¶ä»æœ‰æ‰€æ¬ ç¼ºã€‚æŠ¥å‘Šé€šè¿‡ConceptARCè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå‘ç°æ¨¡å‹åœ¨æŸäº›æƒ…å¢ƒä¸‹è¿‡äºä¾èµ–è¡¨é¢æ¨¡å¼è€Œå¿½è§†æ·±å±‚æŠ½è±¡æ¦‚å¿µã€‚åŒæ—¶ï¼ŒæŠ¥å‘Šè¿˜æå‡ºä¸€ç§è¯„ä»·æ¡†æ¶ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenAIçš„o3-previewæ¨ç†æ¨¡å‹åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†äººç±»ã€‚</li>
<li>åœ¨å¤„ç†æŠ½è±¡æ¦‚å¿µæ—¶ï¼Œè¯¥æ¨¡å‹çš„èƒ½åŠ›å¯èƒ½å¹¶æœªè¾¾åˆ°é¢„æœŸæ°´å¹³ã€‚</li>
<li>æ¨¡å‹åœ¨æŸäº›æƒ…å¢ƒä¸‹è¿‡äºä¾èµ–è¡¨é¢æ¨¡å¼è€Œå¿½è§†æ·±å±‚æŠ½è±¡æ¦‚å¿µã€‚</li>
<li>æ¨¡å‹åœ¨è§†è§‰æ¨¡æ€ä¸‹çš„è¡¨ç°è¾ƒæ–‡æœ¬æ¨¡æ€æ›´ä¸ºå¤æ‚ï¼Œéœ€è¦åœ¨è§„åˆ™åº”ç”¨ä¸Šå¾—åˆ°æ›´ç²¾ç»†çš„è¯„ä»·ã€‚</li>
<li>ä½¿ç”¨å•ä¸€çš„è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®åº¦ï¼‰å¯èƒ½ä¼šè¿‡é«˜æˆ–è¿‡ä½åœ°ä¼°è®¡æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼Œèƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bde82ce66cbff65426798cf526238bef~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941610&auth_key=1759941610-0-0-4c51262375904daeeb8ef707f4fe0896&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6a67c2e9ba4ad8c0b4aaa931fa4ba9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941617&auth_key=1759941617-0-0-3826b3263877abe37679c4bca3384392&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c14f05c7dfb19643906017eed4dbb8f0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Model-Reasoning-with-Reward-Models-An-Analytical-Survey"><a href="#Enhancing-Large-Language-Model-Reasoning-with-Reward-Models-An-Analytical-Survey" class="headerlink" title="Enhancing Large Language Model Reasoning with Reward Models: An   Analytical Survey"></a>Enhancing Large Language Model Reasoning with Reward Models: An   Analytical Survey</h2><p><strong>Authors:Qiyuan Liu, Hao Xu, Xuhong Chen, Wei Chen, Yee Whye Teh, Ning Miao</strong></p>
<p>Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we discuss critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½ä¸Šèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬å¯ä»¥åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœŸé—´ä¸ºLLMsæä¾›è®­ç»ƒä¿¡å·ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸®åŠ©ä»å¤šä¸ªå€™é€‰ç­”æ¡ˆä¸­é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹RMsè¿›è¡Œäº†ç³»ç»Ÿä»‹ç»ï¼Œå¹¶å…¨é¢æ¦‚è¿°äº†å®ƒä»¬åœ¨LLMæ¨ç†ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆå›é¡¾RMsçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬å…¶æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢è®¨äº†å®ƒä»¬çš„å…³é”®åº”ç”¨ï¼šï¼ˆ1ï¼‰åœ¨LLMæ¨ç†è¿‡ç¨‹ä¸­æŒ‡å¯¼ç”Ÿæˆå¹¶é€‰æ‹©æœ€ä½³è¾“å‡ºï¼Œï¼ˆ2ï¼‰ä¿ƒè¿›æ•°æ®åˆæˆå’ŒLLMçš„è¿­ä»£è‡ªæˆ‘æ”¹è¿›ï¼Œï¼ˆ3ï¼‰åœ¨åŸºäºRLçš„å¾®è°ƒä¸­æä¾›è®­ç»ƒä¿¡å·ã€‚æœ€åï¼Œæˆ‘ä»¬åŸºäºç°æœ‰ç ”ç©¶å’Œè‡ªå·±çš„å®è¯å‘ç°ï¼Œè®¨è®ºäº†å…³äºRMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œæ”¹è¿›çš„å…³é”®å¼€æ”¾é—®é¢˜ã€‚æˆ‘ä»¬çš„åˆ†ææ—¨åœ¨æä¾›å…³äºRMsåœ¨LLMæ¨ç†ä¸­çš„æœ‰æ•ˆéƒ¨ç½²å’Œè¿›æ­¥çš„å¯è¡Œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01925v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ€§èƒ½å…·æœ‰é‡è¦ä½œç”¨ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ä»‹ç»äº†RMsåŠå…¶åœ¨å¤šé¢†åŸŸè¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„åº”ç”¨ã€‚å†…å®¹åŒ…æ‹¬RMsçš„åŸºæœ¬æ¦‚å¿µã€æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ï¼Œä»¥åŠå…¶å…³é”®åº”ç”¨ï¼Œå¦‚æŒ‡å¯¼ç”Ÿæˆå’Œé€‰æ‹©æœ€ä½³è¾“å‡ºã€ä¿ƒè¿›æ•°æ®åˆæˆå’Œè¿­ä»£è‡ªæ”¹è¿›ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„è®­ç»ƒä¿¡å·æä¾›ç­‰ã€‚åŒæ—¶ï¼Œæ ¹æ®ç°æœ‰ç ”ç©¶å’Œè‡ªèº«ç»éªŒæ¢è®¨äº†RMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œå¢å¼ºç­‰å…³é”®é—®é¢˜ã€‚æ—¨åœ¨æä¾›RMsåœ¨LLMæ¨ç†ä¸­çš„æœ‰æ•ˆéƒ¨ç½²å’Œè¿›æ­¥çš„å®ç”¨è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†æ€§èƒ½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>RMså¯ä»¥é€šè¿‡æä¾›è®­ç»ƒä¿¡å·æ¥å¾®è°ƒLLMsï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸®åŠ©é€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚</li>
<li>æœ¬æ–‡å…¨é¢å›é¡¾äº†RMsçš„åŸºæœ¬æ¦‚å¿µã€æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚</li>
<li>RMsçš„å…³é”®åº”ç”¨åŒ…æ‹¬æŒ‡å¯¼ç”Ÿæˆå’Œé€‰æ‹©æœ€ä½³è¾“å‡ºã€ä¿ƒè¿›æ•°æ®åˆæˆå’ŒLLMçš„è¿­ä»£è‡ªæ”¹è¿›ã€‚</li>
<li>RMsåœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­æä¾›è®­ç»ƒä¿¡å·ï¼Œè¿™æ˜¯æå‡LLMæ€§èƒ½çš„é‡è¦æ‰‹æ®µã€‚</li>
<li>ç›®å‰å…³äºRMsçš„é€‰æ‹©ã€æ³›åŒ–ã€è¯„ä¼°å’Œå¢å¼ºç­‰å…³é”®é—®é¢˜ä»å­˜åœ¨äº‰è®®ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨ä¸ºRMsåœ¨LLMæ¨ç†ä¸­çš„æœ‰æ•ˆéƒ¨ç½²å’Œè¿›æ­¥æä¾›å®ç”¨è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b94637ba53df62728f4ffa7c7d83b399.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-091b5852c251f8586dcc4ee3a1608347~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941639&auth_key=1759941639-0-0-27745393ce8f4321edc6c624d4da093a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b38775bc8ceb4fb76111294b8160752b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941645&auth_key=1759941645-0-0-32b6d822c9822b7234a985cdecbbb255&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b031708e4999e1def7dee9ec0784d782~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941652&auth_key=1759941652-0-0-c24d1e47b7ca4bb3710b84faa12123d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7d03dbc64358d200f7fda4601a954148.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  LEAML Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2465ace51409c291120c1b39011ea43e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029122&auth_key=1760029122-0-0-b0a7d129713c39ccde14d4bcb676ed1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-06  Brain Tumor Classification on MRI in Light of Molecular Markers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
