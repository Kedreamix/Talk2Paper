<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-10-07  Evaluation of preprocessing pipelines in the creation of in-the-wild TTS   datasets">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5b4f47c12030ca1ab313f56172ebd172~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944279&auth_key=1759944279-0-0-3d8bb965d24ac47528d3f50cc1f53dc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-07-更新"><a href="#2025-10-07-更新" class="headerlink" title="2025-10-07 更新"></a>2025-10-07 更新</h1><h2 id="Evaluation-of-preprocessing-pipelines-in-the-creation-of-in-the-wild-TTS-datasets"><a href="#Evaluation-of-preprocessing-pipelines-in-the-creation-of-in-the-wild-TTS-datasets" class="headerlink" title="Evaluation of preprocessing pipelines in the creation of in-the-wild TTS   datasets"></a>Evaluation of preprocessing pipelines in the creation of in-the-wild TTS   datasets</h2><p><strong>Authors:Matías Di Bernardo, Emmanuel Misley, Ignacio Correa, Mateo García Iacovelli, Simón Mellino, Gala Lucía Gonzalez Barrios</strong></p>
<p>This work introduces a reproducible, metric-driven methodology to evaluate preprocessing pipelines for in-the-wild TTS corpora generation. We apply a custom low-cost pipeline to the first in-the-wild Argentine Spanish collection and compare 24 pipeline configurations combining different denoising and quality filtering variants. Evaluation relies on complementary objective measures (PESQ, SI-SDR, SNR), acoustic descriptors (T30, C50), and speech-preservation metrics (F0-STD, MCD). Results expose trade-offs between dataset size, signal quality, and voice preservation; where denoising variants with permissive filtering provide the best overall compromise for our testbed. The proposed methodology allows selecting pipeline configurations without training TTS models for each subset, accelerating and reducing the cost of preprocessing development for low-resource settings. </p>
<blockquote>
<p>本文介绍了一种可复制的、以度量驱动的方法，用于评估用于野外TTS语料库生成的预处理管道。我们将自定义的低成本管道应用于第一个野外阿根廷西班牙收集集，并比较了结合不同降噪和质量过滤方案的24种管道配置。评估依赖于补充的客观度量（PESQ、SI-SDR、SNR）、声学描述符（T30、C50）和语音保存度量（F0-STD、MCD）。结果揭示了数据集大小、信号质量和语音保留之间的权衡；其中允许过滤的降噪变体为测试台提供了最佳的总体平衡。所提出的方法允许在选择管道配置时无需针对每个子集训练TTS模型，从而加快并降低低资源设置下预处理开发的成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03111v1">PDF</a> 5 pages, 4 figures, Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种可复现的、以度量标准驱动的方法，用于评估用于野生TTS语料库生成的预处理管道。通过对首个野生阿根廷西班牙语集合应用自定义的低成本管道，并比较了结合不同降噪和质量过滤变体的24种管道配置。评估依赖于多种客观度量标准（如PESQ、SI-SDR和SNR）、声学描述符（如T30和C50）以及语音保留度量（如F0-STD和MCD）。结果揭示了数据集大小、信号质量和语音保留之间的权衡；其中采用宽松过滤的降噪变体为测试平台提供了最佳总体折衷方案。所提出的方法允许在选择管道配置时无需针对每个子集训练TTS模型，从而加快并降低低资源环境下的预处理开发成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种评估预处理管道的新方法，该方法针对野生TTS语料库生成。</li>
<li>对阿根廷西班牙语集合进行了自定义低成本管道处理。</li>
<li>对比了多种管道配置，涉及不同的降噪和质量过滤变体。</li>
<li>评估基于客观度量标准、声学描述符和语音保留度量。</li>
<li>研究发现数据集大小、信号质量和语音保留之间存在权衡。</li>
<li>宽松过滤的降噪变体在测试平台上表现出最佳总体折衷。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03111">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-777575bdd44cf013e4ec979b98d99302~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944287&auth_key=1759944287-0-0-c91fa74223c451ce0a77eda1582185bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-a320023d75c077ff2bf67716a331f9b6.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8b93ad6e28742ceaf383212e7691f7c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944301&auth_key=1759944301-0-0-6d2201409cf6a8a5f66ede432ddbf7cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Flamed-TTS-Flow-Matching-Attention-Free-Models-for-Efficient-Generating-and-Dynamic-Pacing-Zero-shot-Text-to-Speech"><a href="#Flamed-TTS-Flow-Matching-Attention-Free-Models-for-Efficient-Generating-and-Dynamic-Pacing-Zero-shot-Text-to-Speech" class="headerlink" title="Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating   and Dynamic Pacing Zero-shot Text-to-Speech"></a>Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating   and Dynamic Pacing Zero-shot Text-to-Speech</h2><p><strong>Authors:Hieu-Nghia Huynh-Nguyen, Huynh Nguyen Dang, Ngoc-Son Nguyen, Van Nguyen</strong></p>
<p>Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling models to synthesize speech from text using short, limited-context prompts. These prompts serve as voice exemplars, allowing the model to mimic speaker identity, prosody, and other traits without extensive speaker-specific data. Although recent approaches incorporating language models, diffusion, and flow matching have proven their effectiveness in zero-shot TTS, they still encounter challenges such as unreliable synthesis caused by token repetition or unexpected content transfer, along with slow inference and substantial computational overhead. Moreover, temporal diversity-crucial for enhancing the naturalness of synthesized speech-remains largely underexplored. To address these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that emphasizes low computational cost, low latency, and high speech fidelity alongside rich temporal diversity. To achieve this, we reformulate the flow matching training paradigm and incorporate both discrete and continuous representations corresponding to different attributes of speech. Experimental results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms of intelligibility, naturalness, speaker similarity, acoustic characteristics preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4% compared to the leading zero-shot TTS baselines, while maintaining low latency in inference and high fidelity in generated speech. Code and audio samples are available at our demo page <a target="_blank" rel="noopener" href="https://flamed-tts.github.io/">https://flamed-tts.github.io</a>. </p>
<blockquote>
<p>零样本文本转语音（TTS）最近取得了显著进展，使得模型能够使用简短、有限上下文提示来从文本合成语音。这些提示充当语音范例，使模型能够在没有大量特定说话人数据的情况下模仿说话人的身份、语调和其他特征。虽然最近采用语言模型、扩散和流程匹配的方法在零样本TTS中证明了其有效性，但它们仍然面临挑战，例如由令牌重复或意外内容传输引起的合成不可靠，以及推理速度慢和计算开销大。此外，对于增强合成语音的自然性至关重要的时间多样性仍很大程度上未被探索。为了应对这些挑战，我们提出了Flamed-TTS，这是一种新的零样本TTS框架，侧重于低计算成本、低延迟、高语音保真度和丰富的时间多样性。为了实现这一点，我们重新制定了流程匹配训练范式，并融入了与语音不同属性相对应的离散和连续表示。实验结果表明，Flamed-TTS在可理解性、自然性、说话人相似性、声学特性保持和动态节奏方面超越了最新模型。值得注意的是，Flamed-TTS的词错误率（WER）达到了领先的零样本TTS基准测试的4%，同时在推理过程中保持了低延迟和生成的语音的高保真度。代码和音频样本可在我们的演示页面<a target="_blank" rel="noopener" href="https://flamed-tts.github.io上找到./">https://flamed-tts.github.io上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02848v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>零短文本到语音（TTS）技术近期取得了显著进展，能够通过简短、有限语境的提示来合成语音。近期采用语言模型、扩散和流程匹配等技术的方法虽然有效，但仍面临合成不可靠、内容转移意外、推理速度慢和计算开销大等挑战。重视计算成本低、延迟低、语音保真度高以及时间多样性丰富的Flamed-TTS框架被提出以解决这些问题。实验结果显示，Flamed-TTS在清晰度、自然度、说话人相似度、声学特性保持力和动态速度等方面超过了现有模型。该框架实现了最低的词错误率（WER）为4%，同时保持了推理阶段的低延迟和生成的语音高保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零短文本到语音（TTS）技术已显著进步，可通过简短提示合成语音。</li>
<li>当前方法虽有效，但仍面临合成不可靠、推理速度慢和计算开销大的挑战。</li>
<li>Flamed-TTS框架旨在解决这些问题，强调低计算成本、低延迟和高语音保真度。</li>
<li>Flamed-TTS结合离散和连续表示，对应语音的不同属性。</li>
<li>实验结果显示Flamed-TTS在多个方面超越现有模型，包括清晰度、自然度等。</li>
<li>Flamed-TTS实现了最低的词错误率（WER）为4%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02848">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4dc55e5c2509cd5926c41b1541fc862f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944309&auth_key=1759944309-0-0-15393545816af2d51b7b4452bf0fa30e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-af775b2eddddff48e346b18d3366bc8b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8d08a479c3ffa3d6cc57f24211cb1bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944323&auth_key=1759944323-0-0-3d472fd7f7e792cbd1b65b735267e228&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78cc2fb1f6463d6cbfef20e450873fff~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944329&auth_key=1759944329-0-0-cc23f9abbb755812b001f05e718e2ada&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="On-the-Role-of-Temperature-Sampling-in-Test-Time-Scaling"><a href="#On-the-Role-of-Temperature-Sampling-in-Test-Time-Scaling" class="headerlink" title="On the Role of Temperature Sampling in Test-Time Scaling"></a>On the Role of Temperature Sampling in Test-Time Scaling</h2><p><strong>Authors:Yuheng Wu, Azalia Mirhoseini, Thierry Tambe</strong></p>
<p>Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model’s potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024&#x2F;2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models. </p>
<blockquote>
<p>大型语言模型（LLM）可以通过测试时间缩放（TTS）在推理时间提高推理能力，其中会生成多个推理轨迹并选择最佳的一个。先前的研究表明，增加样本数量K稳步提高准确性。在本文中，我们证明这一趋势并非无限持续：在较大的K值下，进一步的缩放不会产生任何收益，并且无论轨迹数量如何，某些难题仍然无法解决。有趣的是，我们发现不同的采样温度可以解决不同的问题子集，这意味着单一温度的缩放只能探索模型的一部分潜力。因此，我们提出沿温度维度进行缩放，这扩大了大型语言模型的推理边界。在Qwen3（0.6B、1.7B、4B、8B）和五个代表性推理基准测试（AIME 2024&#x2F;2025、MATH500、LiveCodeBench、Hi-ToM）上进行平均，温度缩放比单一温度TTS额外提高了7.3点。温度缩放还使基础模型的性能达到与强化学习（RL）训练的对等水平，而无需进行额外的后训练。我们还对此现象进行了综合分析，并设计了一种多温度投票方法，降低了温度缩放的开销。总体而言，我们的研究结果表明，TTS比先前认为的更强大，温度缩放提供了一种简单有效的方法来解锁基础模型的潜在能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02611v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大规模语言模型（LLM）可以通过测试时缩放（TTS）在推理时间提高推理能力，其中会生成多个推理轨迹并选择最佳的一个。本文发现，随着样本数量K的增加，准确率稳步提高的趋势并非无限持续：在较大的K值下，进一步的缩放不会带来收益，某些难题仍然无法解决，无论生成多少个轨迹。有趣的是，我们发现不同的采样温度可以解决不同的问题子集，意味着单一温度的缩放只能探索模型的一部分潜力。因此，我们提出沿温度维度进行缩放，以扩大LLM的推理边界。在Qwen3和五个代表性推理基准测试上平均，温度缩放较单温度TTS产生了额外的7.3点收益。温度缩放还使基础模型的性能达到与强化学习（RL）训练相当的水平，无需额外的后训练。我们还对此现象进行了综合分析，并设计了一种多温度投票方法，降低了温度缩放的开销。总体而言，我们的研究结果表明TTS比以往认为的更强大，温度缩放是一种简单有效的方法，可以解锁基础模型的潜在能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>测试时缩放（TTS）能够提升大规模语言模型（LLM）的推理能力，通过生成多个推理轨迹并选择最佳轨迹来实现。</li>
<li>单纯增加样本数量K并不能无限提高准确率，在达到一定点后进一步缩放无收益。</li>
<li>不同的采样温度可以解决不同类型的问题，单一温度的缩放只能利用模型的部分潜力。</li>
<li>提出沿温度维度进行缩放，以扩大LLM的推理边界。</li>
<li>温度缩放在多个基准测试上较单温度TTS有显著性能提升。</li>
<li>温度缩放使基础模型的性能与经过强化学习（RL）训练后的模型相当，且无需额外的后训练过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-932f9d56cdb351fa0b7ce076b634138c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944337&auth_key=1759944337-0-0-d47b4e0272e1720abd5ddf714225f797&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c283b73c954c1e5366dbf0b5891cc9b1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4439b891848107dde7992622046f3d93~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944350&auth_key=1759944350-0-0-c6479e18296f1ee15075a492281a85ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff70336f49c5e536ef29a5e80c571cee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944357&auth_key=1759944357-0-0-90ff0a779577a7df1d956f8775f286b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ef1a869ae37903560b1d9153e829292~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944363&auth_key=1759944363-0-0-0ed67277c46c5b4828c20f59876abde6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-42366b44bcae6fa1247b72acfc2b099d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="KAME-Tandem-Architecture-for-Enhancing-Knowledge-in-Real-Time-Speech-to-Speech-Conversational-AI"><a href="#KAME-Tandem-Architecture-for-Enhancing-Knowledge-in-Real-Time-Speech-to-Speech-Conversational-AI" class="headerlink" title="KAME: Tandem Architecture for Enhancing Knowledge in Real-Time   Speech-to-Speech Conversational AI"></a>KAME: Tandem Architecture for Enhancing Knowledge in Real-Time   Speech-to-Speech Conversational AI</h2><p><strong>Authors:So Kuroki, Yotaro Kubo, Takuya Akiba, Yujin Tang</strong></p>
<p>Real-time speech-to-speech (S2S) models excel at generating natural, low-latency conversational responses but often lack deep knowledge and semantic understanding. Conversely, cascaded systems combining automatic speech recognition, a text-based Large Language Model (LLM), and text-to-speech synthesis offer superior knowledge representation at the cost of high latency, which disrupts the flow of natural interaction. This paper introduces a novel hybrid architecture that bridges the gap between these two paradigms. Our framework processes user speech through an S2S transformer for immediate responsiveness while concurrently relaying the query to a powerful back-end LLM. The LLM’s text-based response is then injected in real time to guide the S2S model’s speech generation, effectively infusing its output with rich knowledge without the full latency penalty of a cascaded system. We evaluated our method using a speech-synthesized variant of the MT-Bench benchmark that consists of multi-turn question-answering sessions. The results demonstrate that our system substantially outperforms a baseline S2S model in response correctness, approaching that of a cascaded system, while maintaining a latency on par with the baseline. </p>
<blockquote>
<p>实时语音到语音（S2S）模型擅长生成自然、低延迟的对话响应，但往往缺乏深度知识和语义理解。相反，级联系统将自动语音识别、基于文本的大型语言模型（LLM）和文本到语音合成相结合，以较高的延迟为代价提供了卓越的知识表示，从而破坏了自然交互的流程。本文介绍了一种弥这两种范式之间鸿沟的新型混合架构。我们的框架通过S2S变压器处理用户语音，以实现即时响应，同时并行将查询发送到功能强大的后端LLM。LLM的基于文本的响应然后实时注入，以指导S2S模型的语音生成，有效地将丰富的知识融入其输出中，而不会受到级联系统全延迟的惩罚。我们使用由多轮问答会话组成的MT-Bench基准语音合成变种来评估我们的方法。结果表明，我们的系统在响应正确性方面大大优于基线S 2S模型，接近级联系统的性能，同时保持与基线相当的延迟。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02327v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>实时语音到语音（S2S）模型擅长生成自然、低延迟的对话响应，但往往缺乏深度知识和语义理解。相反，级联系统结合了自动语音识别、基于文本的大型语言模型（LLM）和文本到语音的合成，提供了卓越的知识表示，但代价是较高的延迟，破坏了自然交互的流畅性。本文介绍了一种新型的混合架构，旨在弥合这两种范式之间的差距。该框架通过S2S变压器处理用户语音，以实现即时响应，同时并行将查询传送到后端强大的LLM。LLM的基于文本的响应被实时注入，以引导S2S模型的语音生成，有效地将其输出与丰富的知识融合在一起，而无需承受级联系统的全延迟惩罚。我们使用由多回合问答会话组成的MT-Bench基准语音合成变种对方法进行评估。结果表明，我们的系统在响应正确性方面大幅优于基线S2S模型，接近级联系统的表现，同时保持与基线相当的延迟。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>实时语音到语音（S2S）模型虽然响应迅速，但缺乏深度知识和语义理解。</li>
<li>级联系统结合自动语音识别、大型语言模型和文本到语音的合成，提供卓越的知识表示，但延迟较高。</li>
<li>新混合架构结合S2S模型的即时响应和级联系统的知识表示。</li>
<li>该混合架构通过后端大型语言模型（LLM）实时指导S2S模型的语音生成。</li>
<li>新方法在提高响应正确性的同时，维持低延迟，接近级联系统的性能。</li>
<li>评估使用多回合问答会话的基准语音合成变种，证明了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-74277dbaebf71ca469b28a5df2b31d6c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b7cc39b4693ff43c2347fdfec1d3ce9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944384&auth_key=1759944384-0-0-e879e87efe6a833264f7633c2910ecbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa49017f12cf44ee329bb8b58ac7ba02~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944391&auth_key=1759944391-0-0-33ea1c4a1a877449e4ec9572dc0a9fdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9820faa70602a30856ea2c8b1662aa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944398&auth_key=1759944398-0-0-03a1637956aaebb77806b24987b6409b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SpeechCT-CLIP-Distilling-Text-Image-Knowledge-to-Speech-for-Voice-Native-Multimodal-CT-Analysis"><a href="#SpeechCT-CLIP-Distilling-Text-Image-Knowledge-to-Speech-for-Voice-Native-Multimodal-CT-Analysis" class="headerlink" title="SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for   Voice-Native Multimodal CT Analysis"></a>SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for   Voice-Native Multimodal CT Analysis</h2><p><strong>Authors:Lukas Buess, Jan Geier, David Bani-Harouni, Chantal Pellegrini, Matthias Keicher, Paula Andrea Perez-Toro, Nassir Navab, Andreas Maier, Tomas Arias-Vergara</strong></p>
<p>Spoken communication plays a central role in clinical workflows. In radiology, for example, most reports are created through dictation. Yet, nearly all medical AI systems rely exclusively on written text. In this work, we address this gap by exploring the feasibility of learning visual-language representations directly from spoken radiology reports. Specifically, we synthesize a large-scale dataset (Speech-RATE) of spoken radiology reports and train SpeechCT-CLIP, a contrastive model that aligns speech and 3D CT volumes in a shared representation space. While naive speech-based models underperform compared to text-trained counterparts, we show that knowledge distillation from a pretrained text-image CLIP model effectively transfers semantic alignment capabilities from text to speech, substantially narrowing this gap. Experiments demonstrate improved zero-shot classification F1 from 0.623 to 0.705, recovering 88% of the performance difference, and strong retrieval results without requiring text at inference. These findings highlight speech as a practical alternative to text in multimodal pretraining and open the door to voice-driven diagnostic support tools in clinical practice. </p>
<blockquote>
<p>口头沟通在临床工作中起着核心作用。以放射科为例，大多数报告都是通过口授生成的。然而，几乎所有的医疗人工智能系统都依赖于书面文本。在这项工作中，我们通过探索直接从口头放射学报告中学习视觉语言表示的可能性来解决这一差距。具体来说，我们合成了一个大规模的放射学报告语音数据集（Speech-RATE），并训练了SpeechCT-CLIP对比模型，该模型在共享表示空间中对齐语音和3D CT体积。与基于文本的模型相比，简单的语音模型表现较差，但我们证明了从预训练的文本图像CLIP模型进行知识蒸馏可以有效地将语义对齐能力从文本转移到语音，从而大大缩小了这一差距。实验表明，零样本分类的F1分数从0.623提高到0.705，恢复了88%的性能差异，并且在不需要文本的情况下检索结果强大。这些发现凸显了语音在多模态预训练中作为文本的实用替代品的地位，并为临床实践中的语音驱动诊断支持工具打开了大门。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02322v1">PDF</a> Submitted to ICASSP 2026; under review</p>
<p><strong>Summary</strong></p>
<p>本文解决了医疗领域中口头报告与医疗AI系统之间的鸿沟问题。研究团队构建了一个大规模的口头报告数据集Speech-RATE，并开发了一个对比模型SpeechCT-CLIP，该模型将语音和3D CT体积在共享表示空间中对齐。尽管基于语音的模型在初始状态下表现不如文本训练的模型，但通过从预训练的文本图像CLIP模型进行知识蒸馏，成功将语义对齐能力从文本转移到语音，显著缩小了差距。实验表明，零样本分类的F1分数从0.623提高到0.705，恢复了88%的性能差异，并且无需文本推理即可实现强大的检索结果。这一发现强调了语音在跨模态预训练中的实用性，并为临床实践中使用语音驱动的辅助诊断工具打开了大门。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>口头沟通在临床工作中占据重要地位，但现有医疗AI系统主要依赖书面文本。</li>
<li>研究人员创建了一个大规模的口头报告数据集Speech-RATE。</li>
<li>开发了一个对比模型SpeechCT-CLIP，能够将语音和3D CT体积在共享表示空间中对齐。</li>
<li>基于语音的模型初始表现不如文本训练的模型，但通过知识蒸馏技术从预训练的文本图像CLIP模型转移语义对齐能力，提高了语音模型的表现。</li>
<li>实验结果显示，零样本分类的F1分数显著提高，并实现了强大的检索结果，无需文本推理。</li>
<li>缩小了语音和文本在医疗AI应用中的性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c59446ca9baccf942786d32f7451e589.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b547f600fd234bf096925769bf930a54~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944413&auth_key=1759944413-0-0-a3e8f005b25943886a1a15659a7b90ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fba9f8834fe2a355d1fea5dbb4a68e00~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944420&auth_key=1759944420-0-0-8322672899b8f0d24bf1d2221c809afd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-3536491a7dfadb95c5babdeade5a9633.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4f47c12030ca1ab313f56172ebd172.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AlignDiT-Multimodal-Aligned-Diffusion-Transformer-for-Synchronized-Speech-Generation"><a href="#AlignDiT-Multimodal-Aligned-Diffusion-Transformer-for-Synchronized-Speech-Generation" class="headerlink" title="AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized   Speech Generation"></a>AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized   Speech Generation</h2><p><strong>Authors:Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung</strong></p>
<p>In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/AlignDiT">https://mm.kaist.ac.kr/projects/AlignDiT</a>. </p>
<blockquote>
<p>本文我们解决了多模态到语音生成的任务，该任务旨在从多种输入模式（文本、视频和参考音频）中合成高质量语音。由于该任务在影视制作、配音和虚拟角色等领域的广泛应用，它已受到越来越多的关注。尽管最近有进展，但现有方法仍面临语音清晰度、音视频同步、语音自然度和与参考说话人的声音相似性等方面的局限性。为了解决这些挑战，我们提出了AlignDiT，这是一种多模态对齐扩散Transformer，它可以从对齐的多模态输入中生成准确、同步和自然的声音。AlignDiT基于DiT架构的上下文学习能力，探索了三种有效的策略来进行多模态表示对齐。此外，我们引入了一种新的无分类器引导机制，使模型在语音合成过程中能够自适应地平衡各模态的信息。大量实验表明，AlignDiT在多个基准测试上的质量、同步和说话人相似性方面显著优于现有方法。而且，AlignDiT在各种多模态任务中表现出强大的泛化能力，如视频到语音的合成和视觉强制对齐，一直保持着最先进的表现。演示页面可访问：<a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/AlignDiT%E3%80%82">https://mm.kaist.ac.kr/projects/AlignDiT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20629v2">PDF</a> ACM Multimedia 2025</p>
<p><strong>Summary</strong><br>     本文研究了多模态到语音生成任务，旨在从文本、视频和参考音频等多种输入模态合成高质量语音。针对现有方法的挑战，提出了AlignDiT模型，采用多模态对齐扩散Transformer生成准确、同步且自然的语音。该模型通过三种有效策略实现多模态表示对齐，并引入新型的多模态无分类器引导机制，使模型在语音合成时能够自适应地平衡各模态的信息。实验表明，AlignDiT在多个基准测试中显著优于现有方法，具有强大的跨各种多模态任务的一般化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文关注多模态到语音生成任务，旨在从多种输入模态（文本、视频和参考音频）生成高质量语音。</li>
<li>现有方法面临语音清晰度、音视频同步、语音自然度和与参考语音的相似性等方面的挑战。</li>
<li>提出的AlignDiT模型基于扩散Transformer，实现了多模态表示的对齐。</li>
<li>AlignDiT采用三种有效策略进行多模态对齐，包括扩散过程和上下文学习。</li>
<li>引入的多模态无分类器引导机制使模型在语音合成时能自适应平衡各模态的信息。</li>
<li>实验结果显示AlignDiT在多个基准测试中表现优异，显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-83707036db98012e89567f70cd2efc98.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-feee5d6d1c2e05fa6a8e43b05ff3bc16~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944448&auth_key=1759944448-0-0-c8e20ee82b02ae4ebbf018bc2a1585d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de3c7c3355f4164cb1504cf0541533c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944497&auth_key=1759944497-0-0-15bcc1ebea933be6373fa7ef296b9264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ca2dbdae06818516195f71f6e3eef2ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944504&auth_key=1759944504-0-0-df2fa6986fe7d8a907c7f8552a135c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-10-07  Role of universal function of the nuclear proximity potential A   systematic study on the alpha-decay of heavy/super-heavy nuclei and   α-induced reactions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-b144204b970c44b83070e0ed2780eb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943722&auth_key=1759943722-0-0-625ee05b6d124873902037d8957973e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-07  Wave-GMS Lightweight Multi-Scale Generative Model for Medical Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
