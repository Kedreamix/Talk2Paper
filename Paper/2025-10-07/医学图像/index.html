<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Wave-GMS Lightweight Multi-Scale Generative Model for Medical Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-b144204b970c44b83070e0ed2780eb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943722&auth_key=1759943722-0-0-625ee05b6d124873902037d8957973e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-07-æ›´æ–°"><a href="#2025-10-07-æ›´æ–°" class="headerlink" title="2025-10-07 æ›´æ–°"></a>2025-10-07 æ›´æ–°</h1><h2 id="Wave-GMS-Lightweight-Multi-Scale-Generative-Model-for-Medical-Image-Segmentation"><a href="#Wave-GMS-Lightweight-Multi-Scale-Generative-Model-for-Medical-Image-Segmentation" class="headerlink" title="Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image   Segmentation"></a>Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image   Segmentation</h2><p><strong>Authors:Talha Ahmed, Nehal Ahmed Shaikh, Hassan Mohy-ud-Din</strong></p>
<p>For equitable deployment of AI tools in hospitals and healthcare facilities, we need Deep Segmentation Networks that offer high performance and can be trained on cost-effective GPUs with limited memory and large batch sizes. In this work, we propose Wave-GMS, a lightweight and efficient multi-scale generative model for medical image segmentation. Wave-GMS has a substantially smaller number of trainable parameters, does not require loading memory-intensive pretrained vision foundation models, and supports training with large batch sizes on GPUs with limited memory. We conducted extensive experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument, and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art segmentation performance with superior cross-domain generalizability, while requiring only ~2.6M trainable parameters. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ATPLab-LUMS/Wave-GMS">https://github.com/ATPLab-LUMS/Wave-GMS</a>. </p>
<blockquote>
<p>ä¸ºäº†åœ¨åŒ»é™¢å’ŒåŒ»ç–—ä¿å¥è®¾æ–½ä¸­å®ç°äººå·¥æ™ºèƒ½å·¥å…·çš„å…¬å¹³éƒ¨ç½²ï¼Œæˆ‘ä»¬éœ€è¦æä¾›é«˜æ€§èƒ½çš„æ·±åº¦åˆ†å‰²ç½‘ç»œï¼Œå®ƒèƒ½å¤Ÿåœ¨å…·æœ‰æœ‰é™å†…å­˜å’Œå¤§æ‰¹é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨æˆæœ¬æ•ˆç›Šé«˜çš„GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Wave-GMSï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»é‡çº§ã€é«˜æ•ˆçš„å¤šå°ºåº¦ç”Ÿæˆæ¨¡å‹ã€‚Wave-GMSå…·æœ‰å¤§é‡çš„å¯è®­ç»ƒå‚æ•°ï¼Œä¸éœ€è¦åŠ è½½å†…å­˜å¯†é›†å‹çš„é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¸”æ”¯æŒåœ¨å…·æœ‰æœ‰é™å†…å­˜çš„GPUä¸Šè¿›è¡Œå¤§æ‰¹é‡è®­ç»ƒã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å¼€å¯ç”¨æ•°æ®é›†ï¼ˆBUSã€BUSIã€Kvasir-Instrumentå’ŒHAM10000ï¼‰ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜Wave-GMSè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åªéœ€è¦çº¦2.6Mçš„å¯è®­ç»ƒå‚æ•°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ATPLab-LUMS/Wave-GMS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ATPLab-LUMS/Wave-GMSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03216v1">PDF</a> 5 pages, 1 figure, 4 tables; Submitted to IEEE Conference for   possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æå‡ºä¸€ç§é«˜æ•ˆä¸”è½»é‡çº§çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œWave-GMSï¼Œç”¨äºåœ¨å…·æœ‰æœ‰é™å†…å­˜å’Œå¤§å‹æ‰¹é‡çš„ä½æˆæœ¬GPUä¸Šè®­ç»ƒå¹¶å®ç°é«˜æ°´å¹³çš„åŒ»ç–—å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚Wave-GMSå…·æœ‰è¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œæ— éœ€åŠ è½½å†…å­˜å¯†é›†å‹é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶æ”¯æŒåœ¨å…·æœ‰æœ‰é™å†…å­˜çš„GPUä¸Šè¿›è¡Œå¤§è§„æ¨¡æ‰¹é‡è®­ç»ƒã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWave-GMSå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åªéœ€è¦å¤§çº¦2.6Mçš„å¯è®­ç»ƒå‚æ•°ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ç‰¹å®šç½‘å€ä¸‹è½½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œWave-GMSï¼Œæ—¨åœ¨å®ç°å‡è¡¡éƒ¨ç½²ã€‚è¯¥ç½‘ç»œé’ˆå¯¹åŒ»é™¢çš„å…¬å¹³éœ€æ±‚è€Œè®¾è®¡ã€‚å®ƒåœ¨æ€§èƒ½é«˜å’Œå†…å­˜æœ‰é™çš„GPUä¸Šè¿è¡Œè‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b144204b970c44b83070e0ed2780eb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943731&auth_key=1759943731-0-0-356b69ac3c12a43728511c7a97376f8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7ff4682297805ff0716c235fd1041e03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf3b1c49cef51efa557186a9effa359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef50fd987c4db231eede7fe9e95ad464.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-853dfd48a925888186b5fac39ecfa5cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943758&auth_key=1759943758-0-0-3b785df01e3f00219fb633b8df4ee9e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus"><a href="#SpineBench-A-Clinically-Salient-Level-Aware-Benchmark-Powered-by-the-SpineMed-450k-Corpus" class="headerlink" title="SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the   SpineMed-450k Corpus"></a>SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the   SpineMed-450k Corpus</h2><p><strong>Authors:Ming Zhao, Wenhui Dong, Yang Zhang, Xiang Zheng, Zhonghao Zhang, Zian Zhou, Yunzhi Guan, Liukun Xu, Wei Peng, Zhaoyang Gong, Zhicheng Zhang, Dachuan Li, Xiaosheng Ma, Yuli Ma, Jianing Ni, Changjiang Jiang, Lixia Tian, Qixin Chen, Kaishun Xia, Pingping Liu, Tongshun Zhang, Zhiqiang Liu, Zhongan Bi, Chenyang Si, Tiansheng Sun, Caifeng Shan</strong></p>
<p>Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our modelâ€™s outputs. </p>
<blockquote>
<p>è„Šæ¤ç–¾ç—…å½±å“å…¨çƒ6.19äº¿äººï¼Œæ˜¯å¯¼è‡´æ®‹ç–¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ä»ç„¶å—é™äºç¼ºä¹æ„ŸçŸ¥çº§åˆ«çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚è„Šæ¤ç–¾ç—…çš„ä¸´åºŠå†³ç­–éœ€è¦åœ¨ç‰¹å®šçš„æ¤ä½“æ°´å¹³ä¸Šå¯¹Xå…‰ã€CTå’ŒMRIè¿›è¡Œå¤æ‚æ¨ç†ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯è¿½æº¯çš„ã€ä»¥ä¸´åºŠä¸ºåŸºç¡€çš„æŒ‡ä»¤æ•°æ®ä»¥åŠæ ‡å‡†åŒ–çš„è„Šæ¤ç‰¹å®šåŸºå‡†æµ‹è¯•ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpineMedï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ‰§ä¸šè„Šæ¤å¤–ç§‘åŒ»ç”Ÿå…±åŒè®¾è®¡ç”Ÿæ€ç³»ç»Ÿã€‚å®ƒåŒ…å«SpineMed-450kå’ŒSpineBenchä¸¤ä¸ªç‰¹ç‚¹ã€‚SpineMed-450kæ˜¯ä¸“ä¸ºæ¤ä½“çº§åˆ«çš„è·¨æˆåƒæ¨¡æ€æ¨ç†è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ä¸‡ä¸ªæŒ‡ä»¤å®ä¾‹ã€‚è¿™äº›æ•°æ®é›†æ˜¯ä»å¤šæ ·åŒ–çš„æ¥æºç²¾å¿ƒç­–åˆ’è€Œæˆï¼ŒåŒ…æ‹¬æ•™ç§‘ä¹¦ã€æŒ‡å—ã€å…¬å¼€æ•°æ®é›†ä»¥åŠå¤§çº¦1000ä¸ªåŒ¿ååŒ»é™¢ç—…ä¾‹ï¼Œé€šè¿‡ä½¿ç”¨æœ‰ä¸´åºŠåŒ»ç”Ÿå‚ä¸çš„é—­ç¯ç®¡é“å’Œä¸¤é˜¶æ®µçš„LLMç”Ÿæˆæ–¹æ³•ï¼ˆè‰ç¨¿å’Œä¿®è®¢ï¼‰æ¥ç¡®ä¿é«˜è´¨é‡çš„å¯è¿½æº¯æ•°æ®å¯ç”¨äºé—®ç­”ã€å¤šè½®å’¨è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆã€‚SpineBenchæ˜¯ä¸€ä¸ªä»¥ä¸´åºŠä¸ºåŸºç¡€çš„è¯„ä»·æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸´åºŠé‡è¦è½´ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ°´å¹³è¯†åˆ«ã€ç—…ç†è¯„ä¼°å’Œæ‰‹æœ¯è§„åˆ’ç­‰ã€‚æˆ‘ä»¬å¯¹å‡ ä¸ªæœ€è¿‘å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨SpineBenchä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç²¾ç»†ç²’åº¦çš„æ°´å¹³ç‰¹å®šæ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨SpineMed-450kä¸Šè°ƒä¼˜åï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºä¸€è‡´ä¸”æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°è¯å®äº†æˆ‘ä»¬çš„æ¨¡å‹è¾“å‡ºçš„è¯Šæ–­æ¸…æ™°åº¦å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦åŸºäºåŒ»å­¦å›¾åƒé¢†åŸŸçš„ä¸€é¡¹ç ”ç©¶ï¼Œè¯¥ç ”ç©¶è¡¨æ˜è„ŠæŸ±ç–¾ç—…æ˜¯å…¨çƒå¯¼è‡´æ®‹ç–¾çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå½±å“äººæ•°é«˜è¾¾6.19äº¿ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šæ¨¡æ€æ•°æ®é›†ï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†SpineMedç”Ÿæ€ç³»ç»Ÿï¼Œå…¶ä¸­åŒ…æ‹¬SpineMed-450kæ•°æ®é›†å’ŒSpineBenchè¯„ä¼°æ¡†æ¶ã€‚SpineMed-450kæ•°æ®é›†æ¶µç›–è¶…è¿‡45ä¸‡æ¡æŒ‡ä»¤å®ä¾‹ï¼Œä¸“ä¸ºæ¤ä½“çº§åˆ«çš„è·¨æ¨¡æ€æˆåƒæ¨ç†è®¾è®¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸€ç³»åˆ—æ–¹æ³•ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡SpineMed-450kè®­ç»ƒæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—æ”¹è¿›ã€‚ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°ä¹Ÿç¡®è®¤äº†æ¨¡å‹çš„è¯Šæ–­æ¸…æ™°åº¦å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„ŠæŸ±ç–¾ç—…å½±å“å·¨å¤§ï¼Œå…¨çƒèŒƒå›´å†…æœ‰æ•°äº¿äººå—å½±å“ï¼ŒäºŸéœ€é«˜æ•ˆå‡†ç¡®çš„è¯Šæ–­æ–¹æ³•ã€‚</li>
<li>AIåœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„åº”ç”¨å—é™äºç¼ºä¹å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç‰¹å®šæ¤ä½“çº§åˆ«çš„æ•°æ®ã€‚</li>
<li>æ¨å‡ºSpineMedç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«SpineMed-450kæ•°æ®é›†å’ŒSpineBenchè¯„ä¼°æ¡†æ¶ï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SpineMed-450kæ•°æ®é›†æ˜¯ä¸“ä¸ºæ¤ä½“çº§åˆ«è·¨æ¨¡æ€æˆåƒæ¨ç†è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ä¸‡æ¡æŒ‡ä»¤å®ä¾‹ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡å¤šå…ƒåŒ–æ¥æºåŠä¸¥æ ¼çš„æ•°æ®è´¨é‡ç®¡æ§æµç¨‹è¿›è¡Œæ„å»ºã€‚</li>
<li>å¯¹å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç»è¿‡SpineMed-450kè®­ç»ƒçš„æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-25573149b6424227361c1112676ba1aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943766&auth_key=1759943766-0-0-ac870fc165306e19cd8906cce5cf149c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b7230dd6ad766a06547c8a0502b0f1e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4117ab9e87b9107d07702fbe61696b50.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-93fd4768e85510523a5989600cf6c169~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943807&auth_key=1759943807-0-0-5824136bce389bb02f738ed9d33e834d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-f8a40ffa6e15fb8b1068c38b04c7f5b0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6f6812c8981f4f4132664c7e891d8f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943820&auth_key=1759943820-0-0-7d2cce47c71fefd39733b75f460c944a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HAVIR-HierArchical-Vision-to-Image-Reconstruction-using-CLIP-Guided-Versatile-Diffusion"><a href="#HAVIR-HierArchical-Vision-to-Image-Reconstruction-using-CLIP-Guided-Versatile-Diffusion" class="headerlink" title="HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided   Versatile Diffusion"></a>HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided   Versatile Diffusion</h2><p><strong>Authors:Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou</strong></p>
<p>The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models. </p>
<blockquote>
<p>ä»è„‘æ´»åŠ¨ä¸­é‡å»ºè§†è§‰ä¿¡æ¯ä¿ƒè¿›äº†ç¥ç»ç§‘å­¦ä¸è®¡ç®—æœºè§†è§‰ä¹‹é—´çš„è·¨å­¦ç§‘æ•´åˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶åœ¨å‡†ç¡®æ¢å¤é«˜åº¦å¤æ‚çš„è§†è§‰åˆºæ¿€æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸€éš¾é¢˜æºäºè‡ªç„¶åœºæ™¯çš„ç‰¹æ€§ï¼šä½çº§ç‰¹å¾è¡¨ç°å‡ºå¼‚è´¨æ€§ï¼Œè€Œé«˜çº§ç‰¹å¾ç”±äºä¸Šä¸‹æ–‡é‡å è€Œè¡¨ç°å‡ºè¯­ä¹‰çº ç¼ ã€‚å—è§†è§‰çš®å±‚åˆ†å±‚è¡¨ç¤ºç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†HAVIRæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è§†è§‰çš®å±‚åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡åŒºåŸŸï¼Œå¹¶ä»æ¯ä¸ªåŒºåŸŸä¸­æå–ä¸åŒçš„ç‰¹å¾ã€‚å…·ä½“è€Œè¨€ï¼Œç»“æ„ç”Ÿæˆå™¨ä»ç©ºé—´å¤„ç†ä½“ç´ ä¸­æå–ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œè€Œè¯­ä¹‰æå–å™¨å°†è¯­ä¹‰å¤„ç†ä½“ç´ è½¬æ¢ä¸ºCLIPåµŒå…¥ã€‚è¿™äº›ç»„ä»¶é€šè¿‡é€šç”¨æ‰©æ•£æ¨¡å‹é›†æˆï¼Œåˆæˆæœ€ç»ˆå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHAVIRå³ä½¿åœ¨å¤æ‚åœºæ™¯ä¸­ä¹Ÿèƒ½æé«˜é‡å»ºçš„ç»“æ„å’Œè¯­ä¹‰è´¨é‡ï¼Œå¹¶ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03122v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­é¢ä¸´å¤æ‚è§†è§‰åˆºæ¿€æ¢å¤éš¾é¢˜ï¼Œæºäºè‡ªç„¶åœºæ™¯çš„ä½å±‚æ¬¡ç‰¹å¾å¼‚è´¨æ€§å’Œé«˜å±‚æ¬¡ç‰¹å¾çš„è¯­ä¹‰çº ç¼ ã€‚å€Ÿé‰´è§†è§‰çš®å±‚å±‚æ¬¡è¡¨ç¤ºç†è®ºï¼Œæå‡ºHAVIRæ¨¡å‹ï¼Œåˆ†ä¸ºç»“æ„ç”Ÿæˆå™¨å’Œè¯­ä¹‰æå–å™¨ï¼Œåˆ†åˆ«æå–ç©ºé—´å¤„ç†ä½“ç´ çš„ç»“æ„ä¿¡æ¯å’Œè¯­ä¹‰å¤„ç†ä½“ç´ çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡é€šç”¨æ‰©æ•£æ¨¡å‹åˆæˆæœ€ç»ˆå›¾åƒã€‚å®éªŒè¯æ˜HAVIRèƒ½æé«˜é‡å»ºå›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰è´¨é‡ï¼Œä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒé‡å»ºé¢ä¸´å¤æ‚è§†è§‰åˆºæ¿€æ¢å¤çš„æŒ‘æˆ˜ã€‚</li>
<li>æŒ‘æˆ˜æºäºè‡ªç„¶åœºæ™¯çš„ä½å±‚æ¬¡ç‰¹å¾å¼‚è´¨æ€§å’Œé«˜å±‚æ¬¡ç‰¹å¾çš„è¯­ä¹‰çº ç¼ ã€‚</li>
<li>HAVIRæ¨¡å‹å€Ÿé‰´è§†è§‰çš®å±‚å±‚æ¬¡è¡¨ç¤ºç†è®ºï¼Œåˆ†ä¸ºç»“æ„ç”Ÿæˆå™¨å’Œè¯­ä¹‰æå–å™¨ã€‚</li>
<li>ç»“æ„ç”Ÿæˆå™¨ä»ç©ºé—´å¤„ç†ä½“ç´ ä¸­æå–ç»“æ„ä¿¡æ¯å¹¶è½¬æ¢ä¸ºæ½œåœ¨æ‰©æ•£å…ˆéªŒã€‚</li>
<li>è¯­ä¹‰æå–å™¨å°†è¯­ä¹‰å¤„ç†ä½“ç´ è½¬æ¢ä¸ºCLIPåµŒå…¥ã€‚</li>
<li>HAVIRæ¨¡å‹é€šè¿‡é€šç”¨æ‰©æ•£æ¨¡å‹åˆæˆæœ€ç»ˆå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1cfd2e61c79af339471288ba4fcfad07~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943827&auth_key=1759943827-0-0-c5f8169752c447c7058065ab7f3cef36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-fb0693ea58fdb813f086fc69380a43da.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dd7309b3a8b9b771cfaf09a2084da7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943841&auth_key=1759943841-0-0-810ae277a19696f29c96773f9d7f69a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aabffc6dbdc9e9ede5a3b1612c981351~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943847&auth_key=1759943847-0-0-6f382ef32e89f4b51b616c67cfe44a37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-a38c148322e282ea16103ddd06658a6b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER"><a href="#Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER" class="headerlink" title="Semantic Similarity in Radiology Reports via LLMs and NER"></a>Semantic Similarity in Radiology Reports via LLMs and NER</h2><p><strong>Authors:Beth Pearson, Ahmed Adnan, Zahraa Abdallah</strong></p>
<p>Radiology report evaluation is a crucial part of radiologistsâ€™ training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +&#x2F;- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports%7D%7Bgithub.com/otmive/llama/_reports%7D">https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}</a> </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šè¯„ä¼°æ˜¯æ”¾å°„ç§‘åŒ»ç”ŸåŸ¹è®­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºç¡®ä¿è¯Šæ–­å‡†ç¡®æ€§èµ·ç€å…³é”®ä½œç”¨ã€‚ä½œä¸ºæ ‡å‡†æŠ¥å‘Šå·¥ä½œæµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œåˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸ä¼šç¼–å†™åˆæ­¥æŠ¥å‘Šï¼Œç„¶åç”±é«˜çº§æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œå®¡æŸ¥å’Œç¼–è¾‘ä»¥äº§ç”Ÿæœ€ç»ˆæŠ¥å‘Šã€‚è¯†åˆ«åˆæ­¥æŠ¥å‘Šå’Œæœ€ç»ˆæŠ¥å‘Šä¹‹é—´çš„è¯­ä¹‰å·®å¼‚å¯¹äºåˆçº§åŒ»ç”Ÿè‡³å…³é‡è¦ï¼Œæ—¢æ˜¯è®­ç»ƒå·¥å…·ï¼Œä¹Ÿæœ‰åŠ©äºå‘ç°ä¸´åºŠçŸ¥è¯†ä¸Šçš„å·®è·ã€‚å°½ç®¡äººå·¥æ™ºèƒ½åœ¨æ”¾å°„å­¦é¢†åŸŸæ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä½†ç”±äºéœ€è¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMåœ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­æä¾›å¯è§£é‡Šå’Œå‡†ç¡®æ¯”è¾ƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæ¯”è¾ƒäº†å¤šä¸ªLLMåœ¨æ¯”è¾ƒæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸºäºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„æ›´ä¼ ç»Ÿçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨æä¾›è¯­ä¹‰ç›¸ä¼¼æ€§å‡†ç¡®åé¦ˆæ–¹é¢éƒ½å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-EntScoreï¼Œä¸€ç§è¯­ä¹‰ç›¸ä¼¼æ€§è¯„åˆ†æ–¹æ³•ï¼Œç»“åˆäº†Llama 3.1å’ŒNERï¼Œä½¿ç”¨å¯è°ƒæƒé‡æ¥å¼ºè°ƒæˆ–æ·¡åŒ–ç‰¹å®šç±»å‹çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆä¸€ä¸ªé‡åŒ–çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç”¨äºè·Ÿè¸ªè¿›åº¦ï¼Œå¹¶å¯¹åˆ†æ•°è¿›è¡Œè§£é‡Šï¼Œæ—¨åœ¨æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œä»¥å®¡æŸ¥å’Œæ”¹è¿›æŠ¥å‘Šã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„çœŸå®åˆ†æ•°ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†67%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡å’Œ93%çš„Â±1å†…çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•ç‹¬ä½¿ç”¨çš„LLMså’ŒNERã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports">github.com&#x2F;otmive&#x2F;llama_reports</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03102v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦æŠ¥å‘Šå¯¹æ¯”ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å¯¹æ¯”äº†LLMså’Œä¼ ç»Ÿå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå‘ç°ä¸¤è€…åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§åé¦ˆæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†Llama-EntScoreæ–¹æ³•ï¼Œç»“åˆLlama 3.1å’ŒNERï¼Œå¯é‡åŒ–æ”¾å°„å­¦æŠ¥å‘Šçš„ç›¸ä¼¼æ€§ï¼Œå¹¶è§£é‡Šè¯„åˆ†ï¼Œä¸ºæŠ¥å‘Šå®¡æŸ¥å’Œç²¾è¿›æä¾›æŒ‡å¯¼ã€‚è¯¥æ–¹æ³•è¾ƒLLMså’ŒNERç‹¬ç«‹ä½¿ç”¨è¡¨ç°æ›´ä¼˜ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„çœŸå®è¯„åˆ†ç›¸æ¯”ï¼Œå‡†ç¡®ç‡é«˜è¾¾93%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„å­¦æŠ¥å‘Šè¯„ä¼°æ˜¯æ”¾å°„ç§‘åŒ»ç”ŸåŸ¹è®­çš„å…³é”®ç¯èŠ‚ï¼Œæœ‰åŠ©äºç¡®ä¿è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>åˆæ­¥æŠ¥å‘Šä¸æœ€ç»ˆæŠ¥å‘Šé—´çš„è¯­ä¹‰å·®å¼‚å¯¹åˆçº§åŒ»ç”Ÿè€Œè¨€æ˜¯é‡è¦çš„è®­ç»ƒå·¥å…·ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦æŠ¥å‘Šå¯¹æ¯”ä¸­çš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜ï¼Œéœ€ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>LLMså’Œä¼ ç»Ÿçš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹æ³•åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§åé¦ˆæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Llama-EntScoreæ–¹æ³•ç»“åˆLlama 3.1å’ŒNERï¼Œæä¾›é‡åŒ–ç›¸ä¼¼åº¦è¯„åˆ†å¹¶è§£é‡Šï¼Œæ—¨åœ¨æŒ‡å¯¼æŠ¥å‘Šå®¡æŸ¥å’Œç²¾è¿›ã€‚</li>
<li>Llama-EntScoreæ–¹æ³•è¾ƒLLMså’ŒNERç‹¬ç«‹ä½¿ç”¨è¡¨ç°æ›´ä¼˜ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„çœŸå®è¯„åˆ†ç›¸æ¯”ï¼Œå‡†ç¡®ç‡è¾ƒé«˜ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f98c50394379a9217910538618acceb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943861&auth_key=1759943861-0-0-bb45e9e76db8510df95bd9755d5d4960&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Flip-Distribution-Alignment-VAE-for-Multi-Phase-MRI-Synthesis"><a href="#Flip-Distribution-Alignment-VAE-for-Multi-Phase-MRI-Synthesis" class="headerlink" title="Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis"></a>Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis</h2><p><strong>Authors:Xiaoyan Kui, Qianmu Xiao, Qqinsong Li, Zexin Ji, JIelin Zhang, Beiji Zou</strong></p>
<p>Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/QianMuXiao/FDA-VAE">https://github.com/QianMuXiao/FDA-VAE</a>. </p>
<blockquote>
<p>åœ¨å¤šé˜¶æ®µå¢å¼ºï¼ˆCEï¼‰MRIåˆæˆä¸­ï¼Œåˆ†ç¦»å…±äº«å’Œç‹¬ç«‹ç‰¹å¾è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨å‚æ•°æ•ˆç‡è¾ƒä½çš„æ·±åº¦è‡ªç¼–ç å™¨ç”Ÿæˆå™¨ï¼Œå¹¶ä¸”ç¼ºä¹å¯è§£é‡Šçš„è®­ç»ƒç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Flipåˆ†å¸ƒå¯¹é½å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆFDA-VAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šé˜¶æ®µCE MRIåˆæˆçš„è½»é‡çº§ç‰¹å¾è§£è€¦VAEæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¾“å…¥å›¾åƒå’Œç›®æ ‡å›¾åƒç¼–ç ä¸ºä¸¤ä¸ªæ½œåœ¨åˆ†å¸ƒï¼Œè¿™ä¸¤ä¸ªåˆ†å¸ƒå…³äºæ ‡å‡†æ­£æ€åˆ†å¸ƒæ˜¯å¯¹ç§°çš„ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ†ç¦»äº†å…±äº«å’Œç‹¬ç«‹ç‰¹å¾ã€‚Yå½¢åŒå‘è®­ç»ƒç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾åˆ†ç¦»çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„åŸºäºæ·±åº¦è‡ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯åˆæˆæ–¹æ³•ç›¸æ¯”ï¼ŒFDA-VAEåœ¨å‡å°‘æ¨¡å‹å‚æ•°å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æé«˜äº†åˆæˆè´¨é‡ã€‚æºä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/QianMuXiao/FDA-VAE">https://github.com/QianMuXiao/FDA-VAE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02970v1">PDF</a> This paper has been early accept by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFlip Distribution Alignment Variational Autoencoderï¼ˆFDA-VAEï¼‰çš„è½»é‡çº§ç‰¹å¾è§£è€¦å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ï¼Œç”¨äºå¤šé˜¶æ®µå¯¹æ¯”å¢å¼ºMRIåˆæˆã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ†ç¦»å…±äº«å’Œç‹¬ç«‹ç‰¹å¾ï¼Œé‡‡ç”¨Yå½¢åŒå‘è®­ç»ƒç­–ç•¥è¿›ä¸€æ­¥æé«˜ç‰¹å¾åˆ†ç¦»çš„å¯è§£é‡Šæ€§ã€‚ä¸ç°æœ‰åŸºäºæ·±åº¦è‡ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯åˆæˆæ–¹æ³•ç›¸æ¯”ï¼ŒFDA-VAEåœ¨å‡å°‘æ¨¡å‹å‚æ•°å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œæœ‰æ•ˆæé«˜åˆæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FDA-VAEæ¨¡å‹ç”¨äºå¤šé˜¶æ®µå¯¹æ¯”å¢å¼ºMRIåˆæˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨æ·±åº¦è‡ªç¼–ç å™¨ç”Ÿæˆå™¨ï¼Œå­˜åœ¨å‚æ•°æ•ˆç‡ä½å’Œç¼ºä¹å¯è§£é‡Šè®­ç»ƒç­–ç•¥çš„é—®é¢˜ã€‚</li>
<li>FDA-VAEå°†è¾“å…¥å’Œç›®æ ‡å›¾åƒç¼–ç ä¸ºä¸¤ä¸ªå…³äºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„å¯¹ç§°æ½œåœ¨åˆ†å¸ƒï¼Œæœ‰æ•ˆåˆ†ç¦»å…±äº«å’Œç‹¬ç«‹ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨Yå½¢åŒå‘è®­ç»ƒç­–ç•¥æé«˜ç‰¹å¾åˆ†ç¦»çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>FDA-VAEæ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°å’Œæ¨ç†æ—¶é—´ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFDA-VAEæœ‰æ•ˆæé«˜åˆæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d0139c345ad1e973083485365699e76.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a50d0a7eb291e0590bd2bb6ba2a673b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943875&auth_key=1759943875-0-0-5031dcce5ba4ddb831e6f84be9c18b0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c923a527d5c9e483dcf5ce14435c6f43.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-024673361653e92a15abb95ac3191849~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943889&auth_key=1759943889-0-0-98a40994847436de91d0afab3e37e6ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multimodal-Carotid-Risk-Stratification-with-Large-Vision-Language-Models-Benchmarking-Fine-Tuning-and-Clinical-Insights"><a href="#Multimodal-Carotid-Risk-Stratification-with-Large-Vision-Language-Models-Benchmarking-Fine-Tuning-and-Clinical-Insights" class="headerlink" title="Multimodal Carotid Risk Stratification with Large Vision-Language   Models: Benchmarking, Fine-Tuning, and Clinical Insights"></a>Multimodal Carotid Risk Stratification with Large Vision-Language   Models: Benchmarking, Fine-Tuning, and Clinical Insights</h2><p><strong>Authors:Daphne Tsolissou, Theofanis Ganitidis, Konstantinos Mitsis, Stergios CHristodoulidis, Maria Vakalopoulou, Konstantina Nikita</strong></p>
<p>Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation. </p>
<blockquote>
<p>å¯¹é¢ˆåŠ¨è„‰ç²¥æ ·æ–‘å—ç–¾ç—…è¿›è¡Œå¯é çš„é£é™©è¯„ä¼°ä»ç„¶æ˜¯ä¸´åºŠä¸Šçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦ä»¥é€æ˜å’Œä¸´åºŠåŒ»ç”Ÿå¯è§£é‡Šçš„æ–¹å¼æ•´åˆå„ç§ä¸´åºŠå’Œæˆåƒä¿¡æ¯ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æœ€å…ˆè¿›çš„è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€é¢ˆåŠ¨è„‰æ–‘å—è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡æ•´åˆè¶…å£°æˆåƒï¼ˆUSIï¼‰ä¸ç»“æ„åŒ–ä¸´åºŠã€äººå£ç»Ÿè®¡ã€å®éªŒå®¤å’Œè›‹ç™½è´¨ç”Ÿç‰©æ ‡å¿—ç‰©æ•°æ®ã€‚æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿç°å®è¯Šæ–­åœºæ™¯çš„æ¡†æ¶ï¼Œé€šè¿‡è®¿è°ˆå¼é—®é¢˜åºåˆ—è¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—å¼€æºLVLMsï¼Œæ—¢åŒ…æ‹¬é€šç”¨æ¨¡å‹ä¹ŸåŒ…æ‹¬åŒ»å­¦è°ƒä¼˜æ¨¡å‹ã€‚é›¶æ ·æœ¬å®éªŒè¡¨æ˜ï¼Œå³ä½¿å®ƒä»¬éå¸¸å¼ºå¤§ï¼Œå¹¶éæ‰€æœ‰çš„LVLMséƒ½èƒ½å‡†ç¡®åœ°è¯†åˆ«æˆåƒæ¨¡æ€å’Œè§£å‰–å­¦ç»“æ„ï¼Œè€Œå®ƒä»¬åœ¨å‡†ç¡®çš„é£é™©åˆ†ç±»æ–¹é¢çš„è¡¨ç°éƒ½å¾ˆå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å°†LLaVa-NeXT-Vicunaé€‚åº”äºè¶…å£°é¢†åŸŸï¼Œä»è€Œæå¤§åœ°æé«˜äº†ä¸­é£é£é™©åˆ†å±‚çš„æ•ˆæœã€‚ä»¥æ–‡æœ¬å½¢å¼æ•´åˆå¤šæ¨¡æ€è¡¨æ ¼æ•°æ®è¿›ä¸€æ­¥æé«˜äº†ç‰¹å¼‚æ€§å’Œå¹³è¡¡ç²¾åº¦ï¼Œä¸åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„å…ˆå‰å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†LVLMsåœ¨è¶…å£°å¿ƒåŠ¨å›¾å¿ƒè¡€ç®¡é£é™©é¢„æµ‹ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€æ•´åˆã€æ¨¡å‹æ ¡å‡†å’Œé¢†åŸŸé€‚åº”å¯¹äºä¸´åºŠè½¬åŒ–çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02922v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æ¢è®¨äº†å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€é¢ˆåŠ¨è„‰æ–‘å—è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡æ•´åˆè¶…å£°æˆåƒä¸ç»“æ„åŒ–ä¸´åºŠã€äººå£å­¦ã€å®éªŒå®¤å’Œè›‹ç™½è´¨ç”Ÿç‰©æ ‡å¿—ç‰©æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®è¯Šæ–­åœºæ™¯ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLVLMsåœ¨è¯†åˆ«æˆåƒæ¨¡æ€å’Œè§£å‰–ç»“æ„æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸”é£é™©åˆ†ç±»å‡†ç¡®æ€§è¾ƒä½ã€‚é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰é€‚åº”è¶…å£°é¢†åŸŸåï¼Œæ€§èƒ½æœ‰æ‰€æå‡ã€‚å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®çš„æ–‡æœ¬å½¢å¼è¿›ä¸€æ­¥æé«˜äº†ç‰¹å¼‚æ€§å’Œå¹³è¡¡ç²¾åº¦ï¼Œä¸åŸºäºåŒä¸€æ•°æ®é›†çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åŸºå‡†æµ‹è¯•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€æ•´åˆã€æ¨¡å‹æ ¡å‡†å’Œé¢†åŸŸé€‚åº”åœ¨è¶…å£°å¿ƒè¡€ç®¡é£é™©é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯é çš„é¢ˆåŠ¨è„‰ç²¥æ ·ç–¾ç—…é£é™©è¯„ä¼°æ˜¯ä¸´åºŠä¸Šçš„é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆå„ç§ä¸´åºŠå’Œæˆåƒä¿¡æ¯ï¼Œä¸”è¿™äº›ä¿¡æ¯å¿…é¡»é€æ˜ä¸”å¯¹ä¸´åºŠåŒ»ç”Ÿå…·æœ‰å¯è§£é‡Šæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€é¢ˆåŠ¨è„‰æ–‘å—è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•´åˆè¶…å£°æˆåƒä¸å¤šç§ä¸´åºŠæ•°æ®ã€‚</li>
<li>é›¶æ ·æœ¬å®éªŒè¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰LVLMséƒ½èƒ½å‡†ç¡®è¯†åˆ«æˆåƒæ¨¡æ€å’Œè§£å‰–ç»“æ„ï¼Œä¸”é£é™©åˆ†ç±»å‡†ç¡®æ€§æ™®éè¾ƒä½ã€‚</li>
<li>é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼ŒLVLMsåœ¨è¶…å£°é¢†åŸŸçš„é€‚åº”æ€§å¾—åˆ°æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å’ä¸­é£é™©åˆ†å±‚æ–¹é¢ã€‚</li>
<li>å¤šæ¨¡æ€è¡¨æ ¼æ•°æ®çš„æ•´åˆè¿›ä¸€æ­¥æé«˜äº†è¯Šæ–­çš„ç‰¹å¼‚æ€§å’Œå¹³è¡¡ç²¾åº¦ã€‚</li>
<li>ä¸åŸºäºåŒä¸€æ•°æ®é›†çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç›¸æ¯”ï¼ŒLVLMsè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-899e34734ce94677000c46b6ddff4035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a638422e612fc138616f20e4575f45d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2786f5404aaa532e116bb825b31bd191.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d3fe2e35ff3679831c151f6727d0489~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943917&auth_key=1759943917-0-0-947ca59ed70e1e5f3d68e96ef333c6fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9386c0f5ae7218fad3db0b74fb6a4cb8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943924&auth_key=1759943924-0-0-f730a35e3a69980e0f6d9c50b38bf67c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PyRadiomics-cuda-a-GPU-accelerated-3D-features-extraction-from-medical-images-within-PyRadiomics"><a href="#PyRadiomics-cuda-a-GPU-accelerated-3D-features-extraction-from-medical-images-within-PyRadiomics" class="headerlink" title="PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical   images within PyRadiomics"></a>PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical   images within PyRadiomics</h2><p><strong>Authors:Jakub Lisowski, Piotr Tyrakowski, Szymon ZyguÅ‚a, Krzysztof Kaczmarski</strong></p>
<p>PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library, designed to address the computational challenges of extracting three-dimensional shape features from medical images. By offloading key geometric computations to GPU hardware it dramatically reduces processing times for large volumetric datasets. The system maintains full compatibility with the original PyRadiomics API, enabling seamless integration into existing AI workflows without code modifications. This transparent acceleration facilitates efficient, scalable radiomics analysis, supporting rapid feature extraction essential for high-throughput AI pipeline. Tests performed on a typical computational cluster, budget and home devices prove usefulness in all scenarios. PyRadiomics-cuda is implemented in Python and C&#x2F;CUDA and is freely available under the BSD license at <a target="_blank" rel="noopener" href="https://github.com/mis-wut/pyradiomics-CUDA">https://github.com/mis-wut/pyradiomics-CUDA</a> Additionally PyRadiomics-cuda test suite is available at <a target="_blank" rel="noopener" href="https://github.com/mis-wut/pyradiomics-cuda-data-gen">https://github.com/mis-wut/pyradiomics-cuda-data-gen</a>. It provides detailed handbook and sample scripts suited for different kinds of workflows plus detailed installation instructions. The dataset used for testing is available at Kaggle <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19">https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19</a> </p>
<blockquote>
<p>PyRadiomics-cudaæ˜¯PyRadiomicsåº“çš„GPUåŠ é€Ÿæ‰©å±•ï¼Œæ—¨åœ¨è§£å†³ä»åŒ»å­¦å›¾åƒä¸­æå–ä¸‰ç»´å½¢çŠ¶ç‰¹å¾çš„è®¡ç®—æŒ‘æˆ˜ã€‚é€šè¿‡å°†å…³é”®çš„å‡ ä½•è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°GPUç¡¬ä»¶ä¸Šï¼Œå®ƒæå¤§åœ°å‡å°‘äº†å¤„ç†å¤§å‹ä½“ç§¯æ•°æ®é›†çš„å¤„ç†æ—¶é—´ã€‚è¯¥ç³»ç»Ÿä¸åŸå§‹PyRadiomics APIä¿æŒå®Œå…¨å…¼å®¹æ€§ï¼Œæ— éœ€ä¿®æ”¹ä»£ç å³å¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„AIå·¥ä½œæµç¨‹ä¸­ã€‚è¿™ç§é€æ˜çš„åŠ é€Ÿä¿ƒè¿›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„æ”¾å°„å­¦åˆ†æï¼Œæ”¯æŒé«˜é€ŸAIç®¡é“æ‰€éœ€çš„å…³é”®ç‰¹å¾æå–ã€‚åœ¨å…¸å‹çš„è®¡ç®—é›†ç¾¤ã€é¢„ç®—å’Œå®¶åº­è®¾å¤‡ä¸Šè¿›è¡Œçš„æµ‹è¯•è¯æ˜äº†å…¶åœ¨æ‰€æœ‰åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚PyRadiomics-cudaä½¿ç”¨Pythonå’ŒC&#x2F;CUDAå®ç°ï¼Œå¯åœ¨BSDè®¸å¯ä¸‹å…è´¹ä½¿ç”¨ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/mis-wut/pyradiomics-CUDA">https://github.com/mis-wut/pyradiomics-CUDA</a>ã€‚æ­¤å¤–ï¼ŒPyRadiomics-cudaæµ‹è¯•å¥—ä»¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mis-wut/pyradiomics-cuda-data-gen">https://github.com/mis-wut/pyradiomics-cuda-data-gen</a>ä¸Šæ‰¾åˆ°ã€‚å®ƒæä¾›äº†é’ˆå¯¹ä¸åŒç±»å‹å·¥ä½œæµç¨‹çš„è¯¦ç»†æ‰‹å†Œå’Œæ ·æœ¬è„šæœ¬ï¼Œä»¥åŠè¯¦ç»†çš„å®‰è£…è¯´æ˜ã€‚ç”¨äºæµ‹è¯•çš„æ•°æ®é›†å¯åœ¨Kaggleä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19">https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PyRadiomics-cudaæ˜¯PyRadiomicsåº“çš„GPUåŠ é€Ÿæ‰©å±•ï¼Œæ—¨åœ¨è§£å†³ä»åŒ»å­¦å›¾åƒä¸­æå–ä¸‰ç»´å½¢çŠ¶ç‰¹å¾çš„è®¡ç®—æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡å¸è½½å…³é”®å‡ ä½•è®¡ç®—åˆ°GPUç¡¬ä»¶ï¼Œæ˜¾è‘—å‡å°‘äº†å¤§å‹ä½“ç§¯æ•°æ®é›†çš„å¤„ç†æ—¶é—´ã€‚è¯¥ç³»ç»Ÿä¸åŸå§‹PyRadiomics APIå®Œå…¨å…¼å®¹ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„äººå·¥æ™ºèƒ½å·¥ä½œæµç¨‹ä¸­ï¼Œæ— éœ€è¿›è¡Œä»£ç ä¿®æ”¹ã€‚æ­¤é€æ˜çš„åŠ é€Ÿä¿ƒè¿›é«˜æ•ˆã€å¯æ‰©å±•çš„æ”¾å°„å­¦åˆ†æï¼Œæ”¯æŒå¿«é€Ÿç‰¹å¾æå–ï¼Œå¯¹äºé«˜æµé‡çš„äººå·¥æ™ºèƒ½ç®¡é“è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PyRadiomics-cudaæ˜¯PyRadiomicsåº“çš„æ‰©å±•ï¼Œä¸“ä¸ºåŠ é€ŸåŒ»å­¦å›¾åƒä¸­çš„ä¸‰ç»´å½¢çŠ¶ç‰¹å¾æå–è®¡ç®—è€Œè®¾è®¡ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨GPUç¡¬ä»¶ï¼ŒPyRadiomics-cudaæ˜¾è‘—å‡å°‘äº†å¤§å‹æ•°æ®é›†çš„å¤„ç†æ—¶é—´ã€‚</li>
<li>ç³»ç»Ÿä¸åŸå§‹PyRadiomics APIå…¼å®¹ï¼Œå¯è½»æ¾é›†æˆåˆ°ç°æœ‰çš„äººå·¥æ™ºèƒ½å·¥ä½œæµç¨‹ä¸­ã€‚</li>
<li>é€æ˜çš„åŠ é€Ÿæœºåˆ¶ä¿ƒè¿›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„æ”¾å°„å­¦åˆ†æã€‚</li>
<li>PyRadiomics-cudaæ”¯æŒå¿«é€Ÿç‰¹å¾æå–ï¼Œå¯¹äºé«˜æµé‡çš„äººå·¥æ™ºèƒ½ç®¡é“è‡³å…³é‡è¦ã€‚</li>
<li>PyRadiomics-cudaåœ¨å¸¸è§„è®¡ç®—é›†ç¾¤ã€ç»æµå‹è®¾å¤‡ä»¥åŠå®¶ç”¨è®¾å¤‡ä¸Šå‡ç»è¿‡æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§åœºæ™¯ä¸‹çš„å®ç”¨æ€§ã€‚</li>
<li>PyRadiomics-cudaæä¾›è¯¦ç»†æ‰‹å†Œã€é€‚ç”¨äºä¸åŒå·¥ä½œæµç¨‹çš„ç¤ºä¾‹è„šæœ¬ä»¥åŠè¯¦ç»†çš„å®‰è£…è¯´æ˜ã€‚æµ‹è¯•æ•°æ®é›†å¯åœ¨Kaggleä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26b53ec9a0ca648e5ca375c14b930b3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31dd5c57dd0e3753553363126c135136.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1248455fc5683f49bf5d4e3da81ed4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e75009714b54350ad456b079e1a2042.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-75a1a416b65d69a49bc795fef8a5c9cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943958&auth_key=1759943958-0-0-d0954d9c857e069a243589f626408bb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Med-K2N-Flexible-K-to-N-Modality-Translation-for-Medical-Image-Synthesis"><a href="#Med-K2N-Flexible-K-to-N-Modality-Translation-for-Medical-Image-Synthesis" class="headerlink" title="Med-K2N: Flexible K-to-N Modality Translation for Medical Image   Synthesis"></a>Med-K2N: Flexible K-to-N Modality Translation for Medical Image   Synthesis</h2><p><strong>Authors:Feng Yuan, Yifan Gao, Yuehua Ye, Haoyue Li, Xin Gao</strong></p>
<p>Cross-modal medical image synthesis research focuses on reconstructing missing imaging modalities from available ones to support clinical diagnosis. Driven by clinical necessities for flexible modality reconstruction, we explore K to N medical generation, where three critical challenges emerge: How can we model the heterogeneous contributions of different modalities to various target tasks? How can we ensure fusion quality control to prevent degradation from noisy information? How can we maintain modality identity consistency in multi-output generation? Driven by these clinical necessities, and drawing inspiration from SAM2â€™s sequential frame paradigm and cliniciansâ€™ progressive workflow of incrementally adding and selectively integrating multi-modal information, we treat multi-modal medical data as sequential frames with quality-driven selection mechanisms. Our key idea is to â€œlearnâ€ adaptive weights for each modality-task pair and â€œmemorizeâ€ beneficial fusion patterns through progressive enhancement. To achieve this, we design three collaborative modules: PreWeightNet for global contribution assessment, ThresholdNet for adaptive filtering, and EffiWeightNet for effective weight computation. Meanwhile, to maintain modality identity consistency, we propose the Causal Modality Identity Module (CMIM) that establishes causal constraints between generated images and target modality descriptions using vision-language modeling. Extensive experimental results demonstrate that our proposed Med-K2N outperforms state-of-the-art methods by significant margins on multiple benchmarks. Source code is available. </p>
<blockquote>
<p>è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆç ”ç©¶æ—¨åœ¨åˆ©ç”¨ç°æœ‰æˆåƒæ¨¡å¼é‡å»ºç¼ºå¤±çš„æˆåƒæ¨¡å¼ï¼Œä»¥æ”¯æŒä¸´åºŠè¯Šæ–­ã€‚å—ä¸´åºŠå¯¹çµæ´»æ¨¡æ€é‡å»ºçš„éœ€æ±‚é©±åŠ¨ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»Kåˆ°Nçš„åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå…¶ä¸­å‡ºç°äº†ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæˆ‘ä»¬å¦‚ä½•å»ºæ¨¡ä¸åŒæ¨¡æ€å¯¹å„é¡¹ä»»åŠ¡çš„å¼‚è´¨è´¡çŒ®ï¼Ÿæˆ‘ä»¬å¦‚ä½•ç¡®ä¿èåˆè´¨é‡æ§åˆ¶ï¼Œé˜²æ­¢å™ªå£°ä¿¡æ¯çš„é€€åŒ–ï¼Ÿæˆ‘ä»¬å¦‚ä½•åœ¨å¤šè¾“å‡ºç”Ÿæˆä¸­ä¿æŒæ¨¡æ€èº«ä»½çš„ä¸€è‡´æ€§ï¼Ÿå—è¿™äº›ä¸´åºŠéœ€æ±‚çš„é©±åŠ¨ï¼Œå¹¶å—åˆ°SAM2çš„é¡ºåºæ¡†æ¶èŒƒå¼å’Œä¸´åºŠåŒ»ç”Ÿé€æ­¥æ·»åŠ å’Œé€‰æ‹©æ€§æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯çš„å·¥ä½œæµç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬å°†å¤šæ¨¡æ€åŒ»å­¦æ•°æ®è§†ä¸ºå…·æœ‰è´¨é‡é©±åŠ¨é€‰æ‹©æœºåˆ¶çš„é¡ºåºå¸§ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œå­¦ä¹ â€æ¯ä¸ªæ¨¡æ€ä»»åŠ¡å¯¹çš„è‡ªé€‚åº”æƒé‡ï¼Œå¹¶é€šè¿‡é€æ­¥å¢å¼ºæ¥â€œè®°å¿†â€æœ‰ç›Šçš„èåˆæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªåä½œæ¨¡å—ï¼šç”¨äºå…¨å±€è´¡çŒ®è¯„ä¼°çš„PreWeightNetã€ç”¨äºè‡ªé€‚åº”è¿‡æ»¤çš„ThresholdNetã€ä»¥åŠç”¨äºæœ‰æ•ˆæƒé‡è®¡ç®—EffiWeightNetã€‚åŒæ—¶ï¼Œä¸ºäº†ä¿æŒæ¨¡æ€èº«ä»½çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœæ¨¡æ€èº«ä»½æ¨¡å—ï¼ˆCMIMï¼‰ï¼Œå®ƒä½¿ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡åœ¨ç”Ÿæˆçš„å›¾åƒå’Œç›®æ ‡æ¨¡æ€æè¿°ä¹‹é—´å»ºç«‹å› æœçº¦æŸã€‚å¹¿æ³›çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„Med-K2Nåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02815v1">PDF</a> ICLR2026 under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆï¼Œæ—¨åœ¨ä»å·²æœ‰çš„æˆåƒæ¨¡æ€é‡å»ºç¼ºå¤±çš„æ¨¡æ€ï¼Œä»¥æ”¯æŒä¸´åºŠè¯Šæ–­ã€‚æ–‡ç« æå‡ºäº†Med-K2Næ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®çš„é¡ºåºæ¡†æ¶å’Œä¸´åºŠåŒ»ç”Ÿé€æ­¥çš„å·¥ä½œæµç¨‹ï¼Œå¯¹å¤šæ¨¡æ€åŒ»å­¦æ•°æ®è¿›è¡Œå¤„ç†ã€‚é€šè¿‡è®¾è®¡PreWeightNetã€ThresholdNetå’ŒEffiWeightNetä¸‰ä¸ªåä½œæ¨¡å—ä»¥åŠCausal Modality Identity Moduleï¼ˆCMIMï¼‰æ¥ä¼˜åŒ–åˆæˆè¿‡ç¨‹å¹¶ç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¿æŒç›®æ ‡æ¨¡æ€çš„èº«ä»½ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆæ—¨åœ¨ä»ç°æœ‰æˆåƒæ¨¡æ€é‡å»ºç¼ºå¤±çš„æ¨¡æ€ï¼Œä»¥è¾…åŠ©ä¸´åºŠè¯Šæ–­ã€‚</li>
<li>Med-K2Næ–¹æ³•åŸºäºä¸´åºŠéœ€æ±‚é©±åŠ¨ï¼Œæ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®çš„é¡ºåºæ¡†æ¶å’ŒåŒ»ç”Ÿé€æ­¥é›†æˆå¤šæ¨¡æ€ä¿¡æ¯çš„æµç¨‹ã€‚</li>
<li>Med-K2Né€šè¿‡è®¾è®¡PreWeightNetã€ThresholdNetå’ŒEffiWeightNetä¸‰ä¸ªåä½œæ¨¡å—ï¼Œå®ç°è‡ªé€‚åº”æƒé‡å­¦ä¹ å’Œèåˆæ¨¡å¼çš„è®°å¿†ã€‚</li>
<li>ä¸ºä¿æŒæ¨¡æ€èº«ä»½ä¸€è‡´æ€§ï¼Œæå‡ºäº†Causal Modality Identity Moduleï¼ˆCMIMï¼‰ã€‚</li>
<li>CMIMåˆ©ç”¨è§†è§‰è¯­è¨€å»ºæ¨¡ï¼Œåœ¨ç”Ÿæˆçš„å›¾åƒå’Œç›®æ ‡æ¨¡æ€æè¿°ä¹‹é—´å»ºç«‹å› æœçº¦æŸã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒMed-K2Nåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-55e9b38c3325b4414f38537bb26fed11~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943966&auth_key=1759943966-0-0-e733971947de5659dc276b85e0ec31ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83b6a9baedac45c6510cb1d4361fa097~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943973&auth_key=1759943973-0-0-b552d5bce6317f9970f7966ef8b401dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-293e6167cd89919cac742742a27d4469.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Align-Your-Query-Representation-Alignment-for-Multimodality-Medical-Object-Detection"><a href="#Align-Your-Query-Representation-Alignment-for-Multimodality-Medical-Object-Detection" class="headerlink" title="Align Your Query: Representation Alignment for Multimodality Medical   Object Detection"></a>Align Your Query: Representation Alignment for Multimodality Medical   Object Detection</h2><p><strong>Authors:Ara Seo, Bryan Sangwoo Kim, Hyungjin Chung, Jong Chul Ye</strong></p>
<p>Medical object detection suffers when a single detector is trained on mixed medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces. To address this challenge, we turn to representation alignment, an approach that has proven effective for bringing features from different sources into a shared space. Specifically, we target the representations of DETR-style object queries and propose a simple, detector-agnostic framework to align them with modality context. First, we define modality tokens: compact, text-derived embeddings encoding imaging modality that are lightweight and require no extra annotations. We integrate the modality tokens into the detection process via Multimodality Context Attention (MoCA), mixing object-query representations via self-attention to propagate modality context within the query set. This preserves DETR-style architectures and adds negligible latency while injecting modality cues into object queries. We further introduce QueryREPA, a short pretraining stage that aligns query representations to their modality tokens using a task-specific contrastive objective with modality-balanced batches. Together, MoCA and QueryREPA produce modality-aware, class-faithful queries that transfer effectively to downstream training. Across diverse modalities trained altogether, the proposed approach consistently improves AP with minimal overhead and no architectural modifications, offering a practical path toward robust multimodality medical object detection. Project page: <a target="_blank" rel="noopener" href="https://araseo.github.io/alignyourquery/">https://araseo.github.io/alignyourquery/</a>. </p>
<blockquote>
<p>åŒ»å­¦å¯¹è±¡æ£€æµ‹åœ¨å•ä¸€æ£€æµ‹å™¨å¯¹æ··åˆåŒ»å­¦æ¨¡æ€ï¼ˆå¦‚CXRã€CTã€MRIï¼‰è¿›è¡Œè®­ç»ƒæ—¶ä¼šé‡åˆ°å›°éš¾ï¼Œè¿™æ˜¯ç”±äºç»Ÿè®¡æ•°æ®çš„å¼‚è´¨æ€§å’Œè¡¨ç¤ºç©ºé—´çš„åˆ†ç¦»æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è½¬å‘è¡¨ç¤ºå¯¹é½ï¼Œè¿™æ˜¯ä¸€ç§å·²è¢«è¯æ˜å¯ä»¥å°†ä¸åŒæ¥æºçš„ç‰¹å¾å¸¦å…¥å…±äº«ç©ºé—´çš„æœ‰æ•ˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é’ˆå¯¹DETRé£æ ¼çš„å¯¹è±¡æŸ¥è¯¢è¡¨ç¤ºï¼Œæå‡ºä¸€ä¸ªç®€å•ã€é€šç”¨çš„æ¡†æ¶ï¼Œå°†å…¶ä¸æ¨¡æ€ä¸Šä¸‹æ–‡å¯¹é½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡æ€ä»¤ç‰Œï¼šç´§å‡‘ã€æ–‡æœ¬è¡ç”Ÿçš„åµŒå…¥ç¼–ç æˆåƒæ¨¡æ€ï¼Œå®ƒä»¬è½»ä¾¿ä¸”æ— éœ€é¢å¤–æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ³¨æ„åŠ›ï¼ˆMoCAï¼‰å°†æ¨¡æ€ä»¤ç‰Œé›†æˆåˆ°æ£€æµ‹è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›å°†å¯¹è±¡æŸ¥è¯¢è¡¨ç¤ºæ··åˆï¼Œåœ¨æŸ¥è¯¢é›†ä¸­ä¼ æ’­æ¨¡æ€ä¸Šä¸‹æ–‡ã€‚è¿™ä¿ç•™äº†DETRé£æ ¼æ¶æ„ï¼Œå¢åŠ äº†å¾®ä¸è¶³é“çš„å»¶è¿Ÿï¼ŒåŒæ—¶å°†æ¨¡æ€çº¿ç´¢æ³¨å…¥å¯¹è±¡æŸ¥è¯¢ä¸­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†QueryREPAï¼Œè¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„å¯¹æ¯”ç›®æ ‡å’Œæ¨¡æ€å¹³è¡¡æ‰¹æ¬¡å°†æŸ¥è¯¢è¡¨ç¤ºä¸ä»–ä»¬çš„æ¨¡æ€ä»¤ç‰Œå¯¹é½ã€‚MoCAå’ŒQueryREPAå…±åŒä½œç”¨ï¼Œäº§ç”Ÿæ„ŸçŸ¥æ¨¡æ€çš„ç±»å¿ å®æŸ¥è¯¢ï¼Œæœ‰æ•ˆåœ°è½¬ç§»åˆ°ä¸‹æ¸¸è®­ç»ƒä¸­ã€‚é€šè¿‡ä¸åŒçš„æ¨¡æ€å…±åŒè®­ç»ƒï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®åº¦ï¼ˆAPï¼‰ä¸Šå®ç°äº†ä¸€è‡´çš„æ”¹è¿›ï¼Œå¸¦æ¥äº†æœ€å°çš„å¼€é”€å¹¶ä¸”ä¸éœ€è¦ä¿®æ”¹æ¶æ„ã€‚è¿™ä¸ºæœç€ç¨³å¥çš„å¤šæ¨¡æ€åŒ»å­¦å¯¹è±¡æ£€æµ‹æä¾›äº†ä¸€æ¡å®ç”¨é€”å¾„ã€‚<a target="_blank" rel="noopener" href="https://araseo.github.io/alignyourquery/">é¡¹ç›®é¡µé¢</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02789v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://araseo.github.io/alignyourquery/">https://araseo.github.io/alignyourquery/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒå¤šæ¨¡æ€æ£€æµ‹ä¸­çš„å¼‚è´¨ç»Ÿè®¡å’Œç¦»æ•£è¡¨ç¤ºç©ºé—´é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºè¡¨ç¤ºå¯¹é½çš„æ–¹æ³•ï¼Œé€šè¿‡å®šä¹‰æ¨¡æ€ä»¤ç‰Œå¹¶é›†æˆåˆ°æ£€æµ‹è¿‡ç¨‹ä¸­ï¼Œå®ç°å¯¹ä¸åŒæ¨¡æ€çš„å…±äº«ç©ºé—´è¡¨ç¤ºã€‚å¼•å…¥å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ³¨æ„åŠ›ï¼ˆMoCAï¼‰å’ŒQueryREPAé¢„è®­ç»ƒé˜¶æ®µï¼Œæœ‰æ•ˆæé«˜å¤šæ¨¡æ€åŒ»å­¦å¯¹è±¡æ£€æµ‹çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å¯¹è±¡æ£€æµ‹åœ¨æ··åˆåŒ»å­¦æ¨¡æ€ï¼ˆå¦‚CXRã€CTã€MRIï¼‰ä¸Šçš„å•ä¸€æ£€æµ‹å™¨è®­ç»ƒä¼šé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¼‚è´¨ç»Ÿè®¡å’Œç¦»æ•£è¡¨ç¤ºç©ºé—´é€ æˆã€‚</li>
<li>æå‡ºä½¿ç”¨è¡¨ç¤ºå¯¹é½æ–¹æ³•æ¥è§£å†³æ­¤æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆå°†ä¸åŒæ¥æºçš„ç‰¹å¾å¸¦å…¥å…±äº«ç©ºé—´ã€‚</li>
<li>ç›®æ ‡æ˜¯é’ˆå¯¹DETRé£æ ¼çš„å¯¹è±¡æŸ¥è¯¢è¡¨ç¤ºï¼Œæå‡ºä¸€ä¸ªç®€å•ã€æ£€æµ‹å™¨æ— å…³çš„æ¡†æ¶æ¥è¿›è¡Œå¯¹é½ï¼Œå¹¶ä¸æ¨¡æ€ä¸Šä¸‹æ–‡é›†æˆã€‚</li>
<li>å®šä¹‰æ¨¡æ€ä»¤ç‰Œï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘çš„ã€åŸºäºæ–‡æœ¬çš„åµŒå…¥ï¼Œç¼–ç æˆåƒæ¨¡æ€ï¼Œè½»ä¾¿ä¸”æ— éœ€é¢å¤–æ³¨é‡Šã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ³¨æ„åŠ›ï¼ˆMoCAï¼‰å°†æ¨¡æ€ä»¤ç‰Œé›†æˆåˆ°æ£€æµ‹è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æ··åˆå¯¹è±¡æŸ¥è¯¢è¡¨ç¤ºï¼Œåœ¨æŸ¥è¯¢é›†ä¸­ä¼ æ’­æ¨¡æ€ä¸Šä¸‹æ–‡ã€‚</li>
<li>å¼•å…¥QueryREPAé¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡å¯¹æŸ¥è¯¢è¡¨ç¤ºä¸æ¨¡æ€ä»¤ç‰Œè¿›è¡Œå¯¹é½ï¼Œä½¿ç”¨å…·æœ‰æ¨¡æ€å¹³è¡¡æ‰¹æ¬¡çš„ä»»åŠ¡ç‰¹å®šå¯¹æ¯”ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bf80da5a04a235dc57f662e8f8f275d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e1604ed9686ba2891bade64c35bb078.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4bc1bd9727978aaae38e05a4b7c9600~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944000&auth_key=1759944000-0-0-88e0d39e0ba87b38b618f2bb467da74a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Generalized-Category-Discovery-for-Brain-Tumor-Classification-in-Digital-Pathology"><a href="#Hierarchical-Generalized-Category-Discovery-for-Brain-Tumor-Classification-in-Digital-Pathology" class="headerlink" title="Hierarchical Generalized Category Discovery for Brain Tumor   Classification in Digital Pathology"></a>Hierarchical Generalized Category Discovery for Brain Tumor   Classification in Digital Pathology</h2><p><strong>Authors:Matthias Perkonigg, Patrick Rockenschaub, Georg GÃ¶bel, Adelheid WÃ¶hrer</strong></p>
<p>Accurate brain tumor classification is critical for intra-operative decision making in neuro-oncological surgery. However, existing approaches are restricted to a fixed set of predefined classes and are therefore unable to capture patterns of tumor types not available during training. Unsupervised learning can extract general-purpose features, but it lacks the ability to incorporate prior knowledge from labelled data, and semi-supervised methods often assume that all potential classes are represented in the labelled data. Generalized Category Discovery (GCD) aims to bridge this gap by categorizing both known and unknown classes within unlabelled data. To reflect the hierarchical structure of brain tumor taxonomies, in this work, we introduce Hierarchical Generalized Category Discovery for Brain Tumor Classification (HGCD-BT), a novel approach that integrates hierarchical clustering with contrastive learning. Our method extends contrastive learning based GCD by incorporating a novel semi-supervised hierarchical clustering loss. We evaluate HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images, achieving a +28% improvement in accuracy over state-of-the-art GCD methods for patch-level classification, particularly in identifying previously unseen tumor categories. Furthermore, we demonstrate the generalizability of HGCD-BT on slide-level classification of hematoxylin and eosin stained whole-slide images from the Digital Brain Tumor Atlas, confirming its utility across imaging modalities. </p>
<blockquote>
<p>ç²¾ç¡®çš„è„‘è‚¿ç˜¤åˆ†ç±»å¯¹äºç¥ç»è‚¿ç˜¤å¤–ç§‘æ‰‹æœ¯ä¸­çš„å†³ç­–åˆ¶å®šè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…é™äºä¸€ç»„é¢„å®šä¹‰çš„ç±»åˆ«ï¼Œå› æ­¤æ— æ³•æ•è·è®­ç»ƒæœŸé—´æœªå‡ºç°çš„è‚¿ç˜¤ç±»å‹æ¨¡å¼ã€‚æ— ç›‘ç£å­¦ä¹ å¯ä»¥æå–é€šç”¨ç‰¹å¾ï¼Œä½†å®ƒç¼ºä¹å°†å…ˆéªŒçŸ¥è¯†èå…¥æ ‡æ³¨æ•°æ®çš„èƒ½åŠ›ï¼Œè€ŒåŠç›‘ç£æ–¹æ³•é€šå¸¸å‡è®¾æ‰€æœ‰æ½œåœ¨ç±»åˆ«éƒ½åœ¨æ ‡æ³¨æ•°æ®ä¸­æœ‰æ‰€ä½“ç°ã€‚å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰æ—¨åœ¨é€šè¿‡æ— æ ‡ç­¾æ•°æ®ä¸­çš„å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«åˆ†ç±»æ¥ç¼©å°è¿™ä¸€å·®è·ã€‚ä¸ºäº†åæ˜ è„‘è‚¿ç˜¤åˆ†ç±»çš„å±‚æ¬¡ç»“æ„ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å¼•å…¥äº†ç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»çš„åˆ†å±‚å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆHGCD-BTï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å±‚æ¬¡èšç±»ä¸å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡èå…¥ä¸€ç§æ–°å‹åŠç›‘ç£å±‚æ¬¡èšç±»æŸå¤±æ‰©å±•äº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„GCDã€‚æˆ‘ä»¬åœ¨OpenSRHï¼ˆä¸€ä¸ªæ¨¡æ‹Ÿæ‹‰æ›¼ç»„ç»‡è„‘è‚¿ç˜¤å›¾åƒæ•°æ®é›†ï¼‰ä¸Šå¯¹HGCD-BTè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸å‰æ²¿çš„GCDæ–¹æ³•åœ¨è¡¥ä¸çº§åˆ«åˆ†ç±»ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†+28%ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å…ˆå‰æœªè§è¿‡çš„è‚¿ç˜¤ç±»åˆ«æ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ•°å­—è„‘è‚¿ç˜¤å›¾è°±ä¸­çš„è‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²å…¨ç‰‡å›¾åƒåˆ‡ç‰‡çº§åˆ«çš„åˆ†ç±»ä¸ŠéªŒè¯äº†HGCD-BTçš„é€šç”¨æ€§ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæˆåƒæ¨¡å¼ä¸­çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»çš„æ–°æ–¹æ³•â€”â€”åˆ†å±‚å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆHGCD-BTï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åˆ†å±‚èšç±»å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ—¨åœ¨å¼¥è¡¥ç°æœ‰æ–¹æ³•å¯¹å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«åˆ†ç±»çš„ç©ºç™½ã€‚åœ¨OpenSRHæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHGCD-BTåœ¨æ–‘å—çº§åˆ«åˆ†ç±»çš„å‡†ç¡®åº¦æ¯”æœ€æ–°GCDæ–¹æ³•æé«˜äº†28%ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å…ˆå‰æœªè§è¿‡çš„è‚¿ç˜¤ç±»åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒHGCD-BTåœ¨å…¨è„‘è‚¿ç˜¤å›¾è°±ä¸­çš„HEæŸ“è‰²å…¨å¹»ç¯ç‰‡å›¾åƒå¹»ç¯ç‰‡çº§åˆ«åˆ†ç±»ä¸Šçš„è¡¨ç°ä¹Ÿè¯æ˜äº†å…¶è·¨æˆåƒæ¨¡å¼çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤åˆ†ç±»å¯¹äºç¥ç»è‚¿ç˜¤å¤–ç§‘æ‰‹æœ¯ä¸­çš„å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºé¢„è®¾ç±»åˆ«é›†ï¼Œæ— æ³•è¯†åˆ«è®­ç»ƒæœŸé—´æœªæ¶µç›–çš„è‚¿ç˜¤ç±»å‹æ¨¡å¼ã€‚</li>
<li>åˆ†å±‚å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆHGCDï¼‰æ—¨åœ¨åˆ†ç±»å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«ï¼Œå¼¥è¡¥è¿™ä¸€ç¼ºé™·ã€‚</li>
<li>HGCD-BTæ˜¯HGCDåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä¸­çš„å…·ä½“åº”ç”¨ï¼Œç»“åˆäº†åˆ†å±‚èšç±»å’Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>HGCD-BTåœ¨OpenSRHæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„GCDæ–¹æ³•ï¼Œå°¤å…¶åœ¨è¯†åˆ«å…ˆå‰æœªè§çš„è‚¿ç˜¤ç±»åˆ«æ–¹é¢ã€‚</li>
<li>HGCD-BTåœ¨è·¨æˆåƒæ¨¡å¼ï¼ˆå¦‚ä¸åŒæŸ“è‰²æŠ€æœ¯å’Œå›¾åƒç±»å‹ï¼‰ä¸‹å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ac40c4856276cdcb13a0cef7ac023d99~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944007&auth_key=1759944007-0-0-7d38d61dc93b0f76c1f288ef18a2db5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-840f95102431dfb549a4b3d358ff16da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f83ea7d56e4163b3ca5ee78d1058c8d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Causal-Adapter-Taming-Text-to-Image-Diffusion-for-Faithful-Counterfactual-Generation"><a href="#Causal-Adapter-Taming-Text-to-Image-Diffusion-for-Faithful-Counterfactual-Generation" class="headerlink" title="Causal-Adapter: Taming Text-to-Image Diffusion for Faithful   Counterfactual Generation"></a>Causal-Adapter: Taming Text-to-Image Diffusion for Faithful   Counterfactual Generation</h2><p><strong>Authors:Lei Tong, Zhihua Liu, Chaochao Lu, Dino Oglic, Tom Diethe, Philip Teare, Sotirios A. Tsaftaris, Chen Jin</strong></p>
<p>We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91% MAE reduction on Pendulum for accurate attribute control and 87% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å› æœé€‚é…å™¨ï¼ˆCausal-Adapterï¼‰è¿™ä¸€æ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºé€‚åº”å†»ç»“çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ä¸»å¹²ç½‘ç»œä»¥è¿›è¡Œåäº‹å®å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç›®æ ‡å±æ€§ä¸Šå®æ–½å› æœå¹²é¢„ï¼Œå¹¶æŒç»­åœ°å°†è¿™äº›æ•ˆåº”ä¼ æ’­åˆ°å› æœä¾èµ–é¡¹ä¸Šï¼Œè€Œä¸ä¼šæ”¹å˜å›¾åƒçš„æ ¸å¿ƒèº«ä»½ã€‚ä¸ä»¥å¾€ä¾èµ–æç¤ºå·¥ç¨‹ä¸”æ²¡æœ‰æ˜ç¡®çš„å› æœç»“æ„çš„æ–¹æ³•ç›¸æ¯”ï¼Œå› æœé€‚é…å™¨åˆ©ç”¨ç»“æ„å› æœå»ºæ¨¡å¹¶ç»“åˆä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼šæç¤ºå¯¹é½æ³¨å…¥ï¼Œå°†å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥å¯¹é½ä»¥å®ç°ç²¾ç¡®è¯­ä¹‰æ§åˆ¶ï¼›æ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ï¼Œä»¥è§£å¼€å±æ€§å› ç´ å¹¶å‡å°‘è™šå‡å…³è”ã€‚å› æœé€‚é…å™¨åœ¨åˆæˆå’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ‘†é”¤æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾91%çš„MAEé™ä½ä»¥å®ç°ç²¾ç¡®çš„å±æ€§æ§åˆ¶ï¼Œåœ¨ADNIæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾87%çš„FIDé™ä½ä»¥å®ç°é«˜ä¿çœŸMRIå›¾åƒç”Ÿæˆã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ç¨³å¥ä¸”å¯æ³›åŒ–çš„åäº‹å®ç¼–è¾‘ï¼Œå…·æœ‰å¯é çš„å±æ€§ä¿®æ”¹å’Œå¼ºå¤§çš„èº«ä»½ä¿ç•™åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24798v3">PDF</a> 9 pages, 26 figures</p>
<p><strong>Summary</strong></p>
<p>Causal-Adapteræ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºé€‚åº”å†·å†»æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£ä¸»å¹²ä»¥è¿›è¡Œåäº‹å®å›¾åƒç”Ÿæˆã€‚é€šè¿‡å› æœå¹²é¢„ç›®æ ‡å±æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æ”¹å˜å›¾åƒæ ¸å¿ƒèº«ä»½çš„æƒ…å†µä¸‹ï¼ŒæŒç»­åœ°å°†å¹²é¢„æ•ˆæœä¼ æ’­åˆ°å› æœä¾èµ–é¡¹ã€‚ä¸ä¾èµ–æç¤ºå·¥ç¨‹è€Œæ²¡æœ‰æ˜ç¡®å› æœç»“æ„çš„æ—§æ–¹æ³•ä¸åŒï¼ŒCausal-Adapteråˆ©ç”¨ç»“æ„å› æœå»ºæ¨¡ï¼Œè¾…ä»¥ä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼šæç¤ºå¯¹é½æ³¨å…¥å’Œæ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ã€‚å‰è€…å°†å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œä»¥å®ç°ç²¾ç¡®è¯­ä¹‰æ§åˆ¶ï¼›åè€…ç”¨äºåˆ†è§£å±æ€§å› ç´ å¹¶å‡å°‘å¶ç„¶å…³è”ã€‚Causal-Adapteråœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œåœ¨Pendulumä¸Šå®ç°é«˜è¾¾91%çš„MAEå‡å°‘ï¼Œè¿›è¡Œå‡†ç¡®çš„å±æ€§æ§åˆ¶ï¼Œå¹¶åœ¨ADNIä¸Šå®ç°87%çš„FIDå‡å°‘ï¼Œç”¨äºé«˜ä¿çœŸMRIå›¾åƒç”Ÿæˆã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ç¨³å¥ã€é€šç”¨çš„åäº‹å®ç¼–è¾‘ï¼Œå…·æœ‰å¿ è¯šçš„å±æ€§ä¿®æ”¹å’Œå¼ºå¤§çš„èº«ä»½ä¿ç•™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Causal-Adapteræ˜¯ä¸€ä¸ªé€‚åº”æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºåäº‹å®å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å› æœå¹²é¢„ç›®æ ‡å±æ€§ï¼Œå¹¶æŒç»­åœ°å°†å¹²é¢„æ•ˆæœä¼ æ’­åˆ°å› æœä¾èµ–é¡¹ï¼ŒåŒæ—¶ä¿æŒå›¾åƒçš„æ ¸å¿ƒèº«ä»½ä¸å˜ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒCausal-Adapteråˆ©ç”¨ç»“æ„å› æœå»ºæ¨¡ï¼Œå¹¶è¾…ä»¥ä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼šæç¤ºå¯¹é½æ³¨å…¥å’Œæ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ã€‚</li>
<li>æç¤ºå¯¹é½æ³¨å…¥ç­–ç•¥èƒ½å¤Ÿå°†å› æœå±æ€§ä¸æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„è¯­ä¹‰æ§åˆ¶ã€‚</li>
<li>æ¡ä»¶ä»¤ç‰Œå¯¹æ¯”æŸå¤±ç­–ç•¥æœ‰åŠ©äºåˆ†è§£å±æ€§å› ç´ å¹¶å‡å°‘å¶ç„¶å…³è”ã€‚</li>
<li>Causal-Adapteråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬åœ¨Pendulumä¸Šçš„MAEå‡å°‘91%ï¼Œä»¥åŠåœ¨ADNIä¸Šçš„FIDå‡å°‘87%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e28ce603b03cbc92ef82f96b51a91b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944028&auth_key=1759944028-0-0-4816069cb5d931267363b4b9a5c72636&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-1f91aabcf0a92ff67f7b3aac155b1709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e4b0d05ef2fa28981b9f5a7ae403ede.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3afac3e97d200b9e29b522d2fc74472b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944050&auth_key=1759944050-0-0-8afdf6aaffc93ac47260a5e6c3b519d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-acc1f9c10b5d013a6b0e98327d00595a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12"><a href="#Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12" class="headerlink" title="Robust Pan-Cancer Mitotic Figure Detection with YOLOv12"></a>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</h2><p><strong>Authors:RaphaÃ«l Bourgade, Guillaume Balezo, Thomas Walter</strong></p>
<p>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data. </p>
<blockquote>
<p>æœ‰ä¸åˆ†è£‚å›¾åƒåœ¨è‚¿ç˜¤ç—…ç†å­¦ä¸­æ˜¯ä¸€ä¸ªå…³é”®çš„ç»„ç»‡é¢„åç‰¹å¾ï¼Œä¸ºè‚¿ç˜¤ä¾µè¢­æ€§å’Œå¢æ®–æä¾›äº†é‡è¦è§è§£ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿æ˜¯ç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶ä¹‹é—´ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ‰ä¸åˆ†è£‚åŸŸæ³›åŒ–ï¼ˆMIDOGï¼‰2025æŒ‘æˆ˜æ ‡å¿—ç€æ—¨åœ¨å¼€å‘ç¨³å¥çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ç®—æ³•çš„ç¬¬ä¸‰æ¬¡å›½é™…ç«èµ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„æœ‰ä¸åˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæ­¥æµ‹è¯•é›†ï¼ˆä»…çƒ­ç‚¹ï¼‰ä¸Šè¾¾åˆ°äº†0.801çš„F1åˆ†æ•°ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•æ’è¡Œæ¦œä¸Šä»¥0.7216çš„F1åˆ†æ•°æ’åç¬¬äºŒï¼Œæ¶µç›–äº†å¤æ‚ä¸”å¼‚è´¨çš„å…¨å¹»ç¯ç‰‡åŒºåŸŸï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02593v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‚¿ç˜¤ç—…ç†å­¦ä¸­ï¼Œæœ‰ä¸åˆ†è£‚å›¾åƒä½œä¸ºå…³é”®çš„ç»„ç»‡é¢„åç‰¹å¾ï¼Œå¯¹è‚¿ç˜¤ä¾µè¢­æ€§å’Œå¢æ®–æä¾›é‡è¦è§è§£ã€‚ç„¶è€Œï¼Œå…¶è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå­˜åœ¨æ˜¾è‘—çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒMItosis DOmain Generalizationï¼ˆMIDOGï¼‰æŒ‘æˆ˜æ˜¯ä¸€ä¸ªå›½é™…ç«èµ›çš„ç¬¬ä¸‰ç‰ˆï¼Œæ—¨åœ¨å¼€å‘ç¨³å¥çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ç®—æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„æœ‰ä¸åˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆæ­¥æµ‹è¯•é›†ä¸Šå–å¾—äº†F1åˆ†æ•°ä¸º0.801ï¼ˆä»…é’ˆå¯¹çƒ­ç‚¹åŒºåŸŸï¼‰ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•æ’è¡Œæ¦œä¸Šä»¥F1åˆ†æ•°ä¸º0.7216æ’åç¬¬äºŒï¼Œè¦†ç›–äº†å¤æ‚ä¸”å¼‚è´¨çš„æ•´å¼ å¹»ç¯ç‰‡åŒºåŸŸï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ‰ä¸åˆ†è£‚å›¾åƒæ˜¯è‚¿ç˜¤ç—…ç†å­¦ä¸­é‡è¦çš„ç»„ç»‡é¢„åç‰¹å¾ï¼Œèƒ½åæ˜ è‚¿ç˜¤çš„ä¾µè¢­æ€§å’Œå¢æ®–æƒ…å†µã€‚</li>
<li>æœ‰ä¸åˆ†è£‚å›¾åƒçš„è¯†åˆ«å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸åŒè§‚å¯Ÿè€…ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å·®å¼‚ã€‚</li>
<li>MItosis DOmain Generalization (MIDOG) æŒ‘æˆ˜æ˜¯å›½é™…ä¸Šçš„ç«èµ›ï¼Œæ—¨åœ¨å¼€å‘ç¨³å¥çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ç®—æ³•ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨åŸºäºYOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„æ–¹æ³•ï¼Œè¿›è¡Œæœ‰ä¸åˆ†è£‚å›¾åƒæ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæ­¥æµ‹è¯•é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„F1åˆ†æ•°ï¼ˆ0.801ï¼‰ã€‚</li>
<li>åœ¨æœ€ç»ˆæµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„æ•´å¼ å¹»ç¯ç‰‡åŒºåŸŸä¸Šå–å¾—äº†F1åˆ†æ•°ä¸º0.7216ï¼Œå¹¶æ’åç¬¬äºŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-626f3ad46f602c428490ccabc34cf294.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40ceb8503e1c39aa24ef169cdec97f7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b50a6c9e4da7805379473edf9282fb4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Tutorial-on-MRI-Reconstruction-From-Modern-Methods-to-Clinical-Implications"><a href="#A-Tutorial-on-MRI-Reconstruction-From-Modern-Methods-to-Clinical-Implications" class="headerlink" title="A Tutorial on MRI Reconstruction: From Modern Methods to Clinical   Implications"></a>A Tutorial on MRI Reconstruction: From Modern Methods to Clinical   Implications</h2><p><strong>Authors:Tolga Ã‡ukur, Salman U. H. Dar, Valiyeh Ansarian Nezhad, Yohan Jun, Tae Hyung Kim, Shohei Fujita, Berkin Bilgic</strong></p>
<p>MRI is an indispensable clinical tool, offering a rich variety of tissue contrasts to support broad diagnostic and research applications. Clinical exams routinely acquire multiple structural sequences that provide complementary information for differential diagnosis, while research protocols often incorporate advanced functional, diffusion, spectroscopic, and relaxometry sequences to capture multidimensional insights into tissue structure and composition. However, these capabilities come at the cost of prolonged scan times, which reduce patient throughput, increase susceptibility to motion artifacts, and may require trade-offs in image quality or diagnostic scope. Over the last two decades, advances in image reconstruction algorithmsâ€“alongside improvements in hardware and pulse sequence designâ€“have made it possible to accelerate acquisitions while preserving diagnostic quality. Central to this progress is the ability to incorporate prior information to regularize the solutions to the reconstruction problem. In this tutorial, we overview the basics of MRI reconstruction and highlight state-of-the-art approaches, beginning with classical methods that rely on explicit hand-crafted priors, and then turning to deep learning methods that leverage a combination of learned and crafted priors to further push the performance envelope. We also explore the translational aspects and eventual clinical implications of these methods. We conclude by discussing future directions to address remaining challenges in MRI reconstruction. The tutorial is accompanied by a Python toolbox (<a target="_blank" rel="noopener" href="https://github.com/tutorial-MRI-recon/tutorial">https://github.com/tutorial-MRI-recon/tutorial</a>) to demonstrate select methods discussed in the article. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸´åºŠä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œæä¾›äº†ä¸°å¯Œçš„ç»„ç»‡å¯¹æ¯”åº¦ï¼Œæ”¯æŒå¹¿æ³›çš„è¯Šæ–­å’ŒåŒ»å­¦ç ”ç©¶åº”ç”¨ã€‚åœ¨ä¸´åºŠæ£€æŸ¥ä¸­ï¼Œé€šå¸¸è·å–å¤šä¸ªç»“æ„åºåˆ—ï¼Œä¸ºé‰´åˆ«è¯Šæ–­æä¾›è¡¥å……ä¿¡æ¯ï¼›è€Œç ”ç©¶åè®®é€šå¸¸åŒ…å«é«˜çº§çš„åŠŸèƒ½ã€æ‰©æ•£ã€å…‰è°±å’Œæ¾å¼›åºåˆ—ï¼Œä»¥æ•æ‰å…³äºç»„ç»‡ç»“æ„å’Œç»„æˆçš„å¤šç»´åº¦è§è§£ã€‚ç„¶è€Œï¼Œè¿™äº›åŠŸèƒ½éœ€è¦é•¿æ—¶é—´çš„æ‰«æï¼Œé™ä½äº†æ‚£è€…é€šè¿‡ç‡ï¼Œå¢åŠ äº†è¿åŠ¨ä¼ªå½±çš„æ˜“æ„Ÿæ€§ï¼Œå¹¶å¯èƒ½éœ€è¦ç‰ºç‰²å›¾åƒè´¨é‡æˆ–è¯Šæ–­èŒƒå›´ã€‚åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œéšç€å›¾åƒé‡å»ºç®—æ³•çš„è¿›æ­¥ä»¥åŠç¡¬ä»¶å’Œè„‰å†²åºåˆ—è®¾è®¡çš„æ”¹è¿›ï¼Œåœ¨åŠ é€Ÿé‡‡é›†çš„åŒæ—¶ä¿æŒè¯Šæ–­è´¨é‡æˆä¸ºå¯èƒ½ã€‚è¿™ç§è¿›æ­¥çš„æ ¸å¿ƒæ˜¯èå…¥å…ˆéªŒä¿¡æ¯ä»¥è§„èŒƒåŒ–é‡å»ºé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ç£å…±æŒ¯æˆåƒé‡å»ºçš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†æœ€æ–°æ–¹æ³•ï¼Œé¦–å…ˆä»ä¾èµ–æ˜¾å¼æ‰‹å·¥å…ˆéªŒçš„ç»å…¸æ–¹æ³•å¼€å§‹ï¼Œç„¶åè½¬å‘åˆ©ç”¨å­¦ä¹ å’Œæ‰‹å·¥å…ˆéªŒç›¸ç»“åˆæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥çªç ´æ€§èƒ½æé™ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¿™äº›æ–¹æ³•çš„ç¿»è¯‘æ–¹é¢ä»¥åŠæœ€ç»ˆçš„ä¸´åºŠå½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è§£å†³ç£å…±æŒ¯æˆåƒé‡å»ºä¸­å‰©ä½™æŒ‘æˆ˜çš„æœªæ¥æ–¹å‘ã€‚æœ¬æ•™ç¨‹è¿˜é™„å¸¦äº†ä¸€ä¸ªPythonå·¥å…·ç®±ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/tutorial-MRI-recon/tutorial%EF%BC%89%EF%BC%8C%E4%BB%A5%E6%BC%94%E7%A4%BA%E6%96%87%E7%AB%A0%E4%B8%AD%E8%AE%A8%E8%AE%BA%E7%9A%84%E9%80%89%E6%8B%A9%E6%80%A7%E6%96%B9%E6%B3%95%E3%80%82">https://github.com/tutorial-MRI-recon/tutorialï¼‰ï¼Œä»¥æ¼”ç¤ºæ–‡ç« ä¸­è®¨è®ºçš„é€‰æ‹©æ€§æ–¹æ³•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16715v2">PDF</a> Accepted for publication in IEEE Transactions on Biomedical   Engineering. The final published version is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TBME.2025.3617575">https://doi.org/10.1109/TBME.2025.3617575</a></p>
<p><strong>Summary</strong><br>     æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸´åºŠä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œå…·æœ‰å¤šç§ç»„ç»‡å¯¹æ¯”åº¦ï¼Œæ”¯æŒå¹¿æ³›çš„è¯Šæ–­å’Œç§‘ç ”åº”ç”¨ã€‚ä¸´åºŠæ£€æŸ¥ç»å¸¸è·å–å¤šä¸ªç»“æ„åºåˆ—ï¼Œä¸ºé‰´åˆ«è¯Šæ–­æä¾›è¡¥å……ä¿¡æ¯ï¼Œè€Œç ”ç©¶åè®®é€šå¸¸åŒ…å«é«˜çº§åŠŸèƒ½ã€æ‰©æ•£ã€å…‰è°±å’Œæ¾å¼›åºåˆ—ï¼Œä»¥è·å–å¯¹ç»„ç»‡ç»“æ„å’Œç»„æˆçš„å¤šç»´åº¦è§è§£ã€‚ç„¶è€Œï¼Œè¿™äº›åŠŸèƒ½éœ€è¦å»¶é•¿æ‰«ææ—¶é—´ï¼Œè¿™é™ä½äº†æ‚£è€…é€šè¿‡ç‡ï¼Œå¢åŠ äº†è¿åŠ¨ä¼ªå½±çš„æ˜“æ„Ÿæ€§ï¼Œå¹¶å¯èƒ½å½±å“å›¾åƒè´¨é‡æˆ–è¯Šæ–­èŒƒå›´ã€‚è¿‡å»äºŒåå¹´æ¥ï¼Œå›¾åƒé‡å»ºç®—æ³•çš„è¿›æ­¥ä»¥åŠç¡¬ä»¶å’Œè„‰å†²åºåˆ—è®¾è®¡çš„æ”¹è¿›ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè¯Šæ–­è´¨é‡çš„åŒæ—¶åŠ é€Ÿé‡‡é›†ã€‚æœ¬æ–‡æ¦‚è¿°äº†æ ¸ç£å…±æŒ¯æˆåƒé‡å»ºçš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶ä»‹ç»äº†æœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¾èµ–æ˜ç¡®æ‰‹å·¥å…ˆéªŒçš„ç»å…¸æ–¹æ³•ï¼Œä»¥åŠåˆ©ç”¨å­¦ä¹ å’Œæ‰‹å·¥å…ˆéªŒç»“åˆçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¿™äº›æ–¹æ³•çš„ä¸´åºŠè½¬åŒ–æ–¹é¢åŠå…¶æœ€ç»ˆçš„ä¸´åºŠå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIæ˜¯ä¸´åºŠä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œå…·æœ‰å¤šç§ç»„ç»‡å¯¹æ¯”åº¦ï¼Œå¹¿æ³›åº”ç”¨äºè¯Šæ–­å’Œç§‘ç ”ã€‚</li>
<li>ä¸´åºŠæ£€æŸ¥ç»å¸¸è·å–å¤šä¸ªç»“æ„åºåˆ—ä»¥æ”¯æŒè¯Šæ–­ï¼Œè€Œç ”ç©¶åè®®åŒ…å«é«˜çº§åºåˆ—ä»¥è·å–å¤šç»´åº¦è§è§£ã€‚</li>
<li>æ‰«ææ—¶é—´å»¶é•¿ä¼šå½±å“æ‚£è€…é€šè¿‡ç‡ã€å¢åŠ è¿åŠ¨ä¼ªå½±é£é™©å¹¶å¯èƒ½å½±å“å›¾åƒè´¨é‡å’Œè¯Šæ–­èŒƒå›´ã€‚</li>
<li>è¿‡å»çš„äºŒåå¹´ä¸­ï¼ŒMRIå›¾åƒé‡å»ºç®—æ³•å–å¾—äº†è¿›æ­¥ï¼Œå¯ä»¥åŠ é€Ÿé‡‡é›†è¿‡ç¨‹å¹¶ä¿æŒè¯Šæ–­è´¨é‡ã€‚</li>
<li>æœ€æ–°æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨æ˜ç¡®æ‰‹å·¥å…ˆéªŒçš„ç»å…¸æ–¹æ³•å’Œç»“åˆå­¦ä¹ å’Œæ‰‹å·¥å…ˆéªŒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>è¿™äº›å›¾åƒé‡å»ºæ–¹æ³•çš„ä¸´åºŠè½¬åŒ–åŠå…¶æœ€ç»ˆä¸´åºŠå½±å“æ˜¯è¢«æ¢è®¨çš„é‡è¦è¯é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-384b82e8afe7aedd3c4591433453187d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944108&auth_key=1759944108-0-0-69ad66ca34122bc11d8a9d85e2e42bf6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-b1cfac3b3faf57fa5cb27b6059fe9a8d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a43b2532f4eaf0758455a5ef107afde8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944122&auth_key=1759944122-0-0-b0c1783ca933ded8ad6147d24e800f04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91360251cbceee5c0d9d340f26ec1373~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944128&auth_key=1759944128-0-0-6bba58bf1b48abbc765170c452af66e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-da5fde8b0c6fede4dd174f43b906b90d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-042130fb9680fc66943e0ad5f0b496d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944142&auth_key=1759944142-0-0-4b36b780288639cda5f6b7a74861a573&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the model output on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸæ¨¡å‹è¾“å‡ºåœ¨ä¸€ç»„é¢„å®šä¹‰å’Œå¯äººç±»è§£é‡Šçš„æ¦‚å¿µä¸Šæ¥è§£å†³åè€…é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡è¿™äº›æ¦‚å¿µè§£é‡Šå®ç°çš„å¢åŠ çš„è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœéœ€è¦æ·»åŠ æ–°æ¦‚å¿µï¼Œæ•´ä¸ªç³»ç»Ÿéœ€è¦é‡æ–°è®­ç»ƒã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•è®ºCBVLMï¼Œè¯¥æ–¹æ³•è§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©LVLMåŸºäºä¹‹å‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è€Œä¸”ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­å»ºç«‹åœ¨é¢„æµ‹çš„æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMsçš„å°‘é‡æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªåŒ»å­¦æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»å­¦ï¼‰LVLMçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æ˜¾ç¤ºCBVLMæŒç»­ä¼˜äºCBMså’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ï¼Œè€Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåªéœ€ä½¿ç”¨å°‘æ•°å‡ ä¸ªæ ‡æ³¨æ ·æœ¬ã€‚æ›´å¤šå…³äºæˆ‘ä»¬é¡¹ç›®é¡µé¢çš„ä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%E3%80%82">https://cristianopatricio.github.io/CBVLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v2">PDF</a> Accepted for publication in Computers in Biology and Medicine</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å·¥ä½œæµä¸­åº”ç”¨çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§ç¼ºä¹çš„é—®é¢˜ã€‚æå‡ºä¸€ç§åä¸ºCBVLMçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚CBVLMèƒ½å¤Ÿé€šè¿‡é¢„æµ‹æ¦‚å¿µå¹¶åŸºäºè¿™äº›é¢„æµ‹è¿›è¡Œåˆ†ç±»ï¼Œç¡®ä¿è¯Šæ–­çš„åˆç†æ€§ã€‚é€šè¿‡ç®€å•çš„æ“ä½œå’Œå‡ ä¸ªæ ‡æ³¨ä¾‹å­ï¼ŒCBVLMèƒ½å¤§å¹…åº¦é™ä½æ ‡æ³¨æˆæœ¬ã€‚å®éªŒç»“æœè¯å®CBVLMåœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å·¥ä½œæµä¸­é¢ä¸´æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰è§£å†³äº†ç³»ç»Ÿè§£é‡Šæ€§é—®é¢˜ï¼Œä½†å¢åŠ äº†æ ‡æ³¨è´Ÿæ‹…ï¼Œä¸”æ·»åŠ æ–°æ¦‚å¿µéœ€è¦æ•´ä¸ªç³»ç»Ÿé‡æ–°è®­ç»ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºCBVLMæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>CBVLMé€šè¿‡é¢„æµ‹æ¦‚å¿µå¹¶åŸºäºè¿™äº›é¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œç¡®ä¿è¯Šæ–­çš„åˆç†æ€§ã€‚</li>
<li>CBVLMé€šè¿‡ç®€å•çš„æ“ä½œå’Œå‡ ä¸ªæ ‡æ³¨ä¾‹å­å¤§å¹…åº¦é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœè¯å®CBVLMåœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºCBMså’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-374c7e42a539d57fc818f563580763e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36b80d22716191da8fc4c178f927e398.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b386e2ae3c4946b3e59685e95f4804b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944163&auth_key=1759944163-0-0-c60d2f9e5f17c0ad5a756d3b0df55958&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-be192d43deb0d0b33637393de5cd9c1f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="WaveNet-SF-A-Hybrid-Network-for-Retinal-Disease-Detection-Based-on-Wavelet-Transform-in-Spatial-Frequency-Domain"><a href="#WaveNet-SF-A-Hybrid-Network-for-Retinal-Disease-Detection-Based-on-Wavelet-Transform-in-Spatial-Frequency-Domain" class="headerlink" title="WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on   Wavelet Transform in Spatial-Frequency Domain"></a>WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on   Wavelet Transform in Spatial-Frequency Domain</h2><p><strong>Authors:Jilan Cheng, Guoli Long, Zeyu Zhang, Zhenjia Qi, Hanyu Wang, Libin Lu, Shuihua Wang, Yudong Zhang, Jin Hong</strong></p>
<p>Retinal diseases are a leading cause of vision impairment and blindness, with timely diagnosis being critical for effective treatment. Optical Coherence Tomography (OCT) has become a standard imaging modality for retinal disease diagnosis, but OCT images often suffer from issues such as speckle noise, complex lesion shapes, and varying lesion sizes, making interpretation challenging. In this paper, we propose a novel framework, WaveNet-SF, to enhance retinal disease detection by integrating the spatial-domain and frequency-domain learning. The framework utilizes wavelet transforms to decompose OCT images into low- and high-frequency components, enabling the model to extract both global structural features and fine-grained details. To improve lesion detection, we introduce a Multi-Scale Wavelet Spatial Attention (MSW-SA) module, which enhances the modelâ€™s focus on regions of interest at multiple scales. Additionally, a High-Frequency Feature Compensation (HFFC) block is incorporated to recover edge information lost during wavelet decomposition, suppress noise, and preserve fine details crucial for lesion detection. Our approach achieves state-of-the-art (SOTA) classification accuracies of 97.82% and 99.58% on the OCT-C8 and OCT2017 datasets, respectively, surpassing existing methods. These results demonstrate the efficacy of WaveNet-SF in addressing the challenges of OCT image analysis and its potential as a powerful tool for retinal disease diagnosis. </p>
<blockquote>
<p>è§†ç½‘è†œç–¾ç—…æ˜¯å¯¼è‡´è§†åŠ›å’Œå¤±æ˜çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼ŒåŠæ—¶è¯Šæ–­å¯¹æœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å·²æˆä¸ºè§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„æ ‡å‡†æˆåƒæ–¹å¼ï¼Œä½†OCTå›¾åƒå¸¸å¸¸å­˜åœ¨æ–‘ç‚¹å™ªå£°ã€ç—…å˜å½¢çŠ¶å¤æ‚ã€ç—…å˜å¤§å°ä¸ä¸€ç­‰é—®é¢˜ï¼Œä½¿å¾—è§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶WaveNet-SFï¼Œé€šè¿‡ç»“åˆç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ ï¼Œå¢å¼ºè§†ç½‘è†œç–¾ç—…çš„æ£€æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å°æ³¢å˜æ¢å°†OCTå›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæå–å…¨å±€ç»“æ„ç‰¹å¾å’Œç»†ç²’åº¦ç»†èŠ‚ã€‚ä¸ºäº†æé«˜ç—…å˜æ£€æµ‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›ï¼ˆMSW-SAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šå¢å¼ºæ¨¡å‹å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„å…³æ³¨ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†ä¸€ç§é«˜é¢‘ç‰¹å¾è¡¥å¿ï¼ˆHFFCï¼‰å—ï¼Œä»¥æ¢å¤å°æ³¢åˆ†è§£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„è¾¹ç¼˜ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å™ªå£°ï¼Œå¹¶ä¿ç•™å¯¹ç—…å˜æ£€æµ‹è‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨OCT-C8å’ŒOCT2017æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®ç‡97.82%å’Œ99.58%ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†WaveNet-SFåœ¨è§£å†³OCTå›¾åƒåˆ†ææŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…¶ä½œä¸ºè§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„å¼ºå¤§å·¥å…·æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11854v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWaveNet-SFçš„æ–°æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè§†ç½‘è†œç–¾ç—…æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢å°†OCTå›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä»è€Œæå–å…¨å±€ç»“æ„ç‰¹å¾å’Œç²¾ç»†ç»†èŠ‚ã€‚é€šè¿‡å¼•å…¥å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›æ¨¡å—å’Œé«˜é¢‘ç‰¹å¾è¡¥å¿å—ï¼Œæé«˜äº†ç—…å˜æ£€æµ‹æ€§èƒ½ã€‚WaveNet-SFåœ¨OCT-C8å’ŒOCT2017æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œåˆ†åˆ«ä¸º97.82%å’Œ99.58%ï¼Œè¡¨ç°å‡ºå¯¹OCTå›¾åƒåˆ†æçš„æœ‰æ•ˆæ€§ï¼Œå¹¶æœ‰æœ›æˆä¸ºè§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œç–¾ç—…æ˜¯è§†åŠ›å—æŸå’Œå¤±æ˜çš„ä¸»è¦åŸå› ï¼ŒåŠæ—¶è¯Šæ–­å¯¹æœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>OCTï¼ˆå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼‰æ˜¯è§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„æ ‡å‡†æˆåƒæŠ€æœ¯ï¼Œä½†OCTå›¾åƒå­˜åœ¨æ–‘ç‚¹å™ªå£°ã€ç—…å˜å½¢çŠ¶å¤æ‚å’Œç—…å˜å¤§å°ä¸ä¸€ç­‰é—®é¢˜ï¼Œä½¿å¾—è§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ¡†æ¶WaveNet-SFï¼Œç»“åˆç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ ï¼Œä»¥æé«˜è§†ç½‘è†œç–¾ç—…çš„æ£€æµ‹ã€‚</li>
<li>åˆ©ç”¨å°æ³¢å˜æ¢åˆ†è§£OCTå›¾åƒä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä»¥ä¾¿æå–å…¨å±€ç»“æ„ç‰¹å¾å’Œç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆMSW-SAï¼‰ï¼Œæé«˜ç—…å˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åŠ å…¥äº†é«˜é¢‘ç‰¹å¾è¡¥å¿å—ï¼ˆHFFCï¼‰ï¼Œä»¥æ¢å¤ä¸¢å¤±çš„è¾¹ç¼˜ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å™ªå£°ï¼Œå¹¶ä¿ç•™å¯¹ç—…å˜æ£€æµ‹è‡³å…³é‡è¦çš„ç²¾ç»†ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5583636c22ca4dcc8095131bceb43eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944179&auth_key=1759944179-0-0-729ddb5da8428ad87b041b1dfff2143c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-de957feb486ce0f2c59a3e98bf1251a2.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ceab4f7344ddba3140fbb2674e8a37b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944193&auth_key=1759944193-0-0-046f3fe7174ef68a571e534f93047ff2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification"><a href="#MIAFEx-An-Attention-based-Feature-Extraction-Method-for-Medical-Image-Classification" class="headerlink" title="MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification"></a>MIAFEx: An Attention-based Feature Extraction Method for Medical Image   Classification</h2><p><strong>Authors:Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata</strong></p>
<p>Feature extraction techniques are crucial in medical image classification; however, classical feature extractors, in addition to traditional machine learning classifiers, often exhibit significant limitations in providing sufficient discriminative information for complex image sets. While Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown promise in feature extraction, they are prone to overfitting due to the inherent characteristics of medical imaging data, including small sample sizes or high intra-class variance. In this work, the Medical Image Attention-based Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable refinement mechanism to enhance the classification token within the Transformer encoder architecture. This mechanism adjusts the token based on learned weights, improving the extraction of salient features and enhancing the modelâ€™s adaptability to the challenges presented by medical imaging data. The MIAFEx output feature quality is compared against classical feature extractors using traditional and hybrid classifiers. Also, the performance of these features is compared against modern CNN and ViT models in classification tasks, demonstrating their superiority in accuracy and robustness across multiple complex medical imaging datasets. This advantage is particularly pronounced in scenarios with limited training data, where traditional and modern models often struggle to generalize effectively. The source code of this proposal can be found at <a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx</a> </p>
<blockquote>
<p>ç‰¹å¾æå–æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼›ç„¶è€Œï¼Œé™¤äº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨å¤–ï¼Œç»å…¸çš„ç‰¹å¾æå–å™¨åœ¨ä¸ºå¤æ‚çš„å›¾åƒé›†æä¾›è¶³å¤Ÿçš„åˆ¤åˆ«ä¿¡æ¯æ—¶å¾€å¾€å­˜åœ¨é‡å¤§å±€é™ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨ç‰¹å¾æå–æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºåŒ»å­¦æˆåƒæ•°æ®å›ºæœ‰çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ ·æœ¬é‡å°æˆ–ç±»å†…æ–¹å·®é«˜ï¼Œå®ƒä»¬å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚æœ¬æ–‡æå‡ºäº†åŸºäºåŒ»å­¦å›¾åƒæ³¨æ„åŠ›çš„ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„ç»†åŒ–æœºåˆ¶æ¥æé«˜Transformerç¼–ç å™¨æ¶æ„ä¸­çš„åˆ†ç±»æ ‡è®°ã€‚è¯¥æœºåˆ¶æ ¹æ®å­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´æ ‡è®°ï¼Œæé«˜äº†æ˜¾è‘—ç‰¹å¾çš„æå–èƒ½åŠ›ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åŒ»å­¦æˆåƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”èƒ½åŠ›ã€‚MIAFExçš„è¾“å‡ºç‰¹å¾è´¨é‡ä½¿ç”¨ä¼ ç»Ÿå’Œæ··åˆåˆ†ç±»å™¨ä¸ç»å…¸ç‰¹å¾æå–å™¨è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè¿™äº›ç‰¹å¾åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ç°ä»£CNNå’ŒViTæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜å®ƒä»¬åœ¨å¤šä¸ªå¤æ‚çš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚è¿™ä¸€ä¼˜åŠ¿åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å°¤ä¸ºçªå‡ºï¼Œä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æ¨å¹¿ã€‚è¯¥ææ¡ˆçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08562v3">PDF</a> This is the preprint version of an article that has been accepted for   publication in Knowledge-Based Systems</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç‰¹å¾æå–æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿç‰¹å¾æå–å™¨ç»“åˆä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨å¤„ç†å¤æ‚å›¾åƒé›†æ—¶ï¼Œæä¾›è¶³å¤Ÿåˆ¤åˆ«ä¿¡æ¯æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨ç‰¹å¾æå–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç”±äºåŒ»å­¦æˆåƒæ•°æ®çš„å°æ ·æœ¬è§„æ¨¡æˆ–é«˜ç±»å†…æ–¹å·®ç­‰å›ºæœ‰ç‰¹æ€§ï¼Œæ˜“å‡ºç°è¿‡æ‹Ÿåˆã€‚æœ¬æ–‡æå‡ºåŒ»å­¦å›¾åƒæ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨å¯å­¦ä¹ ç»†åŒ–æœºåˆ¶çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè½¬æ¢å™¨ç¼–ç å™¨æ¶æ„ä¸­çš„åˆ†ç±»ä»¤ç‰Œã€‚è¿™ç§æœºåˆ¶åŸºäºå­¦ä¹ åˆ°çš„æƒé‡è°ƒæ•´ä»¤ç‰Œï¼Œæ”¹è¿›äº†æ˜¾è‘—ç‰¹å¾çš„æå–ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åŒ»å­¦æˆåƒæ•°æ®æŒ‘æˆ˜çš„é€‚åº”æ€§ã€‚MIAFExçš„ç‰¹å¾è¾“å‡ºè´¨é‡ä¸ç»å…¸ç‰¹å¾æå–å™¨ä½¿ç”¨ä¼ ç»Ÿå’Œæ··åˆåˆ†ç±»å™¨è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè¿™äº›ç‰¹å¾ä¸ç°ä»£CNNå’ŒViTæ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒï¼Œåœ¨å¤šä¸ªå¤æ‚åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šå±•ç¤ºäº†å…¶åœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿå’Œç°ä»£æ¨¡å‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆæ¨å¹¿ï¼Œè€ŒMIAFExçš„ä¼˜åŠ¿æ›´ä¸ºçªå‡ºã€‚ç›¸å…³æºä»£ç å¯è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx%E3%80%82">https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFExã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç‰¹å¾æå–çš„é‡è¦æ€§åŠå…¶å¯¹ä¼ ç»Ÿæ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>CNNå’ŒViTåœ¨åŒ»å­¦å›¾åƒç‰¹å¾æå–ä¸­çš„åº”ç”¨åŠå…¶å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒæ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼ˆMIAFExï¼‰ï¼Œç»“åˆå¯å­¦ä¹ ç»†åŒ–æœºåˆ¶ï¼Œå¢å¼ºåˆ†ç±»ä»¤ç‰Œã€‚</li>
<li>MIAFExåœ¨å¤šä¸ªå¤æ‚åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šï¼Œä¸ç°ä»£CNNå’ŒViTæ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>MIAFExåœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚</li>
<li>æºä»£ç å¯åœ¨æŒ‡å®šGitHubä»“åº“æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0764cdb800960a558ab18b10639ef1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2caa5136a9f3f5d46967b9c9a449f40c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944209&auth_key=1759944209-0-0-6e915fe8bc0af5697e4e994b9f212d3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc8fea52bb5c10ae6a0ce3041de773d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944215&auth_key=1759944215-0-0-366a1bd220eb629124685915af1059d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-and-Its-Application-in-Medical-Image-Segmentation"><a href="#tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-and-Its-Application-in-Medical-Image-Segmentation" class="headerlink" title="tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   and Its Application in Medical Image Segmentation"></a>tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   and Its Application in Medical Image Segmentation</h2><p><strong>Authors:Guanghua He, Wangang Cheng, Hancan Zhu, Xiaohao Cai, Gaohang Yu</strong></p>
<p>Transfer learning, by leveraging knowledge from pre-trained models, has significantly enhanced the performance of target tasks. However, as deep neural networks scale up, full fine-tuning introduces substantial computational and storage challenges in resource-constrained environments, limiting its widespread adoption. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed to reduce computational complexity and storage requirements by minimizing the number of updated parameters. While matrix decomposition-based PEFT methods, such as LoRA, show promise, they struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-dimensional tensors offer a more natural representation of neural network weights, allowing for a more comprehensive capture of higher-order features and multi-dimensional interactions. In this paper, we propose tCURLoRA, a novel fine-tuning method based on tensor CUR decomposition. By concatenating pre-trained weight matrices into a three-dimensional tensor and applying tensor CUR decomposition, we update only the lower-order tensor components during fine-tuning, effectively reducing computational and storage overhead. Experimental results demonstrate that tCURLoRA outperforms existing PEFT methods in medical image segmentation tasks. </p>
<blockquote>
<p>å€ŸåŠ©é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œè¿ç§»å­¦ä¹ å·²æ˜¾è‘—æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å¼•å‘äº†å·¨å¤§çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æ›´æ–°çš„å‚æ•°æ•°é‡æ¥é™ä½è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨è¦æ±‚ã€‚è™½ç„¶åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•è·æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸åï¼Œé«˜ç»´å¼ é‡æä¾›äº†ç¥ç»ç½‘ç»œæƒé‡çš„æ›´è‡ªç„¶è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚æˆ‘ä»¬å°†é¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µåˆå¹¶æˆä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åªæ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œæœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒtCURLoRAä¼˜äºç°æœ‰çš„PEFTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02227v3">PDF</a> Accepted to MICCAI 2025. The final version of record is published in   Springer LNCS</p>
<p><strong>æ‘˜è¦</strong><br>    å€ŸåŠ©é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œè¿ç§»å­¦ä¹ å·²æ˜¾è‘—æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒä¼šåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å¼•å…¥å·¨å¤§çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä»¥å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¤æ‚æ€§ï¼Œå¹¶æœ€å°åŒ–æ›´æ–°çš„å‚æ•°æ•°é‡ã€‚è™½ç„¶åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•è·æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸åï¼Œé«˜ç»´å¼ é‡æä¾›æ›´è‡ªç„¶çš„ç¥ç»ç½‘ç»œæƒé‡è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚é€šè¿‡å°†é¢„è®­ç»ƒæƒé‡çŸ©é˜µè¿æ¥æˆä¸‰ç»´å¼ é‡å¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œæˆ‘ä»¬åªåœ¨å¾®è°ƒæ—¶æ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œæœ‰æ•ˆåœ°å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒtCURLoRAåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰PEFTæ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿ç§»å­¦ä¹ åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†æé«˜äº†ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®Œå…¨å¾®è°ƒåœ¨èµ„æºå—é™ç¯å¢ƒä¸­é¢ä¸´å·¨å¤§çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ã€‚</li>
<li>å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ—¨åœ¨å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•å¦‚LoRAåŸºäºçŸ©é˜µåˆ†è§£ï¼Œéš¾ä»¥æ•è·æ¨¡å‹æƒé‡çš„æ‰€æœ‰é«˜ç»´ç»“æ„ç‰¹å¾ã€‚</li>
<li>é«˜ç»´å¼ é‡ä¸ºç¥ç»ç½‘ç»œæƒé‡æä¾›æ›´è‡ªç„¶çš„è¡¨ç¤ºï¼Œèƒ½æ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œäº¤äº’ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„tCURLoRAæ–¹æ³•åŸºäºå¼ é‡CURåˆ†è§£ï¼Œæœ‰æ•ˆå‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒtCURLoRAåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–PEFTæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e2c57bd48625cb698ba8e6f7076d8077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93cd65146f89f45e2d9c947573f54f50.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LoRA-PT-Low-Rank-Adapting-UNETR-for-Hippocampus-Segmentation-Using-Principal-Tensor-Singular-Values-and-Vectors"><a href="#LoRA-PT-Low-Rank-Adapting-UNETR-for-Hippocampus-Segmentation-Using-Principal-Tensor-Singular-Values-and-Vectors" class="headerlink" title="LoRA-PT: Low-Rank Adapting UNETR for Hippocampus Segmentation Using   Principal Tensor Singular Values and Vectors"></a>LoRA-PT: Low-Rank Adapting UNETR for Hippocampus Segmentation Using   Principal Tensor Singular Values and Vectors</h2><p><strong>Authors:Guanghua He, Wangang Cheng, Hancan Zhu, Gaohang Yu</strong></p>
<p>The hippocampus is an important brain structure involved in various psychiatric disorders, and its automatic and accurate segmentation is vital for studying these diseases. Recently, deep learning-based methods have made significant progress in hippocampus segmentation. However, training deep neural network models requires substantial computational resources, time, and a large amount of labeled training data, which is frequently scarce in medical image segmentation. To address these issues, we propose LoRA-PT, a novel parameter-efficient fine-tuning (PEFT) method that transfers the pre-trained UNETR model from the BraTS2021 dataset to the hippocampus segmentation task. Specifically, LoRA-PT divides the parameter matrix of the transformer structure into three distinct sizes, yielding three third-order tensors. These tensors are decomposed using tensor singular value decomposition to generate low-rank tensors consisting of the principal singular values and vectors, with the remaining singular values and vectors forming the residual tensor. During fine-tuning, only the low-rank tensors (i.e., the principal tensor singular values and vectors) are updated, while the residual tensors remain unchanged. We validated the proposed method on three public hippocampus datasets, and the experimental results show that LoRA-PT outperformed state-of-the-art PEFT methods in segmentation accuracy while significantly reducing the number of parameter updates. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT">https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT</a>. </p>
<blockquote>
<p>æµ·é©¬ä½“æ˜¯å‚ä¸å¤šç§ç²¾ç¥ç–¾ç—…çš„é‡è¦è„‘ç»“æ„ï¼Œå…¶è‡ªåŠ¨å’Œå‡†ç¡®çš„åˆ†å‰²å¯¹äºç ”ç©¶è¿™äº›ç–¾ç—…è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨æµ·é©¬ä½“åˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€æ—¶é—´å’Œå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç»å¸¸æ˜¯ç¨€ç¼ºçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA-PTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå®ƒå°†BraTS2021æ•°æ®é›†é¢„è®­ç»ƒçš„UNETRæ¨¡å‹è½¬ç§»åˆ°æµ·é©¬ä½“åˆ†å‰²ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒLoRA-PTå°†transformerç»“æ„çš„å‚æ•°çŸ©é˜µåˆ†æˆä¸‰ç§ä¸åŒå¤§å°ï¼Œäº§ç”Ÿä¸‰ä¸ªä¸‰é˜¶å¼ é‡ã€‚è¿™äº›å¼ é‡é€šè¿‡å¼ é‡å¥‡å¼‚å€¼åˆ†è§£è¿›è¡Œåˆ†è§£ï¼Œç”Ÿæˆç”±ä¸»è¦å¥‡å¼‚å€¼å’Œå‘é‡ç»„æˆçš„ä½é˜¶å¼ é‡ï¼Œå…¶ä½™çš„å¥‡å¼‚å€¼å’Œå‘é‡å½¢æˆæ®‹å·®å¼ é‡ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œåªæ›´æ–°ä½é˜¶å¼ é‡ï¼ˆå³ä¸»è¦å¼ é‡çš„å¥‡å¼‚å€¼å’Œå‘é‡ï¼‰ï¼Œè€Œæ®‹å·®å¼ é‡ä¿æŒä¸å˜ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€çš„æµ·é©¬ä½“æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆ†å‰²ç²¾åº¦ä¸Šï¼ŒLoRA-PTè¶…è¶Šäº†æœ€å…ˆè¿›çš„PEFTæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°æ›´æ–°çš„æ•°é‡ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT">https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11292v3">PDF</a> Accepted to Artificial Intelligence in Medicine (2025). Final   published version available online</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•LoRA-PTï¼Œç”¨äºä»BraTS2021æ•°æ®é›†è¿ç§»è‡³æµ·é©¬ä½“åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£å˜å‹å™¨ç»“æ„å‚æ•°çŸ©é˜µï¼Œä»…æ›´æ–°ä¸»è¦å¥‡å¼‚å€¼å‘é‡ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦å¹¶å‡å°‘å‚æ•°æ›´æ–°æ•°é‡ï¼Œåœ¨å…¬å¼€æµ·é©¬ä½“æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ·é©¬ä½“åœ¨ç²¾ç¥ç–¾ç—…çš„ç ”ç©¶ä¸­çš„é‡è¦æ€§åŠå…¶è‡ªåŠ¨å‡†ç¡®åˆ†å‰²çš„å¿…è¦æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨æµ·é©¬ä½“åˆ†å‰²ä¸­çš„æœ€æ–°è¿›å±•åŠé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—èµ„æºã€æ—¶é—´å’Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>æå‡ºçš„LoRA-PTæ–¹æ³•æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿä»BraTS2021æ•°æ®é›†è¿ç§»è‡³æµ·é©¬ä½“åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>LoRA-PTæ–¹æ³•é€šè¿‡åˆ†è§£å˜å‹å™¨ç»“æ„å‚æ•°çŸ©é˜µæ¥æå‡æ€§èƒ½ã€‚</li>
<li>åœ¨å…¬å¼€æµ·é©¬ä½“æ•°æ®é›†ä¸Šï¼ŒLoRA-PTæ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›PEFTæ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå…·æœ‰è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦å¹¶æ˜¾è‘—å‡å°‘å‚æ•°æ›´æ–°æ•°é‡ã€‚</li>
<li>LoRA-PTæ–¹æ³•çš„æºä»£ç å·²å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ecf376aa53382a0dd0a8b49dabe35773~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944237&auth_key=1759944237-0-0-88eaf4a55b2418c4c7523dba660090fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-35a5a2876aec1eb0335ccbb1f81bd5ec.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-838dc2e2a64a1355d608205ac075b4aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944251&auth_key=1759944251-0-0-5017873b7c309243e14098025e393786&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DiffCut-Catalyzing-Zero-Shot-Semantic-Segmentation-with-Diffusion-Features-and-Recursive-Normalized-Cut"><a href="#DiffCut-Catalyzing-Zero-Shot-Semantic-Segmentation-with-Diffusion-Features-and-Recursive-Normalized-Cut" class="headerlink" title="DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion   Features and Recursive Normalized Cut"></a>DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion   Features and Recursive Normalized Cut</h2><p><strong>Authors:Paul Couairon, Mustafa Shukor, Jean-Emmanuel Haugeard, Matthieu Cord, Nicolas Thome</strong></p>
<p>Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised image segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we demonstrate that the utilization of these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation. Specifically, we leverage a recursive Normalized Cut algorithm that softly regulates the granularity of detected objects and produces well-defined segmentation maps that precisely capture intricate image details. Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks. Project page at <a target="_blank" rel="noopener" href="https://diffcut-segmentation.github.io/">https://diffcut-segmentation.github.io</a> </p>
<blockquote>
<p>è·¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ç­‰å¤šä¸ªé¢†åŸŸï¼ŒåŸºç¡€æ¨¡å‹å·²å±•ç°å‡ºå¼ºå¤§çš„å·¥å…·æ½œåŠ›ã€‚å°½ç®¡æ—©æœŸçš„å·¥ä½œå·²ç»è§£å†³äº†æ— ç›‘ç£å›¾åƒåˆ†å‰²çš„é—®é¢˜ï¼Œä½†å®ƒä»¬æ˜¾è‘—è½åäºç›‘ç£æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£UNetç¼–ç å™¨ä½œä¸ºåŸºç¡€è§†è§‰ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥DiffCutï¼Œä¸€ç§æ— ç›‘ç£é›¶åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…åˆ©ç”¨æœ€ç»ˆè‡ªæ³¨æ„æ¨¡å—çš„è¾“å‡ºç‰¹æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜åœ¨åŸºäºå›¾çš„åˆ†å‰²ç®—æ³•ä¸­ä½¿ç”¨è¿™äº›æ‰©æ•£ç‰¹å¾ï¼Œåœ¨é›¶åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨é€’å½’å½’ä¸€åŒ–åˆ‡å‰²ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è½»æŸ”åœ°è°ƒèŠ‚æ£€æµ‹å¯¹è±¡çš„ç²’åº¦ï¼Œå¹¶äº§ç”Ÿå®šä¹‰æ˜ç¡®çš„åˆ†å‰²å›¾ï¼Œèƒ½å¤Ÿç²¾ç¡®åœ°æ•è·å›¾åƒç»†èŠ‚ã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†æ‰©æ•£UNetç¼–ç å™¨ä¸­åµŒå…¥çš„éå¸¸ç²¾ç¡®è¯­ä¹‰çŸ¥è¯†ï¼Œä¹‹åå¯ä»¥ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€è§†è§‰ç¼–ç å™¨ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://diffcutsegmentat/">https://diffcutsegmentat</a> ion.github.io</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02842v4">PDF</a> NeurIPS 2024. Project page at <a target="_blank" rel="noopener" href="https://diffcut-segmentation.github.io/">https://diffcut-segmentation.github.io</a>.   Code at <a target="_blank" rel="noopener" href="https://github.com/PaulCouairon/DiffCut">https://github.com/PaulCouairon/DiffCut</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£UNetç¼–ç å™¨çš„åŸºç¡€è§†è§‰ç¼–ç å™¨ï¼Œç”¨äºæ— ç›‘ç£é›¶åˆ†å‰²ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥æ‰©æ•£ç‰¹å¾å¹¶åˆ©ç”¨åŸºäºå›¾çš„åˆ†å‰²ç®—æ³•ï¼Œè¯¥æ¨¡å‹åœ¨é›¶åˆ†å‰²ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é‡‡ç”¨é€’å½’å½’ä¸€åŒ–åˆ‡å‰²ç®—æ³•ï¼Œèƒ½å¤Ÿç²¾ç»†æ£€æµ‹å¯¹è±¡ç²’åº¦å¹¶ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²åœ°å›¾ã€‚è¯¥å·¥ä½œè¯æ˜äº†æ‰©æ•£UNetç¼–ç å™¨ä¸­åµŒå…¥çš„è¯­ä¹‰çŸ¥è¯†çš„å‡†ç¡®æ€§ï¼Œå¯ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›åŸºç¡€è§†è§‰ç¼–ç å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ‰©æ•£UNetç¼–ç å™¨ä½œä¸ºåŸºç¡€è§†è§‰ç¼–ç å™¨ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ã€‚</li>
<li>æå‡ºDiffCutæ–¹æ³•ï¼Œå®ç°äº†æ— ç›‘ç£é›¶åˆ†å‰²ä»»åŠ¡çš„é«˜æ•ˆå®Œæˆã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£ç‰¹å¾å¹¶ç»“åˆåŸºäºå›¾çš„åˆ†å‰²ç®—æ³•ï¼Œæ€§èƒ½è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨é€’å½’å½’ä¸€åŒ–åˆ‡å‰²ç®—æ³•ï¼Œèƒ½ç²¾å‡†æ•è·å›¾åƒç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è°ƒèŠ‚æ£€æµ‹å¯¹è±¡ç²’åº¦çš„èƒ½åŠ›ã€‚</li>
<li>è¯æ˜æ‰©æ•£UNetç¼–ç å™¨ä¸­åµŒå…¥çš„è¯­ä¹‰çŸ¥è¯†çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-17e501bffacda545f565100d07a7bb85~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944258&auth_key=1759944258-0-0-1159ab846aaa7c5e8c490f329934e8eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38012e3a3002d675954a9a4c0af94494~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944265&auth_key=1759944265-0-0-0f6b794e2c1963e1b56648c055c74c79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10724cea89d27141f495b0a7f896522a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944272&auth_key=1759944272-0-0-ef1bd80119feeef6c2a7cb2f247e7bfd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5b4f47c12030ca1ab313f56172ebd172~resize:0:q75.jpg?source=1f5c5e47&expiration=1759944279&auth_key=1759944279-0-0-3d8bb965d24ac47528d3f50cc1f53dc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Evaluation of preprocessing pipelines in the creation of in-the-wild TTS   datasets
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5dd7309b3a8b9b771cfaf09a2084da7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943396&auth_key=1759943396-0-0-150deb2778e9abd165fc3002977856db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Product-Quantised Image Representation for High-Quality Image Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
