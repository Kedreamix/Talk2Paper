<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-10-07  UniShield An Adaptive Multi-Agent Framework for Unified Forgery Image   Detection and Localization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-edc21980ae5bba5cc5ea5f442de9e6d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942193&auth_key=1759942193-0-0-e26f9efc69c20f29e0f00a0249f71bcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-07-更新"><a href="#2025-10-07-更新" class="headerlink" title="2025-10-07 更新"></a>2025-10-07 更新</h1><h2 id="UniShield-An-Adaptive-Multi-Agent-Framework-for-Unified-Forgery-Image-Detection-and-Localization"><a href="#UniShield-An-Adaptive-Multi-Agent-Framework-for-Unified-Forgery-Image-Detection-and-Localization" class="headerlink" title="UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image   Detection and Localization"></a>UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image   Detection and Localization</h2><p><strong>Authors:Qing Huang, Zhipei Xu, Xuanyu Zhang, Jian Zhang</strong></p>
<p>With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability. </p>
<blockquote>
<p>随着图像生成技术的快速发展，合成图像越来越逼真，给社会带来了重大风险，如虚假信息和欺诈。因此，图像伪造检测与定位（FIDL）对于维护信息完整性和社会安全至关重要。尽管现有的特定领域检测方法表现出令人印象深刻的性能，但由于其专业化范围狭窄、跨域泛化能力较差以及缺乏集成自适应框架，其实践适用性仍然有限。为了解决这些问题，我们提出了UniShield，这是一种新型的多智能体统一系统，能够检测和定位不同领域的图像伪造，包括图像操纵、文档操纵、深度伪造和人工智能生成的图像。UniShield创新地将感知智能体与检测智能体相结合。感知智能体能智能地分析图像特征以动态选择适当的检测模型，而检测智能体则将各种专业检测器整合到一个统一框架中，并生成可解释的报告。大量实验表明，UniShield达到了最先进的水平，超过了现有的统一方法和特定领域检测器，突显了其卓越的实践适用性、适应性和可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03161v1">PDF</a> </p>
<p><strong>Summary</strong><br>图像生成技术的快速发展使得合成图像越来越逼真，对社会造成了如虚假信息和欺诈等风险。因此，伪造图像检测与定位（FIDL）对于维护信息完整性和社会安全至关重要。尽管现有的特定领域检测方法表现出色，但由于其专业局限性、跨域泛化能力弱以及缺乏集成自适应框架，其实践应用仍然受限。为解决这些问题，我们提出了UniShield这一基于多智能体的统一系统，可在多个不同领域检测并定位图像伪造，包括图像操作、文档操作、深度伪造和AI生成的图像。UniShield创新地整合了感知智能体和检测智能体，感知智能体能智能分析图像特征以动态选择适合的检测模型，而检测智能体则将各种专业检测器整合到一个统一框架中并生成可解释的报告。实验表明，UniShield达到了业界最佳水平，超越了现有的统一方法和特定领域检测器，凸显了其出色的实用性、适应性和可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成技术的快速发展导致合成图像越来越逼真，对社会造成风险，如虚假信息和欺诈。</li>
<li>伪造图像检测与定位（FIDL）对于维护信息完整性和社会安全至关重要。</li>
<li>现有特定领域检测方法存在专业局限性、跨域泛化能力弱以及缺乏集成自适应框架的问题。</li>
<li>UniShield是一个基于多智能体的统一系统，可在多个领域检测并定位图像伪造。</li>
<li>UniShield整合了感知智能体和检测智能体，分别负责智能分析图像特征和整合检测器。</li>
<li>UniShield通过动态选择适合的检测模型和提高检测效率，实现了出色的实用性、适应性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03161">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43b8a0a1147e4e62c6b82244d7383420.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-553fc46dc56793395927701a8d675b83~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942208&auth_key=1759942208-0-0-2856de037abf0629f37c648476dd3bba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b2d8d4688bc3af61bc2b632a9eaf1e68.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-7964c45c6698d5c40db9bdaf3d18710d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942264&auth_key=1759942264-0-0-2664159a93c7fed05d504cff3bf922d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a1b7a4102fae8281c9b5a0aff2d8929f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942271&auth_key=1759942271-0-0-16db40340e17cd8ca2ba1903cc647c5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-f976399ef3ebad0a00a06302f1694244.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AudioToolAgent-An-Agentic-Framework-for-Audio-Language-Models"><a href="#AudioToolAgent-An-Agentic-Framework-for-Audio-Language-Models" class="headerlink" title="AudioToolAgent: An Agentic Framework for Audio-Language Models"></a>AudioToolAgent: An Agentic Framework for Audio-Language Models</h2><p><strong>Authors:Gijs Wijngaard, Elia Formisano, Michel Dumontier</strong></p>
<p>Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com&#x2F;GLJS&#x2F;AudioToolAgent </p>
<blockquote>
<p>大型音频语言模型（LALMs）在音频理解任务上表现良好，但缺乏近期大型语言模型（LLMs）中的多步推理和工具调用功能。本文提出了AudioToolAgent框架，它通过中央的LLM代理来协调音频语言模型作为工具，该代理可以访问用于音频问答和语音到文本的工具适配器。该代理选择工具、提出跟进问题并比较输出以进行验证。在MMAU、MMAR和MMAU-Pro上的实验显示，其准确度达到最新水平：MMAU上高达74.10%，MMAR上为68.80%，MMAU-Pro上为57.96%。通过蒙特卡洛采样计算Shapley值来跨374种配置确定有效的代理工具组合。模块化设计允许集成新工具并消除数据和培训成本的使用。代码和复现材料可在github.com&#x2F;GLJS&#x2F;AudioToolAgent找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文介绍了AudioToolAgent框架，它通过中央LLM代理协调音频语言模型作为工具，通过工具适配器进行音频问答和语音转文本。代理能够选择工具、提出跟进问题并进行输出验证，以实现多步推理和工具调用。实验结果表明，该框架在MMAU、MMAR和MMAU-Pro任务上达到了最先进的准确性。其模块化设计允许集成新工具，并降低了数据和培训成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioToolAgent框架利用LLM代理协调音频语言模型工具，提高了音频理解任务的性能。</li>
<li>通过工具适配器进行音频问答和语音转文本。</li>
<li>代理具有选择工具、提出跟进问题和输出验证的能力，实现了多步推理和工具调用。</li>
<li>实验结果表明，AudioToolAgent在MMAU、MMAR和MMAU-Pro任务上达到了最先进的准确性，最高达到74.10%、68.80%和57.96%。</li>
<li>Monte Carlo采样用于计算374种配置的形状值，以确定有效的代理工具组合。</li>
<li>框架的模块化设计允许集成新工具，并降低了数据和培训成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ea7964c4e3c5e3b1281eab16778cc669~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942285&auth_key=1759942285-0-0-94b1b7129e9da59a02cf64f9077f08ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-6e27812764dd054c50bcc9692bfcf7a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0482b1dd4db176346fac0b66bdc84f8c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-50f13705b522ad435c3b8925bce4c94e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942306&auth_key=1759942306-0-0-27f3c59507ae27a50960c7cee26fe8ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ARMs-Adaptive-Red-Teaming-Agent-against-Multimodal-Models-with-Plug-and-Play-Attacks"><a href="#ARMs-Adaptive-Red-Teaming-Agent-against-Multimodal-Models-with-Plug-and-Play-Attacks" class="headerlink" title="ARMs: Adaptive Red-Teaming Agent against Multimodal Models with   Plug-and-Play Attacks"></a>ARMs: Adaptive Red-Teaming Agent against Multimodal Models with   Plug-and-Play Attacks</h2><p><strong>Authors:Zhaorun Chen, Xun Liu, Mintong Kang, Jiawei Zhang, Minzhou Pan, Shuang Yang, Bo Li</strong></p>
<p>As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms into ARMs via model context protocol (MCP). To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack exploration algorithm. Extensive experiments on instance- and policy-based benchmarks show that ARMs achieves SOTA attack success rates, exceeding baselines by an average of 52.1% and surpassing 90% on Claude-4-Sonnet. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety dataset comprising over 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness of VLMs while preserving their general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats. </p>
<blockquote>
<p>随着视觉语言模型（VLMs）的普及，它们的多模态界面也带来了新的安全漏洞，使得安全评估充满挑战且至关重要。现有的红队努力要么局限于一组有限的对抗模式，要么严重依赖于手动工程，缺乏对新出现的大规模现实世界VLM漏洞的可扩展探索。为了弥补这一差距，我们提出了ARMs，这是一种自适应的红队代理，能够针对VLMs进行系统的全面风险评估。给定目标的有害行为或风险定义，ARMs能够自动优化各种红队策略，通过增强推理的多步骤协同操作，有效地从目标VLMs中引出有害输出。我们提出了11种新型的多模态攻击策略，涵盖了多样化的VLM对抗模式（例如，推理劫持、上下文隐身），并通过模型上下文协议（MCP）将1.7种红队算法集成到ARMs中。为了平衡攻击的多样性和有效性，我们设计了一种分层内存和epsilon贪婪攻击探索算法。在实例和基于政策的基准测试上的大量实验表明，ARMs达到了先进的攻击成功率，平均超出基线52.1%，在Claude-4-Sonnet上的成功率超过90%。我们证明ARMs生成的红队实例的多样性更高，揭示了VLM的新兴漏洞。利用ARMs，我们构建了ARMs-Bench，这是一个大规模的多模态安全数据集，包含超过3万多个红队实例，涵盖51种多样的风险类别，既有现实世界的多模态威胁也有监管风险。利用ARMs-Bench进行微调可以显著提高VLMs的稳健性，同时保留其通用性，为应对新兴威胁提高多模态安全对齐提供了切实可行的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02677v1">PDF</a> 60 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了随着视觉语言模型（VLMs）的普及，其多模态接口引入新的安全漏洞，使得安全评估充满挑战。为此，本文提出了一种自适应的红队代理（ARMs），能够针对VLMs进行全面风险评估。ARMs能够自动优化多种红队策略，有效引发目标VLMs的有害输出。实验结果表明，ARMs的攻击成功率达到领先水平，同时揭示了VLMs的新兴漏洞。基于此，构建了ARMs-Bench大规模多模态安全数据集，用于提升VLMs的稳健性并应对新兴威胁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）的多模态接口引入新的安全漏洞，使得安全评估变得重要且困难。</li>
<li>现有红队努力存在局限性，缺乏对新出现的现实世界中VLM漏洞的可扩展探索。</li>
<li>提出一种自适应的红队代理（ARMs），能够针对VLMs进行全面风险评估，自动优化红队策略以引发有害输出。</li>
<li>ARMs实现了高水平的攻击成功率，并揭示了VLMs的新兴漏洞。</li>
<li>ARMs通过模型上下文协议（MCP）集成了多种红队算法。</li>
<li>提出了一种分层记忆和epsilon贪婪攻击探索算法来平衡攻击多样性和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f09190dac2d76f5e520b4828f647be50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edc21980ae5bba5cc5ea5f442de9e6d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c8b3404fb6a968b6494855ff27022ed.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd5a36ac385fabc0ce685735b8ddd321~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942336&auth_key=1759942336-0-0-7bd5a0cca10f6959f7979c594860b5ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AutoMaAS-Self-Evolving-Multi-Agent-Architecture-Search-for-Large-Language-Models"><a href="#AutoMaAS-Self-Evolving-Multi-Agent-Architecture-Search-for-Large-Language-Models" class="headerlink" title="AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large   Language Models"></a>AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large   Language Models</h2><p><strong>Authors:Bo Ma, Hang Li, ZeHua Hu, XiaoFan Gui, LuYao Liu, Simon Liu</strong></p>
<p>Multi-agent systems powered by large language models have demonstrated remarkable capabilities across diverse domains, yet existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements. This paper introduces AutoMaAS, a self-evolving multi-agent architecture search framework that leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Our approach incorporates four key innovations: (1) automatic operator generation, fusion, and elimination based on performance-cost analysis, (2) dynamic cost-aware optimization with real-time parameter adjustment, (3) online feedback integration for continuous architecture refinement, and (4) enhanced interpretability through decision tracing mechanisms. Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1% performance improvement while reducing inference costs by 3-5% compared to state-of-the-art methods. The framework shows superior transferability across datasets and LLM backbones, establishing a new paradigm for automated multi-agent system design in the era of large language models. </p>
<blockquote>
<p>由大型语言模型驱动的多智能体系统已在多个领域展现出卓越的能力。然而，现有的自动化设计方法寻求单一解决方案，无法根据查询复杂性和领域要求调整资源分配。本文介绍了AutoMaAS，一种自我进化的多智能体架构搜索框架，它利用神经网络架构搜索原理，通过动态操作符生命周期管理和自动化机器学习技术，自动发现最佳智能体配置。我们的方法结合了四项关键创新：(1)基于性能成本分析的自动操作符生成、融合和消除；(2)具有实时参数调整的动态成本感知优化；(3)在线反馈集成，用于持续架构改进；(4)通过决策跟踪机制的增强可解释性。在六个基准测试上的大量实验表明，与最先进的方法相比，AutoMaAS实现了1.0-7.1%的性能提升，同时降低了3-5%的推理成本。该框架在数据集和LLM骨干网之间表现出卓越的可迁移性，为大型语言模型时代自动化多智能体系统设计建立了新范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02669v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型驱动的多智能体系统已在多个领域展现出卓越性能。然而，现有的自动化设计通常采用单一方案，未能根据查询的复杂性和领域需求进行资源分配的适应性调整。本研究引入AutoMaAS框架，这是一个自我进化的多智能体架构搜索框架，利用神经网络架构搜索原理，通过动态操作生命周期管理和自动化机器学习技术，自动发现最佳智能体配置。它包括四项关键技术：基于性能成本分析的自动操作生成、融合和消除，动态成本感知优化与实时参数调整，在线反馈集成用于持续架构优化，以及通过决策跟踪机制增强可解释性。在六个基准测试上的实验表明，AutoMaAS在性能上实现了与现有技术相比1.0-7.1%的提升，同时降低了3-5%的推理成本。该框架在数据集和LLM主干上表现出优越的迁移性，为大型语言模型时代自动化多智能体系统设计树立了新范例。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>大型语言模型驱动的多智能体系统在多个领域展现出卓越性能。</li>
<li>现有自动化设计缺乏根据查询复杂性和领域需求进行资源分配的适应性调整。</li>
<li>AutoMaAS框架是一个自我进化的多智能体架构搜索框架，能自动发现最佳智能体配置。</li>
<li>AutoMaAS包含四项关键技术：自动操作生成、融合和消除，动态成本感知优化，在线反馈集成和决策跟踪机制增强可解释性。</li>
<li>实验表明AutoMaAS在性能上有所提升，并降低了推理成本。</li>
<li>AutoMaAS框架展现出优越的迁移性，适用于不同的数据集和LLM主干。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f7aab8180c9e8b6754493674d4c2f472~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942344&auth_key=1759942344-0-0-6ce445efa7cc7df14d089f0a47f05f4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7467ef9c70ecb035b5d0612a6a02448f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942351&auth_key=1759942351-0-0-b676fe3af1d8812c18feed2bc3d97a89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92ba1c0f24a74ec7bacf8685a90ee891~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942357&auth_key=1759942357-0-0-bd31386c15e4f8b2886c7ac1a6232c67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2329932708c6d196b275eab0308542c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942364&auth_key=1759942364-0-0-c61761b9061d5efb721411c1930a1fbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e7f5cb7baab16d7aa4d68695393c0b29.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-461a42d5bd5d4418a1a6673aee1471ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942377&auth_key=1759942377-0-0-4258108394a42fcc44220ca55dd100f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RedCodeAgent-Automatic-Red-teaming-Agent-against-Diverse-Code-Agents"><a href="#RedCodeAgent-Automatic-Red-teaming-Agent-against-Diverse-Code-Agents" class="headerlink" title="RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents"></a>RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents</h2><p><strong>Authors:Chengquan Guo, Chulin Xie, Yu Yang, Zhaorun Chen, Zinan Lin, Xander Davies, Yarin Gal, Dawn Song, Bo Li</strong></p>
<p>Code agents have gained widespread adoption due to their strong code generation capabilities and integration with code interpreters, enabling dynamic execution, debugging, and interactive programming capabilities. While these advancements have streamlined complex workflows, they have also introduced critical safety and security risks. Current static safety benchmarks and red-teaming tools are inadequate for identifying emerging real-world risky scenarios, as they fail to cover certain boundary conditions, such as the combined effects of different jailbreak tools. In this work, we propose RedCodeAgent, the first automated red-teaming agent designed to systematically uncover vulnerabilities in diverse code agents. With an adaptive memory module, RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the most effective red-teaming tools and tool combinations in a tailored toolbox for a given input query, thus identifying vulnerabilities that might otherwise be overlooked. For reliable evaluation, we develop simulated sandbox environments to additionally evaluate the execution results of code agents, mitigating potential biases of LLM-based judges that only rely on static code. Through extensive evaluations across multiple state-of-the-art code agents, diverse risky scenarios, and various programming languages, RedCodeAgent consistently outperforms existing red-teaming methods, achieving higher attack success rates and lower rejection rates with high efficiency. We further validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium, exposing previously unidentified security risks. By automating and optimizing red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective safety assessments of code agents. </p>
<blockquote>
<p>代码代理由于其强大的代码生成能力和与代码解释器的集成，实现了动态执行、调试和交互编程功能，得到了广泛的应用。虽然这些进步使复杂的工作流程更加顺畅，但它们也引入了关键的安全风险。当前的静态安全基准和红色团队工具不足以识别新兴的现实世界风险场景，因为它们无法覆盖某些边界条件，例如不同越狱工具的组合效应。在这项工作中，我们提出了RedCodeAgent，这是第一个自动化红色团队代理，旨在系统地发现各种代码代理中的漏洞。RedCodeAgent具有自适应内存模块，可以利用现有的越狱知识，动态选择最有效的红色团队工具和工具组合，为给定的输入查询定制工具箱，从而发现可能被忽视的漏洞。为了可靠地评估，我们开发了模拟沙箱环境来额外评估代码代理的执行结果，减轻只依赖静态代码的语言模型评委的潜在偏见。通过对多个最先进的代码代理、多样化的风险场景和各种编程语言的广泛评估，RedCodeAgent始终优于现有的红色团队方法，以高效率实现了更高的攻击成功率和更低的拒绝率。我们进一步在现实世界中的代码助手（如Cursor和Codeium）上验证了RedCodeAgent，暴露了以前未发现的安全风险。通过自动化和优化红色团队流程，RedCodeAgent实现了代码代理的可扩展、自适应和有效的安全评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02609v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了代码代理的广泛应用及其强大的代码生成能力和与代码解释器的集成功能，如动态执行、调试和交互式编程功能。然而，这些进展也带来了关键的安全风险。当前的安全基准和红色团队工具不足以识别新兴的现实世界风险场景，因为它们无法覆盖某些边界条件，如不同越狱工具的联合效应。为此，本文提出了RedCodeAgent，它是第一个自动化的红色团队代理，旨在系统地发现各种代码代理中的漏洞。通过自适应内存模块，RedCodeAgent可以利用现有的越狱知识，动态选择最有效的红色团队工具和工具组合，为给定输入查询量身定制工具箱，从而发现可能被忽视的漏洞。为了可靠评估，本文开发了模拟沙箱环境来评估代码代理的执行结果，以减轻仅依赖静态代码的LLM评委的潜在偏见。通过广泛评估多个最先进的代码代理、多样化的风险场景和各种编程语言，RedCodeAgent始终优于现有的红色团队方法，实现了更高的攻击成功率和更低的拒绝率，且效率高。RedCodeAgent还进一步验证了现实世界的代码助手，如Cursor和Codeium，揭示了之前未识别的安全风险。通过自动化和优化红色团队流程，RedCodeAgent实现了对代码代理的可扩展、自适应和有效的安全评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码代理具有强大的代码生成和与代码解释器集成的能力，支持动态执行、调试和交互式编程。</li>
<li>现有安全基准和红色团队工具在识别新兴现实风险场景方面存在不足。</li>
<li>RedCodeAgent是首个自动化红色团队代理，可系统地发现代码代理中的漏洞。</li>
<li>RedCodeAgent具有自适应内存模块，可利用越狱知识，为特定输入选择最有效的红色团队工具和组合。</li>
<li>为了可靠评估，开发了模拟沙箱环境来评估代码代理的执行结果，减轻对静态代码的依赖。</li>
<li>RedCodeAgent在多个代码代理、风险场景和编程语言上的评估表现优异，攻击成功率高且效率高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02609">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8f0acf95913ea79f3e0f5831b7563fb9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942384&auth_key=1759942384-0-0-e603b3902a3bb900594d0f956e55437e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d10a2d4126c0eca359464a5d23457f20~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942391&auth_key=1759942391-0-0-8ddd2f538ab0d3b25fc7450671976195&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Orchestrating-Human-AI-Teams-The-Manager-Agent-as-a-Unifying-Research-Challenge"><a href="#Orchestrating-Human-AI-Teams-The-Manager-Agent-as-a-Unifying-Research-Challenge" class="headerlink" title="Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research   Challenge"></a>Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research   Challenge</h2><p><strong>Authors:Charlie Masters, Advaith Vellanki, Jiangbo Shangguan, Bart Kultys, Jonathan Gilmore, Alastair Moore, Stefano V. Albrecht</strong></p>
<p>While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime - underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems. </p>
<blockquote>
<p>虽然代理智能AI在自动化单个任务方面取得了进展，但管理复杂的多代理工作流程仍然是一个具有挑战性的问题。本文旨在为在动态人机团队中协调合作的自主代理系统提供研究愿景。我们提出“自主管理代理”作为核心挑战：该代理能够将复杂目标分解为任务图，分配给人类和AI工作者，监控进度，适应变化条件，并保持透明的利益相关者沟通。我们将工作流程管理形式化为部分可观察的随机游戏，并确定了四个基本挑战：（1）层次分解的组合推理，（2）变化偏好下的多目标优化，（3）临时团队的协调和规划，以及（4）设计和遵守治理规则。为了推进这一议程，我们发布了MA-Gym，这是一个用于多代理工作流程编排的开源模拟和评估框架。在20个工作流程中评估基于GPT-5的管理代理，我们发现他们在目标完成、约束遵守和流程运行时间方面的联合优化存在困难，这突显出工作流程管理是一个困难的开放性问题。最后，我们探讨了自主管理系统对组织和伦理的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02557v1">PDF</a> Accepted as an oral paper for the conference for Distributed   Artificial Intelligence (DAI 2025). 8 pages, 2 figures</p>
<p><strong>Summary</strong><br>自动化个体任务的人工智能发展迅猛，但管理复杂的多智能体工作流程仍是一个挑战。本文提出自主智能系统的研究愿景，着重研究智能自主管理系统如何协同人类与AI之间的合作。提出了核心难题为自主管理者智能体（Agent），其能分解复杂目标为任务图谱，为AI和人类工作者分配任务，监控进度，适应变化条件并保持利益相关者间的透明沟通。将工作流程管理形式化为部分可观察的随机博弈，并确定了四大基础挑战：层次分解的组合推理、变化偏好下的多目标优化、临时团队的协调与规划以及设计与实施的治理与合规性。为推进此议程，我们发布了MA-Gym，这是一个开源模拟与评估框架，用于多智能体工作流程编排。评估基于GPT-5的管理者智能体发现它们在20个工作流程中对目标完成、约束遵守和工作流程运行时间的联合优化上遇到困难，突显出工作流程管理作为一个难题的存在。最后探讨了自主管理系统在组织学和伦理学上的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化个体任务的人工智能发展快速，但管理复杂的多智能体工作流程仍然具有挑战性。</li>
<li>自主智能系统研究侧重于智能自主管理系统如何协同人类与AI的合作。</li>
<li>自主管理者智能体能分解复杂目标为任务图谱并进行任务分配，同时监控进度、适应变化条件并维持透明沟通。</li>
<li>工作流程管理被形式化为部分可观察的随机博弈，并存在四大基础挑战：层次分解的组合推理、多目标优化、临时团队的协调与规划以及治理与合规性挑战。</li>
<li>MA-Gym是一个开源模拟与评估框架，用于多智能体工作流程编排。</li>
<li>GPT-5管理者智能体在联合优化目标完成、约束遵守和工作流程运行时间方面遇到困难。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6991e8c01b1db0b15b8f14e1555758e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942400&auth_key=1759942400-0-0-1904b23d6328588523174b4f2745deeb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99ceae1cc6a1f0ab929bfffcf527c056~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942407&auth_key=1759942407-0-0-90f3d1e6674bd8f3a458e231f087703d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods – whether sparse or dense – often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance – a setting we term agentic retrieval. The benchmark consists of 26K expert-annotated examples on S&amp;P-500 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. </p>
<blockquote>
<p>在金融领域，准确的信息检索（IR）至关重要，投资者必须从大量的文档集合中识别出相关信息。传统的IR方法，无论是稀疏的还是密集的，往往在检索准确性方面存在不足，因为它不仅需要捕捉语义相似性，还需要对文档结构和特定领域的知识进行精细的推理。最近，大型语言模型（LLM）的进步为具有多步推理的检索提供了新的机会。在这种方法中，模型通过迭代推理对与给定查询最相关的信息进行排名。然而，金融领域还没有一个基准测试来评估这种能力。为了弥补这一空白，我们引入了FinAgentBench，这是第一个用于评估金融领域中具有多步推理的检索的大型基准测试——我们称之为代理检索。该基准测试包含有关标准普尔500上市公司的2.6万个专家注释示例，并评估LLM代理是否能（1）在候选文档类型中识别出最相关的文档类型，以及（2）在所选文档中定位关键段落。我们的评估框架明确地将这两个推理步骤分开，以解决上下文限制的问题。这种设计能够为我们理解金融领域中以检索为中心的LLM行为提供定量依据。我们评估了一系列最先进的模型，并进一步展示了有针对性的微调如何显着提高代理检索性能。我们的基准测试为研究复杂、特定领域的任务中金融领域的LLM行为提供了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v4">PDF</a> 6 pages</p>
<p><strong>Summary</strong><br>     为提高金融领域的信息检索准确性，研究者推出了FinAgentBench基准测试，用于评估具有多步骤推理能力的语言模型。该基准测试包含2.6万个专家注释的关于标普500上市公司的例子，旨在评估语言模型代理能否识别最相关的文档类型并定位关键段落。该基准测试为理解针对金融复杂任务的检索中心语言模型行为提供了定量基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融领域的信息检索至关重要，投资者需从大量文档中找到相关信息。</li>
<li>传统信息检索方法（无论是稀疏还是密集）在检索准确性方面常常不足。</li>
<li>大型语言模型（LLMs）的最新进展为具有多步骤推理的检索提供了新的机会。</li>
<li>FinAgentBench是首个针对金融领域多步骤推理评估的大型基准测试。</li>
<li>该基准测试包含专家注释的例子，旨在评估语言模型代理识别最相关文档类型和定位关键段落的能力。</li>
<li>基准测试的设计提供了理解针对金融复杂任务的检索中心语言模型行为的定量基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e67b298766d64441c67ca8ac9c11fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942416&auth_key=1759942416-0-0-ee829e87642ec14996410498ba3bd27f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33fb8858b61d505b4d3e5c36a8853ae3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942423&auth_key=1759942423-0-0-7abbadb6dce4df64b44e49016612b641&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6a46765e47c81459dca0d6bcc678956~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942429&auth_key=1759942429-0-0-ddba7ccaf6a02f08f6de733ebadbb962&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c9ab698bd07fc75334d1a2448d3b6e4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942436&auth_key=1759942436-0-0-f625a1b00d04c4da51e70930e3ada6e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b53eeebce52e81ffd410e01298428d7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942442&auth_key=1759942442-0-0-da8c0c18f43849ceca262d60351c14ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EconWebArena-Benchmarking-Autonomous-Agents-on-Economic-Tasks-in-Realistic-Web-Environments"><a href="#EconWebArena-Benchmarking-Autonomous-Agents-on-Economic-Tasks-in-Realistic-Web-Environments" class="headerlink" title="EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in   Realistic Web Environments"></a>EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in   Realistic Web Environments</h2><p><strong>Authors:Zefang Liu, Yinzhu Quan</strong></p>
<p>We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence. </p>
<blockquote>
<p>我们介绍了EconWebArena，这是一个在真实网络环境中评估自主代理在复杂、多模式经济任务上表现的标准。该标准包含来自82个权威网站的360个精心挑选的任务，涵盖宏观经济、劳动力、金融、贸易和公共政策等领域。每个任务都挑战代理在实时网站上导航、解释结构化和可视化内容、与真实界面进行交互，并通过多步骤工作流程提取精确且时间敏感的数据。我们通过提示多个大型语言模型（LLM）生成候选任务来构建这个标准，随后进行严格的人工筛选以确保清晰度、可行性和来源可靠性。与以前的工作不同，EconWebArena强调忠实于权威数据来源和基于网络的经济推理的需求。我们评估了一组先进的多模式LLM作为网络代理的表现，分析失败案例，并进行剔除研究以评估视觉定位、基于计划的推理和交互设计的影响。我们的结果揭示了巨大的性能差距，并强调了定位、导航和多模式理解方面的持续挑战，将EconWebArena定位为经济网络情报的严格测试平台。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08136v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文介绍了EconWebArena，这是一个用于评估现实网络环境中复杂多模式经济任务的自主代理性能的基准测试。该基准测试包含来自82个权威网站的360个精选任务，涵盖宏观经济、劳动、金融、贸易和公共政策等领域。EconWebArena强调对权威数据源的忠实性和基于网络的经济推理需求。文章评估了多种先进的多模式大型语言模型作为网络代理的性能，分析了失败情况，并通过废除研究评估了视觉定位、基于计划的推理和交互设计的影响。研究结果显示出巨大的性能差距，并指出了定位、导航和多模式理解方面的持续挑战，将EconWebArena定位为经济网络情报的严格测试平台。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>EconWebArena是一个用于评估自主代理在现实网络环境中完成复杂经济任务性能的基准测试。</li>
<li>该测试包含来自不同领域的360个任务，强调权威数据源的忠实性和基于网络的经济推理。</li>
<li>通过多个大型语言模型评估了作为网络代理的性能。</li>
<li>分析了代理在完成任务时的失败情况，并进行了废除研究以评估不同因素（如视觉定位、计划推理和交互设计）的影响。</li>
<li>研究结果显示出显著的性能差距，尤其是在定位、导航和多模式理解方面存在挑战。</li>
<li>EconWebArena为经济网络情报提供了一个严格的测试平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08136">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c77a7bf7c6db45149cae32f1108dbcf.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9433c7cbd06a0c6ed3f45c907ca73117~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942457&auth_key=1759942457-0-0-4aac0e3f254e90c99e542b9610072251&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-9417ff44d35c8dc4ab41314e64be89f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b48b2d3f384c0b6ae453df3d6cd030.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ebe83300324500474b4ea5f54c900f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942477&auth_key=1759942477-0-0-1412fc6ec5b48288bb9a901ab66b90e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a0ba92b1f497aa4a467820acf849036~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942484&auth_key=1759942484-0-0-7f4b54c6c0c2b9a097a71135bd380b1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning"><a href="#Grounding-Multimodal-LLMs-to-Embodied-Agents-that-Ask-for-Help-with-Reinforcement-Learning" class="headerlink" title="Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning"></a>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with   Reinforcement Learning</h2><p><strong>Authors:Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi</strong></p>
<p>Embodied agents operating in household environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent is tasked with a single or multi-object rearrangement task using an under-specified instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To address this challenge, we propose a novel approach that fine-tunes multi-modal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines including GPT-4o as well as supervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned MLLM outperforms all baselines by a significant margin (10.4-16.5%), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL. </p>
<blockquote>
<p>在家庭环境中运行的实体代理必须解释模糊和未指定的人类指令。一个能干的家用机器人应该能够识别模糊性，并提出相关澄清问题，以准确推断用户意图，从而实现更有效的任务执行。为了研究这个问题，我们引入了“问再行动”任务，在该任务中，实体代理需要在家庭环境中使用未指定的指令完成单对象或多对象重新布置任务。代理必须在部分可观察的情况下解决模糊问题，同时策略性地提出最少但相关的问题。为了应对这一挑战，我们提出了一种新的方法，它通过在线强化学习（RL）微调多模态大型语言模型（MLLMs），将其作为视觉语言行动（VLA）策略。我们的方法消除了训练此类代理需要大量人类演示或手工工程奖励的需求。我们在任务上与强大的零基准线进行了基准测试，包括GPT-4o以及经过监督训练的MLLMs。我们的结果表明，我们的RL微调MLLM在所有的基线测试中表现出显著的优越性（提升了10.4%~16.5%），并在新型场景和任务中具有良好的泛化能力。据我们所知，这是首次展示将MLLMs适应为VLA代理，这些代理可以使用LLM生成的奖励与在线RL来行动和寻求帮助。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00907v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种适应于家庭环境的智能体技术。智能体需要根据模糊和未明确的人类指令进行任务执行。为了准确推断用户意图并提高任务执行效率，智能体需要识别模糊性并提出相关澄清问题。为此，本文引入了“问后行动”任务，并提出了一种使用在线强化学习和大型语言模型奖励微调的多模态策略，解决了在部分可观察环境下的导航和澄清问题。该方法无需大量人类演示或手动设计的奖励，并且在基准测试中显著优于其他方法，包括GPT-4o等零样本基线。这是首次将大型语言模型适应为视觉语言行动智能体，能够利用在线强化学习和语言模型生成的奖励进行行动和寻求帮助。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>智能体在家庭环境中需要处理模糊和未明确的人类指令。</li>
<li>智能体通过提出相关澄清问题来推断用户意图。</li>
<li>引入了“问后行动”任务以模拟智能体的实际操作环境。</li>
<li>提出了一种使用在线强化学习和大型语言模型奖励微调的多模态策略。</li>
<li>该方法无需大量人类演示或手动设计的奖励。</li>
<li>在基准测试中，该方法显著优于其他方法，包括GPT-4o等零样本基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4e6bff9328268d6953574044726cfa46~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942492&auth_key=1759942492-0-0-5b396c5c086bbc6624d6571cffd4608e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73d8f16acf62c8b4923d695093b44c26~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942499&auth_key=1759942499-0-0-6389929dfd794ca49831da8751010e42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-245141559a152456485cf5b9e7cafc44.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DatawiseAgent-A-Notebook-Centric-LLM-Agent-Framework-for-Adaptive-and-Robust-Data-Science-Automation"><a href="#DatawiseAgent-A-Notebook-Centric-LLM-Agent-Framework-for-Adaptive-and-Robust-Data-Science-Automation" class="headerlink" title="DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and   Robust Data Science Automation"></a>DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and   Robust Data Science Automation</h2><p><strong>Authors:Ziming You, Yumiao Zhang, Dexuan Xu, Yiwei Lou, Yandong Yan, Wei Wang, Huaming Zhang, Yu Huang</strong></p>
<p>Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability. </p>
<blockquote>
<p>现有的大型语言模型（LLM）代理在自动化数据科学方面显示出潜力，但它们仍受到任务范围狭窄、任务间和模型间泛化能力有限以及对最新（SOTA）LLM过度依赖的限制。我们推出了DatawiseAgent，这是一个以笔记本为中心的LLM代理框架，用于自适应和稳健的数据科学自动化。DatawiseAgent以人类数据科学家在计算笔记本中的工作方式为基础，引入了一种统一的交互表示和基于有限状态转换器（FSTs）的多阶段架构。这种设计实现了灵活的长周期规划、渐进的解决方案开发和从执行失败中的稳健恢复。在多种数据科学场景和模型上的广泛实验表明，DatawiseAgent通过超越强大的基线（如AutoGen和TaskWeaver）持续实现最佳性能，显示出卓越的有效性和适应性。进一步的评估显示，在较弱或较小的模型下，性能优雅降级，突出了其稳健性和可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07044v2">PDF</a> The camera-ready version for EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>DatawiseAgent是一个针对自适应和稳健数据科学自动化的笔记本中心化的大型语言模型（LLM）代理框架。它通过引入统一的交互表示和多阶段架构，实现了灵活的长周期规划、渐进的解决方案开发和从执行故障中的稳健恢复。它在多样化的数据科学场景和模型中的实验表现优秀，超越了AutoGen和TaskWeaver等强基线，显示出优越的有效性和适应性。即使在较弱或较小的模型下，也能保持优雅的降级性能，突显了其稳健性和可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DatawiseAgent是一个针对数据科学自动化的新型大型语言模型（LLM）代理框架。</li>
<li>它以笔记本为中心，模拟人类数据科学家的工作方式。</li>
<li>通过引入统一的交互表示和多阶段架构，实现了灵活规划和稳健恢复。</li>
<li>该框架具备优秀的效果和适应性，超越了一些现有基线。</li>
<li>DatawiseAgent能够支持从弱模型到强模型的性能退化，显示了其稳健性。</li>
<li>它的设计可以支持模型的逐步发展和渐进解决方案的开发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b166310409610ac7754f21d7435989d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942513&auth_key=1759942513-0-0-19ef3d5fd0fad306eb6889e0abf9c8d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f238da88971a788b9ee500b0a9d7d415~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942520&auth_key=1759942520-0-0-9c9dba909a706bccda5e4791e4027e53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6dab3780ffc1f3791ff1ab58207675d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942527&auth_key=1759942527-0-0-8592bf6fd1c163940f039c1c32c468d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b7c0fa322a111ac3c924df049ae971e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942534&auth_key=1759942534-0-0-1a51547612e52810883c275a13750d2c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-3737228305f404bdd71e1854e307f53b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Programming-with-Pixels-Can-Computer-Use-Agents-do-Software-Engineering"><a href="#Programming-with-Pixels-Can-Computer-Use-Agents-do-Software-Engineering" class="headerlink" title="Programming with Pixels: Can Computer-Use Agents do Software   Engineering?"></a>Programming with Pixels: Can Computer-Use Agents do Software   Engineering?</h2><p><strong>Authors:Pranjal Aggarwal, Sean Welleck</strong></p>
<p>Computer-use agents (CUAs) hold the promise of performing a wide variety of general tasks, but current evaluations have primarily focused on simple scenarios. It therefore remains unclear whether such generalist agents can automate more sophisticated and specialized work such as software engineering (SWE). To investigate this, we introduce $\texttt{Programming with Pixels}$ (PwP), the first comprehensive computer-use environment for software engineering, where agents visually control an IDE to perform diverse software engineering tasks. To enable holistic evaluation, we also introduce \texttt{PwP-Bench}, a benchmark of 15 existing and new software-engineering tasks spanning multiple modalities, programming languages, and skillsets. We perform an extensive evaluation of state-of-the-art open-weight and closed-weight CUAs and find that when interacting purely visually, they perform significantly worse than specialized coding agents. However, when the same CUAs are given direct access to just two APIs-file editing and bash operations-performance jumps, often reaching the levels of specialized agents despite having a task-agnostic design. Furthermore, when given access to additional IDE tools via text APIs, all models show further gains. Our analysis shows that current CUAs fall short mainly due to limited visual grounding and the inability to take full advantage of the rich environment, leaving clear room for future improvements.PwP establishes software engineering as a natural domain for benchmarking whether generalist computer-use agents can reach specialist-level performance on sophisticated tasks. Code and data released at <a target="_blank" rel="noopener" href="https://programmingwithpixels.com/">https://programmingwithpixels.com</a> </p>
<blockquote>
<p>计算机使用代理（CUAs）具有执行各种通用任务的潜力，但目前的评估主要集中在简单场景上。因此，尚不清楚这种通用代理是否能够自动化更复杂和专门的工作，如软件工程（SWE）。为了调查这一点，我们引入了“Programming with Pixels”（PwP），这是第一个用于软件工程的全面计算机使用环境，其中代理通过视觉控制集成开发环境（IDE）来执行多种软件工程任务。为了实现整体评估，我们还介绍了PwP-Bench，这是一个涵盖多种模式、编程语言和技能集的15项现有和新软件工程任务的基准测试。我们对最先进的开放权重和封闭权重的CUAs进行了广泛评估，发现当仅通过视觉交互时，它们的表现远远落后于专业编码代理。但是，当为这些相同的CUA提供直接访问仅两个API（文件编辑和bash操作）时，性能会得到提升，尽管它们具有与任务无关的设计，但往往能达到专业代理的水平。此外，当通过文本API访问其他IDE工具时，所有模型都表现出进一步的增益。我们的分析表明，当前CUAs的主要不足在于视觉定位有限以及无法充分利用丰富的环境，这为未来的改进留下了明确的空间。PwP确立了软件工程作为衡量通用计算机使用代理是否能在复杂任务上达到专业代理水平的自然领域。代码和数据发布在<a target="_blank" rel="noopener" href="https://programmingwithpixels.com./">https://programmingwithpixels.com。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18525v2">PDF</a> </p>
<p><strong>Summary</strong>：本研究引入了首个用于软件工程领域的综合计算机使用环境，即Programming with Pixels（PwP）。它使代理能够可视化地控制IDE来执行各种软件工程任务。为进行整体评估，还推出了PwP-Bench基准测试，涵盖多种模态、编程语言和技能集的15项现有和新软件工程任务。评估发现，当仅通过视觉交互时，当前最先进的通用代理在软件工程任务上的表现较差，但当给予直接访问某些API时，性能有所提升。分析表明，当前通用代理主要因视觉定位有限和无法充分利用丰富环境而表现不足，但仍存在改进空间。PwP为评估通用代理在复杂任务上是否能达到专业级别提供了重要平台。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究提出了用于软件工程的计算机使用环境Programming with Pixels（PwP），允许代理可视化控制IDE执行多种任务。</li>
<li>引入了PwP-Bench基准测试，包含多种软件工程任务的基准测试。</li>
<li>评估发现仅通过视觉交互的通用代理在软件工程任务上表现较差。</li>
<li>当通用代理被赋予直接访问某些API时，性能有所提升，有时甚至能达到专业代理的水平。</li>
<li>当前通用代理的主要短板在于视觉定位有限和无法充分利用环境资源。</li>
<li>研究结果显示未来存在改进空间，可通过提升视觉识别和API利用能力来提升通用代理的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-89913dbcbc277bc4fd93a6e1482d6f2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09be6190956a83fe86e9f756a4fd70b0.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-022b1710ba59b5c4126ebc75f919001d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942562&auth_key=1759942562-0-0-dc531513ecd0c6a0a8fe9734a9ca60fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-374c7e42a539d57fc818f563580763e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942570&auth_key=1759942570-0-0-5d75e51ea2fc4d1c4575ad0cb79eef92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-10-07  CBVLM Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7d03dbc64358d200f7fda4601a954148.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-10-07  LEAML Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
