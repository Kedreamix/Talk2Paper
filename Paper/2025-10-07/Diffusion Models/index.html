<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-07  Product-Quantised Image Representation for High-Quality Image Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5dd7309b3a8b9b771cfaf09a2084da7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943396&auth_key=1759943396-0-0-150deb2778e9abd165fc3002977856db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-07-更新"><a href="#2025-10-07-更新" class="headerlink" title="2025-10-07 更新"></a>2025-10-07 更新</h1><h2 id="Product-Quantised-Image-Representation-for-High-Quality-Image-Synthesis"><a href="#Product-Quantised-Image-Representation-for-High-Quality-Image-Synthesis" class="headerlink" title="Product-Quantised Image Representation for High-Quality Image Synthesis"></a>Product-Quantised Image Representation for High-Quality Image Synthesis</h2><p><strong>Authors:Denis Zavadski, Nikita Philip Tatsch, Carsten Rother</strong></p>
<p>Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis. </p>
<blockquote>
<p>产品量化（PQ）是一种用于可扩展向量编码的经典方法，但在高保真图像生成中的潜在表示应用有限。在这项工作中，我们介绍了PQGAN，这是一个将PQ集成到众所周知的VQGAN的向量量化（VQ）框架中的量化图像自编码器。PQGAN在重建性能上实现了对最先进方法的显著改进，包括量化方法和它们的连续对应物。我们达到的PSNR分数为37dB，而先前的工作只达到27dB，并且能够将FID、LPIPS和CMMD分数降低高达96%。我们成功的关键是全面分析了码本大小、嵌入维度和子空间分解之间的相互作用，以向量和标量量化作为特殊情况。我们获得了新的发现，即在缩放嵌入维度时，VQ和PQ的性能表现相反。此外，我们的分析显示了PQ的性能趋势，有助于选择最佳超参数。最后，我们证明PQGAN可以无缝集成到预训练的扩散模型中。这使得要么生成过程更快、更节省计算资源，要么在不增加成本的情况下将输出分辨率翻倍，从而定位PQ作为图像合成中离散潜在表示的强大扩展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03191v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了PQGAN，一种将产品量化（PQ）融入VQGAN的向量量化框架的量化图像自编码器。PQGAN在重建性能上较现有方法有明显改进，实现了37dB的PSNR分数，并降低了FID、LPIPS和CMMD分数高达96%。其成功的关键在于对代码本大小、嵌入维度和子空间分解之间的相互作用进行了深入分析。此外，PQGAN可无缝集成到预训练的扩散模型中，可实现更快、更高效的生成或输出分辨率翻倍而无需额外成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PQGAN结合了产品量化（PQ）和向量量化（VQ）框架，实现了高保真图像生成中的潜表示。</li>
<li>PQGAN在重建性能上较现有方法有明显改进，PSNR分数达到37dB。</li>
<li>PQGAN降低了FID、LPIPS和CMMD分数高达96%。</li>
<li>通过分析代码本大小、嵌入维度和子空间分解的相互作用，PQGAN取得了成功。</li>
<li>VQ和PQ在扩大嵌入维度时表现相反，这是PQGAN的新发现。</li>
<li>PQGAN的分析展示了PQ的性能趋势，有助于选择最佳超参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03191">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6dfccbd83297c60e6950d81240c8431c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943404&auth_key=1759943404-0-0-248c905c858ce8596576ee6075df50f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-732389776851c44379334d0e689a65bc.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca5724cd56fb2eb28ca673adc2128bb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943440&auth_key=1759943440-0-0-5d8a68bf203b6f9fccec3680fe85da28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfcc14f874dc033cc7ef4c9663d9dcf1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943447&auth_key=1759943447-0-0-4d180d51a960ee3d5b01a9f03894929f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HAVIR-HierArchical-Vision-to-Image-Reconstruction-using-CLIP-Guided-Versatile-Diffusion"><a href="#HAVIR-HierArchical-Vision-to-Image-Reconstruction-using-CLIP-Guided-Versatile-Diffusion" class="headerlink" title="HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided   Versatile Diffusion"></a>HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided   Versatile Diffusion</h2><p><strong>Authors:Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou</strong></p>
<p>The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models. </p>
<blockquote>
<p>从脑活动中重建视觉信息促进了神经科学和计算机视觉之间的跨学科融合。然而，现有方法仍然面临着在复杂视觉刺激中准确恢复方面的挑战。这一难题源于自然场景的特性：低级特征表现出异质性，而高级特征则因上下文重叠而显示出语义纠缠。受视觉皮层层次表示理论的启发，我们提出了HAVIR模型，该模型将视觉皮层分为两个层次区域，并从每个区域中提取不同的特征。具体来说，结构生成器从空间处理体素中提取结构信息并将其转换为潜在扩散先验信息，而语义提取器将语义处理体素转换为CLIP嵌入。这些组件通过通用扩散模型进行集成，以合成最终图像。实验结果表明，即使在复杂场景中，HAVIR也能提高重建的结构和语义质量，并且优于现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于视觉皮层层次表示理论的HAVIR模型，该模型将视觉皮层分为两个层次区域，分别提取不同特征。通过结构生成器提取空间处理体素的结构信息并转换为潜在扩散先验，语义提取器则将语义处理体素转换为CLIP嵌入。通过通用扩散模型集成这些组件以合成最终图像。实验结果表明，HAVIR模型在复杂场景下能提升重建图像的结构和语义质量，并优于现有模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建视觉信息面临挑战：现有方法难以准确恢复高度复杂的视觉刺激，源于自然场景的特性，即低级别特征的异质性和高级别特征的语义纠缠。</li>
<li>HAVIR模型提出基于视觉皮层层次表示理论：该模型灵感来源于视觉皮层的层次表示理论，将视觉皮层分为两个层次区域进行特征提取。</li>
<li>结构生成器和语义提取器的功能：结构生成器从空间处理体素中提取结构信息并转换为潜在扩散先验，而语义提取器则将语义处理体素转换为CLIP嵌入。</li>
<li>Versatile Diffusion模型的集成作用：通过通用扩散模型集成结构生成器和语义提取器的输出，以合成最终图像。</li>
<li>HAVIR模型的优势：实验结果表明，HAVIR模型在复杂场景下能提升重建图像的结构和语义质量。</li>
<li>HAVIR模型性能超越现有方法：相比其他模型，HAVIR表现出更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cfd2e61c79af339471288ba4fcfad07.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb0693ea58fdb813f086fc69380a43da~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943461&auth_key=1759943461-0-0-e352d52dc8c3c46af5dcf47e39be438d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dd7309b3a8b9b771cfaf09a2084da7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943468&auth_key=1759943468-0-0-d6a712c25944a7bae5e684a07fc16379&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aabffc6dbdc9e9ede5a3b1612c981351~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943474&auth_key=1759943474-0-0-ef4d5668f821c8eefc0028da9c8806ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a38c148322e282ea16103ddd06658a6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943481&auth_key=1759943481-0-0-840e645113a59035da4804d8f1f6df64&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Latent-Diffusion-Unlearning-Protecting-Against-Unauthorized-Personalization-Through-Trajectory-Shifted-Perturbations"><a href="#Latent-Diffusion-Unlearning-Protecting-Against-Unauthorized-Personalization-Through-Trajectory-Shifted-Perturbations" class="headerlink" title="Latent Diffusion Unlearning: Protecting Against Unauthorized   Personalization Through Trajectory Shifted Perturbations"></a>Latent Diffusion Unlearning: Protecting Against Unauthorized   Personalization Through Trajectory Shifted Perturbations</h2><p><strong>Authors:Naresh Kumar Devulapally, Shruti Agarwal, Tejas Gokhale, Vishnu Suresh Lokhande</strong></p>
<p>Text-to-image diffusion models have demonstrated remarkable effectiveness in rapid and high-fidelity personalization, even when provided with only a few user images. However, the effectiveness of personalization techniques has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. To mitigate such unauthorized usage and model replication, the idea of generating &#96;&#96;unlearnable’’ training samples utilizing image poisoning techniques has emerged. Existing methods for this have limited imperceptibility as they operate in the pixel space which results in images with noise and artifacts. In this work, we propose a novel model-based perturbation strategy that operates within the latent space of diffusion models. Our method alternates between denoising and inversion while modifying the starting point of the denoising trajectory: of diffusion models. This trajectory-shifted sampling ensures that the perturbed images maintain high visual fidelity to the original inputs while being resistant to inversion and personalization by downstream generative models. This approach integrates unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a practical and imperceptible defense against unauthorized model adaptation. We validate our approach on four benchmark datasets to demonstrate robustness against state-of-the-art inversion attacks. Results demonstrate that our method achieves significant improvements in imperceptibility ($\sim 8 % -10%$ on perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10%$ on average across five adversarial settings), highlighting its effectiveness in safeguarding sensitive data. </p>
<blockquote>
<p>文本到图像的扩散模型在快速高保真个性化方面表现出显著的有效性，即使只提供少量用户图像也是如此。然而，个性化技术的有效性引发了有关数据隐私、知识产权保护和未经授权使用等问题的担忧。为了缓解未经授权的使用和模型复制问题，利用图像中毒技术生成“不可学习”训练样本的想法应运而生。现有的方法有限的隐蔽性是因为它们在像素空间操作，导致图像出现噪声和伪影。在这项工作中，我们提出了一种基于模型的新型扰动策略，该策略在扩散模型的潜在空间内运行。我们的方法交替进行去噪和反演，同时改变去噪轨迹的起点：扩散模型。这种轨迹偏移采样确保扰动图像在保持对原始输入高视觉保真度的同时，抵抗下游生成模型的反演和个性化。这种方法将不可学习性集成到潜在扩散模型（LDM）的框架中，为对抗未经授权的模型适应提供了实用且隐蔽的防御手段。我们在四个基准数据集上验证了我们的方法，以展示其对抗最先进反演攻击的稳健性。结果表明，我们的方法在感知度量（包括PSNR、SSIM和FID）上实现了显著的可觉察性改进（约8%-10%），并在五种对抗设置上平均提高了约10%的稳健性，这突显了其在保护敏感数据方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03089v1">PDF</a> </p>
<p><strong>Summary</strong>：本文提出一种基于模型扰动策略的防未授权使用策略，通过修改扩散模型的去噪轨迹起点来生成不可学习的训练样本，能够在保持图像高保真度的同时抵抗下游生成模型的逆向工程和个性化操作。该策略集成在潜在扩散模型（Latent Diffusion Models，简称LDM）框架中，通过验证显示，其在保护敏感数据方面具有良好的实用性和隐蔽性优势。此方法能显著提升防护隐蔽性感知度量标准（如PSNR、SSIM和FID等）约8%-10%，并在五种对抗环境中平均提高约10%的稳健性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>文本到图像扩散模型的高效个性化能力引发关于数据隐私和知识产权的担忧。</li>
<li>为应对未经授权的使用和模型复制问题，提出利用图像中毒技术生成不可学习的训练样本。</li>
<li>现有方法因在像素空间操作而存在可见噪声和伪影的问题。</li>
<li>提出一种新型基于模型扰动的策略，在扩散模型的潜在空间内操作。</li>
<li>通过交替去噪和反转操作，修改去噪轨迹起点以生成具有高质量且对下游生成模型逆向工程和个性化抵抗的图像。</li>
<li>该策略集成在潜在扩散模型框架中，提升了防御策略在实际应用中的实用性和隐蔽性优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9bffd78992c0d64df37b905f9a0104ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943489&auth_key=1759943489-0-0-9b6a636efad6a6fcd1ec35996e5f8af8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-559da5ba9ab9b20ab77e7a0834f7ef59~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943496&auth_key=1759943496-0-0-1ec74a34e141d91eff613cd734a41c53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4f394bd8bbf009d3046a0e0986d7383~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943503&auth_key=1759943503-0-0-139ce65d11f3e50e22de430d6bb3f824&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-4509cdf4acf699173f9d2d8895913a10.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PEO-Training-Free-Aesthetic-Quality-Enhancement-in-Pre-Trained-Text-to-Image-Diffusion-Models-with-Prompt-Embedding-Optimization"><a href="#PEO-Training-Free-Aesthetic-Quality-Enhancement-in-Pre-Trained-Text-to-Image-Diffusion-Models-with-Prompt-Embedding-Optimization" class="headerlink" title="PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained   Text-to-Image Diffusion Models with Prompt Embedding Optimization"></a>PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained   Text-to-Image Diffusion Models with Prompt Embedding Optimization</h2><p><strong>Authors:Hovhannes Margaryan, Bo Wan, Tinne Tuytelaars</strong></p>
<p>This paper introduces a novel approach to aesthetic quality improvement in pre-trained text-to-image diffusion models when given a simple prompt. Our method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model as a backbone and optimizes the text embedding of a given simple and uncurated prompt to enhance the visual quality of the generated image. We achieve this by a tripartite objective function that improves the aesthetic fidelity of the generated image, ensures adherence to the optimized text embedding, and minimal divergence from the initial prompt. The latter is accomplished through a prompt preservation term. Additionally, PEO is training-free and backbone-independent. Quantitative and qualitative evaluations confirm the effectiveness of the proposed method, exceeding or equating the performance of state-of-the-art text-to-image and prompt adaptation methods. </p>
<blockquote>
<p>本文介绍了一种在给定简单提示时，提高预训练文本到图像扩散模型美学质量的新方法。我们的方法被称为提示嵌入优化（PEO），它利用预训练的文本到图像扩散模型作为骨干，并通过优化给定简单未整理的提示的文本嵌入来增强生成图像的可视质量。我们通过三方目标函数实现这一点，该函数提高了生成图像的美学保真度，确保遵循优化后的文本嵌入，并且与初始提示的偏差最小。后者是通过提示保留项来实现的。此外，PEO无需训练且独立于骨干网。定量和定性评估证实了所提出方法的有效性，其性能超过了或等同于最先进的文本到图像和提示适应方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02599v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种改进预训练文本到图像扩散模型美学质量的新方法，称为Prompt Embedding Optimization（PEO）。该方法利用预训练的文本到图像扩散模型作为骨干，优化给定简单未整理提示的文本嵌入，以提高生成图像的可视质量。通过三方目标函数实现，旨在提高生成图像的美学保真度，确保优化文本嵌入的遵循，并尽量减少与初始提示的偏差。此外，PEO具有无训练和独立于骨干的特点。定量和定性评估证明了该方法的有效性，超越了或等同于最先进文本到图像和提示适应方法的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种名为Prompt Embedding Optimization (PEO)的新方法，用于改进预训练文本到图像扩散模型的美学质量。</li>
<li>PEO利用预训练的文本到图像扩散模型作为骨干，优化给定提示的文本嵌入。</li>
<li>通过三方目标函数提高生成图像的美学质量，确保遵循优化后的文本嵌入，并尽量减少与初始提示的偏差。</li>
<li>PEO具有无训练的特点，意味着它不需要额外的训练过程。</li>
<li>PEO独立于骨干，意味着它可以应用于不同的预训练文本到图像扩散模型。</li>
<li>定量和定性评估表明PEO的有效性，在某些方面甚至超越了当前最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cdf901f97302d7cf868c0d2382ac9d8f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943518&auth_key=1759943518-0-0-a4721c0e73506c2bc29cc63685b40436&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49e56f5b198027d55e9415667e3ebd8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759973475&auth_key=1759973475-0-0-7072700cd32dc2ba34792c1892f652d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73d168a8727330789daf8871f6858da8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943591&auth_key=1759943591-0-0-be8c30d69d8086e033a9bed805f39af6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-d22b372bec6aa3acb54c14f2de7f8881.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebf170f4ac189c5a9912555733c4acd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943605&auth_key=1759943605-0-0-9ab0240ae77df5aca75b2fc0fa9650a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-1abfd9fa4e2d34752ba9fa7d9d3a8fa5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-a-distance-measure-from-the-information-estimation-geometry-of-data"><a href="#Learning-a-distance-measure-from-the-information-estimation-geometry-of-data" class="headerlink" title="Learning a distance measure from the information-estimation geometry of   data"></a>Learning a distance measure from the information-estimation geometry of   data</h2><p><strong>Authors:Guy Ohayon, Pierre-Etienne H. Fiquet, Florentin Guth, Jona Ballé, Eero P. Simoncelli</strong></p>
<p>We introduce the Information-Estimation Metric (IEM), a novel form of distance function derived from an underlying continuous probability density over a domain of signals. The IEM is rooted in a fundamental relationship between information theory and estimation theory, which links the log-probability of a signal with the errors of an optimal denoiser, applied to noisy observations of the signal. In particular, the IEM between a pair of signals is obtained by comparing their denoising error vectors over a range of noise amplitudes. Geometrically, this amounts to comparing the score vector fields of the blurred density around the signals over a range of blur levels. We prove that the IEM is a valid global metric and derive a closed-form expression for its local second-order approximation, which yields a Riemannian metric. For Gaussian-distributed signals, the IEM coincides with the Mahalanobis distance. But for more complex distributions, it adapts, both locally and globally, to the geometry of the distribution. In practice, the IEM can be computed using a learned denoiser (analogous to generative diffusion models) and solving a one-dimensional integral. To demonstrate the value of our framework, we learn an IEM on the ImageNet database. Experiments show that this IEM is competitive with or outperforms state-of-the-art supervised image quality metrics in predicting human perceptual judgments. </p>
<blockquote>
<p>我们介绍了信息估计度量（IEM），这是一种新的距离函数形式，它源于信号域上潜在连续概率密度的分布。IEM 源于信息论和估计论之间的基本关系，它联系了一个信号的对数概率与应用于该信号的带噪声观测的最佳去噪器的误差。特别地，一对信号之间的 IEM 是通过比较它们在一系列噪声幅度下的去噪误差向量而获得的。从几何角度上讲，这相当于在一系列模糊级别下比较围绕信号的模糊密度得分向量场。我们证明了 IEM 是一个有效的全局度量，并推导出了其局部二阶近似的闭式表达式，这产生了一个黎曼度量。对于高斯分布的信号，IEM 与马氏距离相符。但对于更复杂的分布，它会在局部和全局适应分布几何。在实践中，可以使用学习到的去噪器（类似于生成扩散模型）并求解一维积分来计算 IEM。为了证明我们框架的价值，我们在 ImageNet 数据库上学习了 IEM。实验表明，该 IEM 在预测人类感知判断方面与最先进的监督图像质量指标竞争或优于后者。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02514v1">PDF</a> Code available at   <a target="_blank" rel="noopener" href="https://github.com/ohayonguy/information-estimation-metric">https://github.com/ohayonguy/information-estimation-metric</a></p>
<p><strong>Summary</strong><br>     信息估计度量（IEM）是一种新型的距离函数，它基于信号域上的连续概率密度分布。IEM 源于信息论与估计论之间的基本关系，它将信号的日志概率与应用于噪声观测信号的最佳去噪器的误差联系起来。通过对一系列噪声幅度下的去噪误差向量进行比较，获得两个信号之间的 IEM。从几何角度来看，相当于比较一系列模糊度下的围绕信号的模糊密度得分向量场。证明 IEM 是一种有效的全局度量，并推导出其局部二阶近似的封闭形式表达式，从而产生黎曼度量。对于高斯分布的信号，IEM 与马氏距离相符；而对于更复杂的分布，IEM 可适应分布的局部和全局几何特性。实践中，可使用学习到的去噪器（类似于生成扩散模型）并求解一维积分来计算 IEM。在 ImageNet 数据库上学习的 IEM 表明，它在预测人类感知判断方面表现出竞争力，甚至超过现有的最先进的监督图像质量度量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>信息估计度量（IEM）是一种新型距离函数，基于信号域上的连续概率密度分布。</li>
<li>IEM 源于信息论与估计论的结合，关联信号的日志概率与去噪器误差。</li>
<li>通过比较不同噪声幅度下的去噪误差向量，计算两个信号之间的 IEM。</li>
<li>IEM 在几何上表现为比较模糊密度得分向量场。</li>
<li>IEM 被证明是一种有效的全局度量，并具备黎曼度量的局部二阶近似形式。</li>
<li>对于不同分布的信号，IEM 能自适应调整，尤其对于复杂分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c42ee7948574418b46a79aebf7fcf8e4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb2d0711af70b6cacbb5ef08312826c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943627&auth_key=1759943627-0-0-2689dc640ec9b3e1faae7e40b145b9a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80674e346482949e4408e286ad5f7eab~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943634&auth_key=1759943634-0-0-7c32a4614d65e5039a5999836a723c89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction"><a href="#UniVerse-Unleashing-the-Scene-Prior-of-Video-Diffusion-Models-for-Robust-Radiance-Field-Reconstruction" class="headerlink" title="UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction"></a>UniVerse: Unleashing the Scene Prior of Video Diffusion Models for   Robust Radiance Field Reconstruction</h2><p><strong>Authors:Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</strong></p>
<p>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations. However, these methods rely heavily on dense observations for robustly optimizing model parameters. To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process. To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images. Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies. Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> </p>
<blockquote>
<p>本文解决了稳健重建的挑战，即从一个不一致的多视角图像集中重建一个三维场景的任务。一些近期的工作尝试通过整合图像退化建模到神经三维场景表示中来同时移除图像的不一致性和进行重建。然而，这些方法严重依赖于密集的观测结果以稳健地优化模型参数。为了解决这一问题，我们提议将稳健重建分解成两个子任务：恢复和重建，这自然地简化了优化过程。为此，我们引入了UniVerse，这是一个基于视频扩散模型的稳健重建的统一框架。具体来说，UniVerse首先把不一致的图像转换成初始视频，然后使用专门设计的视频扩散模型将它们恢复成一致的图像，并最终从这些恢复的图像中重建三维场景。与针对每个视图的退化建模相比，扩散模型从大规模数据中学习一般的场景先验，使其适用于多种图像不一致性。在合成和真实世界数据集上的大量实验证明了我们的方法在稳健重建中的强大通用性和卓越性能。此外，UniVerse可以控制重建的三维场景的风格。项目页面：<a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01669v2">PDF</a> page: <a target="_blank" rel="noopener" href="https://jin-cao-tma.github.io/UniVerse.github.io/">https://jin-cao-tma.github.io/UniVerse.github.io/</a> code:   <a target="_blank" rel="noopener" href="https://github.com/zju3dv/UniVerse">https://github.com/zju3dv/UniVerse</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于视频扩散模型的稳健重建方法UniVerse，用于从一组不一致的多视角图像重建3D场景。该方法将重建过程分解为两个子任务：修复和重建，简化了优化过程。它通过视频扩散模型将不一致的图像转换为初始视频，再恢复成一致图像，并从这些恢复后的图像重建3D场景。此方法具有强大的泛化能力和卓越性能，并且可以控制重建3D场景的风格。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文解决了从多个不一致的视角图像中重建3D场景的难题。</li>
<li>提出一种基于视频扩散模型的稳健重建方法——UniVerse。</li>
<li>UniVerse将重建过程分解为修复和重建两个子任务，简化了优化流程。</li>
<li>UniVerse使用视频扩散模型将不一致的图像转换为初始视频，然后恢复成一致图像。</li>
<li>该方法具有强大的泛化能力，可在合成和真实世界数据集上表现出卓越性能。</li>
<li>UniVerse能处理多种图像不一致性，因为扩散模型从大规模数据中学习场景的一般先验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-724373b69b7a18ea1670905f4450b13d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943642&auth_key=1759943642-0-0-75428336e07a8958a8659537453293b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5501dfb87dc50804467653c7e96b4c1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943649&auth_key=1759943649-0-0-b796d5ab3a6696693052f74c9fb1804b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-ab4ce4622f5a6100842c5ee32d7cbe5e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4e918d3000208f12a9d78ed9099a266~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943664&auth_key=1759943664-0-0-f8f2f9c21146fb305f3a0e658bc99997&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation"><a href="#RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation" class="headerlink" title="RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation"></a>RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation</h2><p><strong>Authors:Liheng Zhang, Lexi Pang, Hang Ye, Xiaoxuan Ma, Yizhou Wang</strong></p>
<p>Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., canny edge) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning-based approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. Through an empirical analysis of existing methods, we identify a key limitation: the sampling schedule of condition features, previously unexplored, fails to account for the evolving interplay between structure preservation and domain alignment throughout diffusion steps. Inspired by this observation, we propose a flexible training-free framework that decouples the sampling schedule of condition features from the denoising process, and systematically investigate the spectrum of feature injection schedules for a higher-quality structure guidance in the feature space. Specifically, we find that condition features sampled from a single timestep are sufficient, yielding a simple yet efficient schedule that balances structure alignment and appearance quality. We further enhance the sampling process by introducing a restart refinement schedule, and improve the visual quality with an appearance-rich prompting strategy. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art results across diverse zero-shot conditioning scenarios. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著的成功。最近的研究努力将这些模型扩展到结合条件图像（例如，Canny边缘）进行精细的空间控制。其中，特征注入方法作为一种无需训练的传统微调方法的替代方案而出现。然而，它们经常遭受结构错位、条件泄漏和视觉伪影的问题，尤其是当条件图像与自然的RGB分布相差很大时。通过对现有方法的实证分析，我们确定了一个关键限制：之前未被探索的条件特征的采样时间表未能考虑到结构保存在扩散步骤中的不断演变和领域对齐的交互作用。受此观察的启发，我们提出了一个灵活的无需训练框架，该框架将条件特征的采样时间表与去噪过程解耦，并系统地研究了特征注入时间表的频谱，以在特征空间中实现更高质量的结构引导。具体来说，我们发现从单一时间步长采样的条件特征就足够了，从而产生了一个简单而高效的时间表，平衡了结构对齐和外观质量。我们进一步通过引入重启细化时间表来增强采样过程，并使用丰富的外观提示策略提高了视觉质量。这些设计共同实现了无需训练的生成，既丰富结构又丰富外观。大量实验表明，我们的方法在多种零样本条件场景下面实现了最佳结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02792v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型已成功实现根据文本提示生成高质量图像。近期研究尝试将条件图像纳入其中以实现精细的空间控制。特征注入方法作为一种无训练的方法，被提出以解决传统微调方法所遇到的许多问题，例如结构不对齐、条件泄漏和视觉伪影等。本文分析了现有方法的采样时间表，发现其对扩散步骤中的结构保持和领域对齐的相互作用考虑不足。因此，本文提出了一个灵活的无训练框架，将条件特征的采样时间表与去噪过程分离，并系统地研究了特征注入的时间表，以在特征空间中实现更高质量的结构引导。实验表明，该方法在零样本条件场景下的结果达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像（T2I）扩散模型可根据文本提示生成高质量图像。</li>
<li>条件图像纳入模型可实现更精细的空间控制。</li>
<li>特征注入方法作为一种无训练方案解决了传统微调方法的问题，但仍面临结构不对齐等问题。</li>
<li>本文发现了现有方法采样时间表的不足，提出了灵活的框架进行系统性的研究。</li>
<li>单步采样条件特征策略找到了结构对齐和外观质量之间的平衡。</li>
<li>通过引入重启优化采样过程和外观丰富的提示策略，进一步提高了视觉质量和结构丰富性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02792">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c45788f3da04c35c0fd852a7f7612657~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943671&auth_key=1759943671-0-0-a20ec1dc59035d48571dfe1d2e916507&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78e1b3e6ac1cce4c521df78bdc662461~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943678&auth_key=1759943678-0-0-fdba0a8ad56153e3754d09d63d071830&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-064daca12f72582d450ea7a4e53d8893~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943685&auth_key=1759943685-0-0-3d4653e95098e0f6f104b417ad8835a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f6d62a7a2a3d2448f3ffeffcd73f9bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943693&auth_key=1759943693-0-0-e3c9c9f05b374930f359b4750bd5ed0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Vulnerability-of-Concept-Erasure-and-a-New-Method"><a href="#Rethinking-the-Vulnerability-of-Concept-Erasure-and-a-New-Method" class="headerlink" title="Rethinking the Vulnerability of Concept Erasure and a New Method"></a>Rethinking the Vulnerability of Concept Erasure and a New Method</h2><p><strong>Authors:Alex D. Richardson, Kaicheng Zhang, Lucas Beerens, Dongdong Chen</strong></p>
<p>The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. In response, concept erasure (defense) methods have been developed to “unlearn” specific concepts through post-hoc finetuning. However, recent concept restoration (attack) methods have demonstrated that these supposedly erased concepts can be recovered using adversarially crafted prompts, revealing a critical vulnerability in current defense mechanisms. In this work, we first investigate the fundamental sources of adversarial vulnerability and reveal that vulnerabilities are pervasive in the prompt embedding space of concept-erased models, a characteristic inherited from the original pre-unlearned model. Furthermore, we introduce <strong>RECORD</strong>, a novel coordinate-descent-based restoration algorithm that consistently outperforms existing restoration methods by up to 17.8 times. We conduct extensive experiments to assess its compute-performance tradeoff and propose acceleration strategies. </p>
<blockquote>
<p>文本到图像扩散模型的普及引发了重大隐私和安全问题，尤其是关于生成版权或有害图像的问题。作为回应，已经开发了概念消除（防御）方法，通过事后微调来“遗忘”特定概念。然而，最新的概念恢复（攻击）方法已经证明，这些被认为已经被删除的概念可以使用对抗性构建的提示进行恢复，这揭示了当前防御机制中的关键漏洞。在这项工作中，我们首先研究对抗性漏洞的根本来源，并揭示概念消除模型的提示嵌入空间中普遍存在漏洞，这是从原始未学习模型继承的特征。此外，我们引入了基于坐标下降的新恢复算法<strong>RECORD</strong>，它始终优于现有恢复方法，最高可达1.7倍。我们进行了大量实验来评估其计算性能折衷，并提出了加速策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17537v3">PDF</a> </p>
<p><strong>Summary</strong><br>     文本到图像扩散模型的普及引发了关于生成版权或有害图像的隐私和安全担忧。为此，开发了概念消除（防御）方法，通过微调后的模型进行特定的概念遗忘。然而，最新的概念恢复（攻击）方法证明，这些被消除的概念可以通过对抗性构造的提示进行恢复，揭示了当前防御机制的重大漏洞。本研究深入探讨了对抗性漏洞的根本来源，揭示了概念消除模型的提示嵌入空间中普遍存在漏洞，这一特性继承了未学习前的原始模型的特点。此外，我们引入了新型的基于坐标下降的恢复算法RECORD，该算法在性能上一直优于现有恢复方法，最高提升了达17.8倍。我们进行了大量实验来评估其计算性能权衡并提出了加速策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像扩散模型的普及引发了关于生成版权或有害图像的隐私和安全担忧。</li>
<li>概念消除（防御）方法旨在通过微调模型来遗忘特定概念。</li>
<li>现有的概念消除方法存在重大漏洞，可以被概念恢复（攻击）方法利用。</li>
<li>对抗性构造的提示可用于恢复被消除的概念。</li>
<li>概念消除模型的提示嵌入空间普遍存在漏洞，这一特性继承自原始模型。</li>
<li>引入了新型的基于坐标下降的恢复算法RECORD，其性能显著优于现有恢复方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17537">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b3d0f91645e5e18a6f4cefe3b8393ee7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943701&auth_key=1759943701-0-0-e78c67a327f7e4187a2b337ca9772e3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-c643f7e64cfbf237f41a337bbdf2892b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-679c81a8e796b3f0d9e8aff57d4cbe7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943715&auth_key=1759943715-0-0-a299214e92264c9a9f11745af0273856&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-b144204b970c44b83070e0ed2780eb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943722&auth_key=1759943722-0-0-625ee05b6d124873902037d8957973e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-07  Wave-GMS Lightweight Multi-Scale Generative Model for Medical Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-408eb3e041df5b2193760a92cbca85a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759943359&auth_key=1759943359-0-0-c338526f778e1efcc48fce56b1307a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-07  ROGR Relightable 3D Objects using Generative Relighting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
