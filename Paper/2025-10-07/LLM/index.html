<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-07  LEAML Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7d03dbc64358d200f7fda4601a954148.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-07-更新"><a href="#2025-10-07-更新" class="headerlink" title="2025-10-07 更新"></a>2025-10-07 更新</h1><h2 id="LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models"><a href="#LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models" class="headerlink" title="LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models"></a>LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models</h2><p><strong>Authors:Ci-Siang Lin, Min-Hung Chen, Yu-Yang Sheng, Yu-Chiang Frank Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在通用视觉基准测试中取得了出色的表现，但在特定领域的非常规任务（如医学成像）中却表现挣扎，因为医学成像等领域的标注数据既有限又昂贵。我们引入了LEAML，这是一个标签高效适应框架，它充分利用了稀缺的标注VQA样本和大量的未标注图像。我们的方法使用通过标题蒸馏进行正则化的QA生成器，为未标注数据生成与领域相关的伪问答对。重要的是，我们只选择性地更新与问答最相关的神经元，使QA生成器在蒸馏过程中能够有效地获取特定领域的知识。在胃镜和体育问答方面的实验表明，在最小监督下，LEAML始终优于标准微调方法，突显了我们提出的LEAML框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在通用视觉基准测试中，多模态大型语言模型（MLLMs）表现优异，但在专业领域如医疗成像等面对分布外的任务时面临挑战。针对此问题，我们提出了LEAML框架，该框架利用稀缺的标签化问答样本和大量的未标签图像。我们的方法通过利用标题蒸馏正则化的问答生成器为未标记数据生成领域相关的伪问答对。我们仅选择性更新与问答最相关的神经元，这允许问答生成器在蒸馏过程中有效地获取特定领域的知识。在胃肠道内窥镜和体育问答中的实验表明，在监督不足的情况下，LEAML持续优于标准微调，凸显了LEAML框架的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在通用视觉基准测试中表现良好，但在特定领域的OOD任务中面临挑战。</li>
<li>LEAML框架旨在解决专业领域中的监督不足问题。</li>
<li>LEAML结合了标签化的问答样本和大量的未标签图像。</li>
<li>利用标题蒸馏正则化的问答生成器为未标记数据生成伪问答对。</li>
<li>LEAML通过选择性更新与问答最相关的神经元，提高了效率。</li>
<li>实验表明LEAML在特定领域的任务中表现优于标准微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5b79807c519c3ca2f4e455ab278133aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941668&auth_key=1759941668-0-0-1207f9822a1f4695578284bbc847d91f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ec21f0d346bf730c0c493ed7e80c663a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941676&auth_key=1759941676-0-0-209aca2b2f1de4c911cdebf607f20b79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92691d448828cf4134dae0e0e3d7e079~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941682&auth_key=1759941682-0-0-f57040c675e03b6a85ac4a7c4ece1737&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b430d4f56cba0150dd6ff1702a33d197~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941689&auth_key=1759941689-0-0-3d27f706b054db13f4cd441677ac2792&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eab65026192958a73f94102384e30ee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941696&auth_key=1759941696-0-0-fbecd606741f74026ba0e3d2cb118cc0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment"><a href="#Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment" class="headerlink" title="Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment"></a>Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment</h2><p><strong>Authors:Hongxiang Zhang, Yuan Tian, Tianyi Zhang</strong></p>
<p>To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the model’s attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between &#96;&#96;non-reasoning’’ models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining. </p>
<blockquote>
<p>为了解决大型语言模型（LLM）的复杂推理任务，基于提示的方法为微调强化学习提供了一种轻量级的替代方案。然而，随着推理链的延伸，关键的中间步骤和原始提示会被上下文所淹没，得不到足够的关注，从而导致错误。在本文中，我们提出了Self-Anchor，这是一种利用推理内在结构来引导LLM注意的新型管道。Self-Anchor将推理轨迹分解为结构化计划，并自动将模型注意力与最相关的推理步骤对齐，使模型在整个生成过程中保持关注。我们的实验表明，Self-Anchor在六个基准测试上优于SOTA提示方法。值得注意的是，Self-Anchor显著减少了“非推理”模型和专用推理模型之间的性能差距，具有使大多数LLM无需重新训练即可处理复杂推理任务的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03223v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于LLM的大型语言模型的推理任务中，随着推理链条的延伸，关键中间步骤和原始提示容易被忽视导致错误。本文提出一种名为Self-Anchor的新型管道，利用推理的内在结构来引导模型注意力。Self-Anchor将推理轨迹分解为结构化计划，并自动将模型注意力与最相关的推理步骤对齐，使模型在生成过程中保持关注重点。实验表明，Self-Anchor在六个基准测试中优于最新提示方法。特别的是，Self-Anchor显著减少了“非推理”模型和专用推理模型之间的性能差距，使大多数LLM能够处理复杂的推理任务而无需重新训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在处理复杂推理任务时面临挑战，随着推理链条的延长，关键步骤和原始提示容易在语境中被忽略。</li>
<li>Self-Anchor是一种新型管道，旨在解决这一问题，通过利用推理的内在结构来引导LLM的注意力。</li>
<li>Self-Anchor将推理轨迹分解为结构化计划，使模型能够更清晰地识别并关注关键推理步骤。</li>
<li>Self-Anchor能够自动对齐模型注意力与最相关的推理步骤，提升模型的推理能力。</li>
<li>实验表明，Self-Anchor在多个基准测试中表现优异，优于当前的最新提示方法。</li>
<li>Self-Anchor显著缩小了“非推理”模型和专用推理模型之间的性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fcf24557400a1d7e4547b0de3d609453~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941707&auth_key=1759941707-0-0-e5a6a0c63b870b687b2e85a09a1e363d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-09f2edf6b829d103931b0d7c19fa2d2b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-67073c8199e5318b258734fb99e06fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941721&auth_key=1759941721-0-0-15fda53a147bbe2a00d48418bcbc8219&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-ae2ebde980e7552d108d63e3a41d704f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward"><a href="#Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward" class="headerlink" title="Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward"></a>Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward</h2><p><strong>Authors:Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17%$ average accuracy on five math benchmarks, an improvement of $2.66%$ over prior methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>. </p>
<blockquote>
<p>强化学习与可验证奖励（RLVR）已推动大型语言模型在复杂推理方面的应用，但其可扩展性常常受到训练瓶颈的阻碍，当策略熵崩溃时，性能会达到平台期，这表明探索的丧失。之前的方法通常通过保持高策略熵来解决这个问题，但控制有意义探索的精确机制仍未得到充分探索。我们的分析表明，对熵的无选择性关注可能放大无关标记并破坏训练稳定性。本文研究了RLVR中的探索动态，并发现了一个关键问题：有价值的低概率探索标记逐渐被淘汰，我们称之为“推理火花”。我们发现，虽然在预训练模型中这些火花非常丰富，但在RLVR期间由于过度惩罚而系统地被消灭，导致探索退化。为了解决这一问题，我们引入了低概率正则化（Lp-Reg）。其核心机制是对策略进行正则化以符合启发式代理分布。该代理通过过滤掉假定为噪声的标记并重新归一化剩余候选者的分布来构建。结果是噪声较少的代理，其中推理火花的概率得到放大，然后作为软正则化目标，通过KL散度保护这些有价值的标记不被淘汰。实验表明，Lp-Reg能够在大约1000步内实现稳定的在线策略训练，这是基线熵控制方法崩溃的领域。这种持续的探索导致了最先进的性能，在五个数学基准测试上达到了60.17%的平均准确率，比先前的方法提高了2.66%。代码可在<a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03222v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>强化学习与可验证奖励（RLVR）在推动大型语言模型进行复杂推理方面取得了进展，但其可扩展性经常受到训练瓶颈的阻碍，表现为性能平台化，标志着探索的丧失。之前的方法通常通过保持高政策熵来解决这一问题，但精确控制有意义探索的机制仍然知之甚少。本文分析了RLVR中的探索动态，并发现了一个关键问题：宝贵的低概率探索性令牌（“推理火花”）的逐渐消除。尽管这些火花在预训练模型中非常丰富，但在RLVR中却被系统消除，原因是过度惩罚导致探索退化。为了解决这一问题，我们引入了低概率正则化（Lp-Reg）。其核心机制是对策略进行正则化，使其朝向启发式代理分布。该代理通过过滤掉假定为噪声令牌并重新归一化剩余候选者的分布来构建。结果是一个噪声较少的代理，其中推理火花的可能性被放大，然后作为软正则化目标，通过KL散度保护这些有价值的令牌免受消除。实验表明，Lp-Reg能够在大约1000步内实现稳定的在策略训练，这是基线熵控制方法崩溃的时期。这种持续的探索导致了最先进的性能，在五个数学基准测试上平均准确度达到60.17%，比先前的方法提高了2.66%。代码可在<a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RLVR在复杂推理中表现优异，但遇到训练瓶颈，表现为性能平台化。</li>
<li>现有方法主要关注保持政策熵，但缺乏对有意义探索的精确控制。</li>
<li>本文发现低概率探索性令牌（推理火花）的逐渐消除是关键问题。</li>
<li>推理火花在预训练模型中丰富，但在RLVR训练过程中被过度惩罚而消除。</li>
<li>引入Low-probability Regularization（Lp-Reg）来解决这一问题，通过构建启发式代理分布来保护推理火花。</li>
<li>Lp-Reg能够实现稳定的在策略训练，并在多个基准测试中达到最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03222">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2289f2071c6a3d853493a987799c9b0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2298cc1ee3eb4d89a92e5f76a36b629f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b85257ac4774e6fae767b379593e2f1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941749&auth_key=1759941749-0-0-6f447ced39e16f384505c2230da9c5be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-33a6a6faec6f1c58b0db4bdbfadfe523.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models"><a href="#Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models" class="headerlink" title="Cache-to-Cache: Direct Semantic Communication Between Large Language   Models"></a>Cache-to-Cache: Direct Semantic Communication Between Large Language   Models</h2><p><strong>Authors:Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang</strong></p>
<p>Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model’s KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-nics/C2C">https://github.com/thu-nics/C2C</a>. </p>
<blockquote>
<p>多LLM系统利用不同大型语言模型的互补优势，实现了单一模型无法达到的性能和效率提升。在现有设计中，LLM通过文本进行交流，强制将内部表示转换为输出令牌序列。这一过程既丢失了丰富的语义信息，又产生了逐个令牌的生成延迟。针对这些局限性，我们提出一个问题：LLM能否以文本以外的方式进行交流？Oracle实验表明，丰富KV-Cache语义可以提高响应质量，而不增加缓存大小，支持KV-Cache作为跨模型通信的有效媒介。因此，我们提出了Cache-to-Cache（C2C），这是一种LLM之间直接语义交流的新范式。C2C使用神经网络将源模型的KV缓存投影并与目标模型的缓存融合，以实现直接的语义转换。一种可学习的门控机制选择受益于缓存通信的目标层。与文本交流相比，C2C利用了两个模型的深层专业语义，避免了显式的中间文本生成。实验表明，C2C比单个模型平均提高了8.5-10.5%的准确率。它进一步优于文本交流范式，大约提高了3.0-5.0%的性能，同时实现了平均2.0倍的延迟速度提升。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/thu-nics/C2C%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-nics/C2C找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多LLM系统通过利用不同大型语言模型的互补优势，实现了性能和效率的提升。现有设计中，LLM通过文本进行交流，导致语义信息丢失并产生逐词生成延迟。为解决这些问题，研究提出Cache-to-Cache（C2C）通信范式，实现LLM之间的直接语义交流。C2C利用神经网络将源模型的KV缓存投影并与目标模型融合，实现语义的直接传递。实验表明，C2C相比文本通信，利用了两者的深度语义，避免了中间文本的显式生成，提高了平均准确率和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多LLM系统可以整合不同LLM的互补优势，提升性能和效率。</li>
<li>现有LLM通过文本交流存在语义信息丢失和延迟问题。</li>
<li>Cache-to-Cache（C2C）通信范式可实现LLM间的直接语义交流。</li>
<li>C2C利用神经网络投影和融合模型KV缓存，实现语义直接传递。</li>
<li>C2C相比文本通信，能提高平均准确率并避免中间文本的显式生成。</li>
<li>C2C通信范式在效率和性能上优于单个模型和文本通信。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03215">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e7f3be347073a8f261b3bc310179ebe1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941784&auth_key=1759941784-0-0-862f7588392a64b517203862efa381cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-58c3ac152e12e4e408ce0a5e38942fd8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-26097a4e47f8fa3739bb98ef26159c20~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941798&auth_key=1759941798-0-0-89991e36faf4224053dc862ba068cffb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f523a19302faff638e5a462cfa61ec0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941804&auth_key=1759941804-0-0-77008ea5c847eb1e0af2ee4eefd752d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Correlates-of-Language-Models-Are-Specific-to-Human-Language"><a href="#Neural-Correlates-of-Language-Models-Are-Specific-to-Human-Language" class="headerlink" title="Neural Correlates of Language Models Are Specific to Human Language"></a>Neural Correlates of Language Models Are Specific to Human Language</h2><p><strong>Authors:Iñigo Parra</strong></p>
<p>Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models. </p>
<blockquote>
<p>先前的工作已经显示出大型语言模型的隐藏状态与语言任务中的功能性磁共振成像（fMRI）大脑响应之间存在关联。这些关联被视为这些模型与大脑状态在代表性方面的相似性证据。本研究旨在测试这些先前结果是否能够对多种可能的担忧保持稳健。具体来说，本研究表明：（i）在降维之后仍然可以发现之前的结果，因此这并非归因于维数诅咒；（ii）在使用新的相似性度量方法时确认了之前的结果；（iii）大脑表示与模型之间的关联特定于经过人类语言训练的模型；（iv）结果依赖于模型中是否存在位置编码。这些结果证实了先前研究的结果并使其得到加强，为关于最新大型语言模型的生物合理性和可解释性的争论做出了贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03156v1">PDF</a> To be presented at NeurIPS 2025 Workshops</p>
<p><strong>Summary</strong>：大型语言模型的隐藏状态与语言任务中的fMRI脑反应之间存在关联，被视为这些模型与脑状态的表征相似性的证据。本研究测试了这些先前结果是否稳健可靠。具体而言，本研究发现：i）降维后仍然出现之前的结果，因此不是由于维数诅咒所致；ii）使用新的相似性度量标准后确认了之前的结果；iii）脑表征与模型之间的关联特定于经过人类语言训练的模型；iv）结果依赖于模型中是否存在位置编码。这些结果证实了先前研究的结果，为关于最新大型语言模型的生物合理性和可解释性的争论做出了贡献。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型的隐藏状态与fMRI脑反应之间的关联被视为模型与脑状态表征相似性的证据。</li>
<li>研究结果稳健，不因降维而消失，排除维数诅咒的影响。</li>
<li>新的相似性度量标准验证了先前的研究结果。</li>
<li>脑与模型之间的关联特定于经过人类语言训练的模型。</li>
<li>结果依赖于模型中是否存在位置编码。</li>
<li>研究结果证实了先前的研究，为大型语言模型的生物合理性提供了支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1a603f71807503af91aa7674de14a8be~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941833&auth_key=1759941833-0-0-87e9aaf13f9dc60011f892c511775bc2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-4d7cb37d2e8e1aaa888aa5f3c357235d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-87d97ecf792ec91fe83d9ba3ee6b85d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941847&auth_key=1759941847-0-0-e30280f98f53e8955bc36d45b5db3a02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-5bba301ba7211195aff62ea8bc5f5087.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EditLens-Quantifying-the-Extent-of-AI-Editing-in-Text"><a href="#EditLens-Quantifying-the-Extent-of-AI-Editing-in-Text" class="headerlink" title="EditLens: Quantifying the Extent of AI Editing in Text"></a>EditLens: Quantifying the Extent of AI Editing in Text</h2><p><strong>Authors:Katherine Thai, Bradley Emi, Elyas Masrour, Mohit Iyyer</strong></p>
<p>A significant proportion of queries to large language models ask them to edit user-provided text, rather than generate new text from scratch. While previous work focuses on detecting fully AI-generated text, we demonstrate that AI-edited text is distinguishable from human-written and AI-generated text. First, we propose using lightweight similarity metrics to quantify the magnitude of AI editing present in a text given the original human-written text and validate these metrics with human annotators. Using these similarity metrics as intermediate supervision, we then train EditLens, a regression model that predicts the amount of AI editing present within a text. Our model achieves state-of-the-art performance on both binary (F1&#x3D;94.7%) and ternary (F1&#x3D;90.4%) classification tasks in distinguishing human, AI, and mixed writing. Not only do we show that AI-edited text can be detected, but also that the degree of change made by AI to human writing can be detected, which has implications for authorship attribution, education, and policy. Finally, as a case study, we use our model to analyze the effects of AI-edits applied by Grammarly, a popular writing assistance tool. To encourage further research, we commit to publicly releasing our models and dataset. </p>
<blockquote>
<p>很大一部分对大型语言模型的查询要求它们编辑用户提供的文本，而不是从头开始生成新文本。虽然之前的工作主要集中在检测完全由人工智能生成的文本，但我们证明人工智能编辑的文本与人类书写和人工智能生成的文本是可以区分的。首先，我们提出使用轻量级相似度度量标准来衡量给定原始人类书写文本中AI编辑的程度，并使用人类注释者对这些度量标准进行验证。使用这些相似度度量作为中间监督，我们然后训练了EditLens，一个回归模型，可以预测文本中AI编辑的程度。我们的模型在二元（F1&#x3D;94.7%）和三元（F1&#x3D;90.4%）分类任务中实现了区分人类、人工智能和混合写作方面的最新技术性能。我们不仅证明了可以检测到AI编辑的文本，而且证明了可以检测到AI对人类写作所做的改变程度，这对作者归属、教育和政策都有影响。最后，作为一个案例研究，我们使用我们的模型分析了流行写作助手Grammarly应用的AI编辑的影响。为了鼓励进一步研究，我们承诺公开发布我们的模型和数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03154v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型处理用户查询时，有很大一部分是编辑用户提供的文本，而非从头生成新文本。我们展示了AI编辑的文本与人工撰写和AI生成的文本之间的区别。我们提出使用轻量级相似度指标来衡量给定原始人类文本中AI编辑的程度，并通过人类注释者进行验证。基于这些相似度指标作为中间监督，我们训练了EditLens模型，该模型能够预测文本中的AI编辑程度。模型在区分人类、AI和混合写作的二元和三元分类任务上均达到最新技术水平。我们不仅证明了可以检测到AI编辑的文本，而且可以检测到AI对人类写作所做的改变程度，这对作者归属、教育和政策都有影响。最后，我们以Grammarly这一流行的写作辅助工具为例，使用我们的模型分析了AI编辑的效果。我们致力于公开我们的模型和数据集，以鼓励进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在处理查询时，更多是对用户提供的文本进行编辑，而非生成新文本。</li>
<li>AI编辑的文本与人工撰写和AI生成的文本之间存在显著区别。</li>
<li>提出了使用轻量级相似度指标来衡量原始人类文本中AI编辑的程度。</li>
<li>EditLens模型可以预测文本中的AI编辑程度，并在分类任务上表现优异。</li>
<li>AI编辑的文本及其对人类写作的改变程度可以被检测，对多个领域有影响。</li>
<li>以Grammarly为例，展示了AI编辑的实际应用及其效果分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03154">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2b6400a38530529656695b1507c0d445~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941861&auth_key=1759941861-0-0-9a700a417403e34f769267b3ce2d849c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d03dbc64358d200f7fda4601a954148~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941868&auth_key=1759941868-0-0-5361adcf625799da8607ad5e68a22291&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-ddbb64fdd0cd5d198debb6621ce121b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2d3c458346a96eef87e6365153f8ff6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Improving-Cooperation-in-Collaborative-Embodied-AI"><a href="#Improving-Cooperation-in-Collaborative-Embodied-AI" class="headerlink" title="Improving Cooperation in Collaborative Embodied AI"></a>Improving Cooperation in Collaborative Embodied AI</h2><p><strong>Authors:Hima Jacob Leven Suprabha, Laxmi Nag Laxminarayan Nagesh, Ajith Nair, Alvin Reuben Amal Selvaster, Ayan Khan, Raghuram Damarla, Sanju Hannah Samuel, Sreenithi Saravana Perumal, Titouan Puech, Venkataramireddy Marella, Vishal Sonar, Alessandro Suglia, Oliver Lemon</strong></p>
<p>The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations. </p>
<blockquote>
<p>将大型语言模型（LLM）集成到多智能体系统中为协作推理和与人工智能代理人的合作开启了新的可能性。本文探讨了不同的提示方法，并评估了它们在提高智能体协作行为和决策制定方面的有效性。我们增强了CoELA（一个为利用LLM进行多智能体通信、推理和任务协调而设计的协作实体代理框架），该框架适用于共享虚拟空间。通过系统实验，我们研究了不同的LLM和提示工程策略，以找出能最大化协作性能的优化组合。此外，我们通过集成语音功能，实现了无缝的基于语音的协作交互。我们的研究结果突出了提示优化在提高协作智能体性能方面的有效性；例如，我们最好的组合与Gemma3相比，将系统运行效率提高了22%。此外，语音集成还为迭代系统开发和演示提供了更具吸引力的用户界面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03153v1">PDF</a> In proceedings of UKCI 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）多智能体系统的集成开启了协作推理和智能体协作的新可能性。本文探讨了不同的提示方法，并评估了它们在提高智能体协作行为和决策制定方面的有效性。我们增强了CoELA框架，该框架旨在利用LLM构建协作实体智能体，用于共享虚拟空间中的多智能体通信、推理和任务协调。通过系统实验，我们研究了不同的LLM和提示工程策略，以找出优化组合，最大化协作性能。此外，我们通过集成语音功能，实现无缝的基于语音的协作交互。研究结果表明提示优化在提高协作智能体性能方面的有效性；例如，我们的最佳组合将使用Gemma3的系统运行效率提高了22%，与原始CoELA系统相比。此外，语音集成为用户提供了一个更具吸引力的界面，用于迭代系统开发和演示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs和多智能体系统的集成提高了协作推理和AI智能体协作的可能性。</li>
<li>研究探讨了不同提示方法在提高智能体协作行为和决策制定中的有效性。</li>
<li>CoELA框架被增强，利用LLM进行多智能体通信、推理和任务协调。</li>
<li>系统实验研究了不同的LLM和提示工程策略，找到了优化组合以最大化协作性能。</li>
<li>语音功能的集成实现了无缝的基于语音的协作交互，增强了用户体验。</li>
<li>提示优化在提高协作智能体性能方面效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-15b419a24ee8d6d6607fbd9e63c60738~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941889&auth_key=1759941889-0-0-01802e33ec50c5b5a805668bbfa71d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f09cf3fed438a65740cf5fee4658431~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941896&auth_key=1759941896-0-0-502d260b14a4d38b3ec24ccd758703af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning"><a href="#MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning" class="headerlink" title="MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning"></a>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning</h2><p><strong>Authors:Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang</strong></p>
<p>Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method. </p>
<blockquote>
<p>视觉导航策略被广泛认为是一个具有前景的方向，因为它通过使用以自我为中心的视觉观察来模仿人类的导航方式。然而，视觉观察的光学信息难以像激光雷达点云或深度地图那样进行显式建模，这需要智能模型和大规模数据。为此，我们提出利用视觉-语言-动作（VLA）模型的智能，以师徒方式从合成专家数据中学习各种导航能力。具体来说，我们实现了VLA模型MM-Nav，它是一种基于预训练的大型语言模型和视觉基础模型的多视角VLA（具有360度观察）。对于大规模导航数据，我们从三位强化学习（RL）专家收集专家数据，这些专家使用特权深度信息在三个针对不同导航能力量身定制的挑战环境中接受训练：到达、挤压和避免。我们迭代地使用来自RL专家的在线收集数据来训练我们的VLA模型，其中训练比例是根据个人能力的表现进行动态平衡的。通过合成环境的广泛实验，我们证明我们的模型具有很强的泛化能力。此外，我们发现我们的学生VLA模型的表现优于RL老师，这证明了整合多种能力的协同作用。大量现实世界实验进一步证实了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03142v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://pku-epic.github.io/MM-Nav-Web/">https://pku-epic.github.io/MM-Nav-Web/</a></p>
<p><strong>Summary</strong></p>
<p>视觉导航策略被广泛认为是一个有前途的方向，因为它通过使用以自我为中心的观察来进行导航，从而模仿人类。然而，视觉观察的光学信息难以像激光雷达点云或深度图那样进行显式建模，这需要智能模型和大规模数据。为此，我们提出利用视觉语言行动（VLA）模型的智能，以师徒方式从合成专家数据中学习各种导航能力。我们实现了基于预训练的大型语言模型和视觉基础模型的MM-Nav多视图VLA模型（具有360度观察）。为了获取大规模导航数据，我们从三个强化学习专家收集的专家数据中收集数据，这些专家使用特权深度信息在三个针对不同导航能力的量身定制的环境中接受训练：到达、挤压和避免。我们迭代地训练我们的VLA模型，使用从强化学习专家在线收集的数据，其中训练比率是根据对个别能力的表现进行动态平衡的。在合成环境中的大量实验表明，我们的模型具有很强的泛化能力。此外，我们发现我们的学生VLA模型的表现优于强化学习老师，证明了整合多种能力的协同效应。在现实世界中的大量实验进一步证实了我们的方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉导航策略模仿人类使用视觉观察进行导航，被认为是前景广阔的方向。</li>
<li>光学信息的建模是视觉导航中的一大挑战，需要智能模型和大规模数据。</li>
<li>提出利用VLA模型的智能，从合成专家数据中学习多种导航能力。</li>
<li>实现名为MM-Nav的多视图VLA模型，具备360度观察能力。</li>
<li>通过收集来自强化学习专家的数据来训练模型，这些专家在具有挑战性的环境中接受训练，针对不同导航能力如到达、挤压和避免。</li>
<li>模型展现出强大的泛化能力，并且学生VLA模型表现优于强化学习老师，证明了整合多种能力的协同效应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-96cd04118467124c38a9a5b7da6d6ff7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b9a3fee8905f84ecc88ece04e69effd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941912&auth_key=1759941912-0-0-9c814764b4acd2a5dbf5c08a47dfaa0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-196afbdc75a5fb50763f34351da8d5ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941919&auth_key=1759941919-0-0-691f315e25efc7482ce4b11309a3821a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a911de169e5d64cc5b31e0556734134c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941925&auth_key=1759941925-0-0-32484fb6590d96b7194ed3ecade2f031&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bc59b9dfb0f76dc6d772656d7646701~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941931&auth_key=1759941931-0-0-e7f3814df976e568215cdeae13971462&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER"><a href="#Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER" class="headerlink" title="Semantic Similarity in Radiology Reports via LLMs and NER"></a>Semantic Similarity in Radiology Reports via LLMs and NER</h2><p><strong>Authors:Beth Pearson, Ahmed Adnan, Zahraa Abdallah</strong></p>
<p>Radiology report evaluation is a crucial part of radiologists’ training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +&#x2F;- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports%7D%7Bgithub.com/otmive/llama/_reports%7D">https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}</a> </p>
<blockquote>
<p>放射学报告评估是放射科医生培训的重要组成部分，对于确保诊断的准确性起着关键作用。作为标准报告工作流程的一部分，初级放射科医生通常会编写初步报告，然后由高级放射科医生进行审查和编辑以产生最终报告。识别初步报告和最终报告之间的语义差异对初级医生至关重要，既作为培训工具，也有助于发现临床知识方面的不足。尽管人工智能在放射学领域是一个快速发展的领域，但由于需要专业领域的知识，大型语言模型（LLM）的应用仍然具有挑战性。在本文中，我们探讨了LLM在放射学报告中提供可解释和准确比较的能力。我们首先比较了多个LLM在比较放射学报告方面的性能。然后我们评估了一种基于命名实体识别（NER）的更传统的方法。然而，这两种方法在提供语义相似性方面的准确反馈方面都表现出局限性。为了解决这一问题，我们提出了Llama-EntScore，一种使用Llama 3.1和NER组合的语义相似性评分方法，具有可调权重，可以强调或淡化特定类型的差异。我们的方法生成定量相似性分数以跟踪进度，并对分数进行解释，旨在为审查和改进报告提供有价值的指导。我们发现，与放射科医生提供的真实分数相比，我们的方法达到了67%的精确匹配准确率和93%的准确率（在+&#x2F;- 1范围内）——超越了独立使用的LLM和NER。代码可在<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports">github.com&#x2F;otmive&#x2F;llama_reports</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03102v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>本文探讨了大型语言模型（LLMs）在放射学报告比较中的应用。研究通过结合Llama 3.1模型和命名实体识别（NER）技术，提出了一种新的语义相似性评分方法Llama-EntScore。该方法用于评估初步报告和最终报告之间的语义差异，旨在帮助放射科医生培训和知识提升。实验表明，该方法相较于单一的LLMs和NER模型表现出更高的准确性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>放射学报告评估是放射科医生培训的重要组成部分，对于确保诊断准确性至关重要。</li>
<li>初步报告通常由初级放射科医生编制，然后由高级放射科医生审查并编辑成最终报告。</li>
<li>语义差异识别对于初级医生既是训练工具，也有助于发现临床知识的差距。</li>
<li>大型语言模型（LLMs）在放射学领域的应用具有挑战性，需要专业的领域知识。</li>
<li>本文探讨了LLMs在放射学报告比较中的应用，并提出了Llama-EntScore方法。</li>
<li>Llama-EntScore方法结合了Llama 3.1模型和NER技术，旨在评估初步报告和最终报告之间的语义相似性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4f98c50394379a9217910538618acceb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941938&auth_key=1759941938-0-0-94ba641fe129ed4dac572160e096ae94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods – whether sparse or dense – often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance – a setting we term agentic retrieval. The benchmark consists of 26K expert-annotated examples on S&amp;P-500 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. </p>
<blockquote>
<p>准确的情报检索（IR）在金融领域至关重要，投资者需要从大量文档中找出相关信息。传统的IR方法，无论是稀疏的还是密集的，往往在检索准确性方面存在不足，因为它不仅需要捕捉语义相似性，还需要对文档结构和特定领域的知识进行精细的推理。大型语言模型（LLM）的最新进展为具有多步骤推理的检索提供了新的机会，该模型通过迭代推理来排列段落，确定哪些信息对给定查询最为相关。然而，金融领域缺乏评估这种能力的基准测试。为了弥补这一空白，我们引入了FinAgentBench，这是第一个用于评估金融领域多步骤推理检索的大型基准测试——我们称之为代理检索。该基准测试包含关于标准普尔500上市公司的2.6万个专家注释示例，并评估LLM代理是否能（1）在候选者中识别出最相关的文档类型，以及（2）在选定文档中指出关键段落。我们的评估框架明确地将这两个推理步骤分开，以解决上下文局限性。这种设计能够提供定量依据，以了解金融领域中以检索为中心的LLM行为。我们评估了一系列最先进的模型，并进一步展示了有针对性的微调如何显着提高代理检索性能。我们的基准测试为研究金融领域的复杂特定任务中以检索为中心的LLM行为提供了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v4">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>在金融领域中，准确的信息检索（IR）至关重要。传统IR方法往往无法准确检索信息，因为除了捕捉语义相似性外，还需要对文档结构和特定领域的知识进行精细推理。最近的大型语言模型（LLM）的进步为具有多步骤推理的检索提供了新的机会。然而，金融领域缺乏评估这种能力的基准测试。为解决此空白，我们引入了FinAgentBench，这是第一个用于评估金融领域中具有多步骤推理的检索的大型基准测试。该基准测试包含针对标普500上市公司的2.6万份专家注释样本，并评估LLM代理是否能确定候选文档中最相关的文档类型以及在选定文档中定位关键段落。我们的评估框架明确地将这两个推理步骤分开，以解决上下文局限性问题。这种设计为理解金融领域中以检索为中心的LLM行为提供了定量依据。我们评估了一系列最新模型，并展示了有针对性的微调如何显着提高代理检索性能。我们的基准测试为研究金融领域的复杂特定任务中针对检索为中心的LLM行为奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融领域的信息检索至关重要，传统方法存在局限性。</li>
<li>大型语言模型（LLM）的进步为金融信息检索中的多步骤推理提供了新机会。</li>
<li>缺乏评估金融领域中多步骤推理检索能力的基准测试。</li>
<li>引入FinAgentBench作为首个针对金融领域的多步骤推理检索的基准测试。</li>
<li>该基准测试包含针对标普500上市公司的专家注释样本，评估LLM代理识别最相关文档类型和定位关键段落的能力。</li>
<li>评估框架明确区分识别文档类型与定位关键段落两个步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e67b298766d64441c67ca8ac9c11fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941947&auth_key=1759941947-0-0-7f0a26acb0bd4c96908c6d72dbbcf034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33fb8858b61d505b4d3e5c36a8853ae3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941954&auth_key=1759941954-0-0-b18ec72922f4b5a948a53acd5c6e945c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6a46765e47c81459dca0d6bcc678956~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941960&auth_key=1759941960-0-0-f489d09f87f09810168eb4fa46680503&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c9ab698bd07fc75334d1a2448d3b6e4e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b53eeebce52e81ffd410e01298428d7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941974&auth_key=1759941974-0-0-04279aff47a989727a657a1f196e7284&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quantum-RAG-and-PunGPT2-Advancing-Low-Resource-Language-Generation-and-Retrieval-for-the-Punjabi-Language"><a href="#Quantum-RAG-and-PunGPT2-Advancing-Low-Resource-Language-Generation-and-Retrieval-for-the-Punjabi-Language" class="headerlink" title="Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and   Retrieval for the Punjabi Language"></a>Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and   Retrieval for the Punjabi Language</h2><p><strong>Authors:Jaskaranjeet Singh, Rakesh Thakur</strong></p>
<p>Despite rapid advances in large language models (LLMs), low-resource languages remain excluded from NLP, limiting digital access for millions. We present PunGPT2, the first fully open-source Punjabi generative model suite, trained on a 35GB corpus covering literature, religious texts, news, social discourse, etc. PunGPT2 captures Punjabi’s syntactic and morphological richness through a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce Pun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS retriever over a curated Punjabi knowledge base, and Pun-Instruct, an instruction-tuned variant using QLoRA for robust zero-shot summarization, translation, and question answering. Our key innovation, Quantum-RAG, fuses sparse, dense, and quantum kernel embeddings for efficient, context-aware retrieval with low memory overhead, marking the first practical quantum-inspired retrieval in a low-resource LLM. Our models outperform multilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench, and a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and +3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts, hyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval benchmark, and all model weights, establishing new state-of-the-art results for Punjabi language generation and retrieval. </p>
<blockquote>
<p>尽管大型语言模型（LLM）迅速进步，但低资源语言仍然被排除在NLP之外，限制了数百万人的数字访问。我们推出了PunGPT2，这是第一个完全开源的旁遮普语生成模型套件，它基于一个包含文献、宗教文本、新闻、社会话语等内容的35GB语料库进行训练。PunGPT2通过针对古尔姆基和沙姆基脚本优化的分词器捕捉旁遮普语的句法和形态丰富性。我们引入了Pun-RAG，这是一个结合了PunGPT2与针对精选旁遮普知识库的FAISS检索器的检索增强框架，以及Pun-Instruct，这是一个使用QLoRA进行稳健的零射击摘要、翻译和问答的指令调整变体。我们的关键创新是Quantum-RAG，它融合了稀疏、密集和量子核嵌入，用于实现具有低内存开销的高效、上下文感知检索，这是低资源LLM中首个实用的量子启发式检索。我们的模型在FLORES-200、IndicGenBench和新的PunjabiEval套件上的表现超过了多语言基线（mBERT、mT5、MuRIL、BLOOM）。Quantum-RAG在PunjabiEval上的Recall@10比FAISS高出7.4，BLEU比mT5高出3.5。我们公开发布所有的训练脚本、超参数、评估管道、35GB的旁遮普语料库、PunjabiEval基准测试以及所有的模型权重，为旁遮普语的语言生成和检索树立了新的最新成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01918v2">PDF</a> </p>
<p><strong>Summary</strong>：<br>虽然大型语言模型（LLM）发展迅速，但低资源语言在NLP中仍被边缘化，限制了数百万人的数字访问。为此，我们推出了PunGPT2，这是第一个完全开源的旁遮普语生成模型套件，经过35GB语料库的训练，涵盖了文献、宗教文本、新闻、社会话语等。我们还介绍了Pun-RAG和Quantum-RAG，前者是一个与PunGPT2集成的检索增强框架，后者融合了稀疏、密集和量子核嵌入，实现了高效、语境感知的检索和内存低消耗。我们的模型在FLORES-200、IndicGenBench和新的PunjabiEval套件上的表现超过了多语言基线，建立了旁遮普语语言和检索的新里程碑。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>低资源语言在NLP中仍然面临挑战，限制了数字访问。</li>
<li>PunGPT2是首个完全开源的旁遮普语生成模型套件，经过35GB语料库训练。</li>
<li>Pun-RAG是一个与PunGPT2集成的检索增强框架，提供了语境感知的检索功能。</li>
<li>Quantum-RAG融合了稀疏、密集和量子核嵌入，实现了高效检索和内存低消耗。</li>
<li>旁遮普语模型在多个基准测试上的表现超过了多语言基线。</li>
<li>Quantum-RAG在PunjabiEval上的召回率和BLEU得分均有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f018e6328df42c6130fcd8242c6d012f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941983&auth_key=1759941983-0-0-3b8cc9561388752c3ec7a6ac18d4649c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9724358f86dce1254addeeb19f82b725~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941991&auth_key=1759941991-0-0-b42a9f2b300098edfabeba6df5339558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-2c56bf6a1d323bcb92ba14c3a508744f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cd9da55f2b222af35e277286d560989~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942004&auth_key=1759942004-0-0-d70c655ad9b9b8b7dd6690b2ce8fdcf1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51c1f24660326f89a485d23749d36650~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942010&auth_key=1759942010-0-0-d45446b208b17fd5d304d2943440d607&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b4eaddd005adcb03b78ae16d323b8b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453a015e82a72d1959ffb9326c251652.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c4e716a969b71347636f2c484f449c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942030&auth_key=1759942030-0-0-1a3cf5e44f0f804924c87b2e00f89c95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-5d416cbdfa00f795afb54fd908e2c47b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Tuning-LLM-based-Code-Optimization-via-Meta-Prompting-An-Industrial-Perspective"><a href="#Tuning-LLM-based-Code-Optimization-via-Meta-Prompting-An-Industrial-Perspective" class="headerlink" title="Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial   Perspective"></a>Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial   Perspective</h2><p><strong>Authors:Jingzhi Gong, Rafail Giavrimis, Paul Brookes, Vardan Voskanyan, Fan Wu, Mari Ashiga, Matthew Truscott, Mike Basios, Leslie Kanthan, Jie Xu, Zheng Wang</strong></p>
<p>There is a growing interest in leveraging multiple large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge: prompts optimized for one LLM often fail with others, requiring expensive model-specific prompt engineering. This cross-model prompt engineering bottleneck severely limits the practical deployment of multi-LLM systems in production environments. We introduce Meta-Prompted Code Optimization (MPCO), a framework that automatically generates high-quality, task-specific prompts across diverse LLMs while maintaining industrial efficiency requirements. MPCO leverages metaprompting to dynamically synthesize context-aware optimization prompts by integrating project metadata, task requirements, and LLM-specific contexts. It is an essential part of the ARTEMIS code optimization platform for automated validation and scaling. Our comprehensive evaluation on five real-world codebases with 366 hours of runtime benchmarking demonstrates MPCO’s effectiveness: it achieves overall performance improvements up to 19.06% with the best statistical rank across all systems compared to baseline methods. Analysis shows that 96% of the top-performing optimizations stem from meaningful edits. Through systematic ablation studies and meta-prompter sensitivity analysis, we identify that comprehensive context integration is essential for effective meta-prompting and that major LLMs can serve effectively as meta-prompters, providing actionable insights for industrial practitioners. </p>
<blockquote>
<p>对于利用多个大型语言模型（LLM）进行自动化代码优化的兴趣正在增长。然而，部署多个LLM的工业平台面临一个关键问题：针对一个LLM优化的提示往往在其他LLM上失败，这需要进行昂贵的模型特定提示工程。这种跨模型提示工程瓶颈严重限制了多LLM系统在生产环境中的实际部署。我们引入了Meta提示代码优化（MPCO）框架，该框架能够在保持工业效率要求的同时，在多种LLM中自动生成高质量的任务特定提示。MPCO利用元提示技术，通过整合项目元数据、任务要求和LLM特定上下文，动态合成上下文感知优化提示。它是ARTEMIS代码优化平台的自动化验证和扩展的重要组成部分。我们在五个真实世界的代码库上进行了全面的评估，通过366小时的运行时基准测试证明了MPCO的有效性：与基准方法相比，它在总体上实现了高达19.06%的性能提升，在所有系统中表现最佳。分析表明，96%的最佳性能优化源于有意义的编辑。通过系统的剔除研究和元提示敏感性分析，我们发现全面的上下文整合对于有效的元提示至关重要，并且主要的LLM可以有效地作为元提示器，为工业从业者提供可操作的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01443v2">PDF</a> Accepted by ASE’25 Industry Showcase</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了如何利用多个大型语言模型（LLM）进行自动化代码优化的问题。针对部署多个LLM的工业平台面临的关键挑战，即针对一个LLM优化的提示在其它LLM上经常失效，需要昂贵的模型特定提示工程。为此，引入了Meta-Prompted代码优化（MPCO）框架，该框架能够自动生成高质量的任务特定提示，适用于不同的LLM，同时满足工业效率要求。MPCO利用元提示技术，通过整合项目元数据、任务要求和LLM特定上下文，动态合成上下文感知的优化提示。它是ARTEMIS代码优化平台的重要组成部分，用于自动化验证和扩展。在五个真实世界代码库上的综合评估显示，MPCO总体上提高了高达19.06%的性能，并且在所有系统中表现最佳。分析表明，96%的最佳优化来自于有意义的编辑。通过系统的剖析研究和元提示敏感性分析，发现全面的上下文整合对于有效的元提示至关重要，主要LLM可以有效地作为元提示器，为工业从业者提供可操作的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多个大型语言模型（LLM）在自动化代码优化中的应用日益受到关注。</li>
<li>部署多个LLM的工业平台面临的一个关键挑战是模型间的提示工程瓶颈。</li>
<li>Meta-Prompted代码优化（MPCO）框架能够自动生成跨不同LLM的高质量任务特定提示。</li>
<li>MPCO结合项目元数据、任务要求和LLM特定上下文来动态合成优化提示。</li>
<li>MPCO是ARTEMIS代码优化平台的重要组成部分，能够提高自动化验证和扩展的效率。</li>
<li>在真实世界代码库上的评估显示，MPCO相比基准方法提高了显著的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01443">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a924c29e2fab299369e1d977e2b6d8b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-62974e4e2a709eb28a657328c30f4f67~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942051&auth_key=1759942051-0-0-4cff8eb69de6ceaea9ae5e3d0562d809&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fc6f75c2bcb61bd77315e240b5d2af1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942058&auth_key=1759942058-0-0-340450541458e3088da9fc3a1a5a14b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b8f4e27f6c317c7589547e89b860d7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942065&auth_key=1759942065-0-0-78788138b2bb503f4d1b2fd214b415f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7000e98e3adb75fe069adcd1326b166b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-69cc2ee04777369d27b14f16789f9b77~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942079&auth_key=1759942079-0-0-0fc1a6730cec1b50cbb3db773377e6b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Highly-Efficient-and-Effective-LLMs-with-Multi-Boolean-Architectures"><a href="#Highly-Efficient-and-Effective-LLMs-with-Multi-Boolean-Architectures" class="headerlink" title="Highly Efficient and Effective LLMs with Multi-Boolean Architectures"></a>Highly Efficient and Effective LLMs with Multi-Boolean Architectures</h2><p><strong>Authors:Ba-Hien Tran, Van Minh Nguyen</strong></p>
<p>Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques. </p>
<blockquote>
<p>权重二值化作为一种策略，已在大规模语言模型（LLM）的复杂度降低方面展现出巨大潜力。现有方法分为训练后二值化，这种方法虽然简单但会导致性能严重损失，以及依赖于全精度潜在权重的训练感知方法，这增加了复杂性和限制了效率。我们提出了一种新的框架，该框架使用多核布尔参数表示LLM，并首次实现了布尔域内LLM的直接微调，无需使用潜在权重。这提高了表示能力，并在微调期间和推理期间大大降低了复杂度。在多种LLM上的广泛实验表明，我们的方法优于最新的超低比特量化和二值化技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22811v2">PDF</a> Preprint. Under Review</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于多核布尔参数表示的大型语言模型（LLM）新型框架，可直接在布尔域进行LLM微调，无需使用潜在权重，提高了表示能力，并大幅降低了微调与推理时的复杂度。此框架突破现有训练后二元化与训练感知方法的局限，实验证明其性能优于最近的超低精度量化与二元化技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的复杂性可通过权重二值化策略降低。</li>
<li>现有方法包括训练后的二值化，虽然简单但性能损失严重；以及依赖全精度潜在权重的训练感知方法，增加了复杂性和限制了效率。</li>
<li>提出的框架首次实现了在布尔域内直接微调LLM，消除了对潜在权重的需要。</li>
<li>该框架提高了表示能力，并大幅降低了微调与推理时的复杂度。</li>
<li>提出的框架优于现有的超低精度量化与二元化技术。</li>
<li>框架采用多核布尔参数表示，增强了模型的灵活性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22811">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a6a56bd3c4562642fac4b8ffa33cff1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942087&auth_key=1759942087-0-0-863322f516dc8657dabd4a59268f70f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93979abd28bf2c76d33e7d19117e5f24~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942095&auth_key=1759942095-0-0-ca3b769f47d7700fe5d11583f92f22ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning"><a href="#To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning" class="headerlink" title="To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning"></a>To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning</h2><p><strong>Authors:Tian Qin, David Alvarez-Melis, Samy Jelassi, Eran Malach</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test time-compute: parallel sampling with best-of-N selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling-especially under a fixed compute budget-remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models intro suboptimal strategies, and (2) explicit CoT supervision can discourage implicit (non verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步显著提高了其推理能力，特别是通过涉及搜索和回溯的技术。回溯通过启用序列化、线性化的探索（通过长链条思维生成），自然地扩展了测试时间的计算。然而，这并不是扩展测试时间计算的唯一策略：并行采样与最佳N选择相结合，可以同时生成多种解决方案。尽管序贯搜索的采用日益增加，但其相对于并行采样的优势，特别是在固定计算预算下，仍鲜为人知。在本文中，我们在两个具有挑战性的推理任务CountDown和数独上系统地比较了这两种方法。令人惊讶的是，我们发现序贯搜索在CountDown上的表现不如并行采样，但在数独上的表现优于并行采样，这表明回溯并非普遍有益。我们确定了可能导致回溯降低性能的两大因素：（1）在固定搜索轨迹上进行训练会使模型陷入次优策略；（2）明确的思维链监督可能会抑制隐含（未言语化）的推理。我们将分析扩展到强化学习（RL），发现具有回溯能力的模型从RL微调中受益匪浅，而没有回溯能力的模型则收效甚微。总体而言，这些发现挑战了回溯普遍提高LLM推理能力的假设，反而揭示了任务结构、训练数据、模型规模和学习范式之间的复杂交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07052v2">PDF</a> COLM 2025 Camera Ready</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的近期进展显著提高了其推理能力，主要通过搜索和回溯技术实现。本文通过对比顺序搜索和并行采样两种方法，发现其在不同推理任务上的表现存在差异。顺序搜索在解决数独问题时表现较好，但在倒计时任务上表现不佳。研究发现，训练固定搜索轨迹和显式思维监督可能导致回溯性能下降。强化学习（RL）微调对具有回溯能力的模型有很大益处，而对无回溯模型则效果有限。这些发现挑战了回溯普遍提升LLM推理能力的假设，揭示了任务结构、训练数据、模型规模和学习范式之间的复杂交互。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）的推理能力通过搜索和回溯技术得到显著提高。</li>
<li>顺序搜索和并行采样在推理任务上的表现存在差异，并无普遍优势。</li>
<li>训练固定搜索轨迹和显式思维监督可能导致回溯性能下降。</li>
<li>强化学习（RL）微调对具有回溯能力的模型有很大益处。</li>
<li>任务结构、训练数据、模型规模和学习范式之间交互影响LLM的推理能力。</li>
<li>顺序搜索在解决数独问题时表现较好，但在倒计时任务上表现不佳。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9962eccf4c32fd86f63381d6b104c2b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942102&auth_key=1759942102-0-0-900e564e8c22dad712c744db2dcba4d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-52252374602a9cfa3a9231203049348a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c72d4ddc3f55d24aaf9cd2de04fc74a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942116&auth_key=1759942116-0-0-179815929d1654ba7a0ef1fe8647d7af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-642b9dc5483a0af8acd3c1b68ef3de7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942123&auth_key=1759942123-0-0-2c4f0cbbd3dc84fc3e3dc9d61cb2a2f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLAMAFUZZ-Large-Language-Model-Enhanced-Greybox-Fuzzing"><a href="#LLAMAFUZZ-Large-Language-Model-Enhanced-Greybox-Fuzzing" class="headerlink" title="LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing"></a>LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing</h2><p><strong>Authors:Hongxiang Zhang, Yuyang Rong, Yifeng He, Hao Chen</strong></p>
<p>Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in programs. However, randomized mutation strategies have limited the fuzzer’s performance on structured data. Specialized fuzzers can handle complex structured data, but require additional efforts in grammar and suffer from low throughput.   In this paper, we explore the potential of utilizing the Large Language Model to enhance greybox fuzzing for structured data. We utilize the pre-trained knowledge of LLM about data conversion and format to generate new valid inputs. We further fine-tuned it with paired mutation seeds to learn structured format and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ, integrates the power of LLM to understand and mutate structured data to fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor by 41 bugs on average. We also identified 47 unique bugs across all trials. Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in real-world program sets on average. We also demonstrate a case study to explain how LLMs enhance the fuzzing process in terms of code coverage. </p>
<blockquote>
<p>灰盒模糊测试在揭示程序和漏洞中的错误方面取得了成功。然而，随机突变策略在结构化数据上限制了模糊测试的性能。专业模糊测试器可以处理复杂结构化数据，但需要额外的语法努力，并且吞吐量较低。在本文中，我们探讨了利用大型语言模型提升结构化数据的灰盒模糊测试潜力。我们利用LLM关于数据转换和格式的预训练知识来生成新的有效输入。我们进一步使用配对突变种子对其进行微调，以有效学习结构化格式和突变策略。我们基于LLM的模糊测试器LLAMAFUZZ结合了LLM理解和突变结构化数据的能力来进行模糊测试。我们在基于错误的基准测试Magma和各种实际程序上进行了实验。LLAMAFUZZ平均比我们的顶级竞争对手多发现了41个错误。我们还识别出了所有试验中的47个独特错误。此外，LLAMAFUZZ在触发错误和达到错误方面都表现出了一致的性能。与AFL++相比，LLAMAFUZZ在真实程序集上的平均分支覆盖率提高了27.19%。我们还通过一个案例研究来解释LLM如何提升模糊测试过程中的代码覆盖率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07714v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了利用大型语言模型（LLM）增强灰盒模糊测试在结构化数据上的潜力。通过利用LLM的预训练知识进行数据转换和格式生成新的有效输入，并通过配对突变种子进行微调，学习结构化格式和突变策略。基于LLM的模糊测试器LLAMAFUZZ结合了LLM理解和突变结构化数据的能力，对标准基准Magma和各种真实程序进行了实验。结果显示，LLAMAFUZZ平均比顶级竞争对手多发现41个bug，在所有试验中发现了47个独特bug。此外，LLAMAFUZZ在bug触发和bug检测方面都表现出了一致的性能。相较于AFL++，LLAMAFUZZ在真实程序集上的平均分支覆盖率提高了27.19%。同时，通过案例研究展示了LLM如何增强模糊测试过程中的代码覆盖率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM被用于增强灰盒模糊测试在结构化数据上的性能。</li>
<li>LLAMAFUZZ利用LLM的预训练知识生成新的有效输入并学习结构化数据和突变策略。</li>
<li>在标准基准Magma和真实程序上的实验显示，LLAMAFUZZ比顶级竞争对手更优秀。</li>
<li>LLAMAFUZZ发现了更多的独特bug，并且在bug触发和检测方面表现稳定。</li>
<li>与AFL++相比，LLAMAFUZZ在真实程序集上的平均分支覆盖率有所提高。</li>
<li>LLAMAFUZZ通过结合LLM的能力，提高了模糊测试过程中的代码覆盖率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0c85c8349b40c3d1225d2e4b0024a4fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942131&auth_key=1759942131-0-0-9ea666f14d4419947badac9e8684b725&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-70a7275acd60da698ab0933828002de3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f166c761e3bcc2556bc2de57371303c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942144&auth_key=1759942144-0-0-ce8528ed40f3cee09811fb7f91a92cc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-bcad7992bba636c9d953285c25daa8b8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-82dcc7c751f2318e4e9174ab20a262ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942157&auth_key=1759942157-0-0-21004d7d11bde1f221b809cb6b1aa421&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a5b989ff71c1ec9aaf4c8f59d987f5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942164&auth_key=1759942164-0-0-7db4ac93cf27dc5dd3afbadecddbc064&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-How-CodeLLMs-Mis-Predict-Types-with-Activation-Steering"><a href="#Understanding-How-CodeLLMs-Mis-Predict-Types-with-Activation-Steering" class="headerlink" title="Understanding How CodeLLMs (Mis)Predict Types with Activation Steering"></a>Understanding How CodeLLMs (Mis)Predict Types with Activation Steering</h2><p><strong>Authors:Francesca Lucchetti, Arjun Guha</strong></p>
<p>Large Language Models (LLMs) are widely used by software engineers for programming tasks. However, research shows that LLMs often lack a deep understanding of program semantics. Even minor changes to syntax, such as renaming variables, can significantly degrade performance across various tasks. In this work, we examine the task of type prediction: given a partially typed program, can a model predict a missing type annotations such that the resulting program is more typed? We construct a dataset of adversarial examples where models initially predict the correct types, but begin to fail after semantically irrelevant edits. This is problematic, as models should ideally generalize across different syntactic forms of semantically equivalent code. This lack of robustness suggests that models may have a shallow understanding of code semantics. Despite this, we provide evidence that LLMs do, in fact, learn robust mechanisms for type prediction-though these mechanisms often fail to activate in adversarial scenarios. By using activation steering, a method that manipulates a model’s internal activations to guide it toward using latent knowledge, we restore accurate predictions on adversarial inputs. We show that steering successfully activates a type prediction mechanism that is shared by both Python and TypeScript, and is more effective than prompting with in-context examples. Across five different models, our comprehensive evaluation demonstrates that LLMs can learn generalizable representations of code semantics that transfer across programming languages. </p>
<blockquote>
<p>大型语言模型（LLM）被软件工程师广泛应用于编程任务。然而，研究表明，LLM通常缺乏对程序语义的深入理解。即使是对语法的微小更改，如变量重命名，也可能在各种任务中显著降低性能。在这项工作中，我们研究了类型预测任务：给定一个部分类型的程序，模型能否预测缺失的类型注释，以使程序更具类型化？我们构建了一个对抗性示例数据集，模型最初预测的类型是正确的，但在语义无关编辑后开始失败。这是一个问题，因为模型理想情况下应该能够概括语义等效代码的不同语法形式。这种稳健性的缺乏表明，模型可能对代码语义的理解很浅薄。尽管如此，我们提供了证据表明LLM实际上学习了用于类型预测的稳定机制——尽管这些机制在对抗场景中往往无法激活。通过使用激活转向（一种通过操纵模型的内部激活来引导其使用潜在知识的方法），我们恢复了对抗性输入上的准确预测。我们证明了转向成功地激活了Python和TypeScript共有的类型预测机制，并且比使用上下文示例进行提示更为有效。我们在五个不同模型上的全面评估表明，LLM可以学习可概括的代码语义表示，这些表示可以跨编程语言进行迁移。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01903v3">PDF</a> 40 pages, 67 figures. To be published at BlackBoxNLP 2025</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在编程任务中被广泛应用，但对程序语义的理解往往不够深入。研究指出，即使语法上的微小变化，如变量重命名，也可能对模型在各种任务上的性能产生重大影响。本研究旨在解决类型预测问题：给定部分类型的程序，模型能否预测缺失的类型注释以使程序更加完整？研究发现，尽管模型最初能正确预测类型，但在语义无关编辑后容易出错。虽然模型缺乏稳健性表明它们对代码语义的理解可能很浅薄，但证据表明LLM实际上学习了用于类型预测的稳健机制。通过使用激活控制法激活模型内部的潜在知识导向机制，我们能够成功恢复对敌对输入的准确预测。研究表明，控制法激活的类型预测机制在Python和TypeScript中都有共享，且比上下文示例提示更为有效。全面评估显示LLM能够学习跨编程语言的代码语义的一般表示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs广泛应用于编程任务，但对程序语义的理解有限。</li>
<li>微小语法变化可能导致LLMs在类型预测等任务上的性能显著下降。</li>
<li>模型在语义无关编辑后容易在类型预测任务上出错，表明其缺乏稳健性。</li>
<li>LLMs实际上学习了用于类型预测的稳健机制，但这些机制在特定情境下可能无法激活。</li>
<li>通过激活控制法，可以成功恢复LLM对敌对输入的准确类型预测。</li>
<li>控制法激活的类型预测机制在Python和TypeScript中具有共享性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.01903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-22dce702642a369e8c4a9950fc464191~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942172&auth_key=1759942172-0-0-a31d198c8b6182c09588bc097da93ffd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-738561eca57f5484b794ff46fd5b5860.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-846d65bf7db4eefd8ab33b55476952fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942186&auth_key=1759942186-0-0-54c309626e7598bd1d7c1f244c749d5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-edc21980ae5bba5cc5ea5f442de9e6d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942193&auth_key=1759942193-0-0-e26f9efc69c20f29e0f00a0249f71bcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-10-07  UniShield An Adaptive Multi-Agent Framework for Unified Forgery Image   Detection and Localization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ae2ebde980e7552d108d63e3a41d704f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941177&auth_key=1759941177-0-0-24735fb633f4732c73b466185ff34858&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-07  Self-Anchor Large Language Model Reasoning via Step-by-step Attention   Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
