<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  LEAML Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7d03dbc64358d200f7fda4601a954148.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-07-æ›´æ–°"><a href="#2025-10-07-æ›´æ–°" class="headerlink" title="2025-10-07 æ›´æ–°"></a>2025-10-07 æ›´æ–°</h1><h2 id="LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models"><a href="#LEAML-Label-Efficient-Adaptation-to-Out-of-Distribution-Visual-Tasks-for-Multimodal-Large-Language-Models" class="headerlink" title="LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models"></a>LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks   for Multimodal Large Language Models</h2><p><strong>Authors:Ci-Siang Lin, Min-Hung Chen, Yu-Yang Sheng, Yu-Chiang Frank Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„éå¸¸è§„ä»»åŠ¡ï¼ˆå¦‚åŒ»å­¦æˆåƒï¼‰ä¸­å´è¡¨ç°æŒ£æ‰ï¼Œå› ä¸ºåŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„æ ‡æ³¨æ•°æ®æ—¢æœ‰é™åˆæ˜‚è´µã€‚æˆ‘ä»¬å¼•å…¥äº†LEAMLï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡ç­¾é«˜æ•ˆé€‚åº”æ¡†æ¶ï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†ç¨€ç¼ºçš„æ ‡æ³¨VQAæ ·æœ¬å’Œå¤§é‡çš„æœªæ ‡æ³¨å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é€šè¿‡æ ‡é¢˜è’¸é¦è¿›è¡Œæ­£åˆ™åŒ–çš„QAç”Ÿæˆå™¨ï¼Œä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆä¸é¢†åŸŸç›¸å…³çš„ä¼ªé—®ç­”å¯¹ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åªé€‰æ‹©æ€§åœ°æ›´æ–°ä¸é—®ç­”æœ€ç›¸å…³çš„ç¥ç»å…ƒï¼Œä½¿QAç”Ÿæˆå™¨åœ¨è’¸é¦è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è·å–ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ã€‚åœ¨èƒƒé•œå’Œä½“è‚²é—®ç­”æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œåœ¨æœ€å°ç›‘ç£ä¸‹ï¼ŒLEAMLå§‹ç»ˆä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•ï¼Œçªæ˜¾äº†æˆ‘ä»¬æå‡ºçš„LEAMLæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é€šç”¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸå¦‚åŒ»ç–—æˆåƒç­‰é¢å¯¹åˆ†å¸ƒå¤–çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LEAMLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç¨€ç¼ºçš„æ ‡ç­¾åŒ–é—®ç­”æ ·æœ¬å’Œå¤§é‡çš„æœªæ ‡ç­¾å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨æ ‡é¢˜è’¸é¦æ­£åˆ™åŒ–çš„é—®ç­”ç”Ÿæˆå™¨ä¸ºæœªæ ‡è®°æ•°æ®ç”Ÿæˆé¢†åŸŸç›¸å…³çš„ä¼ªé—®ç­”å¯¹ã€‚æˆ‘ä»¬ä»…é€‰æ‹©æ€§æ›´æ–°ä¸é—®ç­”æœ€ç›¸å…³çš„ç¥ç»å…ƒï¼Œè¿™å…è®¸é—®ç­”ç”Ÿæˆå™¨åœ¨è’¸é¦è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°è·å–ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ã€‚åœ¨èƒƒè‚ é“å†…çª¥é•œå’Œä½“è‚²é—®ç­”ä¸­çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›‘ç£ä¸è¶³çš„æƒ…å†µä¸‹ï¼ŒLEAMLæŒç»­ä¼˜äºæ ‡å‡†å¾®è°ƒï¼Œå‡¸æ˜¾äº†LEAMLæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨é€šç”¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„OODä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LEAMLæ¡†æ¶æ—¨åœ¨è§£å†³ä¸“ä¸šé¢†åŸŸä¸­çš„ç›‘ç£ä¸è¶³é—®é¢˜ã€‚</li>
<li>LEAMLç»“åˆäº†æ ‡ç­¾åŒ–çš„é—®ç­”æ ·æœ¬å’Œå¤§é‡çš„æœªæ ‡ç­¾å›¾åƒã€‚</li>
<li>åˆ©ç”¨æ ‡é¢˜è’¸é¦æ­£åˆ™åŒ–çš„é—®ç­”ç”Ÿæˆå™¨ä¸ºæœªæ ‡è®°æ•°æ®ç”Ÿæˆä¼ªé—®ç­”å¯¹ã€‚</li>
<li>LEAMLé€šè¿‡é€‰æ‹©æ€§æ›´æ–°ä¸é—®ç­”æœ€ç›¸å…³çš„ç¥ç»å…ƒï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜LEAMLåœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5b79807c519c3ca2f4e455ab278133aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941668&auth_key=1759941668-0-0-1207f9822a1f4695578284bbc847d91f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ec21f0d346bf730c0c493ed7e80c663a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941676&auth_key=1759941676-0-0-209aca2b2f1de4c911cdebf607f20b79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92691d448828cf4134dae0e0e3d7e079~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941682&auth_key=1759941682-0-0-f57040c675e03b6a85ac4a7c4ece1737&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b430d4f56cba0150dd6ff1702a33d197~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941689&auth_key=1759941689-0-0-3d27f706b054db13f4cd441677ac2792&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eab65026192958a73f94102384e30ee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941696&auth_key=1759941696-0-0-fbecd606741f74026ba0e3d2cb118cc0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment"><a href="#Self-Anchor-Large-Language-Model-Reasoning-via-Step-by-step-Attention-Alignment" class="headerlink" title="Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment"></a>Self-Anchor: Large Language Model Reasoning via Step-by-step Attention   Alignment</h2><p><strong>Authors:Hongxiang Zhang, Yuan Tian, Tianyi Zhang</strong></p>
<p>To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the modelâ€™s attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between &#96;&#96;non-reasoningâ€™â€™ models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining. </p>
<blockquote>
<p>ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•ä¸ºå¾®è°ƒå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§è½»é‡çº§çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œéšç€æ¨ç†é“¾çš„å»¶ä¼¸ï¼Œå…³é”®çš„ä¸­é—´æ­¥éª¤å’ŒåŸå§‹æç¤ºä¼šè¢«ä¸Šä¸‹æ–‡æ‰€æ·¹æ²¡ï¼Œå¾—ä¸åˆ°è¶³å¤Ÿçš„å…³æ³¨ï¼Œä»è€Œå¯¼è‡´é”™è¯¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Self-Anchorï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ¨ç†å†…åœ¨ç»“æ„æ¥å¼•å¯¼LLMæ³¨æ„çš„æ–°å‹ç®¡é“ã€‚Self-Anchorå°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œå¹¶è‡ªåŠ¨å°†æ¨¡å‹æ³¨æ„åŠ›ä¸æœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤å¯¹é½ï¼Œä½¿æ¨¡å‹åœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå…³æ³¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSelf-Anchoråœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºSOTAæç¤ºæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSelf-Anchoræ˜¾è‘—å‡å°‘äº†â€œéæ¨ç†â€æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå…·æœ‰ä½¿å¤§å¤šæ•°LLMæ— éœ€é‡æ–°è®­ç»ƒå³å¯å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03223v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºLLMçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œéšç€æ¨ç†é“¾æ¡çš„å»¶ä¼¸ï¼Œå…³é”®ä¸­é—´æ­¥éª¤å’ŒåŸå§‹æç¤ºå®¹æ˜“è¢«å¿½è§†å¯¼è‡´é”™è¯¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSelf-Anchorçš„æ–°å‹ç®¡é“ï¼Œåˆ©ç”¨æ¨ç†çš„å†…åœ¨ç»“æ„æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ã€‚Self-Anchorå°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œå¹¶è‡ªåŠ¨å°†æ¨¡å‹æ³¨æ„åŠ›ä¸æœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤å¯¹é½ï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå…³æ³¨é‡ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼ŒSelf-Anchoråœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°æç¤ºæ–¹æ³•ã€‚ç‰¹åˆ«çš„æ˜¯ï¼ŒSelf-Anchoræ˜¾è‘—å‡å°‘äº†â€œéæ¨ç†â€æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä½¿å¤§å¤šæ•°LLMèƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéšç€æ¨ç†é“¾æ¡çš„å»¶é•¿ï¼Œå…³é”®æ­¥éª¤å’ŒåŸå§‹æç¤ºå®¹æ˜“åœ¨è¯­å¢ƒä¸­è¢«å¿½ç•¥ã€‚</li>
<li>Self-Anchoræ˜¯ä¸€ç§æ–°å‹ç®¡é“ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨æ¨ç†çš„å†…åœ¨ç»“æ„æ¥å¼•å¯¼LLMçš„æ³¨æ„åŠ›ã€‚</li>
<li>Self-Anchorå°†æ¨ç†è½¨è¿¹åˆ†è§£ä¸ºç»“æ„åŒ–è®¡åˆ’ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æ¸…æ™°åœ°è¯†åˆ«å¹¶å…³æ³¨å…³é”®æ¨ç†æ­¥éª¤ã€‚</li>
<li>Self-Anchorèƒ½å¤Ÿè‡ªåŠ¨å¯¹é½æ¨¡å‹æ³¨æ„åŠ›ä¸æœ€ç›¸å…³çš„æ¨ç†æ­¥éª¤ï¼Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSelf-Anchoråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå½“å‰çš„æœ€æ–°æç¤ºæ–¹æ³•ã€‚</li>
<li>Self-Anchoræ˜¾è‘—ç¼©å°äº†â€œéæ¨ç†â€æ¨¡å‹å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fcf24557400a1d7e4547b0de3d609453~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941707&auth_key=1759941707-0-0-e5a6a0c63b870b687b2e85a09a1e363d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-09f2edf6b829d103931b0d7c19fa2d2b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-67073c8199e5318b258734fb99e06fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941721&auth_key=1759941721-0-0-15fda53a147bbe2a00d48418bcbc8219&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-ae2ebde980e7552d108d63e3a41d704f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward"><a href="#Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward" class="headerlink" title="Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward"></a>Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward</h2><p><strong>Authors:Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17%$ average accuracy on five math benchmarks, an improvement of $2.66%$ over prior methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ–¹é¢çš„åº”ç”¨ï¼Œä½†å…¶å¯æ‰©å±•æ€§å¸¸å¸¸å—åˆ°è®­ç»ƒç“¶é¢ˆçš„é˜»ç¢ï¼Œå½“ç­–ç•¥ç†µå´©æºƒæ—¶ï¼Œæ€§èƒ½ä¼šè¾¾åˆ°å¹³å°æœŸï¼Œè¿™è¡¨æ˜æ¢ç´¢çš„ä¸§å¤±ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡ä¿æŒé«˜ç­–ç•¥ç†µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ§åˆ¶æœ‰æ„ä¹‰æ¢ç´¢çš„ç²¾ç¡®æœºåˆ¶ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯¹ç†µçš„æ— é€‰æ‹©æ€§å…³æ³¨å¯èƒ½æ”¾å¤§æ— å…³æ ‡è®°å¹¶ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†RLVRä¸­çš„æ¢ç´¢åŠ¨æ€ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ¢ç´¢æ ‡è®°é€æ¸è¢«æ·˜æ±°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ¨ç†ç«èŠ±â€ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­è¿™äº›ç«èŠ±éå¸¸ä¸°å¯Œï¼Œä½†åœ¨RLVRæœŸé—´ç”±äºè¿‡åº¦æƒ©ç½šè€Œç³»ç»Ÿåœ°è¢«æ¶ˆç­ï¼Œå¯¼è‡´æ¢ç´¢é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯å¯¹ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ä»¥ç¬¦åˆå¯å‘å¼ä»£ç†åˆ†å¸ƒã€‚è¯¥ä»£ç†é€šè¿‡è¿‡æ»¤æ‰å‡å®šä¸ºå™ªå£°çš„æ ‡è®°å¹¶é‡æ–°å½’ä¸€åŒ–å‰©ä½™å€™é€‰è€…çš„åˆ†å¸ƒæ¥æ„å»ºã€‚ç»“æœæ˜¯å™ªå£°è¾ƒå°‘çš„ä»£ç†ï¼Œå…¶ä¸­æ¨ç†ç«èŠ±çš„æ¦‚ç‡å¾—åˆ°æ”¾å¤§ï¼Œç„¶åä½œä¸ºè½¯æ­£åˆ™åŒ–ç›®æ ‡ï¼Œé€šè¿‡KLæ•£åº¦ä¿æŠ¤è¿™äº›æœ‰ä»·å€¼çš„æ ‡è®°ä¸è¢«æ·˜æ±°ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿåœ¨å¤§çº¦1000æ­¥å†…å®ç°ç¨³å®šçš„åœ¨çº¿ç­–ç•¥è®­ç»ƒï¼Œè¿™æ˜¯åŸºçº¿ç†µæ§åˆ¶æ–¹æ³•å´©æºƒçš„é¢†åŸŸã€‚è¿™ç§æŒç»­çš„æ¢ç´¢å¯¼è‡´äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†60.17%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œæ¯”å…ˆå‰çš„æ–¹æ³•æé«˜äº†2.66%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03222v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å…¶å¯æ‰©å±•æ€§ç»å¸¸å—åˆ°è®­ç»ƒç“¶é¢ˆçš„é˜»ç¢ï¼Œè¡¨ç°ä¸ºæ€§èƒ½å¹³å°åŒ–ï¼Œæ ‡å¿—ç€æ¢ç´¢çš„ä¸§å¤±ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡ä¿æŒé«˜æ”¿ç­–ç†µæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ç²¾ç¡®æ§åˆ¶æœ‰æ„ä¹‰æ¢ç´¢çš„æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬æ–‡åˆ†æäº†RLVRä¸­çš„æ¢ç´¢åŠ¨æ€ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå®è´µçš„ä½æ¦‚ç‡æ¢ç´¢æ€§ä»¤ç‰Œï¼ˆâ€œæ¨ç†ç«èŠ±â€ï¼‰çš„é€æ¸æ¶ˆé™¤ã€‚å°½ç®¡è¿™äº›ç«èŠ±åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­éå¸¸ä¸°å¯Œï¼Œä½†åœ¨RLVRä¸­å´è¢«ç³»ç»Ÿæ¶ˆé™¤ï¼ŒåŸå› æ˜¯è¿‡åº¦æƒ©ç½šå¯¼è‡´æ¢ç´¢é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯å¯¹ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä½¿å…¶æœå‘å¯å‘å¼ä»£ç†åˆ†å¸ƒã€‚è¯¥ä»£ç†é€šè¿‡è¿‡æ»¤æ‰å‡å®šä¸ºå™ªå£°ä»¤ç‰Œå¹¶é‡æ–°å½’ä¸€åŒ–å‰©ä½™å€™é€‰è€…çš„åˆ†å¸ƒæ¥æ„å»ºã€‚ç»“æœæ˜¯ä¸€ä¸ªå™ªå£°è¾ƒå°‘çš„ä»£ç†ï¼Œå…¶ä¸­æ¨ç†ç«èŠ±çš„å¯èƒ½æ€§è¢«æ”¾å¤§ï¼Œç„¶åä½œä¸ºè½¯æ­£åˆ™åŒ–ç›®æ ‡ï¼Œé€šè¿‡KLæ•£åº¦ä¿æŠ¤è¿™äº›æœ‰ä»·å€¼çš„ä»¤ç‰Œå…å—æ¶ˆé™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿåœ¨å¤§çº¦1000æ­¥å†…å®ç°ç¨³å®šçš„åœ¨ç­–ç•¥è®­ç»ƒï¼Œè¿™æ˜¯åŸºçº¿ç†µæ§åˆ¶æ–¹æ³•å´©æºƒçš„æ—¶æœŸã€‚è¿™ç§æŒç»­çš„æ¢ç´¢å¯¼è‡´äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡å‡†ç¡®åº¦è¾¾åˆ°60.17%ï¼Œæ¯”å…ˆå‰çš„æ–¹æ³•æé«˜äº†2.66%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRåœ¨å¤æ‚æ¨ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†é‡åˆ°è®­ç»ƒç“¶é¢ˆï¼Œè¡¨ç°ä¸ºæ€§èƒ½å¹³å°åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä¿æŒæ”¿ç­–ç†µï¼Œä½†ç¼ºä¹å¯¹æœ‰æ„ä¹‰æ¢ç´¢çš„ç²¾ç¡®æ§åˆ¶ã€‚</li>
<li>æœ¬æ–‡å‘ç°ä½æ¦‚ç‡æ¢ç´¢æ€§ä»¤ç‰Œï¼ˆæ¨ç†ç«èŠ±ï¼‰çš„é€æ¸æ¶ˆé™¤æ˜¯å…³é”®é—®é¢˜ã€‚</li>
<li>æ¨ç†ç«èŠ±åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­ä¸°å¯Œï¼Œä½†åœ¨RLVRè®­ç»ƒè¿‡ç¨‹ä¸­è¢«è¿‡åº¦æƒ©ç½šè€Œæ¶ˆé™¤ã€‚</li>
<li>å¼•å…¥Low-probability Regularizationï¼ˆLp-Regï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ„å»ºå¯å‘å¼ä»£ç†åˆ†å¸ƒæ¥ä¿æŠ¤æ¨ç†ç«èŠ±ã€‚</li>
<li>Lp-Regèƒ½å¤Ÿå®ç°ç¨³å®šçš„åœ¨ç­–ç•¥è®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2289f2071c6a3d853493a987799c9b0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2298cc1ee3eb4d89a92e5f76a36b629f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b85257ac4774e6fae767b379593e2f1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941749&auth_key=1759941749-0-0-6f447ced39e16f384505c2230da9c5be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-33a6a6faec6f1c58b0db4bdbfadfe523.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models"><a href="#Cache-to-Cache-Direct-Semantic-Communication-Between-Large-Language-Models" class="headerlink" title="Cache-to-Cache: Direct Semantic Communication Between Large Language   Models"></a>Cache-to-Cache: Direct Semantic Communication Between Large Language   Models</h2><p><strong>Authors:Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang</strong></p>
<p>Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source modelâ€™s KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-nics/C2C">https://github.com/thu-nics/C2C</a>. </p>
<blockquote>
<p>å¤šLLMç³»ç»Ÿåˆ©ç”¨ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå®ç°äº†å•ä¸€æ¨¡å‹æ— æ³•è¾¾åˆ°çš„æ€§èƒ½å’Œæ•ˆç‡æå‡ã€‚åœ¨ç°æœ‰è®¾è®¡ä¸­ï¼ŒLLMé€šè¿‡æ–‡æœ¬è¿›è¡Œäº¤æµï¼Œå¼ºåˆ¶å°†å†…éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºè¾“å‡ºä»¤ç‰Œåºåˆ—ã€‚è¿™ä¸€è¿‡ç¨‹æ—¢ä¸¢å¤±äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œåˆäº§ç”Ÿäº†é€ä¸ªä»¤ç‰Œçš„ç”Ÿæˆå»¶è¿Ÿã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼šLLMèƒ½å¦ä»¥æ–‡æœ¬ä»¥å¤–çš„æ–¹å¼è¿›è¡Œäº¤æµï¼ŸOracleå®éªŒè¡¨æ˜ï¼Œä¸°å¯ŒKV-Cacheè¯­ä¹‰å¯ä»¥æé«˜å“åº”è´¨é‡ï¼Œè€Œä¸å¢åŠ ç¼“å­˜å¤§å°ï¼Œæ”¯æŒKV-Cacheä½œä¸ºè·¨æ¨¡å‹é€šä¿¡çš„æœ‰æ•ˆåª’ä»‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Cache-to-Cacheï¼ˆC2Cï¼‰ï¼Œè¿™æ˜¯ä¸€ç§LLMä¹‹é—´ç›´æ¥è¯­ä¹‰äº¤æµçš„æ–°èŒƒå¼ã€‚C2Cä½¿ç”¨ç¥ç»ç½‘ç»œå°†æºæ¨¡å‹çš„KVç¼“å­˜æŠ•å½±å¹¶ä¸ç›®æ ‡æ¨¡å‹çš„ç¼“å­˜èåˆï¼Œä»¥å®ç°ç›´æ¥çš„è¯­ä¹‰è½¬æ¢ã€‚ä¸€ç§å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶é€‰æ‹©å—ç›Šäºç¼“å­˜é€šä¿¡çš„ç›®æ ‡å±‚ã€‚ä¸æ–‡æœ¬äº¤æµç›¸æ¯”ï¼ŒC2Cåˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„æ·±å±‚ä¸“ä¸šè¯­ä¹‰ï¼Œé¿å…äº†æ˜¾å¼çš„ä¸­é—´æ–‡æœ¬ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒC2Cæ¯”å•ä¸ªæ¨¡å‹å¹³å‡æé«˜äº†8.5-10.5%çš„å‡†ç¡®ç‡ã€‚å®ƒè¿›ä¸€æ­¥ä¼˜äºæ–‡æœ¬äº¤æµèŒƒå¼ï¼Œå¤§çº¦æé«˜äº†3.0-5.0%çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†å¹³å‡2.0å€çš„å»¶è¿Ÿé€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-nics/C2C%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-nics/C2Cæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šLLMç³»ç»Ÿé€šè¿‡åˆ©ç”¨ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡çš„æå‡ã€‚ç°æœ‰è®¾è®¡ä¸­ï¼ŒLLMé€šè¿‡æ–‡æœ¬è¿›è¡Œäº¤æµï¼Œå¯¼è‡´è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±å¹¶äº§ç”Ÿé€è¯ç”Ÿæˆå»¶è¿Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºCache-to-Cacheï¼ˆC2Cï¼‰é€šä¿¡èŒƒå¼ï¼Œå®ç°LLMä¹‹é—´çš„ç›´æ¥è¯­ä¹‰äº¤æµã€‚C2Cåˆ©ç”¨ç¥ç»ç½‘ç»œå°†æºæ¨¡å‹çš„KVç¼“å­˜æŠ•å½±å¹¶ä¸ç›®æ ‡æ¨¡å‹èåˆï¼Œå®ç°è¯­ä¹‰çš„ç›´æ¥ä¼ é€’ã€‚å®éªŒè¡¨æ˜ï¼ŒC2Cç›¸æ¯”æ–‡æœ¬é€šä¿¡ï¼Œåˆ©ç”¨äº†ä¸¤è€…çš„æ·±åº¦è¯­ä¹‰ï¼Œé¿å…äº†ä¸­é—´æ–‡æœ¬çš„æ˜¾å¼ç”Ÿæˆï¼Œæé«˜äº†å¹³å‡å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šLLMç³»ç»Ÿå¯ä»¥æ•´åˆä¸åŒLLMçš„äº’è¡¥ä¼˜åŠ¿ï¼Œæå‡æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>ç°æœ‰LLMé€šè¿‡æ–‡æœ¬äº¤æµå­˜åœ¨è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±å’Œå»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>Cache-to-Cacheï¼ˆC2Cï¼‰é€šä¿¡èŒƒå¼å¯å®ç°LLMé—´çš„ç›´æ¥è¯­ä¹‰äº¤æµã€‚</li>
<li>C2Cåˆ©ç”¨ç¥ç»ç½‘ç»œæŠ•å½±å’Œèåˆæ¨¡å‹KVç¼“å­˜ï¼Œå®ç°è¯­ä¹‰ç›´æ¥ä¼ é€’ã€‚</li>
<li>C2Cç›¸æ¯”æ–‡æœ¬é€šä¿¡ï¼Œèƒ½æé«˜å¹³å‡å‡†ç¡®ç‡å¹¶é¿å…ä¸­é—´æ–‡æœ¬çš„æ˜¾å¼ç”Ÿæˆã€‚</li>
<li>C2Cé€šä¿¡èŒƒå¼åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºå•ä¸ªæ¨¡å‹å’Œæ–‡æœ¬é€šä¿¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e7f3be347073a8f261b3bc310179ebe1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941784&auth_key=1759941784-0-0-862f7588392a64b517203862efa381cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-58c3ac152e12e4e408ce0a5e38942fd8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-26097a4e47f8fa3739bb98ef26159c20~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941798&auth_key=1759941798-0-0-89991e36faf4224053dc862ba068cffb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f523a19302faff638e5a462cfa61ec0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941804&auth_key=1759941804-0-0-77008ea5c847eb1e0af2ee4eefd752d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Correlates-of-Language-Models-Are-Specific-to-Human-Language"><a href="#Neural-Correlates-of-Language-Models-Are-Specific-to-Human-Language" class="headerlink" title="Neural Correlates of Language Models Are Specific to Human Language"></a>Neural Correlates of Language Models Are Specific to Human Language</h2><p><strong>Authors:IÃ±igo Parra</strong></p>
<p>Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models. </p>
<blockquote>
<p>å…ˆå‰çš„å·¥ä½œå·²ç»æ˜¾ç¤ºå‡ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€ä¸è¯­è¨€ä»»åŠ¡ä¸­çš„åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰å¤§è„‘å“åº”ä¹‹é—´å­˜åœ¨å…³è”ã€‚è¿™äº›å…³è”è¢«è§†ä¸ºè¿™äº›æ¨¡å‹ä¸å¤§è„‘çŠ¶æ€åœ¨ä»£è¡¨æ€§æ–¹é¢çš„ç›¸ä¼¼æ€§è¯æ®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æµ‹è¯•è¿™äº›å…ˆå‰ç»“æœæ˜¯å¦èƒ½å¤Ÿå¯¹å¤šç§å¯èƒ½çš„æ‹…å¿§ä¿æŒç¨³å¥ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬ç ”ç©¶è¡¨æ˜ï¼šï¼ˆiï¼‰åœ¨é™ç»´ä¹‹åä»ç„¶å¯ä»¥å‘ç°ä¹‹å‰çš„ç»“æœï¼Œå› æ­¤è¿™å¹¶éå½’å› äºç»´æ•°è¯…å’’ï¼›ï¼ˆiiï¼‰åœ¨ä½¿ç”¨æ–°çš„ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•æ—¶ç¡®è®¤äº†ä¹‹å‰çš„ç»“æœï¼›ï¼ˆiiiï¼‰å¤§è„‘è¡¨ç¤ºä¸æ¨¡å‹ä¹‹é—´çš„å…³è”ç‰¹å®šäºç»è¿‡äººç±»è¯­è¨€è®­ç»ƒçš„æ¨¡å‹ï¼›ï¼ˆivï¼‰ç»“æœä¾èµ–äºæ¨¡å‹ä¸­æ˜¯å¦å­˜åœ¨ä½ç½®ç¼–ç ã€‚è¿™äº›ç»“æœè¯å®äº†å…ˆå‰ç ”ç©¶çš„ç»“æœå¹¶ä½¿å…¶å¾—åˆ°åŠ å¼ºï¼Œä¸ºå…³äºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©åˆç†æ€§å’Œå¯è§£é‡Šæ€§çš„äº‰è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03156v1">PDF</a> To be presented at NeurIPS 2025 Workshops</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€ä¸è¯­è¨€ä»»åŠ¡ä¸­çš„fMRIè„‘ååº”ä¹‹é—´å­˜åœ¨å…³è”ï¼Œè¢«è§†ä¸ºè¿™äº›æ¨¡å‹ä¸è„‘çŠ¶æ€çš„è¡¨å¾ç›¸ä¼¼æ€§çš„è¯æ®ã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†è¿™äº›å…ˆå‰ç»“æœæ˜¯å¦ç¨³å¥å¯é ã€‚å…·ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶å‘ç°ï¼šiï¼‰é™ç»´åä»ç„¶å‡ºç°ä¹‹å‰çš„ç»“æœï¼Œå› æ­¤ä¸æ˜¯ç”±äºç»´æ•°è¯…å’’æ‰€è‡´ï¼›iiï¼‰ä½¿ç”¨æ–°çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†åç¡®è®¤äº†ä¹‹å‰çš„ç»“æœï¼›iiiï¼‰è„‘è¡¨å¾ä¸æ¨¡å‹ä¹‹é—´çš„å…³è”ç‰¹å®šäºç»è¿‡äººç±»è¯­è¨€è®­ç»ƒçš„æ¨¡å‹ï¼›ivï¼‰ç»“æœä¾èµ–äºæ¨¡å‹ä¸­æ˜¯å¦å­˜åœ¨ä½ç½®ç¼–ç ã€‚è¿™äº›ç»“æœè¯å®äº†å…ˆå‰ç ”ç©¶çš„ç»“æœï¼Œä¸ºå…³äºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©åˆç†æ€§å’Œå¯è§£é‡Šæ€§çš„äº‰è®ºåšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€ä¸fMRIè„‘ååº”ä¹‹é—´çš„å…³è”è¢«è§†ä¸ºæ¨¡å‹ä¸è„‘çŠ¶æ€è¡¨å¾ç›¸ä¼¼æ€§çš„è¯æ®ã€‚</li>
<li>ç ”ç©¶ç»“æœç¨³å¥ï¼Œä¸å› é™ç»´è€Œæ¶ˆå¤±ï¼Œæ’é™¤ç»´æ•°è¯…å’’çš„å½±å“ã€‚</li>
<li>æ–°çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†éªŒè¯äº†å…ˆå‰çš„ç ”ç©¶ç»“æœã€‚</li>
<li>è„‘ä¸æ¨¡å‹ä¹‹é—´çš„å…³è”ç‰¹å®šäºç»è¿‡äººç±»è¯­è¨€è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>ç»“æœä¾èµ–äºæ¨¡å‹ä¸­æ˜¯å¦å­˜åœ¨ä½ç½®ç¼–ç ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯å®äº†å…ˆå‰çš„ç ”ç©¶ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©åˆç†æ€§æä¾›äº†æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1a603f71807503af91aa7674de14a8be~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941833&auth_key=1759941833-0-0-87e9aaf13f9dc60011f892c511775bc2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-4d7cb37d2e8e1aaa888aa5f3c357235d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-87d97ecf792ec91fe83d9ba3ee6b85d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941847&auth_key=1759941847-0-0-e30280f98f53e8955bc36d45b5db3a02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-5bba301ba7211195aff62ea8bc5f5087.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EditLens-Quantifying-the-Extent-of-AI-Editing-in-Text"><a href="#EditLens-Quantifying-the-Extent-of-AI-Editing-in-Text" class="headerlink" title="EditLens: Quantifying the Extent of AI Editing in Text"></a>EditLens: Quantifying the Extent of AI Editing in Text</h2><p><strong>Authors:Katherine Thai, Bradley Emi, Elyas Masrour, Mohit Iyyer</strong></p>
<p>A significant proportion of queries to large language models ask them to edit user-provided text, rather than generate new text from scratch. While previous work focuses on detecting fully AI-generated text, we demonstrate that AI-edited text is distinguishable from human-written and AI-generated text. First, we propose using lightweight similarity metrics to quantify the magnitude of AI editing present in a text given the original human-written text and validate these metrics with human annotators. Using these similarity metrics as intermediate supervision, we then train EditLens, a regression model that predicts the amount of AI editing present within a text. Our model achieves state-of-the-art performance on both binary (F1&#x3D;94.7%) and ternary (F1&#x3D;90.4%) classification tasks in distinguishing human, AI, and mixed writing. Not only do we show that AI-edited text can be detected, but also that the degree of change made by AI to human writing can be detected, which has implications for authorship attribution, education, and policy. Finally, as a case study, we use our model to analyze the effects of AI-edits applied by Grammarly, a popular writing assistance tool. To encourage further research, we commit to publicly releasing our models and dataset. </p>
<blockquote>
<p>å¾ˆå¤§ä¸€éƒ¨åˆ†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŸ¥è¯¢è¦æ±‚å®ƒä»¬ç¼–è¾‘ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹ç”Ÿæˆæ–°æ–‡æœ¬ã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ£€æµ‹å®Œå…¨ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ–‡æœ¬ï¼Œä½†æˆ‘ä»¬è¯æ˜äººå·¥æ™ºèƒ½ç¼–è¾‘çš„æ–‡æœ¬ä¸äººç±»ä¹¦å†™å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¯ä»¥åŒºåˆ†çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨è½»é‡çº§ç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†æ¥è¡¡é‡ç»™å®šåŸå§‹äººç±»ä¹¦å†™æ–‡æœ¬ä¸­AIç¼–è¾‘çš„ç¨‹åº¦ï¼Œå¹¶ä½¿ç”¨äººç±»æ³¨é‡Šè€…å¯¹è¿™äº›åº¦é‡æ ‡å‡†è¿›è¡ŒéªŒè¯ã€‚ä½¿ç”¨è¿™äº›ç›¸ä¼¼åº¦åº¦é‡ä½œä¸ºä¸­é—´ç›‘ç£ï¼Œæˆ‘ä»¬ç„¶åè®­ç»ƒäº†EditLensï¼Œä¸€ä¸ªå›å½’æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹æ–‡æœ¬ä¸­AIç¼–è¾‘çš„ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨äºŒå…ƒï¼ˆF1&#x3D;94.7%ï¼‰å’Œä¸‰å…ƒï¼ˆF1&#x3D;90.4%ï¼‰åˆ†ç±»ä»»åŠ¡ä¸­å®ç°äº†åŒºåˆ†äººç±»ã€äººå·¥æ™ºèƒ½å’Œæ··åˆå†™ä½œæ–¹é¢çš„æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚æˆ‘ä»¬ä¸ä»…è¯æ˜äº†å¯ä»¥æ£€æµ‹åˆ°AIç¼–è¾‘çš„æ–‡æœ¬ï¼Œè€Œä¸”è¯æ˜äº†å¯ä»¥æ£€æµ‹åˆ°AIå¯¹äººç±»å†™ä½œæ‰€åšçš„æ”¹å˜ç¨‹åº¦ï¼Œè¿™å¯¹ä½œè€…å½’å±ã€æ•™è‚²å’Œæ”¿ç­–éƒ½æœ‰å½±å“ã€‚æœ€åï¼Œä½œä¸ºä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹åˆ†æäº†æµè¡Œå†™ä½œåŠ©æ‰‹Grammarlyåº”ç”¨çš„AIç¼–è¾‘çš„å½±å“ã€‚ä¸ºäº†é¼“åŠ±è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ‰¿è¯ºå…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03154v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œæœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯ç¼–è¾‘ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè€Œéä»å¤´ç”Ÿæˆæ–°æ–‡æœ¬ã€‚æˆ‘ä»¬å±•ç¤ºäº†AIç¼–è¾‘çš„æ–‡æœ¬ä¸äººå·¥æ’°å†™å’ŒAIç”Ÿæˆçš„æ–‡æœ¬ä¹‹é—´çš„åŒºåˆ«ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨è½»é‡çº§ç›¸ä¼¼åº¦æŒ‡æ ‡æ¥è¡¡é‡ç»™å®šåŸå§‹äººç±»æ–‡æœ¬ä¸­AIç¼–è¾‘çš„ç¨‹åº¦ï¼Œå¹¶é€šè¿‡äººç±»æ³¨é‡Šè€…è¿›è¡ŒéªŒè¯ã€‚åŸºäºè¿™äº›ç›¸ä¼¼åº¦æŒ‡æ ‡ä½œä¸ºä¸­é—´ç›‘ç£ï¼Œæˆ‘ä»¬è®­ç»ƒäº†EditLensæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹æ–‡æœ¬ä¸­çš„AIç¼–è¾‘ç¨‹åº¦ã€‚æ¨¡å‹åœ¨åŒºåˆ†äººç±»ã€AIå’Œæ··åˆå†™ä½œçš„äºŒå…ƒå’Œä¸‰å…ƒåˆ†ç±»ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬ä¸ä»…è¯æ˜äº†å¯ä»¥æ£€æµ‹åˆ°AIç¼–è¾‘çš„æ–‡æœ¬ï¼Œè€Œä¸”å¯ä»¥æ£€æµ‹åˆ°AIå¯¹äººç±»å†™ä½œæ‰€åšçš„æ”¹å˜ç¨‹åº¦ï¼Œè¿™å¯¹ä½œè€…å½’å±ã€æ•™è‚²å’Œæ”¿ç­–éƒ½æœ‰å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬ä»¥Grammarlyè¿™ä¸€æµè¡Œçš„å†™ä½œè¾…åŠ©å·¥å…·ä¸ºä¾‹ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹åˆ†æäº†AIç¼–è¾‘çš„æ•ˆæœã€‚æˆ‘ä»¬è‡´åŠ›äºå…¬å¼€æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥é¼“åŠ±è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æŸ¥è¯¢æ—¶ï¼Œæ›´å¤šæ˜¯å¯¹ç”¨æˆ·æä¾›çš„æ–‡æœ¬è¿›è¡Œç¼–è¾‘ï¼Œè€Œéç”Ÿæˆæ–°æ–‡æœ¬ã€‚</li>
<li>AIç¼–è¾‘çš„æ–‡æœ¬ä¸äººå·¥æ’°å†™å’ŒAIç”Ÿæˆçš„æ–‡æœ¬ä¹‹é—´å­˜åœ¨æ˜¾è‘—åŒºåˆ«ã€‚</li>
<li>æå‡ºäº†ä½¿ç”¨è½»é‡çº§ç›¸ä¼¼åº¦æŒ‡æ ‡æ¥è¡¡é‡åŸå§‹äººç±»æ–‡æœ¬ä¸­AIç¼–è¾‘çš„ç¨‹åº¦ã€‚</li>
<li>EditLensæ¨¡å‹å¯ä»¥é¢„æµ‹æ–‡æœ¬ä¸­çš„AIç¼–è¾‘ç¨‹åº¦ï¼Œå¹¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>AIç¼–è¾‘çš„æ–‡æœ¬åŠå…¶å¯¹äººç±»å†™ä½œçš„æ”¹å˜ç¨‹åº¦å¯ä»¥è¢«æ£€æµ‹ï¼Œå¯¹å¤šä¸ªé¢†åŸŸæœ‰å½±å“ã€‚</li>
<li>ä»¥Grammarlyä¸ºä¾‹ï¼Œå±•ç¤ºäº†AIç¼–è¾‘çš„å®é™…åº”ç”¨åŠå…¶æ•ˆæœåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2b6400a38530529656695b1507c0d445~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941861&auth_key=1759941861-0-0-9a700a417403e34f769267b3ce2d849c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d03dbc64358d200f7fda4601a954148~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941868&auth_key=1759941868-0-0-5361adcf625799da8607ad5e68a22291&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-ddbb64fdd0cd5d198debb6621ce121b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2d3c458346a96eef87e6365153f8ff6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Improving-Cooperation-in-Collaborative-Embodied-AI"><a href="#Improving-Cooperation-in-Collaborative-Embodied-AI" class="headerlink" title="Improving Cooperation in Collaborative Embodied AI"></a>Improving Cooperation in Collaborative Embodied AI</h2><p><strong>Authors:Hima Jacob Leven Suprabha, Laxmi Nag Laxminarayan Nagesh, Ajith Nair, Alvin Reuben Amal Selvaster, Ayan Khan, Raghuram Damarla, Sanju Hannah Samuel, Sreenithi Saravana Perumal, Titouan Puech, Venkataramireddy Marella, Vishal Sonar, Alessandro Suglia, Oliver Lemon</strong></p>
<p>The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ä¸ºåä½œæ¨ç†å’Œä¸äººå·¥æ™ºèƒ½ä»£ç†äººçš„åˆä½œå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸åŒçš„æç¤ºæ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨æé«˜æ™ºèƒ½ä½“åä½œè¡Œä¸ºå’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¢å¼ºäº†CoELAï¼ˆä¸€ä¸ªä¸ºåˆ©ç”¨LLMè¿›è¡Œå¤šæ™ºèƒ½ä½“é€šä¿¡ã€æ¨ç†å’Œä»»åŠ¡åè°ƒè€Œè®¾è®¡çš„åä½œå®ä½“ä»£ç†æ¡†æ¶ï¼‰ï¼Œè¯¥æ¡†æ¶é€‚ç”¨äºå…±äº«è™šæ‹Ÿç©ºé—´ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„LLMå’Œæç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œä»¥æ‰¾å‡ºèƒ½æœ€å¤§åŒ–åä½œæ€§èƒ½çš„ä¼˜åŒ–ç»„åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆè¯­éŸ³åŠŸèƒ½ï¼Œå®ç°äº†æ— ç¼çš„åŸºäºè¯­éŸ³çš„åä½œäº¤äº’ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æç¤ºä¼˜åŒ–åœ¨æé«˜åä½œæ™ºèƒ½ä½“æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ€å¥½çš„ç»„åˆä¸Gemma3ç›¸æ¯”ï¼Œå°†ç³»ç»Ÿè¿è¡Œæ•ˆç‡æé«˜äº†22%ã€‚æ­¤å¤–ï¼Œè¯­éŸ³é›†æˆè¿˜ä¸ºè¿­ä»£ç³»ç»Ÿå¼€å‘å’Œæ¼”ç¤ºæä¾›äº†æ›´å…·å¸å¼•åŠ›çš„ç”¨æˆ·ç•Œé¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03153v1">PDF</a> In proceedings of UKCI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é›†æˆå¼€å¯äº†åä½œæ¨ç†å’Œæ™ºèƒ½ä½“åä½œçš„æ–°å¯èƒ½æ€§ã€‚æœ¬æ–‡æ¢è®¨äº†ä¸åŒçš„æç¤ºæ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨æé«˜æ™ºèƒ½ä½“åä½œè¡Œä¸ºå’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¢å¼ºäº†CoELAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨LLMæ„å»ºåä½œå®ä½“æ™ºèƒ½ä½“ï¼Œç”¨äºå…±äº«è™šæ‹Ÿç©ºé—´ä¸­çš„å¤šæ™ºèƒ½ä½“é€šä¿¡ã€æ¨ç†å’Œä»»åŠ¡åè°ƒã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„LLMå’Œæç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œä»¥æ‰¾å‡ºä¼˜åŒ–ç»„åˆï¼Œæœ€å¤§åŒ–åä½œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆè¯­éŸ³åŠŸèƒ½ï¼Œå®ç°æ— ç¼çš„åŸºäºè¯­éŸ³çš„åä½œäº¤äº’ã€‚ç ”ç©¶ç»“æœè¡¨æ˜æç¤ºä¼˜åŒ–åœ¨æé«˜åä½œæ™ºèƒ½ä½“æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼›ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æœ€ä½³ç»„åˆå°†ä½¿ç”¨Gemma3çš„ç³»ç»Ÿè¿è¡Œæ•ˆç‡æé«˜äº†22%ï¼Œä¸åŸå§‹CoELAç³»ç»Ÿç›¸æ¯”ã€‚æ­¤å¤–ï¼Œè¯­éŸ³é›†æˆä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªæ›´å…·å¸å¼•åŠ›çš„ç•Œé¢ï¼Œç”¨äºè¿­ä»£ç³»ç»Ÿå¼€å‘å’Œæ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é›†æˆæé«˜äº†åä½œæ¨ç†å’ŒAIæ™ºèƒ½ä½“åä½œçš„å¯èƒ½æ€§ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä¸åŒæç¤ºæ–¹æ³•åœ¨æé«˜æ™ºèƒ½ä½“åä½œè¡Œä¸ºå’Œå†³ç­–åˆ¶å®šä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>CoELAæ¡†æ¶è¢«å¢å¼ºï¼Œåˆ©ç”¨LLMè¿›è¡Œå¤šæ™ºèƒ½ä½“é€šä¿¡ã€æ¨ç†å’Œä»»åŠ¡åè°ƒã€‚</li>
<li>ç³»ç»Ÿå®éªŒç ”ç©¶äº†ä¸åŒçš„LLMå’Œæç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œæ‰¾åˆ°äº†ä¼˜åŒ–ç»„åˆä»¥æœ€å¤§åŒ–åä½œæ€§èƒ½ã€‚</li>
<li>è¯­éŸ³åŠŸèƒ½çš„é›†æˆå®ç°äº†æ— ç¼çš„åŸºäºè¯­éŸ³çš„åä½œäº¤äº’ï¼Œå¢å¼ºäº†ç”¨æˆ·ä½“éªŒã€‚</li>
<li>æç¤ºä¼˜åŒ–åœ¨æé«˜åä½œæ™ºèƒ½ä½“æ€§èƒ½æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-15b419a24ee8d6d6607fbd9e63c60738~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941889&auth_key=1759941889-0-0-01802e33ec50c5b5a805668bbfa71d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f09cf3fed438a65740cf5fee4658431~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941896&auth_key=1759941896-0-0-502d260b14a4d38b3ec24ccd758703af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning"><a href="#MM-Nav-Multi-View-VLA-Model-for-Robust-Visual-Navigation-via-Multi-Expert-Learning" class="headerlink" title="MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning"></a>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via   Multi-Expert Learning</h2><p><strong>Authors:Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, He Wang</strong></p>
<p>Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method. </p>
<blockquote>
<p>è§†è§‰å¯¼èˆªç­–ç•¥è¢«å¹¿æ³›è®¤ä¸ºæ˜¯ä¸€ä¸ªå…·æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå› ä¸ºå®ƒé€šè¿‡ä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰è§‚å¯Ÿæ¥æ¨¡ä»¿äººç±»çš„å¯¼èˆªæ–¹å¼ã€‚ç„¶è€Œï¼Œè§†è§‰è§‚å¯Ÿçš„å…‰å­¦ä¿¡æ¯éš¾ä»¥åƒæ¿€å…‰é›·è¾¾ç‚¹äº‘æˆ–æ·±åº¦åœ°å›¾é‚£æ ·è¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œè¿™éœ€è¦æ™ºèƒ½æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ™ºèƒ½ï¼Œä»¥å¸ˆå¾’æ–¹å¼ä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å„ç§å¯¼èˆªèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å®ç°äº†VLAæ¨¡å‹MM-Navï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„å¤šè§†è§’VLAï¼ˆå…·æœ‰360åº¦è§‚å¯Ÿï¼‰ã€‚å¯¹äºå¤§è§„æ¨¡å¯¼èˆªæ•°æ®ï¼Œæˆ‘ä»¬ä»ä¸‰ä½å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸“å®¶æ”¶é›†ä¸“å®¶æ•°æ®ï¼Œè¿™äº›ä¸“å®¶ä½¿ç”¨ç‰¹æƒæ·±åº¦ä¿¡æ¯åœ¨ä¸‰ä¸ªé’ˆå¯¹ä¸åŒå¯¼èˆªèƒ½åŠ›é‡èº«å®šåˆ¶çš„æŒ‘æˆ˜ç¯å¢ƒä¸­æ¥å—è®­ç»ƒï¼šåˆ°è¾¾ã€æŒ¤å‹å’Œé¿å…ã€‚æˆ‘ä»¬è¿­ä»£åœ°ä½¿ç”¨æ¥è‡ªRLä¸“å®¶çš„åœ¨çº¿æ”¶é›†æ•°æ®æ¥è®­ç»ƒæˆ‘ä»¬çš„VLAæ¨¡å‹ï¼Œå…¶ä¸­è®­ç»ƒæ¯”ä¾‹æ˜¯æ ¹æ®ä¸ªäººèƒ½åŠ›çš„è¡¨ç°è¿›è¡ŒåŠ¨æ€å¹³è¡¡çš„ã€‚é€šè¿‡åˆæˆç¯å¢ƒçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å­¦ç”ŸVLAæ¨¡å‹çš„è¡¨ç°ä¼˜äºRLè€å¸ˆï¼Œè¿™è¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒä½œç”¨ã€‚å¤§é‡ç°å®ä¸–ç•Œå®éªŒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03142v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://pku-epic.github.io/MM-Nav-Web/">https://pku-epic.github.io/MM-Nav-Web/</a></p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å¯¼èˆªç­–ç•¥è¢«å¹¿æ³›è®¤ä¸ºæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œå› ä¸ºå®ƒé€šè¿‡ä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§‚å¯Ÿæ¥è¿›è¡Œå¯¼èˆªï¼Œä»è€Œæ¨¡ä»¿äººç±»ã€‚ç„¶è€Œï¼Œè§†è§‰è§‚å¯Ÿçš„å…‰å­¦ä¿¡æ¯éš¾ä»¥åƒæ¿€å…‰é›·è¾¾ç‚¹äº‘æˆ–æ·±åº¦å›¾é‚£æ ·è¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œè¿™éœ€è¦æ™ºèƒ½æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„æ™ºèƒ½ï¼Œä»¥å¸ˆå¾’æ–¹å¼ä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å„ç§å¯¼èˆªèƒ½åŠ›ã€‚æˆ‘ä»¬å®ç°äº†åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„MM-Navå¤šè§†å›¾VLAæ¨¡å‹ï¼ˆå…·æœ‰360åº¦è§‚å¯Ÿï¼‰ã€‚ä¸ºäº†è·å–å¤§è§„æ¨¡å¯¼èˆªæ•°æ®ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªå¼ºåŒ–å­¦ä¹ ä¸“å®¶æ”¶é›†çš„ä¸“å®¶æ•°æ®ä¸­æ”¶é›†æ•°æ®ï¼Œè¿™äº›ä¸“å®¶ä½¿ç”¨ç‰¹æƒæ·±åº¦ä¿¡æ¯åœ¨ä¸‰ä¸ªé’ˆå¯¹ä¸åŒå¯¼èˆªèƒ½åŠ›çš„é‡èº«å®šåˆ¶çš„ç¯å¢ƒä¸­æ¥å—è®­ç»ƒï¼šåˆ°è¾¾ã€æŒ¤å‹å’Œé¿å…ã€‚æˆ‘ä»¬è¿­ä»£åœ°è®­ç»ƒæˆ‘ä»¬çš„VLAæ¨¡å‹ï¼Œä½¿ç”¨ä»å¼ºåŒ–å­¦ä¹ ä¸“å®¶åœ¨çº¿æ”¶é›†çš„æ•°æ®ï¼Œå…¶ä¸­è®­ç»ƒæ¯”ç‡æ˜¯æ ¹æ®å¯¹ä¸ªåˆ«èƒ½åŠ›çš„è¡¨ç°è¿›è¡ŒåŠ¨æ€å¹³è¡¡çš„ã€‚åœ¨åˆæˆç¯å¢ƒä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å­¦ç”ŸVLAæ¨¡å‹çš„è¡¨ç°ä¼˜äºå¼ºåŒ–å­¦ä¹ è€å¸ˆï¼Œè¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒæ•ˆåº”ã€‚åœ¨ç°å®ä¸–ç•Œä¸­çš„å¤§é‡å®éªŒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¯¼èˆªç­–ç•¥æ¨¡ä»¿äººç±»ä½¿ç”¨è§†è§‰è§‚å¯Ÿè¿›è¡Œå¯¼èˆªï¼Œè¢«è®¤ä¸ºæ˜¯å‰æ™¯å¹¿é˜”çš„æ–¹å‘ã€‚</li>
<li>å…‰å­¦ä¿¡æ¯çš„å»ºæ¨¡æ˜¯è§†è§‰å¯¼èˆªä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ™ºèƒ½æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®ã€‚</li>
<li>æå‡ºåˆ©ç”¨VLAæ¨¡å‹çš„æ™ºèƒ½ï¼Œä»åˆæˆä¸“å®¶æ•°æ®ä¸­å­¦ä¹ å¤šç§å¯¼èˆªèƒ½åŠ›ã€‚</li>
<li>å®ç°åä¸ºMM-Navçš„å¤šè§†å›¾VLAæ¨¡å‹ï¼Œå…·å¤‡360åº¦è§‚å¯Ÿèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ”¶é›†æ¥è‡ªå¼ºåŒ–å­¦ä¹ ä¸“å®¶çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›ä¸“å®¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­æ¥å—è®­ç»ƒï¼Œé’ˆå¯¹ä¸åŒå¯¼èˆªèƒ½åŠ›å¦‚åˆ°è¾¾ã€æŒ¤å‹å’Œé¿å…ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”å­¦ç”ŸVLAæ¨¡å‹è¡¨ç°ä¼˜äºå¼ºåŒ–å­¦ä¹ è€å¸ˆï¼Œè¯æ˜äº†æ•´åˆå¤šç§èƒ½åŠ›çš„ååŒæ•ˆåº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96cd04118467124c38a9a5b7da6d6ff7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b9a3fee8905f84ecc88ece04e69effd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941912&auth_key=1759941912-0-0-9c814764b4acd2a5dbf5c08a47dfaa0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-196afbdc75a5fb50763f34351da8d5ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941919&auth_key=1759941919-0-0-691f315e25efc7482ce4b11309a3821a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a911de169e5d64cc5b31e0556734134c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941925&auth_key=1759941925-0-0-32484fb6590d96b7194ed3ecade2f031&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bc59b9dfb0f76dc6d772656d7646701~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941931&auth_key=1759941931-0-0-e7f3814df976e568215cdeae13971462&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER"><a href="#Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER" class="headerlink" title="Semantic Similarity in Radiology Reports via LLMs and NER"></a>Semantic Similarity in Radiology Reports via LLMs and NER</h2><p><strong>Authors:Beth Pearson, Ahmed Adnan, Zahraa Abdallah</strong></p>
<p>Radiology report evaluation is a crucial part of radiologistsâ€™ training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +&#x2F;- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports%7D%7Bgithub.com/otmive/llama/_reports%7D">https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}</a> </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šè¯„ä¼°æ˜¯æ”¾å°„ç§‘åŒ»ç”ŸåŸ¹è®­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºç¡®ä¿è¯Šæ–­çš„å‡†ç¡®æ€§èµ·ç€å…³é”®ä½œç”¨ã€‚ä½œä¸ºæ ‡å‡†æŠ¥å‘Šå·¥ä½œæµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œåˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸ä¼šç¼–å†™åˆæ­¥æŠ¥å‘Šï¼Œç„¶åç”±é«˜çº§æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œå®¡æŸ¥å’Œç¼–è¾‘ä»¥äº§ç”Ÿæœ€ç»ˆæŠ¥å‘Šã€‚è¯†åˆ«åˆæ­¥æŠ¥å‘Šå’Œæœ€ç»ˆæŠ¥å‘Šä¹‹é—´çš„è¯­ä¹‰å·®å¼‚å¯¹åˆçº§åŒ»ç”Ÿè‡³å…³é‡è¦ï¼Œæ—¢ä½œä¸ºåŸ¹è®­å·¥å…·ï¼Œä¹Ÿæœ‰åŠ©äºå‘ç°ä¸´åºŠçŸ¥è¯†æ–¹é¢çš„ä¸è¶³ã€‚å°½ç®¡äººå·¥æ™ºèƒ½åœ¨æ”¾å°„å­¦é¢†åŸŸæ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä½†ç”±äºéœ€è¦ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMåœ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­æä¾›å¯è§£é‡Šå’Œå‡†ç¡®æ¯”è¾ƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæ¯”è¾ƒäº†å¤šä¸ªLLMåœ¨æ¯”è¾ƒæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚ç„¶åæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç§åŸºäºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„æ›´ä¼ ç»Ÿçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨æä¾›è¯­ä¹‰ç›¸ä¼¼æ€§æ–¹é¢çš„å‡†ç¡®åé¦ˆæ–¹é¢éƒ½è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-EntScoreï¼Œä¸€ç§ä½¿ç”¨Llama 3.1å’ŒNERç»„åˆçš„è¯­ä¹‰ç›¸ä¼¼æ€§è¯„åˆ†æ–¹æ³•ï¼Œå…·æœ‰å¯è°ƒæƒé‡ï¼Œå¯ä»¥å¼ºè°ƒæˆ–æ·¡åŒ–ç‰¹å®šç±»å‹çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆå®šé‡ç›¸ä¼¼æ€§åˆ†æ•°ä»¥è·Ÿè¸ªè¿›åº¦ï¼Œå¹¶å¯¹åˆ†æ•°è¿›è¡Œè§£é‡Šï¼Œæ—¨åœ¨ä¸ºå®¡æŸ¥å’Œæ”¹è¿›æŠ¥å‘Šæä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿæä¾›çš„çœŸå®åˆ†æ•°ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†67%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡å’Œ93%çš„å‡†ç¡®ç‡ï¼ˆåœ¨+&#x2F;- 1èŒƒå›´å†…ï¼‰â€”â€”è¶…è¶Šäº†ç‹¬ç«‹ä½¿ç”¨çš„LLMå’ŒNERã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports">github.com&#x2F;otmive&#x2F;llama_reports</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03102v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦æŠ¥å‘Šæ¯”è¾ƒä¸­çš„åº”ç”¨ã€‚ç ”ç©¶é€šè¿‡ç»“åˆLlama 3.1æ¨¡å‹å’Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¯„åˆ†æ–¹æ³•Llama-EntScoreã€‚è¯¥æ–¹æ³•ç”¨äºè¯„ä¼°åˆæ­¥æŠ¥å‘Šå’Œæœ€ç»ˆæŠ¥å‘Šä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œæ—¨åœ¨å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”ŸåŸ¹è®­å’ŒçŸ¥è¯†æå‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå•ä¸€çš„LLMså’ŒNERæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ”¾å°„å­¦æŠ¥å‘Šè¯„ä¼°æ˜¯æ”¾å°„ç§‘åŒ»ç”ŸåŸ¹è®­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºç¡®ä¿è¯Šæ–­å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>åˆæ­¥æŠ¥å‘Šé€šå¸¸ç”±åˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿç¼–åˆ¶ï¼Œç„¶åç”±é«˜çº§æ”¾å°„ç§‘åŒ»ç”Ÿå®¡æŸ¥å¹¶ç¼–è¾‘æˆæœ€ç»ˆæŠ¥å‘Šã€‚</li>
<li>è¯­ä¹‰å·®å¼‚è¯†åˆ«å¯¹äºåˆçº§åŒ»ç”Ÿæ—¢æ˜¯è®­ç»ƒå·¥å…·ï¼Œä¹Ÿæœ‰åŠ©äºå‘ç°ä¸´åºŠçŸ¥è¯†çš„å·®è·ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸçš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†LLMsåœ¨æ”¾å°„å­¦æŠ¥å‘Šæ¯”è¾ƒä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†Llama-EntScoreæ–¹æ³•ã€‚</li>
<li>Llama-EntScoreæ–¹æ³•ç»“åˆäº†Llama 3.1æ¨¡å‹å’ŒNERæŠ€æœ¯ï¼Œæ—¨åœ¨è¯„ä¼°åˆæ­¥æŠ¥å‘Šå’Œæœ€ç»ˆæŠ¥å‘Šä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f98c50394379a9217910538618acceb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941938&auth_key=1759941938-0-0-94ba641fe129ed4dac572160e096ae94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering"><a href="#FinAgentBench-A-Benchmark-Dataset-for-Agentic-Retrieval-in-Financial-Question-Answering" class="headerlink" title="FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering"></a>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial   Question Answering</h2><p><strong>Authors:Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</strong></p>
<p>Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods â€“ whether sparse or dense â€“ often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance â€“ a setting we term agentic retrieval. The benchmark consists of 26K expert-annotated examples on S&amp;P-500 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. </p>
<blockquote>
<p>å‡†ç¡®çš„æƒ…æŠ¥æ£€ç´¢ï¼ˆIRï¼‰åœ¨é‡‘èé¢†åŸŸè‡³å…³é‡è¦ï¼ŒæŠ•èµ„è€…éœ€è¦ä»å¤§é‡æ–‡æ¡£ä¸­æ‰¾å‡ºç›¸å…³ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„IRæ–¹æ³•ï¼Œæ— è®ºæ˜¯ç¨€ç–çš„è¿˜æ˜¯å¯†é›†çš„ï¼Œå¾€å¾€åœ¨æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› ä¸ºå®ƒä¸ä»…éœ€è¦æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿˜éœ€è¦å¯¹æ–‡æ¡£ç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œç²¾ç»†çš„æ¨ç†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£æ¨ç†æ¥æ’åˆ—æ®µè½ï¼Œç¡®å®šå“ªäº›ä¿¡æ¯å¯¹ç»™å®šæŸ¥è¯¢æœ€ä¸ºç›¸å…³ã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸç¼ºä¹è¯„ä¼°è¿™ç§èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FinAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸå¤šæ­¥éª¤æ¨ç†æ£€ç´¢çš„å¤§å‹åŸºå‡†æµ‹è¯•â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºä»£ç†æ£€ç´¢ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å…³äºæ ‡å‡†æ™®å°”500ä¸Šå¸‚å…¬å¸çš„2.6ä¸‡ä¸ªä¸“å®¶æ³¨é‡Šç¤ºä¾‹ï¼Œå¹¶è¯„ä¼°LLMä»£ç†æ˜¯å¦èƒ½ï¼ˆ1ï¼‰åœ¨å€™é€‰è€…ä¸­è¯†åˆ«å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹ï¼Œä»¥åŠï¼ˆ2ï¼‰åœ¨é€‰å®šæ–‡æ¡£ä¸­æŒ‡å‡ºå…³é”®æ®µè½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡å±€é™æ€§ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæä¾›å®šé‡ä¾æ®ï¼Œä»¥äº†è§£é‡‘èé¢†åŸŸä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥å±•ç¤ºäº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¦‚ä½•æ˜¾ç€æé«˜ä»£ç†æ£€ç´¢æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºç ”ç©¶é‡‘èé¢†åŸŸçš„å¤æ‚ç‰¹å®šä»»åŠ¡ä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºæä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14052v4">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨é‡‘èé¢†åŸŸä¸­ï¼Œå‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è‡³å…³é‡è¦ã€‚ä¼ ç»ŸIRæ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®æ£€ç´¢ä¿¡æ¯ï¼Œå› ä¸ºé™¤äº†æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§å¤–ï¼Œè¿˜éœ€è¦å¯¹æ–‡æ¡£ç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œç²¾ç»†æ¨ç†ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºå…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œé‡‘èé¢†åŸŸç¼ºä¹è¯„ä¼°è¿™ç§èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FinAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸä¸­å…·æœ‰å¤šæ­¥éª¤æ¨ç†çš„æ£€ç´¢çš„å¤§å‹åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«é’ˆå¯¹æ ‡æ™®500ä¸Šå¸‚å…¬å¸çš„2.6ä¸‡ä»½ä¸“å®¶æ³¨é‡Šæ ·æœ¬ï¼Œå¹¶è¯„ä¼°LLMä»£ç†æ˜¯å¦èƒ½ç¡®å®šå€™é€‰æ–‡æ¡£ä¸­æœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹ä»¥åŠåœ¨é€‰å®šæ–‡æ¡£ä¸­å®šä½å…³é”®æ®µè½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜ç¡®åœ°å°†è¿™ä¸¤ä¸ªæ¨ç†æ­¥éª¤åˆ†å¼€ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡å±€é™æ€§é—®é¢˜ã€‚è¿™ç§è®¾è®¡ä¸ºç†è§£é‡‘èé¢†åŸŸä¸­ä»¥æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºæä¾›äº†å®šé‡ä¾æ®ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¦‚ä½•æ˜¾ç€æé«˜ä»£ç†æ£€ç´¢æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºç ”ç©¶é‡‘èé¢†åŸŸçš„å¤æ‚ç‰¹å®šä»»åŠ¡ä¸­é’ˆå¯¹æ£€ç´¢ä¸ºä¸­å¿ƒçš„LLMè¡Œä¸ºå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èé¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºé‡‘èä¿¡æ¯æ£€ç´¢ä¸­çš„å¤šæ­¥éª¤æ¨ç†æä¾›äº†æ–°æœºä¼šã€‚</li>
<li>ç¼ºä¹è¯„ä¼°é‡‘èé¢†åŸŸä¸­å¤šæ­¥éª¤æ¨ç†æ£€ç´¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¼•å…¥FinAgentBenchä½œä¸ºé¦–ä¸ªé’ˆå¯¹é‡‘èé¢†åŸŸçš„å¤šæ­¥éª¤æ¨ç†æ£€ç´¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«é’ˆå¯¹æ ‡æ™®500ä¸Šå¸‚å…¬å¸çš„ä¸“å®¶æ³¨é‡Šæ ·æœ¬ï¼Œè¯„ä¼°LLMä»£ç†è¯†åˆ«æœ€ç›¸å…³æ–‡æ¡£ç±»å‹å’Œå®šä½å…³é”®æ®µè½çš„èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶æ˜ç¡®åŒºåˆ†è¯†åˆ«æ–‡æ¡£ç±»å‹ä¸å®šä½å…³é”®æ®µè½ä¸¤ä¸ªæ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e67b298766d64441c67ca8ac9c11fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941947&auth_key=1759941947-0-0-7f0a26acb0bd4c96908c6d72dbbcf034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33fb8858b61d505b4d3e5c36a8853ae3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941954&auth_key=1759941954-0-0-b18ec72922f4b5a948a53acd5c6e945c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6a46765e47c81459dca0d6bcc678956~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941960&auth_key=1759941960-0-0-f489d09f87f09810168eb4fa46680503&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-c9ab698bd07fc75334d1a2448d3b6e4e.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b53eeebce52e81ffd410e01298428d7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941974&auth_key=1759941974-0-0-04279aff47a989727a657a1f196e7284&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quantum-RAG-and-PunGPT2-Advancing-Low-Resource-Language-Generation-and-Retrieval-for-the-Punjabi-Language"><a href="#Quantum-RAG-and-PunGPT2-Advancing-Low-Resource-Language-Generation-and-Retrieval-for-the-Punjabi-Language" class="headerlink" title="Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and   Retrieval for the Punjabi Language"></a>Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and   Retrieval for the Punjabi Language</h2><p><strong>Authors:Jaskaranjeet Singh, Rakesh Thakur</strong></p>
<p>Despite rapid advances in large language models (LLMs), low-resource languages remain excluded from NLP, limiting digital access for millions. We present PunGPT2, the first fully open-source Punjabi generative model suite, trained on a 35GB corpus covering literature, religious texts, news, social discourse, etc. PunGPT2 captures Punjabiâ€™s syntactic and morphological richness through a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce Pun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS retriever over a curated Punjabi knowledge base, and Pun-Instruct, an instruction-tuned variant using QLoRA for robust zero-shot summarization, translation, and question answering. Our key innovation, Quantum-RAG, fuses sparse, dense, and quantum kernel embeddings for efficient, context-aware retrieval with low memory overhead, marking the first practical quantum-inspired retrieval in a low-resource LLM. Our models outperform multilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench, and a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and +3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts, hyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval benchmark, and all model weights, establishing new state-of-the-art results for Punjabi language generation and retrieval. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿè¿›æ­¥ï¼Œä½†ä½èµ„æºè¯­è¨€ä»ç„¶è¢«æ’é™¤åœ¨NLPä¹‹å¤–ï¼Œé™åˆ¶äº†æ•°ç™¾ä¸‡äººçš„æ•°å­—è®¿é—®ã€‚æˆ‘ä»¬æ¨å‡ºäº†PunGPT2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¼€æºçš„æ—é®æ™®è¯­ç”Ÿæˆæ¨¡å‹å¥—ä»¶ï¼Œå®ƒåŸºäºä¸€ä¸ªåŒ…å«æ–‡çŒ®ã€å®—æ•™æ–‡æœ¬ã€æ–°é—»ã€ç¤¾ä¼šè¯è¯­ç­‰å†…å®¹çš„35GBè¯­æ–™åº“è¿›è¡Œè®­ç»ƒã€‚PunGPT2é€šè¿‡é’ˆå¯¹å¤å°”å§†åŸºå’Œæ²™å§†åŸºè„šæœ¬ä¼˜åŒ–çš„åˆ†è¯å™¨æ•æ‰æ—é®æ™®è¯­çš„å¥æ³•å’Œå½¢æ€ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Pun-RAGï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†PunGPT2ä¸é’ˆå¯¹ç²¾é€‰æ—é®æ™®çŸ¥è¯†åº“çš„FAISSæ£€ç´¢å™¨çš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œä»¥åŠPun-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨QLoRAè¿›è¡Œç¨³å¥çš„é›¶å°„å‡»æ‘˜è¦ã€ç¿»è¯‘å’Œé—®ç­”çš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯Quantum-RAGï¼Œå®ƒèåˆäº†ç¨€ç–ã€å¯†é›†å’Œé‡å­æ ¸åµŒå…¥ï¼Œç”¨äºå®ç°å…·æœ‰ä½å†…å­˜å¼€é”€çš„é«˜æ•ˆã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢ï¼Œè¿™æ˜¯ä½èµ„æºLLMä¸­é¦–ä¸ªå®ç”¨çš„é‡å­å¯å‘å¼æ£€ç´¢ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨FLORES-200ã€IndicGenBenchå’Œæ–°çš„PunjabiEvalå¥—ä»¶ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å¤šè¯­è¨€åŸºçº¿ï¼ˆmBERTã€mT5ã€MuRILã€BLOOMï¼‰ã€‚Quantum-RAGåœ¨PunjabiEvalä¸Šçš„Recall@10æ¯”FAISSé«˜å‡º7.4ï¼ŒBLEUæ¯”mT5é«˜å‡º3.5ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ‰€æœ‰çš„è®­ç»ƒè„šæœ¬ã€è¶…å‚æ•°ã€è¯„ä¼°ç®¡é“ã€35GBçš„æ—é®æ™®è¯­æ–™åº“ã€PunjabiEvalåŸºå‡†æµ‹è¯•ä»¥åŠæ‰€æœ‰çš„æ¨¡å‹æƒé‡ï¼Œä¸ºæ—é®æ™®è¯­çš„è¯­è¨€ç”Ÿæˆå’Œæ£€ç´¢æ ‘ç«‹äº†æ–°çš„æœ€æ–°æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01918v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•è¿…é€Ÿï¼Œä½†ä½èµ„æºè¯­è¨€åœ¨NLPä¸­ä»è¢«è¾¹ç¼˜åŒ–ï¼Œé™åˆ¶äº†æ•°ç™¾ä¸‡äººçš„æ•°å­—è®¿é—®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PunGPT2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¼€æºçš„æ—é®æ™®è¯­ç”Ÿæˆæ¨¡å‹å¥—ä»¶ï¼Œç»è¿‡35GBè¯­æ–™åº“çš„è®­ç»ƒï¼Œæ¶µç›–äº†æ–‡çŒ®ã€å®—æ•™æ–‡æœ¬ã€æ–°é—»ã€ç¤¾ä¼šè¯è¯­ç­‰ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Pun-RAGå’ŒQuantum-RAGï¼Œå‰è€…æ˜¯ä¸€ä¸ªä¸PunGPT2é›†æˆçš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œåè€…èåˆäº†ç¨€ç–ã€å¯†é›†å’Œé‡å­æ ¸åµŒå…¥ï¼Œå®ç°äº†é«˜æ•ˆã€è¯­å¢ƒæ„ŸçŸ¥çš„æ£€ç´¢å’Œå†…å­˜ä½æ¶ˆè€—ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨FLORES-200ã€IndicGenBenchå’Œæ–°çš„PunjabiEvalå¥—ä»¶ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å¤šè¯­è¨€åŸºçº¿ï¼Œå»ºç«‹äº†æ—é®æ™®è¯­è¯­è¨€å’Œæ£€ç´¢çš„æ–°é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä½èµ„æºè¯­è¨€åœ¨NLPä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ•°å­—è®¿é—®ã€‚</li>
<li>PunGPT2æ˜¯é¦–ä¸ªå®Œå…¨å¼€æºçš„æ—é®æ™®è¯­ç”Ÿæˆæ¨¡å‹å¥—ä»¶ï¼Œç»è¿‡35GBè¯­æ–™åº“è®­ç»ƒã€‚</li>
<li>Pun-RAGæ˜¯ä¸€ä¸ªä¸PunGPT2é›†æˆçš„æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œæä¾›äº†è¯­å¢ƒæ„ŸçŸ¥çš„æ£€ç´¢åŠŸèƒ½ã€‚</li>
<li>Quantum-RAGèåˆäº†ç¨€ç–ã€å¯†é›†å’Œé‡å­æ ¸åµŒå…¥ï¼Œå®ç°äº†é«˜æ•ˆæ£€ç´¢å’Œå†…å­˜ä½æ¶ˆè€—ã€‚</li>
<li>æ—é®æ™®è¯­æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å¤šè¯­è¨€åŸºçº¿ã€‚</li>
<li>Quantum-RAGåœ¨PunjabiEvalä¸Šçš„å¬å›ç‡å’ŒBLEUå¾—åˆ†å‡æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f018e6328df42c6130fcd8242c6d012f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941983&auth_key=1759941983-0-0-3b8cc9561388752c3ec7a6ac18d4649c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9724358f86dce1254addeeb19f82b725~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941991&auth_key=1759941991-0-0-b42a9f2b300098edfabeba6df5339558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-2c56bf6a1d323bcb92ba14c3a508744f.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cd9da55f2b222af35e277286d560989~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942004&auth_key=1759942004-0-0-d70c655ad9b9b8b7dd6690b2ce8fdcf1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51c1f24660326f89a485d23749d36650~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942010&auth_key=1759942010-0-0-d45446b208b17fd5d304d2943440d607&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-b4eaddd005adcb03b78ae16d323b8b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453a015e82a72d1959ffb9326c251652.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c4e716a969b71347636f2c484f449c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942030&auth_key=1759942030-0-0-1a3cf5e44f0f804924c87b2e00f89c95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-5d416cbdfa00f795afb54fd908e2c47b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Tuning-LLM-based-Code-Optimization-via-Meta-Prompting-An-Industrial-Perspective"><a href="#Tuning-LLM-based-Code-Optimization-via-Meta-Prompting-An-Industrial-Perspective" class="headerlink" title="Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial   Perspective"></a>Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial   Perspective</h2><p><strong>Authors:Jingzhi Gong, Rafail Giavrimis, Paul Brookes, Vardan Voskanyan, Fan Wu, Mari Ashiga, Matthew Truscott, Mike Basios, Leslie Kanthan, Jie Xu, Zheng Wang</strong></p>
<p>There is a growing interest in leveraging multiple large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge: prompts optimized for one LLM often fail with others, requiring expensive model-specific prompt engineering. This cross-model prompt engineering bottleneck severely limits the practical deployment of multi-LLM systems in production environments. We introduce Meta-Prompted Code Optimization (MPCO), a framework that automatically generates high-quality, task-specific prompts across diverse LLMs while maintaining industrial efficiency requirements. MPCO leverages metaprompting to dynamically synthesize context-aware optimization prompts by integrating project metadata, task requirements, and LLM-specific contexts. It is an essential part of the ARTEMIS code optimization platform for automated validation and scaling. Our comprehensive evaluation on five real-world codebases with 366 hours of runtime benchmarking demonstrates MPCOâ€™s effectiveness: it achieves overall performance improvements up to 19.06% with the best statistical rank across all systems compared to baseline methods. Analysis shows that 96% of the top-performing optimizations stem from meaningful edits. Through systematic ablation studies and meta-prompter sensitivity analysis, we identify that comprehensive context integration is essential for effective meta-prompting and that major LLMs can serve effectively as meta-prompters, providing actionable insights for industrial practitioners. </p>
<blockquote>
<p>å¯¹äºåˆ©ç”¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–ä»£ç ä¼˜åŒ–çš„å…´è¶£æ­£åœ¨å¢é•¿ã€‚ç„¶è€Œï¼Œéƒ¨ç½²å¤šä¸ªLLMçš„å·¥ä¸šå¹³å°é¢ä¸´ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šé’ˆå¯¹ä¸€ä¸ªLLMä¼˜åŒ–çš„æç¤ºå¾€å¾€åœ¨å…¶ä»–LLMä¸Šå¤±è´¥ï¼Œè¿™éœ€è¦è¿›è¡Œæ˜‚è´µçš„æ¨¡å‹ç‰¹å®šæç¤ºå·¥ç¨‹ã€‚è¿™ç§è·¨æ¨¡å‹æç¤ºå·¥ç¨‹ç“¶é¢ˆä¸¥é‡é™åˆ¶äº†å¤šLLMç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²ã€‚æˆ‘ä»¬å¼•å…¥äº†Metaæç¤ºä»£ç ä¼˜åŒ–ï¼ˆMPCOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¿æŒå·¥ä¸šæ•ˆç‡è¦æ±‚çš„åŒæ—¶ï¼Œåœ¨å¤šç§LLMä¸­è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ä»»åŠ¡ç‰¹å®šæç¤ºã€‚MPCOåˆ©ç”¨å…ƒæç¤ºæŠ€æœ¯ï¼Œé€šè¿‡æ•´åˆé¡¹ç›®å…ƒæ•°æ®ã€ä»»åŠ¡è¦æ±‚å’ŒLLMç‰¹å®šä¸Šä¸‹æ–‡ï¼ŒåŠ¨æ€åˆæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¼˜åŒ–æç¤ºã€‚å®ƒæ˜¯ARTEMISä»£ç ä¼˜åŒ–å¹³å°çš„è‡ªåŠ¨åŒ–éªŒè¯å’Œæ‰©å±•çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œçš„ä»£ç åº“ä¸Šè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œé€šè¿‡366å°æ—¶çš„è¿è¡Œæ—¶åŸºå‡†æµ‹è¯•è¯æ˜äº†MPCOçš„æœ‰æ•ˆæ€§ï¼šä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æ€»ä½“ä¸Šå®ç°äº†é«˜è¾¾19.06%çš„æ€§èƒ½æå‡ï¼Œåœ¨æ‰€æœ‰ç³»ç»Ÿä¸­è¡¨ç°æœ€ä½³ã€‚åˆ†æè¡¨æ˜ï¼Œ96%çš„æœ€ä½³æ€§èƒ½ä¼˜åŒ–æºäºæœ‰æ„ä¹‰çš„ç¼–è¾‘ã€‚é€šè¿‡ç³»ç»Ÿçš„å‰”é™¤ç ”ç©¶å’Œå…ƒæç¤ºæ•æ„Ÿæ€§åˆ†æï¼Œæˆ‘ä»¬å‘ç°å…¨é¢çš„ä¸Šä¸‹æ–‡æ•´åˆå¯¹äºæœ‰æ•ˆçš„å…ƒæç¤ºè‡³å…³é‡è¦ï¼Œå¹¶ä¸”ä¸»è¦çš„LLMå¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºå…ƒæç¤ºå™¨ï¼Œä¸ºå·¥ä¸šä»ä¸šè€…æä¾›å¯æ“ä½œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01443v2">PDF</a> Accepted by ASEâ€™25 Industry Showcase</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–ä»£ç ä¼˜åŒ–çš„é—®é¢˜ã€‚é’ˆå¯¹éƒ¨ç½²å¤šä¸ªLLMçš„å·¥ä¸šå¹³å°é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå³é’ˆå¯¹ä¸€ä¸ªLLMä¼˜åŒ–çš„æç¤ºåœ¨å…¶å®ƒLLMä¸Šç»å¸¸å¤±æ•ˆï¼Œéœ€è¦æ˜‚è´µçš„æ¨¡å‹ç‰¹å®šæç¤ºå·¥ç¨‹ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†Meta-Promptedä»£ç ä¼˜åŒ–ï¼ˆMPCOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ä»»åŠ¡ç‰¹å®šæç¤ºï¼Œé€‚ç”¨äºä¸åŒçš„LLMï¼ŒåŒæ—¶æ»¡è¶³å·¥ä¸šæ•ˆç‡è¦æ±‚ã€‚MPCOåˆ©ç”¨å…ƒæç¤ºæŠ€æœ¯ï¼Œé€šè¿‡æ•´åˆé¡¹ç›®å…ƒæ•°æ®ã€ä»»åŠ¡è¦æ±‚å’ŒLLMç‰¹å®šä¸Šä¸‹æ–‡ï¼ŒåŠ¨æ€åˆæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¼˜åŒ–æç¤ºã€‚å®ƒæ˜¯ARTEMISä»£ç ä¼˜åŒ–å¹³å°çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºè‡ªåŠ¨åŒ–éªŒè¯å’Œæ‰©å±•ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œä»£ç åº“ä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒMPCOæ€»ä½“ä¸Šæé«˜äº†é«˜è¾¾19.06%çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰ç³»ç»Ÿä¸­è¡¨ç°æœ€ä½³ã€‚åˆ†æè¡¨æ˜ï¼Œ96%çš„æœ€ä½³ä¼˜åŒ–æ¥è‡ªäºæœ‰æ„ä¹‰çš„ç¼–è¾‘ã€‚é€šè¿‡ç³»ç»Ÿçš„å‰–æç ”ç©¶å’Œå…ƒæç¤ºæ•æ„Ÿæ€§åˆ†æï¼Œå‘ç°å…¨é¢çš„ä¸Šä¸‹æ–‡æ•´åˆå¯¹äºæœ‰æ•ˆçš„å…ƒæç¤ºè‡³å…³é‡è¦ï¼Œä¸»è¦LLMå¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºå…ƒæç¤ºå™¨ï¼Œä¸ºå·¥ä¸šä»ä¸šè€…æä¾›å¯æ“ä½œçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ä¼˜åŒ–ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚</li>
<li>éƒ¨ç½²å¤šä¸ªLLMçš„å·¥ä¸šå¹³å°é¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯æ¨¡å‹é—´çš„æç¤ºå·¥ç¨‹ç“¶é¢ˆã€‚</li>
<li>Meta-Promptedä»£ç ä¼˜åŒ–ï¼ˆMPCOï¼‰æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆè·¨ä¸åŒLLMçš„é«˜è´¨é‡ä»»åŠ¡ç‰¹å®šæç¤ºã€‚</li>
<li>MPCOç»“åˆé¡¹ç›®å…ƒæ•°æ®ã€ä»»åŠ¡è¦æ±‚å’ŒLLMç‰¹å®šä¸Šä¸‹æ–‡æ¥åŠ¨æ€åˆæˆä¼˜åŒ–æç¤ºã€‚</li>
<li>MPCOæ˜¯ARTEMISä»£ç ä¼˜åŒ–å¹³å°çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œèƒ½å¤Ÿæé«˜è‡ªåŠ¨åŒ–éªŒè¯å’Œæ‰©å±•çš„æ•ˆç‡ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œä»£ç åº“ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMPCOç›¸æ¯”åŸºå‡†æ–¹æ³•æé«˜äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a924c29e2fab299369e1d977e2b6d8b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-62974e4e2a709eb28a657328c30f4f67~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942051&auth_key=1759942051-0-0-4cff8eb69de6ceaea9ae5e3d0562d809&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fc6f75c2bcb61bd77315e240b5d2af1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942058&auth_key=1759942058-0-0-340450541458e3088da9fc3a1a5a14b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b8f4e27f6c317c7589547e89b860d7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942065&auth_key=1759942065-0-0-78788138b2bb503f4d1b2fd214b415f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-7000e98e3adb75fe069adcd1326b166b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-69cc2ee04777369d27b14f16789f9b77~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942079&auth_key=1759942079-0-0-0fc1a6730cec1b50cbb3db773377e6b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Highly-Efficient-and-Effective-LLMs-with-Multi-Boolean-Architectures"><a href="#Highly-Efficient-and-Effective-LLMs-with-Multi-Boolean-Architectures" class="headerlink" title="Highly Efficient and Effective LLMs with Multi-Boolean Architectures"></a>Highly Efficient and Effective LLMs with Multi-Boolean Architectures</h2><p><strong>Authors:Ba-Hien Tran, Van Minh Nguyen</strong></p>
<p>Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques. </p>
<blockquote>
<p>æƒé‡äºŒå€¼åŒ–ä½œä¸ºä¸€ç§ç­–ç•¥ï¼Œå·²åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚åº¦é™ä½æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•åˆ†ä¸ºè®­ç»ƒåäºŒå€¼åŒ–ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶ç®€å•ä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸¥é‡æŸå¤±ï¼Œä»¥åŠä¾èµ–äºå…¨ç²¾åº¦æ½œåœ¨æƒé‡çš„è®­ç»ƒæ„ŸçŸ¥æ–¹æ³•ï¼Œè¿™å¢åŠ äº†å¤æ‚æ€§å’Œé™åˆ¶äº†æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¤šæ ¸å¸ƒå°”å‚æ•°è¡¨ç¤ºLLMï¼Œå¹¶é¦–æ¬¡å®ç°äº†å¸ƒå°”åŸŸå†…LLMçš„ç›´æ¥å¾®è°ƒï¼Œæ— éœ€ä½¿ç”¨æ½œåœ¨æƒé‡ã€‚è¿™æé«˜äº†è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶åœ¨å¾®è°ƒæœŸé—´å’Œæ¨ç†æœŸé—´å¤§å¤§é™ä½äº†å¤æ‚åº¦ã€‚åœ¨å¤šç§LLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„è¶…ä½æ¯”ç‰¹é‡åŒ–å’ŒäºŒå€¼åŒ–æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22811v2">PDF</a> Preprint. Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šæ ¸å¸ƒå°”å‚æ•°è¡¨ç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–°å‹æ¡†æ¶ï¼Œå¯ç›´æ¥åœ¨å¸ƒå°”åŸŸè¿›è¡ŒLLMå¾®è°ƒï¼Œæ— éœ€ä½¿ç”¨æ½œåœ¨æƒé‡ï¼Œæé«˜äº†è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶å¤§å¹…é™ä½äº†å¾®è°ƒä¸æ¨ç†æ—¶çš„å¤æ‚åº¦ã€‚æ­¤æ¡†æ¶çªç ´ç°æœ‰è®­ç»ƒåäºŒå…ƒåŒ–ä¸è®­ç»ƒæ„ŸçŸ¥æ–¹æ³•çš„å±€é™ï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºæœ€è¿‘çš„è¶…ä½ç²¾åº¦é‡åŒ–ä¸äºŒå…ƒåŒ–æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ€§å¯é€šè¿‡æƒé‡äºŒå€¼åŒ–ç­–ç•¥é™ä½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒ…æ‹¬è®­ç»ƒåçš„äºŒå€¼åŒ–ï¼Œè™½ç„¶ç®€å•ä½†æ€§èƒ½æŸå¤±ä¸¥é‡ï¼›ä»¥åŠä¾èµ–å…¨ç²¾åº¦æ½œåœ¨æƒé‡çš„è®­ç»ƒæ„ŸçŸ¥æ–¹æ³•ï¼Œå¢åŠ äº†å¤æ‚æ€§å’Œé™åˆ¶äº†æ•ˆç‡ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶é¦–æ¬¡å®ç°äº†åœ¨å¸ƒå°”åŸŸå†…ç›´æ¥å¾®è°ƒLLMï¼Œæ¶ˆé™¤äº†å¯¹æ½œåœ¨æƒé‡çš„éœ€è¦ã€‚</li>
<li>è¯¥æ¡†æ¶æé«˜äº†è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶å¤§å¹…é™ä½äº†å¾®è°ƒä¸æ¨ç†æ—¶çš„å¤æ‚åº¦ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ä¼˜äºç°æœ‰çš„è¶…ä½ç²¾åº¦é‡åŒ–ä¸äºŒå…ƒåŒ–æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨å¤šæ ¸å¸ƒå°”å‚æ•°è¡¨ç¤ºï¼Œå¢å¼ºäº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a6a56bd3c4562642fac4b8ffa33cff1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942087&auth_key=1759942087-0-0-863322f516dc8657dabd4a59268f70f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93979abd28bf2c76d33e7d19117e5f24~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942095&auth_key=1759942095-0-0-ca3b769f47d7700fe5d11583f92f22ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning"><a href="#To-Backtrack-or-Not-to-Backtrack-When-Sequential-Search-Limits-Model-Reasoning" class="headerlink" title="To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning"></a>To Backtrack or Not to Backtrack: When Sequential Search Limits Model   Reasoning</h2><p><strong>Authors:Tian Qin, David Alvarez-Melis, Samy Jelassi, Eran Malach</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test time-compute: parallel sampling with best-of-N selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling-especially under a fixed compute budget-remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models intro suboptimal strategies, and (2) explicit CoT supervision can discourage implicit (non verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ¶‰åŠæœç´¢å’Œå›æº¯çš„æŠ€æœ¯ã€‚å›æº¯é€šè¿‡å¯ç”¨åºåˆ—åŒ–ã€çº¿æ€§åŒ–çš„æ¢ç´¢ï¼ˆé€šè¿‡é•¿é“¾æ¡æ€ç»´ç”Ÿæˆï¼‰ï¼Œè‡ªç„¶åœ°æ‰©å±•äº†æµ‹è¯•æ—¶é—´çš„è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—çš„å”¯ä¸€ç­–ç•¥ï¼šå¹¶è¡Œé‡‡æ ·ä¸æœ€ä½³Né€‰æ‹©ç›¸ç»“åˆï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡åºè´¯æœç´¢çš„é‡‡ç”¨æ—¥ç›Šå¢åŠ ï¼Œä½†å…¶ç›¸å¯¹äºå¹¶è¡Œé‡‡æ ·çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œä»é²œä¸ºäººçŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡CountDownå’Œæ•°ç‹¬ä¸Šç³»ç»Ÿåœ°æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åºè´¯æœç´¢åœ¨CountDownä¸Šçš„è¡¨ç°ä¸å¦‚å¹¶è¡Œé‡‡æ ·ï¼Œä½†åœ¨æ•°ç‹¬ä¸Šçš„è¡¨ç°ä¼˜äºå¹¶è¡Œé‡‡æ ·ï¼Œè¿™è¡¨æ˜å›æº¯å¹¶éæ™®éæœ‰ç›Šã€‚æˆ‘ä»¬ç¡®å®šäº†å¯èƒ½å¯¼è‡´å›æº¯é™ä½æ€§èƒ½çš„ä¸¤å¤§å› ç´ ï¼šï¼ˆ1ï¼‰åœ¨å›ºå®šæœç´¢è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒä¼šä½¿æ¨¡å‹é™·å…¥æ¬¡ä¼˜ç­–ç•¥ï¼›ï¼ˆ2ï¼‰æ˜ç¡®çš„æ€ç»´é“¾ç›‘ç£å¯èƒ½ä¼šæŠ‘åˆ¶éšå«ï¼ˆæœªè¨€è¯­åŒ–ï¼‰çš„æ¨ç†ã€‚æˆ‘ä»¬å°†åˆ†ææ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå‘ç°å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹ä»RLå¾®è°ƒä¸­å—ç›ŠåŒªæµ…ï¼Œè€Œæ²¡æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹åˆ™æ”¶æ•ˆç”šå¾®ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›å‘ç°æŒ‘æˆ˜äº†å›æº¯æ™®éæé«˜LLMæ¨ç†èƒ½åŠ›çš„å‡è®¾ï¼Œåè€Œæ­ç¤ºäº†ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07052v2">PDF</a> COLM 2025 Camera Ready</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸè¿›å±•æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡æœç´¢å’Œå›æº¯æŠ€æœ¯å®ç°ã€‚æœ¬æ–‡é€šè¿‡å¯¹æ¯”é¡ºåºæœç´¢å’Œå¹¶è¡Œé‡‡æ ·ä¸¤ç§æ–¹æ³•ï¼Œå‘ç°å…¶åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚é¡ºåºæœç´¢åœ¨è§£å†³æ•°ç‹¬é—®é¢˜æ—¶è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å€’è®¡æ—¶ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒå›ºå®šæœç´¢è½¨è¿¹å’Œæ˜¾å¼æ€ç»´ç›‘ç£å¯èƒ½å¯¼è‡´å›æº¯æ€§èƒ½ä¸‹é™ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒå¯¹å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹æœ‰å¾ˆå¤§ç›Šå¤„ï¼Œè€Œå¯¹æ— å›æº¯æ¨¡å‹åˆ™æ•ˆæœæœ‰é™ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å›æº¯æ™®éæå‡LLMæ¨ç†èƒ½åŠ›çš„å‡è®¾ï¼Œæ­ç¤ºäº†ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡æœç´¢å’Œå›æº¯æŠ€æœ¯å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>é¡ºåºæœç´¢å’Œå¹¶è¡Œé‡‡æ ·åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œå¹¶æ— æ™®éä¼˜åŠ¿ã€‚</li>
<li>è®­ç»ƒå›ºå®šæœç´¢è½¨è¿¹å’Œæ˜¾å¼æ€ç»´ç›‘ç£å¯èƒ½å¯¼è‡´å›æº¯æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒå¯¹å…·æœ‰å›æº¯èƒ½åŠ›çš„æ¨¡å‹æœ‰å¾ˆå¤§ç›Šå¤„ã€‚</li>
<li>ä»»åŠ¡ç»“æ„ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è§„æ¨¡å’Œå­¦ä¹ èŒƒå¼ä¹‹é—´äº¤äº’å½±å“LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¡ºåºæœç´¢åœ¨è§£å†³æ•°ç‹¬é—®é¢˜æ—¶è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å€’è®¡æ—¶ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9962eccf4c32fd86f63381d6b104c2b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942102&auth_key=1759942102-0-0-900e564e8c22dad712c744db2dcba4d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-52252374602a9cfa3a9231203049348a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c72d4ddc3f55d24aaf9cd2de04fc74a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942116&auth_key=1759942116-0-0-179815929d1654ba7a0ef1fe8647d7af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-642b9dc5483a0af8acd3c1b68ef3de7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942123&auth_key=1759942123-0-0-2c4f0cbbd3dc84fc3e3dc9d61cb2a2f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLAMAFUZZ-Large-Language-Model-Enhanced-Greybox-Fuzzing"><a href="#LLAMAFUZZ-Large-Language-Model-Enhanced-Greybox-Fuzzing" class="headerlink" title="LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing"></a>LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing</h2><p><strong>Authors:Hongxiang Zhang, Yuyang Rong, Yifeng He, Hao Chen</strong></p>
<p>Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in programs. However, randomized mutation strategies have limited the fuzzerâ€™s performance on structured data. Specialized fuzzers can handle complex structured data, but require additional efforts in grammar and suffer from low throughput.   In this paper, we explore the potential of utilizing the Large Language Model to enhance greybox fuzzing for structured data. We utilize the pre-trained knowledge of LLM about data conversion and format to generate new valid inputs. We further fine-tuned it with paired mutation seeds to learn structured format and mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ, integrates the power of LLM to understand and mutate structured data to fuzzing. We conduct experiments on the standard bug-based benchmark Magma and a wide variety of real-world programs. LLAMAFUZZ outperforms our top competitor by 41 bugs on average. We also identified 47 unique bugs across all trials. Moreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and bug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in real-world program sets on average. We also demonstrate a case study to explain how LLMs enhance the fuzzing process in terms of code coverage. </p>
<blockquote>
<p>ç°ç›’æ¨¡ç³Šæµ‹è¯•åœ¨æ­ç¤ºç¨‹åºå’Œæ¼æ´ä¸­çš„é”™è¯¯æ–¹é¢å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œéšæœºçªå˜ç­–ç•¥åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šé™åˆ¶äº†æ¨¡ç³Šæµ‹è¯•çš„æ€§èƒ½ã€‚ä¸“ä¸šæ¨¡ç³Šæµ‹è¯•å™¨å¯ä»¥å¤„ç†å¤æ‚ç»“æ„åŒ–æ•°æ®ï¼Œä½†éœ€è¦é¢å¤–çš„è¯­æ³•åŠªåŠ›ï¼Œå¹¶ä¸”ååé‡è¾ƒä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡ç»“æ„åŒ–æ•°æ®çš„ç°ç›’æ¨¡ç³Šæµ‹è¯•æ½œåŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨LLMå…³äºæ•°æ®è½¬æ¢å’Œæ ¼å¼çš„é¢„è®­ç»ƒçŸ¥è¯†æ¥ç”Ÿæˆæ–°çš„æœ‰æ•ˆè¾“å…¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨é…å¯¹çªå˜ç§å­å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥æœ‰æ•ˆå­¦ä¹ ç»“æ„åŒ–æ ¼å¼å’Œçªå˜ç­–ç•¥ã€‚æˆ‘ä»¬åŸºäºLLMçš„æ¨¡ç³Šæµ‹è¯•å™¨LLAMAFUZZç»“åˆäº†LLMç†è§£å’Œçªå˜ç»“æ„åŒ–æ•°æ®çš„èƒ½åŠ›æ¥è¿›è¡Œæ¨¡ç³Šæµ‹è¯•ã€‚æˆ‘ä»¬åœ¨åŸºäºé”™è¯¯çš„åŸºå‡†æµ‹è¯•Magmaå’Œå„ç§å®é™…ç¨‹åºä¸Šè¿›è¡Œäº†å®éªŒã€‚LLAMAFUZZå¹³å‡æ¯”æˆ‘ä»¬çš„é¡¶çº§ç«äº‰å¯¹æ‰‹å¤šå‘ç°äº†41ä¸ªé”™è¯¯ã€‚æˆ‘ä»¬è¿˜è¯†åˆ«å‡ºäº†æ‰€æœ‰è¯•éªŒä¸­çš„47ä¸ªç‹¬ç‰¹é”™è¯¯ã€‚æ­¤å¤–ï¼ŒLLAMAFUZZåœ¨è§¦å‘é”™è¯¯å’Œè¾¾åˆ°é”™è¯¯æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ€§èƒ½ã€‚ä¸AFL++ç›¸æ¯”ï¼ŒLLAMAFUZZåœ¨çœŸå®ç¨‹åºé›†ä¸Šçš„å¹³å‡åˆ†æ”¯è¦†ç›–ç‡æé«˜äº†27.19%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶æ¥è§£é‡ŠLLMå¦‚ä½•æå‡æ¨¡ç³Šæµ‹è¯•è¿‡ç¨‹ä¸­çš„ä»£ç è¦†ç›–ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07714v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºç°ç›’æ¨¡ç³Šæµ‹è¯•åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šçš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡Œæ•°æ®è½¬æ¢å’Œæ ¼å¼ç”Ÿæˆæ–°çš„æœ‰æ•ˆè¾“å…¥ï¼Œå¹¶é€šè¿‡é…å¯¹çªå˜ç§å­è¿›è¡Œå¾®è°ƒï¼Œå­¦ä¹ ç»“æ„åŒ–æ ¼å¼å’Œçªå˜ç­–ç•¥ã€‚åŸºäºLLMçš„æ¨¡ç³Šæµ‹è¯•å™¨LLAMAFUZZç»“åˆäº†LLMç†è§£å’Œçªå˜ç»“æ„åŒ–æ•°æ®çš„èƒ½åŠ›ï¼Œå¯¹æ ‡å‡†åŸºå‡†Magmaå’Œå„ç§çœŸå®ç¨‹åºè¿›è¡Œäº†å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLAMAFUZZå¹³å‡æ¯”é¡¶çº§ç«äº‰å¯¹æ‰‹å¤šå‘ç°41ä¸ªbugï¼Œåœ¨æ‰€æœ‰è¯•éªŒä¸­å‘ç°äº†47ä¸ªç‹¬ç‰¹bugã€‚æ­¤å¤–ï¼ŒLLAMAFUZZåœ¨bugè§¦å‘å’Œbugæ£€æµ‹æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ€§èƒ½ã€‚ç›¸è¾ƒäºAFL++ï¼ŒLLAMAFUZZåœ¨çœŸå®ç¨‹åºé›†ä¸Šçš„å¹³å‡åˆ†æ”¯è¦†ç›–ç‡æé«˜äº†27.19%ã€‚åŒæ—¶ï¼Œé€šè¿‡æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†LLMå¦‚ä½•å¢å¼ºæ¨¡ç³Šæµ‹è¯•è¿‡ç¨‹ä¸­çš„ä»£ç è¦†ç›–ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¢«ç”¨äºå¢å¼ºç°ç›’æ¨¡ç³Šæµ‹è¯•åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>LLAMAFUZZåˆ©ç”¨LLMçš„é¢„è®­ç»ƒçŸ¥è¯†ç”Ÿæˆæ–°çš„æœ‰æ•ˆè¾“å…¥å¹¶å­¦ä¹ ç»“æ„åŒ–æ•°æ®å’Œçªå˜ç­–ç•¥ã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†Magmaå’ŒçœŸå®ç¨‹åºä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒLLAMAFUZZæ¯”é¡¶çº§ç«äº‰å¯¹æ‰‹æ›´ä¼˜ç§€ã€‚</li>
<li>LLAMAFUZZå‘ç°äº†æ›´å¤šçš„ç‹¬ç‰¹bugï¼Œå¹¶ä¸”åœ¨bugè§¦å‘å’Œæ£€æµ‹æ–¹é¢è¡¨ç°ç¨³å®šã€‚</li>
<li>ä¸AFL++ç›¸æ¯”ï¼ŒLLAMAFUZZåœ¨çœŸå®ç¨‹åºé›†ä¸Šçš„å¹³å‡åˆ†æ”¯è¦†ç›–ç‡æœ‰æ‰€æé«˜ã€‚</li>
<li>LLAMAFUZZé€šè¿‡ç»“åˆLLMçš„èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡ç³Šæµ‹è¯•è¿‡ç¨‹ä¸­çš„ä»£ç è¦†ç›–ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0c85c8349b40c3d1225d2e4b0024a4fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942131&auth_key=1759942131-0-0-9ea666f14d4419947badac9e8684b725&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-70a7275acd60da698ab0933828002de3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f166c761e3bcc2556bc2de57371303c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942144&auth_key=1759942144-0-0-ce8528ed40f3cee09811fb7f91a92cc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-bcad7992bba636c9d953285c25daa8b8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-82dcc7c751f2318e4e9174ab20a262ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942157&auth_key=1759942157-0-0-21004d7d11bde1f221b809cb6b1aa421&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a5b989ff71c1ec9aaf4c8f59d987f5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942164&auth_key=1759942164-0-0-7db4ac93cf27dc5dd3afbadecddbc064&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-How-CodeLLMs-Mis-Predict-Types-with-Activation-Steering"><a href="#Understanding-How-CodeLLMs-Mis-Predict-Types-with-Activation-Steering" class="headerlink" title="Understanding How CodeLLMs (Mis)Predict Types with Activation Steering"></a>Understanding How CodeLLMs (Mis)Predict Types with Activation Steering</h2><p><strong>Authors:Francesca Lucchetti, Arjun Guha</strong></p>
<p>Large Language Models (LLMs) are widely used by software engineers for programming tasks. However, research shows that LLMs often lack a deep understanding of program semantics. Even minor changes to syntax, such as renaming variables, can significantly degrade performance across various tasks. In this work, we examine the task of type prediction: given a partially typed program, can a model predict a missing type annotations such that the resulting program is more typed? We construct a dataset of adversarial examples where models initially predict the correct types, but begin to fail after semantically irrelevant edits. This is problematic, as models should ideally generalize across different syntactic forms of semantically equivalent code. This lack of robustness suggests that models may have a shallow understanding of code semantics. Despite this, we provide evidence that LLMs do, in fact, learn robust mechanisms for type prediction-though these mechanisms often fail to activate in adversarial scenarios. By using activation steering, a method that manipulates a modelâ€™s internal activations to guide it toward using latent knowledge, we restore accurate predictions on adversarial inputs. We show that steering successfully activates a type prediction mechanism that is shared by both Python and TypeScript, and is more effective than prompting with in-context examples. Across five different models, our comprehensive evaluation demonstrates that LLMs can learn generalizable representations of code semantics that transfer across programming languages. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«è½¯ä»¶å·¥ç¨‹å¸ˆå¹¿æ³›åº”ç”¨äºç¼–ç¨‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼ŒLLMé€šå¸¸ç¼ºä¹å¯¹ç¨‹åºè¯­ä¹‰çš„æ·±å…¥ç†è§£ã€‚å³ä½¿æ˜¯å¯¹è¯­æ³•çš„å¾®å°æ›´æ”¹ï¼Œå¦‚å˜é‡é‡å‘½åï¼Œä¹Ÿå¯èƒ½åœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç±»å‹é¢„æµ‹ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªéƒ¨åˆ†ç±»å‹çš„ç¨‹åºï¼Œæ¨¡å‹èƒ½å¦é¢„æµ‹ç¼ºå¤±çš„ç±»å‹æ³¨é‡Šï¼Œä»¥ä½¿ç¨‹åºæ›´å…·ç±»å‹åŒ–ï¼Ÿæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯¹æŠ—æ€§ç¤ºä¾‹æ•°æ®é›†ï¼Œæ¨¡å‹æœ€åˆé¢„æµ‹çš„ç±»å‹æ˜¯æ­£ç¡®çš„ï¼Œä½†åœ¨è¯­ä¹‰æ— å…³ç¼–è¾‘åå¼€å§‹å¤±è´¥ã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹ç†æƒ³æƒ…å†µä¸‹åº”è¯¥èƒ½å¤Ÿæ¦‚æ‹¬è¯­ä¹‰ç­‰æ•ˆä»£ç çš„ä¸åŒè¯­æ³•å½¢å¼ã€‚è¿™ç§ç¨³å¥æ€§çš„ç¼ºä¹è¡¨æ˜ï¼Œæ¨¡å‹å¯èƒ½å¯¹ä»£ç è¯­ä¹‰çš„ç†è§£å¾ˆæµ…è–„ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†è¯æ®è¡¨æ˜LLMå®é™…ä¸Šå­¦ä¹ äº†ç”¨äºç±»å‹é¢„æµ‹çš„ç¨³å®šæœºåˆ¶â€”â€”å°½ç®¡è¿™äº›æœºåˆ¶åœ¨å¯¹æŠ—åœºæ™¯ä¸­å¾€å¾€æ— æ³•æ¿€æ´»ã€‚é€šè¿‡ä½¿ç”¨æ¿€æ´»è½¬å‘ï¼ˆä¸€ç§é€šè¿‡æ“çºµæ¨¡å‹çš„å†…éƒ¨æ¿€æ´»æ¥å¼•å¯¼å…¶ä½¿ç”¨æ½œåœ¨çŸ¥è¯†çš„æ–¹æ³•ï¼‰ï¼Œæˆ‘ä»¬æ¢å¤äº†å¯¹æŠ—æ€§è¾“å…¥ä¸Šçš„å‡†ç¡®é¢„æµ‹ã€‚æˆ‘ä»¬è¯æ˜äº†è½¬å‘æˆåŠŸåœ°æ¿€æ´»äº†Pythonå’ŒTypeScriptå…±æœ‰çš„ç±»å‹é¢„æµ‹æœºåˆ¶ï¼Œå¹¶ä¸”æ¯”ä½¿ç”¨ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œæç¤ºæ›´ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬åœ¨äº”ä¸ªä¸åŒæ¨¡å‹ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒLLMå¯ä»¥å­¦ä¹ å¯æ¦‚æ‹¬çš„ä»£ç è¯­ä¹‰è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå¯ä»¥è·¨ç¼–ç¨‹è¯­è¨€è¿›è¡Œè¿ç§»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01903v3">PDF</a> 40 pages, 67 figures. To be published at BlackBoxNLP 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†å¯¹ç¨‹åºè¯­ä¹‰çš„ç†è§£å¾€å¾€ä¸å¤Ÿæ·±å…¥ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå³ä½¿è¯­æ³•ä¸Šçš„å¾®å°å˜åŒ–ï¼Œå¦‚å˜é‡é‡å‘½åï¼Œä¹Ÿå¯èƒ½å¯¹æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç±»å‹é¢„æµ‹é—®é¢˜ï¼šç»™å®šéƒ¨åˆ†ç±»å‹çš„ç¨‹åºï¼Œæ¨¡å‹èƒ½å¦é¢„æµ‹ç¼ºå¤±çš„ç±»å‹æ³¨é‡Šä»¥ä½¿ç¨‹åºæ›´åŠ å®Œæ•´ï¼Ÿç ”ç©¶å‘ç°ï¼Œå°½ç®¡æ¨¡å‹æœ€åˆèƒ½æ­£ç¡®é¢„æµ‹ç±»å‹ï¼Œä½†åœ¨è¯­ä¹‰æ— å…³ç¼–è¾‘åå®¹æ˜“å‡ºé”™ã€‚è™½ç„¶æ¨¡å‹ç¼ºä¹ç¨³å¥æ€§è¡¨æ˜å®ƒä»¬å¯¹ä»£ç è¯­ä¹‰çš„ç†è§£å¯èƒ½å¾ˆæµ…è–„ï¼Œä½†è¯æ®è¡¨æ˜LLMå®é™…ä¸Šå­¦ä¹ äº†ç”¨äºç±»å‹é¢„æµ‹çš„ç¨³å¥æœºåˆ¶ã€‚é€šè¿‡ä½¿ç”¨æ¿€æ´»æ§åˆ¶æ³•æ¿€æ´»æ¨¡å‹å†…éƒ¨çš„æ½œåœ¨çŸ¥è¯†å¯¼å‘æœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤ŸæˆåŠŸæ¢å¤å¯¹æ•Œå¯¹è¾“å…¥çš„å‡†ç¡®é¢„æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ§åˆ¶æ³•æ¿€æ´»çš„ç±»å‹é¢„æµ‹æœºåˆ¶åœ¨Pythonå’ŒTypeScriptä¸­éƒ½æœ‰å…±äº«ï¼Œä¸”æ¯”ä¸Šä¸‹æ–‡ç¤ºä¾‹æç¤ºæ›´ä¸ºæœ‰æ•ˆã€‚å…¨é¢è¯„ä¼°æ˜¾ç¤ºLLMèƒ½å¤Ÿå­¦ä¹ è·¨ç¼–ç¨‹è¯­è¨€çš„ä»£ç è¯­ä¹‰çš„ä¸€èˆ¬è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¹¿æ³›åº”ç”¨äºç¼–ç¨‹ä»»åŠ¡ï¼Œä½†å¯¹ç¨‹åºè¯­ä¹‰çš„ç†è§£æœ‰é™ã€‚</li>
<li>å¾®å°è¯­æ³•å˜åŒ–å¯èƒ½å¯¼è‡´LLMsåœ¨ç±»å‹é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æ¨¡å‹åœ¨è¯­ä¹‰æ— å…³ç¼–è¾‘åå®¹æ˜“åœ¨ç±»å‹é¢„æµ‹ä»»åŠ¡ä¸Šå‡ºé”™ï¼Œè¡¨æ˜å…¶ç¼ºä¹ç¨³å¥æ€§ã€‚</li>
<li>LLMså®é™…ä¸Šå­¦ä¹ äº†ç”¨äºç±»å‹é¢„æµ‹çš„ç¨³å¥æœºåˆ¶ï¼Œä½†è¿™äº›æœºåˆ¶åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹å¯èƒ½æ— æ³•æ¿€æ´»ã€‚</li>
<li>é€šè¿‡æ¿€æ´»æ§åˆ¶æ³•ï¼Œå¯ä»¥æˆåŠŸæ¢å¤LLMå¯¹æ•Œå¯¹è¾“å…¥çš„å‡†ç¡®ç±»å‹é¢„æµ‹ã€‚</li>
<li>æ§åˆ¶æ³•æ¿€æ´»çš„ç±»å‹é¢„æµ‹æœºåˆ¶åœ¨Pythonå’ŒTypeScriptä¸­å…·æœ‰å…±äº«æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.01903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-22dce702642a369e8c4a9950fc464191~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942172&auth_key=1759942172-0-0-a31d198c8b6182c09588bc097da93ffd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-738561eca57f5484b794ff46fd5b5860.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-846d65bf7db4eefd8ab33b55476952fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942186&auth_key=1759942186-0-0-54c309626e7598bd1d7c1f244c749d5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-edc21980ae5bba5cc5ea5f442de9e6d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759942193&auth_key=1759942193-0-0-e26f9efc69c20f29e0f00a0249f71bcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  UniShield An Adaptive Multi-Agent Framework for Unified Forgery Image   Detection and Localization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ae2ebde980e7552d108d63e3a41d704f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759941177&auth_key=1759941177-0-0-24735fb633f4732c73b466185ff34858&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-07  Self-Anchor Large Language Model Reasoning via Step-by-step Attention   Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
