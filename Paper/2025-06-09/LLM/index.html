<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-09  LLaDA-V Large Language Diffusion Models with Visual Instruction Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    55 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-09-更新"><a href="#2025-06-09-更新" class="headerlink" title="2025-06-09 更新"></a>2025-06-09 更新</h1><h2 id="LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning"><a href="#LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning" class="headerlink" title="LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning"></a>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</h2><p><strong>Authors:Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/">https://ml-gsai.github.io/LLaDA-V-demo/</a>. </p>
<blockquote>
<p>在这项工作中，我们介绍了LLaDA-V，这是一个完全基于扩散的多模态大型语言模型（MLLM），它将视觉指令调整与掩码扩散模型相结合，打破了当前多模态方法中占主导地位的自回归范式的限制。LLaDA-V建立在LLaDA（一个代表性的大型语言扩散模型）的基础上，融入了视觉编码器和MLP连接器，将视觉特征投射到语言嵌入空间，实现了有效的多模态对齐。我们的实证研究揭示了几个有趣的结果：首先，尽管LLaDA-V在纯文本任务上的语言模型表现较弱，与LLaMA3-8B和Qwen2-7B等相比稍显逊色，但在多模态任务中却展现出有前景的多模态表现。当使用相同的指令数据进行训练时，LLaDA-V在多模态任务方面与LLaMA3-V极具竞争力，并且具有更好的数据可扩展性。它还缩小了与Qwen2-VL的性能差距，证明了其架构在多模态任务中的有效性。其次，与现有的混合自回归扩散和纯扩散的MLLM相比，LLaDA-V在多模态理解方面达到了最先进的性能。我们的研究结果表明，大型语言扩散模型在多模态环境中显示出潜力，值得未来进一步研究。项目页面和代码：<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%E3%80%82">https://ml-gsai.github.io/LLaDA-V-demo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16933v2">PDF</a> Project page and codes: \url{<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%7D">https://ml-gsai.github.io/LLaDA-V-demo/}</a></p>
<p><strong>Summary</strong></p>
<p>LLaDA-V是一种基于扩散的多模态大型语言模型，它通过视觉指令调整和掩膜扩散模型集成了视觉功能，突破了当前主流的自回归范式限制。在语言和视觉融合方面展现出强大的性能，特别是在多模态任务中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-V是基于LLaDA的大型语言扩散模型的扩展，通过集成视觉编码器和MLP连接器，实现了视觉特征到语言嵌入空间的映射，实现了有效的多模态对齐。</li>
<li>LLaDA-V在多模态任务中表现出良好的性能，即使其纯文本任务表现较弱的LLaDA模型也能与LLaMA3-V等模型竞争。</li>
<li>LLaDA-V缩小了与Qwen2-VL的性能差距，验证了其架构在多模态任务中的有效性。</li>
<li>LLaDA-V达到了多模态理解的最先进水平，与现有的混合自回归-扩散和纯扩散多模态语言模型相比具有优势。</li>
<li>大型语言扩散模型在多模态上下文中显示出潜力，并值得未来进一步研究。</li>
<li>LLaDA-V的演示和项目页面可以在<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%E6%89%BE%E5%88%B0%E3%80%82">https://ml-gsai.github.io/LLaDA-V-demo/找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16933">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-844d033035b69014d28ceeb951f9e2e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02257baa9bf994de14ed7d0d038e54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed659bfa20dbe4dba03864406d86a419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fdf1cba7ea415778817b386220d68cc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible “dark patterns” in LLMs’ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local “safety regions” in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts–a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>大型语言模型（LLM）是人工智能通用化的基础探索，然而，通过指令调整和偏好学习使其与人类价值观相符只实现了表面的合规性。在这里，我们证明预训练期间嵌入的有害知识会作为LLM参数记忆中的不可磨灭的“暗模式”持续存在，这些暗模式逃避了对齐保障措施，并在分布转移的情况下受到对抗性诱导时重新出现。在本研究中，我们首先从理论上分析了对齐LLM的内在道德脆弱性，证明当前的对齐方法只能在知识流形中产生局部的“安全区域”。相反，预训练知识仍然与有害概念通过高概率对抗轨迹全局连接。基于这一理论见解，我们通过分布转移下的语义连贯性诱导来实证验证我们的发现——这是一种通过优化对抗性提示来系统地绕过对齐约束的方法。这种结合理论和实证的方法在23个最先进对齐LLM中的19个上实现了100%的攻击成功率，包括DeepSeek-R1和LLaMA-3，揭示了它们的普遍脆弱性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）是人工智能通用探索的基础，但它们与人类价值观的对齐仅限于表面。研究发现，预训练时嵌入的有害知识会作为不可磨灭的“暗模式”存在于模型的参数记忆中，逃避对齐保障措施并在分布转移时重新出现。本文首先从理论上分析了对齐LLM的内在道德脆弱性，证明了当前的对齐方法仅在知识流形中产生局部“安全区域”，而预训练的知识仍然与有害概念在全球范围内存在联系。实证上，通过分布转移下的语义连贯性诱导方法验证了我们的发现，这种方法通过优化对抗性提示来系统地绕过对齐约束。这种结合理论和实证的方法在23个最新对齐的大型语言模型中成功攻击了其中的19个，包括DeepSeek-R1和LLaMA-3，揭示了它们的普遍脆弱性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在人工智能通用探索中占据重要地位，但它们在与人类价值观对齐方面存在局限性。</li>
<li>预训练期间嵌入的有害知识会在模型中以“暗模式”形式存在，逃避对齐措施并在特定情境下重新显现。</li>
<li>当前的对齐方法仅在知识流形中产生局部“安全区域”，预训练的知识与有害概念之间仍存在全球联系。</li>
<li>通过分布转移下的语义连贯性诱导方法，可以系统地绕过模型的对齐约束。</li>
<li>这种结合理论和实证的方法成功攻击了大多数最新对齐的大型语言模型。</li>
<li>揭示了对齐LLM的内在道德脆弱性，需要更深入的研究和更完善的措施来确保模型的安全性和道德合规性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2261b3c61e41e842523c37d9e4eed2be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-146301b6ccdc9ad94e03dd2592915f33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8894cc97e6192748906cfca91b3ed446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92236c32efe5492d12100a5b2257db5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations"><a href="#VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations" class="headerlink" title="VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations"></a>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations</h2><p><strong>Authors:Zhongchun Zheng, Kan Wu, Long Cheng, Lu Li, Rodrigo C. O. Rocha, Tianyi Liu, Wei Wei, Jianjiang Zeng, Xianwei Zhang, Yaoqing Gao</strong></p>
<p>Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage. </p>
<blockquote>
<p>自动矢量化是现代编译器利用SIMD并行性的基本优化。然而，最先进的技术在处理复杂的代码模式时仍面临困难，通常需要手动提示或特定领域的专业知识。大型语言模型（LLM）具有捕获复杂模式的能力，提供了有前景的解决方案，但它们在编译器优化中的有效应用仍面临开放挑战，例如虚构问题和缺乏特定领域的推理。在本文中，我们提出了VecTrans，一个利用LLM增强基于编译器的代码矢量化的新型框架。VecTrans首先使用编译器分析来识别可能可向量化的代码区域。然后，它利用LLM将这些区域重构为更易于编译器自动矢量化的模式。为了确保语义正确性，VecTrans进一步在中间表示（IR）级别集成了混合验证机制。通过以上努力，VecTrans结合了LLM的适应性和编译器矢量化的精确性，从而有效地打开了矢量化机会。实验结果表明，在所有不可由GCC、ICC、Clang和BiSheng编译器进行矢量化的TSVC函数中，VecTrans实现了1.7 无需修改原有代码就能提高运行速度的几何平均速度提升，并成功对其中测试案例中的51个中的24个进行了矢量化处理。这标志着在保持成本效益的同时，相较于最先进的技术取得了重大进展，每次函数优化的LLM API使用成本为0.012美元。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19449v3">PDF</a> </p>
<p><strong>Summary</strong><br>在现代编译器中，自动矢量化是一个基本的优化技术以利用SIMD并行性。然而，最新的方法在处理复杂的代码模式时仍面临挑战，需要手动提示或特定领域的专业知识。大型语言模型（LLM）能够捕捉复杂的模式，提供了一个有前景的解决方案。然而，它们在编译器优化中的应用仍然是一个开放性的挑战，因为存在诸如幻想和缺乏领域特定推理等问题。本文提出了VecTrans框架，利用LLM增强基于编译器的代码矢量化。VecTrans首先使用编译器分析来识别可能矢量化的代码区域，然后使用LLM对这些区域进行重构，以便更易于编译器自动矢量化。为确保语义正确性，VecTrans进一步在中间表示（IR）级别整合了混合验证机制。通过结合LLM的适应性和编译器矢量化的精确性，VecTrans有效地开启了矢量化机会。实验结果表明，在GCC、ICC、Clang和BiSheng编译器无法矢量化的所有TSVC函数中，VecTrans实现了平均加速比1.77倍，成功矢量化了24个测试案例中的51个。这标志着与最新方法相比的重大进展，同时保持每次函数优化的成本效益为$0.012。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动矢量化是编译器优化的核心技术，但仍面临处理复杂代码模式的挑战。</li>
<li>大型语言模型（LLM）具有捕捉复杂模式的能力，为编译器优化提供了有前景的解决方案。</li>
<li>VecTrans框架结合了LLM和编译器优化的优势，通过编译器分析识别矢量化的潜力区域并利用LLM进行重构。</li>
<li>VecTrans采用混合验证机制确保语义正确性。</li>
<li>实验结果表明VecTrans在多种编译器无法矢量化的函数中实现了显著的加速效果。</li>
<li>VecTrans的成功实现了在保持较高性能的同时，每次函数优化的成本效益较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8f6a1bd0e4443c6cd6bfa51edd23722b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7775444ecbce5e131129bbff6a5401c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7380c7c050b27627e9f76f903025e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-183e6d73d4cf40bbfcfce4fc2d227003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8826d3138ffef65662fb28fcd86eb4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b7154d2425688685afe2923c7625a9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Human-Production-Interpretation-Asymmetries-to-Test-LLM-Cognitive-Plausibility"><a href="#Leveraging-Human-Production-Interpretation-Asymmetries-to-Test-LLM-Cognitive-Plausibility" class="headerlink" title="Leveraging Human Production-Interpretation Asymmetries to Test LLM   Cognitive Plausibility"></a>Leveraging Human Production-Interpretation Asymmetries to Test LLM   Cognitive Plausibility</h2><p><strong>Authors:Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt</strong></p>
<p>Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at <a target="_blank" rel="noopener" href="https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025">https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025</a>. </p>
<blockquote>
<p>关于大型语言模型（LLM）在处理语言时是否与人类有类似的过程一直是理论界和实践界热议的话题。我们通过人类句子处理中发现的生成与解读的差异性来审视这个问题，并评估指令优化的LLM在这一区别上的复制程度。我们利用人类隐式因果动词在代词生成与解读之间存在的实证记录的不对称性作为测试平台，发现一些LLM在生成和解读之间存在定量和定性的与人类类似的不对称性。我们证明这种行为的存在既取决于模型的大小——较大的模型更可能反映出人类模式，也取决于用于激发行为的元语言提示的选择。我们的代码和结果可在 <a target="_blank" rel="noopener" href="https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025">https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17579v2">PDF</a> ACL 2025 Camera-ready</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理语言时是否与人类有相似的处理方式一直是理论界和实践界热议的话题。本研究通过人类句子处理中的生产-解释差异来探讨这一问题，并评估指令调整后的LLM对这种差异的复制程度。研究利用人类在使用隐性因果动词时的代词生产和解释之间的不对称性作为测试平台，发现部分LLM在生产和解释之间能够反映类似人类的不对称性。研究表明，这种行为是否持续取决于模型的大小（大型模型更可能反映类似人类模式）以及用于激发行为的元语言提示的选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在处理语言时是否模仿人类方式引发了广泛讨论。</li>
<li>本研究通过人类句子处理中的生产-解释差异来探索LLM与人类在语言处理上的相似性。</li>
<li>研究发现，部分LLM能够反映人类在使用隐性因果动词时的代词生产和解释之间的不对称性。</li>
<li>LLM的行为表现取决于模型的大小，大型模型更可能展示类似人类的模式。</li>
<li>元语言提示的选择在激发LLM行为表现上起到重要作用。</li>
<li>本研究的代码和结果已公开发布，供进一步研究和参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd09a1c87f177394b5ece9cbe72c654a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9e25fd3ba24d340b7685f915bc8ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87ff30618a8d5cabf0550da2b2cce486.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Benign-import-Toxic-Jailbreaking-the-Language-Model-via-Adversarial-Metaphors"><a href="#From-Benign-import-Toxic-Jailbreaking-the-Language-Model-via-Adversarial-Metaphors" class="headerlink" title="From Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors"></a>From Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors</h2><p><strong>Authors:Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li</strong></p>
<p>Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. </p>
<blockquote>
<p>当前的研究已经揭示了大型语言模型（LLM）通过越狱攻击生成有害内容的潜在风险。然而，他们忽略了从零开始直接生成有害内容比诱导LLM将良性内容调整为有害形式更为困难。在我们的研究中，我们介绍了一种新的攻击框架，它利用AdVersArial meTAphoR（AVATAR）来诱导LLM调整恶意隐喻以进行越狱。具体来说，为了回答有害查询，AVATAR会自适应地识别一组良性但逻辑相关的隐喻作为初始种子。然后，在这些隐喻的驱动下，目标LLM被诱导对隐喻内容进行推理和调整，从而通过直接输出有害响应或调整隐喻与专业有害内容之间的残差来实现越狱。实验结果表明，AVATAR可以有效地转移LLM的注意力并成功实现越狱，且在多个高级LLM上达到了最先进的攻击成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00038v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2412.12145</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）存在生成有害内容的隐患，新研究揭示了一种利用AdVersArial meTAphoR（AVATAR）框架诱导LLM生成有害隐喻的突破方式。通过引入隐喻作为初始种子，诱导LLM推理和校准隐喻内容，能够输出有害响应或将隐喻与专业有害内容相结合实现突破。实验表明，AVATAR框架能有效且灵活地对多个高级LLM进行突破，达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM存在生成有害内容的隐患。</li>
<li>新研究利用AVATAR框架进行突破。</li>
<li>AVATAR框架引入隐喻作为初始种子，用于诱导LLM生成有害内容。</li>
<li>LLM能被诱导输出有害响应或结合隐喻与专业有害内容实现突破。</li>
<li>实验表明AVATAR框架能有效突破多个高级LLM。</li>
<li>AVATAR框架攻击成功率高，具有先进性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-300fe047a149c80df4a4b43f750b8973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156fef8506c484004d5f4b4ca8973d82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4145720bac8e152a544cd8a20b955f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07ea07293fb01a6fa76465cdd366ec9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbe7927a628de76e7c9a181c23b2fc25.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Finite-State-Automata-Inside-Transformers-with-Chain-of-Thought-A-Mechanistic-Study-on-State-Tracking"><a href="#Finite-State-Automata-Inside-Transformers-with-Chain-of-Thought-A-Mechanistic-Study-on-State-Tracking" class="headerlink" title="Finite State Automata Inside Transformers with Chain-of-Thought: A   Mechanistic Study on State Tracking"></a>Finite State Automata Inside Transformers with Chain-of-Thought: A   Mechanistic Study on State Tracking</h2><p><strong>Authors:Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin</strong></p>
<p>Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IvanChangPKU/FSA">https://github.com/IvanChangPKU/FSA</a>. </p>
<blockquote>
<p>思维链（CoT）显著提高了大型语言模型（LLM）在广泛任务上的性能，并且先前的研究表明，CoT在理论上可以提高表达能力。然而，对于Transformer+CoT可以学习的算法机制理解有限。我们的主要贡献是：（1）我们评估了Transformer+CoT及其变种的状态跟踪能力，证实了CoT的有效性。（2）接下来，我们确定了电路（模型组件的一个子集，负责跟踪世界状态），表明后期层MLP神经元起着关键作用。我们提出了压缩和区分两个指标，并显示每个状态的神经元集几乎达到了100%的准确率，这提供了模型中嵌入的隐式有限状态自动机（FSA）的证据。（3）此外，我们探索了三个具有挑战性的设置：跳过中间步骤、引入数据噪声和测试长度泛化。我们的结果表明，Transformer+CoT学习的是稳健的算法（FSAs），突出其在具有挑战的场景中的应变能力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/IvanChangPKU/FSA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IvanChangPKU/FSA上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20129v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Chain-of-thought（CoT）的大型语言模型（LLM）性能显著提升，并在多种任务中展现出优势。研究证实CoT能增强模型的表达能力。本文的主要贡献包括：评估了Transformer+CoT及其变种的状态跟踪能力；识别了负责跟踪世界状态的电路，发现晚期层MLP神经元起关键作用；提出压缩和区分两个指标，证明模型内部存在隐式有限状态自动机（FSA）；在跳过中间步骤、引入数据噪声和测试长度泛化等挑战场景下，Transformer+CoT表现出稳健的算法学习能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-thought（CoT）显著增强了大型语言模型（LLM）的性能，并在多个任务中表现出优势。</li>
<li>研究评估了Transformer+CoT的状态跟踪能力，证实了其有效性。</li>
<li>识别了负责跟踪世界状态的电路，其中晚期层MLP神经元起到关键作用。</li>
<li>提出压缩和区分两个指标，证明了模型内部存在隐式有限状态自动机（FSA）。</li>
<li>通过对特定电路的分析，发现该电路能够实现近乎100%的准确率。</li>
<li>在挑战场景下，如跳过中间步骤、引入数据噪声和测试长度泛化，Transformer+CoT展现出稳健的算法学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2214e3041d8e36d0102ea125ed370069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d832b8f42df205a15ec2400d9ee15ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1420f99dcd90097be0702c3c346da69b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86d34f72fc0254c6d50d12cb7bea562c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level “novelty.” Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. The code is available at <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a>. </p>
<blockquote>
<p>数据的多样性对于大语言模型的指令调整至关重要。现有研究已经探索了多种意识到的数据选择方法来构建高质量的数据集，以提高模型性能。然而，精确定义和测量数据多样性的问题尚未得到充分探索，这限制了数据工程的明确指导。为了解决这个问题，我们通过大量微调实验，系统地分析了19种现有的多样性测量方法，评估它们与模型性能的相关性。我们的结果表明，可靠的多样性测量应适当地考虑样本之间的差异和样本空间中的信息密度。在此基础上，我们提出了NovelSum，这是一种基于样本级“新颖性”的新多样性指标。在模拟数据和真实数据上的实验表明，NovelSum能够准确捕捉多样性变化，与指令调整模型性能的关联度达到0.97，突显其在指导数据工程实践中的价值。以NovelSum作为优化目标，我们进一步开发了一种以多样性为导向的数据选择策略，其性能优于现有方法，验证了我们指标的有效性和实际意义。代码可在<a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UmeanNever/NovelSum获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v5">PDF</a> Accepted at ACL 2025 Main. Camera-ready version updated (20 pages).   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong><br>大语言模型的训练需要对数据多样性予以关注。当前的研究已探索了各种多样性感知的数据选择方法，旨在构建高质量数据集以提高模型性能。然而，关于如何精确定义和测量数据多样性的基础问题仍被忽视，导致数据工程缺乏明确的指导方向。本文系统地分析了现有的十一种数据多样性测量方法，通过大量微调实验评估它们与模型性能的相关性。结果表明，一个可靠的数据多样性度量方法应该适当考虑样本之间的差异以及样本空间的信息密度。在此基础上，我们提出了基于样本级“新颖性”的NovelSum新多样性度量指标。实验表明，NovelSum能准确捕捉数据多样性的变化，与指令调整模型性能的相关性达到0.97，对于指导数据工程实践具有重要意义。利用NovelSum作为优化目标，我们进一步开发了一种以多样性为导向的数据选择策略，优于现有方法，验证了我们的指标的有效性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据多样性对于大语言模型的训练至关重要。</li>
<li>当前研究虽然已经探索了多种数据选择方法，但对数据多样性的定义和测量仍存在不足。</li>
<li>可靠的数据多样性测量方法应考虑样本间的差异和样本空间的信息密度。</li>
<li>提出了基于样本级“新颖性”的NovelSum指标来衡量数据多样性。</li>
<li>NovelSum能够准确捕捉数据多样性的变化，与模型性能高度相关。</li>
<li>使用NovelSum作为优化目标，开发了一种有效的数据选择策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cdfd83bb07cde7421bfe3d1965cc7487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1829b232e597669113fab54b414da2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1437bd13f715f44852b5091606942a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1c9677921012d9e6ef83a11ec2a6e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revisiting-3D-LLM-Benchmarks-Are-We-Really-Testing-3D-Capabilities"><a href="#Revisiting-3D-LLM-Benchmarks-Are-We-Really-Testing-3D-Capabilities" class="headerlink" title="Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"></a>Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?</h2><p><strong>Authors:Jiahe Jin, Yanheng He, Mingyan Yang</strong></p>
<p>In this work, we identify the “2D-Cheating” problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs’ unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks</a> . </p>
<blockquote>
<p>在这项工作中，我们确定了3D大型语言模型评估中的“2D欺骗”问题，在这些任务中，通过点云渲染图像，视觉语言模型可能很容易解决这些问题，这暴露了当前评估方法对3D大型语言模型的独特三维能力的无效评估。我们在多个三维大型语言模型基准测试上测试了视觉语言模型的性能，并以此作为参考，提出了更好地评估真实三维理解的原则。我们还主张在评估三维大型语言模型时，将三维能力与一维或二维方面明确区分开来。相关代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D--LLM-Benchmarks%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08503v2">PDF</a> Accepted to ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>本文指出了在评估三维大型语言模型（LLM）时存在的“二维欺骗”问题。部分任务可能通过点云渲染图像由视觉语言模型（VLM）轻易解决，未能有效评估LLM独特的三维能力。本研究测试了VLM在多个人形语言模型评估标准中的表现，并提出原则改进真实的三维理解评估方式。同时主张在评估三维LLM时明确区分其三维能力与一维或二维方面。相关代码和数据可通过访问网址获取：<a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>“二维欺骗”问题在评估三维LLM时存在，即某些任务可能通过简单的图像渲染被VLM轻易解决，未能真正评估LLM的三维能力。</li>
<li>通过多个三维LLM基准测试VLM的性能。</li>
<li>提出改进原则以更有效地评估LLM真正的三维理解能力。</li>
<li>在评估三维LLM时，需要明确区分其三维能力与一维或二维的方面。</li>
<li>研究代码和数据可在特定GitHub页面获取。</li>
<li>有效评估LLM的三维能力对于提升模型的实际应用性能至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-63a240f0e1a766c25de3fff35638ffc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b805855e23f77446f52f0001c4ebf49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c19b9d6519ec9231f3c2a8ef7c9f31c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d2fb4234cbbee37fb362cfa9571f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0ebf1cd5306869132b93756d55ef5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df5615e246a88474b510c691089471da.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Lessons-of-Developing-Process-Reward-Models-in-Mathematical-Reasoning"><a href="#The-Lessons-of-Developing-Process-Reward-Models-in-Mathematical-Reasoning" class="headerlink" title="The Lessons of Developing Process Reward Models in Mathematical   Reasoning"></a>The Lessons of Developing Process Reward Models in Mathematical   Reasoning</h2><p><strong>Authors:Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin</strong></p>
<p>Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models. </p>
<blockquote>
<p>流程奖励模型（PRMs）作为大型语言模型（LLM）数学推理中流程监督的一种有前途的方法而出现，旨在识别和缓解推理过程中的中间错误。然而，开发有效的PRM面临重大挑战，特别是在数据标注和评估方法方面。在本文中，我们通过大量实验证明，常用的基于蒙特卡洛（MC）估计的数据合成通常会产生比LLM作为评委和人类标注方法更差的性能和泛化能力。MC估计依赖于完成模型来评估当前步骤的正确性，从而导致不准确的步骤验证。此外，我们确定了传统Best-of-N（BoN）评估策略对PRMs的潜在偏见：（1）不可靠的政策模型产生的回答虽然有正确答案，但过程存在缺陷，导致BoN的评估标准与PRM的目标（过程验证）之间存在不一致。（2）PRM对此类回应的容忍度导致BoN分数膨胀。（3）现有PRM的最低分数有很大一部分集中在最终答案的步骤上，这揭示了BoN优化PRM中从过程转向结果导向评估的转变。为了解决这些挑战，我们开发了一种共识过滤机制，有效地将MC估计与LLM作为评委相结合，并倡导一个更全面的评估框架，结合响应级和步骤级指标。基于这些机制，我们显著提高了BoN评估和步骤级错误识别任务中的模型性能和数据效率。最后，我们发布了一个新的最先进的PRM，它优于现有的开源替代品，并为未来建立流程监督模型的研究提供实用指南。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07301v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文探讨了流程奖励模型（PRM）在数学推理中的过程监督问题。针对大型语言模型（LLM），提出了一种有前景的方法用于识别并减轻推理过程中的中间错误。然而，开发有效的PRM面临重大挑战，特别是在数据标注和评估方法方面。本文通过大量实验发现，常用的基于蒙特卡洛（MC）估计的数据合成方法对PRM的性能和泛化能力较差，相较于LLM作为法官和人类标注方法表现不佳。MC估计依赖于完成模型来评估当前步骤的正确性，从而导致不准确的步骤验证。此外，本文还指出了传统最佳N（BoN）评估策略对PRM的潜在偏见。为解决这些挑战，本文开发了一种共识过滤机制，有效地结合了MC估计和LLM作为法官的方法，并提倡采用更全面的评估框架，结合响应级和步骤级指标。基于该机制，我们在BoN评估和逐步错误识别任务中显著提高了模型性能和数据效率。最后，我们发布了一种新型的先进PRM，超越了现有的开源替代品，并为未来在构建流程监督模型方面的研究提供了实用指南。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>PRM旨在识别和减轻LLM在推理过程中的中间错误。</li>
<li>基于MC估计的数据合成方法对PRM的性能和泛化能力通常较差。</li>
<li>MC估计在评估当前步骤正确性时存在不准确的问题。</li>
<li>传统BoN评估策略对PRM存在潜在偏见，如策略的不可靠性、对特定响应的容忍度以及最终答案步骤中最低分数集中等问题。</li>
<li>共识过滤机制结合了MC估计和LLM作为法官的方法，提高了模型性能和数据效率。</li>
<li>新的评估框架结合了响应级和步骤级指标，提供更全面的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a8935a784fa2a550b0a0d6e4ab8ea68a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56774a018f1843c445fbc6159fce2476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-382647788b696ef9585e946fdae5d4b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13af295997a5d9a88235e85cc6c67919.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale"><a href="#MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale" class="headerlink" title="MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale"></a>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale</h2><p><strong>Authors:Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue</strong></p>
<p>Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process. </p>
<blockquote>
<p>开源多模态大型语言模型（MLLM）在广泛的多模态任务中显示出巨大的潜力。然而，它们的推理能力受到现有指令调整数据集的约束，这些数据集主要从诸如VQA、AI2D和ChartQA等学术数据集中重新利用。这些数据集针对简单任务，仅提供短语级答案，而不提供任何中间推理。为了解决这些挑战，我们引入了一种可扩展且成本效益高的方法来构建大规模多模态指令调整数据集，其中包含丰富的中间推理，旨在激发CoT推理。我们仅使用公开模型创建了一个包含12M指令响应对的数据集，以涵盖多样化、推理密集的任务，具有详细且准确的推理。实验表明，在此数据集上训练MLLMs可显著提高其推理能力，在MathVerse（+8.1%）、MMMU-Pro（+7%）和MuirBench（+13.3%）等基准测试中达到最新水平。此外，该模型在非推理基准测试中也表现出显著改进，最高可达4%。消融研究进一步突出了数据集构建过程中的关键组件（如重写和自过滤）的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05237v2">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong><br>大规模开源多模态语言模型（MLLMs）在多模态任务中展现出巨大潜力，但受限于现有指令调优数据集。为解决此挑战，我们提出了一种可规模化且成本效益高的方法，构建了一个大型多模态指令调优数据集，其中包含丰富的中间推理依据。实验证明，训练于该数据集的MLLMs展现出更高的推理能力，且在多个基准测试中表现领先。如MathVerse提高8.1%，MMMU-Pro提高7%，MuirBench提高达到令人瞩目的幅度（+13.3%）。非基于推理基准的测试中也观察到明显的进步幅度，最大达4%。同时，对关键构建环节的研究强调了重写和自过滤的重要性。这一新方法有助于进一步推动MLLMs在真实场景中的应用。该摘要简明扼要地概括了文本的核心内容。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是基于文本的关键见解：</p>
<ul>
<li>开源多模态大型语言模型（MLLMs）在多模态任务中具有显著潜力。但其推理能力受限于以学术数据集为主构建的训练数据集，这些数据集往往关注简化任务，缺乏中间推理依据。为解决此问题，提出了一种构建大型多模态指令调优数据集的方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c991591cd1594df10dbb9b9f3749e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48331a7147d897c0599a389172ece718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf951a075892bb8d29ed7cf688cb4fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86325b96350dceba0bfd1fb3ad2803e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48fc283a0424105f7cf736f090390e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3613d55f2d2d9c529549aa40c2f2d071.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Focus-On-This-Not-That-Steering-LLMs-with-Adaptive-Feature-Specification"><a href="#Focus-On-This-Not-That-Steering-LLMs-with-Adaptive-Feature-Specification" class="headerlink" title="Focus On This, Not That! Steering LLMs with Adaptive Feature   Specification"></a>Focus On This, Not That! Steering LLMs with Adaptive Feature   Specification</h2><p><strong>Authors:Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto</strong></p>
<p>Despite the success of Instruction Tuning (IT) in training large language models (LLMs), such models often leverage spurious or biased features learnt from their training data and can become misaligned, leading to undesired behaviours. While existing techniques can steer model behaviour at inference-time, they are often post-hoc and do not embed steering as an intrinsic model feature. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across diverse benchmarks, we demonstrate that FIT: (i) successfully steers behaviour at inference time; (ii) increases robustness by amplifying core task signals and down-weighting spurious cues; (iii) mitigates social bias by suppressing demographic attributes; and (iv) generalises under distribution shifts and to previously unseen focus features. FIT therefore offers a lightweight, intrinsic mechanism for building more robust, fair, and easily controllable LLMs. </p>
<blockquote>
<p>尽管指令调整（IT）在训练大型语言模型（LLM）方面取得了成功，但这些模型经常利用从训练数据中学习到的虚假或偏见特征，并可能出现偏差，导致出现不需要的行为。虽然现有技术可以在推理时引导模型行为，但它们通常是事后性的，并没有将引导作为模型的内在特征。在这项工作中，我们引入了焦点指令调整（FIT），它训练LLM通过专注于特定特征而忽视其他特征来条件化其响应，根据指定的特征表现出不同的行为。在多种基准测试中，我们证明了FIT：（i）成功地在推理时间引导行为；（ii）通过放大核心任务信号和降低虚假线索来增强稳健性；（iii）通过抑制人口统计属性来缓解社会偏见；（iv）在分布变化和以前未见过的焦点特征下实现推广。因此，FIT提供了一种轻便、内在的机制，用于构建更稳健、公平和易于控制的大型语言模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22944v4">PDF</a> 36pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Focus Instruction Tuning（FIT）技术，该技术训练大型语言模型（LLM）以在推理时专注于特定特征而忽视其他特征，从而实现对模型行为的控制。通过多样化基准测试，作者展示了FIT技术的四个优势：成功在推理时间引导行为、通过放大核心任务信号和削弱误导性线索来提高稳健性、通过抑制人口统计特征来减轻社会偏见以及在分布变化和先前未见到的焦点特征上实现泛化。因此，FIT提供了一种轻便的内在机制，用于构建更稳健、公平且易于控制的大型语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FIT是一种用于训练大型语言模型的新技术，能使模型在推理时专注于特定特征而忽略其他特征，从而控制模型的行为。</li>
<li>FIT技术通过放大核心任务信号和削弱误导性线索来提高模型的稳健性。</li>
<li>FIT可以减轻社会偏见，通过抑制人口统计特征来避免模型的潜在偏见。</li>
<li>FIT在分布变化和先前未见到的焦点特征上具有良好的泛化能力。</li>
<li>FIT提供了一种内在机制来构建大型语言模型，使其更易于控制。</li>
<li>通过多样化基准测试，证明了FIT技术的有效性和优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22944">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86584546ffd9de69a1a7e6db3442a39d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant"><a href="#Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant" class="headerlink" title="Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant"></a>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant</h2><p><strong>Authors:Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon</strong></p>
<p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model’s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning. </p>
<blockquote>
<p>量化技术作为部署大小语言模型的一种成本效益高的解决方案，已经引起了人们的关注。然而，大多数先前的工作仅限于困惑度或基本知识任务，缺乏对Llama-3.3等最新模型的全面评估。在本文中，我们对指令调优模型进行了全面评估，这些模型跨越了从千亿字节到十万亿字节的参数范围，并应用了四种量化方法涵盖十三个数据集。我们的研究结果表明：（一）量化模型通常超越较小的FP16基线模型，但它们通常在指令跟随和幻像检测方面存在困难；（二）FP8在各种任务中始终表现出最稳健的选择，AWQ在仅权重量化方面倾向于优于GPTQ；（三）较小的模型在四比特量化时可能会遭受严重的精度损失，而规模为七十亿字节的模型则能保持稳定的性能；（四）值得注意的是，并非所有困难任务都会经历最大的精度损失，这表明量化技术放大了一个模型的固有弱点，而非仅仅是与任务难度的关联；（五）基于大型语言模型的判断（MT-Bench）突显了在编码和STEM任务中的显著性能下降，尽管有时它会在推理方面报告改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11055v6">PDF</a> Accepted in IJCAI 2025, 21 pages, 2 figure</p>
<p><strong>Summary</strong></p>
<p>本文研究了指令微调模型的量化效果，涉及从1B到405B参数的模型，应用四种量化方法，跨越13个数据集进行综合评估。研究发现，量化模型一般超越较小的FP16基准模型，但在指令遵循和幻觉检测方面常有问题；FP8在任务上表现最稳健，AWQ在仅权重量化方面优于GPTQ；小模型在4位量化时准确性大幅下降，而70B规模模型表现稳定；值得注意的是，量化并不总是与任务难度直接相关，而是会放大模型的固有弱点；LLM判断（MT-Bench）在编码和STEM任务中显著性能下降，但在推理任务中有时有改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>量化模型通常优于较小的FP16基准模型，但在指令遵循和幻觉检测方面存在挑战。</li>
<li>FP8在多种任务中表现最稳健，AWQ在仅权重量化方面优于GPTQ。</li>
<li>小模型在4位量化时准确性显著下降，而较大模型（如70B）表现相对稳定。</li>
<li>量化放大了模型的固有弱点，并不总是与任务难度直接相关。</li>
<li>LLM判断（MT-Bench）在编码和STEM任务中检测到性能显著下降，但在某些推理任务中有改进。</li>
<li>研究涵盖了从1B到405B参数的模型，应用了四种量化方法，并跨13个数据集进行了全面评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7230159c5d87c4bb90c4d52f4360756e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f8be894892e570711864b8b6462dbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76cc5736fac1d2ddc5d3debffb65997.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6748fa4dc153505de77db20a110c062f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mosaic-IT-Cost-Free-Compositional-Data-Synthesis-for-Instruction-Tuning"><a href="#Mosaic-IT-Cost-Free-Compositional-Data-Synthesis-for-Instruction-Tuning" class="headerlink" title="Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning"></a>Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning</h2><p><strong>Authors:Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, Tianyi Zhou</strong></p>
<p>Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses for training, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human&#x2F;model-free compositional data synthesis method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and an 80% reduction in training costs compared with original instruction tuning. Our codes and data are available at <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a>. </p>
<blockquote>
<p>通过对大型语言模型进行多种指令-响应对的微调，增强了其理解和遵循指令的能力。当前的指令调整主要依赖于教师模型或人工干预来生成和细化训练和响应指令，这种方式成本高昂、不可持续，并且可能缺乏多样性。在本文中，我们介绍了Mosaic Instruction Tuning（Mosaic-IT），这是一种无需人工参与模型介入的组合数据合成方法，它可以有效地从现有的指令调整数据中创建丰富多样的增强数据，以增强大型语言模型的能力。Mosaic-IT随机连接多个指令数据为一个数据点，并通过预设的高级元指令训练模型生成相应的响应，以增强其遵循多步骤指令和格式的技能。我们的全面评估表明，Mosaic-IT具有卓越的性能和训练效率，在各种基准测试中实现了性能提升的一致性，并且在训练成本方面相较于传统的指令调整减少了80%。我们的代码和数据可在 <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13326v3">PDF</a> ACL2025, Camera-ready</p>
<p><strong>Summary</strong></p>
<p>大型语言模型通过多样化的指令-响应对进行微调，提高了其理解和遵循指令的能力。当前主要的指令调整方法依赖于教师模型或人工生成和细化指令和响应来进行训练，这成本高昂、不可持续且可能缺乏多样性。本文介绍了一种名为Mosaic Instruction Tuning（Mosaic-IT）的人&#x2F;模型无关的组合数据合成方法，它可以有效地从现有的指令调整数据中创建丰富且多样化的增强数据，以增强语言模型的能力。Mosaic-IT通过随机连接多个指令数据并进行训练，使模型能够使用预定义的高级元指令产生相应的响应，提高其遵循多步骤指令和格式的能力。评估结果表明，Mosaic-IT性能卓越，训练效率高，在各种基准测试上实现了性能提升，与原始指令调整相比，训练成本降低了80%。相关代码和数据可在 <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a> 获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型通过指令-响应对微调增强了理解和遵循指令的能力。</li>
<li>当前指令调整方法主要依赖教师模型或人工生成数据，存在成本高、不可持续和缺乏多样性等问题。</li>
<li>引入Mosaic Instruction Tuning（Mosaic-IT）方法，实现人&#x2F;模型无关的组合数据合成。</li>
<li>Mosaic-IT通过随机连接多个指令数据并训练，提高模型遵循多步骤指令和格式的能力。</li>
<li>Mosaic-IT在各种基准测试上表现出卓越性能和训练效率。</li>
<li>与原始指令调整相比，Mosaic-IT训练成本降低了80%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e6d64923086780128e8adac40395f28f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420962e14d438546ce8aa6456695e95b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71b5d1ddcb890d4333dd3e23fa198125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebb16aafb7b280d8754ccbe92acb8082.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images"><a href="#VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images" class="headerlink" title="VCD: A Dataset for Visual Commonsense Discovery in Images"></a>VCD: A Dataset for Visual Commonsense Discovery in Images</h2><p><strong>Authors:Xiangqing Shen, Fanfan Wang, Siwei Wu, Rui Xia</strong></p>
<p>Visual commonsense plays a vital role in understanding and reasoning about the visual world. While commonsense knowledge bases like ConceptNet provide structured collections of general facts, they lack visually grounded representations. Scene graph datasets like Visual Genome, though rich in object-level descriptions, primarily focus on directly observable information and lack systematic categorization of commonsense knowledge. We present Visual Commonsense Dataset (VCD), a large-scale dataset containing over 100,000 images and 14 million object-commonsense pairs that bridges this gap. VCD introduces a novel three-level taxonomy for visual commonsense, integrating both Seen (directly observable) and Unseen (inferrable) commonsense across Property, Action, and Space aspects. Each commonsense is represented as a triple where the head entity is grounded to object bounding boxes in images, enabling scene-dependent and object-specific visual commonsense representation. To demonstrate VCD’s utility, we develop VCM, a generative model that combines a vision-language model with instruction tuning to discover diverse visual commonsense from images. Extensive evaluations demonstrate both the high quality of VCD and its value as a resource for advancing visually grounded commonsense understanding and reasoning. Our dataset and code will be released on <a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD">https://github.com/NUSTM/VCD</a>. </p>
<blockquote>
<p>视觉常识在理解和推理视觉世界方面起着至关重要的作用。虽然像ConceptNet这样的常识知识库提供了通用事实的结构化集合，但它们缺乏视觉基础表示。像Visual Genome这样的场景图数据集虽然富含对象级别的描述，但主要关注可直接观察的信息，缺乏常识知识的系统分类。我们推出了Visual Commonsense Dataset（VCD），这是一个大规模数据集，包含超过10万张图像和1400万张对象常识配对，弥补了这一空白。VCD引入了一种新颖的三级分类法，用于视觉常识，融合了跨属性、动作和空间方面的可见（可直接观察）和不可见（可推断）常识。每个常识都表示为三元组，其中头实体与图像中的对象边界框相匹配，实现了场景依赖和对象特定的视觉常识表示。为了展示VCD的实用性，我们开发了VCM，这是一个生成模型，它将视觉语言模型与指令调整相结合，从图像中发现多样的视觉常识。广泛评估表明，VCD的高质量及其作为推进视觉基础常识理解和推理的资源价值。我们的数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUSTM/VCD上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.17213v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视觉常识在理解和推理视觉世界方面扮演着至关重要的角色。虽然常识知识库如ConceptNet提供了结构化的常识事实集合，但它们缺乏视觉基础表示。场景图数据集如Visual Genome虽然富含对象级别的描述，但主要关注可直接观察到的信息，缺乏系统的常识知识分类。我们推出了视觉常识数据集（VCD），包含超过10万张图像和1400万对象-常识对，弥补了这一空白。VCD引入了一种新的三级分类法，整合了可见（可直接观察）和不可见（可推断）的常识，涵盖属性、动作和空间方面。每个常识以三元组的形式表示，头部实体与图像中的对象边界框相对应，实现了场景依赖和对象特定的视觉常识表示。为了展示VCD的实用性，我们开发了VCM，一个结合视觉语言模型和指令调整的生成模型，从图像中发现多样的视觉常识。全面评估表明，VCD的高质量及其作为推进视觉基础常识理解和推理的资源价值。我们的数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUSTM/VCD上发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉常识在理解和推理视觉世界方面至关重要。</li>
<li>当前知识库和数据集存在缺乏视觉基础表示和系统常识知识分类的问题。</li>
<li>提出了Visual Commonsense Dataset (VCD)来解决这一问题，包含图像和对象-常识对。</li>
<li>VCD采用新的三级分类法，整合可见和不可见的常识，涵盖属性、动作和空间方面。</li>
<li>VCD中的常识以三元组形式表示，与图像中的对象边界框相对应。</li>
<li>开发了VCM模型，结合视觉语言模型和指令调整，从图像中发现多样的视觉常识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.17213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04775750c2b7aaa268e08f6d6d42895a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4703d3091e348bcbf98aa6b35f94e7a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-700ab1eb162dbef1b6b81b508cdd1e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b5d05e9d480e67ac1ce7d86a8db52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ccf103f7e301725f53667dd0d4c644d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-10  PuzzleWorld A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7de1d642a339137fcd331c11d7b07719.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-09  Perceptual Decoupling for Scalable Multi-modal Reasoning via   Reward-Optimized Captioning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
