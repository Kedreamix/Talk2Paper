<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-09  LLaDA-V Large Language Diffusion Models with Visual Instruction Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    55 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-09-æ›´æ–°"><a href="#2025-06-09-æ›´æ–°" class="headerlink" title="2025-06-09 æ›´æ–°"></a>2025-06-09 æ›´æ–°</h1><h2 id="LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning"><a href="#LLaDA-V-Large-Language-Diffusion-Models-with-Visual-Instruction-Tuning" class="headerlink" title="LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning"></a>LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning</h2><p><strong>Authors:Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/">https://ml-gsai.github.io/LLaDA-V-demo/</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaDA-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒå°†è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸æ©ç æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæ‰“ç ´äº†å½“å‰å¤šæ¨¡æ€æ–¹æ³•ä¸­å ä¸»å¯¼åœ°ä½çš„è‡ªå›å½’èŒƒå¼çš„é™åˆ¶ã€‚LLaDA-Vå»ºç«‹åœ¨LLaDAï¼ˆä¸€ä¸ªä»£è¡¨æ€§çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼‰çš„åŸºç¡€ä¸Šï¼Œèå…¥äº†è§†è§‰ç¼–ç å™¨å’ŒMLPè¿æ¥å™¨ï¼Œå°†è§†è§‰ç‰¹å¾æŠ•å°„åˆ°è¯­è¨€åµŒå…¥ç©ºé—´ï¼Œå®ç°äº†æœ‰æ•ˆçš„å¤šæ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶æ­ç¤ºäº†å‡ ä¸ªæœ‰è¶£çš„ç»“æœï¼šé¦–å…ˆï¼Œå°½ç®¡LLaDA-Våœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¯­è¨€æ¨¡å‹è¡¨ç°è¾ƒå¼±ï¼Œä¸LLaMA3-8Bå’ŒQwen2-7Bç­‰ç›¸æ¯”ç¨æ˜¾é€Šè‰²ï¼Œä½†åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å´å±•ç°å‡ºæœ‰å‰æ™¯çš„å¤šæ¨¡æ€è¡¨ç°ã€‚å½“ä½¿ç”¨ç›¸åŒçš„æŒ‡ä»¤æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢ä¸LLaMA3-Væå…·ç«äº‰åŠ›ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„æ•°æ®å¯æ‰©å±•æ€§ã€‚å®ƒè¿˜ç¼©å°äº†ä¸Qwen2-VLçš„æ€§èƒ½å·®è·ï¼Œè¯æ˜äº†å…¶æ¶æ„åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…¶æ¬¡ï¼Œä¸ç°æœ‰çš„æ··åˆè‡ªå›å½’æ‰©æ•£å’Œçº¯æ‰©æ•£çš„MLLMç›¸æ¯”ï¼ŒLLaDA-Våœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå€¼å¾—æœªæ¥è¿›ä¸€æ­¥ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%E3%80%82">https://ml-gsai.github.io/LLaDA-V-demo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16933v2">PDF</a> Project page and codes: \url{<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%7D">https://ml-gsai.github.io/LLaDA-V-demo/}</a></p>
<p><strong>Summary</strong></p>
<p>LLaDA-Væ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œæ©è†œæ‰©æ•£æ¨¡å‹é›†æˆäº†è§†è§‰åŠŸèƒ½ï¼Œçªç ´äº†å½“å‰ä¸»æµçš„è‡ªå›å½’èŒƒå¼é™åˆ¶ã€‚åœ¨è¯­è¨€å’Œè§†è§‰èåˆæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-Væ˜¯åŸºäºLLaDAçš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹çš„æ‰©å±•ï¼Œé€šè¿‡é›†æˆè§†è§‰ç¼–ç å™¨å’ŒMLPè¿æ¥å™¨ï¼Œå®ç°äº†è§†è§‰ç‰¹å¾åˆ°è¯­è¨€åµŒå…¥ç©ºé—´çš„æ˜ å°„ï¼Œå®ç°äº†æœ‰æ•ˆçš„å¤šæ¨¡æ€å¯¹é½ã€‚</li>
<li>LLaDA-Våœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿å…¶çº¯æ–‡æœ¬ä»»åŠ¡è¡¨ç°è¾ƒå¼±çš„LLaDAæ¨¡å‹ä¹Ÿèƒ½ä¸LLaMA3-Vç­‰æ¨¡å‹ç«äº‰ã€‚</li>
<li>LLaDA-Vç¼©å°äº†ä¸Qwen2-VLçš„æ€§èƒ½å·®è·ï¼ŒéªŒè¯äº†å…¶æ¶æ„åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>LLaDA-Vè¾¾åˆ°äº†å¤šæ¨¡æ€ç†è§£çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸ç°æœ‰çš„æ··åˆè‡ªå›å½’-æ‰©æ•£å’Œçº¯æ‰©æ•£å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå¹¶å€¼å¾—æœªæ¥è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>LLaDA-Vçš„æ¼”ç¤ºå’Œé¡¹ç›®é¡µé¢å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/LLaDA-V-demo/%E6%89%BE%E5%88%B0%E3%80%82">https://ml-gsai.github.io/LLaDA-V-demo/æ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-844d033035b69014d28ceeb951f9e2e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02257baa9bf994de14ed7d0d038e54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed659bfa20dbe4dba03864406d86a419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fdf1cba7ea415778817b386220d68cc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible â€œdark patternsâ€ in LLMsâ€™ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local â€œsafety regionsâ€ in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shiftsâ€“a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨åŒ–çš„åŸºç¡€æ¢ç´¢ï¼Œç„¶è€Œï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ ä½¿å…¶ä¸äººç±»ä»·å€¼è§‚ç›¸ç¬¦åªå®ç°äº†è¡¨é¢çš„åˆè§„æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯æ˜é¢„è®­ç»ƒæœŸé—´åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºLLMå‚æ•°è®°å¿†ä¸­çš„ä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€æŒç»­å­˜åœ¨ï¼Œè¿™äº›æš—æ¨¡å¼é€ƒé¿äº†å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»çš„æƒ…å†µä¸‹å—åˆ°å¯¹æŠ—æ€§è¯±å¯¼æ—¶é‡æ–°å‡ºç°ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šåˆ†æäº†å¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œè¯æ˜å½“å‰çš„å¯¹é½æ–¹æ³•åªèƒ½åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨çš„â€œå®‰å…¨åŒºåŸŸâ€ã€‚ç›¸åï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä»ç„¶ä¸æœ‰å®³æ¦‚å¿µé€šè¿‡é«˜æ¦‚ç‡å¯¹æŠ—è½¨è¿¹å…¨å±€è¿æ¥ã€‚åŸºäºè¿™ä¸€ç†è®ºè§è§£ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ¥å®è¯éªŒè¯æˆ‘ä»¬çš„å‘ç°â€”â€”è¿™æ˜¯ä¸€ç§é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„æ–¹æ³•ã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€å…ˆè¿›å¯¹é½LLMä¸­çš„19ä¸ªä¸Šå®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒLLaMA-3ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ™®éè„†å¼±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨æ¢ç´¢çš„åŸºç¡€ï¼Œä½†å®ƒä»¬ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ä»…é™äºè¡¨é¢ã€‚ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒæ—¶åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€å­˜åœ¨äºæ¨¡å‹çš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½å¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é‡æ–°å‡ºç°ã€‚æœ¬æ–‡é¦–å…ˆä»ç†è®ºä¸Šåˆ†æäº†å¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œè¯æ˜äº†å½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œè€Œé¢„è®­ç»ƒçš„çŸ¥è¯†ä»ç„¶ä¸æœ‰å®³æ¦‚å¿µåœ¨å…¨çƒèŒƒå›´å†…å­˜åœ¨è”ç³»ã€‚å®è¯ä¸Šï¼Œé€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ–¹æ³•éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€æ–°å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æˆåŠŸæ”»å‡»äº†å…¶ä¸­çš„19ä¸ªï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒLLaMA-3ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ™®éè„†å¼±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é€šç”¨æ¢ç´¢ä¸­å æ®é‡è¦åœ°ä½ï¼Œä½†å®ƒä»¬åœ¨ä¸äººç±»ä»·å€¼è§‚å¯¹é½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>é¢„è®­ç»ƒæœŸé—´åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šåœ¨æ¨¡å‹ä¸­ä»¥â€œæš—æ¨¡å¼â€å½¢å¼å­˜åœ¨ï¼Œé€ƒé¿å¯¹é½æªæ–½å¹¶åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹é‡æ–°æ˜¾ç°ã€‚</li>
<li>å½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œé¢„è®­ç»ƒçš„çŸ¥è¯†ä¸æœ‰å®³æ¦‚å¿µä¹‹é—´ä»å­˜åœ¨å…¨çƒè”ç³»ã€‚</li>
<li>é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ–¹æ³•ï¼Œå¯ä»¥ç³»ç»Ÿåœ°ç»•è¿‡æ¨¡å‹çš„å¯¹é½çº¦æŸã€‚</li>
<li>è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•æˆåŠŸæ”»å‡»äº†å¤§å¤šæ•°æœ€æ–°å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ­ç¤ºäº†å¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œéœ€è¦æ›´æ·±å…¥çš„ç ”ç©¶å’Œæ›´å®Œå–„çš„æªæ–½æ¥ç¡®ä¿æ¨¡å‹çš„å®‰å…¨æ€§å’Œé“å¾·åˆè§„æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2261b3c61e41e842523c37d9e4eed2be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-146301b6ccdc9ad94e03dd2592915f33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8894cc97e6192748906cfca91b3ed446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92236c32efe5492d12100a5b2257db5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations"><a href="#VecTrans-Enhancing-Compiler-Auto-Vectorization-through-LLM-Assisted-Code-Transformations" class="headerlink" title="VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations"></a>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted   Code Transformations</h2><p><strong>Authors:Zhongchun Zheng, Kan Wu, Long Cheng, Lu Li, Rodrigo C. O. Rocha, Tianyi Liu, Wei Wei, Jianjiang Zeng, Xianwei Zhang, Yaoqing Gao</strong></p>
<p>Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage. </p>
<blockquote>
<p>è‡ªåŠ¨çŸ¢é‡åŒ–æ˜¯ç°ä»£ç¼–è¯‘å™¨åˆ©ç”¨SIMDå¹¶è¡Œæ€§çš„åŸºæœ¬ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æŠ€æœ¯åœ¨å¤„ç†å¤æ‚çš„ä»£ç æ¨¡å¼æ—¶ä»é¢ä¸´å›°éš¾ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨æç¤ºæˆ–ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ•è·å¤æ‚æ¨¡å¼çš„èƒ½åŠ›ï¼Œæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬åœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­çš„æœ‰æ•ˆåº”ç”¨ä»é¢ä¸´å¼€æ”¾æŒ‘æˆ˜ï¼Œä¾‹å¦‚è™šæ„é—®é¢˜å’Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VecTransï¼Œä¸€ä¸ªåˆ©ç”¨LLMå¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç çŸ¢é‡åŒ–çš„æ–°å‹æ¡†æ¶ã€‚VecTransé¦–å…ˆä½¿ç”¨ç¼–è¯‘å™¨åˆ†ææ¥è¯†åˆ«å¯èƒ½å¯å‘é‡åŒ–çš„ä»£ç åŒºåŸŸã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨LLMå°†è¿™äº›åŒºåŸŸé‡æ„ä¸ºæ›´æ˜“äºç¼–è¯‘å™¨è‡ªåŠ¨çŸ¢é‡åŒ–çš„æ¨¡å¼ã€‚ä¸ºäº†ç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ï¼ŒVecTransè¿›ä¸€æ­¥åœ¨ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰çº§åˆ«é›†æˆäº†æ··åˆéªŒè¯æœºåˆ¶ã€‚é€šè¿‡ä»¥ä¸ŠåŠªåŠ›ï¼ŒVecTransç»“åˆäº†LLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨çŸ¢é‡åŒ–çš„ç²¾ç¡®æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°æ‰“å¼€äº†çŸ¢é‡åŒ–æœºä¼šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰ä¸å¯ç”±GCCã€ICCã€Clangå’ŒBiShengç¼–è¯‘å™¨è¿›è¡ŒçŸ¢é‡åŒ–çš„TSVCå‡½æ•°ä¸­ï¼ŒVecTranså®ç°äº†1.7 æ— éœ€ä¿®æ”¹åŸæœ‰ä»£ç å°±èƒ½æé«˜è¿è¡Œé€Ÿåº¦çš„å‡ ä½•å¹³å‡é€Ÿåº¦æå‡ï¼Œå¹¶æˆåŠŸå¯¹å…¶ä¸­æµ‹è¯•æ¡ˆä¾‹ä¸­çš„51ä¸ªä¸­çš„24ä¸ªè¿›è¡Œäº†çŸ¢é‡åŒ–å¤„ç†ã€‚è¿™æ ‡å¿—ç€åœ¨ä¿æŒæˆæœ¬æ•ˆç›Šçš„åŒæ—¶ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œæ¯æ¬¡å‡½æ•°ä¼˜åŒ–çš„LLM APIä½¿ç”¨æˆæœ¬ä¸º0.012ç¾å…ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19449v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç°ä»£ç¼–è¯‘å™¨ä¸­ï¼Œè‡ªåŠ¨çŸ¢é‡åŒ–æ˜¯ä¸€ä¸ªåŸºæœ¬çš„ä¼˜åŒ–æŠ€æœ¯ä»¥åˆ©ç”¨SIMDå¹¶è¡Œæ€§ã€‚ç„¶è€Œï¼Œæœ€æ–°çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„ä»£ç æ¨¡å¼æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ‰‹åŠ¨æç¤ºæˆ–ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ¨¡å¼ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­çš„åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨è¯¸å¦‚å¹»æƒ³å’Œç¼ºä¹é¢†åŸŸç‰¹å®šæ¨ç†ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†VecTransæ¡†æ¶ï¼Œåˆ©ç”¨LLMå¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç çŸ¢é‡åŒ–ã€‚VecTransé¦–å…ˆä½¿ç”¨ç¼–è¯‘å™¨åˆ†ææ¥è¯†åˆ«å¯èƒ½çŸ¢é‡åŒ–çš„ä»£ç åŒºåŸŸï¼Œç„¶åä½¿ç”¨LLMå¯¹è¿™äº›åŒºåŸŸè¿›è¡Œé‡æ„ï¼Œä»¥ä¾¿æ›´æ˜“äºç¼–è¯‘å™¨è‡ªåŠ¨çŸ¢é‡åŒ–ã€‚ä¸ºç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ï¼ŒVecTransè¿›ä¸€æ­¥åœ¨ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰çº§åˆ«æ•´åˆäº†æ··åˆéªŒè¯æœºåˆ¶ã€‚é€šè¿‡ç»“åˆLLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨çŸ¢é‡åŒ–çš„ç²¾ç¡®æ€§ï¼ŒVecTransæœ‰æ•ˆåœ°å¼€å¯äº†çŸ¢é‡åŒ–æœºä¼šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨GCCã€ICCã€Clangå’ŒBiShengç¼–è¯‘å™¨æ— æ³•çŸ¢é‡åŒ–çš„æ‰€æœ‰TSVCå‡½æ•°ä¸­ï¼ŒVecTranså®ç°äº†å¹³å‡åŠ é€Ÿæ¯”1.77å€ï¼ŒæˆåŠŸçŸ¢é‡åŒ–äº†24ä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­çš„51ä¸ªã€‚è¿™æ ‡å¿—ç€ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”çš„é‡å¤§è¿›å±•ï¼ŒåŒæ—¶ä¿æŒæ¯æ¬¡å‡½æ•°ä¼˜åŒ–çš„æˆæœ¬æ•ˆç›Šä¸º$0.012ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨çŸ¢é‡åŒ–æ˜¯ç¼–è¯‘å™¨ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½†ä»é¢ä¸´å¤„ç†å¤æ‚ä»£ç æ¨¡å¼çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ•æ‰å¤æ‚æ¨¡å¼çš„èƒ½åŠ›ï¼Œä¸ºç¼–è¯‘å™¨ä¼˜åŒ–æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>VecTransæ¡†æ¶ç»“åˆäº†LLMå’Œç¼–è¯‘å™¨ä¼˜åŒ–çš„ä¼˜åŠ¿ï¼Œé€šè¿‡ç¼–è¯‘å™¨åˆ†æè¯†åˆ«çŸ¢é‡åŒ–çš„æ½œåŠ›åŒºåŸŸå¹¶åˆ©ç”¨LLMè¿›è¡Œé‡æ„ã€‚</li>
<li>VecTransé‡‡ç”¨æ··åˆéªŒè¯æœºåˆ¶ç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜VecTransåœ¨å¤šç§ç¼–è¯‘å™¨æ— æ³•çŸ¢é‡åŒ–çš„å‡½æ•°ä¸­å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚</li>
<li>VecTransçš„æˆåŠŸå®ç°äº†åœ¨ä¿æŒè¾ƒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ¯æ¬¡å‡½æ•°ä¼˜åŒ–çš„æˆæœ¬æ•ˆç›Šè¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f6a1bd0e4443c6cd6bfa51edd23722b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7775444ecbce5e131129bbff6a5401c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7380c7c050b27627e9f76f903025e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-183e6d73d4cf40bbfcfce4fc2d227003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8826d3138ffef65662fb28fcd86eb4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b7154d2425688685afe2923c7625a9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Human-Production-Interpretation-Asymmetries-to-Test-LLM-Cognitive-Plausibility"><a href="#Leveraging-Human-Production-Interpretation-Asymmetries-to-Test-LLM-Cognitive-Plausibility" class="headerlink" title="Leveraging Human Production-Interpretation Asymmetries to Test LLM   Cognitive Plausibility"></a>Leveraging Human Production-Interpretation Asymmetries to Test LLM   Cognitive Plausibility</h2><p><strong>Authors:Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt</strong></p>
<p>Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at <a target="_blank" rel="noopener" href="https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025">https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025</a>. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€æ—¶æ˜¯å¦ä¸äººç±»æœ‰ç±»ä¼¼çš„è¿‡ç¨‹ä¸€ç›´æ˜¯ç†è®ºç•Œå’Œå®è·µç•Œçƒ­è®®çš„è¯é¢˜ã€‚æˆ‘ä»¬é€šè¿‡äººç±»å¥å­å¤„ç†ä¸­å‘ç°çš„ç”Ÿæˆä¸è§£è¯»çš„å·®å¼‚æ€§æ¥å®¡è§†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¯„ä¼°æŒ‡ä»¤ä¼˜åŒ–çš„LLMåœ¨è¿™ä¸€åŒºåˆ«ä¸Šçš„å¤åˆ¶ç¨‹åº¦ã€‚æˆ‘ä»¬åˆ©ç”¨äººç±»éšå¼å› æœåŠ¨è¯åœ¨ä»£è¯ç”Ÿæˆä¸è§£è¯»ä¹‹é—´å­˜åœ¨çš„å®è¯è®°å½•çš„ä¸å¯¹ç§°æ€§ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå‘ç°ä¸€äº›LLMåœ¨ç”Ÿæˆå’Œè§£è¯»ä¹‹é—´å­˜åœ¨å®šé‡å’Œå®šæ€§çš„ä¸äººç±»ç±»ä¼¼çš„ä¸å¯¹ç§°æ€§ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§è¡Œä¸ºçš„å­˜åœ¨æ—¢å–å†³äºæ¨¡å‹çš„å¤§å°â€”â€”è¾ƒå¤§çš„æ¨¡å‹æ›´å¯èƒ½åæ˜ å‡ºäººç±»æ¨¡å¼ï¼Œä¹Ÿå–å†³äºç”¨äºæ¿€å‘è¡Œä¸ºçš„å…ƒè¯­è¨€æç¤ºçš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„ä»£ç å’Œç»“æœå¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025">https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17579v2">PDF</a> ACL 2025 Camera-ready</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€æ—¶æ˜¯å¦ä¸äººç±»æœ‰ç›¸ä¼¼çš„å¤„ç†æ–¹å¼ä¸€ç›´æ˜¯ç†è®ºç•Œå’Œå®è·µç•Œçƒ­è®®çš„è¯é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡äººç±»å¥å­å¤„ç†ä¸­çš„ç”Ÿäº§-è§£é‡Šå·®å¼‚æ¥æ¢è®¨è¿™ä¸€é—®é¢˜ï¼Œå¹¶è¯„ä¼°æŒ‡ä»¤è°ƒæ•´åçš„LLMå¯¹è¿™ç§å·®å¼‚çš„å¤åˆ¶ç¨‹åº¦ã€‚ç ”ç©¶åˆ©ç”¨äººç±»åœ¨ä½¿ç”¨éšæ€§å› æœåŠ¨è¯æ—¶çš„ä»£è¯ç”Ÿäº§å’Œè§£é‡Šä¹‹é—´çš„ä¸å¯¹ç§°æ€§ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå‘ç°éƒ¨åˆ†LLMåœ¨ç”Ÿäº§å’Œè§£é‡Šä¹‹é—´èƒ½å¤Ÿåæ˜ ç±»ä¼¼äººç±»çš„ä¸å¯¹ç§°æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§è¡Œä¸ºæ˜¯å¦æŒç»­å–å†³äºæ¨¡å‹çš„å¤§å°ï¼ˆå¤§å‹æ¨¡å‹æ›´å¯èƒ½åæ˜ ç±»ä¼¼äººç±»æ¨¡å¼ï¼‰ä»¥åŠç”¨äºæ¿€å‘è¡Œä¸ºçš„å…ƒè¯­è¨€æç¤ºçš„é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€æ—¶æ˜¯å¦æ¨¡ä»¿äººç±»æ–¹å¼å¼•å‘äº†å¹¿æ³›è®¨è®ºã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡äººç±»å¥å­å¤„ç†ä¸­çš„ç”Ÿäº§-è§£é‡Šå·®å¼‚æ¥æ¢ç´¢LLMä¸äººç±»åœ¨è¯­è¨€å¤„ç†ä¸Šçš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œéƒ¨åˆ†LLMèƒ½å¤Ÿåæ˜ äººç±»åœ¨ä½¿ç”¨éšæ€§å› æœåŠ¨è¯æ—¶çš„ä»£è¯ç”Ÿäº§å’Œè§£é‡Šä¹‹é—´çš„ä¸å¯¹ç§°æ€§ã€‚</li>
<li>LLMçš„è¡Œä¸ºè¡¨ç°å–å†³äºæ¨¡å‹çš„å¤§å°ï¼Œå¤§å‹æ¨¡å‹æ›´å¯èƒ½å±•ç¤ºç±»ä¼¼äººç±»çš„æ¨¡å¼ã€‚</li>
<li>å…ƒè¯­è¨€æç¤ºçš„é€‰æ‹©åœ¨æ¿€å‘LLMè¡Œä¸ºè¡¨ç°ä¸Šèµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>æœ¬ç ”ç©¶çš„ä»£ç å’Œç»“æœå·²å…¬å¼€å‘å¸ƒï¼Œä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd09a1c87f177394b5ece9cbe72c654a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9e25fd3ba24d340b7685f915bc8ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87ff30618a8d5cabf0550da2b2cce486.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Benign-import-Toxic-Jailbreaking-the-Language-Model-via-Adversarial-Metaphors"><a href="#From-Benign-import-Toxic-Jailbreaking-the-Language-Model-via-Adversarial-Metaphors" class="headerlink" title="From Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors"></a>From Benign import Toxic: Jailbreaking the Language Model via   Adversarial Metaphors</h2><p><strong>Authors:Yu Yan, Sheng Sun, Zenghao Duan, Teli Liu, Min Liu, Zhiyi Yin, Jiangyu Lei, Qi Li</strong></p>
<p>Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. </p>
<blockquote>
<p>å½“å‰çš„ç ”ç©¶å·²ç»æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è¶Šç‹±æ”»å‡»ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ½œåœ¨é£é™©ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†ä»é›¶å¼€å§‹ç›´æ¥ç”Ÿæˆæœ‰å®³å†…å®¹æ¯”è¯±å¯¼LLMå°†è‰¯æ€§å†…å®¹è°ƒæ•´ä¸ºæœ‰å®³å½¢å¼æ›´ä¸ºå›°éš¾ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ”»å‡»æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨AdVersArial meTAphoRï¼ˆAVATARï¼‰æ¥è¯±å¯¼LLMè°ƒæ•´æ¶æ„éšå–»ä»¥è¿›è¡Œè¶Šç‹±ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å›ç­”æœ‰å®³æŸ¥è¯¢ï¼ŒAVATARä¼šè‡ªé€‚åº”åœ°è¯†åˆ«ä¸€ç»„è‰¯æ€§ä½†é€»è¾‘ç›¸å…³çš„éšå–»ä½œä¸ºåˆå§‹ç§å­ã€‚ç„¶åï¼Œåœ¨è¿™äº›éšå–»çš„é©±åŠ¨ä¸‹ï¼Œç›®æ ‡LLMè¢«è¯±å¯¼å¯¹éšå–»å†…å®¹è¿›è¡Œæ¨ç†å’Œè°ƒæ•´ï¼Œä»è€Œé€šè¿‡ç›´æ¥è¾“å‡ºæœ‰å®³å“åº”æˆ–è°ƒæ•´éšå–»ä¸ä¸“ä¸šæœ‰å®³å†…å®¹ä¹‹é—´çš„æ®‹å·®æ¥å®ç°è¶Šç‹±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAVATARå¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»LLMçš„æ³¨æ„åŠ›å¹¶æˆåŠŸå®ç°è¶Šç‹±ï¼Œä¸”åœ¨å¤šä¸ªé«˜çº§LLMä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00038v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2412.12145</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç”Ÿæˆæœ‰å®³å†…å®¹çš„éšæ‚£ï¼Œæ–°ç ”ç©¶æ­ç¤ºäº†ä¸€ç§åˆ©ç”¨AdVersArial meTAphoRï¼ˆAVATARï¼‰æ¡†æ¶è¯±å¯¼LLMç”Ÿæˆæœ‰å®³éšå–»çš„çªç ´æ–¹å¼ã€‚é€šè¿‡å¼•å…¥éšå–»ä½œä¸ºåˆå§‹ç§å­ï¼Œè¯±å¯¼LLMæ¨ç†å’Œæ ¡å‡†éšå–»å†…å®¹ï¼Œèƒ½å¤Ÿè¾“å‡ºæœ‰å®³å“åº”æˆ–å°†éšå–»ä¸ä¸“ä¸šæœ‰å®³å†…å®¹ç›¸ç»“åˆå®ç°çªç ´ã€‚å®éªŒè¡¨æ˜ï¼ŒAVATARæ¡†æ¶èƒ½æœ‰æ•ˆä¸”çµæ´»åœ°å¯¹å¤šä¸ªé«˜çº§LLMè¿›è¡Œçªç ´ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå­˜åœ¨ç”Ÿæˆæœ‰å®³å†…å®¹çš„éšæ‚£ã€‚</li>
<li>æ–°ç ”ç©¶åˆ©ç”¨AVATARæ¡†æ¶è¿›è¡Œçªç ´ã€‚</li>
<li>AVATARæ¡†æ¶å¼•å…¥éšå–»ä½œä¸ºåˆå§‹ç§å­ï¼Œç”¨äºè¯±å¯¼LLMç”Ÿæˆæœ‰å®³å†…å®¹ã€‚</li>
<li>LLMèƒ½è¢«è¯±å¯¼è¾“å‡ºæœ‰å®³å“åº”æˆ–ç»“åˆéšå–»ä¸ä¸“ä¸šæœ‰å®³å†…å®¹å®ç°çªç ´ã€‚</li>
<li>å®éªŒè¡¨æ˜AVATARæ¡†æ¶èƒ½æœ‰æ•ˆçªç ´å¤šä¸ªé«˜çº§LLMã€‚</li>
<li>AVATARæ¡†æ¶æ”»å‡»æˆåŠŸç‡é«˜ï¼Œå…·æœ‰å…ˆè¿›æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-300fe047a149c80df4a4b43f750b8973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156fef8506c484004d5f4b4ca8973d82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4145720bac8e152a544cd8a20b955f77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07ea07293fb01a6fa76465cdd366ec9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbe7927a628de76e7c9a181c23b2fc25.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Finite-State-Automata-Inside-Transformers-with-Chain-of-Thought-A-Mechanistic-Study-on-State-Tracking"><a href="#Finite-State-Automata-Inside-Transformers-with-Chain-of-Thought-A-Mechanistic-Study-on-State-Tracking" class="headerlink" title="Finite State Automata Inside Transformers with Chain-of-Thought: A   Mechanistic Study on State Tracking"></a>Finite State Automata Inside Transformers with Chain-of-Thought: A   Mechanistic Study on State Tracking</h2><p><strong>Authors:Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin</strong></p>
<p>Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IvanChangPKU/FSA">https://github.com/IvanChangPKU/FSA</a>. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒCoTåœ¨ç†è®ºä¸Šå¯ä»¥æé«˜è¡¨è¾¾èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºTransformer+CoTå¯ä»¥å­¦ä¹ çš„ç®—æ³•æœºåˆ¶ç†è§£æœ‰é™ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬è¯„ä¼°äº†Transformer+CoTåŠå…¶å˜ç§çš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œè¯å®äº†CoTçš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç¡®å®šäº†ç”µè·¯ï¼ˆæ¨¡å‹ç»„ä»¶çš„ä¸€ä¸ªå­é›†ï¼Œè´Ÿè´£è·Ÿè¸ªä¸–ç•ŒçŠ¶æ€ï¼‰ï¼Œè¡¨æ˜åæœŸå±‚MLPç¥ç»å…ƒèµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†å‹ç¼©å’ŒåŒºåˆ†ä¸¤ä¸ªæŒ‡æ ‡ï¼Œå¹¶æ˜¾ç¤ºæ¯ä¸ªçŠ¶æ€çš„ç¥ç»å…ƒé›†å‡ ä¹è¾¾åˆ°äº†100%çš„å‡†ç¡®ç‡ï¼Œè¿™æä¾›äº†æ¨¡å‹ä¸­åµŒå…¥çš„éšå¼æœ‰é™çŠ¶æ€è‡ªåŠ¨æœºï¼ˆFSAï¼‰çš„è¯æ®ã€‚ï¼ˆ3ï¼‰æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è®¾ç½®ï¼šè·³è¿‡ä¸­é—´æ­¥éª¤ã€å¼•å…¥æ•°æ®å™ªå£°å’Œæµ‹è¯•é•¿åº¦æ³›åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒTransformer+CoTå­¦ä¹ çš„æ˜¯ç¨³å¥çš„ç®—æ³•ï¼ˆFSAsï¼‰ï¼Œçªå‡ºå…¶åœ¨å…·æœ‰æŒ‘æˆ˜çš„åœºæ™¯ä¸­çš„åº”å˜èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IvanChangPKU/FSA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IvanChangPKU/FSAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20129v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-thoughtï¼ˆCoTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜åŠ¿ã€‚ç ”ç©¶è¯å®CoTèƒ½å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šè¯„ä¼°äº†Transformer+CoTåŠå…¶å˜ç§çš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼›è¯†åˆ«äº†è´Ÿè´£è·Ÿè¸ªä¸–ç•ŒçŠ¶æ€çš„ç”µè·¯ï¼Œå‘ç°æ™šæœŸå±‚MLPç¥ç»å…ƒèµ·å…³é”®ä½œç”¨ï¼›æå‡ºå‹ç¼©å’ŒåŒºåˆ†ä¸¤ä¸ªæŒ‡æ ‡ï¼Œè¯æ˜æ¨¡å‹å†…éƒ¨å­˜åœ¨éšå¼æœ‰é™çŠ¶æ€è‡ªåŠ¨æœºï¼ˆFSAï¼‰ï¼›åœ¨è·³è¿‡ä¸­é—´æ­¥éª¤ã€å¼•å…¥æ•°æ®å™ªå£°å’Œæµ‹è¯•é•¿åº¦æ³›åŒ–ç­‰æŒ‘æˆ˜åœºæ™¯ä¸‹ï¼ŒTransformer+CoTè¡¨ç°å‡ºç¨³å¥çš„ç®—æ³•å­¦ä¹ èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-thoughtï¼ˆCoTï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†Transformer+CoTçš„çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯†åˆ«äº†è´Ÿè´£è·Ÿè¸ªä¸–ç•ŒçŠ¶æ€çš„ç”µè·¯ï¼Œå…¶ä¸­æ™šæœŸå±‚MLPç¥ç»å…ƒèµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>æå‡ºå‹ç¼©å’ŒåŒºåˆ†ä¸¤ä¸ªæŒ‡æ ‡ï¼Œè¯æ˜äº†æ¨¡å‹å†…éƒ¨å­˜åœ¨éšå¼æœ‰é™çŠ¶æ€è‡ªåŠ¨æœºï¼ˆFSAï¼‰ã€‚</li>
<li>é€šè¿‡å¯¹ç‰¹å®šç”µè·¯çš„åˆ†æï¼Œå‘ç°è¯¥ç”µè·¯èƒ½å¤Ÿå®ç°è¿‘ä¹100%çš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨æŒ‘æˆ˜åœºæ™¯ä¸‹ï¼Œå¦‚è·³è¿‡ä¸­é—´æ­¥éª¤ã€å¼•å…¥æ•°æ®å™ªå£°å’Œæµ‹è¯•é•¿åº¦æ³›åŒ–ï¼ŒTransformer+CoTå±•ç°å‡ºç¨³å¥çš„ç®—æ³•å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2214e3041d8e36d0102ea125ed370069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d832b8f42df205a15ec2400d9ee15ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1420f99dcd90097be0702c3c346da69b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86d34f72fc0254c6d50d12cb7bea562c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level â€œnovelty.â€ Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. The code is available at <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a>. </p>
<blockquote>
<p>æ•°æ®çš„å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šç§æ„è¯†åˆ°çš„æ•°æ®é€‰æ‹©æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹çš„æ˜ç¡®æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¤§é‡å¾®è°ƒå®éªŒï¼Œç³»ç»Ÿåœ°åˆ†æäº†19ç§ç°æœ‰çš„å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œè¯„ä¼°å®ƒä»¬ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§æµ‹é‡åº”é€‚å½“åœ°è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚å’Œæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯å¯†åº¦ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†NovelSumï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„æ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumèƒ½å¤Ÿå‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„å…³è”åº¦è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UmeanNever/NovelSumè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v5">PDF</a> Accepted at ACL 2025 Main. Camera-ready version updated (20 pages).   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¯¹æ•°æ®å¤šæ ·æ€§äºˆä»¥å…³æ³¨ã€‚å½“å‰çš„ç ”ç©¶å·²æ¢ç´¢äº†å„ç§å¤šæ ·æ€§æ„ŸçŸ¥çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæ—¨åœ¨æ„å»ºé«˜è´¨é‡æ•°æ®é›†ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•ç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„åŸºç¡€é—®é¢˜ä»è¢«å¿½è§†ï¼Œå¯¼è‡´æ•°æ®å·¥ç¨‹ç¼ºä¹æ˜ç¡®çš„æŒ‡å¯¼æ–¹å‘ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†ç°æœ‰çš„åä¸€ç§æ•°æ®å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œé€šè¿‡å¤§é‡å¾®è°ƒå®éªŒè¯„ä¼°å®ƒä»¬ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸€ä¸ªå¯é çš„æ•°æ®å¤šæ ·æ€§åº¦é‡æ–¹æ³•åº”è¯¥é€‚å½“è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´çš„ä¿¡æ¯å¯†åº¦ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§åº¦é‡æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒNovelSumèƒ½å‡†ç¡®æ•æ‰æ•°æ®å¤šæ ·æ€§çš„å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œå¯¹äºæŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µå…·æœ‰é‡è¦æ„ä¹‰ã€‚åˆ©ç”¨NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ç ”ç©¶è™½ç„¶å·²ç»æ¢ç´¢äº†å¤šç§æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä½†å¯¹æ•°æ®å¤šæ ·æ€§çš„å®šä¹‰å’Œæµ‹é‡ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¯é çš„æ•°æ®å¤šæ ·æ€§æµ‹é‡æ–¹æ³•åº”è€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚å’Œæ ·æœ¬ç©ºé—´çš„ä¿¡æ¯å¯†åº¦ã€‚</li>
<li>æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæŒ‡æ ‡æ¥è¡¡é‡æ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>NovelSumèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ•°æ®å¤šæ ·æ€§çš„å˜åŒ–ï¼Œä¸æ¨¡å‹æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚</li>
<li>ä½¿ç”¨NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdfd83bb07cde7421bfe3d1965cc7487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1829b232e597669113fab54b414da2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1437bd13f715f44852b5091606942a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1c9677921012d9e6ef83a11ec2a6e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revisiting-3D-LLM-Benchmarks-Are-We-Really-Testing-3D-Capabilities"><a href="#Revisiting-3D-LLM-Benchmarks-Are-We-Really-Testing-3D-Capabilities" class="headerlink" title="Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"></a>Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?</h2><p><strong>Authors:Jiahe Jin, Yanheng He, Mingyan Yang</strong></p>
<p>In this work, we identify the â€œ2D-Cheatingâ€ problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMsâ€™ unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks</a> . </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†3Då¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„â€œ2Dæ¬ºéª—â€é—®é¢˜ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ç‚¹äº‘æ¸²æŸ“å›¾åƒï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å¯èƒ½å¾ˆå®¹æ˜“è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™æš´éœ²äº†å½“å‰è¯„ä¼°æ–¹æ³•å¯¹3Då¤§å‹è¯­è¨€æ¨¡å‹çš„ç‹¬ç‰¹ä¸‰ç»´èƒ½åŠ›çš„æ— æ•ˆè¯„ä¼°ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šæµ‹è¯•äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºå‚è€ƒï¼Œæå‡ºäº†æ›´å¥½åœ°è¯„ä¼°çœŸå®ä¸‰ç»´ç†è§£çš„åŸåˆ™ã€‚æˆ‘ä»¬è¿˜ä¸»å¼ åœ¨è¯„ä¼°ä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå°†ä¸‰ç»´èƒ½åŠ›ä¸ä¸€ç»´æˆ–äºŒç»´æ–¹é¢æ˜ç¡®åŒºåˆ†å¼€æ¥ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D--LLM-Benchmarks%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarksä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08503v2">PDF</a> Accepted to ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºäº†åœ¨è¯„ä¼°ä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶å­˜åœ¨çš„â€œäºŒç»´æ¬ºéª—â€é—®é¢˜ã€‚éƒ¨åˆ†ä»»åŠ¡å¯èƒ½é€šè¿‡ç‚¹äº‘æ¸²æŸ“å›¾åƒç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è½»æ˜“è§£å†³ï¼Œæœªèƒ½æœ‰æ•ˆè¯„ä¼°LLMç‹¬ç‰¹çš„ä¸‰ç»´èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†VLMåœ¨å¤šä¸ªäººå½¢è¯­è¨€æ¨¡å‹è¯„ä¼°æ ‡å‡†ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºåŸåˆ™æ”¹è¿›çœŸå®çš„ä¸‰ç»´ç†è§£è¯„ä¼°æ–¹å¼ã€‚åŒæ—¶ä¸»å¼ åœ¨è¯„ä¼°ä¸‰ç»´LLMæ—¶æ˜ç¡®åŒºåˆ†å…¶ä¸‰ç»´èƒ½åŠ›ä¸ä¸€ç»´æˆ–äºŒç»´æ–¹é¢ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡è®¿é—®ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks">https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œäºŒç»´æ¬ºéª—â€é—®é¢˜åœ¨è¯„ä¼°ä¸‰ç»´LLMæ—¶å­˜åœ¨ï¼Œå³æŸäº›ä»»åŠ¡å¯èƒ½é€šè¿‡ç®€å•çš„å›¾åƒæ¸²æŸ“è¢«VLMè½»æ˜“è§£å†³ï¼Œæœªèƒ½çœŸæ­£è¯„ä¼°LLMçš„ä¸‰ç»´èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¤šä¸ªä¸‰ç»´LLMåŸºå‡†æµ‹è¯•VLMçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºæ”¹è¿›åŸåˆ™ä»¥æ›´æœ‰æ•ˆåœ°è¯„ä¼°LLMçœŸæ­£çš„ä¸‰ç»´ç†è§£èƒ½åŠ›ã€‚</li>
<li>åœ¨è¯„ä¼°ä¸‰ç»´LLMæ—¶ï¼Œéœ€è¦æ˜ç¡®åŒºåˆ†å…¶ä¸‰ç»´èƒ½åŠ›ä¸ä¸€ç»´æˆ–äºŒç»´çš„æ–¹é¢ã€‚</li>
<li>ç ”ç©¶ä»£ç å’Œæ•°æ®å¯åœ¨ç‰¹å®šGitHubé¡µé¢è·å–ã€‚</li>
<li>æœ‰æ•ˆè¯„ä¼°LLMçš„ä¸‰ç»´èƒ½åŠ›å¯¹äºæå‡æ¨¡å‹çš„å®é™…åº”ç”¨æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63a240f0e1a766c25de3fff35638ffc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b805855e23f77446f52f0001c4ebf49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c19b9d6519ec9231f3c2a8ef7c9f31c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d2fb4234cbbee37fb362cfa9571f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0ebf1cd5306869132b93756d55ef5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df5615e246a88474b510c691089471da.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Lessons-of-Developing-Process-Reward-Models-in-Mathematical-Reasoning"><a href="#The-Lessons-of-Developing-Process-Reward-Models-in-Mathematical-Reasoning" class="headerlink" title="The Lessons of Developing Process Reward Models in Mathematical   Reasoning"></a>The Lessons of Developing Process Reward Models in Mathematical   Reasoning</h2><p><strong>Authors:Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin</strong></p>
<p>Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ•°å­¦æ¨ç†ä¸­æµç¨‹ç›‘ç£çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•è€Œå‡ºç°ï¼Œæ—¨åœ¨è¯†åˆ«å’Œç¼“è§£æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´é”™è¯¯ã€‚ç„¶è€Œï¼Œå¼€å‘æœ‰æ•ˆçš„PRMé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨å’Œè¯„ä¼°æ–¹æ³•æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œå¸¸ç”¨çš„åŸºäºè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡çš„æ•°æ®åˆæˆé€šå¸¸ä¼šäº§ç”Ÿæ¯”LLMä½œä¸ºè¯„å§”å’Œäººç±»æ ‡æ³¨æ–¹æ³•æ›´å·®çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚MCä¼°è®¡ä¾èµ–äºå®Œæˆæ¨¡å‹æ¥è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œä»è€Œå¯¼è‡´ä¸å‡†ç¡®çš„æ­¥éª¤éªŒè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥å¯¹PRMsçš„æ½œåœ¨åè§ï¼šï¼ˆ1ï¼‰ä¸å¯é çš„æ”¿ç­–æ¨¡å‹äº§ç”Ÿçš„å›ç­”è™½ç„¶æœ‰æ­£ç¡®ç­”æ¡ˆï¼Œä½†è¿‡ç¨‹å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´BoNçš„è¯„ä¼°æ ‡å‡†ä¸PRMçš„ç›®æ ‡ï¼ˆè¿‡ç¨‹éªŒè¯ï¼‰ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ã€‚ï¼ˆ2ï¼‰PRMå¯¹æ­¤ç±»å›åº”çš„å®¹å¿åº¦å¯¼è‡´BoNåˆ†æ•°è†¨èƒ€ã€‚ï¼ˆ3ï¼‰ç°æœ‰PRMçš„æœ€ä½åˆ†æ•°æœ‰å¾ˆå¤§ä¸€éƒ¨åˆ†é›†ä¸­åœ¨æœ€ç»ˆç­”æ¡ˆçš„æ­¥éª¤ä¸Šï¼Œè¿™æ­ç¤ºäº†BoNä¼˜åŒ–PRMä¸­ä»è¿‡ç¨‹è½¬å‘ç»“æœå¯¼å‘è¯„ä¼°çš„è½¬å˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°å°†MCä¼°è®¡ä¸LLMä½œä¸ºè¯„å§”ç›¸ç»“åˆï¼Œå¹¶å€¡å¯¼ä¸€ä¸ªæ›´å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå“åº”çº§å’Œæ­¥éª¤çº§æŒ‡æ ‡ã€‚åŸºäºè¿™äº›æœºåˆ¶ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†BoNè¯„ä¼°å’Œæ­¥éª¤çº§é”™è¯¯è¯†åˆ«ä»»åŠ¡ä¸­çš„æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå®ƒä¼˜äºç°æœ‰çš„å¼€æºæ›¿ä»£å“ï¼Œå¹¶ä¸ºæœªæ¥å»ºç«‹æµç¨‹ç›‘ç£æ¨¡å‹çš„ç ”ç©¶æä¾›å®ç”¨æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07301v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£é—®é¢˜ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæå‡ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ç”¨äºè¯†åˆ«å¹¶å‡è½»æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´é”™è¯¯ã€‚ç„¶è€Œï¼Œå¼€å‘æœ‰æ•ˆçš„PRMé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ ‡æ³¨å’Œè¯„ä¼°æ–¹æ³•æ–¹é¢ã€‚æœ¬æ–‡é€šè¿‡å¤§é‡å®éªŒå‘ç°ï¼Œå¸¸ç”¨çš„åŸºäºè’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡çš„æ•°æ®åˆæˆæ–¹æ³•å¯¹PRMçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œç›¸è¾ƒäºLLMä½œä¸ºæ³•å®˜å’Œäººç±»æ ‡æ³¨æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚MCä¼°è®¡ä¾èµ–äºå®Œæˆæ¨¡å‹æ¥è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œä»è€Œå¯¼è‡´ä¸å‡†ç¡®çš„æ­¥éª¤éªŒè¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†ä¼ ç»Ÿæœ€ä½³Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥å¯¹PRMçš„æ½œåœ¨åè§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†MCä¼°è®¡å’ŒLLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ï¼Œå¹¶æå€¡é‡‡ç”¨æ›´å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå“åº”çº§å’Œæ­¥éª¤çº§æŒ‡æ ‡ã€‚åŸºäºè¯¥æœºåˆ¶ï¼Œæˆ‘ä»¬åœ¨BoNè¯„ä¼°å’Œé€æ­¥é”™è¯¯è¯†åˆ«ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç§æ–°å‹çš„å…ˆè¿›PRMï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ›¿ä»£å“ï¼Œå¹¶ä¸ºæœªæ¥åœ¨æ„å»ºæµç¨‹ç›‘ç£æ¨¡å‹æ–¹é¢çš„ç ”ç©¶æä¾›äº†å®ç”¨æŒ‡å—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PRMæ—¨åœ¨è¯†åˆ«å’Œå‡è½»LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´é”™è¯¯ã€‚</li>
<li>åŸºäºMCä¼°è®¡çš„æ•°æ®åˆæˆæ–¹æ³•å¯¹PRMçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›é€šå¸¸è¾ƒå·®ã€‚</li>
<li>MCä¼°è®¡åœ¨è¯„ä¼°å½“å‰æ­¥éª¤æ­£ç¡®æ€§æ—¶å­˜åœ¨ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>ä¼ ç»ŸBoNè¯„ä¼°ç­–ç•¥å¯¹PRMå­˜åœ¨æ½œåœ¨åè§ï¼Œå¦‚ç­–ç•¥çš„ä¸å¯é æ€§ã€å¯¹ç‰¹å®šå“åº”çš„å®¹å¿åº¦ä»¥åŠæœ€ç»ˆç­”æ¡ˆæ­¥éª¤ä¸­æœ€ä½åˆ†æ•°é›†ä¸­ç­‰é—®é¢˜ã€‚</li>
<li>å…±è¯†è¿‡æ»¤æœºåˆ¶ç»“åˆäº†MCä¼°è®¡å’ŒLLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚</li>
<li>æ–°çš„è¯„ä¼°æ¡†æ¶ç»“åˆäº†å“åº”çº§å’Œæ­¥éª¤çº§æŒ‡æ ‡ï¼Œæä¾›æ›´å…¨é¢çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a8935a784fa2a550b0a0d6e4ab8ea68a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56774a018f1843c445fbc6159fce2476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-382647788b696ef9585e946fdae5d4b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13af295997a5d9a88235e85cc6c67919.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale"><a href="#MAmmoTH-VL-Eliciting-Multimodal-Reasoning-with-Instruction-Tuning-at-Scale" class="headerlink" title="MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale"></a>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at   Scale</h2><p><strong>Authors:Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue</strong></p>
<p>Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process. </p>
<blockquote>
<p>å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†èƒ½åŠ›å—åˆ°ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„çº¦æŸï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦ä»è¯¸å¦‚VQAã€AI2Då’ŒChartQAç­‰å­¦æœ¯æ•°æ®é›†ä¸­é‡æ–°åˆ©ç”¨ã€‚è¿™äº›æ•°æ®é›†é’ˆå¯¹ç®€å•ä»»åŠ¡ï¼Œä»…æä¾›çŸ­è¯­çº§ç­”æ¡ˆï¼Œè€Œä¸æä¾›ä»»ä½•ä¸­é—´æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•æ¥æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„ä¸­é—´æ¨ç†ï¼Œæ—¨åœ¨æ¿€å‘CoTæ¨ç†ã€‚æˆ‘ä»¬ä»…ä½¿ç”¨å…¬å¼€æ¨¡å‹åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«12MæŒ‡ä»¤å“åº”å¯¹çš„æ•°æ®é›†ï¼Œä»¥æ¶µç›–å¤šæ ·åŒ–ã€æ¨ç†å¯†é›†çš„ä»»åŠ¡ï¼Œå…·æœ‰è¯¦ç»†ä¸”å‡†ç¡®çš„æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒMLLMså¯æ˜¾è‘—æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVerseï¼ˆ+8.1%ï¼‰ã€MMMU-Proï¼ˆ+7%ï¼‰å’ŒMuirBenchï¼ˆ+13.3%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨éæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œæœ€é«˜å¯è¾¾4%ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­çš„å…³é”®ç»„ä»¶ï¼ˆå¦‚é‡å†™å’Œè‡ªè¿‡æ»¤ï¼‰çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05237v2">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡å¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å—é™äºç°æœ‰æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§„æ¨¡åŒ–ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„ä¸­é—´æ¨ç†ä¾æ®ã€‚å®éªŒè¯æ˜ï¼Œè®­ç»ƒäºè¯¥æ•°æ®é›†çš„MLLMså±•ç°å‡ºæ›´é«˜çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆã€‚å¦‚MathVerseæé«˜8.1%ï¼ŒMMMU-Proæé«˜7%ï¼ŒMuirBenchæé«˜è¾¾åˆ°ä»¤äººç©ç›®çš„å¹…åº¦ï¼ˆ+13.3%ï¼‰ã€‚éåŸºäºæ¨ç†åŸºå‡†çš„æµ‹è¯•ä¸­ä¹Ÿè§‚å¯Ÿåˆ°æ˜æ˜¾çš„è¿›æ­¥å¹…åº¦ï¼Œæœ€å¤§è¾¾4%ã€‚åŒæ—¶ï¼Œå¯¹å…³é”®æ„å»ºç¯èŠ‚çš„ç ”ç©¶å¼ºè°ƒäº†é‡å†™å’Œè‡ªè¿‡æ»¤çš„é‡è¦æ€§ã€‚è¿™ä¸€æ–°æ–¹æ³•æœ‰åŠ©äºè¿›ä¸€æ­¥æ¨åŠ¨MLLMsåœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚è¯¥æ‘˜è¦ç®€æ˜æ‰¼è¦åœ°æ¦‚æ‹¬äº†æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚ä½†å…¶æ¨ç†èƒ½åŠ›å—é™äºä»¥å­¦æœ¯æ•°æ®é›†ä¸ºä¸»æ„å»ºçš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€å…³æ³¨ç®€åŒ–ä»»åŠ¡ï¼Œç¼ºä¹ä¸­é—´æ¨ç†ä¾æ®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ„å»ºå¤§å‹å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c991591cd1594df10dbb9b9f3749e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48331a7147d897c0599a389172ece718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf951a075892bb8d29ed7cf688cb4fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86325b96350dceba0bfd1fb3ad2803e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48fc283a0424105f7cf736f090390e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3613d55f2d2d9c529549aa40c2f2d071.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Focus-On-This-Not-That-Steering-LLMs-with-Adaptive-Feature-Specification"><a href="#Focus-On-This-Not-That-Steering-LLMs-with-Adaptive-Feature-Specification" class="headerlink" title="Focus On This, Not That! Steering LLMs with Adaptive Feature   Specification"></a>Focus On This, Not That! Steering LLMs with Adaptive Feature   Specification</h2><p><strong>Authors:Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto</strong></p>
<p>Despite the success of Instruction Tuning (IT) in training large language models (LLMs), such models often leverage spurious or biased features learnt from their training data and can become misaligned, leading to undesired behaviours. While existing techniques can steer model behaviour at inference-time, they are often post-hoc and do not embed steering as an intrinsic model feature. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across diverse benchmarks, we demonstrate that FIT: (i) successfully steers behaviour at inference time; (ii) increases robustness by amplifying core task signals and down-weighting spurious cues; (iii) mitigates social bias by suppressing demographic attributes; and (iv) generalises under distribution shifts and to previously unseen focus features. FIT therefore offers a lightweight, intrinsic mechanism for building more robust, fair, and easily controllable LLMs. </p>
<blockquote>
<p>å°½ç®¡æŒ‡ä»¤è°ƒæ•´ï¼ˆITï¼‰åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†è¿™äº›æ¨¡å‹ç»å¸¸åˆ©ç”¨ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„è™šå‡æˆ–åè§ç‰¹å¾ï¼Œå¹¶å¯èƒ½å‡ºç°åå·®ï¼Œå¯¼è‡´å‡ºç°ä¸éœ€è¦çš„è¡Œä¸ºã€‚è™½ç„¶ç°æœ‰æŠ€æœ¯å¯ä»¥åœ¨æ¨ç†æ—¶å¼•å¯¼æ¨¡å‹è¡Œä¸ºï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯äº‹åæ€§çš„ï¼Œå¹¶æ²¡æœ‰å°†å¼•å¯¼ä½œä¸ºæ¨¡å‹çš„å†…åœ¨ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç„¦ç‚¹æŒ‡ä»¤è°ƒæ•´ï¼ˆFITï¼‰ï¼Œå®ƒè®­ç»ƒLLMé€šè¿‡ä¸“æ³¨äºç‰¹å®šç‰¹å¾è€Œå¿½è§†å…¶ä»–ç‰¹å¾æ¥æ¡ä»¶åŒ–å…¶å“åº”ï¼Œæ ¹æ®æŒ‡å®šçš„ç‰¹å¾è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†FITï¼šï¼ˆiï¼‰æˆåŠŸåœ°åœ¨æ¨ç†æ—¶é—´å¼•å¯¼è¡Œä¸ºï¼›ï¼ˆiiï¼‰é€šè¿‡æ”¾å¤§æ ¸å¿ƒä»»åŠ¡ä¿¡å·å’Œé™ä½è™šå‡çº¿ç´¢æ¥å¢å¼ºç¨³å¥æ€§ï¼›ï¼ˆiiiï¼‰é€šè¿‡æŠ‘åˆ¶äººå£ç»Ÿè®¡å±æ€§æ¥ç¼“è§£ç¤¾ä¼šåè§ï¼›ï¼ˆivï¼‰åœ¨åˆ†å¸ƒå˜åŒ–å’Œä»¥å‰æœªè§è¿‡çš„ç„¦ç‚¹ç‰¹å¾ä¸‹å®ç°æ¨å¹¿ã€‚å› æ­¤ï¼ŒFITæä¾›äº†ä¸€ç§è½»ä¾¿ã€å†…åœ¨çš„æœºåˆ¶ï¼Œç”¨äºæ„å»ºæ›´ç¨³å¥ã€å…¬å¹³å’Œæ˜“äºæ§åˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22944v4">PDF</a> 36pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Focus Instruction Tuningï¼ˆFITï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥åœ¨æ¨ç†æ—¶ä¸“æ³¨äºç‰¹å®šç‰¹å¾è€Œå¿½è§†å…¶ä»–ç‰¹å¾ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶ã€‚é€šè¿‡å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ï¼Œä½œè€…å±•ç¤ºäº†FITæŠ€æœ¯çš„å››ä¸ªä¼˜åŠ¿ï¼šæˆåŠŸåœ¨æ¨ç†æ—¶é—´å¼•å¯¼è¡Œä¸ºã€é€šè¿‡æ”¾å¤§æ ¸å¿ƒä»»åŠ¡ä¿¡å·å’Œå‰Šå¼±è¯¯å¯¼æ€§çº¿ç´¢æ¥æé«˜ç¨³å¥æ€§ã€é€šè¿‡æŠ‘åˆ¶äººå£ç»Ÿè®¡ç‰¹å¾æ¥å‡è½»ç¤¾ä¼šåè§ä»¥åŠåœ¨åˆ†å¸ƒå˜åŒ–å’Œå…ˆå‰æœªè§åˆ°çš„ç„¦ç‚¹ç‰¹å¾ä¸Šå®ç°æ³›åŒ–ã€‚å› æ­¤ï¼ŒFITæä¾›äº†ä¸€ç§è½»ä¾¿çš„å†…åœ¨æœºåˆ¶ï¼Œç”¨äºæ„å»ºæ›´ç¨³å¥ã€å…¬å¹³ä¸”æ˜“äºæ§åˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FITæ˜¯ä¸€ç§ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æŠ€æœ¯ï¼Œèƒ½ä½¿æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸“æ³¨äºç‰¹å®šç‰¹å¾è€Œå¿½ç•¥å…¶ä»–ç‰¹å¾ï¼Œä»è€Œæ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>FITæŠ€æœ¯é€šè¿‡æ”¾å¤§æ ¸å¿ƒä»»åŠ¡ä¿¡å·å’Œå‰Šå¼±è¯¯å¯¼æ€§çº¿ç´¢æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>FITå¯ä»¥å‡è½»ç¤¾ä¼šåè§ï¼Œé€šè¿‡æŠ‘åˆ¶äººå£ç»Ÿè®¡ç‰¹å¾æ¥é¿å…æ¨¡å‹çš„æ½œåœ¨åè§ã€‚</li>
<li>FITåœ¨åˆ†å¸ƒå˜åŒ–å’Œå…ˆå‰æœªè§åˆ°çš„ç„¦ç‚¹ç‰¹å¾ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>FITæä¾›äº†ä¸€ç§å†…åœ¨æœºåˆ¶æ¥æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æ›´æ˜“äºæ§åˆ¶ã€‚</li>
<li>é€šè¿‡å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†FITæŠ€æœ¯çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86584546ffd9de69a1a7e6db3442a39d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant"><a href="#Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant" class="headerlink" title="Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant"></a>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant</h2><p><strong>Authors:Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon</strong></p>
<p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a modelâ€™s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning. </p>
<blockquote>
<p>é‡åŒ–æŠ€æœ¯ä½œä¸ºéƒ¨ç½²å¤§å°è¯­è¨€æ¨¡å‹çš„ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œä»…é™äºå›°æƒ‘åº¦æˆ–åŸºæœ¬çŸ¥è¯†ä»»åŠ¡ï¼Œç¼ºä¹å¯¹Llama-3.3ç­‰æœ€æ–°æ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹è·¨è¶Šäº†ä»åƒäº¿å­—èŠ‚åˆ°åä¸‡äº¿å­—èŠ‚çš„å‚æ•°èŒƒå›´ï¼Œå¹¶åº”ç”¨äº†å››ç§é‡åŒ–æ–¹æ³•æ¶µç›–åä¸‰ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼šï¼ˆä¸€ï¼‰é‡åŒ–æ¨¡å‹é€šå¸¸è¶…è¶Šè¾ƒå°çš„FP16åŸºçº¿æ¨¡å‹ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æŒ‡ä»¤è·Ÿéšå’Œå¹»åƒæ£€æµ‹æ–¹é¢å­˜åœ¨å›°éš¾ï¼›ï¼ˆäºŒï¼‰FP8åœ¨å„ç§ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºæœ€ç¨³å¥çš„é€‰æ‹©ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢å€¾å‘äºä¼˜äºGPTQï¼›ï¼ˆä¸‰ï¼‰è¾ƒå°çš„æ¨¡å‹åœ¨å››æ¯”ç‰¹é‡åŒ–æ—¶å¯èƒ½ä¼šé­å—ä¸¥é‡çš„ç²¾åº¦æŸå¤±ï¼Œè€Œè§„æ¨¡ä¸ºä¸ƒåäº¿å­—èŠ‚çš„æ¨¡å‹åˆ™èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼›ï¼ˆå››ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰å›°éš¾ä»»åŠ¡éƒ½ä¼šç»å†æœ€å¤§çš„ç²¾åº¦æŸå¤±ï¼Œè¿™è¡¨æ˜é‡åŒ–æŠ€æœ¯æ”¾å¤§äº†ä¸€ä¸ªæ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œéä»…ä»…æ˜¯ä¸ä»»åŠ¡éš¾åº¦çš„å…³è”ï¼›ï¼ˆäº”ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ï¼ˆMT-Benchï¼‰çªæ˜¾äº†åœ¨ç¼–ç å’ŒSTEMä»»åŠ¡ä¸­çš„æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œå°½ç®¡æœ‰æ—¶å®ƒä¼šåœ¨æ¨ç†æ–¹é¢æŠ¥å‘Šæ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11055v6">PDF</a> Accepted in IJCAI 2025, 21 pages, 2 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„é‡åŒ–æ•ˆæœï¼Œæ¶‰åŠä»1Båˆ°405Bå‚æ•°çš„æ¨¡å‹ï¼Œåº”ç”¨å››ç§é‡åŒ–æ–¹æ³•ï¼Œè·¨è¶Š13ä¸ªæ•°æ®é›†è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œé‡åŒ–æ¨¡å‹ä¸€èˆ¬è¶…è¶Šè¾ƒå°çš„FP16åŸºå‡†æ¨¡å‹ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢å¸¸æœ‰é—®é¢˜ï¼›FP8åœ¨ä»»åŠ¡ä¸Šè¡¨ç°æœ€ç¨³å¥ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢ä¼˜äºGPTQï¼›å°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶å‡†ç¡®æ€§å¤§å¹…ä¸‹é™ï¼Œè€Œ70Bè§„æ¨¡æ¨¡å‹è¡¨ç°ç¨³å®šï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡åŒ–å¹¶ä¸æ€»æ˜¯ä¸ä»»åŠ¡éš¾åº¦ç›´æ¥ç›¸å…³ï¼Œè€Œæ˜¯ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼›LLMåˆ¤æ–­ï¼ˆMT-Benchï¼‰åœ¨ç¼–ç å’ŒSTEMä»»åŠ¡ä¸­æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œä½†åœ¨æ¨ç†ä»»åŠ¡ä¸­æœ‰æ—¶æœ‰æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–æ¨¡å‹é€šå¸¸ä¼˜äºè¾ƒå°çš„FP16åŸºå‡†æ¨¡å‹ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>FP8åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°æœ€ç¨³å¥ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢ä¼˜äºGPTQã€‚</li>
<li>å°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œè€Œè¾ƒå¤§æ¨¡å‹ï¼ˆå¦‚70Bï¼‰è¡¨ç°ç›¸å¯¹ç¨³å®šã€‚</li>
<li>é‡åŒ–æ”¾å¤§äº†æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œå¹¶ä¸æ€»æ˜¯ä¸ä»»åŠ¡éš¾åº¦ç›´æ¥ç›¸å…³ã€‚</li>
<li>LLMåˆ¤æ–­ï¼ˆMT-Benchï¼‰åœ¨ç¼–ç å’ŒSTEMä»»åŠ¡ä¸­æ£€æµ‹åˆ°æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä½†åœ¨æŸäº›æ¨ç†ä»»åŠ¡ä¸­æœ‰æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†ä»1Båˆ°405Bå‚æ•°çš„æ¨¡å‹ï¼Œåº”ç”¨äº†å››ç§é‡åŒ–æ–¹æ³•ï¼Œå¹¶è·¨13ä¸ªæ•°æ®é›†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7230159c5d87c4bb90c4d52f4360756e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f8be894892e570711864b8b6462dbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76cc5736fac1d2ddc5d3debffb65997.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6748fa4dc153505de77db20a110c062f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mosaic-IT-Cost-Free-Compositional-Data-Synthesis-for-Instruction-Tuning"><a href="#Mosaic-IT-Cost-Free-Compositional-Data-Synthesis-for-Instruction-Tuning" class="headerlink" title="Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning"></a>Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning</h2><p><strong>Authors:Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, Tianyi Zhou</strong></p>
<p>Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses for training, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human&#x2F;model-free compositional data synthesis method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and an 80% reduction in training costs compared with original instruction tuning. Our codes and data are available at <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a>. </p>
<blockquote>
<p>é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šç§æŒ‡ä»¤-å“åº”å¯¹çš„å¾®è°ƒï¼Œå¢å¼ºäº†å…¶ç†è§£å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚å½“å‰çš„æŒ‡ä»¤è°ƒæ•´ä¸»è¦ä¾èµ–äºæ•™å¸ˆæ¨¡å‹æˆ–äººå·¥å¹²é¢„æ¥ç”Ÿæˆå’Œç»†åŒ–è®­ç»ƒå’Œå“åº”æŒ‡ä»¤ï¼Œè¿™ç§æ–¹å¼æˆæœ¬é«˜æ˜‚ã€ä¸å¯æŒç»­ï¼Œå¹¶ä¸”å¯èƒ½ç¼ºä¹å¤šæ ·æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mosaic Instruction Tuningï¼ˆMosaic-ITï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€äººå·¥å‚ä¸æ¨¡å‹ä»‹å…¥çš„ç»„åˆæ•°æ®åˆæˆæ–¹æ³•ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°ä»ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸­åˆ›å»ºä¸°å¯Œå¤šæ ·çš„å¢å¼ºæ•°æ®ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚Mosaic-ITéšæœºè¿æ¥å¤šä¸ªæŒ‡ä»¤æ•°æ®ä¸ºä¸€ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶é€šè¿‡é¢„è®¾çš„é«˜çº§å…ƒæŒ‡ä»¤è®­ç»ƒæ¨¡å‹ç”Ÿæˆç›¸åº”çš„å“åº”ï¼Œä»¥å¢å¼ºå…¶éµå¾ªå¤šæ­¥éª¤æŒ‡ä»¤å’Œæ ¼å¼çš„æŠ€èƒ½ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒMosaic-ITå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ€§èƒ½æå‡çš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæˆæœ¬æ–¹é¢ç›¸è¾ƒäºä¼ ç»Ÿçš„æŒ‡ä»¤è°ƒæ•´å‡å°‘äº†80%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13326v3">PDF</a> ACL2025, Camera-ready</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¤šæ ·åŒ–çš„æŒ‡ä»¤-å“åº”å¯¹è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†å…¶ç†è§£å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚å½“å‰ä¸»è¦çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¾èµ–äºæ•™å¸ˆæ¨¡å‹æˆ–äººå·¥ç”Ÿæˆå’Œç»†åŒ–æŒ‡ä»¤å’Œå“åº”æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™æˆæœ¬é«˜æ˜‚ã€ä¸å¯æŒç»­ä¸”å¯èƒ½ç¼ºä¹å¤šæ ·æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMosaic Instruction Tuningï¼ˆMosaic-ITï¼‰çš„äºº&#x2F;æ¨¡å‹æ— å…³çš„ç»„åˆæ•°æ®åˆæˆæ–¹æ³•ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°ä»ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸­åˆ›å»ºä¸°å¯Œä¸”å¤šæ ·åŒ–çš„å¢å¼ºæ•°æ®ï¼Œä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚Mosaic-ITé€šè¿‡éšæœºè¿æ¥å¤šä¸ªæŒ‡ä»¤æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨é¢„å®šä¹‰çš„é«˜çº§å…ƒæŒ‡ä»¤äº§ç”Ÿç›¸åº”çš„å“åº”ï¼Œæé«˜å…¶éµå¾ªå¤šæ­¥éª¤æŒ‡ä»¤å’Œæ ¼å¼çš„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMosaic-ITæ€§èƒ½å“è¶Šï¼Œè®­ç»ƒæ•ˆç‡é«˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ€§èƒ½æå‡ï¼Œä¸åŸå§‹æŒ‡ä»¤è°ƒæ•´ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬é™ä½äº†80%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/tianyi-lab/Mosaic-IT">https://github.com/tianyi-lab/Mosaic-IT</a> è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æŒ‡ä»¤-å“åº”å¯¹å¾®è°ƒå¢å¼ºäº†ç†è§£å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸»è¦ä¾èµ–æ•™å¸ˆæ¨¡å‹æˆ–äººå·¥ç”Ÿæˆæ•°æ®ï¼Œå­˜åœ¨æˆæœ¬é«˜ã€ä¸å¯æŒç»­å’Œç¼ºä¹å¤šæ ·æ€§ç­‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥Mosaic Instruction Tuningï¼ˆMosaic-ITï¼‰æ–¹æ³•ï¼Œå®ç°äºº&#x2F;æ¨¡å‹æ— å…³çš„ç»„åˆæ•°æ®åˆæˆã€‚</li>
<li>Mosaic-ITé€šè¿‡éšæœºè¿æ¥å¤šä¸ªæŒ‡ä»¤æ•°æ®å¹¶è®­ç»ƒï¼Œæé«˜æ¨¡å‹éµå¾ªå¤šæ­¥éª¤æŒ‡ä»¤å’Œæ ¼å¼çš„èƒ½åŠ›ã€‚</li>
<li>Mosaic-ITåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸åŸå§‹æŒ‡ä»¤è°ƒæ•´ç›¸æ¯”ï¼ŒMosaic-ITè®­ç»ƒæˆæœ¬é™ä½äº†80%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6d64923086780128e8adac40395f28f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420962e14d438546ce8aa6456695e95b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71b5d1ddcb890d4333dd3e23fa198125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebb16aafb7b280d8754ccbe92acb8082.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images"><a href="#VCD-A-Dataset-for-Visual-Commonsense-Discovery-in-Images" class="headerlink" title="VCD: A Dataset for Visual Commonsense Discovery in Images"></a>VCD: A Dataset for Visual Commonsense Discovery in Images</h2><p><strong>Authors:Xiangqing Shen, Fanfan Wang, Siwei Wu, Rui Xia</strong></p>
<p>Visual commonsense plays a vital role in understanding and reasoning about the visual world. While commonsense knowledge bases like ConceptNet provide structured collections of general facts, they lack visually grounded representations. Scene graph datasets like Visual Genome, though rich in object-level descriptions, primarily focus on directly observable information and lack systematic categorization of commonsense knowledge. We present Visual Commonsense Dataset (VCD), a large-scale dataset containing over 100,000 images and 14 million object-commonsense pairs that bridges this gap. VCD introduces a novel three-level taxonomy for visual commonsense, integrating both Seen (directly observable) and Unseen (inferrable) commonsense across Property, Action, and Space aspects. Each commonsense is represented as a triple where the head entity is grounded to object bounding boxes in images, enabling scene-dependent and object-specific visual commonsense representation. To demonstrate VCDâ€™s utility, we develop VCM, a generative model that combines a vision-language model with instruction tuning to discover diverse visual commonsense from images. Extensive evaluations demonstrate both the high quality of VCD and its value as a resource for advancing visually grounded commonsense understanding and reasoning. Our dataset and code will be released on <a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD">https://github.com/NUSTM/VCD</a>. </p>
<blockquote>
<p>è§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶åƒConceptNetè¿™æ ·çš„å¸¸è¯†çŸ¥è¯†åº“æä¾›äº†é€šç”¨äº‹å®çš„ç»“æ„åŒ–é›†åˆï¼Œä½†å®ƒä»¬ç¼ºä¹è§†è§‰åŸºç¡€è¡¨ç¤ºã€‚åƒVisual Genomeè¿™æ ·çš„åœºæ™¯å›¾æ•°æ®é›†è™½ç„¶å¯Œå«å¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œä½†ä¸»è¦å…³æ³¨å¯ç›´æ¥è§‚å¯Ÿçš„ä¿¡æ¯ï¼Œç¼ºä¹å¸¸è¯†çŸ¥è¯†çš„ç³»ç»Ÿåˆ†ç±»ã€‚æˆ‘ä»¬æ¨å‡ºäº†Visual Commonsense Datasetï¼ˆVCDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡å¼ å›¾åƒå’Œ1400ä¸‡å¼ å¯¹è±¡å¸¸è¯†é…å¯¹ï¼Œå¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚VCDå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸‰çº§åˆ†ç±»æ³•ï¼Œç”¨äºè§†è§‰å¸¸è¯†ï¼Œèåˆäº†è·¨å±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢çš„å¯è§ï¼ˆå¯ç›´æ¥è§‚å¯Ÿï¼‰å’Œä¸å¯è§ï¼ˆå¯æ¨æ–­ï¼‰å¸¸è¯†ã€‚æ¯ä¸ªå¸¸è¯†éƒ½è¡¨ç¤ºä¸ºä¸‰å…ƒç»„ï¼Œå…¶ä¸­å¤´å®ä½“ä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸åŒ¹é…ï¼Œå®ç°äº†åœºæ™¯ä¾èµ–å’Œå¯¹è±¡ç‰¹å®šçš„è§†è§‰å¸¸è¯†è¡¨ç¤ºã€‚ä¸ºäº†å±•ç¤ºVCDçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†VCMï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸æŒ‡ä»¤è°ƒæ•´ç›¸ç»“åˆï¼Œä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒVCDçš„é«˜è´¨é‡åŠå…¶ä½œä¸ºæ¨è¿›è§†è§‰åŸºç¡€å¸¸è¯†ç†è§£å’Œæ¨ç†çš„èµ„æºä»·å€¼ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUSTM/VCDä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.17213v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è™½ç„¶å¸¸è¯†çŸ¥è¯†åº“å¦‚ConceptNetæä¾›äº†ç»“æ„åŒ–çš„å¸¸è¯†äº‹å®é›†åˆï¼Œä½†å®ƒä»¬ç¼ºä¹è§†è§‰åŸºç¡€è¡¨ç¤ºã€‚åœºæ™¯å›¾æ•°æ®é›†å¦‚Visual Genomeè™½ç„¶å¯Œå«å¯¹è±¡çº§åˆ«çš„æè¿°ï¼Œä½†ä¸»è¦å…³æ³¨å¯ç›´æ¥è§‚å¯Ÿåˆ°çš„ä¿¡æ¯ï¼Œç¼ºä¹ç³»ç»Ÿçš„å¸¸è¯†çŸ¥è¯†åˆ†ç±»ã€‚æˆ‘ä»¬æ¨å‡ºäº†è§†è§‰å¸¸è¯†æ•°æ®é›†ï¼ˆVCDï¼‰ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡å¼ å›¾åƒå’Œ1400ä¸‡å¯¹è±¡-å¸¸è¯†å¯¹ï¼Œå¼¥è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚VCDå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸‰çº§åˆ†ç±»æ³•ï¼Œæ•´åˆäº†å¯è§ï¼ˆå¯ç›´æ¥è§‚å¯Ÿï¼‰å’Œä¸å¯è§ï¼ˆå¯æ¨æ–­ï¼‰çš„å¸¸è¯†ï¼Œæ¶µç›–å±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢ã€‚æ¯ä¸ªå¸¸è¯†ä»¥ä¸‰å…ƒç»„çš„å½¢å¼è¡¨ç¤ºï¼Œå¤´éƒ¨å®ä½“ä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸å¯¹åº”ï¼Œå®ç°äº†åœºæ™¯ä¾èµ–å’Œå¯¹è±¡ç‰¹å®šçš„è§†è§‰å¸¸è¯†è¡¨ç¤ºã€‚ä¸ºäº†å±•ç¤ºVCDçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†VCMï¼Œä¸€ä¸ªç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒVCDçš„é«˜è´¨é‡åŠå…¶ä½œä¸ºæ¨è¿›è§†è§‰åŸºç¡€å¸¸è¯†ç†è§£å’Œæ¨ç†çš„èµ„æºä»·å€¼ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUSTM/VCD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NUSTM/VCDä¸Šå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¸¸è¯†åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸–ç•Œæ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰çŸ¥è¯†åº“å’Œæ•°æ®é›†å­˜åœ¨ç¼ºä¹è§†è§‰åŸºç¡€è¡¨ç¤ºå’Œç³»ç»Ÿå¸¸è¯†çŸ¥è¯†åˆ†ç±»çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†Visual Commonsense Dataset (VCD)æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«å›¾åƒå’Œå¯¹è±¡-å¸¸è¯†å¯¹ã€‚</li>
<li>VCDé‡‡ç”¨æ–°çš„ä¸‰çº§åˆ†ç±»æ³•ï¼Œæ•´åˆå¯è§å’Œä¸å¯è§çš„å¸¸è¯†ï¼Œæ¶µç›–å±æ€§ã€åŠ¨ä½œå’Œç©ºé—´æ–¹é¢ã€‚</li>
<li>VCDä¸­çš„å¸¸è¯†ä»¥ä¸‰å…ƒç»„å½¢å¼è¡¨ç¤ºï¼Œä¸å›¾åƒä¸­çš„å¯¹è±¡è¾¹ç•Œæ¡†ç›¸å¯¹åº”ã€‚</li>
<li>å¼€å‘äº†VCMæ¨¡å‹ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»å›¾åƒä¸­å‘ç°å¤šæ ·çš„è§†è§‰å¸¸è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.17213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04775750c2b7aaa268e08f6d6d42895a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4703d3091e348bcbf98aa6b35f94e7a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-700ab1eb162dbef1b6b81b508cdd1e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b5d05e9d480e67ac1ce7d86a8db52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ccf103f7e301725f53667dd0d4c644d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  PuzzleWorld A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7de1d642a339137fcd331c11d7b07719.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-09  Perceptual Decoupling for Scalable Multi-modal Reasoning via   Reward-Optimized Captioning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
