<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Query-Kontext An Unified Multimodal Model for Image Generation and   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-02-æ›´æ–°"><a href="#2025-10-02-æ›´æ–°" class="headerlink" title="2025-10-02 æ›´æ–°"></a>2025-10-02 æ›´æ–°</h1><h2 id="Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing"><a href="#Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing" class="headerlink" title="Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing"></a>Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing</h2><p><strong>Authors:Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</strong></p>
<p>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal &#96;&#96;kontextâ€™â€™ composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion modelâ€™s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLMâ€™s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰å’Œç¼–è¾‘ï¼ˆTI2Iï¼‰æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯ä½œä¸ºé›†æˆäº†å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¿˜æ˜¯ä½œä¸ºæ—©æœŸç†è§£å’Œç”Ÿæˆæ¨¡å¼èåˆçš„å¤©çœŸç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å½“å‰ç»Ÿä¸€æ¡†æ¶ä¸­ï¼ŒåŒ…å«æŒ‡ä»¤ç†è§£ã€æ¥åœ°å’Œå›¾åƒå¼•ç”¨åœ¨å†…çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œåœ¨èº«ä»½ä¿ç•™å’Œå¿ å®é‡å»ºæ–¹é¢ä¸é«˜æ¸…ç»¼åˆæ¯æ¯ç›¸å…³ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Query-Kontextè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç”±è¯­ä¹‰çº¿ç´¢å’Œæ¥è‡ªå¤šæ¨¡æ€è¾“å…¥çš„ç²—ç²’åº¦å›¾åƒæ¡ä»¶ç»„æˆçš„å¤šæ¨¡æ€&#96;&#96;kontextâ€™â€™ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ã€‚è¿™ç§è®¾è®¡å°†å¤æ‚çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›å§”æ‰˜ç»™å¼ºå¤§çš„VLMï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜è´¨é‡è§†è§‰åˆæˆçš„ä½œç”¨ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ä¸‰é˜¶æ®µçš„æ¸è¿›è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€kontextä»¤ç‰Œå°†VLMè¿æ¥åˆ°è½»é‡çº§æ‰©æ•£å¤´ï¼Œä»¥é‡Šæ”¾VLMçš„ç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ­¤å¤´æ‰©å±•åˆ°å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜è§†è§‰ç»†èŠ‚å’Œé€¼çœŸåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥ä½çº§åˆ«å›¾åƒç¼–ç å™¨ï¼Œä»¥æé«˜å›¾åƒä¿çœŸåº¦å¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç»¼åˆæ•°æ®ç®¡é“ï¼Œæ•´åˆçœŸå®ã€åˆæˆå’Œå¼€æºæ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„å¤šæ¨¡æ€å‚è€ƒåˆ°å›¾åƒåœºæ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€å®šåˆ¶ç”Ÿæˆå’Œå¤šä¸»é¢˜ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ç›¸åŒ¹é…ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†ä»»åŠ¡ç‰¹å®šçš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26641v1">PDF</a> 23 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Unified Multimodal Modelsï¼ˆUMMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰å’Œç¼–è¾‘ï¼ˆTI2Iï¼‰ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚é€šè¿‡å¼•å…¥Query-Kontextæ–°å‹æ–¹æ³•ï¼Œç»“åˆè¯­ä¹‰çº¿ç´¢å’Œç²—ç²’åº¦å›¾åƒæ¡ä»¶ç¼–ç çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œå®ç°äº†å¯¹ç»Ÿä¸€æ¡†æ¶ä¸­å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›å§”æ‰˜ç»™å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä¿ç•™æ‰©æ•£æ¨¡å‹ç”¨äºé«˜è´¨é‡è§†è§‰åˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åŒ¹é…äº†å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ï¼Œç”šè‡³åœ¨éƒ¨åˆ†æƒ…å†µä¸‹è¶…è¶Šäº†ä»»åŠ¡ç‰¹å®šçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMMsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>Query-Kontextæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ¡¥æ¥VLMå’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯­ä¹‰çº¿ç´¢å’Œç²—ç²’åº¦å›¾åƒæ¡ä»¶ç¼–ç æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„ç”Ÿæˆæ¨ç†èƒ½åŠ›å’Œè§†è§‰åˆæˆè´¨é‡ã€‚</li>
<li>æ–¹æ³•åŒ¹é…äº†å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ï¼Œå¹¶åœ¨éƒ¨åˆ†æƒ…å†µä¸‹è¶…è¶Šäº†ä»»åŠ¡ç‰¹å®šæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–çš„å¤šæ¨¡æ€å‚è€ƒåˆ°å›¾åƒçš„åœºæ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€è‡ªå®šä¹‰ç”Ÿæˆå’Œå¤šä¸»ä½“ç»„åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"><a href="#MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation" class="headerlink" title="MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"></a>MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</h2><p><strong>Authors:Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang</strong></p>
<p>Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿçš„è¿åŠ¨çš„è§†é¢‘ä»ç„¶æå…·æŒ‘æˆ˜æ€§ã€‚è¿™ä¸€éš¾åº¦æºäºå‡†ç¡®å»ºæ¨¡è¿åŠ¨çš„å¤æ‚æ€§ï¼Œå…¶ä¸­åŒ…æ‹¬æ•æ‰ç‰©ç†çº¦æŸã€ç‰©ä½“äº¤äº’å’Œç‰¹å®šé¢†åŸŸçš„åŠ¨æ€ï¼Œè¿™äº›ä¸å®¹æ˜“åœ¨å¤šç§åœºæ™¯ä¸­é€šç”¨åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionRAGï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºå‹æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¿åŠ¨é€‚åº”ï¼ˆCAMAï¼‰ä»ç›¸å…³å‚è€ƒè§†é¢‘ä¸­é€‚åº”è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜è¿åŠ¨çœŸå®æ€§ã€‚ä¸»è¦æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆiï¼‰åŸºäºæ£€ç´¢çš„ç®¡é“ä½¿ç”¨è§†é¢‘ç¼–ç å™¨å’Œä¸“ç”¨é‡é‡‡æ ·å™¨æå–é«˜çº§è¿åŠ¨ç‰¹å¾ï¼Œä»¥è’¸é¦è¯­ä¹‰è¿åŠ¨è¡¨ç¤ºï¼›ï¼ˆiiï¼‰é€šè¿‡å› æœå˜å‹å™¨æ¶æ„å®ç°è¿åŠ¨é€‚åº”çš„ä¸Šä¸‹æ–‡å†…å­¦ä¹ æ–¹æ³•ï¼›ï¼ˆiiiï¼‰åŸºäºæ³¨æ„åŠ›çš„è¿åŠ¨æ³¨å…¥é€‚é…å™¨æ— ç¼é›†æˆè½¬ç§»çš„è¿åŠ¨ç‰¹å¾åˆ°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸå’Œå„ç§åŸºç¡€æ¨¡å‹ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å—åŒ–è®¾è®¡é€šè¿‡ç®€å•åœ°æ›´æ–°æ£€ç´¢æ•°æ®åº“è€Œæ— éœ€é‡æ–°è®­ç»ƒä»»ä½•ç»„ä»¶ï¼Œå®ç°äº†é›¶é•œå¤´è·¨åŸŸæ¨å¹¿ã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡å®ç°æœ‰æ•ˆçš„è¿åŠ¨å…ˆéªŒæ£€ç´¢å’Œè½¬ç§»ï¼Œæé«˜äº†è§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œä¿ƒè¿›äº†çœŸå®è¿åŠ¨åŠ¨æ€çš„åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26391v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿçš„è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionRAGæ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¿åŠ¨é€‚åº”ï¼ˆCAMAï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨ç›¸å…³å‚è€ƒè§†é¢‘çš„è¿åŠ¨å…ˆéªŒä¿¡æ¯æé«˜è¿åŠ¨çœŸå®æ€§ã€‚åŒ…æ‹¬åŸºäºæ£€ç´¢çš„ç®¡é“ã€ä¸Šä¸‹æ–‡å†…å­¦ä¹ æ–¹æ³•å’Œæ³¨æ„åŠ›é©±åŠ¨çš„æ³¨å…¥é€‚é…å™¨ç­‰æŠ€æœ¯åˆ›æ–°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå’ŒåŸºå‡†æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸”æ¨ç†æ—¶çš„è®¡ç®—å¼€é”€å¾ˆå°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å—åŒ–è®¾è®¡å¯é€šè¿‡ç®€å•æ›´æ–°æ£€ç´¢æ•°æ®åº“å®ç°é›¶æ ·æœ¬è·¨åŸŸæ¨å¹¿ã€‚è¯¥ç ”ç©¶æé«˜äº†è§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå®ç°äº†æœ‰æ•ˆçš„è¿åŠ¨å…ˆéªŒæ£€ç´¢å’Œè½¬ç§»ï¼Œä¿ƒè¿›äº†çœŸå®è¿åŠ¨åŠ¨æ€çš„åˆæˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç”ŸæˆçœŸå®è§†é¢‘ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MotionRAGæ¡†æ¶é€šè¿‡é€‚åº”è¿åŠ¨å…ˆéªŒæé«˜è§†é¢‘è¿åŠ¨çœŸå®æ€§ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬åŸºäºæ£€ç´¢çš„ç®¡é“ã€ä¸Šä¸‹æ–‡å†…å­¦ä¹ æ–¹æ³•å’Œæ³¨æ„åŠ›é©±åŠ¨çš„æ³¨å…¥é€‚é…å™¨ç­‰æŠ€æœ¯åˆ›æ–°ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå’Œæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¨ç†è®¡ç®—å¼€é”€å°ã€‚</li>
<li>æ¨¡å—åŒ–è®¾è®¡å¯å®ç°é›¶æ ·æœ¬è·¨åŸŸæ¨å¹¿ï¼Œæ›´æ–°æ£€ç´¢æ•°æ®åº“å³å¯ã€‚</li>
<li>è¯¥ç ”ç©¶æé«˜äº†è§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå®ç°äº†è¿åŠ¨å…ˆéªŒçš„æœ‰æ•ˆæ£€ç´¢å’Œè½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Data-to-Energy-Stochastic-Dynamics"><a href="#Data-to-Energy-Stochastic-Dynamics" class="headerlink" title="Data-to-Energy Stochastic Dynamics"></a>Data-to-Energy Stochastic Dynamics</h2><p><strong>Authors:Kirill Tamogashev, Nikolay Malkin</strong></p>
<p>The Schr&quot;odinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schr&quot;odinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schr&quot;odinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: <a target="_blank" rel="noopener" href="https://github.com/mmacosha/d2e-stochastic-dynamics">https://github.com/mmacosha/d2e-stochastic-dynamics</a> </p>
<blockquote>
<p>è–›å®šè°”æ¡¥é—®é¢˜æ¶‰åŠæ‰¾åˆ°ä¸€ä¸ªéšæœºåŠ¨åŠ›ç³»ç»Ÿï¼Œè¯¥åŠ¨åŠ›ç³»ç»Ÿè¿æ¥ä¸¤ä¸ªè¾¹ç¼˜åˆ†å¸ƒï¼Œå¹¶æœ€å°åŒ–ç‰¹å®šçš„è¿è¾“æˆæœ¬ã€‚è¿™ä¸ªé—®é¢˜ä»£è¡¨äº†æœ€ä¼˜ä¼ è¾“åœ¨éšæœºæƒ…å†µä¸‹çš„æ¨å¹¿ï¼Œç”±äºå…¶ä¸æ‰©æ•£æ¨¡å‹ã€æµé‡åŒ¹é…ä»¥åŠä¸è‡ªç„¶ç§‘å­¦åº”ç”¨çš„è”ç³»è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œæ‰€æœ‰ç°æœ‰çš„ç®—æ³•éƒ½åªèƒ½åœ¨ä¸¤ä¸ªåˆ†å¸ƒéƒ½æœ‰æ ·æœ¬çš„æƒ…å†µä¸‹æ¨æ–­å‡ºè¿™ç§åŠ¨æ€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å½“åˆ†å¸ƒä¹‹ä¸€ï¼ˆæˆ–ä¸¤è€…ï¼‰ç”±æœªå½’ä¸€åŒ–çš„å¯†åº¦ç»™å‡ºä¸”æ— æ³•ä½¿ç”¨æ•°æ®æ ·æœ¬æ—¶å»ºæ¨¡è–›å®šè°”æ¡¥çš„é¦–ä¸ªé€šç”¨æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•ä¾èµ–äºå°†æ•°æ®æ— å…³çš„è¿­ä»£æ¯”ä¾‹æ‹Ÿåˆï¼ˆIPFï¼‰ç¨‹åºæ¨å¹¿åˆ°æ— æ•°æ®çš„æƒ…å†µï¼Œè¿™æ˜¯å—è¿‘æœŸç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£é‡‡æ ·å™¨è®­ç»ƒæ–¹é¢çš„æœ€æ–°è¿›å±•çš„å¯å‘ã€‚æˆ‘ä»¬åœ¨åˆæˆé—®é¢˜ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ•°æ®åˆ°èƒ½é‡IPFçš„æœ‰æ•ˆæ€§ï¼Œå‘ç°å®ƒå¯ä»¥æˆåŠŸåœ°åœ¨å¤šå³°åˆ†å¸ƒä¹‹é—´å­¦ä¹ ä¼ è¾“ã€‚ä½œä¸ºå¯¹å‡è®¾å›ºå®šæ—¶é—´ç¦»æ•£åŒ–æ–¹æ¡ˆçš„åŠ¨æ€è¿›è¡Œå¼ºåŒ–å­¦ä¹ å…¬å¼åŒ–çš„æ¬¡è¦ç»“æœï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡å­¦ä¹ åŠ¨åŠ›å­¦çš„æ‰©æ•£ç³»æ•°ï¼Œç°æœ‰çš„æ•°æ®åˆ°æ•°æ®è–›å®šè°”æ¡¥ç®—æ³•å¯ä»¥å¤§å¤§æ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ–°å¼€å‘çš„ç®—æ³•åº”ç”¨äºç”Ÿæˆæ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­çš„åéªŒåˆ†å¸ƒé‡‡æ ·é—®é¢˜ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ç§æ— æ•°æ®å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ–¹æ³•ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/mmacosha/d2e-stochastic-dynamics">https://github.com/mmacosha/d2e-stochastic-dynamics</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†SchrÃ¶dingeræ¡¥é—®é¢˜åœ¨æ ·æœ¬ä¸å¯ç”¨çš„æƒ…å†µä¸‹çš„å»ºæ¨¡æ–¹æ³•ã€‚å½“åˆ†å¸ƒç”±æœªå½’ä¸€åŒ–çš„å¯†åº¦ç»™å‡ºæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¿­ä»£æ¯”ä¾‹æ‹Ÿåˆï¼ˆIPFï¼‰çš„ç®—æ³•ï¼Œé€‚ç”¨äºæ•°æ®ä¸å¯ç”¨çš„åœºåˆã€‚æ­¤æ–¹æ³•ä¸ä»…èƒ½åœ¨åˆæˆé—®é¢˜ä¸ŠæˆåŠŸå­¦ä¹ å¤šæ¨¡æ€åˆ†å¸ƒä¹‹é—´çš„ä¼ è¾“ï¼Œè€Œä¸”èƒ½æ”¹å–„ç°æœ‰çš„æ•°æ®åˆ°æ•°æ®çš„SchrÃ¶dingeræ¡¥ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ åŠ¨åŠ›å­¦çš„æ‰©æ•£ç³»æ•°æ¥æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†æ–°å¼€å‘çš„ç®—æ³•åº”ç”¨äºç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­çš„åéªŒåˆ†å¸ƒé‡‡æ ·é—®é¢˜ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ç§æ•°æ®æ— å…³çš„å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SchrÃ¶dingeræ¡¥é—®é¢˜æ—¨åœ¨å¯»æ‰¾è¿æ¥ä¸¤ä¸ªè¾¹ç¼˜åˆ†å¸ƒçš„éšæœºåŠ¨åŠ›ç³»ç»Ÿï¼Œå¹¶æœ€å°åŒ–ç‰¹å®šçš„è¿è¾“æˆæœ¬ã€‚å®ƒæ˜¯æœ€ä½³è¿è¾“åœ¨éšæœºæƒ…å†µä¸‹çš„æ¨å¹¿ï¼Œä¸æ‰©æ•£æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…æœ‰è”ç³»ï¼Œåœ¨è‡ªç„¶ç§‘å­¦ä¸­æœ‰åº”ç”¨ã€‚</li>
<li>ç°æœ‰ç®—æ³•ä»…åœ¨å¯ä»¥ä»ä¸¤ä¸ªåˆ†å¸ƒä¸­è·å–æ ·æœ¬çš„æƒ…å†µä¸‹æ¨æ–­åŠ¨æ€ã€‚è¿™ç¯‡è®ºæ–‡è§£å†³äº†å½“åˆ†å¸ƒç”±æœªå½’ä¸€åŒ–çš„å¯†åº¦ç»™å‡ºä¸”æ— æ³•è·å–æ•°æ®æ ·æœ¬æ—¶çš„å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„ç®—æ³•åŸºäºè¿­ä»£æ¯”ä¾‹æ‹Ÿåˆï¼ˆIPFï¼‰çš„æ¨å¹¿ï¼Œé€‚ç”¨äºæ•°æ®ä¸å¯ç”¨çš„æƒ…å†µï¼Œå—åˆ°å¼ºåŒ–å­¦ä¹ çš„å¯å‘ã€‚å®ƒåœ¨åˆæˆé—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½æˆåŠŸå­¦ä¹ å¤šæ¨¡æ€åˆ†å¸ƒä¹‹é—´çš„ä¼ è¾“ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å…¬å¼å‡å®šåŠ¨åŠ›å­¦çš„å›ºå®šæ—¶é—´ç¦»æ•£æ–¹æ¡ˆï¼Œå‘ç°å¯ä»¥é€šè¿‡å­¦ä¹ åŠ¨åŠ›å­¦çš„æ‰©æ•£ç³»æ•°æ¥æ”¹è¿›ç°æœ‰çš„æ•°æ®åˆ°æ•°æ®çš„SchrÃ¶dingeræ¡¥ç®—æ³•ã€‚</li>
<li>æ–°å¼€å‘çš„ç®—æ³•åº”ç”¨äºç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­çš„åéªŒåˆ†å¸ƒé‡‡æ ·é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§æ•°æ®æ— å…³çš„å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†å…³äºå¦‚ä½•è§£å†³SchrÃ¶dingeræ¡¥é—®é¢˜çš„æ–°è§è§£ï¼Œå°¤å…¶æ˜¯åœ¨æ ·æœ¬ä¸å¯ç”¨çš„æƒ…å¢ƒä¸‹ã€‚å®ƒç»“åˆäº†ä¸åŒé¢†åŸŸçš„çŸ¥è¯†ï¼ˆå¦‚æœ€ä½³è¿è¾“ã€æ‰©æ•£æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ç­‰ï¼‰ï¼Œä¸ºå¤„ç†å¤æ‚çš„éšæœºåŠ¨æ€ç³»ç»Ÿæä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26364v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26364v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EchoGen-Generating-Visual-Echoes-in-Any-Scene-via-Feed-Forward-Subject-Driven-Auto-Regressive-Model"><a href="#EchoGen-Generating-Visual-Echoes-in-Any-Scene-via-Feed-Forward-Subject-Driven-Auto-Regressive-Model" class="headerlink" title="EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward   Subject-Driven Auto-Regressive Model"></a>EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward   Subject-Driven Auto-Regressive Model</h2><p><strong>Authors:Ruixiao Dong, Zhendong Wang, Keli Liu, Li Li, Ying Chen, Kai Li, Daowen Li, Houqiang Li</strong></p>
<p>Subject-driven generation is a critical task in creative AI; yet current state-of-the-art methods present a stark trade-off. They either rely on computationally expensive, per-subject fine-tuning, sacrificing efficiency and zero-shot capability, or employ feed-forward architectures built on diffusion models, which are inherently plagued by slow inference speeds. Visual Auto-Regressive (VAR) models are renowned for their rapid sampling speeds and strong generative quality, making them an ideal yet underexplored foundation for resolving this tension. To bridge this gap, we introduce EchoGen, a pioneering framework that empowers VAR models with subject-driven generation capabilities. The core design of EchoGen is an effective dual-path injection strategy that disentangles a subjectâ€™s high-level semantic identity from its low-level fine-grained details, enabling enhanced controllability and fidelity. We employ a semantic encoder to extract the subjectâ€™s abstract identity, which is injected through decoupled cross-attention to guide the overall composition. Concurrently, a content encoder captures intricate visual details, which are integrated via a multi-modal attention mechanism to ensure high-fidelity texture and structural preservation. To the best of our knowledge, EchoGen is the first feed-forward subject-driven framework built upon VAR models. Both quantitative and qualitative results substantiate our design, demonstrating that EchoGen achieves subject fidelity and image quality comparable to state-of-the-art diffusion-based methods with significantly lower sampling latency. Code and models will be released soon. </p>
<blockquote>
<p>ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ˜¯åˆ›é€ æ€§AIä¸­çš„å…³é”®ä»»åŠ¡ï¼›ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å‘ˆç°å‡ºæ˜æ˜¾çš„æƒè¡¡ã€‚å®ƒä»¬è¦ä¹ˆä¾èµ–äºè®¡ç®—é‡å¤§ã€é’ˆå¯¹æ¯ä¸ªä¸»é¢˜çš„å¾®è°ƒï¼Œç‰ºç‰²äº†æ•ˆç‡å’Œé›¶æ ·æœ¬èƒ½åŠ›ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„å‰é¦ˆæ¶æ„ï¼Œè¿™å›ºæœ‰åœ°å¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹ä»¥å…¶å¿«é€Ÿé‡‡æ ·é€Ÿåº¦å’Œå¼ºå¤§çš„ç”Ÿæˆè´¨é‡è€Œé—»åï¼Œæ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ç†æƒ³ä½†å°šæœªè¢«å……åˆ†æ¢ç´¢çš„åŸºç¡€ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EchoGenï¼Œè¿™æ˜¯ä¸€ä¸ªèµ‹èƒ½VARæ¨¡å‹ä¸»é¢˜é©±åŠ¨ç”Ÿæˆèƒ½åŠ›çš„å¼€åˆ›æ€§æ¡†æ¶ã€‚EchoGençš„æ ¸å¿ƒè®¾è®¡æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„åŒè·¯å¾„æ³¨å…¥ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†ä¸€ä¸ªä¸»é¢˜çš„é«˜çº§è¯­ä¹‰èº«ä»½ä¸å…¶ä½çº§ç²¾ç»†ç»†èŠ‚åˆ†å¼€ï¼Œä»è€Œå¢å¼ºäº†å¯æ§æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬é‡‡ç”¨è¯­ä¹‰ç¼–ç å™¨æå–ä¸»é¢˜çš„æŠ½è±¡èº«ä»½ï¼Œé€šè¿‡è§£è€¦äº¤å‰æ³¨æ„åŠ›æ³¨å…¥æ¥å¼•å¯¼æ•´ä½“æ„å›¾ã€‚åŒæ—¶ï¼Œå†…å®¹ç¼–ç å™¨æ•æ‰å¤æ‚çš„è§†è§‰ç»†èŠ‚ï¼Œé€šè¿‡å¤šæ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶é›†æˆï¼Œä»¥ç¡®ä¿é«˜ä¿çœŸçº¹ç†å’Œç»“æ„ä¿ç•™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒEchoGenæ˜¯åŸºäºVARæ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‰é¦ˆä¸»é¢˜é©±åŠ¨æ¡†æ¶ã€‚å®šé‡å’Œå®šæ€§ç»“æœéƒ½è¯æ˜äº†æˆ‘ä»¬è®¾è®¡çš„åˆç†æ€§ï¼Œè¯æ˜EchoGenåœ¨ä¸»é¢˜ä¿çœŸåº¦å’Œå›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼ŒåŒæ—¶é‡‡æ ·å»¶è¿Ÿæ›´ä½ã€‚ä»£ç å’Œæ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26127v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EchoGenæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹ä¸ä¸»é¢˜é©±åŠ¨ç”Ÿæˆç›¸ç»“åˆï¼Œå®ç°äº†å¿«é€Ÿé‡‡æ ·å’Œé«˜è´¨é‡ç”Ÿæˆã€‚EchoGençš„æ ¸å¿ƒè®¾è®¡æ˜¯ä¸€ç§æœ‰æ•ˆçš„åŒè·¯å¾„æ³¨å…¥ç­–ç•¥ï¼Œèƒ½å¤Ÿåˆ†ç¦»ä¸»é¢˜çš„é«˜çº§è¯­ä¹‰èº«ä»½å’Œä½çº§ç»†èŠ‚ï¼Œæé«˜äº†å¯æ§æ€§å’Œä¿çœŸåº¦ã€‚å®ƒé‡‡ç”¨è¯­ä¹‰ç¼–ç å™¨æå–ä¸»é¢˜æŠ½è±¡èº«ä»½ï¼Œé€šè¿‡è§£è€¦äº¤å‰æ³¨æ„åŠ›å¼•å¯¼æ•´ä½“æ„å›¾ã€‚åŒæ—¶ï¼Œå†…å®¹ç¼–ç å™¨æ•æ‰å¤æ‚è§†è§‰ç»†èŠ‚ï¼Œé€šè¿‡å¤šæ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿é«˜ä¿çœŸçº¹ç†å’Œç»“æ„ä¿ç•™ã€‚EchoGenæ˜¯é¦–ä¸ªåŸºäºVARæ¨¡å‹çš„å‘å‰ä¼ æ’­ä¸»é¢˜é©±åŠ¨æ¡†æ¶ï¼Œå®ç°äº†ä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸å½“çš„ä¸»é¢˜ä¿çœŸåº¦å’Œå›¾åƒè´¨é‡ï¼ŒåŒæ—¶é‡‡æ ·å»¶è¿Ÿæ›´ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EchoGenæ¡†æ¶ç»“åˆäº†è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆï¼Œå®ç°äº†å¿«é€Ÿé‡‡æ ·å’Œé«˜è´¨é‡ç”Ÿæˆã€‚</li>
<li>EchoGené‡‡ç”¨åŒè·¯å¾„æ³¨å…¥ç­–ç•¥ï¼Œåˆ†ç¦»ä¸»é¢˜çš„é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚ï¼Œæé«˜å¯æ§æ€§å’Œä¿çœŸåº¦ã€‚</li>
<li>è¯­ä¹‰ç¼–ç å™¨æå–ä¸»é¢˜æŠ½è±¡èº«ä»½ï¼Œé€šè¿‡è§£è€¦äº¤å‰æ³¨æ„åŠ›å¼•å¯¼æ•´ä½“æ„å›¾ã€‚</li>
<li>å†…å®¹ç¼–ç å™¨è´Ÿè´£æ•æ‰å¤æ‚è§†è§‰ç»†èŠ‚ï¼Œç¡®ä¿é«˜ä¿çœŸçº¹ç†å’Œç»“æ„ä¿ç•™ã€‚</li>
<li>EchoGenæ˜¯é¦–ä¸ªåŸºäºVARæ¨¡å‹çš„å‘å‰ä¼ æ’­ä¸»é¢˜é©±åŠ¨æ¡†æ¶ã€‚</li>
<li>EchoGençš„ä¸»é¢˜ä¿çœŸåº¦å’Œå›¾åƒè´¨é‡ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EVODiff-Entropy-aware-Variance-Optimized-Diffusion-Inference"><a href="#EVODiff-Entropy-aware-Variance-Optimized-Diffusion-Inference" class="headerlink" title="EVODiff: Entropy-aware Variance Optimized Diffusion Inference"></a>EVODiff: Entropy-aware Variance Optimized Diffusion Inference</h2><p><strong>Authors:Shigui Li, Wei Chen, Delu Zeng</strong></p>
<p>Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ShiguiLi/EVODiff">https://github.com/ShiguiLi/EVODiff</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢å’Œè®­ç»ƒæ¨ç†å·®å¼‚ç­‰é—®é¢˜ã€‚è™½ç„¶åƒDPM-Solverè¿™æ ·çš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨å¯ä»¥åŠ é€Ÿå»å™ªæ¨ç†ï¼Œä½†å®ƒä»¬åœ¨ä¿¡æ¯ä¼ è¾“æ•ˆç‡æ–¹é¢ç¼ºä¹ç†è®ºåŸºç¡€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ä¿¡æ¯ç†è®ºçš„è§’åº¦ä»‹ç»äº†æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼Œå‘ç°æˆåŠŸçš„å»å™ªä»æ ¹æœ¬ä¸Šå‡å°‘äº†åå‘è½¬æ¢ä¸­çš„æ¡ä»¶ç†µã€‚è¿™ä¸€åŸåˆ™è®©æˆ‘ä»¬æ·±å…¥ç†è§£äº†æ¨ç†è¿‡ç¨‹ï¼šï¼ˆ1ï¼‰æ•°æ®é¢„æµ‹å‚æ•°åŒ–è¡¨ç°ä¼˜äºå™ªå£°å¯¹ç…§ï¼›ï¼ˆ2ï¼‰ä¼˜åŒ–æ¡ä»¶æ–¹å·®æä¾›äº†ä¸€ç§æ— å‚è€ƒçš„æ–¹å¼æ¥æœ€å°åŒ–è½¬æ¢å’Œé‡å»ºè¯¯å·®ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„ç†µæ„ŸçŸ¥æ–¹å·®ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºEVODiffï¼Œå®ƒé€šè¿‡å»å™ªè¿‡ç¨‹ä¸­ä¼˜åŒ–æ¡ä»¶ç†µæ¥ç³»ç»Ÿåœ°å‡å°‘ä¸ç¡®å®šæ€§ã€‚å¯¹æ‰©æ•£æ¨¡å‹çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬è§è§£çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¸”æŒç»­åœ°ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ã€‚ä¾‹å¦‚ï¼Œä¸DPM-Solver++ç›¸æ¯”ï¼ŒEVODiffåœ¨CIFAR-10ä¸Šçš„é‡å»ºé”™è¯¯é™ä½äº†45.5%ï¼ˆåœ¨10æ¬¡å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰ä¸­ï¼ŒFIDä»5.10æé«˜åˆ°2.78ï¼‰ï¼Œåœ¨ImageNet-256ä¸Šé«˜è´¨é‡æ ·æœ¬çš„NFEæˆæœ¬é™ä½äº†25%ï¼ˆä»20æ¬¡é™è‡³15æ¬¡NFEï¼‰ï¼ŒåŒæ—¶æ”¹è¿›äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå¹¶å‡å°‘äº†ä¼ªå½±ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShiguiLi/EVODiff%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ShiguiLi/EVODiffä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26096v1">PDF</a> NeurIPS 2025, 40 pages, 14 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢å’Œè®­ç»ƒæ¨ç†å·®å¼‚ç­‰é—®é¢˜ã€‚è™½ç„¶åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨å¦‚DPM-SolveråŠ é€Ÿäº†å»å™ªæ¨ç†ï¼Œä½†å®ƒä»¬åœ¨ä¿¡æ¯ä¼ è¾“æ•ˆç‡æ–¹é¢ç¼ºä¹ç†è®ºåŸºç¡€ã€‚æœ¬æ–‡ä»‹ç»äº†ä»ä¿¡æ¯ç†è®ºè§’åº¦å¯¹æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†æˆåŠŸçš„å»å™ªåœ¨æ ¹æœ¬ä¸Šå‡å°‘äº†åå‘è½¬æ¢ä¸­çš„æ¡ä»¶ç†µã€‚è¿™ä¸€åŸç†ä¸ºæˆ‘ä»¬æ·±å…¥æ´å¯Ÿæ¨ç†è¿‡ç¨‹æä¾›äº†å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰æ•°æ®é¢„æµ‹å‚æ•°åŒ–ä¼˜äºå™ªå£°å‚æ•°åŒ–ï¼›ï¼ˆ2ï¼‰ä¼˜åŒ–æ¡ä»¶æ–¹å·®æä¾›äº†ä¸€ç§æ— å‚è€ƒçš„æ–¹æ³•æ¥æœ€å°åŒ–è½¬æ¢å’Œé‡å»ºè¯¯å·®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºæ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„ç†µæ„ŸçŸ¥æ–¹å·®ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºEVODiffï¼Œå®ƒé€šè¿‡å»å™ªè¿‡ç¨‹ä¸­ä¼˜åŒ–æ¡ä»¶ç†µæ¥ç³»ç»Ÿåœ°é™ä½ä¸ç¡®å®šæ€§ã€‚åœ¨æ‰©æ•£æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„è§è§£ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¸”ä¸€è‡´åœ°ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ã€‚ä¾‹å¦‚ï¼Œä¸DPM-Solver++ç›¸æ¯”ï¼ŒEVODiffåœ¨CIFAR-10ä¸Šçš„é‡å»ºè¯¯å·®é™ä½äº†45.5%ï¼ˆåœ¨10æ¬¡å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰æ—¶ï¼ŒFIDä»5.10é™ä½åˆ°2.78ï¼‰ï¼Œåœ¨ImageNet-256ä¸Šé«˜è´¨é‡æ ·æœ¬çš„NFEæˆæœ¬é™ä½äº†25%ï¼ˆä»20æ¬¡é™è‡³15æ¬¡NFEï¼‰ï¼ŒåŒæ—¶æ”¹è¿›äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå¹¶å‡å°‘äº†ä¼ªå½±ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å“è¶Šæ€§èƒ½ä¸å…¶è¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦å’Œè®­ç»ƒæ¨ç†å·®å¼‚æœ‰å…³ã€‚</li>
<li>åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨å¦‚DPM-Solverè™½ç„¶åœ¨åŠ é€Ÿå»å™ªæ¨ç†æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨ä¿¡æ¯ä¼ è¾“æ•ˆç‡æ–¹é¢ç¼ºä¹ç†è®ºåŸºç¡€ã€‚</li>
<li>æˆåŠŸçš„å»å™ªèƒ½å¤Ÿå‡å°‘åå‘è½¬æ¢ä¸­çš„æ¡ä»¶ç†µï¼Œè¿™æ˜¯ç†è§£æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„å…³é”®ã€‚</li>
<li>æ•°æ®é¢„æµ‹å‚æ•°åŒ–æ¯”å™ªå£°å‚æ•°åŒ–æ›´æœ‰æ•ˆã€‚</li>
<li>ä¼˜åŒ–æ¡ä»¶æ–¹å·®æä¾›äº†ä¸€ç§æ— å‚è€ƒçš„æ–¹æ³•ï¼Œå¯ä»¥æœ€å°åŒ–è½¬æ¢å’Œé‡å»ºè¯¯å·®ã€‚</li>
<li>æå‡ºçš„EVODiffæ–¹æ³•é€šè¿‡ä¼˜åŒ–å»å™ªè¿‡ç¨‹ä¸­çš„æ¡ä»¶ç†µï¼Œç³»ç»Ÿåœ°é™ä½äº†ä¸ç¡®å®šæ€§ï¼Œæ˜¾è‘—ä¼˜äºå…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ã€‚</li>
<li>EVODiffåœ¨å¤šä¸ªå®éªŒä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¦‚é™ä½é‡å»ºè¯¯å·®ã€å‡å°‘NFEæˆæœ¬ï¼Œæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå¹¶å‡å°‘ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Free-Reward-Guided-Image-Editing-via-Trajectory-Optimal-Control"><a href="#Training-Free-Reward-Guided-Image-Editing-via-Trajectory-Optimal-Control" class="headerlink" title="Training-Free Reward-Guided Image Editing via Trajectory Optimal Control"></a>Training-Free Reward-Guided Image Editing via Trajectory Optimal Control</h2><p><strong>Authors:Jinho Chang, Jaemin Kim, Jong Chul Ye</strong></p>
<p>Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹çš„è¿›å±•åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆä¸­å±•ç¤ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ä¸€ä¸ªçªå‡ºçš„ç ”ç©¶æ–¹å‘æ˜¯å¥–åŠ±å¼•å¯¼æŒ‡å¯¼ï¼Œå®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»¥ç¬¦åˆç‰¹å®šç›®æ ‡ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€å¥–åŠ±å¼•å¯¼æ–¹æ³•åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡â€”â€”éœ€è¦åœ¨ä¿ç•™æºå›¾åƒè¯­ä¹‰å†…å®¹çš„åŒæ—¶æé«˜ç›®æ ‡å¥–åŠ±â€”â€”å´å¾ˆå°‘è¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ— éœ€è®­ç»ƒã€å¥–åŠ±å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ–°æ¡†æ¶ã€‚æˆ‘ä»¬å°†ç¼–è¾‘è¿‡ç¨‹åˆ¶å®šä¸ºä¸€ä¸ªè½¨è¿¹æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹è¢«è§†ä¸ºä¸€æ¡å¯æ§è½¨è¿¹ï¼Œèµ·æºäºæºå›¾åƒï¼Œå¹¶ä¸”ä¼´éšçŠ¶æ€è¢«è¿­ä»£æ›´æ–°ä»¥å¼•å¯¼ç¼–è¾‘è¿‡ç¨‹ã€‚é€šè¿‡åœ¨ä¸åŒç¼–è¾‘ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºåæ¼”çš„æ— éœ€è®­ç»ƒæŒ‡å¯¼åŸºçº¿ï¼Œåœ¨å¥–åŠ±æœ€å¤§åŒ–å’Œå¯¹æºå›¾åƒçš„ä¿çœŸåº¦ä¹‹é—´å–å¾—äº†ä¼˜è¶Šå¹³è¡¡ï¼Œä¸”æ— éœ€å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25845v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å¥–åŠ±å¼•å¯¼å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç¼–è¾‘è¿‡ç¨‹è§†ä¸ºè½¨è¿¹æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œå°†æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹è§†ä¸ºä»æºå›¾åƒå¼€å§‹çš„å¯æ§è½¨è¿¹ï¼Œå¹¶é€šè¿‡è¿­ä»£æ›´æ–°ä¼´éšçŠ¶æ€æ¥å¼•å¯¼ç¼–è¾‘è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„ç¼–è¾‘ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºäºåæ¼”çš„æ— éœ€è®­ç»ƒæŒ‡å¯¼çš„åŸºçº¿ï¼Œåœ¨å¥–åŠ±æœ€å¤§åŒ–å’Œä¿æŒæºå›¾åƒä¿çœŸåº¦ä¹‹é—´å–å¾—äº†ä¼˜è¶Šå¹³è¡¡ï¼Œä¸”æ— éœ€å¥–åŠ±é»‘å®¢æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±å¼•å¯¼æ–¹æ³•åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å›¾åƒåˆæˆèƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†å°†å¥–åŠ±å¼•å¯¼æ–¹æ³•åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¥–åŠ±å¼•å¯¼å›¾åƒç¼–è¾‘æ–°æ¡†æ¶ã€‚</li>
<li>å°†ç¼–è¾‘è¿‡ç¨‹è§†ä¸ºè½¨è¿¹æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹ä½œä¸ºå¯æ§è½¨è¿¹ã€‚</li>
<li>é€šè¿‡è¿­ä»£æ›´æ–°ä¼´éšçŠ¶æ€æ¥å¼•å¯¼ç¼–è¾‘è¿‡ç¨‹ï¼Œå®ç°æºå›¾åƒè¯­ä¹‰å†…å®¹çš„ä¿ç•™å’Œç›®æ ‡å¥–åŠ±çš„å¢å¼ºã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç¼–è¾‘ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Editable-Noise-Map-Inversion-Encoding-Target-image-into-Noise-For-High-Fidelity-Image-Manipulation"><a href="#Editable-Noise-Map-Inversion-Encoding-Target-image-into-Noise-For-High-Fidelity-Image-Manipulation" class="headerlink" title="Editable Noise Map Inversion: Encoding Target-image into Noise For   High-Fidelity Image Manipulation"></a>Editable Noise Map Inversion: Encoding Target-image into Noise For   High-Fidelity Image Manipulation</h2><p><strong>Authors:Mingyu Kang, Yong Suk Choi</strong></p>
<p>Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å®ç°æœ‰æ•ˆå›¾åƒç¼–è¾‘çš„å…³é”®ç­–ç•¥æ˜¯å°†æºå›¾åƒåè½¬ä¸ºä¸ç›®æ ‡å›¾åƒç›¸å…³çš„å¯ç¼–è¾‘å™ªå£°å›¾ã€‚ç„¶è€Œï¼Œä»¥å‰çš„åè½¬æ–¹æ³•åœ¨é¢å¯¹ç´§å¯†éµå¾ªç›®æ ‡æ–‡æœ¬æç¤ºæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ç§é™åˆ¶çš„äº§ç”Ÿæ˜¯å› ä¸ºåè½¬çš„å™ªå£°å›¾è™½ç„¶èƒ½å¤Ÿä½¿æºå›¾åƒè¿›è¡Œå¿ å®çš„é‡å»ºï¼Œä½†é™åˆ¶äº†æ‰€éœ€çš„ç¼–è¾‘çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯ç¼–è¾‘å™ªå£°å›¾åè½¬ï¼ˆENMåè½¬ï¼‰è¿™ä¸€æ–°å‹åè½¬æŠ€æœ¯ï¼Œå¯»æ‰¾æœ€ä½³å™ªå£°å›¾ï¼Œä»¥ç¡®ä¿å†…å®¹å’Œå¯ç¼–è¾‘æ€§çš„ä¿ç•™ã€‚æˆ‘ä»¬åˆ†æäº†å™ªå£°å›¾çš„å±æ€§ï¼Œä»¥æé«˜å…¶å¯ç¼–è¾‘æ€§ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¯ç¼–è¾‘çš„å™ªå£°ç»†åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–é‡å»ºå’Œç¼–è¾‘å™ªå£°å›¾ä¹‹é—´çš„å·®å¼‚ï¼Œä¸æ‰€éœ€çš„ç¼–è¾‘ä¿æŒä¸€è‡´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¹¿æ³›å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼ŒENMåè½¬åœ¨ä¿ç•™å’Œç¼–è¾‘ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•å¹¶ç¬¦åˆç›®æ ‡æç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥è½»æ¾åº”ç”¨äºè§†é¢‘ç¼–è¾‘ï¼Œå®ç°è·¨å¸§çš„æ—¶é—´ä¸€è‡´æ€§å’Œå†…å®¹æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25776v1">PDF</a> ICML 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸­ä¹Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸€ç§æœ‰æ•ˆçš„å›¾åƒç¼–è¾‘ç­–ç•¥æ˜¯é€šè¿‡å°†æºå›¾åƒåè½¬ä¸ºä¸ç›®æ ‡å›¾åƒç›¸å…³çš„å¯ç¼–è¾‘å™ªå£°å›¾æ¥å®ç°ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„åè½¬æ–¹æ³•åœ¨é¢å¯¹ç´§è´´ç›®æ ‡æ–‡æœ¬æç¤ºçš„è¦æ±‚æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚é™åˆ¶äº§ç”Ÿäºåè½¬çš„å™ªå£°å›¾ï¼Œè™½ç„¶èƒ½å¤Ÿå¿ å®é‡å»ºæºå›¾åƒï¼Œä½†é™åˆ¶äº†æ‰€éœ€çš„çµæ´»æ€§ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯ç¼–è¾‘å™ªå£°å›¾åè½¬ï¼ˆENMåè½¬ï¼‰è¿™ä¸€æ–°é¢–çš„åè½¬æŠ€æœ¯ï¼Œå¯»æ‰¾æœ€ä½³çš„å™ªå£°å›¾ï¼Œç¡®ä¿å†…å®¹ä¿ç•™å’Œå¯ç¼–è¾‘æ€§ã€‚æˆ‘ä»¬åˆ†æäº†å™ªå£°å›¾çš„å±æ€§ï¼Œä»¥æé«˜å…¶å¯ç¼–è¾‘æ€§ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¯ç¼–è¾‘å™ªå£°ä¼˜åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–é‡å»ºå’Œç¼–è¾‘å™ªå£°å›¾ä¹‹é—´çš„å·®å¼‚ï¼Œä¸æ‰€éœ€çš„ç¼–è¾‘ä¿æŒä¸€è‡´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒENMåè½¬æ³•åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡çš„ä¿ç•™å’Œç¼–è¾‘ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å¯è½»æ¾åº”ç”¨äºè§†é¢‘ç¼–è¾‘ï¼Œå®ç°è·¨å¸§çš„æ—¶é—´ä¸€è‡´æ€§å’Œå†…å®¹æ“ä½œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æœ‰æ•ˆçš„å›¾åƒç¼–è¾‘ç­–ç•¥æ˜¯é€šè¿‡å°†æºå›¾åƒåè½¬ä¸ºä¸ç›®æ ‡å›¾åƒç›¸å…³çš„å¯ç¼–è¾‘å™ªå£°å›¾æ¥å®ç°ã€‚</li>
<li>ä»¥å¾€çš„å›¾åƒåè½¬æ–¹æ³•åœ¨ç´§è´´ç›®æ ‡æ–‡æœ¬æç¤ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ENMåè½¬æŠ€æœ¯æ—¨åœ¨å¯»æ‰¾æœ€ä½³å™ªå£°å›¾ï¼Œç¡®ä¿å†…å®¹ä¿ç•™å’Œç¼–è¾‘çµæ´»æ€§ã€‚</li>
<li>é€šè¿‡åˆ†æå™ªå£°å›¾çš„å±æ€§æ¥æé«˜å…¶å¯ç¼–è¾‘æ€§ã€‚</li>
<li>ENMåè½¬æ³•é€šè¿‡æœ€å°åŒ–é‡å»ºå’Œç¼–è¾‘å™ªå£°å›¾ä¹‹é—´çš„å·®å¼‚ï¼Œä¸æ‰€éœ€çš„ç¼–è¾‘ä¿æŒä¸€è‡´ã€‚</li>
<li>å¤§é‡å®éªŒè¯æ˜ï¼ŒENMåè½¬æ³•åœ¨å›¾åƒå’Œè§†é¢‘çš„ç¼–è¾‘ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs"><a href="#Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs" class="headerlink" title="Free Lunch Alignment of Text-to-Image Diffusion Models without   Preference Image Pairs"></a>Free Lunch Alignment of Text-to-Image Diffusion Models without   Preference Image Pairs</h2><p><strong>Authors:Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao</strong></p>
<p>Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables â€œfree-lunchâ€ alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at <a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment">https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç¡®ä¿æ–‡æœ¬å’Œç”Ÿæˆå›¾åƒä¹‹é—´çš„å‡†ç¡®å¯¹é½å¯¹äºæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç°æœ‰ç ”ç©¶é‡‡ç”¨å¢å¼ºå­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰çš„æ–¹æ³•ï¼Œä½¿T2Iè¾“å‡ºä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆç›´æ¥ä¾èµ–äºé…å¯¹å›¾åƒåå¥½æ•°æ®ï¼Œè¦ä¹ˆéœ€è¦ä¸€ä¸ªå­¦ä¹ çš„å¥–åŠ±å‡½æ•°ï¼Œä¸¤è€…éƒ½ä¸¥é‡ä¾èµ–äºæ˜‚è´µçš„é«˜è´¨é‡äººç±»æ³¨é‡Šï¼Œå› æ­¤é¢ä¸´å¯æ‰©å±•æ€§é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„å‡†ç¡®å¯¹é½ä»æ˜¯æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰æ¥ä¼˜åŒ–å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦å¤§é‡é«˜è´¨é‡çš„äººç±»æ ‡æ³¨æ•°æ®ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†æ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œæ— éœ€é…å¯¹å›¾åƒåå¥½æ•°æ®å³å¯å®ç°T2Iæ¨¡å‹çš„â€œå…è´¹åˆé¤â€å¯¹é½ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹ä»¥åå¥½åŒ¹é…æç¤ºè€Œéä¸åŒ¹é…æç¤ºæ¥å®ç°å¯¹é½ï¼Œä¸åŒ¹é…æç¤ºæ˜¯é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ‰°åŠ¨åŸå§‹æ ‡é¢˜æ„å»ºçš„ã€‚æˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä¸ç°æœ‰çš„åŸºäºåå¥½çš„ç®—æ³•ç›¸ç»“åˆã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå…¶åŸå§‹æ–¹æ³•ï¼Œå¹¶æé«˜äº†äººç±»åå¥½å¾—åˆ†å’Œæ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„å‡†ç¡®å¯¹é½ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰æ¥æé«˜æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¯¹é½ç²¾åº¦ï¼Œä½†è¿™ç§æ–¹æ³•éœ€è¦å¤§é‡é«˜è´¨é‡çš„äººç±»æ ‡æ³¨æ•°æ®ï¼Œå­˜åœ¨å¯æ‰©å±•æ€§é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œæ— éœ€é…å¯¹å›¾åƒåå¥½æ•°æ®å³å¯å®ç°T2Iæ¨¡å‹çš„â€œå…è´¹åˆé¤â€å¯¹é½ã€‚</li>
<li>TPOæ¡†æ¶é€šè¿‡è®­ç»ƒæ¨¡å‹ä»¥åå¥½åŒ¹é…æç¤ºæ¥å®ç°å¯¹é½ï¼Œè¿™äº›åŒ¹é…æç¤ºæ˜¯é€šè¿‡æ‰°åŠ¨åŸå§‹æ ‡é¢˜æ„å»ºçš„ã€‚</li>
<li>TPOæ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä¸ç°æœ‰çš„åŸºäºåå¥½çš„ç®—æ³•ç›¸ç»“åˆï¼Œæé«˜æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹é½ç²¾åº¦ã€‚</li>
<li>å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºTPOçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæé«˜äº†äººç±»åå¥½å¾—åˆ†å’Œå¯¹é½ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On"><a href="#ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On" class="headerlink" title="ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On"></a>ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On</h2><p><strong>Authors:Junseo Park, Hyeryung Jang</strong></p>
<p>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines. </p>
<blockquote>
<p>è™šæ‹Ÿè¯•è¡£ï¼ˆVITONï¼‰æ—¨åœ¨ç”Ÿæˆç”¨æˆ·ç©¿æˆ´ç›®æ ‡æœé¥°çš„ç°å®å›¾åƒï¼Œè¦æ±‚ç²¾ç¡®å¯¹é½æœé¥°çš„è¯•ç©¿åŒºåŸŸï¼Œå¹¶åœ¨éè¯•ç©¿åŒºåŸŸå¿ å®åœ°ä¿ç•™èº«ä»½å’ŒèƒŒæ™¯ã€‚è™½ç„¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å·²ç»æ”¹è¿›äº†å¯¹é½å’Œç»†èŠ‚åˆæˆï¼Œä½†ä¿ç•™éè¯•ç©¿åŒºåŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸€ç§å¸¸è§çš„åå¤„ç†æ–¹æ³•ç›´æ¥ç”¨åŸå§‹å†…å®¹æ›¿æ¢è¿™äº›åŒºåŸŸï¼Œä½†çªå…€çš„è¿‡æ¸¡é€šå¸¸ä¼šäº§ç”Ÿè¾¹ç•Œä¼ªå½±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†VITONé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªçº¿æ€§é€†é—®é¢˜ï¼Œå¹¶é‡‡ç”¨è½¨è¿¹å¯¹é½æ±‚è§£å™¨ï¼Œé€æ­¥å¼ºåˆ¶æ‰§è¡Œæµ‹é‡ä¸€è‡´æ€§ï¼Œå‡å°‘éè¯•ç©¿åŒºåŸŸçš„çªå…€å˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ±‚è§£å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä»å­˜åœ¨è¯­ä¹‰æ¼‚ç§»é—®é¢˜ï¼Œå¯¼è‡´å‡ºç°ä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†ART-VITONï¼Œä¸€ç§æµ‹é‡æŒ‡å¯¼çš„æ‰©æ•£æ¡†æ¶ï¼Œç¡®ä¿æµ‹é‡éµå¾ªæ€§åŒæ—¶ä¿æŒæ— ä¼ªå½±åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†åŸºäºæ®‹å·®å…ˆéªŒçš„åˆå§‹åŒ–ï¼Œä»¥å‡è½»è®­ç»ƒ-æ¨æ–­ä¸åŒ¹é…é—®é¢˜ï¼Œä»¥åŠæ— ä¼ªå½±æµ‹é‡æŒ‡å¯¼çš„é‡‡æ ·ï¼Œå®ƒç»“åˆäº†æ•°æ®ä¸€è‡´æ€§ã€é¢‘ç‡çº§åˆ«çš„æ ¡æ­£å’Œå‘¨æœŸæ€§çš„æ ‡å‡†å»å™ªã€‚åœ¨VITON-HDã€DressCodeå’ŒSHHQ-1.0ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒART-VITONæœ‰æ•ˆåœ°ä¿ç•™äº†èº«ä»½å’ŒèƒŒæ™¯ï¼Œæ¶ˆé™¤äº†è¾¹ç•Œä¼ªå½±ï¼Œå¹¶ä¸”ç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢éƒ½æœ‰äº†ä¸€è‡´çš„æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25749v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è™šæ‹Ÿè¯•ç©¿ï¼ˆVITONï¼‰æŠ€æœ¯åœ¨ç”Ÿæˆäººç©¿ç€ç›®æ ‡æœè£…çš„é€¼çœŸå›¾åƒæ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨éè¯•ç©¿åŒºåŸŸçš„èº«ä»½å’ŒèƒŒæ™¯ä¿ç•™æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æµ‹é‡å¼•å¯¼æ‰©æ•£æ¡†æ¶ART-VITONï¼Œé€šè¿‡ç»“åˆæ®‹å·®å…ˆéªŒåˆå§‹åŒ–å’Œæ— ç‘•ç–µæµ‹é‡å¼•å¯¼é‡‡æ ·ï¼Œç¡®ä¿æµ‹é‡ä¸€è‡´æ€§å¹¶ç»´æŒåˆæˆç»“æœçš„æ¸…æ™°åº¦ã€‚å®éªŒè¯æ˜ï¼ŒART-VITONåœ¨èº«ä»½å’ŒèƒŒæ™¯ä¿ç•™ã€æ¶ˆé™¤è¾¹ç•Œä¼ªå½±ã€æé«˜è§†è§‰ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VITONæŠ€æœ¯æ—¨åœ¨ç”Ÿæˆç›®æ ‡æœè£…çš„é€¼çœŸå›¾åƒï¼Œéœ€è¦ç²¾ç¡®å¯¹é½æœè£…å¹¶åœ¨éè¯•ç©¿åŒºåŸŸä¿æŒèº«ä»½å’ŒèƒŒæ™¯çš„å¿ å®ä¿ç•™ã€‚</li>
<li>LDMåœ¨æé«˜å¯¹é½å’Œç»†èŠ‚åˆæˆèƒ½åŠ›æ–¹é¢è¡¨ç°çªå‡ºï¼Œä½†ä¿ç•™éè¯•ç©¿åŒºåŸŸä»å­˜åœ¨æŒ‘æˆ˜ã€‚å¸¸è§çš„åå¤„ç†ç­–ç•¥å­˜åœ¨è¾¹ç•Œä¼ªå½±é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡æ”¹é©VITONä¸ºçº¿æ€§é€†é—®é¢˜å¹¶é‡‡ç”¨è½¨è¿¹å¯¹é½æ±‚è§£å™¨æ¥å‡å°‘éè¯•ç©¿åŒºåŸŸçš„çªå…€å˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ±‚è§£å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä»ä¼šå‡ºç°è¯­ä¹‰æ¼‚ç§»é—®é¢˜ï¼Œå¯¼è‡´ä¼ªå½±ã€‚</li>
<li>æå‡ºART-VITONæ¡†æ¶ï¼Œç¡®ä¿æµ‹é‡ä¸€è‡´æ€§åŒæ—¶ç»´æŒæ— ç‘•ç–µçš„åˆæˆç»“æœã€‚é€šè¿‡ç»“åˆæ®‹å·®å…ˆéªŒåˆå§‹åŒ–å’Œæ— ç‘•ç–µæµ‹é‡å¼•å¯¼é‡‡æ ·ï¼Œè¯¥æ¡†æ¶å®ç°äº†æ•°æ®ä¸€è‡´æ€§ã€é¢‘ç‡çº§æ ¡æ­£å’Œå‘¨æœŸæ€§æ ‡å‡†å»å™ªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LieHMR-Autoregressive-Human-Mesh-Recovery-with-SO-3-Diffusion"><a href="#LieHMR-Autoregressive-Human-Mesh-Recovery-with-SO-3-Diffusion" class="headerlink" title="LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion"></a>LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion</h2><p><strong>Authors:Donghwan Kim, Tae-Kyun Kim</strong></p>
<p>We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹ä»å•å¼ RGBå›¾åƒè¿›è¡Œäººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰çš„é—®é¢˜ï¼Œå°†å…¶è¡¨è¿°ä¸ºä»¥å›¾åƒä¸ºæ¡ä»¶çš„äººä½“å§¿æ€å’Œå½¢çŠ¶ç”Ÿæˆã€‚ä»2Dè§‚å¯Ÿæ¢å¤3Däººä½“å§¿æ€æœ¬è´¨ä¸Šæ˜¯å…·æœ‰æ­§ä¹‰çš„ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½å›å½’äº†å•ä¸€ç¡®å®šæ€§è¾“å‡ºã€‚æ¦‚ç‡æ–¹æ³•è¯•å›¾é€šè¿‡ç”Ÿæˆå¤šä¸ªåˆç†è¾“å‡ºå¯¹æ­§ä¹‰è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ ·æœ¬å¤šæ ·æ€§ä¹‹é—´å¾€å¾€å­˜åœ¨æƒè¡¡ï¼Œå…¶å•ä¸€é¢„æµ‹å¹¶ä¸å…·å¤‡ä¸æœ€æ–°ç¡®å®šæ€§æ¨¡å‹ç«äº‰çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹2Dè§‚æµ‹è¿›è¡Œè‰¯å¥½å¯¹é½çš„å»ºæ¨¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†$SO(3)$æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ¡ä»¶ä¸¢å¼ƒç”Ÿæˆä½œä¸ºä¸‰ç»´æ—‹è½¬çš„ä½“æ€å‚æ•°åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒæ— æ¡ä»¶åœ°å’Œæœ‰é€‰æ‹©åœ°å–å†³äºå›¾åƒè§‚æµ‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨å˜å‹å™¨å­¦ä¹ äººä½“å…³èŠ‚çš„å±‚æ¬¡ç»“æ„ã€‚ä¸åŒäºå°†å˜å‹å™¨ç”¨ä½œå»å™ªæ¨¡å‹çš„åšæ³•ï¼Œæ—¶é—´ç‹¬ç«‹å˜å‹å™¨æå–å…³èŠ‚çš„æ½œåœ¨å‘é‡ï¼Œå¹¶ä¸”åŸºäºå°å‹å¤šå±‚æ„ŸçŸ¥æœºçš„å»å™ªæ¨¡å‹å­¦ä¹ åŸºäºæ½œåœ¨å‘é‡çš„å…³èŠ‚åˆ†å¸ƒã€‚æˆ‘ä»¬å®éªŒæ€§åœ°è¯æ˜å¹¶åˆ†æäº†æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°é¢„æµ‹å§¿æ€æ¦‚ç‡åˆ†å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25739v1">PDF</a> 17 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³ä»å•ä¸€RGBå›¾åƒè¿›è¡Œäººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰çš„é—®é¢˜ï¼Œå°†å…¶è¡¨è¿°ä¸ºå›¾åƒæ¡ä»¶ä¸‹çš„äººä½“å§¿æ€å’Œå½¢çŠ¶ç”Ÿæˆã€‚é’ˆå¯¹ä»2Dè§‚æµ‹æ¢å¤3Däººä½“å§¿æ€çš„å›ºæœ‰æ¨¡ç³Šæ€§ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å›å½’äº†å•ä¸€ç¡®å®šæ€§è¾“å‡ºã€‚æ¦‚ç‡æ–¹æ³•è¯•å›¾é€šè¿‡ç”Ÿæˆå¤šä¸ªåˆç†è¾“å‡ºå¯¹æ¨¡ç³Šæ€§è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ ·æœ¬å¤šæ ·æ€§ä¹‹é—´å¾€å¾€å­˜åœ¨æƒè¡¡ï¼Œå…¶å•ä¸€é¢„æµ‹å¹¶ä¸å…·å¤‡ä¸æœ€æ–°ç¡®å®šæ€§æ¨¡å‹ç«äº‰çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå¯¹2Dè§‚æµ‹è¿›è¡Œè‰¯å¥½å¯¹é½çš„åˆ†å¸ƒå»ºæ¨¡ã€‚ç‰¹åˆ«æ˜¯å¼•å…¥äº†$SO(3)$æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ¡ä»¶ä¸¢å¼ƒï¼Œæ— æ¡ä»¶åœ°è¡¨ç¤ºå§¿æ€å‚æ•°çš„åˆ†å¸ƒï¼Œå¹¶æ ¹æ®å›¾åƒè§‚æµ‹è¿›è¡Œæ¡ä»¶åŒ–ã€‚è¯¥æ¨¡å‹å­¦ä¹ ä½¿ç”¨å˜å‹å™¨çš„äººä½“å…³èŠ‚å±‚æ¬¡ç»“æ„ï¼Œæ—¶é—´ç‹¬ç«‹å˜å‹å™¨æå–å…³èŠ‚çš„æ½œåœ¨å‘é‡ï¼ŒåŸºäºå°å‹å¤šå±‚æ„ŸçŸ¥æœºçš„å»å™ªæ¨¡å‹å­¦ä¹ åŸºäºæ½œåœ¨å‘é‡çš„å…³èŠ‚åˆ†å¸ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆåœ°é¢„æµ‹å‡†ç¡®çš„å§¿æ€æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³ä»å•ä¸€RGBå›¾åƒè¿›è¡Œäººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰çš„é—®é¢˜ï¼Œè¡¨è¿°ä¸ºå›¾åƒæ¡ä»¶ä¸‹çš„äººä½“å§¿æ€å’Œå½¢çŠ¶ç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å›å½’å•ä¸€ç¡®å®šæ€§è¾“å‡ºçš„é—®é¢˜ï¼Œæ¦‚ç‡æ–¹æ³•è¯•å›¾é€šè¿‡ç”Ÿæˆå¤šä¸ªè¾“å‡ºå¯¹æ¨¡ç³Šæ€§è¿›è¡Œå»ºæ¨¡ä½†å­˜åœ¨æƒè¡¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥$SO(3)$æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ¡ä»¶ä¸¢å¼ƒï¼Œå¯¹å§¿æ€å‚æ•°çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å˜å‹å™¨å’Œæ—¶é—´ç‹¬ç«‹å˜å‹å™¨æŠ€æœ¯ï¼Œæå–å…³èŠ‚çš„æ½œåœ¨å‘é‡ã€‚</li>
<li>åŸºäºå¤šå±‚æ„ŸçŸ¥æœºçš„å»å™ªæ¨¡å‹å­¦ä¹ æ½œåœ¨å‘é‡ä¸‹çš„å…³èŠ‚åˆ†å¸ƒã€‚</li>
<li>æ¨¡å‹èƒ½æœ‰æ•ˆé¢„æµ‹å‡†ç¡®çš„å§¿æ€æ¦‚ç‡åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space"><a href="#DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space" class="headerlink" title="DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space"></a>DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space</h2><p><strong>Authors:Wenkun He, Yuchao Gu, Junyu Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Haocheng Xi, Muyang Li, Ligeng Zhu, Jincheng Yu, Junsong Chen, Enze Xie, Song Han, Han Cai</strong></p>
<p>Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base modelâ€™s latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base modelâ€™s inherent generation quality. We verify DC-Genâ€™s effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: <a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen">https://github.com/dc-ai-projects/DC-Gen</a>. </p>
<blockquote>
<p>ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ï¼ˆå¦‚4Kå›¾åƒç”Ÿæˆï¼‰æ—¶é¢ä¸´é‡å¤§çš„æ•ˆç‡æŒ‘æˆ˜ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶ä»å„ä¸ªæ–¹é¢åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹ï¼Œä½†å¾ˆå°‘å¤„ç†æ½œåœ¨ç©ºé—´ä¸­çš„å›ºæœ‰å†—ä½™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ä»‹ç»äº†DC-Genï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡åˆ©ç”¨æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´æ¥åŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é€šç”¨æ¡†æ¶ã€‚DC-Genä¸åŒäºæ˜‚è´µçš„ä»å¤´å¼€å§‹è®­ç»ƒæ–¹æ³•ï¼Œå®ƒé‡‡ç”¨é«˜æ•ˆçš„åè®­ç»ƒç®¡é“æ¥ä¿ç•™åŸºç¡€æ¨¡å‹çš„è´¨é‡ã€‚åœ¨è¿™ç§èŒƒå¼ä¸­çš„å…³é”®æŒ‘æˆ˜æ˜¯åŸºç¡€æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¹‹é—´çš„è¡¨ç¤ºå·®è·ï¼Œè¿™å¯èƒ½å¯¼è‡´ç›´æ¥å¾®è°ƒæ—¶çš„ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼ŒDC-Gené¦–å…ˆé€šè¿‡è½»é‡çº§çš„åµŒå…¥å¯¹é½è®­ç»ƒæ¥å¼¥åˆè¡¨ç¤ºå·®è·ã€‚ä¸€æ—¦æ½œåœ¨åµŒå…¥å¯¹é½ï¼Œåªéœ€å°‘é‡çš„LoRAå¾®è°ƒå³å¯è§£é”åŸºç¡€æ¨¡å‹çš„å›ºæœ‰ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬åœ¨SANAå’ŒFLUX.1-Kreaä¸ŠéªŒè¯äº†DC-Gençš„æœ‰æ•ˆæ€§ã€‚DC-Gen-SANAå’ŒDC-Gen-FLUXæ¨¡å‹åœ¨è´¨é‡ä¸Šä¸åŸºç¡€æ¨¡å‹ç›¸å½“ï¼Œä½†å…·æœ‰æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚å…·ä½“æ¥è¯´ï¼ŒDC-Gen-FLUXåœ¨NVIDIA H100 GPUä¸Šå°†4Kå›¾åƒçš„ç”Ÿæˆå»¶è¿Ÿå‡å°‘äº†53å€ã€‚å½“ä¸NVFP4 SVDQuantç»“åˆæ—¶ï¼ŒDC-Gen-FLUXåœ¨å•ä¸ªNVIDIA 5090 GPUä¸Šåªéœ€3.5ç§’å³å¯ç”Ÿæˆ4Kå›¾åƒï¼Œä¸åŸºç¡€FLUX.1-Kreaæ¨¡å‹ç›¸æ¯”ï¼Œæ€»å»¶è¿Ÿé™ä½äº†138å€ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen%E3%80%82">https://github.com/dc-ai-projects/DC-Genã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25180v1">PDF</a> Tech Report. The first three authors contributed equally to this work</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDC-Gençš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´æ¥åŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚DC-Gené‡‡ç”¨é«˜æ•ˆçš„è®­ç»ƒåæµç¨‹ï¼Œé¿å…äº†æ˜‚è´µçš„ä»å¤´å¼€å§‹è®­ç»ƒçš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„è´¨é‡ã€‚è¯¥æ¡†æ¶è§£å†³äº†åŸºç¡€æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¹‹é—´çš„è¡¨ç¤ºå·®è·é—®é¢˜ï¼Œé€šè¿‡è½»é‡çº§çš„åµŒå…¥å¯¹é½è®­ç»ƒæ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚ä¸€æ—¦å¯¹é½æ½œåœ¨åµŒå…¥ï¼Œåªéœ€å°‘é‡çš„LoRAå¾®è°ƒå³å¯è§£é”åŸºç¡€æ¨¡å‹çš„å›ºæœ‰ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒDC-Genåœ¨SANAå’ŒFLUX.1-Kreaæ¨¡å‹ä¸Šå…·æœ‰è‰¯å¥½çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯DC-Gen-FLUXåœ¨NVIDIA H100 GPUä¸Šå°†ç”Ÿæˆ4Kå›¾åƒçš„å»¶è¿Ÿå‡å°‘äº†53å€ã€‚å½“ä¸NVFP4 SVDQuantç»“åˆæ—¶ï¼ŒDC-Gen-FLUXåœ¨å•ä¸ªNVIDIA 5090 GPUä¸Šä»…ç”¨äº†3.5ç§’å°±ç”Ÿæˆäº†ä¸€ä¸ª4Kå›¾åƒï¼Œä¸åŸºç¡€FLUX.1-Kreaæ¨¡å‹ç›¸æ¯”ï¼Œæ€»å»¶è¿Ÿå‡å°‘äº†é«˜è¾¾138å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DC-Genæ¡†æ¶æ—¨åœ¨åŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>DC-Gené€šè¿‡é«˜æ•ˆçš„è®­ç»ƒåæµç¨‹é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„é«˜æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„è´¨é‡ã€‚</li>
<li>æ¡†æ¶è§£å†³äº†åŸºç¡€æ¨¡å‹ä¸æ·±åº¦å‹ç¼©æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºå·®è·é—®é¢˜ï¼Œé€šè¿‡åµŒå…¥å¯¹é½è®­ç»ƒè¿›è¡Œå¼¥åˆã€‚</li>
<li>DC-Genèƒ½æé«˜æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>DC-Genæ˜¾è‘—é™ä½äº†ç”Ÿæˆ4Kå›¾åƒæ‰€éœ€çš„å»¶è¿Ÿæ—¶é—´ã€‚</li>
<li>ä¸å…¶ä»–æŠ€æœ¯ç»“åˆï¼ŒDC-Genèƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œå®ç°æ›´å¿«çš„å›¾åƒç”Ÿæˆé€Ÿåº¦ã€‚</li>
<li>DC-Genå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ•ˆæ–‡æœ¬åˆ°å›¾åƒè½¬æ¢çš„åœºåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GLASS-Flows-Transition-Sampling-for-Alignment-of-Flow-and-Diffusion-Models"><a href="#GLASS-Flows-Transition-Sampling-for-Alignment-of-Flow-and-Diffusion-Models" class="headerlink" title="GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion   Models"></a>GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion   Models</h2><p><strong>Authors:Peter Holderrieth, Uriel Singer, Tommi Jaakkola, Ricky T. Q. Chen, Yaron Lipman, Brian Karrer</strong></p>
<p>The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a â€œflow matching model within a flow matching modelâ€ to sample Markov transitions. As we show in this work, this â€œinnerâ€ flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models. </p>
<blockquote>
<p>ä½¿ç”¨å¥–åŠ±å¯¹é½ç®—æ³•å¯ä»¥åœ¨æ¨ç†æ—¶é—´æ˜¾è‘—æé«˜æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†æ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦é™åˆ¶ã€‚è™½ç„¶æå‡ºäº†å‡ ç§ç®—æ³•ï¼Œä½†æˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªå…±åŒçš„ç“¶é¢ˆåœ¨äºè¿™äº›ç®—æ³•æ‰€ä¾èµ–çš„é‡‡æ ·æ–¹æ³•ï¼šè®¸å¤šç®—æ³•éœ€è¦é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰é‡‡æ ·è¿›è¡Œé©¬å°”å¯å¤«è½¬æ¢é‡‡æ ·ï¼Œè¿™æ˜¾è‘—åœ°ä¸å¦‚å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·é«˜æ•ˆä¸”æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†æ¶ˆé™¤è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†GLASS Flowsè¿™ä¸€æ–°å‹é‡‡æ ·èŒƒå¼ï¼Œå®ƒæ¨¡æ‹Ÿâ€œæµåŒ¹é…æ¨¡å‹å†…çš„æµåŒ¹é…æ¨¡å‹â€æ¥é‡‡æ ·é©¬å°”å¯å¤«è½¬æ¢ã€‚æˆ‘ä»¬åœ¨æœ¬å·¥ä½œä¸­å±•ç¤ºï¼Œè¿™ä¸ªâ€œå†…éƒ¨â€æµåŒ¹é…æ¨¡å‹å¯ä»¥ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æ£€ç´¢å‡ºæ¥ï¼Œæ— éœ€ä»»ä½•å†è®­ç»ƒï¼Œç»“åˆäº†å¸¸å¾®åˆ†æ–¹ç¨‹çš„é«˜æ•ˆæ€§å’Œéšæœºå¾®åˆ†æ–¹ç¨‹çš„éšæœºæ¼”åŒ–ã€‚åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†GLASS Flowsæ¶ˆé™¤äº†éšæœºæ¼”åŒ–å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç»“åˆè´¹æ›¼-å¡å…‹è½¬å‘ï¼ŒGLASS Flowsæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºæµå’Œæ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾çš„ä¸€ç§ç®€å•ã€å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25170v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½å¯é€šè¿‡ä½¿ç”¨å¥–åŠ±å¯¹é½ç®—æ³•åœ¨æ¨ç†æ—¶é—´æ˜¾è‘—æé«˜ï¼Œä½†æ•ˆç‡ä»æ˜¯ä¸»è¦é™åˆ¶ã€‚è™½ç„¶å·²æå‡ºå¤šç§ç®—æ³•ï¼Œä½†æ¼”ç¤ºè¡¨æ˜ï¼Œè¿™äº›ç®—æ³•æ‰€ä¾èµ–çš„é‡‡æ ·æ–¹æ³•æ˜¯å…±åŒç“¶é¢ˆï¼šè®¸å¤šç®—æ³•éœ€è¦é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰é‡‡æ ·è¿›è¡Œé©¬å°”å¯å¤«è¿‡æ¸¡é‡‡æ ·ï¼Œè¿™æ˜¾è‘—é™ä½äº†æ•ˆç‡å¹¶ä¸”å¾€å¾€æ€§èƒ½ä¸å¦‚å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·ã€‚ä¸ºäº†æ¶ˆé™¤è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†GLASSæµè¿™ä¸€æ–°é‡‡æ ·èŒƒå¼ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿâ€œæµåŒ¹é…æ¨¡å‹å†…çš„æµåŒ¹é…æ¨¡å‹â€æ¥é‡‡æ ·é©¬å°”å¯å¤«è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨å·¥ä½œä¸­å±•ç¤ºï¼Œè¿™ä¸ªâ€œå†…éƒ¨â€æµåŒ¹é…æ¨¡å‹å¯ä»¥ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–ï¼Œæ— éœ€ä»»ä½•å†è®­ç»ƒï¼Œç»“åˆäº†ODEçš„æ•ˆç‡ä¸SDEçš„éšæœºæ¼”å˜ã€‚åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºGLASSæµæ¶ˆé™¤äº†éšæœºæ¼”å˜ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç»“åˆè´¹æ›¼-å¡å…‹è½¬å‘ï¼ŒGLASSæµæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæˆä¸ºæµå’Œæ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´æ‰©å±•çš„ç®€å•å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±å¯¹é½ç®—æ³•å¯ä»¥æé«˜æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹çš„æ¨ç†æ—¶é—´æ€§èƒ½ã€‚</li>
<li>æ•ˆç‡æ˜¯æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹çš„ä¸»è¦é™åˆ¶ä¹‹ä¸€ã€‚</li>
<li>ç°æœ‰ç®—æ³•ä¾èµ–äºé€šè¿‡SDEé‡‡æ ·çš„é©¬å°”å¯å¤«è¿‡æ¸¡é‡‡æ ·ï¼Œè¿™ä¸å¤ªé«˜æ•ˆä¸”æ€§èƒ½æœ‰é™ã€‚</li>
<li>GLASSæµæ˜¯ä¸€ç§æ–°é‡‡æ ·èŒƒå¼ï¼Œæ¨¡æ‹Ÿâ€œæµåŒ¹é…æ¨¡å‹å†…çš„æµåŒ¹é…æ¨¡å‹â€ä»¥é‡‡æ ·é©¬å°”å¯å¤«è¿‡æ¸¡ã€‚</li>
<li>GLASSæµçš„â€œå†…éƒ¨â€æµåŒ¹é…æ¨¡å‹å¯ä»¥ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–ï¼Œæ— éœ€å†è®­ç»ƒã€‚</li>
<li>GLASSæµç»“åˆäº†ODEçš„æ•ˆç‡ä¸SDEçš„éšæœºæ¼”å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25170v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25170v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Aligning-Visual-Foundation-Encoders-to-Tokenizers-for-Diffusion-Models"><a href="#Aligning-Visual-Foundation-Encoders-to-Tokenizers-for-Diffusion-Models" class="headerlink" title="Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models"></a>Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models</h2><p><strong>Authors:Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, Kai Zhang</strong></p>
<p>In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å¯¹é½ï¼Œä»¥ä½œä¸ºå›¾åƒç”Ÿæˆä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ ‡è®°å™¨ã€‚ä¸åŒäºä»å¤´å¼€å§‹è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸»è¦å¼ºè°ƒä½å±‚æ¬¡ç»†èŠ‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºç¡€ç¼–ç å™¨çš„ä¸°å¯Œè¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸‰é˜¶æ®µå¯¹é½ç­–ç•¥ï¼šï¼ˆ1ï¼‰å†»ç»“ç¼–ç å™¨ï¼Œå¹¶è®­ç»ƒé€‚é…å™¨å’Œè§£ç å™¨ä»¥å»ºç«‹è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼›ï¼ˆ2ï¼‰é€šè¿‡é¢å¤–çš„è¯­ä¹‰ä¿ç•™æŸå¤±è”åˆä¼˜åŒ–æ‰€æœ‰ç»„ä»¶ï¼Œä½¿ç¼–ç å™¨èƒ½å¤Ÿæ•è·æ„ŸçŸ¥ç»†èŠ‚ï¼ŒåŒæ—¶ä¿ç•™é«˜çº§è¯­ä¹‰ï¼›ï¼ˆ3ï¼‰å¯¹è§£ç å™¨è¿›è¡Œå¾®è°ƒä»¥æé«˜é‡å»ºè´¨é‡ã€‚è¿™ç§å¯¹é½äº§ç”Ÿäº†ä¸°å¯Œçš„è¯­ä¹‰å›¾åƒæ ‡è®°å™¨ï¼Œå¯¹æ‰©æ•£æ¨¡å‹æœ‰ç›Šã€‚åœ¨ImageNet 256x256ä¸Šï¼Œæˆ‘ä»¬çš„æ ‡è®°å™¨åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›ï¼Œä»…åœ¨64ä¸ªå‘¨æœŸå†…å°±è¾¾åˆ°äº†1.90çš„gFIDï¼Œå¹¶åœ¨æœ‰æ— åˆ†ç±»å™¨å¼•å¯¼çš„æƒ…å†µä¸‹éƒ½æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚æ‰©å±•åˆ°LAIONï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ ‡è®°å™¨è®­ç»ƒçš„2Bå‚æ•°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç›¸åŒè®­ç»ƒæ­¥éª¤ä¸‹å§‹ç»ˆä¼˜äºFLUX VAEã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€å•ã€å¯æ‰©å±•ï¼Œä¸ºè¿ç»­æ ‡è®°å™¨è®¾è®¡å»ºç«‹äº†ä¸€ä¸ªè¯­ä¹‰åŸºç¡€çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25162v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://aligntok.github.io/">https://aligntok.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å¯¹é½ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ ‡è®°å™¨ã€‚è¯¥æ–¹æ³•ä¸åŒäºä»å¤´å¼€å§‹è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œè€Œä¾§é‡äºåˆ©ç”¨åŸºç¡€ç¼–ç å™¨çš„ä¸°å¯Œè¯­ä¹‰ç»“æ„ã€‚å¼•å…¥çš„ä¸‰é˜¶æ®µå¯¹é½ç­–ç•¥åŒ…æ‹¬ï¼šå†»ç»“ç¼–ç å™¨ï¼Œè®­ç»ƒé€‚é…å™¨å’Œè§£ç å™¨ä»¥å»ºç«‹è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼›è”åˆä¼˜åŒ–æ‰€æœ‰ç»„ä»¶ï¼Œå¹¶æ·»åŠ è¯­ä¹‰ä¿ç•™æŸå¤±ï¼Œä½¿ç¼–ç å™¨èƒ½å¤Ÿæ•è·æ„ŸçŸ¥ç»†èŠ‚å¹¶ä¿ç•™é«˜çº§è¯­ä¹‰ï¼›ä»¥åŠæ”¹è¿›è§£ç å™¨çš„é‡å»ºè´¨é‡ã€‚è¿™ç§å¯¹é½æ–¹å¼ç”Ÿæˆäº†æœ‰ç›Šäºæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰ä¸°å¯Œçš„å›¾åƒæ ‡è®°å™¨ã€‚åœ¨ImageNet 256x256ä¸Šï¼Œè¯¥æ ‡è®°å™¨åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›ï¼Œä»…64ä¸ªå‘¨æœŸå°±è¾¾åˆ°äº†1.90çš„gFIDï¼Œå¹¶åœ¨æœ‰å’Œæ— åˆ†ç±»å™¨å¼•å¯¼çš„æƒ…å†µä¸‹éƒ½æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚åœ¨LAIONä¸Šæ‰©å±•åˆ°2Bå‚æ•°çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œä½¿ç”¨æ­¤æ ‡è®°å™¨çš„è¡¨ç°å§‹ç»ˆä¼˜äºFLUX VAEã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•ç®€å•ã€å¯æ‰©å±•ï¼Œå¹¶ä¸ºè¿ç»­æ ‡è®°å™¨è®¾è®¡å»ºç«‹äº†è¯­ä¹‰åŸºç¡€çš„æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä½¿ç”¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å¯¹é½ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ ‡è®°å™¨çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µå¯¹é½ç­–ç•¥ï¼ŒåŒ…æ‹¬å»ºç«‹è¯­ä¹‰æ½œåœ¨ç©ºé—´ã€è”åˆä¼˜åŒ–ç»„ä»¶å¹¶æ·»åŠ è¯­ä¹‰ä¿ç•™æŸå¤±ï¼Œä»¥åŠæ”¹è¿›è§£ç å™¨ã€‚</li>
<li>æ–¹æ³•åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›ï¼Œå¹¶åœ¨ImageNetä¸Šè¾¾åˆ°äº†è¾ƒé«˜çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨LAIONæ•°æ®é›†ä¸Šæ‰©å±•åˆ°2Bå‚æ•°çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¯¹æ¯”FLUX VAEï¼Œä½¿ç”¨æ­¤æ ‡è®°å™¨çš„æ¨¡å‹åœ¨ç›¸åŒè®­ç»ƒæ­¥éª¤ä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>è¯¥æ–¹æ³•ç®€å•ã€å¯æ‰©å±•ï¼Œä¸ºè¿ç»­æ ‡è®°å™¨è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"><a href="#FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing" class="headerlink" title="FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing"></a>FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing</h2><p><strong>Authors:Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit">https://github.com/JunyiWuCode/FlashEdit</a>. </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†æ˜¾è‘—çš„å“è´¨ï¼Œä½†ä¾ç„¶é¢ä¸´éš¾ä»¥æ‰¿å—çš„å»¶è¿Ÿé—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†FlashEditï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸã€å®æ—¶çš„å›¾åƒç¼–è¾‘ã€‚å®ƒçš„æ•ˆç‡æºäºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€æ­¥å¼åè½¬å’Œç¼–è¾‘ï¼ˆOSIEï¼‰ç®¡é“ï¼Œç»•è¿‡æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ï¼›ï¼ˆ2ï¼‰èƒŒæ™¯å±è”½ï¼ˆBG-Shieldï¼‰æŠ€æœ¯ï¼Œé€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹ä»…ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾æ¥ä¿è¯èƒŒæ™¯ä¿å­˜ï¼›ï¼ˆ3ï¼‰ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ï¼Œé€šè¿‡æŠ‘åˆ¶è¯­ä¹‰æ³„éœ²åˆ°èƒŒæ™¯æ¥ç¡®ä¿ç²¾ç¡®ã€å±€éƒ¨ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlashEditåœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘æ—¶é—´ä¸åˆ°0.2ç§’ï¼Œä¸å…ˆå‰çš„å¤šæ­¥éª¤æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†è¶…è¿‡150å€ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunyiWuCode/FlashEditä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22244v3">PDF</a> We need to improve our work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶FlashEditï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦çš„å®æ—¶å›¾åƒç¼–è¾‘ã€‚å…¶æ•ˆç‡æºäºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯ç»•è¿‡æ˜‚è´µè¿­ä»£è¿‡ç¨‹çš„ä¸€æ­¥åè½¬å’Œç¼–è¾‘ï¼ˆOSIEï¼‰ç®¡é“ï¼›äºŒæ˜¯é€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾æ¥ä¿è¯èƒŒæ™¯ä¿ç•™çš„èƒŒæ™¯å±è”½ï¼ˆBG-Shieldï¼‰æŠ€æœ¯ï¼›ä¸‰æ˜¯æŠ‘åˆ¶èƒŒæ™¯è¯­ä¹‰æ³„éœ²ä»¥å®ç°ç²¾ç¡®å±€éƒ¨ç¼–è¾‘çš„ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ã€‚FlashEditä¿æŒäº†å“è¶Šçš„å›¾åƒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°0.2ç§’å†…å®Œæˆç¼–è¾‘æ“ä½œï¼Œä¸å…ˆå‰å¤šæ­¥éª¤æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†è¶…è¿‡150å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FlashEditæ˜¯ä¸€ä¸ªç”¨äºå®ç°é«˜ä¿çœŸåº¦å®æ—¶å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ•ˆç‡æ¥æºäºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ­¥åè½¬å’Œç¼–è¾‘ç®¡é“ï¼ˆOSIEï¼‰ã€èƒŒæ™¯å±è”½æŠ€æœ¯å’Œç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>FlashEditèƒ½å¤Ÿåœ¨ä¸åˆ°0.2ç§’å†…å®Œæˆå›¾åƒç¼–è¾‘æ“ä½œï¼Œç›¸æ¯”ä»¥å‰çš„æ–¹æ³•å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚</li>
<li>OSIEç®¡é“ç»•è¿‡äº†æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>èƒŒæ™¯å±è”½æŠ€æœ¯é€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾ï¼Œä¿è¯äº†èƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚</li>
<li>ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†ç²¾ç¡®çš„å±€éƒ¨ç¼–è¾‘ï¼ŒæŠ‘åˆ¶äº†è¯­ä¹‰æ³„éœ²åˆ°èƒŒæ™¯ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒFlashEditä¿æŒäº†ä¼˜ç§€çš„èƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TADA-Improved-Diffusion-Sampling-with-Training-free-Augmented-Dynamics"><a href="#TADA-Improved-Diffusion-Sampling-with-Training-free-Augmented-Dynamics" class="headerlink" title="TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics"></a>TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics</h2><p><strong>Authors:Tianrong Chen, Huangjie Zheng, David Berthelot, Jiatao Gu, Josh Susskind, Shuangfei Zhai</strong></p>
<p>Diffusion models have demonstrated exceptional capabilities in generating high-fidelity images but typically suffer from inefficient sampling. Many solver designs and noise scheduling strategies have been proposed to dramatically improve sampling speeds. In this paper, we introduce a new sampling method that is up to $186%$ faster than the current state of the art solver for comparative FID on ImageNet512. This new sampling method is training-free and uses an ordinary differential equation (ODE) solver. The key to our method resides in using higher-dimensional initial noise, allowing to produce more detailed samples with less function evaluations from existing pretrained diffusion models. In addition, by design our solver allows to control the level of detail through a simple hyper-parameter at no extra computational cost. We present how our approach leverages momentum dynamics by establishing a fundamental equivalence between momentum diffusion models and conventional diffusion models with respect to their training paradigms. Moreover, we observe the use of higher-dimensional noise naturally exhibits characteristics similar to stochastic differential equations (SDEs). Finally, we demonstrate strong performances on a set of representative pretrained diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover models in both pixel and latent spaces, as well as class and text conditional settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/apple/ml-tada">https://github.com/apple/ml-tada</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†é€šå¸¸å­˜åœ¨é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºäº†æ˜¾è‘—æé«˜é‡‡æ ·é€Ÿåº¦ï¼Œå·²ç»æå‡ºäº†è®¸å¤šæ±‚è§£å™¨è®¾è®¡å’Œå™ªå£°è°ƒåº¦ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•ï¼Œå…¶é€Ÿåº¦æ¯”å½“å‰æœ€å…ˆè¿›çš„æ±‚è§£å™¨åœ¨ImageNet512ä¸Šè¿›è¡Œæ¯”è¾ƒæ—¶å¿«è¾¾186%ã€‚è¿™ç§æ–°çš„é‡‡æ ·æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºä½¿ç”¨é«˜ç»´åˆå§‹å™ªå£°ï¼Œä»è€Œåˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»¥è¾ƒå°‘çš„å‡½æ•°è¯„ä¼°äº§ç”Ÿæ›´è¯¦ç»†çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ±‚è§£å™¨è®¾è®¡å…è®¸é€šè¿‡ç®€å•çš„è¶…å‚æ•°æ§åˆ¶ç»†èŠ‚å±‚æ¬¡ï¼Œè€Œæ— éœ€é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•å¦‚ä½•åˆ©ç”¨åŠ¨é‡åŠ¨åŠ›å­¦ï¼Œé€šè¿‡å»ºç«‹åŠ¨é‡æ‰©æ•£æ¨¡å‹ä¸å¸¸è§„æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒèŒƒå¼æ–¹é¢çš„åŸºæœ¬ç­‰ä»·å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é«˜ç»´å™ªå£°çš„ä½¿ç”¨è‡ªç„¶è¡¨ç°å‡ºä¸éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰ç›¸ä¼¼çš„ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸€ç»„ä»£è¡¨æ€§çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬EDMã€EDM2å’ŒStable-Diffusion 3ï¼Œè¿™äº›æ¨¡å‹æ¶µç›–äº†åƒç´ å’Œæ½œåœ¨ç©ºé—´ä»¥åŠç±»å’Œæ–‡æœ¬æ¡ä»¶è®¾ç½®ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/apple/ml-tada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/apple/ml-tadaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21757v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨ï¼Œæ— éœ€è®­ç»ƒï¼Œå³å¯æ˜¾è‘—æå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ•ˆç‡ï¼Œé€Ÿåº¦æ¯”å½“å‰æœ€å…ˆè¿›çš„æ±‚è§£å™¨å¿«è¾¾186%ã€‚æ–°æ–¹æ³•åˆ©ç”¨é«˜ç»´åˆå§‹å™ªå£°ï¼Œå‡å°‘å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼Œäº§ç”Ÿæ›´è¯¦ç»†çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡ï¼Œè¯¥æ–¹æ³•å¯é€šè¿‡ç®€å•è¶…å‚æ•°æ§åˆ¶ç»†èŠ‚å±‚æ¬¡ï¼Œæ— éœ€é¢å¤–è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•è¿˜åˆ©ç”¨åŠ¨é‡åŠ¨åŠ›å­¦ï¼Œå»ºç«‹åŠ¨é‡æ‰©æ•£æ¨¡å‹ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„è®­ç»ƒèŒƒå¼ç­‰ä»·å…³ç³»ã€‚ä½¿ç”¨é«˜ç»´å™ªå£°å±•ç°å‡ºä¸éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰ç›¸ä¼¼çš„ç‰¹æ€§ã€‚åœ¨å¤šä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºæ€§èƒ½ï¼ŒåŒ…æ‹¬EDMã€EDM2ã€Stable-Diffusion 3ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°çš„é‡‡æ ·æ–¹æ³•ï¼Œæ˜¾è‘—æå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ•ˆç‡ï¼Œé€Ÿåº¦æ¯”å½“å‰æœ€å…ˆè¿›çš„æ±‚è§£å™¨å¿«è¾¾186%ã€‚</li>
<li>ä½¿ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨ï¼Œæ— éœ€è®­ç»ƒã€‚</li>
<li>åˆ©ç”¨é«˜ç»´åˆå§‹å™ªå£°ï¼Œå‡å°‘å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼Œäº§ç”Ÿæ›´è¯¦ç»†çš„æ ·æœ¬ã€‚</li>
<li>é€šè¿‡ç®€å•è¶…å‚æ•°æ§åˆ¶ç»†èŠ‚å±‚æ¬¡ï¼Œæ— éœ€é¢å¤–è®¡ç®—æˆæœ¬ã€‚</li>
<li>æ–¹æ³•å»ºç«‹åŠ¨é‡æ‰©æ•£æ¨¡å‹ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„è®­ç»ƒèŒƒå¼ç­‰ä»·å…³ç³»ã€‚</li>
<li>ä½¿ç”¨é«˜ç»´å™ªå£°å±•ç°å‡ºä¸éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰ç›¸ä¼¼çš„ç‰¹æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºæ€§èƒ½ï¼ŒåŒ…æ‹¬EDMã€EDM2ã€Stable-Diffusion 3ç­‰æ¨¡å‹çš„åƒç´ å’Œæ½œåœ¨ç©ºé—´ã€ç±»åˆ«å’Œæ–‡æœ¬æ¡ä»¶è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InverseBench-Benchmarking-Plug-and-Play-Diffusion-Priors-for-Inverse-Problems-in-Physical-Sciences"><a href="#InverseBench-Benchmarking-Plug-and-Play-Diffusion-Priors-for-Inverse-Problems-in-Physical-Sciences" class="headerlink" title="InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse   Problems in Physical Sciences"></a>InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse   Problems in Physical Sciences</h2><p><strong>Authors:Hongkai Zheng, Wenda Chu, Bingliang Zhang, Zihui Wu, Austin Wang, Berthy T. Feng, Caifeng Zou, Yu Sun, Nikola Kovachki, Zachary E. Ross, Katherine L. Bouman, Yisong Yue</strong></p>
<p>Plug-and-play diffusion priors (PnPDP) have emerged as a promising research direction for solving inverse problems.   However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \textsc{InverseBench}, a framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as optical tomography, medical imaging, black hole imaging, seismology, and fluid dynamics. With \textsc{InverseBench}, we benchmark 14 inverse problem algorithms that use plug-and-play diffusion priors against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. To facilitate further research and development, we open-source the codebase, along with datasets and pre-trained models, at <a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/">https://devzhk.github.io/InverseBench/</a>. </p>
<blockquote>
<p>å³æ’å³ç”¨æ‰©æ•£å…ˆéªŒï¼ˆPnPDPï¼‰å·²æˆä¸ºè§£å†³åé—®é¢˜çš„ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒæ¢å¤ä¸Šï¼Œè¿™äº›ç®—æ³•åœ¨ç§‘å­¦åé—®é¢˜ä¸­çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textsc{InverseBench}æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨äº”ä¸ªä¸åŒçš„ç§‘å­¦åé—®é¢˜ä¸Šè¯„ä¼°æ‰©æ•£æ¨¡å‹ã€‚è¿™äº›é—®é¢˜å‘ˆç°å‡ºç‹¬ç‰¹çš„ç»“æ„æŒ‘æˆ˜ï¼Œä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼Œæºäºå…³é”®ç§‘å­¦åº”ç”¨ï¼Œå¦‚å…‰å­¦å±‚ææˆåƒã€åŒ»å­¦æˆåƒã€é»‘æ´æˆåƒã€åœ°éœ‡å­¦å’Œæµä½“åŠ¨åŠ›å­¦ã€‚é€šè¿‡\textsc{InverseBench}ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨å³æ’å³ç”¨æ‰©æ•£å…ˆéªŒçš„14ç§åé—®é¢˜ç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¸å¼ºå¤§ã€ç‰¹å®šé¢†åŸŸçš„åŸºå‡†çº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œä¸ºç°æœ‰ç®—æ³•çš„ä¼˜ç¼ºç‚¹æä¾›äº†å®è´µçš„æ–°è§è§£ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/%E5%85%AC%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://devzhk.github.io/InverseBench/å…¬å¼€æºä»£ç åº“ã€æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11043v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£å…ˆéªŒçš„â€œå³æ’å³ç”¨â€ï¼ˆPnPDPï¼‰ä¸ºæ±‚è§£åé—®é¢˜æä¾›äº†ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒæ¢å¤é¢†åŸŸï¼Œè¿™äº›ç®—æ³•åœ¨ç§‘å­¦åé—®é¢˜ä¸Šçš„è¡¨ç°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†\textsc{InverseBench}æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨äº”ä¸ªä¸åŒçš„ç§‘å­¦åé—®é¢˜ä¸Šè¯„ä¼°æ‰©æ•£æ¨¡å‹çš„è¡¨ç°ã€‚è¿™äº›é—®é¢˜å…·æœ‰ç‹¬ç‰¹çš„ç»“æ„æŒ‘æˆ˜ï¼Œä¸åŒäºç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¿æ³›åº”ç”¨äºå…³é”®ç§‘å­¦åº”ç”¨ï¼Œå¦‚å…‰å­¦å±‚ææˆåƒã€åŒ»å­¦æˆåƒã€é»‘æ´æˆåƒã€åœ°éœ‡å­¦å’Œæµä½“åŠ¨åŠ›å­¦ã€‚é€šè¿‡ä½¿ç”¨\textsc{InverseBench}ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨æ‰©æ•£å…ˆéªŒçš„14ç§åé—®é¢˜ç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¸å¼ºå¤§ä¸“å±çš„åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œä¸ºç°æœ‰ç®—æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹æä¾›äº†æœ‰ä»·å€¼çš„æ–°è§è§£ã€‚ä¸ºä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæˆ‘ä»¬åœ¨ <a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/">https://devzhk.github.io/InverseBench/</a> å¼€æºäº†ä»£ç åº“ã€æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å…ˆéªŒçš„â€œå³æ’å³ç”¨â€ï¼ˆPnPDPï¼‰ä¸ºåé—®é¢˜ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨è‡ªç„¶å›¾åƒæ¢å¤ï¼Œç§‘å­¦åé—®é¢˜é¢†åŸŸå°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>\textsc{InverseBench}æ¡†æ¶ç”¨äºè¯„ä¼°æ‰©æ•£æ¨¡å‹åœ¨äº”ä¸ªç§‘å­¦åé—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ç§‘å­¦åé—®é¢˜å…·æœ‰ç‹¬ç‰¹çš„ç»“æ„æŒ‘æˆ˜ï¼ŒåŒºåˆ«äºç°æœ‰çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>\textsc{InverseBench}æ¶µç›–äº†å…³é”®ç§‘å­¦åº”ç”¨ï¼Œå¦‚å…‰å­¦å±‚ææˆåƒã€åŒ»å­¦æˆåƒç­‰ã€‚</li>
<li>é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œå¯¹ä½¿ç”¨æ‰©æ•£å…ˆéªŒçš„ç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†ç°æœ‰ç®—æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CE-SDWV-Effective-and-Efficient-Concept-Erasure-for-Text-to-Image-Diffusion-Models-via-a-Semantic-Driven-Word-Vocabulary"><a href="#CE-SDWV-Effective-and-Efficient-Concept-Erasure-for-Text-to-Image-Diffusion-Models-via-a-Semantic-Driven-Word-Vocabulary" class="headerlink" title="CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image   Diffusion Models via a Semantic-Driven Word Vocabulary"></a>CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image   Diffusion Models via a Semantic-Driven Word Vocabulary</h2><p><strong>Authors:Jiahang Tu, Qian Feng, Jiahua Dong, Hanbin Zhao, Chao Zhang, Nicu Sebe, Hui Qian</strong></p>
<p>Large-scale text-to-image (T2I) diffusion models have achieved remarkable generative performance about various concepts. With the limitation of privacy and safety in practice, the generative capability concerning NSFW (Not Safe For Work) concepts is undesirable, e.g., producing sexually explicit photos, and licensed images. The concept erasure task for T2I diffusion models has attracted considerable attention and requires an effective and efficient method. To achieve this goal, we propose a CE-SDWV framework, which removes the target concepts (e.g., NSFW concepts) of T2I diffusion models in the text semantic space by only adjusting the text condition tokens and does not need to re-train the original T2I diffusion modelâ€™s weights. Specifically, our framework first builds a target concept-related word vocabulary to enhance the representation of the target concepts within the text semantic space, and then utilizes an adaptive semantic component suppression strategy to ablate the target concept-related semantic information in the text condition tokens. To further adapt the above text condition tokens to the original image semantic space, we propose an end-to-end gradient-orthogonal token optimization strategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate the effectiveness and efficiency of our method. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TtuHamg/CE-SDWV">https://github.com/TtuHamg/CE-SDWV</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨å„ç§æ¦‚å¿µä¸Šå–å¾—äº†æ˜¾è‘—çš„ç”Ÿæˆæ€§èƒ½ã€‚ä½†åœ¨å®è·µä¸­ï¼Œç”±äºéšç§å’Œå®‰å…¨æ€§çš„é™åˆ¶ï¼Œå…³äºNSFWï¼ˆä¸é€‚åˆå·¥ä½œï¼‰æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›å¹¶ä¸ç†æƒ³ï¼Œä¾‹å¦‚ç”Ÿæˆè‰²æƒ…ç…§ç‰‡å’Œè®¸å¯å›¾åƒã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ï¼Œéœ€è¦ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†CE-SDWVæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨æ–‡æœ¬è¯­ä¹‰ç©ºé—´ä¸­ä»…è°ƒæ•´æ–‡æœ¬æ¡ä»¶ä»¤ç‰Œæ¥æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µï¼ˆä¾‹å¦‚NSFWæ¦‚å¿µï¼‰ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒåŸå§‹T2Iæ‰©æ•£æ¨¡å‹çš„æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆæ„å»ºä¸€ä¸ªä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„è¯æ±‡è¡¨ï¼Œä»¥å¢å¼ºç›®æ ‡æ¦‚å¿µåœ¨æ–‡æœ¬è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œç„¶åé‡‡ç”¨è‡ªé€‚åº”è¯­ä¹‰æˆåˆ†æŠ‘åˆ¶ç­–ç•¥ï¼Œæ¶ˆé™¤ç›®æ ‡æ¦‚å¿µç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯åœ¨æ–‡æœ¬æ¡ä»¶ä»¤ç‰Œä¸­ã€‚ä¸ºäº†è¿›ä¸€æ­¥å°†ä¸Šè¿°æ–‡æœ¬æ¡ä»¶ä»¤ç‰Œé€‚åº”åˆ°åŸå§‹å›¾åƒè¯­ä¹‰ç©ºé—´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¢¯åº¦æ­£äº¤ä»¤ç‰Œä¼˜åŒ–ç­–ç•¥ã€‚åœ¨I2På’ŒUnlearnCanvasåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TtuHamg/CE-SDWV%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TtuHamg/CE-SDWVæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15562v2">PDF</a> 25 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠä¸å®‰å…¨å’Œä¸é€‚å®œå·¥ä½œåœºåˆï¼ˆNSFWï¼‰æ¦‚å¿µæ—¶çš„éšç§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªCE-SDWVæ¡†æ¶ï¼Œé€šè¿‡åœ¨æ–‡æœ¬è¯­ä¹‰ç©ºé—´ä¸­è°ƒæ•´æ–‡æœ¬æ¡ä»¶ä»¤ç‰Œæ¥æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸå§‹T2Iæ‰©æ•£æ¨¡å‹çš„æƒé‡ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç›®æ ‡æ¦‚å¿µç›¸å…³è¯æ±‡è¡¨å’Œé‡‡ç”¨è‡ªé€‚åº”è¯­ä¹‰ç»„ä»¶æŠ‘åˆ¶ç­–ç•¥æ¥æ¶ˆé™¤ä¸NSFWç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¢¯åº¦æ­£äº¤ä»¤ç‰Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥é€‚åº”åŸå§‹å›¾åƒè¯­ä¹‰ç©ºé—´ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•æœ‰æ•ˆä¸”é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå„ç§æ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨å®è·µä¸­å­˜åœ¨éšç§å’Œå®‰å…¨é™åˆ¶é—®é¢˜ï¼Œä¾‹å¦‚ç”Ÿæˆä¸é€‚å®œå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„æ¦‚å¿µï¼Œå¦‚æ€§æš—ç¤ºç…§ç‰‡å’Œå—ç‰ˆæƒä¿æŠ¤çš„å›¾ç‰‡ã€‚</li>
<li>CE-SDWVæ¡†æ¶æ—¨åœ¨æ¶ˆé™¤T2Iæ‰©æ•£æ¨¡å‹ä¸­çš„ç›®æ ‡æ¦‚å¿µï¼Œå¦‚NSFWæ¦‚å¿µï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹æƒé‡ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç›®æ ‡æ¦‚å¿µç›¸å…³è¯æ±‡è¡¨å’Œè‡ªé€‚åº”è¯­ä¹‰ç»„ä»¶æŠ‘åˆ¶ç­–ç•¥æ¥æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µçš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨ç«¯åˆ°ç«¯çš„æ¢¯åº¦æ­£äº¤ä»¤ç‰Œä¼˜åŒ–ç­–ç•¥ï¼Œä»¥é€‚åº”åŸå§‹å›¾åƒè¯­ä¹‰ç©ºé—´ã€‚</li>
<li>åœ¨I2På’ŒUnlearnCanvasåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unpicking-Data-at-the-Seams-Understanding-Disentanglement-in-VAEs"><a href="#Unpicking-Data-at-the-Seams-Understanding-Disentanglement-in-VAEs" class="headerlink" title="Unpicking Data at the Seams: Understanding Disentanglement in VAEs"></a>Unpicking Data at the Seams: Understanding Disentanglement in VAEs</h2><p><strong>Authors:Carl Allen</strong></p>
<p>A generative latent variable model is said to be disentangled when varying a single latent co-ordinate changes a single aspect of samples generated, e.g. object position or facial expression in an image. Related phenomena are seen in several generative paradigms, including state-of-the-art diffusion models, but disentanglement is most notably observed in Variational Autoencoders (VAEs), where oft-used diagonal posterior covariances are argued to be the cause. We make this picture precise. From a known exact link between optimal Gaussian posteriors and decoder derivatives, we show how diagonal posteriors â€œlockâ€ a decoderâ€™s local axes so that density over the data manifold factorises along independent one-dimensional seams that map to axis-aligned directions in latent space. This gives a clear definition of disentanglement, explains why it emerges in VAEs and shows that, under stated assumptions, ground truth factors are identifiable even with a symmetric prior. </p>
<blockquote>
<p>å½“ä¸€ä¸ªç”Ÿæˆæ½œåœ¨å˜é‡æ¨¡å‹ä¸­çš„å•ä¸€æ½œåœ¨åæ ‡å‘ç”Ÿå˜åŒ–æ—¶ï¼Œåªä¼šæ”¹å˜ç”Ÿæˆæ ·æœ¬çš„å•ä¸€æ–¹é¢ï¼Œä¾‹å¦‚å›¾åƒä¸­çš„å¯¹è±¡ä½ç½®æˆ–é¢éƒ¨è¡¨æƒ…ï¼Œæ­¤æ—¶è¯¥æ¨¡å‹è¢«è®¤ä¸ºæ˜¯è§£çº ç¼ çš„ã€‚è¿™ç§ç°è±¡åœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸­éƒ½å¯ä»¥çœ‹åˆ°ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä¸­ï¼Œè§£çº ç¼ ç°è±¡æœ€ä¸ºæ˜æ˜¾ï¼Œäººä»¬è®¤ä¸ºå¸¸ç”¨çš„å¯¹è§’åéªŒåæ–¹å·®æ˜¯é€ æˆè¿™ä¸€ç°è±¡çš„åŸå› ã€‚æˆ‘ä»¬ä½¿è¿™ä¸€è§‚ç‚¹ç²¾ç¡®åŒ–ã€‚ä»å·²çŸ¥çš„æœ€ä¼˜é«˜æ–¯åéªŒå’Œè§£ç å™¨å¯¼æ•°ä¹‹é—´çš„å…³è”å‡ºå‘ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å¯¹è§’åéªŒä¼šâ€œé”å®šâ€è§£ç å™¨çš„å±€éƒ¨è½´ï¼Œä½¿å¾—æ•°æ®æµå½¢ä¸Šçš„å¯†åº¦èƒ½å¤Ÿæ²¿ç€ç‹¬ç«‹çš„ä¸€ç»´æ¥ç¼è¿›è¡Œåˆ†è§£ï¼Œè¿™äº›æ¥ç¼æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„è½´å¯¹é½æ–¹å‘ã€‚è¿™ä¸ºè§£çº ç¼ æä¾›äº†æ˜ç¡®çš„å®šä¹‰ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆå®ƒä¼šåœ¨VAEsä¸­å‡ºç°ï¼Œå¹¶è¡¨æ˜åœ¨å‡è®¾æ¡ä»¶ä¸‹ï¼Œå³ä½¿ä½¿ç”¨å¯¹ç§°å…ˆéªŒï¼ŒçœŸå®å› ç´ ä¹Ÿæ˜¯å¯ä»¥è¯†åˆ«çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22559v6">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ½œåœ¨å˜é‡æ¨¡å‹çš„è§£è€¦ç°è±¡æ˜¯æŒ‡æ”¹å˜å•ä¸€çš„æ½œåœ¨åæ ‡ä¼šå½±å“ç”Ÿæˆæ ·æœ¬çš„å•ä¸€ç‰¹å¾ï¼Œå¦‚å›¾åƒçš„ç‰©ä½“ä½ç½®æˆ–é¢éƒ¨è¡¨æƒ…ã€‚è¿™ç§ç°è±¡åœ¨åŒ…æ‹¬æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹åœ¨å†…çš„å¤šç§ç”ŸæˆèŒƒå¼ä¸­éƒ½å¯ä»¥çœ‹åˆ°ï¼Œä½†åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä¸­å°¤å…¶æ˜æ˜¾ã€‚æˆ‘ä»¬é€šè¿‡ç²¾ç¡®æè¿°æœ€ä¼˜é«˜æ–¯åéªŒä¸è§£ç å™¨å¯¼æ•°ä¹‹é—´çš„å·²çŸ¥è”ç³»ï¼Œå±•ç¤ºäº†å¯¹è§’åéªŒå¦‚ä½•â€œé”å®šâ€è§£ç å™¨çš„å±€éƒ¨è½´ï¼Œä½¿æ•°æ®æµå½¢ä¸Šçš„å¯†åº¦æ²¿ç‹¬ç«‹çš„ä¸€ç»´æ¥ç¼åˆ†è§£ï¼Œè¿™äº›æ¥ç¼æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„è½´å¯¹é½æ–¹å‘ã€‚è¿™ä¸ºè§£è€¦ç°è±¡æä¾›äº†æ˜ç¡®çš„å®šä¹‰ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆå®ƒä¼šåœ¨VAEsä¸­å‡ºç°ï¼Œå¹¶è¡¨æ˜åœ¨æ—¢å®šçš„å‡è®¾ä¸‹ï¼Œå³ä½¿ä½¿ç”¨å¯¹ç§°å…ˆéªŒï¼ŒçœŸå®å› ç´ ä¹Ÿæ˜¯å¯è¯†åˆ«çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ½œåœ¨å˜é‡æ¨¡å‹çš„è§£è€¦ç°è±¡ï¼šæ”¹å˜å•ä¸€æ½œåœ¨åæ ‡ä¼šå½±å“ç”Ÿæˆæ ·æœ¬çš„å•ä¸€ç‰¹å¾ã€‚</li>
<li>è§£è€¦ç°è±¡åœ¨å¤šç§ç”ŸæˆèŒƒå¼ä¸­å¯è§ï¼Œå°¤å…¶åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä¸­ã€‚</li>
<li>å¯¹è§’åéªŒåœ¨è§£è€¦ç°è±¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå®ƒä»¬â€œé”å®šâ€è§£ç å™¨çš„å±€éƒ¨è½´ã€‚</li>
<li>æ•°æ®æµå½¢ä¸Šçš„å¯†åº¦æ²¿ç‹¬ç«‹çš„ä¸€ç»´æ¥ç¼åˆ†è§£ï¼Œè¿™äº›æ¥ç¼æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´çš„è½´å¯¹é½æ–¹å‘ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å…³äºè§£è€¦ç°è±¡çš„æ˜ç¡®å®šä¹‰ã€‚</li>
<li>è®ºæ–‡è§£é‡Šäº†ä¸ºä»€ä¹ˆè§£è€¦ç°è±¡åœ¨VAEsä¸­å‡ºç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_ç‰™é½¿ä¿®å¤/2509.12069v2/page_2_0.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  U-Mamba2 Scaling State Space Models for Dental Anatomy Segmentation in   CBCT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_NeRF/2509.25191v1/page_5_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  VGGT-X When VGGT Meets Dense Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
