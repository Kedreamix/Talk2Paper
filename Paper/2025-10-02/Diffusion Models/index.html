<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-02  Query-Kontext An Unified Multimodal Model for Image Generation and   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-02-更新"><a href="#2025-10-02-更新" class="headerlink" title="2025-10-02 更新"></a>2025-10-02 更新</h1><h2 id="Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing"><a href="#Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing" class="headerlink" title="Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing"></a>Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing</h2><p><strong>Authors:Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</strong></p>
<p>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal &#96;&#96;kontext’’ composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model’s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM’s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases. </p>
<blockquote>
<p>统一多模态模型（UMMs）在文本到图像生成（T2I）和编辑（TI2I）方面表现出了显著的性能，无论是作为集成了强大视觉语言模型（VLM）和基于扩散的生成器的统一框架，还是作为早期理解和生成模式融合的天真统一多模态模型。我们认为，在当前统一框架中，包含指令理解、接地和图像引用在内的多模态生成推理的关键能力，在身份保留和忠实重建方面与高清综合息息相关。在此工作中，我们引入了Query-Kontext这一新方法，它通过由语义线索和来自多模态输入的粗粒度图像条件组成的多模态&#96;&#96;kontext’’，在视觉语言模型（VLM）和扩散模型之间建立了桥梁。这种设计将复杂的多模态生成推理能力委托给强大的VLM，同时保留扩散模型进行高质量视觉合成的作用。为了实现这一点，我们提出了一种分三阶段的渐进训练策略。首先，我们通过多模态kontext令牌将VLM连接到轻量级扩散头，以释放VLM的生成推理能力。其次，我们将此头扩展到大型预训练扩散模型，以提高视觉细节和逼真度。最后，我们引入低级别图像编码器，以提高图像保真度并在下游任务上进行指令调整。此外，我们建立了一个综合数据管道，整合真实、合成和开源数据集，涵盖多样化的多模态参考到图像场景，包括图像生成、指令驱动编辑、定制生成和多主题组合。实验表明，我们的方法与强大的统一基线相匹配，甚至在某些情况下超过了任务特定的最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26641v1">PDF</a> 23 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Unified Multimodal Models（UMMs）在文本到图像生成（T2I）和编辑（TI2I）任务中的卓越性能。通过引入Query-Kontext新型方法，结合语义线索和粗粒度图像条件编码的多模态上下文，实现了对统一框架中多模态生成推理能力的强化。该方法通过分阶段训练策略，将多模态生成推理能力委托给强大的视觉语言模型（VLM），保留扩散模型用于高质量视觉合成。实验表明，该方法匹配了强大的统一基线，甚至在部分情况下超越了任务特定的最先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMMs在文本到图像生成和编辑任务中表现卓越。</li>
<li>Query-Kontext方法通过多模态上下文桥接VLM和扩散模型。</li>
<li>语义线索和粗粒度图像条件编码提高了模型的性能。</li>
<li>分阶段训练策略增强了模型的生成推理能力和视觉合成质量。</li>
<li>方法匹配了强大的统一基线，并在部分情况下超越了任务特定方法。</li>
<li>模型能够处理多样化的多模态参考到图像的场景，包括图像生成、指令驱动编辑、自定义生成和多主体组合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26641">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26641v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"><a href="#MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation" class="headerlink" title="MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"></a>MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</h2><p><strong>Authors:Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang</strong></p>
<p>Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics. </p>
<blockquote>
<p>随着扩散模型的发展，图像到视频的生成已经取得了显著的进步，但生成具有真实感的运动的视频仍然极具挑战性。这一难度源于准确建模运动的复杂性，其中包括捕捉物理约束、物体交互和特定领域的动态，这些不容易在多种场景中通用化。为了解决这一问题，我们提出了MotionRAG，这是一个增强型检索框架，通过上下文感知运动适应（CAMA）从相关参考视频中适应运动先验知识，提高运动真实性。主要技术创新包括：（i）基于检索的管道使用视频编码器和专用重采样器提取高级运动特征，以蒸馏语义运动表示；（ii）通过因果变压器架构实现运动适应的上下文内学习方法；（iii）基于注意力的运动注入适配器无缝集成转移的运动特征到预训练的视频扩散模型中。大量实验表明，我们的方法在多个领域和各种基础模型上都取得了显著的改进，推理过程中的计算开销微乎其微。此外，我们的模块化设计通过简单地更新检索数据库而无需重新训练任何组件，实现了零镜头跨域推广。这项研究通过实现有效的运动先验检索和转移，提高了视频生成系统的核心能力，促进了真实运动动态的合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26391v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>随着扩散模型的发展，图像到视频的生成取得了显著的进步，但生成具有真实感的视频仍然是一个巨大的挑战。为解决这一问题，我们提出了MotionRAG框架，通过上下文感知运动适应（CAMA）技术，利用相关参考视频的运动先验信息提高运动真实性。包括基于检索的管道、上下文内学习方法和注意力驱动的注入适配器等技术创新。实验证明，该方法在不同领域和基准模型上取得了显著改进，且推理时的计算开销很小。此外，我们的模块化设计可通过简单更新检索数据库实现零样本跨域推广。该研究提高了视频生成系统的核心能力，实现了有效的运动先验检索和转移，促进了真实运动动态的合成。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型在图像到视频生成方面取得显著进步，但生成真实视频仍存在挑战。</li>
<li>MotionRAG框架通过适应运动先验提高视频运动真实性。</li>
<li>框架包括基于检索的管道、上下文内学习方法和注意力驱动的注入适配器等技术创新。</li>
<li>实验证明该方法在不同领域和模型上表现优异，推理计算开销小。</li>
<li>模块化设计可实现零样本跨域推广，更新检索数据库即可。</li>
<li>该研究提高了视频生成系统的核心能力，实现了运动先验的有效检索和转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26391v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Data-to-Energy-Stochastic-Dynamics"><a href="#Data-to-Energy-Stochastic-Dynamics" class="headerlink" title="Data-to-Energy Stochastic Dynamics"></a>Data-to-Energy Stochastic Dynamics</h2><p><strong>Authors:Kirill Tamogashev, Nikolay Malkin</strong></p>
<p>The Schr&quot;odinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schr&quot;odinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schr&quot;odinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: <a target="_blank" rel="noopener" href="https://github.com/mmacosha/d2e-stochastic-dynamics">https://github.com/mmacosha/d2e-stochastic-dynamics</a> </p>
<blockquote>
<p>薛定谔桥问题涉及找到一个随机动力系统，该动力系统连接两个边缘分布，并最小化特定的运输成本。这个问题代表了最优传输在随机情况下的推广，由于其与扩散模型、流量匹配以及与自然科学应用的联系而备受关注。然而，所有现有的算法都只能在两个分布都有样本的情况下推断出这种动态。在本文中，我们提出了一种当分布之一（或两者）由未归一化的密度给出且无法使用数据样本时建模薛定谔桥的首个通用方法。我们的算法依赖于将数据无关的迭代比例拟合（IPF）程序推广到无数据的情况，这是受近期离策略强化学习在扩散采样器训练方面的最新进展的启发。我们在合成问题上验证了所提出的数据到能量IPF的有效性，发现它可以成功地在多峰分布之间学习传输。作为对假设固定时间离散化方案的动态进行强化学习公式化的次要结果，我们发现通过学习动力学的扩散系数，现有的数据到数据薛定谔桥算法可以大大改进。最后，我们将新开发的算法应用于生成模型潜在空间中的后验分布采样问题，从而创建了一种无数据图像到图像的转换方法。代码：<a target="_blank" rel="noopener" href="https://github.com/mmacosha/d2e-stochastic-dynamics">https://github.com/mmacosha/d2e-stochastic-dynamics</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文解决了Schrödinger桥问题在样本不可用的情况下的建模方法。当分布由未归一化的密度给出时，我们提出了一种基于迭代比例拟合（IPF）的算法，适用于数据不可用的场合。此方法不仅能在合成问题上成功学习多模态分布之间的传输，而且能改善现有的数据到数据的Schrödinger桥算法，通过学习动力学的扩散系数来提高性能。此外，将新开发的算法应用于生成模型的潜在空间中的后验分布采样问题，从而创建了一种数据无关的图像到图像的转换方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Schrödinger桥问题旨在寻找连接两个边缘分布的随机动力系统，并最小化特定的运输成本。它是最佳运输在随机情况下的推广，与扩散模型和流动匹配有联系，在自然科学中有应用。</li>
<li>现有算法仅在可以从两个分布中获取样本的情况下推断动态。这篇论文解决了当分布由未归一化的密度给出且无法获取数据样本时的建模方法。</li>
<li>论文提出的算法基于迭代比例拟合（IPF）的推广，适用于数据不可用的情况，受到强化学习的启发。它在合成问题上表现出色，能成功学习多模态分布之间的传输。</li>
<li>强化学习公式假定动力学的固定时间离散方案，发现可以通过学习动力学的扩散系数来改进现有的数据到数据的Schrödinger桥算法。</li>
<li>新开发的算法应用于生成模型的潜在空间中的后验分布采样问题，提供了一种数据无关的图像到图像的转换方法。</li>
<li>该研究提供了关于如何解决Schrödinger桥问题的新见解，尤其是在样本不可用的情境下。它结合了不同领域的知识（如最佳运输、扩散模型、强化学习等），为处理复杂的随机动态系统提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26364">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26364v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26364v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EchoGen-Generating-Visual-Echoes-in-Any-Scene-via-Feed-Forward-Subject-Driven-Auto-Regressive-Model"><a href="#EchoGen-Generating-Visual-Echoes-in-Any-Scene-via-Feed-Forward-Subject-Driven-Auto-Regressive-Model" class="headerlink" title="EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward   Subject-Driven Auto-Regressive Model"></a>EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward   Subject-Driven Auto-Regressive Model</h2><p><strong>Authors:Ruixiao Dong, Zhendong Wang, Keli Liu, Li Li, Ying Chen, Kai Li, Daowen Li, Houqiang Li</strong></p>
<p>Subject-driven generation is a critical task in creative AI; yet current state-of-the-art methods present a stark trade-off. They either rely on computationally expensive, per-subject fine-tuning, sacrificing efficiency and zero-shot capability, or employ feed-forward architectures built on diffusion models, which are inherently plagued by slow inference speeds. Visual Auto-Regressive (VAR) models are renowned for their rapid sampling speeds and strong generative quality, making them an ideal yet underexplored foundation for resolving this tension. To bridge this gap, we introduce EchoGen, a pioneering framework that empowers VAR models with subject-driven generation capabilities. The core design of EchoGen is an effective dual-path injection strategy that disentangles a subject’s high-level semantic identity from its low-level fine-grained details, enabling enhanced controllability and fidelity. We employ a semantic encoder to extract the subject’s abstract identity, which is injected through decoupled cross-attention to guide the overall composition. Concurrently, a content encoder captures intricate visual details, which are integrated via a multi-modal attention mechanism to ensure high-fidelity texture and structural preservation. To the best of our knowledge, EchoGen is the first feed-forward subject-driven framework built upon VAR models. Both quantitative and qualitative results substantiate our design, demonstrating that EchoGen achieves subject fidelity and image quality comparable to state-of-the-art diffusion-based methods with significantly lower sampling latency. Code and models will be released soon. </p>
<blockquote>
<p>主题驱动生成是创造性AI中的关键任务；然而，当前最先进的方法呈现出明显的权衡。它们要么依赖于计算量大、针对每个主题的微调，牺牲了效率和零样本能力，要么采用基于扩散模型的前馈架构，这固有地导致推理速度较慢。视觉自回归（VAR）模型以其快速采样速度和强大的生成质量而闻名，是解决这个问题的理想但尚未被充分探索的基础。为了弥补这一差距，我们引入了EchoGen，这是一个赋能VAR模型主题驱动生成能力的开创性框架。EchoGen的核心设计是一个有效的双路径注入策略，该策略将一个主题的高级语义身份与其低级精细细节分开，从而增强了可控性和保真度。我们采用语义编码器提取主题的抽象身份，通过解耦交叉注意力注入来引导整体构图。同时，内容编码器捕捉复杂的视觉细节，通过多模态注意力机制集成，以确保高保真纹理和结构保留。据我们所知，EchoGen是基于VAR模型的第一个前馈主题驱动框架。定量和定性结果都证明了我们设计的合理性，证明EchoGen在主题保真度和图像质量方面达到了与最先进的扩散模型相当的水平，同时采样延迟更低。代码和模型将很快发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26127v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了EchoGen框架，该框架将视觉自回归（VAR）模型与主题驱动生成相结合，实现了快速采样和高质量生成。EchoGen的核心设计是一种有效的双路径注入策略，能够分离主题的高级语义身份和低级细节，提高了可控性和保真度。它采用语义编码器提取主题抽象身份，通过解耦交叉注意力引导整体构图。同时，内容编码器捕捉复杂视觉细节，通过多模态注意力机制确保高保真纹理和结构保留。EchoGen是首个基于VAR模型的向前传播主题驱动框架，实现了与基于扩散的方法相当的主题保真度和图像质量，同时采样延迟更低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EchoGen框架结合了视觉自回归（VAR）模型和主题驱动生成，实现了快速采样和高质量生成。</li>
<li>EchoGen采用双路径注入策略，分离主题的高级语义和低级细节，提高可控性和保真度。</li>
<li>语义编码器提取主题抽象身份，通过解耦交叉注意力引导整体构图。</li>
<li>内容编码器负责捕捉复杂视觉细节，确保高保真纹理和结构保留。</li>
<li>EchoGen是首个基于VAR模型的向前传播主题驱动框架。</li>
<li>EchoGen的主题保真度和图像质量与最先进的扩散方法相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26127v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EVODiff-Entropy-aware-Variance-Optimized-Diffusion-Inference"><a href="#EVODiff-Entropy-aware-Variance-Optimized-Diffusion-Inference" class="headerlink" title="EVODiff: Entropy-aware Variance Optimized Diffusion Inference"></a>EVODiff: Entropy-aware Variance Optimized Diffusion Inference</h2><p><strong>Authors:Shigui Li, Wei Chen, Delu Zeng</strong></p>
<p>Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ShiguiLi/EVODiff">https://github.com/ShiguiLi/EVODiff</a>. </p>
<blockquote>
<p>扩散模型（DMs）在图像生成方面表现出色，但存在推理速度慢和训练推理差异等问题。虽然像DPM-Solver这样的基于梯度的求解器可以加速去噪推理，但它们在信息传输效率方面缺乏理论基础。在这项工作中，我们从信息理论的角度介绍了扩散模型推理过程，发现成功的去噪从根本上减少了反向转换中的条件熵。这一原则让我们深入理解了推理过程：（1）数据预测参数化表现优于噪声对照；（2）优化条件方差提供了一种无参考的方式来最小化转换和重建误差。基于这些见解，我们提出了一种用于扩散模型生成过程的熵感知方差优化方法，称为EVODiff，它通过去噪过程中优化条件熵来系统地减少不确定性。对扩散模型的广泛实验验证了我们见解的有效性，并证明我们的方法显著且持续地优于最先进的基于梯度的求解器。例如，与DPM-Solver++相比，EVODiff在CIFAR-10上的重建错误降低了45.5%（在10次函数评估（NFE）中，FID从5.10提高到2.78），在ImageNet-256上高质量样本的NFE成本降低了25%（从20次降至15次NFE），同时改进了文本到图像的生成并减少了伪影。代码可在<a target="_blank" rel="noopener" href="https://github.com/ShiguiLi/EVODiff%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ShiguiLi/EVODiff上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26096v1">PDF</a> NeurIPS 2025, 40 pages, 14 figures</p>
<p><strong>摘要</strong><br>    扩散模型在图像生成方面表现出色，但存在推理速度慢和训练推理差异等问题。虽然基于梯度的求解器如DPM-Solver加速了去噪推理，但它们在信息传输效率方面缺乏理论基础。本文介绍了从信息理论角度对扩散模型推理过程的研究，揭示了成功的去噪在根本上减少了反向转换中的条件熵。这一原理为我们深入洞察推理过程提供了关键见解：（1）数据预测参数化优于噪声参数化；（2）优化条件方差提供了一种无参考的方法来最小化转换和重建误差。基于此，我们提出了一个用于扩散模型生成过程的熵感知方差优化方法，称为EVODiff，它通过去噪过程中优化条件熵来系统地降低不确定性。在扩散模型上的广泛实验验证了我们的见解，并证明我们的方法显著且一致地优于最先进的基于梯度的求解器。例如，与DPM-Solver++相比，EVODiff在CIFAR-10上的重建误差降低了45.5%（在10次函数评估（NFE）时，FID从5.10降低到2.78），在ImageNet-256上高质量样本的NFE成本降低了25%（从20次降至15次NFE），同时改进了文本到图像的生成并减少了伪影。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在图像生成方面的卓越性能与其较慢的推理速度和训练推理差异有关。</li>
<li>基于梯度的求解器如DPM-Solver虽然在加速去噪推理方面有效，但在信息传输效率方面缺乏理论基础。</li>
<li>成功的去噪能够减少反向转换中的条件熵，这是理解扩散模型推理过程的关键。</li>
<li>数据预测参数化比噪声参数化更有效。</li>
<li>优化条件方差提供了一种无参考的方法，可以最小化转换和重建误差。</li>
<li>提出的EVODiff方法通过优化去噪过程中的条件熵，系统地降低了不确定性，显著优于先进的基于梯度的求解器。</li>
<li>EVODiff在多个实验上实现了显著的性能提升，如降低重建误差、减少NFE成本，改进文本到图像的生成并减少伪影。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.26096v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Free-Reward-Guided-Image-Editing-via-Trajectory-Optimal-Control"><a href="#Training-Free-Reward-Guided-Image-Editing-via-Trajectory-Optimal-Control" class="headerlink" title="Training-Free Reward-Guided Image Editing via Trajectory Optimal Control"></a>Training-Free Reward-Guided Image Editing via Trajectory Optimal Control</h2><p><strong>Authors:Jinho Chang, Jaemin Kim, Jong Chul Ye</strong></p>
<p>Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking. </p>
<blockquote>
<p>近期扩散模型和流匹配模型的进展在高保真图像合成中展示了显著的能力。一个突出的研究方向是奖励引导指导，它在推理过程中引导生成过程以符合特定目标。然而，将这一奖励引导方法应用于图像编辑任务——需要在保留源图像语义内容的同时提高目标奖励——却很少被探索。在这项工作中，我们介绍了一种无需训练、奖励引导的图像编辑新框架。我们将编辑过程制定为一个轨迹最优控制问题，其中扩散模型的逆向过程被视为一条可控轨迹，起源于源图像，并且伴随状态被迭代更新以引导编辑过程。通过在不同编辑任务上的广泛实验，我们证明我们的方法显著优于现有的基于反演的无需训练指导基线，在奖励最大化和对源图像的保真度之间取得了优越平衡，且无需奖励黑客行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25845v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的奖励引导图像编辑的新框架。该框架将编辑过程视为轨迹最优控制问题，将扩散模型的逆向过程视为从源图像开始的可控轨迹，并通过迭代更新伴随状态来引导编辑过程。实验表明，该方法在不同的编辑任务上显著优于基于反演的无需训练指导的基线，在奖励最大化和保持源图像保真度之间取得了优越平衡，且无需奖励黑客手段。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励引导方法在扩散模型中显示出显著的图像合成能力。</li>
<li>本文首次探索了将奖励引导方法应用于图像编辑任务。</li>
<li>提出了一种无需训练的奖励引导图像编辑新框架。</li>
<li>将编辑过程视为轨迹最优控制问题，利用扩散模型的逆向过程作为可控轨迹。</li>
<li>通过迭代更新伴随状态来引导编辑过程，实现源图像语义内容的保留和目标奖励的增强。</li>
<li>实验证明，该方法在多种编辑任务上显著优于现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25845">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25845v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Editable-Noise-Map-Inversion-Encoding-Target-image-into-Noise-For-High-Fidelity-Image-Manipulation"><a href="#Editable-Noise-Map-Inversion-Encoding-Target-image-into-Noise-For-High-Fidelity-Image-Manipulation" class="headerlink" title="Editable Noise Map Inversion: Encoding Target-image into Noise For   High-Fidelity Image Manipulation"></a>Editable Noise Map Inversion: Encoding Target-image into Noise For   High-Fidelity Image Manipulation</h2><p><strong>Authors:Mingyu Kang, Yong Suk Choi</strong></p>
<p>Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames. </p>
<blockquote>
<p>文本到图像的扩散模型在生成高质量和多样化的图像方面取得了显著的成功。在此基础上，扩散模型在文本引导的图像编辑方面也表现出了卓越的性能。实现有效图像编辑的关键策略是将源图像反转为与目标图像相关的可编辑噪声图。然而，以前的反转方法在面对紧密遵循目标文本提示时面临挑战。这种限制的产生是因为反转的噪声图虽然能够使源图像进行忠实的重建，但限制了所需的编辑灵活性。为了解决这个问题，我们提出了可编辑噪声图反转（ENM反转）这一新型反转技术，寻找最佳噪声图，以确保内容和可编辑性的保留。我们分析了噪声图的属性，以提高其可编辑性。基于这一分析，我们的方法引入了一种可编辑的噪声细化，通过最小化重建和编辑噪声图之间的差异，与所需的编辑保持一致。大量实验表明，在广泛图像编辑任务中，ENM反转在保留和编辑保真度方面均优于现有方法并符合目标提示。我们的方法还可以轻松应用于视频编辑，实现跨帧的时间一致性和内容操作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25776v1">PDF</a> ICML 2025</p>
<p><strong>摘要</strong><br>    文本到图像的扩散模型在生成高质量、多样化的图像方面取得了显著的成功。在此基础上，扩散模型在文本引导的图像编辑中也表现出了卓越的性能。一种有效的图像编辑策略是通过将源图像反转为与目标图像相关的可编辑噪声图来实现。然而，以往的反转方法在面对紧贴目标文本提示的要求时面临挑战。限制产生于反转的噪声图，虽然能够忠实重建源图像，但限制了所需的灵活性编辑。为了解决这个问题，我们提出了可编辑噪声图反转（ENM反转）这一新颖的反转技术，寻找最佳的噪声图，确保内容保留和可编辑性。我们分析了噪声图的属性，以提高其可编辑性。基于这一分析，我们的方法引入了一种可编辑噪声优化，通过最小化重建和编辑噪声图之间的差异，与所需的编辑保持一致。大量实验表明，ENM反转法在图像编辑任务的保留和编辑保真度方面优于现有方法，并且可轻松应用于视频编辑，实现跨帧的时间一致性和内容操作。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在文本引导的图像编辑中表现出卓越性能。</li>
<li>有效的图像编辑策略是通过将源图像反转为与目标图像相关的可编辑噪声图来实现。</li>
<li>以往的图像反转方法在紧贴目标文本提示方面存在挑战。</li>
<li>ENM反转技术旨在寻找最佳噪声图，确保内容保留和编辑灵活性。</li>
<li>通过分析噪声图的属性来提高其可编辑性。</li>
<li>ENM反转法通过最小化重建和编辑噪声图之间的差异，与所需的编辑保持一致。</li>
<li>大量实验证明，ENM反转法在图像和视频的编辑任务中具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25776">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25776v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs"><a href="#Free-Lunch-Alignment-of-Text-to-Image-Diffusion-Models-without-Preference-Image-Pairs" class="headerlink" title="Free Lunch Alignment of Text-to-Image Diffusion Models without   Preference Image Pairs"></a>Free Lunch Alignment of Text-to-Image Diffusion Models without   Preference Image Pairs</h2><p><strong>Authors:Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao</strong></p>
<p>Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables “free-lunch” alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at <a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment">https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment</a>. </p>
<blockquote>
<p>基于扩散的文本到图像（T2I）模型的最新进展在从文本提示生成高质量图像方面取得了显著的成功。然而，确保文本和生成图像之间的准确对齐对于最先进的扩散模型来说仍然是一个巨大的挑战。为了解决这一问题，现有研究采用增强学习结合人类反馈（RLHF）的方法，使T2I输出与人类偏好对齐。然而，这些方法要么直接依赖于配对图像偏好数据，要么需要一个学习的奖励函数，两者都严重依赖于昂贵的高质量人类注释，因此面临可扩展性限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著成功，但文本与生成图像之间的准确对齐仍是挑战。现有研究采用强化学习结合人类反馈（RLHF）来优化对齐。然而，这种方法需要大量高质量的人类标注数据，存在可扩展性限制。本文提出了文本偏好优化（TPO）框架，无需配对图像偏好数据即可实现T2I模型的“免费午餐”对齐。通过训练模型以偏好匹配提示而非不匹配提示来实现对齐，不匹配提示是通过使用大型语言模型扰动原始标题构建的。我们的框架具有通用性，可与现有的基于偏好的算法相结合。定量和定性评估表明，我们的方法始终优于其原始方法，并提高了人类偏好得分和文本到图像的对齐精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到图像生成领域取得了显著进展，但文本与生成图像之间的准确对齐仍是挑战。</li>
<li>现有研究采用强化学习结合人类反馈（RLHF）来提高文本与图像之间的对齐精度，但这种方法需要大量高质量的人类标注数据，存在可扩展性限制。</li>
<li>本文提出了文本偏好优化（TPO）框架，无需配对图像偏好数据即可实现T2I模型的“免费午餐”对齐。</li>
<li>TPO框架通过训练模型以偏好匹配提示来实现对齐，这些匹配提示是通过扰动原始标题构建的。</li>
<li>TPO框架具有通用性，可与现有的基于偏好的算法相结合，提高文本到图像的对齐精度。</li>
<li>定量和定性评估表明，基于TPO的方法在多个基准测试中表现优于传统方法，提高了人类偏好得分和对齐精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25771v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On"><a href="#ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On" class="headerlink" title="ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On"></a>ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On</h2><p><strong>Authors:Junseo Park, Hyeryung Jang</strong></p>
<p>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines. </p>
<blockquote>
<p>虚拟试衣（VITON）旨在生成用户穿戴目标服饰的现实图像，要求精确对齐服饰的试穿区域，并在非试穿区域忠实地保留身份和背景。虽然潜在扩散模型（LDM）已经改进了对齐和细节合成，但保留非试穿区域仍然具有挑战性。一种常见的后处理方法直接用原始内容替换这些区域，但突兀的过渡通常会产生边界伪影。为了克服这一问题，我们将VITON重新表述为一个线性逆问题，并采用轨迹对齐求解器，逐步强制执行测量一致性，减少非试穿区域的突兀变化。然而，现有的求解器在生成过程中仍存在语义漂移问题，导致出现伪影。我们提出了ART-VITON，一种测量指导的扩散框架，确保测量遵循性同时保持无伪影合成。我们的方法整合了基于残差先验的初始化，以减轻训练-推断不匹配问题，以及无伪影测量指导的采样，它结合了数据一致性、频率级别的校正和周期性的标准去噪。在VITON-HD、DressCode和SHHQ-1.0上的实验表明，ART-VITON有效地保留了身份和背景，消除了边界伪影，并且相较于最先进的基线方法，在视觉保真度和稳健性方面都有了一致的提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25749v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>基于潜在扩散模型（LDM）的虚拟试穿（VITON）技术在生成人穿着目标服装的逼真图像方面已取得进展，但在非试穿区域的身份和背景保留方面仍面临挑战。为解决这一问题，本文提出一种测量引导扩散框架ART-VITON，通过结合残差先验初始化和无瑕疵测量引导采样，确保测量一致性并维持合成结果的清晰度。实验证明，ART-VITON在身份和背景保留、消除边界伪影、提高视觉保真度和稳健性方面均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VITON技术旨在生成目标服装的逼真图像，需要精确对齐服装并在非试穿区域保持身份和背景的忠实保留。</li>
<li>LDM在提高对齐和细节合成能力方面表现突出，但保留非试穿区域仍存在挑战。常见的后处理策略存在边界伪影问题。</li>
<li>本文通过改革VITON为线性逆问题并采用轨迹对齐求解器来减少非试穿区域的突兀变化。然而，现有求解器在生成过程中仍会出现语义漂移问题，导致伪影。</li>
<li>提出ART-VITON框架，确保测量一致性同时维持无瑕疵的合成结果。通过结合残差先验初始化和无瑕疵测量引导采样，该框架实现了数据一致性、频率级校正和周期性标准去噪。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25749v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LieHMR-Autoregressive-Human-Mesh-Recovery-with-SO-3-Diffusion"><a href="#LieHMR-Autoregressive-Human-Mesh-Recovery-with-SO-3-Diffusion" class="headerlink" title="LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion"></a>LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion</h2><p><strong>Authors:Donghwan Kim, Tae-Kyun Kim</strong></p>
<p>We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively. </p>
<blockquote>
<p>我们针对从单张RGB图像进行人体网格恢复（HMR）的问题，将其表述为以图像为条件的人体姿态和形状生成。从2D观察恢复3D人体姿态本质上是具有歧义的，大多数现有方法都回归了单一确定性输出。概率方法试图通过生成多个合理输出对歧义进行建模。然而，这些方法在准确性和样本多样性之间往往存在权衡，其单一预测并不具备与最新确定性模型竞争的能力。为了克服这些局限性，我们提出了一种新型方法，该方法对2D观测进行良好对齐的建模。特别是，我们引入了$SO(3)$扩散模型，该模型通过条件丢弃生成作为三维旋转的体态参数分布，该分布无条件地和有选择地取决于图像观测。我们的模型使用变压器学习人体关节的层次结构。不同于将变压器用作去噪模型的做法，时间独立变压器提取关节的潜在向量，并且基于小型多层感知机的去噪模型学习基于潜在向量的关节分布。我们实验性地证明并分析了我们的模型能够准确地预测姿态概率分布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25739v1">PDF</a> 17 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>本文解决从单一RGB图像进行人体网格恢复（HMR）的问题，将其表述为图像条件下的人体姿态和形状生成。针对从2D观测恢复3D人体姿态的固有模糊性，大多数现有方法回归了单一确定性输出。概率方法试图通过生成多个合理输出对模糊性进行建模。然而，这些方法在准确性和样本多样性之间往往存在权衡，其单一预测并不具备与最新确定性模型竞争的能力。为了克服这些限制，本文提出了一种新型方法，对2D观测进行良好对齐的分布建模。特别是引入了$SO(3)$扩散模型，该模型通过条件丢弃，无条件地表示姿态参数的分布，并根据图像观测进行条件化。该模型学习使用变压器的人体关节层次结构，时间独立变压器提取关节的潜在向量，基于小型多层感知机的去噪模型学习基于潜在向量的关节分布。实验证明，该模型能有效地预测准确的姿态概率分布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>解决从单一RGB图像进行人体网格恢复（HMR）的问题，表述为图像条件下的人体姿态和形状生成。</li>
<li>现有方法存在回归单一确定性输出的问题，概率方法试图通过生成多个输出对模糊性进行建模但存在权衡问题。</li>
<li>引入$SO(3)$扩散模型，通过条件丢弃，对姿态参数的分布进行建模。</li>
<li>模型结合了变压器和时间独立变压器技术，提取关节的潜在向量。</li>
<li>基于多层感知机的去噪模型学习潜在向量下的关节分布。</li>
<li>模型能有效预测准确的姿态概率分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25739v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space"><a href="#DC-Gen-Post-Training-Diffusion-Acceleration-with-Deeply-Compressed-Latent-Space" class="headerlink" title="DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space"></a>DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed   Latent Space</h2><p><strong>Authors:Wenkun He, Yuchao Gu, Junyu Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Haocheng Xi, Muyang Li, Ligeng Zhu, Jincheng Yu, Junsong Chen, Enze Xie, Song Han, Han Cai</strong></p>
<p>Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model’s latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model’s inherent generation quality. We verify DC-Gen’s effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: <a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen">https://github.com/dc-ai-projects/DC-Gen</a>. </p>
<blockquote>
<p>现有的文本到图像的扩散模型在生成高质量图像方面表现出色，但在扩展到高分辨率（如4K图像生成）时面临重大的效率挑战。尽管之前的研究从各个方面加速了扩散模型，但很少处理潜在空间中的固有冗余。为了弥补这一空白，本文介绍了DC-Gen，这是一个通过利用深度压缩的潜在空间来加速文本到图像扩散模型的通用框架。DC-Gen不同于昂贵的从头开始训练方法，它采用高效的后训练管道来保留基础模型的质量。在这种范式中的关键挑战是基础模型的潜在空间与深度压缩的潜在空间之间的表示差距，这可能导致直接微调时的不稳定性。为了克服这一点，DC-Gen首先通过轻量级的嵌入对齐训练来弥合表示差距。一旦潜在嵌入对齐，只需少量的LoRA微调即可解锁基础模型的固有生成质量。我们在SANA和FLUX.1-Krea上验证了DC-Gen的有效性。DC-Gen-SANA和DC-Gen-FLUX模型在质量上与基础模型相当，但具有显著的速度提升。具体来说，DC-Gen-FLUX在NVIDIA H100 GPU上将4K图像的生成延迟减少了53倍。当与NVFP4 SVDQuant结合时，DC-Gen-FLUX在单个NVIDIA 5090 GPU上只需3.5秒即可生成4K图像，与基础FLUX.1-Krea模型相比，总延迟降低了138倍。代码地址：<a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen%E3%80%82">https://github.com/dc-ai-projects/DC-Gen。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25180v1">PDF</a> Tech Report. The first three authors contributed equally to this work</p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种名为DC-Gen的通用框架，该框架通过利用深度压缩的潜在空间来加速文本到图像的扩散模型。DC-Gen采用高效的训练后流程，避免了昂贵的从头开始训练的方法，同时保留了基础模型的质量。该框架解决了基础模型的潜在空间与深度压缩的潜在空间之间的表示差距问题，通过轻量级的嵌入对齐训练来弥合这一差距。一旦对齐潜在嵌入，只需少量的LoRA微调即可解锁基础模型的固有生成质量。实验证明，DC-Gen在SANA和FLUX.1-Krea模型上具有良好的有效性。特别是DC-Gen-FLUX在NVIDIA H100 GPU上将生成4K图像的延迟减少了53倍。当与NVFP4 SVDQuant结合时，DC-Gen-FLUX在单个NVIDIA 5090 GPU上仅用了3.5秒就生成了一个4K图像，与基础FLUX.1-Krea模型相比，总延迟减少了高达138倍。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DC-Gen框架旨在加速文本到图像的扩散模型，利用深度压缩的潜在空间以提高效率。</li>
<li>DC-Gen通过高效的训练后流程避免了从头开始训练的高成本，同时保留了基础模型的质量。</li>
<li>框架解决了基础模型与深度压缩潜在空间的表示差距问题，通过嵌入对齐训练进行弥合。</li>
<li>DC-Gen能提高模型的生成速度，同时保持或提高图像质量。</li>
<li>DC-Gen显著降低了生成4K图像所需的延迟时间。</li>
<li>与其他技术结合，DC-Gen能进一步提升性能，实现更快的图像生成速度。</li>
<li>DC-Gen具有广泛的应用前景，特别是在需要高效文本到图像转换的场合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25180v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GLASS-Flows-Transition-Sampling-for-Alignment-of-Flow-and-Diffusion-Models"><a href="#GLASS-Flows-Transition-Sampling-for-Alignment-of-Flow-and-Diffusion-Models" class="headerlink" title="GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion   Models"></a>GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion   Models</h2><p><strong>Authors:Peter Holderrieth, Uriel Singer, Tommi Jaakkola, Ricky T. Q. Chen, Yaron Lipman, Brian Karrer</strong></p>
<p>The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a “flow matching model within a flow matching model” to sample Markov transitions. As we show in this work, this “inner” flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models. </p>
<blockquote>
<p>使用奖励对齐算法可以在推理时间显著提高流匹配和扩散模型的性能，但效率仍然是一个主要限制。虽然提出了几种算法，但我们证明了一个共同的瓶颈在于这些算法所依赖的采样方法：许多算法需要通过随机微分方程（SDE）采样进行马尔可夫转换采样，这显著地不如常微分方程（ODE）采样高效且性能较差。为了消除这一瓶颈，我们引入了GLASS Flows这一新型采样范式，它模拟“流匹配模型内的流匹配模型”来采样马尔可夫转换。我们在本工作中展示，这个“内部”流匹配模型可以从预训练模型中检索出来，无需任何再训练，结合了常微分方程的高效性和随机微分方程的随机演化。在大型文本到图像模型中，我们证明了GLASS Flows消除了随机演化和效率之间的权衡。结合费曼-卡克转向，GLASS Flows提高了文本到图像生成的最先进性能，使其成为流和扩散模型推理时间缩放的一种简单、即插即用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25170v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>流匹配和扩散模型的性能可通过使用奖励对齐算法在推理时间显著提高，但效率仍是主要限制。虽然已提出多种算法，但演示表明，这些算法所依赖的采样方法是共同瓶颈：许多算法需要通过随机微分方程（SDE）采样进行马尔可夫过渡采样，这显著降低了效率并且往往性能不如常微分方程（ODE）采样。为了消除这一瓶颈，我们引入了GLASS流这一新采样范式，它通过模拟“流匹配模型内的流匹配模型”来采样马尔可夫过渡。我们在工作中展示，这个“内部”流匹配模型可以从预训练模型中提取，无需任何再训练，结合了ODE的效率与SDE的随机演变。在大型文本到图像模型中，我们展示GLASS流消除了随机演变与效率之间的权衡。结合费曼-卡克转向，GLASS流提高了文本到图像生成的最先进性能，成为流和扩散模型推理时间扩展的简单即插即用解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励对齐算法可以提高流匹配和扩散模型的推理时间性能。</li>
<li>效率是流匹配和扩散模型的主要限制之一。</li>
<li>现有算法依赖于通过SDE采样的马尔可夫过渡采样，这不太高效且性能有限。</li>
<li>GLASS流是一种新采样范式，模拟“流匹配模型内的流匹配模型”以采样马尔可夫过渡。</li>
<li>GLASS流的“内部”流匹配模型可以从预训练模型中提取，无需再训练。</li>
<li>GLASS流结合了ODE的效率与SDE的随机演变。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25170v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25170v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Aligning-Visual-Foundation-Encoders-to-Tokenizers-for-Diffusion-Models"><a href="#Aligning-Visual-Foundation-Encoders-to-Tokenizers-for-Diffusion-Models" class="headerlink" title="Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models"></a>Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models</h2><p><strong>Authors:Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, Kai Zhang</strong></p>
<p>In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design. </p>
<blockquote>
<p>在这项工作中，我们提出了将预训练的视觉编码器对齐，以作为图像生成中潜在扩散模型的标记器。不同于从头开始训练变分自编码器（VAE）主要强调低层次细节，我们的方法利用基础编码器的丰富语义结构。我们引入了一个三阶段对齐策略：（1）冻结编码器，并训练适配器和解码器以建立语义潜在空间；（2）通过额外的语义保留损失联合优化所有组件，使编码器能够捕获感知细节，同时保留高级语义；（3）对解码器进行微调以提高重建质量。这种对齐产生了丰富的语义图像标记器，对扩散模型有益。在ImageNet 256x256上，我们的标记器加速了扩散模型的收敛，仅在64个周期内就达到了1.90的gFID，并在有无分类器引导的情况下都提高了生成质量。扩展到LAION，使用我们的标记器训练的2B参数文本到图像模型在相同训练步骤下始终优于FLUX VAE。总的来说，我们的方法简单、可扩展，为连续标记器设计建立了一个语义基础的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25162v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://aligntok.github.io/">https://aligntok.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出将预训练的视觉编码器对齐，用作图像生成中潜在扩散模型的标记器。该方法不同于从头开始训练变分自编码器（VAE），而侧重于利用基础编码器的丰富语义结构。引入的三阶段对齐策略包括：冻结编码器，训练适配器和解码器以建立语义潜在空间；联合优化所有组件，并添加语义保留损失，使编码器能够捕获感知细节并保留高级语义；以及改进解码器的重建质量。这种对齐方式生成了有益于扩散模型的语义丰富的图像标记器。在ImageNet 256x256上，该标记器加速了扩散模型的收敛，仅64个周期就达到了1.90的gFID，并在有和无分类器引导的情况下都提高了生成质量。在LAION上扩展到2B参数的文本到图像模型，使用此标记器的表现始终优于FLUX VAE。总体而言，该方法简单、可扩展，并为连续标记器设计建立了语义基础的模式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了使用预训练视觉编码器对齐作为潜在扩散模型的标记器的方法。</li>
<li>采用三阶段对齐策略，包括建立语义潜在空间、联合优化组件并添加语义保留损失，以及改进解码器。</li>
<li>方法加速了扩散模型的收敛，并在ImageNet上达到了较高的生成质量。</li>
<li>该方法在LAION数据集上扩展到2B参数的文本到图像模型时表现优异。</li>
<li>对比FLUX VAE，使用此标记器的模型在相同训练步骤下表现更佳。</li>
<li>该方法简单、可扩展，为连续标记器设计提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25162">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.25162v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"><a href="#FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing" class="headerlink" title="FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing"></a>FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing</h2><p><strong>Authors:Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit">https://github.com/JunyiWuCode/FlashEdit</a>. </p>
<blockquote>
<p>使用扩散模型进行文本引导的图像编辑已经取得了显著的品质，但依然面临难以承受的延迟问题，阻碍了其在现实世界的应用。我们引入了FlashEdit，这是一个新型框架，旨在实现高保真、实时的图像编辑。它的效率源于三个关键创新点：（1）一步式反转和编辑（OSIE）管道，绕过昂贵的迭代过程；（2）背景屏蔽（BG-Shield）技术，通过选择性修改仅编辑区域内的特征来保证背景保存；（3）稀疏空间交叉注意力（SSCA）机制，通过抑制语义泄露到背景来确保精确、局部编辑。大量实验表明，FlashEdit在保持背景一致性和结构完整性的同时，编辑时间不到0.2秒，与先前的多步骤方法相比，速度提高了超过150倍。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunyiWuCode/FlashEdit上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22244v3">PDF</a> We need to improve our work</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对扩散模型的新框架FlashEdit，旨在实现高保真度的实时图像编辑。其效率源于三个关键创新点：一是绕过昂贵迭代过程的一步反转和编辑（OSIE）管道；二是通过选择性修改编辑区域内的特征来保证背景保留的背景屏蔽（BG-Shield）技术；三是抑制背景语义泄露以实现精确局部编辑的稀疏空间交叉注意力（SSCA）机制。FlashEdit保持了卓越的图像背景一致性和结构完整性，能够在不到0.2秒内完成编辑操作，与先前多步骤方法相比，速度提高了超过150倍。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FlashEdit是一个用于实现高保真度实时图像编辑的新框架。</li>
<li>该框架的效率来源于三个关键创新点：一步反转和编辑管道（OSIE）、背景屏蔽技术和稀疏空间交叉注意力机制。</li>
<li>FlashEdit能够在不到0.2秒内完成图像编辑操作，相比以前的方法大大提高了效率。</li>
<li>OSIE管道绕过了昂贵的迭代过程，提高了效率。</li>
<li>背景屏蔽技术通过选择性修改编辑区域内的特征，保证了背景的一致性。</li>
<li>稀疏空间交叉注意力机制实现了精确的局部编辑，抑制了语义泄露到背景。</li>
<li>实验证明，FlashEdit保持了优秀的背景一致性和结构完整性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2509.22244v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TADA-Improved-Diffusion-Sampling-with-Training-free-Augmented-Dynamics"><a href="#TADA-Improved-Diffusion-Sampling-with-Training-free-Augmented-Dynamics" class="headerlink" title="TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics"></a>TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics</h2><p><strong>Authors:Tianrong Chen, Huangjie Zheng, David Berthelot, Jiatao Gu, Josh Susskind, Shuangfei Zhai</strong></p>
<p>Diffusion models have demonstrated exceptional capabilities in generating high-fidelity images but typically suffer from inefficient sampling. Many solver designs and noise scheduling strategies have been proposed to dramatically improve sampling speeds. In this paper, we introduce a new sampling method that is up to $186%$ faster than the current state of the art solver for comparative FID on ImageNet512. This new sampling method is training-free and uses an ordinary differential equation (ODE) solver. The key to our method resides in using higher-dimensional initial noise, allowing to produce more detailed samples with less function evaluations from existing pretrained diffusion models. In addition, by design our solver allows to control the level of detail through a simple hyper-parameter at no extra computational cost. We present how our approach leverages momentum dynamics by establishing a fundamental equivalence between momentum diffusion models and conventional diffusion models with respect to their training paradigms. Moreover, we observe the use of higher-dimensional noise naturally exhibits characteristics similar to stochastic differential equations (SDEs). Finally, we demonstrate strong performances on a set of representative pretrained diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover models in both pixel and latent spaces, as well as class and text conditional settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/apple/ml-tada">https://github.com/apple/ml-tada</a>. </p>
<blockquote>
<p>扩散模型在生成高保真图像方面表现出卓越的能力，但通常存在采样效率低下的问题。为了显著提高采样速度，已经提出了许多求解器设计和噪声调度策略。在本文中，我们介绍了一种新的采样方法，其速度比当前最先进的求解器在ImageNet512上进行比较时快达186%。这种新的采样方法无需训练，并使用常微分方程（ODE）求解器。我们的方法的关键在于使用高维初始噪声，从而利用现有的预训练扩散模型以较少的函数评估产生更详细的样本。此外，我们的求解器设计允许通过简单的超参数控制细节层次，而无需额外的计算成本。我们展示了我们的方法如何利用动量动力学，通过建立动量扩散模型与常规扩散模型在训练范式方面的基本等价关系。此外，我们观察到高维噪声的使用自然表现出与随机微分方程（SDEs）相似的特征。最后，我们在一组代表性的预训练扩散模型上展示了强大的性能，包括EDM、EDM2和Stable-Diffusion 3，这些模型涵盖了像素和潜在空间以及类和文本条件设置。代码可在<a target="_blank" rel="noopener" href="https://github.com/apple/ml-tada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/apple/ml-tada找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21757v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的采样方法，该方法使用常微分方程（ODE）求解器，无需训练，即可显著提升扩散模型生成图像的效率，速度比当前最先进的求解器快达186%。新方法利用高维初始噪声，减少函数评估次数，产生更详细的样本。此外，通过设计，该方法可通过简单超参数控制细节层次，无需额外计算成本。该方法还利用动量动力学，建立动量扩散模型与传统扩散模型之间的训练范式等价关系。使用高维噪声展现出与随机微分方程（SDEs）相似的特性。在多个预训练扩散模型上表现出强性能，包括EDM、EDM2、Stable-Diffusion 3等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入新的采样方法，显著提升扩散模型生成图像的效率，速度比当前最先进的求解器快达186%。</li>
<li>使用常微分方程（ODE）求解器，无需训练。</li>
<li>利用高维初始噪声，减少函数评估次数，产生更详细的样本。</li>
<li>通过简单超参数控制细节层次，无需额外计算成本。</li>
<li>方法建立动量扩散模型与传统扩散模型之间的训练范式等价关系。</li>
<li>使用高维噪声展现出与随机微分方程（SDEs）相似的特性。</li>
<li>在多个预训练扩散模型上表现出强性能，包括EDM、EDM2、Stable-Diffusion 3等模型的像素和潜在空间、类别和文本条件设置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21757">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2506.21757v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InverseBench-Benchmarking-Plug-and-Play-Diffusion-Priors-for-Inverse-Problems-in-Physical-Sciences"><a href="#InverseBench-Benchmarking-Plug-and-Play-Diffusion-Priors-for-Inverse-Problems-in-Physical-Sciences" class="headerlink" title="InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse   Problems in Physical Sciences"></a>InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse   Problems in Physical Sciences</h2><p><strong>Authors:Hongkai Zheng, Wenda Chu, Bingliang Zhang, Zihui Wu, Austin Wang, Berthy T. Feng, Caifeng Zou, Yu Sun, Nikola Kovachki, Zachary E. Ross, Katherine L. Bouman, Yisong Yue</strong></p>
<p>Plug-and-play diffusion priors (PnPDP) have emerged as a promising research direction for solving inverse problems.   However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \textsc{InverseBench}, a framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as optical tomography, medical imaging, black hole imaging, seismology, and fluid dynamics. With \textsc{InverseBench}, we benchmark 14 inverse problem algorithms that use plug-and-play diffusion priors against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. To facilitate further research and development, we open-source the codebase, along with datasets and pre-trained models, at <a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/">https://devzhk.github.io/InverseBench/</a>. </p>
<blockquote>
<p>即插即用扩散先验（PnPDP）已成为解决反问题的一个前景广阔的研究方向。然而，当前的研究主要集中在自然图像恢复上，这些算法在科学反问题中的性能在很大程度上尚未被探索。为了弥补这一空白，我们引入了\textsc{InverseBench}框架，该框架在五个不同的科学反问题上评估扩散模型。这些问题呈现出独特的结构挑战，与现有基准测试不同，源于关键科学应用，如光学层析成像、医学成像、黑洞成像、地震学和流体动力学。通过\textsc{InverseBench}，我们对使用即插即用扩散先验的14种反问题算法进行了基准测试，与强大、特定领域的基准线进行了比较，为现有算法的优缺点提供了宝贵的新见解。为了促进进一步的研究和开发，我们在<a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/%E5%85%AC%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://devzhk.github.io/InverseBench/公开源代码库、数据集和预训练模型。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11043v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散先验的“即插即用”（PnPDP）为求解反问题提供了一个前景广阔的研究方向。然而，当前的研究主要集中在自然图像恢复领域，这些算法在科学反问题上的表现尚未得到充分探索。为解决这一空白，我们推出了\textsc{InverseBench}框架，该框架在五个不同的科学反问题上评估扩散模型的表现。这些问题具有独特的结构挑战，不同于现有的基准测试，广泛应用于关键科学应用，如光学层析成像、医学成像、黑洞成像、地震学和流体动力学。通过使用\textsc{InverseBench}，我们对使用扩散先验的14种反问题算法进行了基准测试，与强大专属的基线进行了比较，为现有算法的优点和缺点提供了有价值的新见解。为促进进一步的研究和开发，我们在 <a target="_blank" rel="noopener" href="https://devzhk.github.io/InverseBench/">https://devzhk.github.io/InverseBench/</a> 开源了代码库、数据集和预训练模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散先验的“即插即用”（PnPDP）为反问题研究提供了新的方向。</li>
<li>当前研究主要关注自然图像恢复，科学反问题领域尚未充分探索。</li>
<li>\textsc{InverseBench}框架用于评估扩散模型在五个科学反问题上的表现。</li>
<li>科学反问题具有独特的结构挑战，区别于现有的基准测试。</li>
<li>\textsc{InverseBench}涵盖了关键科学应用，如光学层析成像、医学成像等。</li>
<li>通过基准测试，对使用扩散先验的算法进行了评估，揭示了现有算法的优点和缺点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11043">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2503.11043v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CE-SDWV-Effective-and-Efficient-Concept-Erasure-for-Text-to-Image-Diffusion-Models-via-a-Semantic-Driven-Word-Vocabulary"><a href="#CE-SDWV-Effective-and-Efficient-Concept-Erasure-for-Text-to-Image-Diffusion-Models-via-a-Semantic-Driven-Word-Vocabulary" class="headerlink" title="CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image   Diffusion Models via a Semantic-Driven Word Vocabulary"></a>CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image   Diffusion Models via a Semantic-Driven Word Vocabulary</h2><p><strong>Authors:Jiahang Tu, Qian Feng, Jiahua Dong, Hanbin Zhao, Chao Zhang, Nicu Sebe, Hui Qian</strong></p>
<p>Large-scale text-to-image (T2I) diffusion models have achieved remarkable generative performance about various concepts. With the limitation of privacy and safety in practice, the generative capability concerning NSFW (Not Safe For Work) concepts is undesirable, e.g., producing sexually explicit photos, and licensed images. The concept erasure task for T2I diffusion models has attracted considerable attention and requires an effective and efficient method. To achieve this goal, we propose a CE-SDWV framework, which removes the target concepts (e.g., NSFW concepts) of T2I diffusion models in the text semantic space by only adjusting the text condition tokens and does not need to re-train the original T2I diffusion model’s weights. Specifically, our framework first builds a target concept-related word vocabulary to enhance the representation of the target concepts within the text semantic space, and then utilizes an adaptive semantic component suppression strategy to ablate the target concept-related semantic information in the text condition tokens. To further adapt the above text condition tokens to the original image semantic space, we propose an end-to-end gradient-orthogonal token optimization strategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate the effectiveness and efficiency of our method. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TtuHamg/CE-SDWV">https://github.com/TtuHamg/CE-SDWV</a>. </p>
<blockquote>
<p>大规模文本到图像（T2I）扩散模型在各种概念上取得了显著的生成性能。但在实践中，由于隐私和安全性的限制，关于NSFW（不适合工作）概念的生成能力并不理想，例如生成色情照片和许可图像。文本到图像扩散模型的概念消除任务引起了人们的广泛关注，需要一种有效且高效的方法。为实现这一目标，我们提出了CE-SDWV框架，该框架通过在文本语义空间中仅调整文本条件令牌来消除目标概念（例如NSFW概念），而无需重新训练原始T2I扩散模型的权重。具体来说，我们的框架首先构建一个与目标概念相关的词汇表，以增强目标概念在文本语义空间中的表示，然后采用自适应语义成分抑制策略，消除目标概念相关的语义信息在文本条件令牌中。为了进一步将上述文本条件令牌适应到原始图像语义空间，我们提出了一种端到端的梯度正交令牌优化策略。在I2P和UnlearnCanvas基准测试上的大量实验证明了我们方法的有效性和效率。代码可在<a target="_blank" rel="noopener" href="https://github.com/TtuHamg/CE-SDWV%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TtuHamg/CE-SDWV找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15562v2">PDF</a> 25 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>本文关注大规模文本到图像（T2I）扩散模型在处理涉及不安全和不适宜工作场合（NSFW）概念时的隐私问题。为此，提出了一个CE-SDWV框架，通过在文本语义空间中调整文本条件令牌来消除目标概念，无需重新训练原始T2I扩散模型的权重。该框架通过构建目标概念相关词汇表和采用自适应语义组件抑制策略来消除与NSFW相关的语义信息。此外，还提出了一种端到端的梯度正交令牌优化策略，以适应原始图像语义空间。实验证明该方法有效且高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型文本到图像（T2I）扩散模型在生成各种概念方面表现出色。</li>
<li>在实践中存在隐私和安全限制问题，例如生成不适宜工作场合（NSFW）的概念，如性暗示照片和受版权保护的图片。</li>
<li>CE-SDWV框架旨在消除T2I扩散模型中的目标概念，如NSFW概念，而无需重新训练模型权重。</li>
<li>该框架通过构建目标概念相关词汇表和自适应语义组件抑制策略来消除目标概念的语义信息。</li>
<li>采用端到端的梯度正交令牌优化策略，以适应原始图像语义空间。</li>
<li>在I2P和UnlearnCanvas基准测试上的实验证明了该方法的有效性和高效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2501.15562v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unpicking-Data-at-the-Seams-Understanding-Disentanglement-in-VAEs"><a href="#Unpicking-Data-at-the-Seams-Understanding-Disentanglement-in-VAEs" class="headerlink" title="Unpicking Data at the Seams: Understanding Disentanglement in VAEs"></a>Unpicking Data at the Seams: Understanding Disentanglement in VAEs</h2><p><strong>Authors:Carl Allen</strong></p>
<p>A generative latent variable model is said to be disentangled when varying a single latent co-ordinate changes a single aspect of samples generated, e.g. object position or facial expression in an image. Related phenomena are seen in several generative paradigms, including state-of-the-art diffusion models, but disentanglement is most notably observed in Variational Autoencoders (VAEs), where oft-used diagonal posterior covariances are argued to be the cause. We make this picture precise. From a known exact link between optimal Gaussian posteriors and decoder derivatives, we show how diagonal posteriors “lock” a decoder’s local axes so that density over the data manifold factorises along independent one-dimensional seams that map to axis-aligned directions in latent space. This gives a clear definition of disentanglement, explains why it emerges in VAEs and shows that, under stated assumptions, ground truth factors are identifiable even with a symmetric prior. </p>
<blockquote>
<p>当一个生成潜在变量模型中的单一潜在坐标发生变化时，只会改变生成样本的单一方面，例如图像中的对象位置或面部表情，此时该模型被认为是解纠缠的。这种现象在多种生成模型中都可以看到，包括最先进的扩散模型，但在变分自编码器（VAEs）中，解纠缠现象最为明显，人们认为常用的对角后验协方差是造成这一现象的原因。我们使这一观点精确化。从已知的最优高斯后验和解码器导数之间的关联出发，我们展示了如何对角后验会“锁定”解码器的局部轴，使得数据流形上的密度能够沿着独立的一维接缝进行分解，这些接缝映射到潜在空间中的轴对齐方向。这为解纠缠提供了明确的定义，解释了为什么它会在VAEs中出现，并表明在假设条件下，即使使用对称先验，真实因素也是可以识别的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22559v6">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>     生成潜在变量模型的解耦现象是指改变单一的潜在坐标会影响生成样本的单一特征，如图像的物体位置或面部表情。这种现象在包括最先进的扩散模型在内的多种生成范式中都可以看到，但在变分自编码器（VAEs）中尤其明显。我们通过精确描述最优高斯后验与解码器导数之间的已知联系，展示了对角后验如何“锁定”解码器的局部轴，使数据流形上的密度沿独立的一维接缝分解，这些接缝映射到潜在空间中的轴对齐方向。这为解耦现象提供了明确的定义，解释了为什么它会在VAEs中出现，并表明在既定的假设下，即使使用对称先验，真实因素也是可识别的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成潜在变量模型的解耦现象：改变单一潜在坐标会影响生成样本的单一特征。</li>
<li>解耦现象在多种生成范式中可见，尤其在变分自编码器（VAEs）中。</li>
<li>对角后验在解耦现象中扮演重要角色，它们“锁定”解码器的局部轴。</li>
<li>数据流形上的密度沿独立的一维接缝分解，这些接缝映射到潜在空间的轴对齐方向。</li>
<li>论文提供了关于解耦现象的明确定义。</li>
<li>论文解释了为什么解耦现象在VAEs中出现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22559">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_Diffusion Models/2410.22559v6/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_牙齿修复/2509.12069v2/page_2_0.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-10-02  U-Mamba2 Scaling State Space Models for Dental Anatomy Segmentation in   CBCT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-02\./crop_NeRF/2509.25191v1/page_5_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-02  VGGT-X When VGGT Meets Dense Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
