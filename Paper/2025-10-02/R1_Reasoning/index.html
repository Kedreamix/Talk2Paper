<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Query-Kontext An Unified Multimodal Model for Image Generation and   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-271f8ed29c861540ff3eb27a6dbdd428.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-02-æ›´æ–°"><a href="#2025-10-02-æ›´æ–°" class="headerlink" title="2025-10-02 æ›´æ–°"></a>2025-10-02 æ›´æ–°</h1><h2 id="Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing"><a href="#Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing" class="headerlink" title="Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing"></a>Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing</h2><p><strong>Authors:Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</strong></p>
<p>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal &#96;&#96;kontextâ€™â€™ composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion modelâ€™s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLMâ€™s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰å’Œç¼–è¾‘ï¼ˆTI2Iï¼‰æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯ä½œä¸ºé›†æˆäº†å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¿˜æ˜¯ä½œä¸ºæ—©æœŸç†è§£å’Œç”Ÿæˆæ¨¡æ€èåˆçš„ç®€å•ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å½“å‰ç»Ÿä¸€æ¡†æ¶ä¸­ï¼ŒåŒ…å«æŒ‡ä»¤ç†è§£ã€æ¥åœ°å’Œå›¾åƒå¼•ç”¨åœ¨å†…çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œå¯¹äºèº«ä»½ä¿ç•™å’Œå¿ å®é‡å»ºä»¥åŠä¸é«˜ä¿çœŸåˆæˆæœ‰ç€æœ¬è´¨çš„è”ç³»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Query-Kontextï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç”±æ¥è‡ªå¤šæ¨¡æ€è¾“å…¥çš„è¯­ä¹‰çº¿ç´¢å’Œç²—ç²’åº¦å›¾åƒæ¡ä»¶ç»„æˆçš„å¤šæ¨¡æ€&#96;&#96;kontextâ€™â€™æ¥æ¡¥æ¢VLMå’Œæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–¹æ³•ã€‚è¿™ç§è®¾è®¡å°†å¤æ‚çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›å§”æ‰˜ç»™å¼ºå¤§çš„VLMï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜è´¨é‡è§†è§‰åˆæˆçš„ä½œç”¨ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ä¸‰é˜¶æ®µçš„æ¸è¿›è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€kontextä»¤ç‰Œå°†VLMè¿æ¥åˆ°è½»é‡çº§çš„æ‰©æ•£å¤´ï¼Œä»¥é‡Šæ”¾VLMçš„ç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ­¤å¤´æ‰©å±•åˆ°å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜è§†è§‰ç»†èŠ‚å’Œé€¼çœŸåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªä½çº§å›¾åƒç¼–ç å™¨ï¼Œä»¥æé«˜å›¾åƒä¿çœŸåº¦å¹¶å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç»¼åˆæ•°æ®ç®¡é“ï¼Œèåˆäº†çœŸå®ã€åˆæˆå’Œå¼€æºæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å¤šæ¨¡æ€å‚è€ƒåˆ°å›¾åƒåœºæ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€å®šåˆ¶ç”Ÿæˆå’Œå¤šä¸»ä½“ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ç›¸åŒ¹é…ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†ä»»åŠ¡ç‰¹å®šçš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26641v1">PDF</a> 23 pages, 10 figures</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“èåˆæ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºQuery-Kontextæ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰çº¿ç´¢å’Œç²—ç²’åº¦å›¾åƒæ¡ä»¶çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ¡¥æ¥è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ã€‚é‡‡ç”¨ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œé‡Šæ”¾VLMçš„ç”Ÿæˆæ¨ç†èƒ½åŠ›ï¼Œå¢å¼ºè§†è§‰ç»†èŠ‚å’ŒçœŸå®æ„Ÿï¼Œå¹¶å¼•å…¥ä½çº§åˆ«å›¾åƒç¼–ç å™¨æé«˜å›¾åƒä¿çœŸåº¦ã€‚å»ºç«‹ç»¼åˆæ•°æ®ç®¡é“ï¼Œæ¶µç›–å¤šç§å¤šåª’ä½“å‚è€ƒè½¬å›¾åƒåœºæ™¯ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åŒ¹é…å¼ºå¤§ç»Ÿä¸€åŸºçº¿ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMMsåœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€šè¿‡å¼ºå¤§çš„VLMä¸æ‰©æ•£æ¨¡å‹ç»“åˆå®ç°é«˜æ€§èƒ½ã€‚</li>
<li>Query-Kontextæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ¡¥æ¥VLMå’Œæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…å«è¯­ä¹‰çº¿ç´¢å’Œå›¾åƒæ¡ä»¶çš„ç¼–ç ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä»¥é‡Šæ”¾VLMçš„ç”Ÿæˆæ¨ç†èƒ½åŠ›ï¼Œå¹¶å¢å¼ºè§†è§‰è´¨é‡å’Œå›¾åƒä¿çœŸåº¦ã€‚</li>
<li>ç»¼åˆæ•°æ®ç®¡é“çš„å»ºç«‹æ¶µç›–äº†å¤šç§å¤šåª’ä½“å‚è€ƒè½¬å›¾åƒåœºæ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€è‡ªå®šä¹‰ç”Ÿæˆå’Œå¤šä¸»ä½“ç»„åˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒQuery-Kontextæ–¹æ³•æ€§èƒ½å¼ºå¤§ï¼Œä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒè¯­ä¹‰ç†è§£å’Œç”Ÿæˆæ¨¡æ€çš„èåˆï¼Œä¸ºå®ç°æ›´çœŸå®çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d0d8d28870244817477459c0436f743.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61bdcf79c9e56baaa148262412fbbddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffcc422d5fb70fa9fcb5affe9ef9867b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ebd26c49e1984a2d508bbca9dc5e1b" align="middle">
<img src="https://picx.zhimg.com/v2-3b31a6e9fe32231c994b9067429a34e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-708b81740936f91df3d4b2abd48f20d5" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AccidentBench-Benchmarking-Multimodal-Understanding-and-Reasoning-in-Vehicle-Accidents-and-Beyond"><a href="#AccidentBench-Benchmarking-Multimodal-Understanding-and-Reasoning-in-Vehicle-Accidents-and-Beyond" class="headerlink" title="AccidentBench: Benchmarking Multimodal Understanding and Reasoning in   Vehicle Accidents and Beyond"></a>AccidentBench: Benchmarking Multimodal Understanding and Reasoning in   Vehicle Accidents and Beyond</h2><p><strong>Authors:Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos</strong></p>
<p>Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated questionâ€“answer pairs spanning multiple video lengths (short&#x2F;medium&#x2F;long) and difficulty levels (easy&#x2F;medium&#x2F;hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning. By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/SafeRL-Lab/AccidentBench">https://github.com/SafeRL-Lab/AccidentBench</a> </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬éœ€è¦åŸºå‡†æµ‹è¯•æ¥ä¸¥æ ¼è¯„ä¼°å…¶åœ¨å®‰å…¨å…³é”®ã€åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†AccidentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç»“åˆäº†è½¦è¾†äº‹æ•…åœºæ™¯å’ŒBeyondåŸŸï¼ˆå¼ºè°ƒç©ºé—´å’Œæ—¶é—´æ¨ç†çš„èˆªç©ºå’Œæ°´ä¸Šå®‰å…¨å…³é”®è®¾ç½®ï¼Œå¦‚å¯¼èˆªã€å®šä½ã€å¤šè½¦è¾†è¿åŠ¨ï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å¤§çº¦2000ä¸ªè§†é¢‘å’Œè¶…è¿‡19000ä¸ªäººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–å¤šç§è§†é¢‘é•¿åº¦ï¼ˆçŸ­&#x2F;ä¸­&#x2F;é•¿ï¼‰å’Œéš¾åº¦çº§åˆ«ï¼ˆå®¹æ˜“&#x2F;ä¸­ç­‰&#x2F;å›°éš¾ï¼‰ã€‚ä»»åŠ¡ç³»ç»Ÿåœ°æ£€æµ‹æ ¸å¿ƒèƒ½åŠ›ï¼šæ—¶é—´ã€ç©ºé—´å’Œæ„å›¾çš„ç†è§£å’Œæ¨ç†ã€‚é€šè¿‡ä»¥äº‹æ•…ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸æ›´å¹¿æ³›çš„èˆªç©ºå’Œæ°´ä¸Šå®‰å…¨å…³é”®åœºæ™¯çš„èåˆï¼ŒAccidentBenchä¸ºåœ¨ç°å®ä¸–ç•Œçš„å¤šå˜ç¯å¢ƒä¸‹è¯„ä¼°æ¨¡å‹æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”ç‰©ç†åŸºç¡€çš„æµ‹è¯•å¹³å°ã€‚å¯¹æœ€æ–°æ¨¡å‹ï¼ˆå¦‚Gemini-2.5 Proå’ŒGPT-5ï¼‰çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨é¢å¯¹æœ€å›°éš¾çš„ä»»åŠ¡å’Œæœ€é•¿è§†é¢‘æ—¶ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°çº¦18%çš„å‡†ç¡®ç‡ï¼Œè¿™æ­ç¤ºäº†åœ¨å®é™…ä¸–ç•Œçš„æ—¶é—´ã€ç©ºé—´å’Œæ„å›¾æ¨ç†æ–¹é¢ä»å­˜åœ¨å·¨å¤§å·®è·ã€‚AccidentBenchæ—¨åœ¨æ­ç¤ºè¿™äº›å…³é”®å·®è·ï¼Œå¹¶æ¨åŠ¨å¼€å‘æ›´å®‰å…¨ã€æ›´ç¨³å¥ã€æ›´å¥½åœ°é€‚åº”ç°å®å®‰å…¨æŒ‘æˆ˜çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/SafeRL-Lab/AccidentBench">https://github.com/SafeRL-Lab/AccidentBench</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26636v1">PDF</a> </p>
<p><strong>Summary</strong><br>äº‹æ•…åŸºå‡†ï¼ˆAccidentBenchï¼‰æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œç»“åˆäº†è½¦è¾†äº‹æ•…åœºæ™¯ä¸å…¶ä»–å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚ç©ºä¸­å’Œæ°´ä¸Šï¼‰ï¼Œå¼ºè°ƒæ—¶ç©ºæ¨ç†å’Œæ„å›¾ç†è§£ã€‚å®ƒåŒ…å«å¤§çº¦2000ä¸ªè§†é¢‘å’Œè¶…è¿‡1ä¸‡9åƒä¸ªäººç±»æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„å¤æ‚å¤šå˜ç¯å¢ƒä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡è®¾è®¡å¤æ‚çš„ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¦‚æ—¶é—´ã€ç©ºé—´å’Œæ„å›¾æ¨ç†ã€‚å°½ç®¡æœ€å¼ºçš„æ¨¡å‹åœ¨æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡åªæœ‰çº¦18%ï¼Œä½†äº‹æ•…åŸºå‡†è®¾è®¡æ—¨åœ¨æ­ç¤ºæ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„æ¨ç†èƒ½åŠ›ä¸Šçš„å·®è·ï¼Œå¹¶æ¨åŠ¨å¼€å‘æ›´å®‰å…¨ã€æ›´ç¨³å¥ä¸”æ›´ç¬¦åˆç°å®å®‰å…¨æŒ‘æˆ˜çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AccidentBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒç»“åˆäº†è½¦è¾†äº‹æ•…åœºæ™¯ä»¥åŠå…¶ä»–å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚ç©ºä¸­å’Œæ°´ä¸Šï¼‰ï¼Œå¼ºè°ƒæ—¶ç©ºæ¨ç†å’Œæ„å›¾ç†è§£çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥åŸºå‡†åŒ…å«å¤§é‡è§†é¢‘å’Œæ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–ä¸åŒè§†é¢‘é•¿åº¦å’Œéš¾åº¦çº§åˆ«ã€‚</li>
<li>äº‹æ•…åŸºå‡†è®¾è®¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¦‚æ—¶é—´ã€ç©ºé—´å’Œæ„å›¾æ¨ç†ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨äº‹æ•…åŸºå‡†ä¸Šçš„å‡†ç¡®ç‡è¾ƒä½ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„æ¨ç†èƒ½åŠ›ä¸Šçš„å·®è·ã€‚</li>
<li>AccidentBenchçš„ç›®æ ‡æ˜¯æ¨åŠ¨å¼€å‘æ›´å®‰å…¨ã€æ›´ç¨³å¥çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ›´å¥½åœ°åº”å¯¹ç°å®å®‰å…¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a46d009810d11653a6af8537ef01e31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06beeb8541fc9a407090428bc7cd8eea" align="middle">
<img src="https://picx.zhimg.com/v2-78de19bc49d2d950265d6007b8bcf349" align="middle">
<img src="https://picx.zhimg.com/v2-53f2e837c1383dcfa89bf1f8e174f3fb" align="middle">
<img src="https://pic1.zhimg.com/v2-3435ef45288d800d0a43133f8da36c04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2c345646e04ad3a4a187a239404375" align="middle">
<img src="https://picx.zhimg.com/v2-08a7de0757fffb7a50ac68b67457521e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models"><a href="#Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models" class="headerlink" title="Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models"></a>Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models</h2><p><strong>Authors:Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ä¸åŸºäºç»“æœçš„RLç›¸æ¯”ï¼Œè¿‡ç¨‹ç›‘ç£RLï¼ˆPSRLï¼‰ä½œä¸ºä¸€ç§æ›´æœ‰æ•ˆçš„èŒƒå¼è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PSRLæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæ— è®ºæ˜¯åœ¨åˆ†æ”¯ä½ç½®è¿˜æ˜¯åœ¨é‡‡æ ·æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„PSRLæ¡†æ¶ï¼ˆAttnRLï¼‰ï¼Œä¸ºæ¨ç†æ¨¡å‹å®ç°é«˜æ•ˆæ¢ç´¢ã€‚å—åˆæ­¥è§‚å¯Ÿå¯å‘ï¼Œè¡¨ç°é«˜æ³¨æ„åŠ›å¾—åˆ†çš„æ­¥éª¤ä¸æ¨ç†è¡Œä¸ºç›¸å…³ï¼Œæˆ‘ä»¬å»ºè®®åœ¨é«˜ä½å€¼çš„ä½ç½®è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è€ƒè™‘äº†é—®é¢˜çš„éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¿æŒéé›¶ä¼˜åŠ¿å€¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„é‡‡æ ·æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸ºPSRLè®¾è®¡äº†ä¸€ä¸ªä¸€æ­¥ç¦»çº¿è®­ç»ƒç®¡é“ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½å’Œé‡‡æ ·ä»¥åŠè®­ç»ƒæ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç›¸è¾ƒäºç»“æœå¯¼å‘çš„RLï¼Œè¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PSRLæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡æœ‰é™çš„é—®é¢˜ï¼Œæ¶‰åŠåˆ†æ”¯ä½ç½®å’Œé‡‡æ ·ä¸¤ä¸ªæ–¹é¢ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹PSRLæ¡†æ¶â€”â€”AttnRLï¼Œä¸ºæ¨ç†æ¨¡å‹æä¾›é«˜æ•ˆæ¢ç´¢èƒ½åŠ›ã€‚æ ¹æ®åˆæ­¥è§‚å¯Ÿï¼Œé«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤ä¸æ¨ç†è¡Œä¸ºç›¸å…³ï¼Œæˆ‘ä»¬ä»é«˜ä»·å€¼ä½ç½®è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‚åº”æ€§é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¿æŒéé›¶ä¼˜åŠ¿å€¼ã€‚ä¸ºæé«˜é‡‡æ ·æ•ˆç‡ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€æ­¥ç¦»çº¿ç­–ç•¥è®­ç»ƒç®¡é“ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½å’Œé‡‡æ ·åŠè®­ç»ƒæ•ˆç‡æ–¹é¢å‡æŒç»­è¶…è¶Šå…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸Šå…·æœ‰æ˜¾è‘—æˆåŠŸã€‚</li>
<li>è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰ç›¸æ¯”ç»“æœå¯¼å‘çš„RLå±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ¨¡å¼ã€‚</li>
<li>ç°æœ‰PSRLæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡æœ‰é™çš„é—®é¢˜ï¼Œæ¶‰åŠåˆ†æ”¯ä½ç½®å’Œé‡‡æ ·ã€‚</li>
<li>æ–°å‹PSRLæ¡†æ¶â€”â€”AttnRLè¢«å¼•å…¥ï¼Œè¯¥æ¡†æ¶ä»é«˜ä»·å€¼ä½ç½®è¿›è¡Œåˆ†æ”¯ï¼Œæé«˜æ¢ç´¢æ•ˆç‡ã€‚</li>
<li>AttnRLè€ƒè™‘äº†é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°æ¥å¼€å‘é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>AttnRLè®¾è®¡äº†ä¸€æ­¥ç¦»çº¿ç­–ç•¥è®­ç»ƒç®¡é“ï¼Œä»¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b57737dd9b619b1c297a3cd43ea6e2d" align="middle">
<img src="https://picx.zhimg.com/v2-2d256feefee706f30bbb907165a1ddf4" align="middle">
<img src="https://picx.zhimg.com/v2-f1bb174b108721e88599d031f7f6917c" align="middle">
<img src="https://picx.zhimg.com/v2-e3cfd7d0102ce8de5ea42b84c858aeab" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Recursive-Self-Aggregation-Unlocks-Deep-Thinking-in-Large-Language-Models"><a href="#Recursive-Self-Aggregation-Unlocks-Deep-Thinking-in-Large-Language-Models" class="headerlink" title="Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models"></a>Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models</h2><p><strong>Authors:Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</strong></p>
<p>Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains â€“ not just the final answers â€“ and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at <a target="_blank" rel="noopener" href="https://github.com/HyperPotatoNeo/RSA">https://github.com/HyperPotatoNeo/RSA</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•é€šè¿‡å¢åŠ ç”¨äºé¢„æµ‹çš„æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—é‡ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚æ¨ç†æ—¶é—´çš„è®¡ç®—å¯ä»¥é€šè¿‡é€‰æ‹©å¤šä¸ªç‹¬ç«‹è§£å†³æ–¹æ¡ˆè¿›è¡Œå¹¶è¡Œç¼©æ”¾ï¼Œæˆ–é€šè¿‡è‡ªæˆ‘å®Œå–„è¿›è¡Œé¡ºåºç¼©æ”¾ã€‚æˆ‘ä»¬æå‡ºäº†é€’å½’è‡ªèšåˆï¼ˆRSAï¼‰è¿™ä¸€æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œå®ƒå—åˆ°è¿›åŒ–æ–¹æ³•çš„å¯å‘ï¼Œç»“åˆäº†å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾çš„ä¼˜ç‚¹ã€‚RSAçš„æ¯ä¸€æ­¥éƒ½é€šè¿‡å¯¹å€™é€‰æ¨ç†é“¾çš„å­é›†è¿›è¡Œèšåˆæ¥ä¼˜åŒ–ä¸€ä¸ªå€™é€‰ç¾¤ä½“ï¼Œä»è€Œäº§ç”Ÿä¸€ç»„æ”¹è¿›çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆç„¶åè¢«ç”¨ä½œä¸‹ä¸€è½®çš„å€™é€‰æ± ã€‚RSAåˆ©ç”¨æ¨ç†é“¾ä¸­åµŒå…¥çš„ä¸°å¯Œä¿¡æ¯â€”â€”ä¸ä»…ä»…æ˜¯æœ€ç»ˆçš„ç­”æ¡ˆâ€”â€”å¹¶åˆ©ç”¨ä»ä¸åŒæ€è€ƒé“¾ä¸­éƒ¨åˆ†æ­£ç¡®çš„ä¸­é—´æ­¥éª¤è¿›è¡Œå¼•å¯¼ã€‚ç»éªŒè¡¨æ˜ï¼ŒRSAåœ¨å¢åŠ è®¡ç®—é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRSAä½¿Qwen3-4B-Instruct-2507èƒ½å¤Ÿåœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆåŒ…æ‹¬DeepSeek-R1å’Œo3-miniï¼ˆé«˜çº§ï¼‰ï¼‰ä¸­å–å¾—å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨AIME-25ã€HMMT-25ã€Reasoning Gymã€LiveCodeBench-v6å’ŒSuperGPQAä¸Šè¶…è¶Šäº†çº¯ç²¹çš„å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡ä¸€ç§æ–°å‹èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒæ¨¡å‹æ¥ç»„åˆè§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥è·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/HyperPotatoNeo/RSA">https://github.com/HyperPotatoNeo/RSA</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26626v1">PDF</a> 24 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>RSAï¼ˆé€’å½’è‡ªèšåˆï¼‰æ˜¯ä¸€ç§æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆå¹¶è¡Œå’Œé¡ºåºç¼©æ”¾çš„ä¼˜åŠ¿ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾ç‚¼å€™é€‰æ¨ç†é“¾å¹¶åˆ©ç”¨æ¨ç†é“¾ä¸­çš„ä¸°å¯Œä¿¡æ¯æ¥æé«˜æ€§èƒ½ï¼Œä½¿ä¸åŒæ€ç»´é“¾ä¸­çš„éƒ¨åˆ†æ­£ç¡®ä¸­é—´æ­¥éª¤èƒ½å¤Ÿäº’ç›¸å¼•å¯¼ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒRSAåœ¨å¢åŠ è®¡ç®—é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚RSAä½¿Qwen3-4B-Instruct-2507èƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šä¸å¤§å¾—å¤šçš„æ¨ç†æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶åœ¨ä¸€äº›ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¶Šäº†çº¯å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾ç­–ç•¥ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ç»„åˆè§£å†³æ–¹æ¡ˆï¼ŒRSAå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ›´å¤šè¯¦æƒ…å¯é€šè¿‡è®¿é—®å…¬å¼€ä»£ç é“¾æ¥è·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSAæ˜¯ä¸€ç§åŸºäºè¿›åŒ–æ–¹æ³•çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ï¼Œç»“åˆäº†å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾çš„ä¼˜åŠ¿ã€‚</li>
<li>RSAé€šè¿‡ç²¾ç‚¼å€™é€‰æ¨ç†é“¾å¹¶åˆ©ç”¨å…¶ä¸­çš„ä¸°å¯Œä¿¡æ¯æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>RSAå…è®¸éƒ¨åˆ†æ­£ç¡®çš„ä¸­é—´æ­¥éª¤åœ¨ä¸åŒæ€ç»´é“¾ä¸­äº’ç›¸å¼•å¯¼ï¼Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šï¼ŒRSAæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>RSAä½¿å¾—è¾ƒå°è§„æ¨¡çš„æ¨¡å‹å¦‚Qwen3-4B-Instruct-2507èƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šä¸å¤§å¾—å¤šçš„æ¨ç†æ¨¡å‹ç«äº‰ã€‚</li>
<li>RSAåœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†çº¯å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdc4774d8546712cbdd72d31c057582e" align="middle">
<img src="https://picx.zhimg.com/v2-ba8d094778b884fb914c501034be57f7" align="middle">
<img src="https://picx.zhimg.com/v2-9cca95255e96310eaaba4068e3377bd4" align="middle">
<img src="https://picx.zhimg.com/v2-4b90768f427a647c5243156305c14f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-364de5b22ab25d78dfc7c43dc6a64f58" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Linking-Process-to-Outcome-Conditional-Reward-Modeling-for-LLM-Reasoning"><a href="#Linking-Process-to-Outcome-Conditional-Reward-Modeling-for-LLM-Reasoning" class="headerlink" title="Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning"></a>Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning</h2><p><strong>Authors:Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren</strong></p>
<p>Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth. </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œé€šè¿‡æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€æ­¥æ¨ç†ä»¥è·å–æœ€ç»ˆç­”æ¡ˆï¼Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMè¦ä¹ˆå­¤ç«‹åœ°å¤„ç†æ¯ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ— æ³•æ•æ‰æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œè¦ä¹ˆåœ¨å°†è¿‡ç¨‹å¥–åŠ±ä¸æœ€ç»ˆç»“æœå¯¹é½æ–¹é¢é‡åˆ°å›°éš¾ã€‚å› æ­¤ï¼Œå¥–åŠ±ä¿¡å·ä¸å°Šé‡åºåˆ—æ¨ç†ä¸­çš„æ—¶é—´å› æœå…³ç³»ï¼Œå¹¶é¢ä¸´æ¨¡ç³Šçš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚è¿™äº›å±€é™æ€§ä½¿å¾—ä¸‹æ¸¸æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26578v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰PRMåœ¨å¤„ç†æ¨ç†æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»å’Œå¯¹æœ€ç»ˆç»“æœçš„å¥–åŠ±å¯¹é½æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºæ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ï¼Œå°†LLMæ¨ç†è§†ä¸ºå¯¼å‘æ­£ç¡®ç­”æ¡ˆçš„æš‚æ—¶è¿‡ç¨‹ã€‚CRMçš„å¥–åŠ±ä¸ä»…å–å†³äºå…ˆå‰çš„æ­¥éª¤ï¼Œè¿˜æ˜ç¡®ä¸æ¨ç†è½¨è¿¹çš„æœ€ç»ˆç»“æœç›¸å…³è”ï¼Œä»è€Œæ•æ‰æ¨ç†æ­¥éª¤ä¹‹é—´çš„å› æœå…³ç³»ï¼Œè§£å†³ä¿¡ç”¨åˆ†é…æ¨¡ç³Šé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒCRMåœ¨å¤šç§é‡‡æ ·æ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ›´ç¨³å¥ä¸”èƒ½ç¨³å®šæå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æœ‰åŠ©äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰PRMåœ¨å¤„ç†æ¨ç†æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»å’Œå¯¹æœ€ç»ˆç»“æœçš„å¥–åŠ±å¯¹é½æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>æ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰å°†LLMæ¨ç†è§†ä¸ºå¯¼å‘æ­£ç¡®ç­”æ¡ˆçš„æš‚æ—¶è¿‡ç¨‹ã€‚</li>
<li>CRMé€šè¿‡æ¡ä»¶æ¦‚ç‡è§„åˆ™æ•æ‰æ¨ç†æ­¥éª¤é—´çš„å› æœå…³ç³»ï¼Œå¹¶è§£å†³ä¿¡ç”¨åˆ†é…æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>CRMé€šè¿‡ä¸€è‡´çš„æ¦‚ç‡å»ºæ¨¡ï¼Œä½¿å¥–åŠ±æ›´å¯é åœ°è¿›è¡Œè·¨æ ·æœ¬æ¯”è¾ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCRMåœ¨å¤šç§é‡‡æ ·æ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ›´ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d59a146df525ec2e2bb97853be64d68" align="middle">
<img src="https://picx.zhimg.com/v2-e6dc68c69b3285c265f4825a3c070a59" align="middle">
<img src="https://pic1.zhimg.com/v2-f97dd050fb402ed4de690e006eafb52b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark"><a href="#Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark" class="headerlink" title="Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark"></a>Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark</h2><p><strong>Authors:Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, YaÃ¯r Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng</strong></p>
<p>While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced â€œcritical pointâ€), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular &amp; optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools. </p>
<blockquote>
<p>éšç€æ‹¥æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢å–å¾—å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬èƒ½å¦æœ‰æ•ˆåº”å¯¹å‰æ²¿ç‰©ç†ç ”ç©¶ä¸­é‡åˆ°çš„å¤æ‚ã€å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Ÿå…³é”®çš„æ˜¯ï¼Œç‰©ç†å­¦å®¶å¸Œæœ›LLMsè¾…åŠ©å®Œæˆå“ªäº›æ¨ç†ä»»åŠ¡ï¼Ÿä¸ºäº†è§£ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CritPtï¼ˆåˆ©ç”¨ç»¼åˆæ€ç»´è¿›è¡Œå¤æ‚ç ”ç©¶â€”â€”ç‰©ç†æµ‹è¯•ï¼Œç®€ç§°â€œä¸´ç•Œç‚¹â€ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æœªå‘è¡¨çš„ã€ç ”ç©¶çº§åˆ«çš„æ¨ç†ä»»åŠ¡è®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¿æ³›è¦†ç›–äº†ç°ä»£ç‰©ç†ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡èšæ€ç‰©ç†ã€é‡å­åŠ›å­¦ã€åŸå­ã€åˆ†å­ä¸å…‰å­¦ç‰©ç†ã€å¤©ä½“ç‰©ç†ã€é«˜èƒ½ç‰©ç†ã€æ•°å­¦ç‰©ç†ã€ç»Ÿè®¡ç‰©ç†ã€æ ¸ç‰©ç†ã€éçº¿æ€§åŠ¨åŠ›å­¦ã€æµä½“åŠ›å­¦å’Œç”Ÿç‰©ç‰©ç†å­¦ã€‚CritPtç”±71ä¸ªå¤åˆç ”ç©¶æŒ‘æˆ˜æ„æˆï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå…¥é—¨çº§åˆ«çš„å…¨å°ºå¯¸ç ”ç©¶é¡¹ç›®ï¼ŒåŒæ—¶åˆ†è§£ä¸º190ä¸ªæ›´ç®€å•çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ï¼Œä»¥è·å–æ›´ç²¾ç»†çš„è§è§£ã€‚æ‰€æœ‰é—®é¢˜å‡ç”±50å¤šåæ´»è·ƒçš„ç‰©ç†ç ”ç©¶è€…æ ¹æ®ä»–ä»¬è‡ªå·±çš„ç ”ç©¶å…¨æ–°åˆ›ä½œã€‚æ¯ä¸ªé—®é¢˜éƒ½ç»è¿‡æ‰‹å·¥ç­›é€‰ï¼Œä»¥å¾—å‡ºä¸æ˜“çŒœæµ‹ä¸”å¯æœºå™¨éªŒè¯çš„ç­”æ¡ˆï¼Œå¹¶ç”±é’ˆå¯¹é«˜çº§ç‰©ç†ç‰¹å®šè¾“å‡ºæ ¼å¼è¿›è¡Œå¤§é‡å®šåˆ¶çš„è‡ªåŠ¨è¯„åˆ†ç®¡é“è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨å•ç‹¬çš„æ£€æŸ¥ç‚¹ä¸Šæ˜¾ç¤ºå‡ºåˆæ­¥çš„å¸Œæœ›ï¼Œä½†å®ƒä»¬ä»ç„¶è¿œè¿œä¸èƒ½å¯é åœ°è§£å†³å…¨å°ºå¯¸çš„ç ”ç©¶æŒ‘æˆ˜ï¼šåŸºç¡€æ¨¡å‹ä¸­çš„æœ€ä½³å¹³å‡å‡†ç¡®ç‡ä»…ä¸ºGPT-5ï¼ˆé«˜çº§ç‰ˆï¼‰è¾¾åˆ°çš„4.0%ï¼Œé…å¤‡ç¼–ç å·¥å…·æ—¶å‡†ç¡®ç‡é€‚åº¦ä¸Šå‡è‡³çº¦10%ã€‚é€šè¿‡CritPtæä¾›çš„ç°å®ä¸”æ ‡å‡†åŒ–çš„è¯„ä¼°ï¼Œæˆ‘ä»¬çªå‡ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸ç°å®ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ï¼Œä¸ºå¼€å‘ç§‘å­¦åŸºç¡€çš„AIå·¥å…·æä¾›äº†æŒ‡å¯¼åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26574v1">PDF</a> 39 pages, 6 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢çš„èƒ½åŠ›å·²ç»å¾—åˆ°è¿…é€Ÿæå‡ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚ã€å¼€æ”¾æ€§çš„ç‰©ç†ç ”ç©¶æŒ‘æˆ˜æ–¹é¢èƒ½å¦è¿›è¡Œæœ‰æ•ˆæ¨ç†ä»å­˜åœ¨ç–‘é—®ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº†CritPtåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«71ä¸ªæ¨¡æ‹Ÿå®Œæ•´ç ”ç©¶é¡¹ç›®çš„å¤åˆæŒ‘æˆ˜ï¼Œæ—¨åœ¨æµ‹è¯•LLMsåœ¨æœªå…¬å¼€çš„ã€ç ”ç©¶çº§åˆ«çš„æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å°½ç®¡å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨å­¤ç«‹çš„æ£€æŸ¥ç‚¹ä¸Šæœ‰æ—©æœŸè¡¨ç°ï¼Œä½†åœ¨è§£å†³å…¨é¢çš„ç ”ç©¶è§„æ¨¡æŒ‘æˆ˜æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚å› æ­¤ï¼Œå½“å‰æ¨¡å‹çš„èƒ½åŠ›ä¸çœŸå®çš„ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMså·²æ˜¾ç¤ºå‡ºåœ¨é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢çš„å¿«é€Ÿè¿›æ­¥ã€‚</li>
<li>CritPtæ˜¯é¦–ä¸ªé’ˆå¯¹LLMsçš„æµ‹è¯•åŸºå‡†ï¼Œæ—¨åœ¨æµ‹è¯•å…¶åœ¨æœªå‘è¡¨çš„ç ”ç©¶çº§åˆ«ç‰©ç†æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>CritPtåŒ…å«æ¨¡æ‹Ÿå®Œæ•´ç ”ç©¶é¡¹ç›®çš„å¤åˆæŒ‘æˆ˜å’Œæ›´ç²¾ç»†çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨è§£å†³å…¨é¢çš„ç‰©ç†ç ”ç©¶æŒ‘æˆ˜æ–¹é¢ä»æœ‰é™ï¼Œå¹³å‡å‡†ç¡®ç‡ä»…ä¸º4%ã€‚</li>
<li>CritPtæä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œå¼•å¯¼AIå·¥å…·ç§‘å­¦å‘å±•çš„åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c8c97a57af2545ca836897bec26ad1c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-785387efd68ebd5cbc444841d911aad2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbd6e27faed92b6a96b6a925c78681b" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation"><a href="#Stable-Cinemetrics-Structured-Taxonomy-and-Evaluation-for-Professional-Video-Generation" class="headerlink" title="Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional   Video Generation"></a>Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional   Video Generation</h2><p><strong>Authors:Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</strong></p>
<p>Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä½¿ç”¨æˆ·å¯ä»¥æ ¹æ®æç¤ºè¿›è¡Œé«˜ä¿çœŸè§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰ä¸“ä¸šè§†é¢‘ç”Ÿæˆçš„å¤æ‚æ€§å’Œè¦æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨³å®šç”µå½±æŒ‡æ ‡ï¼ˆStable Cinemetricsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå°†ç”µå½±åˆ¶ä½œçš„æ§åˆ¶æ­£è§„åŒ–ä¸ºå››ä¸ªåˆ†ç¦»ã€åˆ†å±‚çš„åˆ†ç±»æ³•ï¼šè®¾ç½®ã€äº‹ä»¶ã€ç…§æ˜å’Œæ‘„åƒæœºã€‚è¿™äº›åˆ†ç±»æ³•å…±åŒå®šä¹‰äº†76ä¸ªåŸºäºè¡Œä¸šå®è·µçš„ç²¾ç»†æ§åˆ¶èŠ‚ç‚¹ã€‚ä½¿ç”¨è¿™äº›åˆ†ç±»æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸ä¸“ä¸šç”¨ä¾‹å¯¹é½çš„æç¤ºåŸºå‡†æµ‹è¯•ï¼Œå¹¶å¼€å‘äº†ç”¨äºæç¤ºåˆ†ç±»å’Œé—®é¢˜ç”Ÿæˆçš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿå¯¹æ¯ä¸ªæ§åˆ¶ç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„ä¼°ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œæ¶‰åŠ10å¤šä¸ªæ¨¡å‹å’Œ2ä¸‡å¤šä¸ªè§†é¢‘ï¼Œç”±80å¤šåç”µå½±ä¸“ä¸šäººå£«è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬çš„åˆ†æï¼Œæ— è®ºæ˜¯ç²—ç•¥çš„è¿˜æ˜¯ç²¾ç»†çš„ï¼Œéƒ½è¡¨æ˜å³ä½¿æ˜¯æœ€å¼ºå¤§çš„å½“å‰æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾çš„å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹ä»¶å’Œä¸æ‘„åƒæœºç›¸å…³çš„æ§åˆ¶æ–¹é¢ã€‚ä¸ºäº†è¿›è¡Œå¯è¯„ä¼°çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ä¸“å®¶æ³¨é‡Šå¯¹é½çš„è§†å¬è¯­è¨€æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬åŸºçº¿ã€‚SCINEæ˜¯é¦–æ¬¡åœ¨ä¸“ä¸šè§†é¢‘ç”Ÿæˆé¢†åŸŸå®šä½è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œå¼•å…¥ä»¥ç”µå½±æ§åˆ¶ä¸ºä¸­å¿ƒçš„åˆ†ç±»æ³•ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–è¯„ä¼°ç®¡é“å’Œè¯¦ç»†åˆ†ææ¥æ”¯æŒå®ƒä»¬ï¼Œä»¥æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26555v1">PDF</a> NeurIPS 2025. Project Page : <a target="_blank" rel="noopener" href="https://stable-cinemetrics.github.io/">https://stable-cinemetrics.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Stable Cinemetricsè¿™ä¸€ç»“æ„åŒ–è¯„ä¼°æ¡†æ¶çš„æ¨å‡ºï¼Œè¯¥æ¡†æ¶æ­£å¼å°†ç”µå½±åˆ¶ä½œæ§åˆ¶åˆ†è§£æˆå››ä¸ªç‹¬ç«‹ã€åˆ†å±‚çš„åˆ†ç±»ï¼šè®¾ç½®ã€äº‹ä»¶ã€ç…§æ˜å’Œæ‘„åƒæœºã€‚é€šè¿‡è¿™å››å¤§åˆ†ç±»ï¼Œå®šä¹‰äº†åŸºäºè¡Œä¸šå®è·µçš„76ä¸ªç²¾ç»†æ§åˆ¶èŠ‚ç‚¹ã€‚åŒæ—¶æ„å»ºäº†å¯¹é½ä¸“ä¸šç”¨ä¾‹çš„åŸºå‡†æç¤ºï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æç¤ºåˆ†ç±»å’Œé—®é¢˜ç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿç‹¬ç«‹è¯„ä¼°æ¯ä¸ªæ§åˆ¶ç»´åº¦ã€‚è¿›è¡Œäº†ä¸€é¡¹æ¶µç›–è¶…è¿‡å…«ä¸‡åä¸“ä¸šå½±è§†äººå‘˜çš„è¶…è¿‡åä¸‡ä¸ªè§†é¢‘çš„å¤§è§„æ¨¡äººç±»ç ”ç©¶ã€‚åˆ†æè¡¨æ˜ï¼Œå³ä½¿åœ¨äº‹ä»¶å’Œæ‘„åƒæœºç›¸å…³æ§åˆ¶æ–¹é¢ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜æ˜¾å·®è·ã€‚ä¸ºäº†è¿›è¡Œå¯è¯„ä¼°çš„è¯„ä¼°ï¼Œè®­ç»ƒäº†ä¸€ç§ä¸ä¸“å®¶æ³¨é‡Šå¯¹é½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¶…è¿‡äº†ç°æœ‰çš„é›¶æ ·æœ¬åŸºçº¿ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–‡ç‡å…ˆå°†ä¸“ä¸šè§†é¢‘ç”Ÿæˆå®šä½åœ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¤§èƒŒæ™¯ä¸‹ï¼Œæ¨å‡ºäº†å›´ç»•ç”µå½±æ§åˆ¶çš„åˆ†ç±»æ”¯æŒå®ƒä»¬å¹¶å…·æœ‰ç»“æ„åŒ–çš„è¯„ä¼°æµç¨‹å’Œè¯¦ç»†åˆ†æä»¥æŒ‡å¯¼æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Stable Cinemetricsæ¡†æ¶ä¸ºä¸“ä¸šè§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ä¸ªç»“æ„åŒ–è¯„ä¼°æ–¹æ³•ï¼Œæ¶µç›–è®¾ç½®ã€äº‹ä»¶ã€ç…§æ˜å’Œæ‘„åƒæœºå››å¤§å±‚æ¬¡åˆ†ç±»ã€‚</li>
<li>å®šä¹‰äº†ä¸€ä¸ªåŒ…å«76ä¸ªæ§åˆ¶èŠ‚ç‚¹çš„åŸºå‡†æç¤ºé›†ï¼Œåæ˜ äº†è¡Œä¸šå®è·µã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“æ¥ç‹¬ç«‹è¯„ä¼°æ¯ä¸ªæ§åˆ¶ç»´åº¦å¹¶è¿›è¡Œæç¤ºåˆ†ç±»å’Œé—®é¢˜ç”Ÿæˆã€‚</li>
<li>è¿›è¡Œçš„å¤§è§„æ¨¡äººç±»ç ”ç©¶è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ä¸“ä¸šè§†é¢‘ç”Ÿæˆé¢†åŸŸå­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹ä»¶å’Œæ‘„åƒæœºç›¸å…³æ§åˆ¶æ–¹é¢ã€‚</li>
<li>è®­ç»ƒäº†ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å™¨ï¼Œä¸ä¸“å®¶æ³¨é‡Šå¯¹é½å¹¶è¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬åŸºçº¿ã€‚</li>
<li>SCINEæ˜¯é¦–ä¸ªå°†ä¸“ä¸šè§†é¢‘ç”Ÿæˆå®šä½åœ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒŒæ™¯ä¸‹çš„æ–¹æ³•ï¼Œæ¨å‡ºäº†ä¸€ç³»åˆ—åŸºäºç”µå½±æ§åˆ¶çš„åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76d4f767e8454f67ed8cc148db329ff7" align="middle">
<img src="https://picx.zhimg.com/v2-9e43591198047df0eaf1cbcd71215ce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6abe0115d4bdfdd3816c64411651985" align="middle">
<img src="https://picx.zhimg.com/v2-061f14b17109f04507df7890e864d1a1" align="middle">
<img src="https://picx.zhimg.com/v2-948da7c81819225fb51b0594c48150b5" align="middle">
<img src="https://picx.zhimg.com/v2-27c58c722aac01d7a7364aaa4a35b7b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Verified-Code-Reasoning-by-LLMs"><a href="#Towards-Verified-Code-Reasoning-by-LLMs" class="headerlink" title="Towards Verified Code Reasoning by LLMs"></a>Towards Verified Code Reasoning by LLMs</h2><p><strong>Authors:Meghana Sistla, Gogul Balakrishnan, Pat Rondon, JosÃ© Cambronero, Michele Tufano, Satish Chandra</strong></p>
<p>While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).   As a result of this lack of trustworthiness, the agentâ€™s answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agentâ€™s response and, subsequently, using formal verification and program analysis tools to verify the agentâ€™s reasoning steps.   We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agentâ€™s reasoning on 13&#x2F;20 examples, and for the program equivalence queries, the formal verification step successfully caught 6&#x2F;8 incorrect judgments made by the agent. </p>
<blockquote>
<p>åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½ä»£ç†è™½ç„¶èƒ½å¤Ÿåº”å¯¹å„ç§ä»£ç æ¨ç†é—®é¢˜ï¼Œä½†ç­”æ¡ˆå¹¶ä¸æ€»æ˜¯æ­£ç¡®ã€‚è¿™å¯¼è‡´åœ¨éœ€è¦é«˜ç²¾åº¦çš„åœºæ™¯ä¸­ï¼Œæ™ºèƒ½ä»£ç†æ— æ³•å‘æŒ¥åº”æœ‰çš„ä½œç”¨ï¼Œä¾‹å¦‚åœ¨ï¼ˆ1ï¼‰å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆç†è§£æ–°çš„ä»£ç åº“ã€ï¼ˆ2ï¼‰åœ¨ä»£ç å®¡æŸ¥æœŸé—´å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆä»¥åŠåœ¨ï¼ˆ3ï¼‰ç¡®ä¿ç”±è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç³»ç»Ÿç”Ÿæˆçš„ä»£ç æ»¡è¶³æŸäº›è¦æ±‚ï¼ˆä¾‹å¦‚ä¿®å¤é”™è¯¯ã€æé«˜å¯è¯»æ€§ã€å®ç°åŠŸèƒ½ï¼‰ã€‚ç”±äºè¿™ç§ç¼ºä¹å¯ä¿¡åº¦çš„é—®é¢˜ï¼Œéœ€è¦åœ¨ä¿¡ä»»æ™ºèƒ½ä»£ç†çš„ç­”æ¡ˆä¹‹å‰æ‰‹åŠ¨éªŒè¯å®ƒä»¬ã€‚æ‰‹åŠ¨ç¡®è®¤æ¥è‡ªä»£ç æ¨ç†ä»£ç†çš„å“åº”éœ€è¦äººåŠ›ï¼Œå¹¶å¯èƒ½å¯¼è‡´å¼€å‘äººå‘˜ç”Ÿäº§ç‡é™ä½ï¼Œä»è€Œå‰Šå¼±äº†ä»£ç†çš„è¾…åŠ©æ•ˆç›Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§é€šè¿‡éªŒè¯å…¶æ¨ç†æ­¥éª¤æ¥è‡ªåŠ¨éªŒè¯ä»£ç æ¨ç†ä»£ç†æä¾›çš„ç­”æ¡ˆçš„æ–¹æ³•ã€‚åœ¨å¾ˆé«˜çš„å±‚æ¬¡ä¸Šï¼Œè¯¥æ–¹æ³•ç”±æå–ä»£ç†å“åº”çš„æ­£å¼è¡¨ç¤ºå½¢å¼å¼€å§‹ï¼Œéšåä½¿ç”¨å½¢å¼åŒ–éªŒè¯å’Œç¨‹åºåˆ†æå·¥å…·æ¥éªŒè¯ä»£ç†çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬å°†æ­¤æ–¹æ³•åº”ç”¨äºä¸€ç»„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…æ‹¬ç”±æ£€æŸ¥å™¨æ£€æµ‹åˆ°çš„20ä¸ªæœªåˆå§‹åŒ–å˜é‡é”™è¯¯å’Œ20ä¸ªç¨‹åºç­‰ä»·æŸ¥è¯¢ã€‚å¯¹äºæœªåˆå§‹åŒ–å˜é‡é”™è¯¯ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤èƒ½å¤ŸéªŒè¯ä»£ç†å¯¹å…¶ä¸­13ä¸ªæ¡ˆä¾‹çš„æ¨ç†ï¼›è€Œå¯¹äºç¨‹åºç­‰ä»·æŸ¥è¯¢ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤æˆåŠŸæ•æ‰åˆ°äº†ä»£ç†ä½œå‡ºçš„8ä¸ªæ¡ˆä¾‹ä¸­ä¸æ­£ç¡®çš„åˆ¤æ–­ä¸­çš„6ä¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26546v1">PDF</a> 43 pages</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†èƒ½å¤Ÿå¤„ç†å¤šç§ä»£ç æ¨ç†é—®é¢˜ï¼Œä½†ç­”æ¡ˆå¹¶ä¸æ€»æ˜¯æ­£ç¡®ã€‚è¿™é™åˆ¶äº†ä»£ç†åœ¨éœ€è¦é«˜ç²¾ç¡®åº¦åœºæ™¯çš„åº”ç”¨ï¼Œå¦‚å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆç†è§£æ–°ä»£ç åŸºç¡€ã€å‚ä¸ä»£ç å®¡æŸ¥ä¼šè¯ä»¥åŠç¡®ä¿è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç³»ç»Ÿç”Ÿæˆçš„ä»£ç ç¬¦åˆè¦æ±‚ã€‚å› æ­¤ï¼Œéœ€è¦å¯¹ä»£ç†çš„ç­”æ¡ˆè¿›è¡Œæ‰‹åŠ¨éªŒè¯ï¼Œè¿™ä¼šæ¶ˆè€—äººåŠ›å¹¶å¯èƒ½å¯¼è‡´å¼€å‘æ•ˆç‡é™ä½ï¼Œå‰Šå¼±ä»£ç†çš„ååŠ©æ•ˆç›Šã€‚æœ¬æ–‡æè¿°äº†ä¸€ç§é€šè¿‡éªŒè¯å…¶æ¨ç†æ­¥éª¤æ¥è‡ªåŠ¨éªŒè¯ä»£ç æ¨ç†ä»£ç†ç­”æ¡ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æå–ä»£ç†å“åº”çš„å½¢å¼è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å½¢å¼åŒ–éªŒè¯å’Œç¨‹åºåˆ†æå·¥å…·æ¥éªŒè¯ä»£ç†çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºä¸€ç»„æœªåˆå§‹åŒ–å˜é‡é”™è¯¯çš„åŸºå‡†æµ‹è¯•é›†å’Œä¸€ç»„ç¨‹åºç­‰ä»·æŸ¥è¯¢ã€‚å¯¹äºæœªåˆå§‹åŒ–å˜é‡é”™è¯¯ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤èƒ½å¤Ÿåœ¨13&#x2F;20çš„ç¤ºä¾‹ä¸­éªŒè¯ä»£ç†çš„æ¨ç†ï¼›å¯¹äºç¨‹åºç­‰ä»·æŸ¥è¯¢ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤æˆåŠŸæ•è·äº†ä»£ç†åšå‡ºçš„6&#x2F;8ä¸ªé”™è¯¯åˆ¤æ–­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLM-based agents can handle various code reasoning questions but answers may not always be accurate.</li>
<li>Inaccuracy in agent answers requires manual verification, leading to reduced developer productivity.</li>
<li>This paper proposes a method to automatically validate the answers of a code reasoning agent by verifying its reasoning steps.</li>
<li>Formal verification and program analysis tools are used in this method.</li>
<li>The method was tested on a benchmark set of uninitialized variable errors and program equivalence queries.</li>
<li>For uninitialized variable errors, the method successfully validated the agentâ€™s reasoning for 13&#x2F;20 examples.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e9b49ce41fc23266b3d77e4cfc8184e" align="middle">
<img src="https://picx.zhimg.com/v2-77841954c65f0980ce26fce1676f1c63" align="middle">
<img src="https://picx.zhimg.com/v2-38fcd48d296c4218694ac84ab3c77884" align="middle">
<img src="https://pica.zhimg.com/v2-257169335ab6fde266106d29758c44b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-916f62967da2d25cf24ecb40aa82f2cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea01157cbec1e51a82f7a2e9728f7bf4" align="middle">
<img src="https://picx.zhimg.com/v2-9b23fe611dd97ae7347bc00e0b399eb1" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap"><a href="#Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap" class="headerlink" title="Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap"></a>Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap</h2><p><strong>Authors:Yueqian Lin, Zhengmian Hu, Qinsi Wang, Yudong Liu, Hengfan Zhang, Jayakumar Subramanian, Nikos Vlassis, Hai Helen Li, Yiran Chen</strong></p>
<p>We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing â€œthinking timeâ€ yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding&#x2F;consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†è¯­éŸ³æ¨ç†èƒ½åŠ›è¯„ä¼°ï¼ˆVERAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å®æ—¶å¯¹è¯çº¦æŸä¸‹è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ã€‚VERAåŒ…å«2931ä¸ªæœ¬åœ°è¯­éŸ³ç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µå–è‡ªæ—¢å®šçš„æ–‡æœ¬åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ†ä¸ºäº”ä¸ªèµ›é“ï¼ˆæ•°å­¦ã€ç½‘ç»œã€ç§‘å­¦ã€é•¿æ–‡æœ¬ã€äº‹å®ï¼‰ã€‚æ¯ä¸ªé¡¹ç›®éƒ½é€‚åº”è¯­éŸ³äº¤äº’ï¼ŒåŒæ—¶ä¿æŒæ¨ç†éš¾åº¦ã€‚VERAèƒ½å¤Ÿåœ¨æ¨¡å‹å®¶æ—å†…è¿›è¡Œç›´æ¥æ–‡æœ¬è¯­éŸ³æ¯”è¾ƒï¼Œå¹¶æ”¯æŒåˆ†ææ¶æ„é€‰æ‹©å¦‚ä½•å½±å“å¯é æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ä¸ªå½“ä»£è¯­éŸ³ç³»ç»Ÿä»¥åŠå¼ºå¤§çš„æ–‡æœ¬åŸºå‡†æµ‹è¯•ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´ä¸”æ˜¾è‘—çš„æ¨¡æ€å·®è·ï¼šåœ¨æ•°å­¦ç«èµ›ä¸­ï¼Œé¢†å…ˆçš„æ–‡æœ¬æ¨¡å‹è¾¾åˆ°74.8%çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶è¯­éŸ³å¯¹åº”æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º6.1%ï¼›è·¨èµ›é“å®è§‚å¹³å‡ï¼Œæœ€ä½³æ–‡æœ¬æ¨¡å‹è¾¾åˆ°54.0%ï¼Œè€Œè¯­éŸ³æ¨¡å‹ä»…ä¸º11.3%ã€‚å»¶è¿Ÿ-å‡†ç¡®æ€§åˆ†ææ˜¾ç¤ºäº†ä¸€ä¸ªä½å»¶è¿Ÿå¹³å°ï¼Œåœ¨æ­¤å¹³å°ä¸Šå¿«é€Ÿè¯­éŸ³ç³»ç»Ÿçš„å‡†ç¡®ç‡çº¦ä¸º10%ï¼Œè€Œè¦è¾¾åˆ°æ–‡æœ¬æ€§èƒ½åˆ™éœ€è¦ç‰ºç‰²å®æ—¶äº¤äº’ã€‚è¯Šæ–­å®éªŒè¡¨æ˜ï¼Œå¸¸è§çš„ç¼“è§£æªæ–½å¹¶ä¸è¶³å¤Ÿã€‚å¢åŠ â€œæ€è€ƒæ—¶é—´â€åªä¼šå¸¦æ¥å¾®ä¸è¶³é“çš„æ”¶ç›Šï¼›ä¸€ä¸ªåˆ†ç¦»çš„çº§è”ç³»ç»Ÿï¼Œå°†æ¨ç†ä¸å™è¿°åˆ†å¼€ï¼Œå¯ä»¥æé«˜å‡†ç¡®æ€§ï¼Œä½†ä»è¿œè¿œè½åäºæ–‡æœ¬ï¼Œå¹¶å¼•å…¥å…¸å‹çš„æ¥åœ°&#x2F;ä¸€è‡´æ€§é”™è¯¯ã€‚å¤±è´¥åˆ†æè¿›ä¸€æ­¥æ˜¾ç¤ºäº†åŸç”Ÿæµåª’ä½“ã€ç«¯åˆ°ç«¯å’Œçº§è”è®¾è®¡ä¹‹é—´çš„ä¸åŒé”™è¯¯ç‰¹å¾ã€‚VERAä¸ºä»æ€è€ƒåˆ°è¯´è¯è§£è€¦çš„æ¶æ„æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„æµ‹è¯•å¹³å°å’Œæœ‰é’ˆå¯¹æ€§çš„è¯Šæ–­å·¥å…·ï¼Œä¸ºå‘æ—¢æµç•…åˆå¯é æ¨ç†çš„å®æ—¶è¯­éŸ³åŠ©æ‰‹å‘å±•æä¾›äº†æœ‰åŸåˆ™çš„è¡¡é‡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26542v1">PDF</a> Code and data available at <a target="_blank" rel="noopener" href="https://github.com/linyueqian/VERA">https://github.com/linyueqian/VERA</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å®æ—¶å¯¹è¯çš„é™åˆ¶ä¸‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVERAçš„è¯­éŸ³æ¨ç†èƒ½åŠ›è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«äº†äº”å¤§é¢†åŸŸã€å…±ä¸¤åƒä¹ç™¾ä¸‰åä¸€é›†çš„è¯­éŸ³åŸå§‹ç‰‡æ®µï¼Œå¯ä»¥ç”¨äºè¯„ä¼°è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æ–‡ç« åˆ†æäº†12ç§ç°ä»£è¯­éŸ³ç³»ç»ŸåŠå…¶ä¸é¡¶å°–æ–‡æœ¬æ¨¡å‹çš„å·®è·ï¼Œå‘ç°è¯­éŸ³æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ™®éè¾ƒå·®ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ä¸åŒæ¶æ„é€‰æ‹©å¯¹å¯é æ€§çš„å½±å“ï¼Œå¹¶æä¾›äº†é’ˆå¯¹æ¶æ„ä¼˜åŒ–çš„å»ºè®®å’Œè¯Šæ–­æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼ŒVERAæä¾›äº†ä¸€ä¸ªå¯é‡å¤ä½¿ç”¨çš„æµ‹è¯•å¹³å°ï¼Œä¸ºå¼€å‘æ—¢æµç•…åˆå…·å¤‡å®æ—¶æ¨ç†èƒ½åŠ›çš„è¯­éŸ³åŠ©æ‰‹æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VERAåŒ…å«äº”å¤§é¢†åŸŸã€å…±ä¸¤åƒä¹ç™¾ä¸‰åä¸€é›†çš„è¯­éŸ³åŸå§‹ç‰‡æ®µã€‚</li>
<li>è¯­éŸ³ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›æ™®éè¾ƒå·®ï¼Œä¸é¡¶å°–æ–‡æœ¬æ¨¡å‹å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>ä¸åŒæ¶æ„é€‰æ‹©ä¼šå½±å“è¯­éŸ³ç³»ç»Ÿçš„å¯é æ€§ã€‚</li>
<li>å¢åŠ â€œæ€è€ƒæ—¶é—´â€å¯¹æå‡è¯­éŸ³ç³»ç»Ÿæ€§èƒ½ä½œç”¨ç”šå¾®ã€‚</li>
<li>åˆ†ç¦»æ¨ç†å’Œå™è¿°çš„æ¶æ„èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†ä»æœªèƒ½è¾¾åˆ°æ–‡æœ¬æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c928f4b72727355ef4248023b1452c57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05f433408fe64cdd8d00120710857eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ef95ac35a88562545ef94075641310" align="middle">
<img src="https://picx.zhimg.com/v2-0f619f0b8ac1d9e26b96438d9c45c2eb" align="middle">
<img src="https://picx.zhimg.com/v2-eb95fd25fee79ff19ef189e065f78982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cefd7b8480dc50e6ad969eb8a76f801" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Transferable-Agentic-Knowledge-Graph-RAG-via-Reinforcement-Learning"><a href="#Efficient-and-Transferable-Agentic-Knowledge-Graph-RAG-via-Reinforcement-Learning" class="headerlink" title="Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning"></a>Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning</h2><p><strong>Authors:Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</strong></p>
<p>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Jinyeop3110/KG-R1">https://github.com/Jinyeop3110/KG-R1</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æŠ€æœ¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç»“æ„åŒ–çš„ã€å¯éªŒè¯çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç›¸ç»“åˆï¼Œä»¥å‡å°‘å¹»è§‰å¹¶æš´éœ²æ¨ç†ç—•è¿¹ã€‚ç„¶è€Œï¼Œè®¸å¤šKG-RAGç³»ç»Ÿç»„åˆäº†å¤šä¸ªLLMæ¨¡å—ï¼ˆä¾‹å¦‚è§„åˆ’ã€æ¨ç†å’Œå“åº”ï¼‰ï¼Œå¢åŠ äº†æ¨ç†æˆæœ¬å¹¶ä¸ç‰¹å®šçš„ç›®æ ‡çŸ¥è¯†å›¾è°±ç»‘å®šè¡Œä¸ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä»£ç†çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-R1ï¼‰æ¡†æ¶ã€‚KG-R1åˆ©ç”¨å•ä¸ªä»£ç†ä¸çŸ¥è¯†å›¾è°±ä½œä¸ºç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åœ¨æ¯ä¸€æ­¥è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„ä¿¡æ¯èå…¥å…¶æ¨ç†å’Œç”Ÿæˆä¸­ã€‚è¯¥è¿‡ç¨‹é€šè¿‡ç«¯åˆ°ç«¯çš„RLè¿›è¡Œä¼˜åŒ–ã€‚åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å—æ§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢æœ‰æ•ˆç‡ä¹Ÿæœ‰å¯è½¬ç§»æ€§ï¼šä½¿ç”¨Qwen-2.5-3Bï¼ŒKG-R1åœ¨æé«˜ç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆæ ‡è®°çš„æ•°é‡å°‘äºä¹‹å‰ä½¿ç”¨æ›´å¤§åŸºç¡€æ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹çš„å¤šæ¨¡å—å·¥ä½œæµç¨‹æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒKG-R1å³æ’å³ç”¨ï¼šè®­ç»ƒåï¼Œå®ƒåœ¨æ–°çš„çŸ¥è¯†å›¾è°±ä¸Šæ— éœ€ä¿®æ”¹å°±èƒ½ä¿æŒé«˜åº¦çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç‰¹æ€§ä½¿KG-R1æˆä¸ºç°å®ä¸–ç•Œéƒ¨ç½²ä¸­é¢‡æœ‰å‰é€”çš„KG-RAGæ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯å…¬å¼€è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/Jinyeop3110/KG-R1%E3%80%82">https://github.com/Jinyeop3110/KG-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26383v1">PDF</a> 10 pages, 5 figures. Submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKG-RAGï¼‰æŠ€æœ¯ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¯éªŒè¯çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œæ—¨åœ¨å‡å°‘è™šæ„å†…å®¹å¹¶å±•ç¤ºæ¨ç†è½¨è¿¹ã€‚é’ˆå¯¹ç°æœ‰KG-RAGç³»ç»Ÿå­˜åœ¨å¤šæ¨¡å—ã€æ¨ç†æˆæœ¬é«˜ä»¥åŠä¸ç‰¹å®šçŸ¥è¯†å›¾è°±ç»‘å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„KG-R1ä»£ç†æ¡†æ¶ã€‚KG-R1åˆ©ç”¨å•ä¸ªä»£ç†ä¸çŸ¥è¯†å›¾è°±ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ åœ¨æ¯ä¸€æ­¥è¿›è¡Œæ£€ç´¢ï¼Œå¹¶å°†å…¶çº³å…¥æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚è¯¥è¿‡ç¨‹é€šè¿‡ç«¯åˆ°ç«¯çš„RLè¿›è¡Œä¼˜åŒ–ã€‚åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒKG-R1æ—¢é«˜æ•ˆåˆå…·å¯è¿ç§»æ€§ï¼Œä½¿ç”¨Qwen-2.5-3Bæ¨¡å‹æé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œä¸”ç”Ÿæˆä»¤ç‰Œå°‘äºä½¿ç”¨æ›´å¤§åŸºç¡€æˆ–å¾®è°ƒæ¨¡å‹çš„å¤šæ¨¡å—å·¥ä½œæµç¨‹æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒKG-R1å®ç°äº†å³æ’å³ç”¨ï¼šè®­ç»ƒåï¼Œå®ƒåœ¨æ–°çš„çŸ¥è¯†å›¾è°±ä¸Šæ— éœ€ä¿®æ”¹å°±èƒ½ä¿æŒé«˜åº¦çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç‰¹æ€§ä½¿KG-R1æˆä¸ºç°å®ä¸–ç•Œéƒ¨ç½²çš„æœ‰å‰é€”çš„KG-RAGæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KG-RAGç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’ŒçŸ¥è¯†å›¾è°±ï¼Œæ—¨åœ¨æé«˜å‡†ç¡®æ€§å¹¶å±•ç¤ºæ¨ç†è½¨è¿¹ã€‚</li>
<li>ç°æœ‰KG-RAGç³»ç»Ÿå­˜åœ¨å¤šæ¨¡å—ã€é«˜æ¨ç†æˆæœ¬å’Œç‰¹å®šçŸ¥è¯†å›¾è°±ç»‘å®šçš„é—®é¢˜ã€‚</li>
<li>KG-R1æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚</li>
<li>KG-R1åˆ©ç”¨å•ä¸ªä»£ç†ä¸çŸ¥è¯†å›¾è°±äº¤äº’ï¼Œåœ¨æ¯ä¸€æ­¥è¿›è¡Œæ£€ç´¢å¹¶çº³å…¥æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>KG-R1åœ¨çŸ¥è¯†å›¾è°±é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºé«˜æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
<li>KG-R1æé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œå¹¶ç”Ÿæˆè¾ƒå°‘çš„ä»¤ç‰Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46103d3b48b9feb5aaecbe0cdd64b115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a57f2b36b5b6aa52b3aaa3dcce5bb2a1" align="middle">
<img src="https://picx.zhimg.com/v2-10ddfae4d76a7f98708a0cce9cae8d66" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MR-2-Bench-Going-Beyond-Matching-to-Reasoning-in-Multimodal-Retrieval"><a href="#MR-2-Bench-Going-Beyond-Matching-to-Reasoning-in-Multimodal-Retrieval" class="headerlink" title="MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval"></a>MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval</h2><p><strong>Authors:Junjie Zhou, Ze Liu, Lei Xiong, Jin-Ge Yao, Yueze Wang, Shitao Xiao, Fenfen Lin, Miguel Hu Chen, Zhicheng Dou, Siqi Bao, Defu Lian, Yongping Xiong, Zheng Liu</strong></p>
<p>Multimodal retrieval is becoming a crucial component of modern AI applications, yet its evaluation lags behind the demands of more realistic and challenging scenarios. Existing benchmarks primarily probe surface-level semantic correspondence (e.g., object-text matching) while failing to assess the deeper reasoning required to capture complex relationships between visual and textual information. To address this gap, we introduce MR$^2$-Bench, a reasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents the following critical values: 1) all tasks are reasoning-driven, going beyond shallow matching to effectively assess modelsâ€™ capacity for logical, spatial, and causal inference; 2) it features diverse multimodal data, such as natural images, diagrams, and visual puzzles, enabling comprehensive evaluation across content types; 3) it supports complex queries and documents containing multiple images and covers diverse retrieval scenarios, more accurately reflecting real-world applications. Our benchmark contains 1,309 curated queries, derived either from manual collection and annotation or from selective consolidation of public datasets. Despite achieving strong results on existing benchmarks, current state-of-the-art models still struggle on MR$^2$-Bench: for example, the leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but only 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the increased challenge posed by our benchmark and the pressing need for further advances in reasoning-intensive multimodal retrieval. The dataset and evaluation code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/VectorSpaceLab/MR2-Bench">https://github.com/VectorSpaceLab/MR2-Bench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ£€ç´¢å·²æˆä¸ºç°ä»£äººå·¥æ™ºèƒ½åº”ç”¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶è¯„ä¼°å´è½åäºæ›´ç°å®å’Œæœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯çš„éœ€æ±‚ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦æ¢ç´¢è¡¨é¢çº§çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡æ–‡æœ¬åŒ¹é…ï¼‰ï¼Œè€Œæ— æ³•è¯„ä¼°æ•è·è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´å¤æ‚å…³ç³»æ‰€éœ€çš„æ›´æ·±å±‚æ¬¡æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MR$^2$-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€æ£€ç´¢çš„æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ã€‚MR$^2$-Benchå…·æœ‰ä»¥ä¸‹å…³é”®ä»·å€¼ï¼š1ï¼‰æ‰€æœ‰ä»»åŠ¡å‡åŸºäºæ¨ç†é©±åŠ¨ï¼Œè¶…è¶Šæµ…å±‚åŒ¹é…ï¼Œæœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨é€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼›2ï¼‰å®ƒæ‹¥æœ‰å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¦‚è‡ªç„¶å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å„ç§å†…å®¹ç±»å‹çš„æ£€ç´¢ï¼›3ï¼‰å®ƒæ”¯æŒå¤æ‚çš„æŸ¥è¯¢å’ŒåŒ…å«å¤šä¸ªå›¾åƒçš„æ–‡æ¡£ï¼Œå¹¶è¦†ç›–å„ç§æ£€ç´¢åœºæ™¯ï¼Œæ›´èƒ½å‡†ç¡®åæ˜ å®é™…åº”ç”¨ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«1309ä¸ªç²¾é€‰æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢æ¥è‡ªæ‰‹åŠ¨æ”¶é›†å’Œæ³¨é‡Šï¼Œæˆ–æ˜¯ä»å…¬å…±æ•°æ®é›†çš„ç²¾é€‰æ•´åˆã€‚å°½ç®¡åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¼ºåŠ²çš„ç»“æœï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨MR$^2$-Benchä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼šä¾‹å¦‚ï¼Œé¢†å…ˆçš„Seed1.6-Embeddingæ¨¡å‹åœ¨MMEBä¸Šçš„Recall@1ä¸º77.78ï¼Œä½†åœ¨MR$^2$-Benchä¸Šä»…ä¸º9.91ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½å·®è·çªæ˜¾äº†æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ‰€å¸¦æ¥çš„å¢åŠ æŒ‘æˆ˜ï¼Œä»¥åŠæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€æ£€ç´¢çš„è¿«åˆ‡éœ€æ±‚ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/VectorSpaceLab/MR2-Bench%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/VectorSpaceLab/MR2-Benchä¸Šå…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26378v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤šåª’ä½“æ£€ç´¢åœ¨ç°ä»£AIåº”ç”¨ä¸­æˆä¸ºè‡³å…³é‡è¦çš„éƒ¨åˆ†ï¼Œç„¶è€Œå…¶è¯„ä¼°æ–¹å¼å´è½åäºå®é™…éœ€æ±‚ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦æµ‹è¯•è¡¨é¢çº§åˆ«çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¦‚ç‰©ä½“ä¸æ–‡æœ¬åŒ¹é…ç­‰ï¼Œè€Œæ— æ³•è¯„ä¼°æ·±åº¦æ¨ç†æ•æ‰è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†MR$^2$-Benchï¼Œä¸€ä¸ªç”¨äºå¤šåª’ä½“æ£€ç´¢çš„æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ã€‚å®ƒè¶…è¶Šæµ…å±‚æ¬¡çš„åŒ¹é…ä»»åŠ¡ï¼Œä¸“æ³¨äºé€»è¾‘ã€ç©ºé—´å’Œå› æœæ¨ç†çš„æµ‹è¯•ï¼›æ¶µç›–å¤šç§å¤šåª’ä½“æ•°æ®å¦‚è‡ªç„¶å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ç­‰ï¼›æ”¯æŒå¤æ‚æŸ¥è¯¢å’ŒåŒ…å«å¤šä¸ªå›¾åƒçš„æ–‡æ¡£æ£€ç´¢åœºæ™¯ï¼Œæ›´å‡†ç¡®åœ°åæ˜ å®é™…åº”ç”¨æƒ…å†µã€‚è™½ç„¶å½“å‰æœ€å…ˆè¿›æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨MR$^2$-Benchä¸Šè¡¨ç°æŒ£æ‰ã€‚å·¨å¤§æ€§èƒ½å·®è·å‡¸æ˜¾äº†æµ‹è¯•éœ€æ±‚æå‡ä¸å¤šåª’ä½“æ£€ç´¢è¿›æ­¥ä¹‹é—´çš„å·®è·ã€‚<strong>Key Takeaways</strong>:</p>
<ol>
<li>MR$^2$-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹å¤šåª’ä½“æ£€ç´¢çš„æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„è¡¨é¢çº§åˆ«è¯­ä¹‰å¯¹åº”æµ‹è¯•ï¼Œç€é‡äºè¯„ä¼°æ¨¡å‹æ•æ‰å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>MR$^2$-BenchåŒ…å«å¤šç§ç±»å‹çš„å¤šåª’ä½“æ•°æ®ï¼Œå¦‚è‡ªç„¶å›¾åƒã€å›¾è¡¨å’Œè§†è§‰è°œé¢˜ç­‰ã€‚</li>
<li>è¯¥æµ‹è¯•æ”¯æŒå¤æ‚æŸ¥è¯¢å’ŒåŒ…å«å¤šä¸ªå›¾åƒçš„æ–‡æ¡£æ£€ç´¢åœºæ™¯ï¼Œåæ˜ çœŸå®åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨MR$^2$-Benchä¸Šçš„è¡¨ç°è¡¨æ˜æ¨ç†èƒ½åŠ›è¿˜æœ‰å·¨å¤§çš„æå‡ç©ºé—´ã€‚</li>
<li>MR$^2$-Benchæä¾›äº†ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ ‡å‡†æ¥æ¯”è¾ƒå’Œä¼˜åŒ–å¤šåª’ä½“æ£€ç´¢æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b939a2d03e7de0be66982b1b0c4bd68" align="middle">
<img src="https://picx.zhimg.com/v2-71112689b52c496afc93bf113294cd0b" align="middle">
<img src="https://picx.zhimg.com/v2-b27931be4b8cd0b3c202e1467ad7ae85" align="middle">
<img src="https://picx.zhimg.com/v2-f273f5b9f1ffc2fe77f73aed49dbeb30.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SafeBehavior-Simulating-Human-Like-Multistage-Reasoning-to-Mitigate-Jailbreak-Attacks-in-Large-Language-Models"><a href="#SafeBehavior-Simulating-Human-Like-Multistage-Reasoning-to-Mitigate-Jailbreak-Attacks-in-Large-Language-Models" class="headerlink" title="SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate   Jailbreak Attacks in Large Language Models"></a>SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate   Jailbreak Attacks in Large Language Models</h2><p><strong>Authors:Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, Kaizhu Huang</strong></p>
<p>Large Language Models (LLMs) have achieved impressive performance across diverse natural language processing tasks, but their growing power also amplifies potential risks such as jailbreak attacks that circumvent built-in safety mechanisms. Existing defenses including input paraphrasing, multi step evaluation, and safety expert models often suffer from high computational costs, limited generalization, or rigid workflows that fail to detect subtle malicious intent embedded in complex contexts. Inspired by cognitive science findings on human decision making, we propose SafeBehavior, a novel hierarchical jailbreak defense mechanism that simulates the adaptive multistage reasoning process of humans. SafeBehavior decomposes safety evaluation into three stages: intention inference to detect obvious input risks, self introspection to assess generated responses and assign confidence based judgments, and self revision to adaptively rewrite uncertain outputs while preserving user intent and enforcing safety constraints. We extensively evaluate SafeBehavior against five representative jailbreak attack types including optimization based, contextual manipulation, and prompt based attacks and compare it with seven state of the art defense baselines. Experimental results show that SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios, offering an efficient and human inspired approach to safeguarding LLMs against jailbreak attempts. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒç§ç±»çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ï¼Œä½†å®ƒä»¬æ—¥ç›Šå¢å¼ºçš„èƒ½åŠ›ä¹Ÿæ”¾å¤§äº†æ½œåœ¨é£é™©ï¼Œæ¯”å¦‚ç»•è¿‡å†…ç½®å®‰å…¨æœºåˆ¶çš„è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰é˜²å¾¡æªæ–½ï¼ŒåŒ…æ‹¬è¾“å…¥åŒä¹‰æ›¿æ¢ã€å¤šæ­¥è¯„ä¼°å’Œå®‰å…¨ä¸“å®¶æ¨¡å‹ç­‰ï¼Œå¸¸å¸¸é¢ä¸´é«˜è®¡ç®—æˆæœ¬ã€æœ‰é™çš„æ³›åŒ–èƒ½åŠ›æˆ–åƒµç¡¬çš„å·¥ä½œæµç¨‹ç­‰é—®é¢˜ï¼Œæ— æ³•æ£€æµ‹åˆ°å¤æ‚è¯­å¢ƒä¸­åµŒå…¥çš„å¾®å¦™æ¶æ„æ„å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26345v1">PDF</a> 27 pages, 5 figure</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ½œåœ¨é£é™©ï¼Œå¦‚è¶Šç‹±æ”»å‡»ç­‰ï¼Œå¯èƒ½ä¼šç»•è¿‡å†…ç½®çš„å®‰å…¨æœºåˆ¶ã€‚ç°æœ‰çš„é˜²å¾¡ç­–ç•¥å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™æˆ–å·¥ä½œæµç¨‹åƒµåŒ–ç­‰é—®é¢˜ï¼Œæ— æ³•æ£€æµ‹å¤æ‚ä¸Šä¸‹æ–‡ä¸­çš„å¾®å¦™æ¶æ„æ„å›¾ã€‚å—è®¤çŸ¥ç§‘å­¦å…³äºäººç±»å†³ç­–åˆ¶å®šçš„å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SafeBehaviorï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ‹Ÿäººç±»è‡ªé€‚åº”å¤šé˜¶æ®µæ¨ç†è¿‡ç¨‹çš„æ–°å‹å±‚æ¬¡åŒ–è¶Šç‹±é˜²å¾¡æœºåˆ¶ã€‚SafeBehaviorå°†å®‰å…¨è¯„ä¼°åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæ„å›¾æ¨æ–­ï¼Œæ£€æµ‹æ˜æ˜¾çš„è¾“å…¥é£é™©ï¼›è‡ªæˆ‘åæ€ï¼Œè¯„ä¼°ç”Ÿæˆçš„å“åº”å¹¶æ ¹æ®åˆ¤æ–­åˆ†é…ä¿¡å¿ƒï¼›è‡ªæˆ‘ä¿®è®¢ï¼Œè‡ªé€‚åº”é‡å†™ä¸ç¡®å®šçš„è¾“å‡ºï¼ŒåŒæ—¶ä¿ç•™ç”¨æˆ·æ„å›¾å¹¶å¼ºåˆ¶æ‰§è¡Œå®‰å…¨çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeBehavioråœ¨å¤šç§å¨èƒåœºæ™¯ä¸‹æ˜¾è‘—æé«˜äº†é²æ£’æ€§å’Œé€‚åº”æ€§ï¼Œä¸ºé˜²èŒƒLLMsçš„è¶Šç‹±æ”»å‡»æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å—äººç±»å¯å‘çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨æ½œåœ¨é£é™©ï¼Œå¦‚è¶Šç‹±æ”»å‡»ã€‚</li>
<li>ç°æœ‰é˜²å¾¡ç­–ç•¥é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™åŠå·¥ä½œæµç¨‹åƒµåŒ–ç­‰é—®é¢˜ã€‚</li>
<li>SafeBehavioræ˜¯ä¸€ç§æ¨¡æ‹Ÿäººç±»å¤šé˜¶æ®µæ¨ç†è¿‡ç¨‹çš„å±‚æ¬¡åŒ–è¶Šç‹±é˜²å¾¡æœºåˆ¶ã€‚</li>
<li>SafeBehaviorå°†å®‰å…¨è¯„ä¼°åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ„å›¾æ¨æ–­ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘ä¿®è®¢ã€‚</li>
<li>SafeBehavioré€šè¿‡è¿™ä¸‰ä¸ªé˜¶æ®µæé«˜LLMså¯¹å¤šç§è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>SafeBehaviorèƒ½å¤Ÿåœ¨ä¿ç•™ç”¨æˆ·æ„å›¾çš„åŒæ—¶å¼ºåˆ¶æ‰§è¡Œå®‰å…¨çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0621e197d59b16590494d7b44275276a" align="middle">
<img src="https://picx.zhimg.com/v2-288fc7ffc946103aff7a0d4339e06c2f" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Memory-Driven-Self-Improvement-for-Decision-Making-with-Large-Language-Models"><a href="#Memory-Driven-Self-Improvement-for-Decision-Making-with-Large-Language-Models" class="headerlink" title="Memory-Driven Self-Improvement for Decision Making with Large Language   Models"></a>Memory-Driven Self-Improvement for Decision Making with Large Language   Models</h2><p><strong>Authors:Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang</strong></p>
<p>Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40% on in-distribution tasks and over 75% when generalized to unseen tasks in ALFWorld. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œå·²æˆä¸ºåºåˆ—å†³ç­–ï¼ˆSDMï¼‰ä»»åŠ¡çš„æœ‰æ•ˆè¡ŒåŠ¨ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™ç§å¹¿æ³›ä½†é€šç”¨çš„çŸ¥è¯†å¯¹äºå…·æœ‰æœ‰é™ä»»åŠ¡ç›¸å…³æ•°æ®çš„å…·ä½“å†³ç­–ä»»åŠ¡å¾€å¾€æ˜¯ä¸å¤Ÿçš„ï¼Œä½¿å¾—å°†LLMæœ‰æ•ˆåœ°é€‚åº”ç‰¹å®šçš„SDMä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®°å¿†é©±åŠ¨çš„è‡ªæ”¹è¿›æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†LLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸ç‰¹å®šé¢†åŸŸç»éªŒçš„ç´§å‡‘è®°å¿†ã€‚è®°å¿†ä¿ç•™äº†è¿‡å»çš„äº¤äº’å’Œç›¸å…³çš„Qå€¼ï¼Œä»è€Œæ•è·ä¸å†³ç­–ç›¸å…³çš„çŸ¥è¯†ï¼Œæœ‰åŠ©äºå‡†ç¡®çš„ä»·å€¼è¯„ä¼°å¹¶ä¼˜åŒ–LLMçš„å…ˆéªŒçŸ¥è¯†ã€‚åè¿‡æ¥ï¼Œä¼˜åŒ–åçš„LLMå…ˆéªŒçŸ¥è¯†ä¼šäº§ç”Ÿæ›´é«˜å›æŠ¥çš„è½¨è¿¹ï¼Œè¿›ä¸€æ­¥ä¸°å¯Œè®°å¿†ï¼Œå½¢æˆäº†ä¸€ç§è‡ªç„¶çš„è‡ªæ”¹è¿›æ¡†æ¶ï¼Œå…¶ä¸­è®°å¿†å’ŒLLMå…ˆéªŒçŸ¥è¯†ç›¸äº’åŠ å¼ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è®°å¿†é©±åŠ¨æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºLLMçš„åŸºçº¿æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨ALFWorldä¸­çš„å†…éƒ¨ä»»åŠ¡ä¸Šæ€§èƒ½æé«˜è¶…è¿‡40%ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šæ€§èƒ½æé«˜è¶…è¿‡75%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26340v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åºåˆ—å†³ç­–ï¼ˆSDMï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰æ•ˆçš„è¡ŒåŠ¨ç­–ç•¥ï¼Œå¾—ç›Šäºå…¶ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹å®šå†³ç­–ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯æ•°æ®æœ‰é™çš„æƒ…å¢ƒä¸‹ï¼Œè¿™ç§æ³›åŒ–çŸ¥è¯†å¸¸æ˜¾ä¸è¶³ï¼Œå¯¼è‡´LLMséš¾ä»¥é«˜æ•ˆé€‚åº”ç‰¹å®šSDMä»»åŠ¡ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ä»¥è®°å¿†é©±åŠ¨çš„è‡ªæå‡æ¡†æ¶ï¼Œç»“åˆLLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç‰¹å®šç»éªŒçš„ç´§å‡‘è®°å¿†ã€‚è®°å¿†ä¿ç•™è¿‡å»äº¤äº’ä¸å…³è”Qå€¼ï¼Œä»è€Œæ•è·å†³ç­–ç›¸å…³çŸ¥è¯†ï¼Œä¿ƒè¿›å‡†ç¡®ä»·å€¼ä¼°è®¡å¹¶ä¼˜åŒ–LLMå…ˆéªŒã€‚ä¼˜åŒ–åçš„LLMå…ˆéªŒç”Ÿæˆæ›´é«˜å¥–åŠ±è½¨è¿¹ï¼Œè¿›ä¸€æ­¥ä¸°å¯Œè®°å¿†ï¼Œå½¢æˆè‡ªç„¶è‡ªæå‡æ¡†æ¶ï¼Œå…¶ä¸­è®°å¿†ä¸LLMå…ˆéªŒç›¸äº’å¼ºåŒ–ã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è®°å¿†é©±åŠ¨æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸LLMåŸºçº¿æ–¹æ³•ï¼Œå¦‚åœ¨ALFWorldç¯å¢ƒä¸­ï¼Œå¯¹å·²çŸ¥ä»»åŠ¡çš„æ€§èƒ½æå‡è¶…è¿‡40%ï¼Œå¯¹æœªçŸ¥ä»»åŠ¡çš„æ€§èƒ½æå‡è¶…è¿‡75%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åºåˆ—å†³ç­–ï¼ˆSDMï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ•ˆèƒ½ï¼Œä¸»è¦å¾—ç›Šäºå…¶å¹¿æ³›çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒLLMsçš„æ³›åŒ–çŸ¥è¯†å¸¸å¸¸ä¸è¶³ä»¥åº”å¯¹ç‰¹å®šå†³ç­–ä»»åŠ¡ã€‚</li>
<li>æå‡ºä¸€ç§è®°å¿†é©±åŠ¨çš„è‡ªæå‡æ¡†æ¶ï¼Œç»“åˆLLMçš„é€šç”¨å…ˆéªŒçŸ¥è¯†ä¸é¢†åŸŸç‰¹å®šç»éªŒã€‚</li>
<li>è®°å¿†ç³»ç»Ÿèƒ½å¤Ÿä¿ç•™è¿‡å»çš„äº¤äº’å’Œå…³è”çš„Qå€¼ï¼Œä»è€Œè·å–å†³ç­–ç›¸å…³çŸ¥è¯†ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä¿ƒè¿›å‡†ç¡®çš„ä»·å€¼ä¼°è®¡å¹¶ä¼˜åŒ–LLMçš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ä¼˜åŒ–åçš„LLMèƒ½å¤Ÿç”Ÿæˆæ›´é«˜å¥–åŠ±çš„è½¨è¿¹ï¼Œè¿›ä¸€æ­¥ä¸°å¯Œè®°å¿†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34f34e91983956beb2b914fdf69bea48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dda2968cedd6329d32682b8698940cd" align="middle">
<img src="https://picx.zhimg.com/v2-fa5103bbd210b7acb8ec5ea773396bd2" align="middle">
<img src="https://pic1.zhimg.com/v2-ddcb7def8b20960e0a60a9df582882d6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-Token-Rollout-Guiding-Supervised-Fine-Tuning-of-LLMs-with-Policy-Gradient"><a href="#One-Token-Rollout-Guiding-Supervised-Fine-Tuning-of-LLMs-with-Policy-Gradient" class="headerlink" title="One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy   Gradient"></a>One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy   Gradient</h2><p><strong>Authors:Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, Bei Yu</strong></p>
<p>Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo &#96;&#96;rolloutâ€™â€™ by sampling multiple candidate tokens from the current policyâ€™s distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs. </p>
<blockquote>
<p>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ˜¯é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»è¦æ–¹æ³•ï¼Œä½†åœ¨ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯¹æ¯”ä¸­ï¼Œå®ƒå¾€å¾€é¢ä¸´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§æ€§èƒ½å·®å¼‚ä¸ä»…ä»…æºäºæŸå¤±å‡½æ•°ï¼Œæ›´æºäºä¸€ä¸ªæ ¹æœ¬æ€§çš„ä¸åŒï¼šSFTæ˜¯ä»é¢„å…ˆæ”¶é›†å¥½çš„å›ºå®šæ•°æ®é›†ä¸­å­¦ä¹ ï¼Œè€ŒRLåˆ™æ˜¯åˆ©ç”¨å½“å‰ç­–ç•¥çš„åœ¨çº¿æ•°æ®è¿›è¡Œé‡‡æ ·ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†å•ä»¤ç‰Œæ»šåŠ¨ï¼ˆOTRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¾®è°ƒç®—æ³•ï¼Œé‡‡ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¥æŒ‡å¯¼SFTã€‚OTRé€šè¿‡æŠŠæ¯ä¸ªä»¤ç‰Œçš„ç”Ÿæˆè§†ä¸ºä¸€ä¸ªå•ç‹¬çš„å¼ºåŒ–å­¦ä¹ è½¨è¿¹ï¼Œä»è€Œé‡æ–°æ„å»ºäº†è‡ªå›å½’å­¦ä¹ è¿‡ç¨‹ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå®ƒé€šè¿‡å¯¹å½“å‰æ”¿ç­–åˆ†å¸ƒè¿›è¡Œå¤šä¸ªå€™é€‰ä»¤ç‰Œçš„é‡‡æ ·ï¼Œæ¥æ‰§è¡ŒMonte Carloâ€œæ»šåŠ¨â€ã€‚ç„¶åï¼Œæ¥è‡ªç›‘ç£æ•°æ®çš„çœŸå®ä»¤ç‰Œè¢«ç”¨æ¥ä¸ºè¿™äº›æ ·æœ¬æä¾›å¥–åŠ±ä¿¡å·ã€‚åœ¨æˆ‘ä»¬çš„ç®—æ³•ä¸­ï¼Œç­–ç•¥æ¢¯åº¦çš„å¼•å¯¼ä½¿å¾—é™æ€çš„ç¦»çº¿ç›‘ç£æ•°æ®å˜ä¸ºåŠ¨æ€çš„åœ¨çº¿ä¿¡å·ï¼Œåœ¨ä»¤ç‰Œçº§åˆ«ä¸Šæ•è·åœ¨çº¿å­¦ä¹ çš„æ³›åŒ–ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…äº†å…¨å¥ç”Ÿæˆçš„æ˜‚è´µå¼€é”€ã€‚é€šè¿‡åœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œé€šç”¨é¢†åŸŸæ¨ç†çš„ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†OTRå§‹ç»ˆä¼˜äºæ ‡å‡†SFTã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœç¡®ç«‹äº†OTRä½œä¸ºå¼ºå¤§ä¸”å®ç”¨çš„LLMå¾®è°ƒæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æœ‰åŠ›è¯æ˜æ•°æ®çš„åœ¨çº¿æ€§è´¨æ˜¯æ¨åŠ¨æ³›åŒ–çš„å…³é”®å› ç´ ï¼Œä¸ºLLMçš„å¾®è°ƒæä¾›äº†æœ‰å‰æ™¯çš„æ–°æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œOne-Token Rollout (OTR)â€çš„æ–°å‹ç›‘ç£å¾®è°ƒç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ã€‚å®ƒé€šè¿‡å¯¹æ¯ä¸ªç”Ÿæˆçš„ä»¤ç‰Œè¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¥æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œåˆ©ç”¨ç›‘ç£æ•°æ®ä¸­çš„çœŸå®ä»¤ç‰Œä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œå¼•å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒOTRåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜æ•°æ®åœ¨ç­–ç•¥å†³ç­–ä¸­çš„å³æ—¶æ€§æ˜¯æé«˜æ³›åŒ–çš„å…³é”®è¦ç´ ã€‚è¿™ä¸ºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œæºè‡ªæ•°æ®ä½¿ç”¨çš„å·®å¼‚ï¼Œè€Œéä»…ä»…æŸå¤±å‡½æ•°çš„ä¸åŒã€‚</li>
<li>SFTä½¿ç”¨å›ºå®šçš„é¢„æ”¶é›†æ•°æ®é›†ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™åˆ©ç”¨å½“å‰ç­–ç•¥çš„å³æ—¶æ•°æ®ã€‚</li>
<li>One-Token Rollout (OTR)æ˜¯ä¸€ç§æ–°å‹çš„ç›‘ç£å¾®è°ƒç®—æ³•ï¼Œå®ƒé€šè¿‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå°†å¼ºåŒ–å­¦ä¹ å¼•å…¥ç›‘ç£å¾®è°ƒè¿‡ç¨‹ã€‚</li>
<li>OTRåœ¨æ¯ä¸ªæ­¥éª¤ä¸­æ¨¡æ‹Ÿç”Ÿæˆä»¤ç‰Œçš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ç›‘ç£æ•°æ®ä¸­çš„çœŸå®ä»¤ç‰Œä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚</li>
<li>OTRç®—æ³•é€šè¿‡åˆ©ç”¨å³æ—¶ç­–ç•¥æ•°æ®æ¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdc91936e0ad100f243c8d10b7dee982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfc9b243c6ad952131ece2d956eab66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a385abf971fcb04df3bccecf9051935.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Interactive-Learning-for-LLM-Reasoning"><a href="#Interactive-Learning-for-LLM-Reasoning" class="headerlink" title="Interactive Learning for LLM Reasoning"></a>Interactive Learning for LLM Reasoning</h2><p><strong>Authors:Hehai Lin, Shilei Cao, Minzhi Li, Sudong Wang, Haotian Wu, Linyi Yang, Juepeng Zheng, Chengwei Qin</strong></p>
<p>Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMsâ€™ independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLMâ€™s reward distribution characteristics into anotherâ€™s reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ™ºèƒ½ä½“å­¦ä¹ æ–¹æ³•å·²ç»æ„å»ºäº†äº¤äº’å¼è®­ç»ƒç¯å¢ƒï¼Œä»¥æ˜ç¡®ä¿ƒè¿›å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„åä½œï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬éœ€è¦é‡æ–°æ‰§è¡ŒMASæ‰èƒ½è·å¾—æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»è®¤çŸ¥ä¸ç¬¦â€”â€”äººç±»å¯ä»¥é€šè¿‡ä¸ä»–äººçš„äº’åŠ¨æ¥å¢å¼ºè‡ªå·±çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æœªæ¥ç‹¬ç«‹è§£å†³é—®é¢˜ã€‚ä¸ºäº†ç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æ˜¯å¦èƒ½å¢å¼ºLLMçš„ç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ILRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ååŒå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ã€‚å…·ä½“è€Œè¨€ï¼ŒåŠ¨æ€äº¤äº’é¦–å…ˆæ ¹æ®é—®é¢˜çš„éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”åœ°é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚ç„¶åï¼ŒLLMé€šè¿‡æ€æƒ³å…±äº«ï¼ˆIdea Sharingï¼‰ã€æ€æƒ³åˆ†æï¼ˆIdea Analysisï¼‰å’Œèåˆï¼ˆIdea Fusionï¼‰è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡ä»¿äººç±»è®¨è®ºçš„åˆ›æ–°äº¤äº’æ¨¡å¼ï¼Œç„¶åå¾—å‡ºå„è‡ªçš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨æ„ŸçŸ¥æ ¡å‡†ä¸­ï¼ŒILRé‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒLLMï¼ŒåŒæ—¶å°†ä¸€ä¸ªLLMçš„å¥–åŠ±åˆ†å¸ƒç‰¹å¾é›†æˆåˆ°å¦ä¸€ä¸ªçš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œä»è€Œæé«˜å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹å®¶æ—ä¸­çš„ä¸‰ä¸ªLLMä¸ŠéªŒè¯äº†ILRçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒILRå§‹ç»ˆä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ï¼Œæ¯”æœ€å¼ºåŸºçº¿æé«˜äº†é«˜è¾¾5%ã€‚æˆ‘ä»¬è¿˜å‘ç°æ€æƒ³å…±äº«å¯ä»¥åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­å¢å¼ºå¼ºå¤§LLMçš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”ä¸çº¯ç²¹çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥ç›¸æ¯”ï¼ŒåŠ¨æ€äº¤äº’ç±»å‹å¯ä»¥ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26306v1">PDF</a> The code will be released later</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°æœ‰å¤šæ™ºèƒ½ä½“å­¦ä¹ çš„æ–¹æ³•é€šè¿‡æ„å»ºäº¤äº’å¼è®­ç»ƒç¯å¢ƒæ¥ä¿ƒè¿›å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„åä½œï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬éœ€è¦é‡æ–°æ‰§è¡ŒMASæ¥è·å¾—æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»è®¤çŸ¥ä¸åŒï¼Œäººç±»å¯ä»¥é€šè¿‡ä¸ä»–äººäº’åŠ¨å¢å¼ºè‡ªèº«æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æœªæ¥ç‹¬ç«‹è§£å†³é—®é¢˜ã€‚ä¸ºäº†ç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æ˜¯å¦èƒ½å¢å¼ºLLMçš„ç‹¬ç«‹é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ILRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸååŒå­¦ä¹ æ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ã€‚å…·ä½“è€Œè¨€ï¼ŒåŠ¨æ€äº¤äº’æ ¹æ®é—®é¢˜çš„éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”åœ°é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚LLMé€šè¿‡æ¨¡ä»¿äººç±»è®¨è®ºçš„åˆ›æ–°äº¤äº’æ¨¡å¼Idea3ï¼ˆæ€æƒ³å…±äº«ã€æ€æƒ³åˆ†æå’Œæ€æƒ³èåˆï¼‰äº¤æ¢ä¿¡æ¯ï¼Œç„¶åå¾—å‡ºå„è‡ªçš„æœ€ç»ˆç­”æ¡ˆã€‚åœ¨æ„ŸçŸ¥æ ¡å‡†ä¸­ï¼ŒILRé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒLLMï¼ŒåŒæ—¶å°†ä¸€ä¸ªLLMçš„å¥–åŠ±åˆ†å¸ƒç‰¹æ€§é›†æˆåˆ°å¦ä¸€ä¸ªLLMçš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œä»è€Œæé«˜å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¸­çš„ä¸‰ä¸ªä¸åŒè§„æ¨¡çš„LLMä¸ŠéªŒè¯äº†ILRçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒILRåœ¨å„æ–¹é¢å‡ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ï¼Œä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”æé«˜äº†é«˜è¾¾5%ã€‚æˆ‘ä»¬è¿˜å‘ç°Idea3å¯ä»¥å¢å¼ºå¼ºLLMåœ¨å¤šæ™ºèƒ½ä½“æ¨ç†ä¸­çš„ç¨³å¥æ€§ï¼Œè€ŒåŠ¨æ€äº¤äº’ç±»å‹ç›¸å¯¹äºçº¯ç²¹çš„åˆä½œæˆ–ç«äº‰ç­–ç•¥å¯ä»¥ä¿ƒè¿›å¤šæ™ºèƒ½ä½“å­¦ä¹ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å­¦ä¹ é€šè¿‡æ„å»ºäº¤äº’å¼è®­ç»ƒç¯å¢ƒä¿ƒè¿›LLMé—´çš„åä½œã€‚</li>
<li>ILRæ¡†æ¶å¼•å…¥åŠ¨æ€äº¤äº’å’Œæ„ŸçŸ¥æ ¡å‡†ä»¥å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>åŠ¨æ€äº¤äº’æ ¹æ®é—®é¢˜éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”é€‰æ‹©åˆä½œæˆ–ç«äº‰ç­–ç•¥ã€‚</li>
<li>Idea3äº¤äº’æ¨¡å¼æ¨¡ä»¿äººç±»è®¨è®ºï¼Œæœ‰åŠ©äºLLMäº¤æ¢ä¿¡æ¯å¹¶å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚</li>
<li>æ„ŸçŸ¥æ ¡å‡†é€šè¿‡GRPOè®­ç»ƒLLMï¼Œæé«˜å¤šæ™ºèƒ½ä½“äº¤äº’çš„å‡èšåŠ›ã€‚</li>
<li>ILRåœ¨å¤šä¸ªLLMä¸ŠéªŒè¯æœ‰æ•ˆï¼Œå¹¶åœ¨æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå•æ™ºèƒ½ä½“å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-271f8ed29c861540ff3eb27a6dbdd428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d428ccc45900ea3daf9be38941fb8b05" align="middle">
<img src="https://picx.zhimg.com/v2-9c9e460d7d988f375d06468f38f54f5c" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PRPO-Paragraph-level-Policy-Optimization-for-Vision-Language-Deepfake-Detection"><a href="#PRPO-Paragraph-level-Policy-Optimization-for-Vision-Language-Deepfake-Detection" class="headerlink" title="PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake   Detection"></a>PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake   Detection</h2><p><strong>Authors:Tuan Nguyen, Naseem Khan, Khang Tran, NhatHai Phan, Issa Khalil</strong></p>
<p>The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55&#x2F;5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection. </p>
<blockquote>
<p>åˆæˆåª’ä½“çš„è¿…é€Ÿå´›èµ·ä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹æˆä¸ºç½‘ç»œå®‰å…¨å’Œä¿¡ä»»çš„å…³é”®æŒ‘æˆ˜ã€‚ç”±äºå¤§å‹é«˜è´¨é‡æ•°æ®é›†çš„ç¨€ç¼ºï¼Œè¿›å±•ä»ç„¶å—åˆ°é™åˆ¶ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹çš„è¡¨ç°å´ä¸å°½äººæ„ï¼Œå¾€å¾€äº§ç”Ÿä¸è§†è§‰è¯æ®ä¸ç¬¦æˆ–è‡†æƒ³çš„è§£é‡Šã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥ç”¨äºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ¨ç†æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶æå‡ºæ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ®µè½çº§åˆ«å°†LLMæ¨ç†ä¸å›¾åƒå†…å®¹å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒPRPOåœ¨æ£€æµ‹å‡†ç¡®ç‡ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶è·å¾—äº†æœ€é«˜æ¨ç†è¯„åˆ†4.55&#x2F;5.0ã€‚è¿›ä¸€æ­¥çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œåœ¨æµ‹è¯•æ¡ä»¶ä¸‹ï¼ŒPRPOæ˜¾è‘—ä¼˜äºGRPOã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨è§†è§‰è¯æ®çš„åŸºç¡€ä¸Šå®ç°å¤šæ¨¡æ€æ¨ç†çš„é‡è¦æ€§ï¼Œä»¥å®ç°æ›´å¯é ã€æ›´å¯è§£é‡Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åˆæˆåª’ä½“çš„è¿…é€Ÿå´›èµ·ä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹æˆä¸ºç½‘ç»œå®‰å…¨ä¸ä¿¡ä»»çš„å…³é”®æŒ‘æˆ˜ã€‚ç”±äºç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œè¿›å±•ä»ç„¶å—é™ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢çš„è¡¨ç°ä¸ä½³ï¼Œå¾€å¾€äº§ç”Ÿä¸è§†è§‰è¯æ®ä¸ç¬¦æˆ–è™šå¹»çš„è§£é‡Šã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨ç†æ³¨é‡Šæ•°æ®é›†å¹¶æå‡ºæ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯åœ¨æ®µè½çº§åˆ«å°†LLMæ¨ç†ä¸å›¾åƒå†…å®¹å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒPRPOå¤§å¹…æé«˜äº†æ£€æµ‹ç²¾åº¦å¹¶è·å¾—äº†æœ€é«˜æ¨ç†åˆ†æ•°4.55&#x2F;5.0ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œåœ¨æµ‹è¯•æ¡ä»¶ä¸‹PRPOæ˜¾è‘—ä¼˜äºGRPOã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨è§†è§‰è¯æ®çš„åŸºç¡€ä¸Šå®ç°å¤šæ¨¡æ€æ¨ç†çš„é‡è¦æ€§ï¼Œä»¥è¿›è¡Œæ›´å¯é å’Œå¯è§£é‡Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆåª’ä½“çš„æ™®åŠä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹æˆä¸ºç½‘ç»œå®‰å…¨çš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„åˆ¶çº¦ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢çš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ¨ç†æ³¨é‡Šæ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†æ®µè½çº§ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æé«˜æ£€æµ‹ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPRPOåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d792ed5df963e407580af59f0af94c95" align="middle">
<img src="https://picx.zhimg.com/v2-216dba91803f79ece78fbc9da5634c2c" align="middle">
<img src="https://picx.zhimg.com/v2-4c22b9eacf49db75e60c58f1d98886c2" align="middle">
<img src="https://picx.zhimg.com/v2-22a21b688b81ff4abc7a96d622869ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef31a419cc4b2c000407e0602473fa9c" align="middle">
<img src="https://picx.zhimg.com/v2-4d3fc4df4dae98791ee7e8fa4dc27e1d" align="middle">
<img src="https://picx.zhimg.com/v2-4b1fcc629076f1bb88ba63f5cf0b6842" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EnScale-Temporally-consistent-multivariate-generative-downscaling-via-proper-scoring-rules"><a href="#EnScale-Temporally-consistent-multivariate-generative-downscaling-via-proper-scoring-rules" class="headerlink" title="EnScale: Temporally-consistent multivariate generative downscaling via   proper scoring rules"></a>EnScale: Temporally-consistent multivariate generative downscaling via   proper scoring rules</h2><p><strong>Authors:Maybritt Schillinger, Maxim Samarin, Xinwei Shen, Reto Knutti, Nicolai Meinshausen</strong></p>
<p>The practical use of future climate projections from global circulation models (GCMs) is often limited by their coarse spatial resolution, requiring downscaling to generate high-resolution data. Regional climate models (RCMs) provide this refinement, but are computationally expensive. To address this issue, machine learning models can learn the downscaling function, mapping coarse GCM outputs to high-resolution fields. Among these, generative approaches aim to capture the full conditional distribution of RCM data given coarse-scale GCM data, which is characterized by large variability and thus challenging to model accurately. We introduce EnScale, a generative machine learning framework that emulates the full GCM-to-RCM map by training on multiple pairs of GCM and corresponding RCM data. It first adjusts large-scale mismatches between GCM and coarsened RCM data, followed by a super-resolution step to generate high-resolution fields. Both steps employ generative models optimized with the energy score, a proper scoring rule. Compared to state-of-the-art ML downscaling approaches, our setup reduces computational cost by about one order of magnitude. EnScale jointly emulates multiple variables â€“ temperature, precipitation, solar radiation, and wind â€“ spatially consistent over an area in Central Europe. In addition, we propose a variant EnScale-t that enables temporally consistent downscaling. We establish a comprehensive evaluation framework across various categories including calibration, spatial structure, extremes, and multivariate dependencies. Comparison with diverse benchmarks demonstrates EnScaleâ€™s strong performance and computational efficiency. EnScale offers a promising approach for accurate and temporally consistent RCM emulation. </p>
<blockquote>
<p>å®é™…åº”ç”¨ä¸­ï¼Œå…¨çƒç¯æµæ¨¡å‹ï¼ˆGCMsï¼‰çš„æœªæ¥æ°”å€™é¢„æµ‹å¸¸å¸¸å—åˆ°å…¶ç©ºé—´åˆ†è¾¨ç‡è¾ƒä½çš„é™åˆ¶ï¼Œéœ€è¦é™å°ºåº¦ç”Ÿæˆé«˜åˆ†è¾¨ç‡æ•°æ®ã€‚åŒºåŸŸæ°”å€™æ¨¡å‹ï¼ˆRCMsï¼‰å¯ä»¥æä¾›è¿™ç§ç²¾ç»†åŒ–çš„æ•°æ®ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥å­¦ä¹ é™å°ºåº¦å‡½æ•°ï¼Œå°†ç²—åˆ†è¾¨ç‡çš„GCMè¾“å‡ºæ˜ å°„åˆ°é«˜åˆ†è¾¨ç‡åœºã€‚å…¶ä¸­ï¼Œç”Ÿæˆå¼æ–¹æ³•æ—¨åœ¨æ•æ‰ç»™å®šç²—å°ºåº¦GCMæ•°æ®çš„RCMæ•°æ®çš„å…¨æ¡ä»¶åˆ†å¸ƒã€‚ç”±äºRCMæ•°æ®å…·æœ‰è¾ƒå¤§çš„å˜å¼‚æ€§ï¼Œå› æ­¤å‡†ç¡®å»ºæ¨¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†EnScaleï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒå¤šå¯¹GCMå’Œç›¸åº”çš„RCMæ•°æ®æ¥æ¨¡æ‹Ÿå®Œæ•´çš„GCMåˆ°RCMçš„æ˜ å°„ã€‚å®ƒé¦–å…ˆè°ƒæ•´GCMå’Œç®€åŒ–RCMæ•°æ®ä¹‹é—´çš„å®è§‚ä¸åŒ¹é…ï¼Œç„¶åé€šè¿‡è¶…åˆ†è¾¨ç‡æ­¥éª¤ç”Ÿæˆé«˜åˆ†è¾¨ç‡åœºã€‚è¿™ä¸¤ä¸ªæ­¥éª¤éƒ½ä½¿ç”¨ä»¥èƒ½é‡è¯„åˆ†ä¼˜åŒ–è¿‡çš„ç”Ÿæˆæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§é€‚å½“çš„è¯„åˆ†è§„åˆ™ã€‚ä¸æœ€å…ˆè¿›çš„MLé™å°ºåº¦æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è®¾ç½®å°†è®¡ç®—æˆæœ¬é™ä½äº†å¤§çº¦ä¸€ä¸ªæ•°é‡çº§ã€‚EnScaleå¯ä»¥åŒæ—¶æ¨¡æ‹Ÿå¤šä¸ªå˜é‡â€”â€”æ¸©åº¦ã€é™æ°´ã€å¤ªé˜³è¾å°„å’Œé£åŠ›â€”â€”åœ¨ç©ºé—´ä¸Šä¸€è‡´è¦†ç›–ä¸­æ¬§åœ°åŒºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§EnScale-tå˜ä½“ï¼Œèƒ½å¤Ÿå®ç°æ—¶é—´ä¸€è‡´çš„é™å°ºåº¦ã€‚æˆ‘ä»¬å»ºç«‹äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ ¡å‡†ã€ç©ºé—´ç»“æ„ã€æç«¯äº‹ä»¶å’Œå¤šå…ƒä¾èµ–ç­‰å„ä¸ªç±»åˆ«ã€‚ä¸å„ç§åŸºå‡†çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒEnScaleå…·æœ‰å¼ºå¤§çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚EnScaleä¸ºå‡†ç¡®å’Œæ—¶é—´ä¸Šä¸€è‡´çš„RCMæ¨¡æ‹Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26258v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ä½¿ç”¨å…¨çƒç¯æµæ¨¡å‹ï¼ˆGCMsï¼‰çš„æœªæ¥æ°”å€™é¢„æµ‹åœ¨å®é™…åº”ç”¨ä¸­å¸¸å—é™äºå…¶è¾ƒä½çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œéœ€è¦é™å°ºåº¦ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡æ•°æ®ã€‚åŒºåŸŸæ°”å€™æ¨¡å‹ï¼ˆRCMsï¼‰è™½èƒ½æä¾›è¿™ä¸€ç»†åŒ–ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å¯å­¦ä¹ é™å°ºåº¦å‡½æ•°ï¼Œå°†ç²—ç³™çš„GCMè¾“å‡ºæ˜ å°„åˆ°é«˜åˆ†è¾¨ç‡åœºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEnScaleçš„ç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¤šå¯¹GCMå’Œç›¸åº”RCMæ•°æ®çš„è®­ç»ƒæ¥æ¨¡æ‹Ÿå®Œæ•´çš„GCMåˆ°RCMçš„æ˜ å°„ã€‚å®ƒé¦–å…ˆè°ƒæ•´GCMå’Œç®€åŒ–RCMæ•°æ®ä¹‹é—´çš„å¤§å°ºåº¦ä¸åŒ¹é…ï¼Œç„¶åé€šè¿‡è¶…åˆ†è¾¨ç‡æ­¥éª¤ç”Ÿæˆé«˜åˆ†è¾¨ç‡åœºã€‚è¿™ä¸¤ä¸ªæ­¥éª¤éƒ½ä½¿ç”¨ä»¥èƒ½é‡åˆ†æ•°ä¸ºä¼˜åŒ–ç›®æ ‡çš„ç”Ÿæˆæ¨¡å‹ã€‚ä¸æœ€å…ˆè¿›çš„MLé™å°ºåº¦æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è®¾ç½®å°†è®¡ç®—æˆæœ¬é™ä½äº†å¤§çº¦ä¸€ä¸ªæ•°é‡çº§ã€‚EnScaleå¯è”åˆæ¨¡æ‹Ÿæ¸©åº¦ã€é™æ°´ã€å¤ªé˜³è¾å°„å’Œé£åŠ›ç­‰å¤šä¸ªå˜é‡ï¼Œå¹¶åœ¨ä¸­æ¬§åœ°åŒºå®ç°ç©ºé—´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†EnScale-tå˜ä½“ä»¥å®ç°æ—¶é—´ä¸€è‡´çš„é™å°ºåº¦ã€‚æˆ‘ä»¬å»ºç«‹äº†åŒ…æ‹¬æ ¡å‡†ã€ç©ºé—´ç»“æ„ã€æç«¯äº‹ä»¶å’Œå¤šå…ƒä¾èµ–åœ¨å†…çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚ä¸å¤šç§åŸºå‡†çš„æ¯”è¾ƒè¡¨æ˜ï¼ŒEnScaleå…·æœ‰å¼ºå¤§çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚EnScaleä¸ºå‡†ç¡®å’Œæ—¶é—´ä¸€è‡´çš„RCMæ¨¡æ‹Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>EnScaleæ˜¯ä¸€ä¸ªç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæ¨¡æ‹ŸGCMåˆ°RCMçš„å®Œæ•´æ˜ å°„ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è°ƒæ•´å¤§å°ºåº¦ä¸åŒ¹é…å’Œè¶…çº§åˆ†è¾¨ç‡ç”Ÿæˆæ­¥éª¤ï¼Œä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡æ•°æ®ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„MLé™å°ºåº¦æ–¹æ³•ç›¸æ¯”ï¼ŒEnScaleçš„è®¡ç®—æˆæœ¬é™ä½äº†å¤§çº¦ä¸€ä¸ªæ•°é‡çº§ã€‚</li>
<li>EnScaleèƒ½å¤Ÿè”åˆæ¨¡æ‹Ÿå¤šä¸ªæ°”å€™å˜é‡ï¼Œå¦‚æ¸©åº¦ã€é™æ°´ã€å¤ªé˜³è¾å°„å’Œé£åŠ›ï¼Œå¹¶åœ¨ä¸­æ¬§åœ°åŒºå®ç°ç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>EnScale-tå˜ä½“å®ç°äº†æ—¶é—´ä¸€è‡´çš„é™å°ºåº¦ã€‚</li>
<li>å»ºç«‹äº†åŒ…æ‹¬æ ¡å‡†ã€ç©ºé—´ç»“æ„ã€æç«¯äº‹ä»¶å’Œå¤šå…ƒä¾èµ–çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0a7d13a327ffff99fc3d9ca9614c03d" align="middle">
<img src="https://pic1.zhimg.com/v2-b5c4af0af17420a85ba677494a8bde05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e67ab48eb773160fdd78c42efc6f2d" align="middle">
<img src="https://picx.zhimg.com/v2-5fea4ef252a02c06d489a0c391678907.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diversity-Incentivized-Exploration-for-Versatile-Reasoning"><a href="#Diversity-Incentivized-Exploration-for-Versatile-Reasoning" class="headerlink" title="Diversity-Incentivized Exploration for Versatile Reasoning"></a>Diversity-Incentivized Exploration for Versatile Reasoning</h2><p><strong>Authors:Zican Hu, Shilin Zhang, Yafu Li, Jianhao Yan, Xuyang Hu, Leyang Cui, Xiaoye Qu, Chunlin Chen, Yu Cheng, Zhi Wang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \textbf{DIVER} (\textbf{D}iversity-\textbf{I}ncentivized Exploration for \textbf{V}ersatil\textbf{E} \textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NJU-RL/DIVER">https://github.com/NJU-RL/DIVER</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›çš„é‡è¦èŒƒå¼ã€‚ç”±äºæ¨ç†ä»»åŠ¡ä¸­çš„çŠ¶æ€åŠ¨ä½œç©ºé—´åºå¤§å’Œå¥–åŠ±ç¨€ç–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é¢ä¸´æ¢ç´¢ä¸è¶³å’Œæ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DIVERï¼ˆå¤šæ ·æ¿€åŠ±æ¢ç´¢ä¿ƒè¿›æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œå¼ºè°ƒå…¨å±€åºåˆ—æ°´å¹³å¤šæ ·æ€§åœ¨æ¿€åŠ±æ·±åº¦æ¢ç´¢ä»¥ä¿ƒè¿›å¤šæ ·åŒ–æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹åˆæ­¥å®è¯ç ”ç©¶ï¼Œæ­ç¤ºäº†å…¨å±€å¤šæ ·æ€§ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å¼ºçƒˆæ­£ç›¸å…³å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥å…¨å±€å¤šæ ·æ€§æ¿€åŠ±ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œä»¥ä¿ƒè¿›è¯­ä¹‰ç»“æ„ç©ºé—´ä¸­çš„æ·±åº¦æ¢ç´¢ã€‚ç»“åˆè¿™ç§å†…åœ¨å¥–åŠ±ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ½œåŠ›çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä»¥ä¿æŒæœ€ä¼˜ç­–ç•¥çš„ä¸å˜æ€§ï¼Œå¹¶è®¾è®¡ç®€å•çš„å¯å‘å¼æ–¹æ³•æ¥ç¼“è§£å¯èƒ½çš„å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åŸŸå†…è¿˜æ˜¯åŸŸå¤–ä»»åŠ¡ä¸Šï¼ŒDIVERåœ¨å„ç§æ¢ç´¢ç­–ç•¥æ–¹é¢çš„è¡¨ç°éƒ½ä¼˜äºRLVRåŸºçº¿ï¼Œåœ¨Pass@1å’ŒPass@kè¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NJU-RL/DIVER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NJU-RL/DIVERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26209v1">PDF</a> 26 pages, 10 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦èŒƒå¼ã€‚ç°æœ‰æ–¹æ³•ç”±äºæ¨ç†ä»»åŠ¡ä¸­çš„çŠ¶æ€åŠ¨ä½œç©ºé—´åºå¤§å’Œå¥–åŠ±ç¨€ç–ï¼Œå¸¸å¸¸é¢ä¸´æ¢ç´¢ä¸è¶³å’Œæ ·æœ¬æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºDIVERï¼ˆé’ˆå¯¹å¤šæ ·åŒ–æ¿€åŠ±çš„æ¢ç´¢ä»¥ä¿ƒè¿›é€šç”¨æ¨ç†ï¼‰ï¼Œå¼ºè°ƒå…¨å±€åºåˆ—çº§åˆ«å¤šæ ·æ€§åœ¨æ¿€åŠ±æ·±åº¦æ¢ç´¢ä¸­çš„å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œæ­ç¤ºäº†å…¨å±€å¤šæ ·æ€§ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å¼ºæ­£ç›¸å…³å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥å…¨å±€å¤šæ ·æ€§æ¿€åŠ±ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œä»¥ä¿ƒè¿›è¯­ä¹‰ç»“æ„åŒ–ç©ºé—´ä¸­çš„æ·±åº¦æ¢ç´¢ã€‚ç»“åˆè¿™ä¸€å†…åœ¨å¥–åŠ±ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ½œåŠ›çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä»¥ä¿æŒæœ€ä¼˜ç­–ç•¥çš„ä¸å˜æ€§ï¼Œå¹¶è®¾è®¡ç®€å•çš„å¯å‘å¼æ–¹æ³•æ¥é˜²æ­¢å¯èƒ½çš„å¥–åŠ±æ“çºµã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸Šï¼ŒDIVERåœ¨Pass@1å’ŒPass@kè¯„ä¼°ä¸­éƒ½ä¼˜äºå…¶ä»–RLVRåŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NJU-RL/DIVER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NJU-RL/DIVERæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æ¢ç´¢ä¸è¶³å’Œæ ·æœ¬æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>DIVERæ¡†æ¶å¼ºè°ƒå…¨å±€åºåˆ—çº§åˆ«å¤šæ ·æ€§åœ¨æ¿€åŠ±æ·±åº¦æ¢ç´¢ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜å…¨å±€å¤šæ ·æ€§ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³»ã€‚</li>
<li>å¼•å…¥å…¨å±€å¤šæ ·æ€§æ¿€åŠ±ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼Œä¿ƒè¿›è¯­ä¹‰ç»“æ„åŒ–ç©ºé—´ä¸­çš„æ·±åº¦æ¢ç´¢ã€‚</li>
<li>å¼€å‘åŸºäºæ½œåŠ›çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä¿æŒæœ€ä¼˜ç­–ç•¥çš„ä¸å˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15f56e615e9b3ca6a7ed54ecd86a3ed7" align="middle">
<img src="https://picx.zhimg.com/v2-8c2f690cc736bbea40e7ccf557d90d9a" align="middle">
<img src="https://picx.zhimg.com/v2-8841d5678ae7e7d45870cef47e2e8688" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Human-MME-A-Holistic-Evaluation-Benchmark-for-Human-Centric-Multimodal-Large-Language-Models"><a href="#Human-MME-A-Holistic-Evaluation-Benchmark-for-Human-Centric-Multimodal-Large-Language-Models" class="headerlink" title="Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal   Large Language Models"></a>Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal   Large Language Models</h2><p><strong>Authors:Yuansen Liu, Haiming Tang, Jinlong Peng, Jiangning Zhang, Xiaozhong Ji, Qingdong He, Donghao Luo, Zhenye Gan, Junwei Zhu, Yunhang Shen, Chaoyou Fu, Chengjie Wang, Xiaobin Hu, Shuicheng Yan</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks. However, their capacity to comprehend human-centric scenes has rarely been explored, primarily due to the absence of comprehensive evaluation benchmarks that take into account both the human-oriented granular level and higher-dimensional causal reasoning ability. Such high-quality evaluation benchmarks face tough obstacles, given the physical complexity of the human body and the difficulty of annotating granular structures. In this paper, we propose Human-MME, a curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric scene understanding. Compared with other existing benchmarks, our work provides three key features: 1. Diversity in human scene, spanning 4 primary visual domains with 15 secondary domains and 43 sub-fields to ensure broad scenario coverage. 2. Progressive and diverse evaluation dimensions, evaluating the human-based activities progressively from the human-oriented granular perception to the higher-dimensional reasoning, consisting of eight dimensions with 19,945 real-world image question pairs and an evaluation suite. 3. High-quality annotations with rich data paradigms, constructing the automated annotation pipeline and human-annotation platform, supporting rigorous manual labeling to facilitate precise and reliable model assessment. Our benchmark extends the single-target understanding to the multi-person and multi-image mutual understanding by constructing the choice, short-answer, grounding, ranking and judgment question components, and complex questions of their combination. The extensive experiments on 17 state-of-the-art MLLMs effectively expose the limitations and guide future MLLMs research toward better human-centric image understanding. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/Yuan-Hou/Human-MME">https://github.com/Yuan-Hou/Human-MME</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç†è§£ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åœºæ™¯æ–¹é¢çš„èƒ½åŠ›å¾ˆå°‘è¢«æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ç»¼åˆè€ƒè™‘äººç±»å¯¼å‘çš„é¢—ç²’åº¦å’Œé«˜ç»´å› æœæ¨ç†èƒ½åŠ›çš„å…¨é¢è¯„ä¼°åŸºå‡†ã€‚é‰´äºäººä½“ç‰©ç†ç»“æ„çš„å¤æ‚æ€§ä»¥åŠé¢—ç²’ç»“æ„æ ‡æ³¨çš„å›°éš¾ï¼Œé«˜è´¨é‡è¯„ä¼°åŸºå‡†é¢ä¸´ç€è‰°å·¨çš„éšœç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Human-MMEï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°å¯¹MLLMsåœ¨ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£è¿›è¡Œè¯„ä¼°ã€‚ä¸å…¶ä»–ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ï¼š</p>
</blockquote>
<ol>
<li><p>äººç±»åœºæ™¯çš„å¤šæ ·æ€§ï¼Œæ¶µç›–4ä¸ªä¸»è¦è§†è§‰é¢†åŸŸï¼ŒåŒ…æ‹¬15ä¸ªæ¬¡çº§é¢†åŸŸå’Œ43ä¸ªå­é¢†åŸŸï¼Œä»¥ç¡®ä¿å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚</p>
</li>
<li><p>æ¸è¿›ä¸”å¤šæ ·çš„è¯„ä¼°ç»´åº¦ï¼Œä»ä»¥äººä¸ºåŸºç¡€çš„æ´»åŠ¨è¿›è¡Œæ¸è¿›è¯„ä¼°ï¼Œä»äººç±»å¯¼å‘çš„é¢—ç²’æ„ŸçŸ¥åˆ°é«˜ç»´æ¨ç†ï¼ŒåŒ…æ‹¬å…«ä¸ªç»´åº¦ï¼ŒåŒ…å«19945ä¸ªçœŸå®ä¸–ç•Œå›¾åƒé—®é¢˜å¯¹å’Œä¸€ä¸ªè¯„ä¼°å¥—ä»¶ã€‚</p>
</li>
</ol>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26165v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººç±»ä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†â€”â€”Human-MMEã€‚ç›¸è¾ƒäºå…¶ä»–åŸºå‡†ï¼ŒHuman-MMEå…·å¤‡ä¸‰å¤§ç‰¹ç‚¹ï¼šæ¶µç›–å¤šç§äººç±»åœºæ™¯ã€æ¸è¿›ä¸”å¤šæ ·çš„è¯„ä¼°ç»´åº¦ä»¥åŠé«˜è´¨é‡çš„æ•°æ®æ ‡æ³¨ã€‚Human-MMEçš„æ„å»ºä¿ƒè¿›äº†å¤šç›®æ ‡ç†è§£å‘å¤šäººã€å¤šå›¾åƒç›¸äº’ç†è§£çš„è½¬å˜ï¼Œä¸ºMLLMsçš„æœªæ¥å‘å±•æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human-MMEæ˜¯ä¸€ä¸ªé’ˆå¯¹MLLMsåœ¨äººç±»ä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†ã€‚</li>
<li>Human-MMEæ¶µç›–äº†å¤šç§äººç±»åœºæ™¯ï¼ŒåŒ…æ‹¬4ä¸ªä¸»è¦è§†è§‰é¢†åŸŸå’Œ15ä¸ªæ¬¡çº§é¢†åŸŸï¼Œç¡®ä¿äº†å¹¿æ³›çš„åœºæ™¯è¦†ç›–ã€‚</li>
<li>è¯¥åŸºå‡†å…·æœ‰æ¸è¿›å’Œå¤šæ ·çš„è¯„ä¼°ç»´åº¦ï¼Œä»äººç±»å¯¼å‘çš„é¢—ç²’åº¦æ„ŸçŸ¥åˆ°æ›´é«˜ç»´åº¦çš„æ¨ç†ã€‚</li>
<li>Human-MMEæä¾›äº†é«˜è´¨é‡çš„æ•°æ®æ ‡æ³¨ï¼Œæ”¯æŒä¸¥æ ¼çš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œä¿ƒè¿›äº†ç²¾ç¡®å’Œå¯é çš„æ¨¡å‹è¯„ä¼°ã€‚</li>
<li>Human-MMEä»å•ç›®æ ‡ç†è§£æ‰©å±•åˆ°å¤šäººå’Œå¤šå›¾åƒçš„ç›¸äº’ç†è§£ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒï¼ŒHuman-MMEæ­ç¤ºäº†å½“å‰MLLMsçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1776e6359c8b15279168517297c0c09" align="middle">
<img src="https://picx.zhimg.com/v2-a7695ce2d35c41037b32fa3440b6352a" align="middle">
<img src="https://pica.zhimg.com/v2-34bd04842876adf8dfa6b832b03e9e1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d179e183fc5d8f8618788e12f731701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e273c8c46fd525b659ab252fed7cd90e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforced-Strategy-Optimization-for-Conversational-Recommender-Systems-via-Network-of-Experts"><a href="#Reinforced-Strategy-Optimization-for-Conversational-Recommender-Systems-via-Network-of-Experts" class="headerlink" title="Reinforced Strategy Optimization for Conversational Recommender Systems   via Network-of-Experts"></a>Reinforced Strategy Optimization for Conversational Recommender Systems   via Network-of-Experts</h2><p><strong>Authors:Xiaoyan Zhao</strong></p>
<p>Conversational Recommender Systems (CRSs) provide personalized recommendations through multi-turn interactions. With the strong reasoning abilities of Large Language Models (LLMs), applying them to CRSs has become promising. Yet, existing methods often lack explicit optimization of interaction strategies, relying instead on unified prompts, which can yield suboptimal outcomes. We propose Reinforced Strategy Optimization (RSO), a hierarchical framework that decomposes response generation into macro-level strategy planning and micro-level adaptation within a network-of-experts. A Planner selects strategies (e.g., recommend, explain, encourage), while an Actor generates responses guided by auxiliary experts for preferences and factual grounding. This disentanglement enables more tractable learning. To address limited multi-turn data, we model strategy learning as reinforcement learning with an LLM-based reward for exploration. Experiments show RSO outperforms state-of-the-art baselines, validating the effectiveness of hierarchical strategy optimization. </p>
<blockquote>
<p>å¯¹è¯æ¨èç³»ç»Ÿï¼ˆCRSsï¼‰é€šè¿‡å¤šè½®äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå°†å…¶åº”ç”¨äºCRSså·²ç»æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹æ˜ç¡®çš„ä¼˜åŒ–äº¤äº’ç­–ç•¥ï¼Œè€Œæ˜¯ä¾èµ–äºç»Ÿä¸€çš„æç¤ºï¼Œè¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚æ¬¡æ¡†æ¶ï¼Œå°†å“åº”ç”Ÿæˆåˆ†è§£ä¸ºå®è§‚å±‚é¢çš„ç­–ç•¥è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„ä¸“å®¶ç½‘ç»œå†…çš„é€‚åº”ã€‚è§„åˆ’å™¨é€‰æ‹©ç­–ç•¥ï¼ˆä¾‹å¦‚æ¨èã€è§£é‡Šã€é¼“åŠ±ï¼‰ï¼Œè€Œè¡Œä¸ºè€…æ ¹æ®è¾…åŠ©ä¸“å®¶ç”Ÿæˆåå¥½å’Œäº‹å®åŸºç¡€çš„å“åº”ã€‚è¿™ç§åˆ†è§£ä½¿å¾—å­¦ä¹ æ›´å®¹æ˜“è¿›è¡Œã€‚ä¸ºäº†è§£å†³æœ‰é™çš„å¤šè½®æ•°æ®é—®é¢˜ï¼Œæˆ‘ä»¬å°†ç­–ç•¥å­¦ä¹ å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨åŸºäºLLMçš„å¥–åŠ±è¿›è¡Œæ¢ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒRSOä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å±‚æ¬¡ç­–ç•¥ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26093v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¯¹è¯æ¨èç³»ç»Ÿï¼ˆCRSsï¼‰é€šè¿‡å¤šè½®äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›åº”ç”¨äºCRSså…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„äº¤äº’ç­–ç•¥ä¼˜åŒ–ï¼Œä¾èµ–äºç»Ÿä¸€æç¤ºï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚æœ¬æ–‡æå‡ºå¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰çš„åˆ†å±‚æ¡†æ¶ï¼Œå°†å“åº”ç”Ÿæˆåˆ†è§£ä¸ºå®è§‚ç­–ç•¥è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„ä¸“å®¶ç½‘ç»œå†…çš„é€‚åº”ã€‚è§„åˆ’å™¨é€‰æ‹©ç­–ç•¥ï¼ˆå¦‚æ¨èã€è§£é‡Šã€é¼“åŠ±ï¼‰ï¼Œè€ŒActoræ ¹æ®è¾…åŠ©ä¸“å®¶ç”Ÿæˆå“åº”ä»¥å®ç°åå¥½å’Œäº‹å®ä¾æ®ã€‚å®éªŒè¯æ˜RSOä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿ï¼ŒéªŒè¯äº†åˆ†å±‚ç­–ç•¥ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹è¯æ¨èç³»ç»Ÿé€šè¿‡å¤šè½®äº¤äº’æä¾›ä¸ªæ€§åŒ–æ¨èã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„äº¤äº’ç­–ç•¥ä¼˜åŒ–ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚</li>
<li>æå‡ºå¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆRSOï¼‰çš„åˆ†å±‚æ¡†æ¶ï¼ŒåŒ…æ‹¬ç­–ç•¥è§„åˆ’å’Œå“åº”ç”Ÿæˆã€‚</li>
<li>ç­–ç•¥å­¦ä¹ è¢«å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±è¿›è¡Œæ¢ç´¢ã€‚</li>
<li>å®éªŒè¯æ˜RSOä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†åˆ†å±‚ç­–ç•¥ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9ad217bf54a9a435377e4c44608c5be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82abb63255d58aa7d35f7950e00a251" align="middle">
<img src="https://picx.zhimg.com/v2-409d4b3296d0641aa41d16b1c726cf1d" align="middle">
<img src="https://picx.zhimg.com/v2-71eb2af6aa2c0e697a0450b125bac247" align="middle">
<img src="https://picx.zhimg.com/v2-81777cba7ba0effbbda2e328e3ef447f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fcf7e3c4d1d0bc233c1c7767c41958a" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ba8d094778b884fb914c501034be57f7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  MLA A Multisensory Language-Action Model for Multimodal Understanding   and Forecasting in Robotic Manipulation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e53971478f121e128da20576fc95caa2" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  3DiFACE Synthesizing and Editing Holistic 3D Facial Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
