<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-10-02  Voice Evaluation of Reasoning Ability Diagnosing the Modality-Induced   Performance Gap">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d74e5a95829cb9039559b584f76a172f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087740&auth_key=1760087740-0-0-0ec976e0c001e3378aadc9f1c4cf1b79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-02-更新"><a href="#2025-10-02-更新" class="headerlink" title="2025-10-02 更新"></a>2025-10-02 更新</h1><h2 id="Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap"><a href="#Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap" class="headerlink" title="Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap"></a>Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap</h2><p><strong>Authors:Yueqian Lin, Zhengmian Hu, Qinsi Wang, Yudong Liu, Hengfan Zhang, Jayakumar Subramanian, Nikos Vlassis, Hai Helen Li, Yiran Chen</strong></p>
<p>We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing “thinking time” yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding&#x2F;consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned. </p>
<blockquote>
<p>我们推出了语音推理能力评估（VERA），这是一个基准测试，用于评估实时对话约束下语音交互系统中的推理能力。VERA包含从既定文本基准测试中得出的2931个原生语音片段，这些片段被组织成五个赛道（数学、网络、科学、长语境、事实）。每个项目都适应语音交互，同时保留推理难度。VERA能够在模型家族内进行直接的文本语音比较，并支持分析架构选择如何影响可靠性。我们评估了12个当代语音系统以及强大的文本基准，并观察到了一致且显著的模态差距：在数学竞赛中，领先的文本模型达到74.8%的准确率，而其语音对应模型只达到6.1%；跨赛道的宏观平均，最佳文本模型达到54.0%，而语音模型为11.3%。延迟-准确性分析显示了一个低延迟平台，在这个平台上，快速语音系统聚集在约10%的准确率，而要达到文本性能则需要牺牲实时交互。诊断实验表明常见的缓解措施并不足够。增加“思考时间”产生的收益微乎其微；一个分离的级联，将推理与叙述分开，可以提高准确性，但仍远远落后于文本，并引入典型的接地&#x2F;一致性错误。故障分析进一步显示了原生流媒体、端到端和级联设计之间的不同错误签名。VERA为脱离思考和说话过程的架构提供了一个可重复的测试平台和有针对性的诊断，为实时语音助手提供了一-种既有流利度又可靠推理的衡量进步的原则性方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26542v1">PDF</a> Code and data available at <a target="_blank" rel="noopener" href="https://github.com/linyueqian/VERA">https://github.com/linyueqian/VERA</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了语音推理能力评估基准（VERA），用于评估语音交互系统在面对真实对话时的推理能力。VERA包含从已建立的文本基准数据中衍生的2,931个语音本集，分为五个轨道（数学、网络、科学、长语境、事实）。每个项目都适应了语音交互，同时保留了推理难度。VERA使直接文本与语音对比成为可能，支持对模型家族内部架构选择对可靠性的影响的分析。通过对当前语音系统与强大文本基准的评估，发现显著的、一致的模态差距。竞争数学题中，顶尖文本模型准确率达74.8%，而语音模型仅达6.1%。宏观平均轨道上，最佳文本模型准确率为54%，而语音模型仅为11.3%。延迟准确分析揭示低延迟平台区域，快速语音系统准确率约为10%，而接近文本性能则需要牺牲实时交互。诊断实验表明常见的缓解方法不足以提高性能。增加思考时间带来的收益微乎其微；一个分离的级联设计改善了准确性，但仍远未达到文本性能，并引入特定的定位或一致性错误。失败分析显示本地流媒体、端到端和级联设计有各自独特的错误特征。VERA为架构提供了一个可重复的测试平台和有针对性的诊断工具，这些架构将思考与说话分开，为构建既流畅又可靠的实时语音助手提供了衡量进步的原则方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERA是一个用于评估语音交互系统推理能力的基准。</li>
<li>VERA包含多个轨道，涵盖不同的领域和语境。</li>
<li>语音系统与文本系统性能存在显著差异，尤其在复杂任务上。</li>
<li>延迟与准确性之间存在权衡，快速响应往往导致准确性下降。</li>
<li>增加思考时间对改善语音系统性能效果有限。</li>
<li>分离推理与叙述的级联设计能提高准确性，但仍存在特定误差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26542">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c928f4b72727355ef4248023b1452c57~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087748&auth_key=1760087748-0-0-77b64f45301f4f109cb1950087709dc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05f433408fe64cdd8d00120710857eca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087755&auth_key=1760087755-0-0-643d488203a75a735726b9d043715b68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6ef95ac35a88562545ef94075641310~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087761&auth_key=1760087761-0-0-0708c0c37c30f6d5eae92f88d5c4dd18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f619f0b8ac1d9e26b96438d9c45c2eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087768&auth_key=1760087768-0-0-9c1f9b0287acabca2f872c6d22faefe6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb95fd25fee79ff19ef189e065f78982~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104255&auth_key=1760104255-0-0-db21d4f3e7b01e55dba766286762889e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5cefd7b8480dc50e6ad969eb8a76f801~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087839&auth_key=1760087839-0-0-6abf61e9f826a32deaa595ecdfa1964b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Analysis-of-Joint-Nonlinear-Spatial-Filtering-for-Spatial-Aliasing-Reduction"><a href="#An-Analysis-of-Joint-Nonlinear-Spatial-Filtering-for-Spatial-Aliasing-Reduction" class="headerlink" title="An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction"></a>An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction</h2><p><strong>Authors:Alina Mannanova, Jakob Kienegger, Timo Gerkmann</strong></p>
<p>The performance of traditional linear spatial filters for speech enhancement is constrained by the physical size and number of channels of microphone arrays. For instance, for large microphone distances and high frequencies, spatial aliasing may occur, leading to unwanted enhancement of signals from non-target directions. Recently, it has been proposed to replace linear beamformers by nonlinear deep neural networks for joint spatial-spectral processing. While it has been shown that such approaches result in higher performance in terms of instrumental quality metrics, in this work we highlight their ability to efficiently handle spatial aliasing. In particular, we show that joint spatial and tempo-spectral processing is more robust to spatial aliasing than traditional approaches that perform spatial processing alone or separately with tempo-spectral filtering. The results provide another strong motivation for using deep nonlinear networks in multichannel speech enhancement, beyond their known benefits in managing non-Gaussian noise and multiple speakers, especially when microphone arrays with rather large microphone distances are used. </p>
<blockquote>
<p>传统线性空间滤波器在语音增强方面的性能受到麦克风阵列的物理尺寸和通道数量的限制。例如，在较大的麦克风距离和较高频率下，可能会发生空间混叠，导致来自非目标方向信号的意外增强。最近，有人建议使用非线性深度神经网络替代线性波束形成器进行联合空间频谱处理。虽然已有研究表明，这种方法在仪器质量指标方面表现出更高的性能，但在这项工作中，我们重点强调了它们处理空间混叠的有效性。特别是我们显示联合空间和时态频谱处理比传统方法更稳健，后者单独进行空间处理或时态频谱滤波的分离处理会出现混叠。结果除了已知的在对抗非高斯噪声和多说话人时具有优势之外，在多通道语音增强领域中使用深度非线性网络的动机愈发强烈，尤其是在使用具有较大麦克风距离的麦克风阵列时更是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25982v1">PDF</a> Submitted to ICASSP 2026. This work has been submitted to the IEEE   for possible publication</p>
<p><strong>摘要</strong></p>
<p>传统的线性空间滤波器在进行语音增强时的性能受限于麦克风阵列的物理尺寸和通道数量。在大麦克风距离和高频情况下，会发生空间混叠，导致非目标方向信号的意外增强。最近提议用非线性深度神经网络替代线性波束形成器进行联合空间光谱处理。虽然已证明这些方法在仪器质量指标方面表现更好，但在这项工作中，我们重点强调了它们处理空间混叠的能力。特别是，我们显示联合空间和时序光谱处理比传统方法更稳健对抗空间混叠，后者单独或分别进行时频滤波处理。结果除了它们在管理非高斯噪声和多说话人方面的已知优势外，还提供了在多通道语音增强中使用深度非线性网络的另一个强烈动机，特别是在使用间距较大的麦克风阵列时更是如此。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>传统线性空间滤波器在语音增强方面受限于麦克风阵列的物理尺寸和通道数量。</li>
<li>在大麦克风距离和高频时，会发生空间混叠，导致性能下降。</li>
<li>非线性深度神经网络已被提议用于联合空间光谱处理，以提高性能。</li>
<li>深度神经网络在处理空间混叠方面表现出色，与传统的空间处理方法相比更加稳健。</li>
<li>联合空间和时序光谱处理可以提供更好的稳健性，对抗空间混叠问题。</li>
<li>使用深度非线性网络进行多通道语音增强具有优势，尤其是在处理非高斯噪声和多说话人场景时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a14660b06fee3fab20a57c14e3e7060f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087847&auth_key=1760087847-0-0-03d151f1ae44e293700fad75fb94c332&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9ab0a8e398fb77127dd05a88f230dbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104261&auth_key=1760104261-0-0-939ce75e64768958bb51f07d71c9940d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e284d309c504d6cda41107655bdb9fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104267&auth_key=1760104267-0-0-1ccda7b0c53b6ee218b07f03ab684d11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-02798b693d23efe9e7107b276debbfdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104274&auth_key=1760104274-0-0-415e72026b0226c88d86382336afc624&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-02a10e5f2660c86f520aeb71b938b2e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104280&auth_key=1760104280-0-0-7846346cc435bc1c2df2de6b95d7fec0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17d3b9ca57f6aac43d39913c28421742~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104286&auth_key=1760104286-0-0-e7f56d6588a596be810f56e72ea5ba87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Detecting-Hope-Across-Languages-Multiclass-Classification-for-Positive-Online-Discourse"><a href="#Detecting-Hope-Across-Languages-Multiclass-Classification-for-Positive-Online-Discourse" class="headerlink" title="Detecting Hope Across Languages: Multiclass Classification for Positive   Online Discourse"></a>Detecting Hope Across Languages: Multiclass Classification for Positive   Online Discourse</h2><p><strong>Authors:T. O. Abiola, K. D. Abiodun, O. E. Olumide, O. O. Adebanji, O. Hiram Calvo, Grigori Sidorov</strong></p>
<p>The detection of hopeful speech in social media has emerged as a critical task for promoting positive discourse and well-being. In this paper, we present a machine learning approach to multiclass hope speech detection across multiple languages, including English, Urdu, and Spanish. We leverage transformer-based models, specifically XLM-RoBERTa, to detect and categorize hope speech into three distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope. Our proposed methodology is evaluated on the PolyHope dataset for the PolyHope-M 2025 shared task, achieving competitive performance across all languages. We compare our results with existing models, demonstrating that our approach significantly outperforms prior state-of-the-art techniques in terms of macro F1 scores. We also discuss the challenges in detecting hope speech in low-resource languages and the potential for improving generalization. This work contributes to the development of multilingual, fine-grained hope speech detection models, which can be applied to enhance positive content moderation and foster supportive online communities. </p>
<blockquote>
<p>在社交媒体中检测带有积极情绪的话语已成为促进积极对话和福祉的重要任务。本文提出了一种基于机器学习的方法，用于多语言环境中的多元希望言论检测，包括英语、乌尔都语和西班牙语。我们利用基于transformer的模型，特别是XLM-RoBERTa，将希望话语分为三种不同的类别：通用希望、现实希望和非现实希望，并进行检测与分类。我们提出的方法在PolyHope数据集上对PolyHope-M 2025共享任务进行了评估，在所有语言上均取得了具有竞争力的表现。我们将结果与现有模型进行了比较，证明我们的方法在宏观F1得分方面大大优于现有先进技术。我们还讨论了在低资源语言中检测希望言论的挑战以及提高泛化能力的潜力。这项工作为多语种精细粒度的希望言论检测模型的发展做出了贡献，可应用于积极内容的管理和支持型在线社区的建设。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25752v1">PDF</a> </p>
<p><strong>Summary</strong><br>社交媒体中希望言论的检测对促进积极对话和心理健康至关重要。本文提出一种跨多种语言的希望言论机器学习检测方案，包括英语、乌尔都语和西班牙语。我们利用基于transformer的模型XLM-RoBERTa来检测和分类希望言论为三种不同类别：通用型希望、现实型希望和幻想型希望。该方法在PolyHope数据集上针对PolyHope-M 2025共享任务进行评估，所有语言上的表现均表现优异。相较于现有模型，我们的方法在提高宏观F1分数方面显著优于先前最先进的技巧。我们还讨论了在低资源语言中检测希望言论的挑战以及提高泛化能力的潜力。本研究为开发多语言、精细化的希望言论检测模型做出贡献，可应用于增强积极内容管理和促进支持性在线社区。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>希望言论检测在社交媒体中对于促进积极对话和心理健康至关重要。</li>
<li>提出一种跨多种语言的希望言论机器学习检测方案，包括英语、乌尔都语和西班牙语。</li>
<li>利用基于transformer的模型XLM-RoBERTa进行希望言论检测和分类。</li>
<li>将希望言论分为三种类别：通用型希望、现实型希望和幻想型希望。</li>
<li>在PolyHope数据集上的评估表现优异，并显著优于先前最先进的技巧。</li>
<li>讨论了在低资源语言中进行希望言论检测的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25752">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3d456422670ec3e48d1de9aed662f918~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104294&auth_key=1760104294-0-0-b1e1ebb7ee71c3f0ed5aed582e1f3d0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f6a3615689ae0df04d6c5e9c8aeaa157~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104301&auth_key=1760104301-0-0-6fabe6a7f0f0d88f326e939f3c2c2c82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a6bf72de695307a8cf35a6d0bb5fed2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104308&auth_key=1760104308-0-0-a34f98d5369ae20009b5a93422c7c0d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e1385134fc7a88920da8d962405ff9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104314&auth_key=1760104314-0-0-de79552c090fa43657e8909b4a0e64c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83a420989d0427f0d29e50817655e1dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104321&auth_key=1760104321-0-0-307eb796c47a513694010792df58f855&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba94312796163914b06e2eb121de43d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104327&auth_key=1760104327-0-0-1505e623035aad027b43f9556fed904b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f14e22c914d80ab9762f57947f0ba3b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104333&auth_key=1760104333-0-0-ce6eda58bb1a4fac895a26c1c2c3a74e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edb5e7cdb9bc5b6036debd1c1c70e100~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104340&auth_key=1760104340-0-0-b3894aba397d8470dd34bb42c3d7901b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LTA-L2S-Lexical-Tone-Aware-Lip-to-Speech-Synthesis-for-Mandarin-with-Cross-Lingual-Transfer-Learning"><a href="#LTA-L2S-Lexical-Tone-Aware-Lip-to-Speech-Synthesis-for-Mandarin-with-Cross-Lingual-Transfer-Learning" class="headerlink" title="LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with   Cross-Lingual Transfer Learning"></a>LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with   Cross-Lingual Transfer Learning</h2><p><strong>Authors:Kang Yang, Yifan Liang, Fangkun Liu, Zhenping Xie, Chengshi Zheng</strong></p>
<p>Lip-to-speech (L2S) synthesis for Mandarin is a significant challenge, hindered by complex viseme-to-phoneme mappings and the critical role of lexical tones in intelligibility. To address this issue, we propose Lexical Tone-Aware Lip-to-Speech (LTA-L2S). To tackle viseme-to-phoneme complexity, our model adapts an English pre-trained audio-visual self-supervised learning (SSL) model via a cross-lingual transfer learning strategy. This strategy not only transfers universal knowledge learned from extensive English data to the Mandarin domain but also circumvents the prohibitive cost of training such a model from scratch. To specifically model lexical tones and enhance intelligibility, we further employ a flow-matching model to generate the F0 contour. This generation process is guided by ASR-fine-tuned SSL speech units, which contain crucial suprasegmental information. The overall speech quality is then elevated through a two-stage training paradigm, where a flow-matching postnet refines the coarse spectrogram from the first stage. Extensive experiments demonstrate that LTA-L2S significantly outperforms existing methods in both speech intelligibility and tonal accuracy. </p>
<blockquote>
<p>针对普通话的唇音到语音（L2S）合成是一个重大挑战，受到复杂唇动到音素映射以及词汇音在可理解性中的关键作用的阻碍。为了解决这个问题，我们提出了词汇音感知的唇音到语音（LTA-L2S）合成方法。为了解决唇动到音素的复杂性，我们的模型通过跨语言迁移学习策略，对一个英语预训练的音频视觉自我监督学习（SSL）模型进行适配。这一策略不仅将从大量英语数据中学习到的通用知识转移到普通话领域，而且避免了从头开始训练此类模型的昂贵成本。为了专门建模词汇音并增强可理解性，我们进一步采用流匹配模型来生成F0轮廓。这一生成过程由经过ASR精细调整的SSL语音单元引导，这些单元包含关键的超音段信息。通过两阶段训练范式，整体语音质量得到提升，其中流匹配后网络对第一阶段产生的粗糙频谱图进行细化。大量实验表明，LTA-L2S在语音可理解性和音调准确性方面都显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25670v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>针对普通话的唇动到语音合成（L2S）存在挑战，主要由于复杂的唇动到音素映射以及词汇音调在可理解性中的关键作用。我们提出了词汇音调感知的唇动到语音合成（LTA-L2S）来解决这一问题。我们的模型通过跨语言迁移学习策略，借鉴英语预训练的音频视觉自我监督学习（SSL）模型来解决音素映射的复杂性。这一策略不仅将通用知识从大量的英语数据转移到普通话领域，而且避免了从头开始训练此类模型的昂贵成本。为了专门建模词汇的音调并增强可理解性，我们进一步采用流匹配模型来生成F0轮廓。这一生成过程由ASR微调SSL语音单元引导，这些单元包含关键的超分段信息。总体而言，通过两阶段训练模式提高语音质量，第一阶段产生的粗略频谱图通过流匹配后网进行细化。实验表明，LTA-L2S在语音可理解性和音调准确性方面均显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>普通话的唇动到语音合成面临挑战，主要是由于复杂的音素映射和词汇音调的重要性。</li>
<li>提出了词汇音调感知的唇动到语音合成（LTA-L2S）来解决这一问题。</li>
<li>通过跨语言迁移学习策略，借鉴英语预训练的音频视觉自我监督学习模型来解决音素映射的复杂性。</li>
<li>采用流匹配模型生成F0轮廓，以增强语音的音调表现和可理解性。</li>
<li>ASR微调SSL语音单元提供关键的超分段信息，指导F0轮廓的生成。</li>
<li>两阶段训练模式用于提高语音质量，其中包括流匹配后网对第一阶段产生的粗略频谱图的细化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-df5262eb29dc7274526644a08b320244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104347&auth_key=1760104347-0-0-e6c7a3671aceba18ee6cca87fde5ecb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d57f471a646f08535744e3bc589490be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104355&auth_key=1760104355-0-0-847bd7afb63f72b8ec733a021aeb8bbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d313306de4a2777124ef9c7d0bbaa7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104361&auth_key=1760104361-0-0-bdd23838b44aa552754220ee83fe863b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-Emotion-Graphs-for-Compositional-Prompting-in-Zero-Shot-Speech-Emotion-Recognition"><a href="#Plug-and-Play-Emotion-Graphs-for-Compositional-Prompting-in-Zero-Shot-Speech-Emotion-Recognition" class="headerlink" title="Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot   Speech Emotion Recognition"></a>Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot   Speech Emotion Recognition</h2><p><strong>Authors:Jiacheng Shi, Hongfei Du, Y. Alicia Hong, Ye Gao</strong></p>
<p>Large audio-language models (LALMs) exhibit strong zero-shot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a framework that introduces structured Emotion Graphs (EGs) to guide LALMs in emotion inference without fine-tuning. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. Embedded into prompts, EGs provide interpretable and compositional representations that enhance LALM reasoning. Experiments across SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy over zero-shot baselines. </p>
<blockquote>
<p>大型音频语言模型（LALM）在语音任务中表现出强大的零样本性能，但由于缺乏副语言建模和有限的跨模态推理，在语音情感识别（SER）方面遇到了困难。我们提出了面向情感推理的组成思维链提示（CCoT-Emo）框架，该框架引入结构化情感图（EG）来指导LALM进行情感推断，而无需微调。每个情感图编码七种声学特征（例如音调、语速、抖动、颤音）、文本情感、关键词和跨模态关联。嵌入到提示中，情感图提供可解释和组合表示，增强了LALM推理。在SER基准测试上的实验表明，CCoT-Emo优于先前最佳技术，并提高了零样本基准测试的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25458v1">PDF</a> </p>
<p><strong>Summary</strong>：针对大型音频语言模型（LALM）在语音情感识别（SER）方面的不足，提出了基于情感图的情绪推理框架Compositional Chain-of-Thought Prompting（CCoT-Emo）。该框架通过引入情感图来引导LALM进行情感推理，无需微调。情感图结合了语音的声学特征、文本情感、关键词和跨模态关联，嵌入到提示中，提供了可解释和组合性的表示，提高了LALM的推理能力。实验表明，CCoT-Emo在SER基准测试中表现优于先前的方法，并在零样本基准上提高了准确性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型音频语言模型（LALMs）在语音情感识别（SER）方面存在挑战，主要是由于缺乏旁语言学建模和跨模态推理。</li>
<li>提出了Compositional Chain-of-Thought Prompting for Emotion Reasoning（CCoT-Emo）框架，通过引入情感图来改进LALMs在SER方面的性能。</li>
<li>情感图结合了语音的声学特征、文本情感和关键词，以及跨模态关联。</li>
<li>嵌入式情感图提供可解释和组合性的表示，增强LALM的推理能力。</li>
<li>CCoT-Emo框架无需微调，即可提高LALMs在SER任务上的性能。</li>
<li>实验结果表明，CCoT-Emo在SER基准测试中表现优于先前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1f26a90754d65cbd0ea4b9d96653d049~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104369&auth_key=1760104369-0-0-405e44f09c2b8eae00d5e2ec9d27e02b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-209cd58abc22cd4f47e7bcf02673576f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104376&auth_key=1760104376-0-0-332802bf5c10d5d0089bb431cfea9988&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6189f8f6de56edc25c323358a60e86f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104383&auth_key=1760104383-0-0-94188860d0d8f08afa3af1e18fe4456b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13a4f1521b0d5f1456bbdbfb3b508035~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104390&auth_key=1760104390-0-0-1096f2b56bb8e60d5fbd902a0a0857ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VoiceBridge-Designing-Latent-Bridge-Models-for-General-Speech-Restoration-at-Scale"><a href="#VoiceBridge-Designing-Latent-Bridge-Models-for-General-Speech-Restoration-at-Scale" class="headerlink" title="VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale"></a>VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale</h2><p><strong>Authors:Chi Zhang, Zehua Chen, Kaiwen Zheng, Jun Zhu</strong></p>
<p>Bridge models have recently been explored for speech enhancement tasks such as denoising, dereverberation, and super-resolution, while these efforts are typically confined to a single task or small-scale datasets, with constrained general speech restoration (GSR) capability at scale. In this work, we introduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs), capable of reconstructing high-fidelity speech at full-band (\textit{i.e.,} 48<del>kHz) from various distortions. By compressing speech waveform into continuous latent representations, VoiceBridge models the</del>\textit{diverse LQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\textit{a single latent-to-latent generative process} backed by a scalable transformer architecture. To better inherit the advantages of bridge models from the data domain to the latent space, we present an energy-preserving variational autoencoder, enhancing the alignment between the waveform and latent space over varying energy levels. Furthermore, to address the difficulty of HQ reconstruction from distinctively different LQ priors, we propose a joint neural prior, uniformly alleviating the reconstruction burden of LBM. At last, considering the key requirement of GSR systems, human perceptual quality, a perceptually aware fine-tuning stage is designed to mitigate the cascading mismatch in generation while improving perceptual alignment. Extensive validation across in-domain and out-of-domain tasks and datasets (\textit{e.g.}, refining recent zero-shot speech and podcast generation results) demonstrates the superior performance of VoiceBridge. Demo samples can be visited at: <a target="_blank" rel="noopener" href="https://voicebridge-demo.github.io/">https://VoiceBridge-demo.github.io/</a>. </p>
<blockquote>
<p>最近，桥梁模型已被探索用于语音增强任务，如去噪、消除回声和超分辨率。然而，这些努力通常局限于单一任务或小规模数据集，大规模通用语音恢复（GSR）能力受限。在这项工作中，我们引入了VoiceBridge，这是一个基于潜在桥梁模型（LBM）的GSR系统，能够从各种失真中重建全频带（即48kHz）的高保真语音。通过将语音波形压缩成连续潜在表示，VoiceBridge用一个潜在空间到潜在空间的生成过程来模拟GSR中的多种LQ-to-HQ任务（即从低质量到高质量）。为了更好地从数据域继承桥梁模型的优势到潜在空间，我们提出了一种能量保持式变分自动编码器，增强了波形和潜在空间在不同能量水平上的对齐。此外，为了解决从明显不同的LQ先验中进行HQ重建的困难，我们提出了一个联合神经先验，统一缓解了LBM的重建负担。最后，考虑到GSR系统的关键需求——人类感知质量，设计了一个感知意识微调阶段，旨在减轻生成过程中的级联不匹配问题，同时提高感知对齐。在域内和域外任务和数据集上的广泛验证（例如，改进最近的零样本语音和播客生成结果）表明VoiceBridge的卓越性能。演示样品可访问：<a target="_blank" rel="noopener" href="https://voicebridge-demo.github.io/%E3%80%82">https://VoiceBridge-demo.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25275v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了基于潜在桥梁模型（LBMs）的VoiceBridge系统，用于通用语音恢复（GSR）。该系统能够重建高质量的全频带语音，并具有从各种失真中恢复语音的能力。通过压缩语音波形到连续的潜在表示，VoiceBridge用一个潜在到潜在生成过程对各种低质量到高质量的任务进行建模。通过引入能量保持变分自编码器，增强波形和潜在空间之间在不同能量水平上的对齐。针对从独特低质量先验重构高质量语音的困难，提出了一种联合神经先验，减轻了LBM的重构负担。最后，考虑到GSR系统的关键需求——人类感知质量，设计了一个感知意识微调阶段，以缓解生成过程中的级联不匹配问题，同时提高感知对齐。在跨领域任务和数据集上的广泛验证表明VoiceBridge的卓越性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VoiceBridge是一个基于潜在桥梁模型（LBMs）的通用语音恢复（GSR）系统。</li>
<li>能够重建高质量的全频带（即48 kHz）语音，并从各种失真中恢复语音。</li>
<li>通过连续潜在表示压缩语音波形，用一个潜在到潜在生成过程对各种低质量到高质量的任务进行建模。</li>
<li>引入能量保持变分自编码器，增强波形和潜在空间之间在不同能量水平的对齐。</li>
<li>提出了一种联合神经先验，以处理从独特低质量先验重构高质量语音的难题。</li>
<li>设计了一个感知意识微调阶段，以提高语音恢复的感知质量和缓解生成过程中的不匹配问题。</li>
<li>VoiceBridge在跨领域任务和数据集上的表现经过广泛验证，并展示了其卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-753fa7aeaad52253e795f5c525861223~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104398&auth_key=1760104398-0-0-7ba3e4f42c0a23ad7010a82fcc06d4f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a72c8e3435a05fe7739f00884815727f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104405&auth_key=1760104405-0-0-4b629c7a50a112b288101592e4771cef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-527c3c28fbf313182a21faa731596ab8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104412&auth_key=1760104412-0-0-6f269a5968ca0783c76bdb79381058a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-113056ddee9bf190f2f997576b8030a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104419&auth_key=1760104419-0-0-35ecb953bf593378181a1fb60008d8c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36052514ba00e6141472da8bdf530c06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104426&auth_key=1760104426-0-0-1e6c9449d473aa8a13630e521b6c02ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning"><a href="#VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning" class="headerlink" title="VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning"></a>VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning</h2><p><strong>Authors:Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song</strong></p>
<p>Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models. </p>
<blockquote>
<p>视频条件的声音和语音生成，包括视频到声音（V2S）和视觉文本到语音（VisualTTS）任务，传统上被视为单独的任务，并且在统一框架内对其进行探索的尝试有限。最近尝试将V2S和VisualTTS统一起来，在处理不同的条件类型（例如，不同的视频和文本条件）方面面临挑战，并且需要复杂的训练阶段。统一这两个任务仍然是一个悬而未决的问题。为了弥合这一差距，我们提出了VSSFlow，它无缝地将V2S和VisualTTS任务集成到一个统一的流匹配框架中。VSSFlow使用了一种新型的条件聚合机制来处理不同的输入信号。我们发现，在引入条件的过程中，跨注意力和自注意力层表现出不同的归纳偏置。因此，VSSFlow利用这些归纳偏置来有效地处理不同的表示形式：跨注意力用于模糊的视频条件，自注意力用于更确定的语音文本。此外，与我们普遍认为的联合训练这两个任务需要复杂的训练策略并可能降低性能相反，我们发现VSSFlow受益于声音和语音生成的端到端联合学习过程，而无需在训练阶段进行额外设计。详细分析将其归因于任务之间学到的共享通用音频先验，这加速了收敛，增强了条件生成，并稳定了无分类器指导过程。大量实验表明，VSSFlow超越了V2S和VisualTTS基准测试领域的最新技术，突显出统一生成模型的关键潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24773v2">PDF</a> Paper Under Review</p>
<p><strong>Summary</strong></p>
<p>本文提出了VSSFlow，一个将视频转声音（V2S）和视觉文本转语音（VisualTTS）任务统一于一个框架的方法。VSSFlow使用新型的条件聚合机制来处理不同的输入信号，利用交叉注意力和自注意力层的不同诱导偏差来处理不同的表现。此外，研究发现联合训练两个任务无需复杂的训练策略，反而能从端到端的联合学习过程获益，这得益于任务间共享的通用音频先验知识。VSSFlow在V2S和VisualTTS基准测试中均超越现有技术，展现了统一生成模型的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VSSFlow是一个统一的框架，整合了视频转声音（V2S）和视觉文本转语音（VisualTTS）任务。</li>
<li>VSSFlow采用新型条件聚合机制处理不同输入信号。</li>
<li>交叉注意力和自注意力层在VSSFlow中用于处理不同的表现。</li>
<li>联合训练两个任务无需复杂策略，得益于端到端的联合学习过程。</li>
<li>任务间共享的通用音频先验知识加速了收敛，增强了条件生成，稳定了无分类器指导过程。</li>
<li>VSSFlow在V2S和VisualTTS基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24773">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-67983a5214bf034a608ca1004e6a7bfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104433&auth_key=1760104433-0-0-87d9420e0cc790a90012f54566704883&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa383b8af756e9c393499fe593a9596d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104440&auth_key=1760104440-0-0-74df8b87afb443402053493d66a50f49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3beec9a7fa106c34c7d5761339c47c73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104447&auth_key=1760104447-0-0-7b92e343868c2602649ae7dbf39c3c09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow"><a href="#MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow" class="headerlink" title="MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow"></a>MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow</h2><p><strong>Authors:Yike Zhu, Boyi Kang, Ziqian Wang, Xingchen Li, Zihan Zhang, Wenjie Li, Longshuai Xiao, Wei Xue, Lei Xie</strong></p>
<p>Speech enhancement (SE) recovers clean speech from noisy signals and is vital for applications such as telecommunications and automatic speech recognition (ASR). While generative approaches achieve strong perceptual quality, they often rely on multi-step sampling (diffusion&#x2F;flow-matching) or large language models, limiting real-time deployment. To mitigate these constraints, we present MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to predict an average-velocity field for one-step latent refinement and conditions the model on self-supervised learning (SSL) representations rather than VAE latents. This design accelerates inference and provides robust acoustic-semantic guidance during training. In the Interspeech 2020 DNS Challenge blind test set and simulated test set, MeanFlowSE attains state-of-the-art (SOTA) level perceptual quality and competitive intelligibility while significantly lowering both real-time factor (RTF) and model size compared with recent generative competitors, making it suitable for practical use. The code will be released upon publication at <a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE">https://github.com/Hello3orld/MeanFlowSE</a>. </p>
<blockquote>
<p>语音增强（SE）从带噪声的信号中恢复清洁语音，对电信和自动语音识别（ASR）等应用至关重要。虽然生成方法达到了很强的感知质量，但它们通常依赖于多步采样（扩散&#x2F;流匹配）或大型语言模型，限制了实时部署。为了缓解这些约束，我们提出了MeanFlowSE，这是一个一步生成SE框架。它采用MeanFlow来预测一步潜在精化的平均速度场，并以自我监督学习（SSL）表示而不是VAE潜在进行模型训练。这种设计加速了推理，并在训练过程中提供了稳健的声学语义指导。在Interspeech 2020 DNS Challenge的盲测试集和模拟测试集中，MeanFlowSE达到了最先进的感知质量和有竞争力的清晰度，同时大大降低了实时因子（RTF）和模型大小，与最近的生成竞争对手相比，适合实际应用。代码将在<a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Hello3orld/MeanFlowSE上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23299v2">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>摘要</strong></p>
<p>语音增强（SE）从噪声信号中恢复清洁语音，对电信和自动语音识别（ASR）等应用至关重要。虽然生成方法可以达到很强的感知质量，但它们通常依赖于多步采样（扩散&#x2F;流匹配）或大型语言模型，限制了实时部署。为了缓解这些约束，我们提出了MeanFlowSE，这是一个一步生成SE框架。它采用MeanFlow预测平均速度场进行一步潜在细化，并在自我监督学习（SSL）表示的基础上对模型进行条件设置，而不是VAE潜变量。这种设计加速了推理，并在训练过程中提供了稳健的声学语义指导。在Interspeech 2020 DNS Challenge的盲测试集和模拟测试集中，MeanFlowSE达到了先进的感知质量和有竞争力的可理解性，同时大大降低了实时因子（RTF）和模型大小，与最近的生成竞争对手相比，适合实际应用。代码将在<a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Hello3orld/MeanFlowSE发布。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>语音增强（SE）在电信和自动语音识别（ASR）等应用中至关重要。</li>
<li>生成方法虽然能达成高感知质量，但存在实时部署的局限性。</li>
<li>MeanFlowSE是一个一步生成SE框架，采用MeanFlow预测平均速度场进行潜在细化。</li>
<li>MeanFlowSE在自我监督学习（SSL）表示的基础上进行条件设置，加速推理并提供稳健的声学语义指导。</li>
<li>在Interspeech 2020 DNS Challenge的测试中，MeanFlowSE达到先进感知质量和竞争力可理解性。</li>
<li>与其他生成方法相比，MeanFlowSE降低实时因子（RTF）和模型大小。</li>
<li>MeanFlowSE适合实际应用，代码将在相关仓库发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b409c0d5ddc4350f7c4ca4fe3f726dab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104454&auth_key=1760104454-0-0-213f93b8fc61e40a3c4148973d0e8b5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6001b1a912c0553937b4279f34ed1085~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104461&auth_key=1760104461-0-0-6b86c9206bb396e9343a0b1e5ccca400&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0b72033986e0860109dbe26d764c7aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104468&auth_key=1760104468-0-0-cf3bc355a156c57a8f8057d977f4238a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2564543a513ee6228feb84dafc4fc5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104475&auth_key=1760104475-0-0-53be85806e3b70f758e11a3b6ec56f39&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>我们实验了一个低延迟、端到端的语音到语音通信模型，以优化其适用于实时对话应用。通过分析语音到语音（V-2-V）系统所必需的关键组件，即自动语音识别（ASR）、文本到语音（TTS）和对话管理，我们的工作分析了如何在保持高质量交互的同时减少处理时间，以确定优化V-2-V系统的关键杠杆。我们的工作发现，生成充满情感、逼真的语音的TTS组件（包括自然停顿和感叹）对实时因子（RTF）的影响最大。所试验的V-2-V架构利用CSM1b，通过摄取先前的音频和文本交换，理解对话的语调以及语境，从而生成语境准确的语音。我们探索了通过TTS解码器优化剩余向量量化（RVQ）迭代的方法，虽然这会降低生成的语音质量。我们的实验评估还表明，对于基于CSM的V-2-V实现，最重要的优化可以通过减少RVQ迭代次数以及与Mimi中使用的代码簿相结合来实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v2">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved. Its accepted at AIML Systems 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了针对实时语音对话应用的端到端语音通信模型的优化策略。通过分析自动语音识别（ASR）、文本到语音（TTS）和对话管理等关键环节，探索减少处理时间的同时保持高质量互动的方法。研究发现，生成情感丰富、带有自然停顿和语气词的语音的TTS组件对实时因子（RTF）影响最大。实验的V-2-V架构通过利用CSM技术，可以理解和生成与对话内容和语气相关的语音，同时通过优化TTS解码器的残差向量量化（RVQ）迭代来提升效率。实验结果展示了对RVQ迭代的优化可以在一定程度上减少生成的语音质量损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究针对端到端语音通信模型的优化策略，用于实时语音对话应用。</li>
<li>分析自动语音识别（ASR）、文本到语音（TTS）和对话管理等关键环节对实时因子（RTF）的影响。</li>
<li>发现TTS组件是影响RTF最大的部分，它能生成带有情感和自然停顿的语音。</li>
<li>利用CSM技术理解和生成与对话内容和语气相关的语音，提升交互质量。</li>
<li>优化TTS解码器的RVQ迭代以提升效率，同时减少生成的语音质量损失。</li>
<li>实验评估表明，减少RVQ迭代次数和代码本的使用是实现V-2-V系统优化的关键手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-59c821b57aeaa9534dba35b42a820d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104483&auth_key=1760104483-0-0-c3eabb15f00bb310e45bef4241d5db12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d49adfa80e0c9bd41043f26e3e711e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104490&auth_key=1760104490-0-0-9bbb8bcf7ee78dadc2be86ca43122633&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b34709ac926874eba4eb35aeca95e069~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104496&auth_key=1760104496-0-0-9dc5d0c2b8d26965feb6a71d9a73c4f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9be302502aa6d7901d79ac3ba0d45e1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104503&auth_key=1760104503-0-0-deeac05fc7d3e52a6c0a1fd687f86e58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0415f218174ef0ecb084055ac4fadefd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104510&auth_key=1760104510-0-0-16d3c88a0a9a3a2ba630a2f35584d9d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c674493e85b2953c23110196944d153~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104517&auth_key=1760104517-0-0-c9c3e845bf6823a7be49ee2d749caae4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0750e443c0ec570d2f4ab834004e6752~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104523&auth_key=1760104523-0-0-06cba7efc505972cf2d1fa2e90096bcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding"><a href="#SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding" class="headerlink" title="SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding"></a>SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding</h2><p><strong>Authors:Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao</strong></p>
<p>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">https://github.com/ShawnPi233/SynParaSpeech</a>. </p>
<blockquote>
<p>类语言声音，如笑声和叹息声，对于合成更真实、更吸引人的语音至关重要。然而，现有方法通常依赖于专有数据集，而公开可用的资源往往存在语音不完整、时间戳不准确或缺失、以及现实世界相关性有限等问题。为了解决这些问题，我们提出了一种自动生成大规模类语言数据集的框架，并应用该框架构建了SynParaSpeech数据集。该数据集包含6大类类语言数据，包含118.75小时的数据和精确的时间戳，所有语音均来自自然对话语音。我们的贡献在于引入第一种自动生成大规模类语言数据集的方法，并发布SynParaSpeech语料库，它通过更自然的类语言合成推进语音生成，并通过改进类语言事件检测提高语音理解。数据集和音频样本可在<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShawnPi233/SynParaSpeech获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14946v3">PDF</a> Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, including reprinting&#x2F;republishing this material for advertising or   promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>摘要</strong></p>
<p>文本指出，副语言声音（如笑声和叹息声）对于合成更真实、更吸引人的语音至关重要。然而，现有方法通常依赖于专有数据集，而公开可用的资源往往存在语音不完整、时间戳不准确或缺失、以及现实相关性有限等问题。为解决这些问题，文本提出了一种用于生成大规模副语言数据集的自动化框架，并应用该框架构建了SynParaSpeech数据集。该数据集包含6个副语言类别，共有118.75小时的数据和精确的时间戳，所有内容均来自自然对话语音。文本的主要贡献在于引入了一种构建大规模副语言数据集的自动化方法，并发布了SynParaSpeech语料库。该语料库通过更自然的副语言合成推进了语音生成，并通过提高副语言事件检测改善了语音理解。数据集和音频样本可在<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShawnPi233/SynParaSpeech获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>副语言声音对于合成更真实、更吸引人的语音至关重要。</li>
<li>现有数据集存在依赖专有数据、公开资源语音不完整、时间戳不准确或缺失等问题。</li>
<li>提出了一个自动化框架来生成大规模副语言数据集。</li>
<li>构建了包含6个副语言类别、118.75小时数据和精确时间戳的SynParaSpeech数据集。</li>
<li>该数据集来自自然对话语音，提高了现实相关性。</li>
<li>通过更自然的副语言合成推进了语音生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c9c4678560e9292fc00ffb87a0ce9026~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104531&auth_key=1760104531-0-0-9b2715bcd53ecea79884d9ad4925943e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-507156ea9b4c0462ebe753d15034235b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104538&auth_key=1760104538-0-0-b8814d324746e189fce4e704b86747e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63aea4e362ae86f0c8b5c569feb0438e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104545&auth_key=1760104545-0-0-cad678c000dd6d1ceb0e44876926a896&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-931a22f68f5e006e3bb2ea344d6cf4f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104552&auth_key=1760104552-0-0-e76bf03829780060c1966b5ccb7bde03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs"><a href="#FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs" class="headerlink" title="FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs"></a>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</h2><p><strong>Authors:Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman</strong></p>
<p>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology’s applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a>. </p>
<blockquote>
<p>语音分词能够实现对语音的离散表示，并促进语音语言建模。然而，现有的神经编码器主要捕捉低级别的声学特征，忽略了人类语音所固有的语义和上下文线索。尽管最近的努力引入了来自自监督语音模型的语义表示或结合了预训练语言模型的上下文表示，但在对齐和统一语义和上下文表示方面仍存在挑战。我们推出了FuseCodec，它通过强大的跨模态对齐和全局信息监督，统一了声学、语义和上下文表示。我们提出了三种互补的技术：（i）潜在表示融合，直接将语义和上下文特征集成到编码器潜在空间，以实现稳健和统一的表示学习；（ii）全局语义上下文监督，用全局池化和广播的表示来监督离散令牌，以增强时间一致性和跨模态对齐；（iii）临时对齐上下文监督，通过在局部窗口中动态匹配上下文和语音令牌来加强对齐，以实现精细的令牌级监督。我们还推出了FuseCodec-TTS，证明我们的方法适用于零样本语音合成。经验上，FuseCodec在LibriSpeech上达到了最先进的性能，在转录准确性、感知质量、清晰度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。结果突出了上下文和语义引导的分词法在语音分词和下游任务中的有效性。代码和预训练模型可在<a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a>中获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11425v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Speech tokenization的重要性及其在语音语言建模中的应用。现有神经编码方法主要关注低级别声学特征，忽略了语音的语义和上下文线索。文章提出了FuseCodec，通过强大的跨模态对齐和全局监督，融合了声学、语义和上下文表示。采用三种技术：潜代表融合、全局语义上下文监督、时间对齐的上下文监督。FuseCodec-TTS的引入证明了该方法在零样本语音合成中的适用性。实验结果表明，FuseCodec在LibriSpeech上的表现达到最新水平，超越了EnCodec、SpeechTokenizer和DAC，在转录准确性、感知质量、清晰度和说话人相似性方面有所改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech tokenization对于离散表示和语音语言建模至关重要。</li>
<li>现有神经编码方法主要关注声学特征而忽略了语义和上下文线索。</li>
<li>FuseCodec融合了声学、语义和上下文表示，通过强大的跨模态对齐和全局监督实现。</li>
<li>FuseCodec采用潜代表融合、全局语义上下文监督和时间对齐的上下文监督三种技术。</li>
<li>FuseCodec-TTS证明了该方法在零样本语音合成中的适用性。</li>
<li>FuseCodec在LibriSpeech上的表现优于其他模型，如EnCodec、SpeechTokenizer和DAC。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11425">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0720c936330af45dbfa84de9dcfcb7be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104559&auth_key=1760104559-0-0-3891bd52a6f35a2ac657209ec3c702f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6999087191f6f9a62755b349dfb59dbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104566&auth_key=1760104566-0-0-e0976dbb2064e3f23654ed3e1f5888a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d74e5a95829cb9039559b584f76a172f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104573&auth_key=1760104573-0-0-b9d4a82d915f6b810b516d2da0969145&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a16ba197cab782ddf753fb5c6e17f38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104580&auth_key=1760104580-0-0-54ca3240144387bf2836da904350b075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling"><a href="#Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling" class="headerlink" title="Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"></a>Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</h2><p><strong>Authors:Neil Zeghidour, Eugene Kharitonov, Manu Orsini, Václav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick Pérez, Laurent Mazaré, Alexandre Défossez</strong></p>
<p>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> </p>
<blockquote>
<p>我们介绍了延迟流建模（DSM），这是一种用于流式、多模态序列到序列学习的灵活公式。序列到序列生成通常以一种离线的方式进行，模型在生成第一个输出时间步之前会消耗完整的输入序列。相比之下，流式序列到序列则依赖于学习策略，以决定何时推进输入流，或写入输出流。然而，DSM使用仅解码的语言模型对已经时间对齐的流进行建模。通过将对齐移动到预处理步骤，并在流之间引入适当的延迟，DSM提供了任意输出流的流式推理，适用于多种序列到序列问题，可从任何输入组合生成。特别是给定文本和音频流时，语音识别（ASR）对应于延迟的文本流，而相反则为文本到语音（TTS）模型。我们为这两个主要的序列到序列任务进行了广泛的实验，结果表明，DSM在提供最先进的性能和延迟的同时，还支持任意长的序列，甚至与离线基准相竞争。相关代码、样本和演示可在 <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08753v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了延迟流建模（DSM），这是一种用于流式、多模态序列到序列学习的灵活方法。与传统的离线序列到序列生成不同，DSM对已经时间对齐的流进行建模，通过预处理步骤完成对齐，并在流之间引入适当的延迟，以实现任意输出序列的流式推断。这种方法适用于许多序列到序列问题，包括语音识别和文本到语音转换。实验表明，DSM具有最先进的性能和低延迟，支持任意长序列，与离线基准测试具有竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>延迟流建模（DSM）是一种灵活的序列到序列学习方法，适用于流式和多模态数据。</li>
<li>DSM通过预处理步骤完成流之间的时间对齐，并引入延迟。</li>
<li>DSM支持任意输出序列的流式推断，适用于多种序列到序列问题。</li>
<li>DSM在语音识别和文本到语音转换等任务中具有卓越性能。</li>
<li>DSM具有先进的性能和低延迟，能够处理任意长序列。</li>
<li>DSM与离线基准测试具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-70ff91ccbdaebebb45f0bd79f253971e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104587&auth_key=1760104587-0-0-ae1f4d33e229eedc380c6b19e2ee0e03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5b42e42e834063e3174a20fa29ab19f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104594&auth_key=1760104594-0-0-4002cf8cc2c152ef4d0088fccebe557e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d631c769a50d4ff489a42e5de7e69d29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104601&auth_key=1760104601-0-0-280f09d897f34ba02787d02a5153be48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Leveraging-Mamba-with-Full-Face-Vision-for-Audio-Visual-Speech-Enhancement"><a href="#Leveraging-Mamba-with-Full-Face-Vision-for-Audio-Visual-Speech-Enhancement" class="headerlink" title="Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement"></a>Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement</h2><p><strong>Authors:Rong Chao, Wenze Ren, You-Jin Li, Kuo-Hsuan Hung, Sung-Feng Huang, Szu-Wei Fu, Wen-Huang Cheng, Yu Tsao</strong></p>
<p>Recent Mamba-based models have shown promise in speech enhancement by efficiently modeling long-range temporal dependencies. However, models like Speech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios and struggle in complex multi-speaker environments such as the cocktail party problem. To overcome this, we introduce AVSEMamba, an audio-visual speech enhancement model that integrates full-face visual cues with a Mamba-based temporal backbone. By leveraging spatiotemporal visual information, AVSEMamba enables more accurate extraction of target speech in challenging conditions. Evaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba outperforms other monaural baselines in speech intelligibility (STOI), perceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves \textbf{1st place} on the monaural leaderboard. </p>
<blockquote>
<p>近期基于Mamba的模型在通过有效建模远程时间依赖性进行语音增强方面显示出良好的前景。然而，像语音增强Mamba（SEMamba）这样的模型仅限于单人场景，在复杂的多人环境（如鸡尾酒会问题）中表现困难。为了克服这一难题，我们引入了AVSEMamba，这是一个视听语音增强模型，它结合了基于Mamba的时间骨架和全脸视觉线索。通过利用时空视觉信息，AVSEMamba能够在具有挑战性的条件下更准确地提取目标语音。在AVSEC-4挑战的开发集和盲测试集上进行了评估，AVSEMamba在语音清晰度（STOI）、感知质量（PESQ）和非侵入质量（UTMOS）方面都优于其他单耳基线测试，并在单耳测试排行榜上取得了第一名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13624v2">PDF</a> Accepted to Interspeech 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>Mamba模型在语音增强领域展现出良好性能，能够高效建模长时序依赖关系。然而，面对复杂的多说话人环境如鸡尾酒会问题，现有模型如SEMamba仅限于单说话人场景。为解决此问题，本文提出了AVSEMamba音频视觉语音增强模型，它融合了面部视觉线索与Mamba的时序主干网络。借助时空视觉信息，AVSEMamba能在复杂条件下更准确地提取目标语音。在AVSEC-4挑战的开发和盲测试集上评估，AVSEMamba在语音清晰度（STOI）、感知质量（PESQ）和非侵入质量（UTMOS）方面超越其他单声道基线，并在单声道排行榜上获得第一名。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mamba模型在语音增强领域表现优异，擅长建模长时序依赖关系。</li>
<li>现有模型如SEMamba在复杂多说话人环境下面临挑战。</li>
<li>AVSEMamba是一个音频视觉语音增强模型，融合了面部视觉线索和Mamba的时序主干网络。</li>
<li>AVSEMamba借助时空视觉信息，能更准确地提取目标语音。</li>
<li>在AVSEC-4挑战的开发和盲测试集上，AVSEMamba性能超越其他模型。</li>
<li>AVSEMamba在语音清晰度、感知质量和非侵入质量方面表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13624">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ad0c75cc1bde08dd31dc65ba82f71399~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104608&auth_key=1760104608-0-0-3111f0d55353ab9b9d1076b479c4c701&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-263dd6e78dcf69c91ef7fe7b0a59e22f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104615&auth_key=1760104615-0-0-31eb7242d0e44dd317c9bfd7a1ed0d6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56c73a80002f947d74c23a30246feec4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104622&auth_key=1760104622-0-0-5b72d28b1b94be2dc38140de58a88cf8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="IML-Spikeformer-Input-aware-Multi-Level-Spiking-Transformer-for-Speech-Processing"><a href="#IML-Spikeformer-Input-aware-Multi-Level-Spiking-Transformer-for-Speech-Processing" class="headerlink" title="IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech   Processing"></a>IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech   Processing</h2><p><strong>Authors:Zeyang Song, Shimin Zhang, Yuhong Chou, Jibin Wu, Haizhou Li</strong></p>
<p>Spiking Neural Networks (SNNs), inspired by biological neural mechanisms, represent a promising neuromorphic computing paradigm that offers energy-efficient alternatives to traditional Artificial Neural Networks (ANNs). Despite proven effectiveness, SNN architectures have struggled to achieve competitive performance on large-scale speech processing tasks. Two key challenges hinder progress: (1) the high computational overhead during training caused by multi-timestep spike firing, and (2) the absence of large-scale SNN architectures tailored to speech processing tasks. To overcome the issues, we introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking Transformer architecture specifically designed for large-scale speech processing. Central to our design is the Input-aware Multi-Level Spike (IMLS) mechanism, which simulates multi-timestep spike firing within a single timestep using an adaptive, input-aware thresholding scheme. IML-Spikeformer further integrates a Re-parameterized Spiking Self-Attention (RepSSA) module with a Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module enhances the precision of attention maps and enables modeling of multi-scale temporal dependencies in speech signals. Experiments demonstrate that IML-Spikeformer achieves word error rates of 6.0% on AiShell-1 and 3.4% on Librispeech-960, comparable to conventional ANN transformers while reducing theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$ respectively. IML-Spikeformer marks an advance of scalable SNN architectures for large-scale speech processing in both task performance and energy efficiency. Our source code and model checkpoints are publicly available at github.com&#x2F;Pooookeman&#x2F;IML-Spikeformer. </p>
<blockquote>
<p>脉冲神经网络（Spiking Neural Networks，SNNs），受生物神经机制的启发，代表了一种有前景的类脑计算范式，为传统的人工神经网络（ANNs）提供了节能的替代方案。尽管已经证明了其有效性，但SNN架构在大规模语音识别任务上实现具有竞争力的性能仍存在困难。两个关键挑战阻碍了进展：（1）由于多时间步长的脉冲发射导致的训练过程中的高计算开销；（2）缺乏针对语音识别任务定制的大规模SNN架构。为了克服这些问题，我们引入了输入感知多级Spikeformer，即IML-Spikeformer，这是一种专门用于大规模语音处理的脉冲Transformer架构。设计的核心在于输入感知多级脉冲（IMLS）机制，该机制使用自适应的、输入感知的阈值方案，在单个时间步长内模拟多时间步长的脉冲发射。IML-Spikeformer进一步结合了参数化脉冲自注意力（RepSSA）模块和分层衰减掩码（HDM），形成了HD-RepSSA模块。该模块提高了注意力图的精度，并实现了语音信号中多尺度时间依赖关系的建模。实验表明，IML-Spikeformer在AiShell-1上的词错误率为6.0%，在Librispeech-960上的词错误率为3.4%，与传统ANN变压器相当，同时分别降低了4.64×和4.32×的理论推理能耗。IML-Spikeformer标志着大规模语音处理的可扩展SNN架构在任务性能和能源效率方面的进步。我们的源代码和模型检查点可在github.com&#x2F;Pooookeman&#x2F;IML-Spikeformer上公开获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07396v2">PDF</a> Accepted by TNNLS</p>
<p><strong>摘要</strong></p>
<p>受生物神经机制启发的脉冲神经网络（SNNs）是一种前景看好的神经形态计算范例，为传统的人工神经网络（ANNs）提供了能源效率更高的替代方案。然而，SNN架构在大规模语音识别任务上的表现竞争力有限。本文引入Input-aware Multi-Level Spikeformer（IML-Spikeformer），一种专门用于大规模语音处理的脉冲Transformer架构。其核心设计的Input-aware Multi-Level Spike（IMLS）机制能够在单个时间步内模拟多时间步的脉冲发射。此外，IML-Spikeformer集成了带有层次衰减掩膜（HDM）的Re-parameterized Spiking Self-Attention（RepSSA）模块，形成HD-RepSSA模块，提高了注意力图的精度，并实现了语音信号多尺度时间依赖性的建模。实验表明，IML-Spikeformer在AiShell-1上的词错误率为6.0%，在Librispeech-960上为3.4%，与传统ANN变压器相当，同时理论上降低了4.64倍和4.32倍的推理能耗。IML-Spikeformer标志着大规模语音识别任务中可扩展SNN架构在任务性能和能源效率方面的进步。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>脉冲神经网络（SNNs）是一种受生物神经启发的计算范例，相较于传统人工神经网络（ANNs）更节能。</li>
<li>SNN面临两大挑战：多时间步脉冲发射导致的高计算开销，以及缺乏针对语音处理任务的大规模SNN架构。</li>
<li>IML-Spikeformer，一种专为大规模语音处理设计的脉冲Transformer架构被提出。</li>
<li>IML-Spikeformer的Input-aware Multi-Level Spike（IMLS）机制能在单个时间步内模拟多时间步的脉冲发射。</li>
<li>HD-RepSSA模块通过增强注意力图的精度和实现多尺度时间依赖性建模，提高了语音处理的性能。</li>
<li>IML-Spikeformer在大型语音识别任务上表现出色，词错误率与常规ANN变压器相当。</li>
<li>IML-Spikeformer相比传统方法大幅降低了推理能耗。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-badef6d76997d9c6194383b259ec2569~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104629&auth_key=1760104629-0-0-d2f2e03be4deec7542d5ac727ef4abfa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45d664199fcc4f11c6363dda9e009aa5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104637&auth_key=1760104637-0-0-e0f84eee661cebfdca1df06c9806e7d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ab4a57fb10e64ffa14b198530f2f0df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104643&auth_key=1760104643-0-0-5c8821e47d8565958273ddea6e55f2b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection"><a href="#EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection" class="headerlink" title="EnvSDD: Benchmarking Environmental Sound Deepfake Detection"></a>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</h2><p><strong>Authors:Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Haohe Liu, Wenwu Wang, Mark D Plumbley</strong></p>
<p>Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains. </p>
<blockquote>
<p>音频生成系统如今能够创造出非常逼真的声音场景，这既可以增强媒体制作，也存在潜在风险。几项研究已经研究了语音或歌唱声音的深度伪造。然而，环境声音具有不同的特性，这可能会使检测语音和歌唱深度伪造的方法对真实世界的声音效果较差。此外，用于环境声音深度伪造检测的现有数据集在规模和音频类型方面有限。为了弥补这一空白，我们引入了EnvSDD，这是为此任务设计的大规模定制数据集，包含45.25小时的真实音频和316.74小时的伪造音频。测试集包括各种条件，以评估其泛化能力，例如未见过的生成模型和未见过的数据集。我们还提出了一种基于预训练音频基础模型的音频深度伪造检测系统。在EnvSDD上的结果证明，我们提出的系统优于语音和歌唱领域的最先进的系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19203v2">PDF</a> Proceedings of Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频生成系统所带来现实音效增强的同时存在的潜在风险。针对环境声音的特性，现有的语音和歌唱声音深度伪造检测手段可能对其并不完全适用。为了填补这一空白，文章推出了EnvSDD数据集，用于专门的环境声音深度伪造检测任务，其中包括真实音频与合成音频。同时，文章还提出了一种基于预训练音频基础模型的音频深度伪造检测系统，该系统在EnvSDD上的表现优于语音和歌唱领域的最新系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频生成系统能生成逼真的声音场景，增强媒体制作效果，但也存在潜在风险。</li>
<li>环境声音的特性使得现有的语音和歌唱深度伪造检测手段可能不够有效。</li>
<li>推出EnvSDD数据集，用于专门的环境声音深度伪造检测任务。</li>
<li>EnvSDD数据集包括真实音频与合成音频，提供了大量的样本以供学习和检测。</li>
<li>数据集中的测试集包含了各种条件以评估检测模型的泛化能力。</li>
<li>提出了一种基于预训练音频基础模型的音频深度伪造检测系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19203">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f599d745b17a5a82f4aff1da2437dea0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104651&auth_key=1760104651-0-0-2c29a5b63f5407e2a2a7a7a65490c4a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-175f2ceebf353149462cd6398c7ebad7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104658&auth_key=1760104658-0-0-91eee68f87826c649a9235768dc51a81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79f46b021bebe01d090f7948a18aea0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104664&auth_key=1760104664-0-0-1ee1ea9c9873ecd137c83140fd7eb076&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6cd9d5898ab52261cb44eb6eccf311a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104671&auth_key=1760104671-0-0-06327c8478d2329efecdb421cc857e41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66ee140547c030fe34bf3065db693d9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104678&auth_key=1760104678-0-0-20c76e3452e59eb1e041b83f1659ef02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes"><a href="#MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes" class="headerlink" title="MemeIntel: Explainable Detection of Propagandistic and Hateful Memes"></a>MemeIntel: Explainable Detection of Propagandistic and Hateful Memes</h2><p><strong>Authors:Mohamed Bayan Kmainasi, Abul Hasnat, Md Arid Hasan, Ali Ezzat Shahroor, Firoj Alam</strong></p>
<p>The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to jointly modeling label detection and the generation of explanation-based rationales, which often leads to degraded classification performance when trained simultaneously. To address this challenge, we introduce MemeXplain, an explanation-enhanced dataset for propagandistic memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results show that this strategy significantly improves both label detection and explanation generation quality over the base model, outperforming the current state-of-the-art with an absolute improvement of ~1.4% (Acc) on ArMeme and ~2.2% (Acc) on Hateful Memes. For reproducibility and future research, we aim to make the MemeXplain dataset and scripts publicly available (<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel">https://github.com/MohamedBayan/MemeIntel</a>). </p>
<blockquote>
<p>社交媒体上多模式内容的激增，为理解和调节复杂且依赖于情境的议题（如假信息、仇恨言论和宣传）带来了重大挑战。虽然已有研究致力于开发资源和提出自动检测新方法，但对联合建模标签检测和基于解释的理性生成给予的关注有限，这往往导致在同时训练时分类性能下降。为了应对这一挑战，我们推出了MemeXplain，这是一个用于阿拉伯语的宣传性meme和英语的仇恨言论meme的解释增强数据集，成为这些任务的首个大规模资源。为了解决这些任务，我们提出了一种多阶段优化方法并训练了视觉语言模型（VLMs）。我们的结果表明，该策略在标签检测器和解释生成质量方面显著改进了基础模型，在ArMeme上绝对提高了约1.4%（准确率），在仇恨言论Memes上提高了约2.2%（准确率）。为了可复制性和未来研究，我们旨在使MemeXplain数据集和脚本公开可用（<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel%EF%BC%89%E3%80%82">https://github.com/MohamedBayan/MemeIntel）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16612v2">PDF</a> disinformation, misinformation, factuality, harmfulness, fake news,   propaganda, hateful meme, multimodality, text, images</p>
<p><strong>Summary</strong></p>
<p>社交媒体上多模态内容的激增，为理解和调节复杂、依赖语境的问题（如错误信息、仇恨言论和宣传）带来了重大挑战。尽管已有一些自动检测资源的开发和新方法的提出，但在联合建模标签检测和基于解释的理性生成方面关注较少，这往往导致同时训练时分类性能下降。为应对这一挑战，我们推出了MemeXplain数据集，包含阿拉伯语的宣传性meme和英语的仇恨性meme，成为首个大规模资源用于这些任务。为解决这些任务，我们提出了多阶段优化方法并训练了视觉语言模型（VLMs）。结果显示，此策略在标签检测和解释生成质量上均显著提高了基础模型性能，相较于当前最先进技术，在ArMeme上准确率提高了约1.4%，在仇恨性Memes上准确率提高了约2.2%。我们旨在让MemeXplain数据集和脚本公开可用，以供未来研究（<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel%EF%BC%89%E3%80%82">https://github.com/MohamedBayan/MemeIntel）。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体多模态内容带来的挑战：理解和调节复杂、依赖语境的问题如错误信息、仇恨言论和宣传等具有难度。</li>
<li>数据集缺失问题：尽管已有自动检测资源的开发和新方法提出，但在联合建模标签检测和基于解释的理性生成方面关注不足。</li>
<li>MemeXplain数据集的推出：包含阿拉伯语的宣传性meme和英语的仇恨性meme，成为大规模资源用于相关任务。</li>
<li>解决方案：采用多阶段优化方法并训练视觉语言模型（VLMs）。</li>
<li>性能和效果：策略显著提高标签检测和解释生成质量，相较于当前技术有显著提升。</li>
<li>数据集和脚本的公开可用性：旨在促进未来研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c2b04c4d8b0632d1634ed280a2bf28ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104686&auth_key=1760104686-0-0-413729ee6f24c29321d482aa4a7a41fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d569422eece23911b4afa79c5ad07260~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104692&auth_key=1760104692-0-0-bf397a4df481efeced957136ab7a9011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92a03fec9eb3fcbdd0bfaed24cbcef10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104699&auth_key=1760104699-0-0-586b5efc252acec2008411a68a2105d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d990a6dba051d3911fc6add2b99d055c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104706&auth_key=1760104706-0-0-e253e64eb3a7670f7827fd82de154f93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ef2e35985bb4bc71f3d2163a6a83fde3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087920&auth_key=1760087920-0-0-bfb3cbf7acab7fa43fca3d84b9a9e78a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-10-02  DGM4+ Dataset Extension for Global Scene Inconsistency
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4dfbf28d610f69265c2f4832e1284378~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087644&auth_key=1760087644-0-0-e75978e49e40e66a25ee7e7095e16735&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-10-02  Generalized Contrastive Learning for Universal Multimodal Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
