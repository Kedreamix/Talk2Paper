<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Voice Evaluation of Reasoning Ability Diagnosing the Modality-Induced   Performance Gap">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d74e5a95829cb9039559b584f76a172f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087740&auth_key=1760087740-0-0-0ec976e0c001e3378aadc9f1c4cf1b79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-02-æ›´æ–°"><a href="#2025-10-02-æ›´æ–°" class="headerlink" title="2025-10-02 æ›´æ–°"></a>2025-10-02 æ›´æ–°</h1><h2 id="Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap"><a href="#Voice-Evaluation-of-Reasoning-Ability-Diagnosing-the-Modality-Induced-Performance-Gap" class="headerlink" title="Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap"></a>Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced   Performance Gap</h2><p><strong>Authors:Yueqian Lin, Zhengmian Hu, Qinsi Wang, Yudong Liu, Hengfan Zhang, Jayakumar Subramanian, Nikos Vlassis, Hai Helen Li, Yiran Chen</strong></p>
<p>We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing â€œthinking timeâ€ yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding&#x2F;consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†è¯­éŸ³æ¨ç†èƒ½åŠ›è¯„ä¼°ï¼ˆVERAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å®æ—¶å¯¹è¯çº¦æŸä¸‹è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ã€‚VERAåŒ…å«ä»æ—¢å®šæ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­å¾—å‡ºçš„2931ä¸ªåŸç”Ÿè¯­éŸ³ç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µè¢«ç»„ç»‡æˆäº”ä¸ªèµ›é“ï¼ˆæ•°å­¦ã€ç½‘ç»œã€ç§‘å­¦ã€é•¿è¯­å¢ƒã€äº‹å®ï¼‰ã€‚æ¯ä¸ªé¡¹ç›®éƒ½é€‚åº”è¯­éŸ³äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™æ¨ç†éš¾åº¦ã€‚VERAèƒ½å¤Ÿåœ¨æ¨¡å‹å®¶æ—å†…è¿›è¡Œç›´æ¥çš„æ–‡æœ¬è¯­éŸ³æ¯”è¾ƒï¼Œå¹¶æ”¯æŒåˆ†ææ¶æ„é€‰æ‹©å¦‚ä½•å½±å“å¯é æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ä¸ªå½“ä»£è¯­éŸ³ç³»ç»Ÿä»¥åŠå¼ºå¤§çš„æ–‡æœ¬åŸºå‡†ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´ä¸”æ˜¾è‘—çš„æ¨¡æ€å·®è·ï¼šåœ¨æ•°å­¦ç«èµ›ä¸­ï¼Œé¢†å…ˆçš„æ–‡æœ¬æ¨¡å‹è¾¾åˆ°74.8%çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶è¯­éŸ³å¯¹åº”æ¨¡å‹åªè¾¾åˆ°6.1%ï¼›è·¨èµ›é“çš„å®è§‚å¹³å‡ï¼Œæœ€ä½³æ–‡æœ¬æ¨¡å‹è¾¾åˆ°54.0%ï¼Œè€Œè¯­éŸ³æ¨¡å‹ä¸º11.3%ã€‚å»¶è¿Ÿ-å‡†ç¡®æ€§åˆ†ææ˜¾ç¤ºäº†ä¸€ä¸ªä½å»¶è¿Ÿå¹³å°ï¼Œåœ¨è¿™ä¸ªå¹³å°ä¸Šï¼Œå¿«é€Ÿè¯­éŸ³ç³»ç»Ÿèšé›†åœ¨çº¦10%çš„å‡†ç¡®ç‡ï¼Œè€Œè¦è¾¾åˆ°æ–‡æœ¬æ€§èƒ½åˆ™éœ€è¦ç‰ºç‰²å®æ—¶äº¤äº’ã€‚è¯Šæ–­å®éªŒè¡¨æ˜å¸¸è§çš„ç¼“è§£æªæ–½å¹¶ä¸è¶³å¤Ÿã€‚å¢åŠ â€œæ€è€ƒæ—¶é—´â€äº§ç”Ÿçš„æ”¶ç›Šå¾®ä¹å…¶å¾®ï¼›ä¸€ä¸ªåˆ†ç¦»çš„çº§è”ï¼Œå°†æ¨ç†ä¸å™è¿°åˆ†å¼€ï¼Œå¯ä»¥æé«˜å‡†ç¡®æ€§ï¼Œä½†ä»è¿œè¿œè½åäºæ–‡æœ¬ï¼Œå¹¶å¼•å…¥å…¸å‹çš„æ¥åœ°&#x2F;ä¸€è‡´æ€§é”™è¯¯ã€‚æ•…éšœåˆ†æè¿›ä¸€æ­¥æ˜¾ç¤ºäº†åŸç”Ÿæµåª’ä½“ã€ç«¯åˆ°ç«¯å’Œçº§è”è®¾è®¡ä¹‹é—´çš„ä¸åŒé”™è¯¯ç­¾åã€‚VERAä¸ºè„±ç¦»æ€è€ƒå’Œè¯´è¯è¿‡ç¨‹çš„æ¶æ„æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„æµ‹è¯•å¹³å°å’Œæœ‰é’ˆå¯¹æ€§çš„è¯Šæ–­ï¼Œä¸ºå®æ—¶è¯­éŸ³åŠ©æ‰‹æä¾›äº†ä¸€-ç§æ—¢æœ‰æµåˆ©åº¦åˆå¯é æ¨ç†çš„è¡¡é‡è¿›æ­¥çš„åŸåˆ™æ€§æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26542v1">PDF</a> Code and data available at <a target="_blank" rel="noopener" href="https://github.com/linyueqian/VERA">https://github.com/linyueqian/VERA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†è¯­éŸ³æ¨ç†èƒ½åŠ›è¯„ä¼°åŸºå‡†ï¼ˆVERAï¼‰ï¼Œç”¨äºè¯„ä¼°è¯­éŸ³äº¤äº’ç³»ç»Ÿåœ¨é¢å¯¹çœŸå®å¯¹è¯æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚VERAåŒ…å«ä»å·²å»ºç«‹çš„æ–‡æœ¬åŸºå‡†æ•°æ®ä¸­è¡ç”Ÿçš„2,931ä¸ªè¯­éŸ³æœ¬é›†ï¼Œåˆ†ä¸ºäº”ä¸ªè½¨é“ï¼ˆæ•°å­¦ã€ç½‘ç»œã€ç§‘å­¦ã€é•¿è¯­å¢ƒã€äº‹å®ï¼‰ã€‚æ¯ä¸ªé¡¹ç›®éƒ½é€‚åº”äº†è¯­éŸ³äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨ç†éš¾åº¦ã€‚VERAä½¿ç›´æ¥æ–‡æœ¬ä¸è¯­éŸ³å¯¹æ¯”æˆä¸ºå¯èƒ½ï¼Œæ”¯æŒå¯¹æ¨¡å‹å®¶æ—å†…éƒ¨æ¶æ„é€‰æ‹©å¯¹å¯é æ€§çš„å½±å“çš„åˆ†æã€‚é€šè¿‡å¯¹å½“å‰è¯­éŸ³ç³»ç»Ÿä¸å¼ºå¤§æ–‡æœ¬åŸºå‡†çš„è¯„ä¼°ï¼Œå‘ç°æ˜¾è‘—çš„ã€ä¸€è‡´çš„æ¨¡æ€å·®è·ã€‚ç«äº‰æ•°å­¦é¢˜ä¸­ï¼Œé¡¶å°–æ–‡æœ¬æ¨¡å‹å‡†ç¡®ç‡è¾¾74.8%ï¼Œè€Œè¯­éŸ³æ¨¡å‹ä»…è¾¾6.1%ã€‚å®è§‚å¹³å‡è½¨é“ä¸Šï¼Œæœ€ä½³æ–‡æœ¬æ¨¡å‹å‡†ç¡®ç‡ä¸º54%ï¼Œè€Œè¯­éŸ³æ¨¡å‹ä»…ä¸º11.3%ã€‚å»¶è¿Ÿå‡†ç¡®åˆ†ææ­ç¤ºä½å»¶è¿Ÿå¹³å°åŒºåŸŸï¼Œå¿«é€Ÿè¯­éŸ³ç³»ç»Ÿå‡†ç¡®ç‡çº¦ä¸º10%ï¼Œè€Œæ¥è¿‘æ–‡æœ¬æ€§èƒ½åˆ™éœ€è¦ç‰ºç‰²å®æ—¶äº¤äº’ã€‚è¯Šæ–­å®éªŒè¡¨æ˜å¸¸è§çš„ç¼“è§£æ–¹æ³•ä¸è¶³ä»¥æé«˜æ€§èƒ½ã€‚å¢åŠ æ€è€ƒæ—¶é—´å¸¦æ¥çš„æ”¶ç›Šå¾®ä¹å…¶å¾®ï¼›ä¸€ä¸ªåˆ†ç¦»çš„çº§è”è®¾è®¡æ”¹å–„äº†å‡†ç¡®æ€§ï¼Œä½†ä»è¿œæœªè¾¾åˆ°æ–‡æœ¬æ€§èƒ½ï¼Œå¹¶å¼•å…¥ç‰¹å®šçš„å®šä½æˆ–ä¸€è‡´æ€§é”™è¯¯ã€‚å¤±è´¥åˆ†ææ˜¾ç¤ºæœ¬åœ°æµåª’ä½“ã€ç«¯åˆ°ç«¯å’Œçº§è”è®¾è®¡æœ‰å„è‡ªç‹¬ç‰¹çš„é”™è¯¯ç‰¹å¾ã€‚VERAä¸ºæ¶æ„æä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„æµ‹è¯•å¹³å°å’Œæœ‰é’ˆå¯¹æ€§çš„è¯Šæ–­å·¥å…·ï¼Œè¿™äº›æ¶æ„å°†æ€è€ƒä¸è¯´è¯åˆ†å¼€ï¼Œä¸ºæ„å»ºæ—¢æµç•…åˆå¯é çš„å®æ—¶è¯­éŸ³åŠ©æ‰‹æä¾›äº†è¡¡é‡è¿›æ­¥çš„åŸåˆ™æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­éŸ³äº¤äº’ç³»ç»Ÿæ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚</li>
<li>VERAåŒ…å«å¤šä¸ªè½¨é“ï¼Œæ¶µç›–ä¸åŒçš„é¢†åŸŸå’Œè¯­å¢ƒã€‚</li>
<li>è¯­éŸ³ç³»ç»Ÿä¸æ–‡æœ¬ç³»ç»Ÿæ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šã€‚</li>
<li>å»¶è¿Ÿä¸å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå¿«é€Ÿå“åº”å¾€å¾€å¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>å¢åŠ æ€è€ƒæ—¶é—´å¯¹æ”¹å–„è¯­éŸ³ç³»ç»Ÿæ€§èƒ½æ•ˆæœæœ‰é™ã€‚</li>
<li>åˆ†ç¦»æ¨ç†ä¸å™è¿°çš„çº§è”è®¾è®¡èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†ä»å­˜åœ¨ç‰¹å®šè¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c928f4b72727355ef4248023b1452c57~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087748&auth_key=1760087748-0-0-77b64f45301f4f109cb1950087709dc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05f433408fe64cdd8d00120710857eca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087755&auth_key=1760087755-0-0-643d488203a75a735726b9d043715b68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6ef95ac35a88562545ef94075641310~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087761&auth_key=1760087761-0-0-0708c0c37c30f6d5eae92f88d5c4dd18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f619f0b8ac1d9e26b96438d9c45c2eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087768&auth_key=1760087768-0-0-9c1f9b0287acabca2f872c6d22faefe6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb95fd25fee79ff19ef189e065f78982~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104255&auth_key=1760104255-0-0-db21d4f3e7b01e55dba766286762889e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5cefd7b8480dc50e6ad969eb8a76f801~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087839&auth_key=1760087839-0-0-6abf61e9f826a32deaa595ecdfa1964b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Analysis-of-Joint-Nonlinear-Spatial-Filtering-for-Spatial-Aliasing-Reduction"><a href="#An-Analysis-of-Joint-Nonlinear-Spatial-Filtering-for-Spatial-Aliasing-Reduction" class="headerlink" title="An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction"></a>An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing   Reduction</h2><p><strong>Authors:Alina Mannanova, Jakob Kienegger, Timo Gerkmann</strong></p>
<p>The performance of traditional linear spatial filters for speech enhancement is constrained by the physical size and number of channels of microphone arrays. For instance, for large microphone distances and high frequencies, spatial aliasing may occur, leading to unwanted enhancement of signals from non-target directions. Recently, it has been proposed to replace linear beamformers by nonlinear deep neural networks for joint spatial-spectral processing. While it has been shown that such approaches result in higher performance in terms of instrumental quality metrics, in this work we highlight their ability to efficiently handle spatial aliasing. In particular, we show that joint spatial and tempo-spectral processing is more robust to spatial aliasing than traditional approaches that perform spatial processing alone or separately with tempo-spectral filtering. The results provide another strong motivation for using deep nonlinear networks in multichannel speech enhancement, beyond their known benefits in managing non-Gaussian noise and multiple speakers, especially when microphone arrays with rather large microphone distances are used. </p>
<blockquote>
<p>ä¼ ç»Ÿçº¿æ€§ç©ºé—´æ»¤æ³¢å™¨åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ€§èƒ½å—åˆ°éº¦å…‹é£é˜µåˆ—çš„ç‰©ç†å°ºå¯¸å’Œé€šé“æ•°é‡çš„é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨è¾ƒå¤§çš„éº¦å…‹é£è·ç¦»å’Œè¾ƒé«˜é¢‘ç‡ä¸‹ï¼Œå¯èƒ½ä¼šå‘ç”Ÿç©ºé—´æ··å ï¼Œå¯¼è‡´æ¥è‡ªéç›®æ ‡æ–¹å‘ä¿¡å·çš„æ„å¤–å¢å¼ºã€‚æœ€è¿‘ï¼Œæœ‰äººå»ºè®®ä½¿ç”¨éçº¿æ€§æ·±åº¦ç¥ç»ç½‘ç»œæ›¿ä»£çº¿æ€§æ³¢æŸå½¢æˆå™¨è¿›è¡Œè”åˆç©ºé—´é¢‘è°±å¤„ç†ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨ä»ªå™¨è´¨é‡æŒ‡æ ‡æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å¼ºè°ƒäº†å®ƒä»¬å¤„ç†ç©ºé—´æ··å çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬æ˜¾ç¤ºè”åˆç©ºé—´å’Œæ—¶æ€é¢‘è°±å¤„ç†æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ç¨³å¥ï¼Œåè€…å•ç‹¬è¿›è¡Œç©ºé—´å¤„ç†æˆ–æ—¶æ€é¢‘è°±æ»¤æ³¢çš„åˆ†ç¦»å¤„ç†ä¼šå‡ºç°æ··å ã€‚ç»“æœé™¤äº†å·²çŸ¥çš„åœ¨å¯¹æŠ—éé«˜æ–¯å™ªå£°å’Œå¤šè¯´è¯äººæ—¶å…·æœ‰ä¼˜åŠ¿ä¹‹å¤–ï¼Œåœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºé¢†åŸŸä¸­ä½¿ç”¨æ·±åº¦éçº¿æ€§ç½‘ç»œçš„åŠ¨æœºæ„ˆå‘å¼ºçƒˆï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨å…·æœ‰è¾ƒå¤§éº¦å…‹é£è·ç¦»çš„éº¦å…‹é£é˜µåˆ—æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25982v1">PDF</a> Submitted to ICASSP 2026. This work has been submitted to the IEEE   for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¼ ç»Ÿçš„çº¿æ€§ç©ºé—´æ»¤æ³¢å™¨åœ¨è¿›è¡Œè¯­éŸ³å¢å¼ºæ—¶çš„æ€§èƒ½å—é™äºéº¦å…‹é£é˜µåˆ—çš„ç‰©ç†å°ºå¯¸å’Œé€šé“æ•°é‡ã€‚åœ¨å¤§éº¦å…‹é£è·ç¦»å’Œé«˜é¢‘æƒ…å†µä¸‹ï¼Œä¼šå‘ç”Ÿç©ºé—´æ··å ï¼Œå¯¼è‡´éç›®æ ‡æ–¹å‘ä¿¡å·çš„æ„å¤–å¢å¼ºã€‚æœ€è¿‘æè®®ç”¨éçº¿æ€§æ·±åº¦ç¥ç»ç½‘ç»œæ›¿ä»£çº¿æ€§æ³¢æŸå½¢æˆå™¨è¿›è¡Œè”åˆç©ºé—´å…‰è°±å¤„ç†ã€‚è™½ç„¶å·²è¯æ˜è¿™äº›æ–¹æ³•åœ¨ä»ªå™¨è´¨é‡æŒ‡æ ‡æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å¼ºè°ƒäº†å®ƒä»¬å¤„ç†ç©ºé—´æ··å çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ˜¾ç¤ºè”åˆç©ºé—´å’Œæ—¶åºå…‰è°±å¤„ç†æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ç¨³å¥å¯¹æŠ—ç©ºé—´æ··å ï¼Œåè€…å•ç‹¬æˆ–åˆ†åˆ«è¿›è¡Œæ—¶é¢‘æ»¤æ³¢å¤„ç†ã€‚ç»“æœé™¤äº†å®ƒä»¬åœ¨ç®¡ç†éé«˜æ–¯å™ªå£°å’Œå¤šè¯´è¯äººæ–¹é¢çš„å·²çŸ¥ä¼˜åŠ¿å¤–ï¼Œè¿˜æä¾›äº†åœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­ä½¿ç”¨æ·±åº¦éçº¿æ€§ç½‘ç»œçš„å¦ä¸€ä¸ªå¼ºçƒˆåŠ¨æœºï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨é—´è·è¾ƒå¤§çš„éº¦å…‹é£é˜µåˆ—æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¼ ç»Ÿçº¿æ€§ç©ºé—´æ»¤æ³¢å™¨åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢å—é™äºéº¦å…‹é£é˜µåˆ—çš„ç‰©ç†å°ºå¯¸å’Œé€šé“æ•°é‡ã€‚</li>
<li>åœ¨å¤§éº¦å…‹é£è·ç¦»å’Œé«˜é¢‘æ—¶ï¼Œä¼šå‘ç”Ÿç©ºé—´æ··å ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>éçº¿æ€§æ·±åº¦ç¥ç»ç½‘ç»œå·²è¢«æè®®ç”¨äºè”åˆç©ºé—´å…‰è°±å¤„ç†ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¤„ç†ç©ºé—´æ··å æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ä¼ ç»Ÿçš„ç©ºé—´å¤„ç†æ–¹æ³•ç›¸æ¯”æ›´åŠ ç¨³å¥ã€‚</li>
<li>è”åˆç©ºé—´å’Œæ—¶åºå…‰è°±å¤„ç†å¯ä»¥æä¾›æ›´å¥½çš„ç¨³å¥æ€§ï¼Œå¯¹æŠ—ç©ºé—´æ··å é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦éçº¿æ€§ç½‘ç»œè¿›è¡Œå¤šé€šé“è¯­éŸ³å¢å¼ºå…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éé«˜æ–¯å™ªå£°å’Œå¤šè¯´è¯äººåœºæ™¯æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a14660b06fee3fab20a57c14e3e7060f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087847&auth_key=1760087847-0-0-03d151f1ae44e293700fad75fb94c332&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9ab0a8e398fb77127dd05a88f230dbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104261&auth_key=1760104261-0-0-939ce75e64768958bb51f07d71c9940d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e284d309c504d6cda41107655bdb9fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104267&auth_key=1760104267-0-0-1ccda7b0c53b6ee218b07f03ab684d11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-02798b693d23efe9e7107b276debbfdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104274&auth_key=1760104274-0-0-415e72026b0226c88d86382336afc624&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-02a10e5f2660c86f520aeb71b938b2e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104280&auth_key=1760104280-0-0-7846346cc435bc1c2df2de6b95d7fec0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17d3b9ca57f6aac43d39913c28421742~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104286&auth_key=1760104286-0-0-e7f56d6588a596be810f56e72ea5ba87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Detecting-Hope-Across-Languages-Multiclass-Classification-for-Positive-Online-Discourse"><a href="#Detecting-Hope-Across-Languages-Multiclass-Classification-for-Positive-Online-Discourse" class="headerlink" title="Detecting Hope Across Languages: Multiclass Classification for Positive   Online Discourse"></a>Detecting Hope Across Languages: Multiclass Classification for Positive   Online Discourse</h2><p><strong>Authors:T. O. Abiola, K. D. Abiodun, O. E. Olumide, O. O. Adebanji, O. Hiram Calvo, Grigori Sidorov</strong></p>
<p>The detection of hopeful speech in social media has emerged as a critical task for promoting positive discourse and well-being. In this paper, we present a machine learning approach to multiclass hope speech detection across multiple languages, including English, Urdu, and Spanish. We leverage transformer-based models, specifically XLM-RoBERTa, to detect and categorize hope speech into three distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope. Our proposed methodology is evaluated on the PolyHope dataset for the PolyHope-M 2025 shared task, achieving competitive performance across all languages. We compare our results with existing models, demonstrating that our approach significantly outperforms prior state-of-the-art techniques in terms of macro F1 scores. We also discuss the challenges in detecting hope speech in low-resource languages and the potential for improving generalization. This work contributes to the development of multilingual, fine-grained hope speech detection models, which can be applied to enhance positive content moderation and foster supportive online communities. </p>
<blockquote>
<p>åœ¨ç¤¾äº¤åª’ä½“ä¸­æ£€æµ‹å¸¦æœ‰ç§¯ææƒ…ç»ªçš„è¯è¯­å·²æˆä¸ºä¿ƒè¿›ç§¯æå¯¹è¯å’Œç¦ç¥‰çš„é‡è¦ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºå¤šè¯­è¨€ç¯å¢ƒä¸­çš„å¤šå…ƒå¸Œæœ›è¨€è®ºæ£€æµ‹ï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¹Œå°”éƒ½è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºtransformerçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯XLM-RoBERTaï¼Œå°†å¸Œæœ›è¯è¯­åˆ†ä¸ºä¸‰ç§ä¸åŒçš„ç±»åˆ«ï¼šé€šç”¨å¸Œæœ›ã€ç°å®å¸Œæœ›å’Œéç°å®å¸Œæœ›ï¼Œå¹¶è¿›è¡Œæ£€æµ‹ä¸åˆ†ç±»ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨PolyHopeæ•°æ®é›†ä¸Šå¯¹PolyHope-M 2025å…±äº«ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨æ‰€æœ‰è¯­è¨€ä¸Šå‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†ç»“æœä¸ç°æœ‰æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®è§‚F1å¾—åˆ†æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†åœ¨ä½èµ„æºè¯­è¨€ä¸­æ£€æµ‹å¸Œæœ›è¨€è®ºçš„æŒ‘æˆ˜ä»¥åŠæé«˜æ³›åŒ–èƒ½åŠ›çš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šè¯­ç§ç²¾ç»†ç²’åº¦çš„å¸Œæœ›è¨€è®ºæ£€æµ‹æ¨¡å‹çš„å‘å±•åšå‡ºäº†è´¡çŒ®ï¼Œå¯åº”ç”¨äºç§¯æå†…å®¹çš„ç®¡ç†å’Œæ”¯æŒå‹åœ¨çº¿ç¤¾åŒºçš„å»ºè®¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25752v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“ä¸­å¸Œæœ›è¨€è®ºçš„æ£€æµ‹å¯¹ä¿ƒè¿›ç§¯æå¯¹è¯å’Œå¿ƒç†å¥åº·è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§è·¨å¤šç§è¯­è¨€çš„å¸Œæœ›è¨€è®ºæœºå™¨å­¦ä¹ æ£€æµ‹æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¹Œå°”éƒ½è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºtransformerçš„æ¨¡å‹XLM-RoBERTaæ¥æ£€æµ‹å’Œåˆ†ç±»å¸Œæœ›è¨€è®ºä¸ºä¸‰ç§ä¸åŒç±»åˆ«ï¼šé€šç”¨å‹å¸Œæœ›ã€ç°å®å‹å¸Œæœ›å’Œå¹»æƒ³å‹å¸Œæœ›ã€‚è¯¥æ–¹æ³•åœ¨PolyHopeæ•°æ®é›†ä¸Šé’ˆå¯¹PolyHope-M 2025å…±äº«ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œæ‰€æœ‰è¯­è¨€ä¸Šçš„è¡¨ç°å‡è¡¨ç°ä¼˜å¼‚ã€‚ç›¸è¾ƒäºç°æœ‰æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜å®è§‚F1åˆ†æ•°æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æŠ€å·§ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†åœ¨ä½èµ„æºè¯­è¨€ä¸­æ£€æµ‹å¸Œæœ›è¨€è®ºçš„æŒ‘æˆ˜ä»¥åŠæé«˜æ³›åŒ–èƒ½åŠ›çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºå¼€å‘å¤šè¯­è¨€ã€ç²¾ç»†åŒ–çš„å¸Œæœ›è¨€è®ºæ£€æµ‹æ¨¡å‹åšå‡ºè´¡çŒ®ï¼Œå¯åº”ç”¨äºå¢å¼ºç§¯æå†…å®¹ç®¡ç†å’Œä¿ƒè¿›æ”¯æŒæ€§åœ¨çº¿ç¤¾åŒºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸Œæœ›è¨€è®ºæ£€æµ‹åœ¨ç¤¾äº¤åª’ä½“ä¸­å¯¹äºä¿ƒè¿›ç§¯æå¯¹è¯å’Œå¿ƒç†å¥åº·è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºä¸€ç§è·¨å¤šç§è¯­è¨€çš„å¸Œæœ›è¨€è®ºæœºå™¨å­¦ä¹ æ£€æµ‹æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¹Œå°”éƒ½è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚</li>
<li>åˆ©ç”¨åŸºäºtransformerçš„æ¨¡å‹XLM-RoBERTaè¿›è¡Œå¸Œæœ›è¨€è®ºæ£€æµ‹å’Œåˆ†ç±»ã€‚</li>
<li>å°†å¸Œæœ›è¨€è®ºåˆ†ä¸ºä¸‰ç§ç±»åˆ«ï¼šé€šç”¨å‹å¸Œæœ›ã€ç°å®å‹å¸Œæœ›å’Œå¹»æƒ³å‹å¸Œæœ›ã€‚</li>
<li>åœ¨PolyHopeæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æŠ€å·§ã€‚</li>
<li>è®¨è®ºäº†åœ¨ä½èµ„æºè¯­è¨€ä¸­è¿›è¡Œå¸Œæœ›è¨€è®ºæ£€æµ‹çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3d456422670ec3e48d1de9aed662f918~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104294&auth_key=1760104294-0-0-b1e1ebb7ee71c3f0ed5aed582e1f3d0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f6a3615689ae0df04d6c5e9c8aeaa157~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104301&auth_key=1760104301-0-0-6fabe6a7f0f0d88f326e939f3c2c2c82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a6bf72de695307a8cf35a6d0bb5fed2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104308&auth_key=1760104308-0-0-a34f98d5369ae20009b5a93422c7c0d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e1385134fc7a88920da8d962405ff9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104314&auth_key=1760104314-0-0-de79552c090fa43657e8909b4a0e64c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83a420989d0427f0d29e50817655e1dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104321&auth_key=1760104321-0-0-307eb796c47a513694010792df58f855&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba94312796163914b06e2eb121de43d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104327&auth_key=1760104327-0-0-1505e623035aad027b43f9556fed904b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f14e22c914d80ab9762f57947f0ba3b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104333&auth_key=1760104333-0-0-ce6eda58bb1a4fac895a26c1c2c3a74e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edb5e7cdb9bc5b6036debd1c1c70e100~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104340&auth_key=1760104340-0-0-b3894aba397d8470dd34bb42c3d7901b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LTA-L2S-Lexical-Tone-Aware-Lip-to-Speech-Synthesis-for-Mandarin-with-Cross-Lingual-Transfer-Learning"><a href="#LTA-L2S-Lexical-Tone-Aware-Lip-to-Speech-Synthesis-for-Mandarin-with-Cross-Lingual-Transfer-Learning" class="headerlink" title="LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with   Cross-Lingual Transfer Learning"></a>LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with   Cross-Lingual Transfer Learning</h2><p><strong>Authors:Kang Yang, Yifan Liang, Fangkun Liu, Zhenping Xie, Chengshi Zheng</strong></p>
<p>Lip-to-speech (L2S) synthesis for Mandarin is a significant challenge, hindered by complex viseme-to-phoneme mappings and the critical role of lexical tones in intelligibility. To address this issue, we propose Lexical Tone-Aware Lip-to-Speech (LTA-L2S). To tackle viseme-to-phoneme complexity, our model adapts an English pre-trained audio-visual self-supervised learning (SSL) model via a cross-lingual transfer learning strategy. This strategy not only transfers universal knowledge learned from extensive English data to the Mandarin domain but also circumvents the prohibitive cost of training such a model from scratch. To specifically model lexical tones and enhance intelligibility, we further employ a flow-matching model to generate the F0 contour. This generation process is guided by ASR-fine-tuned SSL speech units, which contain crucial suprasegmental information. The overall speech quality is then elevated through a two-stage training paradigm, where a flow-matching postnet refines the coarse spectrogram from the first stage. Extensive experiments demonstrate that LTA-L2S significantly outperforms existing methods in both speech intelligibility and tonal accuracy. </p>
<blockquote>
<p>é’ˆå¯¹æ™®é€šè¯çš„å”‡éŸ³åˆ°è¯­éŸ³ï¼ˆL2Sï¼‰åˆæˆæ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå—åˆ°å¤æ‚å”‡åŠ¨åˆ°éŸ³ç´ æ˜ å°„ä»¥åŠè¯æ±‡éŸ³åœ¨å¯ç†è§£æ€§ä¸­çš„å…³é”®ä½œç”¨çš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯æ±‡éŸ³æ„ŸçŸ¥çš„å”‡éŸ³åˆ°è¯­éŸ³ï¼ˆLTA-L2Sï¼‰åˆæˆæ–¹æ³•ã€‚ä¸ºäº†è§£å†³å”‡åŠ¨åˆ°éŸ³ç´ çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è·¨è¯­è¨€è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œå¯¹ä¸€ä¸ªè‹±è¯­é¢„è®­ç»ƒçš„éŸ³é¢‘è§†è§‰è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹è¿›è¡Œé€‚é…ã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…å°†ä»å¤§é‡è‹±è¯­æ•°æ®ä¸­å­¦ä¹ åˆ°çš„é€šç”¨çŸ¥è¯†è½¬ç§»åˆ°æ™®é€šè¯é¢†åŸŸï¼Œè€Œä¸”é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒæ­¤ç±»æ¨¡å‹çš„æ˜‚è´µæˆæœ¬ã€‚ä¸ºäº†ä¸“é—¨å»ºæ¨¡è¯æ±‡éŸ³å¹¶å¢å¼ºå¯ç†è§£æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨æµåŒ¹é…æ¨¡å‹æ¥ç”ŸæˆF0è½®å»“ã€‚è¿™ä¸€ç”Ÿæˆè¿‡ç¨‹ç”±ç»è¿‡ASRç²¾ç»†è°ƒæ•´çš„SSLè¯­éŸ³å•å…ƒå¼•å¯¼ï¼Œè¿™äº›å•å…ƒåŒ…å«å…³é”®çš„è¶…éŸ³æ®µä¿¡æ¯ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæ•´ä½“è¯­éŸ³è´¨é‡å¾—åˆ°æå‡ï¼Œå…¶ä¸­æµåŒ¹é…åç½‘ç»œå¯¹ç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ç²—ç³™é¢‘è°±å›¾è¿›è¡Œç»†åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLTA-L2Såœ¨è¯­éŸ³å¯ç†è§£æ€§å’ŒéŸ³è°ƒå‡†ç¡®æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25670v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹æ™®é€šè¯çš„å”‡åŠ¨åˆ°è¯­éŸ³åˆæˆï¼ˆL2Sï¼‰å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¤æ‚çš„å”‡åŠ¨åˆ°éŸ³ç´ æ˜ å°„ä»¥åŠè¯æ±‡éŸ³è°ƒåœ¨å¯ç†è§£æ€§ä¸­çš„å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†è¯æ±‡éŸ³è°ƒæ„ŸçŸ¥çš„å”‡åŠ¨åˆ°è¯­éŸ³åˆæˆï¼ˆLTA-L2Sï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è·¨è¯­è¨€è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œå€Ÿé‰´è‹±è¯­é¢„è®­ç»ƒçš„éŸ³é¢‘è§†è§‰è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹æ¥è§£å†³éŸ³ç´ æ˜ å°„çš„å¤æ‚æ€§ã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…å°†é€šç”¨çŸ¥è¯†ä»å¤§é‡çš„è‹±è¯­æ•°æ®è½¬ç§»åˆ°æ™®é€šè¯é¢†åŸŸï¼Œè€Œä¸”é¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒæ­¤ç±»æ¨¡å‹çš„æ˜‚è´µæˆæœ¬ã€‚ä¸ºäº†ä¸“é—¨å»ºæ¨¡è¯æ±‡çš„éŸ³è°ƒå¹¶å¢å¼ºå¯ç†è§£æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨æµåŒ¹é…æ¨¡å‹æ¥ç”ŸæˆF0è½®å»“ã€‚è¿™ä¸€ç”Ÿæˆè¿‡ç¨‹ç”±ASRå¾®è°ƒSSLè¯­éŸ³å•å…ƒå¼•å¯¼ï¼Œè¿™äº›å•å…ƒåŒ…å«å…³é”®çš„è¶…åˆ†æ®µä¿¡æ¯ã€‚æ€»ä½“è€Œè¨€ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼æé«˜è¯­éŸ³è´¨é‡ï¼Œç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ç²—ç•¥é¢‘è°±å›¾é€šè¿‡æµåŒ¹é…åç½‘è¿›è¡Œç»†åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒLTA-L2Såœ¨è¯­éŸ³å¯ç†è§£æ€§å’ŒéŸ³è°ƒå‡†ç¡®æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™®é€šè¯çš„å”‡åŠ¨åˆ°è¯­éŸ³åˆæˆé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå¤æ‚çš„éŸ³ç´ æ˜ å°„å’Œè¯æ±‡éŸ³è°ƒçš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†è¯æ±‡éŸ³è°ƒæ„ŸçŸ¥çš„å”‡åŠ¨åˆ°è¯­éŸ³åˆæˆï¼ˆLTA-L2Sï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡è·¨è¯­è¨€è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œå€Ÿé‰´è‹±è¯­é¢„è®­ç»ƒçš„éŸ³é¢‘è§†è§‰è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ¨¡å‹æ¥è§£å†³éŸ³ç´ æ˜ å°„çš„å¤æ‚æ€§ã€‚</li>
<li>é‡‡ç”¨æµåŒ¹é…æ¨¡å‹ç”ŸæˆF0è½®å»“ï¼Œä»¥å¢å¼ºè¯­éŸ³çš„éŸ³è°ƒè¡¨ç°å’Œå¯ç†è§£æ€§ã€‚</li>
<li>ASRå¾®è°ƒSSLè¯­éŸ³å•å…ƒæä¾›å…³é”®çš„è¶…åˆ†æ®µä¿¡æ¯ï¼ŒæŒ‡å¯¼F0è½®å»“çš„ç”Ÿæˆã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ç”¨äºæé«˜è¯­éŸ³è´¨é‡ï¼Œå…¶ä¸­åŒ…æ‹¬æµåŒ¹é…åç½‘å¯¹ç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ç²—ç•¥é¢‘è°±å›¾çš„ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-df5262eb29dc7274526644a08b320244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104347&auth_key=1760104347-0-0-e6c7a3671aceba18ee6cca87fde5ecb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d57f471a646f08535744e3bc589490be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104355&auth_key=1760104355-0-0-847bd7afb63f72b8ec733a021aeb8bbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d313306de4a2777124ef9c7d0bbaa7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104361&auth_key=1760104361-0-0-bdd23838b44aa552754220ee83fe863b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-Emotion-Graphs-for-Compositional-Prompting-in-Zero-Shot-Speech-Emotion-Recognition"><a href="#Plug-and-Play-Emotion-Graphs-for-Compositional-Prompting-in-Zero-Shot-Speech-Emotion-Recognition" class="headerlink" title="Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot   Speech Emotion Recognition"></a>Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot   Speech Emotion Recognition</h2><p><strong>Authors:Jiacheng Shi, Hongfei Du, Y. Alicia Hong, Ye Gao</strong></p>
<p>Large audio-language models (LALMs) exhibit strong zero-shot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a framework that introduces structured Emotion Graphs (EGs) to guide LALMs in emotion inference without fine-tuning. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. Embedded into prompts, EGs provide interpretable and compositional representations that enhance LALM reasoning. Experiments across SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy over zero-shot baselines. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹å‰¯è¯­è¨€å»ºæ¨¡å’Œæœ‰é™çš„è·¨æ¨¡æ€æ¨ç†ï¼Œåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†é¢å‘æƒ…æ„Ÿæ¨ç†çš„ç»„æˆæ€ç»´é“¾æç¤ºï¼ˆCCoT-Emoï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥ç»“æ„åŒ–æƒ…æ„Ÿå›¾ï¼ˆEGï¼‰æ¥æŒ‡å¯¼LALMè¿›è¡Œæƒ…æ„Ÿæ¨æ–­ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚æ¯ä¸ªæƒ…æ„Ÿå›¾ç¼–ç ä¸ƒç§å£°å­¦ç‰¹å¾ï¼ˆä¾‹å¦‚éŸ³è°ƒã€è¯­é€Ÿã€æŠ–åŠ¨ã€é¢¤éŸ³ï¼‰ã€æ–‡æœ¬æƒ…æ„Ÿã€å…³é”®è¯å’Œè·¨æ¨¡æ€å…³è”ã€‚åµŒå…¥åˆ°æç¤ºä¸­ï¼Œæƒ…æ„Ÿå›¾æä¾›å¯è§£é‡Šå’Œç»„åˆè¡¨ç¤ºï¼Œå¢å¼ºäº†LALMæ¨ç†ã€‚åœ¨SERåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCCoT-Emoä¼˜äºå…ˆå‰æœ€ä½³æŠ€æœ¯ï¼Œå¹¶æé«˜äº†é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25458v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†åŸºäºæƒ…æ„Ÿå›¾çš„æƒ…ç»ªæ¨ç†æ¡†æ¶Compositional Chain-of-Thought Promptingï¼ˆCCoT-Emoï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æƒ…æ„Ÿå›¾æ¥å¼•å¯¼LALMè¿›è¡Œæƒ…æ„Ÿæ¨ç†ï¼Œæ— éœ€å¾®è°ƒã€‚æƒ…æ„Ÿå›¾ç»“åˆäº†è¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€æ–‡æœ¬æƒ…æ„Ÿã€å…³é”®è¯å’Œè·¨æ¨¡æ€å…³è”ï¼ŒåµŒå…¥åˆ°æç¤ºä¸­ï¼Œæä¾›äº†å¯è§£é‡Šå’Œç»„åˆæ€§çš„è¡¨ç¤ºï¼Œæé«˜äº†LALMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒCCoT-Emoåœ¨SERåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶åœ¨é›¶æ ·æœ¬åŸºå‡†ä¸Šæé«˜äº†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹æ—è¯­è¨€å­¦å»ºæ¨¡å’Œè·¨æ¨¡æ€æ¨ç†ã€‚</li>
<li>æå‡ºäº†Compositional Chain-of-Thought Prompting for Emotion Reasoningï¼ˆCCoT-Emoï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æƒ…æ„Ÿå›¾æ¥æ”¹è¿›LALMsåœ¨SERæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>æƒ…æ„Ÿå›¾ç»“åˆäº†è¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€æ–‡æœ¬æƒ…æ„Ÿå’Œå…³é”®è¯ï¼Œä»¥åŠè·¨æ¨¡æ€å…³è”ã€‚</li>
<li>åµŒå…¥å¼æƒ…æ„Ÿå›¾æä¾›å¯è§£é‡Šå’Œç»„åˆæ€§çš„è¡¨ç¤ºï¼Œå¢å¼ºLALMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CCoT-Emoæ¡†æ¶æ— éœ€å¾®è°ƒï¼Œå³å¯æé«˜LALMsåœ¨SERä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCCoT-Emoåœ¨SERåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1f26a90754d65cbd0ea4b9d96653d049~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104369&auth_key=1760104369-0-0-405e44f09c2b8eae00d5e2ec9d27e02b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-209cd58abc22cd4f47e7bcf02673576f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104376&auth_key=1760104376-0-0-332802bf5c10d5d0089bb431cfea9988&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6189f8f6de56edc25c323358a60e86f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104383&auth_key=1760104383-0-0-94188860d0d8f08afa3af1e18fe4456b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13a4f1521b0d5f1456bbdbfb3b508035~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104390&auth_key=1760104390-0-0-1096f2b56bb8e60d5fbd902a0a0857ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VoiceBridge-Designing-Latent-Bridge-Models-for-General-Speech-Restoration-at-Scale"><a href="#VoiceBridge-Designing-Latent-Bridge-Models-for-General-Speech-Restoration-at-Scale" class="headerlink" title="VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale"></a>VoiceBridge: Designing Latent Bridge Models for General Speech   Restoration at Scale</h2><p><strong>Authors:Chi Zhang, Zehua Chen, Kaiwen Zheng, Jun Zhu</strong></p>
<p>Bridge models have recently been explored for speech enhancement tasks such as denoising, dereverberation, and super-resolution, while these efforts are typically confined to a single task or small-scale datasets, with constrained general speech restoration (GSR) capability at scale. In this work, we introduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs), capable of reconstructing high-fidelity speech at full-band (\textit{i.e.,} 48<del>kHz) from various distortions. By compressing speech waveform into continuous latent representations, VoiceBridge models the</del>\textit{diverse LQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\textit{a single latent-to-latent generative process} backed by a scalable transformer architecture. To better inherit the advantages of bridge models from the data domain to the latent space, we present an energy-preserving variational autoencoder, enhancing the alignment between the waveform and latent space over varying energy levels. Furthermore, to address the difficulty of HQ reconstruction from distinctively different LQ priors, we propose a joint neural prior, uniformly alleviating the reconstruction burden of LBM. At last, considering the key requirement of GSR systems, human perceptual quality, a perceptually aware fine-tuning stage is designed to mitigate the cascading mismatch in generation while improving perceptual alignment. Extensive validation across in-domain and out-of-domain tasks and datasets (\textit{e.g.}, refining recent zero-shot speech and podcast generation results) demonstrates the superior performance of VoiceBridge. Demo samples can be visited at: <a target="_blank" rel="noopener" href="https://voicebridge-demo.github.io/">https://VoiceBridge-demo.github.io/</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ¡¥æ¢æ¨¡å‹å·²è¢«æ¢ç´¢ç”¨äºè¯­éŸ³å¢å¼ºä»»åŠ¡ï¼Œå¦‚å»å™ªã€æ¶ˆé™¤å›å£°å’Œè¶…åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œè¿™äº›åŠªåŠ›é€šå¸¸å±€é™äºå•ä¸€ä»»åŠ¡æˆ–å°è§„æ¨¡æ•°æ®é›†ï¼Œå¤§è§„æ¨¡é€šç”¨è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰èƒ½åŠ›å—é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBridgeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ½œåœ¨æ¡¥æ¢æ¨¡å‹ï¼ˆLBMï¼‰çš„GSRç³»ç»Ÿï¼Œèƒ½å¤Ÿä»å„ç§å¤±çœŸä¸­é‡å»ºå…¨é¢‘å¸¦ï¼ˆå³48kHzï¼‰çš„é«˜ä¿çœŸè¯­éŸ³ã€‚é€šè¿‡å°†è¯­éŸ³æ³¢å½¢å‹ç¼©æˆè¿ç»­æ½œåœ¨è¡¨ç¤ºï¼ŒVoiceBridgeç”¨ä¸€ä¸ªæ½œåœ¨ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´çš„ç”Ÿæˆè¿‡ç¨‹æ¥æ¨¡æ‹ŸGSRä¸­çš„å¤šç§LQ-to-HQä»»åŠ¡ï¼ˆå³ä»ä½è´¨é‡åˆ°é«˜è´¨é‡ï¼‰ã€‚ä¸ºäº†æ›´å¥½åœ°ä»æ•°æ®åŸŸç»§æ‰¿æ¡¥æ¢æ¨¡å‹çš„ä¼˜åŠ¿åˆ°æ½œåœ¨ç©ºé—´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½é‡ä¿æŒå¼å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¢å¼ºäº†æ³¢å½¢å’Œæ½œåœ¨ç©ºé—´åœ¨ä¸åŒèƒ½é‡æ°´å¹³ä¸Šçš„å¯¹é½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ä»æ˜æ˜¾ä¸åŒçš„LQå…ˆéªŒä¸­è¿›è¡ŒHQé‡å»ºçš„å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè”åˆç¥ç»å…ˆéªŒï¼Œç»Ÿä¸€ç¼“è§£äº†LBMçš„é‡å»ºè´Ÿæ‹…ã€‚æœ€åï¼Œè€ƒè™‘åˆ°GSRç³»ç»Ÿçš„å…³é”®éœ€æ±‚â€”â€”äººç±»æ„ŸçŸ¥è´¨é‡ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥æ„è¯†å¾®è°ƒé˜¶æ®µï¼Œæ—¨åœ¨å‡è½»ç”Ÿæˆè¿‡ç¨‹ä¸­çš„çº§è”ä¸åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶æé«˜æ„ŸçŸ¥å¯¹é½ã€‚åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯ï¼ˆä¾‹å¦‚ï¼Œæ”¹è¿›æœ€è¿‘çš„é›¶æ ·æœ¬è¯­éŸ³å’Œæ’­å®¢ç”Ÿæˆç»“æœï¼‰è¡¨æ˜VoiceBridgeçš„å“è¶Šæ€§èƒ½ã€‚æ¼”ç¤ºæ ·å“å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://voicebridge-demo.github.io/%E3%80%82">https://VoiceBridge-demo.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25275v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ½œåœ¨æ¡¥æ¢æ¨¡å‹ï¼ˆLBMsï¼‰çš„VoiceBridgeç³»ç»Ÿï¼Œç”¨äºé€šç”¨è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿé‡å»ºé«˜è´¨é‡çš„å…¨é¢‘å¸¦è¯­éŸ³ï¼Œå¹¶å…·æœ‰ä»å„ç§å¤±çœŸä¸­æ¢å¤è¯­éŸ³çš„èƒ½åŠ›ã€‚é€šè¿‡å‹ç¼©è¯­éŸ³æ³¢å½¢åˆ°è¿ç»­çš„æ½œåœ¨è¡¨ç¤ºï¼ŒVoiceBridgeç”¨ä¸€ä¸ªæ½œåœ¨åˆ°æ½œåœ¨ç”Ÿæˆè¿‡ç¨‹å¯¹å„ç§ä½è´¨é‡åˆ°é«˜è´¨é‡çš„ä»»åŠ¡è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å¼•å…¥èƒ½é‡ä¿æŒå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå¢å¼ºæ³¢å½¢å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´åœ¨ä¸åŒèƒ½é‡æ°´å¹³ä¸Šçš„å¯¹é½ã€‚é’ˆå¯¹ä»ç‹¬ç‰¹ä½è´¨é‡å…ˆéªŒé‡æ„é«˜è´¨é‡è¯­éŸ³çš„å›°éš¾ï¼Œæå‡ºäº†ä¸€ç§è”åˆç¥ç»å…ˆéªŒï¼Œå‡è½»äº†LBMçš„é‡æ„è´Ÿæ‹…ã€‚æœ€åï¼Œè€ƒè™‘åˆ°GSRç³»ç»Ÿçš„å…³é”®éœ€æ±‚â€”â€”äººç±»æ„ŸçŸ¥è´¨é‡ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥æ„è¯†å¾®è°ƒé˜¶æ®µï¼Œä»¥ç¼“è§£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„çº§è”ä¸åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶æé«˜æ„ŸçŸ¥å¯¹é½ã€‚åœ¨è·¨é¢†åŸŸä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯è¡¨æ˜VoiceBridgeçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VoiceBridgeæ˜¯ä¸€ä¸ªåŸºäºæ½œåœ¨æ¡¥æ¢æ¨¡å‹ï¼ˆLBMsï¼‰çš„é€šç”¨è¯­éŸ³æ¢å¤ï¼ˆGSRï¼‰ç³»ç»Ÿã€‚</li>
<li>èƒ½å¤Ÿé‡å»ºé«˜è´¨é‡çš„å…¨é¢‘å¸¦ï¼ˆå³48 kHzï¼‰è¯­éŸ³ï¼Œå¹¶ä»å„ç§å¤±çœŸä¸­æ¢å¤è¯­éŸ³ã€‚</li>
<li>é€šè¿‡è¿ç»­æ½œåœ¨è¡¨ç¤ºå‹ç¼©è¯­éŸ³æ³¢å½¢ï¼Œç”¨ä¸€ä¸ªæ½œåœ¨åˆ°æ½œåœ¨ç”Ÿæˆè¿‡ç¨‹å¯¹å„ç§ä½è´¨é‡åˆ°é«˜è´¨é‡çš„ä»»åŠ¡è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>å¼•å…¥èƒ½é‡ä¿æŒå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå¢å¼ºæ³¢å½¢å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´åœ¨ä¸åŒèƒ½é‡æ°´å¹³çš„å¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è”åˆç¥ç»å…ˆéªŒï¼Œä»¥å¤„ç†ä»ç‹¬ç‰¹ä½è´¨é‡å…ˆéªŒé‡æ„é«˜è´¨é‡è¯­éŸ³çš„éš¾é¢˜ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥æ„è¯†å¾®è°ƒé˜¶æ®µï¼Œä»¥æé«˜è¯­éŸ³æ¢å¤çš„æ„ŸçŸ¥è´¨é‡å’Œç¼“è§£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>VoiceBridgeåœ¨è·¨é¢†åŸŸä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ç»è¿‡å¹¿æ³›éªŒè¯ï¼Œå¹¶å±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-753fa7aeaad52253e795f5c525861223~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104398&auth_key=1760104398-0-0-7ba3e4f42c0a23ad7010a82fcc06d4f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a72c8e3435a05fe7739f00884815727f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104405&auth_key=1760104405-0-0-4b629c7a50a112b288101592e4771cef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-527c3c28fbf313182a21faa731596ab8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104412&auth_key=1760104412-0-0-6f269a5968ca0783c76bdb79381058a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-113056ddee9bf190f2f997576b8030a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104419&auth_key=1760104419-0-0-35ecb953bf593378181a1fb60008d8c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36052514ba00e6141472da8bdf530c06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104426&auth_key=1760104426-0-0-1e6c9449d473aa8a13630e521b6c02ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning"><a href="#VSSFlow-Unifying-Video-conditioned-Sound-and-Speech-Generation-via-Joint-Learning" class="headerlink" title="VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning"></a>VSSFlow: Unifying Video-conditioned Sound and Speech Generation via   Joint Learning</h2><p><strong>Authors:Xin Cheng, Yuyue Wang, Xihua Wang, Yihan Wu, Kaisi Guan, Yijing Chen, Peng Zhang, Xiaojiang Liu, Meng Cao, Ruihua Song</strong></p>
<p>Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models. </p>
<blockquote>
<p>è§†é¢‘æ¡ä»¶çš„å£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬è§†é¢‘åˆ°å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ï¼Œä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹å…¶è¿›è¡Œæ¢ç´¢çš„å°è¯•æœ‰é™ã€‚æœ€è¿‘å°è¯•å°†V2Så’ŒVisualTTSç»Ÿä¸€èµ·æ¥ï¼Œåœ¨å¤„ç†ä¸åŒçš„æ¡ä»¶ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„è§†é¢‘å’Œæ–‡æœ¬æ¡ä»¶ï¼‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”éœ€è¦å¤æ‚çš„è®­ç»ƒé˜¶æ®µã€‚ç»Ÿä¸€è¿™ä¸¤ä¸ªä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VSSFlowï¼Œå®ƒæ— ç¼åœ°å°†V2Så’ŒVisualTTSä»»åŠ¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æµåŒ¹é…æ¡†æ¶ä¸­ã€‚VSSFlowä½¿ç”¨äº†ä¸€ç§æ–°å‹çš„æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼•å…¥æ¡ä»¶çš„è¿‡ç¨‹ä¸­ï¼Œè·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚è¡¨ç°å‡ºä¸åŒçš„å½’çº³åç½®ã€‚å› æ­¤ï¼ŒVSSFlowåˆ©ç”¨è¿™äº›å½’çº³åç½®æ¥æœ‰æ•ˆåœ°å¤„ç†ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼šè·¨æ³¨æ„åŠ›ç”¨äºæ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶ï¼Œè‡ªæ³¨æ„åŠ›ç”¨äºæ›´ç¡®å®šçš„è¯­éŸ³æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œä¸æˆ‘ä»¬æ™®éè®¤ä¸ºçš„è”åˆè®­ç»ƒè¿™ä¸¤ä¸ªä»»åŠ¡éœ€è¦å¤æ‚çš„è®­ç»ƒç­–ç•¥å¹¶å¯èƒ½é™ä½æ€§èƒ½ç›¸åï¼Œæˆ‘ä»¬å‘ç°VSSFlowå—ç›Šäºå£°éŸ³å’Œè¯­éŸ³ç”Ÿæˆçš„ç«¯åˆ°ç«¯è”åˆå­¦ä¹ è¿‡ç¨‹ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œé¢å¤–è®¾è®¡ã€‚è¯¦ç»†åˆ†æå°†å…¶å½’å› äºä»»åŠ¡ä¹‹é—´å­¦åˆ°çš„å…±äº«é€šç”¨éŸ³é¢‘å…ˆéªŒï¼Œè¿™åŠ é€Ÿäº†æ”¶æ•›ï¼Œå¢å¼ºäº†æ¡ä»¶ç”Ÿæˆï¼Œå¹¶ç¨³å®šäº†æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVSSFlowè¶…è¶Šäº†V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•é¢†åŸŸçš„æœ€æ–°æŠ€æœ¯ï¼Œçªæ˜¾å‡ºç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å…³é”®æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24773v2">PDF</a> Paper Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†VSSFlowï¼Œä¸€ä¸ªå°†è§†é¢‘è½¬å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ç»Ÿä¸€äºä¸€ä¸ªæ¡†æ¶çš„æ–¹æ³•ã€‚VSSFlowä½¿ç”¨æ–°å‹çš„æ¡ä»¶èšåˆæœºåˆ¶æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ä¿¡å·ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚çš„ä¸åŒè¯±å¯¼åå·®æ¥å¤„ç†ä¸åŒçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è”åˆè®­ç»ƒä¸¤ä¸ªä»»åŠ¡æ— éœ€å¤æ‚çš„è®­ç»ƒç­–ç•¥ï¼Œåè€Œèƒ½ä»ç«¯åˆ°ç«¯çš„è”åˆå­¦ä¹ è¿‡ç¨‹è·ç›Šï¼Œè¿™å¾—ç›Šäºä»»åŠ¡é—´å…±äº«çš„é€šç”¨éŸ³é¢‘å…ˆéªŒçŸ¥è¯†ã€‚VSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå±•ç°äº†ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VSSFlowæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ•´åˆäº†è§†é¢‘è½¬å£°éŸ³ï¼ˆV2Sï¼‰å’Œè§†è§‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVisualTTSï¼‰ä»»åŠ¡ã€‚</li>
<li>VSSFlowé‡‡ç”¨æ–°å‹æ¡ä»¶èšåˆæœºåˆ¶å¤„ç†ä¸åŒè¾“å…¥ä¿¡å·ã€‚</li>
<li>äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚åœ¨VSSFlowä¸­ç”¨äºå¤„ç†ä¸åŒçš„è¡¨ç°ã€‚</li>
<li>è”åˆè®­ç»ƒä¸¤ä¸ªä»»åŠ¡æ— éœ€å¤æ‚ç­–ç•¥ï¼Œå¾—ç›Šäºç«¯åˆ°ç«¯çš„è”åˆå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ä»»åŠ¡é—´å…±äº«çš„é€šç”¨éŸ³é¢‘å…ˆéªŒçŸ¥è¯†åŠ é€Ÿäº†æ”¶æ•›ï¼Œå¢å¼ºäº†æ¡ä»¶ç”Ÿæˆï¼Œç¨³å®šäº†æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿‡ç¨‹ã€‚</li>
<li>VSSFlowåœ¨V2Så’ŒVisualTTSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-67983a5214bf034a608ca1004e6a7bfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104433&auth_key=1760104433-0-0-87d9420e0cc790a90012f54566704883&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa383b8af756e9c393499fe593a9596d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104440&auth_key=1760104440-0-0-74df8b87afb443402053493d66a50f49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3beec9a7fa106c34c7d5761339c47c73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104447&auth_key=1760104447-0-0-7b92e343868c2602649ae7dbf39c3c09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow"><a href="#MeanFlowSE-One-Step-Generative-Speech-Enhancement-via-MeanFlow" class="headerlink" title="MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow"></a>MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow</h2><p><strong>Authors:Yike Zhu, Boyi Kang, Ziqian Wang, Xingchen Li, Zihan Zhang, Wenjie Li, Longshuai Xiao, Wei Xue, Lei Xie</strong></p>
<p>Speech enhancement (SE) recovers clean speech from noisy signals and is vital for applications such as telecommunications and automatic speech recognition (ASR). While generative approaches achieve strong perceptual quality, they often rely on multi-step sampling (diffusion&#x2F;flow-matching) or large language models, limiting real-time deployment. To mitigate these constraints, we present MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to predict an average-velocity field for one-step latent refinement and conditions the model on self-supervised learning (SSL) representations rather than VAE latents. This design accelerates inference and provides robust acoustic-semantic guidance during training. In the Interspeech 2020 DNS Challenge blind test set and simulated test set, MeanFlowSE attains state-of-the-art (SOTA) level perceptual quality and competitive intelligibility while significantly lowering both real-time factor (RTF) and model size compared with recent generative competitors, making it suitable for practical use. The code will be released upon publication at <a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE">https://github.com/Hello3orld/MeanFlowSE</a>. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä»å¸¦å™ªå£°çš„ä¿¡å·ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ï¼Œå¯¹ç”µä¿¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶ç”Ÿæˆæ–¹æ³•è¾¾åˆ°äº†å¾ˆå¼ºçš„æ„ŸçŸ¥è´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤šæ­¥é‡‡æ ·ï¼ˆæ‰©æ•£&#x2F;æµåŒ¹é…ï¼‰æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å®æ—¶éƒ¨ç½²ã€‚ä¸ºäº†ç¼“è§£è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬æå‡ºäº†MeanFlowSEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€æ­¥ç”ŸæˆSEæ¡†æ¶ã€‚å®ƒé‡‡ç”¨MeanFlowæ¥é¢„æµ‹ä¸€æ­¥æ½œåœ¨ç²¾åŒ–çš„å¹³å‡é€Ÿåº¦åœºï¼Œå¹¶ä»¥è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡¨ç¤ºè€Œä¸æ˜¯VAEæ½œåœ¨è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚è¿™ç§è®¾è®¡åŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›äº†ç¨³å¥çš„å£°å­¦è¯­ä¹‰æŒ‡å¯¼ã€‚åœ¨Interspeech 2020 DNS Challengeçš„ç›²æµ‹è¯•é›†å’Œæ¨¡æ‹Ÿæµ‹è¯•é›†ä¸­ï¼ŒMeanFlowSEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ„ŸçŸ¥è´¨é‡å’Œæœ‰ç«äº‰åŠ›çš„æ¸…æ™°åº¦ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†å®æ—¶å› å­ï¼ˆRTFï¼‰å’Œæ¨¡å‹å¤§å°ï¼Œä¸æœ€è¿‘çš„ç”Ÿæˆç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œé€‚åˆå®é™…åº”ç”¨ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Hello3orld/MeanFlowSEä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23299v2">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä»å™ªå£°ä¿¡å·ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ï¼Œå¯¹ç”µä¿¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶ç”Ÿæˆæ–¹æ³•å¯ä»¥è¾¾åˆ°å¾ˆå¼ºçš„æ„ŸçŸ¥è´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤šæ­¥é‡‡æ ·ï¼ˆæ‰©æ•£&#x2F;æµåŒ¹é…ï¼‰æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å®æ—¶éƒ¨ç½²ã€‚ä¸ºäº†ç¼“è§£è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬æå‡ºäº†MeanFlowSEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€æ­¥ç”ŸæˆSEæ¡†æ¶ã€‚å®ƒé‡‡ç”¨MeanFlowé¢„æµ‹å¹³å‡é€Ÿåº¦åœºè¿›è¡Œä¸€æ­¥æ½œåœ¨ç»†åŒ–ï¼Œå¹¶åœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡¨ç¤ºçš„åŸºç¡€ä¸Šå¯¹æ¨¡å‹è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œè€Œä¸æ˜¯VAEæ½œå˜é‡ã€‚è¿™ç§è®¾è®¡åŠ é€Ÿäº†æ¨ç†ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›äº†ç¨³å¥çš„å£°å­¦è¯­ä¹‰æŒ‡å¯¼ã€‚åœ¨Interspeech 2020 DNS Challengeçš„ç›²æµ‹è¯•é›†å’Œæ¨¡æ‹Ÿæµ‹è¯•é›†ä¸­ï¼ŒMeanFlowSEè¾¾åˆ°äº†å…ˆè¿›çš„æ„ŸçŸ¥è´¨é‡å’Œæœ‰ç«äº‰åŠ›çš„å¯ç†è§£æ€§ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†å®æ—¶å› å­ï¼ˆRTFï¼‰å’Œæ¨¡å‹å¤§å°ï¼Œä¸æœ€è¿‘çš„ç”Ÿæˆç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œé€‚åˆå®é™…åº”ç”¨ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hello3orld/MeanFlowSE%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Hello3orld/MeanFlowSEå‘å¸ƒã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰åœ¨ç”µä¿¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç”Ÿæˆæ–¹æ³•è™½ç„¶èƒ½è¾¾æˆé«˜æ„ŸçŸ¥è´¨é‡ï¼Œä½†å­˜åœ¨å®æ—¶éƒ¨ç½²çš„å±€é™æ€§ã€‚</li>
<li>MeanFlowSEæ˜¯ä¸€ä¸ªä¸€æ­¥ç”ŸæˆSEæ¡†æ¶ï¼Œé‡‡ç”¨MeanFlowé¢„æµ‹å¹³å‡é€Ÿåº¦åœºè¿›è¡Œæ½œåœ¨ç»†åŒ–ã€‚</li>
<li>MeanFlowSEåœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡¨ç¤ºçš„åŸºç¡€ä¸Šè¿›è¡Œæ¡ä»¶è®¾ç½®ï¼ŒåŠ é€Ÿæ¨ç†å¹¶æä¾›ç¨³å¥çš„å£°å­¦è¯­ä¹‰æŒ‡å¯¼ã€‚</li>
<li>åœ¨Interspeech 2020 DNS Challengeçš„æµ‹è¯•ä¸­ï¼ŒMeanFlowSEè¾¾åˆ°å…ˆè¿›æ„ŸçŸ¥è´¨é‡å’Œç«äº‰åŠ›å¯ç†è§£æ€§ã€‚</li>
<li>ä¸å…¶ä»–ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼ŒMeanFlowSEé™ä½å®æ—¶å› å­ï¼ˆRTFï¼‰å’Œæ¨¡å‹å¤§å°ã€‚</li>
<li>MeanFlowSEé€‚åˆå®é™…åº”ç”¨ï¼Œä»£ç å°†åœ¨ç›¸å…³ä»“åº“å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b409c0d5ddc4350f7c4ca4fe3f726dab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104454&auth_key=1760104454-0-0-213f93b8fc61e40a3c4148973d0e8b5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6001b1a912c0553937b4279f34ed1085~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104461&auth_key=1760104461-0-0-6b86c9206bb396e9343a0b1e5ccca400&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0b72033986e0860109dbe26d764c7aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104468&auth_key=1760104468-0-0-cf3bc355a156c57a8f8057d977f4238a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2564543a513ee6228feb84dafc4fc5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104475&auth_key=1760104475-0-0-53be85806e3b70f758e11a3b6ec56f39&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents"><a href="#i-LAVA-Insights-on-Low-Latency-Voice-2-Voice-Architecture-for-Agents" class="headerlink" title="i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents"></a>i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents</h2><p><strong>Authors:Anupam Purwar, Aditya Choudhary</strong></p>
<p>We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi. </p>
<blockquote>
<p>æˆ‘ä»¬å®éªŒäº†ä¸€ä¸ªä½å»¶è¿Ÿã€ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³é€šä¿¡æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–å…¶é€‚ç”¨äºå®æ—¶å¯¹è¯åº”ç”¨ã€‚é€šè¿‡åˆ†æè¯­éŸ³åˆ°è¯­éŸ³ï¼ˆV-2-Vï¼‰ç³»ç»Ÿæ‰€å¿…éœ€çš„å…³é”®ç»„ä»¶ï¼Œå³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ï¼Œæˆ‘ä»¬çš„å·¥ä½œåˆ†æäº†å¦‚ä½•åœ¨ä¿æŒé«˜è´¨é‡äº¤äº’çš„åŒæ—¶å‡å°‘å¤„ç†æ—¶é—´ï¼Œä»¥ç¡®å®šä¼˜åŒ–V-2-Vç³»ç»Ÿçš„å…³é”®æ æ†ã€‚æˆ‘ä»¬çš„å·¥ä½œå‘ç°ï¼Œç”Ÿæˆå……æ»¡æƒ…æ„Ÿã€é€¼çœŸçš„è¯­éŸ³çš„TTSç»„ä»¶ï¼ˆåŒ…æ‹¬è‡ªç„¶åœé¡¿å’Œæ„Ÿå¹ï¼‰å¯¹å®æ—¶å› å­ï¼ˆRTFï¼‰çš„å½±å“æœ€å¤§ã€‚æ‰€è¯•éªŒçš„V-2-Væ¶æ„åˆ©ç”¨CSM1bï¼Œé€šè¿‡æ‘„å–å…ˆå‰çš„éŸ³é¢‘å’Œæ–‡æœ¬äº¤æ¢ï¼Œç†è§£å¯¹è¯çš„è¯­è°ƒä»¥åŠè¯­å¢ƒï¼Œä»è€Œç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡TTSè§£ç å™¨ä¼˜åŒ–å‰©ä½™å‘é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£çš„æ–¹æ³•ï¼Œè™½ç„¶è¿™ä¼šé™ä½ç”Ÿæˆçš„è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¿˜è¡¨æ˜ï¼Œå¯¹äºåŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€é‡è¦çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠä¸Mimiä¸­ä½¿ç”¨çš„ä»£ç ç°¿ç›¸ç»“åˆæ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20971v2">PDF</a> This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)   architecture, identifying that the Text-to-Speech (TTS) component has the   highest impact on real-time performance. By reducing the number of Residual   Vector Quantization (RVQ) iterations in the TTS model, latency can be   effectively halved. Its accepted at AIML Systems 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹å®æ—¶è¯­éŸ³å¯¹è¯åº”ç”¨çš„ç«¯åˆ°ç«¯è¯­éŸ³é€šä¿¡æ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡åˆ†æè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ç­‰å…³é”®ç¯èŠ‚ï¼Œæ¢ç´¢å‡å°‘å¤„ç†æ—¶é—´çš„åŒæ—¶ä¿æŒé«˜è´¨é‡äº’åŠ¨çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œã€å¸¦æœ‰è‡ªç„¶åœé¡¿å’Œè¯­æ°”è¯çš„è¯­éŸ³çš„TTSç»„ä»¶å¯¹å®æ—¶å› å­ï¼ˆRTFï¼‰å½±å“æœ€å¤§ã€‚å®éªŒçš„V-2-Væ¶æ„é€šè¿‡åˆ©ç”¨CSMæŠ€æœ¯ï¼Œå¯ä»¥ç†è§£å’Œç”Ÿæˆä¸å¯¹è¯å†…å®¹å’Œè¯­æ°”ç›¸å…³çš„è¯­éŸ³ï¼ŒåŒæ—¶é€šè¿‡ä¼˜åŒ–TTSè§£ç å™¨çš„æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰è¿­ä»£æ¥æå‡æ•ˆç‡ã€‚å®éªŒç»“æœå±•ç¤ºäº†å¯¹RVQè¿­ä»£çš„ä¼˜åŒ–å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘ç”Ÿæˆçš„è¯­éŸ³è´¨é‡æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯è¯­éŸ³é€šä¿¡æ¨¡å‹çš„ä¼˜åŒ–ç­–ç•¥ï¼Œç”¨äºå®æ—¶è¯­éŸ³å¯¹è¯åº”ç”¨ã€‚</li>
<li>åˆ†æè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œå¯¹è¯ç®¡ç†ç­‰å…³é”®ç¯èŠ‚å¯¹å®æ—¶å› å­ï¼ˆRTFï¼‰çš„å½±å“ã€‚</li>
<li>å‘ç°TTSç»„ä»¶æ˜¯å½±å“RTFæœ€å¤§çš„éƒ¨åˆ†ï¼Œå®ƒèƒ½ç”Ÿæˆå¸¦æœ‰æƒ…æ„Ÿå’Œè‡ªç„¶åœé¡¿çš„è¯­éŸ³ã€‚</li>
<li>åˆ©ç”¨CSMæŠ€æœ¯ç†è§£å’Œç”Ÿæˆä¸å¯¹è¯å†…å®¹å’Œè¯­æ°”ç›¸å…³çš„è¯­éŸ³ï¼Œæå‡äº¤äº’è´¨é‡ã€‚</li>
<li>ä¼˜åŒ–TTSè§£ç å™¨çš„RVQè¿­ä»£ä»¥æå‡æ•ˆç‡ï¼ŒåŒæ—¶å‡å°‘ç”Ÿæˆçš„è¯­éŸ³è´¨é‡æŸå¤±ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå‡å°‘RVQè¿­ä»£æ¬¡æ•°å’Œä»£ç æœ¬çš„ä½¿ç”¨æ˜¯å®ç°V-2-Vç³»ç»Ÿä¼˜åŒ–çš„å…³é”®æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-59c821b57aeaa9534dba35b42a820d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104483&auth_key=1760104483-0-0-c3eabb15f00bb310e45bef4241d5db12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d49adfa80e0c9bd41043f26e3e711e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104490&auth_key=1760104490-0-0-9bbb8bcf7ee78dadc2be86ca43122633&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b34709ac926874eba4eb35aeca95e069~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104496&auth_key=1760104496-0-0-9dc5d0c2b8d26965feb6a71d9a73c4f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9be302502aa6d7901d79ac3ba0d45e1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104503&auth_key=1760104503-0-0-deeac05fc7d3e52a6c0a1fd687f86e58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0415f218174ef0ecb084055ac4fadefd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104510&auth_key=1760104510-0-0-16d3c88a0a9a3a2ba630a2f35584d9d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c674493e85b2953c23110196944d153~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104517&auth_key=1760104517-0-0-c9c3e845bf6823a7be49ee2d749caae4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0750e443c0ec570d2f4ab834004e6752~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104523&auth_key=1760104523-0-0-06cba7efc505972cf2d1fa2e90096bcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding"><a href="#SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding" class="headerlink" title="SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding"></a>SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding</h2><p><strong>Authors:Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao</strong></p>
<p>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">https://github.com/ShawnPi233/SynParaSpeech</a>. </p>
<blockquote>
<p>ç±»è¯­è¨€å£°éŸ³ï¼Œå¦‚ç¬‘å£°å’Œå¹æ¯å£°ï¼Œå¯¹äºåˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“æœ‰æ•°æ®é›†ï¼Œè€Œå…¬å¼€å¯ç”¨çš„èµ„æºå¾€å¾€å­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ã€ä»¥åŠç°å®ä¸–ç•Œç›¸å…³æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡ç±»è¯­è¨€æ•°æ®é›†çš„æ¡†æ¶ï¼Œå¹¶åº”ç”¨è¯¥æ¡†æ¶æ„å»ºäº†SynParaSpeechæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«6å¤§ç±»ç±»è¯­è¨€æ•°æ®ï¼ŒåŒ…å«118.75å°æ—¶çš„æ•°æ®å’Œç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œæ‰€æœ‰è¯­éŸ³å‡æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºå¼•å…¥ç¬¬ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡ç±»è¯­è¨€æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶å‘å¸ƒSynParaSpeechè¯­æ–™åº“ï¼Œå®ƒé€šè¿‡æ›´è‡ªç„¶çš„ç±»è¯­è¨€åˆæˆæ¨è¿›è¯­éŸ³ç”Ÿæˆï¼Œå¹¶é€šè¿‡æ”¹è¿›ç±»è¯­è¨€äº‹ä»¶æ£€æµ‹æé«˜è¯­éŸ³ç†è§£ã€‚æ•°æ®é›†å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShawnPi233/SynParaSpeechè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14946v3">PDF</a> Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, including reprinting&#x2F;republishing this material for advertising or   promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºï¼Œå‰¯è¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå¹æ¯å£°ï¼‰å¯¹äºåˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“æœ‰æ•°æ®é›†ï¼Œè€Œå…¬å¼€å¯ç”¨çš„èµ„æºå¾€å¾€å­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ã€ä»¥åŠç°å®ç›¸å…³æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¹¶åº”ç”¨è¯¥æ¡†æ¶æ„å»ºäº†SynParaSpeechæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«6ä¸ªå‰¯è¯­è¨€ç±»åˆ«ï¼Œå…±æœ‰118.75å°æ—¶çš„æ•°æ®å’Œç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œæ‰€æœ‰å†…å®¹å‡æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ã€‚æ–‡æœ¬çš„ä¸»è¦è´¡çŒ®åœ¨äºå¼•å…¥äº†ä¸€ç§æ„å»ºå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†SynParaSpeechè¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“é€šè¿‡æ›´è‡ªç„¶çš„å‰¯è¯­è¨€åˆæˆæ¨è¿›äº†è¯­éŸ³ç”Ÿæˆï¼Œå¹¶é€šè¿‡æé«˜å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ”¹å–„äº†è¯­éŸ³ç†è§£ã€‚æ•°æ®é›†å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShawnPi233/SynParaSpeechè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‰¯è¯­è¨€å£°éŸ³å¯¹äºåˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨ä¾èµ–ä¸“æœ‰æ•°æ®ã€å…¬å¼€èµ„æºè¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶æ¥ç”Ÿæˆå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†ã€‚</li>
<li>æ„å»ºäº†åŒ…å«6ä¸ªå‰¯è¯­è¨€ç±»åˆ«ã€118.75å°æ—¶æ•°æ®å’Œç²¾ç¡®æ—¶é—´æˆ³çš„SynParaSpeechæ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ï¼Œæé«˜äº†ç°å®ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡æ›´è‡ªç„¶çš„å‰¯è¯­è¨€åˆæˆæ¨è¿›äº†è¯­éŸ³ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c9c4678560e9292fc00ffb87a0ce9026~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104531&auth_key=1760104531-0-0-9b2715bcd53ecea79884d9ad4925943e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-507156ea9b4c0462ebe753d15034235b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104538&auth_key=1760104538-0-0-b8814d324746e189fce4e704b86747e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63aea4e362ae86f0c8b5c569feb0438e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104545&auth_key=1760104545-0-0-cad678c000dd6d1ceb0e44876926a896&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-931a22f68f5e006e3bb2ea344d6cf4f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104552&auth_key=1760104552-0-0-e76bf03829780060c1966b5ccb7bde03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs"><a href="#FuseCodec-Semantic-Contextual-Fusion-and-Supervision-for-Neural-Codecs" class="headerlink" title="FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs"></a>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</h2><p><strong>Authors:Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman</strong></p>
<p>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodologyâ€™s applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a>. </p>
<blockquote>
<p>è¯­éŸ³åˆ†è¯èƒ½å¤Ÿå®ç°å¯¹è¯­éŸ³çš„ç¦»æ•£è¡¨ç¤ºï¼Œå¹¶ä¿ƒè¿›è¯­éŸ³è¯­è¨€å»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¥ç»ç¼–ç å™¨ä¸»è¦æ•æ‰ä½çº§åˆ«çš„å£°å­¦ç‰¹å¾ï¼Œå¿½ç•¥äº†äººç±»è¯­éŸ³æ‰€å›ºæœ‰çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›å¼•å…¥äº†æ¥è‡ªè‡ªç›‘ç£è¯­éŸ³æ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºæˆ–ç»“åˆäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä½†åœ¨å¯¹é½å’Œç»Ÿä¸€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†FuseCodecï¼Œå®ƒé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ä¿¡æ¯ç›‘ç£ï¼Œç»Ÿä¸€äº†å£°å­¦ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥çš„æŠ€æœ¯ï¼šï¼ˆiï¼‰æ½œåœ¨è¡¨ç¤ºèåˆï¼Œç›´æ¥å°†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ç‰¹å¾é›†æˆåˆ°ç¼–ç å™¨æ½œåœ¨ç©ºé—´ï¼Œä»¥å®ç°ç¨³å¥å’Œç»Ÿä¸€çš„è¡¨ç¤ºå­¦ä¹ ï¼›ï¼ˆiiï¼‰å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£ï¼Œç”¨å…¨å±€æ± åŒ–å’Œå¹¿æ’­çš„è¡¨ç¤ºæ¥ç›‘ç£ç¦»æ•£ä»¤ç‰Œï¼Œä»¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§å’Œè·¨æ¨¡æ€å¯¹é½ï¼›ï¼ˆiiiï¼‰ä¸´æ—¶å¯¹é½ä¸Šä¸‹æ–‡ç›‘ç£ï¼Œé€šè¿‡åœ¨å±€éƒ¨çª—å£ä¸­åŠ¨æ€åŒ¹é…ä¸Šä¸‹æ–‡å’Œè¯­éŸ³ä»¤ç‰Œæ¥åŠ å¼ºå¯¹é½ï¼Œä»¥å®ç°ç²¾ç»†çš„ä»¤ç‰Œçº§ç›‘ç£ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†FuseCodec-TTSï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºé›¶æ ·æœ¬è¯­éŸ³åˆæˆã€‚ç»éªŒä¸Šï¼ŒFuseCodecåœ¨LibriSpeechä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è½¬å½•å‡†ç¡®æ€§ã€æ„ŸçŸ¥è´¨é‡ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢è¶…è¶Šäº†EnCodecã€SpeechTokenizerå’ŒDACã€‚ç»“æœçªå‡ºäº†ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å¼•å¯¼çš„åˆ†è¯æ³•åœ¨è¯­éŸ³åˆ†è¯å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mubtasimahasan/FuseCodec">https://github.com/mubtasimahasan/FuseCodec</a>ä¸­è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11425v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Speech tokenizationçš„é‡è¦æ€§åŠå…¶åœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰ç¥ç»ç¼–ç æ–¹æ³•ä¸»è¦å…³æ³¨ä½çº§åˆ«å£°å­¦ç‰¹å¾ï¼Œå¿½ç•¥äº†è¯­éŸ³çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚æ–‡ç« æå‡ºäº†FuseCodecï¼Œé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ç›‘ç£ï¼Œèåˆäº†å£°å­¦ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚é‡‡ç”¨ä¸‰ç§æŠ€æœ¯ï¼šæ½œä»£è¡¨èåˆã€å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£ã€æ—¶é—´å¯¹é½çš„ä¸Šä¸‹æ–‡ç›‘ç£ã€‚FuseCodec-TTSçš„å¼•å…¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆä¸­çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFuseCodecåœ¨LibriSpeechä¸Šçš„è¡¨ç°è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œè¶…è¶Šäº†EnCodecã€SpeechTokenizerå’ŒDACï¼Œåœ¨è½¬å½•å‡†ç¡®æ€§ã€æ„ŸçŸ¥è´¨é‡ã€æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech tokenizationå¯¹äºç¦»æ•£è¡¨ç¤ºå’Œè¯­éŸ³è¯­è¨€å»ºæ¨¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç¥ç»ç¼–ç æ–¹æ³•ä¸»è¦å…³æ³¨å£°å­¦ç‰¹å¾è€Œå¿½ç•¥äº†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li>
<li>FuseCodecèåˆäº†å£°å­¦ã€è¯­ä¹‰å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œé€šè¿‡å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½å’Œå…¨å±€ç›‘ç£å®ç°ã€‚</li>
<li>FuseCodecé‡‡ç”¨æ½œä»£è¡¨èåˆã€å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ç›‘ç£å’Œæ—¶é—´å¯¹é½çš„ä¸Šä¸‹æ–‡ç›‘ç£ä¸‰ç§æŠ€æœ¯ã€‚</li>
<li>FuseCodec-TTSè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>FuseCodecåœ¨LibriSpeechä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¦‚EnCodecã€SpeechTokenizerå’ŒDACã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0720c936330af45dbfa84de9dcfcb7be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104559&auth_key=1760104559-0-0-3891bd52a6f35a2ac657209ec3c702f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6999087191f6f9a62755b349dfb59dbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104566&auth_key=1760104566-0-0-e0976dbb2064e3f23654ed3e1f5888a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d74e5a95829cb9039559b584f76a172f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104573&auth_key=1760104573-0-0-b9d4a82d915f6b810b516d2da0969145&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a16ba197cab782ddf753fb5c6e17f38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104580&auth_key=1760104580-0-0-54ca3240144387bf2836da904350b075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling"><a href="#Streaming-Sequence-to-Sequence-Learning-with-Delayed-Streams-Modeling" class="headerlink" title="Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"></a>Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling</h2><p><strong>Authors:Neil Zeghidour, Eugene Kharitonov, Manu Orsini, VÃ¡clav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick PÃ©rez, Laurent MazarÃ©, Alexandre DÃ©fossez</strong></p>
<p>We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæµå¼ã€å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„çµæ´»å…¬å¼ã€‚åºåˆ—åˆ°åºåˆ—ç”Ÿæˆé€šå¸¸ä»¥ä¸€ç§ç¦»çº¿çš„æ–¹å¼è¿›è¡Œï¼Œæ¨¡å‹åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡ºæ—¶é—´æ­¥ä¹‹å‰ä¼šæ¶ˆè€—å®Œæ•´çš„è¾“å…¥åºåˆ—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæµå¼åºåˆ—åˆ°åºåˆ—åˆ™ä¾èµ–äºå­¦ä¹ ç­–ç•¥ï¼Œä»¥å†³å®šä½•æ—¶æ¨è¿›è¾“å…¥æµï¼Œæˆ–å†™å…¥è¾“å‡ºæµã€‚ç„¶è€Œï¼ŒDSMä½¿ç”¨ä»…è§£ç çš„è¯­è¨€æ¨¡å‹å¯¹å·²ç»æ—¶é—´å¯¹é½çš„æµè¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å°†å¯¹é½ç§»åŠ¨åˆ°é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶åœ¨æµä¹‹é—´å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼ŒDSMæä¾›äº†ä»»æ„è¾“å‡ºæµçš„æµå¼æ¨ç†ï¼Œé€‚ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œå¯ä»ä»»ä½•è¾“å…¥ç»„åˆç”Ÿæˆã€‚ç‰¹åˆ«æ˜¯ç»™å®šæ–‡æœ¬å’ŒéŸ³é¢‘æµæ—¶ï¼Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹åº”äºå»¶è¿Ÿçš„æ–‡æœ¬æµï¼Œè€Œç›¸ååˆ™ä¸ºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸¤ä¸ªä¸»è¦çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒDSMåœ¨æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå»¶è¿Ÿçš„åŒæ—¶ï¼Œè¿˜æ”¯æŒä»»æ„é•¿çš„åºåˆ—ï¼Œç”šè‡³ä¸ç¦»çº¿åŸºå‡†ç›¸ç«äº‰ã€‚ç›¸å…³ä»£ç ã€æ ·æœ¬å’Œæ¼”ç¤ºå¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/delayed-streams-modeling">https://github.com/kyutai-labs/delayed-streams-modeling</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08753v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæµå¼ã€å¤šæ¨¡æ€åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„çµæ´»æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿åºåˆ—åˆ°åºåˆ—ç”Ÿæˆä¸åŒï¼ŒDSMå¯¹å·²ç»æ—¶é—´å¯¹é½çš„æµè¿›è¡Œå»ºæ¨¡ï¼Œé€šè¿‡é¢„å¤„ç†æ­¥éª¤å®Œæˆå¯¹é½ï¼Œå¹¶åœ¨æµä¹‹é—´å¼•å…¥é€‚å½“çš„å»¶è¿Ÿï¼Œä»¥å®ç°ä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºè®¸å¤šåºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDSMå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½å’Œä½å»¶è¿Ÿï¼Œæ”¯æŒä»»æ„é•¿åºåˆ—ï¼Œä¸ç¦»çº¿åŸºå‡†æµ‹è¯•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»¶è¿Ÿæµå»ºæ¨¡ï¼ˆDSMï¼‰æ˜¯ä¸€ç§çµæ´»çš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ æ–¹æ³•ï¼Œé€‚ç”¨äºæµå¼å’Œå¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>DSMé€šè¿‡é¢„å¤„ç†æ­¥éª¤å®Œæˆæµä¹‹é—´çš„æ—¶é—´å¯¹é½ï¼Œå¹¶å¼•å…¥å»¶è¿Ÿã€‚</li>
<li>DSMæ”¯æŒä»»æ„è¾“å‡ºåºåˆ—çš„æµå¼æ¨æ–­ï¼Œé€‚ç”¨äºå¤šç§åºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚</li>
<li>DSMåœ¨è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ç­‰ä»»åŠ¡ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>DSMå…·æœ‰å…ˆè¿›çš„æ€§èƒ½å’Œä½å»¶è¿Ÿï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åºåˆ—ã€‚</li>
<li>DSMä¸ç¦»çº¿åŸºå‡†æµ‹è¯•å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-70ff91ccbdaebebb45f0bd79f253971e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104587&auth_key=1760104587-0-0-ae1f4d33e229eedc380c6b19e2ee0e03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5b42e42e834063e3174a20fa29ab19f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104594&auth_key=1760104594-0-0-4002cf8cc2c152ef4d0088fccebe557e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d631c769a50d4ff489a42e5de7e69d29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104601&auth_key=1760104601-0-0-280f09d897f34ba02787d02a5153be48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Leveraging-Mamba-with-Full-Face-Vision-for-Audio-Visual-Speech-Enhancement"><a href="#Leveraging-Mamba-with-Full-Face-Vision-for-Audio-Visual-Speech-Enhancement" class="headerlink" title="Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement"></a>Leveraging Mamba with Full-Face Vision for Audio-Visual Speech   Enhancement</h2><p><strong>Authors:Rong Chao, Wenze Ren, You-Jin Li, Kuo-Hsuan Hung, Sung-Feng Huang, Szu-Wei Fu, Wen-Huang Cheng, Yu Tsao</strong></p>
<p>Recent Mamba-based models have shown promise in speech enhancement by efficiently modeling long-range temporal dependencies. However, models like Speech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios and struggle in complex multi-speaker environments such as the cocktail party problem. To overcome this, we introduce AVSEMamba, an audio-visual speech enhancement model that integrates full-face visual cues with a Mamba-based temporal backbone. By leveraging spatiotemporal visual information, AVSEMamba enables more accurate extraction of target speech in challenging conditions. Evaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba outperforms other monaural baselines in speech intelligibility (STOI), perceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves \textbf{1st place} on the monaural leaderboard. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºMambaçš„æ¨¡å‹åœ¨é€šè¿‡æœ‰æ•ˆå»ºæ¨¡è¿œç¨‹æ—¶é—´ä¾èµ–æ€§è¿›è¡Œè¯­éŸ³å¢å¼ºæ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œåƒè¯­éŸ³å¢å¼ºMambaï¼ˆSEMambaï¼‰è¿™æ ·çš„æ¨¡å‹ä»…é™äºå•äººåœºæ™¯ï¼Œåœ¨å¤æ‚çš„å¤šäººç¯å¢ƒï¼ˆå¦‚é¸¡å°¾é…’ä¼šé—®é¢˜ï¼‰ä¸­è¡¨ç°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AVSEMambaï¼Œè¿™æ˜¯ä¸€ä¸ªè§†å¬è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œå®ƒç»“åˆäº†åŸºäºMambaçš„æ—¶é—´éª¨æ¶å’Œå…¨è„¸è§†è§‰çº¿ç´¢ã€‚é€šè¿‡åˆ©ç”¨æ—¶ç©ºè§†è§‰ä¿¡æ¯ï¼ŒAVSEMambaèƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹æ›´å‡†ç¡®åœ°æå–ç›®æ ‡è¯­éŸ³ã€‚åœ¨AVSEC-4æŒ‘æˆ˜çš„å¼€å‘é›†å’Œç›²æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒAVSEMambaåœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆSTOIï¼‰ã€æ„ŸçŸ¥è´¨é‡ï¼ˆPESQï¼‰å’Œéä¾µå…¥è´¨é‡ï¼ˆUTMOSï¼‰æ–¹é¢éƒ½ä¼˜äºå…¶ä»–å•è€³åŸºçº¿æµ‹è¯•ï¼Œå¹¶åœ¨å•è€³æµ‹è¯•æ’è¡Œæ¦œä¸Šå–å¾—äº†ç¬¬ä¸€åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13624v2">PDF</a> Accepted to Interspeech 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>Mambaæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸå±•ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œèƒ½å¤Ÿé«˜æ•ˆå»ºæ¨¡é•¿æ—¶åºä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œé¢å¯¹å¤æ‚çš„å¤šè¯´è¯äººç¯å¢ƒå¦‚é¸¡å°¾é…’ä¼šé—®é¢˜ï¼Œç°æœ‰æ¨¡å‹å¦‚SEMambaä»…é™äºå•è¯´è¯äººåœºæ™¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AVSEMambaéŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œå®ƒèåˆäº†é¢éƒ¨è§†è§‰çº¿ç´¢ä¸Mambaçš„æ—¶åºä¸»å¹²ç½‘ç»œã€‚å€ŸåŠ©æ—¶ç©ºè§†è§‰ä¿¡æ¯ï¼ŒAVSEMambaèƒ½åœ¨å¤æ‚æ¡ä»¶ä¸‹æ›´å‡†ç¡®åœ°æå–ç›®æ ‡è¯­éŸ³ã€‚åœ¨AVSEC-4æŒ‘æˆ˜çš„å¼€å‘å’Œç›²æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ŒAVSEMambaåœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆSTOIï¼‰ã€æ„ŸçŸ¥è´¨é‡ï¼ˆPESQï¼‰å’Œéä¾µå…¥è´¨é‡ï¼ˆUTMOSï¼‰æ–¹é¢è¶…è¶Šå…¶ä»–å•å£°é“åŸºçº¿ï¼Œå¹¶åœ¨å•å£°é“æ’è¡Œæ¦œä¸Šè·å¾—ç¬¬ä¸€åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mambaæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œæ“…é•¿å»ºæ¨¡é•¿æ—¶åºä¾èµ–å…³ç³»ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¦‚SEMambaåœ¨å¤æ‚å¤šè¯´è¯äººç¯å¢ƒä¸‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>AVSEMambaæ˜¯ä¸€ä¸ªéŸ³é¢‘è§†è§‰è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œèåˆäº†é¢éƒ¨è§†è§‰çº¿ç´¢å’ŒMambaçš„æ—¶åºä¸»å¹²ç½‘ç»œã€‚</li>
<li>AVSEMambaå€ŸåŠ©æ—¶ç©ºè§†è§‰ä¿¡æ¯ï¼Œèƒ½æ›´å‡†ç¡®åœ°æå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>åœ¨AVSEC-4æŒ‘æˆ˜çš„å¼€å‘å’Œç›²æµ‹è¯•é›†ä¸Šï¼ŒAVSEMambaæ€§èƒ½è¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
<li>AVSEMambaåœ¨è¯­éŸ³æ¸…æ™°åº¦ã€æ„ŸçŸ¥è´¨é‡å’Œéä¾µå…¥è´¨é‡æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ad0c75cc1bde08dd31dc65ba82f71399~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104608&auth_key=1760104608-0-0-3111f0d55353ab9b9d1076b479c4c701&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-263dd6e78dcf69c91ef7fe7b0a59e22f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104615&auth_key=1760104615-0-0-31eb7242d0e44dd317c9bfd7a1ed0d6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56c73a80002f947d74c23a30246feec4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104622&auth_key=1760104622-0-0-5b72d28b1b94be2dc38140de58a88cf8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="IML-Spikeformer-Input-aware-Multi-Level-Spiking-Transformer-for-Speech-Processing"><a href="#IML-Spikeformer-Input-aware-Multi-Level-Spiking-Transformer-for-Speech-Processing" class="headerlink" title="IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech   Processing"></a>IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech   Processing</h2><p><strong>Authors:Zeyang Song, Shimin Zhang, Yuhong Chou, Jibin Wu, Haizhou Li</strong></p>
<p>Spiking Neural Networks (SNNs), inspired by biological neural mechanisms, represent a promising neuromorphic computing paradigm that offers energy-efficient alternatives to traditional Artificial Neural Networks (ANNs). Despite proven effectiveness, SNN architectures have struggled to achieve competitive performance on large-scale speech processing tasks. Two key challenges hinder progress: (1) the high computational overhead during training caused by multi-timestep spike firing, and (2) the absence of large-scale SNN architectures tailored to speech processing tasks. To overcome the issues, we introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking Transformer architecture specifically designed for large-scale speech processing. Central to our design is the Input-aware Multi-Level Spike (IMLS) mechanism, which simulates multi-timestep spike firing within a single timestep using an adaptive, input-aware thresholding scheme. IML-Spikeformer further integrates a Re-parameterized Spiking Self-Attention (RepSSA) module with a Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module enhances the precision of attention maps and enables modeling of multi-scale temporal dependencies in speech signals. Experiments demonstrate that IML-Spikeformer achieves word error rates of 6.0% on AiShell-1 and 3.4% on Librispeech-960, comparable to conventional ANN transformers while reducing theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$ respectively. IML-Spikeformer marks an advance of scalable SNN architectures for large-scale speech processing in both task performance and energy efficiency. Our source code and model checkpoints are publicly available at github.com&#x2F;Pooookeman&#x2F;IML-Spikeformer. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networksï¼ŒSNNsï¼‰ï¼Œå—ç”Ÿç‰©ç¥ç»æœºåˆ¶çš„å¯å‘ï¼Œä»£è¡¨äº†ä¸€ç§æœ‰å‰æ™¯çš„ç±»è„‘è®¡ç®—èŒƒå¼ï¼Œä¸ºä¼ ç»Ÿçš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰æä¾›äº†èŠ‚èƒ½çš„æ›¿ä»£æ–¹æ¡ˆã€‚å°½ç®¡å·²ç»è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œä½†SNNæ¶æ„åœ¨å¤§è§„æ¨¡è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ä»å­˜åœ¨å›°éš¾ã€‚ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜é˜»ç¢äº†è¿›å±•ï¼šï¼ˆ1ï¼‰ç”±äºå¤šæ—¶é—´æ­¥é•¿çš„è„‰å†²å‘å°„å¯¼è‡´çš„è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜è®¡ç®—å¼€é”€ï¼›ï¼ˆ2ï¼‰ç¼ºä¹é’ˆå¯¹è¯­éŸ³è¯†åˆ«ä»»åŠ¡å®šåˆ¶çš„å¤§è§„æ¨¡SNNæ¶æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾“å…¥æ„ŸçŸ¥å¤šçº§Spikeformerï¼Œå³IML-Spikeformerï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤§è§„æ¨¡è¯­éŸ³å¤„ç†çš„è„‰å†²Transformeræ¶æ„ã€‚è®¾è®¡çš„æ ¸å¿ƒåœ¨äºè¾“å…¥æ„ŸçŸ¥å¤šçº§è„‰å†²ï¼ˆIMLSï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä½¿ç”¨è‡ªé€‚åº”çš„ã€è¾“å…¥æ„ŸçŸ¥çš„é˜ˆå€¼æ–¹æ¡ˆï¼Œåœ¨å•ä¸ªæ—¶é—´æ­¥é•¿å†…æ¨¡æ‹Ÿå¤šæ—¶é—´æ­¥é•¿çš„è„‰å†²å‘å°„ã€‚IML-Spikeformerè¿›ä¸€æ­¥ç»“åˆäº†å‚æ•°åŒ–è„‰å†²è‡ªæ³¨æ„åŠ›ï¼ˆRepSSAï¼‰æ¨¡å—å’Œåˆ†å±‚è¡°å‡æ©ç ï¼ˆHDMï¼‰ï¼Œå½¢æˆäº†HD-RepSSAæ¨¡å—ã€‚è¯¥æ¨¡å—æé«˜äº†æ³¨æ„åŠ›å›¾çš„ç²¾åº¦ï¼Œå¹¶å®ç°äº†è¯­éŸ³ä¿¡å·ä¸­å¤šå°ºåº¦æ—¶é—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒIML-Spikeformeråœ¨AiShell-1ä¸Šçš„è¯é”™è¯¯ç‡ä¸º6.0%ï¼Œåœ¨Librispeech-960ä¸Šçš„è¯é”™è¯¯ç‡ä¸º3.4%ï¼Œä¸ä¼ ç»ŸANNå˜å‹å™¨ç›¸å½“ï¼ŒåŒæ—¶åˆ†åˆ«é™ä½äº†4.64Ã—å’Œ4.32Ã—çš„ç†è®ºæ¨ç†èƒ½è€—ã€‚IML-Spikeformeræ ‡å¿—ç€å¤§è§„æ¨¡è¯­éŸ³å¤„ç†çš„å¯æ‰©å±•SNNæ¶æ„åœ¨ä»»åŠ¡æ€§èƒ½å’Œèƒ½æºæ•ˆç‡æ–¹é¢çš„è¿›æ­¥ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨github.com&#x2F;Pooookeman&#x2F;IML-Spikeformerä¸Šå…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07396v2">PDF</a> Accepted by TNNLS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å—ç”Ÿç‰©ç¥ç»æœºåˆ¶å¯å‘çš„è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰æ˜¯ä¸€ç§å‰æ™¯çœ‹å¥½çš„ç¥ç»å½¢æ€è®¡ç®—èŒƒä¾‹ï¼Œä¸ºä¼ ç»Ÿçš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰æä¾›äº†èƒ½æºæ•ˆç‡æ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒSNNæ¶æ„åœ¨å¤§è§„æ¨¡è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ç«äº‰åŠ›æœ‰é™ã€‚æœ¬æ–‡å¼•å…¥Input-aware Multi-Level Spikeformerï¼ˆIML-Spikeformerï¼‰ï¼Œä¸€ç§ä¸“é—¨ç”¨äºå¤§è§„æ¨¡è¯­éŸ³å¤„ç†çš„è„‰å†²Transformeræ¶æ„ã€‚å…¶æ ¸å¿ƒè®¾è®¡çš„Input-aware Multi-Level Spikeï¼ˆIMLSï¼‰æœºåˆ¶èƒ½å¤Ÿåœ¨å•ä¸ªæ—¶é—´æ­¥å†…æ¨¡æ‹Ÿå¤šæ—¶é—´æ­¥çš„è„‰å†²å‘å°„ã€‚æ­¤å¤–ï¼ŒIML-Spikeformeré›†æˆäº†å¸¦æœ‰å±‚æ¬¡è¡°å‡æ©è†œï¼ˆHDMï¼‰çš„Re-parameterized Spiking Self-Attentionï¼ˆRepSSAï¼‰æ¨¡å—ï¼Œå½¢æˆHD-RepSSAæ¨¡å—ï¼Œæé«˜äº†æ³¨æ„åŠ›å›¾çš„ç²¾åº¦ï¼Œå¹¶å®ç°äº†è¯­éŸ³ä¿¡å·å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§çš„å»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒIML-Spikeformeråœ¨AiShell-1ä¸Šçš„è¯é”™è¯¯ç‡ä¸º6.0%ï¼Œåœ¨Librispeech-960ä¸Šä¸º3.4%ï¼Œä¸ä¼ ç»ŸANNå˜å‹å™¨ç›¸å½“ï¼ŒåŒæ—¶ç†è®ºä¸Šé™ä½äº†4.64å€å’Œ4.32å€çš„æ¨ç†èƒ½è€—ã€‚IML-Spikeformeræ ‡å¿—ç€å¤§è§„æ¨¡è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å¯æ‰©å±•SNNæ¶æ„åœ¨ä»»åŠ¡æ€§èƒ½å’Œèƒ½æºæ•ˆç‡æ–¹é¢çš„è¿›æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰æ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»å¯å‘çš„è®¡ç®—èŒƒä¾‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰æ›´èŠ‚èƒ½ã€‚</li>
<li>SNNé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¤šæ—¶é—´æ­¥è„‰å†²å‘å°„å¯¼è‡´çš„é«˜è®¡ç®—å¼€é”€ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹è¯­éŸ³å¤„ç†ä»»åŠ¡çš„å¤§è§„æ¨¡SNNæ¶æ„ã€‚</li>
<li>IML-Spikeformerï¼Œä¸€ç§ä¸“ä¸ºå¤§è§„æ¨¡è¯­éŸ³å¤„ç†è®¾è®¡çš„è„‰å†²Transformeræ¶æ„è¢«æå‡ºã€‚</li>
<li>IML-Spikeformerçš„Input-aware Multi-Level Spikeï¼ˆIMLSï¼‰æœºåˆ¶èƒ½åœ¨å•ä¸ªæ—¶é—´æ­¥å†…æ¨¡æ‹Ÿå¤šæ—¶é—´æ­¥çš„è„‰å†²å‘å°„ã€‚</li>
<li>HD-RepSSAæ¨¡å—é€šè¿‡å¢å¼ºæ³¨æ„åŠ›å›¾çš„ç²¾åº¦å’Œå®ç°å¤šå°ºåº¦æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡ï¼Œæé«˜äº†è¯­éŸ³å¤„ç†çš„æ€§èƒ½ã€‚</li>
<li>IML-Spikeformeråœ¨å¤§å‹è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯é”™è¯¯ç‡ä¸å¸¸è§„ANNå˜å‹å™¨ç›¸å½“ã€‚</li>
<li>IML-Spikeformerç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å¤§å¹…é™ä½äº†æ¨ç†èƒ½è€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-badef6d76997d9c6194383b259ec2569~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104629&auth_key=1760104629-0-0-d2f2e03be4deec7542d5ac727ef4abfa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45d664199fcc4f11c6363dda9e009aa5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104637&auth_key=1760104637-0-0-e0f84eee661cebfdca1df06c9806e7d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ab4a57fb10e64ffa14b198530f2f0df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104643&auth_key=1760104643-0-0-5c8821e47d8565958273ddea6e55f2b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection"><a href="#EnvSDD-Benchmarking-Environmental-Sound-Deepfake-Detection" class="headerlink" title="EnvSDD: Benchmarking Environmental Sound Deepfake Detection"></a>EnvSDD: Benchmarking Environmental Sound Deepfake Detection</h2><p><strong>Authors:Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Haohe Liu, Wenwu Wang, Mark D Plumbley</strong></p>
<p>Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains. </p>
<blockquote>
<p>éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿå¦‚ä»Šèƒ½å¤Ÿåˆ›é€ å‡ºéå¸¸é€¼çœŸçš„å£°éŸ³åœºæ™¯ï¼Œè¿™æ—¢å¯ä»¥å¢å¼ºåª’ä½“åˆ¶ä½œï¼Œä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚å‡ é¡¹ç ”ç©¶å·²ç»ç ”ç©¶äº†è¯­éŸ³æˆ–æ­Œå”±å£°éŸ³çš„æ·±åº¦ä¼ªé€ ã€‚ç„¶è€Œï¼Œç¯å¢ƒå£°éŸ³å…·æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œè¿™å¯èƒ½ä¼šä½¿æ£€æµ‹è¯­éŸ³å’Œæ­Œå”±æ·±åº¦ä¼ªé€ çš„æ–¹æ³•å¯¹çœŸå®ä¸–ç•Œçš„å£°éŸ³æ•ˆæœè¾ƒå·®ã€‚æ­¤å¤–ï¼Œç”¨äºç¯å¢ƒå£°éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’ŒéŸ³é¢‘ç±»å‹æ–¹é¢æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†EnvSDDï¼Œè¿™æ˜¯ä¸ºæ­¤ä»»åŠ¡è®¾è®¡çš„å¤§è§„æ¨¡å®šåˆ¶æ•°æ®é›†ï¼ŒåŒ…å«45.25å°æ—¶çš„çœŸå®éŸ³é¢‘å’Œ316.74å°æ—¶çš„ä¼ªé€ éŸ³é¢‘ã€‚æµ‹è¯•é›†åŒ…æ‹¬å„ç§æ¡ä»¶ï¼Œä»¥è¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹å’Œæœªè§è¿‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚åœ¨EnvSDDä¸Šçš„ç»“æœè¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿä¼˜äºè¯­éŸ³å’Œæ­Œå”±é¢†åŸŸçš„æœ€å…ˆè¿›çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19203v2">PDF</a> Proceedings of Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿæ‰€å¸¦æ¥ç°å®éŸ³æ•ˆå¢å¼ºçš„åŒæ—¶å­˜åœ¨çš„æ½œåœ¨é£é™©ã€‚é’ˆå¯¹ç¯å¢ƒå£°éŸ³çš„ç‰¹æ€§ï¼Œç°æœ‰çš„è¯­éŸ³å’Œæ­Œå”±å£°éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹æ‰‹æ®µå¯èƒ½å¯¹å…¶å¹¶ä¸å®Œå…¨é€‚ç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ–‡ç« æ¨å‡ºäº†EnvSDDæ•°æ®é›†ï¼Œç”¨äºä¸“é—¨çš„ç¯å¢ƒå£°éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ï¼Œå…¶ä¸­åŒ…æ‹¬çœŸå®éŸ³é¢‘ä¸åˆæˆéŸ³é¢‘ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨EnvSDDä¸Šçš„è¡¨ç°ä¼˜äºè¯­éŸ³å’Œæ­Œå”±é¢†åŸŸçš„æœ€æ–°ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ç”Ÿæˆç³»ç»Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„å£°éŸ³åœºæ™¯ï¼Œå¢å¼ºåª’ä½“åˆ¶ä½œæ•ˆæœï¼Œä½†ä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚</li>
<li>ç¯å¢ƒå£°éŸ³çš„ç‰¹æ€§ä½¿å¾—ç°æœ‰çš„è¯­éŸ³å’Œæ­Œå”±æ·±åº¦ä¼ªé€ æ£€æµ‹æ‰‹æ®µå¯èƒ½ä¸å¤Ÿæœ‰æ•ˆã€‚</li>
<li>æ¨å‡ºEnvSDDæ•°æ®é›†ï¼Œç”¨äºä¸“é—¨çš„ç¯å¢ƒå£°éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>EnvSDDæ•°æ®é›†åŒ…æ‹¬çœŸå®éŸ³é¢‘ä¸åˆæˆéŸ³é¢‘ï¼Œæä¾›äº†å¤§é‡çš„æ ·æœ¬ä»¥ä¾›å­¦ä¹ å’Œæ£€æµ‹ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æµ‹è¯•é›†åŒ…å«äº†å„ç§æ¡ä»¶ä»¥è¯„ä¼°æ£€æµ‹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f599d745b17a5a82f4aff1da2437dea0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104651&auth_key=1760104651-0-0-2c29a5b63f5407e2a2a7a7a65490c4a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-175f2ceebf353149462cd6398c7ebad7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104658&auth_key=1760104658-0-0-91eee68f87826c649a9235768dc51a81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79f46b021bebe01d090f7948a18aea0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104664&auth_key=1760104664-0-0-1ee1ea9c9873ecd137c83140fd7eb076&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6cd9d5898ab52261cb44eb6eccf311a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104671&auth_key=1760104671-0-0-06327c8478d2329efecdb421cc857e41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66ee140547c030fe34bf3065db693d9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104678&auth_key=1760104678-0-0-20c76e3452e59eb1e041b83f1659ef02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes"><a href="#MemeIntel-Explainable-Detection-of-Propagandistic-and-Hateful-Memes" class="headerlink" title="MemeIntel: Explainable Detection of Propagandistic and Hateful Memes"></a>MemeIntel: Explainable Detection of Propagandistic and Hateful Memes</h2><p><strong>Authors:Mohamed Bayan Kmainasi, Abul Hasnat, Md Arid Hasan, Ali Ezzat Shahroor, Firoj Alam</strong></p>
<p>The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to jointly modeling label detection and the generation of explanation-based rationales, which often leads to degraded classification performance when trained simultaneously. To address this challenge, we introduce MemeXplain, an explanation-enhanced dataset for propagandistic memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results show that this strategy significantly improves both label detection and explanation generation quality over the base model, outperforming the current state-of-the-art with an absolute improvement of ~1.4% (Acc) on ArMeme and ~2.2% (Acc) on Hateful Memes. For reproducibility and future research, we aim to make the MemeXplain dataset and scripts publicly available (<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel">https://github.com/MohamedBayan/MemeIntel</a>). </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡å¼å†…å®¹çš„æ¿€å¢ï¼Œä¸ºç†è§£å’Œè°ƒèŠ‚å¤æ‚ä¸”ä¾èµ–äºæƒ…å¢ƒçš„è®®é¢˜ï¼ˆå¦‚å‡ä¿¡æ¯ã€ä»‡æ¨è¨€è®ºå’Œå®£ä¼ ï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶è‡´åŠ›äºå¼€å‘èµ„æºå’Œæå‡ºè‡ªåŠ¨æ£€æµ‹æ–°æ–¹æ³•ï¼Œä½†å¯¹è”åˆå»ºæ¨¡æ ‡ç­¾æ£€æµ‹å’ŒåŸºäºè§£é‡Šçš„ç†æ€§ç”Ÿæˆç»™äºˆçš„å…³æ³¨æœ‰é™ï¼Œè¿™å¾€å¾€å¯¼è‡´åœ¨åŒæ—¶è®­ç»ƒæ—¶åˆ†ç±»æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MemeXplainï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé˜¿æ‹‰ä¼¯è¯­çš„å®£ä¼ æ€§memeå’Œè‹±è¯­çš„ä»‡æ¨è¨€è®ºmemeçš„è§£é‡Šå¢å¼ºæ•°æ®é›†ï¼Œæˆä¸ºè¿™äº›ä»»åŠ¡çš„é¦–ä¸ªå¤§è§„æ¨¡èµ„æºã€‚ä¸ºäº†è§£å†³è¿™äº›ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å¹¶è®­ç»ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨æ ‡ç­¾æ£€æµ‹å™¨å’Œè§£é‡Šç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—æ”¹è¿›äº†åŸºç¡€æ¨¡å‹ï¼Œåœ¨ArMemeä¸Šç»å¯¹æé«˜äº†çº¦1.4%ï¼ˆå‡†ç¡®ç‡ï¼‰ï¼Œåœ¨ä»‡æ¨è¨€è®ºMemesä¸Šæé«˜äº†çº¦2.2%ï¼ˆå‡†ç¡®ç‡ï¼‰ã€‚ä¸ºäº†å¯å¤åˆ¶æ€§å’Œæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ—¨åœ¨ä½¿MemeXplainæ•°æ®é›†å’Œè„šæœ¬å…¬å¼€å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel%EF%BC%89%E3%80%82">https://github.com/MohamedBayan/MemeIntelï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16612v2">PDF</a> disinformation, misinformation, factuality, harmfulness, fake news,   propaganda, hateful meme, multimodality, text, images</p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡æ€å†…å®¹çš„æ¿€å¢ï¼Œä¸ºç†è§£å’Œè°ƒèŠ‚å¤æ‚ã€ä¾èµ–è¯­å¢ƒçš„é—®é¢˜ï¼ˆå¦‚é”™è¯¯ä¿¡æ¯ã€ä»‡æ¨è¨€è®ºå’Œå®£ä¼ ï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å·²æœ‰ä¸€äº›è‡ªåŠ¨æ£€æµ‹èµ„æºçš„å¼€å‘å’Œæ–°æ–¹æ³•çš„æå‡ºï¼Œä½†åœ¨è”åˆå»ºæ¨¡æ ‡ç­¾æ£€æµ‹å’ŒåŸºäºè§£é‡Šçš„ç†æ€§ç”Ÿæˆæ–¹é¢å…³æ³¨è¾ƒå°‘ï¼Œè¿™å¾€å¾€å¯¼è‡´åŒæ—¶è®­ç»ƒæ—¶åˆ†ç±»æ€§èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MemeXplainæ•°æ®é›†ï¼ŒåŒ…å«é˜¿æ‹‰ä¼¯è¯­çš„å®£ä¼ æ€§memeå’Œè‹±è¯­çš„ä»‡æ¨æ€§memeï¼Œæˆä¸ºé¦–ä¸ªå¤§è§„æ¨¡èµ„æºç”¨äºè¿™äº›ä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™äº›ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å¹¶è®­ç»ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ­¤ç­–ç•¥åœ¨æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨ArMemeä¸Šå‡†ç¡®ç‡æé«˜äº†çº¦1.4%ï¼Œåœ¨ä»‡æ¨æ€§Memesä¸Šå‡†ç¡®ç‡æé«˜äº†çº¦2.2%ã€‚æˆ‘ä»¬æ—¨åœ¨è®©MemeXplainæ•°æ®é›†å’Œè„šæœ¬å…¬å¼€å¯ç”¨ï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/MohamedBayan/MemeIntel%EF%BC%89%E3%80%82">https://github.com/MohamedBayan/MemeIntelï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¤šæ¨¡æ€å†…å®¹å¸¦æ¥çš„æŒ‘æˆ˜ï¼šç†è§£å’Œè°ƒèŠ‚å¤æ‚ã€ä¾èµ–è¯­å¢ƒçš„é—®é¢˜å¦‚é”™è¯¯ä¿¡æ¯ã€ä»‡æ¨è¨€è®ºå’Œå®£ä¼ ç­‰å…·æœ‰éš¾åº¦ã€‚</li>
<li>æ•°æ®é›†ç¼ºå¤±é—®é¢˜ï¼šå°½ç®¡å·²æœ‰è‡ªåŠ¨æ£€æµ‹èµ„æºçš„å¼€å‘å’Œæ–°æ–¹æ³•æå‡ºï¼Œä½†åœ¨è”åˆå»ºæ¨¡æ ‡ç­¾æ£€æµ‹å’ŒåŸºäºè§£é‡Šçš„ç†æ€§ç”Ÿæˆæ–¹é¢å…³æ³¨ä¸è¶³ã€‚</li>
<li>MemeXplainæ•°æ®é›†çš„æ¨å‡ºï¼šåŒ…å«é˜¿æ‹‰ä¼¯è¯­çš„å®£ä¼ æ€§memeå’Œè‹±è¯­çš„ä»‡æ¨æ€§memeï¼Œæˆä¸ºå¤§è§„æ¨¡èµ„æºç”¨äºç›¸å…³ä»»åŠ¡ã€‚</li>
<li>è§£å†³æ–¹æ¡ˆï¼šé‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ³•å¹¶è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>æ€§èƒ½å’Œæ•ˆæœï¼šç­–ç•¥æ˜¾è‘—æé«˜æ ‡ç­¾æ£€æµ‹å’Œè§£é‡Šç”Ÿæˆè´¨é‡ï¼Œç›¸è¾ƒäºå½“å‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ•°æ®é›†å’Œè„šæœ¬çš„å…¬å¼€å¯ç”¨æ€§ï¼šæ—¨åœ¨ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c2b04c4d8b0632d1634ed280a2bf28ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104686&auth_key=1760104686-0-0-413729ee6f24c29321d482aa4a7a41fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d569422eece23911b4afa79c5ad07260~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104692&auth_key=1760104692-0-0-bf397a4df481efeced957136ab7a9011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92a03fec9eb3fcbdd0bfaed24cbcef10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104699&auth_key=1760104699-0-0-586b5efc252acec2008411a68a2105d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d990a6dba051d3911fc6add2b99d055c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760104706&auth_key=1760104706-0-0-e253e64eb3a7670f7827fd82de154f93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ef2e35985bb4bc71f3d2163a6a83fde3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087920&auth_key=1760087920-0-0-bfb3cbf7acab7fa43fca3d84b9a9e78a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  DGM4+ Dataset Extension for Global Scene Inconsistency
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4dfbf28d610f69265c2f4832e1284378~resize:0:q75.jpg?source=1f5c5e47&expiration=1760087644&auth_key=1760087644-0-0-e75978e49e40e66a25ee7e7095e16735&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Generalized Contrastive Learning for Universal Multimodal Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
