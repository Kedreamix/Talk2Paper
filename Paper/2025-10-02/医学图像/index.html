<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Comparative study of Wavelet transform and Fourier domain filtering for   medical image denoising">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-ba173ca74c1c9eab312ff56cfbc25318~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105431&auth_key=1760105431-0-0-ebafee21bea045d3eafc4386bb01b7a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-02-æ›´æ–°"><a href="#2025-10-02-æ›´æ–°" class="headerlink" title="2025-10-02 æ›´æ–°"></a>2025-10-02 æ›´æ–°</h1><h2 id="Comparative-study-of-Wavelet-transform-and-Fourier-domain-filtering-for-medical-image-denoising"><a href="#Comparative-study-of-Wavelet-transform-and-Fourier-domain-filtering-for-medical-image-denoising" class="headerlink" title="Comparative study of Wavelet transform and Fourier domain filtering for   medical image denoising"></a>Comparative study of Wavelet transform and Fourier domain filtering for   medical image denoising</h2><p><strong>Authors:M. Ali Saif, Bassam M. Mughalles, Ibrahim G. H. Loqman</strong></p>
<p>Denoising of images is a crucial preprocessing step in medical imaging, essential for improving diagnostic clarity. While deep learning methods offer state-of-the-art performance, their computational complexity and data requirements can be prohibitive. In this study we present a comprehensive comparative analysis of two classical, computationally efficient transform-domain techniques: Discrete Wavelet Transform (DWT) and Discrete Fourier Cosine Transform (DFCT) filtering. We evaluated their efficacy in denoising medical images which corrupted by Gaussian, Uniform, Poisson, and Salt-and-Pepper noise. Contrary to the common hypothesis favoring wavelets for their multi-resolution capabilities, our results demonstrate that a block-based DFCT approach consistently and significantly outperforms a global DWT approach across all noise types and performance metrics (SNR, PSNR, IM). We attribute DFCTâ€™s superior performance to its localized processing strategy, which better preserves fine details by operating on small image blocks, effectively adapting to local statistics without introducing global artifacts. This finding underscores the importance of algorithmic selection based on processing methodology, not just transform properties, and positions DFCT as a highly effective and efficient denoising tool for practical medical imaging applications. </p>
<blockquote>
<p>å›¾åƒå»å™ªåœ¨åŒ»å­¦æˆåƒä¸­æ˜¯ä¸€ä¸ªå…³é”®çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå¯¹äºæé«˜è¯Šæ–­æ¸…æ™°åº¦è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶è®¡ç®—å¤æ‚æ€§å’Œæ•°æ®è¦æ±‚å¯èƒ½æ˜¯ç¦æ­¢æ€§çš„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸¤ç§ç»å…¸ä¸”è®¡ç®—æ•ˆç‡é«˜çš„å˜æ¢åŸŸæŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒåˆ†æï¼šç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å’Œç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDFCTï¼‰æ»¤æ³¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨å»é™¤åŒ»å­¦å›¾åƒä¸­ç”±é«˜æ–¯ã€å‡åŒ€ã€æ³Šæ¾å’Œæ¤’ç›å™ªå£°å¼•èµ·çš„å™ªå£°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸æ™®éå‡è®¾å°æ³¢å…·æœ‰å¤šåˆ†è¾¨ç‡èƒ½åŠ›ç›¸åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºå—çš„DFCTæ–¹æ³•åœ¨æ‰€æœ‰è¿™äº›å™ªå£°ç±»å‹å’Œæ€§èƒ½æŒ‡æ ‡ï¼ˆSNRã€PSNRã€IMï¼‰ä¸Šå§‹ç»ˆæ˜¾è‘—ä¼˜äºå…¨å±€DWTæ–¹æ³•ã€‚æˆ‘ä»¬å°†DFCTçš„ä¼˜å¼‚æ€§èƒ½å½’å› äºå…¶å±€éƒ¨å¤„ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡åœ¨è¾ƒå°çš„å›¾åƒå—ä¸Šè¿›è¡Œæ“ä½œæ¥æ›´å¥½åœ°ä¿ç•™ç»†èŠ‚ï¼Œæœ‰æ•ˆåœ°é€‚åº”å±€éƒ¨ç»Ÿè®¡ä¿¡æ¯è€Œä¸ä¼šå¼•å…¥å…¨å±€ä¼ªå½±ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åŸºäºå¤„ç†æ–¹æ³•çš„ç®—æ³•é€‰æ‹©çš„é‡è¦æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯è½¬æ¢å±æ€§ï¼Œå¹¶å°†DFCTå®šä½ä¸ºå®é™…åŒ»å­¦æˆåƒåº”ç”¨ä¸­é«˜æ•ˆä¸”é«˜æ•ˆçš„å»å™ªå·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26608v1">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå»å™ªæ˜¯åŒ»å­¦æˆåƒä¸­çš„å…³é”®é¢„å¤„ç†æ­¥éª¤ï¼Œèƒ½æé«˜è¯Šæ–­æ¸…æ™°åº¦ã€‚æœ¬ç ”ç©¶å¯¹ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDFCTï¼‰å’Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰ä¸¤ç§ä¼ ç»Ÿå˜æ¢åŸŸæŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒåˆ†æï¼Œç”¨äºè¯„ä¼°å®ƒä»¬åœ¨å»å™ªåŒ»å­¦å›¾åƒä¸­çš„æ•ˆæœï¼Œè¿™äº›å›¾åƒå—åˆ°é«˜æ–¯ã€å‡åŒ€ã€æ³Šæ¾å’Œæ¤’ç›å™ªå£°çš„ç ´åã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºå—çš„DFCTæ–¹æ³•åœ¨æ‰€æœ‰å™ªå£°ç±»å‹å’Œæ€§èƒ½æŒ‡æ ‡ï¼ˆä¿¡å™ªæ¯”ã€å³°å€¼ä¿¡å™ªæ¯”ã€å›¾åƒåº¦é‡ï¼‰ä¸Šå‡æ˜¾è‘—ä¼˜äºå…¨å±€DWTæ–¹æ³•ã€‚è¿™ä¸»è¦å½’å› äºDFCTçš„å±€éƒ¨å¤„ç†ç­–ç•¥ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™ç»†èŠ‚ï¼Œé€šè¿‡å¤„ç†å°å›¾åƒå—æ¥é€‚åº”å±€éƒ¨ç»Ÿè®¡ä¿¡æ¯ï¼Œè€Œä¸å¼•å…¥å…¨å±€ä¼ªå½±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå»å™ªæ˜¯æå‡è¯Šæ–­æ¸…æ™°åº¦çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDFCTï¼‰å’Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰åœ¨å»å™ªåŒ»å­¦å›¾åƒä¸­çš„åº”ç”¨ã€‚</li>
<li>DFCTæ–¹æ³•åœ¨æ‰€æœ‰å™ªå£°ç±»å‹å’Œæ€§èƒ½æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>DFCTçš„å±€éƒ¨å¤„ç†ç­–ç•¥å¯ä»¥æ›´å¥½åœ°ä¿ç•™å›¾åƒçš„ç»†èŠ‚ã€‚</li>
<li>DFCTé€šè¿‡å¤„ç†å°å›¾åƒå—æ¥é€‚åº”å±€éƒ¨ç»Ÿè®¡ä¿¡æ¯ï¼Œå‡å°‘äº†å…¨å±€ä¼ªå½±çš„äº§ç”Ÿã€‚</li>
<li>ç®—æ³•çš„é€‰æ‹©ä¸ä»…å–å†³äºå˜æ¢å±æ€§ï¼Œè¿˜å–å†³äºå¤„ç†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e4ac69717195477b9b0d7d8fe978f01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105438&auth_key=1760105438-0-0-3544f7b17a1234b22fd9c1b3193e2c57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba173ca74c1c9eab312ff56cfbc25318~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105445&auth_key=1760105445-0-0-499576c5a440b46c924cd3f44948dd7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-and-Scalable-SEM-Image-Analysis-of-Perovskite-Solar-Cell-Materials-via-a-Deep-Segmentation-Framework"><a href="#Automated-and-Scalable-SEM-Image-Analysis-of-Perovskite-Solar-Cell-Materials-via-a-Deep-Segmentation-Framework" class="headerlink" title="Automated and Scalable SEM Image Analysis of Perovskite Solar Cell   Materials via a Deep Segmentation Framework"></a>Automated and Scalable SEM Image Analysis of Perovskite Solar Cell   Materials via a Deep Segmentation Framework</h2><p><strong>Authors:Jian Guo Pan, Lin Wang, Xia Cai</strong></p>
<p>Scanning Electron Microscopy (SEM) is indispensable for characterizing the microstructure of thin films during perovskite solar cell fabrication. Accurate identification and quantification of lead iodide and perovskite phases are critical because residual lead iodide strongly influences crystallization pathways and defect formation, while the morphology of perovskite grains governs carrier transport and device stability. Yet current SEM image analysis is still largely manual, limiting throughput and consistency. Here, we present an automated deep learning-based framework for SEM image segmentation that enables precise and efficient identification of lead iodide, perovskite and defect domains across diverse morphologies. Built upon an improved YOLOv8x architecture, our model named PerovSegNet incorporates two novel modules: (i) Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and fine-grained feature extraction through group convolutions and channel mixing; and (ii) Separable Adaptive Downsampling module, which jointly preserves fine-scale textures and large-scale structures for more robust boundary recognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet achieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point Operations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing model size and computational load by 24.43% and 25.22%, respectively. Beyond segmentation, the framework provides quantitative grain-level metrics, such as lead iodide&#x2F;perovskite area and count, which can serve as reliable indicators of crystallization efficiency and microstructural quality. These capabilities establish PerovSegNet as a scalable tool for real-time process monitoring and data-driven optimization of perovskite thin-film fabrication.The source code is available at:<a target="_blank" rel="noopener" href="https://github.com/wlyyj/PerovSegNet/tree/master">https://github.com/wlyyj/PerovSegNet/tree/master</a>. </p>
<blockquote>
<p>æ‰«æç”µå­æ˜¾å¾®é•œï¼ˆSEMï¼‰åœ¨é’™é’›çŸ¿å¤ªé˜³èƒ½ç”µæ± åˆ¶å¤‡è¿‡ç¨‹ä¸­è¡¨å¾è–„è†œå¾®è§‚ç»“æ„æ—¶å¿…ä¸å¯å°‘ã€‚å‡†ç¡®è¯†åˆ«å’Œé‡åŒ–ç¢˜åŒ–é“…å’Œé’™é’›çŸ¿ç›¸è‡³å…³é‡è¦ï¼Œå› ä¸ºæ®‹ç•™çš„ç¢˜åŒ–é“…ä¼šä¸¥é‡å½±å“ç»“æ™¶é€”å¾„å’Œç¼ºé™·çš„å½¢æˆï¼Œè€Œé’™é’›çŸ¿æ™¶ç²’çš„å½¢æ€åˆ™æ§åˆ¶ç€è½½æµå­çš„ä¼ è¾“å’Œè®¾å¤‡çš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„SEMå›¾åƒåˆ†æä»ç„¶æ˜¯å¤§éƒ¨åˆ†æ‰‹åŠ¨æ“ä½œï¼Œé™åˆ¶äº†å¤„ç†é€Ÿåº¦å’Œä¸€è‡´æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–SEMå›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå¯ä»¥ç²¾ç¡®æœ‰æ•ˆåœ°è¯†åˆ«ç¢˜åŒ–é“…ã€é’™é’›çŸ¿å’Œç¼ºé™·åŸŸçš„å„ç§å½¢æ€ã€‚æˆ‘ä»¬çš„æ¨¡å‹PerovSegNetå»ºç«‹åœ¨æ”¹è¿›çš„YOLOv8xæ¶æ„ä¹‹ä¸Šï¼Œå¹¶èå…¥äº†ä¸¤ä¸ªæ–°æ¨¡å—ï¼šï¼ˆiï¼‰è‡ªé€‚åº”æ´—ç‰Œè†¨èƒ€å·ç§¯å—ï¼Œå®ƒé€šè¿‡åˆ†ç»„å·ç§¯å’Œé€šé“æ··åˆå¢å¼ºå¤šå°ºåº¦å’Œç»†ç²’åº¦ç‰¹å¾æå–ï¼›ï¼ˆiiï¼‰å¯åˆ†ç¦»è‡ªé€‚åº”ä¸‹é‡‡æ ·æ¨¡å—ï¼Œå®ƒè”åˆä¿ç•™ç»†çº¹ç†å’Œå¤§è§„æ¨¡ç»“æ„ï¼Œä»¥å®ç°æ›´ç¨³å¥çš„è¾¹ç•Œè¯†åˆ«ã€‚PerovSegNetåœ¨10994å¼ SEMå›¾åƒçš„å¢å¼ºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥265.4 Gigaæµ®ç‚¹è¿ç®—è¾¾åˆ°87.25%çš„å¹³å‡ç²¾åº¦ï¼Œè¶…è¶Šäº†åŸºçº¿YOLOv8x-seg 4.08%ï¼ŒåŒæ—¶åˆ†åˆ«å‡å°äº†æ¨¡å‹å¤§å°å’Œè®¡ç®—è´Ÿè½½çš„24.43%å’Œ25.22%ã€‚é™¤äº†åˆ†å‰²åŠŸèƒ½å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›å®šé‡æ™¶ç²’çº§æŒ‡æ ‡ï¼Œå¦‚ç¢˜åŒ–é“…&#x2F;é’™é’›çŸ¿é¢ç§¯å’Œè®¡æ•°ï¼Œè¿™äº›æŒ‡æ ‡å¯ä»¥ä½œä¸ºç»“æ™¶æ•ˆç‡å’Œå¾®è§‚ç»“æ„è´¨é‡çš„å¯é æŒ‡æ ‡ã€‚è¿™äº›åŠŸèƒ½ä½¿PerovSegNetæˆä¸ºç”¨äºå®æ—¶ç›‘æ§å’Œæ•°æ®é©±åŠ¨ä¼˜åŒ–é’™é’›çŸ¿è–„è†œåˆ¶é€ çš„å¯æ‰©å±•å·¥å…·ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/wlyyj/PerovSegNet/tree/master%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wlyyj/PerovSegNet/tree/masterè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26548v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰«æç”µå­æ˜¾å¾®é•œï¼ˆSEMï¼‰å›¾åƒçš„æ·±åº¦å­¦ä¹ åˆ†å‰²æ¡†æ¶PerovSegNetï¼Œèƒ½ç²¾ç¡®æœ‰æ•ˆåœ°è¯†åˆ«é“…ç¢˜åŒ–ç‰©ã€é’™é’›çŸ¿å’Œç¼ºé™·åŸŸã€‚è¯¥æ¨¡å‹é€šè¿‡æ”¹è¿›YOLOv8xæ¶æ„ï¼Œèå…¥è‡ªé€‚åº”æ··åˆå·ç§¯æ¨¡å—å’Œå¯åˆ†ç¦»è‡ªé€‚åº”é™é‡‡æ ·æ¨¡å—ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œå¹¶èƒ½åœ¨å¤§é‡SEMå›¾åƒä¸­è¯†åˆ«ç»†å¾®å·®å¼‚ã€‚è®­ç»ƒæ•°æ®é›†å¢å¼ºè‡³10,994å¼ SEMå›¾åƒï¼Œæ¨¡å‹å¹³å‡ç²¾åº¦è¾¾åˆ°87.25%ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›å®šé‡æ™¶ç²’çº§æŒ‡æ ‡ï¼Œå¯ç”¨äºè¯„ä¼°é’™é’›çŸ¿è–„è†œçš„ç»“æ™¶æ•ˆç‡å’Œå¾®ç»“æ„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PerovSegNetæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºSEMå›¾åƒåˆ†å‰²ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«é“…ç¢˜åŒ–ç‰©ã€é’™é’›çŸ¿å’Œç¼ºé™·åŸŸã€‚</li>
<li>é€šè¿‡æ”¹è¿›YOLOv8xæ¶æ„ï¼Œèå…¥ä¸¤ä¸ªæ–°æ¨¡å—ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒæ•°æ®é›†å¢å¼ºï¼Œæ¨¡å‹å¹³å‡ç²¾åº¦è¾¾åˆ°87.25%ã€‚</li>
<li>æ¡†æ¶æä¾›å®šé‡æ™¶ç²’çº§æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°é’™é’›çŸ¿è–„è†œçš„ç»“æ™¶æ•ˆç‡å’Œå¾®ç»“æ„è´¨é‡ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œè¾ƒåŸºçº¿YOLOv8x-segé«˜å‡º4.08%ï¼ŒåŒæ—¶å‡å°äº†æ¨¡å‹å¤§å°å’Œè®¡ç®—è´Ÿè½½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-753df13cd5222631e8bad0736549edb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105453&auth_key=1760105453-0-0-6353063026b3198238bfc724da7030f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50a2fa2337e94b3629430daa51d305df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105460&auth_key=1760105460-0-0-0be27e1cd45f4be714714fd6ee92850c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a0e4285e19e53efa708f23daa34679f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105468&auth_key=1760105468-0-0-ffdfb9ac380aa4b2b819e2dc47ff4abd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24ed476fcafec94fcbfc4c2273d7206f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105474&auth_key=1760105474-0-0-8a7f38115024287e4d996351ea53d690&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c621b4970140a12b38a73de0f33d83fc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105481&auth_key=1760105481-0-0-5e82b28947a56e2b8d65e9845acf8744&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoSe2-and-WSe2-shell-morphology-control-via-temperature-optimization-during-two-step-growth-of-ZnSe-based-core-shell-nanowires"><a href="#MoSe2-and-WSe2-shell-morphology-control-via-temperature-optimization-during-two-step-growth-of-ZnSe-based-core-shell-nanowires" class="headerlink" title="MoSe2 and WSe2 shell morphology control via temperature optimization   during two-step growth of ZnSe-based core-shell nanowires"></a>MoSe2 and WSe2 shell morphology control via temperature optimization   during two-step growth of ZnSe-based core-shell nanowires</h2><p><strong>Authors:Luize Dipane, Liora Kotlara, Viktors Vibornijs, Katrina Laganovska, Aleksejs Zolotarjovs, Eriks Dipans, Jevgenijs Gabrusenoks, Boris Polyakov, Edgars Butanovs</strong></p>
<p>Achieving uniform and controlled transition metal dichalcogenide (TMD) shell growth on nanowires (NWs) remains a key challenge, limiting the development of high-quality core-shell heterostructures for optoelectronic and photocatalytic applications. In this work, the fabrication of ZnSe-MoSe2 and ZnSe-WSe2 core-shell NWs was successfully demonstrated. ZnSe NWs were grown via the vapor-liquid-solid growth mechanism, while TMD (MoSe2 or WSe2) shells were formed through a two-step process of sacrificial oxide layer deposition via magnetron sputtering followed by selenization process in a chemical vapor transport reactor. As-grown nanostructures were characterized using X-ray diffraction, transmission electron microscopy, X-ray photoelectron spectroscopy, Raman spectroscopy and photoluminescence spectroscopy. It was observed that the TMD shell morphology can be controlled through the selenization process temperature optimization, which arises due to different growth mechanisms discussed here. The studied trends could be further extended to other semiconductor NW and TMD core-shell heterostructure growth, offering promising avenues for advanced nanoscale applications. </p>
<blockquote>
<p>åœ¨çº³ç±³çº¿ä¸Šå®ç°å‡åŒ€å¯æ§çš„è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ï¼ˆTMDï¼‰å¤–å£³ç”Ÿé•¿ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†ç”¨äºå…‰ç”µå­å’Œå…‰å‚¬åŒ–åº”ç”¨çš„é«˜è´¨é‡æ ¸å£³å¼‚è´¨ç»“æ„çš„å‘å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼ŒæˆåŠŸå±•ç¤ºäº†ZnSe-MoSe2å’ŒZnSe-WSe2æ ¸å£³çº³ç±³çº¿çš„åˆ¶å¤‡ã€‚ZnSeçº³ç±³çº¿æ˜¯é€šè¿‡æ°”æ¶²å›ºç”Ÿé•¿æœºåˆ¶ç”Ÿé•¿çš„ï¼Œè€ŒTMDï¼ˆMoSe2æˆ–WSe2ï¼‰å¤–å£³åˆ™æ˜¯é€šè¿‡ç£æ§æº…å°„æ²‰ç§¯ç‰ºç‰²æ°§åŒ–å±‚åï¼Œå†ç»åŒ–å­¦æ°”ç›¸ä¼ è¾“ååº”å™¨è¿›è¡Œç¡’åŒ–è¿‡ç¨‹çš„ä¸¤æ­¥å·¥è‰ºå½¢æˆçš„ã€‚å¯¹æ‰€ç”Ÿé•¿çš„çº³ç±³ç»“æ„è¿›è¡Œäº†Xå°„çº¿è¡å°„ã€é€å°„ç”µå­æ˜¾å¾®é•œã€Xå°„çº¿å…‰ç”µå­å…‰è°±ã€æ‹‰æ›¼å…‰è°±å’Œå…‰è‡´å‘å…‰å…‰è°±è¡¨å¾ã€‚è§‚å¯Ÿå‘ç°ï¼Œé€šè¿‡ä¼˜åŒ–ç¡’åŒ–è¿‡ç¨‹æ¸©åº¦ï¼Œå¯ä»¥æ§åˆ¶TMDå¤–å£³çš„å½¢æ€ï¼Œè¿™æ˜¯ç”±äºè¿™é‡Œè®¨è®ºçš„ä¸åŒç”Ÿé•¿æœºåˆ¶æ‰€å¯¼è‡´çš„ã€‚æ‰€ç ”ç©¶çš„è¶‹åŠ¿å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–åŠå¯¼ä½“çº³ç±³çº¿å’ŒTMDæ ¸å£³å¼‚è´¨ç»“æ„çš„ç”Ÿé•¿ï¼Œä¸ºå…ˆè¿›çš„çº³ç±³åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26312v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åˆ¶é€ ZnSe-MoSe2åŠZnSe-WSe2æ ¸å£³çº³ç±³çº¿æ—¶é¢ä¸´çš„ä¸€é¡¹é‡è¦æŒ‘æˆ˜æ˜¯é‡‘å±äºŒå¤åŒ–ç‰©å£³å±‚çš„å‡åŒ€ç”Ÿé•¿æ§åˆ¶é—®é¢˜ã€‚æœ¬ç ”ç©¶æˆåŠŸå±•ç¤ºäº†æ ¸å£³çº³ç±³çº¿çš„ç”Ÿé•¿è¿‡ç¨‹ï¼Œé‡‡ç”¨æ°”æ¶²å›ºç”Ÿé•¿æœºåˆ¶åˆ¶é€ ZnSeçº³ç±³çº¿ï¼Œé€šè¿‡ç£æ§æº…å°„æ²‰ç§¯ç‰ºç‰²æ°§åŒ–ç‰©å±‚ï¼Œå¹¶åœ¨åŒ–å­¦æ°”ç›¸ä¼ è¾“ååº”å™¨ä¸­è¿›è¡Œç¡’åŒ–è¿‡ç¨‹å½¢æˆTMDå£³å±‚ã€‚å®éªŒè¯æ˜ï¼Œå¯ä»¥é€šè¿‡ä¼˜åŒ–ç¡’åŒ–è¿‡ç¨‹æ¸©åº¦æ¥æ§åˆ¶TMDå£³çš„å½¢æ€ï¼Œè¿™å¯¹æœªæ¥çš„å…ˆè¿›çº³ç±³çº§åº”ç”¨ååˆ†æœ‰åˆ©ã€‚ç ”ç©¶æˆæœè¿˜å¯ä»¥æ‹“å±•åˆ°å…¶ä»–åŠå¯¼ä½“çº³ç±³çº¿å’Œé‡‘å±äºŒå¤åŒ–ç‰©æ ¸å£³å¼‚è´¨ç»“æ„ç‰©çš„ç”Ÿé•¿ç ”ç©¶ä¸­ã€‚è¯¥ç»“æœå…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚é€šè¿‡æ‰«æXå°„çº¿è¡å°„åˆ†æå¯çŸ¥å…¶å¯¹é‡‘å±äºŒå¤åŒ–ç‰©(TMD)çš„ç»“æ„ä¾èµ–æ€§å¯é¢„æœŸå±•ç°è¾ƒä¸ºå¯é ç»“æœçš„ç†è®ºéªŒè¯æ–¹å¼å¯ä¾›å€Ÿé‰´ã€‚æ­¤ç ”ç©¶ä¸ä»…å¯¹äºææ–™ç§‘å­¦é¢†åŸŸæœ‰ç€é‡è¦å½±å“ï¼ŒåŒæ—¶ä¹Ÿåœ¨å…‰ç”µå­å­¦å’Œå…‰å‚¬åŒ–é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶é“è·¯ã€‚è¿™é¡¹ç ”ç©¶å°†æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåˆ›æ–°å‘å±•ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬é«˜æ€§èƒ½çº³ç±³ææ–™çš„åˆæˆä»¥åŠè¿™äº›ææ–™åœ¨ç”µå­è®¾å¤‡ä¸­çš„æ½œåœ¨åº”ç”¨ç­‰å‰æ²¿é—®é¢˜ã€‚æœ¬ç ”ç©¶æœ‰åŠ©äºè¿›ä¸€æ­¥æ¨è¿›å…‰ç”µé¢†åŸŸçš„å‘å±•ï¼Œå¹¶æœ‰æœ›ä¸ºæœªæ¥çš„æŠ€æœ¯é©æ–°æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¯¥ç ”ç©¶æˆæœå¯¹äºæ¨åŠ¨çº³ç±³ææ–™çš„å‘å±•å…·æœ‰é‡å¤§æ„ä¹‰ã€‚å®ƒä¸ä»…è§£å†³äº†é‡‘å±äºŒå¤åŒ–ç‰©å£³å±‚ç”Ÿé•¿çš„å…³é”®é—®é¢˜ï¼Œè€Œä¸”ä¸ºå…ˆè¿›çº³ç±³çº§åº”ç”¨æä¾›äº†å¹¿é˜”çš„å‰æ™¯å’Œæ½œåœ¨çš„å®ç”¨ä»·å€¼ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæœ¬é¡¹ç ”ç©¶å°†å¯¹æ ¸å£³çº³ç±³çº¿çš„å¼€å‘äº§ç”Ÿé‡è¦å½±å“å¹¶æ¨è¿›çº³ç±³ç§‘å­¦é¢†åŸŸçš„å‘å±•è¿›æ­¥å…·æœ‰æ·±è¿œçš„å®è·µæ„ä¹‰å’Œç¤¾ä¼šä»·å€¼ç­‰æ›´å¤šç›¸å…³æŠ€æœ¯é¢†åŸŸä¸­çš„å¹¿é˜”å‘å±•å‰æ™¯æœ‰æœ›å¯¹æ›´å¹¿é˜”çš„ç§‘å­¦åº”ç”¨é¢†åŸŸæä¾›æŠ€æœ¯æ”¯æŒä¸åº”ç”¨æ‹“å±•å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯åˆ›æ–°ä¸å‘å±•è¿›æ­¥ã€‚è¿™ä¸€æˆæœä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å¯ç¤ºå’Œå€Ÿé‰´ã€‚æœªæ¥ï¼Œè¯¥ç ”ç©¶å°†æœ‰æœ›ä¸ºåŠå¯¼ä½“çº³ç±³çº¿å’Œæ ¸å£³å¼‚è´¨ç»“æ„çš„ç ”ç©¶å¸¦æ¥æ–°çš„çªç ´å’Œå‘å±•æ–¹å‘ã€‚<strong>Key Takeaways</strong>:</p>
<p>ä¸€ã€å®ç°äº†ZnSe-MoSe2å’ŒZnSe-WSe2æ ¸å£³çº³ç±³çº¿çš„æˆåŠŸç”Ÿé•¿ã€‚è¿™æ˜¯é€šè¿‡åœ¨çº³ç±³çº¿ä¸Šåˆ©ç”¨æ°”æ¶²å›ºç”Ÿé•¿æœºåˆ¶ç”ŸæˆZnSeæ ¸åï¼Œå†é€šè¿‡ç£æ§æº…å°„æ²‰ç§¯ç‰ºç‰²æ°§åŒ–ç‰©å±‚ï¼Œæœ€åè¿›è¡Œç¡’åŒ–è¿‡ç¨‹å½¢æˆTMDå£³å±‚å®ç°çš„ã€‚</p>
<p>äºŒã€ç ”ç©¶å‘ç°åœ¨ç¡’åŒ–è¿‡ç¨‹ä¸­é€šè¿‡æ¸©åº¦ä¼˜åŒ–èƒ½å¤Ÿæ§åˆ¶å£³å±‚å½¢æ€å˜åŒ–ã€‚æ­¤è§‚å¯Ÿæ˜¯ç”±äºåœ¨æ­¤ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿçš„ä¸åŒç”Ÿé•¿æœºåˆ¶é€ æˆçš„ã€‚è¿™ä¸€å‘ç°å¯¹äºæœªæ¥åŠå¯¼ä½“çº³ç±³çº¿å’Œé‡‘å±äºŒå¤åŒ–ç‰©æ ¸å£³å¼‚è´¨ç»“æ„çš„ç”Ÿé•¿å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a70e413147d7105bbe61d2a74636bca7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105488&auth_key=1760105488-0-0-1866f583c7d53859306ff296221d873b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Liver-Segmentation-and-Fibrosis-Staging-Using-Real-world-MRI-Images"><a href="#Multi-modal-Liver-Segmentation-and-Fibrosis-Staging-Using-Real-world-MRI-Images" class="headerlink" title="Multi-modal Liver Segmentation and Fibrosis Staging Using Real-world MRI   Images"></a>Multi-modal Liver Segmentation and Fibrosis Staging Using Real-world MRI   Images</h2><p><strong>Authors:Yang Zhou, Kunhao Yuan, Ye Wei, Jishizhan Chen</strong></p>
<p>Liver fibrosis represents the accumulation of excessive extracellular matrix caused by sustained hepatic injury. It disrupts normal lobular architecture and function, increasing the chances of cirrhosis and liver failure. Precise staging of fibrosis for early diagnosis and intervention is often invasive, which carries risks and complications. To address this challenge, recent advances in artificial intelligence-based liver segmentation and fibrosis staging offer a non-invasive alternative. As a result, the CARE 2025 Challenge aimed for automated methods to quantify and analyse liver fibrosis in real-world scenarios, using multi-centre, multi-modal, and multi-phase MRI data. This challenge included tasks of precise liver segmentation (LiSeg) and fibrosis staging (LiFS). In this study, we developed an automated pipeline for both tasks across all the provided MRI modalities. This pipeline integrates pseudo-labelling based on multi-modal co-registration, liver segmentation using deep neural networks, and liver fibrosis staging based on shape, textural, appearance, and directional (STAD) features derived from segmentation masks and MRI images. By solely using the released data with limited annotations, our proposed pipeline demonstrated excellent generalisability for all MRI modalities, achieving top-tier performance across all competition subtasks. This approach provides a rapid and reproducible framework for quantitative MRI-based liver fibrosis assessment, supporting early diagnosis and clinical decision-making. Code is available at <a target="_blank" rel="noopener" href="https://github.com/YangForever/care2025_liver_biodreamer">https://github.com/YangForever/care2025_liver_biodreamer</a>. </p>
<blockquote>
<p>è‚è„çº¤ç»´åŒ–æ˜¯ç”±æŒç»­çš„è‚æŸä¼¤å¼•èµ·çš„è¿‡é‡ç»†èƒå¤–åŸºè´¨ç§¯ç´¯ã€‚å®ƒä¼šç ´åæ­£å¸¸çš„è‚å°å¶ç»“æ„å’ŒåŠŸèƒ½ï¼Œå¢åŠ è‚ç¡¬åŒ–å’Œè‚è¡°ç«­çš„é£é™©ã€‚å¯¹çº¤ç»´åŒ–è¿›è¡Œç²¾ç¡®åˆ†æœŸä»¥è¿›è¡Œæ—©æœŸè¯Šæ–­å’Œæ²»ç–—é€šå¸¸å…·æœ‰ä¾µå…¥æ€§ï¼Œè¿™å¸¦æ¥äº†é£é™©å’Œå¹¶å‘ç—‡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œäººå·¥æ™ºèƒ½åœ¨è‚è„åˆ†å‰²å’Œçº¤ç»´åŒ–åˆ†æœŸæ–¹é¢çš„æœ€æ–°è¿›å±•æä¾›äº†ä¸€ç§éä¾µå…¥æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚å› æ­¤ï¼ŒCARE 2025æŒ‘æˆ˜èµ›æ—¨åœ¨åˆ©ç”¨å¤šä¸­å¿ƒã€å¤šæ¨¡æ€å’Œå¤šé˜¶æ®µçš„MRIæ•°æ®ï¼Œå¼€å‘è‡ªåŠ¨åŒ–æ–¹æ³•æ¥é‡åŒ–å¹¶åˆ†æå®é™…åœºæ™¯ä¸­çš„è‚çº¤ç»´åŒ–ã€‚è¯¥æŒ‘æˆ˜åŒ…æ‹¬ç²¾ç¡®çš„è‚è„åˆ†å‰²ï¼ˆLiSegï¼‰å’Œçº¤ç»´åŒ–åˆ†æœŸï¼ˆLiFSï¼‰ä»»åŠ¡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ‰€æœ‰æä¾›çš„MRIæ¨¡å¼å¼€å‘äº†ä¸€ä¸ªç”¨äºè¿™ä¸¤ä¸ªä»»åŠ¡çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚è¯¥ç®¡é“é›†æˆäº†åŸºäºå¤šæ¨¡æ€é…å‡†çš„ä¼ªæ ‡è®°ã€ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè‚è„åˆ†å‰²ä»¥åŠåŸºäºåˆ†å‰²æ©è†œå’ŒMRIå›¾åƒçš„å½¢çŠ¶ã€çº¹ç†ã€å¤–è§‚å’Œæ–¹å‘ï¼ˆSTADï¼‰ç‰¹å¾è¿›è¡Œè‚è„çº¤ç»´åŒ–åˆ†æœŸã€‚ä»…ä½¿ç”¨æœ‰é™æ ‡æ³¨çš„å‘å¸ƒæ•°æ®ï¼Œæˆ‘ä»¬æå‡ºçš„ç®¡é“åœ¨æ‰€æœ‰MRIæ¨¡å¼ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ï¼Œåœ¨æ‰€æœ‰ç«èµ›å­ä»»åŠ¡ä¸­å‡å–å¾—äº†é¡¶å°–çš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ä¸ªå¿«é€Ÿä¸”å¯é‡å¤çš„é‡åŒ–çš„MRIè‚è„çº¤ç»´åŒ–è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒæ—©æœŸè¯Šæ–­å’Œä¸´åºŠå†³ç­–ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YangForever/care2025_liver_biodreamer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YangForever/care2025_liver_biodreamerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26061v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯å®ç°æ— åˆ›è‚è„çº¤ç»´åŒ–åˆ†æœŸè¯„ä¼°ã€‚åŸºäºå¤šæ¨¡æ€MRIæ•°æ®çš„ä¼ªæ ‡è®°å’Œå¤šæ¨¡æ€èåˆï¼Œå»ºç«‹è‡ªåŠ¨åŒ–æµç¨‹å®ç°è‚è„åˆ†å‰²å’Œçº¤ç»´åŒ–åˆ†æœŸè¯„ä¼°ã€‚ä»£ç å…¬å¼€å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚è„çº¤ç»´åŒ–æ˜¯ç”±æŒç»­è‚æŸä¼¤å¼•èµ·çš„è¿‡é‡ç»†èƒå¤–åŸºè´¨ç§¯ç´¯å¯¼è‡´çš„ã€‚</li>
<li>çº¤ç»´åŒ–ä¼šç ´åæ­£å¸¸çš„è‚å°å¶ç»“æ„å’ŒåŠŸèƒ½ï¼Œå¢åŠ è‚ç¡¬åŒ–å’Œè‚è¡°ç«­çš„é£é™©ã€‚</li>
<li>ç›®å‰ç²¾ç¡®çš„çº¤ç»´åŒ–åˆ†æœŸæ–¹æ³•é€šå¸¸å…·æœ‰ä¾µå…¥æ€§ï¼Œå­˜åœ¨é£é™©å’Œå¹¶å‘ç—‡ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨è‚è„åˆ†å‰²å’Œçº¤ç»´åŒ–åˆ†æœŸæ–¹é¢çš„è¿›å±•ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ— åˆ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>CARE 2025æŒ‘æˆ˜æ—¨åœ¨ä½¿ç”¨å¤šä¸­å¿ƒã€å¤šæ¨¡æ€å’Œå¤šé˜¶æ®µçš„MRIæ•°æ®ï¼Œå®ç°è‚è„çº¤ç»´åŒ–çš„è‡ªåŠ¨åŒ–é‡åŒ–å’Œåˆ†æã€‚</li>
<li>ç ”ç©¶ä¸­å¼€å‘çš„è‡ªåŠ¨åŒ–æµç¨‹åŒ…æ‹¬åŸºäºå¤šæ¨¡æ€é…å‡†çš„ä¼ªæ ‡è®°ã€ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè‚è„åˆ†å‰²ä»¥åŠåŸºäºåˆ†å‰²æ©è†œå’ŒMRIå›¾åƒçš„å½¢çŠ¶ã€çº¹ç†ã€å¤–è§‚å’Œæ–¹å‘ç‰¹å¾è¿›è¡Œè‚è„çº¤ç»´åŒ–åˆ†æœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b6081d7d2e3a41f47e5ca3953d0e9560~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105496&auth_key=1760105496-0-0-32d9c306a2f715c2a86c0f91e663bab3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75748aa0aace0450083dd95436fb24ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105504&auth_key=1760105504-0-0-dbddad820d9274de31352c90e251739a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2e0980525a44743fd99d2196a0858f51~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105511&auth_key=1760105511-0-0-91f0a2e4c1e8c775b33d0200f2bf21eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Causally-Guided-Gaussian-Perturbations-for-Out-Of-Distribution-Generalization-in-Medical-Imaging"><a href="#Causally-Guided-Gaussian-Perturbations-for-Out-Of-Distribution-Generalization-in-Medical-Imaging" class="headerlink" title="Causally Guided Gaussian Perturbations for Out-Of-Distribution   Generalization in Medical Imaging"></a>Causally Guided Gaussian Perturbations for Out-Of-Distribution   Generalization in Medical Imaging</h2><p><strong>Authors:Haoran Pei, Yuguang Yang, Kexin Liu, Baochang Zhang</strong></p>
<p>Out-of-distribution (OOD) generalization remains a central challenge in deploying deep learning models to real-world scenarios, particularly in domains such as biomedical images, where distribution shifts are both subtle and pervasive. While existing methods often pursue domain invariance through complex generative models or adversarial training, these approaches may overlook the underlying causal mechanisms of generalization.In this work, we propose Causally-Guided Gaussian Perturbations (CGP)-a lightweight framework that enhances OOD generalization by injecting spatially varying noise into input images, guided by soft causal masks derived from Vision Transformers. By applying stronger perturbations to background regions and weaker ones to foreground areas, CGP encourages the model to rely on causally relevant features rather than spurious correlations.Experimental results on the challenging WILDS benchmark Camelyon17 demonstrate consistent performance gains over state-of-the-art OOD baselines, highlighting the potential of causal perturbation as a tool for reliable and interpretable generalization. </p>
<blockquote>
<p>å°†æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²åˆ°çœŸå®åœºæ™¯æ—¶ï¼Œå°¤å…¶æ˜¯å¤„ç†ç”Ÿç‰©å›¾åƒç­‰é¢†åŸŸæ—¶ï¼Œé¢ä¸´ç€åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå› ä¸ºè¿™é‡Œçš„åˆ†å¸ƒå˜åŒ–æ—¢å¾®å¦™åˆæ™®éã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¤æ‚çš„ç”Ÿæˆæ¨¡å‹æˆ–å¯¹æŠ—è®­ç»ƒè¿½æ±‚åŸŸä¸å˜æ€§ï¼Œä½†è¿™äº›æ–¹æ³•å¯èƒ½ä¼šå¿½ç•¥æ³›åŒ–çš„æ½œåœ¨å› æœæœºåˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å—å› æœå¼•å¯¼çš„é«˜æ–¯æ‰°åŠ¨ï¼ˆCGPï¼‰â€”â€”ä¸€ä¸ªå¢å¼ºå‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡å‘è¾“å…¥å›¾åƒæ³¨å…¥ç©ºé—´å˜åŒ–çš„å™ªå£°æ¥æé«˜OODæ³›åŒ–èƒ½åŠ›ï¼Œè¿™äº›å™ªå£°ç”±åŸºäºè§†è§‰å˜å‹å™¨çš„è½¯å› æœæ©è†œå¼•å¯¼ã€‚é€šè¿‡å¯¹èƒŒæ™¯åŒºåŸŸåº”ç”¨æ›´å¼ºçš„æ‰°åŠ¨ï¼Œå¯¹å‰æ™¯åŒºåŸŸåº”ç”¨è¾ƒå¼±çš„æ‰°åŠ¨ï¼ŒCGPé¼“åŠ±æ¨¡å‹ä¾èµ–äºå› æœç›¸å…³ç‰¹å¾è€Œä¸æ˜¯å¶ç„¶ç›¸å…³æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„WILDSåŸºå‡†æ•°æ®é›†Camelyon17ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„OODåŸºçº¿ç›¸æ¯”ï¼ŒCGPå…·æœ‰æŒç»­çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¿™çªæ˜¾äº†å› æœæ‰°åŠ¨ä½œä¸ºå¯é å’Œå¯è§£é‡Šæ³›åŒ–å·¥å…·çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26027v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCausally-Guided Gaussian Perturbationsï¼ˆCGPï¼‰çš„è½»é‡çº§æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å‘è¾“å…¥å›¾åƒæ³¨å…¥å—è§†è§‰è½¬æ¢å™¨å¼•å¯¼çš„ç©ºé—´å˜åŒ–å™ªå£°ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡åœ¨èƒŒæ™¯åŒºåŸŸåº”ç”¨æ›´å¼ºçš„æ‰°åŠ¨ï¼Œå¹¶åœ¨å‰æ™¯åŒºåŸŸåº”ç”¨è¾ƒå¼±çš„æ‰°åŠ¨ï¼ŒCGPé¼“åŠ±æ¨¡å‹ä¾èµ–äºå› æœç›¸å…³ç‰¹å¾ï¼Œè€Œä¸æ˜¯å¶ç„¶çš„ç›¸å…³æ€§ã€‚åœ¨Camelyon17ç­‰æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„OODåŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰æŒç»­çš„æ€§èƒ½æå‡æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ çš„æ¨¡å‹åœ¨éƒ¨ç½²åˆ°çœŸå®ä¸–ç•Œåœºæ™¯æ—¶é¢ä¸´OODï¼ˆOut-of-Distributionï¼‰æ³›åŒ–æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒç­‰é¢†åŸŸï¼Œåˆ†å¸ƒè½¬ç§»æ˜¯å¾®å¦™ä¸”æ™®éçš„ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¤æ‚çš„ç”Ÿæˆæ¨¡å‹æˆ–å¯¹æŠ—æ€§è®­ç»ƒè¿½æ±‚åŸŸä¸å˜æ€§ï¼Œä½†å¯èƒ½å¿½ç•¥äº†æ³›åŒ–çš„åº•å±‚å› æœæœºåˆ¶ã€‚</li>
<li>CGPæ¡†æ¶é€šè¿‡å‘è¾“å…¥å›¾åƒæ³¨å…¥å—è§†è§‰è½¬æ¢å™¨å¼•å¯¼çš„ç©ºé—´å˜åŒ–å™ªå£°ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>CGPé¼“åŠ±æ¨¡å‹ä¾èµ–äºå› æœç›¸å…³ç‰¹å¾ï¼Œè€Œä¸æ˜¯å¶ç„¶çš„ç›¸å…³æ€§ï¼Œé€šè¿‡åœ¨èƒŒæ™¯åŒºåŸŸåº”ç”¨æ›´å¼ºçš„æ‰°åŠ¨ï¼Œå¹¶åœ¨å‰æ™¯åŒºåŸŸåº”ç”¨è¾ƒå¼±çš„æ‰°åŠ¨æ¥å®ç°ã€‚</li>
<li>åœ¨Camelyon17ç­‰æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜CGPæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1ac4fafbf4f778072926075c9f440a6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105519&auth_key=1760105519-0-0-bdf016c0849d65ce0939c8a9e8e08168&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99d2cb6325955d2e9a483e131a81d78e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105526&auth_key=1760105526-0-0-abd24b83ba93c66e49c888eacb904563&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a4527317636792abf957537efcf2683~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105534&auth_key=1760105534-0-0-07114ebbef53cc0641e41269738a1009&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa18235ec09820a76adfa6c376d4533b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105541&auth_key=1760105541-0-0-ff99fa8ca89b344c0e9470577f4ffb00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-LLM-Approach-for-Visual-Question-Answering-on-Multiparametric-3D-Brain-MRI"><a href="#A-Multimodal-LLM-Approach-for-Visual-Question-Answering-on-Multiparametric-3D-Brain-MRI" class="headerlink" title="A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI"></a>A Multimodal LLM Approach for Visual Question Answering on   Multiparametric 3D Brain MRI</h2><p><strong>Authors:Arvind Murari Vepa, Yannan Yu, Jingru Gan, Anthony Cuturrufo, Weikai Li, Wei Wang, Fabien Scalzo, Yizhou Sun</strong></p>
<p>We introduce mpLLM, a prompt-conditioned hierarchical mixture-of-experts (MoE) architecture for visual question answering over multi-parametric 3D brain MRI (mpMRI). mpLLM routes across modality-level and token-level projection experts to fuse multiple interrelated 3D modalities, enabling efficient training without imageâ€“report pretraining. To address limited image-text paired supervision, mpLLM integrates a synthetic visual question answering (VQA) protocol that generates medically relevant VQA from segmentation annotations, and we collaborate with medical experts for clinical validation. mpLLM outperforms strong medical VLM baselines by 5.3% on average across multiple mpMRI datasets. Our study features three main contributions: (1) the first clinically validated VQA dataset for 3D brain mpMRI, (2) a novel multimodal LLM that handles multiple interrelated 3D modalities, and (3) strong empirical results that demonstrate the medical utility of our methodology. Ablations highlight the importance of modality-level and token-level experts and prompt-conditioned routing. We have included our source code in the supplementary materials and will release our dataset upon publication. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†mpLLMï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šå‚æ•°3Dè„‘MRIï¼ˆmpMRIï¼‰çš„è§†è§‰é—®ç­”çš„æç¤ºæ¡ä»¶åˆ†å±‚ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ã€‚mpLLMåœ¨æ¨¡æ€çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«æŠ•å½±ä¸“å®¶ä¹‹é—´è¿›è¡Œè·¯ç”±ï¼Œä»¥èåˆå¤šä¸ªç›¸äº’å…³è”çš„3Dæ¨¡æ€ï¼Œå®ç°é«˜æ•ˆçš„è®­ç»ƒï¼Œæ— éœ€å›¾åƒæŠ¥å‘Šé¢„è®­ç»ƒã€‚ä¸ºäº†è§£å†³æœ‰é™çš„å›¾åƒæ–‡æœ¬é…å¯¹ç›‘ç£é—®é¢˜ï¼ŒmpLLMé›†æˆäº†ä¸€ä¸ªåˆæˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åè®®ï¼Œè¯¥åè®®ä»åˆ†å‰²æ³¨é‡Šä¸­ç”ŸæˆåŒ»å­¦ç›¸å…³çš„VQAï¼Œå¹¶ä¸åŒ»å­¦ä¸“å®¶è¿›è¡Œåˆä½œè¿›è¡Œä¸´åºŠéªŒè¯ã€‚åœ¨å¤šä¸ªmpMRIæ•°æ®é›†ä¸Šï¼ŒmpLLMå¹³å‡æ¯”å¼ºå¤§çš„åŒ»ç–—VLMåŸºå‡†æµ‹è¯•é«˜å‡º5.3%ã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šï¼ˆ1ï¼‰é¦–ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„3Dè„‘mpMRIçš„VQAæ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ä¸€ç§å¤„ç†å¤šä¸ªç›¸äº’å…³è”çš„3Dæ¨¡æ€çš„æ–°å‹å¤šæ¨¡æ€LLMï¼Œä»¥åŠï¼ˆ3ï¼‰å¼ºæœ‰åŠ›çš„å®è¯ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åŒ»å­¦å®ç”¨æ€§ã€‚æ¶ˆèå®éªŒçªæ˜¾äº†æ¨¡æ€çº§åˆ«å’Œä»¤ç‰Œçº§åˆ«ä¸“å®¶ä»¥åŠæç¤ºæ¡ä»¶è·¯ç”±çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æºä»£ç å·²åŒ…å«åœ¨è¡¥å……ææ–™ä¸­ï¼Œå¹¶åœ¨å‘å¸ƒæ—¶å…¬å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25889v1">PDF</a> 23 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†mpLLMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šå‚æ•°ä¸‰ç»´è„‘MRIï¼ˆmpMRIï¼‰çš„è§†è§‰é—®ç­”çš„æç¤ºæ¡ä»¶åˆ†å±‚æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ã€‚è¯¥æ¶æ„å®ç°äº†è·¨æ¨¡æ€å’Œæ ‡è®°çº§çš„æŠ•å½±ä¸“å®¶è·¯ç”±ï¼Œèåˆäº†å¤šä¸ªç›¸å…³çš„ä¸‰ç»´æ¨¡æ€ï¼Œæ— éœ€å›¾åƒæŠ¥å‘Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå°±èƒ½è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚ä¸ºè§£å†³å›¾åƒæ–‡æœ¬é…å¯¹ç›‘ç£æœ‰é™çš„é—®é¢˜ï¼ŒmpLLMé›†æˆäº†ä¸€ç§åˆæˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åè®®ï¼Œè¯¥åè®®å¯ä»åˆ†å‰²æ³¨é‡Šç”ŸæˆåŒ»å­¦ç›¸å…³çš„VQAï¼Œå¹¶ä¸åŒ»å­¦ä¸“å®¶åˆä½œè¿›è¡Œä¸´åºŠéªŒè¯ã€‚åœ¨å¤šä¸ªmpMRIæ•°æ®é›†ä¸Šï¼ŒmpLLMå¹³å‡æ¯”å¼ºå¤§çš„åŒ»å­¦VLMåŸºçº¿é«˜å‡º5.3%ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é¦–ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„ç”¨äºä¸‰ç»´è„‘mpMRIçš„è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ä¸€ç§å¤„ç†å¤šä¸ªç›¸å…³ä¸‰ç»´æ¨¡æ€çš„æ–°å‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œï¼ˆ3ï¼‰å¼ºæœ‰åŠ›çš„å®è¯ç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•è®ºçš„åŒ»å­¦å®ç”¨æ€§ã€‚åˆ†ç¦»ç ”ç©¶çªå‡ºäº†æ¨¡æ€çº§å’Œæ ‡è®°çº§ä¸“å®¶ä»¥åŠæç¤ºæ¡ä»¶è·¯ç”±çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mpLLMæ˜¯ä¸€ç§ç”¨äºå¤šå‚æ•°ä¸‰ç»´è„‘MRIçš„è§†è§‰é—®ç­”çš„åˆ†å±‚æ··åˆä¸“å®¶æ¶æ„ã€‚</li>
<li>è¯¥æ¶æ„å®ç°äº†è·¨æ¨¡æ€å’Œæ ‡è®°çº§çš„æŠ•å½±ä¸“å®¶è·¯ç”±ï¼Œèåˆäº†å¤šä¸ªç›¸å…³çš„ä¸‰ç»´æ¨¡æ€ã€‚</li>
<li>mpLLMé€šè¿‡é›†æˆåˆæˆè§†è§‰é—®ç­”åè®®ï¼Œç”ŸæˆåŒ»å­¦ç›¸å…³çš„é—®ç­”å¹¶è¿›è¡Œä¸´åºŠéªŒè¯ã€‚</li>
<li>mpLLMåœ¨å¤šä¸ªmpMRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŒ»å­¦VLMåŸºçº¿ï¼Œå¹³å‡é«˜å‡º5.3%ã€‚</li>
<li>æœ¬ç ”ç©¶è´¡çŒ®åŒ…æ‹¬é¦–ä¸ªä¸´åºŠéªŒè¯çš„VQAæ•°æ®é›†ã€æ–°å‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå®è¯ç»“æœã€‚</li>
<li>åˆ†ç¦»ç ”ç©¶è¯æ˜äº†æ¨¡æ€çº§å’Œæ ‡è®°çº§ä¸“å®¶ä»¥åŠæç¤ºæ¡ä»¶è·¯ç”±çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f0133cc19c7a448ddd1414a28bc468d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105548&auth_key=1760105548-0-0-4db51b8be3048ec289f2ad88194957f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-721452b889224d726132f69975ec7cda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105557&auth_key=1760105557-0-0-5a3af9f4a9911fe538da10423e2982dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a8f7c3d77190e33f9a9ff297a112cb1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105564&auth_key=1760105564-0-0-680170943a7d8cd180b044b8b2cec8fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdbaf34cddeb3e9a40448edf4cf3653d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105570&auth_key=1760105570-0-0-1b7485ef4b81577edcd762da5cbb4ac6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dolphin-v1-0-Technical-Report"><a href="#Dolphin-v1-0-Technical-Report" class="headerlink" title="Dolphin v1.0 Technical Report"></a>Dolphin v1.0 Technical Report</h2><p><strong>Authors:Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun zheng, Anjie Le, Hongcheng Guo</strong></p>
<p>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasoundâ€™s complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language framework.To tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical adaptability.The Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI. </p>
<blockquote>
<p>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€æ“ä½œè€…ä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½çš„æ•´åˆã€‚è™½ç„¶å¤§å‹å¤šæ¨¡å¼æ¨¡å‹åœ¨å…¶ä»–åŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†åœ¨åº”å¯¹è¶…å£°çš„å¤æ‚æ€§æ–¹é¢å´é‡åˆ°å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Dolphin v1.0ï¼ˆV1ï¼‰åŠå…¶å¢å¼ºæ¨ç†ç‰ˆæœ¬Dolphin R1ã€‚ä½œä¸ºé¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼è¶…å£°åŸºç¡€æ¨¡å‹ï¼ŒDolphinèƒ½åœ¨å•ä¸€è§†è§‰è¯­è¨€æ¡†æ¶ä¸‹ç»Ÿä¸€å®Œæˆå¤šæ ·åŒ–çš„ä¸´åºŠä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¶…å£°çš„å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€ä¸ªè§„æ¨¡è¾¾2ç™¾ä¸‡çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç»“åˆäº†æ•™ç§‘ä¹¦çŸ¥è¯†ã€å…¬å¼€æ•°æ®ã€åˆæˆæ ·æœ¬å’Œä¸€èˆ¬è¯­æ–™åº“ã€‚è¿™ç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚åº”æ€§ã€‚Dolphinç³»åˆ—é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢†åŸŸä¸“ä¸šåŒ–é¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨å¯¹é½å’ŒåŸºäºå¢å¼ºçš„ç»†åŒ–ã€‚Dolphin v1.0åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¯é çš„æ€§èƒ½ã€‚è€ŒDolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ å¹¶ä½¿ç”¨è¶…å£°ç‰¹å®šå¥–åŠ±å¢å¼ºäº†è¯Šæ–­æ¨ç†çš„é€æ˜åº¦ã€‚åœ¨U2-Benchä¸Šçš„å…«é¡¹è¶…å£°ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒDolphin R1çš„U2åˆ†æ•°ä¸º0.5835ï¼Œæ˜¯ç¬¬äºŒåæ¨¡å‹ï¼ˆ0.2968ï¼‰çš„ä¸¤å€å¤šï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯è®°å½•ã€‚Dolphin v1.0ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ï¼ŒéªŒè¯äº†å…¶ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¢å¼ºæ¨ç†çš„è®­ç»ƒæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ï¼Œçªæ˜¾å…¶åœ¨é«˜é£é™©åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25748v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†Dolphin v1.0åŠå…¶å¢å¼ºç‰ˆDolphin R1ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶å†…èåˆäº†å¤šç§ä¸´åºŠä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†ç»„åˆï¼ŒDolphinç³»åˆ—æ¨¡å‹åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆç­‰æ–¹é¢è¡¨ç°å‡ºå¯é æ€§èƒ½ï¼Œè€ŒDolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸è¶…å£°ç‰¹å®šå¥–åŠ±å¢å¼ºè¯Šæ–­æ¨ç†ã€é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDolphin R1åœ¨U2-Benchä¸Šçš„è¡¨ç°è¾¾åˆ°æ–°çš„æœ€ä½³æ°´å¹³ï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼å¤§å‹æ¨¡å‹åœ¨åŒ»å­¦æˆåƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å¤„ç†è¶…å£°å¤æ‚æ€§æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>Dolphin v1.0åŠå…¶å¢å¼ºç‰ˆDolphin R1æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡å¼è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œèåˆå¤šç§ä¸´åºŠä»»åŠ¡ã€‚</li>
<li>Dolphinç³»åˆ—é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿ç¨³å¥æ„ŸçŸ¥ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚åº”æ€§ã€‚</li>
<li>Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜è¯Šæ–­æ¨ç†ã€é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚</li>
<li>Dolphin R1åœ¨U2-Benchä¸Šçš„è¡¨ç°è¾¾åˆ°æ–°çš„æœ€ä½³æ°´å¹³ï¼Œè¶…è¿‡ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-33dd7f67af1fea8a965900d7b588d31d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105577&auth_key=1760105577-0-0-9e8c334bda29caa0fd06ec41664a4cfe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef6a53f31cec9c70dde32f26ae029686~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105584&auth_key=1760105584-0-0-df1497a8e30c725c50694cc00f036956&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be0e272c9617e5163bfd0b215a0fa2ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105591&auth_key=1760105591-0-0-cb1ab58dc8e361f47d8696c0714a1bbd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-777fe971059d982b2460c8677716db7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105598&auth_key=1760105598-0-0-19f6f6d282bac86d5c989ce89c3da362&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="K-Prism-A-Knowledge-Guided-and-Prompt-Integrated-Universal-Medical-Image-Segmentation-Model"><a href="#K-Prism-A-Knowledge-Guided-and-Prompt-Integrated-Universal-Medical-Image-Segmentation-Model" class="headerlink" title="K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical   Image Segmentation Model"></a>K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical   Image Segmentation Model</h2><p><strong>Authors:Bangwei Guo, Yunhe Gao, Meng Ye, Difei Gu, Yang Zhou, Leon Axel, Dimitris Metaxas</strong></p>
<p>Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\textit{semantic priors}$ learned from annotated datasets, (ii) $\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\textit{what}$ to segment and 2-D dense prompts indicating $\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ¨¡å‹ä»ç„¶é›¶æ•£ã€‚å®ƒä»¬é€šå¸¸åŸºäºå•ä¸€çš„çŸ¥è¯†æºè¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹ç‰¹å®šçš„ä»»åŠ¡ã€æ¨¡æ€æˆ–å™¨å®˜ã€‚è¿™ä¸ä¸´åºŠå®è·µå½¢æˆäº†é²œæ˜çš„å¯¹æ¯”ï¼Œä¸“å®¶åœ¨å®è·µä¸­èƒ½å¤Ÿæ— ç¼åœ°æ•´åˆå„ç§çŸ¥è¯†ï¼šæ¥è‡ªè®­ç»ƒçš„è§£å‰–å…ˆéªŒçŸ¥è¯†ã€åŸºäºå‚è€ƒæ¡ˆä¾‹çš„ç¤ºä¾‹æ¨ç†ä»¥åŠé€šè¿‡å®æ—¶äº’åŠ¨è¿›è¡Œçš„è¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†<strong>K-Prism</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åˆ†å‰²æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆä¸‰ç§çŸ¥è¯†èŒƒå¼æ¥åæ˜ è¿™ç§ä¸´åºŠçµæ´»æ€§ï¼šï¼ˆiï¼‰ä»æ³¨é‡Šæ•°æ®é›†ä¸­å­¦ä¹ çš„<strong>è¯­ä¹‰å…ˆéªŒ</strong>ï¼Œï¼ˆiiï¼‰æ¥è‡ªå°‘é‡å‚è€ƒç¤ºä¾‹çš„<strong>ä¸Šä¸‹æ–‡çŸ¥è¯†</strong>ï¼Œä»¥åŠï¼ˆiiiï¼‰æ¥è‡ªç”¨æˆ·è¾“å…¥ï¼ˆå¦‚ç‚¹å‡»æˆ–æ¶‚é¸¦ï¼‰çš„<strong>äº¤äº’å¼åé¦ˆ</strong>ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè¿™äº›å¼‚è´¨çš„çŸ¥è¯†æºå¯ä»¥ç¼–ç æˆåŒé‡æç¤ºè¡¨ç¤ºï¼šå®šä¹‰<strong>ä»€ä¹ˆ</strong>éœ€è¦åˆ†å‰²çš„1-Dç¨€ç–æç¤ºå’ŒæŒ‡ç¤º<strong>åœ¨å“ªé‡Œ</strong>éœ€è¦æ³¨æ„çš„2-Då¯†é›†æç¤ºï¼Œç„¶åé€šè¿‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è§£ç å™¨è¿›è¡ŒåŠ¨æ€è·¯ç”±ã€‚è¿™ç§è®¾è®¡å®ç°äº†èŒƒå¼ä¹‹é—´çš„çµæ´»åˆ‡æ¢ä»¥åŠåœ¨å„ç§ä»»åŠ¡ä¹‹é—´çš„è”åˆè®­ç»ƒï¼Œè€Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚åœ¨æ¶µç›–å¤šç§æ¨¡æ€ï¼ˆCTã€MRIã€Xå°„çº¿ã€ç—…ç†å­¦ã€è¶…å£°æ³¢ç­‰ï¼‰çš„18ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒK-Prismåœ¨è¯­ä¹‰ã€ä¸Šä¸‹æ–‡å’Œäº¤äº’å¼åˆ†å‰²ç¯å¢ƒä¸­å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚ä»£ç å°†åœ¨å‡ºç‰ˆæ—¶å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25594v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ¨¡å‹ä»ç„¶ç¢ç‰‡åŒ–ã€‚å®ƒä»¬é€šå¸¸ä»…åœ¨å•ä¸€çŸ¥è¯†æºä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡ã€æ¨¡æ€æˆ–å™¨å®˜ã€‚è¿™ä¸ä¸´åºŠå®è·µå½¢æˆé²œæ˜å¯¹æ¯”ï¼Œä¸“å®¶åœ¨å®è·µä¸­èƒ½æ— ç¼æ•´åˆå¤šæ ·åŒ–çŸ¥è¯†ï¼šæ¥è‡ªè®­ç»ƒçš„è§£å‰–å…ˆéªŒçŸ¥è¯†ã€åŸºäºå‚è€ƒæ¡ˆä¾‹çš„èŒƒä¾‹æ¨ç†å’Œé€šè¿‡å®æ—¶äº’åŠ¨çš„è¿­ä»£ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡º$\textbf{K-Prism}$ï¼Œä¸€ä¸ªç»Ÿä¸€çš„åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆä¸‰ç§çŸ¥è¯†èŒƒå¼æ¥åæ˜ è¿™ç§ä¸´åºŠçµæ´»æ€§ï¼šä»æ ‡æ³¨æ•°æ®é›†ä¸­å­¦ä¹ çš„$\textit{è¯­ä¹‰å…ˆéªŒ}$ã€ä»å°‘æ•°æ ·æœ¬å‚è€ƒä¾‹å­ä¸­çš„$\textit{ä¸Šä¸‹æ–‡çŸ¥è¯†}$ã€ä»¥åŠæ¥è‡ªç”¨æˆ·è¾“å…¥ï¼ˆå¦‚ç‚¹å‡»æˆ–æ¶‚é¸¦ï¼‰çš„$\textit{äº¤äº’å¼åé¦ˆ}$ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè¿™äº›å¼‚è´¨çš„çŸ¥è¯†æ¥æºå¯ä»¥ç¼–ç æˆä¸€ç§åŒé‡æç¤ºè¡¨ç¤ºï¼šå®šä¹‰$\textit{ä»€ä¹ˆ}$è¦åˆ†å‰²çš„1-Dç¨€ç–æç¤ºå’ŒæŒ‡ç¤º$\textit{åœ¨å“ªé‡Œ}$è¦æ³¨æ„çš„2-Då¯†é›†æç¤ºï¼Œç„¶åé€šè¿‡æ··åˆä¸“å®¶è§£ç å™¨è¿›è¡ŒåŠ¨æ€è·¯ç”±ã€‚è¿™ç§è®¾è®¡å®ç°äº†èŒƒå¼ä¹‹é—´çš„çµæ´»åˆ‡æ¢å’Œä¸åŒä»»åŠ¡ä¹‹é—´çš„è”åˆè®­ç»ƒï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚åœ¨æ¶µç›–å¤šç§æ¨¡æ€ï¼ˆCTã€MRIã€Xå…‰ã€ç—…ç†å­¦ã€è¶…å£°ç­‰ï¼‰çš„18ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒK-Prismåœ¨è¯­ä¹‰ã€ä¸Šä¸‹æ–‡å’Œäº¤äº’å¼åˆ†å‰²ç¯å¢ƒä¸­å‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å°†åœ¨å‘è¡¨æ—¶å…¬å¼€ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é‡è¦æ€§åŠå…¶ç°æœ‰æ¨¡å‹çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é€šå¸¸å±€é™äºå•ä¸€çŸ¥è¯†æºå’Œç‰¹å®šä»»åŠ¡ã€æ¨¡æ€æˆ–å™¨å®˜ã€‚</li>
<li>ä¸´åºŠå®è·µä¸­ä¸“å®¶æ— ç¼æ•´åˆå¤šæ ·åŒ–çŸ¥è¯†ã€‚</li>
<li>K-Prismæ¡†æ¶çš„æå‡ºï¼Œé€šè¿‡æ•´åˆè¯­ä¹‰å…ˆéªŒã€ä¸Šä¸‹æ–‡çŸ¥è¯†å’Œäº¤äº’å¼åé¦ˆæ¥åæ˜ ä¸´åºŠçµæ´»æ€§ã€‚</li>
<li>K-Prismé€šè¿‡åŒé‡æç¤ºè¡¨ç¤ºå’Œæ··åˆä¸“å®¶è§£ç å™¨å®ç°çŸ¥è¯†èŒƒå¼çš„çµæ´»åˆ‡æ¢å’Œè”åˆè®­ç»ƒã€‚</li>
<li>K-Prismåœ¨å¤šç§å…¬å…±æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ad317c2450116111d22236de0bae9d3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105606&auth_key=1760105606-0-0-a0f1769d566fa5c297d2bff8bdbbd42c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7007a675be3265e7d986c66bf62198de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105613&auth_key=1760105613-0-0-1e7d8d044da046fe531b91f94caa09d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8f05a18966bdf4b6a5610b3ef61cd6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105619&auth_key=1760105619-0-0-2e417c458562f4a2e93f709a3de81761&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GenVarFormer-Predicting-gene-expression-from-long-range-mutations-in-cancer"><a href="#GenVarFormer-Predicting-gene-expression-from-long-range-mutations-in-cancer" class="headerlink" title="GenVarFormer: Predicting gene expression from long-range mutations in   cancer"></a>GenVarFormer: Predicting gene expression from long-range mutations in   cancer</h2><p><strong>Authors:David Laub, Ethan Armand, Arda Pekis, Zekai Chen, Irsyad Adam, Shaun Porwal, Bing Ren, Kevin Brown, Hannah Carter</strong></p>
<p>Distinguishing the rare â€œdriverâ€ mutations that fuel cancer progression from the vast background of â€œpassengerâ€ mutations in the non-coding genome is a fundamental challenge in cancer biology. A primary mechanism that non-coding driver mutations contribute to cancer is by affecting gene expression, potentially from millions of nucleotides away. However, existing predictors of gene expression from mutations are unable to simultaneously handle interactions spanning millions of base pairs, the extreme sparsity of somatic mutations, and generalize to unseen genes. To overcome these limitations, we introduce GenVarFormer (GVF), a novel transformer-based architecture designed to learn mutation representations and their impact on gene expression. GVF efficiently predicts the effect of mutations up to 8 million base pairs away from a gene by only considering mutations and their local DNA context, while omitting the vast intermediate sequence. Using data from 864 breast cancer samples from The Cancer Genome Atlas, we demonstrate that GVF predicts gene expression with 26-fold higher correlation across samples than current models. In addition, GVF is the first model of its kind to generalize to unseen genes and samples simultaneously. Finally, we find that GVF patient embeddings are more informative than ground-truth gene expression for predicting overall patient survival in the most prevalent breast cancer subtype, luminal A. GVF embeddings and gene expression yielded concordance indices of $0.706^{\pm0.136}$ and $0.573^{\pm0.234}$, respectively. Our work establishes a new state-of-the-art for modeling the functional impact of non-coding mutations in cancer and provides a powerful new tool for identifying potential driver events and prognostic biomarkers. </p>
<blockquote>
<p>åŒºåˆ†ä¿ƒè¿›ç™Œç—‡å‘å±•çš„ç½•è§â€œé©±åŠ¨â€çªå˜ä¸éç¼–ç åŸºå› ç»„ä¸­å¤§é‡å­˜åœ¨çš„â€œè¿‡å®¢â€çªå˜æ˜¯ä¸€ä¸ªåœ¨ç™Œç—‡ç”Ÿç‰©å­¦ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚éç¼–ç é©±åŠ¨çªå˜ä¿ƒè¿›ç™Œç—‡çš„ä¸»è¦æœºåˆ¶æ˜¯é€šè¿‡å½±å“åŸºå› è¡¨è¾¾ï¼Œè¿™ç§å½±å“å¯èƒ½æ¥è‡ªæ•°ç™¾ä¸‡ä¸ªæ ¸è‹·é…¸ä¹‹å¤–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºçªå˜çš„åŸºå› è¡¨è¾¾é¢„æµ‹å™¨æ— æ³•åŒæ—¶å¤„ç†è·¨è¶Šæ•°ç™¾ä¸‡ç¢±åŸºçš„äº¤äº’ã€ä½“ç»†èƒçªå˜çš„æç«¯ç¨€ç–æ€§ï¼Œå¹¶ä¸”æ— æ³•æ¨å¹¿åˆ°æœªè§è¿‡çš„åŸºå› ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenVarFormerï¼ˆGVFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨å­¦ä¹ çªå˜çš„è¡¨ç¤ºåŠå…¶å¯¹åŸºå› è¡¨è¾¾çš„å½±å“ã€‚GVFä»…é€šè¿‡è€ƒè™‘çªå˜åŠå…¶å±€éƒ¨DNAä¸Šä¸‹æ–‡ï¼Œå°±èƒ½æœ‰æ•ˆåœ°é¢„æµ‹è·ç¦»åŸºå› é«˜è¾¾8ç™¾ä¸‡ç¢±åŸºå¯¹çš„çªå˜å½±å“ï¼ŒåŒæ—¶çœç•¥äº†å¤§é‡çš„ä¸­é—´åºåˆ—ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±çš„864ä¸ªä¹³è…ºç™Œæ ·æœ¬çš„æ•°æ®è¯æ˜ï¼ŒGVFåœ¨æ ·æœ¬ä¹‹é—´çš„é¢„æµ‹åŸºå› è¡¨è¾¾ä¸å½“å‰æ¨¡å‹ç›¸æ¯”å…·æœ‰26å€çš„é«˜ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒGVFæ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶æ¨å¹¿åˆ°æœªè§è¿‡çš„åŸºå› å’Œæ ·æœ¬çš„æ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°GVFçš„æ‚£è€…åµŒå…¥ä¿¡æ¯æ¯”æœ€å¸¸è§çš„ä¹³è…ºç™Œäºšå‹luminal Aä¸­çš„åŸºå› è¡¨è¾¾æ›´èƒ½é¢„æµ‹æ‚£è€…çš„æ€»ä½“å­˜æ´»æƒ…å†µã€‚GVFåµŒå…¥å’ŒåŸºå› è¡¨è¾¾çš„å¥‘åˆæŒ‡æ•°åˆ†åˆ«ä¸º0.706Â±0.136å’Œ0.573Â±0.234ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†éç¼–ç çªå˜åŠŸèƒ½å½±å“å»ºæ¨¡çš„æœ€æ–°æŠ€æœ¯ï¼Œå¹¶æä¾›äº†ä¸€ç§å¼ºå¤§çš„æ–°å·¥å…·ï¼Œç”¨äºè¯†åˆ«æ½œåœ¨çš„é©±åŠ¨äº‹ä»¶å’Œé¢„åç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25573v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯†åˆ«æ¨åŠ¨ç™Œç—‡å‘å±•çš„ç½•è§â€œé©±åŠ¨â€çªå˜ä¸å¤§é‡éç¼–ç åŸºå› ä¸­çš„â€œä¹˜å®¢â€çªå˜ä¹‹é—´çš„åŒºåˆ†æ˜¯ç™Œç—‡ç”Ÿç‰©å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚éç¼–ç é©±åŠ¨çªå˜ä¸»è¦é€šè¿‡å½±å“åŸºå› è¡¨è¾¾æ¥ä¿ƒè¿›ç™Œç—‡å‘å±•ï¼Œè¿™ç§å½±å“å¯èƒ½æ¥è‡ªæ•°ç™¾ä¸‡ä¸ªæ ¸è‹·é…¸çš„è·ç¦»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå› è¡¨è¾¾çªå˜é¢„æµ‹æ¨¡å‹æ— æ³•åŒæ—¶å¤„ç†è·¨è¶Šæ•°ç™¾ä¸‡ç¢±åŸºçš„ç›¸äº’ä½œç”¨ã€ä½“ç»†èƒçªå˜çš„æç«¯ç¨€ç–æ€§ä»¥åŠå¯¹æœªè§åŸºå› çš„æ¦‚æ‹¬æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenVarFormerï¼ˆGVFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨å­¦ä¹ çªå˜è¡¨ç¤ºåŠå…¶å¯¹åŸºå› è¡¨è¾¾çš„å½±å“ã€‚GVFå¯ä»¥æœ‰æ•ˆåœ°é¢„æµ‹è·ç¦»åŸºå› é•¿è¾¾8ç™¾ä¸‡ç¢±åŸºå¤„çš„çªå˜çš„å½±å“ï¼Œä»…è€ƒè™‘çªå˜å’Œå…¶å±€éƒ¨DNAä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶çœç•¥å¤§é‡ä¸­é—´åºåˆ—ã€‚é€šè¿‡ä½¿ç”¨æ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±çš„864ä¸ªä¹³è…ºç™Œæ ·æœ¬çš„æ•°æ®ï¼Œæˆ‘ä»¬è¯æ˜äº†GVFåœ¨æ ·æœ¬ä¹‹é—´çš„é¢„æµ‹åŸºå› è¡¨è¾¾ä¸å½“å‰æ¨¡å‹ç›¸æ¯”å…·æœ‰26å€æ›´é«˜çš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒGVFæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶æ¦‚æ‹¬æœªè§åŸºå› å’Œæ ·æœ¬çš„æ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°GVFçš„æ‚£è€…åµŒå…¥ä¿¡æ¯æ¯”çœŸå®çš„åŸºå› è¡¨è¾¾å¯¹æœ€å¸¸è§çš„ä¹³è…ºç™Œäºšå‹luminal Açš„æ€»ä½“æ‚£è€…ç”Ÿå­˜æœŸçš„é¢„æµ‹æ›´å…·å‚è€ƒä»·å€¼ã€‚GVFåµŒå…¥å’ŒåŸºå› è¡¨è¾¾çš„å¥‘åˆæŒ‡æ•°åˆ†åˆ«ä¸ºÂ±0.136çš„0.706å’ŒÂ±0.234çš„0.573ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå»ºç«‹éç¼–ç çªå˜åŠŸèƒ½å½±å“çš„æœ€æ–°æ ‡å‡†æä¾›äº†æœ‰åŠ›çš„å·¥å…·ï¼Œä¸ºè¯†åˆ«æ½œåœ¨çš„é©±åŠ¨äº‹ä»¶å’Œé¢„åç”Ÿç‰©æ ‡å¿—ç‰©æä¾›äº†å¼ºå¤§çš„æ–°å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒºåˆ†æ¨åŠ¨ç™Œç—‡å‘å±•çš„ç½•è§â€œé©±åŠ¨â€çªå˜ä¸éç¼–ç åŸºå› ä¸­çš„å¤§é‡â€œä¹˜å®¢â€çªå˜æ˜¯ç™Œç—‡ç”Ÿç‰©å­¦çš„åŸºæœ¬æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>éç¼–ç é©±åŠ¨çªå˜é€šè¿‡å½±å“åŸºå› è¡¨è¾¾ä¿ƒè¿›ç™Œç—‡å‘å±•ï¼Œè¿™ç§å½±å“å¯èƒ½è¿œç¦»åŸºå› æ•°ç™¾ä¸‡ä¸ªæ ¸è‹·é…¸ã€‚</li>
<li>ç°æœ‰é¢„æµ‹æ¨¡å‹æ— æ³•åŒæ—¶å¤„ç†è·¨è¶Šå¤§é‡ç¢±åŸºçš„ç›¸äº’ä½œç”¨ã€çªå˜çš„æç«¯ç¨€ç–æ€§ä»¥åŠå¯¹æœªè§åŸºå› çš„æ¦‚æ‹¬æ€§ã€‚</li>
<li>GenVarFormerï¼ˆGVFï¼‰æ˜¯ä¸€ç§æ–°å‹åŸºäºtransformerçš„æ¶æ„ï¼Œèƒ½æœ‰æ•ˆé¢„æµ‹çªå˜å¯¹åŸºå› è¡¨è¾¾çš„å½±å“ï¼Œå³ä½¿çªå˜ä½ç½®è·ç¦»åŸºå› é•¿è¾¾8ç™¾ä¸‡ç¢±åŸºå¤„ã€‚</li>
<li>GVFåœ¨ä¹³è…ºç™Œæ ·æœ¬æ•°æ®ä¸Šè¡¨ç°å‡ºæ¯”ç°æœ‰æ¨¡å‹æ›´é«˜çš„åŸºå› è¡¨è¾¾é¢„æµ‹ç›¸å…³æ€§ã€‚</li>
<li>GVFèƒ½å¤ŸåŒæ—¶æ¦‚æ‹¬æœªè§åŸºå› å’Œæ ·æœ¬ï¼Œæ˜¯é¦–ä¸ªå…·å¤‡æ­¤èƒ½åŠ›çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7be60160f0bdc5164a5516128a78cbe5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105627&auth_key=1760105627-0-0-700a558a4d8b369692a313bb6d23ffc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59980a03ff1c20f87685cbd221b9eeb7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105634&auth_key=1760105634-0-0-2b3d202582b00b530bbe96c37d16db7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6466737705fd44e818a8c5e1cc2b3032~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105640&auth_key=1760105640-0-0-2452e329455261da4772c9571fff23a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Radiologyâ€™s-Last-Exam-RadLE-Benchmarking-Frontier-Multimodal-AI-Against-Human-Experts-and-a-Taxonomy-of-Visual-Reasoning-Errors-in-Radiology"><a href="#Radiologyâ€™s-Last-Exam-RadLE-Benchmarking-Frontier-Multimodal-AI-Against-Human-Experts-and-a-Taxonomy-of-Visual-Reasoning-Errors-in-Radiology" class="headerlink" title="Radiologyâ€™s Last Exam (RadLE): Benchmarking Frontier Multimodal AI   Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology"></a>Radiologyâ€™s Last Exam (RadLE): Benchmarking Frontier Multimodal AI   Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology</h2><p><strong>Authors:Suvrankar Datta, Divya Buchireddygari, Lakshmi Vennela Chowdary Kaza, Mrudula Bhalke, Kautik Singh, Ayush Pandey, Sonit Sai Vasipalli, Upasana Karnwal, Hakikat Bir Singh Bhatti, Bhavya Ratan Maroo, Sanjana Hebbar, Rahul Joseph, Gurkawal Kaur, Devyani Singh, Akhil V, Dheeksha Devasya Shama Prasad, Nishtha Mahajan, Ayinaparthi Arisha, Rajesh Vanagundi, Reet Nandy, Kartik Vuthoo, Snigdhaa Rajvanshi, Nikhileswar Kondaveeti, Suyash Gunjal, Rishabh Jain, Rajat Jain, Anurag Agrawal</strong></p>
<p>Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level â€œspot diagnosisâ€ cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development. </p>
<blockquote>
<p>é€šç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œæ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ä¸´åºŠåŒ»ç”Ÿå’Œæ‚£è€…ç”¨äºé€šè¿‡å¹¿æ³›å¯ç”¨çš„é¢å‘æ¶ˆè´¹è€…çš„èŠå¤©æœºå™¨äººè¿›è¡ŒåŒ»å­¦å›¾åƒè§£è¯»ã€‚å¤§å¤šæ•°å£°ç§°ä¸“å®¶çº§æ€§èƒ½çš„è¯„ä¼°éƒ½æ˜¯åœ¨åŒ…å«å¸¸è§ç—…ç†çš„å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚å¯¹äºå‰æ²¿æ¨¡å‹åœ¨å›°éš¾è¯Šæ–­ç—…ä¾‹ä¸Šçš„ä¸¥æ ¼è¯„ä¼°ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒ…å«50ä¸ªä¸“å®¶çº§â€œå³æ—¶è¯Šæ–­â€ç—…ä¾‹çš„è¯•ç‚¹åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠå¤šç§æˆåƒæ¨¡å¼ï¼Œä»¥è¯„ä¼°å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹ä¸è‘£äº‹ä¼šè®¤è¯æ”¾å°„ç§‘åŒ»å¸ˆå’Œæ”¾å°„å­¦å®ä¹ ç”Ÿçš„è¡¨ç°ã€‚ä¸ºäº†åæ˜ çœŸå®ä¸–ç•Œçš„ä½¿ç”¨æƒ…å†µï¼Œæˆ‘ä»¬æµ‹è¯•äº†äº”ä¸ªå—æ¬¢è¿çš„å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ¨ç†æ¨¡å¼ï¼Œè¿™äº›æ¨¡å‹åŒ…æ‹¬é€šè¿‡å…¶åŸç”Ÿç½‘ç»œæ¥å£è¿›è¡Œçš„OpenAI o3ã€OpenAI GPT-5ã€Gemini 2.5 Proã€Grok-4å’ŒClaude Opus 4.1ã€‚å‡†ç¡®æ€§ç”±ç›²ä¸“å®¶è¯„åˆ†ï¼Œå¹¶åœ¨ä¸‰æ¬¡ç‹¬ç«‹è¿è¡Œä¸­è¿›è¡Œäº†å¯é‡å¤æ€§çš„è¯„ä¼°ã€‚GPT-5è¿˜åœ¨å„ç§æ¨ç†æ¨¡å¼ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚è¯„ä¼°äº†æ¨ç†è´¨é‡é”™è¯¯ï¼Œå¹¶å®šä¹‰äº†è§†è§‰æ¨ç†é”™è¯¯çš„åˆ†ç±»ã€‚è‘£äº‹ä¼šè®¤è¯çš„æ”¾å°„ç§‘åŒ»å¸ˆçš„è¯Šæ–­å‡†ç¡®æ€§æœ€é«˜ï¼ˆ83%ï¼‰ï¼Œä¼˜äºå®ä¹ ç”Ÿï¼ˆ45%ï¼‰å’Œæ‰€æœ‰AIæ¨¡å‹ï¼ˆGPT-5è¡¨ç°æœ€ä½³ï¼š30%ï¼‰ã€‚GPT-5å’Œo3çš„å¯é æ€§ç›¸å½“é«˜ï¼ŒGemini 2.5 Proå’ŒGrok-4çš„å¯é æ€§å±ä¸­ç­‰æ°´å¹³ï¼Œè€ŒClaude Opus 4.1çš„å¯é æ€§è¾ƒå·®ã€‚è¿™äº›ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯Šæ–­ç—…ä¾‹ä¸­ï¼Œå…ˆè¿›çš„å‰æ²¿æ¨¡å‹ä¸æ”¾å°„ç§‘åŒ»ç”Ÿç›¸æ¯”ä»æœ‰å¾ˆå¤§å·®è·ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•çªæ˜¾äº†é€šç”¨äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„å½“å‰å±€é™æ€§ï¼Œå¹¶è­¦å‘Šä¸è¦åœ¨æ²¡æœ‰ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œä¸´åºŠä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å¯¹æ¨ç†è½¨è¿¹çš„å®šæ€§åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„è§†è§‰æ¨ç†é”™è¯¯åˆ†ç±»ï¼Œä»¥æ›´å¥½åœ°äº†è§£AIæ¨¡å‹çš„å¤±è´¥æ¨¡å¼ï¼Œä¸ºè¯„ä¼°æ ‡å‡†æä¾›ä¿¡æ¯å¹¶æŒ‡å¯¼æ›´ç¨³å¥çš„æ¨¡å‹å¼€å‘ã€‚</p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25559v1">PDF</a> 29 pages, 7 figures, 7 tables, includes Annexure (1). Part of the   work accepted at RSNA 2025 (Cutting Edge Oral Presentation)</p>
<p><strong>Summary</strong><br>    é€šç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰é€šè¿‡é¢å‘å…¬ä¼—çš„èŠå¤©æœºå™¨äººè¢«å¹¿æ³›ç”¨äºåŒ»å­¦å›¾åƒè§£è¯»ï¼Œä½†å…¶åœ¨å¤æ‚è¯Šæ–­æ¡ˆä¾‹ä¸­çš„è¡¨ç°ä»å¾…ä¸¥æ ¼è¯„ä¼°ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŒ…å«50ä¸ªä¸“å®¶çº§â€œå³æ—¶è¯Šæ–­â€æ¡ˆä¾‹çš„è¯•ç‚¹åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å‰æ²¿AIæ¨¡å‹åœ¨å¤šç§æˆåƒæ¨¡æ€ä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸è®¤è¯æ”¾å°„å­¦å®¶å’Œæ”¾å°„å­¦å®ä¹ ç”Ÿè¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-5åœ¨AIæ¨¡å‹ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œä½†æ”¾å°„å­¦ä¸“å®¶ä»è¡¨ç°å‡ºæœ€é«˜çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚è¿™è¡¨æ˜åœ¨å¤æ‚è¯Šæ–­æ¡ˆä¾‹ä¸­ï¼Œå…ˆè¿›çš„å‰æ²¿æ¨¡å‹ä¸æ”¾å°„å¸ˆç›¸æ¯”ä»æœ‰å¾ˆå¤§å·®è·ã€‚æœ¬ç ”ç©¶è­¦å‘Šè¯´ï¼Œè¿™äº›AIæ¨¡å‹åœ¨æœªç»ç›‘ç£çš„ä¸´åºŠä½¿ç”¨æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚åŒæ—¶æä¾›äº†äººå·¥æ™ºèƒ½æ¨¡å‹æ¨ç†ç—•è¿¹çš„å®šæ€§åˆ†æå¹¶æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„è§†è§‰æ¨ç†é”™è¯¯åˆ†ç±»æ³•ï¼Œæ—¨åœ¨æ›´å¥½åœ°äº†è§£å…¶å¤±è´¥æ¨¡å¼ï¼Œä¸ºè¯„ä¼°æ ‡å‡†æä¾›ä¿¡æ¯å¹¶æŒ‡å¯¼æ›´ç¨³å¥çš„æ¨¡å‹å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€AIç³»ç»Ÿé€šè¿‡æ¶ˆè´¹è€…èŠå¤©æœºå™¨äººå¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒè§£è¯»ã€‚</li>
<li>åœ¨å¤æ‚è¯Šæ–­æ¡ˆä¾‹ä¸­ï¼Œå‰æ²¿AIæ¨¡å‹çš„è¡¨ç°ä»å¾…ä¸¥æ ¼è¯„ä¼°ã€‚</li>
<li>æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«å¤šç§æˆåƒæ¨¡æ€çš„ä¸“å®¶çº§è¯Šæ–­æ¡ˆä¾‹çš„è¯•ç‚¹åŸºå‡†æµ‹è¯•ã€‚</li>
<li>GPT-5åœ¨å‰æ²¿AIæ¨¡å‹ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œä½†ä»è½åäºè®¤è¯æ”¾å°„å¸ˆçš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>AIæ¨¡å‹åœ¨æ¨ç†è´¨é‡æ–¹é¢å­˜åœ¨é”™è¯¯ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè§†è§‰æ¨ç†é”™è¯¯çš„åˆ†ç±»æ³•ä»¥æ›´å¥½åœ°ç†è§£å…¶å¤±è´¥æ¨¡å¼ã€‚</li>
<li>è­¦å‘Šè¯´ï¼Œè¿™äº›AIæ¨¡å‹åœ¨æœªç»ç›‘ç£çš„ä¸´åºŠä½¿ç”¨æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d01d83be64f9a13581e7e942c9dc6ac4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105649&auth_key=1760105649-0-0-11cefaa54668b45e077bb590bb93d0d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6a20703b0b9a8ccf998f11a1d110a11~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105657&auth_key=1760105657-0-0-cdeffea8470233b12c661ce5b2f6429c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16fd8d5e491dc8407c5aaf0dfb15abea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105663&auth_key=1760105663-0-0-e58b560cdf9490426c5c4918469af9e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5bca2e58fe54ce317d4564c95850f12~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105670&auth_key=1760105670-0-0-fac0eb185c6d3428239ce335b0ad5d37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Evaluating-Foundation-Models-with-Pathological-Concept-Learning-for-Kidney-Cancer"><a href="#Evaluating-Foundation-Models-with-Pathological-Concept-Learning-for-Kidney-Cancer" class="headerlink" title="Evaluating Foundation Models with Pathological Concept Learning for   Kidney Cancer"></a>Evaluating Foundation Models with Pathological Concept Learning for   Kidney Cancer</h2><p><strong>Authors:Shangqi Gao, Sihan Wang, Yibo Gao, Boming Wang, Xiahai Zhuang, Anne Warren, Grant Stewart, James Jones, Mireia Crispin-Ortuzar</strong></p>
<p>To evaluate the translational capabilities of foundation models, we develop a pathological concept learning approach focused on kidney cancer. By leveraging TNM staging guidelines and pathology reports, we build comprehensive pathological concepts for kidney cancer. Then, we extract deep features from whole slide images using foundation models, construct pathological graphs to capture spatial correlations, and trained graph neural networks to identify these concepts. Finally, we demonstrate the effectiveness of this approach in kidney cancer survival analysis, highlighting its explainability and fairness in identifying low- and high-risk patients. The source code has been released by <a target="_blank" rel="noopener" href="https://github.com/shangqigao/RadioPath">https://github.com/shangqigao/RadioPath</a>. </p>
<blockquote>
<p>ä¸ºäº†è¯„ä¼°åŸºç¡€æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é’ˆå¯¹è‚¾ç™Œçš„ç—…ç†æ€§æ¦‚å¿µå­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬å€ŸåŠ©TNMåˆ†æœŸæŒ‡å—å’Œç—…ç†æŠ¥å‘Šï¼Œä¸ºè‚¾ç™Œæ„å»ºäº†å…¨é¢çš„ç—…ç†æ€§æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºç¡€æ¨¡å‹ä»å…¨å¹»ç¯ç‰‡å›¾åƒä¸­æå–æ·±åº¦ç‰¹å¾ï¼Œæ„å»ºç—…ç†æ€§å›¾è¡¨ä»¥æ•è·ç©ºé—´ç›¸å…³æ€§ï¼Œå¹¶è®­ç»ƒå›¾ç¥ç»ç½‘ç»œä»¥è¯†åˆ«è¿™äº›æ¦‚å¿µã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡è‚¾ç™Œç”Ÿå­˜åˆ†æè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é‡ç‚¹å¼ºè°ƒäº†å…¶åœ¨è¯†åˆ«ä½å±å’Œé«˜å±æ‚£è€…æ—¶çš„å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§ã€‚æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/shangqigao/RadioPath%E3%80%82">https://github.com/shangqigao/RadioPathã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25552v1">PDF</a> Best Paper Award at MICCAI AMAI 2025</p>
<p><strong>Summary</strong><br>åŸºäºè‚¾è„ç™Œçš„TNMåˆ†æœŸæŒ‡å—å’Œç—…ç†æŠ¥å‘Šï¼Œè¯¥ç ”ç©¶æå‡ºä¸€ç§é’ˆå¯¹åŸºç¡€æ¨¡å‹çš„ç—…ç†æ€§æ¦‚å¿µå­¦ä¹ è¯„ä»·ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä½¿ç”¨å…¨ç‰‡å›¾åƒè¿›è¡Œæ·±åº¦ç‰¹å¾æå–ï¼Œå»ºç«‹ç—…ç†æ€§å›¾è°±ä»¥æ•æ‰ç©ºé—´å…³è”ï¼Œå¹¶é€šè¿‡è®­ç»ƒå›¾ç¥ç»ç½‘ç»œæ¥è¯†åˆ«è¿™äº›æ¦‚å¿µã€‚åœ¨è‚¾è„ç™Œç”Ÿå­˜åˆ†æä¸­éªŒè¯äº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨é‰´åˆ«ä½ã€é«˜é£é™©æ‚£è€…æ—¶å±•ç°äº†è§£é‡Šæ€§å’Œå…¬å¹³æ€§ã€‚ç ”ç©¶ç›¸å…³æºä»£ç å¯é€šè¿‡ç‰¹å®šé“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é‡‡ç”¨ç—…ç†æ€§æ¦‚å¿µå­¦ä¹ è¯„ä¼°åŸºç¡€æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ã€‚</li>
<li>åŸºäºè‚¾è„ç™Œçš„TNMåˆ†æœŸæŒ‡å—å’Œç—…ç†æŠ¥å‘Šï¼Œæ„å»ºäº†å…¨é¢çš„ç—…ç†æ€§æ¦‚å¿µã€‚</li>
<li>åˆ©ç”¨å…¨ç‰‡å›¾åƒæå–æ·±åº¦ç‰¹å¾å¹¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¿›è¡Œè¯†åˆ«ã€‚</li>
<li>é€šè¿‡å»ºç«‹ç—…ç†æ€§å›¾è°±æ•æ‰ç©ºé—´å…³è”ã€‚</li>
<li>åœ¨è‚¾è„ç™Œç”Ÿå­˜åˆ†æä¸­éªŒè¯äº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ç­–ç•¥å…·å¤‡è§£é‡Šæ€§å’Œå…¬å¹³æ€§ï¼Œåœ¨é‰´åˆ«ä½ã€é«˜é£é™©æ‚£è€…æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b633b5068d09f16bbb2f9a507a9c1ca1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105677&auth_key=1760105677-0-0-7a263b9cf650f7d1a524f5fabe80f034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e7fcf30c5c6c3622c44468a8ca77745~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105684&auth_key=1760105684-0-0-ee7e10c7afacd79eb92da82c9313d3ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f48e80278d698731ed08c675f3471c3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105691&auth_key=1760105691-0-0-6cfc6e586712209a3a584702d2df0254&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8426c14b662fe59a57a116da5ad9e540~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105697&auth_key=1760105697-0-0-eb3986fefa4e719dfa11a44b590901ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-912bc61315242813efcae6b08a845bb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105704&auth_key=1760105704-0-0-aab6f409d6fbd54c00c932224e0d0977&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement"><a href="#RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement" class="headerlink" title="RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement"></a>RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement</h2><p><strong>Authors:Libo Zhu, Zihan Zhou, Xiaoyang Liu, Weihang Zhang, Keyu Shi, Yifan Fu, Yulun Zhang</strong></p>
<p>Capturing screens is now routine in our everyday lives. But the photographs of emissive displays are often influenced by the flicker-banding (FB), which is alternating bright%u2013dark stripes that arise from temporal aliasing between a cameraâ€™s rolling-shutter readout and the displayâ€™s brightness modulation. Unlike moire degradation, which has been extensively studied, the FB remains underexplored despite its frequent and severe impact on readability and perceived quality. We formulate FB removal as a dedicated restoration task and introduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement, RIFLE, a diffusion-based framework designed to remove FB while preserving fine details. We propose the flicker-banding prior estimator (FPE) that predicts key banding attributes and injects it into the restoration network. Additionally, Masked Loss (ML) is proposed to concentrate supervision on banded regions without sacrificing global fidelity. To overcome data scarcity, we provide a simulation pipeline that synthesizes FB in the luminance domain with stochastic jitter in banding angle, banding spacing, and banding width. Feathered boundaries and sensor noise are also applied for a more realistic simulation. For evaluation, we collect a paired real-world FB dataset with pixel-aligned banding-free references captured via long exposure. Across quantitative metrics and visual comparisons on our real-world dataset, RIFLE consistently outperforms recent image reconstruction baselines from mild to severe flicker-banding. To the best of our knowledge, it is the first work to research the simulation and removal of FB. Our work establishes a great foundation for subsequent research in both the dataset construction and the removal model design. Our dataset and code will be released soon. </p>
<blockquote>
<p>å±å¹•æˆªå›¾ç°åœ¨å·²ç»æˆä¸ºæˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è§„æ“ä½œã€‚ç„¶è€Œï¼Œå‘å…‰æ˜¾ç¤ºå±çš„ç…§ç‰‡å¾€å¾€ä¼šå—åˆ°é¢‘é—ªæ¡çº¹ï¼ˆFBï¼‰çš„å½±å“ï¼Œé¢‘é—ªæ¡çº¹æ˜¯ç”±äºç›¸æœºæ»šåŠ¨å¿«é—¨è¯»å‡ºä¸æ˜¾ç¤ºå±äº®åº¦è°ƒåˆ¶ä¹‹é—´çš„æ—¶é—´æ··å è€Œäº§ç”Ÿçš„æ˜æš—äº¤æ›¿æ¡çº¹ã€‚ä¸æ‘©å°”çº¹é€€åŒ–ï¼ˆå·²è¢«å¹¿æ³›ç ”ç©¶ï¼‰ä¸åŒï¼Œå°½ç®¡é¢‘é—ªæ¡çº¹ç»å¸¸å¯¹å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡äº§ç”Ÿä¸¥é‡çš„å½±å“ï¼Œä½†å®ƒä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚æˆ‘ä»¬å°†é¢‘é—ªæ¡çº¹ç§»é™¤åˆ¶å®šä¸ºä¸€ä¸ªä¸“é—¨çš„æ¢å¤ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†é€šè¿‡æ½œåœ¨æ‰©æ•£å¢å¼ºå»é™¤å›¾åƒé¢‘é—ªæ¡çº¹ï¼ˆRIFLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œæ—¨åœ¨å»é™¤é¢‘é—ªæ¡çº¹åŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†é¢‘é—ªæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰ï¼Œå®ƒå¯ä»¥é¢„æµ‹å…³é”®çš„æ¡çº¹å±æ€§å¹¶å°†å…¶æ³¨å…¥æ¢å¤ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ©è†œæŸå¤±ï¼ˆMLï¼‰ï¼Œä»¥å°†ç›‘ç£é›†ä¸­åœ¨å¸¦çŠ¶åŒºåŸŸä¸Šï¼Œè€Œä¸ç‰ºç‰²å…¨å±€ä¿çœŸåº¦ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåˆæˆé¢‘é—ªæ¡çº¹çš„æ¨¡æ‹Ÿç®¡é“ï¼Œè¯¥ç®¡é“åœ¨äº®åº¦åŸŸä¸­åˆæˆé¢‘é—ªæ¡çº¹ï¼Œå¹¶å¸¦æœ‰æ¡çº¹è§’åº¦ã€æ¡çº¹é—´è·å’Œæ¡çº¹å®½åº¦ä¸­çš„éšæœºæŠ–åŠ¨ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†æ¸å˜çš„è¾¹ç•Œå’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œä»¥è¿›è¡Œæ›´ç°å®çš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªé…å¯¹çš„çœŸå®ä¸–ç•Œé¢‘é—ªæ¡çº¹æ•°æ®é›†ï¼Œé€šè¿‡é•¿æ—¶é—´æ›å…‰æ•è·äº†å…·æœ‰åƒç´ å¯¹é½çš„æ— æ¡çº¹å‚è€ƒã€‚åœ¨æˆ‘ä»¬çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯åœ¨å®šé‡æŒ‡æ ‡è¿˜æ˜¯è§†è§‰æ¯”è¾ƒæ–¹é¢ï¼ŒRIFLEåœ¨è½»å¾®è‡³ä¸¥é‡çš„é¢‘é—ªæ¡çº¹æƒ…å†µä¸‹å‡ä¼˜äºæœ€è¿‘çš„å›¾åƒé‡å»ºåŸºçº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€é¡¹ç ”ç©¶é¢‘é—ªæ¡çº¹æ¨¡æ‹Ÿå’Œå»é™¤çš„å·¥ä½œã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåç»­ç ”ç©¶åœ¨æ•°æ®é›†æ„å»ºå’Œå»é™¤æ¨¡å‹è®¾è®¡æ–¹é¢å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24644v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶äº†å±å¹•æˆªå›¾ä¸­çš„é—ªçƒæ¡çº¹ï¼ˆFBï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶RIFLEï¼Œç”¨äºå»é™¤FBåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æ–‡ç« ä»‹ç»äº†é—ªçƒæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰å’ŒMasked Lossï¼ˆMLï¼‰ï¼Œå¹¶æä¾›äº†åˆæˆFBçš„ä»¿çœŸç®¡é“ã€‚RIFLEåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºåç»­çš„æ•°æ®é›†æ„å»ºå’Œå»é™¤æ¨¡å‹è®¾è®¡ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é—ªçƒæ¡çº¹ï¼ˆFBï¼‰æ˜¯å±å¹•æˆªå›¾å¸¸è§çš„é—®é¢˜ï¼Œå¯¹å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡äº§ç”Ÿä¸¥é‡å½±å“ã€‚</li>
<li>RIFLEæ¡†æ¶æ—¨åœ¨å»é™¤FBï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚ï¼Œæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥é—ªçƒæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰é¢„æµ‹å…³é”®æ¡çº¹å±æ€§å¹¶æ³¨å…¥æ¢å¤ç½‘ç»œã€‚</li>
<li>æå‡ºMasked Lossï¼ˆMLï¼‰é›†ä¸­ç›‘ç£å¸¦çŠ¶åŒºåŸŸï¼Œè€Œä¸ç‰ºç‰²å…¨å±€ä¿çœŸåº¦ã€‚</li>
<li>æä¾›åˆæˆFBçš„ä»¿çœŸç®¡é“ï¼ŒåŒ…æ‹¬äº®åº¦åŸŸåˆæˆå’ŒéšæœºæŠ–åŠ¨å¸¦çŠ¶è§’åº¦ã€å¸¦çŠ¶é—´è·å’Œå¸¦çŠ¶å®½åº¦ã€‚</li>
<li>æ”¶é›†çœŸå®ä¸–ç•ŒFBæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ¯”è¾ƒè¯„ä¼°RIFLEæ€§èƒ½ã€‚</li>
<li>RIFLEåœ¨è½»å¾®è‡³ä¸¥é‡çš„é—ªçƒæ¡çº¹æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºç›¸å…³ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d902db98d4c36b6f8e1f7832eac54800~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105711&auth_key=1760105711-0-0-a4777adddc793d5a071b76b6b7acca23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4feadd52ad0541ebf2adb38d87a2599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105718&auth_key=1760105718-0-0-b2012e1f6e02a280df9b6e0f1c5633da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7397039c6acba70dbac88ac52b82cea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105725&auth_key=1760105725-0-0-c8ebcd28f81c71a92e12edd728182659&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e508562a7e2e0f7a5d81d636f244395~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105732&auth_key=1760105732-0-0-607c9e79d0ee6d4a04bc25db3368c0d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55df5ffaa44c89b39a3d8e29070ab48c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105738&auth_key=1760105738-0-0-05cbbe862d6d2cd83ed697409664f997&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EWC-Guided-Diffusion-Replay-for-Exemplar-Free-Continual-Learning-in-Medical-Imaging"><a href="#EWC-Guided-Diffusion-Replay-for-Exemplar-Free-Continual-Learning-in-Medical-Imaging" class="headerlink" title="EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in   Medical Imaging"></a>EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in   Medical Imaging</h2><p><strong>Authors:Anoushka Harit, William Prew, Zhongtian Sun, Florian Markowetz</strong></p>
<p>Medical imaging foundation models must adapt over time, yet full retraining is often blocked by privacy constraints and cost. We present a continual learning framework that avoids storing patient exemplars by pairing class conditional diffusion replay with Elastic Weight Consolidation. Using a compact Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by more than 30% relative to DER\texttt{++}, and approaches joint training at 0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect forgetting to two measurable factors: fidelity of replay and Fisher weighted parameter drift, highlighting the complementary roles of replay diffusion and synaptic stability. The results indicate a practical route for scalable, privacy aware continual adaptation of clinical imaging models. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåŸºç¡€æ¨¡å‹å¿…é¡»éšæ—¶é—´è¿›è¡Œé€‚åº”ï¼Œç„¶è€Œç”±äºéšç§çº¦æŸå’Œæˆæœ¬è€ƒè™‘ï¼Œå…¨é¢é‡æ–°è®­ç»ƒé€šå¸¸ä¼šè¢«é˜»æ­¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç±»åˆ«æ¡ä»¶æ‰©æ•£å›æ”¾å’Œå¼¹æ€§æƒé‡æ•´åˆï¼Œé¿å…äº†å­˜å‚¨æ‚£è€…æ ·æœ¬ã€‚ä½¿ç”¨ç´§å‡‘çš„æ„¿æ™¯è½¬æ¢å™¨ä¸»å¹²ç½‘ï¼Œæˆ‘ä»¬åœ¨å…«ä¸ªMedMNIST v2ä»»åŠ¡å’ŒCheXpertä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨CheXpertä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†0.851çš„AUROCï¼Œä¸DER++ç›¸æ¯”ï¼Œå‡å°‘äº†è¶…è¿‡30%çš„é—å¿˜ï¼Œå¹¶æ¥è¿‘è”åˆè®­ç»ƒçš„0.869 AUROCï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆå’Œéšç§ä¿æŠ¤ã€‚åˆ†æå°†é—å¿˜ä¸ä¸¤ä¸ªå¯è¡¡é‡çš„å› ç´ è”ç³»èµ·æ¥ï¼šå›æ”¾çš„ä¿çœŸåº¦å’ŒFisheråŠ æƒå‚æ•°æ¼‚ç§»ï¼Œçªå‡ºäº†å›æ”¾æ‰©æ•£å’Œçªè§¦ç¨³å®šæ€§çš„äº’è¡¥ä½œç”¨ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™æ˜¯ä¸€æ¡é’ˆå¯¹ä¸´åºŠå½±åƒæ¨¡å‹çš„å¯æ‰©å±•ã€æ³¨é‡éšç§çš„æŒç»­é€‚åº”çš„å®é™…é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23906v1">PDF</a> Accepted at AI That Keeps Up: NeurIPS 2025 Workshop on Continual and   Compatible Foundation Model Updates</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹éœ€è¦éšæ—¶é—´é€‚åº”ï¼Œä½†å—éšç§çº¦æŸå’Œæˆæœ¬é™åˆ¶ï¼Œå…¨é¢å†è®­ç»ƒä¸å¯è¡Œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é…å¯¹ç±»åˆ«æ¡ä»¶æ‰©æ•£å›æ”¾å’Œå¼¹æ€§æƒé‡æ•´åˆï¼Œé¿å…å­˜å‚¨æ‚£è€…æ ·æœ¬ã€‚ä½¿ç”¨ç´§å‡‘çš„Vision Transformerä¸»å¹²ç½‘ï¼Œæˆ‘ä»¬åœ¨å…«ä¸ªMedMNIST v2ä»»åŠ¡å’ŒCheXpertä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨CheXpertä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†0.851çš„AUROCï¼Œä¸DER++ç›¸æ¯”å‡å°‘äº†è¶…è¿‡30%çš„é—å¿˜ï¼Œå¹¶æ¥è¿‘è”åˆè®­ç»ƒçš„0.869 AUROCï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆå’Œéšç§ä¿æŠ¤ã€‚åˆ†æå°†é—å¿˜ä¸å›æ”¾ä¿çœŸåº¦å’ŒFisheråŠ æƒå‚æ•°æ¼‚ç§»ä¸¤ä¸ªå¯æµ‹å› ç´ è”ç³»èµ·æ¥ï¼Œçªå‡ºäº†å›æ”¾æ‰©æ•£å’Œçªè§¦ç¨³å®šæ€§çš„äº’è¡¥ä½œç”¨ã€‚ç»“æœæŒ‡ç¤ºäº†ä¸´åºŠæˆåƒæ¨¡å‹çš„å¯æ‰©å±•ã€éšç§æ„è¯†æŒç»­é€‚åº”çš„å®é™…é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹éœ€è¦é€‚åº”å˜åŒ–ï¼Œä½†å…¨é¢å†è®­ç»ƒå—é™ã€‚</li>
<li>æå‡ºçš„æŒç»­å­¦ä¹ æ¡†æ¶é¿å…äº†å­˜å‚¨æ‚£è€…æ ·æœ¬ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†ç±»åˆ«æ¡ä»¶æ‰©æ•£å›æ”¾å’Œå¼¹æ€§æƒé‡æ•´åˆã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨CheXpertä¸ŠAUROCè¾¾åˆ°0.851ã€‚</li>
<li>ä¸DER++ç›¸æ¯”ï¼Œå‡å°‘äº†è¶…è¿‡30%çš„é—å¿˜ã€‚</li>
<li>åˆ†æè¡¨æ˜é—å¿˜ä¸å›æ”¾ä¿çœŸåº¦å’Œå‚æ•°æ¼‚ç§»æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3620af3df8d45fc20b53de24e8dbf7f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105747&auth_key=1760105747-0-0-1bc43d2210891bfb0d09adf85b9ed5dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9fdc08e2aa6fb2746d46f1accb04ab74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105754&auth_key=1760105754-0-0-62bacc0232c5f205148a5799c242094a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98c37c4a3ee984c0cd20664abe9c4697~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105761&auth_key=1760105761-0-0-282fd4d64e4d8d26a8d832dcd4f83bba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4db5bd85e928825eee9461ef0c3be910~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105767&auth_key=1760105767-0-0-d5ceaa39d8298c2d326bdf4a4120528d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a6c6d6da6be4f68b23ca19522aee835~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105774&auth_key=1760105774-0-0-9d7a33f7707ad18365f85e636c4c353c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e9ac7221e735b448d04ece623ac46c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105780&auth_key=1760105780-0-0-e606143edaf8329a24f6e3433a98b1f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HieraTok-Multi-Scale-Visual-Tokenizer-Improves-Image-Reconstruction-and-Generation"><a href="#HieraTok-Multi-Scale-Visual-Tokenizer-Improves-Image-Reconstruction-and-Generation" class="headerlink" title="HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and   Generation"></a>HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and   Generation</h2><p><strong>Authors:Cong Chen, Ziyuan Huang, Cheng Zou, Muzhi Zhu, Kaixiang Ji, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen</strong></p>
<p>In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\times$ faster convergence rate and an 18.9% boost in gFID ($16.4 \rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizerâ€™s training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HieraTokï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šå°ºåº¦Vision Transformerï¼ˆViTï¼‰çš„æ–°å‹åˆ†è¯å™¨ï¼Œå…‹æœäº†å»ºæ¨¡å•å°ºåº¦è¡¨ç¤ºçš„å›ºæœ‰å±€é™æ€§ã€‚è¿™æ˜¯é€šè¿‡ä¸¤ä¸ªå…³é”®è®¾è®¡å®ç°çš„ï¼šï¼ˆ1ï¼‰å¯¹åˆ†è¯å™¨ç¼–ç å™¨ç”Ÿæˆçš„ä»¤ç‰Œå›¾è¿›è¡Œå¤šå°ºåº¦é™é‡‡æ ·ï¼Œç”Ÿæˆä¸€ç³»åˆ—å¤šå°ºåº¦ä»¤ç‰Œï¼›ï¼ˆ2ï¼‰ä¸€ç§è§„æ¨¡å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå…è®¸ä»ä½åˆ†è¾¨ç‡å…¨å±€è¯­ä¹‰ç‰¹å¾åˆ°é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚çš„ä¿¡æ¯é€æ­¥æµåŠ¨ã€‚é€šè¿‡ç»“åˆè¿™äº›è®¾è®¡ï¼ŒHieraTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚åœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼Œå¤šå°ºåº¦è§†è§‰åˆ†è¯å™¨åœ¨rFIDæŒ‡æ ‡ä¸Šä¼˜äºå•å°ºåº¦å¯¹åº”ç‰©ï¼ˆä»1.47æå‡è‡³1.07ï¼Œæé«˜äº†27.2%ï¼‰ã€‚å½“é›†æˆåˆ°ä¸‹æ¸¸ç”Ÿæˆæ¡†æ¶ä¸­æ—¶ï¼Œå®ƒå®ç°äº†1.38å€æ›´å¿«çš„æ”¶æ•›ç‡ï¼Œå¹¶åœ¨gFIDä¸Šæå‡äº†18.9%ï¼ˆä»16.4é™è‡³13.3ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ›´å¹³æ»‘ä¸”åˆ†å¸ƒæ›´å‡åŒ€çš„æ½œåœ¨ç©ºé—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§åˆ†è¯å™¨çš„è®­ç»ƒè§„æ¨¡ï¼Œæˆ‘ä»¬åœ¨ViTåˆ†è¯å™¨ä¸­å®ç°äº†æœ€ä½³rFIDä¸º0.45å’ŒgFIDä¸º1.82ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨å›¾åƒé‡å»ºå’Œå›¾åƒç”Ÿæˆä¸­å¼•å…¥äº†å¤šå°ºåº¦ViTåŸºç¡€çš„åˆ†è¯å™¨ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å‘ç°å’Œè®¾è®¡èƒ½æ¨åŠ¨åŸºäºViTçš„åˆ†è¯å™¨åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23736v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„å¤šå°ºåº¦å›¾åƒç”Ÿæˆå’Œé‡å»ºæŠ€æœ¯â€”â€”HieraTokã€‚é€šè¿‡è®¾è®¡ä¸¤ä¸ªå…³é”®æŠ€æœ¯ç‚¹ï¼ŒåŒ…æ‹¬åˆ©ç”¨å¤šå°ºåº¦ä¸‹é‡‡æ ·ç”Ÿæˆå¤šå°ºåº¦tokenåºåˆ—ä»¥åŠé‡‡ç”¨å°ºåº¦å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†ä»ä½åˆ†è¾¨ç‡å…¨å±€è¯­ä¹‰ç‰¹å¾åˆ°é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚çš„ä¿¡æ¯æ¸è¿›æµåŠ¨ã€‚è¯¥æŠ€æœ¯æ˜¾è‘—æé«˜äº†å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡çš„æ•ˆæœï¼Œå®ç°äº†æ›´å¹³æ»‘ä¸”å‡åŒ€åˆ†å¸ƒçš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æŠ€æœ¯æ˜¯é¦–ä¸ªåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆé¢†åŸŸå¼•å…¥çš„å¤šå°ºåº¦ViTæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HieraTokæ˜¯ä¸€ç§åŸºäºVision Transformerçš„å¤šå°ºåº¦å›¾åƒç”Ÿæˆå’Œé‡å»ºæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦ä¸‹é‡‡æ ·å’Œå°ºåº¦å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†ä»å…¨å±€åˆ°å±€éƒ¨çš„ä¿¡æ¯æ¸è¿›æµåŠ¨ã€‚</li>
<li>HieraTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸æ¯”å•å°ºåº¦æ¨¡å‹æå‡äº†27.2%çš„rFIDå¾—åˆ†ã€‚</li>
<li>é›†æˆåˆ°ä¸‹æ¸¸ç”Ÿæˆæ¡†æ¶åï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„gFIDå¾—åˆ†æå‡ã€‚</li>
<li>é€šè¿‡æ‰©å¤§è®­ç»ƒè§„æ¨¡ï¼Œå±•ç¤ºäº†HieraTokåœ¨ViTä»¤ç‰ŒåŒ–å™¨ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥æŠ€æœ¯é¦–æ¬¡å°†å¤šå°ºåº¦ViTåº”ç”¨äºå›¾åƒé‡å»ºå’Œç”Ÿæˆé¢†åŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2af0182de1593727bf806b076ba4521f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105788&auth_key=1760105788-0-0-38993f7e8366806ade4527b07c0f2264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ed8b28a455a50c49940842dd43c909e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105796&auth_key=1760105796-0-0-5de8678a7236762ceb433b5a8832b8b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7b2a4f2a2d54b391d7f47ed9ab9de13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105802&auth_key=1760105802-0-0-2cfdb60227b1221e4573c5ac2afe82b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c766f9ad67406e7f73b3580072462bbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105809&auth_key=1760105809-0-0-9ac47ca43d81ba398fe390f53dcd5454&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7e4acfe7a983c86785de84816641574~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105815&auth_key=1760105815-0-0-14010f35b2d5b46be86e9e0624eeea7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b48ac7f2f1bbf96254e02c1b21bf49c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105822&auth_key=1760105822-0-0-168560dfbb232597d68d0f5a6608bd56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MSD-KMamba-Bidirectional-Spatial-Aware-Multi-Modal-3D-Brain-Segmentation-via-Multi-scale-Self-Distilled-Fusion-Strategy"><a href="#MSD-KMamba-Bidirectional-Spatial-Aware-Multi-Modal-3D-Brain-Segmentation-via-Multi-scale-Self-Distilled-Fusion-Strategy" class="headerlink" title="MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain   Segmentation via Multi-scale Self-Distilled Fusion Strategy"></a>MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain   Segmentation via Multi-scale Self-Distilled Fusion Strategy</h2><p><strong>Authors:Dayu Tan, Ziwei Zhang, Yansan Su, Xin Peng, Yike Dai, Chunhou Zheng, Weimin Zhong</strong></p>
<p>Numerous CNN-Transformer hybrid models rely on high-complexity global attention mechanisms to capture long-range dependencies, which introduces non-linear computational complexity and leads to significant resource consumption. Although knowledge distillation and sparse attention mechanisms can improve efficiency, they often fall short of delivering the high segmentation accuracy necessary for complex tasks. Balancing model performance with computational efficiency remains a critical challenge. In this work, we propose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba, which integrates bidirectional spatial perception with multi-scale self-distillation. The bidirectional spatial aware branch effectively captures long-range spatial context dependencies across brain regions, while also incorporating a powerful nonlinear feature extraction mechanism that further enhances the modelâ€™s ability to learn complex and heterogeneous patterns. In addition, the proposed multi-scale self-distilled fusion strategy strengthens hierarchical feature representations and improves the transfer of semantic information at different resolution levels. By jointly leveraging the bidirectional spatial perception branch and the multi-scale self-distilled fusion strategy, our framework effectively mitigates the bottleneck of quadratic computational complexity in volumetric segmentation, while simultaneously addressing the limitation of insufficient global perception. Extensive experiments on multiple standard benchmark datasets demonstrate that MSD-KMamba consistently outperforms state-of-the-art methods in segmentation accuracy, robustness, and generalization, while maintaining high computational efficiency and favorable scalability. The source code of MSD-KMamba is publicly available at <a target="_blank" rel="noopener" href="https://github.com/daimao-zhang/MSD-KMamba">https://github.com/daimao-zhang/MSD-KMamba</a>. </p>
<blockquote>
<p>è®¸å¤šCNN-Transformeræ··åˆæ¨¡å‹ä¾èµ–äºé«˜å¤æ‚åº¦çš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè¿™å¼•å…¥äº†éçº¿æ€§è®¡ç®—å¤æ‚åº¦å¹¶å¯¼è‡´èµ„æºæ¶ˆè€—æ˜¾è‘—å¢åŠ ã€‚å°½ç®¡çŸ¥è¯†è’¸é¦å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥å®ç°å¤æ‚ä»»åŠ¡æ‰€éœ€çš„é«˜åˆ†å‰²ç²¾åº¦ã€‚å¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„3Då¤šæ¨¡æ€å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œç§°ä¸ºMSD-KMambaï¼Œå®ƒç»“åˆäº†åŒå‘ç©ºé—´æ„ŸçŸ¥å’Œå¤šå°ºåº¦è‡ªè’¸é¦ã€‚åŒå‘ç©ºé—´æ„ŸçŸ¥åˆ†æ”¯æœ‰æ•ˆåœ°æ•æ‰äº†è„‘åŒºä¹‹é—´çš„é•¿è·ç¦»ç©ºé—´ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶é‡‡ç”¨å¼ºå¤§çš„éçº¿æ€§ç‰¹å¾æå–æœºåˆ¶ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å­¦ä¹ å¤æ‚å’Œå¼‚è´¨æ¨¡å¼çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºçš„å¤šå°ºåº¦è‡ªè’¸é¦èåˆç­–ç•¥åŠ å¼ºäº†åˆ†å±‚ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ”¹å–„äº†ä¸åŒåˆ†è¾¨ç‡å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ä¼ è¾“ã€‚é€šè¿‡è”åˆåˆ©ç”¨åŒå‘ç©ºé—´æ„ŸçŸ¥åˆ†æ”¯å’Œå¤šå°ºåº¦è‡ªè’¸é¦èåˆç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æœ‰æ•ˆåœ°ç¼“è§£äº†ä½“ç§¯åˆ†å‰²ä¸­äºŒæ¬¡è®¡ç®—å¤æ‚æ€§çš„ç“¶é¢ˆï¼ŒåŒæ—¶è§£å†³äº†å…¨å±€æ„ŸçŸ¥ä¸è¶³çš„å±€é™æ€§ã€‚åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMSD-KMambaåœ¨åˆ†å‰²ç²¾åº¦ã€ç¨³å¥æ€§å’Œæ³›åŒ–æ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜è®¡ç®—æ•ˆç‡å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚MSD-KMambaçš„æºä»£ç å¯å…¬å¼€è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/daimao-zhang/MSD-KMamba%E3%80%82">https://github.com/daimao-zhang/MSD-KMambaã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMSD-KMambaçš„3Då¤šæ¨¡æ€å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŒå‘ç©ºé—´æ„ŸçŸ¥å’Œå¤šå°ºåº¦è‡ªè’¸é¦æŠ€æœ¯ã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ•æ‰é•¿è·ç¦»ç©ºé—´ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¢å¼ºæ¨¡å‹å­¦ä¹ å¤æ‚å’Œå¼‚è´¨æ¨¡å¼çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨åŒå‘ç©ºé—´æ„ŸçŸ¥åˆ†æ”¯å’Œå¤šå°ºåº¦è‡ªè’¸é¦èåˆç­–ç•¥ï¼ŒMSD-KMambaåœ¨ä½“ç§¯åˆ†å‰²ä¸­æœ‰æ•ˆç¼“è§£äº†äºŒæ¬¡è®¡ç®—å¤æ‚åº¦ç“¶é¢ˆï¼Œå¹¶è§£å†³äº†å…¨å±€æ„ŸçŸ¥ä¸è¶³çš„é—®é¢˜ã€‚åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSD-KMambaåœ¨åˆ†å‰²ç²¾åº¦ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MSD-KMambaæ¡†æ¶ç»“åˆäº†åŒå‘ç©ºé—´æ„ŸçŸ¥å’Œå¤šå°ºåº¦è‡ªè’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³CNN-Transformeræ··åˆæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>åŒå‘ç©ºé—´æ„ŸçŸ¥åˆ†æ”¯èƒ½æœ‰æ•ˆæ•æ‰é•¿è·ç¦»ç©ºé—´ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¹¶å¢å¼ºæ¨¡å‹å­¦ä¹ å¤æ‚å’Œå¼‚è´¨æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>å¤šå°ºåº¦è‡ªè’¸é¦èåˆç­–ç•¥å¼ºåŒ–äº†å±‚æ¬¡ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ”¹å–„äº†ä¸åŒåˆ†è¾¨ç‡çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ä¼ è¾“ã€‚</li>
<li>MSD-KMambaé€šè¿‡ç»“åˆä¸Šè¿°æŠ€æœ¯ï¼Œæœ‰æ•ˆç¼“è§£äº†ä½“ç§¯åˆ†å‰²ä¸­çš„äºŒæ¬¡è®¡ç®—å¤æ‚åº¦ç“¶é¢ˆå’Œå…¨å±€æ„ŸçŸ¥ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSD-KMambaåœ¨åˆ†å‰²ç²¾åº¦ã€é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ã€‚</li>
<li>MSD-KMambaæ¡†æ¶çš„æºä»£ç å·²å…¬å¼€å¯è®¿é—®ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94db11cfbdbcc4fd9a6ea9b69c96ae97~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105830&auth_key=1760105830-0-0-6d524bba08ab34db4a9e337ac3595aaf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c34d8ce9ffc5dd0b2f2278e4768be776~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105838&auth_key=1760105838-0-0-e05e1e5a9fbc0f9bb653734c1c405bec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5332266a5b2f2fb2fae274cd047bcee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105845&auth_key=1760105845-0-0-ea7a13746013d237a53020d248ec0134&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03bc33d93e9323bd2a2d7a0130deeadc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105851&auth_key=1760105851-0-0-69647b74a4e154411855845468c4a370&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-452c7bca9316551e676e9547eaefcdc2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105858&auth_key=1760105858-0-0-21e3563085c32875ff9112f863f3f63f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="S-3-F-Net-A-Multi-Modal-Approach-to-Medical-Image-Classification-via-Spatial-Spectral-Summarizer-Fusion-Network"><a href="#S-3-F-Net-A-Multi-Modal-Approach-to-Medical-Image-Classification-via-Spatial-Spectral-Summarizer-Fusion-Network" class="headerlink" title="S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via   Spatial-Spectral Summarizer Fusion Network"></a>S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via   Spatial-Spectral Summarizer Fusion Network</h2><p><strong>Authors:Md. Saiful Bari Siddiqui, Mohammed Imamul Hassan Bhuiyan</strong></p>
<p>Convolutional Neural Networks have become a cornerstone of medical image analysis due to their proficiency in learning hierarchical spatial features. However, this focus on a single domain is inefficient at capturing global, holistic patterns and fails to explicitly model an imageâ€™s frequency-domain characteristics. To address these challenges, we propose the Spatial-Spectral Summarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns from both spatial and spectral representations simultaneously. The S$^3$F-Net performs a fusion of a deep spatial CNN with our proposed shallow spectral encoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer, which leverages the Convolution Theorem by applying a bank of learnable filters directly to an imageâ€™s full Fourier spectrum via a computation-efficient element-wise multiplication. This allows the SpectralFilter layer to attain a global receptive field instantaneously, with its output being distilled by a lightweight summarizer network. We evaluate S$^3$F-Net across four medical imaging datasets spanning different modalities to validate its efficacy and generalizability. Our framework consistently and significantly outperforms its strong spatial-only baseline in all cases, with accuracy improvements of up to 5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive accuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs better on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11% accuracy, surpassing many top-performing, much deeper models. Our explainability analysis also reveals that the S$^3$F-Net learns to dynamically adjust its reliance on each branch based on the input pathology. These results verify that our dual-domain approach is a powerful and generalizable paradigm for medical image analysis. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œå› å…¶åœ¨å­¦ä¹ å±‚æ¬¡åŒ–ç©ºé—´ç‰¹å¾æ–¹é¢çš„ä¸“é•¿ï¼Œå·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†æçš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œå¯¹å•ä¸€é¢†åŸŸçš„å…³æ³¨åœ¨æ•æ‰å…¨å±€æ•´ä½“æ¨¡å¼æ–¹é¢æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”æœªèƒ½æ˜¾å¼åœ°å»ºæ¨¡å›¾åƒçš„é¢‘åŸŸç‰¹æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´-å…‰è°±ç»¼åˆèåˆç½‘ç»œï¼ˆS$^3$F-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒåˆ†æ”¯æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶ä»ç©ºé—´å’Œå…‰è°±è¡¨ç¤ºä¸­å­¦ä¹ ã€‚S$^3$F-Netå°†æ·±åº¦ç©ºé—´CNNä¸æˆ‘ä»¬æå‡ºçš„æµ…å±‚å…‰è°±ç¼–ç å™¨SpectraNetç›¸èåˆã€‚SpectraNetå…·æœ‰æˆ‘ä»¬æå‡ºçš„SpectralFilterå±‚ï¼Œå®ƒåˆ©ç”¨å·ç§¯å®šç†ï¼Œé€šè¿‡å…ƒç´ çº§ä¹˜æ³•ç›´æ¥å°†ä¸€ç»„å¯å­¦ä¹ çš„æ»¤æ³¢å™¨åº”ç”¨äºå›¾åƒçš„å…¨å‚…ç«‹å¶è°±ï¼Œä»è€Œå®ç°é«˜æ•ˆè®¡ç®—ã€‚è¿™ä½¿å¾—SpectralFilterå±‚èƒ½å¤Ÿç¬é—´è·å¾—å…¨å±€æ„Ÿå—é‡ï¼Œå…¶è¾“å‡ºé€šè¿‡è½»é‡çº§æ‘˜è¦ç½‘ç»œè¿›è¡Œæç‚¼ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸åŒæ¨¡æ€çš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¯„ä¼°äº†S$^3$F-Netçš„æœ‰æ•ˆæ€§æ³›åŒ–æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½å§‹ç»ˆæ˜¾è‘—ä¼˜äºä»…åŸºäºç©ºé—´çš„å¼ºå¤§åŸºçº¿ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾5.13%ã€‚é€šè¿‡å¼ºå¤§çš„åŒçº¿æ€§èåˆï¼ŒS$^3$F-Netåœ¨BRISC2025æ•°æ®é›†ä¸Šå®ç°äº†ç«äº‰æ€§çš„æœ€é«˜å‡†ç¡®ç‡98.76%ã€‚åœ¨çº¹ç†ä¸»å¯¼çš„èƒ¸éƒ¨Xå…‰è‚ºç‚æ•°æ®é›†ä¸Šï¼Œä¸²è”èåˆè¡¨ç°æ›´å¥½ï¼Œè¾¾åˆ°äº†93.11%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†è®¸å¤šè¡¨ç°ä¼˜å¼‚çš„æ›´æ·±å±‚æ¬¡æ¨¡å‹ã€‚æˆ‘ä»¬çš„è§£é‡Šæ€§åˆ†æè¿˜è¡¨æ˜ï¼ŒS$^3$F-Netå­¦ä¼šäº†æ ¹æ®è¾“å…¥çš„ç—…ç†å­¦åŠ¨æ€è°ƒæ•´å¯¹æ¯ä¸ªåˆ†æ”¯çš„ä¾èµ–ã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„åŒåŸŸæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¼ºå¤§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23442v1">PDF</a> Submitted to IEEE Journal of Biomedical and Health Informatics   (JBHI). This preprint includes few additional details not present in the   journal submission</p>
<p><strong>Summary</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰å“è¶Šè¡¨ç°ï¼Œä½†ä¸“æ³¨äºå•ä¸€ç©ºé—´åŸŸå­˜åœ¨å±€é™ã€‚ä¸ºå…‹æœæ­¤æŒ‘æˆ˜ï¼Œæå‡ºç©ºé—´-é¢‘è°±æ‘˜è¦èåˆç½‘ç»œï¼ˆS^3F-Netï¼‰ï¼Œç»“åˆç©ºé—´ä¸é¢‘è°±è¡¨ç¤ºå­¦ä¹ ã€‚S^3F-Netèåˆæ·±åº¦ç©ºé—´CNNä¸æµ…è°±ç¼–ç å™¨SpectraNetï¼Œé‡‡ç”¨é¢‘è°±æ»¤æ³¢å™¨å±‚å¤„ç†å›¾åƒå…¨é¢‘è°±ã€‚è¯¥ç½‘ç»œåœ¨å››ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒåŸºçº¿æ–¹æ³•æ˜¾è‘—æé«˜ç²¾åº¦ï¼Œæœ€é«˜æå‡5.13%ã€‚é€šè¿‡åŒçº¿æ€§èåˆä¸æ‹¼æ¥èåˆï¼Œåœ¨BRISC2025ä¸Chest X-Ray Pneumoniaæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾98.76%ä¸93.11%å‡†ç¡®ç‡ã€‚ç½‘ç»œèƒ½åŠ¨æ€è°ƒæ•´å¯¹ä¸¤åˆ†æ”¯çš„ä¾èµ–ï¼Œå±•ç°å¼ºå¤§ä¸”é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†æèŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é‡è¦ä½œç”¨åŠå…¶ä¸“æ³¨äºå•ä¸€ç©ºé—´åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>S^3F-Netçš„å‡ºç°ï¼Œä½œä¸ºåŒåˆ†æ”¯æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ ç©ºé—´ä¸é¢‘è°±è¡¨ç¤ºã€‚</li>
<li>S^3F-Netèåˆäº†æ·±åº¦ç©ºé—´CNNä¸æµ…è°±ç¼–ç å™¨SpectraNetï¼Œå…¶ä¸­SpectraNetåŒ…å«é¢‘è°±æ»¤æ³¢å™¨å±‚ï¼Œå¯ç›´æ¥åº”ç”¨äºå›¾åƒå…¨é¢‘è°±ã€‚</li>
<li>S^3F-Netåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>åŒçº¿æ€§èåˆä¸æ‹¼æ¥èåˆç­–ç•¥åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ•ˆæœå·®å¼‚ã€‚</li>
<li>S^3F-Netèƒ½åŠ¨æ€è°ƒæ•´å¯¹ç©ºé—´ä¸é¢‘è°±åˆ†æ”¯çš„ä¾èµ–ï¼Œæä¾›å¼ºå¤§çš„åŒ»å­¦å›¾åƒåˆ†æèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0bb07858de20cb67393cb9b2b59a33d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105866&auth_key=1760105866-0-0-3c31de50597adda6ac854cb5b8c73254&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b7aa561f5646a1fae28b0a84686f49d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105873&auth_key=1760105873-0-0-7a03145b76cf826bf907817ce8db293b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding"><a href="#CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding" class="headerlink" title="CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding"></a>CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ç†è§£åœ¨æ”¾å°„å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ä¼šç”Ÿæˆä¸å—ä¸´åºŠæ”¯æŒçš„æè¿°ï¼Œè¢«ç§°ä¸ºåŒ»å­¦å¹»è§‰ï¼Œè¿™åœ¨éœ€è¦å‡†ç¡®æ€§å’Œå›¾åƒåŸºç¡€è¾“å‡ºçš„åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥äº†ä¸¥é‡çš„é£é™©ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°æç¤ºè¯±å¯¼çš„å¹»è§‰åœ¨æ”¾å°„å­¦MLLMsä¸­ä»ç„¶æ™®éå­˜åœ¨ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸´åºŠå¯¹æ¯”ç¼–ç ï¼ˆCCDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ¥è‡ªç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚CCDå¼•å…¥äº†ä¸€ç§åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶ï¼Œç”¨äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç»†åŒ–ä»¤ç‰Œçº§åˆ«çš„é€»è¾‘ï¼Œä»è€Œæé«˜ä¸´åºŠä¿çœŸåº¦ï¼ŒåŒæ—¶ä¸ä¿®æ”¹åŸºç¡€MLLMã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œå¤šä¸ªæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å§‹ç»ˆæé«˜äº†æ€»ä½“æ€§èƒ½ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œå½“åº”ç”¨äºæœ€å…ˆè¿›çš„RRGæ¨¡å‹æ—¶ï¼Œå®ƒåœ¨RadGraph-F1ä¸Šæé«˜äº†é«˜è¾¾17%ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§è½»é‡çº§å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºç¼“è§£åŒ»å­¦å¹»è§‰ï¼Œæœ‰æ•ˆåœ°æ¡¥æ¥äº†ä¸“å®¶æ¨¡å‹å’ŒMLLMsåœ¨æ”¾å°„å­¦é¢†åŸŸçš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23379v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ç†è§£å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼Œè¿™ç±»æ¨¡å‹å¸¸äº§ç”ŸåŒ»å­¦ä¸Šæœªç»è¯å®æè¿°çš„â€œåŒ»å­¦å¹»è§‰â€ï¼Œè¿™åœ¨éœ€è¦ç²¾ç¡®æ€§å’Œå›¾åƒæ”¯æ’‘è¾“å‡ºçš„åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥äº†é£é™©ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸´åºŠå¯¹æ¯”ç¼–ç ï¼ˆCCDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆæ¥è‡ªç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚CCDé€šè¿‡åŒé‡å¯¹æ¯”æœºåˆ¶ä¼˜åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ ‡è®°çº§é€»è¾‘ï¼Œåœ¨ä¸æ”¹å˜åŸºç¡€MLLMçš„å‰æä¸‹æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ï¼Œå…¶RadGraph-F1å¾—åˆ†æé«˜äº†é«˜è¾¾17%ã€‚æœ¬æ–‡æ–¹æ³•ä¸ºç¼“è§£åŒ»å­¦å¹»è§‰é—®é¢˜æä¾›äº†è½»ä¾¿ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°å°†ä¸“å®¶æ¨¡å‹å’ŒMLLMsæ¡¥æ¥èµ·æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸé€šè¿‡ç»“åˆè§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€ç†è§£å–å¾—äº†è¿›æ­¥ã€‚</li>
<li>MLLMsä¼šäº§ç”ŸåŒ»å­¦ä¸Šæœªç»è¯å®çš„æè¿°ï¼Œç§°ä¸ºâ€œåŒ»å­¦å¹»è§‰â€ï¼Œè¿™åœ¨åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥é£é™©ã€‚</li>
<li>ä¸´åºŠå¯¹æ¯”ç¼–ç ï¼ˆCCDï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³MLLMsä¸­çš„åŒ»å­¦å¹»è§‰é—®é¢˜ã€‚</li>
<li>CCDé€šè¿‡æ•´åˆç»“æ„åŒ–ä¸´åºŠä¿¡å·å’ŒåŒé‡å¯¹æ¯”æœºåˆ¶ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>CCDæé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼Œä¸”åœ¨ä¸æ”¹å˜åŸºç¡€MLLMçš„å‰æä¸‹å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-37875193aa0b2021b77baebf70a1d78c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105881&auth_key=1760105881-0-0-b02edff3b5520bf21a52f49a51fe053e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3412e7b58bfccebaa5bc45f58e0bd04c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105888&auth_key=1760105888-0-0-f80c31b3052e2c5f54d7ae2a49242a00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d2c1c62f194a8d3cd92f88de510f6d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105895&auth_key=1760105895-0-0-22a8c46fcc7b4929f33e75cfc098657e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93aa9c93345d9824f91974d4dc0cf83b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105902&auth_key=1760105902-0-0-824e986617977f83f0b7f98eeb2b4c48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Transfer-Learning-and-Machine-Learning-for-Training-Five-Year-Survival-Prognostic-Models-in-Early-Breast-Cancer"><a href="#Transfer-Learning-and-Machine-Learning-for-Training-Five-Year-Survival-Prognostic-Models-in-Early-Breast-Cancer" class="headerlink" title="Transfer Learning and Machine Learning for Training Five Year Survival   Prognostic Models in Early Breast Cancer"></a>Transfer Learning and Machine Learning for Training Five Year Survival   Prognostic Models in Early Breast Cancer</h2><p><strong>Authors:Lisa Pilgram, Kai Yang, Ana-Alicia Beltran-Bless, Gregory R. Pond, Lisa Vandermeer, John Hilton, Marie-France Savard, AndrÃ©anne Leblanc, Lois Sheperd, Bingshu E. Chen, John M. S. Bartlett, Karen J. Taylor, Jane Bayani, Sarah L. Barker, Melanie Spears, Cornelis J. H. van der Velde, Elma Meershoek-Klein Kranenbarg, Luc Dirix, Elizabeth Mallon, Annette Hasenburg, Christos Markopoulos, Lamin Juwara, Fida K. Dankar, Mark Clemons, Khaled El Emam</strong></p>
<p>Prognostic information is essential for decision-making in breast cancer management. Recently trials have predominantly focused on genomic prognostication tools, even though clinicopathological prognostication is less costly and more widely accessible. Machine learning (ML), transfer learning and ensemble integration offer opportunities to build robust prognostication frameworks. We evaluate this potential to improve survival prognostication in breast cancer by comparing de-novo ML, transfer learning from a pre-trained prognostic tool and ensemble integration. Data from the MA.27 trial was used for model training, with external validation on the TEAM trial and a SEER cohort. Transfer learning was applied by fine-tuning the pre-trained prognostic tool PREDICT v3, de-novo ML included Random Survival Forests and Extreme Gradient Boosting, and ensemble integration was realized through a weighted sum of model predictions. Transfer learning, de-novo RSF, and ensemble integration improved calibration in MA.27 over the pre-trained model (ICI reduced from 0.042 in PREDICT v3 to &lt;&#x3D;0.007) while discrimination remained comparable (AUC increased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3 predictions were observed in 23.8-25.8% of MA.27 individuals due to missing information. In contrast, ML models and ensemble integration could predict survival regardless of missing information. Across all models, patient age, nodal status, pathological grading and tumor size had the highest SHAP values, indicating their importance for survival prognostication. External validation in SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and ensemble integration. This study demonstrates that transfer learning, de-novo RSF, and ensemble integration can improve prognostication in situations where relevant information for PREDICT v3 is lacking or where a dataset shift is likely. </p>
<blockquote>
<p>é¢„åä¿¡æ¯å¯¹äºä¹³è…ºç™Œç®¡ç†ä¸­çš„å†³ç­–åˆ¶å®šè‡³å…³é‡è¦ã€‚å°½ç®¡ä¸´åºŠç—…ç†é¢„åç›¸å¯¹æˆæœ¬æ›´ä½ä¸”æ›´æ˜“äºè·å–ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºåŸºå› ç»„é¢„åå·¥å…·ã€‚æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ã€è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•ä¸ºæ„å»ºç¨³å¥çš„é¢„åæ¡†æ¶æä¾›äº†æœºä¼šã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒå…¨æ–°æœºå™¨å­¦ä¹ ã€ä»é¢„è®­ç»ƒé¢„åå·¥å…·è¿›è¡Œè¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•ï¼Œè¯„ä¼°äº†æé«˜ä¹³è…ºç™Œç”Ÿå­˜é¢„åçš„æ½œåŠ›ã€‚ä½¿ç”¨MA.27è¯•éªŒçš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨TEAMè¯•éªŒå’ŒSEERé˜Ÿåˆ—ä¸­è¿›è¡Œå¤–éƒ¨éªŒè¯ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„é¢„åå·¥å…·PREDICT v3åº”ç”¨è¿ç§»å­¦ä¹ ï¼Œå…¨æ–°æœºå™¨å­¦ä¹ åŒ…æ‹¬éšæœºç”Ÿå­˜æ£®æ—å’Œæç«¯æ¢¯åº¦æå‡ï¼Œè€Œé›†æˆæ–¹æ³•åˆ™æ˜¯é€šè¿‡æ¨¡å‹é¢„æµ‹åŠ æƒå’Œæ¥å®ç°ã€‚åœ¨MA.27ä¸­ï¼Œè¿ç§»å­¦ä¹ ã€å…¨æ–°éšæœºæ£®æ—å’Œé›†æˆæ”¹è¿›äº†ç›¸å¯¹äºé¢„è®­ç»ƒæ¨¡å‹çš„æ ¡å‡†ï¼ˆICIä»PREDICT v3ä¸­çš„0.042å‡å°‘åˆ°&lt;&#x3D;0.007ï¼‰ï¼ŒåŒæ—¶é‰´åˆ«èƒ½åŠ›ä¿æŒç›¸å½“ï¼ˆAUCä»PREDICT v3ä¸­çš„0.738å¢åŠ åˆ°0.744-0.799ï¼‰ã€‚ç”±äºä¿¡æ¯ç¼ºå¤±ï¼ŒPREDICT v3çš„é¢„æµ‹åœ¨MA.27ä¸ªä½“ä¸­çš„23.8-25.8%è¢«åˆ¤å®šä¸ºæ— æ•ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœºå™¨æ¨¡å‹å’Œé›†æˆæ–¹æ³•èƒ½å¤Ÿé¢„æµ‹ç”Ÿå­˜æƒ…å†µï¼Œæ— è®ºä¿¡æ¯æ˜¯å¦ç¼ºå¤±ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæ‚£è€…å¹´é¾„ã€èŠ‚ç‚¹çŠ¶æ€ã€ç—…ç†åˆ†çº§å’Œè‚¿ç˜¤å¤§å°å…·æœ‰æœ€é«˜çš„SHAPå€¼ï¼Œè¿™è¡¨æ˜å®ƒä»¬åœ¨ç”Ÿå­˜é¢„åä¸­çš„é‡è¦æ€§ã€‚åœ¨SEERä¸­çš„å¤–éƒ¨éªŒè¯ï¼Œè€ŒéTEAMï¼Œè¯å®äº†è¿ç§»å­¦ä¹ ã€éšæœºæ£®æ—å’Œé›†æˆçš„ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç¼ºä¹ç”¨äºPREDICT v3çš„ç›¸å…³ä¿¡æ¯æˆ–æ•°æ®é›†å¯èƒ½å‘ç”Ÿå˜åŠ¨çš„æƒ…å†µä¸‹ï¼Œè¿ç§»å­¦ä¹ ã€å…¨æ–°éšæœºæ£®æ—å’Œé›†æˆæ–¹æ³•å¯ä»¥æ”¹è¿›é¢„åé¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23268v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¹³è…ºç™Œé¢„åé¢„æµ‹çš„é‡è¦æ€§ï¼Œå¹¶è¯„ä¼°äº†æœºå™¨å­¦ä¹ ã€è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•åœ¨æ”¹å–„ä¹³è…ºç™Œç”Ÿå­˜é¢„åé¢„æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ä½¿ç”¨MA.27è¯•éªŒæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶é€šè¿‡TEAMè¯•éªŒå’ŒSEERé˜Ÿåˆ—è¿›è¡Œå¤–éƒ¨éªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿ç§»å­¦ä¹ ã€å…¨æ–°éšæœºæ£®æ—å’Œé›†æˆæ–¹æ³•æ”¹è¿›äº†MA.27ä¸­çš„æ ¡å‡†ï¼Œä¸”åˆ¤åˆ«èƒ½åŠ›ä¿æŒè‰¯å¥½ã€‚åœ¨ç¼ºå¤±ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å’Œé›†æˆæ–¹æ³•ä»ç„¶èƒ½å¤Ÿé¢„æµ‹ç”Ÿå­˜ã€‚æ‚£è€…å¹´é¾„ã€ç»“èŠ‚çŠ¶æ€ã€ç—…ç†åˆ†çº§å’Œè‚¿ç˜¤å¤§å°å¯¹ç”Ÿå­˜é¢„åé¢„æµ‹æœ€ä¸ºé‡è¦ã€‚åœ¨SEERä¸­è¿›è¡Œçš„å¤–éƒ¨éªŒè¯è¯å®äº†è¿ç§»å­¦ä¹ ã€éšæœºæ£®æ—å’Œé›†æˆæ–¹æ³•çš„å¥½å¤„ï¼Œä½†åœ¨TEAMä¸­æœªå¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œé¢„åä¿¡æ¯å¯¹å†³ç­–è‡³å…³é‡è¦ï¼Œè¿‘æœŸç ”ç©¶ä¸»è¦å…³æ³¨åŸºå› ç»„é¢„åå·¥å…·ï¼Œä½†ä¸´åºŠç—…ç†é¢„åå·¥å…·å…·æœ‰è¾ƒä½æˆæœ¬å’Œæ›´å¹¿æ³›çš„å¯åŠæ€§ã€‚</li>
<li>æœºå™¨å­¦ä¹ ã€è¿ç§»å­¦ä¹ å’Œé›†æˆæ–¹æ³•ä¸ºæé«˜ä¹³è…ºç™Œé¢„åé¢„æµ‹çš„å‡†ç¡®æ€§æä¾›äº†æœºä¼šã€‚</li>
<li>åœ¨MA.27è¯•éªŒä¸­ï¼Œè¿ç§»å­¦ä¹ ã€å…¨æ–°éšæœºæ£®æ—å’Œé›†æˆæ–¹æ³•æ”¹è¿›äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ ¡å‡†ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¥½çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹å’Œé›†æˆæ–¹æ³•èƒ½å¤Ÿåœ¨ç¼ºå¤±ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œç”Ÿå­˜é¢„æµ‹ã€‚</li>
<li>æ‚£è€…å¹´é¾„ã€ç»“èŠ‚çŠ¶æ€ã€ç—…ç†åˆ†çº§å’Œè‚¿ç˜¤å¤§å°æ˜¯ç”Ÿå­˜é¢„åé¢„æµ‹çš„å…³é”®å› ç´ ã€‚</li>
<li>åœ¨SEERè¿›è¡Œçš„å¤–éƒ¨éªŒè¯æ”¯æŒäº†æ–°æ–¹æ³•ï¼ˆè¿ç§»å­¦ä¹ ã€éšæœºæ£®æ—å’Œé›†æˆæ–¹æ³•ï¼‰çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fcb36bb2eeb23ae12daf9042ca0957f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105910&auth_key=1760105910-0-0-d9288551fbc024fb7aba7d3c0283bfb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a1414108f8096bb4e175ffae5f894d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105917&auth_key=1760105917-0-0-c51040a466a9bf1f4b0633e4c5db0bb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Untangling-Vascular-Trees-for-Surgery-and-Interventional-Radiology"><a href="#Untangling-Vascular-Trees-for-Surgery-and-Interventional-Radiology" class="headerlink" title="Untangling Vascular Trees for Surgery and Interventional Radiology"></a>Untangling Vascular Trees for Surgery and Interventional Radiology</h2><p><strong>Authors:Guillaume Houry, Tom Boeken, StÃ©phanie AllassonniÃ¨re, Jean Feydy</strong></p>
<p>The diffusion of minimally invasive, endovascular interventions motivates the development of visualization methods for complex vascular networks. We propose a planar representation of blood vessel trees which preserves the properties that are most relevant to catheter navigation: topology, length and curvature. Taking as input a three-dimensional digital angiography, our algorithm produces a faithful two-dimensional map of the patientâ€™s vessels within a few seconds. To this end, we propose optimized implementations of standard morphological filters and a new recursive embedding algorithm that preserves the global orientation of the vascular network. We showcase our method on peroperative images of the brain, pelvic and knee artery networks. On the clinical side, our method simplifies the choice of devices prior to and during the intervention. This lowers the risk of failure during navigation or device deployment and may help to reduce the gap between expert and common intervention centers. From a research perspective, our method simulates the cadaveric display of artery trees from anatomical dissections. This opens the door to large population studies on the branching patterns and tortuosity of fine human blood vessels. Our code is released under the permissive MIT license as part of the scikit-shapes Python library (<a target="_blank" rel="noopener" href="https://scikit-shapes.github.io/">https://scikit-shapes.github.io</a> ). </p>
<blockquote>
<p>å¾®åˆ›è¡€ç®¡å†…å¹²é¢„çš„æ™®åŠä¿ƒè¿›äº†å¤æ‚è¡€ç®¡ç½‘ç»œå¯è§†åŒ–æ–¹æ³•çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¡€ç®¡æ ‘çš„å¹³é¢è¡¨ç¤ºæ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•ä¿ç•™äº†å¯¹å¯¼ç®¡å¯¼èˆªè‡³å…³é‡è¦çš„å±æ€§ï¼šæ‹“æ‰‘ç»“æ„ã€é•¿åº¦å’Œæ›²ç‡ã€‚ä»¥ä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±ä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨å‡ ç§’å†…ç”Ÿæˆæ‚£è€…è¡€ç®¡çš„å¿ å®äºŒç»´åœ°å›¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†å½¢æ€å­¦æ»¤æ³¢å™¨è¿›è¡Œäº†ä¼˜åŒ–å®ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é€’å½’åµŒå…¥ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¿ç•™äº†è¡€ç®¡ç½‘ç»œçš„å…¨å±€æ–¹å‘ã€‚æˆ‘ä»¬åœ¨å¤§è„‘çš„æœ¯ä¸­å›¾åƒã€ç›†è…”å’Œè†å…³èŠ‚åŠ¨è„‰ç½‘ç»œä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨ä¸´åºŠæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†å¹²é¢„å‰åè®¾å¤‡çš„é€‰æ‹©ã€‚è¿™é™ä½äº†å¯¼èˆªæˆ–è®¾å¤‡éƒ¨ç½²è¿‡ç¨‹ä¸­çš„å¤±è´¥é£é™©ï¼Œå¹¶æœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸æ™®é€šå¹²é¢„ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚ä»ç ”ç©¶è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†è§£å‰–è§£å‰–ä¸­åŠ¨è„‰æ ‘çš„å°¸ä½“æ˜¾ç¤ºã€‚è¿™ä¸ºç ”ç©¶äººç±»ç²¾ç»†è¡€ç®¡çš„åˆ†æ”¯æ¨¡å¼å’Œæ‰­æ›²æ€§æä¾›äº†å¤§è§„æ¨¡äººç¾¤ç ”ç©¶çš„æœºä¼šã€‚æˆ‘ä»¬çš„ä»£ç ä½œä¸ºscikit-shapes Pythonåº“çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨è®¸å¯çš„MITè®¸å¯ä¸‹å‘å¸ƒï¼ˆ<a target="_blank" rel="noopener" href="https://scikit-shapes.github.io)./">https://scikit-shapes.github.ioï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23165v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç®—æ³•é’ˆå¯¹å¤æ‚çš„è¡€ç®¡ç½‘ç»œå¼€å‘äº†ä¸€ç§å¯è§†åŒ–æ–¹æ³•ï¼Œæå‡ºäº†è¡€ç®¡æ ‘çš„å¹³é¢è¡¨ç¤ºæ–¹å¼ï¼Œå¹¶ä¿ç•™äº†å¯¼ç®¡å¯¼èˆªæœ€ç›¸å…³çš„å±æ€§ï¼šæ‹“æ‰‘ã€é•¿åº¦å’Œæ›²ç‡ã€‚é€šè¿‡ä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±æœ¯è¾“å…¥ï¼Œç®—æ³•å¯åœ¨å‡ ç§’å†…ç”Ÿæˆæ‚£è€…è¡€ç®¡çš„å¿ å®äºŒç»´åœ°å›¾ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„è®¾å¤‡é€‰æ‹©ï¼Œé™ä½äº†å¯¼èˆªæˆ–è®¾å¤‡éƒ¨ç½²è¿‡ç¨‹ä¸­çš„å¤±è´¥é£é™©ï¼Œå¹¶æœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸æ™®é€šä»‹å…¥ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ¨¡æ‹Ÿäº†è§£å‰–å­¦ç ”ç©¶ä¸­åŠ¨è„‰æ ‘çš„å±•ç¤ºï¼Œä¸ºç ”ç©¶äººç±»è¡€ç®¡åˆ†æ”¯æ¨¡å¼å’Œå¼¯æ›²åº¦æä¾›äº†æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹å¤æ‚è¡€ç®¡ç½‘ç»œçš„å¯è§†åŒ–æ–¹æ³•ï¼Œå¼ºè°ƒå¯¼ç®¡å¯¼èˆªç›¸å…³çš„æ‹“æ‰‘ã€é•¿åº¦å’Œæ›²ç‡å±æ€§çš„ä¿ç•™ã€‚</li>
<li>é€šè¿‡ä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±æœ¯è¾“å…¥ï¼Œå¿«é€Ÿç”ŸæˆäºŒç»´è¡€ç®¡åœ°å›¾ã€‚</li>
<li>æ–¹æ³•ç®€åŒ–äº†æ‰‹æœ¯ä¸­çš„è®¾å¤‡é€‰æ‹©ï¼Œé™ä½äº†æ‰‹æœ¯é£é™©ã€‚</li>
<li>æœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸æ™®é€šä»‹å…¥ä¸­å¿ƒçš„å·®è·ã€‚</li>
<li>æ¨¡æ‹Ÿäº†è§£å‰–å­¦ä¸­çš„åŠ¨è„‰æ ‘å±•ç¤ºï¼Œä¸ºè¡€ç®¡ç ”ç©¶æä¾›æœºä¼šã€‚</li>
<li>å¯ç”¨äºç ”ç©¶äººç±»è¡€ç®¡çš„åˆ†æ”¯æ¨¡å¼å’Œå¼¯æ›²åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-254a9937bbfced45621fa76c801f1b31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827180&auth_key=1760827180-0-0-6511bf4de130c7c4112c3ef6413ed7b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ed1972fe817bd1a69b3f4082ce156f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827187&auth_key=1760827187-0-0-016f0992d7e19f1737ac45696d85bbaf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-072f6c74979d9600a70dbf3d6a04b70d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827194&auth_key=1760827194-0-0-cb3d69104df9eae7ca0679abdfd3e5f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e5090785f9afbc0bdd9e82d9ce36bd60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827200&auth_key=1760827200-0-0-558fcdbf56c853d26225af09686bf64a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis"><a href="#Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis" class="headerlink" title="Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis"></a>Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis</h2><p><strong>Authors:Ruilang Wang, Shuotong Xu, Bowen Liu, Runlin Huang, Donglong Chen, Weifeng Su</strong></p>
<p>The scarcity of annotated data in specialized domains such as medical imaging presents significant challenges to training robust vision models. While self-supervised masked image modeling (MIM) offers a promising solution, existing approaches largely rely on random high-ratio masking, leading to inefficiency and poor semantic alignment. Moreover, region-aware variants typically depend on reconstruction heuristics or supervised signals, limiting their adaptability across tasks and modalities. We propose Mask What Matters, a controllable text-guided masking framework for self-supervised medical image analysis. By leveraging vision-language models for prompt-based region localization, our method flexibly applies differentiated masking to emphasize diagnostically relevant regions while reducing redundancy in background areas. This controllable design enables better semantic alignment, improved representation learning, and stronger cross-task generalizability. Comprehensive evaluation across multiple medical imaging modalities, including brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1 percentage points in classification accuracy, +1.3 in box average precision (BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it achieves these improvements with substantially lower overall masking ratios (e.g., 40% vs. 70%). This work demonstrates that controllable, text-driven masking can enable semantically aligned self-supervised learning, advancing the development of robust vision models for medical image analysis. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ä¸ºè®­ç»ƒç¨³å¥çš„è§†è§‰æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è‡ªç›‘ç£çš„æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºéšæœºçš„é«˜æ¯”ä¾‹æ©ç ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æ­¤å¤–ï¼ŒåŒºåŸŸæ„ŸçŸ¥å˜ä½“é€šå¸¸ä¾èµ–äºé‡å»ºå¯å‘å¼æˆ–ç›‘ç£ä¿¡å·ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä»»åŠ¡å’Œæ¨¡æ€ä¹‹é—´çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†â€œMask What Mattersâ€ï¼ˆæ©ç å…³é”®ä¿¡æ¯ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªç›‘ç£åŒ»å­¦å›¾åƒåˆ†æçš„å¯æ§æ–‡æœ¬å¼•å¯¼æ©ç æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥çµæ´»åœ°åº”ç”¨å·®å¼‚åŒ–æ©ç ï¼Œä»¥å¼ºè°ƒä¸è¯Šæ–­ç›¸å…³çš„åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ã€‚è¿™ç§å¯æ§çš„è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›äº†è¡¨ç¤ºå­¦ä¹ å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬è„‘MRIã€èƒ¸éƒ¨CTå’Œè‚ºéƒ¨Xå°„çº¿ï¼Œè¡¨æ˜Mask What Matterså§‹ç»ˆä¼˜äºç°æœ‰çš„MIMæ–¹æ³•ï¼ˆä¾‹å¦‚SparKï¼‰ï¼Œåœ¨åˆ†ç±»ç²¾åº¦ä¸Šæé«˜äº†é«˜è¾¾+3.1ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆBoxAPï¼‰æé«˜äº†+1.3ï¼Œæ©è†œå¹³å‡ç²¾åº¦ï¼ˆMaskAPï¼‰åœ¨æ£€æµ‹æ–¹é¢æé«˜äº†+1.1ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å®ç°è¿™äº›æ”¹è¿›çš„åŒæ—¶ï¼Œæ•´ä½“æ©ç æ¯”ä¾‹å¤§å¤§é™ä½ï¼ˆä¾‹å¦‚ï¼Œ40%å¯¹70%ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¯æ§çš„ã€æ–‡æœ¬é©±åŠ¨çš„æ©ç å¯ä»¥å®ç°è¯­ä¹‰å¯¹é½çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ç¨³å¥è§†è§‰æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23054v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰è‡ªç›‘ç£é®æŒ¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æ–¹æ³•å­˜åœ¨éšæœºé«˜æ¯”ä¾‹é®æŒ¡ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºMask What Mattersï¼Œä¸€ä¸ªå¯æ§æ–‡æœ¬å¼•å¯¼çš„é®æŒ¡æ¡†æ¶ï¼Œç”¨äºè‡ªç›‘ç£åŒ»å­¦å›¾åƒåˆ†æã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œçµæ´»åº”ç”¨å·®å¼‚åŒ–é®æŒ¡ï¼Œå¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œå‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ã€‚æ­¤å¯æ§è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›äº†è¡¨å¾å­¦ä¹ å’Œå¢å¼ºäº†è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒMask What Mattersåœ¨åˆ†ç±»ç²¾åº¦ã€æ¡†å¹³å‡ç²¾åº¦å’Œæ©è†œå¹³å‡ç²¾åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰MIMæ–¹æ³•ï¼Œå¹¶å®ç°äº†è¾ƒä½çš„æ•´ä½“é®æŒ¡æ¯”ä¾‹ã€‚è¯¥å·¥ä½œè¯æ˜å¯æ§çš„æ–‡æœ¬é©±åŠ¨é®æŒ¡å¯å®ç°è¯­ä¹‰å¯¹é½çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æçš„ç¨³å¥è§†è§‰æ¨¡å‹å¼€å‘æä¾›äº†å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒé¢†åŸŸé¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£é®æŒ¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰MIMæ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé«˜æ¯”ä¾‹é®æŒ¡ï¼Œå¯¼è‡´æ•ˆç‡å’Œè¯­ä¹‰å¯¹é½é—®é¢˜ã€‚</li>
<li>Mask What Mattersé‡‡ç”¨å¯æ§æ–‡æœ¬å¼•å¯¼çš„é®æŒ¡æ¡†æ¶ï¼Œç”¨äºè‡ªç›‘ç£åŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŒºåŸŸå®šä½ï¼Œå®ç°å·®å¼‚åŒ–é®æŒ¡ï¼Œå¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸã€‚</li>
<li>Mask What Mattersåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰MIMæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-47bd848eccc3debcba5394e4b681a2cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827207&auth_key=1760827207-0-0-f81d93a9c8c225ae664ef4f3158bbf84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7308ccad5ffd63231ea134f6bcf72f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827214&auth_key=1760827214-0-0-53904b732872e6e35a659ed687fcf719&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe1061d4ddbf133c81b10db1428f0a39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827221&auth_key=1760827221-0-0-756b8aa14b9f47244889a593d7cc58cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b9a818f2a1ca134a250753278bd4fc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827228&auth_key=1760827228-0-0-1eeb19209f43b1057b264bd42b1643e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-df5262eb29dc7274526644a08b320244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827235&auth_key=1760827235-0-0-cce8892ce7b776e5cbdb5d5c588d8968&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Go with Your Gut Scaling Confidence for Autoregressive Image Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-af032b5a3f4bb8c65fba0f09cc958c89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760105402&auth_key=1760105402-0-0-1f36339270694aebac78ad0a986f3fe8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  U-Mamba2 Scaling State Space Models for Dental Anatomy Segmentation in   CBCT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
