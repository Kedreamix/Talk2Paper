<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  MLA A Multisensory Language-Action Model for Multimodal Understanding   and Forecasting in Robotic Manipulation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ba8d094778b884fb914c501034be57f7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-02-æ›´æ–°"><a href="#2025-10-02-æ›´æ–°" class="headerlink" title="2025-10-02 æ›´æ–°"></a>2025-10-02 æ›´æ–°</h1><h2 id="MLA-A-Multisensory-Language-Action-Model-for-Multimodal-Understanding-and-Forecasting-in-Robotic-Manipulation"><a href="#MLA-A-Multisensory-Language-Action-Model-for-Multimodal-Understanding-and-Forecasting-in-Robotic-Manipulation" class="headerlink" title="MLA: A Multisensory Language-Action Model for Multimodal Understanding   and Forecasting in Robotic Manipulation"></a>MLA: A Multisensory Language-Action Model for Multimodal Understanding   and Forecasting in Robotic Manipulation</h2><p><strong>Authors:Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang</strong></p>
<p>Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLAâ€™s understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/open-mla">https://sites.google.com/view/open-mla</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰é€šè¿‡ç»§æ‰¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¹¶å­¦ä¹ è¡ŒåŠ¨ç”Ÿæˆï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ³›åŒ–èƒ½åŠ›ã€‚å¤§å¤šæ•°VLAæ¨¡å‹ä¾§é‡äºè§£é‡Šè§†è§‰å’Œè¯­è¨€ä»¥ç”Ÿæˆè¡ŒåŠ¨ï¼Œè€Œæœºå™¨äººå¿…é¡»åœ¨ç©ºé—´ç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œæ„ŸçŸ¥å’Œäº¤äº’ã€‚è¿™ä¸€å·®è·å‡¸æ˜¾äº†å¯¹æœºå™¨äººç‰¹å®šå¤šæ„Ÿå®˜ä¿¡æ¯çš„å…¨é¢ç†è§£çš„éœ€æ±‚ï¼Œè¿™å¯¹äºå®ç°å¤æ‚ä¸”æ¥è§¦ä¸°å¯Œçš„æ§åˆ¶è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ„Ÿå®˜è¯­è¨€è¡ŒåŠ¨ï¼ˆMLAï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ååŒæ„ŸçŸ¥ä¸åŒçš„æ„Ÿå®˜æ¨¡å¼ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å¤šæ„Ÿå®˜ç›®æ ‡ï¼Œä»¥ä¿ƒè¿›ç‰©ç†ä¸–ç•Œå»ºæ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¢å¼ºæ„ŸçŸ¥è¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç¼–ç å™¨å¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ›æ–°åœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹æœ¬èº«é‡æ–°ç”¨ä½œæ„ŸçŸ¥æ¨¡å—ï¼Œé€šè¿‡ä½ç½®å¯¹åº”å…³ç³»ç›´æ¥è§£é‡Šå¤šæ¨¡æ€çº¿ç´¢ï¼Œå¯¹é½2Då›¾åƒã€3Dç‚¹äº‘å’Œè§¦è§‰æ ‡è®°ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºMLAå¯¹ç‰©ç†åŠ¨æ€çš„ç†è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœªæ¥å¤šæ„Ÿå®˜ç”Ÿæˆåè®­ç»ƒç­–ç•¥ï¼Œä½¿MLAèƒ½å¤Ÿæ¨ç†è¯­ä¹‰ã€å‡ ä½•å’Œäº¤äº’ä¿¡æ¯ï¼Œä¸ºè¡ŒåŠ¨ç”Ÿæˆæä¾›æ›´ç¨³å¥çš„æ¡ä»¶ã€‚åœ¨è¯„ä¼°ä¸­ï¼ŒMLAæ¨¡å‹åœ¨å¤æ‚ã€æ¥è§¦ä¸°å¯Œçš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­åˆ†åˆ«æ¯”æœ€æ–°çš„2Då’Œ3DVLAæ–¹æ³•é«˜å‡º12%å’Œ24%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹æœªè§é…ç½®çš„æ”¹è¿›æ³›åŒ–èƒ½åŠ›ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/open-mla">https://sites.google.com/view/open-mla</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26642v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æœºå™¨äººæ“ä½œä»»åŠ¡çš„å¤šæ„Ÿå®˜è¯­è¨€åŠ¨ä½œï¼ˆMLAï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹èåˆäº†è§†è§‰ã€è¯­è¨€å’Œå¤šæ„Ÿå®˜ä¿¡æ¯ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººå¯¹å¤æ‚ã€æ¥è§¦ä¸°å¯Œçš„æ§åˆ¶ä»»åŠ¡çš„é€šç”¨æ€§ã€‚é€šè¿‡å¼•å…¥ç¼–ç å™¨å…è´¹çš„è·¨æ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œä»¥åŠæœªæ¥å¤šæ„Ÿå®˜ç”Ÿæˆçš„åè®­ç»ƒç­–ç•¥ï¼ŒMLAæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„äºŒç»´å’Œä¸‰ç»´VLAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLAæ¨¡å‹å…·å¤‡å¤šæ„Ÿå®˜èåˆèƒ½åŠ›ï¼Œå¯æé«˜å¯¹æœºå™¨äººæ“ä½œä»»åŠ¡çš„é€šç”¨æ€§ã€‚</li>
<li>ç¼–ç å™¨å…è´¹çš„è·¨æ¨¡æ€å¯¹é½æ–¹æ¡ˆå°†å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥ç”¨ä½œæ„ŸçŸ¥æ¨¡å—ï¼Œå®ç°å¯¹å¤šæ¨¡æ€ä¿¡å·çš„ç›´æ¥è§£è¯»ã€‚</li>
<li>MLAæ¨¡å‹é€šè¿‡è·¨æ¨¡æ€å¯¹é½ï¼Œèƒ½å¤Ÿèåˆ2Då›¾åƒã€3Dç‚¹äº‘å’Œè§¦è§‰æ ‡è®°ã€‚</li>
<li>æœªæ¥å¤šæ„Ÿå®˜ç”Ÿæˆçš„åè®­ç»ƒç­–ç•¥å¢å¼ºäº†MLAå¯¹ç‰©ç†åŠ¨æ€çš„ç†è§£ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨ç†è¯­ä¹‰ã€å‡ ä½•å’Œäº¤äº’ä¿¡æ¯ã€‚</li>
<li>MLAæ¨¡å‹åœ¨å¤æ‚ã€æ¥è§¦ä¸°å¯Œçš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç›¸å¯¹äºç°æœ‰çš„VLAæ–¹æ³•æœ‰æ˜æ˜¾çš„æå‡ã€‚</li>
<li>MLAæ¨¡å‹åœ¨æœªè§è¿‡çš„é…ç½®ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d502f7fd94a046e3a46a68cec05b38f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c07b0e2b741f4aed1dc4f90f372c494c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d46ba088b495d28a7fd35b5cd9b89c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975456&auth_key=1759975456-0-0-c3da2a38ce41ec14f37a0bd3a9b1a368&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-610abcc58b6081e13cdd1f6b1b1e1840~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975463&auth_key=1759975463-0-0-653a9f605ef9c2f303f6963469735f60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing"><a href="#Query-Kontext-An-Unified-Multimodal-Model-for-Image-Generation-and-Editing" class="headerlink" title="Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing"></a>Query-Kontext: An Unified Multimodal Model for Image Generation and   Editing</h2><p><strong>Authors:Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang</strong></p>
<p>Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal &#96;&#96;kontextâ€™â€™ composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion modelâ€™s role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLMâ€™s generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰å’Œç¼–è¾‘ï¼ˆTI2Iï¼‰æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯ä½œä¸ºé›†æˆäº†å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¿˜æ˜¯ä½œä¸ºæ—©æœŸç†è§£å’Œç”Ÿæˆæ¨¡å¼èåˆçš„ç®€å•ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å½“å‰ç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œæ¶µç›–æŒ‡ä»¤ç†è§£ã€æ¥åœ°å’Œå›¾åƒå¼•ç”¨ä»¥è¿›è¡Œèº«ä»½ä¿ç•™å’Œå¿ å®é‡å»ºçš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œä¸é«˜ä¿çœŸåˆæˆç´§å¯†ç›¸è¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Query-Kontextï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç”±æ¥è‡ªå¤šæ¨¡æ€è¾“å…¥çš„è¯­ä¹‰çº¿ç´¢å’Œç²—ç²’åº¦å›¾åƒæ¡ä»¶ç»„æˆçš„å¤šæ¨¡æ€â€œkontextâ€æ¥æ¡¥æ¢VLMå’Œæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–¹æ³•ã€‚è¿™ç§è®¾è®¡å°†å¤æ‚çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›å§”æ‰˜ç»™å¼ºå¤§çš„VLMï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜è´¨é‡è§†è§‰åˆæˆçš„ä½œç”¨ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€kontextä»¤ç‰Œå°†VLMè¿æ¥åˆ°è½»é‡çº§æ‰©æ•£å¤´ï¼Œä»¥é‡Šæ”¾VLMçš„ç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ­¤å¤´æ‰©å±•åˆ°å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜è§†è§‰ç»†èŠ‚å’Œé€¼çœŸåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥ä½çº§å›¾åƒç¼–ç å™¨ï¼Œä»¥æé«˜å›¾åƒä¿çœŸåº¦ï¼Œå¹¶å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç»¼åˆæ•°æ®ç®¡é“ï¼Œæ•´åˆçœŸå®ã€åˆæˆå’Œå¼€æºæ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„å¤šæ¨¡æ€å‚è€ƒåˆ°å›¾åƒåœºæ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€æŒ‡ä»¤é©±åŠ¨ç¼–è¾‘ã€å®šåˆ¶ç”Ÿæˆå’Œå¤šä¸»ä½“ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ç›¸åŒ¹é…ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†ä»»åŠ¡ç‰¹å®šçš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26641v1">PDF</a> 23 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆT2Iï¼‰å’Œç¼–è¾‘ï¼ˆTI2Iï¼‰ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æå‡ºäº†ä¸€ç§åä¸ºQuery-Kontextçš„æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥å®ç°å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†å’Œé«˜ä¿çœŸåˆæˆã€‚é€šè¿‡ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼ŒQuery-Kontextåœ¨ç»Ÿä¸€å¤šæ¨¡æ€æ¡†æ¶ä¸‹å®ç°äº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ç›¸åŒ¹é…ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†ä»»åŠ¡ç‰¹å®šçš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMMsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Query-Kontextæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡èåˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Query-Kontextå®ç°äº†å¼ºå¤§çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥æé«˜äº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œè§†è§‰åˆæˆçš„è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ¹é…å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿å¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡ä»»åŠ¡ç‰¹å®šæ–¹æ³•ã€‚</li>
<li>Query-Kontextå»ºç«‹äº†ç»¼åˆæ•°æ®ç®¡é“ï¼Œæ¶µç›–å¤šç§å¤šæ¨¡æ€å‚è€ƒåˆ°å›¾åƒçš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d0d8d28870244817477459c0436f743~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975471&auth_key=1759975471-0-0-bde61e30f7c202d2a4f7dd425d616453&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-61bdcf79c9e56baaa148262412fbbddf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffcc422d5fb70fa9fcb5affe9ef9867b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90ebd26c49e1984a2d508bbca9dc5e1b.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b31a6e9fe32231c994b9067429a34e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975498&auth_key=1759975498-0-0-3959a8d1e056175421a6db97a5fc0518&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-708b81740936f91df3d4b2abd48f20d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975505&auth_key=1759975505-0-0-5885ca7c041c0ed7ae26e9030d7f710c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models"><a href="#Attention-as-a-Compass-Efficient-Exploration-for-Process-Supervised-RL-in-Reasoning-Models" class="headerlink" title="Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models"></a>Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models</h2><p><strong>Authors:Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ä¸åŸºäºç»“æœçš„RLç›¸æ¯”ï¼Œè¿‡ç¨‹ç›‘ç£RLï¼ˆPSRLï¼‰ä½œä¸ºä¸€ç§æ›´æœ‰æ•ˆçš„èŒƒå¼å·²ç»å‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PSRLæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæ— è®ºæ˜¯åœ¨åˆ†æ”¯ä½ç½®è¿˜æ˜¯åœ¨é‡‡æ ·æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„PSRLæ¡†æ¶ï¼ˆAttnRLï¼‰ï¼Œä¸ºæ¨ç†æ¨¡å‹å®ç°é«˜æ•ˆæ¢ç´¢ã€‚å—åˆæ­¥è§‚å¯Ÿå¯å‘ï¼Œé«˜æ³¨æ„åŠ›å¾—åˆ†çš„æ­¥éª¤ä¸æ¨ç†è¡Œä¸ºç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºä»é«˜å€¼ä½ç½®è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‚åº”æ€§é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹å¤„ç†å¤§å°çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¿æŒéé›¶ä¼˜åŠ¿å€¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é‡‡æ ·æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸ºPSRLè®¾è®¡äº†ä¸€ä¸ªä¸€æ­¥ç¦»çº¿è®­ç»ƒç®¡é“ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½å’Œé‡‡æ ·ä»¥åŠè®­ç»ƒæ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26628v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰ç›¸è¾ƒäºç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼å±•ç°å‡ºæ›´å¤§çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„PSRLæ–¹æ³•å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè¡¨ç°åœ¨åˆ†æ”¯ä½ç½®å’Œé‡‡æ ·ä¸¤ä¸ªæ–¹é¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„PSRLæ¡†æ¶ï¼ˆAttnRLï¼‰ï¼Œä¸ºæ¨ç†æ¨¡å‹å®ç°é«˜æ•ˆæ¢ç´¢ã€‚åŸºäºåˆæ­¥è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºä»é«˜å…³æ³¨åº¦å¾—åˆ†çš„ä½ç½®è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‚åº”æ€§é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ•´ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¿æŒéé›¶ä¼˜åŠ¿å€¼ã€‚ä¸ºæé«˜é‡‡æ ·æ•ˆç‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸€æ­¥ç¦»çº¿è®­ç»ƒç®¡é“ç”¨äºPSRLã€‚åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šå…ˆå‰çš„æ–¹æ³•ï¼Œè€Œä¸”åœ¨é‡‡æ ·å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆPSRLï¼‰ç›¸æ¯”ä¼ ç»Ÿç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ æ›´ä¸ºæœ‰æ•ˆã€‚</li>
<li>ç°æœ‰PSRLæ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡ä¸Šå­˜åœ¨é—®é¢˜ï¼Œä½“ç°åœ¨åˆ†æ”¯ä½ç½®å’Œé‡‡æ ·ä¸¤ä¸ªæ–¹é¢ã€‚</li>
<li>æ–°å‹PSRLæ¡†æ¶ï¼ˆAttnRLï¼‰é€šè¿‡é«˜æ•ˆæ¢ç´¢æå‡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AttnRLåŸºäºé«˜å…³æ³¨åº¦å¾—åˆ†çš„ä½ç½®è¿›è¡Œåˆ†æ”¯ï¼Œä¸æ¨ç†è¡Œä¸ºé«˜åº¦ç›¸å…³ã€‚</li>
<li>å¼€å‘å‡ºé€‚åº”æ€§é—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒæ‰¹æ¬¡çš„ä¼˜åŠ¿å€¼ã€‚</li>
<li>ä¸€æ­¥ç¦»çº¿è®­ç»ƒç®¡é“æé«˜äº†PSRLçš„é‡‡æ ·æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b57737dd9b619b1c297a3cd43ea6e2d.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d256feefee706f30bbb907165a1ddf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975519&auth_key=1759975519-0-0-0d0a305ed00fc53ed9eb07a341514d88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1bb174b108721e88599d031f7f6917c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975525&auth_key=1759975525-0-0-6e4bb745e8aee19f568756f7fa29fdcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e3cfd7d0102ce8de5ea42b84c858aeab.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Recursive-Self-Aggregation-Unlocks-Deep-Thinking-in-Large-Language-Models"><a href="#Recursive-Self-Aggregation-Unlocks-Deep-Thinking-in-Large-Language-Models" class="headerlink" title="Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models"></a>Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models</h2><p><strong>Authors:Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</strong></p>
<p>Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains â€“ not just the final answers â€“ and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at <a target="_blank" rel="noopener" href="https://github.com/HyperPotatoNeo/RSA">https://github.com/HyperPotatoNeo/RSA</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•é€šè¿‡å¢åŠ ç”¨äºæ¨æ–­çš„è®¡ç®—é‡æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œä»¥è¿›è¡Œé¢„æµ‹ã€‚æ¨æ–­æ—¶çš„è®¡ç®—é‡å¯ä»¥é€šè¿‡é€‰æ‹©å¤šä¸ªç‹¬ç«‹è§£å†³æ–¹æ¡ˆæ¥å¹¶è¡Œç¼©æ”¾æˆ–é€šè¿‡è‡ªæˆ‘å®Œå–„æ¥é¡ºåºç¼©æ”¾ã€‚æˆ‘ä»¬æå‡ºé€’å½’è‡ªèšåˆï¼ˆRSAï¼‰æ˜¯ä¸€ç§å—è¿›åŒ–æ–¹æ³•å¯å‘çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾çš„ä¼˜åŠ¿ã€‚RSAçš„æ¯ä¸€æ­¥éƒ½ä¼šé€šè¿‡å­é›†çš„èšåˆæ¥å®Œå–„å€™é€‰æ¨ç†é“¾çš„ç§ç¾¤ï¼Œä»è€Œäº§ç”Ÿä¸€æ‰¹æ”¹è¿›åçš„è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆç„¶åè¢«ç”¨ä½œä¸‹ä¸€è½®çš„å€™é€‰æ± ã€‚RSAåˆ©ç”¨æ¨ç†é“¾ä¸­åµŒå…¥çš„ä¸°å¯Œä¿¡æ¯â€”â€”ä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆâ€”â€”å¹¶èƒ½å¤Ÿä»ä¸åŒæ€ç»´é“¾æ¡ä¸­éƒ¨åˆ†æ­£ç¡®çš„ä¸­é—´æ­¥éª¤è¿›è¡Œå¯åŠ¨ã€‚ç»éªŒä¸Šï¼ŒRSAåœ¨å¢åŠ è®¡ç®—é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRSAä½¿Qwen3-4B-Instruct-2507åœ¨åŒ…æ‹¬DeepSeek-R1å’Œo3-miniï¼ˆé«˜çº§ï¼‰ç­‰å¤§å‹æ¨ç†æ¨¡å‹ä¸­å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨AIME-25ã€HMMT-25ã€Reasoning Gymã€LiveCodeBench-v6å’ŒSuperGPQAä¸Šè¶…è¶Šäº†çº¯ç²¹çš„å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡ä¸€ç§æ–°å‹èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•è®­ç»ƒæ¨¡å‹æ¥ç»„åˆè§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥è·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/HyperPotatoNeo/RSA">https://github.com/HyperPotatoNeo/RSA</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26626v1">PDF</a> 24 pages, 9 figures</p>
<p><strong>Summary</strong>ï¼šæå‡ºçš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•â€”â€”é€’å½’è‡ªèšåˆï¼ˆRSAï¼‰èƒ½å¤Ÿåˆ©ç”¨æ¨ç†é“¾ä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°æ€§èƒ½çš„å¤§å¹…æå‡ã€‚é€šè¿‡åˆ©ç”¨é€’å½’è‡ªèšåˆçš„ä¼˜åŠ¿ï¼ŒRSAåœ¨æ¨ç†æ—¶èƒ½å¤Ÿå°†ä¸åŒçš„è§£å†³æ–¹æ¡ˆé€šè¿‡é€æ­¥ç²¾ç‚¼å¹¶ç»“åˆèµ·æ¥ï¼Œä»¥äº§ç”Ÿæ›´å¥½çš„é¢„æµ‹ç»“æœã€‚è¿™ç§æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ã€æ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—é¢„ç®—å¢åŠ çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼ŒRSAè¿˜é€šè¿‡ä¸€ç§æ–°å‹çš„èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ç»„åˆè§£å†³æ–¹æ¡ˆï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RSAæ˜¯ä¸€ç§æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œç»“åˆäº†å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾çš„ä¼˜åŠ¿ã€‚</li>
<li>RSAåˆ©ç”¨æ¨ç†é“¾ä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œé€šè¿‡ç²¾ç‚¼å€™é€‰æ¨ç†é“¾æ¥äº§ç”Ÿæ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>RSAèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—é¢„ç®—å¢åŠ çš„æƒ…å†µä¸‹ã€‚</li>
<li>RSAåœ¨ä¸åŒè§„æ¨¡ã€ä¸åŒå®¶æ—çš„æ¨¡å‹ä¸Šéƒ½è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>RSAé€šè¿‡ä¸å¤§å‹æ¨ç†æ¨¡å‹æ¯”è¾ƒå±•ç°å‡ºç«äº‰åŠ›ï¼Œä¾‹å¦‚DeepSeek-R1å’Œo3-miniã€‚</li>
<li>RSAçš„æ€§èƒ½ä¼˜äºçº¯ç²¹çš„å¹¶è¡Œå’Œé¡ºåºç¼©æ”¾ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdc4774d8546712cbdd72d31c057582e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba8d094778b884fb914c501034be57f7.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-9cca95255e96310eaaba4068e3377bd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975552&auth_key=1759975552-0-0-ac974e6f24b29ac6afb718f08df06029&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-4b90768f427a647c5243156305c14f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-364de5b22ab25d78dfc7c43dc6a64f58.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generating-Difficult-to-Translate-Texts"><a href="#Generating-Difficult-to-Translate-Texts" class="headerlink" title="Generating Difficult-to-Translate Texts"></a>Generating Difficult-to-Translate Texts</h2><p><strong>Authors:VilÃ©m Zouhar, Wenda Xu, Parker Riley, Juraj Juraska, Mara Finkelstein, Markus Freitag, Dan Deutsch</strong></p>
<p>Machine translation benchmarks sourced from the real world are quickly obsoleted, due to most examples being easy for state-of-the-art translation models. This limits the benchmarkâ€™s ability to distinguish which model is better or to reveal modelsâ€™ weaknesses. Current methods for creating difficult test cases, such as subsampling or from-scratch synthesis, either fall short of identifying difficult examples or suffer from a lack of diversity and naturalness. Inspired by the iterative process of human experts probing for model failures, we propose MT-breaker, a method where a large language model iteratively refines a source text to increase its translation difficulty. The LLM iteratively queries a target machine translation model to guide its generation of difficult examples. Our approach generates examples that are more challenging for the target MT model while preserving the diversity of natural texts. While the examples are tailored to a particular machine translation model during the generation, the difficulty also transfers to other models and languages. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œæ¥æºçš„æœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•ç”±äºå¤§å¤šæ•°å®ä¾‹å¯¹äºæœ€å…ˆè¿›çš„ç¿»è¯‘æ¨¡å‹æ¥è¯´è¿‡äºç®€å•è€Œå¾ˆå¿«è¢«æ·˜æ±°ã€‚è¿™é™åˆ¶äº†åŸºå‡†æµ‹è¯•åŒºåˆ†å“ªä¸ªæ¨¡å‹æ›´å¥½æˆ–æ­ç¤ºæ¨¡å‹å¼±ç‚¹çš„èƒ½åŠ›ã€‚å½“å‰åˆ›å»ºå›°éš¾æµ‹è¯•ç”¨ä¾‹çš„æ–¹æ³•ï¼Œå¦‚å­é‡‡æ ·æˆ–ä»å¤´å¼€å§‹åˆæˆï¼Œè¦ä¹ˆä¸èƒ½è¯†åˆ«å‡ºå›°éš¾çš„ä¾‹å­ï¼Œè¦ä¹ˆç¼ºä¹å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚å—äººç±»ä¸“å®¶æ¢æµ‹æ¨¡å‹å¤±è´¥è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºMT-breakeræ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–æºæ–‡æœ¬æ¥å¢åŠ å…¶ç¿»è¯‘éš¾åº¦çš„æ–¹æ³•ã€‚å¤§å‹è¯­è¨€æ¨¡å‹è¿­ä»£åœ°æŸ¥è¯¢ç›®æ ‡æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œä»¥æŒ‡å¯¼å…¶ç”Ÿæˆå›°éš¾çš„ä¾‹å­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºç›®æ ‡æœºå™¨ç¿»è¯‘æ¨¡å‹ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„ä¾‹å­ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æ–‡æœ¬çš„å¤šæ ·æ€§ã€‚è™½ç„¶è¿™äº›ä¾‹å­æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é’ˆå¯¹ç‰¹å®šçš„æœºå™¨ç¿»è¯‘æ¨¡å‹å®šåˆ¶çš„ï¼Œä½†éš¾åº¦ä¹Ÿä¼šè½¬ç§»åˆ°å…¶ä»–æ¨¡å‹å’Œè¯­è¨€ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºæœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œå› ä¸ºå¤§å¤šæ•°æ¥è‡ªçœŸå®ä¸–ç•Œçš„ä¾‹å­å¯¹äºå½“å‰æœ€å…ˆè¿›çš„ç¿»è¯‘æ¨¡å‹æ¥è¯´è¿‡äºç®€å•ï¼Œæ— æ³•åŒºåˆ†æ¨¡å‹ä¼˜åŠ£æˆ–æ­ç¤ºå…¶å¼±ç‚¹ã€‚ç°æœ‰çš„åˆ›å»ºå›°éš¾æµ‹è¯•ç”¨ä¾‹çš„æ–¹æ³•ï¼Œå¦‚å­æŠ½æ ·æˆ–ä»å¤´åˆæˆï¼Œè¦ä¹ˆæ— æ³•è¯†åˆ«å›°éš¾çš„ä¾‹å­ï¼Œè¦ä¹ˆç¼ºä¹å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚å—äººç±»ä¸“å®¶æ¢æŸ¥æ¨¡å‹å¤±è´¥è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MT-breakeræ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿­ä»£åœ°æ”¹è¿›æºæ–‡æœ¬ä»¥å¢åŠ å…¶ç¿»è¯‘éš¾åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£æŸ¥è¯¢ç›®æ ‡æœºå™¨ç¿»è¯‘æ¨¡å‹æ¥æŒ‡å¯¼å›°éš¾ä¾‹å­çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆå¯¹ç›®æ ‡ç¿»è¯‘æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§çš„ä¾‹å­ï¼ŒåŒæ—¶ä¿æŒäº†è‡ªç„¶æ–‡æœ¬çš„å¤šæ ·æ€§ã€‚è™½ç„¶è¿™äº›ä¾‹å­æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é’ˆå¯¹ç‰¹å®šçš„æœºå™¨ç¿»è¯‘æ¨¡å‹å®šåˆ¶çš„ï¼Œä½†å…¶éš¾åº¦ä¹Ÿé€‚ç”¨äºå…¶ä»–æ¨¡å‹å’Œè¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•å› å¤§å¤šæ•°å®ä¾‹è¿‡äºç®€å•è€Œè¿…é€Ÿè¢«æ·˜æ±°ã€‚</li>
<li>ç°æœ‰åˆ›å»ºå›°éš¾æµ‹è¯•ç”¨ä¾‹çš„æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«å›°éš¾ä¾‹å­æˆ–ç¼ºä¹å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MT-breakerï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿­ä»£æ”¹è¿›æºæ–‡æœ¬ï¼Œå¢åŠ ç¿»è¯‘éš¾åº¦ã€‚</li>
<li>MT-breakeré€šè¿‡æŸ¥è¯¢ç›®æ ‡æœºå™¨ç¿»è¯‘æ¨¡å‹æ¥æŒ‡å¯¼å›°éš¾ä¾‹å­çš„ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„ä¾‹å­å¯¹ç›®æ ‡ç¿»è¯‘æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>MT-breakerä¿ç•™è‡ªç„¶æ–‡æœ¬çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7e21def68a31a12955d3d1e087d60cad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975574&auth_key=1759975574-0-0-7dadac53af5ece0f2949bb11efdfda75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-205cfd9d9d38875ddddf4c3c1a222ab1.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0bb02601c6414cc082909f014444f91~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975587&auth_key=1759975587-0-0-41bd1d6d53f05d08243f10791e9c8c47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-327cdd550958cddf59e18220cdea3c83~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975594&auth_key=1759975594-0-0-bb10241668f490c26838c70e297d5aa0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-f36813d84b4ee991df6c1163b8a7a9e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d22f12720089c0bdeafc858abb6f6718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca0551b11b018308291973aa0f90016f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19fcfa7f76b4ac690a554791a6d630ab.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Linking-Process-to-Outcome-Conditional-Reward-Modeling-for-LLM-Reasoning"><a href="#Linking-Process-to-Outcome-Conditional-Reward-Modeling-for-LLM-Reasoning" class="headerlink" title="Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning"></a>Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning</h2><p><strong>Authors:Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren</strong></p>
<p>Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä½œä¸ºä¸€ç§æ–°å…´æ–¹æ³•ï¼Œé€šè¿‡æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€æ­¥æ¨ç†ä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMè¦ä¹ˆå­¤ç«‹åœ°å¤„ç†æ¯ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ— æ³•æ•æ‰æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ï¼Œè¦ä¹ˆåœ¨å°†æµç¨‹å¥–åŠ±ä¸æœ€ç»ˆç»“æœå¯¹é½æ—¶é‡åˆ°å›°éš¾ã€‚å› æ­¤ï¼Œå¥–åŠ±ä¿¡å·æ— æ³•å°Šé‡åºåˆ—æ¨ç†ä¸­çš„æ—¶é—´å› æœå…³ç³»ï¼Œå¹¶é¢ä¸´æ¨¡ç³Šçš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚è¿™äº›å±€é™æ€§ä½¿ä¸‹æ¸¸æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26578v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œç°æœ‰PRMæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å¿½è§†æ¨ç†æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»ã€éš¾ä»¥å°†è¿‡ç¨‹å¥–åŠ±ä¸æœ€ç»ˆæˆæœå¯¹é½ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰ï¼Œå°†LLMæ¨ç†è§†ä¸ºä¸€ä¸ªé€šå‘æ­£ç¡®ç­”æ¡ˆçš„æ—¶ç©ºè¿‡ç¨‹ã€‚CRMçš„å¥–åŠ±ä¸ä»…ä»¥å‰ä¸€æ­¥éª¤ä¸ºæ¡ä»¶ï¼Œè¿˜ä¸æ¨ç†è½¨è¿¹çš„æœ€ç»ˆç»“æœæœ‰æ˜ç¡®è”ç³»ã€‚é€šè¿‡æ‰§è¡Œæ¡ä»¶æ¦‚ç‡è§„åˆ™ï¼ŒCRMæ•æ‰æ¨ç†æ­¥éª¤é—´çš„å› æœå…³ç³»ï¼Œè§£å†³ä¿¡ç”¨åˆ†é…æ¨¡ç³Šé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒCRMåœ¨Best-of-Né‡‡æ ·ã€é›†æŸæœç´¢å’Œå¼ºåŒ–å­¦ä¹ ç­‰æ–¹é¢å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰å¥–åŠ±æ¨¡å‹çš„æ•ˆæœï¼Œä¸ºæå‡LLMæ¨ç†èƒ½åŠ›æä¾›äº†æœ‰åŠ›æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰PRMæ–¹æ³•å­˜åœ¨å¿½ç•¥æ¨ç†æ­¥éª¤é—´ä¾èµ–å’Œå¯¹é½è¿‡ç¨‹å¥–åŠ±ä¸æœ€ç»ˆç»“æœçš„é—®é¢˜ã€‚</li>
<li>æ¡ä»¶å¥–åŠ±å»ºæ¨¡ï¼ˆCRMï¼‰å°†LLMæ¨ç†è§†ä¸ºä¸€ä¸ªè¿‡ç¨‹ï¼Œå¼ºè°ƒæ¯ä¸€æ­¥çš„å¥–åŠ±ä¸æœ€ç»ˆç»“æœçš„è”ç³»ã€‚</li>
<li>CRMé€šè¿‡æ‰§è¡Œæ¡ä»¶æ¦‚ç‡è§„åˆ™æ•æ‰æ¨ç†æ­¥éª¤é—´çš„å› æœå…³ç³»ï¼Œè§£å†³ä¿¡ç”¨åˆ†é…æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>CRMåœ¨å¤šç§å®éªŒè®¾ç½®ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬Best-of-Né‡‡æ ·ã€é›†æŸæœç´¢å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>CRMæ¡†æ¶æ›´ç¨³å¥ï¼Œèƒ½æœ‰æ•ˆæŠµæŠ—å¥–åŠ±æ“çºµï¼Œå¹¶åœ¨ä¸ä¾èµ–çœŸå®éªŒè¯å¥–åŠ±çš„æƒ…å†µä¸‹å®ç°ç¨³å®šçš„ä¸‹æ¸¸æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2d59a146df525ec2e2bb97853be64d68~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975630&auth_key=1759975630-0-0-7003c548eb7540e7534ba0494c09c41f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e6dc68c69b3285c265f4825a3c070a59.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-f97dd050fb402ed4de690e006eafb52b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975644&auth_key=1759975644-0-0-bc44b40f7c3c9017b501ceb27b695848&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark"><a href="#Probing-the-Critical-Point-CritPt-of-AI-Reasoning-a-Frontier-Physics-Research-Benchmark" class="headerlink" title="Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark"></a>Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark</h2><p><strong>Authors:Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, YaÃ¯r Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng</strong></p>
<p>While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced â€œcritical pointâ€), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular &amp; optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools. </p>
<blockquote>
<p>éšç€å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢å–å¾—å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬èƒ½å¦æœ‰æ•ˆåº”å¯¹å‰æ²¿ç‰©ç†ç ”ç©¶ä¸­é‡åˆ°çš„å¤æ‚ã€å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Ÿè‡³å…³é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œç‰©ç†å­¦å®¶å¸Œæœ›LLMè¾…åŠ©å®Œæˆå“ªäº›æ¨ç†ä»»åŠ¡ï¼Ÿä¸ºäº†è§£ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CritPtï¼ˆå¤æ‚ç ”ç©¶ç»¼åˆæ€ç»´ç‰©ç†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æœªå‘è¡¨çš„ã€ç ”ç©¶çº§æ¨ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•å¹¿æ³›è¦†ç›–äº†ç°ä»£ç‰©ç†ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡èšæ€ç‰©ç†ã€é‡å­åŠ›å­¦ã€åŸå­ã€åˆ†å­å’Œå…‰å­¦ç‰©ç†ã€å¤©ä½“ç‰©ç†ã€é«˜èƒ½ç‰©ç†ã€æ•°å­¦ç‰©ç†ã€ç»Ÿè®¡ç‰©ç†ã€æ ¸ç‰©ç†ã€éçº¿æ€§åŠ¨åŠ›å­¦ã€æµä½“åŠ›å­¦å’Œç”Ÿç‰©ç‰©ç†å­¦ç­‰ã€‚CritPtç”±71ä¸ªç»„åˆçš„ç ”ç©¶æŒ‘æˆ˜æ„æˆï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå…¥é—¨çº§åˆ«çš„å…¨é¢ç ”ç©¶é¡¹ç›®ï¼ŒåŒæ—¶åˆ†è§£ä¸º190ä¸ªæ›´ç®€å•çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ï¼Œä»¥è·å–æ›´ç²¾ç»†çš„è§è§£ã€‚æ‰€æœ‰é—®é¢˜å‡ç”±50å¤šåæ´»è·ƒçš„ç‰©ç†ç ”ç©¶è€…æ ¹æ®è‡ªå·±çš„ç ”ç©¶å…¨æ–°åˆ›å»ºã€‚æ¯ä¸ªé—®é¢˜éƒ½ç»è¿‡æ‰‹å·¥ç­›é€‰ï¼Œä»¥å¾—å‡ºç»å¾—èµ·çŒœæµ‹ä¸”å¯æœºå™¨éªŒè¯çš„ç­”æ¡ˆï¼Œå¹¶ç”±é’ˆå¯¹é«˜çº§ç‰©ç†ç‰¹å®šè¾“å‡ºæ ¼å¼è¿›è¡Œå¤§é‡è‡ªå®šä¹‰çš„è‡ªåŠ¨è¯„åˆ†ç®¡é“è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å•ç‹¬çš„æ£€æŸ¥ç‚¹ä¸Šæ˜¾ç¤ºå‡ºæ—©æœŸæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶è¿œè¿œä¸èƒ½å¯é åœ°è§£å†³å…¨é¢çš„ç ”ç©¶è§„æ¨¡æŒ‘æˆ˜ï¼šåŸºç¡€æ¨¡å‹ä¸­æœ€å¥½çš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º4.0%ï¼Œç”±GPT-5ï¼ˆé«˜çº§ï¼‰å®ç°ï¼Œé…å¤‡ç¼–ç å·¥å…·æ—¶é€‚åº¦æå‡è‡³çº¦10%ã€‚é€šè¿‡CritPtæä¾›çš„ç°å®ä¸”æ ‡å‡†åŒ–çš„è¯„ä¼°ï¼Œæˆ‘ä»¬çªå‡ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸ç°å®ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ï¼Œä¸ºç§‘å­¦åŸºç¡€çš„äººå·¥æ™ºèƒ½å·¥å…·çš„å‘å±•æä¾›äº†æŒ‡å¯¼åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26574v1">PDF</a> 39 pages, 6 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›åœ¨é«˜ä¸­æ•°å­¦ç«èµ›å’Œç¼–ç¨‹æ–¹é¢è¿…é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå…¶åœ¨å‰æ²¿ç‰©ç†ç ”ç©¶ä¸­çš„å¤æ‚ã€å¼€æ”¾æ€§çš„æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ¨ç†èƒ½åŠ›å¦‚ä½•ï¼Ÿé’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†CritPtï¼ˆComplex Research using Integrated Thinking - Physics Testï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•LLMåœ¨æœªå…¬å¸ƒçš„ç ”ç©¶çº§æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMåœ¨å­¤ç«‹çš„æ£€æŸ¥ç‚¹ä¸Šæœ‰æ—©æœŸæ‰¿è¯ºï¼Œä½†åœ¨è§£å†³å…¨é¢çš„ç ”ç©¶è§„æ¨¡æŒ‘æˆ˜æ–¹é¢ä»æœ‰å¾ˆå¤§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CritPtåŸºå‡†æµ‹è¯•æ—¨åœ¨æµ‹è¯•LLMåœ¨å‰æ²¿ç‰©ç†ç ”ç©¶é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¶µç›–ç°ä»£ç‰©ç†ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡èšæ€ç‰©ç†ã€é‡å­åŠ›å­¦ã€åŸå­ã€åˆ†å­ä¸å…‰å­¦ç‰©ç†ç­‰ã€‚</li>
<li>ç”±50å¤šåæ´»è·ƒçš„ç‰©ç†ç ”ç©¶è€…åŸºäºè‡ªèº«ç ”ç©¶åˆ›ä½œçš„é—®é¢˜ï¼Œç”¨äºæ¨¡æ‹Ÿå®Œæ•´çš„ç ”ç©¶é¡¹ç›®ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„LLMåœ¨è§£å†³å…¨é¢çš„ç ”ç©¶è§„æ¨¡æŒ‘æˆ˜æ–¹é¢ä»æœ‰è¾ƒå¤§å·®è·ï¼Œå¹³å‡å‡†ç¡®ç‡ä»…ä¸º4.0%ã€‚</li>
<li>GPT-5ï¼ˆé«˜çº§ï¼‰æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ç•¥æœ‰æé«˜ï¼Œè¾¾åˆ°çº¦10%ï¼Œä½†ä»…é™äºé…å¤‡ç¼–ç¨‹å·¥å…·æ—¶ã€‚</li>
<li>å½“å‰æ¨¡å‹èƒ½åŠ›ä¸å®é™…ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´å­˜åœ¨è¾ƒå¤§æ–­å±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c8c97a57af2545ca836897bec26ad1c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975651&auth_key=1759975651-0-0-ac1ddf7fbf0e87c6387083f0c2dbd936&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-785387efd68ebd5cbc444841d911aad2.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-6bbd6e27faed92b6a96b6a925c78681b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975665&auth_key=1759975665-0-0-6b14bdb733880a43b1b218b296b3e4c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Verified-Code-Reasoning-by-LLMs"><a href="#Towards-Verified-Code-Reasoning-by-LLMs" class="headerlink" title="Towards Verified Code Reasoning by LLMs"></a>Towards Verified Code Reasoning by LLMs</h2><p><strong>Authors:Meghana Sistla, Gogul Balakrishnan, Pat Rondon, JosÃ© Cambronero, Michele Tufano, Satish Chandra</strong></p>
<p>While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).   As a result of this lack of trustworthiness, the agentâ€™s answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agentâ€™s response and, subsequently, using formal verification and program analysis tools to verify the agentâ€™s reasoning steps.   We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agentâ€™s reasoning on 13&#x2F;20 examples, and for the program equivalence queries, the formal verification step successfully caught 6&#x2F;8 incorrect judgments made by the agent. </p>
<blockquote>
<p>è™½ç„¶åŸºäºLLMçš„ä»£ç†èƒ½å¤Ÿå¤„ç†å„ç§ä»£ç æ¨ç†é—®é¢˜ï¼Œä½†ç­”æ¡ˆå¹¶ä¸æ€»æ˜¯æ­£ç¡®çš„ã€‚è¿™å¯¼è‡´åœ¨éœ€è¦é«˜ç²¾åº¦çš„æƒ…å¢ƒä¸‹ï¼Œä»£ç†æ— æ³•å‘æŒ¥ç”¨å¤„ï¼Œä¾‹å¦‚åœ¨ä»¥ä¸‹åœºæ™¯ï¼šï¼ˆ1ï¼‰å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆç†è§£æ–°çš„ä»£ç åº“ï¼Œï¼ˆ2ï¼‰åœ¨ä»£ç å®¡æŸ¥ä¼šè¯æœŸé—´å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œï¼ˆ3ï¼‰ç¡®ä¿ç”±è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç³»ç»Ÿç”Ÿæˆçš„ä»£ç æ»¡è¶³æŸäº›è¦æ±‚ï¼ˆä¾‹å¦‚ä¿®å¤é”™è¯¯ã€æé«˜å¯è¯»æ€§ã€å®ç°åŠŸèƒ½ï¼‰ã€‚ç”±äºè¿™ç§ç¼ºä¹å¯ä¿¡åº¦çš„æƒ…å†µï¼Œä»£ç†çš„ç­”æ¡ˆéœ€è¦åœ¨æ‰‹åŠ¨ç¡®è®¤åæ‰èƒ½è¢«ä¿¡ä»»ã€‚æ‰‹åŠ¨ç¡®è®¤æ¥è‡ªä»£ç æ¨ç†ä»£ç†çš„å“åº”éœ€è¦äººåŠ›ï¼Œå¯èƒ½å¯¼è‡´å¼€å‘äººå‘˜ç”Ÿäº§ç‡é™ä½ï¼Œä»è€Œå‰Šå¼±äº†ä»£ç†çš„åŠ©ç†æ•ˆç›Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§é€šè¿‡éªŒè¯å…¶æ¨ç†æ­¥éª¤æ¥è‡ªåŠ¨éªŒè¯ä»£ç æ¨ç†ä»£ç†æä¾›çš„ç­”æ¡ˆçš„æ–¹æ³•ã€‚åœ¨å¾ˆé«˜çš„å±‚æ¬¡ä¸Šï¼Œè¯¥æ–¹æ³•ç”±æå–ä»£ç†å“åº”çš„å½¢å¼è¡¨ç¤ºå¼€å§‹ï¼Œéšåä½¿ç”¨å½¢å¼åŒ–éªŒè¯å’Œç¨‹åºåˆ†æå·¥å…·æ¥éªŒè¯ä»£ç†çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬å°†è¿™ç§æ–¹æ³•åº”ç”¨åˆ°äº†ç”±æ£€æŸ¥å™¨æ£€æµ‹åˆ°çš„20ä¸ªæœªåˆå§‹åŒ–å˜é‡é”™è¯¯å’Œç¨‹åºç­‰ä»·æŸ¥è¯¢çš„åŸºå‡†æµ‹è¯•é›†ä¸Šã€‚å¯¹äºæœªåˆå§‹åŒ–å˜é‡é”™è¯¯ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤èƒ½å¤ŸéªŒè¯ä»£ç†åœ¨å…¶ä¸­çš„13&#x2F;20ä¸ªä¾‹å­ä¸Šçš„æ¨ç†è¿‡ç¨‹ï¼›è€Œå¯¹äºç¨‹åºç­‰ä»·æŸ¥è¯¢ï¼Œå½¢å¼åŒ–éªŒè¯æ­¥éª¤æˆåŠŸæ•è·äº†ä»£ç†ä½œå‡ºçš„é”™è¯¯çš„6&#x2F;8ä¸ªåˆ¤æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26546v1">PDF</a> 43 pages</p>
<p><strong>Summary</strong>ï¼šåŸºäºLLMçš„ä»£ç†èƒ½å¤Ÿå¤„ç†å„ç§ä»£ç æ¨ç†é—®é¢˜ï¼Œä½†ç­”æ¡ˆå¹¶ä¸æ€»æ˜¯æ­£ç¡®ã€‚è¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦é«˜ç²¾åº¦çš„åœºæ™¯ï¼ˆå¦‚å¸®åŠ©è½¯ä»¶å·¥ç¨‹å¸ˆç†è§£æ–°ä»£ç åº“ã€å‚ä¸ä»£ç å®¡æŸ¥ä¼šè¯ä»¥åŠç¡®ä¿è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç³»ç»Ÿç”Ÿæˆçš„ä»£ç ç¬¦åˆè¦æ±‚ï¼‰ä¸­çš„åº”ç”¨ä»·å€¼ã€‚ä¸ºè§£å†³ä¿¡ä»»é—®é¢˜ï¼Œéœ€è¦å¯¹ä»£ç†çš„ç­”æ¡ˆè¿›è¡Œæ‰‹åŠ¨éªŒè¯ï¼Œè¿™ä¼šæ¶ˆè€—äººåŠ›å¹¶é™ä½å¼€å‘è€…çš„å·¥ä½œæ•ˆç‡ï¼Œå‰Šå¼±äº†ä»£ç†çš„è¾…åŠ©ä½œç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡éªŒè¯å…¶æ¨ç†æ­¥éª¤æ¥è‡ªåŠ¨éªŒè¯ä»£ç æ¨ç†ä»£ç†ç­”æ¡ˆçš„æ–¹æ³•ã€‚æ­¤æ–¹æ³•åŒ…æ‹¬æå–ä»£ç†å“åº”çš„å½¢å¼è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å½¢å¼åŒ–éªŒè¯å’Œç¨‹åºåˆ†æå·¥å…·æ¥éªŒè¯ä»£ç†çš„æ¨ç†æ­¥éª¤ã€‚åº”ç”¨äºä¸€ç»„æœªåˆå§‹åŒ–å˜é‡é”™è¯¯å’Œç¨‹åºç­‰ä»·æŸ¥è¯¢çš„åŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æ–¹æ³•åœ¨éªŒè¯æœªåˆå§‹åŒ–å˜é‡é”™è¯¯æ—¶å¯¹ä»£ç†æ¨ç†è¿›è¡Œäº†13&#x2F;20çš„æœ‰æ•ˆéªŒè¯ï¼Œåœ¨ç¨‹åºç­‰ä»·æŸ¥è¯¢ä¸­æˆåŠŸæ•æ‰åˆ°äº†ä»£ç†ä½œå‡ºçš„8&#x2F;6æ¬¡é”™è¯¯åˆ¤æ–­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäºLLMçš„ä»£ç†åœ¨å¤„ç†ä»£ç æ¨ç†é—®é¢˜æ—¶å­˜åœ¨ç­”æ¡ˆä¸æ€»æ˜¯æ­£ç¡®çš„é—®é¢˜ã€‚</li>
<li>è¿™é™åˆ¶äº†ä»£ç†åœ¨éœ€è¦é«˜ç²¾åº¦çš„åº”ç”¨åœºæ™¯ä¸­çš„ä»·å€¼ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹å¸ˆçš„ä»£ç ç†è§£å’Œå®¡æŸ¥ä»¥åŠè‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆç³»ç»Ÿçš„éªŒè¯ã€‚</li>
<li>æ‰‹åŠ¨éªŒè¯ä»£ç†ç­”æ¡ˆä¼šå¢åŠ äººåŠ›æ¶ˆè€—ï¼Œé™ä½å¼€å‘è€…æ•ˆç‡ï¼Œå‰Šå¼±ä»£ç†çš„è¾…åŠ©ä½œç”¨ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨éªŒè¯ä»£ç æ¨ç†ä»£ç†ç­”æ¡ˆçš„æ–¹æ³•ï¼Œé€šè¿‡éªŒè¯å…¶æ¨ç†æ­¥éª¤æ¥è§£å†³ä¿¡ä»»é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬æå–ä»£ç†å“åº”çš„å½¢å¼è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å½¢å¼åŒ–éªŒè¯å’Œç¨‹åºåˆ†æå·¥å…·æ¥è¿›è¡ŒéªŒè¯ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¯¹ä»£ç†æ¨ç†çš„éªŒè¯æ•ˆæœæœ‰é™ï¼Œä½†ä»èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜éªŒè¯æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e9b49ce41fc23266b3d77e4cfc8184e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975672&auth_key=1759975672-0-0-c4bc4504c6a9786e14624f647ffab692&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77841954c65f0980ce26fce1676f1c63~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975680&auth_key=1759975680-0-0-b9df5e84bdf61cab494da8f2cf808684&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38fcd48d296c4218694ac84ab3c77884~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975687&auth_key=1759975687-0-0-7ce2f0754fb30488354d8501b6304392&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-257169335ab6fde266106d29758c44b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975693&auth_key=1759975693-0-0-c503d075351269e94cc35eebd907f767&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-916f62967da2d25cf24ecb40aa82f2cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975700&auth_key=1759975700-0-0-40558be7a2ab80320a3195aba47016e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea01157cbec1e51a82f7a2e9728f7bf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975707&auth_key=1759975707-0-0-a8cf8376a5b0763eb891ff1efa370290&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-9b23fe611dd97ae7347bc00e0b399eb1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TASP-Topology-aware-Sequence-Parallelism"><a href="#TASP-Topology-aware-Sequence-Parallelism" class="headerlink" title="TASP: Topology-aware Sequence Parallelism"></a>TASP: Topology-aware Sequence Parallelism</h2><p><strong>Authors:Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang</strong></p>
<p>Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at <a target="_blank" rel="noopener" href="https://github.com/infinigence/HamiltonAttention">https://github.com/infinigence/HamiltonAttention</a>. </p>
<blockquote>
<p>é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§è€Œé¢ä¸´çº¦æŸã€‚ä¸»æµçš„åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ–¹æ³•â€”â€”ç¯å½¢æ³¨æ„åŠ›ï¼Œè¯•å›¾é€šè¿‡å°†åœ¨åŠ é€Ÿå™¨é—´åˆ†å¸ƒçš„æŸ¥è¯¢åˆ†æˆå¤šä¸ªæŸ¥è¯¢å—æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å…è®¸æ¯ä¸ªQå¼ é‡é€šè¿‡ç¯å½¢å…¨æ”¶é›†é€šä¿¡åŸè¯­è®¿é—®å…¶ä»–åŠ é€Ÿå™¨çš„æ‰€æœ‰KVå¼ é‡ã€‚ç„¶è€Œï¼Œå®ƒè¡¨ç°å‡ºè¾ƒä½çš„é€šä¿¡æ•ˆç‡ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚è¿™ç§ä½æ•ˆç‡æºäºå…¶é‡‡ç”¨çš„ç¯å½¢å…¨æ”¶é›†é€šä¿¡åŸè¯­ä¸ç°ä»£åŠ é€Ÿå™¨çš„æ‰€æœ‰åˆ°æ‰€æœ‰æ‹“æ‰‘ç»“æ„ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ç¯å½¢å…¨æ”¶é›†åŸè¯­ç”±ç¯å½¢æ•°æ®è½¬ç§»çš„è¿­ä»£ç»„æˆï¼Œåªèƒ½åˆ©ç”¨åˆ°æ‰€æœ‰åˆ°æ‰€æœ‰æ‹“æ‰‘çš„ä¸€ä¸ªå¾ˆå°éƒ¨åˆ†ã€‚å—å®Œå…¨æœ‰å‘å›¾çš„å“ˆå¯†é¡¿åˆ†è§£çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°ç°ä»£åŠ é€Ÿå™¨æ‹“æ‰‘å¯ä»¥åˆ†è§£æˆå¤šä¸ªæ­£äº¤ç¯å½¢æ•°æ®è·¯å¾„ï¼Œè¿™äº›è·¯å¾„å¯ä»¥å¹¶å‘ä¼ è¾“æ•°æ®è€Œä¸ä¼šç›¸äº’å¹²æ‰°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ç¯å½¢å…¨æ”¶é›†åŸè¯­ä¹Ÿå¯ä»¥åˆ†è§£ä¸ºåŒæ ·æ•°é‡çš„å¹¶å‘ç¯å½¢æ•°æ®è½¬ç§»ï¼Œæ¯æ¬¡è¿­ä»£æ—¶éƒ½æ˜¯å¦‚æ­¤ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡LLMçš„æ‹“æ‰‘æ„ŸçŸ¥SPæ–¹æ³•TASPï¼Œå®ƒé€šè¿‡å¯¹ç°ä»£åŠ é€Ÿå™¨çš„æ‹“æ‰‘ç»“æ„å’ŒåŸå§‹ç»“æ„è¿›è¡Œåˆ†è§£æ¥å……åˆ†åˆ©ç”¨å…¶é€šä¿¡èƒ½åŠ›ã€‚åœ¨å•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹çš„NVIDIA H100ç³»ç»Ÿä»¥åŠå•èŠ‚ç‚¹çš„AMD MI300Xç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTASPåœ¨ç°ä»£åŠ é€Ÿå™¨æ‹“æ‰‘ä¸Šå®ç°äº†æ¯”ç¯å½¢æ³¨æ„åŠ›æ›´é«˜çš„é€šä¿¡æ•ˆç‡ï¼Œå¹¶ä¸”ç›¸å¯¹äºç¯å½¢æ³¨æ„åŠ›å’Œå…¶å˜ä½“Zigzag-Ring Attentionå®ç°äº†æœ€é«˜è¾¾3.58çš„åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/infinigence/HamiltonAttention%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/infinigence/HamiltonAttentionä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶äºŒæ¬¡å¤æ‚åº¦çº¦æŸé—®é¢˜ã€‚ä¸»æµåºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ–¹æ³•ï¼Œå³Ring Attentionï¼Œé€šè¿‡åˆ†å¸ƒå¼æŸ¥è¯¢æ¥å°è¯•è§£å†³æ­¤é—®é¢˜ï¼Œä½†å­˜åœ¨é€šä¿¡æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä½œè€…åŸºäºç°ä»£åŠ é€Ÿå™¨çš„AlltoAllæ‹“æ‰‘ä¸Ring AllGatheré€šä¿¡åŸå§‹ç»„ä»¶çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæå‡ºä¸€ç§å…¨æ–°çš„æ‹“æ‰‘æ„ŸçŸ¥åºåˆ—å¹¶è¡Œæ–¹æ³•TASPã€‚é€šè¿‡åˆ†è§£ç°ä»£åŠ é€Ÿå™¨çš„æ‹“æ‰‘ç»“æ„å’Œé€šä¿¡åŸå§‹ç»„ä»¶ï¼ŒTASPèƒ½å……åˆ†åˆ©ç”¨ç°ä»£åŠ é€Ÿå™¨çš„é€šä¿¡å®¹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹çš„NVIDIA H100ç³»ç»Ÿä»¥åŠå•èŠ‚ç‚¹çš„AMD MI300Xç³»ç»Ÿä¸Šï¼ŒTASPåœ¨é€šä¿¡æ•ˆç‡ä¸Šä¼˜äºRing Attentionï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†æœ€é«˜è¾¾3.58å€çš„æ€§èƒ½åŠ é€Ÿã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦çº¦æŸæŒ‘æˆ˜ã€‚</li>
<li>ä¸»æµåºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ–¹æ³•ï¼Œå¦‚Ring Attentionï¼Œè™½èƒ½è§£å†³éƒ¨åˆ†é—®é¢˜ï¼Œä½†é€šä¿¡æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>é€šä¿¡æ•ˆç‡ä½ä¸‹æºäºRing AllGatheré€šä¿¡åŸå§‹ç»„ä»¶ä¸ç°ä»£åŠ é€Ÿå™¨AlltoAllæ‹“æ‰‘ç»“æ„çš„ä¸åŒ¹é…ã€‚</li>
<li>é€šè¿‡åˆ†è§£ç°ä»£åŠ é€Ÿå™¨çš„æ‹“æ‰‘ç»“æ„å’Œé€šä¿¡åŸå§‹ç»„ä»¶ï¼Œæå‡ºä¸€ç§æ–°çš„æ‹“æ‰‘æ„ŸçŸ¥åºåˆ—å¹¶è¡Œæ–¹æ³•TASPã€‚</li>
<li>TASPèƒ½å……åˆ†åˆ©ç”¨ç°ä»£åŠ é€Ÿå™¨çš„é€šä¿¡å®¹é‡ï¼Œæé«˜é€šä¿¡æ•ˆç‡ã€‚</li>
<li>åœ¨ä¸åŒç³»ç»Ÿä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTASPç›¸è¾ƒäºRing Attentionæœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7ea1805b708b8bca69dfe0149eec1193~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975721&auth_key=1759975721-0-0-c76307b5a2826b621ca25e8cc7c72be1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1fe1eaaf6bad0e33c6357893a0ca2cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975728&auth_key=1759975728-0-0-7f524e44fb1977e1c15f8d64038b538c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a7f344e3b09326479316c7ef5a1c2fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1759975756&auth_key=1759975756-0-0-57e523e7873a1dde8370e4bc5d529f1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee14aee1fcd31d20f6c8672ea77ea979~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086178&auth_key=1760086178-0-0-397c89e148f5f907e6e96beb6d03fcef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents"><a href="#OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents" class="headerlink" title="OceanGym: A Benchmark Environment for Underwater Embodied Agents"></a>OceanGym: A Benchmark Environment for Underwater Embodied Agents</h2><p><strong>Authors:Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen</strong></p>
<p>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earthâ€™s last unexplored frontiers. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/OceanGPT/OceanGym">https://github.com/OceanGPT/OceanGym</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†OceanGymï¼Œè¿™æ˜¯é’ˆå¯¹æµ·æ´‹æ°´ä¸‹å®ä½“æ™ºèƒ½ä½“è®¾è®¡çš„é¦–ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨æœ€è‹›åˆ»çš„ç°å®ç¯å¢ƒä¹‹ä¸€ä¸­çš„åº”ç”¨ã€‚ä¸é™†åœ°æˆ–ç©ºä¸­é¢†åŸŸä¸åŒï¼Œæ°´ä¸‹ç¯å¢ƒå¸¦æ¥äº†æç«¯çš„æ„ŸçŸ¥å’Œå†³ç­–æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½èƒ½è§åº¦ã€åŠ¨æ€æµ·æ´‹æ°´æµï¼Œä½¿å¾—æœ‰æ•ˆéƒ¨ç½²æ™ºèƒ½ä½“å˜å¾—å¼‚å¸¸å›°éš¾ã€‚OceanGymåŒ…å«å…«ä¸ªç°å®çš„ä»»åŠ¡é¢†åŸŸå’Œä¸€ä¸ªç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨çš„ç»Ÿä¸€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†æ„ŸçŸ¥ã€è®°å¿†å’Œåºåˆ—å†³ç­–ã€‚æ™ºèƒ½ä½“éœ€è¦ç†è§£å…‰å­¦å’Œå£°çº³æ•°æ®ï¼Œè‡ªä¸»æ¢ç´¢å¤æ‚ç¯å¢ƒï¼Œå¹¶åœ¨è¿™äº›æ¶åŠ£æ¡ä»¶ä¸‹å®Œæˆé•¿æœŸç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„MLLMé©±åŠ¨çš„æ™ºèƒ½ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œå‡¸æ˜¾äº†æµ·æ´‹æ°´ä¸‹ç¯å¢ƒä¸­æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ€§çš„æŒä¹…æ€§éš¾é¢˜ã€‚é€šè¿‡æä¾›é«˜ä¿çœŸã€ä¸¥æ ¼è®¾è®¡çš„å¹³å°ï¼ŒOceanGymä¸ºå¼€å‘ç¨³å¥çš„å®ä½“äººå·¥æ™ºèƒ½å¹¶å°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°ç°å®ä¸–ç•Œçš„è‡ªä¸»æµ·æ´‹æ°´ä¸‹è½¦è¾†å»ºç«‹äº†æµ‹è¯•åºŠï¼Œè¿™æ˜¯æœç€èƒ½å¤Ÿæ“ä½œåœ°çƒæœ€åä¸€ä¸ªæœªå¼€å‘å‰æ²¿ä¹‹ä¸€çš„æ™ºèƒ½å®ä½“è¿ˆå‡ºçš„å†³å®šæ€§ä¸€æ­¥ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OceanGPT/OceanGym%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OceanGPT/OceanGymæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26536v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>OceanGymæ˜¯é¦–ä¸ªé’ˆå¯¹æµ·æ´‹æ°´ä¸‹æ™ºèƒ½ä¸»ä½“çš„å…¨é¢åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨æ¨è¿›äººå·¥æ™ºèƒ½åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ç°å®ç¯å¢ƒä¹‹ä¸€ä¸­çš„åº”ç”¨ã€‚è¯¥å¹³å°æ¶µç›–å…«ä¸ªçœŸå®ä»»åŠ¡é¢†åŸŸï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é©±åŠ¨çš„ç»Ÿä¸€ä¸»ä½“æ¡†æ¶ï¼Œé¢ä¸´ä½èƒ½è§åº¦ã€åŠ¨æ€æ´‹æµç­‰æç«¯æ„ŸçŸ¥å’Œå†³ç­–æŒ‘æˆ˜ã€‚OceanGymè¦æ±‚æ™ºèƒ½ä¸»ä½“ç†è§£å…‰å­¦å’Œå£°çº³æ•°æ®ï¼Œè‡ªä¸»æ¢ç´¢å¤æ‚ç¯å¢ƒï¼Œå¹¶åœ¨è¿™äº›æ¶åŠ£æ¡ä»¶ä¸‹å®Œæˆé•¿æœŸç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMé©±åŠ¨çš„æ™ºèƒ½ä¸»ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´ä»å­˜åœ¨å·¨å¤§å·®è·ï¼Œå‡¸æ˜¾äº†æµ·æ´‹æ°´ä¸‹ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”èƒ½åŠ›çš„æŒä¹…æŒ‘æˆ˜ã€‚OceanGymä¸ºå¼€å‘ç¨³å¥çš„åµŒå…¥å¼äººå·¥æ™ºèƒ½æä¾›äº†ä¸€ä¸ªé«˜ä¿çœŸã€ä¸¥è°¨è®¾è®¡çš„å¹³å°ï¼Œå¹¶å°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°ç°å®ä¸–ç•Œçš„è‡ªä¸»æµ·æ´‹æ°´ä¸‹è½¦è¾†ä¸­ï¼Œæ ‡å¿—ç€æœç€æ™ºèƒ½ä¸»ä½“åœ¨åœ°çƒæœ€åçš„å‰æ²¿ä¹‹ä¸€ä¸­è¿è¡Œçš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OceanGymæ˜¯é¦–ä¸ªé’ˆå¯¹æµ·æ´‹æ°´ä¸‹æ™ºèƒ½ä¸»ä½“çš„å…¨é¢åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å¹³å°æ—¨åœ¨æ¨è¿›äººå·¥æ™ºèƒ½åœ¨æµ·æ´‹ç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„ç°å®ç¯å¢ƒã€‚</li>
<li>OceanGymåŒ…å«å…«ä¸ªçœŸå®ä»»åŠ¡é¢†åŸŸï¼Œå¹¶é‡‡ç”¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é©±åŠ¨çš„ç»Ÿä¸€ä¸»ä½“æ¡†æ¶ã€‚</li>
<li>æ™ºèƒ½ä¸»ä½“éœ€è¦é¢å¯¹ä½èƒ½è§åº¦ã€åŠ¨æ€æ´‹æµç­‰æç«¯æ„ŸçŸ¥å’Œå†³ç­–æŒ‘æˆ˜ã€‚</li>
<li>æ™ºèƒ½ä¸»ä½“éœ€è¦ç†è§£å…‰å­¦å’Œå£°çº³æ•°æ®ï¼Œè‡ªä¸»æ¢ç´¢å¤æ‚ç¯å¢ƒï¼Œå®Œæˆé•¿æœŸç›®æ ‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç°æœ‰MLLMé©±åŠ¨çš„æ™ºèƒ½ä¸»ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´åœ¨æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ–¹é¢å­˜åœ¨å·¨å¤§å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-de80cdfa29da524f97005bb4836865b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086185&auth_key=1760086185-0-0-71538224ee2c3d99314023462334833f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8abf1108149bd21633629fe8558c8fc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086193&auth_key=1760086193-0-0-34c2e75d165fd537928d440ef1e3aef6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-752c00b703f42065918dd2ccc544699f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086200&auth_key=1760086200-0-0-44d2efed1ec2026985aa9699c1fa45f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain"><a href="#The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain" class="headerlink" title="The Dragon Hatchling: The Missing Link between the Transformer and   Models of the Brain"></a>The Dragon Hatchling: The Missing Link between the Transformer and   Models of the Brain</h2><p><strong>Authors:Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski, Zuzanna Stamirowska, MichaÅ‚ Bartoszkiewicz</strong></p>
<p>The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.   We introduce &#96;Dragon Hatchlingâ€™ (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.   BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.   BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.   BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. </p>
<blockquote>
<p>è®¡ç®—ç³»ç»Ÿä¸å¤§è„‘ä¹‹é—´çš„å…³ç³»è‡ªçº¦ç¿°Â·å†¯Â·è¯ºä¾æ›¼å’Œè‰¾ä¼¦Â·å›¾çµä»¥æ¥ä¸€ç›´æ¿€åŠ±ç€ç†è®ºå…ˆé©±è€…ã€‚å‡åŒ€ã€æ— æ ‡åº¦çš„ç”Ÿç‰©ç½‘ç»œï¼Œå¦‚å¤§è„‘ï¼Œå…·æœ‰å¼ºå¤§çš„åŠŸèƒ½å±æ€§ï¼ŒåŒ…æ‹¬éšç€æ—¶é—´çš„æ¨ç§»è¿›è¡Œæ¦‚æ‹¬ï¼Œè¿™æ˜¯æœºå™¨é€šç”¨æ¨ç†æ¨¡å‹è·¯å¾„ä¸Šæœºå™¨å­¦ä¹ æ‰€é¢ä¸´çš„ä¸»è¦éšœç¢ã€‚æˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€œé¾™å­µåŒ–å™¨â€ï¼ˆBDHï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„åŸºäºæ— æ ‡åº¦ç”Ÿç‰©çµæ„Ÿç½‘ç»œï¼Œç”±nä¸ªå±€éƒ¨äº¤äº’ç¥ç»å…ƒç²’å­ç»„æˆã€‚BDHç»“åˆäº†å¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œå›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç±»ä¼¼Transformerçš„æ€§èƒ½ã€‚BDHæ˜¯ä¸€ç§å®ç”¨ã€é«˜æ•ˆçš„åŸºäºæ³¨æ„åŠ›çŠ¶æ€ç©ºé—´çš„åºåˆ—å­¦ä¹ æ¶æ„ã€‚é™¤äº†ä½œä¸ºå›¾å½¢æ¨¡å‹ä¹‹å¤–ï¼ŒBDHè¿˜é‡‡ç”¨äº†ä¸€ç§é€‚ç”¨äºGPUçš„å…¬å¼ã€‚å®ƒè¡¨ç°å‡ºç±»ä¼¼Transformerçš„æ‰©å±•å®šå¾‹ï¼šåœ¨ç›¸åŒçš„å‚æ•°æ•°é‡ï¼ˆä»10Måˆ°1Bï¼‰å’Œç›¸åŒçš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œå®è¯ä¸­BDHåœ¨è¯­è¨€ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸GPT2ç›¸åª²ç¾ã€‚BDHå¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ç§å¤§è„‘æ¨¡å‹ã€‚BDHåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å·¥ä½œå†…å­˜å®Œå…¨ä¾èµ–äºåˆ©ç”¨è„‰å†²ç¥ç»å…ƒçš„èµ«å¸ƒå­¦ä¹ æ¥åŠ å¼ºçªè§¦å¯å¡‘æ€§ã€‚æˆ‘ä»¬å®è¯è¯å®ï¼Œæ¯å½“BDHå¤„ç†è¯­è¨€è¾“å…¥æ—¶å¬åˆ°æˆ–æ¨ç†ç‰¹å®šæ¦‚å¿µæ—¶ï¼Œç‰¹å®šå•ä¸ªçªè§¦ä¼šåŠ å¼ºè¿æ¥ã€‚BDHçš„ç¥ç»å…ƒäº¤äº’ç½‘ç»œæ˜¯ä¸€ä¸ªé«˜æ¨¡å—åŒ–ã€é‡å°¾åˆ†å¸ƒç¨‹åº¦çš„å›¾ã€‚BDHæ¨¡å‹åœ¨ç”Ÿç‰©å­¦ä¸Šæ˜¯åˆç†çš„ï¼Œè§£é‡Šäº†äººç±»ç¥ç»å…ƒå®ç°è¯­è¨€çš„ä¸€ç§å¯èƒ½æœºåˆ¶ã€‚BDHæ—¨åœ¨å®ç°å¯è§£é‡Šæ€§ã€‚BDHçš„æ¿€æ´»å‘é‡æ˜¯ç¨€ç–ä¸”æ­£å‘çš„ã€‚æˆ‘ä»¬åœ¨è¯­è¨€ä»»åŠ¡ä¸Šå±•ç¤ºäº†BDHçš„å•è¯­ä¹‰æ€§ã€‚é™¤äº†ç¥ç»å…ƒå’Œæ¨¡å‹å‚æ•°çš„å¯è§£é‡Šæ€§ä¹‹å¤–ï¼ŒçŠ¶æ€çš„å¯è§£é‡Šæ€§æ˜¯BDHæ¶æ„çš„å›ºæœ‰ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26507v1">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/pathwaycom/bdh">https://github.com/pathwaycom/bdh</a> Accompanying   blog: <a target="_blank" rel="noopener" href="https://pathway.com/research/bdh">https://pathway.com/research/bdh</a></p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬æ¢è®¨äº†è®¡ç®—ç³»ç»Ÿå’Œäººè„‘çš„å…³ç³»ä»¥åŠå¯¹äºå®ç°é€šç”¨æ¨ç†æ¨¡å‹çš„é‡è¦å¯ç¤ºã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ— å°ºåº¦ç”Ÿç‰©ç½‘ç»œçš„æ–°å‹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¶æ„â€”â€”â€œé¾™å­µåŒ–å™¨â€ï¼ˆBDHï¼‰ã€‚è¯¥æ¶æ„å…·æœ‰å¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œå›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç±»ä¼¼Transformerçš„æ€§èƒ½ã€‚BDHæ˜¯ä¸€ä¸ªå®ç”¨ã€é«˜æ€§èƒ½çš„çŠ¶æ€ç©ºé—´åºåˆ—å­¦ä¹ æ¶æ„ï¼Œæ—¢æ˜¯ä¸€ä¸ªå›¾å½¢æ¨¡å‹ï¼Œä¹Ÿæ”¯æŒGPUå‹å¥½å‹å…¬å¼ã€‚å®ƒè¡¨ç°å‡ºç±»ä¼¼Transformerçš„è§„æ¨¡æ•ˆåº”ï¼Œåœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸GPT-2ç›¸æå¹¶è®ºã€‚æ­¤å¤–ï¼ŒBDHå¯ä»¥ä½œä¸ºä¸€ç§å¤§è„‘æ¨¡å‹æ¥è¡¨ç¤ºï¼Œå…¶å·¥ä½œè®°å¿†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®Œå…¨ä¾èµ–äºçªè§¦å¯å¡‘æ€§ï¼Œé‡‡ç”¨èµ«å¸ƒå­¦ä¹ è§„åˆ™é€šè¿‡è„‰å†²ç¥ç»å…ƒå®ç°ã€‚ç»éªŒè¯å®ï¼ŒBDHå¤„ç†è¯­è¨€è¾“å…¥æ—¶ï¼Œç‰¹å®šçš„çªè§¦ä¼šåœ¨å¬åˆ°æˆ–æ¨ç†ç‰¹å®šæ¦‚å¿µæ—¶åŠ å¼ºè¿æ¥ã€‚æœ€åï¼ŒBDHçš„è®¾è®¡æ—¨åœ¨å®ç°å¯è§£é‡Šæ€§ï¼Œå…¶æ¿€æ´»å‘é‡ç¨€ç–ä¸”å‘ˆé˜³æ€§ã€‚åœ¨ä»»åŠ¡è¯­è¨€æ–¹é¢å±•ç¤ºäº†å•è¯­ä¹‰æ€§ï¼Œå³çŠ¶æ€çš„å¯è§£é‡Šæ€§ä¸ä»…æ˜¯ç¥ç»å…ƒå’Œæ¨¡å‹å‚æ•°çš„å¯è§£é‡Šæ€§ï¼Œè€Œæ˜¯BDHæ¶æ„çš„å›ºæœ‰ç‰¹æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—ç³»ç»Ÿä¸å¤§è„‘çš„å…³è”ä¸ºç†è®ºå­¦è€…æä¾›äº†åŠ¨åŠ›ï¼Œæ— å°ºåº¦ç”Ÿç‰©ç½‘ç»œå¦‚å¤§è„‘å…·æœ‰å¼ºå¤§çš„å±æ€§ï¼Œå¦‚éšæ—¶é—´æ¦‚æ‹¬åŒ–ï¼Œè¿™å¯¹æœºå™¨å­¦ä¹ å®ç°é€šç”¨æ¨ç†æ¨¡å‹æ˜¯ä¸»è¦éšœç¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¶æ„â€œé¾™å­µåŒ–å™¨â€ï¼ˆBDHï¼‰ï¼ŒåŸºäºæ— å°ºåº¦ç”Ÿç‰©ç½‘ç»œå¯å‘è®¾è®¡ã€‚</li>
<li>BDHç»“åˆäº†å¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œå›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç±»ä¼¼Transformerçš„æ€§èƒ½ã€‚</li>
<li>BDHæ˜¯ä¸€ä¸ªé«˜æ•ˆã€å®ç”¨çš„çŠ¶æ€ç©ºé—´åºåˆ—å­¦ä¹ æ¶æ„ï¼Œæ—¢ä½œä¸ºå›¾å½¢æ¨¡å‹ä¹Ÿå…¼å®¹GPUã€‚</li>
<li>BDHå±•ç°å‡ºç±»ä¼¼Transformerçš„è§„æ¨¡æ•ˆåº”ï¼Œèƒ½åœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸GPT-2ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>BDHå¯è§†ä¸ºä¸€ç§å¤§è„‘æ¨¡å‹ï¼Œå…¶å·¥ä½œè®°å¿†åŸºäºçªè§¦å¯å¡‘æ€§ï¼Œå¹¶é‡‡ç”¨èµ«å¸ƒå­¦ä¹ è§„åˆ™é€šè¿‡è„‰å†²ç¥ç»å…ƒå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6ee66ea96dda57bddc645dc82de47fc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086207&auth_key=1760086207-0-0-29c4da84d05331502e4b2fdd93e4393b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Power-of-Post-Training-for-Small-Language-Models-via-Knowledge-Distillation"><a href="#Revealing-the-Power-of-Post-Training-for-Small-Language-Models-via-Knowledge-Distillation" class="headerlink" title="Revealing the Power of Post-Training for Small Language Models via   Knowledge Distillation"></a>Revealing the Power of Post-Training for Small Language Models via   Knowledge Distillation</h2><p><strong>Authors:Miao Rang, Zhenni Bi, Hang Zhou, Hanting Chen, An Xiao, Tianyu Guo, Kai Han, Xinghao Chen, Yunhe Wang</strong></p>
<p>The rapid advancement of large language models (LLMs) has significantly advanced the capabilities of artificial intelligence across various domains. However, their massive scale and high computational costs render them unsuitable for direct deployment in resource-constrained edge environments. This creates a critical need for high-performance small models that can operate efficiently at the edge. Yet, after pre-training alone, these smaller models often fail to meet the performance requirements of complex tasks. To bridge this gap, we introduce a systematic post-training pipeline that efficiently enhances small model accuracy. Our post training pipeline consists of curriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge distillation. The resulting instruction-tuned model achieves state-of-the-art performance among billion-parameter models, demonstrating strong generalization under strict hardware constraints while maintaining competitive accuracy across a variety of tasks. This work provides a practical and efficient solution for developing high-performance language models on Ascend edge devices. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å·²åœ¨å„ä¸ªé¢†åŸŸæ˜¾è‘—æé«˜äº†äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå…¶åºå¤§çš„è§„æ¨¡å’Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œå®ƒä»¬ä¸é€‚åˆåœ¨èµ„æºå—é™çš„è¾¹ç¼˜ç¯å¢ƒä¸­ç›´æ¥éƒ¨ç½²ã€‚è¿™ä¸ºèƒ½åœ¨è¾¹ç¼˜é«˜æ•ˆè¿è¡Œçš„é«˜æ€§èƒ½å°å‹æ¨¡å‹å¸¦æ¥äº†è¿«åˆ‡éœ€æ±‚ã€‚ç„¶è€Œï¼Œä»…é€šè¿‡é¢„è®­ç»ƒï¼Œè¿™äº›è¾ƒå°çš„æ¨¡å‹å¾€å¾€éš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„æ€§èƒ½è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“å¯ä»¥æœ‰æ•ˆåœ°æé«˜å°æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åè®­ç»ƒç®¡é“åŒ…æ‹¬åŸºäºè¯¾ç¨‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¦»çº¿åœ¨çº¿ç­–ç•¥çŸ¥è¯†è’¸é¦ã€‚æ‰€å¾—æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨ç™¾äº¿å‚æ•°æ¨¡å‹ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œåœ¨ä¸¥æ ¼çš„ç¡¬ä»¶çº¦æŸä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨Ascendè¾¹ç¼˜è®¾å¤‡ä¸Šå¼€å‘é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26497v1">PDF</a> 7</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ¨åŠ¨äº†äººå·¥æ™ºèƒ½åœ¨å¤šä¸ªé¢†åŸŸçš„èƒ½åŠ›æå‡ã€‚ç„¶è€Œï¼Œå…¶åºå¤§çš„è§„æ¨¡å’Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ä½¿å…¶ä¸é€‚åˆåœ¨èµ„æºå—é™çš„è¾¹ç¼˜ç¯å¢ƒä¸­ç›´æ¥éƒ¨ç½²ã€‚å› æ­¤ï¼Œéœ€è¦é«˜æ€§èƒ½çš„å°å‹æ¨¡å‹åœ¨è¾¹ç¼˜è¿›è¡Œé«˜æ•ˆè¿è¡Œã€‚ç„¶è€Œï¼Œä»…é€šè¿‡é¢„è®­ç»ƒï¼Œè¿™äº›å°å‹æ¨¡å‹å¾€å¾€éš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„æ€§èƒ½è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒç®¡é“ï¼Œæœ‰æ•ˆæé«˜å°å‹æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„åè®­ç»ƒç®¡é“åŒ…æ‹¬åŸºäºè¯¾ç¨‹è¡¨çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¦»çº¿åœ¨çº¿æ”¿ç­–çŸ¥è¯†è’¸é¦ã€‚ç»“æœå¾—åˆ°çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨äº¿çº§å‚æ•°æ¨¡å‹ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œåœ¨ä¸¥æ ¼çš„ç¡¬ä»¶çº¦æŸä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›å‡†ç¡®æ€§ã€‚æœ¬å·¥ä½œåœ¨ä¸ºAscendè¾¹ç¼˜è®¾å¤‡å¼€å‘é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æ–¹é¢æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œä½†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜ç¯å¢ƒä¸‹éƒ¨ç½²å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦é«˜æ€§èƒ½å°å‹æ¨¡å‹åœ¨è¾¹ç¼˜è¿›è¡Œé«˜æ•ˆè¿è¡Œã€‚</li>
<li>ä»…æœ‰é¢„è®­ç»ƒçš„æ¨¡å‹éš¾ä»¥æ»¡è¶³å¤æ‚ä»»åŠ¡çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„åè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒçŸ¥è¯†è’¸é¦ã€‚</li>
<li>è¯¥ç®¡é“æé«˜äº†å°å‹æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå®ç°äº†äº¿çº§å‚æ•°æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸¥æ ¼çš„ç¡¬ä»¶çº¦æŸä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒé«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ee4b7b80f3c2746975f1688e26c8c06b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086216&auth_key=1760086216-0-0-f5a524f9358b2a8aa9e29f4cc9b121c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-095db22432324aaa4a837d21f19837e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086223&auth_key=1760086223-0-0-2c6c275caeb0aafdd0771c703cb9ea77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6356bdfeb0a508f83dbad8019b3295ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086230&auth_key=1760086230-0-0-d56df28322015bbea5717acb480dc0a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8329fb02cd2ea2c5c0e589abe4e9440c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086237&auth_key=1760086237-0-0-8f26295ca1b5e63276a219816949cca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1706424e7c2f75bb6dd6a0b0bb3483e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086243&auth_key=1760086243-0-0-5fe587093d8db347a180a0fc1efe298a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5143b2fc7efbbaefd6d9bbc9d5d7e0f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086250&auth_key=1760086250-0-0-93de00c0a4cc8302b147efa218c9c76f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LaTo-Landmark-tokenized-Diffusion-Transformer-for-Fine-grained-Human-Face-Editing"><a href="#LaTo-Landmark-tokenized-Diffusion-Transformer-for-Fine-grained-Human-Face-Editing" class="headerlink" title="LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human   Face Editing"></a>LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human   Face Editing</h2><p><strong>Authors:Zhenghao Zhang, Ziying Zhang, Junchao Liao, Xiangyu Meng, Qiang Hu, Siyu Zhu, Xiaoyun Zhang, Long Qin, Weizhi Wang</strong></p>
<p>Recent multimodal models for instruction-based face editing enable semantic manipulation but still struggle with precise attribute control and identity preservation. Structural facial representations such as landmarks are effective for intermediate supervision, yet most existing methods treat them as rigid geometric constraints, which can degrade identity when conditional landmarks deviate significantly from the source (e.g., large expression or pose changes, inaccurate landmark estimates). To address these limitations, we propose LaTo, a landmark-tokenized diffusion transformer for fine-grained, identity-preserving face editing. Our key innovations include: (1) a landmark tokenizer that directly quantizes raw landmark coordinates into discrete facial tokens, obviating the need for dense pixel-wise correspondence; (2) a location-mapping positional encoding that integrates facial and image tokens for unified processing, enabling flexible yet decoupled geometry-appearance interactions with high efficiency and strong identity preservation; and (3) a landmark predictor that leverages vision-language models to infer target landmarks from instructions and source images, whose structured chain-of-thought improves estimation accuracy and interactive control. To mitigate data scarcity, we curate HFL-150K, to our knowledge the largest benchmark for this task, containing over 150K real face pairs with fine-grained instructions. Extensive experiments show that LaTo outperforms state-of-the-art methods by 7.8% in identity preservation and 4.6% in semantic consistency. Code and dataset will be made publicly available upon acceptance. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€æŒ‡ä»¤å¼äººè„¸ç¼–è¾‘æ¨¡å‹èƒ½å¤Ÿå®ç°è¯­ä¹‰æ“ä½œï¼Œä½†åœ¨ç²¾ç¡®å±æ€§æ§åˆ¶å’Œèº«ä»½ä¿ç•™æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚é¢éƒ¨ç»“æ„è¡¨ç¤ºï¼ˆå¦‚åœ°æ ‡ï¼‰å¯¹äºä¸­é—´ç›‘ç£æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†å…¶è§†ä¸ºåˆšæ€§çš„å‡ ä½•çº¦æŸï¼Œå½“æ¡ä»¶åœ°æ ‡ä¸æºé¢éƒ¨ï¼ˆä¾‹å¦‚ï¼Œå¤§å¹…åº¦è¡¨æƒ…æˆ–å§¿æ€å˜åŒ–ã€åœ°æ ‡ä¼°è®¡ä¸å‡†ç¡®ï¼‰å­˜åœ¨æ˜¾è‘—å·®å¼‚æ—¶ï¼Œä¼šå¯¼è‡´èº«ä»½é™çº§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LaToï¼Œä¸€ç§ç”¨äºç²¾ç»†ç²’åº¦ã€èº«ä»½ä¿ç•™çš„äººè„¸ç¼–è¾‘çš„åŸºäºåœ°æ ‡ç¬¦å·åŒ–çš„æ‰©æ•£è½¬æ¢å™¨ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ°æ ‡åˆ†è¯å™¨ï¼Œå®ƒç›´æ¥å°†åŸå§‹åœ°æ ‡åæ ‡é‡åŒ–æˆç¦»æ•£é¢éƒ¨ç¬¦å·ï¼Œæ— éœ€å¯†é›†åƒç´ çº§çš„å¯¹åº”å…³ç³»ï¼›ï¼ˆ2ï¼‰ä½ç½®æ˜ å°„ä½ç½®ç¼–ç ï¼Œå°†é¢éƒ¨å’Œå›¾åƒç¬¦å·é›†æˆè¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œå®ç°çµæ´»ä½†è§£è€¦çš„å‡ ä½•å¤–è§‚äº¤äº’ï¼Œå…·æœ‰é«˜æ•ˆç‡å¼ºèº«ä»½ä¿ç•™èƒ½åŠ›ï¼›ï¼ˆ3ï¼‰åœ°æ ‡é¢„æµ‹å™¨åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»æŒ‡ä»¤å’Œæºå›¾åƒæ¨æ–­ç›®æ ‡åœ°æ ‡ï¼Œå…¶ç»“æ„åŒ–æ€ç»´é“¾æé«˜äº†ä¼°è®¡ç²¾åº¦å’Œäº¤äº’æ§åˆ¶ã€‚ä¸ºäº†ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†HFL-150Kæ•°æ®é›†ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¯¥ä»»åŠ¡æœ€å¤§çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡150,000ä¸ªç²¾ç»†æŒ‡ä»¤çš„çœŸå®äººè„¸å¯¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨èº«ä»½ä¿ç•™å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢ï¼ŒLaToä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†7.8%å’Œ4.6%ã€‚æ¥å—åå°†å…¬å¼€ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25731v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLaToçš„é‡Œç¨‹ç¢‘æ ‡è®°åŒ–æ‰©æ•£è½¬æ¢å™¨ï¼Œç”¨äºç²¾ç»†ç²’åº¦çš„èº«ä»½ä¿ç•™é¢éƒ¨ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥é‡Œç¨‹ç¢‘æ ‡è®°åŒ–ã€ä½ç½®æ˜ å°„ä½ç½®ç¼–ç å’Œé‡Œç¨‹ç¢‘é¢„æµ‹å™¨ç­‰æŠ€æœ¯ï¼Œå®ç°äº†çµæ´»çš„å‡ ä½•ä¸å¤–è§‚äº¤äº’ã€é«˜æ•ˆçš„èº«ä»½ä¿ç•™ä»¥åŠç²¾ç¡®çš„è¯­ä¹‰æ§åˆ¶ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†HFL-150Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡15ä¸‡å¯¹çœŸå®é¢éƒ¨å›¾åƒå’Œç²¾ç»†æŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLaToåœ¨èº«ä»½ä¿ç•™å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LaToé€šè¿‡å¼•å…¥é‡Œç¨‹ç¢‘æ ‡è®°åŒ–æŠ€æœ¯ï¼Œå°†åŸå§‹é¢éƒ¨ç¼–è¾‘ä¸­çš„å‡ ä½•çº¦æŸè½¬åŒ–ä¸ºç¦»æ•£é¢éƒ¨æ ‡è®°ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>ä½ç½®æ˜ å°„ä½ç½®ç¼–ç æŠ€æœ¯å®ç°äº†é¢éƒ¨å’Œå›¾åƒæ ‡è®°çš„é›†æˆå¤„ç†ï¼Œæé«˜äº†æ•ˆç‡å¹¶å¢å¼ºäº†èº«ä»½ä¿ç•™èƒ½åŠ›ã€‚</li>
<li>LaToåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥æ¨æ–­ç›®æ ‡é‡Œç¨‹ç¢‘ï¼Œä»è€Œæé«˜ä¼°ç®—å‡†ç¡®æ€§å’Œäº¤äº’æ§åˆ¶æ€§ã€‚</li>
<li>HFL-150Kæ•°æ®é›†çš„æ¨å‡ºè§£å†³äº†é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚</li>
<li>LaToåœ¨èº«ä»½ä¿ç•™å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>LaToé€šè¿‡ç²¾ç»†çš„é‡Œç¨‹ç¢‘é¢„æµ‹å’Œé«˜æ•ˆçš„æ¨¡å‹ç»“æ„ï¼Œå®ç°äº†çµæ´»çš„å‡ ä½•ä¸å¤–è§‚äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dcabf52b1b9999bf9d5ede5406680504~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086258&auth_key=1760086258-0-0-17b13e05d9e08c77184943ae91d05e2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98288a41af2662d76562704067535c1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086266&auth_key=1760086266-0-0-5160618cd583ee8536fccaf17cfc467c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c28240f60f185d023ea6316262b1f48f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086273&auth_key=1760086273-0-0-b28aea973e9cda6280045b2a726af6df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97d95046ed8e0d01dc6ec081edf3d3c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086280&auth_key=1760086280-0-0-54369cab1449aba7ef3c9fa04330d8a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0263dc3b76401a4ffb7e7a1f6a016e79~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086287&auth_key=1760086287-0-0-b06405b4b9f8b6b34aac532cd8cb5742&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-061d6bbd76715aa7e8407195748e6452~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086294&auth_key=1760086294-0-0-d44af30630af7fa89524239bf9ce725f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning"><a href="#MASLegalBench-Benchmarking-Multi-Agent-Systems-in-Deductive-Legal-Reasoning" class="headerlink" title="MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning"></a>MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning</h2><p><strong>Authors:Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song</strong></p>
<p>Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“è¶Šèƒ½åŠ›ï¼Œåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå°†MASä¸æ³•å¾‹ä»»åŠ¡é›†æˆæ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»ä¸ºLLMä»£ç†å¼€å‘äº†æ³•å¾‹åŸºå‡†ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªæ˜¯ä¸“é—¨è€ƒè™‘MASçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ã€ä»£ç†ä¸“ä¸šåŒ–å’Œçµæ´»çš„è®­ç»ƒã€‚äº‹å®ä¸Šï¼Œç¼ºä¹è¯„ä¼°æ–¹æ³•é™åˆ¶äº†MASåœ¨æ³•å¾‹é¢†åŸŸçš„æ½œåŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MASLegalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºMASå®šåˆ¶çš„æ³•å¾‹åŸºå‡†ï¼Œé‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•è®¾è®¡ã€‚æˆ‘ä»¬çš„åŸºå‡†ä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼ŒåŒ…å«ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ï¼Œæ¶µç›–æœ‰æ•ˆçš„å¤æ‚æ¨ç†è¿‡ç¨‹ï¼Œåæ˜ äº†ç°å®æ³•å¾‹æƒ…å†µçš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ‰‹åŠ¨è®¾è®¡äº†å„ç§åŸºäºè§’è‰²çš„MASï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æœ€æ–°LLMè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ç°æœ‰æ¨¡å‹å’ŒMASæ¶æ„çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24922v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èåˆæŠ€æœ¯ï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡å±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹æ³•å¾‹ä»»åŠ¡é›†æˆMASçš„é‡è¦æ€§ï¼Œå½“å‰ç¼ºä¹ä¸“é—¨è€ƒè™‘MASç‹¬ç‰¹ä¼˜åŠ¿çš„æ³•å¾‹åŸºå‡†ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæå‡ºMASLegalBenchï¼Œä¸€ä¸ªä¸“ä¸ºMASè®¾è®¡çš„æ³•å¾‹åŸºå‡†ï¼Œé‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•ï¼Œä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–å¹¿æ³›èƒŒæ™¯çŸ¥è¯†ï¼Œæœ‰æ•ˆåæ˜ ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èåˆï¼Œä¸ºè§£å†³å¤æ‚ä»»åŠ¡æä¾›å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰æ³•å¾‹ä»»åŠ¡é›†æˆMASè‡³å…³é‡è¦ï¼Œä½†éœ€è€ƒè™‘MASçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œå¦‚ä»»åŠ¡åˆ†è§£ã€ä»£ç†ä¸“ä¸šåŒ–å’Œçµæ´»è®­ç»ƒã€‚</li>
<li>ç¼ºä¹ä¸“é—¨è€ƒè™‘MASä¼˜åŠ¿çš„æ³•å¾‹åŸºå‡†é™åˆ¶å…¶åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>MASLegalBenchæ˜¯ä¸€ä¸ªä¸ºMASè®¾è®¡çš„æ³•å¾‹åŸºå‡†ï¼Œé‡‡ç”¨æ¼”ç»æ¨ç†æ–¹æ³•ã€‚</li>
<li>MASLegalBenchä»¥GDPRä¸ºåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–å¹¿æ³›èƒŒæ™¯çŸ¥è¯†ï¼Œåæ˜ ç°å®æ³•å¾‹æƒ…å¢ƒçš„å¤æ‚æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒå‘ç°ï¼Œç°æœ‰æ¨¡å‹å’ŒMASæ¶æ„çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ”¹è¿›æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-41324f4c802518bd72bce6d67d6b8271~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086302&auth_key=1760086302-0-0-18173a8ce2ff8590462bfd9503dcf6f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11071a74e7cb408cd1b3b5c178f00d6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101342&auth_key=1760101342-0-0-80d9ec2656dc6429bdf592cd9ce27894&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6678d96a90341802ab220f32f5831761~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086374&auth_key=1760086374-0-0-0f0c7dee7369f7789f1a52d3cd7168cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86c2c517cbc4d5d106c3ac2e8ac49fbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086381&auth_key=1760086381-0-0-e52d44adb44aca2c71ea33b69106ee8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MaskSQL-Safeguarding-Privacy-for-LLM-Based-Text-to-SQL-via-Abstraction"><a href="#MaskSQL-Safeguarding-Privacy-for-LLM-Based-Text-to-SQL-via-Abstraction" class="headerlink" title="MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction"></a>MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction</h2><p><strong>Authors:Sepideh Abedini, Shubhankar Mohapatra, D. B. Emerson, Masoumeh Shafieinejad, Jesse C. Cresswell, Xi He</strong></p>
<p>Large language models (LLMs) have shown promising performance on tasks that require reasoning, such as text-to-SQL, code generation, and debugging. However, regulatory frameworks with strict privacy requirements constrain their integration into sensitive systems. State-of-the-art LLMs are also proprietary, costly, and resource-intensive, making local deployment impractical. Consequently, utilizing such LLMs often requires sharing data with third-party providers, raising privacy concerns and risking noncompliance with regulations. Although fine-tuned small language models (SLMs) can outperform LLMs on certain tasks and be deployed locally to mitigate privacy concerns, they underperform on more complex tasks such as text-to-SQL translation. In this work, we introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a privacy protection mechanism to mask sensitive information in LLM prompts. Unlike redaction, which removes content entirely, or generalization, which broadens tokens, abstraction retains essential information while discarding unnecessary details, striking an effective privacy-utility balance for the text-to-SQL task. Moreover, by providing mechanisms to control the privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range of use cases. Our experimental results show that MaskSQL outperforms leading SLM-based text-to-SQL models and achieves performance approaching state-of-the-art LLM-based models, while preserving privacy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œå¦‚æ–‡æœ¬åˆ°SQLã€ä»£ç ç”Ÿæˆå’Œè°ƒè¯•ã€‚ç„¶è€Œï¼Œå…·æœ‰ä¸¥æ ¼éšç§è¦æ±‚çš„ç›‘ç®¡æ¡†æ¶é™åˆ¶äº†å®ƒä»¬èå…¥æ•æ„Ÿç³»ç»Ÿã€‚å½“å‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯ä¸“æœ‰åŒ–çš„ï¼Œæˆæœ¬é«˜æ˜‚ä¸”èµ„æºå¯†é›†ï¼Œä½¿å¾—æœ¬åœ°éƒ¨ç½²ä¸åˆ‡å®é™…ã€‚å› æ­¤ï¼Œåˆ©ç”¨è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦ä¸ç¬¬ä¸‰æ–¹æä¾›å•†å…±äº«æ•°æ®ï¼Œè¿™å¼•å‘äº†éšç§æ‹…å¿§å¹¶å¯èƒ½è¿åæ³•è§„ã€‚è™½ç„¶ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¯ä»¥åœ¨æŸäº›ä»»åŠ¡ä¸Šä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹å¹¶å¯åœ¨æœ¬åœ°éƒ¨ç½²ä»¥ç¼“è§£éšç§æ‹…å¿§ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°SQLç¿»è¯‘ï¼‰æ—¶è¡¨ç°è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MaskSQLï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ°SQLçš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æŠ½è±¡ä½œä¸ºéšç§ä¿æŠ¤æœºåˆ¶æ¥æ©ç›–å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºä¸­çš„æ•æ„Ÿä¿¡æ¯ã€‚ä¸å®Œå…¨åˆ é™¤å†…å®¹çš„åˆ é™¤æˆ–æ‰©å¤§èŒƒå›´çš„æ³›åŒ–ä¸åŒï¼ŒæŠ½è±¡ä¿ç•™äº†å¿…è¦ä¿¡æ¯åŒæ—¶ä¸¢å¼ƒäº†ä¸å¿…è¦çš„ç»†èŠ‚ï¼Œä¸ºæ–‡æœ¬åˆ°SQLä»»åŠ¡å®ç°äº†æœ‰æ•ˆçš„éšç§æ•ˆç”¨å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æä¾›æ§åˆ¶éšç§æ•ˆç”¨æƒè¡¡çš„æœºåˆ¶ï¼ŒMaskSQLä¿ƒè¿›äº†å…¶åœ¨æ›´å¹¿æ³›çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskSQLä¼˜äºé¢†å…ˆçš„å°å‹è¯­è¨€æ¨¡å‹åŸºç¡€çš„æ–‡æœ¬åˆ°SQLæ¨¡å‹ï¼Œå…¶æ€§èƒ½æ¥è¿‘å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŠ¤äº†éšç§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23459v2">PDF</a> Accepted to the 3rd Workshop on Regulatable ML at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨éœ€è¦æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¦‚æ–‡æœ¬åˆ°SQLã€ä»£ç ç”Ÿæˆå’Œè°ƒè¯•ã€‚ä½†ä¸¥æ ¼éšç§è¦æ±‚çš„ç›‘ç®¡æ¡†æ¶é™åˆ¶äº†å®ƒä»¬åœ¨æŸäº›æ•æ„Ÿç³»ç»Ÿçš„é›†æˆã€‚LLMsé€šå¸¸æ˜¯ä¸“æœ‰ã€æ˜‚è´µä¸”èµ„æºå¯†é›†å‹çš„ï¼Œéš¾ä»¥è¿›è¡Œæœ¬åœ°éƒ¨ç½²ã€‚ä¸ºè§£å†³éšç§æ‹…å¿§ä¸åˆè§„é£é™©ï¼Œæˆ‘ä»¬æå‡ºMaskSQLæ¡†æ¶ï¼Œåˆ©ç”¨æŠ½è±¡ä½œä¸ºéšç§ä¿æŠ¤æœºåˆ¶ï¼Œåœ¨LLMæç¤ºä¸­éšè—æ•æ„Ÿä¿¡æ¯ã€‚æŠ½è±¡æ—¢ä¿ç•™äº†å¿…è¦ä¿¡æ¯åˆå‰”é™¤äº†ä¸å¿…è¦çš„ç»†èŠ‚ï¼Œä¸ºæ–‡æœ¬åˆ°SQLä»»åŠ¡å®ç°äº†æœ‰æ•ˆçš„éšç§æ•ˆç”¨å¹³è¡¡ã€‚MaskSQLè¿˜æä¾›æ§åˆ¶éšç§æ•ˆç”¨çš„æœºåˆ¶ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„ä½¿ç”¨åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMaskSQLä¼˜äºé¢†å…ˆçš„SLM-basedæ–‡æœ¬åˆ°SQLæ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘å…ˆè¿›çš„LLM-basedæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŠ¤éšç§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨éœ€è¦æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éšç§æ•æ„Ÿç³»ç»Ÿçš„é›†æˆä¸­å—åˆ°ç›‘ç®¡å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMsæ˜¯ä¸“æœ‰ã€æ˜‚è´µä¸”èµ„æºå¯†é›†å‹çš„ï¼Œæœ¬åœ°éƒ¨ç½²ä¸åˆ‡å®é™…ã€‚</li>
<li>MaskSQLæ¡†æ¶ä½¿ç”¨æŠ½è±¡ä½œä¸ºéšç§ä¿æŠ¤æœºåˆ¶ï¼Œéšè—LLMæç¤ºä¸­çš„æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>æŠ½è±¡æ–¹æ³•æœ‰æ•ˆå¹³è¡¡äº†éšç§å’Œæ•ˆç”¨ï¼Œä¼˜äºç®€å•çš„åˆ é™¤æˆ–æ³›åŒ–æ–¹æ³•ã€‚</li>
<li>MaskSQLæä¾›äº†æ§åˆ¶éšç§æ•ˆç”¨å¹³è¡¡çš„æœºåˆ¶ï¼Œé€‚ç”¨äºå¤šç§ä½¿ç”¨åœºæ™¯ã€‚</li>
<li>MaskSQLåœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸Šçš„æ€§èƒ½æ¥è¿‘æˆ–ä¼˜äºå…ˆè¿›çš„LLMæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-62945e4bd3eb814dbc043fcef0c88159~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086391&auth_key=1760086391-0-0-76112c9bd360f2fbc5ba20d38a9f8895&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-acdadb4d0fd03cb8c791c86667c5348f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086399&auth_key=1760086399-0-0-e8d6f57670d5abb040c0ae85422ebf97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Critique-to-Verify-Accurate-and-Honest-Test-Time-Scaling-with-RL-Trained-Verifiers"><a href="#Critique-to-Verify-Accurate-and-Honest-Test-Time-Scaling-with-RL-Trained-Verifiers" class="headerlink" title="Critique to Verify: Accurate and Honest Test-Time Scaling with   RL-Trained Verifiers"></a>Critique to Verify: Accurate and Honest Test-Time Scaling with   RL-Trained Verifiers</h2><p><strong>Authors:Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang</strong></p>
<p>Test-time scaling via solution sampling and aggregation has become a key paradigm for improving the reasoning performance of Large Language Models (LLMs). While reward model selection is commonly employed in this approach, it often fails to identify minority-yet-correct answers, which limits its effectiveness beyond that of simple majority voting. We argue that this limitation stems from a lack of informative critique signals during verifier training. To bridge this gap, we introduce Mirror-Critique, a framework that trains a verifier with informative critiques. Our key insight is to leverage the rich critique signal by contrasting model-generated solutions with ground-truth solutions. We deploy a small instruction-tuned model to synthesize high-quality critique data with rejection sampling that teaches the verifier not only what is wrong, but also why. The synthetic data is used to cold-start the LLMs in the RLVR process to further improve the verification ability. The resulting Mirror-Verifier is deployed to evaluate candidate solutions by generating multiple critiques per solution, aggregating them into a verify score used for weighted voting or selective abstention. The experimental results show that our Mirror-Verifier significantly outperforms majority voting in terms of solution accuracy and also improves the solverâ€™s honesty to recognize and abstain from answering beyond its capability boundaries. </p>
<blockquote>
<p>é€šè¿‡è§£å†³æ–¹æ¡ˆé‡‡æ ·å’Œèšåˆè¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ€§èƒ½çš„å…³é”®èŒƒå¼ã€‚è™½ç„¶å¥–åŠ±æ¨¡å‹é€‰æ‹©åœ¨è¿™ç§æ–¹æ³•ä¸­ç»å¸¸è¢«ä½¿ç”¨ï¼Œä½†å®ƒé€šå¸¸æ— æ³•è¯†åˆ«å‡ºå°‘æ•°ä½†æ­£ç¡®çš„ç­”æ¡ˆï¼Œè¿™é™åˆ¶äº†å…¶æ•ˆæœï¼Œè¶…è¿‡äº†ç®€å•çš„å¤šæ•°æŠ•ç¥¨åˆ¶ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸€å±€é™æ€§æºäºéªŒè¯å™¨è®­ç»ƒæœŸé—´ç¼ºä¹ä¿¡æ¯æ€§æ‰¹åˆ¤ä¿¡å·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Mirror-Critiqueæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰ä¿¡æ¯æ€§æ‰¹åˆ¤çš„éªŒè¯å™¨ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯é€šè¿‡å¯¹æ¯”æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸çœŸå®è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨ä¸°å¯Œçš„æ‰¹åˆ¤ä¿¡å·ã€‚æˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªå°å‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œé€šè¿‡æ‹’ç»é‡‡æ ·åˆæˆé«˜è´¨é‡çš„æ‰¹åˆ¤æ•°æ®ï¼Œè¿™ä¸ä»…æ•™ä¼šéªŒè¯å™¨ä»€ä¹ˆæ˜¯é”™è¯¯çš„ï¼Œè¿˜æ•™ä¼šå®ƒä¸ºä»€ä¹ˆé”™è¯¯ã€‚åˆæˆæ•°æ®ç”¨äºåœ¨RLVRè¿‡ç¨‹ä¸­å¯åŠ¨LLMï¼Œä»¥è¿›ä¸€æ­¥æé«˜éªŒè¯èƒ½åŠ›ã€‚æ‰€å¾—çš„Mirror-Verifierè¢«ç”¨æ¥è¯„ä¼°å€™é€‰è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä¸ºæ¯ä¸ªè§£å†³æ–¹æ¡ˆç”Ÿæˆå¤šä¸ªæ‰¹åˆ¤å¹¶å°†å…¶èšåˆä¸ºéªŒè¯åˆ†æ•°ï¼Œç”¨äºåŠ æƒæŠ•ç¥¨æˆ–é€‰æ‹©æ€§å¼ƒæƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Mirror-Verifieråœ¨è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå¤šæ•°æŠ•ç¥¨åˆ¶ï¼Œå¹¶æé«˜äº†æ±‚è§£è€…çš„è¯šå®åº¦ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«å’Œé¿å…è¶…å‡ºå…¶èƒ½åŠ›è¾¹ç•Œçš„ç­”æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23152v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡è§£å†³æ–¹æ¡ˆé‡‡æ ·å’Œèšåˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ€§èƒ½çš„å…³é”®èŒƒå¼ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè™½ç„¶å¥–åŠ±æ¨¡å‹é€‰æ‹©æ˜¯è¿™ä¸€æ–¹æ³•ä¸­çš„å¸¸ç”¨æ‰‹æ®µï¼Œä½†å®ƒå¾€å¾€æ— æ³•è¯†åˆ«å‡ºå°‘æ•°æ­£ç¡®çš„ç­”æ¡ˆï¼Œè¿™é™åˆ¶äº†å…¶æ•ˆæœï¼Œè¶…è¶Šç®€å•çš„å¤šæ•°æŠ•ç¥¨åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†Mirror-Critiqueæ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæºå¸¦ä¿¡æ¯ä¸°å¯Œçš„è¯„è®ºä¿¡å·çš„éªŒè¯å™¨æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¯¹æ¯”æ¨¡å‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¸çœŸå®è§£å†³æ–¹æ¡ˆçš„æ–¹å¼ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è¯„è®ºæ•°æ®ã€‚è¿™äº›æ•°æ®ç”¨äºåœ¨RLVRè¿‡ç¨‹ä¸­å¯åŠ¨LLMï¼Œä»¥æé«˜éªŒè¯èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMirror-Verifieråœ¨è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ±‚è§£å™¨è¯šå®åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºå¤šæ•°æŠ•ç¥¨åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾é€šè¿‡è§£å†³æ–¹æ¡ˆé‡‡æ ·å’Œèšåˆå·²æˆä¸ºæ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹é€‰æ‹©è™½ç„¶å¸¸ç”¨ï¼Œä½†éš¾ä»¥è¯†åˆ«å°‘æ•°æ­£ç¡®ç­”æ¡ˆï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚</li>
<li>Mirror-Critiqueæ¡†æ¶æ—¨åœ¨é€šè¿‡è®­ç»ƒæºå¸¦ä¿¡æ¯ä¸°å¯Œçš„è¯„è®ºä¿¡å·çš„éªŒè¯å™¨æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”æ¨¡å‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¸çœŸå®è§£å†³æ–¹æ¡ˆçš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡çš„è¯„è®ºæ•°æ®ã€‚</li>
<li>åˆ©ç”¨è¿™äº›è¯„è®ºæ•°æ®å†·å¯åŠ¨LLMä»¥æé«˜éªŒè¯èƒ½åŠ›ã€‚</li>
<li>Mirror-Verifieræ˜¾è‘—æé«˜è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ±‚è§£å™¨è¯šå®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d6dda79364da6db33722e701263256a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086407&auth_key=1760086407-0-0-72b8165fe764e00835e44a5b376bdc38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed5382522c38be3f41253ff07fbbeb0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086416&auth_key=1760086416-0-0-d3487f29a2af35afba83d49f2be81922&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b86e20e9087f0d5171cc4763952f5ca4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086423&auth_key=1760086423-0-0-d7fb4971cc98acc705f6b11b032e479e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Comprehensive-Interactive-Change-Understanding-in-Remote-Sensing-A-Large-scale-Dataset-and-Dual-granularity-Enhanced-VLM"><a href="#Towards-Comprehensive-Interactive-Change-Understanding-in-Remote-Sensing-A-Large-scale-Dataset-and-Dual-granularity-Enhanced-VLM" class="headerlink" title="Towards Comprehensive Interactive Change Understanding in Remote   Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM"></a>Towards Comprehensive Interactive Change Understanding in Remote   Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM</h2><p><strong>Authors:Junxiao Xue, Quan Deng, Xuecheng Wu, Kelu Yao, Xinyi Yin, Fei Yu, Wei Zhou, Yanfei Zhong, Yang Liu, Dingkang Yang</strong></p>
<p>Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced vision-guided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S*m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. </p>
<blockquote>
<p>é¥æ„Ÿå˜åŒ–ç†è§£ï¼ˆRSCUï¼‰åœ¨åˆ†æé¥æ„Ÿå›¾åƒä»¥åŠäº†è§£äººç±»æ´»åŠ¨å¦‚ä½•å½±å“ç¯å¢ƒæ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨å¤šæ ·åŒ–çš„å˜åŒ–æè¿°ã€è®¡æ•°å’Œå®šä½ä»»åŠ¡ä¸­ç¼ºä¹æ·±å…¥çš„ç†è§£å’Œäº¤äº’ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†ChangeIMTIï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§å‹äº¤äº’å¼å¤šä»»åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å››ä¸ªäº’è¡¥ä»»åŠ¡ï¼ŒåŒ…æ‹¬å˜åŒ–æè¿°ã€äºŒå…ƒå˜åŒ–åˆ†ç±»ã€å˜åŒ–è®¡æ•°å’Œå˜åŒ–å®šä½ã€‚åŸºäºè¿™ä¸ªæ–°æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§æ–°å‹è§†è§‰å¼•å¯¼çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆChangeVGï¼‰ï¼Œè¯¥æ¨¡å‹å…·æœ‰åŒç²’åº¦æ„è¯†ï¼Œé€‚ç”¨äºåŒæ—¶åºé¥æ„Ÿå›¾åƒï¼ˆå³åŒä¸€åœ°åŒºä¸åŒæ—¶é—´çš„ä¸¤å¼ é¥æ„Ÿå›¾åƒï¼‰ã€‚å¼•å…¥çš„è§†è§‰å¼•å¯¼æ¨¡å—æ˜¯ä¸€ä¸ªåŒåˆ†æ”¯æ¶æ„ï¼ŒååŒç»“åˆäº†ç²¾ç»†çš„ç©ºé—´ç‰¹å¾æå–å’Œé«˜å±‚æ¬¡è¯­ä¹‰æ‘˜è¦ã€‚è¿™äº›ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼è¿›ä¸€æ­¥ä½œä¸ºè¾…åŠ©æç¤ºï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å¯¼å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼ˆä¾‹å¦‚Qwen2.5-VL-7Bï¼‰ï¼Œä»è€Œä¿ƒè¿›åˆ†å±‚è·¨æ¨¡æ€å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªä»»åŠ¡çš„å®éªŒæ¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨å˜åŒ–æè¿°ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»¼åˆS*mæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å¼ºæ–¹æ³•Semantic-CCï¼ˆé«˜å‡º1.39åˆ†ï¼‰ï¼Œè¯¥æŒ‡æ ‡ç»“åˆäº†è¯­ä¹‰ç›¸ä¼¼æ€§å’Œæè¿°å‡†ç¡®æ€§ï¼Œä¸ºå˜åŒ–æè¿°æä¾›äº†æ•´ä½“è¯„ä»·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€ç³»åˆ—æ¶ˆèç ”ç©¶æ¥æ£€éªŒæˆ‘ä»¬æ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿œç¨‹æ„Ÿåº”å˜åŒ–ç†è§£ï¼ˆRSCUï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ•°æ®é›†åœ¨å¤šæ ·å˜åŒ–æè¿°ã€è®¡æ•°å’Œå®šä½ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæ„å»ºäº†ChangeIMTIè¿™ä¸€å¤§å‹å¤šä»»åŠ¡äº’åŠ¨æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å˜åŒ–æè¿°ã€äºŒå…ƒå˜åŒ–åˆ†ç±»ã€å˜åŒ–è®¡æ•°å’Œå˜åŒ–å®šä½å››ä¸ªäº’è¡¥ä»»åŠ¡ã€‚åŸºäºè¯¥æ–°æ•°æ®é›†ï¼Œè®¾è®¡äº†ä¸€ç§å…·æœ‰åŒç²’åº¦æ„è¯†çš„è§†è§‰å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆChangeVGï¼‰ï¼Œç”¨äºå¤„ç†åŒæ—¶æ€é¥æ„Ÿå›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç²¾ç»†çš„ç©ºé—´ç‰¹å¾æå–å’Œé«˜å±‚æ¬¡è¯­ä¹‰æ€»ç»“çš„ç»“åˆï¼Œä¸°å¯Œäº†è¡¨ç¤ºå½¢å¼ï¼Œä½œä¸ºè¾…åŠ©æç¤ºå¼•å¯¼å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä¿ƒè¿›äº†åˆ†å±‚è·¨æ¨¡æ€å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å˜åŒ–æè¿°ä»»åŠ¡ä¸Šä¼˜äºæœ€å¼ºæ–¹æ³•Semantic-CCï¼Œç»¼åˆS*mæŒ‡æ ‡æé«˜äº†1.39ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹æ„Ÿåº”å˜åŒ–ç†è§£ï¼ˆRSCUï¼‰åœ¨åˆ†æé¥æ„Ÿå›¾åƒå’Œäº†è§£äººç±»æ´»åŠ¨å¯¹ç¯å¢ƒå½±å“æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†åœ¨å˜åŒ–æè¿°ã€è®¡æ•°å’Œå®šä½ä»»åŠ¡ä¸­ç¼ºä¹æ·±åº¦ç†è§£å’Œäº’åŠ¨ã€‚</li>
<li>æ„å»ºäº†ChangeIMTIè¿™ä¸€å¤§å‹å¤šä»»åŠ¡äº’åŠ¨æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å››ä¸ªäº’è¡¥ä»»åŠ¡ã€‚</li>
<li>è®¾è®¡äº†å…·æœ‰åŒç²’åº¦æ„è¯†çš„è§†è§‰å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆChangeVGï¼‰ï¼Œé€‚ç”¨äºåŒæ—¶æ€é¥æ„Ÿå›¾åƒã€‚</li>
<li>è§†è§‰å¼•å¯¼æ¨¡å—é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œç»“åˆäº†ç²¾ç»†çš„ç©ºé—´ç‰¹å¾æå–å’Œé«˜å±‚æ¬¡è¯­ä¹‰æ€»ç»“ã€‚</li>
<li>ChangeVGæ–¹æ³•åœ¨å˜åŒ–æè¿°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç»¼åˆS*mæŒ‡æ ‡ä¼˜äºæœ€å¼ºæ–¹æ³•Semantic-CCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-26b2520792c60fd4b4edff91a32878cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086431&auth_key=1760086431-0-0-c59576833d356fccfbb0fdced189ee0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7639a33f2f172376d8a153686912f48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086439&auth_key=1760086439-0-0-858dbea8fff072c7896b39414150339e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d262624cf47e465cc9f69eed387411c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086446&auth_key=1760086446-0-0-e94aa1ab3e40ba62a4d711a59fbd947b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94a2618d2a175424fbf9fbafbbd8f4cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086453&auth_key=1760086453-0-0-f6f7b3ff37b26bd651510be7498a4ecf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="The-Geometry-of-Creative-Variability-How-Credal-Sets-Expose-Calibration-Gaps-in-Language-Models"><a href="#The-Geometry-of-Creative-Variability-How-Credal-Sets-Expose-Calibration-Gaps-in-Language-Models" class="headerlink" title="The Geometry of Creative Variability: How Credal Sets Expose Calibration   Gaps in Language Models"></a>The Geometry of Creative Variability: How Credal Sets Expose Calibration   Gaps in Language Models</h2><p><strong>Authors:Esteban Garces Arias, Julian Rodemann, Christian Heumann</strong></p>
<p>Understanding uncertainty in large language models remains a fundamental challenge, particularly in creative tasks where multiple valid outputs exist. We present a geometric framework using credal sets - convex hulls of probability distributions - to quantify and decompose uncertainty in neural text generation, calibrated against human creative variation. Analyzing 500 creative writing prompts from the WritingPrompts dataset with 10 unique human continuations each, we evaluate four language models across five decoding strategies, generating 100,000 stories. Our credal set analysis reveals substantial gaps in capturing human creative variation, with the best model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We decompose total uncertainty into epistemic and aleatoric components, finding that the choice of decoding strategy contributes 39.4% to 72.0% of total epistemic uncertainty. Model scale shows weak correlation with calibration quality and no significant difference exists between base and instruction-tuned models in calibration quality. Our geometric framework provides actionable insights for improving generation systems for human-AI creative alignment. We release our complete experimental framework. </p>
<blockquote>
<p>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸ç¡®å®šæ€§ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å¤šä¸ªæœ‰æ•ˆè¾“å‡ºçš„åˆ›é€ æ€§ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå‡ ä½•æ¡†æ¶ï¼Œä½¿ç”¨å¯ä¿¡é›†ï¼ˆæ¦‚ç‡åˆ†å¸ƒå‡¸åŒ…ï¼‰æ¥é‡åŒ–å’Œåˆ†è§£ç¥ç»æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶ä¸äººç±»åˆ›é€ æ€§å˜åŒ–è¿›è¡Œæ ¡å‡†ã€‚æˆ‘ä»¬åˆ†æäº†WritingPromptsæ•°æ®é›†ä¸­çš„500ä¸ªåˆ›é€ æ€§å†™ä½œæç¤ºï¼Œæ¯ä¸ªæç¤ºæœ‰10ä¸ªç‹¬ç‰¹çš„äººç±»ç»­å†™ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªè¯­è¨€æ¨¡å‹çš„äº”ç§è§£ç ç­–ç•¥ï¼Œç”Ÿæˆäº†10ä¸‡ä¸ªæ•…äº‹ã€‚æˆ‘ä»¬çš„å¯ä¿¡é›†åˆ†ææ˜¾ç¤ºï¼Œåœ¨æ•æ‰äººç±»åˆ›é€ æ€§å˜åŒ–æ–¹é¢å­˜åœ¨å·¨å¤§å·®è·ï¼Œæœ€ä½³æ¨¡å‹ä¸äººç±»çš„æ ¡å‡†ä»…ä¸º0.434ï¼ˆGemma-2Bï¼Œæ¸©åº¦ä¸º0.7ï¼‰ã€‚æˆ‘ä»¬å°†æ€»ä¸ç¡®å®šæ€§åˆ†è§£ä¸ºè®¤çŸ¥ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§æˆåˆ†ï¼Œå‘ç°è§£ç ç­–ç•¥çš„é€‰æ‹©å¯¹æ€»è®¤çŸ¥ä¸ç¡®å®šæ€§çš„è´¡çŒ®åœ¨39.4%è‡³72.0%ä¹‹é—´ã€‚æ¨¡å‹è§„æ¨¡ä¸æ ¡å‡†è´¨é‡å‘ˆå¼±ç›¸å…³æ€§ï¼ŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨æ ¡å‡†è´¨é‡ä¸Šä¸å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„å‡ ä½•æ¡†æ¶ä¸ºæ”¹è¿›äººç±»-äººå·¥æ™ºèƒ½åˆ›é€ æ€§å¯¹é½çš„ç”Ÿæˆç³»ç»Ÿæä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚æˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„å®éªŒæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23088v1">PDF</a> Accepted at the 2nd UncertaiNLP Workshop @ EMNLP 2025</p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹çš„ç¡®å®šæ€§é—®é¢˜æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å¤šä¸ªæœ‰æ•ˆè¾“å‡ºçš„åˆ›é€ æ€§ä»»åŠ¡ä¸­æ›´æ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‡ ä½•æ¡†æ¶çš„ç®—æ³•ï¼Œä½¿ç”¨å¯ä¿¡é›†ï¼ˆæ¦‚ç‡åˆ†å¸ƒå‡¸åŒ…ï¼‰æ¥é‡åŒ–å¹¶åˆ†è§£ç¥ç»æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶ä¸äººç±»åˆ›é€ æ€§å˜åŒ–è¿›è¡Œæ ¡å‡†ã€‚é€šè¿‡å¯¹WritingPromptsæ•°æ®é›†500ä¸ªåˆ›æ„å†™ä½œæç¤ºçš„åˆ†æï¼Œæ¯ä¸ªæç¤ºæœ‰10ä¸ªä¸åŒçš„äººä¸ºå»¶ç»­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªè¯­è¨€æ¨¡å‹çš„äº”ç§è§£ç ç­–ç•¥ï¼Œç”Ÿæˆäº†åä¸‡ä¸ªæ•…äº‹ã€‚æˆ‘ä»¬çš„å¯ä¿¡é›†åˆ†ææ˜¾ç¤ºï¼Œåœ¨æ•æ‰äººç±»åˆ›é€ æ€§å˜åŒ–æ–¹é¢å­˜åœ¨å·¨å¤§å·®è·ï¼Œæœ€å¥½çš„æ¨¡å‹ä¸äººç±»æ ¡å‡†ä»…è¾¾åˆ°0.434ï¼ˆGemma-2Bæ¨¡å‹ï¼Œæ¸©åº¦ä¸º0.7ï¼‰ã€‚æˆ‘ä»¬å°†æ€»ä¸ç¡®å®šæ€§åˆ†è§£ä¸ºçŸ¥è¯†æ€§å’Œå¶ç„¶æ€§æˆåˆ†ï¼Œå‘ç°è§£ç ç­–ç•¥çš„é€‰æ‹©å¯¹æ€»çŸ¥è¯†ä¸ç¡®å®šæ€§çš„è´¡çŒ®åœ¨39.4%~72.0%ä¹‹é—´ã€‚æ¨¡å‹è§„æ¨¡ä¸æ ¡å‡†è´¨é‡å‘ˆå¼±ç›¸å…³æ€§ï¼ŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨æ ¡å‡†è´¨é‡ä¸Šæ— æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„å‡ ä½•æ¡†æ¶ä¸ºæ”¹è¿›äººç±»-äººå·¥æ™ºèƒ½åˆ›é€ æ€§å¯¹é½çš„ç”Ÿæˆç³»ç»Ÿæä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„å®éªŒæ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»æ–‡æœ¬ç”Ÿæˆä¸­å­˜åœ¨ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­ã€‚</li>
<li>æå‡ºä½¿ç”¨å‡ ä½•æ¡†æ¶å’Œå¯ä¿¡é›†æ¥é‡åŒ–å¹¶åˆ†è§£ä¸ç¡®å®šæ€§ã€‚</li>
<li>é€šè¿‡åˆ†æWritingPromptsæ•°æ®é›†ï¼Œå‘ç°æ•æ‰äººç±»åˆ›é€ æ€§å˜åŒ–æ–¹é¢å­˜åœ¨å·¨å¤§å·®è·ã€‚</li>
<li>æœ€ä½³æ¨¡å‹ä¸äººç±»çš„æ ¡å‡†ç¨‹åº¦æœ‰é™ã€‚</li>
<li>è§£ç ç­–ç•¥å¯¹æ€»çŸ¥è¯†ä¸ç¡®å®šæ€§çš„è´¡çŒ®æ˜¾è‘—ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ä¸æ ¡å‡†è´¨é‡ä¹‹é—´å‘ˆå¼±ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3b8c9ad5812af46e16cf2ef368b425b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086460&auth_key=1760086460-0-0-c38c52cb6ef3473d5aef6c981eba7198&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-074b04c4f81fbbd5fa8aac465f5aa744~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086467&auth_key=1760086467-0-0-010c433c751931c35e3adb164645e5dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4abbc33ac7baf7023b1e9e822fb7292~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086474&auth_key=1760086474-0-0-65a2975c5bf16c8f090d022befe8914e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9be3ab5563f1e212ebefbcb93887d16b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086481&auth_key=1760086481-0-0-c7b07d38a6488ea49d8329f618a9598f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DiTraj-training-free-trajectory-control-for-video-diffusion-transformer"><a href="#DiTraj-training-free-trajectory-control-for-video-diffusion-transformer" class="headerlink" title="DiTraj: training-free trajectory control for video diffusion transformer"></a>DiTraj: training-free trajectory control for video diffusion transformer</h2><p><strong>Authors:Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su, Zhicheng Zhao</strong></p>
<p>Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the objectâ€™s trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokensâ€™ position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¸”æ‹¥æœ‰3Då…¨æ³¨æ„åŠ›ã€‚è½¨è¿¹æ§åˆ¶ä»£è¡¨äº†å¯æ§è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„ä¸€é¡¹å‹å¥½ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡è®­ç»ƒèµ„æºï¼Œè¦ä¹ˆä¸“ä¸ºU-Netè®¾è®¡ï¼Œå¹¶æœªå……åˆ†åˆ©ç”¨DiTçš„å“è¶Šæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiTrajï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ— éœ€è®­ç»ƒçš„è½¨è¿¹æ§åˆ¶æ¡†æ¶ï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆï¼Œå¹¶ä¸ºDiTé‡èº«å®šåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œä¸ºäº†æ³¨å…¥ç‰©ä½“çš„è½¨è¿¹ï¼Œæˆ‘ä»¬æå‡ºå‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼ï¼šæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†ç”¨æˆ·æä¾›çš„æç¤ºè½¬åŒ–ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«æŒ‡å¯¼è§†é¢‘ä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç”Ÿæˆã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†æäº†3Då…¨æ³¨æ„åŠ›ï¼Œå¹¶æ¢ç´¢äº†ä»¤ç‰Œé—´æ³¨æ„åŠ›å¾—åˆ†ä¸ä½ç½®åµŒå…¥ä¹‹é—´çš„ç´§å¯†å…³ç³»ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å¸§æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰ã€‚é€šè¿‡ä»…ä¿®æ”¹å‰æ™¯ä»¤ç‰Œçš„ä½ç½®åµŒå…¥ï¼ŒSTD-RoPEæ¶ˆé™¤äº†è·¨å¸§çš„ç©ºé—´å·®å¼‚ï¼Œå¢å¼ºäº†å®ƒä»¬ä¹‹é—´çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†è½¨è¿¹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´ä½ç½®åµŒå…¥çš„å¯†åº¦å®ç°äº†3Dæ„ŸçŸ¥è½¨è¿¹æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21839v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå€ŸåŠ©3Då…¨æ³¨æ„åŠ›è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚è½¨è¿¹æ§åˆ¶æ˜¯å¯æ§è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªç”¨æˆ·å‹å¥½å‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡è®­ç»ƒèµ„æºï¼Œè¦ä¹ˆä¸“ä¸ºU-Netè®¾è®¡ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨DiTçš„å“è¶Šæ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDiTrajï¼Œä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„è½¨è¿¹æ§åˆ¶çš„ç®€å•æœ‰æ•ˆä¸”æ— éœ€è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºDiTè®¾è®¡ã€‚é€šè¿‡å‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼æ³¨å…¥ç‰©ä½“è½¨è¿¹ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†ç”¨æˆ·æç¤ºè½¬æ¢ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«æŒ‡å¯¼è§†é¢‘ä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç”Ÿæˆã€‚æˆ‘ä»¬åˆ†æäº†3Då…¨æ³¨æ„åŠ›ï¼Œå¹¶å‘ç°ä»¤ç‰Œé—´æ³¨æ„åŠ›å¾—åˆ†ä¸ä½ç½®åµŒå…¥ä¹‹é—´å­˜åœ¨ç´§å¯†å…³è”ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè·¨å¸§æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰ã€‚é€šè¿‡ä»…ä¿®æ”¹å‰æ™¯ä»¤ç‰Œçš„ä½ç½®åµŒå…¥ï¼ŒSTD-RoPEæ¶ˆé™¤äº†è·¨å¸§ç©ºé—´å·®å¼‚ï¼Œå¢å¼ºäº†å®ƒä»¬ä¹‹é—´çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†è½¨è¿¹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å¯†åº¦å®ç°äº†3Dæ„ŸçŸ¥è½¨è¿¹æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è½¨è¿¹æ§åˆ¶åœ¨è§†é¢‘ç”Ÿæˆä¸­å­˜åœ¨èµ„æºéœ€æ±‚å¤§æˆ–ç‰¹å®šäºU-Netè®¾è®¡çš„é—®é¢˜ã€‚</li>
<li>æå‡ºDiTrajæ¡†æ¶ï¼Œä¸€ä¸ªé’ˆå¯¹DiTè®¾è®¡çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè½¨è¿¹æ§åˆ¶çš„æ— è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>é€šè¿‡å‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼æ³¨å…¥ç‰©ä½“è½¨è¿¹ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è½¬æ¢ç”¨æˆ·æç¤ºï¼ŒæŒ‡å¯¼è§†é¢‘çš„å‰æ™¯å’ŒèƒŒæ™¯ç”Ÿæˆã€‚</li>
<li>åˆ†æ3Då…¨æ³¨æ„åŠ›ï¼Œå‘ç°ä»¤ç‰Œé—´æ³¨æ„åŠ›ä¸ä½ç½®åµŒå…¥çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5cbaccefbb961cf59444b844dfa3ce0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086489&auth_key=1760086489-0-0-dd3490b010977fc9910413770c966c2c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c3a6a1385e3edf5cc6f7a33218ff9fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086496&auth_key=1760086496-0-0-cf6f10e6473dabe7e2ca2819b97b350b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6089a4053378a6de0b55cefdba202d50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086503&auth_key=1760086503-0-0-a95039e6030b9198ef72e242ed5f8c84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20fc6882e91771bc93986fb48bbb24ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101349&auth_key=1760101349-0-0-751e6b33159597135a2274703c0df870&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Code2MCP-Transforming-Code-Repositories-into-MCP-Services"><a href="#Code2MCP-Transforming-Code-Repositories-into-MCP-Services" class="headerlink" title="Code2MCP: Transforming Code Repositories into MCP Services"></a>Code2MCP: Transforming Code Repositories into MCP Services</h2><p><strong>Authors:Chaoqian Ouyang, Ling Yue, Shimin Di, Libin Zheng, Linan Yue, Shaowu Pan, Jian Yin, Min-Ling Zhang</strong></p>
<p>The Model Context Protocol (MCP) aims to create a standard for how Large Language Models use tools. However, most current research focuses on selecting tools from an existing pool. A more fundamental, yet largely overlooked, problem is how to populate this pool by converting the vast number of existing software projects into MCP-compatible services. To bridge this gap, we introduce Code2MCP, an agent-based framework that automatically transforms a GitHub repository into a functional MCP service with minimal human intervention. Code2MCP employs a multi-agent workflow for code analysis, environment setup, tool function design, and service generation, enhanced by a self-correcting loop to ensure reliability. We demonstrate that Code2MCP successfully transforms open-source computing libraries in scientific fields such as bioinformatics, mathematics, and fluid dynamics that are not available in existing MCP servers. By providing a novel automated pathway to unlock GitHub, the worldâ€™s largest code repository, for the MCP ecosystem, Code2MCP serves as a catalyst to significantly accelerate the protocolâ€™s adoption and practical application. The code is public at <a target="_blank" rel="noopener" href="https://github.com/DEFENSE-SEU/Code2MCP">https://github.com/DEFENSE-SEU/Code2MCP</a>. </p>
<blockquote>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ—¨åœ¨åˆ›å»ºå¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨å·¥å…·çš„æ ‡å‡†ã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨ä»ç°æœ‰å·¥å…·æ± ä¸­é€‰å·¥å…·ã€‚ä¸€ä¸ªæ›´ä¸ºåŸºç¡€ä½†å¸¸è¢«å¿½è§†çš„é—®é¢˜æ˜¯ï¼Œå¦‚ä½•é€šè¿‡å°†å¤§é‡ç°æœ‰è½¯ä»¶é¡¹ç›®è½¬åŒ–ä¸ºMCPå…¼å®¹æœåŠ¡æ¥å¡«å……è¿™ä¸ªå·¥å…·æ± ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†Code2MCPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†GitHubä»“åº“è½¬åŒ–ä¸ºåŠŸèƒ½å‹çš„MCPæœåŠ¡ï¼Œå¹¶ä¸”åªéœ€æå°‘çš„äººå·¥å¹²é¢„ã€‚Code2MCPé‡‡ç”¨å¤šä»£ç†å·¥ä½œæµç¨‹è¿›è¡Œä»£ç åˆ†æã€ç¯å¢ƒè®¾ç½®ã€å·¥å…·åŠŸèƒ½è®¾è®¡å’ŒæœåŠ¡ç”Ÿæˆï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ä¿®æ­£å¾ªç¯å¢å¼ºå¯é æ€§ã€‚æˆ‘ä»¬è¯æ˜Code2MCPèƒ½å¤ŸæˆåŠŸè½¬åŒ–ç§‘å­¦é¢†åŸŸä¸­çš„å¼€æºè®¡ç®—åº“ï¼Œå¦‚ç”Ÿç‰©ä¿¡æ¯å­¦ã€æ•°å­¦å’Œæµä½“åŠ¨åŠ›å­¦ç­‰é¢†åŸŸä¸­é‚£äº›åœ¨ç°æœ‰MCPæœåŠ¡å™¨ä¸­æ— æ³•è·å¾—çš„åº“ã€‚é€šè¿‡ä¸ºMCPç”Ÿæ€ç³»ç»Ÿè§£é”ä¸–ç•Œä¸Šæœ€å¤§çš„ä»£ç ä»“åº“GitHubæä¾›æ–°å‹è‡ªåŠ¨åŒ–é€”å¾„ï¼ŒCode2MCPæˆä¸ºåŠ é€Ÿè¯¥åè®®é‡‡ç”¨å’Œå®é™…åº”ç”¨çš„å‚¬åŒ–å‰‚ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/DEFENSE-SEU/Code2MCP%E3%80%82">https://github.com/DEFENSE-SEU/Code2MCPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05941v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥å…·ä½¿ç”¨æ ‡å‡†åŒ–é—®é¢˜å—åˆ°å…³æ³¨ï¼Œå½“å‰ç ”ç©¶å¤šèšç„¦äºä»ç°æœ‰å·¥å…·æ± ä¸­ç­›é€‰å·¥å…·ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ›´åŸºç¡€ä½†å¸¸è¢«å¿½è§†çš„é—®é¢˜æ˜¯ï¼Œå¦‚ä½•å°†å¤§é‡ç°æœ‰è½¯ä»¶é¡¹ç›®è½¬åŒ–ä¸ºé€‚ç”¨äºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰çš„æœåŠ¡ä»¥å¡«å……å·¥å…·æ± ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Code2MCPæ¡†æ¶ï¼Œå®ƒèƒ½å°†GitHubä»“åº“è‡ªåŠ¨è½¬åŒ–ä¸ºåŠŸèƒ½æ€§çš„MCPæœåŠ¡ï¼Œåªéœ€æå°‘çš„äººå·¥å¹²é¢„ã€‚Code2MCPé‡‡ç”¨å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹è¿›è¡Œä»£ç åˆ†æã€ç¯å¢ƒè®¾ç½®ã€å·¥å…·åŠŸèƒ½è®¾è®¡å’ŒæœåŠ¡ç”Ÿæˆï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ä¿®æ­£å¾ªç¯ç¡®ä¿å¯é æ€§ã€‚Code2MCPæˆåŠŸè½¬åŒ–äº†ç§‘å­¦é¢†åŸŸå¦‚ç”Ÿç‰©ä¿¡æ¯å­¦ã€æ•°å­¦å’Œæµä½“åŠ¨åŠ›å­¦çš„å¼€æºè®¡ç®—åº“ï¼Œè¿™äº›åº“åœ¨ç°æœ‰çš„MCPæœåŠ¡å™¨ä¸­å¹¶ä¸å¯ç”¨ã€‚Code2MCPä¸ºMCPç”Ÿæ€ç³»ç»Ÿè§£é”äº†ä¸–ç•Œä¸Šæœ€å¤§çš„ä»£ç ä»“åº“GitHubï¼Œä»è€Œä¿ƒè¿›äº†è¯¥åè®®çš„é‡‡ç”¨å’Œå®é™…åº”ç”¨çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Model Context Protocol (MCP) æ—¨åœ¨æ ‡å‡†åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨å·¥å…·çš„æ–¹å¼ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨ä»ç°æœ‰å·¥å…·æ± ä¸­ç­›é€‰å·¥å…·ï¼Œä½†å¦‚ä½•å¡«å……è¿™ä¸ªå·¥å…·æ± çš„é—®é¢˜è¢«å¿½è§†ã€‚</li>
<li>Code2MCP æ˜¯ä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨å°†GitHubä»“åº“è½¬åŒ–ä¸ºåŠŸèƒ½æ€§çš„MCPæœåŠ¡ã€‚</li>
<li>Code2MCP é‡‡ç”¨å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ä»£ç åˆ†æã€ç¯å¢ƒè®¾ç½®ã€å·¥å…·åŠŸèƒ½è®¾è®¡å’ŒæœåŠ¡ç”Ÿæˆã€‚</li>
<li>Code2MCP å…·æœ‰è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½ï¼Œç¡®ä¿å¯é æ€§ã€‚</li>
<li>Code2MCP æˆåŠŸè½¬åŒ–äº†ç§‘å­¦é¢†åŸŸçš„å¼€æºè®¡ç®—åº“ï¼Œå¦‚ç”Ÿç‰©ä¿¡æ¯å­¦ã€æ•°å­¦å’Œæµä½“åŠ¨åŠ›å­¦ã€‚</li>
<li>Code2MCP ä¸ºè§£é”GitHubä½œä¸ºMCPç”Ÿæ€ç³»ç»Ÿçš„èµ„æºæä¾›äº†æ–°å‹è‡ªåŠ¨åŒ–é€”å¾„ï¼ŒåŠ é€Ÿäº†MCPçš„é‡‡ç”¨å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6497072c44db9ce7328d5aa6f052173b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101356&auth_key=1760101356-0-0-837292491b20871a46b6b7e0ab9a5894&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3dd37afeeaf30168ea4508a3bef8c4eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101364&auth_key=1760101364-0-0-fbca21bd174995d7fcdd3ddfa4486a89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b203a38f8a0a3038a25d4e15c8a56457~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101371&auth_key=1760101371-0-0-860a60d9a0fa94067df04e19e6c83500&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b308d908f2e35d66290830d47078421~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101379&auth_key=1760101379-0-0-9844b683ced9656362c623dc4586ff23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-501dd9c713735d9ce328b27922ce969b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101385&auth_key=1760101385-0-0-f1c26b381e4aeb1a89a03c146db52775&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff399e7880c57165cc30aab6a5a3e22c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760101392&auth_key=1760101392-0-0-9e2266e5d45c160b4386201d97e63aeb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-02/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-6c95fafcbda4086ea80cee5e6a942048~resize:0:q75.jpg?source=1f5c5e47&expiration=1760086575&auth_key=1760086575-0-0-8c4e041e663dcb38f5dca9b4e1d861b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  OceanGym A Benchmark Environment for Underwater Embodied Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-02/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-271f8ed29c861540ff3eb27a6dbdd428.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-02  Query-Kontext An Unified Multimodal Model for Image Generation and   Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
