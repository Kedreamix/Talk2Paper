<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  LazyDrag Enabling Stable Drag-Based Editing on Multi-Modal Diffusion   Transformers via Explicit Correspondence">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-17-æ›´æ–°"><a href="#2025-09-17-æ›´æ–°" class="headerlink" title="2025-09-17 æ›´æ–°"></a>2025-09-17 æ›´æ–°</h1><h2 id="LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence"><a href="#LazyDrag-Enabling-Stable-Drag-Based-Editing-on-Multi-Modal-Diffusion-Transformers-via-Explicit-Correspondence" class="headerlink" title="LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion   Transformers via Explicit Correspondence"></a>LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion   Transformers via Explicit Correspondence</h2><p><strong>Authors:Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum</strong></p>
<p>The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a &#96;&#96;tennis ballâ€™â€™, or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms. </p>
<blockquote>
<p>å¯¹åŸºäºæ³¨æ„åŠ›çš„éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–å·²æˆä¸ºåŸºäºæ‹–æ‹½çš„ç¼–è¾‘ä¸­çš„æ ¸å¿ƒç“¶é¢ˆï¼Œå¯¼è‡´åè½¬å¼ºåº¦å‡å¼±ã€æµ‹è¯•æ—¶ä¼˜åŒ–ï¼ˆTTOï¼‰æˆæœ¬é«˜æ˜‚çš„åŸºæœ¬å¦¥åã€‚è¿™ç§å¦¥åä¸¥é‡é™åˆ¶äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒæŠ‘åˆ¶äº†é«˜ä¿çœŸè¡¥å…¨å’Œæ–‡æœ¬å¼•å¯¼çš„åˆ›ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LazyDragï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨çš„åŸºäºæ‹–æ‹½çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œå®ƒç›´æ¥æ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ç”¨æˆ·çš„æ‹–æ‹½è¾“å…¥ç”Ÿæˆä¸€ä¸ªæ˜ç¡®çš„å¯¹åº”å›¾ï¼Œä½œä¸ºå¢å¼ºæ³¨æ„åŠ›æ§åˆ¶çš„å¯é å‚è€ƒã€‚è¿™ä¸€å¯é çš„å‚è€ƒä¸ºç¨³å®šçš„å®Œå…¨å¼ºåº¦åè½¬è¿‡ç¨‹æ‰“å¼€äº†å¯èƒ½æ€§ï¼Œè¿™æ˜¯åŸºäºæ‹–æ‹½çš„ç¼–è¾‘ä»»åŠ¡ä¸­çš„é¦–æ¬¡å®ç°ã€‚å®ƒä¸éœ€è¦TTOï¼Œå¹¶é‡Šæ”¾äº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å› æ­¤ï¼ŒLazyDragè‡ªç„¶åœ°ç»“åˆäº†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶å’Œæ–‡æœ¬æŒ‡å¯¼ï¼Œèƒ½å¤Ÿå®ç°ä»¥å‰æ— æ³•å®ç°çš„å¤æ‚ç¼–è¾‘ï¼šæ‰“å¼€ç‹—çš„å˜´å·´å¹¶è¡¥å…¨å…¶å†…éƒ¨ï¼Œç”Ÿæˆæ–°å¯¹è±¡ï¼ˆå¦‚â€œç½‘çƒâ€ï¼‰ï¼Œæˆ–å¯¹æ¨¡ç³Šçš„æ‹–æ‹½è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ›´æ”¹ï¼ˆå¦‚å°†æ‰‹æ”¾å…¥å£è¢‹ï¼‰ã€‚æ­¤å¤–ï¼ŒLazyDragæ”¯æŒå¤šè½®å·¥ä½œæµç¨‹ï¼Œå¯åŒæ—¶æ‰§è¡Œç§»åŠ¨å’Œç¼©æ”¾æ“ä½œã€‚åœ¨DragBenchä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‹–æ‹½å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¶…è¶Šäº†åŸºçº¿ï¼Œè¿™å·²é€šè¿‡VIEScoreå’Œäººç±»è¯„ä¼°å¾—åˆ°éªŒè¯ã€‚LazyDragä¸ä»…åˆ›é€ äº†æ–°çš„æŠ€æœ¯é¡¶å°–è¡¨ç°ï¼Œè¿˜ä¸ºç¼–è¾‘èŒƒå¼å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12203v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LazyDragï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨çš„æ‹–æ‹½å¼å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ï¼Œé€šè¿‡ç”Ÿæˆç”¨æˆ·æ‹–æ‹½è¾“å…¥çš„æ˜¾å¼å¯¹åº”å›¾ï¼Œæé«˜æ³¨æ„åŠ›æ§åˆ¶ï¼Œä½¿æ‹–æ‹½å¼ç¼–è¾‘ä»»åŠ¡ä¸­é¦–æ¬¡å®ç°ç¨³å®šçš„å…¨å¼ºåº¦åè½¬è¿‡ç¨‹ï¼Œè§£é”æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ–‡æœ¬æŒ‡å¯¼ç›¸ç»“åˆï¼Œå®ç°å¤æ‚ç¼–è¾‘æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†LazyDragä½œä¸ºä¸€ç§æ–°çš„æ‹–æ‹½å¼å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ã€‚</li>
<li>LazyDragé€šè¿‡ç”Ÿæˆæ˜¾å¼å¯¹åº”å›¾æ¥æ¶ˆé™¤å¯¹éšå¼ç‚¹åŒ¹é…çš„ä¾èµ–ï¼Œæé«˜æ³¨æ„åŠ›æ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç¨³å®šçš„å®Œå…¨å¼ºåº¦åè½¬è¿‡ç¨‹ï¼Œè¿™åœ¨æ‹–æ‹½ç¼–è¾‘ä»»åŠ¡ä¸­æ˜¯é¦–æ¬¡å®ç°ã€‚</li>
<li>LazyDragè§£é”äº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ–‡æœ¬æŒ‡å¯¼ç›¸ç»“åˆã€‚</li>
<li>LazyDragæ”¯æŒå¤šè½®å·¥ä½œæµç¨‹ï¼Œå¯åŒæ—¶æ‰§è¡Œç§»åŠ¨å’Œç¼©æ”¾æ“ä½œã€‚</li>
<li>åœ¨DragBenchä¸Šè¯„ä¼°ï¼ŒLazyDragåœ¨æ‹–æ‹½å‡†ç¡®æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¾—åˆ°VIEScoreå’Œäººä¸ºè¯„ä¼°çš„éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12203v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Autoregressive   Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Autoregressive   Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate â€œDivide and Conquerâ€ design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–æ‰©æ•£æ¨¡å‹çš„ç°æœ‰è¯´è¯äººåŠ¨ç”»æ–¹æ³•å¸¸å¸¸é¢ä¸´å¸§é—´é—ªçƒã€èº«ä»½æ¼‚ç§»å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚è¿™äº›å›ºæœ‰çš„è§†é¢‘ç”Ÿæˆç®¡é“é™åˆ¶å…¶åœ¨åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AvatarSyncï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å‚è€ƒå›¾åƒç”ŸæˆçœŸå®å¯æ§çš„è¯´è¯äººåŠ¨ç”»ï¼Œç›´æ¥ç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚</p>
</blockquote>
<p>æ­¤å¤–ï¼ŒAvatarSyncé‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨åŠ›å­¦è§£è€¦ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ„çš„â€œåˆ†è€Œæ²»ä¹‹â€è®¾è®¡ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œé¢éƒ¨å…³é”®å¸§ç”Ÿæˆï¼ˆFKGï¼‰ä¸“æ³¨äºéŸ³ç´ çº§åˆ«çš„è¯­ä¹‰è¡¨ç¤ºï¼Œé€šè¿‡æ–‡æœ¬æˆ–éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå¯¹ä¸€æ˜ å°„ã€‚æ„å»ºäº†éŸ³ç´ åˆ°è§†è§‰çš„æ˜ å°„ï¼Œå°†æŠ½è±¡éŸ³ç´ é”šå®šåˆ°å­—ç¬¦çº§å•å…ƒã€‚ç»“åˆå®šåˆ¶çš„æ–‡æœ¬å¸§å› æœæ³¨æ„åŠ›æ©ç ï¼Œç”Ÿæˆå…³é”®å¸§ã€‚ç¬¬äºŒé˜¶æ®µï¼Œå¸§é—´æ’å€¼å¼ºè°ƒæ—¶é—´è¿è´¯æ€§å’Œè§†è§‰å¹³æ»‘æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ—¶é—´æˆ³æ„ŸçŸ¥è‡ªé€‚åº”ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆçš„åŒå‘ä¸Šä¸‹æ–‡æ¨ç†ã€‚ä¸ºäº†æ”¯æŒéƒ¨ç½²ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æ¨ç†ç®¡é“ï¼Œå‡å°‘äº†å»¶è¿Ÿï¼Œè€Œä¸æŸå®³è§†è§‰ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„è¯´è¯äººåŠ¨ç”»æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–æ‰©æ•£æ¨¡å‹çš„ç°æœ‰è¯´è¯äººåŠ¨ç”»æ–¹æ³•å¸¸å¸¸å­˜åœ¨å¸§é—´é—ªçƒã€èº«ä»½æ¼‚ç§»å’Œæ¨ç†é€Ÿåº¦æ…¢ç­‰é—®é¢˜ï¼Œè¿™äº›å›ºæœ‰çš„è§†é¢‘ç”Ÿæˆç®¡é“é™åˆ¶å…¶åœ¨å„ç§åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAvatarSyncï¼Œä¸€ç§åŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ä»å•ä¸ªå‚è€ƒå›¾åƒç”ŸæˆçœŸå®ä¸”å¯æ§çš„è¯´è¯äººåŠ¨ç”»ï¼Œç›´æ¥ç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚AvatarSyncé‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œè¿™æ˜¯æœ‰æ„ä¸ºä¹‹çš„â€œåˆ†è€Œæ²»ä¹‹â€è®¾è®¡ã€‚ç¬¬ä¸€é˜¶æ®µä¸“æ³¨äºéŸ³ç´ çº§è¯­ä¹‰è¡¨ç¤ºï¼Œåˆ©ç”¨æ–‡æœ¬æˆ–éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå¯¹ä¸€æ˜ å°„æ„å»ºéŸ³ç´ åˆ°è§†è§‰æ˜ å°„ï¼Œå°†æŠ½è±¡éŸ³ç´ é”šå®šåˆ°å­—ç¬¦çº§å•å…ƒã€‚ç»“åˆå®šåˆ¶çš„æ–‡æœ¬å¸§å› æœæ³¨æ„åŠ›æ©ç ï¼Œç”Ÿæˆå…³é”®å¸§ã€‚ç¬¬äºŒé˜¶æ®µç€é‡äºå¸§é—´æ’å€¼ï¼Œå¼ºè°ƒæ—¶é—´è¿è´¯æ€§å’Œè§†è§‰å¹³æ»‘åº¦ã€‚æˆ‘ä»¬å¼•å…¥ä¸€ç§åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å¸¦æœ‰æ—¶é—´æˆ³çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆçš„åŒå‘ä¸Šä¸‹æ–‡æ¨ç†ã€‚ä¸ºæ”¯æŒéƒ¨ç½²ï¼Œæˆ‘ä»¬ä¼˜åŒ–æ¨ç†ç®¡é“ï¼Œé™ä½å»¶è¿Ÿè€Œä¸å½±å“è§†è§‰ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰è¯´è¯äººåŠ¨ç”»æ–¹æ³•ï¼Œæä¾›å¯ä¼¸ç¼©ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AvatarSyncæ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®ä¸”å¯æ§çš„è¯´è¯äººåŠ¨ç”»ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œè§£è€¦è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä¸“æ³¨äºéŸ³ç´ çº§è¯­ä¹‰è¡¨ç¤ºï¼Œæ„å»ºéŸ³ç´ åˆ°è§†è§‰æ˜ å°„å¹¶ç”Ÿæˆå…³é”®å¸§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¼ºè°ƒå¸§é—´æ’å€¼çš„æ—¶ç©ºè¿è´¯æ€§å’Œè§†è§‰å¹³æ»‘åº¦ã€‚</li>
<li>å¼•å…¥å¸¦æœ‰æ—¶é—´æˆ³çš„åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚</li>
<li>ä¼˜åŒ–æ¨ç†ç®¡é“ä»¥é™ä½å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>AvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12052v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12052v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12052v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12052v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12052v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Concept-Erasure-in-Diffusion-Models-A-Theoretical-Perspective-on-Security-and-Robustness"><a href="#Robust-Concept-Erasure-in-Diffusion-Models-A-Theoretical-Perspective-on-Security-and-Robustness" class="headerlink" title="Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on   Security and Robustness"></a>Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on   Security and Robustness</h2><p><strong>Authors:Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, Bo Zhang</strong></p>
<p>Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the modelâ€™s outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œä½†ä¹Ÿåœ¨éšç§ã€å…¬å¹³å’Œå®‰å…¨æ–¹é¢å¸¦æ¥äº†æ—¥ç›Šå¢åŠ çš„é£é™©ã€‚å› æ­¤ï¼Œå­˜åœ¨å¯¹ä»è¿™äº›æ¨¡å‹ä¸­åˆ é™¤æ•æ„Ÿæˆ–æœ‰å®³æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œæˆäººå†…å®¹ã€ä¸ªäººç§å¯†ä¿¡æ¯ã€è‰ºæœ¯é£æ ¼ç­‰ï¼‰çš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†æ‰©æ•£æ¨¡å‹ä¸­çš„ç¨³å¥æ¦‚å¿µåˆ é™¤æ¡†æ¶SCOREï¼ˆå®‰å…¨ä¸”é¢å‘æ¦‚å¿µçš„ç¨³å¥åˆ é™¤ï¼‰ã€‚SCOREå°†æ¦‚å¿µåˆ é™¤åˆ¶å®šä¸ºå¯¹æŠ—ç‹¬ç«‹æ€§é—®é¢˜ï¼Œä»ç†è®ºä¸Šä¿è¯äº†æ¨¡å‹çš„è¾“å‡ºä¸åˆ é™¤çš„æ¦‚å¿µåœ¨ç»Ÿè®¡ä¸Šç‹¬ç«‹ã€‚ä¸åŒäºå…ˆå‰çš„æ–¹æ³•ï¼ŒSCOREæœ€å°åŒ–ç›®æ ‡æ¦‚å¿µä¸ç”Ÿæˆè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œæä¾›å¯è¯æ˜çš„æ¦‚å¿µåˆ é™¤ä¿è¯ã€‚æˆ‘ä»¬æä¾›äº†æ­£å¼è¯æ˜ï¼Œå»ºç«‹äº†æ”¶æ•›å±æ€§å¹¶æ¨å¯¼å‡ºäº†æ®‹ä½™æ¦‚å¿µæ³„æ¼çš„ä¸Šé™ã€‚ä»å®è¯è§’åº¦çœ‹ï¼Œæˆ‘ä»¬åœ¨Stable Diffusionå’ŒFLUXä¸Šå¯¹SCOREè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼šå¯¹è±¡åˆ é™¤ã€æˆäººå†…å®¹ç§»é™¤ã€åäººé¢éƒ¨æŠ‘åˆ¶å’Œè‰ºæœ¯é£æ ¼é—å¿˜ã€‚SCOREæŒç»­ä¼˜äºåŒ…æ‹¬EraseAnythingã€ANTã€MACEã€ESDå’ŒUCEåœ¨å†…çš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œåœ¨æé«˜åˆ é™¤æ•ˆæœé«˜è¾¾12.5%çš„åŒæ—¶ï¼Œä¿æŒæˆ–æé«˜äº†å›¾åƒè´¨é‡ã€‚é€šè¿‡æ•´åˆå¯¹æŠ—ä¼˜åŒ–ã€è½¨è¿¹ä¸€è‡´æ€§ä»¥åŠæ˜¾è‘—æ€§é©±åŠ¨çš„å¾®è°ƒï¼ŒSCOREä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„å®‰å…¨ä¸”ç¨³å¥çš„æ¦‚å¿µåˆ é™¤è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12024v1">PDF</a> Camera ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—çš„å·¨å¤§æˆåŠŸï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å…¶åœ¨éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨æ€§æ–¹é¢æ—¥ç›Šå¢é•¿çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSCOREçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç¨³å¥çš„æ¦‚å¿µå»é™¤ã€‚SCOREå°†æ¦‚å¿µåˆ é™¤åˆ¶å®šä¸ºå¯¹æŠ—ç‹¬ç«‹æ€§é—®é¢˜ï¼Œä»ç†è®ºä¸Šä¿è¯äº†æ¨¡å‹è¾“å‡ºä¸åˆ é™¤æ¦‚å¿µä¹‹é—´çš„ç»Ÿè®¡ç‹¬ç«‹æ€§ã€‚ä¸å…ˆå‰çš„å¯å‘å¼æ–¹æ³•ä¸åŒï¼ŒSCOREæœ€å°åŒ–ç›®æ ‡æ¦‚å¿µä¸ç”Ÿæˆè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œå®ç°å¯è¯æ˜çš„æ¦‚å¿µåˆ é™¤ä¿è¯ã€‚æœ¬æ–‡æä¾›äº†å½¢å¼åŒ–è¯æ˜æ¥å»ºç«‹æ”¶æ•›æ€§è´¨å¹¶æ¨å¯¼å‡ºæ¦‚å¿µæ³„æ¼çš„å‰©ä½™ä¸Šé™ã€‚åœ¨Stable Diffusionå’ŒFLUXä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¯¹è±¡åˆ é™¤ã€NSFWç§»é™¤ã€åäººé¢éƒ¨æŠ‘åˆ¶å’Œè‰ºæœ¯é£æ ¼å»é™¤ç­‰å››ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSCOREå§‹ç»ˆä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬EraseAnythingã€ANTã€MACEã€ESDå’ŒUCEç­‰ï¼Œå®ç°äº†é«˜è¾¾12.5%çš„åˆ é™¤æ•ˆæœæå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“æˆ–æ›´é«˜çš„å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä¹Ÿå­˜åœ¨éšç§ã€å…¬å¹³æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„é£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSCOREçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç¨³å¥åœ°åˆ é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å®šæ¦‚å¿µã€‚</li>
<li>SCOREå°†æ¦‚å¿µåˆ é™¤é—®é¢˜è½¬åŒ–ä¸ºå¯¹æŠ—ç‹¬ç«‹æ€§é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºä¸åˆ é™¤æ¦‚å¿µä¹‹é—´çš„ç»Ÿè®¡ç‹¬ç«‹æ€§ã€‚</li>
<li>SCOREé€šè¿‡æœ€å°åŒ–ç›®æ ‡æ¦‚å¿µä¸ç”Ÿæˆè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯æ¥å®ç°å¯è¯æ˜çš„æ¦‚å¿µåˆ é™¤æ•ˆæœã€‚</li>
<li>æä¾›äº†å½¢å¼åŒ–è¯æ˜æ¥å±•ç¤ºè¯¥æ¡†æ¶çš„æ”¶æ•›æ€§è´¨å’Œæ¦‚å¿µæ³„æ¼çš„å‰©ä½™ä¸Šé™ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSCOREåœ¨æ¦‚å¿µåˆ é™¤æ•ˆæœä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.12024v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-to-Generate-4D-LiDAR-Sequences"><a href="#Learning-to-Generate-4D-LiDAR-Sequences" class="headerlink" title="Learning to Generate 4D LiDAR Sequences"></a>Learning to Generate 4D LiDAR Sequences</h2><p><strong>Authors:Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi</strong></p>
<p>While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation. </p>
<blockquote>
<p>è™½ç„¶ç”Ÿæˆå¼ä¸–ç•Œæ¨¡å‹å·²ç»æ¨åŠ¨äº†è§†é¢‘å’ŒåŸºäºå ç”¨ç‡çš„æ•°æ®åˆæˆçš„å‘å±•ï¼Œä½†æ¿€å…‰é›·è¾¾ç”Ÿæˆåœ¨å‡†ç¡®çš„3Dæ„ŸçŸ¥æ–¹é¢ä»ç„¶è¢«å¿½è§†ã€‚å°†ç”Ÿæˆæ‰©å±•åˆ°4Dæ¿€å…‰é›·è¾¾æ•°æ®å¸¦æ¥äº†å¯æ§æ€§ã€æ—¶é—´ç¨³å®šæ€§å’Œè¯„ä¼°æ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†LiDARCrafterï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è‡ªç”±å½¢å¼è¯­è¨€è½¬æ¢ä¸ºå¯ç¼–è¾‘æ¿€å…‰é›·è¾¾åºåˆ—çš„ç»Ÿä¸€æ¡†æ¶ã€‚æŒ‡ä»¤è¢«è§£æä¸ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åœºæ™¯å›¾ï¼Œä¸‰æ”¯æ‰©æ•£æ¨¡å‹å°†å…¶è½¬åŒ–ä¸ºå¯¹è±¡å¸ƒå±€ã€è½¨è¿¹å’Œå½¢çŠ¶ã€‚èŒƒå›´å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹æ‰«æï¼Œè‡ªå›å½’æ¨¡å—å°†å…¶æ‰©å±•ä¸ºæ—¶é—´è¿è´¯çš„åºåˆ—ã€‚æ˜ç¡®çš„å¸ƒå±€è®¾è®¡è¿›ä¸€æ­¥æ”¯æŒå¯¹è±¡çº§åˆ«çš„ç¼–è¾‘ï¼Œå¦‚æ’å…¥æˆ–é‡æ–°å®šä½ã€‚ä¸ºäº†è¿›è¡Œå…¬å¹³è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†EvalSuiteï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–åœºæ™¯ã€å¯¹è±¡å’Œåºåˆ—çº§åˆ«æŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨nuScenesä¸Šï¼ŒLiDARCrafterè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¿çœŸåº¦ã€å¯æ§æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œä¸ºæ¿€å…‰é›·è¾¾æ¨¡æ‹Ÿå’Œæ•°æ®å¢å¼ºæä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11959v1">PDF</a> Abstract Paper (Non-Archival) @ ICCV 2025 Wild3D Workshop; GitHub   Repo at <a target="_blank" rel="noopener" href="https://lidarcrafter.github.io/">https://lidarcrafter.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>LiDARCrafteræ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å°†è‡ªç”±å½¢å¼çš„è¯­è¨€è½¬æ¢ä¸ºå¯ç¼–è¾‘çš„LiDARåºåˆ—ã€‚å®ƒé€šè¿‡è§£ææŒ‡ä»¤ç”Ÿæˆego-centricåœºæ™¯å›¾ï¼Œå¹¶ç”±ä¸‰åˆ†æ”¯æ‰©æ•£æ¨¡å‹å°†æŒ‡ä»¤è½¬æ¢ä¸ºå¯¹è±¡å¸ƒå±€ã€è½¨è¿¹å’Œå½¢çŠ¶ã€‚èŒƒå›´å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹æ‰«æï¼Œè‡ªå›å½’æ¨¡å—å°†å…¶æ‰©å±•ä¸ºæ—¶é—´è¿è´¯çš„åºåˆ—ã€‚æ˜ç¡®çš„å¸ƒå±€è®¾è®¡è¿›ä¸€æ­¥æ”¯æŒå¯¹è±¡çº§åˆ«çš„ç¼–è¾‘ï¼Œå¦‚æ’å…¥æˆ–é‡æ–°å®šä½ã€‚EvalSuiteåŸºå‡†æµ‹è¯•æ¶µç›–äº†åœºæ™¯ã€å¯¹è±¡å’Œåºåˆ—çº§åˆ«çš„æŒ‡æ ‡ï¼Œä»¥ä¾¿å…¬å¹³è¯„ä¼°ã€‚åœ¨nuScenesä¸Šï¼ŒLiDARCrafterè¾¾åˆ°äº†å…ˆè¿›çš„é€¼çœŸåº¦ã€å¯æ§æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œä¸ºLiDARæ¨¡æ‹Ÿå’Œæ•°æ®å¢å¼ºå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LiDARæ•°æ®ç”Ÿæˆåœ¨ç”Ÿæˆä¸–ç•Œæ¨¡å‹ä¸­ä»ç„¶è¢«å¿½è§†ï¼Œå°½ç®¡å®ƒå¯¹å‡†ç¡®çš„3Dæ„ŸçŸ¥å¾ˆé‡è¦ã€‚</li>
<li>LiDARCrafteræ˜¯ä¸€ä¸ªèƒ½å°†è‡ªç”±å½¢å¼è¯­è¨€è½¬æ¢ä¸ºå¯ç¼–è¾‘LiDARåºåˆ—çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è§£ææŒ‡ä»¤ç”Ÿæˆego-centricåœºæ™¯å›¾ï¼Œå¹¶ç”±ä¸‰åˆ†æ”¯æ‰©æ•£æ¨¡å‹è½¬æ¢æ•°æ®ã€‚</li>
<li>èŒƒå›´å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹æ‰«æï¼Œè‡ªå›å½’æ¨¡å—åˆ™ç¡®ä¿æ—¶é—´çš„è¿è´¯æ€§ã€‚</li>
<li>LiDARCrafteræ”¯æŒå¯¹è±¡çº§åˆ«çš„ç¼–è¾‘ï¼Œå¦‚æ’å…¥æˆ–é‡æ–°å®šä½å¯¹è±¡ã€‚</li>
<li>EvalSuiteåŸºå‡†æµ‹è¯•ç”¨äºå…¬å¹³è¯„ä¼°LiDARæ•°æ®ç”Ÿæˆçš„åœºæ™¯ã€å¯¹è±¡å’Œåºåˆ—çº§åˆ«æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11959v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11959v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11959v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11959v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Do-It-Yourself-DIY-Modifying-Images-for-Poems-in-a-Zero-Shot-Setting-Using-Weighted-Prompt-Manipulation"><a href="#Do-It-Yourself-DIY-Modifying-Images-for-Poems-in-a-Zero-Shot-Setting-Using-Weighted-Prompt-Manipulation" class="headerlink" title="Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting   Using Weighted Prompt Manipulation"></a>Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting   Using Weighted Prompt Manipulation</h2><p><strong>Authors:Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, K J Joseph</strong></p>
<p>Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language. </p>
<blockquote>
<p>è¯—æ­Œæ˜¯ä¸€ç§è¡¨è¾¾æ€§è‰ºæœ¯å½¢å¼ï¼Œé‚€è¯·è¯»è€…è¿›è¡Œå¤šé‡è§£è¯»ï¼Œå› ä¸ºè¯»è€…ç»å¸¸ä¼šå°†ä»–ä»¬è‡ªå·±çš„æƒ…æ„Ÿã€ç»å†å’Œæ–‡åŒ–èƒŒæ™¯å¸¦å…¥å¯¹è¯—æ­Œçš„ç†è§£ä¸­ã€‚æˆ‘ä»¬è®¤è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œæ—¨åœ¨ä¸ºè¯—æ­Œç”Ÿæˆå›¾åƒï¼Œå¹¶åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸­æ”¹è¿›è¿™äº›å›¾åƒï¼Œä½¿è§‚ä¼—èƒ½å¤Ÿæ ¹æ®ä»–ä»¬çš„éœ€æ±‚ä¿®æ”¹å›¾åƒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŠ æƒæç¤ºæ“çºµï¼ˆWPMï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç³»ç»Ÿåœ°ä¿®æ”¹äº†æ‰©æ•£æ¨¡å‹ä¸­æ³¨æ„åŠ›æƒé‡å’Œæ–‡æœ¬åµŒå…¥ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´ç‰¹å®šè¯è¯­çš„é‡è¦æ€§ï¼ŒWPMå¯ä»¥å¢å¼ºæˆ–æŠ‘åˆ¶å®ƒä»¬åœ¨æœ€ç»ˆç”Ÿæˆå›¾åƒä¸­çš„å½±å“ï¼Œä»è€Œå¾—åˆ°è¯­ä¹‰æ›´ä¸°å¯Œã€ä¸Šä¸‹æ–‡æ›´å‡†ç¡®çš„å¯è§†åŒ–å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTï¼‰ï¼Œç»“åˆç°æœ‰çš„è¯—æ­Œæ•°æ®é›†ï¼Œç¡®ä¿åœ¨æ–‡å­¦é¢†åŸŸæ”¹è¿›å›¾åƒç”Ÿæˆçš„å…¨é¢å’Œç»“æ„åŒ–çš„æ–¹æ³•ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å°†åŠ æƒæç¤ºæ“çºµæ•´åˆåˆ°è¯—æ­Œè¯­è¨€çš„å›¾åƒå¢å¼ºä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥åŠ æƒæç¤ºæ“æ§ï¼ˆWPMï¼‰æŠ€æœ¯ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¸ºè¯—æ­Œç”Ÿæˆå›¾åƒå¹¶è¿›è¡Œæ”¹è¿›ï¼Œä½¿è§‚ä¼—èƒ½å¤Ÿæ ¹æ®éœ€æ±‚ä¿®æ”¹å›¾åƒã€‚WPMæŠ€æœ¯é€šè¿‡åŠ¨æ€è°ƒæ•´æ‰©æ•£æ¨¡å‹ä¸­ç‰¹å®šè¯è¯­çš„æ³¨æ„åŠ›æƒé‡å’Œæ–‡æœ¬åµŒå…¥ï¼Œå¢å¼ºæˆ–æŠ‘åˆ¶å®ƒä»¬å¯¹æœ€ç»ˆç”Ÿæˆå›¾åƒçš„å½±å“ï¼Œä»è€Œå®ç°è¯­ä¹‰æ›´ä¸°å¯Œã€ä¸Šä¸‹æ–‡æ›´å‡†ç¡®çš„å¯è§†åŒ–ã€‚è¯¥æ–¹æ³•ç»“åˆæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPTå’Œç°æœ‰è¯—æ­Œæ•°æ®é›†ï¼Œç¡®ä¿åœ¨æ–‡å­¦é¢†åŸŸå®ç°æ›´ä¼˜è´¨å›¾åƒç”Ÿæˆçš„ç»¼åˆç»“æ„åŒ–æ–¹æ³•ã€‚æ­¤ä¸ºé¦–æ¬¡å°è¯•å°†åŠ æƒæç¤ºæ“æ§æŠ€æœ¯åº”ç”¨äºå¢å¼ºè¯—æ­Œè¯­è¨€çš„å›¾åƒè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯—æ­Œæ˜¯ä¸€ç§é‚€è¯·å¤šé‡è§£è¯»çš„è¡¨è¾¾è‰ºæœ¯å½¢å¼ï¼Œè¯»è€…å¸¸å¸¦å…¥è‡ªèº«æƒ…æ„Ÿã€ç»å†å’Œæ–‡åŒ–èƒŒæ™¯ç†è§£è¯—æ­Œã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„Weighted Prompt Manipulation (WPM)æŠ€æœ¯ï¼Œèƒ½åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ä¸ºè¯—æ­Œç”Ÿæˆå¹¶æ”¹è¿›å›¾åƒã€‚</li>
<li>WPMæŠ€æœ¯é€šè¿‡ç³»ç»Ÿåœ°è°ƒæ•´æ‰©æ•£æ¨¡å‹ä¸­ç‰¹å®šè¯è¯­çš„æ³¨æ„åŠ›æƒé‡å’Œæ–‡æœ¬åµŒå…¥ï¼Œå®ç°å›¾åƒç”Ÿæˆçš„ç²¾ç»†åŒ–æ§åˆ¶ã€‚</li>
<li>WPMèƒ½å¤Ÿå¢å¼ºæˆ–æŠ‘åˆ¶è¯è¯­å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“ï¼Œç”Ÿæˆè¯­ä¹‰æ›´ä¸°å¯Œã€ä¸Šä¸‹æ–‡æ›´å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚GPTï¼Œç¡®ä¿æ–‡å­¦é¢†åŸŸå›¾åƒç”Ÿæˆçš„å…¨é¢å’Œç»“æ„åŒ–ã€‚</li>
<li>æ­¤ä¸ºé¦–æ¬¡å°è¯•å°†åŠ æƒæç¤ºæ“æ§æŠ€æœ¯åº”ç”¨äºè¯—æ­Œè¯­è¨€çš„å›¾åƒè¡¨ç°å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11878v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11878v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11878v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11878v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11878v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DRAG-Data-Reconstruction-Attack-using-Guided-Diffusion"><a href="#DRAG-Data-Reconstruction-Attack-using-Guided-Diffusion" class="headerlink" title="DRAG: Data Reconstruction Attack using Guided Diffusion"></a>DRAG: Data Reconstruction Attack using Guided Diffusion</h2><p><strong>Authors:Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen</strong></p>
<p>With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDMâ€™s learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ntuaislab/DRAG">https://github.com/ntuaislab/DRAG</a>. </p>
<blockquote>
<p>éšç€å¤§å‹åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œåˆ†å‰²æ¨ç†ï¼ˆSIï¼‰å·²æˆä¸ºåœ¨è½»é‡çº§è¾¹ç¼˜è®¾å¤‡å’Œäº‘æœåŠ¡å™¨ä¹‹é—´éƒ¨ç½²æ¨¡å‹çš„æµè¡Œè®¡ç®—èŒƒå¼ï¼Œè§£å†³äº†æ•°æ®éšç§å’Œè®¡ç®—æˆæœ¬æ–¹é¢çš„æ‹…å¿§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ•°æ®é‡å»ºæ”»å‡»ä¸»è¦é›†ä¸­åœ¨è¾ƒå°çš„CNNåˆ†ç±»æ¨¡å‹ä¸Šï¼Œè€ŒåŸºç¡€æ¨¡å‹åœ¨åˆ†å‰²æ¨ç†è®¾ç½®ä¸­çš„éšç§é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼•å¯¼æ‰©æ•£çš„æ–°å‹æ•°æ®é‡å»ºæ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åµŒå…¥åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ï¼Œè¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹å­¦ä¹ åˆ°çš„å›¾åƒå…ˆéªŒè¿›è¡Œè¿­ä»£é‡å»ºï¼Œæœ‰æ•ˆåœ°ä»å…¶ä¸­é—´è¡¨ç¤ºç”Ÿæˆä¸åŸå§‹æ•°æ®ç›¸ä¼¼çš„é«˜ä¿çœŸå›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ·±å±‚ä¸­é—´è¡¨ç¤ºæ•°æ®æ–¹é¢ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§è¿˜æ˜¯å®šé‡ä¸Šï¼Œéƒ½æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç»“æœå¼ºè°ƒäº†åˆ†å‰²æ¨ç†åœºæ™¯ä¸­å¤§å‹æ¨¡å‹å¯¹æ›´ç¨³å¥çš„éšç§ä¿æŠ¤æœºåˆ¶çš„è¿«åˆ‡éœ€æ±‚ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/ntuaislab/DRAG">https://github.com/ntuaislab/DRAG</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11724v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>     éšç€å¤§å‹åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œåˆ†å‰²æ¨ç†ï¼ˆSIï¼‰å·²æˆä¸ºéƒ¨ç½²æ¨¡å‹äºè½»é‡çº§è¾¹ç¼˜è®¾å¤‡å’Œäº‘æœåŠ¡å™¨ä¹‹é—´çš„çƒ­é—¨è®¡ç®—èŒƒå¼ï¼Œè§£å†³äº†æ•°æ®éšç§å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é‡å»ºæ”»å‡»ä¸»è¦å…³æ³¨è¾ƒå°çš„CNNåˆ†ç±»æ¨¡å‹ï¼Œå¯¹äºåˆ†å‰²æ¨ç†è®¾ç½®ä¸­åŸºç¡€æ¨¡å‹çš„æ•°æ®éšç§é—®é¢˜æ¢ç´¢ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼•å¯¼æ‰©æ•£çš„æ–°å‹æ•°æ®é‡å»ºæ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹LDMå­¦ä¹ çš„å›¾åƒå…ˆéªŒè¿›è¡Œè¿­ä»£é‡å»ºï¼Œæœ‰æ•ˆåœ°ä»å…¶ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰ç”Ÿæˆé«˜ä¿çœŸåº¦å›¾åƒï¼Œè¿™äº›å›¾åƒç±»ä¼¼äºåŸå§‹æ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ·±å±‚IRsä¸­çš„æ•°æ®æ—¶ï¼Œæ— è®ºåœ¨å®šæ€§è¿˜æ˜¯å®šé‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ã€‚è¿™å‡¸æ˜¾äº†åœ¨åˆ†å‰²æ¨ç†åœºæ™¯ä¸­ä¸ºå¤§æ¨¡å‹æä¾›æ›´ç¨³å¥çš„éšç§ä¿æŠ¤æœºåˆ¶çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†å‰²æ¨ç†ï¼ˆSIï¼‰å·²æˆä¸ºéƒ¨ç½²å¤§å‹æ¨¡å‹çš„ä¸€ç§æµè¡Œè®¡ç®—èŒƒå¼ï¼Œè§£å†³äº†æ•°æ®éšç§å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ•°æ®é‡å»ºæ”»å‡»ä¸»è¦å…³æ³¨è¾ƒå°çš„CNNæ¨¡å‹ï¼ŒåŸºç¡€æ¨¡å‹åœ¨åˆ†å‰²æ¨ç†è®¾ç½®ä¸­çš„éšç§é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®é‡å»ºæ”»å‡»æ–¹æ³•ï¼ŒåŸºäºå¼•å¯¼æ‰©æ•£å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æ–¹æ³•é€šè¿‡è¿­ä»£é‡å»ºç”Ÿæˆé«˜ä¿çœŸåº¦å›¾åƒï¼Œè¿™äº›å›¾åƒä»åŸºç¡€æ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰ç”Ÿæˆï¼Œç±»ä¼¼äºåŸå§‹æ•°æ®ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œæ–°å‹æ”»å‡»æ–¹æ³•åœ¨é‡å»ºæ·±å±‚IRsä¸­çš„æ•°æ®æ—¶è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>ç»“æœå‡¸æ˜¾äº†ä¸ºå¤§å‹æ¨¡å‹åœ¨åˆ†å‰²æ¨ç†åœºæ™¯ä¸­æä¾›æ›´å¼ºéšç§ä¿æŠ¤æœºåˆ¶çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11724v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11724v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11724v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11724v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11724v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SpeCa-Accelerating-Diffusion-Transformers-with-Speculative-Feature-Caching"><a href="#SpeCa-Accelerating-Diffusion-Transformers-with-Speculative-Feature-Caching" class="headerlink" title="SpeCa: Accelerating Diffusion Transformers with Speculative Feature   Caching"></a>SpeCa: Accelerating Diffusion Transformers with Speculative Feature   Caching</h2><p><strong>Authors:Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, Linfeng Zhang</strong></p>
<p>Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel â€˜Forecast-then-verifyâ€™ acceleration framework that effectively addresses both limitations. SpeCaâ€™s core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/Cache4Diffusion%7D">https://github.com/Shenyi-Z/Cache4Diffusion}</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆçš„æ–¹å¼ï¼Œä½†å®ƒä»¬å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ä»ç„¶å¯¹å®æ—¶åº”ç”¨æ„æˆäº†å·¨å¤§æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹é¢ä¸´ä¸¤å¤§æ ¹æœ¬æŒ‘æˆ˜ï¼šä¸¥æ ¼çš„æ—¶åºä¾èµ–æ€§é˜»æ­¢äº†å¹¶è¡ŒåŒ–ï¼Œä»¥åŠåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­éƒ½éœ€è¦è¿›è¡Œå¯†é›†çš„è®¡ç®—å‰å‘ä¼ é€’ã€‚ä»å¤§å‹è¯­è¨€æ¨¡å‹çš„æŠ•æœºè§£ç ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†SpeCaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„â€œé¢„æµ‹-éªŒè¯â€åŠ é€Ÿæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è¿™ä¸¤ä¸ªé™åˆ¶ã€‚SpeCaçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥æŠ•æœºé‡‡æ ·ï¼ŒåŸºäºå®Œå…¨è®¡ç®—å¾—åˆ°çš„å‚è€ƒæ—¶é—´æ­¥é•¿æ¥é¢„æµ‹åç»­æ—¶é—´æ­¥é•¿çš„ä¸­é—´ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸€ç§æ— å‚æ•°éªŒè¯æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°è¯„ä¼°é¢„æµ‹å¯é æ€§ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶å†³ç­–æ—¶æ¥å—æˆ–æ‹’ç»æ¯ä¸ªé¢„æµ‹ï¼ŒåŒæ—¶äº§ç”Ÿå¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼ŒSpeCaå¼•å…¥äº†æ ·æœ¬è‡ªé€‚åº”è®¡ç®—åˆ†é…ï¼Œæ ¹æ®ç”Ÿæˆå¤æ‚åº¦åŠ¨æ€è°ƒæ•´èµ„æºï¼Œä¸ºç®€å•æ ·æœ¬åˆ†é…å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿ç•™å¯¹å¤æ‚å®ä¾‹çš„å¯†é›†å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨FLUXä¸Šå®ç°äº†6.34å€çš„åŠ é€Ÿï¼Œè´¨é‡ç•¥æœ‰ä¸‹é™ï¼ˆä¸‹é™5.5%ï¼‰ï¼Œåœ¨DiTä¸Šå®ç°äº†7.3å€çš„åŠ é€ŸåŒæ—¶ä¿æŒç”Ÿæˆä¿çœŸåº¦ï¼Œåœ¨HunyuanVideoä¸Šå®ç°äº†6.1å€åŠ é€Ÿï¼ŒVBenchå¾—åˆ†ä¸º79.84%ã€‚éªŒè¯æœºåˆ¶äº§ç”Ÿçš„å¼€é”€æå°ï¼ˆä»…å å…¨æ¨ç†æˆæœ¬çš„1.67%-3.5%ï¼‰ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¨ç†å»ºç«‹äº†æ–°èŒƒå¼ï¼Œå³ä½¿åœ¨æç«¯çš„åŠ é€Ÿæ¯”ä¸‹ä¹Ÿèƒ½ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒï¼š\textbf{<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/Cache4Diffusion%7D%E3%80%82">https://github.com/Shenyi-Z/Cache4Diffusion}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11628v1">PDF</a> 15 pages, 9 figures, ACM Multimedia 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆï¼Œä½†å…¶è®¡ç®—éœ€æ±‚ä»ç„¶å¯¹å®æ—¶åº”ç”¨é€ æˆé˜»ç¢ã€‚æœ¬æ–‡æå‡ºSpeCaæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥â€œé¢„æµ‹-éªŒè¯â€æœºåˆ¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚SpeCaé‡‡ç”¨å‰ç»æ€§é‡‡æ ·å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶åŸºäºå®Œå…¨è®¡ç®—çš„å‚è€ƒæ—¶é—´ç‚¹è¯„ä¼°é¢„æµ‹å¯é æ€§ã€‚æ­¤å¤–ï¼ŒSpeCaè¿˜å¼•å…¥äº†æ ·æœ¬è‡ªé€‚åº”è®¡ç®—åˆ†é…ï¼Œå¯æ ¹æ®ç”Ÿæˆå¤æ‚åº¦åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…ã€‚å®éªŒè¡¨æ˜ï¼ŒSpeCaåœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œä¸é€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚</li>
<li>SpeCaæ¡†æ¶é€šè¿‡å¼•å…¥â€œé¢„æµ‹-éªŒè¯â€æœºåˆ¶è§£å†³äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šä¸¥æ ¼çš„æ—¶é—´ä¾èµ–æ€§å’Œè®¡ç®—å¯†é›†å‹çš„æ­£å‘ä¼ é€’ã€‚</li>
<li>SpeCaé‡‡ç”¨å‰ç»æ€§é‡‡æ ·ï¼ŒåŸºäºå®Œå…¨è®¡ç®—çš„å‚è€ƒæ—¶é—´ç‚¹é¢„æµ‹åç»­æ—¶é—´æ­¥é•¿çš„ä¸­é—´ç‰¹å¾ã€‚</li>
<li>SpeCaå®ç°äº†å‚æ•°åŒ–çš„éªŒè¯æœºåˆ¶ï¼Œæœ‰æ•ˆè¯„ä¼°é¢„æµ‹å¯é æ€§ï¼Œæ”¯æŒå®æ—¶å†³ç­–ã€‚</li>
<li>SpeCaå¼•å…¥æ ·æœ¬è‡ªé€‚åº”è®¡ç®—åˆ†é…ï¼Œæ ¹æ®ç”Ÿæˆå¤æ‚åº¦åŠ¨æ€è°ƒæ•´èµ„æºï¼Œå®ç°æ›´é«˜æ•ˆæ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSpeCaåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.11628v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ToMA-Token-Merge-with-Attention-for-Image-Generation-with-Diffusion-Models"><a href="#ToMA-Token-Merge-with-Attention-for-Image-Generation-with-Diffusion-Models" class="headerlink" title="ToMA: Token Merge with Attention for Image Generation with Diffusion   Models"></a>ToMA: Token Merge with Attention for Image Generation with Diffusion   Models</h2><p><strong>Authors:Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang</strong></p>
<p>Diffusion models excel in high-fidelity image generation but face scalability limits due to transformersâ€™ quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired with optimized attention implementations (e.g., FlashAttention). To bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf method that redesigns token reduction for GPU-aligned efficiency, with three key contributions: 1) a reformulation of token merge as a submodular optimization problem to select diverse tokens; 2) merge&#x2F;unmerge as an attention-like linear transformation via GPU-friendly matrix operations; and 3) exploiting latent locality and sequential redundancy (pattern reuse) to minimize overhead. ToMA reduces SDXL&#x2F;Flux generation latency by 24%&#x2F;23%, respectively (with DINO $\Delta &lt; 0.07$), outperforming prior methods. This work bridges the gap between theoretical and practical efficiency for transformers in diffusion. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå˜å‹å™¨çš„äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚æ€§è€Œé¢ä¸´å¯æ‰©å±•æ€§é™åˆ¶ã€‚åƒToMeSDå’ŒToFuè¿™æ ·çš„å³æ’å³ç”¨ä»¤ç‰Œå‡å°‘æ–¹æ³•é€šè¿‡åˆå¹¶ç”Ÿæˆå›¾åƒä¸­çš„å†—ä½™ä»¤ç‰Œæ¥å‡å°‘æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œä½†å®ƒä»¬ä¾èµ–äºGPUä½æ•ˆæ“ä½œï¼ˆä¾‹å¦‚æ’åºã€åˆ†æ•£å†™å…¥ï¼‰ï¼Œå¼•å…¥äº†ä¸ä¼˜åŒ–çš„æ³¨æ„åŠ›å®ç°é…å¯¹æ—¶çš„å¼€é”€ï¼ŒæŠµæ¶ˆäº†ç†è®ºä¸Šçš„é€Ÿåº¦æå‡ï¼ˆä¾‹å¦‚FlashAttentionï¼‰ã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æ³¨æ„åŠ›çš„ä»¤ç‰Œåˆå¹¶ï¼ˆToMAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç°æˆçš„GPUå¯¹é½æ•ˆç‡æ–¹æ³•é‡æ–°è®¾è®¡äº†ä»¤ç‰Œç¼©å‡ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šé¦–å…ˆï¼Œå°†ä»¤ç‰Œåˆå¹¶é‡æ–°è¡¨è¿°ä¸ºå­æ¨¡å—ä¼˜åŒ–é—®é¢˜ï¼Œä»¥é€‰æ‹©å¤šæ ·åŒ–çš„ä»¤ç‰Œï¼›å…¶æ¬¡ï¼Œå°†åˆå¹¶&#x2F;å–æ¶ˆåˆå¹¶ä½œä¸ºç±»ä¼¼æ³¨æ„åŠ›çš„çº¿æ€§å˜æ¢ï¼Œé€šè¿‡GPUå‹å¥½çš„çŸ©é˜µæ“ä½œå®ç°ï¼›æœ€åï¼Œåˆ©ç”¨æ½œåœ¨å±€éƒ¨æ€§å’Œé¡ºåºå†—ä½™ï¼ˆæ¨¡å¼é‡ç”¨ï¼‰æ¥æœ€å°åŒ–å¼€é”€ã€‚ToMAåˆ†åˆ«å°†SDXL&#x2F;Fluxç”Ÿæˆå»¶è¿Ÿæ—¶é—´å‡å°‘äº†24%&#x2F;23%ï¼ˆä¸DINO $\Delta &lt; 0.07$ï¼‰ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†æ‰©æ•£æ¨¡å‹ä¸­å˜å‹å™¨åœ¨ç†è®ºå’Œå®è·µæ•ˆç‡ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10918v1">PDF</a> In proceedings of the 42nd International Conference on Machine   Learning (ICML 2025). Code available at <a target="_blank" rel="noopener" href="https://github.com/wenboluu/ToMA">https://github.com/wenboluu/ToMA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆä¸­çš„ä¼˜åŠ¿åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³å˜å‹å™¨çš„äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚æ€§å¯¼è‡´çš„å¯æ‰©å±•æ€§é™åˆ¶ã€‚æ–‡ç« ä»‹ç»äº†å³æ’å³ç”¨å‹ä»¤ç‰Œå‡å°‘æ–¹æ³•ï¼Œå¦‚ToMeSDå’ŒToFuï¼Œå®ƒä»¬é€šè¿‡åˆå¹¶ç”Ÿæˆå›¾åƒä¸­çš„å†—ä½™ä»¤ç‰Œæ¥å‡å°‘FLOPsã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºGPUæ•ˆç‡ä¸é«˜çš„æ“ä½œï¼Œå¦‚æ’åºå’Œåˆ†æ•£å†™å…¥ï¼Œå¼•å…¥äº†æŠµæ¶ˆä¸ä¼˜åŒ–æ³¨æ„åŠ›å®ç°ï¼ˆå¦‚FlashAttentionï¼‰é…å¯¹æ—¶çš„ç†è®ºåŠ é€Ÿçš„å¼€é”€ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†Token Merge with Attentionï¼ˆToMAï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®è´¡çŒ®é‡æ–°è®¾è®¡äº†ä»¤ç‰Œå‡å°‘ä»¥æé«˜GPUå¯¹é½æ•ˆç‡ï¼š1ï¼‰å°†ä»¤ç‰Œåˆå¹¶é‡æ–°è¡¨è¿°ä¸ºå­æ¨¡å—ä¼˜åŒ–é—®é¢˜ä»¥é€‰æ‹©å¤šæ ·åŒ–çš„ä»¤ç‰Œï¼›2ï¼‰é€šè¿‡GPUå‹å¥½çš„çŸ©é˜µæ“ä½œï¼Œå°†åˆå¹¶&#x2F;è§£é™¤åˆå¹¶ä¸ºç±»ä¼¼æ³¨æ„åŠ›çš„çº¿æ€§è½¬æ¢ï¼›3ï¼‰åˆ©ç”¨æ½œåœ¨å±€éƒ¨æ€§å’Œé¡ºåºå†—ä½™ï¼ˆæ¨¡å¼é‡ç”¨ï¼‰æ¥æœ€å°åŒ–å¼€é”€ã€‚ToMAå°†SDXL&#x2F;Fluxç”Ÿæˆå»¶è¿Ÿå‡å°‘äº†24%&#x2F;23%ï¼ˆä¸DINOçš„Î”&lt;0.07ç›¸æ¯”ï¼‰ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæ‹‰è¿‘äº†å˜å‹å™¨åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç†è®ºæ•ˆç‡å’Œå®é™…æ•ˆç‡ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå˜å‹å™¨çš„äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ToMeSDå’ŒToFuè™½ç„¶èƒ½å‡å°‘ä»¤ç‰Œæ•°é‡ï¼Œä½†å­˜åœ¨GPUæ•ˆç‡ä¸é«˜çš„é—®é¢˜ï¼Œå¼•å…¥å¼€é”€ï¼ŒæŠµæ¶ˆäº†ç†è®ºåŠ é€Ÿä¼˜åŠ¿ã€‚</li>
<li>ToMAæ–¹æ³•é€šè¿‡ä¸‰é¡¹å…³é”®è´¡çŒ®é‡æ–°è®¾è®¡ä»¤ç‰Œå‡å°‘ç­–ç•¥ä»¥æé«˜GPUæ•ˆç‡ï¼šä»¤ç‰Œåˆå¹¶ä½œä¸ºå­æ¨¡å—ä¼˜åŒ–é—®é¢˜ï¼Œåˆå¹¶&#x2F;è§£é™¤åˆå¹¶ä½œä¸ºç±»ä¼¼æ³¨æ„åŠ›çš„çº¿æ€§è½¬æ¢ï¼Œä»¥åŠåˆ©ç”¨æ½œåœ¨å±€éƒ¨æ€§å’Œé¡ºåºå†—ä½™ã€‚</li>
<li>ToMAæ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘SDXLå’ŒFluxçš„ç”Ÿæˆå»¶è¿Ÿï¼Œä¸”å¯¹å›¾åƒè´¨é‡å½±å“è¾ƒå°ï¼ˆä¸DINOçš„Î”&lt;0.07ç›¸æ¯”ï¼‰ã€‚</li>
<li>ToMAæ–¹æ³•æ‹‰è¿›äº†æ‰©æ•£æ¨¡å‹ä¸­ç†è®ºæ•ˆç‡å’Œå®é™…æ•ˆç‡ä¹‹é—´çš„å·®è·ã€‚</li>
<li>ToMAæ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.10918v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.10918v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.10918v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.10918v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="STADI-Fine-Grained-Step-Patch-Diffusion-Parallelism-for-Heterogeneous-GPUs"><a href="#STADI-Fine-Grained-Step-Patch-Diffusion-Parallelism-for-Heterogeneous-GPUs" class="headerlink" title="STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous   GPUs"></a>STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous   GPUs</h2><p><strong>Authors:Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen</strong></p>
<p>The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADIâ€™s efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆç­‰åº”ç”¨ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œéœ€è¦é«˜æ•ˆçš„å¹¶è¡Œæ¨ç†æŠ€æœ¯æ¥ç®¡ç†å…¶å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£å¹¶è¡Œæ¨ç†æ–¹æ¡ˆåœ¨å¼‚æ„å¤šGPUç¯å¢ƒä¸­å¾€å¾€ä¸èƒ½å……åˆ†åˆ©ç”¨èµ„æºï¼Œå…¶ä¸­ç¡¬ä»¶èƒ½åŠ›çš„å·®å¼‚æˆ–åå°ä»»åŠ¡å¯¼è‡´å·¥ä½œé‡ä¸å¹³è¡¡ã€‚æœ¬æ–‡ä»‹ç»äº†æ—¶ç©ºè‡ªé€‚åº”æ‰©æ•£æ¨ç†ï¼ˆSTADIï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ é€Ÿæ­¤ç±»ç¯å¢ƒä¸­æ‰©æ•£æ¨¡å‹æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ··åˆè°ƒåº¦ç¨‹åºï¼Œå®ƒåœ¨æ—¶é—´å’Œç©ºé—´ç»´åº¦ä¸Šåè°ƒç²¾ç»†ç²’åº¦çš„å¹¶è¡Œæ€§ã€‚åœ¨æ—¶é—´ä¸Šï¼ŒSTADIåœ¨é¢„çƒ­é˜¶æ®µåå¼•å…¥äº†ä¸€ç§æ–°å‹çš„è®¡ç®—æ„ŸçŸ¥æ­¥éª¤åˆ†é…å™¨ï¼Œä½¿ç”¨æœ€å°å…¬å€æ•°æœ€å°åŒ–é‡åŒ–æŠ€æœ¯æ¥å‡å°‘è¾ƒæ…¢GPUä¸Šçš„é™å™ªæ­¥éª¤å’Œæ‰§è¡ŒåŒæ­¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘GPUç©ºé—²æ—¶é—´ï¼ŒSTADIæ‰§è¡Œå¼¹æ€§è¡¥ä¸å¹¶è¡Œæœºåˆ¶ï¼Œæ ¹æ®GPUçš„è®¡ç®—èƒ½åŠ›åˆ†é…ä¸åŒå¤§å°çš„å›¾åƒè¡¥ä¸ï¼Œé€šè¿‡äº’è¡¥çš„ç©ºé—´æœºåˆ¶ç¡®ä¿å·¥ä½œé‡å¹³è¡¡åˆ†å¸ƒã€‚åœ¨è´Ÿè½½ä¸å¹³è¡¡å’Œå¼‚æ„å¤šGPUé›†ç¾¤ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†STADIçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå…¶è´Ÿè½½å‡è¡¡æ€§èƒ½æå‡ï¼Œå¹¶ç¼“è§£äº†æ€§èƒ½ç“¶é¢ˆã€‚ä¸è¡¥ä¸å¹¶è¡Œæ€§ç›¸æ¯”ï¼Œä½œä¸ºä¸€ç§å…ˆè¿›çš„æ‰©æ•£æ¨ç†æ¡†æ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿæœ€å¤šå‡å°‘äº†45%ï¼Œå¹¶åœ¨å¼‚æ„GPUä¸Šæ˜¾è‘—æé«˜äº†èµ„æºåˆ©ç”¨ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04719v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨å›¾åƒç”Ÿæˆç­‰åº”ç”¨ä¸­ï¼Œæ‰©æ•£æ¨¡å‹çš„æ—¥ç›Šå¢é•¿çš„ä½¿ç”¨å¯¼è‡´çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¸ºäº†åŠ é€Ÿåœ¨è¿™ç§ç¯å¢ƒä¸­çš„æ‰©æ•£æ¨¡å‹æ¨ç†ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSpatio-Temporal Adaptive Diffusion Inference (STADI)çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ··åˆè°ƒåº¦ç¨‹åºåœ¨æ—¶é—´å’Œç©ºé—´ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°ç²¾ç»†çš„å¹¶è¡Œæ€§ï¼Œä»¥æé«˜èµ„æºåˆ©ç”¨ç‡å’Œæ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆç­‰åº”ç”¨ä¸­çš„ä½¿ç”¨æ­£åœ¨å¢åŠ ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç°æœ‰æ‰©æ•£å¹¶è¡Œæ¨ç†æ–¹æ¡ˆåœ¨å¼‚æ„å¤šGPUç¯å¢ƒä¸­å¾€å¾€èµ„æºåˆ©ç”¨ä¸è¶³ã€‚</li>
<li>Spatio-Temporal Adaptive Diffusion Inference (STADI)æ¡†æ¶è¢«å¼•å…¥ä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>STADIçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ··åˆè°ƒåº¦ç¨‹åºï¼Œå®ƒåœ¨æ—¶é—´å’Œç©ºé—´ä¸¤ä¸ªç»´åº¦ä¸Šå®ç°ç²¾ç»†çš„å¹¶è¡Œæ€§ã€‚</li>
<li>STADIä½¿ç”¨è®¡ç®—æ„ŸçŸ¥çš„æ­¥éª¤åˆ†é…å™¨ï¼Œåœ¨é¢„çƒ­é˜¶æ®µåå‡å°‘é™å™ªæ­¥éª¤å’Œæ‰§è¡ŒåŒæ­¥ï¼Œä»¥å‡å°‘GPUç©ºé—²æ—¶é—´ã€‚</li>
<li>é€šè¿‡å¼¹æ€§è¡¥ä¸å¹¶è¡Œæœºåˆ¶ï¼ŒSTADIèƒ½å¤Ÿæ ¹æ®GPUçš„è®¡ç®—èƒ½åŠ›åˆ†é…ä¸åŒå¤§å°çš„å›¾åƒè¡¥ä¸ï¼Œç¡®ä¿å·¥ä½œè´Ÿè½½çš„å¹³è¡¡åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04719v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MEPG-Multi-Expert-Planning-and-Generation-for-Compositionally-Rich-Image-Generation"><a href="#MEPG-Multi-Expert-Planning-and-Generation-for-Compositionally-Rich-Image-Generation" class="headerlink" title="MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation"></a>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation</h2><p><strong>Authors:Yuan Zhao, Lin Liu</strong></p>
<p>Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„å›¾åƒè´¨é‡ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´å¤æ‚ã€å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šä¸“å®¶è§„åˆ’å’Œç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰ï¼Œè¯¥æ¡†æ¶ååŒæ•´åˆäº†ä½ç½®æ„ŸçŸ¥å’Œé£æ ¼æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½ç½®é£æ ¼æ„ŸçŸ¥ï¼ˆPSAï¼‰æ¨¡å—ï¼Œå®ƒåˆ©ç”¨ç›‘ç£å¾®è°ƒLLMå°†è¾“å…¥æç¤ºåˆ†è§£æˆç²¾ç¡®çš„ç©ºé—´åæ ‡å’Œé£æ ¼ç¼–ç è¯­ä¹‰æŒ‡ä»¤ï¼›ï¼ˆ2ï¼‰å¤šä¸“å®¶æ‰©æ•£ï¼ˆMEDï¼‰æ¨¡å—ï¼Œé€šè¿‡æœ¬åœ°åŒºåŸŸå’Œå…¨å±€åŒºåŸŸçš„åŠ¨æ€ä¸“å®¶è·¯ç”±å®ç°è·¨åŒºåŸŸç”Ÿæˆã€‚åœ¨æ¯ä¸ªå±€éƒ¨åŒºåŸŸçš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„é—¨æ§æœºåˆ¶ï¼Œé’ˆå¯¹æ¯ä¸ªç©ºé—´åˆ†åŒºé€‰æ‹©ä¸“ä¸šæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œç°å®ä¸»ä¹‰ä¸“å®¶ã€é£æ ¼åŒ–ä¸“å®¶ï¼‰ã€‚è¯¥æ¶æ„æ”¯æŒä¸“å®¶æ¨¡å‹çš„è½»æ¾é›†æˆå’Œæ›¿æ¢ï¼Œå…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œäº¤äº’å¼ç•Œé¢å…è®¸å®æ—¶ç©ºé—´å¸ƒå±€ç¼–è¾‘å’Œä»ä¸“å®¶ç»„åˆä¸­é€‰æ‹©æ¯ä¸ªåŒºåŸŸçš„é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒMEPGåœ¨å›¾åƒè´¨é‡å’Œé£æ ¼å¤šæ ·æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºå…·æœ‰ç›¸åŒèƒŒæ™¯çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04126v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å·²å®ç°äº†æ˜¾è‘—çš„å›¾åƒè´¨é‡ï¼Œä½†ä»é¢ä¸´å¤æ‚å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§å—é™çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†å¤šä¸“å®¶è§„åˆ’ç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰ï¼Œè¯¥æ¡†æ¶ååŒé›†æˆäº†ä½ç½®æ„ŸçŸ¥å’Œé£æ ¼æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼ŒMEPGåœ¨å›¾åƒè´¨é‡å’Œé£æ ¼å¤šæ ·æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è™½å·²å®ç°è‰¯å¥½å›¾åƒè´¨é‡ï¼Œä½†ä»é¢ä¸´å¤„ç†å¤æ‚å¤šå…ƒç´ æç¤ºå’Œé£æ ¼å¤šæ ·æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šä¸“å®¶è§„åˆ’ç”Ÿæˆæ¡†æ¶ï¼ˆMEPGï¼‰ï¼Œé›†æˆä½ç½®æ„ŸçŸ¥å’Œé£æ ¼æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç©ºé—´è¯­ä¹‰ä¸“å®¶æ¨¡å—ä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>MEPGæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä½ç½®é£æ ¼æ„ŸçŸ¥ï¼ˆPSAï¼‰æ¨¡å—å’Œå¤šä¸“å®¶æ‰©æ•£ï¼ˆMEDï¼‰æ¨¡å—ã€‚</li>
<li>PSAæ¨¡å—åˆ©ç”¨ç›‘ç£å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å°†è¾“å…¥æç¤ºåˆ†è§£ä¸ºç²¾ç¡®çš„ç©ºé—´åæ ‡å’Œé£æ ¼ç¼–ç è¯­ä¹‰æŒ‡ä»¤ã€‚</li>
<li>MEDæ¨¡å—é€šè¿‡åŠ¨æ€ä¸“å®¶è·¯ç”±åœ¨å±€éƒ¨åŒºåŸŸå’Œå…¨å±€åŒºåŸŸè¿›è¡Œè·¨åŒºåŸŸç”Ÿæˆã€‚</li>
<li>MEPGæ”¯æŒè½»æ¾é›†æˆå’Œæ›¿æ¢ä¸“å®¶æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2509.04126v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KB-DMGen-Knowledge-Based-Global-Guidance-and-Dynamic-Pose-Masking-for-Human-Image-Generation"><a href="#KB-DMGen-Knowledge-Based-Global-Guidance-and-Dynamic-Pose-Masking-for-Human-Image-Generation" class="headerlink" title="KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for   Human Image Generation"></a>KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for   Human Image Generation</h2><p><strong>Authors:Shibang Liu, Xuemei Xie, Guangming Shi</strong></p>
<p>Recent methods using diffusion models have made significant progress in Human Image Generation (HIG) with various control signals such as pose priors. In HIG, both accurate human poses and coherent visual quality are crucial for image generation. However, most existing methods mainly focus on pose accuracy while neglecting overall image quality, often improving pose alignment at the cost of image quality. To address this, we propose Knowledge-Based Global Guidance and Dynamic pose Masking for human image Generation (KB-DMGen). The Knowledge Base (KB), implemented as a visual codebook, provides coarse, global guidance based on input text-related visual features, improving pose accuracy while maintaining image quality, while the Dynamic pose Mask (DM) offers fine-grained local control to enhance precise pose accuracy. By injecting KB and DM at different stages of the diffusion process, our framework enhances pose accuracy through both global and local control without compromising image quality. Experiments demonstrate the effectiveness of KB-DMGen, achieving new state-of-the-art results in terms of AP and CAP on the HumanArt dataset. The project page and code are available at <a target="_blank" rel="noopener" href="https://lushbng.github.io/KBDMGen">https://lushbng.github.io/KBDMGen</a>. </p>
<blockquote>
<p>è¿‘æœŸä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨äººç±»å›¾åƒç”Ÿæˆï¼ˆHIGï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡å„ç§æ§åˆ¶ä¿¡å·ï¼ˆå¦‚å§¿æ€å…ˆéªŒï¼‰å®ç°äº†é«˜è´¨é‡çš„äººç±»å›¾åƒç”Ÿæˆã€‚åœ¨HIGä¸­ï¼Œå‡†ç¡®çš„äººç±»å§¿æ€å’Œè¿è´¯çš„è§†è§‰è´¨é‡å¯¹äºå›¾åƒç”Ÿæˆéƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å§¿æ€å‡†ç¡®æ€§ï¼Œè€Œå¿½è§†æ•´ä½“å›¾åƒè´¨é‡ï¼Œå¾€å¾€ä»¥æé«˜å§¿æ€å¯¹é½åº¦ä¸ºä»£ä»·æ¥ç‰ºç‰²å›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†çš„å…¨å±€å¼•å¯¼å’ŒåŠ¨æ€å§¿æ€æ©ç çš„äººç±»å›¾åƒç”Ÿæˆæ–¹æ³•ï¼ˆKB-DMGenï¼‰ã€‚çŸ¥è¯†åº“ï¼ˆKBï¼‰ä»¥è§†è§‰ä»£ç ç°¿çš„å½¢å¼å®ç°ï¼ŒåŸºäºè¾“å…¥æ–‡æœ¬ç›¸å…³çš„è§†è§‰ç‰¹å¾æä¾›ç²—ç•¥çš„å…¨å±€æŒ‡å¯¼ï¼Œåœ¨æé«˜å§¿æ€å‡†ç¡®æ€§çš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡ï¼›è€ŒåŠ¨æ€å§¿æ€æ©ç ï¼ˆDMï¼‰åˆ™æä¾›ç²¾ç»†çš„å±€éƒ¨æ§åˆ¶ï¼Œä»¥å¢å¼ºç²¾ç¡®å§¿æ€çš„å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µæ³¨å…¥KBå’ŒDMï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å…¨å±€å’Œå±€éƒ¨æ§åˆ¶æé«˜äº†å§¿æ€å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å›¾åƒè´¨é‡ã€‚å®éªŒè¡¨æ˜KB-DMGençš„æœ‰æ•ˆæ€§ï¼Œåœ¨äººç±»è‰ºæœ¯æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰å’Œç±»åˆ«å¹³å‡ç²¾åº¦ï¼ˆCAPï¼‰çš„æœ€æ–°ç»“æœã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://lushbng.github.io/KBDMGen%E8%AE%BF%E9%97%AE%E3%80%82">https://lushbng.github.io/KBDMGenè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20083v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•åœ¨äººç±»å›¾åƒç”Ÿæˆï¼ˆHIGï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡å§¿åŠ¿å…ˆéªŒç­‰æ§åˆ¶ä¿¡å·å®ç°äº†å‡†ç¡®çš„äººä½“å§¿åŠ¿å’Œè¿è´¯çš„è§†è§‰è´¨é‡ã€‚ä¸ºè§£å†³å¤šæ•°æ–¹æ³•ä»…æ³¨é‡å§¿åŠ¿å‡†ç¡®æ€§è€Œå¿½è§†æ•´ä½“å›¾åƒè´¨é‡çš„é—®é¢˜ï¼Œæå‡ºåŸºäºçŸ¥è¯†çš„å…¨å±€å¼•å¯¼å’ŒåŠ¨æ€å§¿åŠ¿é®ç½©ï¼ˆKB-DMGenï¼‰ã€‚çŸ¥è¯†åº“æä¾›åŸºäºè¾“å…¥æ–‡æœ¬çš„è§†è§‰ç‰¹å¾è¿›è¡Œç²—ç•¥çš„å…¨å±€æŒ‡å¯¼ï¼Œç»´æŒå›¾åƒè´¨é‡çš„åŒæ—¶æé«˜å§¿åŠ¿å‡†ç¡®æ€§ï¼›åŠ¨æ€å§¿åŠ¿é®ç½©åˆ™æä¾›ç²¾ç»†çš„å±€éƒ¨æ§åˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜ç²¾ç¡®å§¿åŠ¿çš„å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨ä¸åŒæ‰©æ•£é˜¶æ®µæ³¨å…¥KBå’ŒDMï¼Œæ¡†æ¶èƒ½åœ¨å…¨å±€å’Œå±€éƒ¨æ§åˆ¶ä¸­æé«˜å§¿åŠ¿å‡†ç¡®æ€§ï¼Œä¸”ä¸ä¼šæŸå®³å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨äººä½“å›¾åƒç”Ÿæˆï¼ˆHIGï¼‰ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç›®å‰æ–¹æ³•å¤šä¾§é‡äºå§¿åŠ¿å‡†ç¡®æ€§ï¼Œä½†å¯èƒ½å¿½è§†æ•´ä½“å›¾åƒè´¨é‡ã€‚</li>
<li>KB-DMGenæ–¹æ³•ç»“åˆäº†çŸ¥è¯†åº“å’ŒåŠ¨æ€å§¿åŠ¿é®ç½©æŠ€æœ¯ã€‚</li>
<li>çŸ¥è¯†åº“æä¾›å…¨å±€æŒ‡å¯¼ä»¥æé«˜å§¿åŠ¿å‡†ç¡®æ€§å¹¶ç»´æŒå›¾åƒè´¨é‡ã€‚</li>
<li>åŠ¨æ€å§¿åŠ¿é®ç½©ç”¨äºç²¾ç»†çš„å±€éƒ¨æ§åˆ¶ï¼Œè¿›ä¸€æ­¥æé«˜å§¿åŠ¿çš„å‡†ç¡®æ€§ã€‚</li>
<li>KB-DMGené€šè¿‡å…¨å±€å’Œå±€éƒ¨æ§åˆ¶å¢å¼ºå§¿åŠ¿å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¸æŸå®³å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.20083v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.20083v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.20083v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.20083v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.20083v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SeeDiff-Off-the-Shelf-Seeded-Mask-Generation-from-Diffusion-Models"><a href="#SeeDiff-Off-the-Shelf-Seeded-Mask-Generation-from-Diffusion-Models" class="headerlink" title="SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models"></a>SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models</h2><p><strong>Authors:Joon Hyun Park, Kumju Jo, Sungyong Baik</strong></p>
<p>Entrusted with the goal of pixel-level object classification, the semantic segmentation networks entail the laborious preparation of pixel-level annotation masks. To obtain pixel-level annotation masks for a given class without human efforts, recent few works have proposed to generate pairs of images and annotation masks by employing image and text relationships modeled by text-to-image generative models, especially Stable Diffusion. However, these works do not fully exploit the capability of text-guided Diffusion models and thus require a pre-trained segmentation network, careful text prompt tuning, or the training of a segmentation network to generate final annotation masks. In this work, we take a closer look at attention mechanisms of Stable Diffusion, from which we draw connections with classical seeded segmentation approaches. In particular, we show that cross-attention alone provides very coarse object localization, which however can provide initial seeds. Then, akin to region expansion in seeded segmentation, we utilize the semantic-correspondence-modeling capability of self-attention to iteratively spread the attention to the whole class from the seeds using multi-scale self-attention maps. We also observe that a simple-text-guided synthetic image often has a uniform background, which is easier to find correspondences, compared to complex-structured objects. Thus, we further refine a mask using a more accurate background mask. Our proposed method, dubbed SeeDiff, generates high-quality masks off-the-shelf from Stable Diffusion, without additional training procedure, prompt tuning, or a pre-trained segmentation network. </p>
<blockquote>
<p>ä»¥åƒç´ çº§ç›®æ ‡åˆ†ç±»ä¸ºä»»åŠ¡ç›®æ ‡çš„è¯­ä¹‰åˆ†å‰²ç½‘ç»œéœ€è¦ç¹ççš„åƒç´ çº§æ³¨é‡Šæ©è†œåˆ¶å¤‡å·¥ä½œã€‚ä¸ºäº†è·å¾—ç»™å®šç±»åˆ«çš„åƒç´ çº§æ³¨é‡Šæ©è†œè€Œä¸ä¾èµ–äººå·¥æ“ä½œï¼Œè¿‘æœŸçš„ä¸€äº›ç ”ç©¶æå‡ºäº†é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯Stable Diffusionï¼‰æ¥å»ºæ¨¡å›¾åƒå’Œæ–‡æœ¬å…³ç³»ï¼Œä»è€Œç”Ÿæˆå›¾åƒå’Œæ³¨é‡Šæ©ç å¯¹ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¹¶æœªå……åˆ†åˆ©ç”¨æ–‡æœ¬å¼•å¯¼å‹æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œå› æ­¤ä»éœ€é¢„å…ˆè®­ç»ƒçš„åˆ†å‰²ç½‘ç»œã€ç²¾ç»†çš„æ–‡æœ¬æç¤ºè°ƒæ•´æˆ–åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ¥ç”Ÿæˆæœ€ç»ˆçš„æ³¨é‡Šæ©è†œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19808v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ–‡æœ¬å¼•å¯¼çš„ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆåƒç´ çº§æ ‡æ³¨æ©è†œçš„æ–¹æ³•ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶ç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSeeDiffçš„æ–¹æ³•ï¼Œæ— éœ€é¢„è®­ç»ƒåˆ†å‰²ç½‘ç»œã€ç²¾ç»†æ–‡æœ¬æç¤ºè°ƒæ•´æˆ–è®­ç»ƒåˆ†å‰²ç½‘ç»œï¼Œå³å¯ç›´æ¥ä»ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆé«˜è´¨é‡æ©è†œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£æ¨¡å‹è¢«ç”¨äºç”Ÿæˆå›¾åƒå’Œæ ‡æ³¨æ©è†œå¯¹ï¼Œä»¥å‡è½»åƒç´ çº§æ ‡æ³¨çš„ç¹é‡å·¥ä½œã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ–‡æœ¬å¼•å¯¼çš„ç¨³å®šæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆæœ€ç»ˆæ ‡æ³¨æ©è†œæ—¶ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ·±å…¥ç ”ç©¶ç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‘ç°äº¤å‰æ³¨æ„åŠ›å¯æä¾›è¾ƒç²—ç³™çš„ç›®æ ‡å®šä½ï¼Œè€Œè‡ªæ³¨æ„åŠ›æœ‰åŠ©äºä»ç§å­ç‚¹æ‰©å±•åˆ°æ•´ä¸ªç±»åˆ«çš„è¯­ä¹‰å¯¹åº”ã€‚</li>
<li>SeeDiffæ–¹æ³•åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ©è†œï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ã€æç¤ºè°ƒæ•´æˆ–é¢„è®­ç»ƒåˆ†å‰²ç½‘ç»œã€‚</li>
<li>ç®€å•æ–‡æœ¬å¼•å¯¼çš„åˆæˆå›¾åƒé€šå¸¸æœ‰ç»Ÿä¸€çš„èƒŒæ™¯ï¼Œè¿™æ›´å®¹æ˜“æ‰¾åˆ°å¯¹åº”å…³ç³»ï¼Œç›¸æ¯”äºå¤æ‚ç»“æ„çš„ç‰©ä½“ã€‚</li>
<li>ä½¿ç”¨æ›´å‡†ç¡®çš„èƒŒæ™¯æ©è†œè¿›ä¸€æ­¥ç²¾ç»†åŒ–ç”Ÿæˆçš„æ©è†œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.19808v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.19808v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.19808v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.19808v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2507.19808v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="STRICT-Stress-Test-of-Rendering-Images-Containing-Text"><a href="#STRICT-Stress-Test-of-Rendering-Images-Containing-Text" class="headerlink" title="STRICT: Stress Test of Rendering Images Containing Text"></a>STRICT: Stress Test of Rendering Images Containing Text</h2><p><strong>Authors:Tianyu Zhang, Xinyu Wang, Lu Li, Zhenghan Tai, Jijun Chi, Jingrui Tian, Hailin He, Suyuchen Wang</strong></p>
<p>While diffusion models have revolutionized text-to-image generation with their ability to synthesize realistic and diverse scenes, they continue to struggle to generate consistent and legible text within images. This shortcoming is commonly attributed to the locality bias inherent in diffusion-based generation, which limits their ability to model long-range spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a benchmark designed to systematically stress-test the ability of diffusion models to render coherent and instruction-aligned text in images. Our benchmark evaluates models across multiple dimensions: (1) the maximum length of readable text that can be generated; (2) the correctness and legibility of the generated text, and (3) the ratio of not following instructions for generating text. We evaluate several state-of-the-art models, including proprietary and open-source variants, and reveal persistent limitations in long-range consistency and instruction-following capabilities. Our findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling. We release our entire evaluation pipeline at <a target="_blank" rel="noopener" href="https://github.com/tianyu-z/STRICT-Bench">https://github.com/tianyu-z/STRICT-Bench</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å‡­å€Ÿåˆæˆé€¼çœŸä¸”å¤šæ ·åŒ–çš„åœºæ™¯çš„èƒ½åŠ›ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä¸­å¼•å‘äº†é©å‘½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»éš¾ä»¥åœ¨å›¾åƒå†…éƒ¨ç”Ÿæˆè¿è´¯ä¸”æ¸…æ™°çš„æ–‡æœ¬ã€‚è¿™ä¸€ç¼ºé™·é€šå¸¸å½’å› äºæ‰©æ•£ç”Ÿæˆä¸­å›ºæœ‰çš„å±€éƒ¨åè§ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹é•¿è·ç¦»ç©ºé—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†$\textbf{STRICT}$ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æµ‹è¯•æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¸­å‘ˆç°è¿è´¯ä¸”ç¬¦åˆæŒ‡ä»¤çš„æ–‡æœ¬çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä»å¤šä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹ï¼šï¼ˆ1ï¼‰å¯ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼›ï¼ˆ2ï¼‰ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ï¼›ï¼ˆ3ï¼‰æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸éµå¾ªæŒ‡ä»¤çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºå˜ä½“åœ¨å†…çš„è‹¥å¹²æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶æ­ç¤ºäº†é•¿æœŸä¸€è‡´æ€§ä»¥åŠéµå¾ªæŒ‡ä»¤èƒ½åŠ›æ–¹é¢çš„æŒç»­å±€é™æ€§ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºæ¶æ„ç“¶é¢ˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€ç”Ÿæˆå»ºæ¨¡ç ”ç©¶æä¾›äº†åŠ¨åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬æ•´ä¸ªè¯„ä¼°ç®¡é“<a target="_blank" rel="noopener" href="https://github.com/tianyu-z/STRICT-Bench%E4%BE%9B%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/tianyu-z/STRICT-Benchä¾›ç ”ç©¶ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18985v2">PDF</a> Accepted as a main conference paper at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºé©å‘½æ€§çš„èƒ½åŠ›ï¼Œèƒ½åˆæˆé€¼çœŸä¸”å¤šæ ·çš„åœºæ™¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥åœ¨å›¾åƒå†…ç”Ÿæˆè¿è´¯å’Œæ¸…æ™°çš„æ–‡æœ¬ã€‚è¿™ä¸€ç¼ºé™·é€šå¸¸å½’å› äºæ‰©æ•£ç”Ÿæˆä¸­çš„å±€éƒ¨åè§ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹é•¿ç¨‹ç©ºé—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSTRICTçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æµ‹è¯•æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¸­å‘ˆç°è¿è´¯ä¸”ç¬¦åˆæŒ‡ä»¤çš„æ–‡æœ¬çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•ä»å¤šä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¯ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€ç”Ÿæˆçš„æ–‡æœ¬çš„æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦ä»¥åŠéµå¾ªæŒ‡ä»¤ç”Ÿæˆæ–‡æœ¬çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºå˜ä½“åœ¨å†…çš„æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ­ç¤ºäº†é•¿ç¨‹ä¸€è‡´æ€§å’Œéµå¾ªæŒ‡ä»¤èƒ½åŠ›æ–¹é¢çš„æŒç»­å±€é™æ€§ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºæ¶æ„ç“¶é¢ˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€ç”Ÿæˆå»ºæ¨¡çš„æœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚æˆ‘ä»¬å·²å‘å¸ƒå®Œæ•´çš„è¯„ä¼°ç®¡é“ï¼š<a target="_blank" rel="noopener" href="https://github.com/tianyu-z/STRICT-Bench">https://github.com/tianyu-z/STRICT-Bench</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”Ÿæˆè¿è´¯å’Œæ¸…æ™°æ–‡æœ¬ä»å­˜åœ¨å›°éš¾ã€‚</li>
<li>å±€éƒ¨åè§é™åˆ¶äº†æ‰©æ•£æ¨¡å‹å¯¹é•¿ç¨‹ç©ºé—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥çš„STRICTåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¸­å‘ˆç°è¿è´¯ä¸”ç¬¦åˆæŒ‡ä»¤æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°æ¨¡å‹è¡¨ç°åŒ…æ‹¬å¯ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€æ–‡æœ¬çš„æ¸…æ™°åº¦å’Œæ­£ç¡®æ€§ï¼Œä»¥åŠéµå¾ªæŒ‡ä»¤çš„æ¯”ä¾‹ã€‚</li>
<li>å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†é•¿ç¨‹ä¸€è‡´æ€§å’Œéµå¾ªæŒ‡ä»¤èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†æ¶æ„ç“¶é¢ˆçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2505.18985v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2505.18985v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2505.18985v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2505.18985v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OSDM-MReg-Multimodal-Image-Registration-based-One-Step-Diffusion-Model"><a href="#OSDM-MReg-Multimodal-Image-Registration-based-One-Step-Diffusion-Model" class="headerlink" title="OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model"></a>OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model</h2><p><strong>Authors:Xiaochen Wei, Weiwei Guo, Wenxian Yu, Feiming Wei, Dongying Li</strong></p>
<p>Multimodal remote sensing image registration aligns images from different sensors for data fusion and analysis. However, existing methods often struggle to extract modality-invariant features when faced with large nonlinear radiometric differences, such as those between SAR and optical images. To address these challenges, we propose OSDM-MReg, a novel multimodal image registration framework that bridges the modality gap through image-to-image translation. Specifically, we introduce a one-step unaligned target-guided conditional diffusion model (UTGOS-CDM) to translate source and target images into a unified representation domain. Unlike traditional conditional DDPM that require hundreds of iterative steps for inference, our model incorporates a novel inverse translation objective during training to enable direct prediction of the translated image in a single step at test time, significantly accelerating the registration process. After translation, we design a multimodal multiscale registration network (MM-Reg) that extracts and fuses both unimodal and translated multimodal images using the proposed multimodal fusion strategy, enhancing the robustness and precision of alignment across scales and modalities. Extensive experiments on the OSdataset demonstrate that OSDM-MReg achieves superior registration accuracy compared to state-of-the-art methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†æ˜¯ä¸ºäº†æ•°æ®èåˆå’Œåˆ†æè€Œå¯¹ä¸åŒä¼ æ„Ÿå™¨çš„å›¾åƒè¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨é¢å¯¹å¦‚SARå’Œå…‰å­¦å›¾åƒä¹‹é—´çš„å¤§éçº¿æ€§è¾å°„å·®å¼‚æ—¶ï¼Œå¾€å¾€éš¾ä»¥æå–æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OSDM-MRegï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šæ¨¡æ€å›¾åƒé…å‡†æ¡†æ¶ï¼Œå®ƒé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥å¼¥åˆæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€æ­¥å¼æœªå¯¹é½ç›®æ ‡å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆUTGOS-CDMï¼‰ï¼Œå°†æºå›¾åƒå’Œç›®æ ‡å›¾åƒç¿»è¯‘æˆä¸€ä¸ªç»Ÿä¸€çš„è¡¨ç°åŸŸã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦æ•°ç™¾ä¸ªè¿­ä»£æ­¥éª¤è¿›è¡Œæ¨æ–­çš„æ¡ä»¶DDPMä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†æ–°é¢–çš„åå‘ç¿»è¯‘ç›®æ ‡ï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿåœ¨å•æ­¥ä¸­ç›´æ¥é¢„æµ‹ç¿»è¯‘åçš„å›¾åƒï¼Œæ˜¾è‘—åŠ é€Ÿäº†é…å‡†è¿‡ç¨‹ã€‚ç¿»è¯‘åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šå°ºåº¦é…å‡†ç½‘ç»œï¼ˆMM-Regï¼‰ï¼Œè¯¥ç½‘ç»œä½¿ç”¨æ‰€æå‡ºçš„å¤šæ¨¡æ€èåˆç­–ç•¥æ¥æå–å’Œèåˆå•æ¨¡æ€å’Œç¿»è¯‘åçš„å¤šæ¨¡æ€å›¾åƒï¼Œæé«˜äº†è·¨å°ºåº¦å’Œæ¨¡æ€å¯¹é½çš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚åœ¨OSdatasetä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOSDM-MRegçš„é…å‡†ç²¾åº¦ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06027v2">PDF</a> This version updates our previous submission. After rerunning the   experiments, we found that the proposed high-frequency perceptual loss did   not improve the overall performance of the model. Therefore, we removed this   component, revised the corresponding ablation studies, and updated the   contributions accordingly. This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†æ—¨åœ¨å°†ä¸åŒä¼ æ„Ÿå™¨çš„å›¾åƒè¿›è¡Œå¯¹é½ï¼Œä»¥ä¾¿æ•°æ®èåˆå’Œåˆ†æã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å¤§å¹…éçº¿æ€§è¾å°„å·®å¼‚ï¼ˆå¦‚SARå’Œå…‰å­¦å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼‰æ—¶éš¾ä»¥æå–æ¨¡æ€ä¸å˜ç‰¹å¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OSDM-MRegæ–°å‹å¤šæ¨¡æ€å›¾åƒé…å‡†æ¡†æ¶ï¼Œå®ƒé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥å¼¥è¡¥æ¨¡æ€å·®å¼‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€æ­¥å¼æœªå¯¹é½ç›®æ ‡å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆUTGOS-CDMï¼‰ï¼Œå°†æºå›¾åƒå’Œç›®æ ‡å›¾åƒç¿»è¯‘åˆ°ç»Ÿä¸€è¡¨ç¤ºåŸŸã€‚ä¸ä¼ ç»Ÿéœ€è¦æ•°ç™¾æ¬¡è¿­ä»£æ¨ç†çš„æ¡ä»¶DDPMä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†é€†å‘ç¿»è¯‘ç›®æ ‡ï¼Œä½¿å¾—æµ‹è¯•æ—¶èƒ½å¤Ÿåœ¨å•æ­¥å†…ç›´æ¥é¢„æµ‹ç¿»è¯‘åçš„å›¾åƒï¼Œå¤§å¹…åŠ é€Ÿé…å‡†è¿‡ç¨‹ã€‚ç¿»è¯‘åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šå°ºåº¦é…å‡†ç½‘ç»œï¼ˆMM-Regï¼‰ï¼Œç»“åˆå•æ¨¡æ€å’Œç¿»è¯‘åçš„å¤šæ¨¡æ€å›¾åƒï¼Œé‡‡ç”¨æå‡ºçš„å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œæé«˜äº†è·¨å°ºåº¦å’Œæ¨¡æ€å¯¹é½çš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚åœ¨OSdatasetä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOSDM-MRegçš„é…å‡†ç²¾åº¦ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†æ˜¯ä¸ºäº†æ•°æ®èåˆå’Œåˆ†æè€Œå°†å¯¹é½æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„å›¾åƒçš„æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å¤§å¹…éçº¿æ€§è¾å°„å·®å¼‚æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>OSDM-MRegæ¡†æ¶é€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥å¼¥è¡¥æ¨¡æ€å·®å¼‚ã€‚</li>
<li>UTGOS-CDMæ¨¡å‹èƒ½å¤Ÿåœ¨å•æ­¥å†…ç›´æ¥é¢„æµ‹ç¿»è¯‘åçš„å›¾åƒï¼ŒåŠ é€Ÿé…å‡†è¿‡ç¨‹ã€‚</li>
<li>MM-Regç½‘ç»œç»“åˆäº†å•æ¨¡æ€å’Œç¿»è¯‘åçš„å¤šæ¨¡æ€å›¾åƒï¼Œæé«˜äº†é…å‡†çš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚</li>
<li>OSDM-MRegåœ¨OSdatasetä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é…å‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2504.06027v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion"><a href="#Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion" class="headerlink" title="Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion"></a>Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion</h2><p><strong>Authors:Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, Anton Obukhov</strong></p>
<p>Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: <a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a> </p>
<blockquote>
<p>æ·±åº¦è¡¥å…¨å°†ç¨€ç–çš„æ·±åº¦æµ‹é‡å€¼å‡çº§ä¸ºåœ¨å¸¸è§„å›¾åƒå¼•å¯¼ä¸‹ç”Ÿæˆçš„å¯†é›†æ·±åº¦å›¾ã€‚é’ˆå¯¹è¿™ä¸€é«˜åº¦ä¸é€‚å®šçš„ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ä¸¥æ ¼å—é™çš„ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œå½“åº”ç”¨äºè®­ç»ƒåŸŸå¤–çš„å›¾åƒæˆ–å¯ç”¨çš„æ·±åº¦æµ‹é‡å€¼ç¨€ç–ã€åˆ†å¸ƒä¸å‡æˆ–å¯†åº¦ä¸åŒæ—¶ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ã€‚å—å•çœ¼æ·±åº¦ä¼°è®¡æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬å°†æ·±åº¦è¡¥å…¨é‡æ–°æ„å»ºä¸ºä¸€ç§ä»¥ç¨€ç–æµ‹é‡ä¸ºå¼•å¯¼çš„å›¾åƒæ¡ä»¶æ·±åº¦å›¾ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•Marigold-DCå»ºç«‹åœ¨ç”¨äºå•çœ¼æ·±åº¦ä¼°è®¡çš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å°†æ·±åº¦è§‚æµ‹å€¼ä½œä¸ºæµ‹è¯•æ—¶çš„æŒ‡å¯¼ï¼Œå¹¶é€šè¿‡ä¸å»å™ªæ‰©æ•£çš„è¿­ä»£æ¨æ–­å¹¶è¡Œè¿è¡Œçš„ä¼˜åŒ–æ–¹æ¡ˆæ¥æ³¨å…¥ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°å¤„ç†æå…¶ç¨€ç–çš„å¼•å¯¼ã€‚æˆ‘ä»¬çš„ç»“æœå»ºè®®ï¼Œç°ä»£å•çœ¼æ·±åº¦å…ˆéªŒå¯ä»¥æå¤§åœ°æé«˜æ·±åº¦è¡¥å…¨çš„é²æ£’æ€§ï¼šæ›´å¥½çš„åšæ³•æ˜¯è§†æ­¤ä»»åŠ¡ä¸ºä»å¯†é›†å›¾åƒåƒç´ æ¢å¤å¯†é›†æ·±åº¦ï¼Œå—ç¨€ç–æ·±åº¦å¼•å¯¼ï¼›è€Œéåœ¨å›¾åƒå¼•å¯¼ä¸‹å¯¹ç¨€ç–æ·±åº¦è¿›è¡Œå¡«å……ä¿®å¤ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13389v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMarigold-DCçš„æ·±åº¦å®Œæˆæ–¹æ³•ï¼Œå®ƒå°†æ·±åº¦å®Œæˆä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåœ¨ç¨€ç–æµ‹é‡å¼•å¯¼ä¸‹ï¼ŒåŸºäºå›¾åƒçš„æ·±åº¦å›¾ç”Ÿæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå•çœ¼æ·±åº¦ä¼°è®¡ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æ–¹æ¡ˆåœ¨æµ‹è¯•æ—¶æ³¨å…¥æ·±åº¦è§‚æµ‹å€¼ï¼Œè¯¥ä¼˜åŒ–æ–¹æ¡ˆä¸å»å™ªæ‰©æ•£çš„è¿­ä»£æ¨æ–­å¹¶è¡Œè¿è¡Œã€‚è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯å¤„ç†å„ç§ç¯å¢ƒï¼Œç”šè‡³å¯¹æç¨€ç–çš„å¼•å¯¼ä¹Ÿèƒ½æœ‰æ•ˆå¤„ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Marigold-DCæ–¹æ³•å°†æ·±åº¦å®Œæˆå®šä¹‰ä¸ºåœ¨ç¨€ç–æµ‹é‡å¼•å¯¼ä¸‹ï¼ŒåŸºäºå›¾åƒçš„æ·±åº¦å›¾ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå•çœ¼æ·±åº¦ä¼°è®¡ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æ–¹æ¡ˆåœ¨æµ‹è¯•æ—¶æ³¨å…¥æ·±åº¦è§‚æµ‹å€¼ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½é€‚åº”å¤šç§ç¯å¢ƒã€‚</li>
<li>Marigold-DCèƒ½æœ‰æ•ˆå¤„ç†æç¨€ç–çš„å¼•å¯¼æƒ…å†µã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨æŸäº›çº¦æŸæ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å›¾åƒè¶…å‡ºè®­ç»ƒåŸŸæˆ–å¯ç”¨æ·±åº¦æµ‹é‡ç¨€ç–ã€åˆ†å¸ƒä¸å‡ã€å¯†åº¦ä¸ä¸€çš„æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>æœ€ä½³å®è·µå¯èƒ½æ˜¯å°†ä»»åŠ¡è§†ä¸ºä»å¯†é›†å›¾åƒåƒç´ ä¸­æ¢å¤å¯†é›†æ·±åº¦ï¼Œç”±ç¨€ç–æ·±åº¦å¼•å¯¼ï¼Œè€Œä¸æ˜¯ä¾é å›¾åƒå¼•å¯¼æ¥å¡«å……ï¼ˆç¨€ç–ï¼‰æ·±åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2412.13389v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2412.13389v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2412.13389v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SCP-Diff-Spatial-Categorical-Joint-Prior-for-Diffusion-Based-Semantic-Image-Synthesis"><a href="#SCP-Diff-Spatial-Categorical-Joint-Prior-for-Diffusion-Based-Semantic-Image-Synthesis" class="headerlink" title="SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic   Image Synthesis"></a>SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic   Image Synthesis</h2><p><strong>Authors:Huan-ang Gao, Mingju Gao, Jiaju Li, Wenyi Li, Rong Zhi, Hao Tang, Hao Zhao</strong></p>
<p>Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has set new state-of-the-art results in SIS on Cityscapes, ADE20K and COCO-Stuff, yielding a FID as low as 10.53 on Cityscapes. The code and models can be accessed via the project page. </p>
<blockquote>
<p>è¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰åœ¨ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿæ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œç›®å‰è¯¥é¢†åŸŸåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æœ€ä½³å®è·µå°šæœªè¾¾åˆ°ç†æƒ³çš„è´¨é‡æ°´å¹³ã€‚éšç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæˆ‘ä»¬å—åˆ°äº†å¯å‘ï¼Œå¯¹ControlNetè¿›è¡Œäº†è¯„ä¼°ï¼Œå› å…¶å…·æœ‰å¯†é›†çš„æ§åˆ¶èƒ½åŠ›è€Œæˆä¸ºäº†ä¸€ç§å€¼å¾—æ³¨æ„çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°äº†å…¶ç»“æœçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå¤§å‹è¯­ä¹‰åŒºåŸŸå†…å­˜åœ¨å¥‡æ€ªçš„å­ç»“æ„ä»¥åŠå†…å®¹ä¸è¯­ä¹‰æ©ç çš„é”™ä½ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å°†è¿™äº›é—®é¢˜çš„åŸå› å½’ç»“ä¸ºå™ªå£°è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸æ¨ç†é˜¶æ®µåº”ç”¨çš„æ ‡å‡†æ­£æ€å…ˆéªŒä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºSISå¼€å‘äº†ç‰¹å®šçš„å™ªå£°å…ˆéªŒï¼ŒåŒ…æ‹¬ç©ºé—´å…ˆéªŒã€åˆ†ç±»å…ˆéªŒä»¥åŠç”¨äºæ¨ç†çš„æ–°å‹ç©ºé—´åˆ†ç±»è”åˆå…ˆéªŒã€‚æˆ‘ä»¬ç§°è¿™ç§æ–¹æ³•ä¸ºSCP-Diffï¼Œå®ƒåœ¨Cityscapesã€ADE20Kå’ŒCOCO-Stuffçš„SISä»»åŠ¡ä¸Šåˆ›ä¸‹äº†æœ€æ–°çºªå½•ï¼Œå…¶ä¸­åœ¨Cityscapesä¸Šçš„FIDä½è‡³10.53ã€‚ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡é¡¹ç›®é¡µé¢è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09638v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://air-discover.github.io/SCP-Diff/">https://air-discover.github.io/SCP-Diff/</a></p>
<p><strong>Summary</strong></p>
<p>è¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰åœ¨ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿæ–¹é¢å±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å½“å‰æœ€ä½³å®è·µå°šæœªè¾¾åˆ°ç†æƒ³çš„è´¨é‡æ°´å¹³ã€‚éšç€æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œæˆ‘ä»¬å¯¹ControlNetæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ–¹æ³•ä»¥å…¶å¯†é›†çš„æ§åˆ¶èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç ”ç©¶å‘ç°ï¼ŒControlNetå­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå¤§å‹è¯­ä¹‰åŒºåŸŸå†…å­˜åœ¨å¥‡æ€ªçš„å­ç»“æ„ä»¥åŠå†…å®¹ä¸è¯­ä¹‰æ©ç çš„ä¸åŒ¹é…ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å°†è¿™äº›é—®é¢˜å½’å› äºå™ªå£°è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸æ¨ç†é˜¶æ®µåº”ç”¨çš„æ ‡å‡†æ­£æ€ä¼˜å…ˆä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºSISå¼€å‘äº†ç‰¹å®šçš„å™ªå£°å…ˆéªŒï¼ŒåŒ…æ‹¬ç©ºé—´ã€ç±»åˆ«å’Œä¸€ç§æ–°çš„ç”¨äºæ¨ç†çš„ç©ºé—´ç±»åˆ«è”åˆå…ˆéªŒï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºSCP-Diffã€‚è¯¥æ–¹æ³•åœ¨Cityscapesã€ADE20Kå’ŒCOCO-Stuffçš„SISä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼ŒCityscapesä¸Šçš„FIDä½è‡³10.53ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰åœ¨ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿæ–¹é¢å‰æ™¯è‰¯å¥½ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨è´¨é‡é—®é¢˜ã€‚</li>
<li>ControlNetæ–¹æ³•åœ¨å¯†é›†æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨å¥‡æ€ªçš„å­ç»“æ„é—®é¢˜å’Œå†…å®¹ä¸è¯­ä¹‰æ©ç ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>è¿™äº›é—®é¢˜è¢«å½’å› äºå™ªå£°è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨ç†é˜¶æ®µä½¿ç”¨çš„æ ‡å‡†æ­£æ€ä¼˜å…ˆä¹‹é—´çš„ä¸åŒ¹é…ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼€å‘äº†ç‰¹å®šçš„å™ªå£°å…ˆéªŒï¼ŒåŒ…æ‹¬ç©ºé—´ã€ç±»åˆ«å’Œä¸€ç§æ–°çš„ç©ºé—´ç±»åˆ«è”åˆå…ˆéªŒã€‚</li>
<li>SCP-Diffæ–¹æ³•åœ¨SISä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼Œå¹¶åœ¨Cityscapesã€ADE20Kå’ŒCOCO-Stuffä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>SCP-Diffæ–¹æ³•é™ä½äº†FIDåˆ†æ•°è‡³10.53ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2403.09638v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2403.09638v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Diffusion Models/2403.09638v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_ç‰™é½¿ä¿®å¤/2509.12069v1/page_0_0.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  U-Mamba2 Scaling State Space Models for Dental Anatomy Segmentation in   CBCT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_NeRF/2509.11275v1/page_3_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Sphere-GAN a GAN-based Approach for Saliency Estimation in 360Â°   Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
