<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  UniPar A Unified LLM-Based Framework for Parallel and Accelerated Code   Translation in HPC">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_5_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-17-æ›´æ–°"><a href="#2025-09-17-æ›´æ–°" class="headerlink" title="2025-09-17 æ›´æ–°"></a>2025-09-17 æ›´æ–°</h1><h2 id="UniPar-A-Unified-LLM-Based-Framework-for-Parallel-and-Accelerated-Code-Translation-in-HPC"><a href="#UniPar-A-Unified-LLM-Based-Framework-for-Parallel-and-Accelerated-Code-Translation-in-HPC" class="headerlink" title="UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code   Translation in HPC"></a>UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code   Translation in HPC</h2><p><strong>Authors:Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren</strong></p>
<p>Translating programs between various parallel programming languages is an important problem in the high-performance computing (HPC) community. Existing tools for this problem are either too narrow in scope and&#x2F;or outdated. Recent explosive growth in the popularity of large language models (LLMs) and their ability to generate and translate code offers a potential alternative approach. Toward that end, we first need to systematically evaluate the ability of LLMs to translate between parallel languages.   In this work, we introduce UniPar, a systematic evaluation framework for LLM-based parallel code translation. Specifically, in this work, we target translations between serial code, CUDA, and OpenMP. Our goal is to assess how well current instruction-tuned LLMs â€“ specifically GPT-4o-mini and LLaMA-3.3-70B-Instruct â€“ can be used out of the box or enhanced through known strategies. We evaluated four major usage modes: hyperparameter optimization for decoding, zero- and few-shot prompting, supervised fine-tuning, and iterative feedback through compiler-based repair. As a part of the evaluation, we construct a new dataset called PARATRANS, covering both serial-to-parallel translation and cross-paradigm transformations.   Our findings reveal that while off-the-shelf models struggle under the default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15% functional correctness), our UniPar methodology â€“ combining fine-tuning, hyperparameter tuning, and compiler-guided repair â€“ improves performance by up to 2X (69% compilation and 33% correctness). We believe that our findings will provide useful insights for researchers to further improve LLMs for the parallel language translation problem.   UniPar source code and PARATRANS dataset are available at our GitHub repository <a target="_blank" rel="noopener" href="https://github.com/Scientific-Computing-Lab/UniPar_AI">https://github.com/Scientific-Computing-Lab/UniPar_AI</a>. </p>
<blockquote>
<p>åœ¨ä¸åŒå¹¶è¡Œç¼–ç¨‹è¯­è¨€ä¹‹é—´çš„ç¨‹åºç¿»è¯‘æ˜¯é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ç¤¾åŒºä¸­çš„é‡è¦é—®é¢˜ã€‚ç°æœ‰å·¥å…·çš„èŒƒå›´è¦ä¹ˆè¿‡äºç‹­çª„ï¼Œè¦ä¹ˆå·²ç»è¿‡æ—¶ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠåŠå…¶ç”Ÿæˆå’Œç¿»è¯‘ä»£ç çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å¹¶è¡Œè¯­è¨€ä¹‹é—´çš„ç¿»è¯‘èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UniParï¼Œä¸€ä¸ªç”¨äºLLMå¹¶è¡Œä»£ç ç¿»è¯‘çš„ç³»ç»Ÿæ€§è¯„ä»·æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°å½“å‰æŒ‡ä»¤è°ƒä¼˜çš„LLMâ€”â€”ç‰¹åˆ«æ˜¯GPT-4o-miniå’ŒLLaMA-3.3-70B-Instructâ€”â€”æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–é€šè¿‡å·²çŸ¥ç­–ç•¥è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ç§ä¸»è¦ä½¿ç”¨æ¨¡å¼ï¼šè§£ç è¶…å‚æ•°ä¼˜åŒ–ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€ç›‘ç£å¾®è°ƒä»¥åŠåŸºäºç¼–è¯‘å™¨çš„è¿­ä»£åé¦ˆä¿®å¤ã€‚ä½œä¸ºè¯„ä¼°çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºPARATRANSçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸²è¡Œåˆ°å¹¶è¡Œç¿»è¯‘å’Œè·¨èŒƒå¼è½¬æ¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ç°æˆçš„æ¨¡å‹åœ¨é»˜è®¤è®¾ç½®ä¸‹è¡¨ç°æŒ£æ‰ï¼ˆä¾‹å¦‚ï¼ŒGPT-4o-miniä»…å®ç°46%çš„ç¼–è¯‘å’Œ15%çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼‰ï¼Œä½†æˆ‘ä»¬çš„UniParæ–¹æ³•â€”â€”ç»“åˆå¾®è°ƒã€è¶…å‚æ•°è°ƒæ•´å’Œç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤â€”â€”å°†æ€§èƒ½æé«˜äº†ä¸¤å€ï¼ˆ69%çš„ç¼–è¯‘å’Œ33%çš„æ­£ç¡®æ€§ï¼‰ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœå°†ä¸ºç ”ç©¶äººå‘˜è¿›ä¸€æ­¥æ”¹è¿›LLMä»¥è§£å†³å¹¶è¡Œè¯­è¨€ç¿»è¯‘é—®é¢˜æä¾›æœ‰ç›Šçš„è§è§£ã€‚UniParæºä»£ç å’ŒPARATRANSæ•°æ®é›†å¯åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Scientific-Computing-Lab/UniPar_AI%E3%80%82">https://github.com/Scientific-Computing-Lab/UniPar_AIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12136v1">PDF</a> Accepted to IEEE HPEC conference 2025. 9 pages, incl references</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniParæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¶è¡Œä»£ç ç¿»è¯‘æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶é’ˆå¯¹ä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å‘ç°ç°æœ‰æ¨¡å‹æ€§èƒ½æœ‰é™ã€‚é€šè¿‡ç»“åˆå¾®è°ƒã€è¶…å‚æ•°è°ƒæ•´å’Œç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤ï¼ŒUniParæ–¹æ³•èƒ½æé«˜æ€§èƒ½ï¼Œè¾¾åˆ°æœ€é«˜69%ç¼–è¯‘æˆåŠŸç‡å’Œ33%åŠŸèƒ½æ­£ç¡®æ€§ã€‚ç›¸å…³æºä»£ç å’Œæ•°æ®é›†å·²ä¸Šä¼ è‡³GitHubä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniParæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¶è¡Œä»£ç ç¿»è¯‘èƒ½åŠ›çš„ç³»ç»Ÿæ€§è¯„ä»·æ¡†æ¶ã€‚</li>
<li>ç ”ç©¶å…³æ³¨ä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é»˜è®¤è®¾ç½®ä¸‹è¡¨ç°ä¸ä½³ï¼ŒGPT-4o-miniåªæœ‰46%ç¼–è¯‘æˆåŠŸç‡å’Œ15%åŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>UniParæ–¹æ³•é€šè¿‡ç»“åˆå¾®è°ƒã€è¶…å‚æ•°è°ƒæ•´å’Œç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤ï¼Œèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæœ€é«˜è¾¾åˆ°69%ç¼–è¯‘æˆåŠŸç‡å’Œ33%æ­£ç¡®æ€§ã€‚</li>
<li>UniParæºä»£ç å’ŒPARATRANSæ•°æ®é›†å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶å‘ç°å¯¹æ”¹è¿›LLMsè§£å†³å¹¶è¡Œè¯­è¨€ç¿»è¯‘é—®é¢˜æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12136v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FS-SAM2-Adapting-Segment-Anything-Model-2-for-Few-Shot-Semantic-Segmentation-via-Low-Rank-Adaptation"><a href="#FS-SAM2-Adapting-Segment-Anything-Model-2-for-Few-Shot-Semantic-Segmentation-via-Low-Rank-Adaptation" class="headerlink" title="FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic   Segmentation via Low-Rank Adaptation"></a>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic   Segmentation via Low-Rank Adaptation</h2><p><strong>Authors:Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente</strong></p>
<p>Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2â€™s video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2â€™s pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/fornib/FS-SAM2">https://github.com/fornib/FS-SAM2</a> </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²è¿‘æœŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚å…¶ç›®æ ‡åœ¨äºå¼€å‘ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚ç°æœ‰å¤§å¤šæ•°æ–¹æ³•é€šè¿‡ä»å¤´å¼€å§‹è®­ç»ƒé™„åŠ æ¨¡å—æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™äº›æ–¹æ³•éœ€è¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒæ‰èƒ½å®ç°æœ€ä½³æ€§èƒ½ã€‚Segment Anything Model 2ï¼ˆSAM2ï¼‰æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºSAM2çš„å°‘æ ·æœ¬åˆ†å‰²æ–¹æ³•ï¼ˆFS-SAM2ï¼‰ï¼Œå…¶ä¸­SAM2çš„è§†é¢‘åŠŸèƒ½è¢«ç›´æ¥é‡æ–°ç”¨äºå°‘æ ·æœ¬ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹åŸå§‹æ¨¡å—åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œä»¥å¤„ç†é€šå¸¸åœ¨æ ‡å‡†æ•°æ®é›†ä¸­å‘ç°çš„å¤šç§å›¾åƒï¼Œè¿™ä¸SAM2é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„æ—¶åºå…³è”å¸§ä¸åŒã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œåªæœ‰å°‘æ•°å‚æ•°è¿›è¡Œå…ƒè®­ç»ƒï¼Œè¿™æœ‰æ•ˆåœ°é€‚åº”äº†SAM2ï¼ŒåŒæ—¶å—ç›Šäºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»ä½•K-shoté…ç½®ã€‚æˆ‘ä»¬åœ¨PASCAL-5iã€COCO-20iå’ŒFSS-1000æ•°æ®é›†ä¸Šè¯„ä¼°äº†FS-SAM2ï¼Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œå¹¶å±•ç¤ºäº†æ¨ç†è¿‡ç¨‹ä¸­çš„å‡ºè‰²è®¡ç®—æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/fornib/FS-SAM2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fornib/FS-SAM2è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12105v1">PDF</a> Accepted at ICIAP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰çš„Few-Shotè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨SAM2çš„è§†é¢‘åŠŸèƒ½ï¼Œé€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ç›´æ¥åº”ç”¨äºfew-shotä»»åŠ¡ã€‚æ­¤æ–¹æ³•ä»…éœ€å°‘é‡å‚æ•°è¿›è¡Œå…ƒè®­ç»ƒï¼Œå³å¯æœ‰æ•ˆé€‚åº”å„ç§å›¾åƒæ•°æ®é›†ï¼Œå®ç°PASCAL-5iã€COCO-20iå’ŒFSS-1000æ•°æ®é›†ä¸Šçš„æ˜¾è‘—ç»“æœï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ¨ç†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotè¯­ä¹‰åˆ†å‰²æ—¨åœ¨åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æœªè§çš„ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>Segment Anything Model 2 (SAM2)æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æå‡ºçš„FS-SAM2æ–¹æ³•åŸºäºSAM2ï¼Œç›´æ¥åˆ©ç”¨å…¶è§†é¢‘åŠŸèƒ½è¿›è¡Œfew-shotä»»åŠ¡ã€‚</li>
<li>é€šè¿‡åº”ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼ŒFS-SAM2èƒ½å¤Ÿå¤„ç†å¤šæ ·åŒ–çš„å›¾åƒï¼Œä¸åŒäºSAM2çš„é¢„è®­ç»ƒä½¿ç”¨çš„æ—¶åºå…³è”å¸§ã€‚</li>
<li>FS-SAM2ä»…éœ€å°‘é‡å‚æ•°è¿›è¡Œå…ƒè®­ç»ƒï¼Œå³å¯æœ‰æ•ˆé€‚åº”å„ç§æ•°æ®é›†ã€‚</li>
<li>åœ¨PASCAL-5iã€COCO-20iå’ŒFSS-1000æ•°æ®é›†ä¸Šï¼ŒFS-SAM2å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12105v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12105v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.12105v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CEMTM-Contextual-Embedding-based-Multimodal-Topic-Modeling"><a href="#CEMTM-Contextual-Embedding-based-Multimodal-Topic-Modeling" class="headerlink" title="CEMTM: Contextual Embedding-based Multimodal Topic Modeling"></a>CEMTM: Contextual Embedding-based Multimodal Topic Modeling</h2><p><strong>Authors:Amirhossein Abaskohi, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini</strong></p>
<p>We introduce CEMTM, a context-enhanced multimodal topic model designed to infer coherent and interpretable topic structures from both short and long documents containing text and images. CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings, and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities. Unlike existing approaches, CEMTM can process multiple images per document without repeated encoding and maintains interpretability through explicit word-topic and document-topic distributions. Extensive experiments on six multimodal benchmarks show that CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. Further analysis shows its effectiveness in downstream few-shot retrieval and its ability to capture visually grounded semantics in complex domains such as scientific articles. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CEMTMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å¢å¼ºçš„å¤šæ¨¡æ€ä¸»é¢˜æ¨¡å‹ï¼Œæ—¨åœ¨ä»åŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„é•¿çŸ­æ–‡ä¸­æ¨æ–­å‡ºè¿è´¯ä¸”å¯è§£é‡Šçš„ä¸»é¢˜ç»“æ„ã€‚CEMTMå»ºç«‹åœ¨ç»è¿‡ç²¾ç»†è°ƒæ•´çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šï¼Œä»¥è·å¾—ä¸Šä¸‹æ–‡åµŒå…¥ï¼Œå¹¶é‡‡ç”¨åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶æ¥æƒè¡¡å¯¹ä¸»é¢˜æ¨æ–­çš„æ ‡è®°çº§è´¡çŒ®ã€‚é‡å»ºç›®æ ‡ä½¿åŸºäºä¸»é¢˜çš„è¡¨è¾¾ä¸æ–‡æ¡£åµŒå…¥å¯¹é½ï¼Œé¼“åŠ±è·¨æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒCEMTMå¯ä»¥å¤„ç†æ¯ç¯‡æ–‡æ¡£çš„å¤šä¸ªå›¾åƒï¼Œæ— éœ€é‡å¤ç¼–ç ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„å•è¯ä¸»é¢˜å’Œæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒä¿æŒå¯è§£é‡Šæ€§ã€‚åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCEMTMå§‹ç»ˆä¼˜äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œå®ç°äº†ä»¤äººç©ç›®çš„å¹³å‡å¤§æ¨¡å‹åˆ†æ•°2.61ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºäº†å…¶åœ¨ä¸‹æ¸¸å°æ ·æœ¬æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…¶åœ¨ç§‘å­¦æ–‡ç« ç­‰å¤æ‚é¢†åŸŸä¸­æ•è·è§†è§‰åŸºç¡€è¯­ä¹‰çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11465v1">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>CEMTMæ˜¯ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å¢å¼ºçš„å¤šæ¨¡æ€ä¸»é¢˜æ¨¡å‹ï¼Œå¯ä»åŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„é•¿çŸ­æ–‡ä¸­æ¨æ–­å‡ºè¿è´¯ä¸”å¯è§£é‡Šçš„ä¸»é¢˜ç»“æ„ã€‚å®ƒåˆ©ç”¨ç²¾ç»†è°ƒæ•´çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è·å–ä¸Šä¸‹æ–‡åµŒå…¥ï¼Œé‡‡ç”¨åˆ†å¸ƒæ³¨æ„åŠ›æœºåˆ¶å¯¹ä¸»é¢˜æ¨æ–­ä¸­çš„æ ‡è®°çº§è´¡çŒ®è¿›è¡ŒåŠ æƒã€‚é‡å»ºç›®æ ‡ä½¿åŸºäºä¸»é¢˜çš„è¡¨ç¤ºä¸æ–‡æ¡£åµŒå…¥å¯¹é½ï¼Œé¼“åŠ±è·¨æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒCEMTMå¯ä»¥å¤„ç†æ¯ç¯‡æ–‡æ¡£çš„å¤šä¸ªå›¾åƒï¼Œæ— éœ€é‡å¤ç¼–ç ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„è¯ä¸»é¢˜å’Œæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒä¿æŒå¯è§£é‡Šæ€§ã€‚åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCEMTMå§‹ç»ˆä¼˜äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿ï¼Œå¹³å‡LLMå¾—åˆ†é«˜è¾¾2.61ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå…¶åœ¨ä¸‹æ¸¸å°‘æ ·æœ¬æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨æ•è·ç§‘å­¦æ–‡ç« ç­‰å¤æ‚é¢†åŸŸçš„è§†è§‰åŸºç¡€è¯­ä¹‰æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CEMTMæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä¸»é¢˜æ¨¡å‹ï¼Œå¯ä»¥ä»é•¿çŸ­æ–‡ä¸­æ¨æ–­è¿è´¯ä¸”å¯è§£é‡Šçš„ä¸»é¢˜ç»“æ„ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è·å–ä¸Šä¸‹æ–‡åµŒå…¥ã€‚</li>
<li>CEMTMé‡‡ç”¨åˆ†å¸ƒæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹ä¸»é¢˜æ¨æ–­ä¸­çš„æ ‡è®°çº§åˆ«è´¡çŒ®è¿›è¡ŒåŠ æƒã€‚</li>
<li>é‡å»ºç›®æ ‡ä½¿åŸºäºä¸»é¢˜çš„è¡¨ç¤ºä¸æ–‡æ¡£åµŒå…¥å¯¹é½ï¼Œä¿æŒè·¨æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>CEMTMå¯ä»¥å¤„ç†æ¯ç¯‡æ–‡æ¡£çš„å¤šä¸ªå›¾åƒï¼Œæ— éœ€é‡å¤ç¼–ç ã€‚</li>
<li>CEMTMå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹³å‡LLMå¾—åˆ†é«˜è¾¾2.61ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11465v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11465v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11465v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11465v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11465v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Intelligent-Reservoir-Decision-Support-An-Integrated-Framework-Combining-Large-Language-Models-Advanced-Prompt-Engineering-and-Multimodal-Data-Fusion-for-Real-Time-Petroleum-Operations"><a href="#Intelligent-Reservoir-Decision-Support-An-Integrated-Framework-Combining-Large-Language-Models-Advanced-Prompt-Engineering-and-Multimodal-Data-Fusion-for-Real-Time-Petroleum-Operations" class="headerlink" title="Intelligent Reservoir Decision Support: An Integrated Framework   Combining Large Language Models, Advanced Prompt Engineering, and Multimodal   Data Fusion for Real-Time Petroleum Operations"></a>Intelligent Reservoir Decision Support: An Integrated Framework   Combining Large Language Models, Advanced Prompt Engineering, and Multimodal   Data Fusion for Real-Time Petroleum Operations</h2><p><strong>Authors:Seyed Kourosh Mahjour, Seyed Saman Mahjour</strong></p>
<p>The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance. </p>
<blockquote>
<p>çŸ³æ²¹å·¥ä¸šåœ¨å‚¨å±‚ç®¡ç†ä¸Šé¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¿«é€Ÿæ•´åˆå¤æ‚çš„è·¨æ¨¡æ€æ•°æ®é›†ä»¥æ”¯æŒå®æ—¶å†³ç­–ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„ç»¼åˆæ¡†æ¶ï¼Œç»“åˆæœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oã€Claude 4 Sonnetã€Gemini 2.5 Proï¼‰ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯å’Œè·¨æ¨¡æ€æ•°æ®èåˆï¼Œç”¨äºå…¨é¢çš„å‚¨å±‚åˆ†æã€‚è¯¥æ¡†æ¶å®ç°äº†é’ˆå¯¹5ä¸‡å¤šåçŸ³æ²¹å·¥ç¨‹æ–‡æ¡£çš„é¢†åŸŸç‰¹å®šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€é“¾å¼æ€ç»´æ¨ç†å’Œå°æ ·æœ¬å­¦ä¹ ï¼Œç”¨äºå¿«é€Ÿç°åœºé€‚åº”ã€‚è·¨æ¨¡æ€æ•´åˆæµç¨‹é€šè¿‡å…·æœ‰è§†è§‰å˜å‹å™¨çš„ä¸“ä¸šAIæ¨¡å‹è§£é‡Šåœ°éœ‡æ•°æ®ã€äº•æ—¥å¿—å’Œç”Ÿäº§æ•°æ®ã€‚åœ¨15ä¸ªä¸åŒå‚¨å±‚ç¯å¢ƒè¿›è¡Œçš„ç°åœºéªŒè¯æ˜¾ç¤ºå‡ºè‰²æ€§èƒ½ï¼š94.2%çš„å‚¨å±‚è¡¨å¾å‡†ç¡®æ€§ã€87.6%çš„ç”Ÿäº§é¢„æµ‹ç²¾åº¦å’Œ91.4%çš„äº•ä½ä¼˜åŒ–æˆåŠŸç‡ã€‚ç³»ç»Ÿå®ç°äºšç§’çº§å“åº”æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒ96.2%çš„å®‰å…¨å¯é æ€§ï¼Œåœ¨è¯„ä¼°æœŸé—´æ— é«˜é£é™©äº‹ä»¶å‘ç”Ÿã€‚ç»æµåˆ†ææ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆæœ¬é™ä½62-78%ï¼ˆå¹³å‡é™ä½72%ï¼‰ï¼Œå›æŠ¥æœŸä¸º8ä¸ªæœˆã€‚å°æ ·æœ¬å­¦ä¹ å°†ç°åœºé€‚åº”æ—¶é—´å‡å°‘72%ï¼Œè€Œè‡ªåŠ¨æç¤ºä¼˜åŒ–åˆ™å®ç°äº†æ¨ç†è´¨é‡æé«˜89%ã€‚è¯¥æ¡†æ¶å¤„ç†å®æ—¶æ•°æ®æµï¼Œå¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡ä¸º96.2%ï¼Œå¹¶å‡å°‘45%çš„ç¯å¢ƒäº‹ä»¶ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„å®éªŒåè®®ã€åŸºçº¿å¯¹æ¯”ã€æ¶ˆèç ”ç©¶å’Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•ï¼Œä»¥ç¡®ä¿ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å°†æœ€å‰æ²¿çš„AIæŠ€æœ¯ä¸çŸ³æ²¹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç›¸ç»“åˆçš„å®é™…é›†æˆï¼Œä»¥æé«˜æ“ä½œæ•ˆç‡ã€å®‰å…¨æ€§å’Œç»æµç»©æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11376v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶é‡‡ç”¨æ–°å‹ç»¼åˆæ¡†æ¶ï¼Œç»“åˆæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Claude 4 Sonnetã€Gemini 2.5 Proï¼‰ä¸å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯å’Œå¤šæ¨¡å¼æ•°æ®èåˆï¼Œç”¨äºçŸ³æ²¹å‚¨å±‚ç»¼åˆåˆ†æã€‚è¯¥æ¡†æ¶å®æ–½é¢†åŸŸç‰¹å®šçš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆè¶…è¿‡5ä¸‡ä»½çŸ³æ²¹å·¥ç¨‹æ–‡çŒ®ã€æ€ç»´é“¾æ¨ç†å’Œå°‘é‡å­¦ä¹ ï¼Œç”¨äºå¿«é€Ÿé€‚åº”æ²¹ç”°ç¯å¢ƒã€‚å¤šæ¨¡å¼é›†æˆå¤„ç†åœ°éœ‡è§£é‡Šã€äº•æ—¥å¿—å’Œç”Ÿäº§æ•°æ®ï¼Œé€šè¿‡ä¸“ä¸šçš„äººå·¥æ™ºèƒ½æ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨å®ç°é«˜æ•ˆåˆ†æã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªç¯å¢ƒä¸­æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œä¾‹å¦‚ï¼šé«˜ç²¾ç¡®åº¦ï¼ˆè¾¾ç™¾åˆ†ä¹‹ä¹åä»¥ä¸Šï¼‰çš„å‚¨å±‚ç‰¹å¾åˆ†æã€ç”Ÿäº§é¢„æµ‹å’Œæ²¹äº•ä½ç½®ä¼˜åŒ–ç­‰ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰å¿«é€Ÿå“åº”æ—¶é—´å’Œé«˜åº¦çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚ç»æµåˆ†æè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½æ˜¾è‘—é™ä½æˆæœ¬å¹¶ç¼©çŸ­å›æŠ¥å‘¨æœŸã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½å¤„ç†å®æ—¶æ•°æ®æµã€æé«˜å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§å¹¶å‡å°‘ç¯å¢ƒäº‹æ•…ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å…ˆè¿›AIæŠ€æœ¯ä¸çŸ³æ²¹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç›¸ç»“åˆçš„å®é™…åº”ç”¨ï¼Œæœ‰åŠ©äºæé«˜è¿è¥æ•ˆç‡ã€å®‰å…¨æ€§å’Œç»æµæ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»¼åˆè¿ç”¨æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒçŸ³æ²¹å‚¨å±‚åˆ†æã€‚</li>
<li>åˆ©ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯å®ç°å¿«é€Ÿé€‚åº”æ²¹ç”°ç¯å¢ƒçš„èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡å¼é›†æˆå¤„ç†åœ°éœ‡ã€äº•æ—¥å¿—å’Œç”Ÿäº§æ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§ã€‚</li>
<li>æ¡†æ¶å…·æœ‰é«˜åº¦å‡†ç¡®æ€§å’Œæ€§èƒ½è¡¨ç°ï¼Œå¦‚å‚¨å±‚ç‰¹å¾åˆ†æã€ç”Ÿäº§é¢„æµ‹å’Œæ²¹äº•ä½ç½®ä¼˜åŒ–ç­‰ã€‚</li>
<li>å…·æœ‰å¿«é€Ÿå“åº”æ—¶é—´å’Œå‡ºè‰²çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½æ˜¾è‘—é™ä½è¿è¥æˆæœ¬å¹¶ç¼©çŸ­å›æŠ¥å‘¨æœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11376v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11376v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ANROT-HELANet-Adverserially-and-Naturally-Robust-Attention-Based-Aggregation-Network-via-The-Hellinger-Distance-for-Few-Shot-Classification"><a href="#ANROT-HELANet-Adverserially-and-Naturally-Robust-Attention-Based-Aggregation-Network-via-The-Hellinger-Distance-for-Few-Shot-Classification" class="headerlink" title="ANROT-HELANet: Adverserially and Naturally Robust Attention-Based   Aggregation Network via The Hellinger Distance for Few-Shot Classification"></a>ANROT-HELANet: Adverserially and Naturally Robust Attention-Based   Aggregation Network via The Hellinger Distance for Few-Shot Classification</h2><p><strong>Authors:Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu N. Duong</strong></p>
<p>Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\epsilon&#x3D;0.30$ and Gaussian noise up to $\sigma&#x3D;0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20% and 1.40% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANetâ€™s combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at <a target="_blank" rel="noopener" href="https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main">https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æŠ€æœ¯æ¶‰åŠä»…ä½¿ç”¨å°‘é‡æ•°æ®æ ·æœ¬è¿›è¡Œæ³›åŒ–å­¦ä¹ ï¼Œå·²ç»å±•ç°å‡ºæ¯”æ™®é€šCNNæ–¹æ³•æ›´æœ‰å‰æ™¯å’Œæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚è™½ç„¶åŸºäºè´å¶æ–¯ä¼°è®¡çš„ä½¿ç”¨Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦çš„æ–¹æ³•å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»å’Œè‡ªç„¶å™ªå£°çš„å½±å“ã€‚æˆ‘ä»¬ä»‹ç»äº†ANROT-HELANetï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹æŠ—æ€§å’Œè‡ªç„¶é²æ£’çš„Hellingerèšåˆç½‘ç»œï¼Œå®ƒæ˜¾è‘—åœ°æ¨è¿›äº†FSLçš„ç¨³å¥æ€§å’Œæ€§èƒ½çš„å‰æ²¿æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸€ç§å¯¹æŠ—æ€§å’Œè‡ªç„¶é²æ£’çš„åŸºäºHellingerè·ç¦»çš„ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œæ˜¾ç¤ºå‡ºå¯¹å¯¹æŠ—æ€§æ‰°åŠ¨é«˜è¾¾$\epsilon&#x3D;0.30$å’Œé«˜æ–¯å™ªå£°é«˜è¾¾$\sigma&#x3D;0.30$çš„éŸ§æ€§ã€‚è¯¥ç½‘ç»œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬åœ¨miniImageNetä¸Šçš„1æ¬¡å’Œ5æ¬¡åœºæ™¯åˆ†åˆ«æé«˜äº†1.20ï¼…å’Œ1.40ï¼…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå®ƒæ¨å¹¿äº†ç”¨äºå¯å˜å°‘é‡æ¨ç†åœºæ™¯çš„ä½™å¼¦ç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†ä¼˜è¶Šçš„å›¾åƒé‡å»ºè´¨é‡ï¼ŒFIDå¾—åˆ†ä¸º2.75ï¼Œä¼˜äºä¼ ç»Ÿçš„VAEï¼ˆ3.43ï¼‰å’ŒWAEï¼ˆ3.38ï¼‰æ–¹æ³•ã€‚åœ¨å››ä¸ªå°‘é‡åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®ï¼ŒANROT-HELANetç»“åˆäº†åŸºäºHellingerè·ç¦»çš„ç‰¹å¾èšåˆã€æ³¨æ„åŠ›æœºåˆ¶å’Œæˆ‘ä»¬çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œåœ¨ä¿æŒå¯¹æŠ—æ€§å’Œè‡ªç„¶æ‰°åŠ¨çš„ç¨³å¥æ€§çš„åŒæ—¶ï¼Œæ ‘ç«‹äº†æ–°çš„ä¸šç•Œæœ€ä½³æ€§èƒ½æ ‡å‡†ã€‚æˆ‘ä»¬çš„ä»£ç ä»“åº“å°†ä½äº<a target="_blank" rel="noopener" href="https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main%E3%80%82">https://github.com/GreedYLearner1146/ANROT-HELANet/tree/mainã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11220v1">PDF</a> Preprint version. The manuscript has been submitted to a journal. All   changes will be transferred to the final version if accepted. Also an   erratum: In Figure 10 and 11, the $\epsilon &#x3D; 0.005$ value should be   $\epsilon &#x3D; 0.05$</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ANROT-HELANetç½‘ç»œåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­çš„å“è¶Šè¡¨ç°ã€‚è¯¥ç½‘ç»œé‡‡ç”¨åŸºäºHellingerè·ç¦»çš„ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œå¯¹æŠ—æ€§æ‰°åŠ¨å’Œè‡ªç„¶å™ªå£°éƒ½æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äºå˜åˆ†å°‘æ ·æœ¬æ¨ç†åœºæ™¯ã€‚ANROT-HELANetè¿˜å…·æœ‰å‡ºè‰²çš„å›¾åƒé‡å»ºè´¨é‡ï¼Œä¸”å¯¹å¯¹æŠ—æ€§å’Œè‡ªç„¶æ‰°åŠ¨å…·æœ‰ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ANROT-HELANetç½‘ç»œåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç½‘ç»œé‡‡ç”¨åŸºäºHellingerè·ç¦»çš„ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œå¢å¼ºäº†å¯¹æŠ—æ€§å’Œè‡ªç„¶å™ªå£°çš„é²æ£’æ€§ã€‚</li>
<li>ANROT-HELANetåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨miniImageNetä¸Šçš„1-shotå’Œ5-shotåœºæ™¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äºå˜åˆ†å°‘æ ·æœ¬æ¨ç†åœºæ™¯ã€‚</li>
<li>ANROT-HELANetå…·æœ‰å‡ºè‰²çš„å›¾åƒé‡å»ºè´¨é‡ï¼ŒFIDåˆ†æ•°ä¸º2.75ï¼Œä¼˜äºä¼ ç»ŸVAEå’ŒWAEæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¯¹å››ä¸ªå°‘æ ·æœ¬åŸºå‡†æ•°æ®é›†çš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†ANROT-HELANetçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11220v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11220v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11220v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CCoMAML-Efficient-Cattle-Identification-Using-Cooperative-Model-Agnostic-Meta-Learning"><a href="#CCoMAML-Efficient-Cattle-Identification-Using-Cooperative-Model-Agnostic-Meta-Learning" class="headerlink" title="CCoMAML: Efficient Cattle Identification Using Cooperative   Model-Agnostic Meta-Learning"></a>CCoMAML: Efficient Cattle Identification Using Cooperative   Model-Agnostic Meta-Learning</h2><p><strong>Authors:Rabin Dulal, Lihong Zheng, Ashad Kabir</strong></p>
<p>Cattle identification is critical for efficient livestock farming management, currently reliant on radio-frequency identification (RFID) ear tags. However, RFID-based systems are prone to failure due to loss, damage, tampering, and vulnerability to external attacks. As a robust alternative, biometric identification using cattle muzzle patterns similar to human fingerprints has emerged as a promising solution. Deep learning techniques have demonstrated success in leveraging these unique patterns for accurate identification. But deep learning models face significant challenges, including limited data availability, disruptions during data collection, and dynamic herd compositions that require frequent model retraining. To address these limitations, this paper proposes a novel few-shot learning framework for real-time cattle identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This model offers great model adaptability to new data through efficient learning from few data samples without retraining. The proposed approach has been rigorously evaluated against current state-of-the-art few-shot learning techniques applied in cattle identification. Comprehensive experimental results demonstrate that our proposed CCoMAML with MHAFF has superior cattle identification performance with 98.46% and 97.91% F1 scores. </p>
<blockquote>
<p>ç‰²ç•œè¯†åˆ«å¯¹äºé«˜æ•ˆçš„ç•œç‰§ä¸šç®¡ç†è‡³å…³é‡è¦ï¼Œç›®å‰ä¸»è¦ä¾èµ–äºå°„é¢‘è¯†åˆ«ï¼ˆRFIDï¼‰è€³æ ‡ã€‚ç„¶è€Œï¼ŒåŸºäºRFIDçš„ç³»ç»Ÿå®¹æ˜“å› ä¸¢å¤±ã€æŸåã€å¹²æ‰°å’Œå¤–éƒ¨æ”»å‡»è€Œå‡ºç°æ•…éšœã€‚ä½œä¸ºä¸€ç§å¯é çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨ç‰²ç•œé¼»çº¹ï¼ˆç±»ä¼¼äºäººç±»æŒ‡çº¹ï¼‰è¿›è¡Œç”Ÿç‰©è¯†åˆ«å·²å´­éœ²å¤´è§’ï¼Œæˆä¸ºä¸€é¡¹å‰æ™¯å¹¿é˜”çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯å·²æˆåŠŸè¯æ˜å¯åˆ©ç”¨è¿™äº›ç‹¬ç‰¹å›¾æ¡ˆè¿›è¡Œå‡†ç¡®è¯†åˆ«ã€‚ä½†æ˜¯ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™ã€æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­çš„ä¸­æ–­ä»¥åŠåŠ¨æ€ç¾¤ä½“æ„æˆéœ€è¦é¢‘ç¹é‡æ–°è®­ç»ƒæ¨¡å‹ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå®æ—¶ç‰²ç•œè¯†åˆ«ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆä½œæ¨¡å‹ä¸å¯çŸ¥å…ƒå­¦ä¹ ï¼ˆCCoMAMLï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›ç‰¹å¾èåˆï¼ˆMHAFFï¼‰ä½œä¸ºç‰¹å¾æå–æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä»å°‘é‡æ•°æ®æ ·æœ¬ä¸­è¿›è¡Œé«˜æ•ˆå­¦ä¹ ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å¿«é€Ÿé€‚åº”æ–°æ•°æ®ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸å½“å‰ç”¨äºç‰²ç•œè¯†åˆ«çš„æœ€å…ˆè¿›çš„å°æ ·æœ¬å­¦ä¹ æŠ€æœ¯è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°æ¯”è¾ƒã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„CCoMAMLä¸MHAFFç›¸ç»“åˆï¼Œåœ¨ç‰²ç•œè¯†åˆ«æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ï¼ŒF1å¾—åˆ†è¾¾åˆ°98.46%å’Œ97.91%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç•œç‰§ä¸šä¸­ç‰²ç•œè¯†åˆ«çš„é‡è¦æ€§ï¼Œç›®å‰ä¸»è¦ä¾èµ–å°„é¢‘è¯†åˆ«ï¼ˆRFIDï¼‰è€³æ ‡ã€‚ç„¶è€Œï¼ŒRFIDç³»ç»Ÿæ˜“å‡ºç°æ•…éšœï¼Œå¦‚ä¸¢å¤±ã€æŸåã€å¹²æ‰°å’Œå¤–éƒ¨æ”»å‡»ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œé‡‡ç”¨ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯ï¼Œå¦‚åˆ©ç”¨ç‰›å£é¼»æ¨¡å¼ï¼ˆç±»ä¼¼äºäººç±»æŒ‡çº¹ï¼‰è¿›è¡Œè¯†åˆ«ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åˆ©ç”¨è¿™äº›ç‹¬ç‰¹æ¨¡å¼è¿›è¡Œå‡†ç¡®è¯†åˆ«æ–¹é¢å·²å–å¾—æˆåŠŸã€‚ä½†æ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´æ•°æ®æœ‰é™ã€æ”¶é›†æ•°æ®æ—¶çš„å¹²æ‰°ä»¥åŠç¾¤ä½“ç»„æˆåŠ¨æ€å˜åŒ–ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦é¢‘ç¹é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå°‘é‡æ•°æ®çš„åä½œæ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼ˆCCoMAMLï¼‰ç»“åˆå¤šå¤´æ³¨æ„åŠ›ç‰¹å¾èåˆï¼ˆMHAFFï¼‰æ¨¡å‹ç”¨äºå®æ—¶ç‰›åªè¯†åˆ«çš„åº”ç”¨æ¡†æ¶ã€‚è¯¥æ¨¡å‹å…·å¤‡é€‚åº”æ–°æ•°æ®çš„èƒ½åŠ›ï¼Œèƒ½ä»å°‘é‡æ•°æ®ä¸­é«˜æ•ˆå­¦ä¹ ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„CCoMAMLä¸MHAFFç»“åˆæ¨¡å‹åœ¨ç‰›åªè¯†åˆ«æ–¹é¢å…·æœ‰ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒF1åˆ†æ•°è¾¾åˆ°98.46%å’Œ97.91%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RFIDåœ¨ç‰²ç•œè¯†åˆ«ä¸­å­˜åœ¨é—®é¢˜ï¼Œå¦‚æ˜“ä¸¢å¤±ã€æŸåå’Œå—åˆ°å¤–éƒ¨æ”»å‡»ç­‰ç¼ºç‚¹ã€‚</li>
<li>ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯å¦‚ç‰›å£é¼»æ¨¡å¼è¯†åˆ«æ˜¯ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²æˆåŠŸåº”ç”¨äºåˆ©ç”¨ç‰›å£é¼»æ¨¡å¼è¿›è¡Œå‡†ç¡®è¯†åˆ«ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç‰›åªè¯†åˆ«é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™ã€æ”¶é›†æ•°æ®æ—¶çš„å¹²æ‰°å’Œç¾¤ä½“ç»„æˆçš„å˜åŒ–ç­‰ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ•°æ®çš„åä½œæ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼ˆCCoMAMLï¼‰ä¸å¤šå¤´æ³¨æ„åŠ›ç‰¹å¾èåˆï¼ˆMHAFFï¼‰æ¨¡å‹çš„æ–°çš„è§£å†³æ–¹æ¡ˆæ¥åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¨¡å‹å…·å¤‡é€‚åº”æ–°æ•°æ®çš„èƒ½åŠ›ï¼Œå¹¶èƒ½ä»å°‘é‡æ•°æ®ä¸­é«˜æ•ˆå­¦ä¹ ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11219v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-Advanced-Convolutional-Neural-Network-for-Bearing-Fault-Diagnosis-under-Limited-Data"><a href="#An-Advanced-Convolutional-Neural-Network-for-Bearing-Fault-Diagnosis-under-Limited-Data" class="headerlink" title="An Advanced Convolutional Neural Network for Bearing Fault Diagnosis   under Limited Data"></a>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis   under Limited Data</h2><p><strong>Authors:Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang</strong></p>
<p>In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32% on case western reserve university (CWRU) dataset and 10% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data. </p>
<blockquote>
<p>åœ¨è½´æ‰¿æ•…éšœè¯Šæ–­é¢†åŸŸï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•æœ€è¿‘å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºæˆæœ¬é«˜æˆ–éšç§æ‹…å¿§ï¼Œé«˜è´¨é‡æ ‡æ³¨æ•°æ®åœ¨ç°å®åœºæ™¯ä¸­éå¸¸ç¨€ç¼ºã€‚å°½ç®¡å°æ ·æœ¬å­¦ä¹ åœ¨è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ•°æ®å¢å¼ºæŠ€æœ¯ç»å¸¸é­å—æ¨¡å¼å´©æºƒï¼Œå¹¶äº§ç”Ÿä½è´¨é‡æ ·æœ¬ï¼Œæ— æ³•æ•è·è½´æ‰¿æ•…éšœæ¨¡å¼çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œå…·æœ‰å±€éƒ¨æ„Ÿå—é‡çš„ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æå–å¤æ‚æŒ¯åŠ¨ä¿¡å·çš„å…¨å±€ç‰¹å¾æ–¹é¢è¡¨ç°ä¸è¶³ã€‚å¦å¤–ï¼Œç°æœ‰æ–¹æ³•æ— æ³•å¯¹æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„å¤æ‚å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ•°æ®å¢å¼ºå’Œå¯¹æ¯”å‚…é‡Œå¶å·ç§¯æ¡†æ¶ï¼ˆDAC-FCFï¼‰ï¼Œç”¨äºåœ¨æœ‰é™æ•°æ®ä¸‹è¿›è¡Œè½´æ‰¿æ•…éšœè¯Šæ–­ã€‚é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ¡ä»¶ä¸€è‡´æ½œåœ¨è¡¨ç¤ºå’Œé‡å»ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCCLR-GANï¼‰ï¼Œä»¥ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨åŸºäºå¯¹æ¯”å­¦ä¹ çš„è”åˆä¼˜åŒ–æœºåˆ¶ï¼Œæ›´å¥½åœ°å¯¹å¯ç”¨è®­ç»ƒæ•°æ®ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸€ç»´å‚…é‡Œå¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D-FCNNï¼‰ï¼Œå®ç°å¯¹è¾“å…¥æ•°æ®çš„å…¨å±€æ„ŸçŸ¥ã€‚å®éªŒè¡¨æ˜ï¼ŒDAC-FCFå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨å‡¯æ–¯è¥¿å‚¨å¤§å­¦ï¼ˆCWRUï¼‰æ•°æ®é›†ä¸Šè¾ƒåŸºçº¿é«˜å‡º32%ï¼Œåœ¨è‡ªæ”¶é›†çš„è¯•éªŒå°ä¸Šé«˜å‡º10%ã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒè¯æ˜äº†æ‰€æå‡ºç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œæ‰€æå‡ºçš„DAC-FCFä¸ºæœ‰é™æ•°æ®ä¸‹çš„è½´æ‰¿æ•…éšœè¯Šæ–­æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11053v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ·±å­¦ä¹ å¹¿æ³›åº”ç”¨äºè½´æ‰¿æ•…éšœè¯Šæ–­é¢†åŸŸçš„åŒæ—¶ï¼Œå› æˆæœ¬é«˜å’Œéšç§é¡¾è™‘å¯¼è‡´é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºã€‚å°‘æ ·æœ¬å­¦ä¹ è™½å…·æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä»å­˜åœ¨å±€é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§å…ˆè¿›çš„æ•°æ®å¢å¼ºå’Œå¯¹æ¯”å‚…é‡Œå¶å·ç§¯æ¡†æ¶ï¼ˆDAC-FCFï¼‰ï¼Œç”¨äºæœ‰é™æ•°æ®ä¸‹çš„è½´æ‰¿æ•…éšœè¯Šæ–­ã€‚é€šè¿‡æ¡ä»¶ä¸€è‡´æ½œåœ¨è¡¨ç¤ºå’Œé‡å»ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCCLR-GANï¼‰ç”Ÿæˆæ›´å¤šæ ·åŒ–æ•°æ®ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–æœºåˆ¶å»ºæ¨¡æœ‰é™è®­ç»ƒæ•°æ®é—´çš„å…³ç³»ï¼Œå¹¶å¼•å…¥ä¸€ç»´å‚…é‡Œå¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D-FCNNï¼‰å®ç°å…¨å±€æ„ŸçŸ¥è¾“å…¥æ•°æ®ã€‚å®éªŒè¯æ˜ï¼ŒDAC-FCFåœ¨æ¡ˆä¾‹è¥¿éƒ¨å‚¨å¤‡å¤§å­¦æ•°æ®é›†ä¸Šè¾ƒåŸºçº¿æ–¹æ³•æé«˜äº†é«˜è¾¾32%ï¼Œåœ¨è‡ªå»ºè¯•éªŒå°ä¸Šæé«˜äº†çº¦10%ï¼Œå¹¿æ³›éªŒè¯è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®ƒä¸ºæœ‰é™æ•°æ®ä¸‹çš„è½´æ‰¿æ•…éšœè¯Šæ–­æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±å­¦ä¹ åœ¨è½´æ‰¿æ•…éšœè¯Šæ–­ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºã€‚</li>
<li>ç°æœ‰å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•åœ¨è½´æ‰¿æ•…éšœè¯Šæ–­é¢†åŸŸå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºDAC-FCFæ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºæŠ€æœ¯CCLR-GANã€å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–æœºåˆ¶åŠå…¨å±€æ„ŸçŸ¥çš„ä¸€ç»´å‚…é‡Œå¶å·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>å®éªŒè¯æ˜DAC-FCFåœ¨è½´æ‰¿æ•…éšœè¯Šæ–­ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11053v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11053v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.11053v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Interpretable-Benchmark-for-Clickbait-Detection-and-Tactic-Attribution"><a href="#An-Interpretable-Benchmark-for-Clickbait-Detection-and-Tactic-Attribution" class="headerlink" title="An Interpretable Benchmark for Clickbait Detection and Tactic   Attribution"></a>An Interpretable Benchmark for Clickbait Detection and Tactic   Attribution</h2><p><strong>Authors:Lihi Nofar, Tomer Portal, Aviv Elbaz, Alexander Apartsin, Yehudit Aperstein</strong></p>
<p>The proliferation of clickbait headlines poses significant challenges to the credibility of information and user trust in digital media. While recent advances in machine learning have improved the detection of manipulative content, the lack of explainability limits their practical adoption. This paper presents a model for explainable clickbait detection that not only identifies clickbait titles but also attributes them to specific linguistic manipulation strategies. We introduce a synthetic dataset generated by systematically augmenting real news headlines using a predefined catalogue of clickbait strategies. This dataset enables controlled experimentation and detailed analysis of model behaviour. We present a two-stage framework for automatic clickbait analysis comprising detection and tactic attribution. In the first stage, we compare a fine-tuned BERT classifier with large language models (LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot prompting and few-shot prompting enriched with illustrative clickbait headlines and their associated persuasive tactics. In the second stage, a dedicated BERT-based classifier predicts the specific clickbait strategies present in each headline. This work advances the development of transparent and trustworthy AI systems for combating manipulative media content. We share the dataset with the research community at <a target="_blank" rel="noopener" href="https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection">https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection</a> </p>
<blockquote>
<p>æ ‡é¢˜æ¬ºè¯ˆå†…å®¹çš„æ³›æ»¥å¯¹æ•°å­—åª’ä½“çš„ä¿¡æ¯å¯ä¿¡åº¦å’Œç”¨æˆ·ä¿¡ä»»åº¦æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»æé«˜äº†å¯¹æ“çºµæ€§å†…å®¹çš„æ£€æµ‹èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹è§£é‡Šæ€§ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ ‡é¢˜æ¬ºè¯ˆæ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…å¯ä»¥è¯†åˆ«æ¬ºè¯ˆæ ‡é¢˜ï¼Œè¿˜å¯ä»¥å°†å®ƒä»¬å½’å› äºç‰¹å®šçš„è¯­è¨€æ“çºµç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ä½¿ç”¨é¢„å®šä¹‰çš„æ ‡é¢˜æ¬ºè¯ˆç­–ç•¥ç›®å½•æ¥å¢å¼ºçœŸå®æ–°é—»æ ‡é¢˜ï¼Œä»¥ç”Ÿæˆè¯¥æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†èƒ½å¤Ÿè¿›è¡Œå—æ§å®éªŒå¹¶å¯¹æ¨¡å‹è¡Œä¸ºè¿›è¡Œè¯¦ç»†åˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨æ ‡é¢˜æ¬ºè¯ˆåˆ†æçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬æ£€æµ‹å’Œç­–ç•¥å½’å› ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å°†å¾®è°ƒè¿‡çš„BERTåˆ†ç±»å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œç‰¹åˆ«æ˜¯GPT-4.0å’ŒGemini 2.4 Flashï¼Œåœ¨é›¶æ ·æœ¬æç¤ºå’Œé€šè¿‡æ’å…¥å…·æœ‰ä»£è¡¨æ€§çš„æ¬ºè¯ˆæ ‡é¢˜åŠå…¶ç›¸å…³è¯´æœç­–ç•¥çš„å°‘é‡æ ·æœ¬æç¤ºä¸‹è¿›è¡Œæ¯”è¾ƒã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä¸€ä¸ªä¸“é—¨çš„åŸºäºBERTçš„åˆ†ç±»å™¨é¢„æµ‹æ¯ä¸ªæ ‡é¢˜ä¸­å­˜åœ¨çš„ç‰¹å®šæ¬ºè¯ˆç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†é€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿåœ¨æ‰“å‡»æ“çºµæ€§åª’ä½“å†…å®¹æ–¹é¢çš„å‘å±•ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection%E4%B8%8A%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%95%8C%E5%85%B1%E4%BA%AB%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/LLM-HITCS25S/ClickbaitTacticsDetectionä¸Šä¸ç ”ç©¶ç•Œå…±äº«è¯¥æ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10937v1">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ç‚¹å‡»è¯±é¥µæ£€æµ‹æ¨¡å‹ï¼Œç”¨äºæ£€æµ‹æ•°å­—åŒ–åª’ä½“ä¸­çš„æ“çºµæ€§æ ‡é¢˜å†…å®¹å¹¶æŒ‡å‡ºå…¶æ‰€ä¾èµ–çš„ç‰¹å®šè¯­è¨€æ“çºµç­–ç•¥ã€‚æ¨¡å‹æ„å»ºåœ¨ä¸€ä¸ªåˆæˆæ•°æ®é›†ä¸Šï¼Œè¯¥æ•°æ®é›†é€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨çœŸå®æ–°é—»æ ‡é¢˜å’Œé¢„è®¾çš„ç‚¹å‡»è¯±é¥µç­–ç•¥ç›®å½•æ¥ç”Ÿæˆã€‚ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªåŠ¨ç‚¹å‡»è¯±é¥µåˆ†ææ¡†æ¶ï¼ŒåŒ…æ‹¬æ£€æµ‹å’Œç­–ç•¥å½’å› ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¯¹æ¯”äº†å¾®è°ƒåçš„BERTåˆ†ç±»å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4.0å’ŒGemini 2.4 Flashï¼‰åœ¨é›¶æ ·æœ¬æç¤ºå’ŒåŒ…å«å…¸å‹ç‚¹å‡»è¯±é¥µæ ‡é¢˜åŠå…¶ç›¸å…³è¯´æœç­–ç•¥çš„å°‘é‡æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨ä¸“é—¨çš„BERTåˆ†ç±»å™¨é¢„æµ‹æ¯ä¸ªæ ‡é¢˜ä¸­ç‰¹å®šçš„ç‚¹å‡»è¯±é¥µç­–ç•¥ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºå¼€å‘é€æ˜ä¸”å¯ä¿¡èµ–çš„AIç³»ç»Ÿæ¥å¯¹æŠ—æ“çºµæ€§åª’ä½“å†…å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‚¹å‡»è¯±é¥µæ ‡é¢˜å¯¹ä¿¡æ¯å¯ä¿¡åº¦å’Œç”¨æˆ·å¯¹æ•°å­—åª’ä½“çš„ä¿¡ä»»åº¦æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>å°½ç®¡æœºå™¨å­¦ä¹ åœ¨æ£€æµ‹æ“çºµæ€§å†…å®¹ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªåŠ¨ç‚¹å‡»è¯±é¥µåˆ†ææ¡†æ¶ï¼ŒåŒ…æ‹¬æ£€æµ‹å’Œç­–ç•¥å½’å› é˜¶æ®µã€‚</li>
<li>åˆæˆæ•°æ®é›†é€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨çœŸå®æ–°é—»æ ‡é¢˜å’Œé¢„è®¾çš„ç‚¹å‡»è¯±é¥µç­–ç•¥ç”Ÿæˆï¼Œä¿ƒè¿›æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶å®éªŒå’Œè¯¦ç»†åˆ†æã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ä¸åŒæ¨¡å‹ï¼ˆå¦‚BERTåˆ†ç±»å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨ç‚¹å‡»è¯±é¥µæ£€æµ‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>ä¸“é—¨çš„BERTåˆ†ç±»å™¨ç”¨äºé¢„æµ‹æ¯ä¸ªæ ‡é¢˜ä¸­ç‰¹å®šçš„ç‚¹å‡»è¯±é¥µç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.10937v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.10937v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.10937v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.10937v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.10937v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models"><a href="#Two-Sides-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models" class="headerlink" title="Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models"></a>Two Sides of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models</h2><p><strong>Authors:Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang</strong></p>
<p>Graph foundation models, inspired by the success of LLMs, are designed to learn the optimal embedding from multi-domain TAGs for the downstream cross-task generalization capability. During our investigation, graph VQ-MAE stands out among the increasingly diverse landscape of GFM architectures. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios. </p>
<blockquote>
<p>å›¾æ¨¡å‹åŸºç¡€ï¼Œå—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆåŠŸçš„å¯å‘ï¼Œæ—¨åœ¨ä»å¤šåŸŸæ ‡ç­¾ï¼ˆTAGsï¼‰å­¦ä¹ æœ€ä½³åµŒå…¥ï¼Œä»¥è·å¾—ä¸‹æ¸¸è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„è°ƒæŸ¥ä¸­ï¼Œå›¾VQ-MAEåœ¨æ—¥ç›Šå¤šæ ·åŒ–çš„GFMæ¶æ„ä¸­è„±é¢–è€Œå‡ºã€‚è¿™å½’åŠŸäºå®ƒå°†å¤šä¸ªé¢†åŸŸçš„æ‹“æ‰‘å’Œæ–‡æœ¬å±æ€§è”åˆç¼–ç åˆ°å…·æœ‰æ¸…æ™°è¯­ä¹‰è¾¹ç•Œçš„ç¦»æ•£åµŒå…¥ç©ºé—´ä¸­çš„èƒ½åŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œé¢†åŸŸæ³›åŒ–å†²çªä¼šå¯¼è‡´ä¸€äº›ä¸æ˜“å¯Ÿè§‰çš„é™·é˜±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸¤ä¸ªé™·é˜±ï¼Œå®ƒä»¬å°±åƒæ˜¯GFMä¼˜åŒ–çš„åŒä¸€æšç¡¬å¸çš„ä¸¤é¢â€”â€”ç¬¬ä¸€é¢æ¨¡å‹é€€åŒ–ï¼šç¼–ç å™¨å’Œä»£ç æœ¬æ— æ³•æ•æ‰è¾“å…¥çš„å¤šæ ·æ€§ï¼›ç¬¬äºŒé¢è¡¨ç¤ºå´©æºƒï¼šéšè—åµŒå…¥å’Œä»£ç æœ¬å‘é‡ç”±äºç‹­çª„è¡¨ç¤ºå­ç©ºé—´çš„çº¦æŸè€Œæ— æ³•ä¿æŒè¯­ä¹‰å¯åˆ†æ€§ã€‚è¿™ä¸¤ä¸ªé™·é˜±ï¼ˆä¾§é¢ï¼‰å…±åŒæŸå®³äº†è§£ç å™¨ï¼Œå¹¶äº§ç”Ÿä½è´¨é‡é‡å»ºçš„ç›‘ç£ï¼Œå¯¼è‡´é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„GFMä¼˜åŒ–å›°å¢ƒï¼ˆç¡¬å¸ï¼‰ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°æŒ‘æˆ˜å½’ç»“ä¸ºä¿¡æ¯ç“¶é¢ˆå’Œæ­£åˆ™åŒ–ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoTï¼ˆæ··åˆå¾®è°ƒå™¨ï¼‰â€”â€”ï¼ˆ1ï¼‰é’ˆå¯¹ä¸¤ä¸ªé™·é˜±çš„ä¿¡æ¯å¾®è°ƒå™¨ï¼Œå®ƒåˆ©ç”¨è¾¹ç¼˜è¯­ä¹‰èåˆç­–ç•¥å’Œå¸¦æœ‰åŸŸæ„ŸçŸ¥è·¯ç”±çš„æ··åˆä»£ç æœ¬ï¼Œä»¥æé«˜ä¿¡æ¯å®¹é‡ã€‚ï¼ˆ2ï¼‰é’ˆå¯¹ä¼˜åŒ–ç¡¬å¸çš„æ­£åˆ™åŒ–å¾®è°ƒå™¨ï¼Œå®ƒåˆ©ç”¨ä¸¤ç§é¢å¤–çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æˆ‘ä»¬æå‡ºçš„ä¿¡æ¯å¾®è°ƒå™¨ä¸­çš„æ¢¯åº¦ç›‘ç£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œä¸ºä¸€ä¸ªçµæ´»çš„æ¶æ„ï¼ŒMoTéµå¾ªGFMçš„æ‰©å±•å®šå¾‹ï¼Œæä¾›äº†å¯æ§çš„æ¨¡å‹è§„æ¨¡ã€‚åœ¨6ä¸ªé¢†åŸŸçš„22ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒMoTåœ¨ç›‘ç£ã€å°æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08401v3">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åŸºç¡€æ¨¡å‹å€Ÿé‰´äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸç»éªŒï¼Œæ—¨åœ¨ä»å¤šåŸŸæ ‡ç­¾ä¸­å­¦ä¹ æœ€ä½³åµŒå…¥ï¼Œä»¥å®ç°ä¸‹æ¸¸è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œå›¾VQ-MAEåœ¨ä¼—å¤šå›¾åŸºç¡€æ¨¡å‹æ¶æ„ä¸­è„±é¢–è€Œå‡ºã€‚è¿™å¾—ç›Šäºå…¶èƒ½å¤Ÿè”åˆç¼–ç å¤šä¸ªé¢†åŸŸçš„æ‹“æ‰‘å’Œæ–‡æœ¬å±æ€§åˆ°å…·æœ‰æ¸…æ™°è¯­ä¹‰è¾¹ç•Œçš„ç¦»æ•£åµŒå…¥ç©ºé—´çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé¢†åŸŸæ³›åŒ–å†²çªä¼šå¼•å‘ä¸€äº›éš¾ä»¥å¯Ÿè§‰çš„é—®é¢˜ï¼Œæœ¬æ–‡å®ä¾‹åŒ–äº†ä¸¤ä¸ªè¿™æ ·çš„é—®é¢˜ï¼Œå®ƒä»¬å°±åƒæ˜¯å›¾åŸºç¡€æ¨¡å‹ä¼˜åŒ–çš„åŒä¸€æšç¡¬å¸çš„ä¸¤é¢â€”â€”æ¨¡å‹é€€åŒ–ï¼šç¼–ç å™¨å’Œä»£ç æœ¬æ— æ³•æ•æ‰è¾“å…¥çš„å¤šæ ·æ€§ï¼›è¡¨ç¤ºå´©æºƒï¼šéšè—åµŒå…¥å’Œä»£ç æœ¬å‘é‡ç”±äºç‹­çª„çš„è¡¨ç¤ºå­ç©ºé—´çš„çº¦æŸè€Œæ— æ³•ä¿æŒè¯­ä¹‰å¯åˆ†æ€§ã€‚è¿™ä¸¤ä¸ªé—®é¢˜å…±åŒå½±å“è§£ç å™¨ï¼Œç”Ÿæˆä½è´¨é‡çš„é‡æ„ç›‘ç£ï¼Œé€ æˆé¢„è®­ç»ƒæ—¶çš„å›¾åŸºç¡€æ¨¡å‹ä¼˜åŒ–å›°å¢ƒï¼ˆâ€œç¡¬å¸â€ï¼‰ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†æ··åˆæ•²æ‰“è€…ï¼ˆMoTï¼‰æ¶æ„ã€‚å…¶é‡‡ç”¨è¾¹ç¼˜è¯­ä¹‰èåˆç­–ç•¥å’Œæ··åˆä»£ç æœ¬è¿›è¡Œä¿¡æ¯æ‰©å®¹å’Œæ”¹è¿›ä¼˜åŒ–çš„ç›‘ç®¡æªæ–½ç­‰ç­–ç•¥ã€‚ä½œä¸ºçµæ´»æ¶æ„ï¼ŒMoTéµå¾ªå›¾åŸºç¡€æ¨¡å‹çš„è§„æ¨¡å®šå¾‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢†åŸŸä¸Šå®ç°äº†æ˜¾è‘—ä¼˜äºå½“å‰æœ€ä¼˜æ°´å¹³çš„æ”¹è¿›ï¼Œæ— è®ºåœ¨ç›‘ç£å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åŸºç¡€æ¨¡å‹å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸç»éªŒï¼Œæ—¨åœ¨å­¦ä¹ å¤šåŸŸæ ‡ç­¾çš„æœ€ä½³åµŒå…¥ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å›¾VQ-MAEåœ¨å¤šç§å›¾åŸºç¡€æ¨¡å‹æ¶æ„ä¸­è¡¨ç°çªå‡ºï¼Œèƒ½å¤Ÿè”åˆç¼–ç æ‹“æ‰‘å’Œæ–‡æœ¬å±æ€§åˆ°å…·æœ‰æ¸…æ™°è¯­ä¹‰è¾¹ç•Œçš„ç¦»æ•£åµŒå…¥ç©ºé—´ã€‚</li>
<li>é¢†åŸŸæ³›åŒ–å†²çªä¼šå¯¼è‡´éš¾ä»¥å¯Ÿè§‰çš„é—®é¢˜ï¼Œå¦‚æ¨¡å‹é€€åŒ–å’Œè¡¨ç¤ºå´©æºƒç­‰æŒ‘æˆ˜ã€‚è¿™äº›é—®é¢˜ä¼šå…±åŒå½±å“è§£ç å™¨æ€§èƒ½å¹¶å½±å“é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›¾åŸºç¡€æ¨¡å‹ä¼˜åŒ–ã€‚è¿™äº›é—®é¢˜è¢«è§†ä¸ºå›¾åŸºç¡€æ¨¡å‹ä¼˜åŒ–çš„å›°å¢ƒï¼ˆâ€œç¡¬å¸â€ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.08401v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.08401v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.08401v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2509.08401v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enkidu-Universal-Frequential-Perturbation-for-Real-Time-Audio-Privacy-Protection-against-Voice-Deepfakes"><a href="#Enkidu-Universal-Frequential-Perturbation-for-Real-Time-Audio-Privacy-Protection-against-Voice-Deepfakes" class="headerlink" title="Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy   Protection against Voice Deepfakes"></a>Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy   Protection against Voice Deepfakes</h2><p><strong>Authors:Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, Tianyu Du, Shouling Ji</strong></p>
<p>The rapid advancement of voice deepfake technologies has raised serious concerns about user audio privacy, as attackers increasingly exploit publicly available voice data to generate convincing fake audio for malicious purposes such as identity theft, financial fraud, and misinformation campaigns. While existing defense methods offer partial protection, they face critical limitations, including weak adaptability to unseen user data, poor scalability to long audio, rigid reliance on white-box knowledge, and high computational and temporal costs during the encryption process. To address these challenges and defend against personalized voice deepfake threats, we propose Enkidu, a novel user-oriented privacy-preserving framework that leverages universal frequential perturbations generated through black-box knowledge and few-shot training on a small amount of user data. These highly malleable frequency-domain noise patches enable real-time, lightweight protection with strong generalization across variable-length audio and robust resistance to voice deepfake attacks, all while preserving perceptual quality and speech intelligibility. Notably, Enkidu achieves over 50 to 200 times processing memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime efficiency (real-time coefficient as low as 0.004) compared to six state-of-the-art countermeasures. Extensive experiments across six mainstream text-to-speech models and five cutting-edge automated speaker verification models demonstrate the effectiveness, transferability, and practicality of Enkidu in defending against both vanilla and adaptive voice deepfake attacks. Our code is currently available. </p>
<blockquote>
<p>éšç€è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”¨æˆ·éŸ³é¢‘éšç§å¼•èµ·äº†ä¸¥é‡å…³æ³¨ã€‚æ”»å‡»è€…è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å…¬å¼€å¯ç”¨çš„è¯­éŸ³æ•°æ®ç”Ÿæˆä»¤äººä¿¡æœçš„è™šå‡éŸ³é¢‘ï¼Œç”¨äºèº«ä»½ç›—çªƒã€é‡‘èæ¬ºè¯ˆå’Œè™šå‡ä¿¡æ¯å®£ä¼ ç­‰æ¶æ„ç›®çš„ã€‚å°½ç®¡ç°æœ‰çš„é˜²å¾¡æ–¹æ³•æä¾›äº†éƒ¨åˆ†ä¿æŠ¤ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å…³é”®çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹æ–°ç”¨æˆ·æ•°æ®çš„é€‚åº”èƒ½åŠ›å¼±ã€å¯¹é•¿éŸ³é¢‘çš„å¯æ‰©å±•æ€§å·®ã€å¯¹ç™½ç›’çŸ¥è¯†çš„åˆšæ€§ä¾èµ–ä»¥åŠåŠ å¯†è¿‡ç¨‹ä¸­çš„è®¡ç®—å’Œæ—¶é—´æˆæœ¬é«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶é˜²èŒƒä¸ªæ€§åŒ–çš„è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒï¼Œæˆ‘ä»¬æå‡ºäº†Enkiduï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ç”¨æˆ·å¯¼å‘çš„éšç§ä¿æŠ¤æ¡†æ¶ã€‚å®ƒåˆ©ç”¨é€šè¿‡é»‘ç›’çŸ¥è¯†å’Œå°‘é‡ç”¨æˆ·æ•°æ®è¿›è¡Œçš„å°‘é‡è®­ç»ƒç”Ÿæˆé€šç”¨é¢‘ç‡æ‰°åŠ¨ã€‚è¿™äº›é«˜åº¦çµæ´»çš„é¢‘åŸŸå™ªå£°å—å®ç°äº†å®æ—¶ã€è½»é‡çº§çš„ä¿æŠ¤ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯è·¨å˜é•¿éŸ³é¢‘è¿è¡Œï¼Œå¹¶å…·æœ‰å¾ˆå¼ºçš„æŠµæŠ—è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†æ„ŸçŸ¥è´¨é‡å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸å…­ç§æœ€å…ˆè¿›çš„å¯¹ç­–ç›¸æ¯”ï¼ŒEnkiduå®ç°äº†é«˜è¾¾50è‡³200å€çš„å†…å­˜å¤„ç†æ•ˆç‡ï¼ˆä½è‡³0.004åƒå…†å­—èŠ‚ï¼‰å’Œ3è‡³7000å€çš„è¿è¡Œæ—¶é—´æ•ˆç‡ï¼ˆå®æ—¶ç³»æ•°ä½è‡³0.004ï¼‰ã€‚åœ¨å…­ä¸ªä¸»æµæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹å’Œäº”ä¸ªå…ˆè¿›çš„è‡ªåŠ¨è¯´è¯äººéªŒè¯æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†Enkiduåœ¨é˜²èŒƒå¸¸è§„å’Œè‡ªé€‚åº”è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€å¯è¿ç§»æ€§å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ç›®å‰å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12932v2">PDF</a> Accepted by ACM MM 2025, Open-sourced</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ‰€å¸¦æ¥çš„ç”¨æˆ·éŸ³é¢‘éšç§ä¿æŠ¤é—®é¢˜ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™æ€§ï¼Œå¦‚éš¾ä»¥é€‚åº”æœªè§è¿‡çš„ç”¨æˆ·æ•°æ®ã€éš¾ä»¥å¤„ç†é•¿éŸ³é¢‘ã€è¿‡äºä¾èµ–ç™½ç›’çŸ¥è¯†ã€åŠ å¯†è¿‡ç¨‹ä¸­çš„è®¡ç®—å’Œæ—¶é—´æˆæœ¬é«˜æ˜‚ç­‰ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç”¨æˆ·å¯¼å‘çš„éšç§ä¿æŠ¤æ¡†æ¶Enkiduï¼Œåˆ©ç”¨é€šè¿‡é»‘ç›’çŸ¥è¯†å’Œå°‘é‡ç”¨æˆ·æ•°æ®è¿›è¡Œçš„å°‘é‡è®­ç»ƒç”Ÿæˆé€šç”¨é¢‘åŸŸæ‰°åŠ¨ã€‚è¿™äº›é«˜åº¦çµæ´»çš„é¢‘åŸŸå™ªå£°è¡¥ä¸å¯å®ç°å®æ—¶ã€è½»é‡çº§çš„ä¿æŠ¤ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’ŒæŠµæŠ—è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæ„ŸçŸ¥è´¨é‡å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ä¸å…­ç§æœ€å…ˆè¿›çš„å¯¹ç­–ç›¸æ¯”ï¼ŒEnkiduåœ¨å¤„ç†å†…å­˜æ–¹é¢å®ç°äº†é«˜è¾¾50è‡³200å€çš„æ•ˆç‡ï¼Œå¹¶åœ¨è¿è¡Œæ—¶æ•ˆç‡æ–¹é¢å®ç°äº†é«˜è¾¾3è‡³7000å€çš„æå‡ã€‚å®éªŒè¯æ˜ï¼ŒEnkiduåœ¨æŠµå¾¡æ™®é€šå’Œè‡ªé€‚åº”è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»æ–¹é¢å‡æœ‰æ•ˆã€å¯è½¬ç§»å¹¶å…·æœ‰å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å¯¹ç”¨æˆ·éŸ³é¢‘éšç§çš„å…³æ³¨ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æ–¹æ³•å­˜åœ¨é€‚åº”æœªè§ç”¨æˆ·æ•°æ®èƒ½åŠ›å·®ã€éš¾ä»¥å¤„ç†é•¿éŸ³é¢‘ã€ä¾èµ–ç™½ç›’çŸ¥è¯†ç­‰å±€é™æ€§ã€‚</li>
<li>Enkiduæ¡†æ¶åˆ©ç”¨é»‘ç›’çŸ¥è¯†å’Œå°‘é‡è®­ç»ƒæ•°æ®ç”Ÿæˆé€šç”¨é¢‘åŸŸæ‰°åŠ¨ã€‚</li>
<li>Enkiduå®ç°å®æ—¶ã€è½»é‡çº§ä¿æŠ¤ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œèƒ½æœ‰æ•ˆæŠµæŠ—è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»ã€‚</li>
<li>Enkiduåœ¨å¤„ç†å†…å­˜å’Œè¿è¡Œæ—¶æ•ˆç‡æ–¹é¢è¾ƒç°æœ‰æ–¹æ³•æ˜¾è‘—æå‡ã€‚</li>
<li>Enkiduåœ¨æŠµå¾¡ä¸åŒç±»å‹è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»æ—¶å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2507.12932v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Diffusion-Duality"><a href="#The-Diffusion-Duality" class="headerlink" title="The Diffusion Duality"></a>The Diffusion Duality</h2><p><strong>Authors:Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov</strong></p>
<p>Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: <a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a> </p>
<blockquote>
<p>å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹å› å…¶å›ºæœ‰çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›è€Œå…·æœ‰å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸è¢«è‡ªå›å½’æ¨¡å‹å’Œæ©è†œæ‰©æ•£æ¨¡å‹æ‰€è¶…è¶Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨ä¸€ä¸ªå…³é”®è§è§£æ¥ç¼©å°è¿™ä¸€æ€§èƒ½å·®è·ï¼šå‡åŒ€çŠ¶æ€æ‰©æ•£è¿‡ç¨‹è‡ªç„¶äº§ç”Ÿäºåº•å±‚çš„é«˜æ–¯æ‰©æ•£ã€‚æˆ‘ä»¬çš„æ–¹æ³•Duoï¼Œå°†é«˜æ–¯æ‰©æ•£çš„å¼ºå¤§æŠ€æœ¯è½¬ç§»åˆ°æ”¹è¿›è®­ç»ƒå’Œé‡‡æ ·ä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±é«˜æ–¯è¿‡ç¨‹å¼•å¯¼çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡å°‘æ–¹å·®ä½¿è®­ç»ƒé€Ÿåº¦ç¿»å€ã€‚ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒçš„æ¨¡å‹åœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„3ä¸ªä¸Šå®ç°äº†é›¶å¯åŠ¨å›°æƒ‘åº¦è¶…è¿‡è‡ªå›å½’æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦ï¼ˆDiscrete Consistency Distillationï¼‰ï¼Œè¯¥ç®—æ³•å°†ä¸€è‡´æ€§è’¸é¦ä»è¿ç»­ç¯å¢ƒé€‚é…åˆ°ç¦»æ•£ç¯å¢ƒã€‚è¿™ä¸€ç®—æ³•é€šè¿‡åŠ é€Ÿé‡‡æ ·é€Ÿåº¦ä¸¤ä¸ªæ•°é‡çº§ï¼Œå®ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å‡ æ­¥ç”Ÿæˆã€‚æˆ‘ä»¬å·²åœ¨é¡¹ç›®é¡µé¢æä¾›ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼š<a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10892v2">PDF</a> ICML 2025. We provide the code at: <a target="_blank" rel="noopener" href="https://github.com/s-sahoo/duo">https://github.com/s-sahoo/duo</a>   [v2]: Camera ready revisions</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å€Ÿé‰´é«˜æ–¯æ‰©æ•£è¿‡ç¨‹ä¸­çš„å…³é”®æŠ€æœ¯æ¥æå‡å…¶æ€§èƒ½ã€‚å¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå‡å°‘è®­ç»ƒæ–¹å·®ï¼ŒåŠ é€Ÿè®­ç»ƒé€Ÿåº¦ã€‚åŒæ—¶ï¼Œæå‡ºç¦»æ•£ä¸€è‡´æ€§è’¸é¦ç®—æ³•ï¼Œå°†ä¸€è‡´æ€§è’¸é¦ä»è¿ç»­ç¯å¢ƒè¿ç§»åˆ°ç¦»æ•£ç¯å¢ƒï¼Œå®ç°åœ¨æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„å°‘æ­¥ç”Ÿæˆï¼Œå¹¶å¤§å¹…åº¦åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•å…·æœ‰è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼Œå¯å®ç°å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>é«˜æ–¯æ‰©æ•£è¿‡ç¨‹ä¸­çš„å…³é”®æŠ€æœ¯è¢«ç”¨æ¥æå‡å‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡å°‘è®­ç»ƒæ–¹å·®æ¥åŠ é€Ÿè®­ç»ƒé€Ÿåº¦ã€‚</li>
<li>åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ‰ä¸‰ä¸ªä¸Šï¼Œä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬å›°æƒ‘åº¦ä¸Šè¶…è¿‡äº†è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>æå‡ºç¦»æ•£ä¸€è‡´æ€§è’¸é¦ç®—æ³•ï¼Œå®ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„å°‘æ­¥ç”Ÿæˆã€‚</li>
<li>ç¦»æ•£ä¸€è‡´æ€§è’¸é¦ç®—æ³•å¯ä»¥å¤§å¹…åº¦åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2506.10892v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2506.10892v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation"><a href="#Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation" class="headerlink" title="Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation"></a>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation</h2><p><strong>Authors:T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough</strong></p>
<p>Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication. </p>
<blockquote>
<p>åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­ç»å¸¸å¯ä»¥å‘ç°å«ä¹‰æ¨¡ç³Šçš„è¯æ±‡ã€‚ç”±äºæ•°æ®æœ‰é™ï¼Œè¯æ±‡çš„æ¨¡ç³Šæ€§ç»™ä¼ ç»Ÿçš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å—åˆ°äº†è¿™äº›é™åˆ¶çš„å½±å“ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¹å–„è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰çš„æ–¹æ³•ï¼Œé‡‡ç”¨ä¸€ç§æ–°é¢–çš„ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒçŸ¥è¯†åº“ï¼ˆKBï¼‰çš„æ–¹æ³•ï¼ŒçŸ¥è¯†åº“ç”±ä¸åŒçš„è¯ä¹‰è§£é‡Šæ„æˆã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§äººæœºç»“åˆçš„æç¤ºå¢å¼ºæ–¹æ³•ï¼Œå…¶ä¸­æç¤ºç”±è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºç»„æˆï¼Œä»¥å¼•å¯¼è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå°‘é‡æç¤ºçš„æ€è€ƒé“¾ï¼ˆCOTï¼‰æç¤ºæ–¹æ³•ï¼Œè¿™é¡¹å·¥ä½œåœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯„ä¼°å·¥ä½œä½¿ç”¨äº†FEWSæµ‹è¯•æ•°æ®å’Œè¯ä¹‰æ ‡ç­¾ã€‚æœ¬ç ”ç©¶æ¨åŠ¨äº†ç¤¾äº¤åª’ä½“å’Œæ•°å­—é€šä¿¡ä¸­çš„å‡†ç¡®è¯æ±‡è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18337v5">PDF</a> 12 pages,6 tables, 1 figure, Proceedings of the 1st International   Conference on NLP &amp; AI for Cyber Security</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£è¯»çš„çŸ¥è¯†åº“ï¼ˆKBï¼‰æ¥æ”¹å–„è¯æ±‡æ„ŸçŸ¥æ¶ˆæ­§ï¼ˆWSDï¼‰çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äººæœºç»“åˆçš„æ–¹å¼è¿›è¡Œæç¤ºå¢å¼ºï¼Œå€ŸåŠ©è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºæ¥å¼•å¯¼LLMã€‚é€šè¿‡é‡‡ç”¨åŸºäºå°‘é‡æ€ç»´çš„é“¾å¼æ€è€ƒï¼ˆCOTï¼‰æç¤ºæ–¹æ³•ï¼Œè¯¥ç ”ç©¶åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯æ±‡æ¨¡ç³Šåœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹ä¼ ç»Ÿçš„è¯æ±‡æ„ŸçŸ¥æ¶ˆæ­§ï¼ˆWSDï¼‰æ–¹æ³•æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç”±äºæ•°æ®æœ‰é™ï¼Œç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å—åˆ°å½±å“ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³è¯æ±‡æ¨¡ç³Šé—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£è¯»çš„çŸ¥è¯†åº“èƒ½æ”¹å–„WSDã€‚</li>
<li>äººæœºç»“åˆçš„æ–¹å¼è¿›è¡Œæç¤ºå¢å¼ºï¼Œå€ŸåŠ©è¯æ€§æ ‡æ³¨ã€åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºå¼•å¯¼LLMã€‚</li>
<li>é‡‡ç”¨åŸºäºChain of Thoughtï¼ˆCOTï¼‰çš„å°‘é‡æç¤ºæ–¹æ³•ï¼Œåœ¨æ€§èƒ½ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.18337v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.18337v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.18337v5/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.18337v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.18337v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Step-wise-Distribution-Alignment-Guided-Style-Prompt-Tuning-for-Source-free-Cross-domain-Few-shot-Learning"><a href="#Step-wise-Distribution-Alignment-Guided-Style-Prompt-Tuning-for-Source-free-Cross-domain-Few-shot-Learning" class="headerlink" title="Step-wise Distribution Alignment Guided Style Prompt Tuning for   Source-free Cross-domain Few-shot Learning"></a>Step-wise Distribution Alignment Guided Style Prompt Tuning for   Source-free Cross-domain Few-shot Learning</h2><p><strong>Authors:Huali Xu, Li Liu, Tianpeng Liu, Shuaifeng Zhi, Shuzhou Sun, Ming-Ming Cheng</strong></p>
<p>Existing cross-domain few-shot learning (CDFSL) methods, which develop source-domain training strategies to enhance model transferability, face challenges with large-scale pre-trained models (LMs) due to inaccessible source data and training strategies. Moreover, fine-tuning LMs for CDFSL demands substantial computational resources, limiting practicality. This paper addresses the source-free CDFSL (SF-CDFSL) problem, tackling few-shot learning (FSL) in the target domain using only pre-trained models and a few target samples without source data or strategies. To overcome the challenge of inaccessible source data, this paper introduces Step-wise Distribution Alignment Guided Style Prompt Tuning (StepSPT), which implicitly narrows domain gaps through prediction distribution optimization. StepSPT proposes a style prompt to align target samples with the desired distribution and adopts a dual-phase optimization process. In the external process, a step-wise distribution alignment strategy factorizes prediction distribution optimization into a multi-step alignment problem to tune the style prompt. In the internal process, the classifier is updated using standard cross-entropy loss. Evaluations on five datasets demonstrate that StepSPT outperforms existing prompt tuning-based methods and SOTAs. Ablation studies further verify its effectiveness. Code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/xuhuali-mxj/StepSPT">https://github.com/xuhuali-mxj/StepSPT</a>. </p>
<blockquote>
<p>ç°æœ‰è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰æ–¹æ³•é€šè¿‡å¼€å‘æºåŸŸè®­ç»ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ï¼Œä½†ç”±äºæ— æ³•è®¿é—®æºæ•°æ®ä»¥åŠè®­ç»ƒç­–ç•¥ï¼Œé¢ä¸´ç€å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆLMsï¼‰çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œä¸ºCDFSLå¾®è°ƒLMéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬æ–‡è§£å†³äº†æ— æºCDFSLï¼ˆSF-CDFSLï¼‰é—®é¢˜ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œä¸€äº›ç›®æ ‡æ ·æœ¬ï¼Œæ— éœ€æºæ•°æ®æˆ–ç­–ç•¥ï¼Œæ¥è§£å†³ç›®æ ‡åŸŸçš„å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœæ— æ³•è®¿é—®æºæ•°æ®çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†é€æ­¥åˆ†å¸ƒå¯¹é½å¼•å¯¼é£æ ¼æç¤ºè°ƒæ•´ï¼ˆStepSPTï¼‰ï¼Œå®ƒé€šè¿‡é¢„æµ‹åˆ†å¸ƒä¼˜åŒ–æ¥éšå«åœ°ç¼©å°åŸŸé—´å·®è·ã€‚StepSPTæå‡ºäº†ä¸€ç§é£æ ¼æç¤ºæ¥ä½¿ç›®æ ‡æ ·æœ¬ä¸æ‰€éœ€çš„åˆ†å¸ƒå¯¹é½ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§åŒé˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨å¤–éƒ¨è¿‡ç¨‹ä¸­ï¼Œé€æ­¥åˆ†å¸ƒå¯¹é½ç­–ç•¥å°†é¢„æµ‹åˆ†å¸ƒä¼˜åŒ–åˆ†è§£ä¸ºå¤šæ­¥å¯¹é½é—®é¢˜ï¼Œä»¥è°ƒæ•´é£æ ¼æç¤ºã€‚åœ¨å†…éƒ¨è¿‡ç¨‹ä¸­ï¼Œåˆ†ç±»å™¨ä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±è¿›è¡Œæ›´æ–°ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒStepSPTä¼˜äºç°æœ‰çš„åŸºäºæç¤ºè°ƒæ•´çš„æ–¹æ³•å’ŒSOTAsã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/xuhuali-mxj/StepSPT%E3%80%82">https://github.com/xuhuali-mxj/StepSPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10070v2">PDF</a> Accepted at IEEE TPAMI, 16 pages, 12 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†æ— æºæ•°æ®çš„è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆSF-CDFSLï¼‰é—®é¢˜ï¼Œå³åœ¨ç›®æ ‡åŸŸè¿›è¡Œå°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ï¼Œä»…ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œä¸€äº›ç›®æ ‡æ ·æœ¬ï¼Œæ— éœ€æºæ•°æ®æˆ–ç­–ç•¥ã€‚å¼•å…¥äº†ä¸€ç§åä¸ºStepSPTçš„æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹åˆ†å¸ƒä¼˜åŒ–æ¥éšå¼ç¼©å°åŸŸå·®è·ï¼Œé‡‡ç”¨é£æ ¼æç¤ºæ¥å¯¹é½ç›®æ ‡æ ·æœ¬ä¸æ‰€éœ€åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨åŒé˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒStepSPTä¼˜äºç°æœ‰çš„æç¤ºè°ƒä¼˜æ–¹æ³•å’ŒSOTAsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æºæ•°æ®ä¸å¯è®¿é—®çš„æŒ‘æˆ˜ï¼šç°æœ‰çš„CDFSLæ–¹æ³•é¢ä¸´ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ—¶æºæ•°æ®ä¸å¯è®¿é—®å’Œè®­ç»ƒç­–ç•¥çš„é™åˆ¶ã€‚</li>
<li>StepSPTæ–¹æ³•ä»‹ç»ï¼šé€šè¿‡é¢„æµ‹åˆ†å¸ƒä¼˜åŒ–éšå¼ç¼©å°åŸŸå·®è·ï¼Œé‡‡ç”¨é£æ ¼æç¤ºä¸æ‰€éœ€åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>åŒé˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ï¼šStepSPTé‡‡ç”¨å¤–éƒ¨å’Œå†…éƒ¨ä¸¤ä¸ªè¿‡ç¨‹çš„ä¼˜åŒ–ï¼Œå¤–éƒ¨è¿‡ç¨‹é€šè¿‡é€æ­¥åˆ†å¸ƒå¯¹é½ç­–ç•¥å°†é¢„æµ‹åˆ†å¸ƒä¼˜åŒ–åˆ†è§£ä¸ºå¤šæ­¥å¯¹é½é—®é¢˜ä»¥è°ƒæ•´é£æ ¼æç¤ºï¼Œå†…éƒ¨è¿‡ç¨‹åˆ™ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±æ›´æ–°åˆ†ç±»å™¨ã€‚</li>
<li>è¯„ä¼°ç»“æœï¼šåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒStepSPTä¼˜äºç°æœ‰çš„æç¤ºè°ƒä¼˜æ–¹æ³•å’Œæœ€ä½³è¡¨ç°æŠ€æœ¯ï¼ˆSOTAsï¼‰ã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç ï¼šç ”ç©¶äººå‘˜çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xuhuali-mxj/StepSPT%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/xuhuali-mxj/StepSPTä¸Šå…¬å¼€ã€‚</a></li>
<li>å®é™…åº”ç”¨ä»·å€¼ï¼šè¯¥ç ”ç©¶è§£å†³äº†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨å°æ ·æœ¬å­¦ä¹ ä¸­çš„å®é™…é—®é¢˜ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.10070v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.10070v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.10070v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.10070v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Few-Shot/2411.10070v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_I2I Translation/2504.06027v2/page_1_0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  BREA-Depth Bronchoscopy Realistic Airway-geometric Depth Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Agent/2509.11944v1/page_2_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Agentic Temporal Graph of Reasoning with Multimodal Language Models A   Potential AI Aid to Healthcare
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
