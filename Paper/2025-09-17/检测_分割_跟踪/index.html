<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  FS-SAM2 Adapting Segment Anything Model 2 for Few-Shot Semantic   Segmentation via Low-Rank Adaptation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-17-æ›´æ–°"><a href="#2025-09-17-æ›´æ–°" class="headerlink" title="2025-09-17 æ›´æ–°"></a>2025-09-17 æ›´æ–°</h1><h2 id="FS-SAM2-Adapting-Segment-Anything-Model-2-for-Few-Shot-Semantic-Segmentation-via-Low-Rank-Adaptation"><a href="#FS-SAM2-Adapting-Segment-Anything-Model-2-for-Few-Shot-Semantic-Segmentation-via-Low-Rank-Adaptation" class="headerlink" title="FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic   Segmentation via Low-Rank Adaptation"></a>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic   Segmentation via Low-Rank Adaptation</h2><p><strong>Authors:Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente</strong></p>
<p>Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2â€™s video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2â€™s pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/fornib/FS-SAM2">https://github.com/fornib/FS-SAM2</a> </p>
<blockquote>
<p>å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²æœ€è¿‘å¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚å…¶ç›®æ ‡æ˜¯è¦å¼€å‘ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ˜¯é€šè¿‡ä»å¤´å¼€å§‹è®­ç»ƒé™„åŠ æ¨¡å—æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ã€‚è¦åœ¨è¿™äº›æ–¹æ³•ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œéœ€è¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒã€‚Segment Anything Model 2ï¼ˆSAM2ï¼‰æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºSAM2çš„å°æ ·æœ¬åˆ†å‰²æ–¹æ³•ï¼ˆFS-SAM2ï¼‰ï¼Œå…¶ä¸­SAM2çš„è§†é¢‘åŠŸèƒ½è¢«ç›´æ¥é‡æ–°ç”¨äºå°æ ·æœ¬ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹åŸå§‹æ¨¡å—åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œä»¥å¤„ç†é€šå¸¸åœ¨æ ‡å‡†æ•°æ®é›†ä¸­å‘ç°çš„å¤šæ ·åŒ–å›¾åƒï¼Œè€Œä¸åŒäºSAM2é¢„è®­ç»ƒä¸­ä½¿ç”¨çš„æ—¶åºå…³è”å¸§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†å‚æ•°è¿›è¡Œå…ƒè®­ç»ƒï¼Œè¿™æœ‰æ•ˆåœ°é€‚åº”äº†SAM2ï¼ŒåŒæ—¶å—ç›Šäºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»ä½•K-shoté…ç½®ã€‚æˆ‘ä»¬åœ¨PASCAL-5iã€COCO-20iå’ŒFSS-1000æ•°æ®é›†ä¸Šå¯¹FS-SAM2è¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°å‡ºå“è¶Šçš„è®¡ç®—æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/fornib/FS-SAM2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fornib/FS-SAM2è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12105v1">PDF</a> Accepted at ICIAP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰çš„Few-Shotè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼ˆFS-SAM2ï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨SAM2çš„è§†é¢‘èƒ½åŠ›ï¼Œé€šè¿‡åº”ç”¨ä½ç§©é€‚é…æŠ€æœ¯ï¼Œç›´æ¥é‡æ–°è®¾è®¡ç”¨äºå°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ã€‚æ–¹æ³•å®ç°äº†è·¨ä¸åŒæ•°æ®é›†çš„æœ‰æ•ˆé€‚åº”ï¼Œæ”¯æŒä»»æ„K-shoté…ç½®ï¼Œå¹¶åœ¨PASCAL-5iã€COCO-20iå’ŒFSS-1000æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚è®¡ç®—æ•ˆç‡é«˜ã€‚å…·ä½“ä¿¡æ¯è¯·è®¿é—®ç›¸åº”GitHubé“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Few-shotè¯­ä¹‰åˆ†å‰²æ—¨åœ¨åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å¯¹æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>SAM2æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€‚</li>
<li>FS-SAM2æ–¹æ³•åŸºäºSAM2çš„è§†é¢‘èƒ½åŠ›é‡æ–°è®¾è®¡ç”¨äºå°‘æ ·æœ¬åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>LoRAæŠ€æœ¯ç”¨äºå¤„ç†æ ‡å‡†æ•°æ®é›†ä¸­å¤šæ ·çš„å›¾åƒï¼Œé€‚åº”äº†SAM2é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„ä¸´æ—¶å…³è”å¸§çš„ä¸åŒç‰¹ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.12105v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.12105v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.12105v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Robustness-Analysis-in-High-Dimensional-Space-Application-to-Semantic-Segmentation-Network"><a href="#Probabilistic-Robustness-Analysis-in-High-Dimensional-Space-Application-to-Semantic-Segmentation-Network" class="headerlink" title="Probabilistic Robustness Analysis in High Dimensional Space: Application   to Semantic Segmentation Network"></a>Probabilistic Robustness Analysis in High Dimensional Space: Application   to Semantic Segmentation Network</h2><p><strong>Authors:Navid Hashemi, Samuel Sasaki, Diego Manzanas Lopez, Ipek Oguz, Meiyi Ma, Taylor T. Johnson</strong></p>
<p>Semantic segmentation networks (SSNs) play a critical role in domains such as medical imaging, autonomous driving, and environmental monitoring, where safety hinges on reliable model behavior under uncertainty. Yet, existing probabilistic verification approaches struggle to scale with the complexity and dimensionality of modern segmentation tasks, often yielding guarantees that are too conservative to be practical. We introduce a probabilistic verification framework that is both architecture-agnostic and scalable to high-dimensional outputs. Our approach combines sampling-based reachability analysis with conformal inference (CI) to deliver provable guarantees while avoiding the excessive conservatism of prior methods. To counteract CIâ€™s limitations in high-dimensional settings, we propose novel strategies that reduce conservatism without compromising rigor. Empirical evaluation on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates that our method provides reliable safety guarantees while substantially tightening bounds compared to SOTA. We also provide a toolbox implementing this technique, available on Github. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼ˆSSNï¼‰åœ¨åŒ»å­¦å½±åƒã€è‡ªåŠ¨é©¾é©¶å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸæ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œè¿™äº›é¢†åŸŸçš„å®‰å…¨æ€§ä¾èµ–äºä¸ç¡®å®šæ¡ä»¶ä¸‹çš„å¯é æ¨¡å‹è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¦‚ç‡éªŒè¯æ–¹æ³•éš¾ä»¥åº”å¯¹ç°ä»£åˆ†å‰²ä»»åŠ¡çš„å¤æ‚æ€§å’Œç»´åº¦ï¼Œå¾€å¾€äº§ç”Ÿçš„ä¿è¯è¿‡äºä¿å®ˆè€Œæ— æ³•å®é™…åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¦‚ç‡éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¢ä¸å—æ¶æ„é™åˆ¶ï¼Œåˆèƒ½é€‚åº”é«˜ç»´è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºé‡‡æ ·çš„å¯è¾¾æ€§åˆ†ææ–¹æ³•å’Œåˆè§„æ¨ç†ï¼ˆCIï¼‰ï¼Œæä¾›äº†å¯è¯æ˜çš„ä¿è¯ï¼ŒåŒæ—¶é¿å…äº†å…ˆå‰æ–¹æ³•çš„è¿‡åº¦ä¿å®ˆæ€§ã€‚ä¸ºäº†å…‹æœCIåœ¨é«˜ç»´ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å‡å°‘ä¿å®ˆæ€§çš„æ–°ç­–ç•¥ï¼ŒåŒæ—¶ä¸å¦¥åäºä¸¥è°¨æ€§ã€‚åœ¨CamVidã€OCTA-500ã€è‚ºåˆ†å‰²å’ŒåŸå¸‚æ™¯è§‚ç­‰å¤§å‹åˆ†å‰²æ¨¡å‹ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æä¾›å¯é çš„å®‰å…¨ä¿è¯çš„åŒæ—¶ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå¤§å¤§ç¼©å°äº†è¾¹ç•Œã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå®ç°è¿™ä¸€æŠ€æœ¯çš„å·¥å…·ç®±ï¼Œå¯ä»¥åœ¨Githubä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11838v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼ˆSSNsï¼‰çš„æ¦‚ç‡éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹äºåŒ»ç–—æˆåƒã€è‡ªåŠ¨é©¾é©¶å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚æ­¤æ¡†æ¶å¯¹æ¶æ„å…·æœ‰ä¸­ç«‹æ€§ï¼Œå¹¶èƒ½é€‚åº”é«˜ç»´è¾“å‡ºã€‚å®ƒç»“åˆäº†åŸºäºé‡‡æ ·çš„å¯è¾¾æ€§åˆ†ææ–¹æ³•å’Œå½¢å¼åŒ–æ¨æ–­ï¼ˆCIï¼‰ï¼Œæä¾›å¯éªŒè¯çš„ä¿è¯ï¼Œé¿å…äº†å…ˆå‰æ–¹æ³•çš„è¿‡åº¦ä¿å®ˆæ€§ã€‚é’ˆå¯¹é«˜ç»´ç¯å¢ƒä¸­CIçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å‡å°‘ä¿å®ˆæ€§çš„æ–°ç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼æ€§ã€‚åœ¨å¤§è§„æ¨¡åˆ†å‰²æ¨¡å‹ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æä¾›å¯é çš„å®‰å…¨ä¿è¯ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œç•Œé™æ›´åŠ ä¸¥æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼ˆSSNsï¼‰åœ¨åŒ»ç–—æˆåƒã€è‡ªåŠ¨é©¾é©¶å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚</li>
<li>ç°æœ‰æ¦‚ç‡éªŒè¯æ–¹æ³•é¢ä¸´ä¸ç°ä»£åˆ†å‰²ä»»åŠ¡å¤æ‚æ€§å’Œç»´åº¦ä¹‹é—´çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çš„æ¦‚ç‡éªŒè¯æ¡†æ¶æ—¢å…·æœ‰æ¶æ„ä¸­ç«‹æ€§ï¼Œåˆèƒ½é€‚åº”é«˜ç»´è¾“å‡ºã€‚</li>
<li>ç»“åˆé‡‡æ ·å¯è¾¾æ€§åˆ†æå’Œå½¢å¼åŒ–æ¨æ–­ï¼ˆCIï¼‰ï¼Œæä¾›å¯éªŒè¯çš„ä¿è¯ã€‚</li>
<li>é’ˆå¯¹é«˜ç»´ç¯å¢ƒä¸­CIçš„å±€é™æ€§ï¼Œæå‡ºäº†å‡å°‘ä¿å®ˆæ€§çš„æ–°ç­–ç•¥ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡åˆ†å‰²æ¨¡å‹ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•çš„å¯é æ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11838v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MAFS-Masked-Autoencoder-for-Infrared-Visible-Image-Fusion-and-Semantic-Segmentation"><a href="#MAFS-Masked-Autoencoder-for-Infrared-Visible-Image-Fusion-and-Semantic-Segmentation" class="headerlink" title="MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic   Segmentation"></a>MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic   Segmentation</h2><p><strong>Authors:Liying Wang, Xiaoli Zhang, Chuanmin Jia, Siwei Ma</strong></p>
<p>Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Abraham-Einstein/MAFS/">https://github.com/Abraham-Einstein/MAFS/</a>. </p>
<blockquote>
<p>çº¢å¤–å¯è§å…‰å›¾åƒèåˆæ–¹æ³•æ—¨åœ¨ç”Ÿæˆå…·æœ‰è‰¯å¥½è§†è§‰è´¨é‡çš„èåˆå›¾åƒï¼Œå¹¶ä¿ƒè¿›é«˜çº§ä»»åŠ¡æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œç°æœ‰çš„è¯­ä¹‰é©±åŠ¨æ–¹æ³•å·²ç»è€ƒè™‘äº†å°†è¯­ä¹‰ä¿¡æ¯æ³¨å…¥ä¸‹æ¸¸åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬éƒ½æ²¡æœ‰ä»å®è§‚çš„ä»»åŠ¡å±‚é¢è§’åº¦ç ”ç©¶åƒç´ çº§å›¾åƒèåˆå’Œè·¨æ¨¡æ€ç‰¹å¾èåˆæ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„æ½œåœ¨ç›¸äº’ä¿ƒè¿›ä½œç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç½‘ç»œç”¨äºå›¾åƒèåˆå’Œè¯­ä¹‰åˆ†å‰²ã€‚MAFSæ˜¯ä¸€ä¸ªå¹¶è¡Œç»“æ„ï¼ŒåŒ…å«ä¸€ä¸ªèåˆå­ç½‘ç»œå’Œä¸€ä¸ªåˆ†å‰²å­ç½‘ç»œã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¼‚æ„ç‰¹å¾èåˆç­–ç•¥ï¼Œä»¥æé«˜å›¾åƒèåˆçš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œé€šè¿‡å°†èåˆå­ç½‘ç»œä¸åˆ†å‰²ä¸»å¹²çº§è”ï¼Œå°†åˆ†å‰²ç›¸å…³çŸ¥è¯†è½¬ç§»åˆ°åŸºäºç‰¹å¾çº§çš„èåˆåˆ†å‰²ä¸­ã€‚åœ¨è¯¥æ¡†æ¶å†…ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µTransformerè§£ç å™¨ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆç²¾ç»†çš„å¤šå°ºåº¦èåˆç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºæœ€å¤§æœ€å°å…¬å¹³æ€§åˆ†é…åŸåˆ™çš„åŠ¨æ€å› å­ï¼Œä»¥ç”Ÿæˆä¸¤ä¸ªä»»åŠ¡çš„è‡ªé€‚åº”æƒé‡ï¼Œå¹¶ä¿è¯å¤šä»»åŠ¡è®­ç»ƒçš„å¹³ç¨³æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Abraham-Einstein/MAFS/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Abraham-Einstein/MAFS/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11817v1">PDF</a> Accepted by TIP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æå‡ºä¸€ç§ç»Ÿä¸€ç½‘ç»œç”¨äºå›¾åƒèåˆå’Œè¯­ä¹‰åˆ†å‰²ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰è‰¯å¥½è§†è§‰è´¨é‡çš„èåˆå›¾åƒå¹¶ä¿ƒè¿›é«˜çº§ä»»åŠ¡æ€§èƒ½ã€‚æ–‡ç« ä»ä»»åŠ¡å±‚é¢å‡ºå‘ï¼Œæ¢è®¨åƒç´ çº§å›¾åƒèåˆä¸è·¨æ¨¡æ€ç‰¹å¾èåˆæ„ŸçŸ¥ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›æ½œåŠ›ã€‚é‡‡ç”¨å¼‚è´¨ç‰¹å¾èåˆç­–ç•¥æé«˜èåˆå›¾åƒçš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä¸²è”èåˆå­ç½‘ç»œå’Œåˆ†å‰²ä¸»å¹²ç½‘ç»œå®ç°åˆ†å‰²çŸ¥è¯†åœ¨ç‰¹å¾å±‚é¢çš„èåˆåº”ç”¨ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µTransformerè§£ç å™¨ï¼Œä»¥é«˜æ•ˆèšåˆç²¾ç»†çš„å¤šå°ºåº¦èåˆç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºæœ€å¤§æœ€å°å…¬å¹³æ€§åˆ†é…åŸåˆ™çš„åŠ¨æ€å› å­ï¼Œä»¥ç”Ÿæˆä¸¤ç§ä»»åŠ¡è‡ªé€‚åº”æƒé‡ï¼Œå¹¶åœ¨å¤šä»»åŠ¡è®­ç»ƒè¿‡ç¨‹ä¸­ä¿è¯å¹³æ»‘æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çº¢å¤–å¯è§å…‰å›¾åƒèåˆæ—¨åœ¨ç”Ÿæˆå…·æœ‰è‰¯å¥½è§†è§‰è´¨é‡çš„èåˆå›¾åƒï¼Œå¹¶ä¿ƒè¿›é«˜çº§ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç°æœ‰è¯­ä¹‰é©±åŠ¨æ–¹æ³•å·²è€ƒè™‘è¯­ä¹‰ä¿¡æ¯æ³¨å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½†ç¼ºä¹ä»å®è§‚ä»»åŠ¡å±‚é¢æ¢è®¨åƒç´ çº§å›¾åƒèåˆä¸è·¨æ¨¡æ€ç‰¹å¾èåˆæ„ŸçŸ¥ä»»åŠ¡é—´çš„ç›¸äº’ä¿ƒè¿›æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§ç»Ÿä¸€ç½‘ç»œç”¨äºå›¾åƒèåˆå’Œè¯­ä¹‰åˆ†å‰²ï¼ŒåŒ…æ‹¬èåˆå­ç½‘ç»œå’Œåˆ†å‰²å­ç½‘ç»œã€‚</li>
<li>å¼•å…¥å¼‚è´¨ç‰¹å¾èåˆç­–ç•¥æé«˜èåˆå›¾åƒçš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¸²è”èåˆå­ç½‘ç»œå’Œåˆ†å‰²ä¸»å¹²ç½‘ç»œï¼Œå®ç°åˆ†å‰²çŸ¥è¯†åœ¨ç‰¹å¾å±‚é¢çš„èåˆåº”ç”¨ã€‚</li>
<li>è®¾è®¡æ–°å‹å¤šé˜¶æ®µTransformerè§£ç å™¨ï¼Œé«˜æ•ˆèšåˆç²¾ç»†çš„å¤šå°ºåº¦èåˆç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11817v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11817v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11817v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11817v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11817v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ActivePose-Active-6D-Object-Pose-Estimation-and-Tracking-for-Robotic-Manipulation"><a href="#ActivePose-Active-6D-Object-Pose-Estimation-and-Tracking-for-Robotic-Manipulation" class="headerlink" title="ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic   Manipulation"></a>ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic   Manipulation</h2><p><strong>Authors:Sheng Liu, Zhe Li, Weiheng Wang, Han Sun, Heng Zhang, Hongpeng Chen, Yusen Qin, Arash Ajoudani, Yizhao Wang</strong></p>
<p>Accurate 6-DoF object pose estimation and tracking are critical for reliable robotic manipulation. However, zero-shot methods often fail under viewpoint-induced ambiguities and fixed-camera setups struggle when objects move or become self-occluded. To address these challenges, we propose an active pose estimation pipeline that combines a Vision-Language Model (VLM) with â€œrobotic imaginationâ€ to dynamically detect and resolve ambiguities in real time. In an offline stage, we render a dense set of views of the CAD model, compute the FoundationPose entropy for each view, and construct a geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy (ambiguous) examples. At runtime, the system: (1) queries the VLM on the live image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete set of candidate camera poses by rendering virtual views, scores each based on a weighted combination of VLM ambiguity probability and FoundationPose entropy, and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated pose estimation. Furthermore, since moving objects may leave the cameraâ€™s field of view, we introduce an active pose tracking module: a diffusion-policy trained via imitation learning, which generates camera trajectories that preserve object visibility and minimize pose ambiguity. Experiments in simulation and real-world show that our approach significantly outperforms classical baselines. </p>
<blockquote>
<p>å‡†ç¡®çš„6è‡ªç”±åº¦ç‰©ä½“å§¿æ€ä¼°è®¡å’Œè·Ÿè¸ªå¯¹äºå¯é çš„æœºå™¨äººæ“ä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé›¶æ ·æœ¬æ–¹æ³•å¸¸å¸¸åœ¨è§†è§’å¼•èµ·çš„æ­§ä¹‰ä¸‹å¤±æ•ˆï¼Œå›ºå®šæ‘„åƒå¤´è®¾ç½®åœ¨ç‰©ä½“ç§»åŠ¨æˆ–è‡ªæˆ‘é®æŒ¡æ—¶å¾ˆéš¾åº”å¯¹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œâ€œæœºå™¨äººæƒ³è±¡â€çš„ä¸»åŠ¨å§¿æ€ä¼°è®¡ç®¡é“ï¼Œä»¥å®æ—¶æ£€æµ‹å’Œè§£å†³æ­§ä¹‰ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹CADæ¨¡å‹çš„å¯†é›†è§†å›¾è¿›è¡Œæ¸²æŸ“ï¼Œè®¡ç®—æ¯ä¸ªè§†å›¾çš„FoundationPoseç†µï¼Œå¹¶æ„å»ºä¸€ä¸ªåŒ…å«ä½ç†µï¼ˆæ— æ­§ä¹‰ï¼‰å’Œé«˜ç†µï¼ˆæœ‰æ­§ä¹‰ï¼‰ç¤ºä¾‹çš„å‡ ä½•æ„ŸçŸ¥æç¤ºã€‚åœ¨è¿è¡Œæ—¶ï¼Œç³»ç»Ÿï¼šï¼ˆ1ï¼‰å¯¹å®æ—¶å›¾åƒæŸ¥è¯¢VLMä»¥è·å–æ­§ä¹‰åˆ†æ•°ï¼›ï¼ˆ2ï¼‰å¦‚æœæ£€æµ‹åˆ°æ­§ä¹‰ï¼Œé€šè¿‡æ¸²æŸ“è™šæ‹Ÿè§†å›¾æ¥æƒ³è±¡ä¸€ç»„ç¦»æ•£çš„å€™é€‰ç›¸æœºå§¿æ€ï¼ŒåŸºäºVLMæ­§ä¹‰æ¦‚ç‡å’ŒFoundationPoseç†µçš„åŠ æƒç»„åˆå¯¹å®ƒä»¬è¿›è¡Œè¯„åˆ†ï¼Œç„¶åå°†ç›¸æœºç§»åŠ¨åˆ°æœ€ä½³ä¸‹ä¸€ä¸ªè§†è§’ï¼ˆNBVï¼‰ä»¥è·å¾—æ˜ç¡®çš„å§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œç”±äºç§»åŠ¨ç‰©ä½“å¯èƒ½ä¼šç¦»å¼€ç›¸æœºçš„è§†é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸»åŠ¨å§¿æ€è·Ÿè¸ªæ¨¡å—ï¼šé€šè¿‡æ¨¡ä»¿å­¦ä¹ è®­ç»ƒçš„æ‰©æ•£ç­–ç•¥ï¼Œç”Ÿæˆèƒ½å¤Ÿä¿æŒç‰©ä½“å¯è§æ€§å¹¶æœ€å°åŒ–å§¿æ€æ­§ä¹‰çš„ç›¸æœºè½¨è¿¹ã€‚ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç»å…¸åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11364v1">PDF</a> 6D Pose, Diffusion Policy</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸â€œæœºå™¨äººæƒ³è±¡â€æŠ€æœ¯çš„ä¸»åŠ¨å§¿æ€ä¼°è®¡ç®¡é“ï¼Œç”¨äºå®æ—¶æ£€æµ‹å’Œè§£å†³è§†è§’å¼•èµ·çš„æ­§ä¹‰é—®é¢˜ã€‚é€šè¿‡ç¦»çº¿é˜¶æ®µå¯¹CADæ¨¡å‹çš„å¯†é›†è§†å›¾æ¸²æŸ“ã€FoundationPoseç†µçš„è®¡ç®—ä»¥åŠåŒ…å«ä½ç†µå’Œé«˜ç†µä¾‹å­çš„å‡ ä½•æ„ŸçŸ¥æç¤ºçš„æ„å»ºï¼Œè¿è¡Œæ—¶ç³»ç»ŸæŸ¥è¯¢VLMè·å–æ­§ä¹‰åˆ†æ•°ï¼Œæ£€æµ‹æ­§ä¹‰æ—¶é€šè¿‡æ¸²æŸ“è™šæ‹Ÿè§†å›¾ç”Ÿæˆä¸€ç»„å€™é€‰ç›¸æœºå§¿æ€ï¼ŒåŸºäºVLMæ­§ä¹‰æ¦‚ç‡å’ŒFoundationPoseç†µçš„åŠ æƒç»„åˆè¿›è¡Œè¯„åˆ†ï¼Œç„¶åç§»åŠ¨ç›¸æœºè‡³ä¸‹ä¸€ä¸ªæœ€ä½³è§†è§’ï¼ˆNBVï¼‰ä»¥è·å¾—æ¸…æ™°çš„å§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³ç§»åŠ¨ç‰©ä½“å¯èƒ½ç¦»å¼€ç›¸æœºè§†é‡çš„é—®é¢˜ï¼Œå¼•å…¥ä¸»åŠ¨å§¿æ€è·Ÿè¸ªæ¨¡å—ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ è®­ç»ƒæ‰©æ•£ç­–ç•¥ï¼Œç”Ÿæˆèƒ½å¤Ÿä¿æŒç‰©ä½“å¯è§æ€§å¹¶æœ€å°åŒ–å§¿æ€æ­§ä¹‰çš„ç›¸æœºè½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>6-DoFå¯¹è±¡å§¿æ€ä¼°è®¡å’Œè·Ÿè¸ªå¯¹äºå¯é çš„æœºå™¨äººæ“ä½œè‡³å…³é‡è¦ã€‚</li>
<li>é›¶å°„å‡»æ–¹æ³•ç»å¸¸åœ¨è§†è§’å¼•èµ·çš„æ­§ä¹‰ä¸‹å¤±è´¥ï¼Œå›ºå®šç›¸æœºè®¾ç½®åœ¨ç‰©ä½“ç§»åŠ¨æˆ–è‡ªé®æŒ¡æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œâ€œæœºå™¨äººæƒ³è±¡â€çš„ä¸»åŠ¨å§¿æ€ä¼°è®¡ç®¡é“ï¼Œä»¥å®æ—¶æ£€æµ‹å’Œè§£å†³æ­§ä¹‰ã€‚</li>
<li>é€šè¿‡CADæ¨¡å‹çš„å¯†é›†è§†å›¾æ¸²æŸ“ã€FoundationPoseç†µçš„è®¡ç®—ä»¥åŠåŒ…å«ä½é«˜ç†µä¾‹å­çš„å‡ ä½•æ„ŸçŸ¥æç¤ºæ„å»ºï¼Œä¸ºç³»ç»Ÿæä¾›æ•°æ®åŸºç¡€ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æŸ¥è¯¢VLMè·å–æ­§ä¹‰åˆ†æ•°ï¼Œé€šè¿‡æ¸²æŸ“è™šæ‹Ÿè§†å›¾ç”Ÿæˆå€™é€‰ç›¸æœºå§¿æ€ï¼Œå¹¶åŸºäºVLMæ­§ä¹‰æ¦‚ç‡å’ŒFoundationPoseç†µè¯„åˆ†é€‰æ‹©æœ€ä½³è§†è§’ã€‚</li>
<li>å¼•å…¥ä¸»åŠ¨å§¿æ€è·Ÿè¸ªæ¨¡å—ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ è®­ç»ƒæ‰©æ•£ç­–ç•¥ï¼Œç”Ÿæˆä¿æŒç‰©ä½“å¯è§æ€§å¹¶å‡å°‘å§¿æ€æ­§ä¹‰çš„ç›¸æœºè½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11364v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Motion-Estimation-for-Multi-Object-Tracking-using-KalmanNet-with-Semantic-Independent-Encoding"><a href="#Motion-Estimation-for-Multi-Object-Tracking-using-KalmanNet-with-Semantic-Independent-Encoding" class="headerlink" title="Motion Estimation for Multi-Object Tracking using KalmanNet with   Semantic-Independent Encoding"></a>Motion Estimation for Multi-Object Tracking using KalmanNet with   Semantic-Independent Encoding</h2><p><strong>Authors:Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long</strong></p>
<p>Motion estimation is a crucial component in multi-object tracking (MOT).   It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches.   The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT.   However, it may yield unsatisfactory results when KFâ€™s parameters are mismatched and objects move in non-stationary.   In this work, we utilize the learning-aided filter to handle the motion estimation of MOT.   In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps.   First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information.   Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements.   To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets.   Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters.   The code is available at (<a target="_blank" rel="noopener" href="https://github.com/SongJgit/filternet">https://github.com/SongJgit/filternet</a> and <a target="_blank" rel="noopener" href="https://github.com/SongJgit/TBDTracker">https://github.com/SongJgit/TBDTracker</a>). </p>
<blockquote>
<p>è¿åŠ¨ä¼°è®¡æ˜¯å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å®ƒé€šè¿‡è¿ç»­å¸§çš„å›¾åƒä¸­ç‰©ä½“çš„ä½ç½®å˜åŒ–æ¥åˆ†æé¢„æµ‹ç‰©ä½“çš„è½¨è¿¹ï¼Œä»è€Œå‡å°‘è·Ÿè¸ªå¤±è´¥å’Œèº«ä»½åˆ‡æ¢çš„æƒ…å†µã€‚åŸºäºçº¿æ€§æ’å®šé€Ÿåº¦æ¨¡å‹çš„å¡å°”æ›¼æ»¤æ³¢å™¨ï¼ˆKFï¼‰æ˜¯MOTä¸­æœ€å¸¸ç”¨çš„æ–¹æ³•ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œå½“KFçš„å‚æ•°ä¸åŒ¹é…ä¸”ç‰©ä½“åšéå¹³ç¨³è¿åŠ¨æ—¶ï¼Œå¯èƒ½ä¼šå¾—åˆ°ä¸æ»¡æ„çš„ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨å­¦ä¹ è¾…åŠ©æ»¤æ³¢å™¨æ¥å¤„ç†MOTçš„è¿åŠ¨ä¼°è®¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSemantic-Independent KalmanNetï¼ˆSIKNetï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸¤æ­¥ä½¿ç”¨è¯­ä¹‰ç‹¬ç«‹ç¼–ç å™¨ï¼ˆSIEï¼‰å¯¹çŠ¶æ€å‘é‡ï¼ˆè¾“å…¥ç‰¹å¾ï¼‰è¿›è¡Œç¼–ç ã€‚é¦–å…ˆï¼ŒSIEä½¿ç”¨å¤§å°ä¸º1çš„1Då·ç§¯ï¼Œæ²¿ç€ä¸åŒçŠ¶æ€å‘é‡ä¸­åŒæ„è¯­ä¹‰å…ƒç´ çš„ç»´åº¦è¿›è¡Œå·ç§¯ï¼Œä»¥ç¼–ç ç‹¬ç«‹çš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶åï¼Œå®ƒé‡‡ç”¨å…¨è¿æ¥å±‚å’Œéçº¿æ€§æ¿€æ´»å±‚æ¥ç¼–ç å¼‚æ„è¯­ä¹‰å…ƒç´ ä¹‹é—´çš„éçº¿æ€§å’Œäº¤å‰ä¾èµ–ä¿¡æ¯ã€‚ä¸ºäº†ç‹¬ç«‹è¯„ä¼°è¿åŠ¨ä¼°è®¡æ¨¡å—åœ¨MOTä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»å‡ ä¸ªå¼€æºçš„MOTæ•°æ®é›†ä¸­æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŠæ¨¡æ‹Ÿæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„SIKNetä¼˜äºä¼ ç»Ÿçš„KFï¼Œå¹¶ä¸”åœ¨è¿åŠ¨ä¼°è®¡æ–¹é¢å®ç°äº†æ¯”ç°æœ‰å­¦ä¹ è¾…åŠ©æ»¤æ³¢å™¨æ›´é«˜çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚ä»£ç å¯åœ¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/SongJgit/filternet%E5%92%8Chttps://github.com/SongJgit/TBDTracker%EF%BC%89%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SongJgit/filternetå’Œhttps://github.com/SongJgit/TBDTrackerï¼‰ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šç›®æ ‡è·Ÿè¸ªä¸­çš„è¿åŠ¨ä¼°è®¡é—®é¢˜ã€‚ä¼ ç»Ÿçš„å¡å°”æ›¼æ»¤æ³¢åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSIKNetçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è¯­ä¹‰ç‹¬ç«‹ç¼–ç å™¨å¤„ç†çŠ¶æ€å‘é‡ï¼Œå¹¶åœ¨å¤§è§„æ¨¡åŠæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSIKNetåœ¨ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»Ÿå¡å°”æ›¼æ»¤æ³¢å’Œå­¦ä¹ è¾…åŠ©æ»¤æ³¢å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿åŠ¨ä¼°è®¡åœ¨å¤šç›®æ ‡è·Ÿè¸ªä¸­è‡³å…³é‡è¦ï¼Œé€šè¿‡é¢„æµ‹å¯¹è±¡è½¨è¿¹æé«˜è·Ÿè¸ªæ•ˆæœå’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¡å°”æ›¼æ»¤æ³¢æ˜¯åŸºäºçº¿æ€§æ’å®šé€Ÿåº¦æ¨¡å‹çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†åœ¨å‚æ•°ä¸åŒ¹é…å’Œéå¹³ç¨³ç‰©ä½“ç§»åŠ¨æ—¶å¯èƒ½è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†åä¸ºSIKNetçš„æ–°æ–¹æ³•ï¼Œä½¿ç”¨è¯­ä¹‰ç‹¬ç«‹ç¼–ç å™¨å¤„ç†çŠ¶æ€å‘é‡ã€‚</li>
<li>SIKNeté€šè¿‡ä¸¤ä¸ªæ­¥éª¤ç¼–ç çŠ¶æ€å‘é‡ï¼šé¦–å…ˆä½¿ç”¨1Då·ç§¯æå–ç‹¬ç«‹è¯­ä¹‰ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨å…¨è¿æ¥å±‚å’Œéçº¿æ€§æ¿€æ´»å±‚ç¼–ç éçº¿æ€§åŠè·¨ä¾èµ–ä¿¡æ¯ã€‚</li>
<li>ä¸ºäº†ç‹¬ç«‹è¯„ä¼°è¿åŠ¨ä¼°è®¡æ¨¡å—æ€§èƒ½ï¼Œä»å¤šä¸ªå¼€æºå¤šç›®æ ‡è·Ÿè¸ªæ•°æ®é›†ä¸­æ„å»ºäº†å¤§è§„æ¨¡åŠæ¨¡æ‹Ÿæ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSIKNetåœ¨ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ä¸Šä¼˜äºä¼ ç»Ÿå¡å°”æ›¼æ»¤æ³¢å’Œå­¦ä¹ è¾…åŠ©æ»¤æ³¢å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11323v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.11323v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OpenUrban3D-Annotation-Free-Open-Vocabulary-Semantic-Segmentation-of-Large-Scale-Urban-Point-Clouds"><a href="#OpenUrban3D-Annotation-Free-Open-Vocabulary-Semantic-Segmentation-of-Large-Scale-Urban-Point-Clouds" class="headerlink" title="OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of   Large-Scale Urban Point Clouds"></a>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of   Large-Scale Urban Point Clouds</h2><p><strong>Authors:Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang</strong></p>
<p>Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºä»»æ„è‡ªç„¶è¯­è¨€æè¿°æ¥è¯†åˆ«å’Œåˆ†å‰²å¯¹è±¡ï¼Œè¿™ä¸ºå¤„ç†è¶…å‡ºå›ºå®šæ ‡ç­¾é›†çš„æ–°é¢–ã€ç²¾ç»†ç²’åº¦æˆ–åŠŸèƒ½å®šä¹‰çš„ç±»åˆ«æä¾›äº†çµæ´»æ€§ã€‚è™½ç„¶è¿™ç§èƒ½åŠ›å¯¹äºæ”¯æŒæ•°å­—å­ªç”Ÿã€æ™ºæ…§åŸå¸‚ç®¡ç†å’ŒåŸå¸‚åˆ†æç­‰åº”ç”¨çš„å¤§è§„æ¨¡åŸå¸‚ç‚¹äº‘è‡³å…³é‡è¦ï¼Œä½†åœ¨è¿™ä¸ªé¢†åŸŸé‡Œå®ƒä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ä¸»è¦éšœç¢åœ¨äºå¤§è§„æ¨¡åŸå¸‚ç‚¹äº‘æ•°æ®é›†ä¸­é«˜è´¨é‡ã€å¯¹é½çš„å¤šè§†è§’å›¾åƒç»å¸¸ç¼ºå¤±ï¼Œä»¥åŠç°æœ‰ä¸‰ç»´ï¼ˆ3Dï¼‰åˆ†å‰²ç®¡é“åœ¨å‡ ä½•ã€å°ºåº¦å’Œå¤–è§‚ä¸Šæœ‰å¾ˆå¤§å·®å¼‚çš„å¤šæ ·åŸå¸‚ç¯å¢ƒä¸­æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenUrban3Dï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§è§„æ¨¡åŸå¸‚åœºæ™¯è¿›è¡Œå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„3Dæ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰å¯¹é½çš„å¤šè§†è§’å›¾åƒã€é¢„è®­ç»ƒçš„ç‚¹äº‘åˆ†å‰²ç½‘ç»œæˆ–æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹è¿è¡Œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¤šè§†è§’ã€å¤šç²’åº¦æ¸²æŸ“ã€æ©è†œçº§è§†è§‰è¯­è¨€ç‰¹å¾æå–å’Œæ ·æœ¬å¹³è¡¡èåˆï¼Œç›´æ¥ä»åŸå§‹ç‚¹äº‘ä¸­ç”Ÿæˆç¨³å¥çš„è¯­ä¹‰ç‰¹å¾ï¼Œç„¶åè¿›è¡Œè’¸é¦åˆ°3Déª¨å¹²æ¨¡å‹ä¸­ã€‚è¿™ç§è®¾è®¡å®ç°äº†é›¶æ ·æœ¬åˆ†å‰²ï¼Œå¯ä»¥å¤„ç†ä»»æ„æ–‡æœ¬æŸ¥è¯¢ï¼ŒåŒæ—¶æ•æ‰è¯­ä¹‰ä¸°å¯Œå’Œå‡ ä½•å…ˆéªŒã€‚åœ¨å¤§è§„æ¨¡åŸå¸‚åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬SensatUrbanå’ŒSUMï¼Œè¡¨æ˜OpenUrban3Dåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œè·¨åœºæ™¯æ³›åŒ–æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†å…¶ä½œä¸ºçµæ´»ä¸”å¯æ‰©å±•çš„3DåŸå¸‚åœºæ™¯ç†è§£è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10842v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OpenUrban3Dï¼Œä¸€ä¸ªé’ˆå¯¹å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯ç›´æ¥ä»åŸå§‹ç‚¹äº‘ç”Ÿæˆç¨³å¥è¯­ä¹‰ç‰¹å¾ï¼Œæ— éœ€å¯¹é½çš„å¤šè§†å›¾å›¾åƒã€é¢„è®­ç»ƒçš„ç‚¹äº‘åˆ†å‰²ç½‘ç»œæˆ–æ‰‹åŠ¨æ ‡æ³¨ã€‚é€šè¿‡å¤šè§†å›¾ã€å¤šç²’åº¦æ¸²æŸ“ã€æ©è†œçº§è§†è§‰è¯­è¨€ç‰¹å¾æå–å’Œæ ·æœ¬å¹³è¡¡èåˆç­‰æ–¹æ³•ï¼Œå®ç°é›¶æ ·æœ¬åˆ†å‰²ä»»æ„æ–‡æœ¬æŸ¥è¯¢ï¼ŒåŒæ—¶æ•æ‰è¯­ä¹‰ä¸°å¯Œæ€§å’Œå‡ ä½•å…ˆéªŒã€‚åœ¨å¤§å‹åŸå¸‚åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOpenUrban3Dåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œè·¨åœºæ™¯æ³›åŒ–æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå±•ç°äº†ä½œä¸ºçµæ´»å¯æ‰©å±•çš„3DåŸå¸‚åœºæ™¯ç†è§£è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-vocabulary semantic segmentation can handle novel, fine-grained, or functionally defined categories beyond fixed label sets, crucial for large-scale urban point clouds.</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤šæ ·åŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œé¢ä¸´ç¼ºä¹é«˜è´¨é‡ã€å¯¹é½çš„å¤šè§†å›¾å›¾åƒå’Œ3Dåˆ†å‰²ç®¡é“çš„æŒ‘æˆ˜ã€‚</li>
<li>OpenUrban3Dæ˜¯é¦–ä¸ªé’ˆå¯¹å¤§è§„æ¨¡åŸå¸‚åœºæ™¯çš„3Då¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œæ— éœ€å¯¹é½çš„å¤šè§†å›¾å›¾åƒã€é¢„è®­ç»ƒç‚¹äº‘åˆ†å‰²ç½‘ç»œæˆ–æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>é€šè¿‡å¤šè§†å›¾ã€å¤šç²’åº¦æ¸²æŸ“ï¼ŒOpenUrban3Dç›´æ¥ä»åŸå§‹ç‚¹äº‘ç”Ÿæˆç¨³å¥è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>è¯¥è®¾è®¡å®ç°é›¶æ ·æœ¬åˆ†å‰²ä»»æ„æ–‡æœ¬æŸ¥è¯¢ï¼ŒåŒæ—¶æ•æ‰è¯­ä¹‰ä¸°å¯Œæ€§å’Œå‡ ä½•å…ˆéªŒã€‚</li>
<li>åœ¨å¤§å‹åŸå¸‚åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOpenUrban3Dåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œè·¨åœºæ™¯æ³›åŒ–æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.10842v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.10842v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.10842v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Similarity-based-Outlier-Detection-for-Noisy-Object-Re-Identification-Using-Beta-Mixtures"><a href="#Similarity-based-Outlier-Detection-for-Noisy-Object-Re-Identification-Using-Beta-Mixtures" class="headerlink" title="Similarity-based Outlier Detection for Noisy Object Re-Identification   Using Beta Mixtures"></a>Similarity-based Outlier Detection for Noisy Object Re-Identification   Using Beta Mixtures</h2><p><strong>Authors:Waqar Ahmad, Evan Murphy, Vladimir A. Krylov</strong></p>
<p>Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning. We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: github.com&#x2F;waqar3411&#x2F;Beta-SOD </p>
<blockquote>
<p>å¯¹è±¡å†è¯†åˆ«ï¼ˆRe-IDï¼‰æ–¹æ³•å¯¹æ ‡ç­¾å™ªå£°é«˜åº¦æ•æ„Ÿï¼Œé€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬é€šè¿‡å°†Re-IDé‡æ–°æ„å»ºä¸ºå—ç›‘ç£çš„å›¾åƒç›¸ä¼¼æ€§ä»»åŠ¡æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå¹¶é‡‡ç”¨Siameseç½‘ç»œæ¶æ„è¿›è¡Œè®­ç»ƒï¼Œä»¥æ•è·åˆ¤åˆ«æ€§çš„æˆå¯¹å…³ç³»ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆODï¼‰æ¡†æ¶ï¼Œç§°ä¸ºBeta-SODï¼ˆåŸºäºç›¸ä¼¼åº¦çš„Betaæ··åˆå¼‚å¸¸æ£€æµ‹ï¼‰ï¼Œå®ƒä½¿ç”¨ä¸¤åˆ†é‡Betaåˆ†å¸ƒæ··åˆæ¨¡å‹å¯¹åµŒå…¥å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬ä¸ºä¸¤ä¸ªBetaåˆ†å¸ƒçš„æ··åˆç‰©å»ºç«‹äº†æ–°çš„å¯è¯†åˆ«ç»“æœï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„å­¦ä¹ ä»»åŠ¡æ˜¯é€‚å®šçš„ã€‚æ‰€æå‡ºçš„ODæ­¥éª¤è¡¥å……äº†Re-IDæ¶æ„ï¼Œç»“åˆäº†äºŒå…ƒäº¤å‰ç†µã€å¯¹æ¯”å’Œä½™å¼¦åµŒå…¥æŸå¤±ï¼Œå…±åŒä¼˜åŒ–ç‰¹å¾çº§åˆ«çš„ç›¸ä¼¼æ€§å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨CUHK03å’ŒMarket-1501æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¡ŒäººRe-IDå»å™ªå’ŒRe-IDä»»åŠ¡ï¼Œä»¥åŠåœ¨VeRi-776æ•°æ®é›†ä¸Šè¿›è¡Œäº†è½¦è¾†Re-IDä»»åŠ¡ï¼Œè¯æ˜äº†Beta-SODçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§å™ªå£°æ°´å¹³ï¼ˆ10-30ï¼…ï¼‰ä¸‹çš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†åœ¨å˜ˆæ‚çš„Re-IDåœºæ™¯ä¸­æ—¢ç¨³å¥åˆå¹¿æ³›åº”ç”¨ã€‚Beta-SODçš„å®ç°å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/waqar3411/Beta-SOD">github.com&#x2F;waqar3411&#x2F;Beta-SOD</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08926v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡è§£å†³æ ‡ç­¾å™ªå£°å¯¼è‡´çš„ç›®æ ‡å†è¯†åˆ«ï¼ˆRe-IDï¼‰æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é€šè¿‡å°†å…¶è§†ä¸ºç›‘ç£å›¾åƒç›¸ä¼¼æ€§ä»»åŠ¡å¹¶é‡‡ç”¨Siameseç½‘ç»œæ¶æ„æ¥æ•è·åˆ¤åˆ«æ€§æˆå¯¹å…³ç³»è¿›è¡Œè®­ç»ƒã€‚å¼•å…¥äº†ä¸€ç§åä¸ºBeta-SODçš„æ–°å‹ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä¸¤ç»„ä»¶Betaåˆ†å¸ƒæ··åˆæ¨¡å‹å¯¹åµŒå…¥å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•å¯æœ‰æ•ˆå»é™¤å™ªå£°å¹¶åœ¨äººå‘˜Re-IDå’Œè½¦è¾†Re-IDçš„å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨å¤šç§å™ªå£°æ°´å¹³ä¸‹å…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§å’Œå¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç›®æ ‡å†è¯†åˆ«ï¼ˆRe-IDï¼‰å—æ ‡ç­¾å™ªå£°å½±å“å¤§ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é‡‡ç”¨Siameseç½‘ç»œæ¶æ„è¿›è¡Œè®­ç»ƒï¼Œå°†Re-IDé‡æ–°å®šä¹‰ä¸ºç›‘ç£å›¾åƒç›¸ä¼¼æ€§ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥æ–°å‹ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹æ¡†æ¶Beta-SODï¼Œåˆ©ç”¨ä¸¤ç»„ä»¶Betaåˆ†å¸ƒæ··åˆæ¨¡å‹å»ºæ¨¡ä½™å¼¦ç›¸ä¼¼æ€§åˆ†å¸ƒã€‚</li>
<li>å»ºç«‹äº†å…³äºä¸¤ä¸ªBetaåˆ†å¸ƒæ··åˆçš„æ–°çš„å¯è¯†åˆ«æ€§ç»“æœï¼Œç¡®ä¿å­¦ä¹ ä»»åŠ¡çš„æ˜ç¡®æ€§ã€‚</li>
<li>Beta-SODä¸Re-IDæ¶æ„ç›¸ç»“åˆï¼Œé€šè¿‡è”åˆä¼˜åŒ–ç‰¹å¾çº§åˆ«çš„ç›¸ä¼¼æ€§å­¦ä¹ æ¥æé«˜å»å™ªå’ŒRe-IDä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>åœ¨äººå‘˜Re-IDçš„CUHK03å’ŒMarket-1501æ•°æ®é›†ä»¥åŠè½¦è¾†Re-IDçš„VeRi-776æ•°æ®é›†ä¸Šå±•ç¤ºäº†Beta-SODçš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.08926v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.08926v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.08926v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2509.08926v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DAOcc-3D-Object-Detection-Assisted-Multi-Sensor-Fusion-for-3D-Occupancy-Prediction"><a href="#DAOcc-3D-Object-Detection-Assisted-Multi-Sensor-Fusion-for-3D-Occupancy-Prediction" class="headerlink" title="DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy   Prediction"></a>DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy   Prediction</h2><p><strong>Authors:Zhen Yang, Yanpeng Dong, Jiayu Wang, Heng Wang, Lichao Ma, Zijian Cui, Qi Liu, Haoran Pei, Kexin Zhang, Chao Zhang</strong></p>
<p>Multi-sensor fusion significantly enhances the accuracy and robustness of 3D semantic occupancy prediction, which is crucial for autonomous driving and robotics. However, most existing approaches depend on high-resolution images and complex networks to achieve top performance, hindering their deployment in practical scenarios. Moreover, current multi-sensor fusion approaches mainly focus on improving feature fusion while largely neglecting effective supervision strategies for those features. To address these issues, we propose DAOcc, a novel multi-modal occupancy prediction framework that leverages 3D object detection supervision to assist in achieving superior performance, while using a deployment-friendly image backbone and practical input resolution. In addition, we introduce a BEV View Range Extension strategy to mitigate performance degradation caused by lower image resolution. Extensive experiments demonstrate that DAOcc achieves new state-of-the-art results on both the Occ3D-nuScenes and Occ3D-Waymo benchmarks, and outperforms previous state-of-the-art methods by a significant margin using only a ResNet-50 backbone and 256*704 input resolution. With TensorRT optimization, DAOcc reaches 104.9 FPS while maintaining 54.2 mIoU on an NVIDIA RTX 4090 GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AlphaPlusTT/DAOcc">https://github.com/AlphaPlusTT/DAOcc</a>. </p>
<blockquote>
<p>å¤šä¼ æ„Ÿå™¨èåˆèƒ½æ˜¾è‘—å¢å¼º3Dè¯­ä¹‰å ç”¨é¢„æµ‹çš„ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œæ¥å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å¤šä¼ æ„Ÿå™¨èåˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç‰¹å¾èåˆä¸Šï¼Œè€Œå¾ˆå°‘å…³æ³¨è¿™äº›ç‰¹å¾çš„æœ‰æ•ˆç›‘ç£ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DAOccï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å ç”¨é¢„æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨3Dç›®æ ‡æ£€æµ‹ç›‘ç£æ¥å¸®åŠ©å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨éƒ¨ç½²å‹å¥½çš„å›¾åƒä¸»å¹²å’Œå®ç”¨çš„è¾“å…¥åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†BEV View Range Extensionç­–ç•¥ï¼Œä»¥ç¼“è§£å› è¾ƒä½å›¾åƒåˆ†è¾¨ç‡å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDAOccåœ¨Occ3D-nuSceneså’ŒOcc3D-WaymoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä»…ä½¿ç”¨ResNet-50ä¸»å¹²å’Œ256*704è¾“å…¥åˆ†è¾¨ç‡å°±å¤§å¤§è¶…è¶Šäº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚é€šè¿‡TensorRTä¼˜åŒ–ï¼ŒDAOccåœ¨NVIDIA RTX 4090 GPUä¸Šè¾¾åˆ°äº†104.9 FPSï¼ŒåŒæ—¶ä¿æŒäº†54.2 mIoUã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AlphaPlusTT/DAOcc%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AlphaPlusTT/DAOccæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19972v3">PDF</a> TCSVT Accepted version (not the final published version)</p>
<p><strong>Summary</strong></p>
<p>å¤šä¼ æ„Ÿå™¨èåˆèƒ½æé«˜3Dè¯­ä¹‰é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œï¼Œä¸åˆ©äºå®é™…åº”ç”¨ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºDAOccæ¡†æ¶ï¼Œåˆ©ç”¨3Dç›®æ ‡æ£€æµ‹ç›‘ç£å®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œä½¿ç”¨éƒ¨ç½²å‹å¥½çš„å›¾åƒä¸»å¹²å’Œå®é™…è¾“å…¥åˆ†è¾¨ç‡ã€‚åŒæ—¶å¼•å…¥BEV View Range Extensionç­–ç•¥ï¼Œç¼“è§£ä½åˆ†è¾¨ç‡å›¾åƒå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚DAOccåœ¨Occ3D-nuSceneså’ŒOcc3D-WaymoåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä½¿ç”¨ResNet-50ä¸»å¹²å’Œ256*704è¾“å…¥åˆ†è¾¨ç‡æ—¶è¡¨ç°æ›´ä¼˜å¼‚ã€‚ç»TensorRTä¼˜åŒ–ï¼ŒDAOccåœ¨NVIDIA RTX 4090 GPUä¸Šè¾¾åˆ°104.9 FPSï¼ŒåŒæ—¶ä¿æŒ54.2 mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä¼ æ„Ÿå™¨èåˆå¯¹å¢å¼º3Dè¯­ä¹‰é¢„æµ‹å‡†ç¡®æ€§ä¸ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>DAOccæ¡†æ¶åˆ©ç”¨3Dç›®æ ‡æ£€æµ‹ç›‘ç£å®ç°é«˜æ€§èƒ½ï¼Œå…·æœ‰éƒ¨ç½²å‹å¥½æ€§ã€‚</li>
<li>BEV View Range Extensionç­–ç•¥æ”¹å–„ä½åˆ†è¾¨ç‡å›¾åƒæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>DAOccåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>DAOccä½¿ç”¨ResNet-50ä¸»å¹²å’Œè¾ƒä½è¾“å…¥åˆ†è¾¨ç‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.19972v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_æ— ç›‘ç£_åŠç›‘ç£_å¯¹æ¯”å­¦ä¹ /2502.00848v3/page_5_0.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Enhancement Without Contrast Stability-Aware Multicenter Machine   Learning for Glioma MRI Imaging
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Vision Transformer/2509.12143v1/page_0_0.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  3DViT-GAT A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
