<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Do machine learning climate models work in changing climate dynamics?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2508.13229v3/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-17-æ›´æ–°"><a href="#2025-09-17-æ›´æ–°" class="headerlink" title="2025-09-17 æ›´æ–°"></a>2025-09-17 æ›´æ–°</h1><h2 id="Do-machine-learning-climate-models-work-in-changing-climate-dynamics"><a href="#Do-machine-learning-climate-models-work-in-changing-climate-dynamics" class="headerlink" title="Do machine learning climate models work in changing climate dynamics?"></a>Do machine learning climate models work in changing climate dynamics?</h2><p><strong>Authors:Maria Conchita Agana Navarro, Geng Li, Theo Wolf, MarÃ­a PÃ©rez-Ortiz</strong></p>
<p>Climate change is accelerating the frequency and severity of unprecedented events, deviating from established patterns. Predicting these out-of-distribution (OOD) events is critical for assessing risks and guiding climate adaptation. While machine learning (ML) models have shown promise in providing precise, high-speed climate predictions, their ability to generalize under distribution shifts remains a significant limitation that has been underexplored in climate contexts. This research systematically evaluates state-of-the-art ML-based climate models in diverse OOD scenarios by adapting established OOD evaluation methodologies to climate data. Experiments on large-scale datasets reveal notable performance variability across scenarios, shedding light on the strengths and limitations of current models. These findings underscore the importance of robust evaluation frameworks and provide actionable insights to guide the reliable application of ML for climate risk forecasting. </p>
<blockquote>
<p>æ°”å€™å˜åŒ–æ­£åœ¨åŠ é€Ÿå‰æ‰€æœªæœ‰äº‹ä»¶çš„é¢‘ç‡å’Œä¸¥é‡ç¨‹åº¦ï¼Œè¿™äº›äº‹ä»¶åç¦»äº†æ—¢å®šçš„æ¨¡å¼ã€‚é¢„æµ‹è¿™äº›ç¦»ç¾¤äº‹ä»¶ï¼ˆOODäº‹ä»¶ï¼‰å¯¹äºè¯„ä¼°é£é™©å’ŒæŒ‡å¯¼æ°”å€™é€‚åº”è‡³å…³é‡è¦ã€‚è™½ç„¶æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨æä¾›ç²¾ç¡®ã€é«˜é€Ÿçš„æ°”å€™é¢„æµ‹æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†åˆ†å¸ƒå˜åŒ–æ—¶çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯æ°”å€™èƒŒæ™¯ä¸‹å°šæœªå……åˆ†æ¢ç´¢çš„é‡å¤§å±€é™ã€‚æœ¬ç ”ç©¶é€šè¿‡é€‚åº”ç°æœ‰çš„ç¦»ç¾¤å€¼è¯„ä¼°æ–¹æ³•å¯¹æ°”å€™æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œç³»ç»Ÿåœ°è¯„ä»·äº†æœ€å…ˆè¿›çš„åŸºäºæœºå™¨å­¦ä¹ çš„æ°”å€™æ¨¡å‹åœ¨ä¸åŒç¦»ç¾¤å€¼åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒæ­ç¤ºäº†ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½å·®å¼‚ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç¨³å¥è¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ï¼Œä»¥æŒ‡å¯¼æœºå™¨å­¦ä¹ åœ¨æ°”å€™é£é™©é¢„æµ‹ä¸­çš„å¯é åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12147v1">PDF</a> 8 pages, 2 figures</p>
<p><strong>Summary</strong>ï¼šæ°”å€™å˜åŒ–åŠ é€Ÿäº†å‰æ‰€æœªæœ‰çš„äº‹ä»¶çš„å‘ç”Ÿé¢‘ç‡å’Œå¼ºåº¦ï¼Œè¿™äº›äº‹ä»¶åç¦»äº†æ—¢å®šçš„æ¨¡å¼ã€‚è™½ç„¶æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æä¾›ç²¾ç¡®ã€é«˜é€Ÿçš„æ°”å€™é¢„æµ‹æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹äºåˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯æ°”å€™èƒŒæ™¯ä¸‹å°šæœªå……åˆ†æ¢ç´¢çš„é‡å¤§å±€é™ã€‚æœ¬ç ”ç©¶é€šè¿‡é€‚åº”æ—¢å®šçš„åˆ†å¸ƒå¤–è¯„ä¼°æ–¹æ³•å¯¹æ°”å€™æ•°æ®è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å¤§è§„æ¨¡æ•°æ®é›†å®éªŒåœ¨ä¸åŒåˆ†å¸ƒå¤–åœºæ™¯ä¸­æ˜¾è‘—æ€§èƒ½è¡¨ç°ä¸åŒã€‚è¿™ä¸ä»…çªæ˜¾äº†å¯é çš„è¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ï¼Œè¿˜ä¸ºåº”ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œæ°”å€™é£é™©é¢„æµ‹æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ°”å€™å˜åŒ–å¯¼è‡´å‰æ‰€æœªæœ‰çš„äº‹ä»¶è¶Šæ¥è¶Šé¢‘ç¹å’Œä¸¥é‡ï¼Œè¿™åç¦»äº†å·²çŸ¥çš„æ¨¡å¼ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ°”å€™é¢„æµ‹ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨åº”å¯¹åˆ†å¸ƒå˜åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç³»ç»Ÿåœ°è¯„ä¼°äº†å…ˆè¿›çš„åŸºäºæœºå™¨å­¦ä¹ çš„æ°”å€™æ¨¡å‹åœ¨ä¸åŒåˆ†å¸ƒå¤–çš„åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡é€‚åº”å·²æœ‰çš„åˆ†å¸ƒå¤–è¯„ä¼°æ–¹æ³•ï¼Œä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå®éªŒã€‚</li>
<li>å‘ç°ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>å¯é çš„è¯„ä¼°æ¡†æ¶å¯¹äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ°”å€™é¢„æµ‹ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12147v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12147v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniPar-A-Unified-LLM-Based-Framework-for-Parallel-and-Accelerated-Code-Translation-in-HPC"><a href="#UniPar-A-Unified-LLM-Based-Framework-for-Parallel-and-Accelerated-Code-Translation-in-HPC" class="headerlink" title="UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code   Translation in HPC"></a>UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code   Translation in HPC</h2><p><strong>Authors:Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren</strong></p>
<p>Translating programs between various parallel programming languages is an important problem in the high-performance computing (HPC) community. Existing tools for this problem are either too narrow in scope and&#x2F;or outdated. Recent explosive growth in the popularity of large language models (LLMs) and their ability to generate and translate code offers a potential alternative approach. Toward that end, we first need to systematically evaluate the ability of LLMs to translate between parallel languages.   In this work, we introduce UniPar, a systematic evaluation framework for LLM-based parallel code translation. Specifically, in this work, we target translations between serial code, CUDA, and OpenMP. Our goal is to assess how well current instruction-tuned LLMs â€“ specifically GPT-4o-mini and LLaMA-3.3-70B-Instruct â€“ can be used out of the box or enhanced through known strategies. We evaluated four major usage modes: hyperparameter optimization for decoding, zero- and few-shot prompting, supervised fine-tuning, and iterative feedback through compiler-based repair. As a part of the evaluation, we construct a new dataset called PARATRANS, covering both serial-to-parallel translation and cross-paradigm transformations.   Our findings reveal that while off-the-shelf models struggle under the default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15% functional correctness), our UniPar methodology â€“ combining fine-tuning, hyperparameter tuning, and compiler-guided repair â€“ improves performance by up to 2X (69% compilation and 33% correctness). We believe that our findings will provide useful insights for researchers to further improve LLMs for the parallel language translation problem.   UniPar source code and PARATRANS dataset are available at our GitHub repository <a target="_blank" rel="noopener" href="https://github.com/Scientific-Computing-Lab/UniPar_AI">https://github.com/Scientific-Computing-Lab/UniPar_AI</a>. </p>
<blockquote>
<p>åœ¨ä¸åŒå¹¶è¡Œç¼–ç¨‹è¯­è¨€ä¹‹é—´çš„ç¨‹åºç¿»è¯‘æ˜¯é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ç¤¾åŒºä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ç°æœ‰çš„å·¥å…·è¦ä¹ˆèŒƒå›´å¤ªçª„ï¼Œè¦ä¹ˆè¿‡æ—¶ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠåŠå…¶ç”Ÿæˆå’Œç¿»è¯‘ä»£ç çš„èƒ½åŠ›æä¾›äº†ä¸€ç§æ½œåœ¨çš„æ›¿ä»£æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å¹¶è¡Œè¯­è¨€ä¹‹é—´çš„ç¿»è¯‘èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UniParï¼Œä¸€ä¸ªç”¨äºLLMå¹¶è¡Œä»£ç ç¿»è¯‘çš„ç³»ç»Ÿæ€§è¯„ä»·æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°å½“å‰æŒ‡ä»¤è°ƒä¼˜çš„LLMâ€”â€”ç‰¹åˆ«æ˜¯GPT-4o-miniå’ŒLLaMA-3.3-70B-Instructâ€”â€”å¯ä»¥å¦‚ä½•ç›´æ¥ä½¿ç”¨æˆ–é€šè¿‡å·²çŸ¥ç­–ç•¥è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ç§ä¸»è¦ä½¿ç”¨æ¨¡å¼ï¼šè§£ç è¶…å‚æ•°ä¼˜åŒ–ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€ç›‘ç£å¾®è°ƒä»¥åŠé€šè¿‡ç¼–è¯‘å™¨ä¿®å¤çš„è¿­ä»£åé¦ˆã€‚ä½œä¸ºè¯„ä¼°çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºPARATRANSçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–ä¸²è¡Œåˆ°å¹¶è¡Œç¿»è¯‘å’Œè·¨èŒƒå¼è½¬æ¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å³æ’å³ç”¨æ¨¡å‹åœ¨é»˜è®¤è®¾ç½®ä¸‹è¡¨ç°æŒ£æ‰ï¼ˆä¾‹å¦‚ï¼ŒGPT-4o-miniä»…å®ç°äº†46%çš„ç¼–è¯‘å’Œ15%çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼‰ï¼Œä½†æˆ‘ä»¬çš„UniParæ–¹æ³•â€”â€”ç»“åˆå¾®è°ƒã€è¶…å‚æ•°è°ƒæ•´å’Œç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤â€”â€”å¯ä»¥å°†æ€§èƒ½æé«˜ä¸¤å€ï¼ˆ69%çš„ç¼–è¯‘å’Œ33%çš„æ­£ç¡®æ€§ï¼‰ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå°†ä¸ºç ”ç©¶äººå‘˜è¿›ä¸€æ­¥æ”¹è¿›LLMä»¥è§£å†³å¹¶è¡Œè¯­è¨€ç¿»è¯‘é—®é¢˜æä¾›æœ‰ç›Šçš„è§è§£ã€‚UniParæºä»£ç å’ŒPARATRANSæ•°æ®é›†å¯åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Scientific-Computing-Lab/UniPar_AI%E3%80%82">https://github.com/Scientific-Computing-Lab/UniPar_AIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12136v1">PDF</a> Accepted to IEEE HPEC conference 2025. 9 pages, incl references</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniParæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¶è¡Œä»£ç ç¿»è¯‘æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ä¸“æ³¨äºæŒ‡ä»¤å¾®è°ƒLLMsåœ¨ä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡å››ç§ä¸»è¦ä½¿ç”¨æ¨¡å¼è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å‡ºå‚æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡å¾®è°ƒã€è¶…å‚æ•°ä¼˜åŒ–å’Œç¼–è¯‘å™¨è¾…åŠ©ä¿®å¤ç­‰æ–¹æ³•ï¼Œæ€§èƒ½å¯æé«˜ä¸€å€ã€‚æ•°æ®é›†PARATRANSå’ŒUniParæºä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¹¶è¡Œç¼–ç¨‹è¯­è¨€çš„ä»£ç ç¿»è¯‘ä¸Šå…·æœ‰æ½œåŠ›ã€‚</li>
<li>UniParæ¡†æ¶ç”¨äºè¯„ä¼°LLMsåœ¨ä»£ç ç¿»è¯‘æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨æŒ‡ä»¤å¾®è°ƒLLMsåœ¨ä¸²è¡Œä»£ç ã€CUDAå’ŒOpenMPä¹‹é—´çš„ç¿»è¯‘ã€‚</li>
<li>å‡ºå‚æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡ä¼˜åŒ–æ–¹æ³•ï¼Œæ€§èƒ½å¯æ˜¾è‘—æé«˜ã€‚</li>
<li>æ–°æ•°æ®é›†PARATRANSç”¨äºç ”ç©¶ä»£ç ç¿»è¯‘ã€‚</li>
<li>UniParæ–¹æ³•å’ŒPARATRANSæ•°æ®é›†å¯¹æé«˜LLMsåœ¨å¹¶è¡Œè¯­è¨€ç¿»è¯‘é—®é¢˜ä¸Šçš„æ€§èƒ½å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12136v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models"><a href="#Look-Again-Think-Slowly-Enhancing-Visual-Reflection-in-Vision-Language-Models" class="headerlink" title="Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language   Models"></a>Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language   Models</h2><p><strong>Authors:Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, Jiajun Zhang</strong></p>
<p>Recent advances in text-only â€œslow-thinkingâ€ reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical challenges: Effective â€œslow thinkingâ€ in VRMs requires \textbf{visual reflection}, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, \textbf{Reflection-V} demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬å‹â€œæ…¢æ€è€ƒâ€æ¨ç†çš„è¿›å±•ä¿ƒä½¿äººä»¬å°†è¿™ç§èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸Šï¼Œä»¥è®­ç»ƒè§†è§‰æ¨ç†æ¨¡å‹ï¼ˆVRMsï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§è½¬ç§»é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼šåœ¨VRMsä¸­è¿›è¡Œæœ‰æ•ˆçš„â€œæ…¢æ€è€ƒâ€éœ€è¦â€œè§†è§‰åæ€â€ï¼Œå³åŸºäºè§†è§‰ä¿¡æ¯è¿›è¡Œæ¨ç†è¿‡ç¨‹æ£€æŸ¥çš„èƒ½åŠ›ã€‚é€šè¿‡å®šé‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°å½“å‰VRMsçš„è§†è§‰åæ€èƒ½åŠ›æœ‰é™ï¼Œéšç€ç”Ÿæˆå“åº”çš„å¢é•¿ï¼Œå®ƒä»¬å¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨è¿…é€Ÿå‡å°‘ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„VRMâ€”â€”â€œReflection-Vâ€ã€‚å®ƒé€šè¿‡æ„å»ºç”¨äºå†·å¯åŠ¨çš„æ¨ç†æ•°æ®å’Œè®¾è®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¥–åŠ±æœºåˆ¶ï¼Œå¢å¼ºäº†è§†è§‰åæ€èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æ™ºèƒ½ä½“åœ¨VLMså’Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´è¿›è¡Œäº¤äº’ï¼Œæ„å»ºä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†æ•°æ®ï¼Œä»¥å®ç°è§†è§‰åæ€æ¨¡å¼çš„å†·å¯åŠ¨å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œåœ¨RLè¿‡ç¨‹ä¸­é‡‡ç”¨åŸºäºè§†è§‰æ³¨æ„åŠ›çš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥é¼“åŠ±åŸºäºè§†è§‰ä¿¡æ¯çš„æ¨ç†ã€‚å› æ­¤ï¼Œâ€œReflection-Vâ€åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œâ€œReflection-Vâ€åœ¨è§†è§‰æ¨ç†è¿‡ç¨‹ä¸­æ›´å¼ºçƒˆä¸”ä¸€è‡´åœ°ä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œè¿™æ˜¾ç¤ºå‡ºè§†è§‰åæ€èƒ½åŠ›å¾—åˆ°äº†æœ‰æ•ˆå¢å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12132v1">PDF</a> EMNLP2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†æ–‡æœ¬æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºæœ‰æ•ˆâ€œæ…¢æ€è€ƒâ€éœ€è¦è§†è§‰åæ€èƒ½åŠ›ã€‚é€šè¿‡å®šé‡åˆ†æå‘ç°å½“å‰è§†è§‰æ¨ç†æ¨¡å‹å­˜åœ¨è§†è§‰åæ€èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œä¸ºæ­¤æå‡ºäº†æ–°å‹è§†è§‰æ¨ç†æ¨¡å‹Reflection-Vã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†æ•°æ®å’Œè®¾è®¡å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œå¢å¼ºè§†è§‰åæ€èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒReflection-Våœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨è§†è§‰æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒäº†æ›´å¼ºçš„è§†è§‰ä¿¡æ¯ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰æ¨ç†æ¨¡å‹é¢ä¸´ç¼ºä¹è§†è§‰åæ€èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>â€œæ…¢æ€è€ƒâ€åœ¨è§†è§‰æ¨ç†ä¸­éœ€è¦è§†è§‰åæ€èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ–°å‹è§†è§‰æ¨ç†æ¨¡å‹Reflection-Vï¼Œä»¥å¢å¼ºè§†è§‰åæ€èƒ½åŠ›ã€‚</li>
<li>Reflection-Vé€šè¿‡æ„å»ºä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†æ•°æ®å®ç°å†·å¯åŠ¨å­¦ä¹ ã€‚</li>
<li>é‡‡ç”¨åŸºäºè§†è§‰æ³¨æ„åŠ›çš„å¥–åŠ±æ¨¡å‹ï¼Œé¼“åŠ±åŸºäºè§†è§‰ä¿¡æ¯çš„æ¨ç†ã€‚</li>
<li>Reflection-Våœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12132v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12132v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12132v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12132v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12132v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models"><a href="#When-Safe-Unimodal-Inputs-Collide-Optimizing-Reasoning-Chains-for-Cross-Modal-Safety-in-Multimodal-Large-Language-Models" class="headerlink" title="When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models"></a>When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for   Cross-Modal Safety in Multimodal Large Language Models</h2><p><strong>Authors:Wei Cai, Shujuan Liu, Jian Zhao, Ziyan Shi, Yusheng Zhao, Yuchen Yuan, Tianle Zhang, Chi Zhang, Xuelong Li</strong></p>
<p>Multimodal Large Language Models (MLLMs) are susceptible to the implicit reasoning risk, wherein innocuous unimodal inputs synergistically assemble into risky multimodal data that produce harmful outputs. We attribute this vulnerability to the difficulty of MLLMs maintaining safety alignment through long-chain reasoning. To address this issue, we introduce Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring interpretable reasoning paths tailored for such a cross-modal challenge. A novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is also designed based on the SSUI dataset to align the MLLMâ€™s internal reasoning process with human safety values. Experimental results show that our SRPO-trained models achieve state-of-the-art results on key safety benchmarks, including the proposed Reasoning Path Benchmark (RSBench), significantly outperforming both open-source and top-tier commercial MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°éšæ€§æ¨ç†é£é™©çš„å½±å“ï¼Œå…¶ä¸­æ— å®³çš„å•æ¨¡æ€è¾“å…¥ä¼šååŒç»„åˆæˆå…·æœ‰é£é™©çš„å¤šæ¨¡æ€æ•°æ®ï¼Œä»è€Œäº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§æ¼æ´æ˜¯ç”±äºMLLMsé€šè¿‡é•¿é“¾æ¨ç†æ¥ä¿æŒå®‰å…¨å¯¹é½çš„éš¾åº¦é€ æˆçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹è¿™ç§è·¨æ¨¡æ€æŒ‘æˆ˜çš„ã€å…·æœ‰å¯è§£é‡Šæ¨ç†è·¯å¾„çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜åŸºäºSSUIæ•°æ®é›†è®¾è®¡äº†ä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œå³Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰ï¼Œä½¿MLLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸äººç±»å®‰å…¨ä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡SRPOè®­ç»ƒçš„æ¨¡å‹åœ¨å…³é”®çš„å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬æå‡ºçš„Reasoning Path Benchmarkï¼ˆRSBenchï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºå’Œé¡¶çº§å•†ä¸šMLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12060v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨éšæ€§æ¨ç†é£é™©ï¼Œæ— è¾œçš„å•æ¨¡æ€è¾“å…¥ä¼šååŒå½¢æˆé£é™©æ€§å¤šæ¨¡æ€æ•°æ®ï¼Œä»è€Œäº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰æ•°æ®é›†å’ŒåŸºäºè¯¥æ•°æ®é›†çš„Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿MLLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸äººç±»å®‰å…¨ä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨SRPOè®­ç»ƒçš„æ¨¡å‹åœ¨å…³é”®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬æå‡ºçš„Reasoning Path Benchmarkï¼ˆRSBenchï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºå’Œé¡¶çº§å•†ä¸šMLLMsã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´éšæ€§æ¨ç†é£é™©ï¼Œæ— è¾œè¾“å…¥å¯äº§ç”Ÿæœ‰å®³è¾“å‡ºã€‚</li>
<li>MLLMsåœ¨ç»´æŒé•¿é“¾æ¨ç†ä¸­çš„å®‰å…¨å¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥Safe-Semantics-but-Unsafe-Interpretationï¼ˆSSUIï¼‰æ•°æ®é›†ï¼Œæä¾›é’ˆå¯¹è·¨æ¨¡æ€æŒ‘æˆ˜çš„å¯è§£é‡Šæ¨ç†è·¯å¾„ã€‚</li>
<li>è®¾è®¡äº†åŸºäºSSUIæ•°æ®é›†çš„Safety-aware Reasoning Path Optimizationï¼ˆSRPOï¼‰è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>SRPOè®­ç»ƒæ¡†æ¶æ—¨åœ¨ä½¿MLLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸äººç±»å®‰å…¨ä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSRPOè®­ç»ƒæ¨¡å‹åœ¨å…³é”®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12060v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12060v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12060v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12060v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.12060v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TabStruct-Measuring-Structural-Fidelity-of-Tabular-Data"><a href="#TabStruct-Measuring-Structural-Fidelity-of-Tabular-Data" class="headerlink" title="TabStruct: Measuring Structural Fidelity of Tabular Data"></a>TabStruct: Measuring Structural Fidelity of Tabular Data</h2><p><strong>Authors:Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik</strong></p>
<p>Evaluating tabular generators remains a challenging problem, as the unique causal structural prior of heterogeneous tabular data does not lend itself to intuitive human inspection. Recent work has introduced structural fidelity as a tabular-specific evaluation dimension to assess whether synthetic data complies with the causal structures of real data. However, existing benchmarks often neglect the interplay between structural fidelity and conventional evaluation dimensions, thus failing to provide a holistic understanding of model performance. Moreover, they are typically limited to toy datasets, as quantifying existing structural fidelity metrics requires access to ground-truth causal structures, which are rarely available for real-world datasets. In this paper, we propose a novel evaluation framework that jointly considers structural fidelity and conventional evaluation dimensions. We introduce a new evaluation metric, $\textbf{global utility}$, which enables the assessment of structural fidelity even in the absence of ground-truth causal structures. In addition, we present $\textbf{TabStruct}$, a comprehensive evaluation benchmark offering large-scale quantitative analysis on 13 tabular generators from nine distinct categories, across 29 datasets. Our results demonstrate that global utility provides a task-independent, domain-agnostic lens for tabular generator performance. We release the TabStruct benchmark suite, including all datasets, evaluation pipelines, and raw results. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SilenceX12138/TabStruct">https://github.com/SilenceX12138/TabStruct</a>. </p>
<blockquote>
<p>è¯„ä¼°è¡¨æ ¼ç”Ÿæˆå™¨ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå¼‚è´¨è¡¨æ ¼æ•°æ®çš„ç‹¬ç‰¹å› æœç»“æ„å¹¶ä¸é€‚åˆç›´è§‚çš„äººå·¥æ£€æŸ¥ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»å¼•å…¥äº†ç»“æ„ä¿çœŸåº¦ä½œä¸ºè¡¨æ ¼ç‰¹å®šçš„è¯„ä¼°ç»´åº¦ï¼Œä»¥è¯„ä¼°åˆæˆæ•°æ®æ˜¯å¦ç¬¦åˆçœŸå®æ•°æ®çš„å› æœç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸å¿½ç•¥äº†ç»“æ„ä¿çœŸåº¦ä¸å¸¸è§„è¯„ä¼°ç»´åº¦ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå› æ­¤æ— æ³•å…¨é¢ç†è§£æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ä»…é™äºå°å‹æ•°æ®é›†ï¼Œå› ä¸ºé‡åŒ–ç°æœ‰ç»“æ„ä¿çœŸåº¦æŒ‡æ ‡éœ€è¦è®¿é—®çœŸå®å› æœç»“æ„ï¼Œè€Œç°å®ä¸–ç•Œçš„æ•°æ®é›†å¾ˆå°‘æä¾›è¿™äº›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè”åˆè€ƒè™‘ç»“æ„ä¿çœŸåº¦å’Œå¸¸è§„è¯„ä¼°ç»´åº¦çš„æ–°å‹è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…¨å±€æ•ˆç”¨ï¼Œå³ä½¿åœ¨ç¼ºä¹çœŸå®å› æœç»“æ„çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è¯„ä¼°ç»“æ„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TabStructåŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¥è‡ªä¹ä¸ªä¸åŒç±»åˆ«çš„13ä¸ªè¡¨æ ¼ç”Ÿæˆå™¨åœ¨29ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡å®šé‡åˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå…¨å±€æ•ˆç”¨ä¸ºè¡¨æ ¼ç”Ÿæˆå™¨æ€§èƒ½æä¾›äº†ä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸæ— å…³çš„è§†è§’ã€‚æˆ‘ä»¬å‘å¸ƒäº†TabStructåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ•°æ®é›†ã€è¯„ä¼°æµç¨‹å’ŒåŸå§‹ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SilenceX12138/TabStruct%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SilenceX12138/TabStructæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11950v1">PDF</a> 55 pages, 60 tables, 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘äº†ç»“æ„ä¿çœŸåº¦å’Œä¼ ç»Ÿè¯„ä¼°ç»´åº¦ã€‚ä¸ºåœ¨æ²¡æœ‰çœŸå®å› æœç»“æ„çš„æƒ…å†µä¸‹è¯„ä¼°ç»“æ„ä¿çœŸåº¦ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…¨å±€æ•ˆç”¨ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†TabStructï¼Œä¸€ä¸ªå¯¹13ä¸ªè¡¨æ ¼ç”Ÿæˆå™¨è¿›è¡Œå¤§è§„æ¨¡å®šé‡åˆ†æçš„ç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œæ¶‰åŠ9ä¸ªç±»åˆ«çš„29ä¸ªæ•°æ®é›†ã€‚ç»“æœæ˜¾ç¤ºï¼Œå…¨å±€æ•ˆç”¨ä¸ºè¡¨æ ¼ç”Ÿæˆå™¨æ€§èƒ½æä¾›äº†ä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸæ— å…³çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„ä¼°è¡¨æ ¼ç”Ÿæˆå™¨æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå¼‚è´¨è¡¨æ ¼æ•°æ®çš„ç‹¬ç‰¹å› æœç»“æ„ä¸é€‚åˆç›´è§‰æ£€æŸ¥ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€å¿½è§†ç»“æ„ä¿çœŸåº¦ä¸ä¼ ç»Ÿè¯„ä¼°ç»´åº¦çš„ç›¸äº’ä½œç”¨ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…¨å±€æ•ˆç”¨ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®å› æœç»“æ„çš„æƒ…å†µä¸‹è¯„ä¼°ç»“æ„ä¿çœŸåº¦ã€‚</li>
<li>æå‡ºTabStructåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«å¤§é‡æ•°æ®é›†å’Œè¯„ä¼°æµç¨‹ã€‚</li>
<li>TabStructæ¶µç›–äº†13ä¸ªè¡¨æ ¼ç”Ÿæˆå™¨ï¼Œæ¶‰åŠ9ä¸ªç±»åˆ«çš„29ä¸ªæ•°æ®é›†çš„å¤§è§„æ¨¡å®šé‡åˆ†æã€‚</li>
<li>å…¨å±€æ•ˆç”¨æä¾›äº†ä¸€ä¸ªä»»åŠ¡ç‹¬ç«‹ã€é¢†åŸŸæ— å…³çš„è¯„ä¼°è§†è§’ï¼Œç”¨äºç†è§£è¡¨æ ¼ç”Ÿæˆå™¨çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11950v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11950v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11950v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11950v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Agentic-Temporal-Graph-of-Reasoning-with-Multimodal-Language-Models-A-Potential-AI-Aid-to-Healthcare"><a href="#Agentic-Temporal-Graph-of-Reasoning-with-Multimodal-Language-Models-A-Potential-AI-Aid-to-Healthcare" class="headerlink" title="Agentic Temporal Graph of Reasoning with Multimodal Language Models: A   Potential AI Aid to Healthcare"></a>Agentic Temporal Graph of Reasoning with Multimodal Language Models: A   Potential AI Aid to Healthcare</h2><p><strong>Authors:Susanta Mitra</strong></p>
<p>Healthcare and medicine are multimodal disciplines that deal with multimodal data for reasoning and diagnosing multiple diseases. Although some multimodal reasoning models have emerged for reasoning complex tasks in scientific domains, their applications in the healthcare domain remain limited and fall short in correct reasoning for diagnosis. To address the challenges of multimodal medical reasoning for correct diagnosis and assist the healthcare professionals, a novel temporal graph-based reasoning process modelled through a directed graph has been proposed in the current work. It helps in accommodating dynamic changes in reasons through backtracking, refining the reasoning content, and creating new or deleting existing reasons to reach the best recommendation or answer. Again, consideration of multimodal data at different time points can enable tracking and analysis of patient health and disease progression. Moreover, the proposed multi-agent temporal reasoning framework provides task distributions and a cross-validation mechanism to further enhance the accuracy of reasoning outputs. A few basic experiments and analysis results justify the novelty and practical utility of the proposed preliminary approach. </p>
<blockquote>
<p>åŒ»ç–—å¥åº·æ˜¯å¤šæ¨¡å¼å­¦ç§‘ï¼Œæ¶‰åŠå¤šæ¨¡å¼æ•°æ®ä»¥è¿›è¡Œæ¨ç†å’Œè¯Šæ–­å¤šç§ç–¾ç—…ã€‚è™½ç„¶ä¸€äº›å¤šæ¨¡å¼æ¨ç†æ¨¡å‹å·²ç»å‡ºç°åœ¨ç§‘å­¦é¢†åŸŸçš„å¤æ‚ä»»åŠ¡æ¨ç†ä¸­ï¼Œä½†å®ƒä»¬åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œåœ¨è¯Šæ–­æ­£ç¡®æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³å¤šæ¨¡å¼åŒ»ç–—æ¨ç†åœ¨æ­£ç¡®è¯Šæ–­æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶å¸®åŠ©åŒ»æŠ¤äººå‘˜ï¼Œå½“å‰å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ—¶åºå›¾çš„æ–°æ¨ç†è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡æœ‰å‘å›¾è¿›è¡Œå»ºæ¨¡ã€‚å®ƒæœ‰åŠ©äºé€šè¿‡å›æº¯æ¥é€‚åº”åŸå› çš„åŠ¨æ€å˜åŒ–ï¼Œç²¾ç‚¼æ¨ç†å†…å®¹ï¼Œå¹¶åˆ›å»ºæ–°çš„æˆ–åˆ é™¤ç°æœ‰çš„åŸå› ï¼Œä»¥å¾—å‡ºæœ€ä½³å»ºè®®æˆ–ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè€ƒè™‘ä¸åŒæ—¶é—´ç‚¹çš„å¤šæ¨¡å¼æ•°æ®å¯ä»¥å®ç°å¯¹æ‚£è€…å¥åº·å’Œç–¾ç—…è¿›å±•çš„è·Ÿè¸ªå’Œåˆ†æã€‚è€Œä¸”ï¼Œæ‰€æå‡ºçš„å¤šæ™ºèƒ½ä½“æ—¶åºæ¨ç†æ¡†æ¶æä¾›ä»»åŠ¡åˆ†é…å’Œäº¤å‰éªŒè¯æœºåˆ¶ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨ç†è¾“å‡ºçš„å‡†ç¡®æ€§ã€‚ä¸€äº›åŸºæœ¬å®éªŒå’Œåˆ†æç»“æœè¯æ˜äº†æ‰€æå‡ºåˆæ­¥æ–¹æ³•çš„æ–°é¢–æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11944v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŒ»ç–—å¥åº·é¢†åŸŸæ¶‰åŠå¤šæ¨¡å¼æ•°æ®æ¨ç†å’Œè¯Šæ–­å¤šç§ç–¾ç—…ã€‚å½“å‰å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ—¶åºå›¾çš„æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡æœ‰å‘å›¾æ¨¡æ‹Ÿï¼Œå¯å›æº¯åŠ¨æ€å˜åŒ–çš„åŸå› ï¼Œå®Œå–„æ¨ç†å†…å®¹ï¼Œåˆ›å»ºæˆ–åˆ é™¤ç°æœ‰åŸå› ï¼Œä»¥å¾—å‡ºæœ€ä½³å»ºè®®æˆ–ç­”æ¡ˆã€‚åŒæ—¶ï¼Œè€ƒè™‘ä¸åŒæ—¶é—´ç‚¹çš„å¤šæ¨¡å¼æ•°æ®å¯è¿½è¸ªå’Œåˆ†ææ‚£è€…å¥åº·å’Œç–¾ç—…è¿›å±•ã€‚åˆæ­¥å®éªŒå’Œåˆ†æç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„åˆ›æ–°æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»ç–—å¥åº·é¢†åŸŸéœ€è¦å¤„ç†å¤šæ¨¡å¼æ•°æ®ä»¥è¿›è¡Œæ¨ç†å’Œè¯Šæ–­å¤šç§ç–¾ç—…ã€‚</li>
<li>å½“å‰æå‡ºçš„ä¸€ç§åŸºäºæ—¶åºå›¾çš„æ¨ç†è¿‡ç¨‹å¯ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„åŸå› ï¼Œå¹¶å¸®åŠ©å®Œå–„æ¨ç†å†…å®¹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ›å»ºæˆ–åˆ é™¤ç°æœ‰åŸå› æ¥å¾—å‡ºæœ€ä½³å»ºè®®æˆ–ç­”æ¡ˆã€‚</li>
<li>è€ƒè™‘ä¸åŒæ—¶é—´ç‚¹çš„å¤šæ¨¡å¼æ•°æ®æœ‰åŠ©äºè¿½è¸ªå’Œåˆ†ææ‚£è€…çš„å¥åº·å’Œç–¾ç—…è¿›å±•ã€‚</li>
<li>å¤šæ¨¡å¼æ¨ç†åœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œéœ€è¦æ›´å¤šçš„ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>æå‡ºçš„å¤šæ™ºèƒ½ä½“æ—¶åºæ¨ç†æ¡†æ¶é€šè¿‡ä»»åŠ¡åˆ†é…å’Œäº¤å‰éªŒè¯æœºåˆ¶è¿›ä¸€æ­¥æé«˜äº†æ¨ç†è¾“å‡ºçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11944v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11944v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FineQuest-Adaptive-Knowledge-Assisted-Sports-Video-Understanding-via-Agent-of-Thoughts-Reasoning"><a href="#FineQuest-Adaptive-Knowledge-Assisted-Sports-Video-Understanding-via-Agent-of-Thoughts-Reasoning" class="headerlink" title="FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via   Agent-of-Thoughts Reasoning"></a>FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via   Agent-of-Thoughts Reasoning</h2><p><strong>Authors:Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao</strong></p>
<p>Video Question Answering (VideoQA) based on Large Language Models (LLMs) has shown potential in general video understanding but faces significant challenges when applied to the inherently complex domain of sports videos. In this work, we propose FineQuest, the first training-free framework that leverages dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for straightforward sports queries and ii) Deliberative Reasoning for more complex ones. To bridge the knowledge gap between general-purpose models and domain-specific sports understanding, FineQuest incorporates SSGraph, a multimodal sports knowledge scene graph spanning nine sports, which encodes both visual instances and domain-specific terminology to enhance reasoning accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA and Diving-QA, derived from the FineGym and FineDiving datasets, enabling diverse and comprehensive evaluation. FineQuest achieves state-of-the-art performance on these benchmarks as well as the existing SPORTU dataset, while maintains strong general VideoQA capabilities. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰åœ¨é€šç”¨è§†é¢‘ç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å½“åº”ç”¨äºå†…åœ¨å¤æ‚çš„ä½“è‚²è§†é¢‘é¢†åŸŸæ—¶ï¼Œé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FineQuestï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒå—åˆ°è®¤çŸ¥ç§‘å­¦çš„å¯å‘ï¼Œé‡‡ç”¨åŒæ¨¡å¼æ¨ç†ï¼šiï¼‰é’ˆå¯¹ç›´æ¥çš„ä½“è‚²æŸ¥è¯¢é‡‡ç”¨ååº”å¼æ¨ç†ï¼Œiiï¼‰é’ˆå¯¹æ›´å¤æ‚çš„æŸ¥è¯¢é‡‡ç”¨å®¡æ…æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥é€šç”¨æ¨¡å‹ä¸ç‰¹å®šé¢†åŸŸä½“è‚²ç†è§£ä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼ŒFineQuestå¼•å…¥äº†SSGraphï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨è¶Šä¹ç§ä½“è‚²çš„å¤šæ¨¡å¼ä½“è‚²çŸ¥è¯†åœºæ™¯å›¾ï¼Œå®ƒç¼–ç è§†è§‰å®ä¾‹å’Œç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ï¼Œä»¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºFineGymå’ŒFineDivingæ•°æ®é›†æ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„ä½“è‚²VideoQAåŸºå‡†æµ‹è¯•ï¼Œå³Gym-QAå’ŒDiving-QAï¼Œå®ƒä»¬èƒ½å¤Ÿè¿›è¡Œå¤šæ ·åŒ–å’Œå…¨é¢çš„è¯„ä¼°ã€‚FineQueståœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä»¥åŠç°æœ‰çš„SPORTUæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨VideoQAèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11796v1">PDF</a> ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æŠ€æœ¯åœ¨é€šç”¨è§†é¢‘ç†è§£é¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨åº”ç”¨äºå¤æ‚çš„ä½“è‚²è§†é¢‘é¢†åŸŸæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºFineQuestï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒå€Ÿé‰´è®¤çŸ¥ç§‘å­¦å¯å‘é‡‡ç”¨åŒæ¨¡å¼æ¨ç†ï¼ši) ç›´è§‚æ¨ç†åº”å¯¹ç›´æ¥çš„ä½“è‚²æŸ¥è¯¢å’Œii) å®¡æ…æ¨ç†åº”å¯¹æ›´å¤æ‚çš„æŸ¥è¯¢ã€‚ä¸ºäº†å¼¥è¡¥é€šç”¨æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸä½“è‚²ç†è§£ä¹‹é—´çš„çŸ¥è¯†é¸¿æ²Ÿï¼ŒFineQuestå¼•å…¥äº†SSGraphï¼Œä¸€ä¸ªè·¨è¶Šä¹ç§ä½“è‚²çš„å¤šæ¨¡å¼ä½“è‚²çŸ¥è¯†åœºæ™¯å›¾ï¼Œå®ƒç¼–ç è§†è§‰å®ä¾‹å’Œç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ï¼Œä»¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„ä½“è‚²VideoQAåŸºå‡†æµ‹è¯•ï¼ŒGym-QAå’ŒDiving-QAï¼Œå®ƒä»¬åˆ†åˆ«æ¥è‡ªFineGymå’ŒFineDivingæ•°æ®é›†ï¼Œä»¥å®ç°å¤šæ ·åŒ–å’Œå…¨é¢çš„è¯„ä¼°ã€‚FineQueståœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä»¥åŠç°æœ‰çš„SPORTUæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨VideoQAèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FineQuestæ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒçš„VideoQAæ¡†æ¶ï¼Œé€‚ç”¨äºä½“è‚²è§†é¢‘é¢†åŸŸã€‚</li>
<li>å®ƒé‡‡ç”¨åŒæ¨¡å¼æ¨ç†ï¼ŒåŒ…æ‹¬ç›´è§‚æ¨ç†å’Œå®¡æ…æ¨ç†ã€‚</li>
<li>FineQuesté€šè¿‡å¼•å…¥SSGraphæ¥å¼¥è¡¥é€šç”¨æ¨¡å‹å’Œä½“è‚²é¢†åŸŸçŸ¥è¯†ä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
<li>SSGraphæ˜¯ä¸€ä¸ªå¤šæ¨¡å¼ä½“è‚²çŸ¥è¯†åœºæ™¯å›¾ï¼ŒåŒ…å«è§†è§‰å®ä¾‹å’Œç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ã€‚</li>
<li>ç ”ç©¶äººå‘˜æ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„ä½“è‚²VideoQAåŸºå‡†æµ‹è¯•ï¼šGym-QAå’ŒDiving-QAã€‚</li>
<li>FineQueståœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11796v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11796v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11796v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11796v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Reasoning-and-Agentic-Systems-in-Time-Series-with-Large-Language-Models"><a href="#A-Survey-of-Reasoning-and-Agentic-Systems-in-Time-Series-with-Large-Language-Models" class="headerlink" title="A Survey of Reasoning and Agentic Systems in Time Series with Large   Language Models"></a>A Survey of Reasoning and Agentic Systems in Time Series with Large   Language Models</h2><p><strong>Authors:Ching Chang, Yidan Shi, Defu Cao, Wei Yang, Jeehyun Hwang, Haixin Wang, Jiacheng Pang, Wei Wang, Yan Liu, Wen-Chih Peng, Tien-Fu Chen</strong></p>
<p>Time series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (<a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/Time-Series-Reasoning-Survey">https://github.com/blacksnail789521/Time-Series-Reasoning-Survey</a>). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ¨ç†å°†æ—¶é—´è§†ä¸ºç¬¬ä¸€è½´çº¿ï¼Œå¹¶å°†ä¸­é—´è¯æ®ç›´æ¥èå…¥ç­”æ¡ˆä¸­ã€‚æœ¬æ–‡é€šè¿‡æ¨ç†æ‹“æ‰‘å®šä¹‰äº†é—®é¢˜å¹¶ç»„ç»‡äº†æ–‡çŒ®ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªå®¶æ—ï¼šä¸€æ­¥ç›´æ¥æ¨ç†ã€å…·æœ‰æ˜ç¡®ä¸­é—´ä½“çš„çº¿æ€§é“¾æ¨ç†å’Œæ¢ç©¶ã€ä¿®è®¢å’Œèšåˆçš„åˆ†æ”¯ç»“æ„æ¨ç†ã€‚æ‹“æ‰‘ä¸é¢†åŸŸçš„ä¸»è¦ç›®æ ‡ç›¸äº¤å‰ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—åˆ†æã€è§£é‡Šå’Œç†è§£ã€å› æœæ¨ç†å’Œå†³ç­–åˆ¶å®šä»¥åŠæ—¶é—´åºåˆ—ç”Ÿæˆç­‰ã€‚åŒæ—¶ï¼Œä¸€ä¸ªç´§å‡‘çš„æ ‡ç­¾é›†è´¯ç©¿è¿™äº›è½´ï¼ŒåŒ…æ‹¬åˆ†è§£å’ŒéªŒè¯ã€é›†æˆã€å·¥å…·ä½¿ç”¨ã€çŸ¥è¯†è®¿é—®ã€å¤šæ¨¡æ€æ€§ã€ä»£ç†å¾ªç¯å’ŒLLMå¯¹é½æœºåˆ¶ç­‰ã€‚æœ¬æ–‡å›é¡¾äº†ä¸åŒé¢†åŸŸçš„æ–¹æ³•å’Œç³»ç»Ÿï¼Œå±•ç¤ºäº†æ¯ç§æ‹“æ‰‘çš„ä¼˜ç¼ºç‚¹ï¼Œä»¥åŠæ”¯æŒç ”ç©¶å’Œéƒ¨ç½²çš„æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œèµ„æºï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/Time-Series-Reasoning-Survey%EF%BC%89%E3%80%82%E6%9C%AC%E6%96%87%E5%BC%BA%E8%B0%83%E4%BA%86%E4%BF%9D%E6%8C%81%E8%AF%81%E6%8D%AE%E5%8F%AF%E8%A7%81%E5%92%8C%E6%97%B6%E5%BA%8F%E5%AF%B9%E9%BD%90%E7%9A%84%E8%AF%84%E4%BC%B0%E5%AE%9E%E8%B7%B5%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E4%BA%86%E5%8C%B9%E9%85%8D%E6%8B%93%E6%89%91%E4%B8%8E%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E3%80%81%E4%B8%8E%E5%8F%AF%E8%A7%82%E5%AF%9F%E5%88%B0%E7%9A%84%E4%BC%AA%E8%BF%B9%E8%BF%9B%E8%A1%8C%E6%8E%A5%E5%9C%B0%E3%80%81%E4%B8%BA%E8%BD%AC%E5%8F%98%E5%92%8C%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E8%BF%9B%E8%A1%8C%E8%A7%84%E5%88%92%E4%BB%A5%E5%8F%8A%E5%B0%86%E6%88%90%E6%9C%AC%E5%92%8C%E5%BB%B6%E8%BF%9F%E8%A7%86%E4%B8%BA%E8%AE%BE%E8%AE%A1%E9%A2%84%E7%AE%97%E7%9A%84%E6%8C%87%E5%AF%BC%E5%8E%9F%E5%88%99%E3%80%82%E6%88%91%E4%BB%AC%E5%BC%BA%E8%B0%83%EF%BC%8C%E6%8E%A8%E7%90%86%E7%BB%93%E6%9E%84%E5%BF%85%E9%A1%BB%E5%9C%A8%E4%BF%9D%E8%AF%81%E6%8E%A5%E5%9C%B0%E5%92%8C%E8%87%AA%E6%A0%A1%E6%AD%A3%E8%83%BD%E5%8A%9B%E7%9A%84%E5%90%8C%E6%97%B6%EF%BC%8C%E5%B9%B3%E8%A1%A1%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC%E5%92%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%BF%9B%E5%B1%95%E5%8F%AF%E8%83%BD%E5%8F%96%E5%86%B3%E4%BA%8E%E5%B0%86%E6%8E%A8%E7%90%86%E8%B4%A8%E9%87%8F%E4%B8%8E%E6%95%88%E7%94%A8%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E4%BB%A5%E5%8F%8A%E8%83%BD%E5%A4%9F%E5%9C%A8%E6%88%90%E6%9C%AC%E5%92%8C%E9%A3%8E%E9%99%A9%E4%B9%8B%E9%97%B4%E8%BF%9B%E8%A1%8C%E6%9D%83%E8%A1%A1%E7%9A%84%E9%97%AD%E7%8E%AF%E6%B5%8B%E8%AF%95%E5%B9%B3%E5%8F%B0%EF%BC%8C%E4%BB%A5%E9%80%82%E5%BA%94%E8%BD%AC%E5%8F%98%E6%84%8F%E8%AF%86%E3%80%81%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E5%92%8C%E9%95%BF%E6%9C%9F%E8%A7%86%E9%87%8E%E8%AE%BE%E7%BD%AE%E3%80%82%E7%BB%BC%E4%B8%8A%E6%89%80%E8%BF%B0%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%96%B9%E5%90%91%E6%A0%87%E5%BF%97%E7%9D%80%E4%BB%8E%E7%8B%AD%E9%9A%98%E7%9A%84%E5%87%86%E7%A1%AE%E6%80%A7%E5%90%91%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%8F%AF%E9%9D%A0%E6%80%A7%E8%BD%AC%E5%8F%98%EF%BC%8C%E4%BD%BF%E7%B3%BB%E7%BB%9F%E4%B8%8D%E4%BB%85%E8%83%BD%E5%A4%9F%E5%88%86%E6%9E%90%EF%BC%8C%E8%80%8C%E4%B8%94%E8%83%BD%E5%A4%9F%E7%90%86%E8%A7%A3%E3%80%81%E8%A7%A3%E9%87%8A%E5%92%8C%E5%8A%A8%E6%80%81%E4%B8%96%E7%95%8C%E8%A1%8C%E5%8A%A8%E5%85%B7%E6%9C%89%E5%8F%AF%E8%BF%BD%E6%BA%AF%E7%9A%84%E8%AF%81%E6%8D%AE%E5%92%8C%E5%8F%AF%E4%BF%A1%E7%9A%84%E7%BB%93%E6%9E%9C%E3%80%82">https://github.com/blacksnail789521/Time-Series-Reasoning-Surveyï¼‰ã€‚æœ¬æ–‡å¼ºè°ƒäº†ä¿æŒè¯æ®å¯è§å’Œæ—¶åºå¯¹é½çš„è¯„ä¼°å®è·µï¼Œå¹¶æä¾›äº†åŒ¹é…æ‹“æ‰‘ä¸ä¸ç¡®å®šæ€§ã€ä¸å¯è§‚å¯Ÿåˆ°çš„ä¼ªè¿¹è¿›è¡Œæ¥åœ°ã€ä¸ºè½¬å˜å’Œæµå¼å¤„ç†è¿›è¡Œè§„åˆ’ä»¥åŠå°†æˆæœ¬å’Œå»¶è¿Ÿè§†ä¸ºè®¾è®¡é¢„ç®—çš„æŒ‡å¯¼åŸåˆ™ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œæ¨ç†ç»“æ„å¿…é¡»åœ¨ä¿è¯æ¥åœ°å’Œè‡ªæ ¡æ­£èƒ½åŠ›çš„åŒæ—¶ï¼Œå¹³è¡¡è®¡ç®—æˆæœ¬å’Œå¯é‡å¤æ€§ã€‚æœªæ¥çš„è¿›å±•å¯èƒ½å–å†³äºå°†æ¨ç†è´¨é‡ä¸æ•ˆç”¨è”ç³»èµ·æ¥çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠèƒ½å¤Ÿåœ¨æˆæœ¬å’Œé£é™©ä¹‹é—´è¿›è¡Œæƒè¡¡çš„é—­ç¯æµ‹è¯•å¹³å°ï¼Œä»¥é€‚åº”è½¬å˜æ„è¯†ã€æµå¼å¤„ç†å’Œé•¿æœŸè§†é‡è®¾ç½®ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™äº›æ–¹å‘æ ‡å¿—ç€ä»ç‹­éš˜çš„å‡†ç¡®æ€§å‘å¤§è§„æ¨¡å¯é æ€§è½¬å˜ï¼Œä½¿ç³»ç»Ÿä¸ä»…èƒ½å¤Ÿåˆ†æï¼Œè€Œä¸”èƒ½å¤Ÿç†è§£ã€è§£é‡Šå’ŒåŠ¨æ€ä¸–ç•Œè¡ŒåŠ¨å…·æœ‰å¯è¿½æº¯çš„è¯æ®å’Œå¯ä¿¡çš„ç»“æœã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11575v1">PDF</a> This paper is currently under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ—¶é—´åºåˆ—æ¨ç†å°†æ—¶é—´è§†ä¸ºé¦–è¦è€ƒé‡è½´ï¼Œå¹¶å°†ä¸­é—´è¯æ®ç›´æ¥çº³å…¥ç­”æ¡ˆä¸­ã€‚è¿™ç¯‡ç»¼è¿°å®šä¹‰äº†é—®é¢˜å¹¶é€šè¿‡æ¨ç†æ‹“æ‰‘å¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»ï¼Œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªå®¶æ—ï¼šä¸€æ­¥ç›´æ¥æ¨ç†ã€å…·æœ‰æ˜ç¡®ä¸­é—´ä½“çš„çº¿æ€§é“¾æ¨ç†å’Œåˆ†æ”¯ç»“æ„æ¨ç†ã€‚æ‹“æ‰‘ä¸é¢†åŸŸçš„ä¸»è¦ç›®æ ‡ç›¸ç»“åˆï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ—¶é—´åºåˆ—åˆ†æã€è§£é‡Šä¸ç†è§£ã€å› æœæ¨æ–­ä¸å†³ç­–åˆ¶å®šä»¥åŠæ—¶é—´åºåˆ—ç”Ÿæˆã€‚åŒæ—¶ï¼Œç´§å‡‘çš„æ ‡ç­¾é›†è·¨è¶Šè¿™äº›è½´ï¼Œæ¶µç›–åˆ†è§£ä¸éªŒè¯ã€é›†æˆã€å·¥å…·ä½¿ç”¨ã€çŸ¥è¯†è®¿é—®ã€å¤šæ¨¡æ€æ€§ã€ä»£ç†å¾ªç¯å’ŒLLMå¯¹é½åˆ¶åº¦ç­‰å†…å®¹ã€‚æœ¬æ–‡å›é¡¾äº†ä¸åŒé¢†åŸŸçš„æ–¹æ³•å’Œç³»ç»Ÿï¼Œå±•ç¤ºäº†æ¯ç§æ‹“æ‰‘çš„ä¼˜ç¼ºç‚¹åŠå…¶åœ¨ä¿çœŸåº¦æˆ–ç¨³å¥æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠæ”¯æŒç ”ç©¶å’Œéƒ¨ç½²çš„æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œèµ„æºã€‚å¼ºè°ƒäº†ä¿æŒè¯æ®å¯è§æ€§å’Œæ—¶é—´å¯¹é½æ€§çš„è¯„ä¼°å®è·µï¼Œå¹¶æä¾›äº†åŒ¹é…æ‹“æ‰‘ä¸ä¸ç¡®å®šæ€§ã€ç”¨å¯è§‚å¯Ÿåˆ°çš„æ–‡ç‰©è¿›è¡Œæ¥åœ°å¤„ç†ã€è§„åˆ’è½¬å˜å’Œæµå¼ä¼ è¾“ä»¥åŠå°†æˆæœ¬å’Œå»¶è¿Ÿè§†ä¸ºè®¾è®¡é¢„ç®—çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œæ¨ç†ç»“æ„å¿…é¡»åœ¨ä¿æŒå¯¹åŸºç¡€çŸ¥è¯†å’Œè‡ªæˆ‘æ ¡æ­£èƒ½åŠ›çš„åŒæ—¶ï¼Œå¹³è¡¡è®¡ç®—æˆæœ¬å’Œå¯é‡å¤æ€§ã€‚æœªæ¥çš„è¿›å±•å¯èƒ½å–å†³äºå°†æ¨ç†è´¨é‡ä¸å®ç”¨æ€§è”ç³»èµ·æ¥çš„åŸºå‡†æµ‹è¯•ä»¥åŠåœ¨æˆæœ¬å’Œé£é™©æƒè¡¡ä¸‹é€‚åº”è½¬å˜æ„è¯†ã€æµå¼ä¼ è¾“å’Œé•¿æœŸè§†é‡è®¾ç½®çš„é—­ç¯æµ‹è¯•å¹³å°ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›æ–¹å‘æ ‡å¿—ç€ä»ç‹­éš˜çš„å‡†ç¡®æ€§å‘å¤§è§„æ¨¡å¯é æ€§è½¬å˜çš„è½¬ç§»ï¼Œä½¿ç³»ç»Ÿä¸ä»…èƒ½å¤Ÿåˆ†æï¼Œè€Œä¸”èƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œé‡‡å–å®é™…è¡ŒåŠ¨åº”å¯¹åŠ¨æ€ä¸–ç•Œï¼ŒåŒæ—¶æ‹¥æœ‰å¯è¿½æº¯çš„è¯æ®å’Œå¯é çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—æ¨ç†å°†æ—¶é—´è§†ä¸ºé¦–è¦å› ç´ ï¼Œå°†ä¸­é—´è¯æ®çº³å…¥ç­”æ¡ˆã€‚</li>
<li>ç»¼è¿°å®šä¹‰äº†é—®é¢˜å¹¶é€šè¿‡æ¨ç†æ‹“æ‰‘åˆ†ç±»æ–‡çŒ®ä¸ºä¸‰ä¸ªå®¶æ—ã€‚</li>
<li>æ¨ç†æ‹“æ‰‘ä¸é¢†åŸŸç›®æ ‡ç›¸ç»“åˆï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—åˆ†æã€è§£é‡Šä¸ç†è§£ç­‰ã€‚</li>
<li>ç´§å‡‘æ ‡ç­¾é›†æ¶µç›–åˆ†è§£ä¸éªŒè¯ã€å·¥å…·ä½¿ç”¨å’Œå¤šæ¨¡æ€æ€§ç­‰å…³é”®æ–¹é¢ã€‚</li>
<li>æ–¹æ³•å’Œç³»ç»Ÿçš„å›é¡¾å±•ç¤ºäº†å„ç§æ‹“æ‰‘çš„ä¼˜ç¼ºç‚¹åŠå±€é™æ€§ã€‚</li>
<li>è¯„ä¼°å®è·µå¼ºè°ƒä¿æŒè¯æ®çš„å¯è§æ€§å’Œæ—¶é—´å¯¹é½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11575v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11575v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11575v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Formal-Reasoning-for-Intelligent-QA-Systems-A-Case-Study-in-the-Educational-Domain"><a href="#Formal-Reasoning-for-Intelligent-QA-Systems-A-Case-Study-in-the-Educational-Domain" class="headerlink" title="Formal Reasoning for Intelligent QA Systems: A Case Study in the   Educational Domain"></a>Formal Reasoning for Intelligent QA Systems: A Case Study in the   Educational Domain</h2><p><strong>Authors:Tuan Bui, An Nguyen, Phat Thai, Minh Hua, Ngan Pham L. N., Ngan Pham T. B., Dung Le, Long Nguyen, Thanh-Tung Tran, Thang Bui, Tho Quan</strong></p>
<p>Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful - serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions.   In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness. </p>
<blockquote>
<p>æ¨ç†å¯¹äºå°é—­é¢†åŸŸçš„é—®ç­”ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ç¨‹åºæ­£ç¡®æ€§å’Œæ”¿ç­–åˆè§„æ€§æ–¹é¢ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†æœ€è¿‘çš„å·¥ä½œè¡¨æ˜ï¼Œå®ƒä»¬çš„æ¨ç†è½¨è¿¹é€šå¸¸ä¸å¿ å®â€”â€”æ›´åƒæ˜¯ä¸€ç§åˆç†çš„è¾©è§£ï¼Œè€Œä¸æ˜¯åŸºäºå› æœçš„æ¨å¯¼ã€‚å°†LLMä¸ç¬¦å·å¼•æ“ï¼ˆä¾‹å¦‚Prover9ã€ZIç­‰ï¼‰ç›¸ç»“åˆçš„å°è¯•æé«˜äº†å¯é æ€§ï¼Œä½†ä»ç„¶å±€é™äºé™æ€å½¢å¼çš„é€»è¾‘ï¼Œå¯¹äºåŠ¨æ€ã€åŸºäºçŠ¶æ€çš„ç†ç”±ï¼ˆå¦‚å¤šæ­¥éª¤è¿›å±•å’Œæ¡ä»¶è½¬æ¢ï¼‰æ„Ÿåˆ°å›°æƒ‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MCFRï¼ˆå½¢å¼æ¨ç†çš„æ¨¡å‹æ£€æŸ¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç¥ç»ç¬¦å·æ¡†æ¶ï¼Œå®ƒå°†LLMä¸æ¨¡å‹æ£€æŸ¥ç›¸ç»“åˆï¼Œæ”¯æŒå±æ€§éªŒè¯ã€‚MCFRå°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆå½¢å¼åŒ–è§„èŒƒï¼Œå¹¶åœ¨è½¬æ¢æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EduMC-QAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçœŸå®å­¦æœ¯æµç¨‹çš„åŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMCFRæé«˜äº†æ¨ç†çš„å¿ å®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºé«˜é£é™©å°é—­é¢†åŸŸä¸­çš„å¯éªŒè¯é—®ç­”æä¾›äº†ä¸€æ¡å¯è¡Œçš„é€”å¾„ã€‚é™¤äº†è¯„ä¼°MCFRå¤–ï¼Œæˆ‘ä»¬è¿˜å°†å…¶æ€§èƒ½ä¸æœ€æ–°çš„LLMï¼ˆå¦‚ChatGPTã€DeepSeekå’ŒClaudeï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥å¯¹å…¶æœ‰æ•ˆæ€§è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11572v1">PDF</a> Published at the 2nd ACM Workshop in AI-powered Question &amp; Answering   Systems (AIQAM â€˜25), co-located with ACM Multimedia 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ–‡ä¸­æå‡ºäº†ä¸€ç§èåˆç¥ç»ç¬¦å·æ¡†æ¶MCFRï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸æ¨¡å‹æ£€æµ‹æ”¯æŒå±æ€§éªŒè¯ï¼Œä»¥æé«˜å°é—­åŸŸé—®ç­”ç³»ç»Ÿä¸­æ¨ç†çš„å¿ å®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶å¯å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸ºå½¢å¼è§„èŒƒå¹¶è¿›è¡ŒéªŒè¯ã€‚ä¸ºæ”¯æŒè¯„ä¼°ï¼Œå¼•å…¥äº†åŸºäºçœŸå®å­¦æœ¯æµç¨‹çš„åŸºå‡†æ•°æ®é›†EduMC-QAã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹æ¯”äº†MCFRä¸ChatGPTã€DeepSeekã€Claudeç­‰å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œçªæ˜¾å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†åœ¨å°é—­åŸŸé—®ç­”ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œæ¶‰åŠç¨‹åºæ­£ç¡®æ€§å’Œæ”¿ç­–éµå®ˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†è½¨è¿¹å¸¸ç¼ºä¹å¿ å®æ€§ã€‚</li>
<li>èåˆLLMsä¸ç¬¦å·å¼•æ“ï¼ˆå¦‚Prover9ã€Z3ï¼‰æé«˜äº†å¯é æ€§ï¼Œä½†åœ¨åŠ¨æ€ã€åŸºäºçŠ¶æ€çš„æ¨ç†æ–¹é¢ä»æœ‰å±€é™ã€‚</li>
<li>MCFRæ¡†æ¶ç»“åˆLLMsä¸æ¨¡å‹æ£€æµ‹ï¼Œæ”¯æŒå±æ€§éªŒè¯ï¼Œæé«˜æ¨ç†å¿ å®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>MCFRå¯å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸ºå½¢å¼è§„èŒƒå¹¶åœ¨è½¬æ¢æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ã€‚</li>
<li>ä¸ºæ”¯æŒè¯„ä¼°ï¼Œå¼•å…¥äº†åŸºäºçœŸå®å­¦æœ¯æµç¨‹çš„åŸºå‡†æ•°æ®é›†EduMC-QAã€‚</li>
<li>MCFRä¸å…¶ä»–å‰æ²¿LLMsçš„å¯¹æ¯”è¯„ä¼°æ˜¾ç¤ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11572v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="D-2-HScore-Reasoning-Aware-Hallucination-Detection-via-Semantic-Breadth-and-Depth-Analysis-in-LLMs"><a href="#D-2-HScore-Reasoning-Aware-Hallucination-Detection-via-Semantic-Breadth-and-Depth-Analysis-in-LLMs" class="headerlink" title="D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic   Breadth and Depth Analysis in LLMs"></a>D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic   Breadth and Depth Analysis in LLMs</h2><p><strong>Authors:Yue Ding, Xiaofang Zhu, Tianze Xia, Junfei Wu, Xinlong Chen, Qiang Liu, Liang Wang</strong></p>
<p>Although large Language Models (LLMs) have achieved remarkable success, their practical application is often hindered by the generation of non-factual content, which is called â€œhallucinationâ€. Ensuring the reliability of LLMsâ€™ outputs is a critical challenge, particularly in high-stakes domains such as finance, security, and healthcare. In this work, we revisit hallucination detection from the perspective of model architecture and generation dynamics. Leveraging the multi-layer structure and autoregressive decoding process of LLMs, we decompose hallucination signals into two complementary dimensions: the semantic breadth of token representations within each layer, and the semantic depth of core concepts as they evolve across layers. Based on this insight, we propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)}, a training-free and label-free framework that jointly measures: (1) \textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of token representations within each layer; and (2) \textbf{Inter-Layer Drift}, which tracks the progressive transformation of key token representations across layers. To ensure drift reflects the evolution of meaningful semantics rather than noisy or redundant tokens, we guide token selection using attention signals. By capturing both the horizontal and vertical dynamics of representation during inference, D$^2$HScore provides an interpretable and lightweight proxy for hallucination detection. Extensive experiments across five open-source LLMs and five widely used benchmarks demonstrate that D$^2$HScore consistently outperforms existing training-free baselines. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†å®ƒä»¬åœ¨å®è·µåº”ç”¨æ—¶å¸¸å—åˆ°ç”Ÿæˆéäº‹å®å†…å®¹ï¼ˆç§°ä¸ºâ€œå¹»è§‰â€ï¼‰çš„é˜»ç¢ã€‚ç¡®ä¿LLMè¾“å‡ºçš„å¯é æ€§æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èã€å®‰å…¨å’ŒåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ¨¡å‹æ¶æ„å’Œç”ŸæˆåŠ¨æ€çš„è§’åº¦é‡æ–°å®¡è§†äº†å¹»è§‰æ£€æµ‹ã€‚æˆ‘ä»¬åˆ©ç”¨LLMçš„å¤šå±‚ç»“æ„å’Œè‡ªå›å½’è§£ç è¿‡ç¨‹ï¼Œå°†å¹»è§‰ä¿¡å·åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥çš„ç»´åº¦ï¼šæ¯å±‚å†…ä»¤ç‰Œè¡¨ç¤ºçš„è¯­ä¹‰å¹¿åº¦ä»¥åŠæ ¸å¿ƒæ¦‚å¿µåœ¨è·¨å±‚æ—¶çš„è¯­ä¹‰æ·±åº¦ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>D$^2$HScoreï¼ˆåŸºäºåˆ†æ•£å’Œæ¼‚ç§»çš„å¹»è§‰è¯„åˆ†ï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒå’Œæ ‡ç­¾çš„å…è´¹æ¡†æ¶ï¼Œå®ƒè”åˆæµ‹é‡ï¼šï¼ˆ1ï¼‰<strong>å±‚å†…åˆ†æ•£åº¦</strong>ï¼Œé‡åŒ–æ¯å±‚å†…ä»¤ç‰Œè¡¨ç¤ºçš„è¯­ä¹‰å¤šæ ·æ€§ï¼›ï¼ˆ2ï¼‰<strong>å±‚é—´æ¼‚ç§»</strong>ï¼Œè·Ÿè¸ªå…³é”®ä»¤ç‰Œè¡¨ç¤ºåœ¨è·¨å±‚çš„æ¸è¿›å¼è½¬æ¢ã€‚ä¸ºäº†ç¡®ä¿æ¼‚ç§»åæ˜ æœ‰æ„ä¹‰è¯­ä¹‰çš„æ¼”å˜ï¼Œè€Œä¸æ˜¯å˜ˆæ‚æˆ–å†—ä½™çš„ä»¤ç‰Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›ä¿¡å·æ¥å¼•å¯¼ä»¤ç‰Œé€‰æ‹©ã€‚é€šè¿‡æ•æ‰æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨ªå‘å’Œçºµå‘åŠ¨æ€ï¼ŒD$^2$HScoreä¸ºå¹»è§‰æ£€æµ‹æä¾›äº†ä¸€ä¸ªå¯è§£é‡Šä¸”è½»é‡çº§çš„ä»£ç†ã€‚åœ¨äº”ä¸ªå¼€æºLLMå’Œäº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒD$^2$HScoreå§‹ç»ˆä¼˜äºç°æœ‰çš„æ— éœ€è®­ç»ƒçš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11569v1">PDF</a> under review</p>
<p><strong>Summary</strong>ï¼šè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä½†å…¶åœ¨é‡‘èã€å®‰å…¨ã€åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„åº”ç”¨å—åˆ°äº†è¾“å‡ºå†…å®¹éäº‹å®æ€§çš„é™åˆ¶ï¼Œå³â€œå¹»è§‰â€ç°è±¡ã€‚æœ¬æ–‡é‡æ–°æ¢è®¨äº†ä»æ¨¡å‹æ¶æ„å’Œç”ŸæˆåŠ¨æ€è§’åº¦æ£€æµ‹å¹»è§‰çš„æ–¹æ³•ã€‚åˆ©ç”¨LLMçš„å¤šå±‚ç»“æ„å’Œè‡ªå›å½’è§£ç è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†å¹»è§‰ä¿¡å·åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥çš„ç»´åº¦ï¼šæ¯å±‚å†…ä»¤ç‰Œè¡¨ç¤ºçš„è¯­ä¹‰å¹¿åº¦ï¼Œä»¥åŠæ ¸å¿ƒæ¦‚å¿µè·¨å±‚æ¼”å˜çš„è¯­ä¹‰æ·±åº¦ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨çš„D$^2$HScoreæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆæµ‹é‡ï¼šï¼ˆ1ï¼‰<strong>Intra-Layer Dispersion</strong>ï¼Œé‡åŒ–æ¯å±‚å†…ä»¤ç‰Œè¡¨ç¤ºçš„è¯­ä¹‰å¤šæ ·æ€§ï¼›ï¼ˆ2ï¼‰<strong>Inter-Layer Drift</strong>ï¼Œè·Ÿè¸ªå…³é”®ä»¤ç‰Œè¡¨ç¤ºè·¨å±‚çš„æ¸è¿›å˜åŒ–ã€‚ä¸ºäº†ç¡®ä¿æ¼‚ç§»åæ˜ æœ‰æ„ä¹‰è¯­ä¹‰çš„æ¼”å˜è€Œéå™ªå£°æˆ–å†—ä½™ä»¤ç‰Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›ä¿¡å·æ¥æŒ‡å¯¼ä»¤ç‰Œé€‰æ‹©ã€‚D$^2$HScoreé€šè¿‡æ•æ‰æ¨ç†è¿‡ç¨‹ä¸­çš„æ°´å¹³ï¼ˆå±‚å†…å¤šæ ·æ€§ï¼‰å’Œå‚ç›´ï¼ˆå±‚é—´å˜åŒ–ï¼‰åŠ¨æ€æ€§ï¼Œæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šä¸”è½»é‡çº§çš„å¹»è§‰æ£€æµ‹ä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶ä¼šå‘ç”Ÿéäº‹å®æ€§çš„å¹»è§‰ç°è±¡ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èã€å®‰å…¨å’ŒåŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸã€‚</li>
<li>æœ¬æ–‡é€šè¿‡é‡æ–°å®¡è§†æ¨¡å‹æ¶æ„å’Œç”ŸæˆåŠ¨æ€æ€§æ¥è§£å†³å¹»è§‰æ£€æµ‹é—®é¢˜ã€‚</li>
<li>æå‡ºäº†D$^2$HScoreæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œèƒ½è”åˆæµ‹é‡å±‚å†…ä»¤ç‰Œçš„è¯­ä¹‰å¤šæ ·æ€§å’Œè·¨å±‚çš„è¯­ä¹‰å˜åŒ–ã€‚</li>
<li>D$^2$HScoreæ¡†æ¶åˆ©ç”¨æ³¨æ„åŠ›ä¿¡å·æ¥æŒ‡å¯¼ä»¤ç‰Œé€‰æ‹©ï¼Œç¡®ä¿æ£€æµ‹åˆ°çš„è¯­ä¹‰å˜åŒ–æ˜¯æœ‰æ„ä¹‰çš„ã€‚</li>
<li>é€šè¿‡åœ¨äº”ä¸ªå¼€æºLLMså’Œäº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¯æ˜D$^2$HScoreåœ¨å¹»è§‰æ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>D$^2$HScoreä¸ºå¹»è§‰æ£€æµ‹æä¾›äº†å¯è§£é‡Šä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ•æ‰æ¨ç†è¿‡ç¨‹ä¸­çš„æ°´å¹³ï¼ˆå±‚å†…ï¼‰å’Œå‚ç›´ï¼ˆå±‚é—´ï¼‰åŠ¨æ€æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11569v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning"><a href="#UI-S1-Advancing-GUI-Automation-via-Semi-online-Reinforcement-Learning" class="headerlink" title="UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning"></a>UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning</h2><p><strong>Authors:Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming Yan, Fei Huang, Jun Xiao, Yueting Zhuang</strong></p>
<p>Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1">https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨åŒ–å¤æ‚ç”¨æˆ·ç•Œé¢äº¤äº’æ–¹é¢å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é¢ä¸´ä¸€ä¸ªåŸºæœ¬å›°å¢ƒï¼šç¦»çº¿RLèƒ½å¤Ÿåœ¨é¢„æ”¶é›†çš„è½¨è¿¹ä¸Šè¿›è¡Œç¨³å®šè®­ç»ƒï¼Œä½†åœ¨å¤šæ­¥ä»»åŠ¡æ‰§è¡Œæ–¹é¢å› ç¼ºä¹è½¨è¿¹çº§åˆ«çš„å¥–åŠ±ä¿¡å·è€Œé‡åˆ°å›°éš¾ï¼›åœ¨çº¿RLé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ•æ‰è¿™äº›ä¿¡å·ï¼Œä½†å­˜åœ¨å¥–åŠ±ç¨€ç–å’Œéƒ¨ç½²æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ–°çš„æ¡†æ¶ä¸‹æ¨¡æ‹Ÿç¦»çº¿è½¨è¿¹ä¸Šçš„åœ¨çº¿RLçš„æ–°èŒƒå¼ã€‚åœ¨æ¯ä¸ªæ»šåŠ¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¿ç•™å¤šè½®å¯¹è¯ä¸­çš„åŸå§‹æ¨¡å‹è¾“å‡ºï¼Œå…¶ä¸­Patchæ¨¡å—è‡ªé€‚åº”åœ°æ¢å¤æ»šåŠ¨å’Œä¸“å®¶è½¨è¿¹ä¹‹é—´çš„åˆ†æ­§ã€‚ä¸ºäº†æ•æ‰é•¿æœŸçš„è®­ç»ƒä¿¡å·ï¼ŒåŠåœ¨çº¿RLå°†æŠ˜ç°æœªæ¥å›æŠ¥å¼•å…¥å¥–åŠ±è®¡ç®—ä¸­ï¼Œå¹¶ä¼˜åŒ–å…·æœ‰åŠ æƒæ­¥éª¤çº§å’Œå‰§é›†çº§çš„ä¼˜åŠ¿ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŠåœ¨çº¿æ€§èƒ½ï¼ˆSOPï¼‰ï¼Œä½œä¸ºæ›´è´´è¿‘çœŸå®åœ¨çº¿æ€§èƒ½çš„å®ç”¨æœ‰æ•ˆä»£ç†ï¼Œç”¨äºç°å®ä¸–ç•Œè¯„ä¼°çš„æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŠåœ¨çº¿RLåœ¨å››ä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¶Š7Bæ¨¡å‹çš„æœ€ä¼˜æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—çš„æå‡ï¼ˆä¾‹å¦‚ï¼Œåœ¨AndroidWorldä¸Šæå‡+12.0%ï¼Œåœ¨AITWä¸Šæå‡+23.8%ï¼‰ï¼Œåœ¨å¼¥åˆç¦»çº¿è®­ç»ƒæ•ˆç‡å’Œåœ¨çº¿å¤šè½®æ¨ç†ä¹‹é—´çš„å·®è·æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1%E3%80%82">https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11543v1">PDF</a> 22 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åŸºäºåŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆSemi-online Reinforcement Learningï¼‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–äº¤äº’æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åœ¨ç¦»çº¿è½¨è¿¹ä¸Šè¿›è¡Œæ¨¡æ‹Ÿåœ¨çº¿å­¦ä¹ æ¥è§£å†³äºŒè€…é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–°æ–¹æ³•å¼•å…¥äº†Patchæ¨¡å—æ¥æ¢å¤æ¨¡å‹è¾“å‡ºä¸å®é™…è½¨è¿¹ä¹‹é—´çš„åå·®ï¼Œå¹¶é‡‡ç”¨åŠ æƒæ­¥éª¤çº§åˆ«å’Œå‰§é›†çº§åˆ«çš„ä¼˜åŠ¿è¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Semi-Online Performanceï¼ˆSOPï¼‰ä½œä¸ºæ›´æ¥è¿‘çœŸå®åœ¨çº¿æ€§èƒ½çš„å®ç”¨è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆæ°´å¹³ï¼Œä¸”åœ¨AndroidWorldå’ŒAITWç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç»“åˆäº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¼˜ç‚¹ã€‚</li>
<li>é‡‡ç”¨äº†Patchæ¨¡å—æ¥æ¢å¤æ¨¡å‹è¾“å‡ºä¸å®é™…è½¨è¿¹ä¹‹é—´çš„åå·®ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŠ æƒæ­¥éª¤çº§åˆ«å’Œå‰§é›†çº§åˆ«çš„ä¼˜åŠ¿è¿›è¡Œä¼˜åŒ–ï¼Œæ•æ‰é•¿æœŸè®­ç»ƒä¿¡å·ã€‚</li>
<li>å¼•å…¥äº†Semi-Online Performanceï¼ˆSOPï¼‰ä½œä¸ºæ›´æ¥è¿‘çœŸå®åœ¨çº¿æ€§èƒ½çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†SOTAæ€§èƒ½ã€‚</li>
<li>åœ¨AndroidWorldå’ŒAITWç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11543v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11543v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11543v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11543v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HARP-Hallucination-Detection-via-Reasoning-Subspace-Projection"><a href="#HARP-Hallucination-Detection-via-Reasoning-Subspace-Projection" class="headerlink" title="HARP: Hallucination Detection via Reasoning Subspace Projection"></a>HARP: Hallucination Detection via Reasoning Subspace Projection</h2><p><strong>Authors:Junjie Hu, Gang Tu, ShengYu Cheng, Jinxin Li, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</strong></p>
<p>Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ï¼ˆhallucinationï¼‰ç°è±¡å¯¹å®ƒä»¬åœ¨å…³é”®å†³ç­–åˆ¶å®šä¸­çš„å¯é åº”ç”¨æ„æˆäº†é‡å¤§éšœç¢ã€‚å°½ç®¡ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•çš„å‡†ç¡®æ€§å·²ç»æé«˜ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åŒºåˆ†è¯­ä¹‰å’Œæ¨ç†ä¿¡æ¯ï¼Œå¹¶åœ¨ä¿æŒç¨³å¥æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HARPï¼ˆåŸºäºæ¨ç†å­ç©ºé—´æŠ•å½±çš„å¹»è§‰æ£€æµ‹ï¼‰è¿™ä¸€æ–°é¢–çš„å¹»è§‰æ£€æµ‹æ¡†æ¶ã€‚HARPè®¤ä¸ºï¼ŒLLMçš„éšè—çŠ¶æ€ç©ºé—´å¯ä»¥åˆ†è§£ä¸ºè¯­ä¹‰å­ç©ºé—´å’Œæ¨ç†å­ç©ºé—´çš„ç›´æ¥å’Œï¼Œå…¶ä¸­å‰è€…ç¼–ç è¯­è¨€è¡¨è¾¾ï¼Œåè€…æ•æ‰å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†Unembeddingå±‚å¯ä»¥è§£å¼€è¿™äº›å­ç©ºé—´ï¼Œå¹¶ä¸”é€šè¿‡å¯¹å…¶å‚æ•°åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œå¯ä»¥è·å¾—è·¨è¶Šè¯­ä¹‰å’Œæ¨ç†å­ç©ºé—´çš„åŸºå‘é‡ã€‚æœ€åï¼ŒHARPå°†éšè—çŠ¶æ€æŠ•å°„åˆ°æ¨ç†å­ç©ºé—´çš„åŸºå‘é‡ä¸Šï¼Œç„¶åå°†å¾—åˆ°çš„æŠ•å½±ç”¨ä½œæ£€æµ‹LLMä¸­å¹»è§‰çš„è¾“å…¥ç‰¹å¾ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›æŠ•å½±ï¼ŒHARPå°†ç‰¹å¾ç»´åº¦é™ä½åˆ°åŸå§‹å¤§å°çš„çº¦5%ï¼Œè¿‡æ»¤æ‰äº†å¤§éƒ¨åˆ†å™ªå£°ï¼Œå¹¶å®ç°äº†å¢å¼ºçš„ç¨³å¥æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHARPè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹»è§‰æ£€æµ‹æ€§èƒ½ï¼›ç‰¹åˆ«æ˜¯åœ¨TrivaQAä¸Šï¼Œå…¶AUROCè¾¾åˆ°äº†92.8%ï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º7.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11536v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHARPçš„æ–°å‹å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è§£éšè—çŠ¶æ€ç©ºé—´ä¸ºè¯­ä¹‰å­ç©ºé—´å’Œæ¨ç†å­ç©ºé—´ï¼Œåˆ©ç”¨Unembeddingå±‚å‚æ•°è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œå°†éšè—çŠ¶æ€æŠ•å½±åˆ°æ¨ç†å­ç©ºé—´çš„åŸºç¡€ä¸Šï¼Œå®ç°äº†å¯¹LLMä¸­çš„å¹»è§‰æ£€æµ‹ã€‚HARPæé«˜äº†æ£€æµ‹æ€§èƒ½ï¼Œå‡å°‘äº†ç‰¹å¾ç»´åº¦ï¼Œè¿‡æ»¤äº†å¤§éƒ¨åˆ†å™ªå£°ï¼Œå¢å¼ºäº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå‡ºå“è¶Šçš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰æ˜¯ä¸€ä¸ªå½±å“å…¶å¯é åº”ç”¨äºå…³é”®å†³ç­–åˆ¶å®šçš„ä¸»è¦éšœç¢ã€‚</li>
<li>ç°æœ‰å¹»è§‰æ£€æµ‹æ–¹æ³•çš„å‡†ç¡®æ€§æœ‰å¾…æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†è¯­ä¹‰å’Œæ¨ç†ä¿¡æ¯ä»¥åŠä¿æŒç¨³å¥æ€§æ–¹é¢ã€‚</li>
<li>HARPæ¡†æ¶é€šè¿‡åˆ†è§£éšè—çŠ¶æ€ç©ºé—´ä¸ºè¯­ä¹‰å­ç©ºé—´å’Œæ¨ç†å­ç©ºé—´æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Unembeddingå±‚èƒ½å¤Ÿè§£å¼€è¿™äº›å­ç©ºé—´ï¼Œé€šè¿‡å¯¹å…¶å‚æ•°è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œè·å¾—è·¨è¶Šè¯­ä¹‰å’Œæ¨ç†å­ç©ºé—´çš„åŸºå‘é‡ã€‚</li>
<li>HARPé€šè¿‡å°†éšè—çŠ¶æ€æŠ•å½±åˆ°æ¨ç†å­ç©ºé—´çš„åŸºå‘é‡ä¸Šï¼Œç„¶åä½¿ç”¨è¿™äº›æŠ•å½±ä½œä¸ºè¾“å…¥ç‰¹å¾æ¥è¿›è¡ŒLLMçš„å¹»è§‰æ£€æµ‹ã€‚</li>
<li>HARPå‡å°‘äº†ç‰¹å¾ç»´åº¦ï¼Œè¿‡æ»¤äº†å¤§éƒ¨åˆ†å™ªå£°ï¼Œå¢å¼ºäº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å¹»è§‰æ£€æµ‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11536v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11536v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11536v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Trading-R1-Financial-Trading-with-LLM-Reasoning-via-Reinforcement-Learning"><a href="#Trading-R1-Financial-Trading-with-LLM-Reasoning-via-Reinforcement-Learning" class="headerlink" title="Trading-R1: Financial Trading with LLM Reasoning via Reinforcement   Learning"></a>Trading-R1: Financial Trading with LLM Reasoning via Reinforcement   Learning</h2><p><strong>Authors:Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang</strong></p>
<p>Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at <a target="_blank" rel="noopener" href="https://github.com/TauricResearch/Trading-R1">https://github.com/TauricResearch/Trading-R1</a>. </p>
<blockquote>
<p>åœ¨é‡‘èé¢†åŸŸçš„äººå·¥æ™ºèƒ½ä¸­ï¼Œå‘å±•å‡ºä¸äººç±»é‡‘èåˆ†æå¸ˆå’Œäº¤æ˜“å‘˜ç›¸å½“çš„ä¸“ä¸šç»“æ„åŒ–æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œå› ä¸ºå¸‚åœºå¯¹é‡‘èAIæœ‰ç€å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦çš„è¦æ±‚ã€‚ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ™é¢ä¸´å°†è‡ªç„¶è¯­è¨€åˆ†æè½¬åŒ–ä¸ºæœ‰çºªå¾‹ã€å¯æ‰§è¡Œçš„äº¤æ˜“çš„æŒ‘æˆ˜ã€‚å°½ç®¡æ¨ç†å‹LLMsåœ¨é€æ­¥è§„åˆ’å’ŒéªŒè¯æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨é£é™©æ•æ„Ÿé‡‘èå†³ç­–ä¸­çš„åº”ç”¨å´è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚æˆ‘ä»¬æ¨å‡ºäº†Trading-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·å¤‡é‡‘èæ„è¯†çš„æ¨¡å‹ï¼Œå®ƒç»“åˆäº†æˆ˜ç•¥æ€ç»´å’Œè§„åˆ’ï¼Œç”¨äºå…¨é¢çš„è®ºæ–‡æ’°å†™ã€åŸºäºäº‹å®çš„åˆ†æå’Œæ³¢åŠ¨è°ƒæ•´å†³ç­–ã€‚Trading-R1é€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ ä¸äº¤æ˜“åŸåˆ™ç›¸ç¬¦çš„æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨ä»æ˜“åˆ°éš¾çš„ä¸‰ä¸ªé˜¶æ®µè¯¾ç¨‹ã€‚è®­ç»ƒä½¿ç”¨çš„æ˜¯Tauric-TR1æ•°æ®åº“ï¼Œè¯¥æ•°æ®åº“åŒ…å«è·¨è¶Š18ä¸ªæœˆã€æ¶‰åŠ14ç§è‚¡ç¥¨å’Œäº”ä¸ªä¸åŒé‡‘èæ•°æ®æºçš„10ä¸‡ä¸ªæ ·æœ¬ã€‚åœ¨å…­ç§ä¸»è¦è‚¡ç¥¨å’ŒETFä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸å¼€æºå’Œä¸“æœ‰æŒ‡ä»¤è·Ÿè¸ªæ¨¡å‹ä»¥åŠæ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒTrading-R1åœ¨é£é™©è°ƒæ•´åçš„å›æŠ¥æ–¹é¢è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶é™ä½äº†ä¸‹è·Œå¹…åº¦ã€‚è¯¥ç³»ç»Ÿç”Ÿæˆç»“æ„åŒ–ã€åŸºäºè¯æ®çš„æŠ•èµ„è®ºæ–‡ï¼Œæ”¯æŒæœ‰çºªå¾‹å’Œå¯è§£é‡Šçš„äº¤æ˜“å†³ç­–ã€‚Trading-R1ç»ˆç«¯å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/TauricResearch/Trading-R1%E4%B8%8A%E3%80%82">https://github.com/TauricResearch/Trading-R1ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11420v1">PDF</a> Tauric Research: <a target="_blank" rel="noopener" href="https://github.com/TauricResearch">https://github.com/TauricResearch</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨äººå·¥æ™ºèƒ½é‡‘èé¢†åŸŸçš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå³å¼€å‘å…·æœ‰ä¸“ä¸šç»“æ„åŒ–æ¨ç†èƒ½åŠ›çš„AIæ¨¡å‹ï¼Œä»¥ä¸äººç±»é‡‘èåˆ†æå¸ˆå’Œäº¤æ˜“å‘˜ç›¸åŒ¹æ•Œã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTrading-R1çš„æ–°å‹è´¢åŠ¡æ„è¯†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç­–ç•¥æ€è€ƒã€è§„åˆ’ã€å…¨é¢çš„è®ºæ–‡æ’°å†™ã€åŸºäºäº‹å®çš„åˆ†æä»¥åŠæ³¢åŠ¨æ€§è°ƒæ•´å†³ç­–åˆ¶å®šã€‚é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„ä¸‰é˜¶æ®µç®€å•åˆ°å¤æ‚è¯¾ç¨‹ï¼ŒTrading-R1æ¨¡å‹ä¸äº¤æ˜“åŸåˆ™ç›¸ä¸€è‡´ã€‚è¯¥æ¨¡å‹ä½¿ç”¨Tauric-TR1-DBæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«è·¨è¶Š18ä¸ªæœˆã€æ¶‰åŠ14ç§è‚¡ç¥¨å’Œäº”ç§ä¸åŒé‡‘èæ•°æ®æºçš„10ä¸‡æ ·æœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒTrading-R1åœ¨ä¸»è¦è‚¡ç¥¨å’ŒETFä¸Šçš„é£é™©è°ƒæ•´å›æŠ¥ç‡æœ‰æ‰€æé«˜ï¼Œä¸ä¼ ç»Ÿçš„å¼€æºå’Œä¸“æœ‰æŒ‡ä»¤è·Ÿè¸ªæ¨¡å‹ä»¥åŠæ¨ç†æ¨¡å‹ç›¸æ¯”ï¼Œå…¶å›æ’¤å¹…åº¦è¾ƒä½ã€‚è¯¥æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–ã€åŸºäºè¯æ®çš„æŠ•èµ„è®ºæ–‡ï¼Œæ”¯æŒçºªå¾‹ä¸¥æ˜ã€å¯è§£é‡Šçš„äº¤æ˜“å†³ç­–ã€‚Trading-R1ç»ˆç«¯å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/TauricResearch/Trading-R1%E5%B9%B3%E5%8F%B0%E3%80%82">https://github.com/TauricResearch/Trading-R1å‘å¸ƒã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>å¼€å‘å…·æœ‰ä¸äººç±»é‡‘èåˆ†æå¸ˆå’Œäº¤æ˜“å‘˜ç›¸å½“çš„ä¸“ä¸šç»“æ„åŒ–æ¨ç†èƒ½åŠ›çš„AIæ¨¡å‹æ˜¯äººå·¥æ™ºèƒ½åœ¨é‡‘èé¢†åŸŸçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>Trading-R1æ˜¯ä¸€ç§æ–°å‹çš„è´¢åŠ¡æ„è¯†æ¨¡å‹ï¼Œç»“åˆäº†ç­–ç•¥æ€è€ƒã€è§„åˆ’ç­‰ç»¼åˆèƒ½åŠ›ã€‚</li>
<li>Trading-R1æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†ä¸‰é˜¶æ®µç®€å•åˆ°å¤æ‚çš„å­¦ä¹ è¯¾ç¨‹ã€‚</li>
<li>Trading-R1æ¨¡å‹ä½¿ç”¨äº†åŒ…å«å¤šç§è‚¡ç¥¨å’Œå¤šç§é‡‘èæ•°æ®æºçš„Tauric-TR1-DBæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Trading-R1åœ¨é£é™©è°ƒæ•´å›æŠ¥ç‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿçš„æ¨¡å‹å’Œæ¨ç†æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´ä½çš„å›æ’¤å¹…åº¦ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–ã€åŸºäºè¯æ®çš„æŠ•èµ„è®ºæ–‡ï¼Œæ”¯æŒçºªå¾‹ä¸¥æ˜å’Œå¯è§£é‡Šçš„äº¤æ˜“å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11420v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11420v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Continually-Adding-New-Languages-to-Multilingual-Language-Models"><a href="#Continually-Adding-New-Languages-to-Multilingual-Language-Models" class="headerlink" title="Continually Adding New Languages to Multilingual Language Models"></a>Continually Adding New Languages to Multilingual Language Models</h2><p><strong>Authors:Abraham Toluwase Owodunni, Sachin Kumar</strong></p>
<p>Multilingual language models are trained on a fixed set of languages, and to support new languages, the models need to be retrained from scratch. This is an expensive endeavor and is often infeasible, as model developers tend not to release their pre-training data. Naive approaches, such as continued pretraining, suffer from catastrophic forgetting; however, mitigation strategies like experience replay cannot be applied due to the lack of original pretraining data. In this work, we investigate the problem of continually adding new languages to a multilingual model, assuming access to pretraining data in only the target languages. We explore multiple approaches to address this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank Adapters (LoRA) to selected initial and final layers while keeping the rest of the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting, and (2) multilingual models encode inputs in the source language in the initial layers, reason in English in intermediate layers, and translate back to the source language in final layers. We experiment with adding multiple combinations of Galician, Swahili, and Urdu to pretrained language models and evaluate each method on diverse multilingual tasks. We find that LayRA provides the overall best tradeoff between preserving modelsâ€™ capabilities in previously supported languages, while being competitive with existing approaches such as LoRA in learning new languages. We also demonstrate that using model arithmetic, the adapted models can be equipped with strong instruction following abilities without access to any instruction tuning data in the target languages. </p>
<blockquote>
<p>è·¨è¯­è¨€è¯­è¨€æ¨¡å‹æ˜¯åœ¨ä¸€ç»„å›ºå®šçš„è¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä¸ºäº†æ”¯æŒæ–°è¯­è¨€ï¼Œè¿™äº›æ¨¡å‹éœ€è¦ä»é›¶å¼€å§‹é‡æ–°è®­ç»ƒã€‚è¿™æ˜¯ä¸€é¡¹æ˜‚è´µçš„ä»»åŠ¡ï¼Œå¹¶ä¸”é€šå¸¸ç”±äºæ¨¡å‹å¼€å‘è€…ä¸æ„¿æ„å‘å¸ƒå…¶é¢„è®­ç»ƒæ•°æ®è€Œå˜å¾—ä¸å¯è¡Œã€‚ç®€å•çš„åšæ³•ï¼Œå¦‚æŒç»­çš„é¢„è®­ç»ƒï¼Œä¼šé­å—ç¾éš¾æ€§é—å¿˜çš„å›°æ‰°ï¼›ç„¶è€Œï¼Œç”±äºç¼ºä¹åŸå§‹é¢„è®­ç»ƒæ•°æ®ï¼Œåº”ç”¨ç¼“è§£ç­–ç•¥ï¼ˆå¦‚ç»éªŒå›æ”¾ï¼‰æ˜¯è¡Œä¸é€šçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•ä¸æ–­å‘è·¨è¯­è¨€æ¨¡å‹æ·»åŠ æ–°è¯­è¨€çš„é—®é¢˜ï¼Œå‡è®¾åªè®¿é—®ç›®æ ‡è¯­è¨€çš„é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¢ç´¢äº†å¤šç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†Layer-Selective LoRAï¼ˆLayRAï¼‰ï¼Œå®ƒåœ¨é€‰å®šçš„åˆå§‹å’Œæœ€ç»ˆå±‚ä¸­æ·»åŠ ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†å†»ç»“ã€‚LayRAå»ºç«‹åœ¨ä¸¤ä¸ªè§è§£ä¹‹ä¸Šï¼šï¼ˆ1ï¼‰LoRAå¯ä»¥å‡å°‘é—å¿˜ï¼›ï¼ˆ2ï¼‰å¤šè¯­è¨€æ¨¡å‹åœ¨åˆå§‹å±‚ä¸­ä»¥æºè¯­è¨€ç¼–ç è¾“å…¥ï¼Œåœ¨ä¸­é—´å±‚ä¸­ä»¥è‹±è¯­è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨æœ€ç»ˆå±‚ä¸­ç¿»è¯‘å›æºè¯­è¨€ã€‚æˆ‘ä»¬é€šè¿‡å‘é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ·»åŠ åŠ æ³°ç½—å°¼äºšè¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œä¹Œå°”éƒ½è¯­çš„å¤šç§ç»„åˆè¿›è¡Œå®éªŒï¼Œå¹¶åœ¨å¤šç§è·¨è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°æ¯ç§æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLayRAåœ¨ä¿ç•™æ¨¡å‹å¯¹å…ˆå‰æ”¯æŒçš„è¯­è¨€çš„èƒ½åŠ›æ–¹é¢æä¾›äº†æœ€ä½³çš„æŠ˜è¡·æ–¹æ¡ˆï¼ŒåŒæ—¶åœ¨å­¦ä¹ æ–°è¯­è¨€æ–¹é¢ä¸ç°æœ‰æ–¹æ³•ï¼ˆå¦‚LoRAï¼‰ç«äº‰ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œé€šè¿‡ä½¿ç”¨æ¨¡å‹ç®—æœ¯ï¼Œå¯ä»¥åœ¨æ— éœ€è®¿é—®ç›®æ ‡è¯­è¨€çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸ºè°ƒæ•´åçš„æ¨¡å‹é…å¤‡å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸ºå·²å­˜åœ¨çš„å¤šè¯­è¨€æ¨¡å‹æŒç»­æ·»åŠ æ–°è¯­è¨€çš„é—®é¢˜ã€‚ç”±äºæ¨¡å‹å¼€å‘è€…é€šå¸¸ä¸ä¼šé‡Šæ”¾é¢„è®­ç»ƒæ•°æ®ï¼Œå› æ­¤éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹æ¥æ”¯æŒæ–°è¯­è¨€æ˜¯ä¸€ä¸ªæ˜‚è´µä¸”ä¸å¯è¡Œçš„æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†å¤šç§æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶æå‡ºäº†Layer-Selective LoRAï¼ˆLayRAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨é€‰å®šåˆå§‹å’Œæœ€ç»ˆå±‚æ·»åŠ ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰åŒæ—¶ä¿æŒæ¨¡å‹å…¶ä½™éƒ¨åˆ†å†»ç»“ï¼Œæ¥å‡å°‘é—å¿˜å¹¶ä¼˜åŒ–å¤šè¯­è¨€æ¨¡å‹çš„ç¼–ç ã€æ¨ç†å’Œç¿»è¯‘è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒLayRAåœ¨ä¿ç•™æ¨¡å‹å¯¹åŸæœ‰è¯­è¨€æ”¯æŒèƒ½åŠ›çš„åŒæ—¶ï¼Œå­¦ä¹ æ–°è¯­è¨€çš„èƒ½åŠ›å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¨¡å‹ç®—æœ¯ï¼Œé€‚åº”åçš„æ¨¡å‹å¯ä»¥åœ¨æ— éœ€è®¿é—®ç›®æ ‡è¯­è¨€çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè·å¾—å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€æ¨¡å‹éœ€è¦æ”¯æŒæ–°è¯­è¨€æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹æˆæœ¬é«˜æ˜‚ä¸”ä¸å¯è¡Œã€‚</li>
<li>Naiveæ–¹æ³•ï¼ˆå¦‚æŒç»­é¢„è®­ç»ƒï¼‰ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ç¼ºä¹åŸå§‹é¢„è®­ç»ƒæ•°æ®ä½¿å¾—ä¸€äº›ç¼“è§£ç­–ç•¥ï¼ˆå¦‚ç»éªŒå›æ”¾ï¼‰æ— æ³•åº”ç”¨ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†Layer-Selective LoRAï¼ˆLayRAï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>LayRAé€šè¿‡åœ¨é€‰å®šå±‚æ·»åŠ ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æ¥å‡å°‘é—å¿˜ï¼Œå¹¶ä¼˜åŒ–æ¨¡å‹çš„ç¼–ç ã€æ¨ç†å’Œç¿»è¯‘è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLayRAåœ¨ä¿ç•™æ¨¡å‹åŸæœ‰è¯­è¨€æ”¯æŒèƒ½åŠ›çš„åŒæ—¶ï¼Œå­¦ä¹ æ–°è¯­è¨€çš„èƒ½åŠ›å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11414v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11414v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Traffic-MLLM-A-Spatio-Temporal-MLLM-with-Retrieval-Augmented-Generation-for-Causal-Inference-in-Traffic"><a href="#Traffic-MLLM-A-Spatio-Temporal-MLLM-with-Retrieval-Augmented-Generation-for-Causal-Inference-in-Traffic" class="headerlink" title="Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation   for Causal Inference in Traffic"></a>Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation   for Causal Inference in Traffic</h2><p><strong>Authors:Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun</strong></p>
<p>As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the modelâ€™s logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities. </p>
<blockquote>
<p>éšç€æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„ä¸æ–­å‘å±•ï¼Œäº¤é€šè§†é¢‘ç†è§£åœ¨ç»¼åˆåœºæ™¯æ„ŸçŸ¥å’Œå› æœåˆ†æä¸­çš„ä½œç”¨è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®å»ºæ¨¡æ—¶ç©ºå› æœå…³ç³»å’Œæ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†æ–¹é¢é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ç²¾ç»†äº¤é€šåˆ†æé‡èº«å®šåˆ¶çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Traffic-MLLMã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5-VLéª¨å¹²ç½‘æ„å»ºï¼Œåˆ©ç”¨é«˜è´¨é‡çš„äº¤é€šç‰¹å®šå¤šæ¨¡æ€æ•°æ®é›†ï¼Œé‡‡ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰è¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†å…¶å¯¹è§†é¢‘åºåˆ—ä¸­è¿ç»­æ—¶ç©ºç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ›æ–°çš„çŸ¥è¯†æç¤ºæ¨¡å—ï¼Œèåˆäº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ï¼Œä½¿ç²¾ç¡®çš„äº¤é€šè§„åˆ™å’Œé¢†åŸŸçŸ¥è¯†èƒ½å¤Ÿæ³¨å…¥æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†é€‚åº”èƒ½åŠ›ã€‚åœ¨TrafficQAå’ŒDriveQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTraffic-MLLMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶å¤„ç†å¤šæ¨¡æ€äº¤é€šæ•°æ®çš„å“è¶Šèƒ½åŠ›ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å±•ç°å‡ºæƒŠäººçš„é›¶å°„å‡»æ¨ç†å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11165v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„ä¸æ–­å‘å±•ï¼Œäº¤é€šè§†é¢‘ç†è§£åœ¨å…¨é¢åœºæ™¯æ„ŸçŸ¥å’Œå› æœåˆ†ææ–¹é¢å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å‡†ç¡®å»ºæ¨¡æ—¶ç©ºå› æœå…³ç³»å’Œæ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Traffic-MLLMå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç²¾ç»†äº¤é€šåˆ†æã€‚è¯¥æ¨¡å‹åŸºäºQwen2.5-VLéª¨æ¶ï¼Œåˆ©ç”¨é«˜è´¨é‡äº¤é€šç‰¹å®šå¤šæ¨¡æ€æ•°æ®é›†å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰è¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œæé«˜äº†å¯¹è§†é¢‘åºåˆ—ä¸­è¿ç»­æ—¶ç©ºç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªèåˆChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„çŸ¥è¯†æç¤ºæ¨¡å—ï¼Œä½¿ç²¾ç¡®æ³¨å…¥äº¤é€šæ³•è§„å’Œé¢†åŸŸçŸ¥è¯†åˆ°æ¨ç†è¿‡ç¨‹ä¸­æˆä¸ºå¯èƒ½ã€‚è¯¥è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†é€‚åº”èƒ½åŠ›ã€‚åœ¨TrafficQAå’ŒDriveQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTraffic-MLLMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶å¤„ç†å¤šæ¨¡æ€äº¤é€šæ•°æ®çš„å“è¶Šèƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ¨ç†å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤é€šè§†é¢‘ç†è§£åœ¨æ™ºèƒ½è¿è¾“ç³»ç»Ÿä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å»ºæ¨¡æ—¶ç©ºå› æœå…³ç³»å’Œæ•´åˆé¢†åŸŸçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚</li>
<li>Traffic-MLLMæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒç»“åˆå¤šæ¨¡æ€æ•°æ®å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰æŠ€æœ¯æ¥æé«˜å¯¹è¿ç»­æ—¶ç©ºç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å¼•å…¥çŸ¥è¯†æç¤ºæ¨¡å—ï¼Œèåˆäº†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œèƒ½ç²¾ç¡®æ³¨å…¥äº¤é€šæ³•è§„å’Œé¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>Traffic-MLLMåœ¨TrafficQAå’ŒDriveQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·å¤‡å¤„ç†å¤šæ¨¡æ€äº¤é€šæ•°æ®ã€é›¶æ ·æœ¬æ¨ç†å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨Qwen2.5-VLä½œä¸ºéª¨æ¶ï¼Œå¹¶åˆ©ç”¨é«˜è´¨é‡äº¤é€šç‰¹å®šå¤šæ¨¡æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>çŸ¥è¯†æç¤ºæ¨¡å—çš„è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†é€‚åº”èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ç†è§£å’Œåº”å¯¹å¤æ‚çš„äº¤é€šåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11165v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11165v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11165v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11165v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.11165v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Nav-R1-Reasoning-and-Navigation-in-Embodied-Scenes"><a href="#Nav-R1-Reasoning-and-Navigation-in-Embodied-Scenes" class="headerlink" title="Nav-R1: Reasoning and Navigation in Embodied Scenes"></a>Nav-R1: Reasoning and Navigation in Embodied Scenes</h2><p><strong>Authors:Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang</strong></p>
<p>Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/Nav-R1">https://github.com/AIGeeksGroup/Nav-R1</a>. Website: <a target="_blank" rel="noopener" href="https://aigeeksgroup.github.io/Nav-R1">https://aigeeksgroup.github.io/Nav-R1</a>. </p>
<blockquote>
<p>æ²‰æµ¸å¼å¯¼èˆªè¦æ±‚æ™ºèƒ½ä½“åœ¨å¤æ‚çš„3Dç¯å¢ƒä¸­è¿›è¡Œæ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„é›†æˆï¼Œä»¥å®ç°ç¨³å¥çš„äº¤äº’ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å­˜åœ¨æ¨ç†è½¨è¿¹ä¸ä¸€è‡´å’Œä¸ç¨³å®šçš„ç¼ºé™·ï¼Œé˜»ç¢äº†åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠéš¾ä»¥å¹³è¡¡é•¿æœŸè§†é‡è¯­ä¹‰æ¨ç†ä¸ä½å»¶è¿Ÿæ§åˆ¶ï¼Œä»¥å®ç°å®æ—¶å¯¼èˆªã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Nav-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ²‰æµ¸å¼ç¯å¢ƒæ¨ç†çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†Nav-CoT-110Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€å¥—å¤§è§„æ¨¡é€æ­¥æ¨ç†é“¾ï¼ˆCoTï¼‰æ²‰æµ¸å¼ä»»åŠ¡æ•°æ®é›†ï¼Œèƒ½å¤Ÿå®ç°ç»“æ„åŒ–æ¨ç†çš„å†·å¯åŠ¨åˆå§‹åŒ–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ ¼å¼ã€ç†è§£å’Œå¯¼èˆªä¸‰ç§äº’è¡¥å¥–åŠ±ï¼Œä»¥æé«˜ç»“æ„éµå¾ªæ€§ã€è¯­ä¹‰é”šå®šå’Œè·¯å¾„å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¿«æ…¢æ¨ç†èŒƒå¼ï¼Œå°†æœ‰æ„è¯­ä¹‰æ¨ç†ä¸ä½å»¶è¿Ÿååº”æ§åˆ¶ç›¸åˆ†ç¦»ï¼Œä»¥å®ç°é«˜æ•ˆä¸”è¿è´¯çš„å¯¼èˆªã€‚åœ¨æ²‰æµ¸å¼AIåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒNav-R1æŒç»­è¶…è¶Šå¼ºåŠ²åŸºå‡†æµ‹è¯•ï¼Œåœ¨æ¨ç†å’Œå¯¼èˆªæ€§èƒ½ä¸Šå¹³å‡æé«˜è¶…è¿‡8%ã€‚åœ¨ç§»åŠ¨æœºå™¨äººä¸Šçš„å®é™…éƒ¨ç½²è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨æœ‰é™è½¦è½½èµ„æºä¸‹çš„ç¨³å¥æ€§ã€‚ä»£ç å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/Nav-R1%E3%80%82%E7%BD%91%E7%AB%99%EF%BC%9Ahttps://aigeeksgroup.github.io/Nav-R1%E3%80%82">https://github.com/AIGeeksGroup/Nav-R1ã€‚ç½‘ç«™ï¼šhttps://aigeeksgroup.github.io/Nav-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10884v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºNav-R1çš„åµŒå…¥å¼åŸºç¡€æ¨¡å‹ï¼Œç”¨äºè§£å†³åµŒå…¥å¼ç¯å¢ƒä¸­çš„æ¨ç†é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†Nav-CoT-110Kï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæé«˜ç»“æ„éµå¾ªã€è¯­ä¹‰å®šä½å’Œè·¯å¾„å‡†ç¡®æ€§ã€‚å¼•å…¥å¿«æ…¢æ¨ç†èŒƒå¼ï¼Œå®ç°é«˜æ•ˆè¿è´¯çš„å¯¼èˆªã€‚åœ¨åµŒå…¥å¼AIåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜è¶…è¿‡8%ï¼Œå¹¶åœ¨ç§»åŠ¨æœºå™¨äººä¸Šå®ç°çœŸå®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nav-R1æ¨¡å‹é›†æˆäº†æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œä½¿ä»£ç†åœ¨å¤æ‚çš„3Dç¯å¢ƒä¸­è¿›è¡Œç¨³å¥äº¤äº’ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ¨ç†è½¨è¿¹ä¸ä¸€è‡´ã€ä¸ç¨³å®šçš„é—®é¢˜ï¼Œé™åˆ¶äº†åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Nav-CoT-110Kæ•°æ®é›†çš„æ„å»ºä¸ºåµŒå…¥å¼ä»»åŠ¡æä¾›äº†ç»“æ„åŒ–æ¨ç†çš„å†·å¯åŠ¨åˆå§‹åŒ–ã€‚</li>
<li>é‡‡ç”¨GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªäº’è¡¥å¥–åŠ±ï¼ˆæ ¼å¼ã€ç†è§£å’Œå¯¼èˆªï¼‰æé«˜ç»“æ„éµå¾ªã€è¯­ä¹‰å®šä½å’Œè·¯å¾„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥å¿«æ…¢æ¨ç†èŒƒå¼ï¼Œå®ç°è¯­ä¹‰æ¨ç†ä¸ä½å»¶è¿Ÿååº”æ§åˆ¶çš„è§£è€¦ï¼Œæé«˜å¯¼èˆªæ•ˆç‡ä¸è¿è´¯æ€§ã€‚</li>
<li>Nav-R1æ¨¡å‹åœ¨åµŒå…¥å¼AIåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹³å‡æé«˜8%ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10884v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GTHNA-Local-global-Graph-Transformer-with-Memory-Reconstruction-for-Holistic-Node-Anomaly-Evaluation"><a href="#GTHNA-Local-global-Graph-Transformer-with-Memory-Reconstruction-for-Holistic-Node-Anomaly-Evaluation" class="headerlink" title="GTHNA: Local-global Graph Transformer with Memory Reconstruction for   Holistic Node Anomaly Evaluation"></a>GTHNA: Local-global Graph Transformer with Memory Reconstruction for   Holistic Node Anomaly Evaluation</h2><p><strong>Authors:Mingkang Li, Xuexiong Luo, Yue Zhang, Yaoyang Li, Fu Lin</strong></p>
<p>Anomaly detection in graph-structured data is an inherently challenging problem, as it requires the identification of rare nodes that deviate from the majority in both their structural and behavioral characteristics. Existing methods, such as those based on graph convolutional networks (GCNs), often suffer from over-smoothing, which causes the learned node representations to become indistinguishable. Furthermore, graph reconstruction-based approaches are vulnerable to anomalous node interference during the reconstruction process, leading to inaccurate anomaly detection. In this work, we propose a novel and holistic anomaly evaluation framework that integrates three key components: a local-global Transformer encoder, a memory-guided reconstruction mechanism, and a multi-scale representation matching strategy. These components work synergistically to enhance the modelâ€™s ability to capture both local and global structural dependencies, suppress the influence of anomalous nodes, and assess anomalies from multiple levels of granularity. Anomaly scores are computed by combining reconstruction errors and memory matching signals, resulting in a more robust evaluation. Extensive experiments on seven benchmark datasets demonstrate that our method outperforms existing state-of-the-art approaches, offering a comprehensive and generalizable solution for anomaly detection across various graph domains. </p>
<blockquote>
<p>å›¾ç»“æ„æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€ä¸ªå…·æœ‰å†…åœ¨æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒéœ€è¦è¯†åˆ«é‚£äº›åœ¨å…¶ç»“æ„å’Œè¡Œä¸ºç‰¹å¾ä¸Šéƒ½åç¦»å¤§å¤šæ•°çš„ç½•è§èŠ‚ç‚¹ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚å›¾å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰çš„æ–¹æ³•ï¼Œå¸¸å¸¸é­å—è¿‡åº¦å¹³æ»‘çš„å›°æ‰°ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„èŠ‚ç‚¹è¡¨ç¤ºæ— æ³•åŒºåˆ†ã€‚æ­¤å¤–ï¼ŒåŸºäºå›¾é‡å»ºçš„æ–¹æ³•åœ¨é‡å»ºè¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å¼‚å¸¸èŠ‚ç‚¹çš„å¹²æ‰°ï¼Œå¯¼è‡´å¼‚å¸¸æ£€æµ‹ä¸å‡†ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œå…¨é¢çš„å¼‚å¸¸è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨-å…¨å±€Transformerç¼–ç å™¨ã€è®°å¿†å¼•å¯¼é‡å»ºæœºåˆ¶å’Œå¤šå°ºåº¦è¡¨ç¤ºåŒ¹é…ç­–ç•¥ã€‚è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œå¢å¼ºäº†æ¨¡å‹æ•æ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼ŒæŠ‘åˆ¶äº†å¼‚å¸¸èŠ‚ç‚¹çš„å½±å“ï¼Œå¹¶ä»å¤šä¸ªç²’åº¦å±‚æ¬¡è¯„ä¼°å¼‚å¸¸ã€‚å¼‚å¸¸åˆ†æ•°æ˜¯é€šè¿‡ç»“åˆé‡å»ºè¯¯å·®å’Œå†…å­˜åŒ¹é…ä¿¡å·è®¡ç®—å¾—å‡ºçš„ï¼Œä»è€Œå¾—åˆ°æ›´ç¨³å¥çš„è¯„ä¼°ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€æ–°æ–¹æ³•ï¼Œä¸ºå„ç§å›¾åŸŸä¸­çš„å¼‚å¸¸æ£€æµ‹æä¾›äº†å…¨é¢å’Œå¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10869v1">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå±€éƒ¨å…¨å±€Transformerç¼–ç å™¨ã€è®°å¿†å¼•å¯¼é‡å»ºæœºåˆ¶å’Œå¤šå°ºåº¦è¡¨ç¤ºåŒ¹é…ç­–ç•¥çš„æ–°å‹å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚å®ƒèƒ½æœ‰æ•ˆæ•æ‰å›¾ç»“æ„ä¸­çš„å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ï¼ŒæŠ‘åˆ¶å¼‚å¸¸èŠ‚ç‚¹çš„å½±å“ï¼Œå¹¶ä»å¤šä¸ªç²’åº¦å±‚é¢è¯„ä¼°å¼‚å¸¸ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºå„ç§å›¾å½¢é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹æä¾›äº†å…¨é¢ä¸”å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼‚å¸¸æ£€æµ‹åœ¨å›¾ç»“æ„æ•°æ®ä¸­æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦è¯†åˆ«åœ¨ç»“æ„å’Œè¡Œä¸ºç‰¹å¾ä¸Šåç¦»å¤§å¤šæ•°ç¾¤ä½“çš„ç½•è§èŠ‚ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚åŸºäºå›¾å·ç§¯ç½‘ç»œçš„æ–¹æ³•å­˜åœ¨è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œå¯¼è‡´å­¦ä¹ åˆ°çš„èŠ‚ç‚¹è¡¨ç¤ºéš¾ä»¥åŒºåˆ†ã€‚</li>
<li>åŸºäºå›¾é‡å»ºçš„æ–¹æ³•åœ¨é‡å»ºè¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å¼‚å¸¸èŠ‚ç‚¹çš„å¹²æ‰°ï¼Œå¯¼è‡´å¼‚å¸¸æ£€æµ‹ä¸å‡†ç¡®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ŒåŒ…æ‹¬å±€éƒ¨å…¨å±€Transformerç¼–ç å™¨ã€è®°å¿†å¼•å¯¼é‡å»ºæœºåˆ¶å’Œå¤šå°ºåº¦è¡¨ç¤ºåŒ¹é…ç­–ç•¥ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¢å¼ºæ¨¡å‹æ•æ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼ŒæŠ‘åˆ¶å¼‚å¸¸èŠ‚ç‚¹çš„å½±å“ï¼Œå¹¶ä»å¤šä¸ªç²’åº¦å±‚é¢è¯„ä¼°å¼‚å¸¸ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10869v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10869v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10869v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10869v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.10869v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="K2-Think-A-Parameter-Efficient-Reasoning-System"><a href="#K2-Think-A-Parameter-Efficient-Reasoning-System" class="headerlink" title="K2-Think: A Parameter-Efficient Reasoning System"></a>K2-Think: A Parameter-Efficient Reasoning System</h2><p><strong>Authors:Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing</strong></p>
<p>K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at k2think.ai, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine. </p>
<blockquote>
<p>K2-Thinkæ˜¯ä¸€ä¸ªæ¨ç†ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨32Bå‚æ•°æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¯ä»¥ä¸GPT-OSS 120Bå’ŒDeepSeek v3.1ç­‰å¤§å‹æ¨¡å‹ç›¸åŒ¹æ•Œç”šè‡³è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå»ºç«‹åœ¨Qwen2.5åŸºç¡€æ¨¡å‹ä¸Šï¼Œé€šè¿‡ç»“åˆå…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯ï¼Œè¯æ˜å°å‹æ¨¡å‹ä¹Ÿå¯ä»¥åœ¨æœ€é«˜çº§åˆ«ä¸Šç«äº‰ã€‚è¯¥æ–¹æ³•åŸºäºå…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±ï¼šé•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒã€å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€æ¨ç†å‰çš„ä»£ç†è®¡åˆ’ã€æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€æŠ•æœºè§£ç å’Œæ¨ç†ä¼˜åŒ–ç¡¬ä»¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä½¿ç”¨å…¬å¼€å¯ç”¨çš„å¼€æºæ•°æ®é›†ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦ç­‰å…¶ä»–é¢†åŸŸä¹Ÿè¡¨ç°å¼ºåŠ²ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒåƒK2-Think 32Bè¿™æ ·æ›´å‚æ•°é«˜æ•ˆçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡é›†æˆçš„åè®­ç»ƒé…æ–¹ï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´è®­ç»ƒå’Œæˆ˜ç•¥æ¨ç†æ—¶é—´å¢å¼ºåŠŸèƒ½ï¼Œä¸å…¶ä»–æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸ç«äº‰ï¼Œè¿™ä½¿å¾—å¼€æºæ¨ç†ç³»ç»Ÿæ›´å®¹æ˜“è®¿é—®å’Œè´Ÿæ‹…å¾—èµ·ã€‚K2-Thinkå¯åœ¨k2think.aiä¸Šå…è´¹è·å¾—ï¼Œé€šè¿‡Cerebrasæ™¶åœ†çº§å¼•æ“æä¾›æ¯ç§’è¶…è¿‡2000ä»¤ç‰Œçš„é¡¶çº§æ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07604v3">PDF</a> To access the K2-Think reasoning system, please visit <a target="_blank" rel="noopener" href="http://www.k2think.ai/">www.k2think.ai</a></p>
<p><strong>æ‘˜è¦</strong><br>    K2-Thinkæ˜¯ä¸€ä¸ªç»“åˆå…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯çš„å°å‹æ¨¡å‹æ¨ç†ç³»ç»Ÿï¼Œå®ƒå‡­å€Ÿå…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶è®¡ç®—æ–¹æ³•ç­‰æŠ€æœ¯ä¼˜åŠ¿å®ç°äº†ä¸æ›´å¤§çš„æ¨¡å‹ç«äº‰çš„å®åŠ›ã€‚å®ƒåˆ©ç”¨å¼€æºæ•°æ®é›†æ„å»ºçš„å…­å¤§æŠ€æœ¯æ”¯æŸ±ä½¿å…¶æˆä¸ºæ¨ç†é¢†åŸŸæœ€ä¼˜åŒ–çš„ç³»ç»Ÿä¹‹ä¸€ï¼Œè¾¾åˆ°äº†å›½é™…é¡¶å°–çš„æ€§èƒ½æ°´å¹³ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å‡ºè‰²çš„æˆç»©ã€‚è¯¥ç³»ç»Ÿå·²å‘å¸ƒåœ¨k2think.aiä¸Šï¼Œæä¾›äº†è¶…è¿‡æ¯ç§’å¤„ç†2,000ä¸ªä»¤ç‰Œçš„è¯·æ±‚é€Ÿåº¦å’Œæœ€å¿«çš„æ¨ç†æ—¶é—´ï¼Œå¼€æ”¾å¼€æºçš„ç‰¹æ€§æ›´ä½¿å…¶å¹¿æ³›åº”ç”¨äºå„ç±»ä½¿ç”¨åœºæ™¯ã€‚æ­¤è®ºæ–‡æŒ‡å‡ºäº†æ›´å…·æ•ˆç‡çš„æ¨¡å‹å¯ä»¥å¸®åŠ©ä¼ä¸šåœ¨å‚æ•°é€‰æ‹©å’Œç®—åŠ›éœ€æ±‚æ–¹é¢åšå‡ºæ›´å¥½çš„å†³ç­–ï¼Œä»è€Œä½¿å¾—æ›´å¤šçš„ç»„ç»‡èƒ½å¤Ÿæ›´å®¹æ˜“æ¥è§¦åˆ°é«˜çº§çš„æ¨ç†ç³»ç»Ÿã€‚K2-Thinkå…·å¤‡å“è¶Šçš„æ€§ä»·æ¯”ï¼Œæœ‰åŠ©äºæé«˜ç³»ç»Ÿç¨³å®šæ€§å’Œæˆæœ¬æ•ˆç›Šã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥ç³»ç»Ÿæ¨åŠ¨äº†äººå·¥æ™ºèƒ½æ¨ç†é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.07604v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.07604v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.07604v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.07604v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Another-Turn-Better-Output-A-Turn-Wise-Analysis-of-Iterative-LLM-Prompting"><a href="#Another-Turn-Better-Output-A-Turn-Wise-Analysis-of-Iterative-LLM-Prompting" class="headerlink" title="Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM   Prompting"></a>Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM   Prompting</h2><p><strong>Authors:Shashidhar Reddy Javaji, Bhavul Gauri, Zining Zhu</strong></p>
<p>Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague &#96;&#96;improve itâ€™â€™ feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration. Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨å¤šç”¨äºå¤šè½®å·¥ä½œæµç¨‹ä¸­ï¼Œä½†æˆ‘ä»¬ä»ç„¶ç¼ºä¹æ˜ç¡®çš„è¡¡é‡æ ‡å‡†æ¥åˆ¤æ–­è¿­ä»£ä½•æ—¶æœ‰å¸®åŠ©ï¼Œä½•æ—¶ä¼šé€‚å¾—å…¶åã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¶µç›–åˆ›æ„ã€ä»£ç å’Œæ•°å­¦çš„è¿­ä»£ä¼˜åŒ–è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„åè®®é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿è¡Œå—æ§çš„12è½®å¯¹è¯ï¼Œåˆ©ç”¨å„ç§æç¤ºï¼Œä»æ¨¡ç³Šçš„â€œæ”¹è¿›å®ƒâ€åé¦ˆåˆ°æœ‰é’ˆå¯¹æ€§çš„æŒ‡å¯¼ï¼Œå¹¶è®°å½•æ¯è½®çš„è¾“å²€ã€‚æˆ‘ä»¬ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„æ£€æŸ¥æ¥è¯„åˆ†ç»“æœï¼ˆä»£ç çš„å•å…ƒæµ‹è¯•ï¼›æ•°å­¦çš„ç­”æ¡ˆç­‰ä»·æ€§å’Œæ¨ç†åˆç†æ€§ï¼›åˆ›æ„çš„æ–°é¢–æ€§å’Œå¯è¡Œæ€§ï¼‰ï¼Œå¹¶é€šè¿‡ä¸‰ä¸ªå®¶æ—çš„æŒ‡æ ‡è·Ÿè¸ªè½®æ¬¡çº§åˆ«çš„è¡Œä¸ºï¼šå„è½®ä¹‹é—´çš„è¯­ä¹‰å˜åŒ–ã€è½®ä¸è½®ä¹‹é—´çš„å˜åŒ–ä»¥åŠè¾“å‡ºå¤§å°çš„å¢é•¿ã€‚åœ¨ä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œæ”¶ç›Šæ˜¯ä¾èµ–äºé¢†åŸŸçš„ï¼šå®ƒä»¬åœ¨åˆ›æ„å’Œä»£ç çš„æ—©æœŸé˜¶æ®µåˆ°è¾¾ï¼Œä½†åœ¨æ•°å­¦é¢†åŸŸï¼Œæ™šæœŸçš„è¿­ä»£å¾ˆé‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å—åˆ°è¯¦ç»†æŒ‡å¯¼çš„æƒ…å†µä¸‹ã€‚ç»è¿‡å‰å‡ è½®åï¼Œæ¨¡ç³Šçš„åé¦ˆé€šå¸¸ä¼šè¾¾åˆ°å³°å€¼æˆ–åå‘æ­£ç¡®æ€§ï¼Œè€Œé’ˆå¯¹æ€§çš„æç¤ºå¯ä»¥å¯é åœ°æ”¹å˜é¢„æœŸçš„è´¨é‡è½´ï¼ˆåˆ›æ„ä¸­çš„æ–°é¢–æ€§å¯¹å¯è¡Œæ€§çš„å½±å“ï¼›ä»£ç ä¸­çš„é€Ÿåº¦ä¸å¯è¯»æ€§ï¼›åœ¨æ•°å­¦ä¸­ï¼Œè¯¦ç»†çš„è¿­ä»£ä¼˜äºæ¢ç´¢å¹¶å¸¦æ¥æ™šæœŸæ”¶ç›Šï¼‰ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ä¸€è‡´é¢†åŸŸæ¨¡å¼ï¼šåˆ›æ„åœ¨å„è½®ä¹‹é—´æ„ä¹‰å˜åŒ–æ›´å¤§ï¼Œä»£ç å€¾å‘äºåœ¨å¤§å°ä¸Šå¢é•¿è€Œè¯­ä¹‰å˜åŒ–è¾ƒå°ï¼Œæ•°å­¦ä»å›ºå®šå¼€å§‹ä½†å¯ä»¥é€šè¿‡åæœŸè¯¦ç»†çš„è¿­ä»£æ‰“ç ´è¿™ä¸€è·¯å¾„ã€‚æ€»ä¹‹ï¼Œè¯¥æ¡†æ¶å’ŒæŒ‡æ ‡ä½¿è¿­ä»£èƒ½å¤Ÿåœ¨æ¨¡å‹ä¹‹é—´è¿›è¡Œæ¯”è¾ƒå’Œè¡¡é‡ï¼Œå¹¶æŒ‡ç¤ºä½•æ—¶éœ€è¦è°ƒæ•´æ–¹å‘ã€åœæ­¢æˆ–æ”¹å˜ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06770v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å¦‚ä½•è¯„ä¼°è¿­ä»£å¸¦æ¥çš„å¸®åŠ©å’Œä¼¤å®³å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªæ¶µç›–æ„æƒ³ã€ä»£ç å’Œæ•°å­¦çš„è¿­ä»£ä¼˜åŒ–è¯„ä¼°æ¡†æ¶ã€‚è¯¥åè®®é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œå—æ§çš„12è½®å¯¹è¯ï¼Œä½¿ç”¨å„ç§æç¤ºï¼Œä»æ¨¡ç³Šçš„â€œæ”¹è¿›å®ƒâ€åé¦ˆåˆ°æœ‰é’ˆå¯¹æ€§çš„æŒ‡å¯¼ï¼Œå¹¶è®°å½•æ¯è½®çš„è¾“å²€ã€‚æˆ‘ä»¬ç”¨åŸŸé€‚å½“çš„æ£€æŸ¥æ¥è¯„åˆ†ç»“æœï¼ˆä»£ç ç”¨å•å…ƒæµ‹è¯•ï¼›æ•°å­¦ç”¨ç­”æ¡ˆç­‰ä»·åŠ ä¸Šæ¨ç†åˆç†æ€§ï¼›æ„æƒ³ç”¨åŸåˆ›æ€§å’Œå¯è¡Œæ€§ï¼‰ï¼Œå¹¶è·Ÿè¸ªè½®æ¬¡çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬è¯­ä¹‰ç§»åŠ¨ã€è½®æ¬¡å˜åŒ–å’Œè¾“å‡ºå¤§å°å¢é•¿ã€‚åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œæ”¶ç›Šæ˜¯åŸŸä¾èµ–çš„ï¼šå®ƒä»¬åœ¨æ„æƒ³å’Œä»£ç çš„æ—©æœŸåˆ°è¾¾ï¼Œä½†åœ¨æ•°å­¦ä¸­ï¼ŒåæœŸè½®æ¬¡å¾ˆé‡è¦ï¼Œåœ¨è¯¦ç»†æŒ‡å¯¼çš„æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚ç»è¿‡å‰å‡ è½®åï¼Œæ¨¡ç³Šçš„åé¦ˆå¾€å¾€ä¼šä½¿æ­£ç¡®æ€§è¾¾åˆ°å¹³å°æœŸæˆ–é€†è½¬ï¼Œè€Œæœ‰é’ˆå¯¹æ€§çš„æç¤ºå¯ä»¥å¯é åœ°æ”¹å˜é¢„æœŸçš„è´¨é‡è½´ï¼ˆæ„æƒ³çš„åˆ›æ–°æ€§å¯¹å¯è¡Œæ€§ï¼›ä»£ç çš„é€Ÿåº¦å¯¹å¯è¯»æ€§ï¼›åœ¨æ•°å­¦ä¸Šï¼Œè¯¦ç»†é˜è¿°èƒœè¿‡æ¢ç´¢å¹¶é©±åŠ¨åæœŸè½®æ¬¡çš„æ”¶ç›Šï¼‰ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ä¸€è‡´çš„åŸŸæ¨¡å¼ï¼šæ„æƒ³åœ¨å„è½®ä¹‹é—´çš„æ„ä¹‰å˜åŠ¨æ›´å¤§ï¼Œä»£ç çš„å¤§å°å¾€å¾€éšç€å¢é•¿è€Œè¯­ä¹‰å˜åŒ–å¾ˆå°ï¼Œæ•°å­¦å¼€å§‹æ—¶å›ºå®šä½†å¯èƒ½å› åæœŸçš„è¯¦ç»†è¿­ä»£è€Œæ‰“ç ´è·¯å¾„ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¡†æ¶å’ŒæŒ‡æ ‡ä½¿è¿­ä»£å¯ä»¥åœ¨æ¨¡å‹ä¹‹é—´è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶æç¤ºä½•æ—¶éœ€è¦è°ƒæ•´ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„åº”ç”¨è¯„ä¼°éœ€è¦æ˜ç¡®çš„è¡¡é‡æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ¶µç›–æ„æƒ³ã€ä»£ç å’Œæ•°å­¦çš„è¿­ä»£ä¼˜åŒ–è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>é€šè¿‡å—æ§çš„12è½®å¯¹è¯è¿›è¡Œå®éªŒç ”ç©¶ï¼Œä½¿ç”¨ä¸åŒçš„æç¤ºå’Œè®°å½•æ¯è½®çš„è¾“å‡ºæ¥è¯„ä¼°è¿­ä»£æ•ˆæœã€‚</li>
<li>æ”¶ç›Šæ˜¯åŸŸä¾èµ–çš„ï¼Œæ—©æœŸè¿­ä»£åœ¨æ„æƒ³å’Œä»£ç ä¸­æ”¶ç›Šè¾ƒå¤§ï¼Œæ•°å­¦ä¸­åæœŸè¿­ä»£å¾ˆé‡è¦ã€‚</li>
<li>æ¨¡ç³Šåé¦ˆå¯¹æ­£ç¡®æ€§çš„å¸®åŠ©æœ‰é™ï¼Œè€Œé’ˆå¯¹æ€§æç¤ºèƒ½æ›´å¯é åœ°æ”¹å˜è´¨é‡ã€‚</li>
<li>è§‚å¯Ÿåˆ°æ„æƒ³ã€ä»£ç å’Œæ•°å­¦åœ¨è¿­ä»£è¿‡ç¨‹ä¸­çš„ä¸åŒæ¨¡å¼å’Œè¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.06770v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.06770v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2509.06770v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RISE-Enhancing-VLM-Image-Annotation-with-Self-Supervised-Reasoning"><a href="#RISE-Enhancing-VLM-Image-Annotation-with-Self-Supervised-Reasoning" class="headerlink" title="RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning"></a>RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning</h2><p><strong>Authors:Suhang Hu, Wei Hu, Yuhang Su, Fan Zhang</strong></p>
<p>Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven â€œannotation-reasoning-annotationâ€ closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs.Code and resources are available at: <a target="_blank" rel="noopener" href="https://github.com/HSH55/RISE">https://github.com/HSH55/RISE</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æƒ…æ„Ÿåˆ†ç±»å’Œä¸Šä¸‹æ–‡é©±åŠ¨çš„å¯¹è±¡æ£€æµ‹ç­‰ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤æ‚æ¨ç†ã€‚æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åªå…³æ³¨æ ‡æ³¨ç»“æœï¼Œå¿½ç•¥äº†æ½œåœ¨çš„æ¨ç†è¿‡ç¨‹ï¼Œè€Œè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰ç”±äºç¼ºä¹é«˜è´¨é‡ã€ç»è¿‡éªŒè¯çš„æ¨ç†é“¾ï¼ˆCoTsï¼‰ï¼Œäº§ç”Ÿäº†ä¸ä¸€è‡´çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬å¼•å…¥äº†RISEï¼ˆReason-Inspire-Strengthen-Expertiseï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ˆRISE-CoTï¼‰ï¼Œä¸€ä¸ªç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„â€œæ ‡æ³¨-æ¨ç†-æ ‡æ³¨â€é—­ç¯ï¼Œé€šè¿‡éªŒè¯å…¶é‡å»ºåŸå§‹æ ‡æ³¨çš„èƒ½åŠ›ï¼Œç”Ÿæˆè§†è§‰åŒ–ã€é€»è¾‘ä¸€è‡´çš„æ¨ç†é“¾ï¼Œè€Œæ— éœ€ç›´æ¥æ³„éœ²ä¿¡æ¯ã€‚åœ¨æ¿€åŠ±å’Œå¼ºåŒ–é˜¶æ®µï¼ˆRISE-R1ï¼‰åˆ™åˆ©ç”¨RISE-CoTå¥–åŠ±è¿‡æ»¤å‡ºçš„é«˜è´¨é‡æ¨ç†é“¾å­é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œéšåè¿›è¡Œå¼ºåŒ–å¾®è°ƒï¼Œä»¥äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†å’Œå‡†ç¡®çš„æ ‡æ³¨ï¼Œåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­å®ç°ä¸“ä¸šçº§è¡¨ç°ã€‚åœ¨å¤æ‚å’Œç®€å•çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒRISEè®­ç»ƒçš„Qwen2-VL-2Bä¼˜äºSFTå’ŒVisual-RFTï¼Œå®ç°äº†ç¨³å¥çš„æ€§èƒ½å’Œå¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚RISEæä¾›äº†ä¸€ä¸ªæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæ¨ç†é“¾çš„è‡ªæˆ‘ç›‘ç£è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨è¿›VLMæ¨ç†ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/HSH5L/RISE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HSH5L/RISEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13229v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†ç•Œè¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æƒ…æ„Ÿåˆ†ç±»å’Œä¸Šä¸‹æ–‡é©±åŠ¨çš„å¯¹è±¡æ£€æµ‹ç­‰ã€‚æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åªå…³æ³¨æ ‡æ³¨ç»“æœï¼Œå¿½è§†åº•å±‚æ¨ç†ã€‚è§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰åˆ™å› ç¼ºä¹é«˜è´¨é‡ã€ç»è¿‡éªŒè¯çš„æ¨ç†é“¾ï¼ˆCoTsï¼‰è€Œäº§ç”Ÿä¸ä¸€è‡´çš„æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºRISEï¼ˆReason-Inspire-Strengthen-Expertiseï¼‰æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µæ¥å…‹æœè¿™äº›å±€é™ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ˆRISE-CoTï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„â€œæ ‡æ³¨-æ¨ç†-æ ‡æ³¨â€é—­ç¯ç”Ÿæˆè§†è§‰åŒ–ã€é€»è¾‘ä¸€è‡´çš„æ¨ç†é“¾ï¼ŒéªŒè¯å…¶é‡å»ºåŸå§‹æ ‡æ³¨çš„èƒ½åŠ›ã€‚åœ¨æ¿€åŠ±ä¸å¼ºåŒ–é˜¶æ®µï¼ˆRISE-R1ï¼‰ï¼Œåˆ©ç”¨RISE-CoTå¥–åŠ±è¿‡æ»¤çš„é«˜è´¨é‡æ¨ç†é“¾å­é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œéšåè¿›è¡Œå¼ºåŒ–å¾®è°ƒä»¥äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†å’Œå‡†ç¡®çš„æ ‡æ³¨ï¼Œå®ç°å¤æ‚è§†è§‰ä»»åŠ¡çš„â€œä¸“ä¸šçŸ¥è¯†â€ã€‚åœ¨å¤æ‚å’Œç®€å•çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸Šï¼ŒRISEè®­ç»ƒçš„Qwen2-VL-2Bæ¨¡å‹è¡¨ç°å‡ºä¼˜äºSFTå’ŒVisual-RFTçš„ç¨³å¥æ€§èƒ½å’Œå¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚RISEæä¾›äº†ä¸€ä¸ªæ— éœ€æ‰‹åŠ¨æ ‡æ³¨æ¨ç†é“¾çš„è‡ªæˆ‘ç›‘ç£è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨åŠ¨VLMçš„æ¨ç†èƒ½åŠ›è¿›æ­¥ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HSH55/RISE">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨å¤„ç†å¤æ‚å›¾åƒæ ‡æ³¨ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ›´ç²¾ç»†çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¿½ç•¥åº•å±‚æ¨ç†ï¼Œå¯¼è‡´æ€§èƒ½å±€é™ã€‚</li>
<li>è§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰å› ç¼ºä¹é«˜è´¨é‡æ¨ç†é“¾è€Œäº§ç”Ÿä¸ä¸€è‡´çš„æ¨ç†ã€‚</li>
<li>RISEæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ¨ç†é˜¶æ®µï¼ˆRISE-CoTï¼‰å’Œæ¿€åŠ±ä¸å¼ºåŒ–é˜¶æ®µï¼ˆRISE-R1ï¼‰ã€‚</li>
<li>RISE-CoTé€šè¿‡é—­ç¯ç”Ÿæˆè§†è§‰åŒ–ã€é€»è¾‘ä¸€è‡´çš„æ¨ç†é“¾ã€‚</li>
<li>RISEåˆ©ç”¨é«˜è´¨é‡æ¨ç†é“¾å­é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œéšåè¿›è¡Œå¼ºåŒ–å¾®è°ƒä»¥æé«˜æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2508.13229v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2508.13229v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2508.13229v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_R1_Reasoning/2508.13229v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_LLM/2509.12152v1/page_5_1.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Survival at Any Cost? LLMs and the Choice Between Self-Preservation and   Human Harm
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_Talking Head Generation/2509.12052v1/page_3_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  AvatarSync Rethinking Talking-Head Animation through Autoregressive   Perspective
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
