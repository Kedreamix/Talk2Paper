<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  Deriving accurate galaxy cluster masses using X-ray thermodynamic   profiles and graph neural networks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-17-æ›´æ–°"><a href="#2025-09-17-æ›´æ–°" class="headerlink" title="2025-09-17 æ›´æ–°"></a>2025-09-17 æ›´æ–°</h1><h2 id="Deriving-accurate-galaxy-cluster-masses-using-X-ray-thermodynamic-profiles-and-graph-neural-networks"><a href="#Deriving-accurate-galaxy-cluster-masses-using-X-ray-thermodynamic-profiles-and-graph-neural-networks" class="headerlink" title="Deriving accurate galaxy cluster masses using X-ray thermodynamic   profiles and graph neural networks"></a>Deriving accurate galaxy cluster masses using X-ray thermodynamic   profiles and graph neural networks</h2><p><strong>Authors:Asif Iqbal, Subhabrata Majumdar, Elena Rasia, Gabriel W. Pratt, Daniel de Andres, Jean-Baptiste Melin, Weiguang Cui</strong></p>
<p>Precise determination of galaxy cluster masses is crucial for establishing reliable mass-observable scaling relations in cluster cosmology. We employ graph neural networks (GNNs) to estimate galaxy cluster masses from radially sampled profiles of the intra-cluster medium (ICM) inferred from X-ray observations. GNNs naturally handle inputs of variable length and resolution by representing each ICM profile as a graph, enabling accurate and flexible modeling across diverse observational conditions. We trained and tested GNN model using state-of-the-art hydrodynamical simulations of galaxy clusters from The Three Hundred Project. The mass estimates using our method exhibit no systematic bias compared to the true cluster masses in the simulations. Additionally, we achieve a scatter in recovered mass versus true mass of about 6%, which is a factor of six smaller than obtained from a standard hydrostatic equilibrium approach. Our algorithm is robust to both data quality and cluster morphology and it is capable of incorporating model uncertainties alongside observational uncertainties. Finally, we apply our technique to XMM-Newton observed galaxy cluster samples and compare the GNN derived mass estimates with those obtained with $Y_{\rm SZ}$-M$<em>{500}$ scaling relations. Our results provide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZ derived masses, with higher mass clusters exhibiting a greater degree of deviation. Furthermore, we find the median bias to be $(1-b)&#x3D;0.85</em>{-14}^{+34}$, albeit with significant dispersion due to its mass dependence. This work takes a significant step towards establishing unbiased observable mass scaling relations by integrating X-ray, SZ and optical datasets using deep learning techniques, thereby enhancing the role of galaxy clusters in precision cosmology. </p>
<blockquote>
<p>ç²¾ç¡®åœ°ç¡®å®šæ˜Ÿç³»å›¢çš„è´¨é‡å¯¹äºå»ºç«‹é›†ç¾¤å®‡å®™å­¦ä¸­å¯é çš„è´¨é‡è§‚æµ‹æ¯”ä¾‹å…³ç³»è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¼°è®¡æ˜Ÿç³»å›¢è´¨é‡ï¼Œè¿™äº›ä¼°è®¡åŸºäºä»Xå°„çº¿è§‚æµ‹æ¨æ–­çš„é›†ç¾¤å†…ä»‹è´¨ï¼ˆICMï¼‰çš„å¾„å‘é‡‡æ ·åˆ†å¸ƒã€‚å›¾ç¥ç»ç½‘ç»œé€šè¿‡å°†æ¯ä¸ªICMåˆ†å¸ƒè¡¨ç¤ºä¸ºå›¾æ¥è‡ªç„¶å¤„ç†å¯å˜é•¿åº¦å’Œåˆ†è¾¨ç‡çš„è¾“å…¥ï¼Œä»è€Œåœ¨å„ç§è§‚æµ‹æ¡ä»¶ä¸‹å®ç°å‡†ç¡®ä¸”çµæ´»å»ºæ¨¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªâ€œä¸‰ç™¾é¡¹ç›®â€æœ€å…ˆè¿›çš„æ˜Ÿç³»å›¢æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ¥è®­ç»ƒå’Œæµ‹è¯•GNNæ¨¡å‹ã€‚ä¸ä½¿ç”¨æ¨¡æ‹Ÿä¸­çš„çœŸå®å›¢ç°‡è´¨é‡ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¾—å‡ºçš„è´¨é‡ä¼°è®¡å€¼æ²¡æœ‰ç³»ç»Ÿæ€§åå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è·å¾—çš„æ¢å¤è´¨é‡ä¸çœŸå®è´¨é‡ä¹‹é—´çš„åå·®çº¦ä¸º6ï¼…ï¼Œè¿™æ˜¯é€šè¿‡ä¼ ç»Ÿçš„é™æ°´åŠ›å­¦å¹³è¡¡æ–¹æ³•è·å¾—çš„æ•£åº¦çš„å…­åˆ†ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„ç®—æ³•å¯¹æ•°æ®è´¨é‡å’Œé›†ç¾¤å½¢æ€å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ï¼Œå¹¶ä¸”å®ƒèƒ½å¤Ÿç»“åˆæ¨¡å‹ä¸ç¡®å®šæ€§å’Œè§‚æµ‹ä¸ç¡®å®šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¯¥æŠ€æœ¯åº”ç”¨äºXMM-ç‰›é¡¿è§‚æµ‹çš„æ˜Ÿç³»å›¢æ ·æœ¬ï¼Œå¹¶å°†GNNè¡ç”Ÿçš„è´¨é‡ä¼°è®¡å€¼ä¸é€šè¿‡Y_SZ-M_500æ¯”ä¾‹å…³ç³»è·å¾—çš„è´¨é‡è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœåœ¨5Ïƒæ°´å¹³ä¸Šæä¾›äº†æœ‰åŠ›çš„è¯æ®è¡¨æ˜å­˜åœ¨ä¸è´¨ç‚¹ç›¸å…³çš„åå·®ï¼Œæ›´é«˜è´¨é‡çš„å›¢ç°‡è¡¨ç°å‡ºæ›´å¤§çš„åå·®ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸­ä½åå·®ä¸ºï¼ˆ1-bï¼‰&#x3D; 0.85_{-14}^{+34}ï¼Œå°½ç®¡ç”±äºå…¶å¯¹è´¨é‡çš„ä¾èµ–æ€§è€Œå…·æœ‰è¾ƒå¤§çš„åˆ†æ•£åº¦ã€‚é€šè¿‡æ•´åˆXå°„çº¿ã€SZå’Œå…‰å­¦æ•°æ®é›†ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å»ºç«‹æ— åçš„å¯è§‚æµ‹è´¨é‡æ¯”ä¾‹å…³ç³»ï¼Œè¿™é¡¹å·¥ä½œè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œä»è€Œå¢å¼ºäº†æ˜Ÿç³»å›¢åœ¨ç²¾ç¡®å®‡å®™å­¦ä¸­çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12199v1">PDF</a> 20 pages, 15 figures, 6 tables, resubmitted to A&amp;A after revision,   comments welcome</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ–¹æ³•ï¼Œæ ¹æ®Xå°„çº¿è§‚æµ‹å¾—å‡ºçš„æ˜Ÿç³»å›¢å†…ä»‹è´¨ï¼ˆICMï¼‰å¾„å‘åˆ†å¸ƒå›¾ä¼°è®¡æ˜Ÿç³»å›¢è´¨é‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿè‡ªç„¶å¤„ç†ä¸åŒé•¿åº¦å’Œåˆ†è¾¨ç‡çš„è¾“å…¥æ•°æ®ï¼Œé€šè¿‡å›¾å½¢è¡¨ç¤ºæ¯ä¸ªICMåˆ†å¸ƒç‰¹å¾ï¼Œåœ¨å¤šç§è§‚æµ‹æ¡ä»¶ä¸‹å®ç°ç²¾ç¡®çµæ´»å»ºæ¨¡ã€‚ç ”ç©¶ä½¿ç”¨The Three Hundred Projectå…ˆè¿›çš„æ°´åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒå¹¶æµ‹è¯•äº†GNNæ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„ä¼°ç®—è´¨é‡ç›¸è¾ƒäºæ¨¡æ‹Ÿä¸­çš„çœŸå®è´¨é‡æ— æ˜æ˜¾ç³»ç»Ÿæ€§åå·®ï¼Œç›¸è¾ƒäºé™æ°´å‹å¹³è¡¡æ–¹æ³•æ‰€å¾—åˆ°çš„ä¼°è®¡è´¨é‡å€¼è¯¯å·®å‡å°‘çº¦é™ä½äº†ç™¾åˆ†ä¹‹å…­çš„æ•£å°„ç¨‹åº¦ã€‚è¿™ä¸€ç®—æ³•ç¨³å¥æ€§è‰¯å¥½ï¼Œèƒ½å¤Ÿåº”å¯¹ä¸åŒè´¨é‡å’Œå½¢æ€ä¸‹çš„æ•°æ®å·®å¼‚å¹¶èƒ½æ•´åˆæ¨¡å‹åŠè§‚æµ‹çš„ä¸ç¡®å®šæ€§ã€‚æœ¬ç ”ç©¶è¿›ä¸€æ­¥åº”ç”¨è¿™ä¸€æŠ€æœ¯äºæ­è½½åœ¨å¤©æ–‡å«æ˜Ÿé›†ç¾¤XMMä¸Šçš„Newtonè§‚æµ‹ç»“æœä¸­ä¼°ç®—å‡ºçš„æ˜Ÿç³»å›¢æ ·æœ¬çš„è´¨é‡ä¸SZè°±ä¸‹çš„ç›¸åº”æ•°å€¼è¿›è¡Œå¯¹æ¯”ã€‚æœ¬ç ”ç©¶ä»¥å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜ï¼šæœ‰åå­˜åœ¨çš„MZå‘ˆç°å‡ºå¼ºå¤§çš„è´¨é‡å’Œç›¸äº’åå·®çš„å½±å“å¹¶ä¸”ä¸»è¦é›†ä¸­åœ¨é«˜è´¨é‡çš„é›†ç¾¤åŒºåŸŸåç¦»ç¨‹åº¦é«˜è¿™å°†åœ¨äº”ä¸ªæ–¹é¢ä½¿ç”¨åŸºæœ¬é€‰æ‹©ä¸Šä¸‹æ³•çš„çš„ç²¾ç»†è§„å®šã€‚ã€‚é€šè¿‡è¿™äº›ç ”ç©¶å’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ä¸ªä¸­ä½æ•°åå·®ä¸ºï¼ˆ1-bï¼‰&#x3D; 0.85_{-14}^{+34}ï¼Œå°½ç®¡ç”±äºè´¨é‡ä¾èµ–æ€§çš„å­˜åœ¨å¯¼è‡´å¼¥æ•£åº¦è¾ƒå¤§ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•´åˆXå°„çº¿ã€SZå’Œå…‰å­¦æ•°æ®é›†å¹¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæœç€å»ºç«‹æ— åè§‚æµ‹è´¨é‡æ¯”ä¾‹å…³ç³»è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä»è€Œæå‡æ˜Ÿç³»å›¢åœ¨ç²¾ç¡®å®‡å®™å­¦ä¸­çš„è§’è‰²ã€‚ç ”ç©¶å±•æœ›è¯¥æŠ€æœ¯çš„å·¨å¤§æ½œåŠ›å°†ä¸ºå»ºç«‹ç²¾ç¡®çš„å®‡å®™å­¦æ¨¡å‹å¥ å®šé‡è¦åŸºç¡€ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¯¥è®ºæ–‡å±•ç¤ºäº†åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œä¼°ç®—æ˜Ÿç³»å›¢è´¨é‡çš„æ–°æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æé«˜å®‡å®™å­¦ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œä¼°ç®—æ˜Ÿç³»å›¢è´¨é‡ï¼Œåˆ©ç”¨Xå°„çº¿è§‚æµ‹çš„ICMå¾„å‘åˆ†å¸ƒæ•°æ®ã€‚æ­¤æ–¹æ³•é€‚åº”ä¸åŒé•¿åº¦å’Œåˆ†è¾¨ç‡çš„æ•°æ®è¾“å…¥ï¼Œå±•ç°å‡ºç²¾ç¡®çµæ´»å»ºæ¨¡çš„èƒ½åŠ›ã€‚<br>äºŒã€åˆ©ç”¨å…ˆè¿›çš„æ°´åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒä¸éªŒè¯ï¼Œè¡¨æ˜è´¨é‡ä¼°ç®—ç»“æœæ— ç³»ç»Ÿæ€§åå·®ä¸”è¯¯å·®åˆ†æ•£è¾ƒå°ã€‚<br>ä¸‰ã€ç ”ç©¶éªŒè¯äº†ç®—æ³•å¯¹æ•°æ®è´¨é‡å’Œé›†ç¾¤å½¢æ€çš„ç¨³å¥æ€§ï¼Œå¹¶è€ƒè™‘äº†æ¨¡å‹åŠè§‚æµ‹çš„ä¸ç¡®å®šæ€§ã€‚<br>å››ã€åº”ç”¨æ­¤æ–¹æ³•äºå®é™…è§‚æµ‹æ•°æ®ï¼Œå‘ç°å¤§è§„æ¨¡æ˜Ÿç³»å›¢çš„è´¨é‡åå·®ç°è±¡ï¼Œå¹¶æŒ‡å‡ºå…¶è´¨é‡ä¾èµ–ç‰¹æ€§ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12199v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12199v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12199v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12199v1/page_4_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Anatomy-X-Ray-Foundation-Model"><a href="#Multi-Anatomy-X-Ray-Foundation-Model" class="headerlink" title="Multi Anatomy X-Ray Foundation Model"></a>Multi Anatomy X-Ray Foundation Model</h2><p><strong>Authors:Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas</strong></p>
<p>X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology. </p>
<blockquote>
<p>Xå°„çº¿æˆåƒåœ¨æ”¾å°„å­¦ä¸­æ— å¤„ä¸åœ¨ï¼Œç„¶è€Œå¤§å¤šæ•°ç°æœ‰çš„AIåŸºç¡€æ¨¡å‹ä»…é™äºèƒ¸éƒ¨è§£å‰–ç»“æ„ï¼Œæ— æ³•æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ä¸´åºŠä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†XR-0ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè§£å‰–ç»“æ„çš„Xå°„çº¿åŸºç¡€æ¨¡å‹ï¼Œå®ƒä½¿ç”¨å¤§è§„æ¨¡ç§æœ‰æ•°æ®é›†è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ•°æ®å›¾åƒè¾¾115ä¸‡å¼ ï¼Œæ¶µç›–äº†å¤šç§è§£å‰–ç»“æ„åŒºåŸŸï¼Œå¹¶åœ¨åŒ…æ‹¬åˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²ã€å®šä½ã€è§†è§‰å®šä½å’ŒæŠ¥å‘Šç”Ÿæˆåœ¨å†…çš„12ä¸ªæ•°æ®é›†å’Œ20ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚XR-0åœ¨å¤§å¤šæ•°å¤šè§£å‰–ç»“æ„ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸“é—¨é’ˆå¯¹èƒ¸éƒ¨çš„åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§£å‰–ç»“æ„å¤šæ ·æ€§å’Œç›‘ç£å¯¹äºæ„å»ºç¨³å¥çš„é€šç”¨åŒ»ç–—è§†è§‰æ¨¡å‹è‡³å…³é‡è¦ï¼Œä¸ºæ”¾å°„å­¦ä¸­å¯æ‰©å±•å’Œå¯é€‚åº”çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12146v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†XR-0å¤šéƒ¨ä½Xå…‰åŸºç¡€æ¨¡å‹çš„ç ”ç©¶ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤§è§„æ¨¡ç§æœ‰æ•°æ®é›†è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæ¶µç›–å¤šç§è§£å‰–åŒºåŸŸï¼Œå¹¶åœ¨åŒ…æ‹¬åˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²ã€å®šä½ã€è§†è§‰å®šä½ä»¥åŠæŠ¥å‘Šç”Ÿæˆåœ¨å†…çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚XR-0åœ¨å¤šæ•°å¤šéƒ¨ä½ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨é’ˆå¯¹èƒ¸éƒ¨çš„ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç«äº‰åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§£å‰–å¤šæ ·æ€§å’Œç›‘ç£å¯¹äºæ„å»ºç¨³å¥ã€é€šç”¨çš„åŒ»å­¦è§†è§‰æ¨¡å‹è‡³å…³é‡è¦ï¼Œä¸ºæ”¾å°„å­¦ä¸­å¯æ‰©å±•å’Œå¯é€‚åº”çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XR-0æ¨¡å‹æ˜¯ä¸€ç§å¤šéƒ¨ä½Xå…‰åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰AIæ¨¡å‹åœ¨æ”¾å°„å­¦ä¸­å±€é™äºèƒ¸éƒ¨è§£å‰–çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ åœ¨å¤§è§„æ¨¡ç§æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–å¤šç§è§£å‰–åŒºåŸŸï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>XR-0åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²ç­‰ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>XR-0åœ¨å¤šæ•°å¤šéƒ¨ä½ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨èƒ¸éƒ¨ç‰¹å®šä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚</li>
<li>è§£å‰–å¤šæ ·æ€§å’Œç›‘ç£å¯¹äºæ„å»ºç¨³å¥ã€é€šç”¨çš„åŒ»å­¦è§†è§‰æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæ¨åŠ¨æ”¾å°„å­¦ä¸­å¯æ‰©å±•å’Œå¯é€‚åº”çš„AIç³»ç»Ÿçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12146v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12146v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12146v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12146v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data"><a href="#3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data" class="headerlink" title="3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data"></a>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data</h2><p><strong>Authors:Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali</strong></p>
<p>Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection. </p>
<blockquote>
<p>æŠ‘éƒç—‡ï¼ˆMDDï¼‰æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¯¹ä¸ªäººç¦ç¥‰å’Œå…¨çƒå…¬å…±å«ç”Ÿéƒ½æœ‰è´Ÿé¢å½±å“ã€‚åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆsMRIï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•è¿›è¡ŒæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ—©æœŸå¹²é¢„æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•è¦ä¹ˆä½¿ç”¨ä½“ç´ çº§ç‰¹å¾ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºé¢„å®šä¹‰è„‘å›¾è°±çš„æ‰‹åŠ¨åŒºåŸŸè¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰å¤æ‚è„‘æ¨¡å¼çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„æµç¨‹ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä»sMRIæ•°æ®ä¸­æå–3DåŒºåŸŸåµŒå…¥ï¼Œå¹¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§å®šä¹‰åŒºåŸŸçš„æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä¸€ç§åŸºäºå›¾è°±çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„ç»“æ„å’ŒåŠŸèƒ½è„‘å›¾è°±ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œç›´æ¥è®­ç»ƒViTsä»å‡åŒ€æå–çš„3Då—ä¸­è¯†åˆ«åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜ç”Ÿæˆäº†ä½™å¼¦ç›¸ä¼¼åº¦å›¾æ¥æ¨¡æ‹ŸåŒºåŸŸé—´çš„å…³ç³»ï¼Œå¹¶å¼•å¯¼åŸºäºGNNçš„åˆ†ç±»ã€‚ä½¿ç”¨REST-meta-MDDæ•°æ®é›†è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ†å±‚10å€äº¤å‰éªŒè¯ï¼Œæœ€ä½³æ¨¡å‹è·å¾—äº†78.98%çš„å‡†ç¡®ç‡ã€76.54%çš„æ•æ„Ÿæ€§ã€81.58%çš„ç‰¹å¼‚æ€§ã€81.58%çš„ç²¾ç¡®åº¦å’Œ78.98%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒåŸºäºå›¾è°±çš„æ¨¡å‹å§‹ç»ˆä¼˜äºåŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†åœ¨ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„è§£å‰–å­¦å…ˆéªŒè¿›è¡ŒæŠ‘éƒç—‡æ£€æµ‹æ—¶çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12143v1">PDF</a> 14 pages, 1 figure, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨Vision Transformersï¼ˆViTsï¼‰ä»ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆsMRIï¼‰æ•°æ®ä¸­æå–ä¸‰ç»´åŒºåŸŸåµŒå…¥ï¼Œç»“åˆGraph Neural Networkï¼ˆGNNï¼‰è¿›è¡Œåˆ†ç±»çš„æŠ‘éƒç—‡è‡ªåŠ¨åŒ–æ£€æµ‹æ¨¡å‹ã€‚æ¢è®¨äº†åŸºäºå›¾è°±çš„æ–¹æ³•å’ŒåŸºäºç«‹æ–¹ä½“æ–¹æ³•çš„ä¸¤ç§åŒºåŸŸå®šä¹‰ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨REST-meta-MDDæ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºå›¾è°±çš„æ–¹æ³•æ€§èƒ½æ›´ä½³ï¼Œå‡†ç¡®æ€§è¾¾åˆ°78.98%ï¼Œæ˜¾ç¤ºå‡ºåœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­åˆ©ç”¨ç‰¹å®šé¢†åŸŸè§£å‰–å…ˆéªŒçš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MDDæ˜¯ä¸€ç§æ™®éçš„å¿ƒç†ç–¾ç—…ï¼Œå¯¹ä¸ªäººå¥åº·å’Œç¤¾ä¼šå¥åº·éƒ½æœ‰è´Ÿé¢å½±å“ã€‚</li>
<li>åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆsMRIï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•è¿›è¡Œè‡ªåŠ¨åŒ–æ£€æµ‹å…·æœ‰æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå®ç°æ—©æœŸå¹²é¢„çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆäº†Vision Transformersï¼ˆViTsï¼‰å’ŒGraph Neural Networkï¼ˆGNNï¼‰çš„ç»Ÿä¸€ç®¡é“æ¨¡å‹ï¼Œç”¨äºä»sMRIæ•°æ®ä¸­æå–ä¸‰ç»´åŒºåŸŸåµŒå…¥å¹¶è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>æ¢è®¨äº†åŸºäºå›¾è°±å’ŒåŸºäºç«‹æ–¹ä½“ä¸¤ç§åŒºåŸŸå®šä¹‰ç­–ç•¥ï¼Œç»“æœæ˜¾ç¤ºåŸºäºå›¾è°±çš„æ–¹æ³•æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>åˆ©ç”¨REST-meta-MDDæ•°æ®é›†è¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®æ€§è¾¾åˆ°78.98%ï¼Œå…¶ä¸­åŸºäºå›¾è°±çš„æ¨¡å‹è¡¨ç°æ›´ç¨³å®šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12143v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12143v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12143v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="End-to-End-4D-Heart-Mesh-Recovery-Across-Full-Stack-and-Sparse-Cardiac-MRI"><a href="#End-to-End-4D-Heart-Mesh-Recovery-Across-Full-Stack-and-Sparse-Cardiac-MRI" class="headerlink" title="End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac   MRI"></a>End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac   MRI</h2><p><strong>Authors:Yihong Chen, Jiancheng Yang, Deniz Sayin Mercadier, Hieu Le, Juerg Schwitter, Pascal Fua</strong></p>
<p>Reconstructing cardiac motion from cine CMR sequences is critical for diagnosis, prediction, and intervention. Existing methods rely on complete CMR stacks to infer full heart motion, limiting their utility in intra-procedural scenarios where only sparse observations are available. We present TetHeart, the first end-to-end framework that unifies full 4D multi-structure heart mesh recovery from both offline full-stack acquisitions and intra-procedural sparse-slice observations. Our method leverages deep deformable tetrahedra, an explicit-implicit hybrid representation, to capture shape and motion in a coherent space shared across cardiac structures. It is initialized from high-quality pre-procedural or offline-acquired full stacks to build detailed, patient-specific heart meshes, which can then be updated using whatever slices are available, from full stacks down to a single slice. We further incorporate several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D feature assembly that dynamically integrates information from arbitrary numbers of slices at any position, combined with a distillation strategy from full-slice to sparse-slice settings to ensure accurate reconstruction under extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on three large public datasets and externally evaluated zero-shot on additional private interventional and public CMR datasets, TetHeart achieves state-of-the-art accuracy and strong generalization in both pre- and intra-procedural settings. </p>
<blockquote>
<p>ä»ç”µå½±å¼CMRåºåˆ—é‡å»ºå¿ƒè„è¿åŠ¨å¯¹äºè¯Šæ–­ã€é¢„æµ‹å’Œå¹²é¢„è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå®Œæ•´çš„CMRå †å æ¥æ¨æ–­å®Œæ•´çš„å¿ƒè„è¿åŠ¨ï¼Œè¿™åœ¨ä»…æä¾›ç¨€ç–è§‚å¯Ÿçš„æœ¯ä¸­åœºæ™¯ä¸­é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†TetHeartï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†ç¦»çº¿å…¨æ ˆé‡‡é›†å’Œæœ¯ä¸­ç¨€ç–åˆ‡ç‰‡è§‚å¯Ÿä¸‹çš„4Då¤šç»“æ„å¿ƒè„ç½‘æ ¼æ¢å¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯å˜å½¢å››é¢ä½“ï¼Œä¸€ç§æ˜¾å¼éšå¼æ··åˆè¡¨ç¤ºæ³•ï¼Œæ¥æ•è·è·¨å¿ƒè„ç»“æ„çš„å…±äº«è¿è´¯ç©ºé—´ä¸­çš„å½¢çŠ¶å’Œè¿åŠ¨ã€‚å®ƒé€šè¿‡å¯¹é«˜è´¨é‡æœ¯å‰æˆ–ç¦»çº¿è·å–çš„å…¨æ ˆè¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥æ„å»ºè¯¦ç»†çš„ã€é’ˆå¯¹ç‰¹å®šæ‚£è€…çš„å¿ƒè„ç½‘æ ¼ï¼Œç„¶åå¯ä»¥ä½¿ç”¨ä»»ä½•å¯ç”¨çš„åˆ‡ç‰‡è¿›è¡Œæ›´æ–°ï¼Œä»å…¨æ ˆåˆ°å•ä¸ªåˆ‡ç‰‡ã€‚æˆ‘ä»¬è¿˜èå…¥äº†å‡ ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰ä¸€ç§ç”¨äºåˆ‡ç‰‡è‡ªé€‚åº”çš„2D-3Dç‰¹å¾ç»„åˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåŠ¨æ€åœ°é›†æˆäº†ä»»æ„ä½ç½®ä»»æ„æ•°é‡çš„åˆ‡ç‰‡çš„ä¿¡æ¯ï¼Œå¹¶ç»“åˆäº†ä¸€ç§ä»å…¨åˆ‡ç‰‡åˆ°ç¨€ç–åˆ‡ç‰‡çš„è’¸é¦ç­–ç•¥ï¼Œä»¥ç¡®ä¿åœ¨æç«¯ç¨€ç–æƒ…å†µä¸‹å®ç°å‡†ç¡®çš„é‡å»ºï¼›ï¼ˆiiï¼‰ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¼±ç›‘ç£è¿åŠ¨å­¦ä¹ æ–¹æ¡ˆï¼Œåªéœ€è¦å…³é”®å¸§ï¼ˆä¾‹å¦‚EDå’ŒESï¼‰æ³¨é‡Šã€‚åœ¨ä¸‰ç»„å¤§å‹å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨é¢å¤–çš„ç§æœ‰ä»‹å…¥å…¬å…±CMRæ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°ï¼ŒTetHeartåœ¨æœ¯å‰å’Œæœ¯ä¸­ç¯å¢ƒä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12090v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTetHeartçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»ç¦»çº¿å…¨æ ˆé‡‡é›†å’Œæœ¯ä¸­ç¨€ç–åˆ‡ç‰‡è§‚æµ‹ä¸­æ¢å¤å…¨4Då¤šç»“æ„å¿ƒè„ç½‘æ ¼ã€‚å®ƒåˆ©ç”¨æ·±åº¦å¯å˜å½¢å››é¢ä½“ï¼Œç»“åˆæ˜¾å¼éšå¼æ··åˆè¡¨ç¤ºæ³•ï¼Œæ•æ‰å¿ƒè„ç»“æ„çš„å½¢çŠ¶å’Œè¿åŠ¨ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å…¨æ ˆåˆ°å•ä¸ªåˆ‡ç‰‡è¿›è¡Œæ›´æ–°ï¼Œå¹¶ä½¿ç”¨åˆ‡ç‰‡è‡ªé€‚åº”çš„2D-3Dç‰¹å¾ç»„è£…æœºåˆ¶å’Œè’¸é¦ç­–ç•¥ç¡®ä¿æç«¯ç¨€ç–æ¡ä»¶ä¸‹çš„å‡†ç¡®é‡å»ºã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨ä¸¤é˜¶æ®µå¼±ç›‘ç£è¿åŠ¨å­¦ä¹ æ–¹æ¡ˆï¼Œä»…éœ€è¦å…³é”®å¸§ï¼ˆå¦‚EDå’ŒESï¼‰æ³¨é‡Šã€‚è¯¥æ¡†æ¶åœ¨å…¬å…±æ•°æ®é›†å’Œç§æœ‰ä»‹å…¥åŠå…¬å…±CMRæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TetHeartæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»ç¦»çº¿å…¨æ ˆé‡‡é›†å’Œæœ¯ä¸­ç¨€ç–åˆ‡ç‰‡è§‚æµ‹ä¸­æ¢å¤å¿ƒè„è¿åŠ¨çš„å®Œæ•´4Då¤šç»“æ„ç½‘æ ¼ã€‚</li>
<li>TetHeartåˆ©ç”¨æ·±åº¦å¯å˜å½¢å››é¢ä½“è¿›è¡Œå¿ƒè„å½¢çŠ¶çš„è¡¨ç¤ºå’Œè¿åŠ¨æ•æ‰ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”ä¸åŒæ•°é‡çš„åˆ‡ç‰‡å¹¶èƒ½åœ¨å…¨æ ˆè‡³å•ä¸ªåˆ‡ç‰‡çš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ–°ã€‚</li>
<li>é€šè¿‡åˆ‡ç‰‡è‡ªé€‚åº”çš„2D-3Dç‰¹å¾ç»„è£…æœºåˆ¶å’Œè’¸é¦ç­–ç•¥ï¼Œç¡®ä¿åœ¨æç«¯ç¨€ç–æ¡ä»¶ä¸‹çš„å‡†ç¡®é‡å»ºã€‚</li>
<li>TetHearté‡‡ç”¨ä¸¤é˜¶æ®µå¼±ç›‘ç£è¿åŠ¨å­¦ä¹ æ–¹æ¡ˆï¼Œä»…éœ€è¦å…³é”®å¸§æ³¨é‡Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12090v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12090v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12090v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="End-to-End-Learning-of-Multi-Organ-Implicit-Surfaces-from-3D-Medical-Imaging-Data"><a href="#End-to-End-Learning-of-Multi-Organ-Implicit-Surfaces-from-3D-Medical-Imaging-Data" class="headerlink" title="End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical   Imaging Data"></a>End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical   Imaging Data</h2><p><strong>Authors:Farahdiba Zarin, Nicolas Padoy, JÃ©rÃ©my Dana, Vinkle Srivastav</strong></p>
<p>The fine-grained surface reconstruction of different organs from 3D medical imaging can provide advanced diagnostic support and improved surgical planning. However, the representation of the organs is often limited by the resolution, with a detailed higher resolution requiring more memory and computing footprint. Implicit representations of objects have been proposed to alleviate this problem in general computer vision by providing compact and differentiable functions to represent the 3D object shapes. However, architectural and data-related differences prevent the direct application of these methods to medical images. This work introduces ImplMORe, an end-to-end deep learning method using implicit surface representations for multi-organ reconstruction from 3D medical images. ImplMORe incorporates local features using a 3D CNN encoder and performs multi-scale interpolation to learn the features in the continuous domain using occupancy functions. We apply our method for single and multiple organ reconstructions using the totalsegmentator dataset. By leveraging the continuous nature of occupancy functions, our approach outperforms the discrete explicit representation based surface reconstruction approaches, providing fine-grained surface details of the organ at a resolution higher than the given input image. The source code will be made publicly available at: <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/ImplMORe">https://github.com/CAMMA-public/ImplMORe</a> </p>
<blockquote>
<p>ä»ä¸‰ç»´åŒ»å­¦æˆåƒä¸­å¯¹ä¸åŒå™¨å®˜çš„ç²¾ç»†è¡¨é¢é‡å»ºå¯ä»¥æä¾›å…ˆè¿›çš„è¯Šæ–­æ”¯æŒå’Œæ”¹è¿›çš„æ‰‹æœ¯è®¡åˆ’ã€‚ç„¶è€Œï¼Œå™¨å®˜çš„è¡¨ç¤ºé€šå¸¸å—åˆ°åˆ†è¾¨ç‡çš„é™åˆ¶ï¼Œæ›´é«˜çš„è¯¦ç»†åˆ†è¾¨ç‡éœ€è¦æ›´å¤šçš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚ä¸ºäº†ç¼“è§£ä¸€èˆ¬è®¡ç®—æœºè§†è§‰ä¸­çš„è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æå‡ºäº†å¯¹è±¡çš„éšå¼è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡æä¾›ç´§å‡‘å’Œå¯å¾®åˆ†çš„å‡½æ•°æ¥è¡¨ç¤ºä¸‰ç»´å¯¹è±¡çš„å½¢çŠ¶ã€‚ç„¶è€Œï¼Œç»“æ„å’Œæ•°æ®ç›¸å…³çš„å·®å¼‚é˜»æ­¢äº†è¿™äº›æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒä¸Šçš„ç›´æ¥åº”ç”¨ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†ImplMOReï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨éšå¼è¡¨é¢è¡¨ç¤ºè¿›è¡Œä¸‰ç»´åŒ»å­¦å›¾åƒå¤šå™¨å®˜é‡å»ºçš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚ImplMOReåˆ©ç”¨ä¸‰ç»´CNNç¼–ç å™¨èå…¥å±€éƒ¨ç‰¹å¾ï¼Œå¹¶æ‰§è¡Œå¤šå°ºåº¦æ’å€¼ï¼Œåˆ©ç”¨å ç”¨å‡½æ•°åœ¨è¿ç»­åŸŸä¸­å­¦ä¹ ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨totalsegmentatoræ•°æ®é›†è¿›è¡Œå•å™¨å®˜å’Œå¤šå™¨å®˜é‡å»ºåº”ç”¨ã€‚é€šè¿‡åˆ©ç”¨å ç”¨å‡½æ•°çš„è¿ç»­æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºç¦»æ•£æ˜¾å¼è¡¨ç¤ºçš„é‡å»ºæ–¹æ³•ï¼Œæä¾›äº†é«˜äºè¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡çš„å™¨å®˜ç²¾ç»†è¡¨é¢ç»†èŠ‚ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/ImplMORe%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/CAMMA-public/ImplMOReå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12068v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒçš„ç²¾ç»†è¡¨é¢é‡å»ºå¯¹äºé«˜çº§è¯Šæ–­å’Œæ‰‹æœ¯è§„åˆ’æœ‰ç€é‡è¦çš„æ”¯æŒä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºåˆ†è¾¨ç‡é™åˆ¶ï¼Œé«˜è§£æåº¦éœ€è¦æ›´å¤šçš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚è¯¥ç ”ç©¶å¼•å…¥äº†ImplMOReæ–¹æ³•ï¼Œä¸€ç§åˆ©ç”¨éšå¼è¡¨é¢è¡¨ç¤ºçš„å¤šå™¨å®˜é‡å»ºæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚å®ƒé€šè¿‡æ•´åˆå±€éƒ¨ç‰¹å¾å’Œè¿›è¡Œå¤šå°ºåº¦æ’å€¼ï¼Œå®ç°äº†åœ¨è¿ç»­åŸŸå†…å­¦ä¹ ç‰¹å¾çš„èƒ½åŠ›ï¼Œä¼˜äºç¦»æ•£æ˜¾å¼è¡¨ç¤ºçš„è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œå¹¶æä¾›äº†é«˜äºè¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡çš„å™¨å®˜ç²¾ç»†è¡¨é¢ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒçš„ç²¾ç»†è¡¨é¢é‡å»ºå¯¹äºè¯Šæ–­å’Œæ‰‹æœ¯è§„åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„åŒ»å­¦å›¾åƒé‡å»ºå—é™äºåˆ†è¾¨ç‡ï¼Œé«˜è§£æåº¦éœ€è¦æ›´å¤šè®¡ç®—èµ„æºã€‚</li>
<li>ImplMOReæ–¹æ³•æ˜¯ä¸€ç§åˆ©ç”¨éšå¼è¡¨é¢è¡¨ç¤ºçš„å¤šå™¨å®˜é‡å»ºæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>ImplMOReç»“åˆäº†å±€éƒ¨ç‰¹å¾å’Œå¤šå°ºåº¦æ’å€¼æŠ€æœ¯ã€‚</li>
<li>ImplMOReæ–¹æ³•åœ¨è¿ç»­åŸŸå†…å­¦ä¹ ç‰¹å¾çš„èƒ½åŠ›æ˜¯å…¶ä¼˜åŠ¿ä¹‹ä¸€ã€‚</li>
<li>ImplMOReæ–¹æ³•ä¼˜äºç¦»æ•£æ˜¾å¼è¡¨ç¤ºçš„è¡¨é¢é‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12068v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12068v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Study-of-Spectral-Variability-between-flaring-and-non-flaring-state-in-M74-X-1"><a href="#A-Study-of-Spectral-Variability-between-flaring-and-non-flaring-state-in-M74-X-1" class="headerlink" title="A Study of Spectral Variability between flaring and non-flaring state in   M74 X-1"></a>A Study of Spectral Variability between flaring and non-flaring state in   M74 X-1</h2><p><strong>Authors:Aman Upadhyay, Tanuman Ghosh, Vikram Rana</strong></p>
<p>We conducted an extensive long-term spectral and timing study of the ultraluminous X-ray source (ULX) M74 X-1, using data taken between 2001 and 2021 by Chandra and XMM-Newton X-ray observatories. Our analysis shows that flares are present in some observations, whereas they are absent in others. Flaring state exhibits two-component spectra at a lower average flux level, whereas the non-flaring state displays single-component spectra at a higher average flux level. The M74 X-1 spectra are best described by the combination of accretion disk and Comptonization components, a dual thermal disk blackbody model, and a modified multi-temperature disk blackbody model. Using the dual thermal disk blackbody model, we obtain cool and hot temperatures of $T_{in}$ (cool) &#x3D; $0.38^{+0.08}<em>{-0.06}$ keV and $T</em>{in}$ (hot) &#x3D; $1.67^{+0.18}<em>{-0.13}$ keV, respectively, suggesting two temperature emitting regions and indicating possible presence of outflowing wind along with the accretion disk. We found a Gaussian feature at $E</em>{line}$ &#x3D; $0.96^{+0.05}<em>{-0.11}$ keV with $\sigma$ &#x3D; $0.11^{+0.13}</em>{-0.06}$ keV in the spectra of the flaring state which can be interpreted as the unresolved wind feature in the system when compared to similar feature seen in other ULXs. Plotting the hardness luminosity diagram, we get a trend of increasing hardness with luminosity, suggesting the presence of geometrical beaming in a low-inclination system. Additionally, using the hot disk blackbody component from the dual thermal disk blackbody model, we estimate the mass of the compact object to be M &#x3D; $7.1^{+1.4}_{-1.3}$ M$_\odot$, classifying it as a stellar-mass black hole and confirming super-Eddington accretion in the system. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹è¶…äº®Xå°„çº¿æºï¼ˆULXï¼‰M74 X-1è¿›è¡Œäº†é•¿æœŸçš„é¢‘è°±å’Œæ—¶é—´ç ”ç©¶ï¼Œä½¿ç”¨çš„æ•°æ®æ¥è‡ªChandraå’ŒXMM-Newton Xå°„çº¿å¤©æ–‡å°åœ¨2001å¹´è‡³2021å¹´é—´çš„è§‚æµ‹æ•°æ®ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨æŸäº›è§‚æµ‹ä¸­å­˜åœ¨è€€æ–‘ï¼Œè€Œåœ¨å¦ä¸€äº›è§‚æµ‹ä¸­åˆ™ä¸å­˜åœ¨ã€‚è€€æ–‘çŠ¶æ€è¡¨ç°å‡ºè¾ƒä½å¹³å‡æµé‡æ°´å¹³çš„åŒç»„åˆ†å…‰è°±ï¼Œè€Œéè€€æ–‘çŠ¶æ€åˆ™è¡¨ç°å‡ºè¾ƒé«˜å¹³å‡æµé‡æ°´å¹³çš„å•ç»„åˆ†å…‰è°±ã€‚M74 X-1çš„é¢‘è°±æœ€å¥½é€šè¿‡ç»“åˆå¸ç§¯ç›˜å’Œåº·æ™®é¡¿åŒ–æˆåˆ†ã€åŒæ¸©ç›˜é»‘ä½“æ¨¡å‹ä»¥åŠä¿®æ­£çš„å¤šæ¸©ç›˜é»‘ä½“æ¨¡å‹æ¥æè¿°ã€‚ä½¿ç”¨åŒæ¸©ç›˜é»‘ä½“æ¨¡å‹ï¼Œæˆ‘ä»¬è·å¾—äº†å†·å´å’Œé«˜æ¸©çš„$T_{in}$ï¼ˆå†·å´ï¼‰&#x3D; $0.38^{+0.08}<em>{-0.06}$ keVå’Œ$T</em>{in}$ï¼ˆçƒ­ï¼‰&#x3D; $1.67^{+0.18}<em>{-0.13}$ keVï¼Œåˆ†åˆ«è¡¨æ˜æœ‰ä¸¤ä¸ªæ¸©åº¦å‘å°„åŒºåŸŸï¼Œå¹¶å¯èƒ½ä¼´æœ‰æµå‡ºé£çš„å­˜åœ¨ã€‚æˆ‘ä»¬åœ¨è€€æ–‘çŠ¶æ€çš„é¢‘è°±ä¸­å‘ç°äº†ä¸€ä¸ªé«˜æ–¯ç‰¹å¾ï¼Œå…¶èƒ½é‡çº¿ä¸º$E</em>{line}$ &#x3D; $0.96^{+0.05}<em>{-0.11}$ keVï¼Œæ ‡å‡†å·®ä¸ºÏƒ &#x3D; $0.11^{+0.13}</em>{-0.06}$ keVã€‚ä¸å…¶ä»–ULXè§‚å¯Ÿåˆ°çš„ç±»ä¼¼ç‰¹å¾ç›¸æ¯”ï¼Œè¿™å¯ä»¥è¢«è§£é‡Šä¸ºç³»ç»Ÿå†…éƒ¨æœªè§£å†³çš„é£ç‰¹å¾ã€‚ç»˜åˆ¶ç¡¬åº¦äº®åº¦å›¾ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ç¡¬åº¦éšäº®åº¦å¢åŠ è€Œå¢åŠ çš„è¶‹åŠ¿ï¼Œè¿™è¡¨æ˜åœ¨ä½å€¾æ–œç³»ç»Ÿä¸­å­˜åœ¨å‡ ä½•é›†æŸæ•ˆåº”ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨åŒæ¸©ç›˜é»‘ä½“æ¨¡å‹ä¸­çš„çƒ­ç›˜é»‘ä½“æˆåˆ†ï¼Œæˆ‘ä»¬ä¼°è®¡ç´§å‡‘ç‰©ä½“çš„è´¨é‡ä¸ºM &#x3D; $7.1^{+1.4}_{-1.3}$ MâŠ™ï¼Œå°†å…¶åˆ†ç±»ä¸ºæ’æ˜Ÿè´¨é‡é»‘æ´ï¼Œå¹¶ç¡®è®¤ç³»ç»Ÿä¸­å­˜åœ¨è¶…çˆ±ä¸é¡¿å¸ç§¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12050v1">PDF</a> Published in The Astrophysical Journal</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹M74 X-1è¿™ä¸€è¶…äº®Xå°„çº¿æºè¿›è¡Œäº†é•¿è¾¾äºŒåå¹´çš„å…‰è°±å’Œæ—¶é—´ç ”ç©¶åˆ†æã€‚å‘ç°å…¶å­˜åœ¨è€€æ–‘å’Œæ— è€€æ–‘ä¸¤ç§çŠ¶æ€ï¼Œæ˜¾ç¤ºä¸åŒçš„å…‰è°±ç‰¹æ€§ã€‚é‡‡ç”¨åŒæ¸©ç›˜é»‘ä½“æ¨¡å‹è§£æå…¶å…‰è°±ï¼Œå‘ç°åŒæ¸©åº¦å‘å°„åŒºåŸŸå¹¶æš—ç¤ºå­˜åœ¨æµå‡ºé£ã€‚è¿˜è§‚å¯Ÿåˆ°ç³»ç»Ÿç¡¬åº¦éšå…‰åº¦å¢åŠ è€Œå¢åŠ çš„è¶‹åŠ¿ï¼Œæš—ç¤ºä½å€¾è§’ç³»ç»Ÿä¸­å­˜åœ¨å‡ ä½•å…‰æŸæ•ˆåº”ã€‚é€šè¿‡æ¨¡å‹ä¼°ç®—å‡ºç´§å‡‘ç‰©ä½“çš„è´¨é‡ä¸ºæ’æ˜Ÿè´¨é‡é»‘æ´ï¼Œå¹¶ç¡®è®¤äº†ç³»ç»Ÿä¸­çš„è¶…çˆ±ä¸é¡¿å¸ç§¯ç°è±¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹M74 X-1è¿›è¡Œäº†é•¿è¾¾äºŒåå¹´çš„å…‰è°±å’Œæ—¶é—´ç ”ç©¶åˆ†æã€‚</li>
<li>æºå­˜åœ¨è€€æ–‘å’Œæ— è€€æ–‘ä¸¤ç§çŠ¶æ€ï¼Œæ˜¾ç¤ºä¸åŒçš„å…‰è°±ç‰¹æ€§ã€‚</li>
<li>åŒæ¸©ç›˜é»‘ä½“æ¨¡å‹æœ€ä½³åœ°æè¿°äº†M74 X-1çš„å…‰è°±ã€‚</li>
<li>å‘ç°äº†åŒæ¸©åº¦å‘å°„åŒºåŸŸå’Œå¯èƒ½çš„æµå‡ºé£å­˜åœ¨ã€‚</li>
<li>ç³»ç»Ÿç¡¬åº¦ä¸å…‰åº¦ä¹‹é—´å‘ˆç°å‡ºå¢åŠ è¶‹åŠ¿ï¼Œæš—ç¤ºå‡ ä½•å…‰æŸæ•ˆåº”ã€‚</li>
<li>é€šè¿‡æ¨¡å‹ä¼°ç®—ï¼Œç´§å‡‘ç‰©ä½“çš„è´¨é‡è¢«åˆ†ç±»ä¸ºæ’æ˜Ÿè´¨é‡é»‘æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_5_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12050v1/page_5_3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Exploring-Efficient-Open-Vocabulary-Segmentation-in-the-Remote-Sensing"><a href="#Exploring-Efficient-Open-Vocabulary-Segmentation-in-the-Remote-Sensing" class="headerlink" title="Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing"></a>Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing</h2><p><strong>Authors:Bingyu Li, Haocheng Dong, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</strong></p>
<p>Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS&#x2F;OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \href{<a target="_blank" rel="noopener" href="https://github.com/LiBingyu01/RSKT-Seg%7D%7B/textcolor%7Bblue%7D%7Bhere%7D%7D">https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}</a>. </p>
<blockquote>
<p>è¿œç¨‹å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆOVRSISï¼‰æ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œå®ƒå°†å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰é€‚åº”äºé¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸã€‚ç”±äºç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†å’Œè‡ªç„¶å›¾åƒä¸é¥æ„Ÿå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè¿™é¡¹ä»»åŠ¡ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºå¹¿æ³›ä½¿ç”¨çš„é¥æ„Ÿåˆ†å‰²æ•°æ®é›†å»ºç«‹äº†æ ‡å‡†åŒ–çš„OVRSISåŸºå‡†ï¼ˆOVRSISBenchï¼‰ï¼Œä½¿å„ç§æ–¹æ³•èƒ½å¤Ÿè¿›è¡Œä¸€è‡´æ€§çš„è¯„ä¼°ã€‚ä½¿ç”¨è¯¥åŸºå‡†ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†è‹¥å¹²å…·æœ‰ä»£è¡¨æ€§çš„OVS&#x2F;OVRSISæ¨¡å‹ï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬åœ¨ç›´æ¥åº”ç”¨äºé¥æ„Ÿåœºæ™¯æ—¶çš„å±€é™æ€§ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é¥æ„Ÿçš„å…¨æ–°å¼€æ”¾è¯æ±‡åˆ†å‰²æ¡†æ¶RSKT-Segã€‚RSKT-Segé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰å¤šæ–¹å‘æˆæœ¬å›¾èšåˆï¼ˆRS-CMAï¼‰æ¨¡å—ï¼Œé€šè¿‡è®¡ç®—å¤šä¸ªæ–¹å‘ä¸Šçš„è§†è§‰è¯­è¨€ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œæ•æ‰æ—‹è½¬ä¸å˜çš„è§†è§‰çº¿ç´¢ï¼›ï¼ˆ2ï¼‰é«˜æ•ˆæˆæœ¬å›¾èåˆï¼ˆRS-Fusionï¼‰å˜å‹å™¨ï¼Œå®ƒé‡‡ç”¨è½»é‡çº§é™ç»´ç­–ç•¥ï¼Œè”åˆå»ºæ¨¡ç©ºé—´è¯­ä¹‰ä¾èµ–æ€§ï¼›ï¼ˆ3ï¼‰é¥æ„ŸçŸ¥è¯†è½¬ç§»ï¼ˆRS-Transferï¼‰æ¨¡å—ï¼Œæ³¨å…¥é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡å¢å¼ºä¸Šé‡‡æ ·ä¿ƒè¿›é¢†åŸŸé€‚åº”ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRSKT-Segåœ¨å¼ºå¤§çš„OVSåŸºå‡†çº¿ä¸Šï¼ŒmIoUæé«˜äº†+3.8ï¼Œ+5.9 mACCï¼ŒåŒæ—¶é€šè¿‡æœ‰æ•ˆçš„èšåˆå®ç°äº†2å€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨[<a target="_blank" rel="noopener" href="https://github.com/LiBingyu01/RSKT-Seg]%EF%BC%88%E7%82%B9%E5%87%BB%E8%93%9D%E8%89%B2%E5%AD%97%E4%BD%93%E6%9F%A5%E7%9C%8B%EF%BC%89%E3%80%82">https://github.com/LiBingyu01/RSKT-Seg]ï¼ˆç‚¹å‡»è“è‰²å­—ä½“æŸ¥çœ‹ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12040v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ï¼ˆOVRSISBenchï¼‰ï¼Œå¯¹å‡ ç§ä»£è¡¨æ€§çš„å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰&#x2F;OVRSISæ¨¡å‹è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ï¼Œå¹¶æ®æ­¤æå‡ºäº†é’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„å®šåˆ¶å¼€æ”¾è¯æ±‡åˆ†å‰²æ¡†æ¶RSKT-Segï¼Œé€šè¿‡å¼•å…¥ä¸‰ä¸ªå…³é”®ç»„ä»¶è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºå»ºç«‹æ ‡å‡†åŒ–çš„å¼€æ”¾è¯æ±‡é¥æ„Ÿå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ï¼ˆOVRSISBenchï¼‰ï¼ŒåŸºäºå¹¿æ³›ä½¿ç”¨çš„é¥æ„Ÿåˆ†å‰²æ•°æ®é›†ï¼Œä¿ƒè¿›ä¸åŒæ–¹æ³•çš„ç»Ÿä¸€è¯„ä¼°ã€‚</li>
<li>ç»¼åˆè¯„ä¼°äº†ä»£è¡¨æ€§OVS&#x2F;OVRSISæ¨¡å‹åœ¨é¥æ„Ÿåœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ¡†æ¶RSKT-Segï¼Œä¸“ä¸ºé¥æ„Ÿé¢†åŸŸå®šåˆ¶ã€‚</li>
<li>RSKT-SegåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå¤šæ–¹å‘ä»£ä»·å›¾èšåˆï¼ˆRS-CMAï¼‰æ¨¡å—ã€é«˜æ•ˆä»£ä»·å›¾èåˆï¼ˆRS-Fusionï¼‰å˜å‹å™¨å’Œé¥æ„ŸçŸ¥è¯†è½¬ç§»ï¼ˆRS-Transferï¼‰æ¨¡å—ã€‚</li>
<li>RS-CMAæ¨¡å—é€šè¿‡è®¡ç®—è·¨å¤šä¸ªæ–¹å‘çš„è§†è§‰è¯­è¨€ä½™å¼¦ç›¸ä¼¼æ€§æ¥æ•æ‰æ—‹è½¬ä¸å˜çš„è§†è§‰çº¿ç´¢ã€‚</li>
<li>RS-Fusionå˜å‹å™¨é€šè¿‡è½»é‡çº§é™ç»´ç­–ç•¥è”åˆå»ºæ¨¡ç©ºé—´å’Œè¯­ä¹‰ä¾èµ–æ€§ã€‚</li>
<li>RS-Transferæ¨¡å—æ³¨å…¥é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡å¢å¼ºä¸Šé‡‡æ ·ä¿ƒè¿›é¢†åŸŸé€‚åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12040v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12040v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12040v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12040v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.12040v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Very-low-field-MRI-scanners-from-the-ideal-to-the-real-permanent-magnet-array"><a href="#Very-low-field-MRI-scanners-from-the-ideal-to-the-real-permanent-magnet-array" class="headerlink" title="Very-low-field MRI scanners: from the ideal to the real permanent magnet   array"></a>Very-low-field MRI scanners: from the ideal to the real permanent magnet   array</h2><p><strong>Authors:Umberto Zanovello, Alessandro Arduino, Vittorio Basso, Luca Zilberti, Alessandro Sola, Andrea Agosto, Luca Toso, Oriano Bottauscio</strong></p>
<p>Very-low-field MRIs are becoming increasingly popular due to their portability and adaptability to different environments. They are being successfully used for various clinical applications, leading to a paradigm shift in the way imaging care is typically performed. The development of low-cost MRI scanner prototypes began a few years ago, with some interesting and promising open-source projects emerging in both hardware and software design. Using permanent magnets (PMs) to generate the static magnetic field B0 can substantially reduce the manufacturing cost of low-field scanners while achieving satisfactory homogeneity. This article focuses on characterizing magnet performance in terms of B0 spatial homogeneity. Specifically, it investigates its sensitivity to various factors and explores the reasons for discrepancies between numerical expectations and actual measurements on fabricated magnets. The analysis also examines the consequences of using different numerical model approximations, revisiting concepts most frequently used in other design contexts. While these assumptions simplify the numerical model and may improve its performance in terms of computational time, this paper demonstrates that they also impact the reliability of the obtained results. </p>
<blockquote>
<p>è¶…ä½åœºMRIå› å…¶ä¾¿æºæ€§å’Œé€‚åº”ä¸åŒç¯å¢ƒçš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚å®ƒä»¬å·²æˆåŠŸåº”ç”¨äºå„ç§ä¸´åºŠï¼Œå¯¼è‡´æˆåƒæŠ¤ç†æ–¹å¼å‘ç”ŸèŒƒå¼è½¬å˜ã€‚å‡ å¹´å‰å°±å¼€å§‹äº†ä½æˆæœ¬MRIæ‰«æä»ªåŸå‹çš„å¼€å‘ï¼Œåœ¨ç¡¬ä»¶å’Œè½¯ä»¶è®¾è®¡æ–¹é¢éƒ½å‡ºç°äº†ä¸€äº›æœ‰è¶£ä¸”å‰æ™¯å…‰æ˜çš„å¼€æºé¡¹ç›®ã€‚ä½¿ç”¨æ°¸ç£ä½“ï¼ˆPMï¼‰äº§ç”Ÿé™æ€ç£åœºB0å¯ä»¥å¤§å¹…åº¦é™ä½ä½åœºæ‰«æä»ªçš„åˆ¶é€ æˆæœ¬ï¼ŒåŒæ—¶å®ç°ä»¤äººæ»¡æ„çš„å‡åŒ€æ€§ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨æ°¸ç£ä½“åœ¨B0ç©ºé—´å‡åŒ€æ€§æ–¹é¢çš„æ€§èƒ½ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒç ”ç©¶äº†å…¶å¯¹å„ç§å› ç´ çš„æ•æ„Ÿæ€§ï¼Œå¹¶æ¢è®¨äº†æ•°å€¼é¢„æœŸä¸å®é™…æµ‹é‡ä¹‹é—´å·®å¼‚çš„æˆå› ã€‚åˆ†æè¿˜æ¢è®¨äº†ä½¿ç”¨ä¸åŒæ•°å€¼æ¨¡å‹è¿‘ä¼¼å€¼çš„å½±å“ï¼Œå¹¶é‡æ–°æ¢è®¨äº†å…¶ä»–è®¾è®¡è¯­å¢ƒä¸­æœ€å¸¸ä½¿ç”¨çš„æ¦‚å¿µã€‚è™½ç„¶è¿™äº›å‡è®¾ç®€åŒ–äº†æ•°å€¼æ¨¡å‹å¹¶å¯èƒ½æé«˜äº†å…¶è®¡ç®—æ—¶é—´æ€§èƒ½ï¼Œä½†æœ¬æ–‡è¯æ˜äº†å®ƒä»¬è¿˜å½±å“äº†ç»“æœçš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11762v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong><br>     æä½åœºMRIå› å…¶ä¾¿æºæ€§å’Œé€‚åº”ä¸åŒç¯å¢ƒçš„èƒ½åŠ›è€Œæ—¥ç›Šæ™®åŠï¼Œæ­£åœ¨ä¸ºå„ç§ä¸´åºŠåº”ç”¨æˆåŠŸåº”ç”¨ï¼Œå¼•å‘äº†æˆåƒæŠ¤ç†æ–¹å¼çš„è½¬å˜ã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶ä½¿ç”¨æ°¸ç£ä½“ç”Ÿæˆé™æ€ç£åœºB0çš„æ€§èƒ½ç‰¹æ€§ï¼Œå°¤å…¶æ˜¯å®ƒå¯¹ä¸åŒå› ç´ çš„æ•æ„Ÿæ€§ï¼Œæ¢è®¨æ•°å€¼é¢„æœŸä¸å®é™…æµ‹é‡ä¹‹é—´å·®å¼‚çš„åŸå› ã€‚åŒæ—¶åˆ†æä¸åŒæ•°å€¼æ¨¡å‹è¿‘ä¼¼å€¼çš„ä½¿ç”¨åæœï¼Œé‡æ–°å®¡è§†åœ¨å…¶ä»–è®¾è®¡ä¸Šä¸‹æ–‡ä¸­ç»å¸¸ä½¿ç”¨çš„æ¦‚å¿µã€‚è¿™äº›å‡è®¾è™½ç„¶ç®€åŒ–äº†æ•°å€¼æ¨¡å‹å¹¶å¯èƒ½æé«˜äº†å…¶è®¡ç®—æ€§èƒ½ï¼Œä½†ä¹Ÿå½±å“äº†ç»“æœçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æä½åœºMRIæŠ€æœ¯å› å…¶ä¾¿æºæ€§å’Œç¯å¢ƒé€‚åº”æ€§è€Œæ—¥ç›Šæ™®åŠï¼Œä¸ºå„ç§ä¸´åºŠåº”ç”¨æä¾›äº†æˆåŠŸçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä½¿ç”¨æ°¸ç£ä½“ï¼ˆPMsï¼‰ç”Ÿæˆé™æ€ç£åœºB0å¯ä»¥æ˜¾è‘—é™ä½ä½åœºæ‰«æä»ªçš„åˆ¶é€ æˆæœ¬ï¼ŒåŒæ—¶å®ç°æ»¡æ„çš„å‡åŒ€æ€§ã€‚</li>
<li>æ–‡ç« é‡ç‚¹ç ”ç©¶æ°¸ç£ä½“æ€§èƒ½ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹ä¸åŒå› ç´ çš„æ•æ„Ÿæ€§ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†æ•°å€¼é¢„æœŸä¸å®é™…æµ‹é‡ä¹‹é—´å·®å¼‚çš„åŸå› ï¼Œè¿™æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ°¸ç£ä½“çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸åŒæ•°å€¼æ¨¡å‹è¿‘ä¼¼å€¼çš„ä½¿ç”¨ä¼šå½±å“ç»“æœçš„å¯é æ€§ï¼Œè¿™æ˜¯é€šè¿‡å®é™…æµ‹é‡ä¸æ•°å€¼æ¨¡å‹å¯¹æ¯”å¾—å‡ºçš„ç»“è®ºã€‚</li>
<li>ç®€åŒ–æ•°å€¼æ¨¡å‹è™½ç„¶å¯ä»¥æé«˜è®¡ç®—æ€§èƒ½ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ç»“æœçš„åå·®å’Œä¸å‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11762v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11762v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11762v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11762v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11762v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Fully-Open-and-Generalizable-Foundation-Model-for-Ultrasound-Clinical-Applications"><a href="#A-Fully-Open-and-Generalizable-Foundation-Model-for-Ultrasound-Clinical-Applications" class="headerlink" title="A Fully Open and Generalizable Foundation Model for Ultrasound Clinical   Applications"></a>A Fully Open and Generalizable Foundation Model for Ultrasound Clinical   Applications</h2><p><strong>Authors:Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H. L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng</strong></p>
<p>Artificial intelligence (AI) that can effectively learn ultrasound representations by integrating multi-source data holds significant promise for advancing clinical care. However, the scarcity of large labeled datasets in real-world clinical environments and the limited generalizability of task-specific models have hindered the development of generalizable clinical AI models for ultrasound applications. In this study, we present EchoCare, a novel ultrasound foundation model for generalist clinical use, developed via self-supervised learning on our curated, publicly available, large-scale dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images, sourced from over 23 countries across 5 continents and acquired via a diverse range of distinct imaging devices, thus encompassing global cohorts that are multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt off-the-shelf vision foundation model architectures, we introduce a hierarchical classifier into EchoCare to enable joint learning of pixel-level and representation-level features, capturing both global anatomical contexts and local ultrasound characteristics. With minimal training, EchoCare outperforms state-of-the-art comparison models across 10 representative ultrasound benchmarks of varying diagnostic difficulties, spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement and report generation. The code and pretrained model are publicly released, rendering EchoCare accessible for fine-tuning and local adaptation, supporting extensibility to additional applications. EchoCare provides a fully open and generalizable foundation model to boost the development of AI technologies for diverse clinical ultrasound applications. </p>
<blockquote>
<p>èƒ½å¤Ÿé€šè¿‡æ•´åˆå¤šæºæ•°æ®æœ‰æ•ˆå­¦ä¹ è¶…å£°è¡¨ç¤ºçš„äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠæŠ¤ç†çš„æ¨è¿›ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸´åºŠç¯å¢ƒä¸­å¤§å‹æ ‡è®°æ•°æ®é›†çš„ç¨€ç¼ºæ€§ä»¥åŠç‰¹å®šä»»åŠ¡æ¨¡å‹çš„æœ‰é™é€šç”¨æ€§ï¼Œé˜»ç¢äº†è¶…å£°åº”ç”¨é€šç”¨ä¸´åºŠäººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EchoCareï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé€šç”¨ä¸´åºŠä½¿ç”¨çš„æ–°å‹è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œå®ƒæ˜¯åŸºäºæˆ‘ä»¬ç²¾é€‰çš„ã€å…¬å¼€å¯ç”¨çš„ã€å¤§è§„æ¨¡æ•°æ®é›†EchoCareDataè¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ è€Œå¼€å‘çš„ã€‚EchoCareDataåŒ…å«æ¥è‡ªäº”å¤§æ´²è¶…è¿‡23ä¸ªå›½å®¶çš„450ä¸‡å¼ è¶…å£°å›¾åƒï¼Œé€šè¿‡å„ç§å„æ ·çš„æˆåƒè®¾å¤‡è·å–ï¼Œæ¶µç›–äº†å¤šä¸­å¿ƒã€å¤šè®¾å¤‡ã€å¤šæ°‘æ—çš„å…¨çƒäººç¾¤ã€‚ä¸åŒäºå…ˆå‰é‡‡ç”¨ç°æˆè§†è§‰åŸºç¡€æ¨¡å‹æ¶æ„çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨EchoCareä¸­å¼•å…¥äº†ä¸€ä¸ªå±‚æ¬¡åˆ†ç±»å™¨ï¼Œä»¥å®ç°åƒç´ çº§å’Œè¡¨ç¤ºçº§ç‰¹å¾çš„è”åˆå­¦ä¹ ï¼Œæ•æ‰å…¨å±€è§£å‰–èƒŒæ™¯å’Œå±€éƒ¨è¶…å£°ç‰¹å¾ã€‚åœ¨å…·æœ‰è¯Šæ–­éš¾åº¦å·®å¼‚çš„åä¸ªä»£è¡¨æ€§è¶…å£°åŸºå‡†æµ‹è¯•ä¸Šï¼ŒEchoCareä»¥æå°‘çš„è®­ç»ƒè¶…è¶Šäº†æœ€å…ˆè¿›çš„å…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†å‰²ã€å™¨å®˜æ£€æµ‹ã€åœ°æ ‡é¢„æµ‹ã€å®šé‡å›å½’ã€æˆåƒå¢å¼ºå’ŒæŠ¥å‘Šç”Ÿæˆç­‰æ–¹é¢ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å‡å·²å…¬å¼€å‘å¸ƒï¼Œä½¿å¾—EchoCareæ˜“äºå¾®è°ƒå¹¶å¯åœ¨æœ¬åœ°è¿›è¡Œé€‚åº”ï¼Œæ”¯æŒæ‰©å±•åˆ°å…¶ä»–åº”ç”¨ã€‚EchoCareæä¾›äº†ä¸€ä¸ªå®Œå…¨å¼€æ”¾å’Œå¯é€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨äº†å„ç§ä¸´åºŠè¶…å£°åº”ç”¨çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11752v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ•´åˆå¤šæºæ•°æ®å­¦ä¹ è¶…å£°è¡¨å¾çš„äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠåŒ»å­¦é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºçœŸå®ä¸´åºŠç¯å¢ƒä¸­å¤§å‹æ ‡è®°æ•°æ®é›†çš„ç¨€ç¼ºæ€§ä»¥åŠç‰¹å®šä»»åŠ¡æ¨¡å‹çš„æœ‰é™é€šç”¨æ€§ï¼Œé™åˆ¶äº†è¶…å£°åº”ç”¨é€šç”¨ä¸´åºŠäººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•ã€‚æœ¬ç ”ç©¶æå‡ºäº†EchoCareï¼Œè¿™æ˜¯ä¸€ç§é¢å‘ä¸´åºŠåŒ»ç”Ÿä½¿ç”¨çš„æ–°å‹è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åœ¨æˆ‘ä»¬ç²¾é€‰çš„ã€å…¬å¼€çš„ã€å¤§è§„æ¨¡æ•°æ®é›†EchoCareDataä¸Šè¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ è€Œå¼€å‘ã€‚EchoCareDataåŒ…å«æ¥è‡ªäº”å¤§æ´²è¶…è¿‡23ä¸ªå›½å®¶çš„450ä¸‡å¼ è¶…å£°å›¾åƒï¼Œå¹¶é€šè¿‡å„ç§æˆåƒè®¾å¤‡è·å–ï¼Œæ¶µç›–äº†å¤šä¸­å¿ƒã€å¤šè®¾å¤‡ã€å¤šæ°‘æ—çš„å…¨çƒé˜Ÿåˆ—ã€‚ä¸å…ˆå‰é‡‡ç”¨ç°æˆçš„è§†è§‰åŸºç¡€æ¨¡å‹æ¶æ„çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬åœ¨EchoCareä¸­å¼•å…¥äº†ä¸€ä¸ªå±‚æ¬¡åˆ†ç±»å™¨ï¼Œä»¥å®ç°å¯¹åƒç´ çº§å’Œè¡¨ç¤ºçº§ç‰¹å¾çš„è”åˆå­¦ä¹ ï¼Œæ•æ‰å…¨å±€è§£å‰–èƒŒæ™¯å’Œå±€éƒ¨è¶…å£°ç‰¹å¾ã€‚ç»è¿‡æœ€å°‘çš„è®­ç»ƒï¼ŒEchoCareåœ¨10ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è¶…å£°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šæœ€æ–°å¯¹æ¯”æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•æ¶µç›–äº†ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†å‰²ã€å™¨å®˜æ£€æµ‹ã€åœ°æ ‡é¢„æµ‹ã€å®šé‡å›å½’ã€å›¾åƒå¢å¼ºå’ŒæŠ¥å‘Šç”Ÿæˆã€‚å…¬å¼€å‘å¸ƒçš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä½¿EchoCareæ˜“äºå¾®è°ƒå¹¶é€‚åº”æœ¬åœ°éœ€æ±‚ï¼Œæ”¯æŒæ‰©å±•åˆ°å…¶ä»–åº”ç”¨ã€‚EchoCareæä¾›äº†ä¸€ä¸ªå®Œå…¨å¼€æ”¾å’Œé€šç”¨çš„åŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨äº†ä¸´åºŠåŒ»å­¦ä¸­å¤šç§è¶…å£°åº”ç”¨çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½é€šè¿‡å­¦ä¹ è¶…å£°è¡¨å¾å¹¶æ•´åˆå¤šæºæ•°æ®åœ¨ä¸´åºŠåŒ»å­¦ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¤§å‹æ ‡è®°æ•°æ®é›†åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„ç¨€ç¼ºæ€§å’Œç‰¹å®šä»»åŠ¡æ¨¡å‹çš„æœ‰é™é€šç”¨æ€§æ˜¯è¶…å£°åº”ç”¨AIæ¨¡å‹å‘å±•çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>EchoCareæ˜¯ä¸€ä¸ªé¢å‘ä¸´åºŠåŒ»ç”Ÿä½¿ç”¨çš„è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œå¯åœ¨å¤šä¸­å¿ƒã€å¤šè®¾å¤‡ã€å¤šæ°‘æ—çš„å…¨çƒé˜Ÿåˆ—ä¸­è¿›è¡Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚</li>
<li>EchoCareé‡‡ç”¨å±‚æ¬¡åˆ†ç±»å™¨è”åˆå­¦ä¹ åƒç´ çº§å’Œè¡¨ç¤ºçº§ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>EchoCareåœ¨å¤šç§è¶…å£°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†å‰²ç­‰ã€‚</li>
<li>EchoCareçš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œæ”¯æŒç»†ç²’åº¦è°ƒæ•´å’Œæœ¬åœ°é€‚åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11752v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11752v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis"><a href="#Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis" class="headerlink" title="Multiple Instance Learning Framework with Masked Hard Instance Mining   for Gigapixel Histopathology Image Analysis"></a>Multiple Instance Learning Framework with Masked Hard Instance Mining   for Gigapixel Histopathology Image Analysis</h2><p><strong>Authors:Wenhao Tang, Sheng Huang, Heng Fang, Fengtao Zhou, Bo Liu, Qingshan Liu</strong></p>
<p>Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/DearCaat/MHIM-MIL">https://github.com/DearCaat/MHIM-MIL</a>. </p>
<blockquote>
<p>å°†ç—…ç†å›¾åƒæ•°å­—åŒ–ä¸ºè¶…é«˜åˆ†è¾¨ç‡çš„å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWhole Slide Images, WSIï¼‰ä¸ºè®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ç”±äºé˜³æ€§ç»„ç»‡åªå è¶…é«˜åˆ†è¾¨ç‡WSIçš„ä¸€å°éƒ¨åˆ†ï¼Œç°æœ‰çš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•é€šå¸¸é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¸“æ³¨äºè¯†åˆ«æ˜¾è‘—å®ä¾‹ã€‚ç„¶è€Œï¼Œè¿™å¯¼è‡´äº†å¯¹å®¹æ˜“åˆ†ç±»çš„å®ä¾‹çš„åå‘ï¼ŒåŒæ—¶å¿½ç•¥äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä¾‹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå›°éš¾æ ·æœ¬å¯¹äºå‡†ç¡®å»ºæ¨¡åˆ¤åˆ«è¾¹ç•Œè‡³å…³é‡è¦ã€‚åœ¨å®ä¾‹å±‚é¢åº”ç”¨è¿™ä¸€ç†å¿µï¼Œæˆ‘ä»¬è¯¦ç»†é˜è¿°äº†ä¸€ç§æ–°å‹çš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ©ç ç¡¬å®ä¾‹æŒ–æ˜ï¼ˆMHIM-MILï¼‰åŠŸèƒ½ï¼Œå®ƒåˆ©ç”¨Siameseç»“æ„ä»¥åŠä¸€è‡´æ€§çº¦æŸæ¥æ¢ç´¢ç¡¬å®ä¾‹ã€‚ä½¿ç”¨ç±»æ„ŸçŸ¥å®ä¾‹æ¦‚ç‡ï¼ŒMHIM-MILåˆ©ç”¨åŠ¨é‡æ•™å¸ˆæ¨¡å‹æ¥æ©ç›–æ˜¾è‘—å®ä¾‹å¹¶éšå«åœ°æŒ–æ˜ç¡¬å®ä¾‹ä»¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚ä¸ºäº†è·å¾—å¤šæ ·ä¸”éå†—ä½™çš„ç¡¬å®ä¾‹ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡éšæœºæ©ç ï¼ŒåŒæ—¶åˆ©ç”¨å…¨å±€å›æ”¶ç½‘ç»œæ¥é™ä½ä¸¢å¤±å…³é”®ç‰¹å¾çš„é£é™©ã€‚æ­¤å¤–ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥æ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼Œè¿™å¯ä»¥è¯†åˆ«å‡ºæ–°çš„ç¡¬å®ä¾‹ä»¥ä¾›åç»­è®­ç»ƒè¿­ä»£ä½¿ç”¨ï¼Œå¹¶ç¨³å®šä¼˜åŒ–ã€‚åœ¨ç™Œç—‡è¯Šæ–­ã€äºšå‹åˆ†ç±»ã€ç”Ÿå­˜åˆ†æä»»åŠ¡å’Œ12ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMHIM-MILåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DearCaat/MHIM-MIL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DearCaat/MHIM-MILä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11526v1">PDF</a> 27 pages, 8 figures</p>
<p><strong>Summary</strong><br>æ•°å­—åŒ–ç—…ç†å›¾åƒä¸ºè®¡ç®—ç—…ç†å­¦æä¾›äº†æ–°çš„é€”å¾„ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šå…³æ³¨é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«é‡è¦å®ä¾‹ï¼Œè€Œå¿½è§†æŒ‘æˆ˜å®ä¾‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¸¦æœ‰æ©ç›–æ€§ç¡¬å®ä¾‹æŒ–æ˜çš„åŸºäºå¤šå®ä¾‹å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æœºåˆ¶æŒ–æ˜éš¾ä»¥åˆ†ç±»çš„å®ä¾‹è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…¨å±€å›æ”¶ç½‘ç»œè·å–å¤šæ ·åŒ–çš„éå†—ä½™ç¡¬å®ä¾‹ï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼Œä»¥æé«˜åç»­è®­ç»ƒè¿­ä»£ä¸­çš„æ€§èƒ½ç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°å­—åŒ–ç—…ç†å›¾åƒä¸ºè®¡ç®—ç—…ç†å­¦å¸¦æ¥æ–°æœºä¼šã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ˜“äºåˆ†ç±»çš„å®ä¾‹ï¼Œä½†æŒ‘æˆ˜å®ä¾‹å¯¹å‡†ç¡®å»ºæ¨¡è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¤šå®ä¾‹å­¦ä¹ çš„æ¡†æ¶MHIM-MILï¼Œç”¨äºæŒ–æ˜éš¾ä»¥åˆ†ç±»çš„å®ä¾‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>MHIM-MILé‡‡ç”¨Siameseç»“æ„å’Œä¸€è‡´æ€§çº¦æŸæ¥æ¢ç´¢éš¾ä»¥åˆ†ç±»çš„å®ä¾‹ã€‚</li>
<li>ä½¿ç”¨ç±»æ„ŸçŸ¥å®ä¾‹æ¦‚ç‡å’ŒåŠ¨é‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œè®­ç»ƒä¼˜åŒ–ã€‚</li>
<li>åˆ©ç”¨å¤§è§„æ¨¡éšæœºæ©ç›–å’Œå…¨å±€å›æ”¶ç½‘ç»œè·å–å¤šæ ·åŒ–çš„éå†—ä½™ç¡¬å®ä¾‹ã€‚</li>
<li>æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11526v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11526v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11526v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ANROT-HELANet-Adverserially-and-Naturally-Robust-Attention-Based-Aggregation-Network-via-The-Hellinger-Distance-for-Few-Shot-Classification"><a href="#ANROT-HELANet-Adverserially-and-Naturally-Robust-Attention-Based-Aggregation-Network-via-The-Hellinger-Distance-for-Few-Shot-Classification" class="headerlink" title="ANROT-HELANet: Adverserially and Naturally Robust Attention-Based   Aggregation Network via The Hellinger Distance for Few-Shot Classification"></a>ANROT-HELANet: Adverserially and Naturally Robust Attention-Based   Aggregation Network via The Hellinger Distance for Few-Shot Classification</h2><p><strong>Authors:Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu N. Duong</strong></p>
<p>Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\epsilon&#x3D;0.30$ and Gaussian noise up to $\sigma&#x3D;0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20% and 1.40% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANetâ€™s combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at <a target="_blank" rel="noopener" href="https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main">https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main</a>. </p>
<blockquote>
<p>å°æ ·å­¦ä¹ ï¼ˆFSLï¼‰æŠ€æœ¯æ¶‰åŠä½¿ç”¨å°‘é‡æ•°æ®æ ·æœ¬è¿›è¡Œæ¨å¹¿å­¦ä¹ ï¼Œå·²ç»å±•ç°å‡ºå¯¹æ™®é€šCNNæ–¹æ³•çš„ä¼˜å¼‚æ€§èƒ½ã€‚è™½ç„¶åŸºäºè´å¶æ–¯ä¼°è®¡çš„æ–¹æ³•ä½¿ç”¨Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦å·²ç»æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»å’Œè‡ªç„¶å™ªå£°çš„å½±å“ã€‚æˆ‘ä»¬å¼•å…¥äº†ANROT-HELANetï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹æŠ—æ€§å’Œè‡ªç„¶é²æ£’çš„Hellingerèšåˆç½‘ç»œï¼Œå®ƒæ˜¾è‘—åœ°æé«˜äº†FSLçš„ç¨³å¥æ€§å’Œæ€§èƒ½çš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸€ç§å¯¹æŠ—æ€§å’Œè‡ªç„¶é²æ£’çš„åŸºäºHellingerè·ç¦»çš„ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œå¯¹é«˜è¾¾$\epsilon&#x3D;0.30$çš„å¯¹æŠ—æ€§æ‰°åŠ¨å’Œé«˜è¾¾$\sigma&#x3D;0.30$çš„é«˜æ–¯å™ªå£°è¡¨ç°å‡ºéŸ§æ€§ã€‚è¯¥ç½‘ç»œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ï¼Œä¾‹å¦‚åœ¨miniImageNetä¸Šçš„1æ¬¡å’Œ5æ¬¡åœºæ™¯åˆ†åˆ«æé«˜äº†1.20ï¼…å’Œ1.40ï¼…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œå®ƒæ¨å¹¿äº†ç”¨äºå˜åˆ†å°æ ·æ¨ç†åœºæ™¯çš„ä½™å¼¦ç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†ä¼˜è¶Šçš„å›¾åƒé‡å»ºè´¨é‡ï¼ŒFIDå¾—åˆ†ä¸º2.75ï¼Œä¼˜äºä¼ ç»Ÿçš„VAEï¼ˆ3.43ï¼‰å’ŒWAEï¼ˆ3.38ï¼‰æ–¹æ³•ã€‚åœ¨å››ä¸ªå°æ ·åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®ï¼ŒANROT-HELANetç»“åˆäº†åŸºäºHellingerè·ç¦»çš„ç‰¹å¾èšåˆã€æ³¨æ„åŠ›æœºåˆ¶å’Œæˆ‘ä»¬çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œåœ¨ä¿æŒå¯¹æŠ—æ€§å’Œè‡ªç„¶æ‰°åŠ¨çš„ç¨³å¥æ€§çš„åŒæ—¶ï¼Œåˆ›é€ äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä»“åº“å°†ä½äº<a target="_blank" rel="noopener" href="https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main%E3%80%82">https://github.com/GreedYLearner1146/ANROT-HELANet/tree/mainã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11220v1">PDF</a> Preprint version. The manuscript has been submitted to a journal. All   changes will be transferred to the final version if accepted. Also an   erratum: In Figure 10 and 11, the $\epsilon &#x3D; 0.005$ value should be   $\epsilon &#x3D; 0.05$</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ANROT-HELANetç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„ç¨³å¥æ€§å’Œæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡å®ç°åŸºäºHellingerè·ç¦»çš„é²æ£’æ€§ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œè¯¥ç½‘ç»œå¯¹å¯¹æŠ—æ€§æ‰°åŠ¨å’Œè‡ªç„¶å™ªå£°å…·æœ‰å¼ºå¤§çš„æŠ—æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼Œè¯¥ç½‘ç»œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å›¾åƒé‡å»ºè´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œç”¨äºå˜åˆ†å°‘æ ·æœ¬æ¨ç†åœºæ™¯ã€‚æ€»ä½“è€Œè¨€ï¼ŒANROT-HELANetç½‘ç»œé€šè¿‡ç»“åˆHellingerè·ç¦»ç‰¹å¾èšåˆã€æ³¨æ„åŠ›æœºåˆ¶å’Œæ–°å‹æŸå¤±å‡½æ•°ï¼Œåœ¨å››ä¸ªå°‘æ ·æœ¬åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æŠ—æ€§å’Œè‡ªç„¶æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ANROT-HELANetç½‘ç»œåœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­å±•ç¤ºäº†å“è¶Šçš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡å®æ–½åŸºäºHellingerè·ç¦»çš„é²æ£’æ€§ç‰¹å¾ç±»èšåˆæ–¹æ¡ˆï¼Œå¢å¼ºäº†å¯¹æŠ—æ€§å’Œè‡ªç„¶å™ªå£°çš„æŠ—æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒANROT-HELANetå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨miniImageNetæ•°æ®é›†ä¸Šçš„1-shotå’Œ5-shotåœºæ™¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„Hellingerç›¸ä¼¼æ€§å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äºå˜åˆ†å°‘æ ·æœ¬æ¨ç†åœºæ™¯ã€‚</li>
<li>ANROT-HELANetåœ¨å›¾åƒé‡å»ºè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºä¼ ç»Ÿçš„VAEå’ŒWAEæ–¹æ³•ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼ŒANROT-HELANetç»“åˆHellingerè·ç¦»ç‰¹å¾èšåˆã€æ³¨æ„åŠ›æœºåˆ¶å’Œæ–°å‹æŸå¤±å‡½æ•°ï¼Œåœ¨å››ä¸ªå°‘æ ·æœ¬åŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11220v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11220v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.11220v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Automated-Radiology-Report-Generation-Based-on-Topic-Keyword-Semantic-Guidance"><a href="#Automated-Radiology-Report-Generation-Based-on-Topic-Keyword-Semantic-Guidance" class="headerlink" title="Automated Radiology Report Generation Based on Topic-Keyword Semantic   Guidance"></a>Automated Radiology Report Generation Based on Topic-Keyword Semantic   Guidance</h2><p><strong>Authors:Jing Xiao, Hongfei Liu, Ruiqi Dong, Jimin Liu, Haoyong Yu</strong></p>
<p>Automated radiology report generation is essential in clinical practice. However, diagnosing radiological images typically requires physicians 5-10 minutes, resulting in a waste of valuable healthcare resources. Existing studies have not fully leveraged knowledge from historical radiology reports, lacking sufficient and accurate prior information. To address this, we propose a Topic-Keyword Semantic Guidance (TKSG) framework. This framework uses BiomedCLIP to accurately retrieve historical similar cases. Supported by multimodal, TKSG accurately detects topic words (disease classifications) and keywords (common symptoms) in diagnoses. The probabilities of topic terms are aggregated into a topic vector, serving as global information to guide the entire decoding process. Additionally, a semantic-guided attention module is designed to refine local decoding with keyword content, ensuring report accuracy and relevance. Experimental results show that our model achieves excellent performance on both IU X-Ray and MIMIC-CXR datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SCNU203/TKSG">https://github.com/SCNU203/TKSG</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œè‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒåŒ»ç”Ÿé€šå¸¸éœ€è¦èŠ±è´¹5-10åˆ†é’Ÿçš„æ—¶é—´æ¥è¯Šæ–­æ”¾å°„å›¾åƒï¼Œè¿™å¯¼è‡´äº†å®è´µçš„åŒ»ç–—èµ„æºè¢«æµªè´¹ã€‚ç°æœ‰çš„ç ”ç©¶å°šæœªå……åˆ†åˆ©ç”¨å†å²æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„çŸ¥è¯†ï¼Œç¼ºä¹å……è¶³å’Œå‡†ç¡®çš„å…ˆéªŒä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºTopic-Keyword Semantic Guidanceï¼ˆTKSGï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨BiomedCLIPæ¥å‡†ç¡®æ£€ç´¢å†å²ç›¸ä¼¼ç—…ä¾‹ã€‚åœ¨å¤šæ¨¡æ€çš„æ”¯æŒä¸‹ï¼ŒTKSGèƒ½å¤Ÿå‡†ç¡®åœ°æ£€æµ‹è¯Šæ–­ä¸­çš„ä¸»é¢˜è¯ï¼ˆç–¾ç—…åˆ†ç±»ï¼‰å’Œå…³é”®è¯ï¼ˆå¸¸è§ç—‡çŠ¶ï¼‰ã€‚ä¸»é¢˜è¯çš„æ¦‚ç‡è¢«èšé›†åˆ°ä¸€ä¸ªä¸»é¢˜å‘é‡ä¸­ï¼Œä½œä¸ºå…¨å±€ä¿¡æ¯æ¥å¼•å¯¼æ•´ä¸ªè§£ç è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ç»†åŒ–å…³é”®è¯å†…å®¹çš„å±€éƒ¨è§£ç ï¼Œç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨IU X-Rayå’ŒMIMIC-CXRæ•°æ®é›†ä¸Šå‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCNU203/TKSG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SCNU203/TKSGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10873v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆåœ¨ä¸´åºŠå®è·µä¸­è‡³å…³é‡è¦ï¼Œä½†è¯Šæ–­å›¾åƒé€šå¸¸éœ€è¦åŒ»ç”ŸèŠ±è´¹å¤§é‡æ—¶é—´ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜å¹¶å……åˆ†åˆ©ç”¨å†å²æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„çŸ¥è¯†ï¼Œæå‡ºäº†Topic-Keyword Semantic Guidanceï¼ˆTKSGï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨BiomedCLIPå‡†ç¡®æ£€ç´¢ç±»ä¼¼ç—…ä¾‹ï¼Œé€šè¿‡å¤šæ¨¡æ€æ”¯æŒï¼Œå‡†ç¡®æ£€æµ‹è¯Šæ–­ä¸­çš„ä¸»é¢˜è¯å’Œå…³é”®è¯ã€‚è¯¥æ¡†æ¶è¿˜è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æé«˜æŠ¥å‘Šå‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨IU X-Rayå’ŒMIMIC-CXRæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆæœ‰åŠ©äºä¼˜åŒ–ä¸´åºŠå®è·µä¸­çš„èµ„æºåˆ©ç”¨ã€‚</li>
<li>å½“å‰è¯Šæ–­è¿‡ç¨‹å­˜åœ¨æ—¶é—´æµªè´¹é—®é¢˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>Topic-Keyword Semantic Guidanceï¼ˆTKSGï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TKSGæ¡†æ¶åˆ©ç”¨BiomedCLIPæŠ€æœ¯å‡†ç¡®æ£€ç´¢å†å²ç›¸ä¼¼ç—…ä¾‹ã€‚</li>
<li>TKSGé€šè¿‡å¤šæ¨¡æ€æ”¯æŒå‡†ç¡®æ£€æµ‹ä¸»é¢˜è¯å’Œå…³é”®è¯ã€‚</li>
<li>TKSGæ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ä»¥æé«˜æŠ¥å‘Šå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.10873v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.10873v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.10873v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.10873v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Statistical-3D-Stomach-Shape-Model-for-Anatomical-Analysis"><a href="#A-Statistical-3D-Stomach-Shape-Model-for-Anatomical-Analysis" class="headerlink" title="A Statistical 3D Stomach Shape Model for Anatomical Analysis"></a>A Statistical 3D Stomach Shape Model for Anatomical Analysis</h2><p><strong>Authors:Erez Posner, Ore Shtalrid, Oded Erell, Daniel Noy, Moshe Bouhnik</strong></p>
<p>Realistic and parameterized 3D models of human anatomy have become invaluable in research, diagnostics, and surgical planning. However, the development of detailed models for internal organs, such as the stomach, has been limited by data availability and methodological challenges. In this paper, we propose a novel pipeline for the generation of synthetic 3D stomach models, enabling the creation of anatomically diverse morphologies informed by established studies on stomach shape variability. Using this pipeline, we construct a dataset of synthetic stomachs. Building on this dataset, we develop a 3D statistical shape model of the stomach, trained to capture natural anatomical variability in a low-dimensional shape space. The model is further refined using CT meshes derived from publicly available datasets through a semi-supervised alignment process, enhancing its ability to generalize to unseen anatomical variations. We evaluated the model on a held-out test set of real stomach CT scans, demonstrating robust generalization and fit accuracy. We make the statistical shape model along with the synthetic dataset publicly available on GitLab: <a target="_blank" rel="noopener" href="https://gitlab.com/Erez.Posner/stomach_pytorch">https://gitlab.com/Erez.Posner/stomach_pytorch</a> to facilitate further research. This work introduces the first statistical 3D shape model of the stomach, with applications ranging from surgical simulation and pre-operative planning to medical education and computational modeling. By combining synthetic data generation, parametric modeling, and real-world validation, our approach represents a significant advancement in organ modeling and opens new possibilities for personalized healthcare solutions. </p>
<blockquote>
<p>çœŸå®ä¸”å‚æ•°åŒ–çš„ä¸‰ç»´äººä½“è§£å‰–æ¨¡å‹åœ¨ç ”ç©¶ã€è¯Šæ–­å’Œæ‰‹æœ¯è§„åˆ’ä¸­å…·æœ‰æ— å¯ä¼°é‡çš„ä»·å€¼ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®å¯ç”¨æ€§å’Œæ–¹æ³•ä¸Šçš„æŒ‘æˆ˜ï¼Œé’ˆå¯¹å†…éƒ¨å™¨å®˜ï¼ˆå¦‚èƒƒï¼‰çš„è¯¦ç»†æ¨¡å‹å¼€å‘å—åˆ°äº†é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆåˆæˆä¸‰ç»´èƒƒæ¨¡å‹çš„æ–°æµç¨‹ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿæ ¹æ®å·²æœ‰çš„å…³äºèƒƒå½¢çŠ¶å˜å¼‚çš„ç ”ç©¶åˆ›å»ºå½¢æ€å„å¼‚çš„è§£å‰–ç»“æ„ã€‚ä½¿ç”¨è¯¥æµç¨‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆæˆèƒƒæ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰ç»´èƒƒç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼Œé€šè¿‡è®­ç»ƒä»¥åœ¨ä½ç»´å½¢çŠ¶ç©ºé—´ä¸­æ•æ‰è‡ªç„¶çš„è§£å‰–å˜å¼‚ã€‚è¯¥æ¨¡å‹è¿›ä¸€æ­¥ä½¿ç”¨é€šè¿‡åŠç›‘ç£å¯¹é½è¿‡ç¨‹ä»å…¬å¼€æ•°æ®é›†ä¸­å¾—å‡ºçš„CTç½‘æ ¼è¿›è¡Œç»†åŒ–ï¼Œæé«˜äº†å…¶å¯¹æœªè§è§£å‰–å˜å¼‚çš„æ¦‚æ‹¬èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ç»„çœŸå®çš„èƒƒCTæ‰«ææµ‹è¯•é›†ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹ï¼Œè¯æ˜äº†å…¶ç¨³å¥çš„æ¦‚æ‹¬èƒ½åŠ›å’Œæ‹Ÿåˆç²¾åº¦ã€‚æˆ‘ä»¬å°†ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ä»¥åŠåˆæˆæ•°æ®é›†åœ¨GitLabä¸Šå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://gitlab.com/Erez.Posner/stomach_pytorch%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E6%8E%A8%E5%87%BA%E4%BA%86%E9%A6%96%E4%B8%AA%E8%83%83%E7%9A%84%E4%B8%89%E7%BB%B4%E7%BB%9F%E8%AE%A1%E5%BD%A2%E7%8A%B6%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%85%B6%E5%BA%94%E7%94%A8%E8%8C%83%E5%9B%B4%E5%8C%85%E6%8B%AC%E6%89%8B%E6%9C%AF%E6%A8%A1%E6%8B%9F%E5%92%8C%E6%9C%AF%E5%89%8D%E8%A7%84%E5%88%92%E5%88%B0%E5%8C%BB%E5%AD%A6%E6%95%99%E8%82%B2%E5%92%8C%E8%AE%A1%E7%AE%97%E5%BB%BA%E6%A8%A1%E3%80%82%E9%80%9A%E8%BF%87%E7%BB%93%E5%90%88%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E3%80%81%E5%8F%82%E6%95%B0%E5%BB%BA%E6%A8%A1%E5%92%8C%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%E9%AA%8C%E8%AF%81%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%E4%BB%A3%E8%A1%A8%E4%BA%86%E5%99%A8%E5%AE%98%E5%BB%BA%E6%A8%A1%E6%96%B9%E9%9D%A2%E7%9A%84%E4%B8%80%E9%A1%B9%E9%87%8D%E5%A4%A7%E8%BF%9B%E5%B1%95%EF%BC%8C%E5%B9%B6%E4%B8%BA%E4%B8%AA%E6%80%A7%E5%8C%96%E5%8C%BB%E7%96%97%E4%BF%9D%E5%81%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%8F%90%E4%BE%9B%E4%BA%86%E6%96%B0%E7%9A%84%E5%8F%AF%E8%83%BD%E6%80%A7%E3%80%82">https://gitlab.com/Erez.Posner/stomach_pytorchï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚è¿™é¡¹å·¥ä½œæ¨å‡ºäº†é¦–ä¸ªèƒƒçš„ä¸‰ç»´ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼Œå…¶åº”ç”¨èŒƒå›´åŒ…æ‹¬æ‰‹æœ¯æ¨¡æ‹Ÿå’Œæœ¯å‰è§„åˆ’åˆ°åŒ»å­¦æ•™è‚²å’Œè®¡ç®—å»ºæ¨¡ã€‚é€šè¿‡ç»“åˆåˆæˆæ•°æ®ç”Ÿæˆã€å‚æ•°å»ºæ¨¡å’Œç°å®ä¸–ç•ŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»£è¡¨äº†å™¨å®˜å»ºæ¨¡æ–¹é¢çš„ä¸€é¡¹é‡å¤§è¿›å±•ï¼Œå¹¶ä¸ºä¸ªæ€§åŒ–åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06464v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆåˆæˆ3Dèƒƒæ¨¡å‹çš„æ–°å‹ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿåˆ›å»ºå½¢æ€å„å¼‚çš„èƒƒæ¨¡å‹ï¼Œä¾æ®çš„æ˜¯å…³äºèƒƒå½¢çŠ¶å˜åŒ–çš„ç ”ç©¶æ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåˆæˆèƒƒæ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†ä¸€ä¸ª3Dç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰è‡ªç„¶è§£å‰–ç»“æ„çš„ä½ç»´å˜åŒ–ã€‚è¯¥æ¨¡å‹é€šè¿‡åŠç›‘ç£å¯¹é½è¿‡ç¨‹ä½¿ç”¨CTç½‘æ ¼è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæé«˜äº†å¯¹æœªè§è§£å‰–å˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹å·²åœ¨çœŸå®èƒƒCTæ‰«æçš„æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œæ‹Ÿåˆç²¾åº¦ã€‚è¯¥ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹åŠåˆæˆæ•°æ®é›†å·²å…¬å¼€åœ¨GitLabä¸Šä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚è¿™ä¸€å·¥ä½œé¦–æ¬¡å¼•å…¥äº†èƒƒçš„3Dç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼Œå¯åº”ç”¨äºæ‰‹æœ¯æ¨¡æ‹Ÿã€æœ¯å‰è§„åˆ’ã€åŒ»å­¦æ•™è‚²å’Œè®¡ç®—å»ºæ¨¡ç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç”Ÿæˆåˆæˆ3Dèƒƒæ¨¡å‹çš„æ–°å‹ç®¡é“ï¼Œèƒ½å¤Ÿåˆ›å»ºå½¢æ€å„å¼‚çš„èƒƒæ¨¡å‹ã€‚</li>
<li>ä¾æ®å…³äºèƒƒå½¢çŠ¶å˜åŒ–çš„ç ”ç©¶æ•°æ®æ„å»ºåˆæˆèƒƒæ•°æ®é›†ã€‚</li>
<li>å¼€å‘äº†3Dç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ï¼Œæ•æ‰è‡ªç„¶è§£å‰–ç»“æ„çš„ä½ç»´å˜åŒ–ã€‚</li>
<li>é€šè¿‡åŠç›‘ç£å¯¹é½è¿‡ç¨‹ä½¿ç”¨CTç½‘æ ¼ä¼˜åŒ–æ¨¡å‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®èƒƒCTæ‰«ææµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ‹Ÿåˆç²¾åº¦ã€‚</li>
<li>ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹åŠåˆæˆæ•°æ®é›†å·²å…¬å¼€åœ¨GitLabä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.06464v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2509.06464v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Comparing-Conditional-Diffusion-Models-for-Synthesizing-Contrast-Enhanced-Breast-MRI-from-Pre-Contrast-Images"><a href="#Comparing-Conditional-Diffusion-Models-for-Synthesizing-Contrast-Enhanced-Breast-MRI-from-Pre-Contrast-Images" class="headerlink" title="Comparing Conditional Diffusion Models for Synthesizing   Contrast-Enhanced Breast MRI from Pre-Contrast Images"></a>Comparing Conditional Diffusion Models for Synthesizing   Contrast-Enhanced Breast MRI from Pre-Contrast Images</h2><p><strong>Authors:Sebastian Ibarra, Javier del Riego, Alessandro Catanese, Julian Cuba, Julian Cardona, Nataly Leon, Jonathan Infante, Karim Lekadir, Oliver Diaz, Richard Osuala</strong></p>
<p>Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at <a target="_blank" rel="noopener" href="https://github.com/sebastibar/conditional-diffusion-breast-MRI">https://github.com/sebastibar/conditional-diffusion-breast-MRI</a>. </p>
<blockquote>
<p>åŠ¨æ€å¢å¼ºï¼ˆDCEï¼‰MRIå¯¹ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå…¶å¯¹é€ å½±å‰‚çš„ä¾èµ–å¼•å‘äº†å®‰å…¨æ€§é—®é¢˜ã€ç¦å¿Œç—‡ã€æˆæœ¬å¢åŠ å’Œå·¥ä½œæµç¨‹å¤æ‚åŒ–ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„DCE-MRIåˆæˆæ–¹æ³•ã€‚åœ¨å•ä¹³è…ºå’Œå…¨ä¹³è…ºç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ä»‹ç»ã€è¯„ä¼°å’Œæ¯”è¾ƒäº†æ€»å…±22ç§ç”Ÿæˆæ¨¡å‹å˜ä½“ã€‚ä¸ºæé«˜ç—…ç¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œæ˜ç¡®çš„è‚¿ç˜¤åˆ†å‰²æ©è†œæ¡ä»¶ã€‚ä½¿ç”¨å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œä¸ç›¸åº”çš„é¢„é€ å½±åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å‘ç°åŸºäºå‡æ³•å›¾åƒçš„æ¨¡å‹åœ¨äº”ç§äº’è¡¥è¯„ä¼°æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºåŸºäºåé€ å½±çš„æ¨¡å‹ã€‚é™¤äº†å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œè¯„ä¼°å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œäº†å•ç‹¬è¯„ä¼°ï¼Œå…¶ä¸­è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥å‡èƒ½æé«˜è¯„ä¼°æŒ‡æ ‡ã€‚åè€…æ˜¾è‘—æé«˜äº†å®šæ€§ç»“æœæ•æ‰é€ å½±å‰‚æ‘„å–çš„æ•ˆæœï¼Œå°½ç®¡å‡è®¾å­˜åœ¨è‚¿ç˜¤å®šä½è¾“å…¥ï¼Œä½†åœ¨ç­›æŸ¥ç¯å¢ƒä¸­æ— æ³•ä¿è¯å…¶å¯ç”¨æ€§ã€‚æ¶‰åŠä¸¤åæ”¾å°„å­¦å®¶å’Œå››ä½MRIæŠ€æœ¯ä¸“å®¶çš„è¯»è€…ç ”ç©¶è¯å®äº†åˆæˆå›¾åƒçš„é«˜åº¦é€¼çœŸæ€§ï¼Œè¡¨æ˜ç”Ÿæˆé€ å½±å¢å¼ºçš„æ½œåœ¨ä¸´åºŠä»·å€¼æ­£æ—¥ç›Šæ˜¾ç°ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/sebastibar/conditional-diffusion-breast-MRI%E5%88%86%E4%BA%AB%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BA%93%E3%80%82">https://github.com/sebastibar/conditional-diffusion-breast-MRIåˆ†äº«æˆ‘ä»¬çš„ä»£ç åº“ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13776v2">PDF</a> 13 pages, 5 figures, submitted and accepted to MICCAI Deepbreath   workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰åœ¨ä¹³è…ºç™Œè¯Šæ–­å’Œæ²»ç–—ä¸­çš„é‡è¦æ€§ï¼Œä½†ç”±äºå…¶ä¾èµ–é€ å½±å‰‚è€Œå¸¦æ¥å®‰å…¨é¡¾è™‘ã€ç¦å¿Œç—‡ã€æˆæœ¬å¢åŠ å’Œå·¥ä½œæµç¨‹å¤æ‚ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶ä¸‹çš„é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„DCE-MRIåˆæˆæ–¹æ³•ï¼Œå¹¶åœ¨å•ä¹³ä¸å…¨ä¹³ç¯å¢ƒä¸‹è¯„ä¼°å’Œæ¯”è¾ƒäº†æ€»è®¡22ç§ç”Ÿæˆæ¨¡å‹å˜ä½“ã€‚ä¸ºæé«˜ç—…ç¶ä¿çœŸåº¦ï¼Œç ”ç©¶å¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œæ˜ç¡®çš„è‚¿ç˜¤åˆ†å‰²æ©è†œæ¡ä»¶ã€‚åœ¨å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºå‡æ³•å›¾åƒçš„æ¨¡å‹åœ¨äº”ç§äº’è¡¥è¯„ä¼°æŒ‡æ ‡ä¸Šä¸€è‡´ä¼˜äºåŸºäºåé€ å½±çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œäº†å•ç‹¬è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©è†œè¾“å…¥æ”¹å–„äº†è¯„ä¼°æŒ‡æ ‡ã€‚å°½ç®¡éœ€è¦å‡è®¾æœ‰è‚¿ç˜¤å®šä½è¾“å…¥ï¼Œè¿™åœ¨ç­›æŸ¥ç¯å¢ƒä¸­å¯èƒ½æ— æ³•ä¿è¯ï¼Œä½†åŒ…å«è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±çš„æ¨¡å‹åœ¨æ•æ‰é€ å½±å‰‚æ‘„å–æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚é€šè¿‡æ¶‰åŠä¸¤åæ”¾å°„ç§‘åŒ»ç”Ÿå’Œå››åç£å…±æŒ¯æˆåƒæŠ€æœ¯ä¸“å®¶çš„è¯»è€…ç ”ç©¶è¯å®äº†åˆæˆå›¾åƒçš„é«˜é€¼çœŸåº¦ï¼Œè¡¨æ˜ç”Ÿæˆå‹é€ å½±å¢å¼ºå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCE-MRIåœ¨ä¹³è…ºç™Œè¯Šæ–­ä¸æ²»ç–—ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†ä¾èµ–é€ å½±å‰‚å¸¦æ¥è¯¸å¤šé—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºé¢„é€ å½±æ¡ä»¶çš„é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹æ¥åˆæˆDCE-MRIã€‚</li>
<li>åœ¨å•ä¹³ä¸å…¨ä¹³ç¯å¢ƒä¸‹è¯„ä¼°å’Œæ¯”è¾ƒäº†å¤šç§ç”Ÿæˆæ¨¡å‹å˜ä½“ã€‚</li>
<li>å¼•å…¥è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°å’Œè‚¿ç˜¤åˆ†å‰²æ©è†œæ¡ä»¶ä»¥æé«˜ç—…ç¶ä¿çœŸåº¦ã€‚</li>
<li>åŸºäºå‡æ³•å›¾åƒçš„æ¨¡å‹åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºåŸºäºåé€ å½±çš„æ¨¡å‹ã€‚</li>
<li>è¯»è€…ç ”ç©¶è¯å®äº†åˆæˆå›¾åƒçš„é«˜é€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2508.13776v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2508.13776v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2508.13776v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2508.13776v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Human-Centered-Approach-to-Identifying-Promises-Risks-Challenges-of-Text-to-Image-Generative-AI-in-Radiology"><a href="#A-Human-Centered-Approach-to-Identifying-Promises-Risks-Challenges-of-Text-to-Image-Generative-AI-in-Radiology" class="headerlink" title="A Human-Centered Approach to Identifying Promises, Risks, &amp; Challenges   of Text-to-Image Generative AI in Radiology"></a>A Human-Centered Approach to Identifying Promises, Risks, &amp; Challenges   of Text-to-Image Generative AI in Radiology</h2><p><strong>Authors:Katelyn Morrison, Arpit Mathur, Aidan Bradshaw, Tom Wartmann, Steven Lundi, Afrooz Zandifar, Weichang Dai, Kayhan Batmanghelich, Motahhare Eslami, Adam Perer</strong></p>
<p>As text-to-image generative models rapidly improve, AI researchers are making significant advances in developing domain-specific models capable of generating complex medical imagery from text prompts. Despite this, these technical advancements have overlooked whether and how medical professionals would benefit from and use text-to-image generative AI (GenAI) in practice. By developing domain-specific GenAI without involving stakeholders, we risk the potential of building models that are either not useful or even more harmful than helpful. In this paper, we adopt a human-centered approach to responsible model development by involving stakeholders in evaluating and reflecting on the promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through exploratory model prompting activities, we uncover the perspectives of medical students, radiology trainees, and radiologists on the role that text-to-CT Scan GenAI can play across medical education, training, and practice. This human-centered approach additionally enabled us to surface technical challenges and domain-specific risks of generating synthetic medical images. We conclude by reflecting on the implications of medical text-to-image GenAI. </p>
<blockquote>
<p>éšç€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è¿…é€Ÿæ”¹è¿›ï¼Œäººå·¥æ™ºèƒ½ç ”ç©¶äººå‘˜åœ¨å¼€å‘èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚åŒ»ç–—å›¾åƒçš„é¢†åŸŸç‰¹å®šæ¨¡å‹æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™äº›æŠ€æœ¯è¿›å±•å¿½ç•¥äº†åŒ»ç–—ä¸“ä¸šäººå£«åœ¨å®è·µä¸­æ˜¯å¦ä¼šä»¥åŠå¦‚ä½•å—ç›Šäºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆäººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ã€‚å¦‚æœæ²¡æœ‰æ¶‰åŠåˆ©ç›Šç›¸å…³è€…ï¼Œå¼€å‘é¢†åŸŸç‰¹å®šçš„GenAIå¯èƒ½ä¼šå¸¦æ¥æ„å»ºæ— ç”¨ç”šè‡³æœ‰å®³æ¨¡å‹çš„é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è®©åˆ©ç›Šç›¸å…³è€…å‚ä¸è¯„ä¼°å’Œåæ€æ–°å‹æ–‡æœ¬åˆ°CTæ‰«æGenAIæ¨¡å‹çš„æ½œåŠ›ã€é£é™©å’ŒæŒ‘æˆ˜ï¼Œé‡‡ç”¨ä»¥äººä¸ºæœ¬çš„æ–¹æ³•æ¥è¿›è¡Œè´Ÿè´£ä»»çš„æ¨¡å‹å¼€å‘ã€‚é€šè¿‡æ¢ç´¢æ€§æ¨¡å‹æç¤ºæ´»åŠ¨ï¼Œæˆ‘ä»¬äº†è§£äº†åŒ»å­¦ç”Ÿã€æ”¾å°„å­¦å®ä¹ è€…å’Œæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹æ–‡æœ¬åˆ°CTæ‰«æGenAIåœ¨åŒ»å­¦æ•™è‚²ã€åŸ¹è®­å’Œå®è·µä¸­æ‰€èµ·ä½œç”¨çš„ä¸åŒè§‚ç‚¹ã€‚è¿™ç§ä»¥äººä¸ºæœ¬çš„æ–¹æ³•è¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿå‘ç°ç”ŸæˆåˆæˆåŒ»ç–—å›¾åƒçš„æŠ€æœ¯æŒ‘æˆ˜å’Œé¢†åŸŸç‰¹å®šé£é™©ã€‚æœ€åï¼Œæˆ‘ä»¬åæ€äº†åŒ»ç–—æ–‡æœ¬åˆ°å›¾åƒGenAIçš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16207v2">PDF</a> 10 pages of main content, Appendix attached after references,   accepted to AAAI&#x2F;ACM AIES 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦è®¨è®ºäº†åŒ»å­¦å›¾åƒç”Ÿæˆæ¨¡å‹çš„å‘å±•æƒ…å†µï¼ŒæŒ‡å‡ºå°½ç®¡æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†ç¼ºä¹å¯¹åŒ»ç–—ä¸“ä¸šäººå£«å¦‚ä½•ä½¿ç”¨è¿™äº›æŠ€æœ¯çš„è€ƒè™‘ã€‚å› æ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨ä»¥äººä¸ºæœ¬çš„æ–¹æ³•ï¼Œé€šè¿‡æ¶‰åŠåˆ©ç›Šç›¸å…³è€…è¯„ä¼°å’Œåæ€æ–‡æœ¬åˆ°CTæ‰«æç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‰æ™¯ã€é£é™©å’ŒæŒ‘æˆ˜ã€‚é€šè¿‡æ¢ç´¢æ€§æ¨¡å‹æç¤ºæ´»åŠ¨ï¼Œæ­ç¤ºäº†åŒ»å­¦ç”Ÿã€æ”¾å°„å­¦å®ä¹ è€…å’Œæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹åŒ»å­¦æ•™è‚²ã€åŸ¹è®­å’Œå®è·µä¸­è¯¥æŠ€æœ¯çš„ä½œç”¨çš„çœ‹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç ”ç©¶è€…åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆæ¨¡å‹ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½ç”±æ–‡æœ¬æç¤ºç”Ÿæˆå¤æ‚çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>æŠ€æœ¯å‘å±•å¿½è§†äº†åŒ»ç–—ä¸“ä¸šäººå£«å¯¹æ­¤ç±»æŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå—ç›Šæƒ…å†µã€‚</li>
<li>åŒ»å­¦æ•™è‚²ã€åŸ¹è®­å’Œå®è·µä¸­æ¶‰åŠåˆ©ç›Šç›¸å…³è€…çš„è¯„ä¼°ä¸åé¦ˆå¯¹äºå‘å±•æ–‡æœ¬åˆ°CTæ‰«æç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>é‡‡ç”¨ä»¥äººä¸ºæœ¬çš„æ–¹æ³•æœ‰åŠ©äºæ­ç¤ºåŒ»å­¦å›¾åƒç”ŸæˆæŠ€æœ¯çš„æ½œåœ¨é£é™©å’ŒæŒ‘æˆ˜ã€‚</li>
<li>åŒ»ç–—ä¸“ä¸šäººå£«å¯¹æ–‡æœ¬åˆ°CTæ‰«æç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„çœ‹æ³•æœ‰åŠ©äºæŒ‡å¯¼æŠ€æœ¯å‘å±•æ–¹å‘å’Œæ”¹è¿›ã€‚</li>
<li>åœ¨å¼€å‘æ­¤ç±»æŠ€æœ¯æ—¶ï¼Œéœ€å…³æ³¨æŠ€æœ¯æŒ‘æˆ˜å’ŒåŒ»å­¦é¢†åŸŸçš„ç‰¹å®šé£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2507.16207v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2507.16207v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2507.16207v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2507.16207v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OSDM-MReg-Multimodal-Image-Registration-based-One-Step-Diffusion-Model"><a href="#OSDM-MReg-Multimodal-Image-Registration-based-One-Step-Diffusion-Model" class="headerlink" title="OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model"></a>OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model</h2><p><strong>Authors:Xiaochen Wei, Weiwei Guo, Wenxian Yu, Feiming Wei, Dongying Li</strong></p>
<p>Multimodal remote sensing image registration aligns images from different sensors for data fusion and analysis. However, existing methods often struggle to extract modality-invariant features when faced with large nonlinear radiometric differences, such as those between SAR and optical images. To address these challenges, we propose OSDM-MReg, a novel multimodal image registration framework that bridges the modality gap through image-to-image translation. Specifically, we introduce a one-step unaligned target-guided conditional diffusion model (UTGOS-CDM) to translate source and target images into a unified representation domain. Unlike traditional conditional DDPM that require hundreds of iterative steps for inference, our model incorporates a novel inverse translation objective during training to enable direct prediction of the translated image in a single step at test time, significantly accelerating the registration process. After translation, we design a multimodal multiscale registration network (MM-Reg) that extracts and fuses both unimodal and translated multimodal images using the proposed multimodal fusion strategy, enhancing the robustness and precision of alignment across scales and modalities. Extensive experiments on the OSdataset demonstrate that OSDM-MReg achieves superior registration accuracy compared to state-of-the-art methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†æ—¨åœ¨å®ç°å¯¹ä¸åŒä¼ æ„Ÿå™¨å›¾åƒçš„èåˆä¸åˆ†æã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹å¦‚SARä¸å…‰å­¦å›¾åƒé—´å­˜åœ¨è¾ƒå¤§éçº¿æ€§è¾å°„å·®å¼‚æ—¶ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥æå–æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OSDM-MRegï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¤šæ¨¡æ€å›¾åƒé…å‡†æ¡†æ¶ï¼Œå®ƒé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥å¼¥åˆä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€æ­¥å¼æœªå¯¹é½ç›®æ ‡å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆUTGOS-CDMï¼‰ï¼Œå°†æºå›¾åƒå’Œç›®æ ‡å›¾åƒç¿»è¯‘åˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¡¨ç¤ºåŸŸä¸­ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦æ•°ç™¾æ¬¡è¿­ä»£æ¨ç†çš„æ¡ä»¶DDPMä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†æ–°é¢–çš„åå‘ç¿»è¯‘ç›®æ ‡ï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶èƒ½å¤Ÿåœ¨å•æ­¥å†…ç›´æ¥é¢„æµ‹ç¿»è¯‘åçš„å›¾åƒï¼Œæ˜¾è‘—åŠ é€Ÿäº†é…å‡†è¿‡ç¨‹ã€‚ç¿»è¯‘åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šå°ºåº¦é…å‡†ç½‘ç»œï¼ˆMM-Regï¼‰ï¼Œå®ƒåˆ©ç”¨æå‡ºçš„å¤šæ¨¡æ€èåˆç­–ç•¥æ¥æå–å’Œèåˆå•æ¨¡æ€å’Œç¿»è¯‘åçš„å¤šæ¨¡æ€å›¾åƒï¼Œæé«˜äº†è·¨å°ºåº¦å’Œè·¨æ¨¡æ€å¯¹é½çš„é²æ£’æ€§å’Œç²¾åº¦ã€‚åœ¨OSdatasetä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOSDM-MRegçš„é…å‡†ç²¾åº¦ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06027v2">PDF</a> This version updates our previous submission. After rerunning the   experiments, we found that the proposed high-frequency perceptual loss did   not improve the overall performance of the model. Therefore, we removed this   component, revised the corresponding ablation studies, and updated the   contributions accordingly. This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†æ˜¯å¯¹æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„å›¾åƒè¿›è¡Œèåˆå’Œåˆ†æçš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å¤§èŒƒå›´çš„éçº¿æ€§è¾å°„å·®å¼‚æ—¶ï¼Œæå–æ¨¡æ€ä¸å˜ç‰¹å¾é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†OSDM-MRegæ¡†æ¶ï¼Œé€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¥ç¼©å°æ¨¡æ€å·®å¼‚ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€æ­¥å¼æœªå¯¹é½ç›®æ ‡å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆUTGOS-CDMï¼‰ï¼Œå°†æºå›¾åƒå’Œç›®æ ‡å›¾åƒç¿»è¯‘æˆä¸€ä¸ªç»Ÿä¸€çš„è¡¨ç°åŸŸã€‚ä¸ä¼ ç»Ÿéœ€è¦æ•°ç™¾æ­¥è¿­ä»£çš„æ¡ä»¶DDPMä¸åŒï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†é€†å‘ç¿»è¯‘ç›®æ ‡ï¼Œä½¿å¾—æµ‹è¯•æ—¶çš„ç¿»è¯‘å›¾åƒé¢„æµ‹æ›´åŠ å¿«é€Ÿå’Œç›´æ¥ã€‚ç¿»è¯‘åï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šå°ºåº¦é…å‡†ç½‘ç»œï¼ˆMM-Regï¼‰ï¼Œé€šè¿‡æå‡ºçš„å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œæå–å’Œèåˆå•æ¨¡æ€å’Œç¿»è¯‘åçš„å¤šæ¨¡æ€å›¾åƒï¼Œæé«˜äº†è·¨å°ºåº¦å’Œæ¨¡æ€å¯¹é½çš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚åœ¨OSdatasetä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOSDM-MRegç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„é…å‡†ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒé…å‡†åœ¨æ•°æ®èåˆå’Œåˆ†æä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤§èŒƒå›´éçº¿æ€§è¾å°„å·®å¼‚ä¸‹æå–æ¨¡æ€ä¸å˜ç‰¹å¾æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>OSDM-MRegæ¡†æ¶é€šè¿‡å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ç¼©å°äº†æ¨¡æ€å·®å¼‚ã€‚</li>
<li>UTGOS-CDMæ¨¡å‹å®ç°äº†å¿«é€Ÿç›´æ¥çš„å›¾åƒç¿»è¯‘é¢„æµ‹ã€‚</li>
<li>è®¾è®¡äº†MM-Regç½‘ç»œè¿›è¡Œå¤šæ¨¡æ€å’Œå¤šå°ºåº¦ä¸‹çš„å›¾åƒé…å‡†ã€‚</li>
<li>æå‡ºçš„å¤šæ¨¡æ€èåˆç­–ç•¥æé«˜äº†é…å‡†çš„ç¨³å¥æ€§å’Œç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2504.06027v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction"><a href="#Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction" class="headerlink" title="Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction"></a>Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction</h2><p><strong>Authors:Thomas Sanchez, Vladyslav Zalevskyi, Angeline Mihailov, Gerard MartÃ­-Juan, Elisenda Eixarch, Andras Jakab, Vincent Dunet, MÃ©riam Koob, Guillaume Auzias, Meritxell Bach Cuadra</strong></p>
<p>Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$<em>{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$</em>{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC &#x3D; 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/">https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/</a> . </p>
<blockquote>
<p>è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰å¯¹äºç¥ç»å½±åƒå­¦ç ”ç©¶çš„å¯é æ€§è‡³å…³é‡è¦ï¼Œè¿™ä¸€ç‚¹é•¿æœŸä»¥æ¥å¤‡å—å…³æ³¨ã€‚å¯¹äºèƒå„¿è„‘éƒ¨MRIï¼ˆç£å…±æŒ¯æˆåƒï¼‰è€Œè¨€å°¤å…¶å¦‚æ­¤ï¼Œå› ä¸ºä¸æˆäººæˆåƒç›¸æ¯”ï¼Œèƒå„¿è„‘éƒ¨MRIçš„é‡‡é›†å’Œå›¾åƒå¤„ç†æŠ€æœ¯æ ‡å‡†åŒ–ç¨‹åº¦è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºèƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å¤„ç†æ­¥éª¤ï¼Œå…¶ä¸­å¤šä¸ªåšçš„2Dåˆ‡ç‰‡å †å åœ¨ä¸€èµ·æ³¨å†Œå¹¶ç»„åˆæˆä¸€ä¸ªå•ä¸€ã€å„å‘åŒæ€§å’Œæ— ä¼ªå½±çš„T2åŠ æƒä½“ç§¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æå–äº†è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚è¿™ç§æ–¹æ³•éå¸¸é€‚åˆé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†å°çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨åŸŸå¤–ï¼ˆOODï¼‰ç¯å¢ƒä¸­éªŒè¯äº†FetMRQC_{SR}ï¼Œå¹¶æŠ¥å‘Šäº†é«˜æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œå³ä½¿åœ¨é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†å¤±è´¥çš„æƒ…å†µï¼Œå¹¶è¡¨æ˜è¿™äº›å¤±è´¥å‘ç”Ÿåœ¨45%çš„å›¾åƒä¸­ï¼Œæ˜¯ç”±äºé…ç½®ä¸æ˜ç¡®ï¼Œå¯¼è‡´ä¸“å®¶è¯„åˆ†æœ‰äº‰è®®ã€‚è¿™äº›ç»“æœä»¤äººé¼“èˆï¼Œå¹¶è¯´æ˜äº†éæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚FetMRQC_{SR}ï¼‰å¦‚ä½•é€‚åº”è¿™ç§å¤šé¢é—®é¢˜ã€‚æˆ‘ä»¬çš„å·¥å…·ä»¥åŠç”¨äºç”Ÿæˆã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„æ‰€æœ‰ä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%E5%8D%A0%E6%9C%89%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr&#x2F;æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10156v4">PDF</a> 14 pages, 5 figures; accepted at the 2025 MICCAI Perinatal, Preterm   and Paediatric Image Analysis (PIPPI) Workshop</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡å…³æ³¨èƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ã€‚æå‡ºä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æå–è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†è¾ƒå°çš„é—®é¢˜ã€‚åœ¨è·¨åŸŸè®¾ç½®ä¸­è¿›è¡ŒéªŒè¯ï¼Œè¡¨ç°å‡ºé«˜æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œå³ä½¿é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è°ƒæŸ¥å¤±è´¥æ¡ˆä¾‹å‘ç°ï¼Œ45%çš„å›¾åƒå› æ¨¡ç³Šé…ç½®è€Œå¯¼è‡´ä¸“å®¶è¯„åˆ†æœ‰äº‰è®®ã€‚ç ”ç©¶æˆæœé¼“èˆäººå¿ƒï¼Œå±•ç¤ºäº†éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚FetMRQC_{SR}åœ¨æ­¤å¤šå…ƒåŒ–é—®é¢˜ä¸­çš„é€‚ç”¨æ€§ã€‚ç›¸å…³å·¥å…·å’Œä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä¸“æ³¨äºèƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ï¼Œå¼ºè°ƒå…¶åœ¨ä¿è¯ç¥ç»å½±åƒå­¦ç ”ç©¶å¯é æ€§ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºé¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”é«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†è¾ƒå°çš„åœºæ™¯ã€‚</li>
<li>åœ¨è·¨åŸŸè®¾ç½®ä¸­éªŒè¯äº†FetMRQC_{SR}çš„é«˜æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œæ˜¾ç¤ºå…¶é¢å¯¹æœªçŸ¥æ¥æºæ•°æ®çš„ç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°å¤±è´¥æ¡ˆä¾‹ä¸­çš„45%æ˜¯ç”±äºå›¾åƒæ¨¡ç³Šé…ç½®å¯¼è‡´çš„ä¸“å®¶è¯„åˆ†äº‰è®®ã€‚</li>
<li>ç»“æœè¡¨æ˜éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚FetMRQC_{SR}åœ¨å¤„ç†è¿™ç§å¤šå…ƒåŒ–é—®é¢˜ä¸Šå…·æœ‰é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2503.10156v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2503.10156v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2503.10156v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Accelerating-Low-field-MRI-From-Compressed-Sensing-to-Deep-Learning-Reconstruction-with-CNNs-and-Transformers"><a href="#Accelerating-Low-field-MRI-From-Compressed-Sensing-to-Deep-Learning-Reconstruction-with-CNNs-and-Transformers" class="headerlink" title="Accelerating Low-field MRI: From Compressed Sensing to Deep Learning   Reconstruction with CNNs and Transformers"></a>Accelerating Low-field MRI: From Compressed Sensing to Deep Learning   Reconstruction with CNNs and Transformers</h2><p><strong>Authors:Efrat Shimron, Shanshan Shan, James Grover, Neha Koonjoo, Sheng Shen, Thomas Boele, Annabel J. Sorby-Adams, John E. Kirsch, Matthew S. Rosen, David E. J. Waddington</strong></p>
<p>Portable, low-field Magnetic Resonance Imaging (MRI) scanners are increasingly being deployed in clinical settings. However, key barriers to their widespread use include low signal-to-noise ratio (SNR), generally low image quality, and long scan durations. Hence, methods for accelerating acquisition and boosting image quality are critically important to enable clinically actionable, high-quality imaging in these systems. Despite the role that compressed sensing (CS) and deep learning (DL)-based methods have played in improving image quality for high-field MRI, their adoption for low-field imaging is still in its infancy, and it remains unclear how robust these methods are in low-SNR regimes.   Here, we propose, investigate, and compare four reconstruction approaches: (i) L1-wavelet CS; (ii) a data-driven network; (iii) an unrolled network; and (iv) a Swin Transformer Cascade. We evaluate their performance across a range of SNR values using publicly available datasets and ultra-low field (6.5 mT) MRI data. Our results show that the unrolled network and Swin Transformer cascade outperform CS and data-driven models. While transformer-based models achieve the highest performance at high SNR, unrolled convolution-based networks are more robust in ultra-low SNR settings and often outperform transformers, indicating that simpler DL architectures may be better suited to low-field MRI.   This work highlights both the potential and limitations of advanced reconstruction techniques in low-field MRI and pinpoints effective DL strategies for addressing SNR challenges. </p>
<blockquote>
<p>ä¾¿æºå¼ã€ä½åœºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä»ªåœ¨ä¸´åºŠç¯å¢ƒä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢å¤šã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›ä½¿ç”¨çš„å…³é”®éšœç¢åŒ…æ‹¬ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä½ã€å›¾åƒè´¨é‡æ™®éè¾ƒå·®å’Œæ‰«ææ—¶é—´é•¿ã€‚å› æ­¤ï¼ŒåŠ é€Ÿé‡‡é›†å’Œæé«˜å›¾åƒè´¨é‡çš„æ–¹æ³•å¯¹äºåœ¨è¿™äº›ç³»ç»Ÿä¸­å®ç°ä¸´åºŠå¯æ“ä½œçš„é«˜è´¨é‡æˆåƒè‡³å…³é‡è¦ã€‚å°½ç®¡å‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•åœ¨æ”¹å–„é«˜åœºMRIå›¾åƒè´¨é‡æ–¹é¢å‘æŒ¥äº†ä½œç”¨ï¼Œä½†å®ƒä»¬åœ¨ä½åœºæˆåƒä¸­çš„åº”ç”¨ä»å¤„äºèµ·æ­¥é˜¶æ®µï¼Œè€Œä¸”åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œåœ¨ä½ä¿¡å™ªæ¯”æƒ…å†µä¸‹å…¶ç¨³å¥æ€§ä»æœ‰å¾…æ˜ç¡®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºã€è°ƒæŸ¥å’Œæ¯”è¾ƒäº†å››ç§é‡å»ºæ–¹æ³•ï¼šï¼ˆiï¼‰L1-å°æ³¢CSï¼›ï¼ˆiiï¼‰æ•°æ®é©±åŠ¨ç½‘ç»œï¼›ï¼ˆiiiï¼‰å±•å¼€ç½‘ç»œï¼›ï¼ˆivï¼‰Swin Transformerçº§è”ã€‚æˆ‘ä»¬ä½¿ç”¨å…¬å¼€æ•°æ®é›†å’Œè¶…ä½åœºï¼ˆ6.5 mTï¼‰MRIæ•°æ®ï¼Œåœ¨å„ç§ä¿¡å™ªæ¯”ä¸‹è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå±•å¼€ç½‘ç»œå’ŒSwin Transformerçº§è”ä¼˜äºCSå’Œæ•°æ®é©±åŠ¨æ¨¡å‹ã€‚è™½ç„¶åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨é«˜ä¿¡å™ªæ¯”ä¸‹è¡¨ç°æœ€ä½³ï¼Œä½†åŸºäºå±•å¼€å·ç§¯çš„ç½‘ç»œåœ¨è¶…ä½ä¿¡å™ªæ¯”ç¯å¢ƒä¸­æ›´ç¨³å¥ï¼Œå¹¶ä¸”å¾€å¾€ä¼˜äºå˜å‹å™¨ï¼Œè¿™è¡¨æ˜å¯¹äºä½åœºMRIï¼Œæ›´ç®€å•çš„DLæ¶æ„å¯èƒ½æ›´é€‚åˆã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å…ˆè¿›é‡å»ºæŠ€æœ¯åœ¨ä½åœºMRIä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†è§£å†³ä¿¡å™ªæ¯”æŒ‘æˆ˜çš„æœ‰æ•ˆæ·±åº¦å­¦ä¹ ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06704v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¾¿æºå¼ä½åœºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä»ªåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿¡å·å™ªå£°æ¯”ï¼ˆSNRï¼‰ä½ã€å›¾åƒè´¨é‡æ™®éè¾ƒå·®å’Œæ‰«ææ—¶é—´é•¿ç­‰é—®é¢˜ã€‚ä¸ºäº†æ”¹å–„å›¾åƒè´¨é‡å¹¶åŠ é€Ÿé‡‡é›†ï¼Œæœ¬æ–‡æå‡ºäº†å››ç§é‡å»ºæ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬åœ¨è·¨è¶Šä¸åŒSNRå€¼æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå·ç§¯ç½‘ç»œåœ¨æœªå±•å¼€çš„ç¥ç»ç½‘ç»œå’ŒåŸºäºæ•°æ®çš„æ¨¡å‹ä¸­æœ‰æ›´å¥½çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨è¶…ä½SNRç¯å¢ƒä¸­æ›´åŠ ç¨³å¥ã€‚è¿™è¡¨æ˜åœ¨ä½åœºMRIä¸­æ›´é€‚ç”¨äºæ›´ç®€å•çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†å…ˆè¿›æŠ€æœ¯ç”¨äºä½åœºMRIçš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹SNRæŒ‘æˆ˜çš„æœ‰æ•ˆæ·±åº¦å­¦ä¹ ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä½åœºMRIæ‰«æä»ªé¢ä¸´ä¿¡å·å™ªå£°æ¯”ä½ã€å›¾åƒè´¨é‡å·®å’Œæ‰«ææ—¶é—´é•¿çš„é—®é¢˜ã€‚</li>
<li>åŠ é€Ÿé‡‡é›†å’Œæé«˜å›¾åƒè´¨é‡æ˜¯æ¨åŠ¨ä½åœºMRIä¸´åºŠåº”ç”¨çš„å…³é”®ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†å››ç§é‡å»ºæ–¹æ³•åœ¨ä½åœºMRIä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æœªå±•å¼€çš„å·ç§¯ç½‘ç»œåœ¨è¶…ä½SNRç¯å¢ƒä¸­è¡¨ç°ç¨³å¥ï¼Œæœ‰æ—¶ä¼˜äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ã€‚</li>
<li>ç»“æœæŒ‡å‡ºç®€å•æ·±åº¦å­¦ä¹ æ¶æ„åœ¨ä½åœºMRIä¸­çš„é€‚ç”¨æ€§æ›´ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.06704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2411.06704v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2411.06704v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Bayesian-Unsupervised-Disentanglement-of-Anatomy-and-Geometry-for-Deep-Groupwise-Image-Registration"><a href="#Bayesian-Unsupervised-Disentanglement-of-Anatomy-and-Geometry-for-Deep-Groupwise-Image-Registration" class="headerlink" title="Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep   Groupwise Image Registration"></a>Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep   Groupwise Image Registration</h2><p><strong>Authors:Xinzhe Luo, Xin Wang, Linda Shapiro, Chun Yuan, Jianfeng Feng, Xiahai Zhuang</strong></p>
<p>This article presents a general Bayesian learning framework for multi-modal groupwise image registration. The method builds on probabilistic modelling of the image generative process, where the underlying common anatomy and geometric variations of the observed images are explicitly disentangled as latent variables. Therefore, groupwise image registration is achieved via hierarchical Bayesian inference. We propose a novel hierarchical variational auto-encoding architecture to realise the inference procedure of the latent variables, where the registration parameters can be explicitly estimated in a mathematically interpretable fashion. Remarkably, this new paradigm learns groupwise image registration in an unsupervised closed-loop self-reconstruction process, sparing the burden of designing complex image-based similarity measures. The computationally efficient disentangled network architecture is also inherently scalable and flexible, allowing for groupwise registration on large-scale image groups with variable sizes. Furthermore, the inferred structural representations from multi-modal images via disentanglement learning are capable of capturing the latent anatomy of the observations with visual semantics. Extensive experiments were conducted to validate the proposed framework, including four different datasets from cardiac, brain, and abdominal medical images. The results have demonstrated the superiority of our method over conventional similarity-based approaches in terms of accuracy, efficiency, scalability, and interpretability. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ç¾¤ç»„å›¾åƒæ³¨å†Œçš„é€šç”¨è´å¶æ–¯å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¦‚ç‡æ¨¡å‹ä¸Šï¼Œå…¶ä¸­è§‚å¯Ÿåˆ°çš„å›¾åƒçš„æ½œåœ¨å…±åŒè§£å‰–ç»“æ„å’Œå‡ ä½•å˜åŒ–è¢«æ˜ç¡®åœ°åˆ†ç¦»ä¸ºæ½œåœ¨å˜é‡ã€‚å› æ­¤ï¼Œç¾¤ç»„å›¾åƒæ³¨å†Œæ˜¯é€šè¿‡åˆ†å±‚è´å¶æ–¯æ¨ç†æ¥å®ç°çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚å˜åˆ†è‡ªåŠ¨ç¼–ç æ¶æ„ï¼Œä»¥å®ç°æ½œåœ¨å˜é‡çš„æ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­æ³¨å†Œå‚æ•°å¯ä»¥ä»¥æ•°å­¦ä¸Šå¯è§£é‡Šçš„æ–¹å¼æ˜ç¡®ä¼°è®¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–°èŒƒå¼åœ¨æ— äººç›‘ç£çš„é—­ç¯è‡ªé‡å»ºè¿‡ç¨‹ä¸­å­¦ä¹ ç¾¤ç»„å›¾åƒæ³¨å†Œï¼Œçœå»äº†è®¾è®¡å¤æ‚å›¾åƒç›¸ä¼¼åº¦æŒ‡æ ‡çš„è´Ÿæ‹…ã€‚è®¡ç®—æ•ˆç‡é«˜çš„è§£è€¦ç½‘ç»œæ¶æ„æœ¬è´¨ä¸Šæ˜¯å¯æ‰©å±•å’Œçµæ´»çš„ï¼Œå…è®¸å¯¹å…·æœ‰ä¸åŒå¤§å°çš„å¤§è§„æ¨¡å›¾åƒç»„è¿›è¡Œç¾¤ç»„æ³¨å†Œã€‚æ­¤å¤–ï¼Œé€šè¿‡è§£è€¦å­¦ä¹ ä»å¤šæ¨¡æ€å›¾åƒæ¨æ–­çš„ç»“æ„è¡¨ç¤ºèƒ½å¤Ÿæ•è·è§‚å¯Ÿå¯¹è±¡çš„æ½œåœ¨è§£å‰–ç»“æ„å¹¶å…·æœ‰è§†è§‰è¯­ä¹‰ã€‚ä¸ºäº†éªŒè¯æ‰€æå‡ºæ¡†æ¶ï¼Œè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬æ¥è‡ªå¿ƒè„ã€å¤§è„‘å’Œè…¹éƒ¨åŒ»å­¦å›¾åƒçš„å››ä¸ªä¸åŒæ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„åŸºäºç›¸ä¼¼åº¦çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.02141v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºå¤šæ¨¡æ€ç¾¤ç»„å›¾åƒæ³¨å†Œçš„é€šç”¨è´å¶æ–¯å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¦‚ç‡å»ºæ¨¡ï¼Œå°†è§‚å¯Ÿåˆ°çš„å›¾åƒçš„åº•å±‚å…±åŒç»“æ„å’Œå‡ ä½•å˜åŒ–ä½œä¸ºæ½œåœ¨å˜é‡æ˜¾å¼åˆ†ç¦»ã€‚é€šè¿‡åˆ†å±‚è´å¶æ–¯æ¨æ–­å®ç°ç¾¤ç»„å›¾åƒæ³¨å†Œã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å±‚å˜åˆ†è‡ªåŠ¨ç¼–ç æ¶æ„æ¥å®ç°æ½œåœ¨å˜é‡çš„æ¨æ–­è¿‡ç¨‹ï¼Œå¯ä»¥æ˜ç¡®ä¼°è®¡æ³¨å†Œå‚æ•°ï¼Œå…·æœ‰æ•°å­¦å¯è§£é‡Šæ€§ã€‚è¯¥æ–°èŒƒä¾‹ä»¥æ— ç›‘ç£çš„é—­ç¯è‡ªé‡å»ºè¿‡ç¨‹å­¦ä¹ ç¾¤ç»„å›¾åƒæ³¨å†Œï¼Œæ— éœ€è®¾è®¡å¤æ‚çš„å›¾åƒç›¸ä¼¼æ€§åº¦é‡ã€‚å…¶è®¡ç®—é«˜æ•ˆçš„åˆ†ç¦»ç½‘ç»œæ¶æ„å…·æœ‰å›ºæœ‰å¯ä¼¸ç¼©æ€§å’Œçµæ´»æ€§ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å¯å˜å°ºå¯¸å›¾åƒç»„çš„ç¾¤ç»„æ³¨å†Œã€‚é€šè¿‡è§£è€¦å­¦ä¹ ä»å¤šæ¨¡æ€å›¾åƒæ¨æ–­çš„ç»“æ„è¡¨ç¤ºèƒ½å¤Ÿæ•æ‰è§‚æµ‹çš„æ½œåœ¨ç»“æ„å¹¶å…·æœ‰è§†è§‰è¯­ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯å­¦ä¹ çš„å¤šæ¨¡æ€ç¾¤ç»„å›¾åƒæ³¨å†Œæ¡†æ¶ã€‚</li>
<li>é€šè¿‡æ¦‚ç‡å»ºæ¨¡å›¾åƒç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ†ç¦»åº•å±‚å…±åŒç»“æ„å’Œå‡ ä½•å˜åŒ–ä¸ºæ½œåœ¨å˜é‡ã€‚</li>
<li>ä½¿ç”¨åˆ†å±‚å˜åˆ†è‡ªåŠ¨ç¼–ç æ¶æ„å®ç°æ½œåœ¨å˜é‡çš„æ¨æ–­ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚è´å¶æ–¯æ¨æ–­è¿›è¡Œç¾¤ç»„å›¾åƒæ³¨å†Œã€‚</li>
<li>å­¦ä¹ è¿‡ç¨‹æ˜¯æ— ç›‘ç£çš„é—­ç¯è‡ªé‡å»ºï¼Œæ— éœ€è®¾è®¡å¤æ‚çš„å›¾åƒç›¸ä¼¼æ€§åº¦é‡ã€‚</li>
<li>æ¡†æ¶å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å¯ä¼¸ç¼©æ€§å¼ºã€é€‚ç”¨äºå¤§è§„æ¨¡å¯å˜å°ºå¯¸å›¾åƒç»„çš„ç‰¹ç‚¹ã€‚</li>
<li>é€šè¿‡è§£è€¦å­¦ä¹ ï¼Œèƒ½å¤Ÿä»å¤šæ¨¡æ€å›¾åƒä¸­æ¨æ–­å‡ºå…·æœ‰è§†è§‰è¯­ä¹‰çš„ç»“æ„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.02141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2401.02141v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2401.02141v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2401.02141v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2401.02141v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SRSNetwork-Siamese-Reconstruction-Segmentation-Networks-based-on-Dynamic-Parameter-Convolution"><a href="#SRSNetwork-Siamese-Reconstruction-Segmentation-Networks-based-on-Dynamic-Parameter-Convolution" class="headerlink" title="SRSNetwork: Siamese Reconstruction-Segmentation Networks based on   Dynamic-Parameter Convolution"></a>SRSNetwork: Siamese Reconstruction-Segmentation Networks based on   Dynamic-Parameter Convolution</h2><p><strong>Authors:Bingkun Nian, Fenghe Tang, Jianrui Ding, Jie Yang, Zhonglong Zheng, Shaohua Kevin Zhou, Wei Liu</strong></p>
<p>Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations.Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/fidshu/SRSNet">https://github.com/fidshu/SRSNet</a>. </p>
<blockquote>
<p>åŠ¨æ€å·ç§¯åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå“è¶Šçš„è¡¨å¾èƒ½åŠ›ï¼Œè¿™æ˜¯éå¸¸é‡è¦çš„ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æœ‰é™å’Œæ‹Ÿåˆèƒ½åŠ›æœ‰é™ï¼Œå½“å®ƒåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰å’Œçº¢å¤–å°ç›®æ ‡åˆ†å‰²ï¼ˆIRSTSï¼‰æ—¶ä¼šå¤±è´¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŠ¨æ€å·ç§¯ï¼Œç§°ä¸ºåŠ¨æ€å‚æ•°å·ç§¯ï¼ˆDPConvï¼‰ï¼Œå®ƒæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ‹Ÿåˆèƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨ç¼–ç å™¨æ·±å±‚ç‰¹å¾è¿›è¡Œé‡å»ºä»»åŠ¡ï¼Œç”Ÿæˆé€‚åº”è¾“å…¥å˜åŒ–çš„DPConvå†…æ ¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒåŸºäºé‡å»ºä»»åŠ¡å¾—åˆ°çš„æ·±å±‚ç‰¹å¾æ„å»ºçš„DPConvèƒ½æ˜¾è‘—æé«˜ä¸‹æ¸¸åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¸ç”±é‡å»ºç½‘ç»œç”Ÿæˆçš„DPConvç›¸ç»“åˆçš„åˆ†å‰²ç½‘ç»œç§°ä¸ºSiameseé‡å»º-åˆ†å‰²ç½‘ç»œï¼ˆSRSï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬äº”ä¸ªåŒ»å­¦æ•°æ®é›†å’Œä¸¤ä¸ªçº¢å¤–æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæœ€è¿‘æå‡ºçš„æ–¹æ³•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœªè§æ¨¡æ€ä¸‹çš„é›¶æ ·æœ¬åˆ†å‰²è¯æ˜äº†DPConvçš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/fidshu/SRSNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fidshu/SRSNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01741v2">PDF</a> Accepted by IEEE Transactions on Image Processing (IEEE-TIP)</p>
<p><strong>Summary</strong></p>
<p>åŠ¨æ€å·ç§¯åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„è¡¨å¾èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œçº¢å¤–å°ç›®æ ‡åˆ†å‰²ä¸­å› æ•°æ®æœ‰é™å’Œæ‹Ÿåˆèƒ½åŠ›æœ‰é™è€Œè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„åŠ¨æ€å·ç§¯æ–¹æ³•â€”â€”åŠ¨æ€å‚æ•°å·ç§¯ï¼ˆDPConvï¼‰ï¼Œå…·æœ‰æ›´å¥½çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨ç¼–ç å™¨æ·±å±‚ç‰¹å¾è¿›è¡Œé‡å»ºä»»åŠ¡ï¼Œç”Ÿæˆé€‚åº”è¾“å…¥å˜åŒ–çš„DPConvå†…æ ¸ã€‚é€šè¿‡åŸºäºæ·±å±‚ç‰¹å¾ç”Ÿæˆçš„DPConvï¼Œä¸‹æ¸¸åˆ†å‰²æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºè¿‘æœŸæå‡ºçš„æ–¹æ³•ï¼Œå¹¶åœ¨æœªè§æ¨¡æ€ä¸‹å®ç°é›¶åˆ†å‰²ï¼Œå±•ç°å‡ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€å·ç§¯åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„è¡¨å¾èƒ½åŠ›ã€‚</li>
<li>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œçº¢å¤–å°ç›®æ ‡åˆ†å‰²é¢†åŸŸï¼ŒåŠ¨æ€å·ç§¯å› æ•°æ®æœ‰é™å’Œæ‹Ÿåˆèƒ½åŠ›æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„åŠ¨æ€å‚æ•°å·ç§¯ï¼ˆDPConvï¼‰å…·æœ‰æ›´å¥½çš„æ‹Ÿåˆèƒ½åŠ›ã€‚</li>
<li>DPConvèƒ½åˆ©ç”¨ç¼–ç å™¨æ·±å±‚ç‰¹å¾è¿›è¡Œé‡å»ºä»»åŠ¡ï¼Œç”Ÿæˆé€‚åº”è¾“å…¥å˜åŒ–çš„DPConvå†…æ ¸ã€‚</li>
<li>DPConvæ˜¾è‘—æé«˜äº†ä¸‹æ¸¸åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDPConvåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–è¿‘æœŸæ–¹æ³•ã€‚</li>
<li>DPConvåœ¨æœªè§æ¨¡æ€ä¸‹å®ç°é›¶åˆ†å‰²ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2312.01741v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2312.01741v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2312.01741v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2312.01741v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_åŒ»å­¦å›¾åƒ/2312.01741v2/page_4_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_TTS/2509.11425v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  FuseCodec Semantic-Contextual Fusion and Supervision for Neural Codecs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_ç‰™é½¿ä¿®å¤/2509.12069v1/page_0_0.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-17  U-Mamba2 Scaling State Space Models for Dental Anatomy Segmentation in   CBCT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
