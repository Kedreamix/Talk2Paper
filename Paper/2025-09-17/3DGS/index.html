<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-17  A Controllable 3D Deepfake Generation Framework with Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-17-更新"><a href="#2025-09-17-更新" class="headerlink" title="2025-09-17 更新"></a>2025-09-17 更新</h1><h2 id="A-Controllable-3D-Deepfake-Generation-Framework-with-Gaussian-Splatting"><a href="#A-Controllable-3D-Deepfake-Generation-Framework-with-Gaussian-Splatting" class="headerlink" title="A Controllable 3D Deepfake Generation Framework with Gaussian Splatting"></a>A Controllable 3D Deepfake Generation Framework with Gaussian Splatting</h2><p><strong>Authors:Wending Liu, Siyun Liang, Huy H. Nguyen, Isao Echizen</strong></p>
<p>We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks. </p>
<blockquote>
<p>我们提出了一种基于3D高斯拼贴的新型3D深度伪造生成框架，该框架能够在完全可控的3D空间中进行逼真的身份保留面部交换和重建。与传统的2D深度伪造方法相比，我们的方法克服了几何不一致性和对新型视角的有限泛化能力的问题。它将参数化头部模型与动态高斯表示相结合，支持多视角一致渲染、精确的表情控制和无缝背景集成。为了解决点基表示中的编辑挑战，我们明确地将头部和背景高斯分离，并使用预先训练的2D指导来优化各视角的面部区域。我们还引入了一个修复模块，以提高极端姿势和表情下的视觉一致性。在NeRSemble上的实验和额外的评估视频表明，我们的方法在身份保留、姿势和表情一致性方面达到了最先进水平的2D方法的性能水平，同时在多视角渲染质量和3D一致性方面显著优于它们。我们的方法填补了三维建模和深度伪造合成之间的空白，为场景感知、可控和沉浸式视觉伪造提供了新的方向，揭示了新兴的高斯拼贴技术在操纵攻击中的潜在威胁。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11624v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究提出了一种基于3D高斯拼贴的新型深度伪造生成框架，可实现全可控的3D空间中真实、身份保留的面部交换和重演。相较于传统的2D深度伪造方法存在的几何不一致性和对新型视角的有限泛化能力，本研究结合参数化头部模型和动态高斯表示，支持多视角一致渲染、精确的表情控制和无缝背景集成。针对点云表示中的编辑挑战，本研究明确区分头部和背景高斯，并使用预训练的2D指导来优化面部区域的跨视角表现。此外，引入修复模块以在极端姿势和表情下增强视觉一致性。实验结果表明，本研究的方法在身份保留、姿势和表情一致性方面达到与最新2D方法相当的性能，同时在多视角渲染质量和3D一致性方面显著优于它们。本研究的方法填补了3D建模和深度伪造合成之间的鸿沟，为场景感知、可控和沉浸式视觉伪造开启了新的方向，揭示了新兴的3D高斯拼贴技术可能用于操纵攻击的风险。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于3D高斯拼贴的新型深度伪造生成框架，用于实现全可控的3D空间中的面部交换和重演。</li>
<li>结合参数化头部模型和动态高斯表示，支持多视角一致渲染、精确的表情控制及无缝背景集成。</li>
<li>通过区分头部和背景高斯，优化面部区域的跨视角表现，并引入修复模块增强视觉一致性。</li>
<li>方法在身份保留、姿势和表情一致性方面表现优异，尤其在多视角渲染质量和3D一致性方面优于现有方法。</li>
<li>填补了3D建模和深度伪造合成之间的鸿沟，为场景感知、可控和沉浸式视觉伪造提供了新方向。</li>
<li>提出的方法具有潜在风险，可能被用于操纵攻击等不正当用途。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11624">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11624v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11624v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11624v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11624v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Gaussian-Plus-SDF-SLAM-High-fidelity-3D-Reconstruction-at-150-fps"><a href="#Gaussian-Plus-SDF-SLAM-High-fidelity-3D-Reconstruction-at-150-fps" class="headerlink" title="Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps"></a>Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps</h2><p><strong>Authors:Zhexi Peng, Kun Zhou, Tianjia Shao</strong></p>
<p>While recent Gaussian-based SLAM methods achieve photorealistic reconstruction from RGB-D data, their computational performance remains a critical bottleneck. State-of-the-art techniques operate at less than 20 fps, significantly lagging behind geometry-centric approaches like KinectFusion (hundreds of fps). This limitation stems from the heavy computational burden: modeling scenes requires numerous Gaussians and complex iterative optimization to fit RGB-D data, where insufficient Gaussian counts or optimization iterations cause severe quality degradation. To address this, we propose a Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field (SDF) for smooth geometry and appearance with 3D Gaussians to capture underrepresented details. The SDF is efficiently constructed via RGB-D fusion (as in geometry-centric methods), while Gaussians undergo iterative optimization. Our representation enables drastic Gaussian reduction (50% fewer) by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization (75% fewer iterations) through targeted appearance refinement. Building upon this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time 3D reconstruction system achieving over 150 fps on real-world Azure Kinect sequences – delivering an order-of-magnitude speedup over state-of-the-art techniques while maintaining comparable reconstruction quality. We will release the source code and data to facilitate future research. </p>
<blockquote>
<p>虽然最近的基于高斯SLAM方法可以从RGB-D数据中实现逼真的重建，但其计算性能仍然是一个关键的瓶颈。目前的技术运行速率低于每秒20帧，远远落后于以几何为中心的KinectFusion方法（数百帧每秒）。这种限制源于巨大的计算负担：建模场景需要大量的高斯和复杂的迭代优化以适应RGB-D数据，高斯数量不足或优化迭代不足会导致严重质量下降。为了解决这一问题，我们提出了一种混合高斯-SDF表示法，结合了彩色有符号距离场（SDF）用于平滑几何形状和外观与三维高斯捕捉细节。SDF通过RGB-D融合（如几何中心方法）有效地构建，而高斯则进行迭代优化。我们的表示法通过避免全场景高斯建模实现了高斯数量大幅减少（减少50%），并且通过有针对性的外观改进实现了高斯优化效率提高（迭代次数减少75%）。在此基础上，我们开发了GPS-SLAM（高斯加SDF SLAM），这是一个实时三维重建系统，在真实世界的Azure Kinect序列上实现超过每秒150帧的速率，相对于目前技术实现了数量级的加速，同时保持了相当不错的重建质量。我们将发布源代码和数据以促进未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11574v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于高斯的SLAM方法能够实现从RGB-D数据的真实感重建，但其计算性能仍是关键瓶颈。最新技术运行速率低于每秒20帧，远远落后于如KinectFusion等以几何为中心的方案（数百帧每秒）。本文提出了一种高斯-SDF混合表示法，结合了彩色化有向距离场（SDF）用于平滑几何和外观与三维高斯捕捉细节。通过RGB-D融合高效地构建SDF，而高斯则通过迭代优化处理。此表示法实现了高斯数量减少一半并有效减少迭代优化次数，提升了重建速度并减少了计算负担。基于该表示法开发了GPS-SLAM系统，实现了实时三维重建，在真实世界Azure Kinect序列上运行速度超过每秒150帧，大幅提高了运行速度并保持相当重建质量。我们将公开源代码和数据以推动未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>最新基于高斯的SLAM方法虽然能实现真实感重建，但计算性能仍是瓶颈，运行速率低于每秒20帧。</li>
<li>提出了一种新的高斯-SDF混合表示法，结合了彩色化SDF与三维高斯捕捉细节，提高几何和外观的平滑度。</li>
<li>通过RGB-D融合高效地构建SDF，而高斯则通过迭代优化处理，减少计算负担。</li>
<li>此混合表示法实现了高斯数量减少一半并有效减少迭代优化次数。</li>
<li>基于该混合表示法开发了GPS-SLAM系统，在真实世界Azure Kinect序列上运行速度超过每秒150帧，大幅提升运行速度并保持重建质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11574">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11574v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11574v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11574v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11574v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ROSGS-Relightable-Outdoor-Scenes-With-Gaussian-Splatting"><a href="#ROSGS-Relightable-Outdoor-Scenes-With-Gaussian-Splatting" class="headerlink" title="ROSGS: Relightable Outdoor Scenes With Gaussian Splatting"></a>ROSGS: Relightable Outdoor Scenes With Gaussian Splatting</h2><p><strong>Authors:Lianjun Liao, Chunhui Zhang, Tong Wu, Henglei Lv, Bailin Deng, Lin Gao</strong></p>
<p>Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene’s geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene’s texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency. </p>
<blockquote>
<p>户外采集的图像数据通常呈现无边界场景和无约束、多变的照明条件，将其分解为几何、反射和照明具有挑战性。近期的研究工作主要集中在利用神经辐射场（NeRF）或3D高斯喷涂（3DGS）表示法来实现这种分解，但仍受到两个主要限制：与NeRF神经网络相关的高计算开销，以及使用低频光照表示形式，这通常导致渲染效率低下和重照明精度不佳。我们提出ROSGS，这是一个两阶段管道，旨在利用高斯喷涂表示法有效地重建可重新照明的户外场景。ROSGS通过利用单眼法线先验知识，首先使用紧凑的2D高斯喷涂（2DGS）表示法重建场景的几何结构，提供高效且准确的几何基础。在此基础上，ROSGS通过混合光照模型分解场景纹理和照明。该模型通过采用球形高斯函数来捕捉阳光的方向性、高频成分，同时通过学习球面谐波系数来模拟剩余的低频天空光，有效地表示了典型的外光照明。定量指标和定性比较均表明，ROSGS在重新照明户外场景方面达到了最新技术水平，并突出了其提供卓越的重照明精度和渲染效率的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于户外图像数据难以分解为几何、反射和照明等部分的问题，最新工作通过Neural Radiance Fields（NeRF）或3D Gaussian Splatting（3DGS）进行尝试但仍面临挑战。本研究提出ROSGS方法，通过两阶段管道利用单眼先验进行高效户外场景重建并利用高斯混合模型实现重光照。初步使用紧凑的二维高斯混合模型重建场景几何，再基于此几何结构通过混合照明模型分解场景纹理和照明。该模型采用球面高斯函数捕捉日光的高频方向成分，并用球面谐波系数学习剩余的低频天空光模型。此方法实现了高水平的户外场景重光照效果和渲染效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>户外图像数据分解面临挑战，包括无界场景、无约束的照明条件等。</li>
<li>最新方法如NeRF和3DGS在解决此问题上虽有所进展，但仍面临计算量大和低频照明表示等问题。</li>
<li>ROSGS是一种高效重建可重新光照户外场景的两阶段管道。</li>
<li>ROSGS使用单眼先验进行几何重建，采用紧凑的二维高斯混合模型为基础。</li>
<li>基于重建的几何结构，ROSGS通过混合照明模型分解场景纹理和照明。</li>
<li>ROSGS采用球面高斯函数捕捉日光的高频方向成分。</li>
<li>ROSGS方法在重光照和渲染效率方面达到了最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11275v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11275v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11275v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11275v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPHERE-Semantic-PHysical-Engaged-REpresentation-for-3D-Semantic-Scene-Completion"><a href="#SPHERE-Semantic-PHysical-Engaged-REpresentation-for-3D-Semantic-Scene-Completion" class="headerlink" title="SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene   Completion"></a>SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene   Completion</h2><p><strong>Authors:Zhiwen Yang, Yuxin Peng</strong></p>
<p>Camera-based 3D Semantic Scene Completion (SSC) is a critical task in autonomous driving systems, assessing voxel-level geometry and semantics for holistic scene perception. While existing voxel-based and plane-based SSC methods have achieved considerable progress, they struggle to capture physical regularities for realistic geometric details. On the other hand, neural reconstruction methods like NeRF and 3DGS demonstrate superior physical awareness, but suffer from high computational cost and slow convergence when handling large-scale, complex autonomous driving scenes, leading to inferior semantic accuracy. To address these issues, we propose the Semantic-PHysical Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel and Gaussian representations for joint exploitation of semantic and physical information. First, the Semantic-guided Gaussian Initialization (SGI) module leverages dual-branch 3D scene representations to locate focal voxels as anchors to guide efficient Gaussian initialization. Then, the Physical-aware Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to model physical-aware contextual details and promote semantic-geometry consistency through focal distribution alignment, generating SSC results with realistic details. Extensive experiments and analyses on the popular SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of SPHERE. The code is available at <a target="_blank" rel="noopener" href="https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025">https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025</a>. </p>
<blockquote>
<p>基于相机的三维语义场景补全（SSC）是自动驾驶系统中的一项关键任务，它评估体素级几何和语义信息以实现整体场景感知。虽然现有的基于体素和基于平面的SSC方法已经取得了很大的进展，但它们在捕捉物理规律以呈现逼真的几何细节方面遇到了困难。另一方面，像NeRF和3DGS这样的神经重建方法表现出卓越的物理感知能力，但在处理大规模、复杂的自动驾驶场景时，其计算成本高昂且收敛缓慢，导致语义准确性较低。为了解决这些问题，我们提出了用于基于相机的SSC的语义物理参与表示（SPHERE），它结合了体素和高斯表示，以联合利用语义和物理信息。首先，语义引导高斯初始化（SGI）模块利用双分支三维场景表示来确定关键体素作为锚点来引导高效的高斯初始化。然后，物理感知谐波增强（PHE）模块结合语义球面谐波来模拟物理感知的上下文细节，并通过焦点分布对齐来促进语义几何一致性，生成具有逼真细节的SSC结果。在流行的SemanticKITTI和SSCBench-KITTI-360基准测试上的大量实验和分析验证了SPHERE的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11171v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种用于相机基底的3D语义场景完成（SSC）的Semantic-PHysical Engaged REpresentation（SPHERE）方法。该方法结合体素和高斯表示，共同利用语义和物理信息。通过Semantic-guided Gaussian Initialization（SGI）模块定位关键体素作为锚点引导高效高斯初始化，并利用Physical-aware Harmonics Enhancement（PHE）模块结合语义球面谐波来建模具有物理意识的上下文细节，生成具有真实细节的SSC结果。在流行的SemanticKITTI和SSCBench-KITTI-360基准测试中验证了SPHERE的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>相机基底的3D语义场景完成（SSC）是自主驾驶系统中的一项重要任务，涉及对整个场景进行像素级别的几何和语义分析。</li>
<li>现有方法如基于体素和基于平面的SSC方法在捕捉真实几何细节的物理规律方面存在困难。</li>
<li>神经重建方法如NeRF和3DGS具有出色的物理感知能力，但在处理大规模、复杂的自主驾驶场景时计算成本高昂、收敛缓慢，导致语义精度不高。</li>
<li>提出的SPHERE方法结合体素和高斯表示，旨在解决上述问题，同时利用语义和物理信息。</li>
<li>SGI模块通过双分支3D场景表示定位关键体素作为锚点，引导高效高斯初始化。</li>
<li>PHE模块结合语义球面谐波来建模物理感知的上下文细节，并通过焦点分布对齐促进语义-几何一致性，生成具有真实细节的SSC结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11171">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11171v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11171v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11171v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11171v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11171v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AD-GS-Alternating-Densification-for-Sparse-Input-3D-Gaussian-Splatting"><a href="#AD-GS-Alternating-Densification-for-Sparse-Input-3D-Gaussian-Splatting" class="headerlink" title="AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting"></a>AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting</h2><p><strong>Authors:Gurutva Patle, Nilay Girgaonkar, Nagabhushan Somraj, Rajiv Soundararajan</strong></p>
<p>3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods. </p>
<blockquote>
<p>3D高斯混合技术（3DGS）在实时新颖视角合成方面取得了令人印象深刻的结果。然而，在稀疏视角设置下，它经常面临挑战，产生诸如浮动、几何不准确和由于观察有限而造成的过度拟合等不想要的人工制品。我们发现，一个关键的贡献因素是未控制的稠密化，其中在没有指导的情况下快速添加高斯基本体可能会损害几何结构并产生人工制品。我们提出了AD-GS，这是一种新型交替稠密化框架，交替进行高稠密化和低稠密化阶段。在高稠密化期间，模型进行激烈稠密化，随后通过基于光度损失的训练来捕捉场景的细节。低稠密化主要涉及高斯体的激进透明度修剪，然后通过伪视图一致性和边缘感知深度平滑对其进行几何正则化。这种交替方法通过仔细控制模型容量增长，在逐步优化场景表示的同时，有助于减少过度拟合。在具有挑战性的数据集上的大量实验表明，与现有方法相比，AD-GS在渲染质量和几何一致性方面显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11003v1">PDF</a> SIGGRAPH Asia 2025</p>
<p><strong>Summary</strong></p>
<p>实时三维场景重建中，高斯平滑技术（3DGS）展现出优秀的性能，但在稀疏视图环境下会出现漂浮物、几何失真和过拟合等问题。为解决这些问题，我们提出了交替密度化框架AD-GS，通过交替进行高密度化和低密度化处理，结合光度损失训练捕捉精细场景细节和正则化几何信息的方式实现。此方式提高了场景表示质量并减少过拟合现象。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS在实时新视角合成中表现出优异结果。</li>
<li>在稀疏视图环境下，现有方法可能产生浮体、几何失真和过拟合等不期望的伪影。</li>
<li>AD-GS是一种交替密度化框架，旨在解决上述问题。</li>
<li>高密度化阶段主要通过积极添加高斯基元来捕获精细场景细节。</li>
<li>低密度化阶段涉及修剪冗余基元和正则化几何信息。</li>
<li>通过交替高密度化和低密度化处理，AD-GS能够控制模型容量增长并逐步提高场景表示质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11003v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11003v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11003v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11003v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.11003v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="T2Bs-Text-to-Character-Blendshapes-via-Video-Generation"><a href="#T2Bs-Text-to-Character-Blendshapes-via-Video-Generation" class="headerlink" title="T2Bs: Text-to-Character Blendshapes via Video Generation"></a>T2Bs: Text-to-Character Blendshapes via Video Generation</h2><p><strong>Authors:Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang</strong></p>
<p>We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques. </p>
<blockquote>
<p>我们提出了T2Bs框架，它通过结合静态文本到3D生成和视频扩散技术，从文本生成高质量的可动画角色头部可变形模型。文本到3D模型可以生成详细的静态几何结构，但缺乏运动合成，而视频扩散模型则会产生具有时间和多视角几何不一致的运动。T2Bs通过利用可变形的3D高斯贴片技术，将静态3D资产与视频输出对齐，从而弥补了这一差距。通过用静态几何结构约束运动，并采用视图相关变形MLP，T2Bs（i）在准确性和表现力方面优于现有的4D生成方法，同时减少了视频伪影和视角不一致性；（ii）重建了平滑、连贯、完全注册的3D几何结构，旨在构建具有多样化和现实面部运动的可变形模型。这能够合成表达力强、可动画的角色头部，超越了当前的4D生成技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了T2Bs框架，它通过结合静态文本到三维生成技术与视频扩散技术，实现了从文本生成高质量、可动画的人物头部可变形模型。该框架弥补了文本到三维模型在动作合成上的不足，以及视频扩散模型在时间和多视角几何不一致的问题。T2Bs利用可变形三维高斯喷绘技术，将静态三维资产与视频输出对齐。通过约束动作与静态几何，并依赖视角变形MLP，T2Bs在准确性和表现力上超越了现有4D生成方法，同时减少了视频伪影和视角不一致问题，重建了平滑、连贯、全面注册的三维几何结构，为构建具有多样化和现实感面部动作的可变形模型提供了可扩展的解决方案。这实现了合成具有表现力、可动画的人物头部，超越了当前的4D生成技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Bs框架结合了静态文本到三维生成与视频扩散技术。</li>
<li>文本到三维模型在动作合成上有不足，视频扩散模型存在时间和多视角几何不一致问题。</li>
<li>T2Bs利用可变形三维高斯喷绘技术对齐静态三维资产与视频输出。</li>
<li>通过约束动作与静态几何，T2Bs提高了准确性和表现力。</li>
<li>T2Bs减少了视频伪影和视角不一致问题。</li>
<li>T2Bs重建了平滑、连贯、全面注册的三维几何结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10678">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.10678v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.10678v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.10678v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Real-time-Photorealistic-Mapping-for-Situational-Awareness-in-Robot-Teleoperation"><a href="#Real-time-Photorealistic-Mapping-for-Situational-Awareness-in-Robot-Teleoperation" class="headerlink" title="Real-time Photorealistic Mapping for Situational Awareness in Robot   Teleoperation"></a>Real-time Photorealistic Mapping for Situational Awareness in Robot   Teleoperation</h2><p><strong>Authors:Ian Page, Pierre Susbielle, Olivier Aycard, Pierre-Brice Wieber</strong></p>
<p>Achieving efficient remote teleoperation is particularly challenging in unknown environments, as the teleoperator must rapidly build an understanding of the site’s layout. Online 3D mapping is a proven strategy to tackle this challenge, as it enables the teleoperator to progressively explore the site from multiple perspectives. However, traditional online map-based teleoperation systems struggle to generate visually accurate 3D maps in real-time due to the high computational cost involved, leading to poor teleoperation performances. In this work, we propose a solution to improve teleoperation efficiency in unknown environments. Our approach proposes a novel, modular and efficient GPU-based integration between recent advancement in gaussian splatting SLAM and existing online map-based teleoperation systems. We compare the proposed solution against state-of-the-art teleoperation systems and validate its performances through real-world experiments using an aerial vehicle. The results show significant improvements in decision-making speed and more accurate interaction with the environment, leading to greater teleoperation efficiency. In doing so, our system enhances remote teleoperation by seamlessly integrating photorealistic mapping generation with real-time performances, enabling effective teleoperation in unfamiliar environments. </p>
<blockquote>
<p>在实现未知环境的远程遥控操作时，达到高效率是一项特别具有挑战性的任务，因为遥控操作员必须迅速了解站点的布局。在线三维映射是应对这一挑战的一种经过验证的策略，因为它能够让操作员从多个角度逐步探索站点。然而，传统的基于在线地图的遥控系统由于涉及的计算成本较高，难以在实时生成视觉准确的三维地图，从而导致遥控操作性能不佳。在这项工作中，我们提出了一种提高未知环境中遥控操作效率的方法。我们的方法提出了一种新颖、模块化且高效的基于GPU的集成方案，该方案结合了高斯Splatting SLAM的最新进展和现有的基于在线地图的遥控系统。我们将所提解决方案与最先进的遥控操作系统进行了比较，并通过使用航空器的真实世界实验验证了其性能。结果表明，决策制定速度显著提高，与环境的交互更为准确，从而提高了遥控操作的效率。通过这种方式，我们的系统通过无缝集成逼真的映射生成与实时性能，有效提高了在陌生环境中的遥控操作效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06433v2">PDF</a> </p>
<p><strong>Summary</strong><br>     在未知环境中实现高效的远程遥控操作具有挑战性，需迅速理解场地布局。在线3D地图是一种应对挑战的有效策略，它能使操作者从不同角度逐步探索场地。然而，传统的基于地图的远程遥控操作系统难以实时生成视觉准确的3D地图，导致操作性能不佳。本研究提出了一种解决方案，该方案将最新发展的高斯映射SLAM技术与现有在线地图遥控操作系统结合，形成高效模块化、基于GPU的集成系统。对比现有先进遥控操作系统并通过无人机进行真实实验验证，结果显示该方案提高了决策速度和与环境的交互准确性，从而提高了操作效率。此系统通过无缝集成真实地图生成与实时性能，增强了在不熟悉环境中的有效遥控操作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在未知环境中实现高效远程遥控操作需要快速理解场地布局。</li>
<li>在线3D地图是应对这一挑战的有效策略，能助力操作者逐步探索场地。</li>
<li>传统基于地图的远程遥控操作系统难以实时生成视觉准确的3D地图。</li>
<li>本研究提出了一种结合高斯映射SLAM技术与现有在线地图遥控操作系统的解决方案。</li>
<li>该方案实现了模块化、基于GPU的高效集成系统。</li>
<li>对比实验显示，该方案提高了决策速度和与环境的交互准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06433">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2509.06433v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Occlusion-Aware-Temporally-Consistent-Amodal-Completion-for-3D-Human-Object-Interaction-Reconstruction"><a href="#Occlusion-Aware-Temporally-Consistent-Amodal-Completion-for-3D-Human-Object-Interaction-Reconstruction" class="headerlink" title="Occlusion-Aware Temporally Consistent Amodal Completion for 3D   Human-Object Interaction Reconstruction"></a>Occlusion-Aware Temporally Consistent Amodal Completion for 3D   Human-Object Interaction Reconstruction</h2><p><strong>Authors:Hyungjun Doh, Dong In Lee, Seunggeun Chi, Pin-Hao Huang, Kwonjoon Lee, Sangpil Kim, Karthik Ramani</strong></p>
<p>We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques. </p>
<blockquote>
<p>我们提出了一种新的从单目视频中重建动态人机交互的框架，该框架克服了与遮挡和时间不一致相关的挑战。传统的3D重建方法通常假设物体是静态的或被摄主体完全可见，当这些假设不成立时，特别是在发生相互遮挡的场景中，会导致性能下降。为了解决这一问题，我们的框架利用模态完成法来推断部分遮挡区域的完整结构。与在单个帧上运行的传统方法不同，我们的方法结合了时间上下文，强制视频序列之间的连贯性，以逐步改进和稳定重建。这种无模板的策略适应各种条件，不依赖预先定义的模型，显著提高了动态场景中细节的恢复。我们使用具有挑战性的单目视频对基于高斯的三维平铺验证了我们的方法，与现有技术相比，在处理遮挡和保持时间稳定性方面表现出更高的精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08137v3">PDF</a> ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种从单目视频中重建动态人机交互的新型框架，该框架克服了与遮挡和时序不一致相关的挑战。传统3D重建方法通常假设物体静态或动态主体完全可见，当这些假设不成立时，特别是在相互遮挡的场景中，性能会下降。为解决这一问题，我们的框架利用模态完成法推断部分遮挡区域的整体结构。与其他只在单个帧上操作的方法不同，我们的方法结合了时间上下文，强制视频序列之间的连贯性，以逐步精细和调整重建。这种无模板的策略适应各种条件，而不依赖于预定义模型，显著提高了动态场景中细节的恢复能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新型框架能从单目视频中重建动态人机交互。</li>
<li>框架克服了遮挡和时序不一致的挑战。</li>
<li>传统3D重建方法在假设不成立时性能下降。</li>
<li>利用模态完成法推断部分遮挡区域的整体结构。</li>
<li>与只在单个帧上操作的方法不同，该框架结合了时间上下文。</li>
<li>框架强制视频序列之间的连贯性，以逐步精细和调整重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08137">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2507.08137v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2507.08137v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2507.08137v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2507.08137v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AnySplat-Feed-forward-3D-Gaussian-Splatting-from-Unconstrained-Views"><a href="#AnySplat-Feed-forward-3D-Gaussian-Splatting-from-Unconstrained-Views" class="headerlink" title="AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views"></a>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</h2><p><strong>Authors:Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai</strong></p>
<p>We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: <a target="_blank" rel="noopener" href="https://city-super.github.io/anysplat/">https://city-super.github.io/anysplat/</a> </p>
<blockquote>
<p>我们介绍了AnySplat，这是一个从未校准的图像集合中合成新型视角的前馈网络。与传统的需要已知相机姿态和场景优化的神经渲染管道，或最近在密集视角的计算重量下而屈服的前馈方法相比，我们的模型可以在一次操作中预测所有内容。单次前向传递会产生一组编码场景几何和外观的3D高斯基本体，以及每个输入图像的相应相机内部和外部参数。这种统一的设计可以轻松扩展到随意捕获的多视角数据集，无需任何姿态注释。在广泛的零样本评估中，AnySplat在稀疏和密集视角场景中均能达到姿态感知基准的质量，同时超越了现有的无姿态方法。此外，与基于优化的神经场相比，它极大地减少了渲染延迟，使得在不受约束的捕获设置中实时合成新型视角成为可能。项目页面：<a target="_blank" rel="noopener" href="https://city-super.github.io/anysplat/">https://city-super.github.io/anysplat/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23716v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://city-super.github.io/anysplat/">https://city-super.github.io/anysplat/</a></p>
<p><strong>Summary</strong></p>
<p>AnySplat是一种基于前馈网络的新型视图合成方法，适用于从未校准的图像集合中进行合成。相较于传统神经渲染管线需要已知相机姿态和场景优化，以及最新因密集视图计算重量过大而表现不佳的前馈方法，AnySplat模型可一次性预测所有内容。单次前馈传递就能产生一组编码场景几何形状和外观的3D高斯原始数据，以及对应输入图像的相机内部参数和外部参数。这种统一设计能够轻松扩展到随意拍摄的多视角数据集，无需任何姿态标注。在零视角评估中，AnySplat在稀疏和密集视图场景中达到了姿态感知基准测试的质量水平，同时超越了现有的无姿态方法。此外，相较于基于优化的神经网络场，它大大减少了渲染延迟，为无约束捕捉设置带来了实时的新型视图合成的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnySplat是一个基于前馈网络的新型视图合成方法。</li>
<li>它适用于从未校准的图像集合中进行合成。</li>
<li>AnySplat模型可一次性预测场景几何、外观以及相机参数。</li>
<li>该模型能够轻松处理多视角数据集，无需任何姿态标注。</li>
<li>AnySplat在质量上达到了姿态感知基准测试的水平。</li>
<li>与其他方法相比，AnySplat大大减少了渲染延迟。</li>
<li>AnySplat为实时新型视图合成带来了可能性，特别是在无约束的捕捉设置下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2505.23716v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2505.23716v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2505.23716v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Tool-as-Interface-Learning-Robot-Policies-from-Observing-Human-Tool-Use"><a href="#Tool-as-Interface-Learning-Robot-Policies-from-Observing-Human-Tool-Use" class="headerlink" title="Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use"></a>Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use</h2><p><strong>Authors:Haonan Chen, Cheng Zhu, Shuijing Liu, Yunzhu Li, Katherine Driggs-Campbell</strong></p>
<p>Tool use is essential for enabling robots to perform complex real-world tasks, but learning such skills requires extensive datasets. While teleoperation is widely used, it is slow, delay-sensitive, and poorly suited for dynamic tasks. In contrast, human videos provide a natural way for data collection without specialized hardware, though they pose challenges on robot learning due to viewpoint variations and embodiment gaps. To address these challenges, we propose a framework that transfers tool-use knowledge from humans to robots. To improve the policy’s robustness to viewpoint variations, we use two RGB cameras to reconstruct 3D scenes and apply Gaussian splatting for novel view synthesis. We reduce the embodiment gap using segmented observations and tool-centric, task-space actions to achieve embodiment-invariant visuomotor policy learning. We demonstrate our framework’s effectiveness across a diverse suite of tool-use tasks, where our learned policy shows strong generalization and robustness to human perturbations, camera motion, and robot base movement. Our method achieves a 71% improvement in task success over teleoperation-based diffusion policies and dramatically reduces data collection time by 77% and 41% compared to teleoperation and the state-of-the-art interface, respectively. </p>
<blockquote>
<p>工具使用对于机器人执行复杂的现实世界任务至关重要，但学习此类技能需要大量的数据集。虽然遥操作被广泛使用，但它速度慢、对延迟敏感，且不适合动态任务。相比之下，人类视频为没有专用硬件的数据收集提供了一种自然的方式，但由于视角变化和实体差距，它们给机器人学习带来了挑战。为了解决这些挑战，我们提出了一个人类到机器人的工具使用知识转移框架。为了改善策略对视角变化的稳健性，我们使用两个RGB相机重建3D场景，并应用高斯喷绘进行新颖视角合成。我们使用分段观察和以工具为中心的任务空间动作来减少实体差距，以实现与实体无关的视触觉策略学习。我们在一系列多样化的工具使用任务中展示了我们的框架的有效性，其中我们的学习策略对人类扰动、相机运动和机器人基础运动表现出强大的泛化和稳健性。我们的方法在任务成功方面实现了与基于遥操作的扩散策略相比的77%改进，与遥操作和最新界面相比，分别将数据采集时间减少了77%和41%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04612v2">PDF</a> Accepted to CoRL 2025. Project page:   <a target="_blank" rel="noopener" href="https://tool-as-interface.github.io/">https://tool-as-interface.github.io</a>. 17 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种从人类向机器人转移工具使用知识的框架，解决机器人在复杂现实任务中工具使用学习的问题。该框架通过两个RGB相机重建三维场景、应用高斯喷射法来增强策略对视角变化的稳健性，并利用分段观察和任务空间动作减少实体化差距，实现身体视觉策略学习。此框架在多工具使用任务上展现出强泛化和稳健性。与基于遥操作的数据策略相比，任务成功率提高了71%，数据收集时间分别减少了77%和41%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人执行复杂现实任务需要工具使用技能的学习，这需要大量的数据集。</li>
<li>遥操作是机器人学习的一种常见方法，但它存在速度慢、对延迟敏感以及不适合动态任务的问题。</li>
<li>人类视频为机器人学习提供了自然的数据收集方式，无需特殊硬件。</li>
<li>视角变化和实体化差距是机器人学习中的挑战。</li>
<li>提出的框架实现了从人类到机器人的工具使用知识转移。</li>
<li>通过两个RGB相机重建三维场景、应用高斯喷射法，增强了策略对视角变化的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.04612v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.04612v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.04612v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.04612v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.04612v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UnIRe-Unsupervised-Instance-Decomposition-for-Dynamic-Urban-Scene-Reconstruction"><a href="#UnIRe-Unsupervised-Instance-Decomposition-for-Dynamic-Urban-Scene-Reconstruction" class="headerlink" title="UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene   Reconstruction"></a>UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene   Reconstruction</h2><p><strong>Authors:Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao</strong></p>
<p>Reconstructing and decomposing dynamic urban scenes is crucial for autonomous driving, urban planning, and scene editing. However, existing methods fail to perform instance-aware decomposition without manual annotations, which is crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian Splatting (3DGS) based approach that decomposes a scene into a static background and individual dynamic instances using only RGB images and LiDAR point clouds. At its core, we introduce 4D superpoints, a novel representation that clusters multi-frame LiDAR points in 4D space, enabling unsupervised instance separation based on spatiotemporal correlations. These 4D superpoints serve as the foundation for our decomposed 4D initialization, i.e., providing spatial and temporal initialization to train a dynamic 3DGS for arbitrary dynamic classes without requiring bounding boxes or object templates.Furthermore, we introduce a smoothness regularization strategy in both 2D and 3D space, further improving the temporal stability.Experiments on benchmark datasets show that our method outperforms existing methods in decomposed dynamic scene reconstruction while enabling accurate and flexible instance-level editing, making it a practical solution for real-world applications. </p>
<blockquote>
<p>重建和分解动态城市场景对于自动驾驶、城市规划和场景编辑至关重要。然而，现有方法无法执行无需手动注释的实例感知分解，这对于实例级场景编辑至关重要。我们提出了UnIRe，这是一种基于3D高斯喷溅（3DGS）的方法，它仅使用RGB图像和激光雷达点云将场景分解为静态背景和单个动态实例。我们的核心思想是引入4D超点，这是一种新颖的表示方法，可以在4D空间中聚集多帧激光雷达点，基于时空相关性实现无监督实例分离。这些4D超点为我们分解的4D初始化提供了基础，即为空间和时间的初始化，以训练动态3DGS，而无需边界框或对象模板。此外，我们在2D和3D空间中引入了平滑正则化策略，进一步提高了时间稳定性。在基准数据集上的实验表明，我们的方法在分解动态场景重建方面优于现有方法，同时实现了精确灵活的实例级编辑，使其成为现实世界应用的实用解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00763v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于三维高斯点云（3DGS）的UnIRe方法，用于重建和分解动态城市场景。该方法采用RGB图像和激光雷达点云数据，实现无需手动标注的实例感知分解。通过引入四维超点（4D superpoints）表示，将激光雷达点云在四维时空进行聚类，为动态实例分割提供时空基础。结合平滑正则化策略，在二维和三维空间中提高时间稳定性。实验表明，该方法在动态场景重建中表现优异，可实现精确灵活的实例级编辑，适用于实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UnIRe方法利用三维高斯点云（3DGS）技术重建和分解动态城市场景。</li>
<li>通过结合RGB图像和激光雷达点云数据，实现实例感知分解，无需手动标注。</li>
<li>引入四维超点（4D superpoints）表示，实现基于时空关联的实例分割。</li>
<li>四维超点为基础进行分解四维初始化，为动态三维高斯点云提供时空初始化，无需边界框或对象模板。</li>
<li>结合二维和三维空间的平滑正则化策略，提高时间稳定性。</li>
<li>实验结果表明，UnIRe方法在动态场景重建中表现优异，实现了精确灵活的实例级编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2504.00763v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Motion-Blender-Gaussian-Splatting-for-Dynamic-Scene-Reconstruction"><a href="#Motion-Blender-Gaussian-Splatting-for-Dynamic-Scene-Reconstruction" class="headerlink" title="Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction"></a>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</h2><p><strong>Authors:Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias</strong></p>
<p>Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph’s links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at <a target="_blank" rel="noopener" href="http://mlzxy.github.io/motion-blender-gs">http://mlzxy.github.io/motion-blender-gs</a>. </p>
<blockquote>
<p>高斯混合法已经成为重建动态场景的高保真工具。然而，现有的方法主要依赖于隐式运动表示，如将运动编码到神经网络或每个高斯参数中，这使得进一步操纵重建运动变得困难。这种缺乏明确的可控性限制了现有方法只能回放记录的运动，阻碍了其在机器人技术中的更广泛应用。为解决这一问题，我们提出了运动混合高斯混合法（MBGS），这是一种使用运动图作为明确和稀疏运动表示的新型框架。图链接的运动通过双四元数蒙皮传播到各个高斯分布中，可学习的权重绘制函数决定了每个链接的影响。运动图和三维高斯分布通过可微分渲染从输入视频中进行联合优化。实验表明，MBGS在极具挑战性的iPhone数据集上达到了最先进的性能，同时在HyperNeRF上表现具有竞争力。我们展示了我们的方法在动画新物体姿态、合成真实机器人演示以及通过视觉规划预测机器人动作方面的应用潜力。源代码、模型和视频演示可在<a target="_blank" rel="noopener" href="http://mlzxy.github.io/motion-blender-gs%E6%89%BE%E5%88%B0%E3%80%82">http://mlzxy.github.io/motion-blender-gs找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09040v3">PDF</a> CoRL 2025</p>
<p><strong>Summary</strong></p>
<p>高斯混合技术已成为重建动态场景的高保真工具。然而，现有方法主要依赖隐式运动表示，如将运动编码到神经网络或每个高斯参数中，这使得难以进一步操纵重建的运动。这种缺乏显式可控性的限制使得现有方法仅限于回放记录的运动，阻碍了其在机器人技术中的更广泛应用。为解决此问题，我们提出使用运动图作为显式且稀疏的运动表示的全新框架——Motion Blender Gaussian Splatting（MBGS）。图链接的运动通过双重四元数蒙皮传播到各个高斯分布，并通过可学习的权重绘制函数确定每个链接的影响。运动图和3D高斯分布通过可微分渲染从输入视频联合优化。实验表明，MBGS在极具挑战性的iPhone数据集上实现了最佳性能，同时在HyperNeRF上表现出竞争力。我们在动画新颖物体姿态、合成真实机器人演示以及通过视觉规划预测机器人动作方面的应用潜力进行了展示。有关源码、模型和演示视频可见网站链接[网站地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高斯混合技术已成为重建动态场景的重要工具，但现有方法在运动控制方面存在局限性。</li>
<li>提出了一种新的方法MBGS，使用运动图作为显式且稀疏的运动表示。</li>
<li>通过双重四元数蒙皮将运动图的链接运动传播到高斯分布。</li>
<li>MBGS通过可学习的权重绘制函数确定链接的影响。</li>
<li>运动图和3D高斯分布通过可微分渲染联合优化。</li>
<li>MBGS在iPhone数据集上实现了最佳性能，同时在HyperNeRF上具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_3DGS/2503.09040v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-17/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_NeRF/2509.11275v1/page_3_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-17  Sphere-GAN a GAN-based Approach for Saliency Estimation in 360°   Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-17/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-17\./crop_元宇宙_虚拟人/2502.20220v2/page_5_0.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-09-17  Avat3r Large Animatable Gaussian Reconstruction Model for High-fidelity   3D Head Avatars
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
