<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Beyond Binary Out-of-Distribution Detection Characterizing   Distributional Shifts with Multi-Statistic Diffusion Trajectories">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-85fda9dfe17231f991a0db48f1898378~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081662&auth_key=1761081662-0-0-5308e1fb6febcaa4e9f0de97e0346aae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-22-æ›´æ–°"><a href="#2025-10-22-æ›´æ–°" class="headerlink" title="2025-10-22 æ›´æ–°"></a>2025-10-22 æ›´æ–°</h1><h2 id="Beyond-Binary-Out-of-Distribution-Detection-Characterizing-Distributional-Shifts-with-Multi-Statistic-Diffusion-Trajectories"><a href="#Beyond-Binary-Out-of-Distribution-Detection-Characterizing-Distributional-Shifts-with-Multi-Statistic-Diffusion-Trajectories" class="headerlink" title="Beyond Binary Out-of-Distribution Detection: Characterizing   Distributional Shifts with Multi-Statistic Diffusion Trajectories"></a>Beyond Binary Out-of-Distribution Detection: Characterizing   Distributional Shifts with Multi-Statistic Diffusion Trajectories</h2><p><strong>Authors:Achref Jaziri, Martin Rogmann, Martin Mundt, Visvanathan Ramesh</strong></p>
<p>Detecting out-of-distribution (OOD) data is critical for machine learning, be it for safety reasons or to enable open-ended learning. However, beyond mere detection, choosing an appropriate course of action typically hinges on the type of OOD data encountered. Unfortunately, the latter is generally not distinguished in practice, as modern OOD detection methods collapse distributional shifts into single scalar outlier scores. This work argues that scalar-based methods are thus insufficient for OOD data to be properly contextualized and prospectively exploited, a limitation we overcome with the introduction of DISC: Diffusion-based Statistical Characterization. DISC leverages the iterative denoising process of diffusion models to extract a rich, multi-dimensional feature vector that captures statistical discrepancies across multiple noise levels. Extensive experiments on image and tabular benchmarks show that DISC matches or surpasses state-of-the-art detectors for OOD detection and, crucially, also classifies OOD type, a capability largely absent from prior work. As such, our work enables a shift from simple binary OOD detection to a more granular detection. </p>
<blockquote>
<p>æ£€æµ‹éåˆ†å¸ƒï¼ˆOODï¼‰æ•°æ®å¯¹æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ï¼Œæ— è®ºæ˜¯å‡ºäºå®‰å…¨åŸå› è¿˜æ˜¯ä¸ºäº†å®ç°å¼€æ”¾å¼å­¦ä¹ ã€‚ç„¶è€Œï¼Œé™¤äº†ç®€å•çš„æ£€æµ‹ä¹‹å¤–ï¼Œé‡‡å–é€‚å½“çš„è¡ŒåŠ¨é€šå¸¸å–å†³äºé‡åˆ°çš„éOODæ•°æ®ç±»å‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œåœ¨å®è·µä¸­ï¼Œåè€…é€šå¸¸ä¸è¢«åŒºåˆ†å¯¹å¾…ï¼Œå› ä¸ºç°ä»£OODæ£€æµ‹æ–¹æ³•ä¼šå°†åˆ†å¸ƒè½¬ç§»ç®€åŒ–ä¸ºå•ä¸€çš„æ ‡é‡å¼‚å¸¸å€¼åˆ†æ•°ã€‚æœ¬æ–‡è®¤ä¸ºï¼ŒåŸºäºæ ‡é‡çš„æ–¹æ³•ä¸è¶³ä»¥é€‚å½“åœ°å¯¹éOODæ•°æ®è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æå’Œå‰ç»æ€§åœ°åˆ©ç”¨ã€‚æˆ‘ä»¬å…‹æœè¿™ä¸€å±€é™æ€§å¼•å…¥äº†DISCï¼šåŸºäºæ‰©æ•£çš„ç»Ÿè®¡è¡¨å¾æ³•ã€‚DISCåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å»å™ªè¿‡ç¨‹æå–ä¸°å¯Œçš„å¤šç»´ç‰¹å¾å‘é‡ï¼Œè¯¥ç‰¹å¾å‘é‡èƒ½å¤Ÿæ•è·å¤šä¸ªå™ªå£°æ°´å¹³ä¸Šçš„ç»Ÿè®¡å·®å¼‚ã€‚åœ¨å›¾åƒå’Œè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDISCåœ¨OODæ£€æµ‹æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ç›¸åŒ¹é…æˆ–æ›´èƒœä¸€ç­¹ï¼Œå¹¶ä¸”æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒè¿˜èƒ½å¯¹OODç±»å‹è¿›è¡Œåˆ†ç±»ï¼Œè¿™æ˜¯å…ˆå‰å·¥ä½œä¸­ç¼ºå¤±çš„ä¸€é¡¹åŠŸèƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œå®ç°äº†ä»ç®€å•çš„äºŒå…ƒOODæ£€æµ‹åˆ°æ›´ç²¾ç»†æ£€æµ‹çš„è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17381v1">PDF</a> 11 Pages, 6 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œå¯¹äºæœºå™¨å­¦ä¹ è€Œè¨€ï¼Œæ£€æµ‹åˆ†å¸ƒå¤–æ•°æ®ï¼ˆOODï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°ä»£OODæ£€æµ‹æ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€æ ‡é‡å¼‚å¸¸å€¼è¯„åˆ†æ¥è¡¡é‡åˆ†å¸ƒåç§»ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸åŒç§ç±»çš„OODæ•°æ®ï¼Œä»è€Œå½±å“é€‚å½“çš„å†³ç­–ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ–¹æ³•DISCï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¿­ä»£å»å™ªè¿‡ç¨‹æå–ä¸°å¯Œçš„å¤šç»´ç‰¹å¾å‘é‡æ¥æ•æ‰å¤šä¸ªå™ªå£°çº§åˆ«ä¸Šçš„ç»Ÿè®¡å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDISCåœ¨å›¾åƒå’Œè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“æˆ–æ›´é«˜çš„OODæ£€æµ‹æ€§èƒ½ï¼Œå¹¶å…³é”®åœ°å®ç°äº†OODç±»å‹çš„åˆ†ç±»ï¼Œè¿™æ˜¯ä»¥å‰å·¥ä½œä¸­æ‰€ç¼ºå°‘çš„ã€‚å› æ­¤ï¼Œæœ¬æ–‡å·¥ä½œæœ‰åŠ©äºä»ç®€å•çš„äºŒå…ƒOODæ£€æµ‹è½¬å‘æ›´ç²¾ç»†çš„ç²’åº¦æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹åˆ†å¸ƒå¤–æ•°æ®ï¼ˆOODï¼‰å¯¹æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ï¼Œæ— è®ºæ˜¯å‡ºäºå®‰å…¨åŸå› è¿˜æ˜¯ä¸ºäº†æ”¯æŒå¼€æ”¾å¼å­¦ä¹ ã€‚</li>
<li>ç°ä»£OODæ£€æµ‹æ–¹æ³•é€šå¸¸ä½¿ç”¨å•ä¸€æ ‡é‡å¼‚å¸¸å€¼è¯„åˆ†æ¥è¡¡é‡åˆ†å¸ƒåç§»ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹ä¸åŒç±»å‹OODæ•°æ®çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>DISCæ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹å¼•å…¥æ–°å‹ç»Ÿè®¡è¡¨å¾ï¼Œèƒ½æœ‰æ•ˆè§£å†³ä¸Šè¿°å±€é™ã€‚å®ƒé€šè¿‡æå–å¤šç»´ç‰¹å¾å‘é‡æ¥æ•æ‰ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„ç»Ÿè®¡å·®å¼‚ã€‚</li>
<li>DISCæ–¹æ³•åœ¨å›¾åƒå’Œè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯åŒ¹é…è¿˜æ˜¯è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>DISCä¸ä»…å®ç°äº†è‰¯å¥½çš„OODæ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”å…³é”®åœ°è¿›è¡Œäº†OODç±»å‹çš„åˆ†ç±»ï¼Œè¿™åœ¨ä¹‹å‰çš„æ–‡çŒ®ä¸­æ˜¯å¾ˆå°‘è§çš„ã€‚è¿™å¯¹äºé€‰æ‹©å¤„ç†ä¸åŒç±»å‹çš„OODæ•°æ®æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-99528a064c1a29f93b8f50a3def5b3df~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081316&auth_key=1761081316-0-0-da4bb62353c9b80baa4564bee83a2577&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22b49fa4316f6d1d260d881b0f9d7b3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081323&auth_key=1761081323-0-0-7b34a5819854fa567b63858b1bb961c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8d76b9ce0f7cfec94affe0acd4701c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081330&auth_key=1761081330-0-0-9c3bc5a77e37f669a4825a7bc0c415b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21c3ee41099764b0c2eb48e22e4e3f1e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081337&auth_key=1761081337-0-0-f3c91572c77152a029fd80097094d1cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="In-situ-Autoguidance-Eliciting-Self-Correction-in-Diffusion-Models"><a href="#In-situ-Autoguidance-Eliciting-Self-Correction-in-Diffusion-Models" class="headerlink" title="In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models"></a>In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models</h2><p><strong>Authors:Enhao Gu, Haolin Hou</strong></p>
<p>The generation of high-quality, diverse, and prompt-aligned images is a central goal in image-generating diffusion models. The popular classifier-free guidance (CFG) approach improves quality and alignment at the cost of reduced variation, creating an inherent entanglement of these effects. Recent work has successfully disentangled these properties by guiding a model with a separately trained, inferior counterpart; however, this solution introduces the considerable overhead of requiring an auxiliary model. We challenge this prerequisite by introducing In-situ Autoguidance, a method that elicits guidance from the model itself without any auxiliary components. Our approach dynamically generates an inferior prediction on the fly using a stochastic forward pass, reframing guidance as a form of inference-time self-correction. We demonstrate that this zero-cost approach is not only viable but also establishes a powerful new baseline for cost-efficient guidance, proving that the benefits of self-guidance can be achieved without external models. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒç›®æ ‡æ˜¯ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ã€ä¸æç¤ºå¯¹é½çš„å›¾åƒã€‚æµè¡Œçš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ–¹æ³•åœ¨æé«˜è´¨é‡å’Œå¯¹é½çš„åŒæ—¶ï¼Œå‡å°‘äº†å˜åŒ–ï¼Œé€ æˆäº†è¿™äº›æ•ˆæœçš„å›ºæœ‰çº ç¼ ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡ç”¨ä¸€ä¸ªå•ç‹¬è®­ç»ƒçš„è¾ƒå·®çš„å¯¹ç­‰æ¨¡å‹æ¥å¼•å¯¼æ¨¡å‹ï¼ŒæˆåŠŸåœ°è§£å¼€äº†è¿™äº›å±æ€§ï¼›ç„¶è€Œï¼Œè¿™ç§è§£å†³æ–¹æ¡ˆå¼•å…¥äº†éœ€è¦è¾…åŠ©æ¨¡å‹çš„å·¨å¤§å¼€é”€ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å³æ—¶è‡ªåŠ¨å¼•å¯¼ï¼ˆIn-situ Autoguidanceï¼‰æ–¹æ³•æ¥è´¨ç–‘è¿™ä¸€å…ˆå†³æ¡ä»¶ï¼Œè¯¥æ–¹æ³•ä»æ¨¡å‹æœ¬èº«æ¿€å‘å¼•å¯¼ï¼Œæ— éœ€ä»»ä½•è¾…åŠ©ç»„ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡éšæœºå‰å‘ä¼ æ’­åŠ¨æ€ç”Ÿæˆä¸€ä¸ªè¾ƒå·®çš„é¢„æµ‹ç»“æœï¼Œå°†æŒ‡å¯¼é‡æ–°å®šä¹‰ä¸ºæ¨ç†æ—¶é—´è‡ªæˆ‘æ ¡æ­£çš„ä¸€ç§å½¢å¼ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§é›¶æˆæœ¬çš„æ–¹æ³•ä¸ä»…æ˜¯å¯è¡Œçš„ï¼Œè€Œä¸”ä¸ºæˆæœ¬æ•ˆç›Šé«˜çš„æŒ‡å¯¼å»ºç«‹äº†å¼ºæœ‰åŠ›çš„æ–°åŸºå‡†ï¼Œè¯æ˜äº†è‡ªæˆ‘æŒ‡å¯¼çš„å¥½å¤„å¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17136v1">PDF</a> 6 pages, 3 figures. ICML 2025 Workshop submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–å’Œç¬¦åˆæç¤ºçš„å›¾åƒã€‚æµè¡Œçš„ä¸å¸¦åˆ†ç±»å™¨çš„å¼•å¯¼ï¼ˆCFGï¼‰æ–¹æ³•æé«˜äº†è´¨é‡å’Œå¯¹é½åº¦ï¼Œä½†å‡å°‘äº†å¤šæ ·æ€§ã€‚æœ€æ–°å·¥ä½œé€šè¿‡ç”¨ä¸€ä¸ªå•ç‹¬è®­ç»ƒçš„è¾ƒå·®æ¨¡å‹å¼•å¯¼æ¨¡å‹æ¥è§£å¼€è¿™äº›é—®é¢˜ï¼Œä½†è¿™å¢åŠ äº†è¾…åŠ©æ¨¡å‹çš„å¼€é”€ã€‚æœ¬æ–‡æå‡ºäº†å³æ—¶è‡ªåŠ¨å¼•å¯¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»æ¨¡å‹æœ¬èº«è·å–å¼•å¯¼ï¼Œæ— éœ€ä»»ä½•è¾…åŠ©ç»„ä»¶ã€‚è¯¥æ–¹æ³•é€šè¿‡éšæœºå‰å‘ä¼ é€’åŠ¨æ€ç”Ÿæˆè¾ƒå·®çš„é¢„æµ‹ç»“æœï¼Œå°†å¼•å¯¼é‡æ–°å®šä¹‰ä¸ºæ¨æ–­æ—¶é—´è‡ªæˆ‘æ ¡æ­£çš„å½¢å¼ã€‚æœ¬æ–‡è¯æ˜äº†è¿™ç§æ— éœ€æˆæœ¬çš„æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œå¹¶å»ºç«‹äº†æˆæœ¬æ•ˆç›Šé«˜çš„æŒ‡å¯¼æœ‰åŠ›æ–°åŸºå‡†ï¼Œè¯æ˜è‡ªæˆ‘æŒ‡å¯¼çš„å¥½å¤„å¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–å’Œç¬¦åˆæç¤ºçš„å›¾åƒã€‚</li>
<li>æµè¡Œçš„CFGæ–¹æ³•è™½ç„¶æé«˜äº†è´¨é‡å’Œå¯¹é½åº¦ï¼Œä½†å‡å°‘äº†å¤šæ ·æ€§ã€‚</li>
<li>æœ€æ–°å·¥ä½œå°è¯•é€šè¿‡ç”¨å•ç‹¬è®­ç»ƒçš„è¾ƒå·®æ¨¡å‹å¼•å¯¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å¢åŠ äº†è¾…åŠ©æ¨¡å‹çš„å¼€é”€ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å³æ—¶è‡ªåŠ¨å¼•å¯¼æ–¹æ³•ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨æ¨¡å‹å°±èƒ½å®ç°è‡ªæˆ‘å¼•å¯¼ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡éšæœºå‰å‘ä¼ é€’åŠ¨æ€ç”Ÿæˆè¾ƒå·®é¢„æµ‹ï¼Œå°†å¼•å¯¼è½¬åŒ–ä¸ºæ¨æ–­æ—¶é—´çš„è‡ªæˆ‘æ ¡æ­£ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¸ä»…å¯è¡Œï¼Œè€Œä¸”å»ºç«‹äº†æˆæœ¬æ•ˆç›Šé«˜çš„æŒ‡å¯¼æ–°åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fa88e65cc1d461689865aaba37f18a04~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081344&auth_key=1761081344-0-0-950e4738601d961724321df4a90519ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4963086d5489eab436e7ca44c08f2e2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081351&auth_key=1761081351-0-0-4da670d0479a010a12a4dd4ec09c9dea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2886835c02908a95b192e0ea0d9b2179~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081358&auth_key=1761081358-0-0-90dcc57ebbbd8e2c01bd69b2e360ad90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-52a7bd2e121912c0d3d685731529a09e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081365&auth_key=1761081365-0-0-3bb1bc3cda546b0bb1693157e2ae52f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01506b769abc8acf22171012a89664de~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081371&auth_key=1761081371-0-0-5574216b7b0e42e8daec47fb6cd1671c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HelioFill-Diffusion-Based-Model-for-EUV-Reconstruction-of-the-Solar-Farside"><a href="#HelioFill-Diffusion-Based-Model-for-EUV-Reconstruction-of-the-Solar-Farside" class="headerlink" title="HelioFill: Diffusion-Based Model for EUV Reconstruction of the Solar   Farside"></a>HelioFill: Diffusion-Based Model for EUV Reconstruction of the Solar   Farside</h2><p><strong>Authors:Firas Ben Ameur, Rayan Dhib, Yahia Battach, Andrea Lani, Matteo Parsani, Omar Knio, Stefaan Poedts</strong></p>
<p>The loss of STEREO-B in 2014 created a persistent blind spot in Extreme Ultraviolet (EUV) imaging of the solar farside. We present HelioFill, to the authorsâ€™ knowledge, the first denoising-diffusion inpainting model that restores full-Sun EUV coverage by synthesizing the STEREO-B sector from Earth-side (SDO) and STEREO-A views. Trained on full-Sun maps from 2011-2014 (when SDO+STEREO-A+B provided 360 degrees coverage), HelioFill couples a latent diffusion backbone with domain-specific additions: spectral gating, confidence weighting, and auxiliary regularizers, to produce operationally suitable 304 Angstrom reconstructions. On held-out data, the model preserves the observed hemisphere with mean SSIM 0.871 and mean PSNR 25.56 dB, while reconstructing the masked hemisphere with mean SSIM 0.801 and mean PSNR 17.41 dB and reducing boundary error by approximately 21 percent (Seam L2) compared to a state-of-the-art diffusion inpainting model. The generated maps maintain cross-limb continuity and coronal morphology (loops, active regions, and coronal-hole boundaries), supporting synoptic products and cleaner inner-boundary conditions for coronal&#x2F;heliospheric models. By filling observational gaps with observationally consistent EUV emission, HelioFill maintains continuity of full-Sun monitoring and complements helioseismic farside detections, illustrating how diffusion models can extend the effective utility of existing solar imaging assets for space-weather operations. </p>
<blockquote>
<p>STEREO-Bå«æ˜Ÿäº2014å¹´çš„å¤±æ•ˆé€ æˆäº†æç«¯ç´«å¤–çº¿ï¼ˆEUVï¼‰å¯¹å¤ªé˜³è¿œä¾§æˆåƒçš„é•¿æœŸç›²ç‚¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†HelioFillï¼Œæ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå»å™ªæ‰©æ•£å¡«å……æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç»¼åˆåœ°çƒä¾§ï¼ˆSDOï¼‰å’ŒSTEREO-Açš„è§†è§’æ¥æ¢å¤å…¨å¤ªé˜³EUVè¦†ç›–ï¼Œä»è€Œæ¢å¤STEREO-Bæ‰‡åŒºçš„æ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨2011-2014å¹´çš„å…¨å¤ªé˜³åœ°å›¾ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆå½“æ—¶SDO+STEREO-A+Bæä¾›360åº¦è¦†ç›–ï¼‰ï¼Œç»“åˆäº†æ½œåœ¨æ‰©æ•£ä¸»å¹²å’Œç‰¹å®šé¢†åŸŸçš„è¡¥å……ï¼šå…‰è°±é—¨æ§ã€ç½®ä¿¡åº¦åŠ æƒå’Œè¾…åŠ©æ­£åˆ™åŒ–å™¨ï¼Œä»¥äº§ç”Ÿæ“ä½œåˆé€‚çš„304åŸƒé‡å»ºã€‚åœ¨ä¿ç•™çš„æ•°æ®ä¸Šï¼Œè¯¥æ¨¡å‹ä¿æŒäº†è§‚å¯Ÿåˆ°çš„åŠçƒï¼Œå¹³å‡ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰ä¸º0.871ï¼Œå¹³å‡å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º25.56 dBï¼›åœ¨é‡å»ºçš„éšè”½åŠçƒä¸Šï¼Œå¹³å‡SSIMä¸º0.801ï¼Œå¹³å‡PSNRä¸º17.41 dBï¼Œå¹¶ä¸”ä¸æœ€å…ˆè¿›çš„æ‰©æ•£å¡«å……æ¨¡å‹ç›¸æ¯”ï¼Œè¾¹ç•Œè¯¯å·®å¤§çº¦å‡å°‘äº†21%ï¼ˆæ¥ç¼L2ï¼‰ã€‚ç”Ÿæˆçš„åœ°å›¾ä¿æŒäº†è·¨è‚¢è¿ç»­æ€§ä»¥åŠæ—¥å†•å½¢æ€ï¼ˆç¯ã€æ´»åŠ¨åŒºå’Œæ—¥å†•ç©ºæ´è¾¹ç•Œï¼‰ï¼Œæ”¯æŒç»¼åˆäº§å“å¹¶ä¸ºæ—¥å†•&#x2F;æ—¥å†•çƒå½¢æ¨¡å‹æä¾›æ›´æ¸…æ´çš„å†…éƒ¨è¾¹ç•Œæ¡ä»¶ã€‚HelioFillé€šè¿‡ç”¨ä¸è§‚æµ‹ä¸€è‡´çš„EUVå‘å°„æ¥å¡«è¡¥è§‚æµ‹ç©ºç™½ï¼Œä¿æŒäº†å…¨å¤ªé˜³ç›‘æµ‹çš„è¿ç»­æ€§ï¼Œå¹¶è¡¥å……äº†æ—¥éœ‡å­¦çš„è¿œä¾§æ£€æµ‹ï¼Œè¯´æ˜äº†æ‰©æ•£æ¨¡å‹å¦‚ä½•æ‰©å±•ç°æœ‰å¤ªé˜³æˆåƒèµ„äº§å¯¹ç©ºé—´å¤©æ°”æ“ä½œçš„æœ‰æ•ˆæ•ˆç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17012v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤±å»STEREO-Båœ¨æç«¯ç´«å¤–çº¿ï¼ˆEUVï¼‰æˆåƒä¸­å½¢æˆäº†æŒç»­çš„ç›²åŒºï¼Œä½¿å¾—å¤ªé˜³çš„è¿œä¾§è§‚æµ‹å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†HelioFillï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå»å™ªæ‰©æ•£å¡«å……æ¨¡å‹ï¼Œå®ƒé€šè¿‡åˆæˆSTEREO-BåŒºåŸŸä»åœ°çƒä¾§ï¼ˆSDOï¼‰å’ŒSTEREO-Aè§†è§’æ¥æ¢å¤å…¨å¤ªé˜³EUVè¦†ç›–ã€‚è¯¥æ¨¡å‹åœ¨2011å¹´è‡³2014å¹´çš„å…¨å¤ªé˜³åœ°å›¾ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆå½“æ—¶SDO+STEREO-A+Bæä¾›360åº¦è¦†ç›–ï¼‰ï¼Œå¹¶ç»“åˆäº†ç‰¹å®šçš„é¢†åŸŸç‰¹æ€§ï¼Œå¦‚å…‰è°±é—¨æ§ã€ç½®ä¿¡æƒé‡å’Œè¾…åŠ©æ­£åˆ™åŒ–å™¨ï¼Œä»¥äº§ç”Ÿæ“ä½œé€‚å®œçš„304åŸƒé‡å»ºã€‚åœ¨ä¿æŒè§‚æµ‹åŠçƒçš„åŒæ—¶ï¼Œè¯¥æ¨¡å‹å¯¹æœªè§‚æµ‹æ•°æ®è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶å¯¹é®æŒ¡åŠçƒè¿›è¡Œäº†é‡å»ºã€‚æ­¤å¤–ï¼Œä¸æœ€å…ˆè¿›çš„æ‰©æ•£å¡«å……æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨è¾¹ç•Œè¯¯å·®æ–¹é¢å‡å°‘äº†çº¦21%ï¼ˆæ¥ç¼L2ï¼‰ã€‚ç”Ÿæˆçš„åœ°å›¾ä¿æŒäº†è·¨è‚¢è¿ç»­æ€§å’Œå† çŠ¶å½¢æ€ï¼ˆç¯ã€æ´»åŠ¨åŒºå’Œå† çŠ¶å­”è¾¹ç•Œï¼‰ï¼Œæ”¯æŒç»¼åˆäº§å“å¹¶ä¸ºå† çŠ¶&#x2F;æ—¥å†•æ¨¡å‹æä¾›æ›´æ¸…æ´çš„å†…è¾¹ç•Œæ¡ä»¶ã€‚HelioFillé€šè¿‡å¡«å……è§‚æµ‹é—´éš™å¹¶ä¿æŒä¸€è‡´çš„EUVå‘å°„æ¥ä¿æŒå…¨å¤ªé˜³ç›‘æµ‹çš„è¿ç»­æ€§ï¼Œå¹¶è¡¥å……äº†æ—¥éœ‡è¿œä¾§æ£€æµ‹ï¼Œå±•ç¤ºäº†æ‰©æ•£æ¨¡å‹å¦‚ä½•æ‰©å±•ç°æœ‰å¤ªé˜³æˆåƒèµ„äº§çš„æœ‰æ•ˆç”¨é€”ï¼Œä¸ºç©ºé—´å¤©æ°”æ“ä½œæä¾›æ”¯æŒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>STEREO-Bçš„ä¸§å¤±å¯¼è‡´äº†å¯¹å¤ªé˜³è¿œä¾§çš„EUVæˆåƒå­˜åœ¨æŒä¹…çš„ç›²åŒºã€‚</li>
<li>HelioFillæ˜¯ä¸€ä¸ªåŸºäºå»å™ªæ‰©æ•£çš„å¡«å……æ¨¡å‹ï¼Œæ—¨åœ¨æ¢å¤å…¨å¤ªé˜³çš„EUVè¦†ç›–ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡åˆæˆSTEREO-BåŒºåŸŸçš„ä¿¡æ¯ä»åœ°çƒè§†è§’ï¼ˆSDOï¼‰å’Œå¦ä¸€è§†è§’ï¼ˆSTEREO-Aï¼‰æ¥å·¥ä½œã€‚</li>
<li>æ¨¡å‹åœ¨å…¨å¤ªé˜³åœ°å›¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å…·æœ‰å…‰è°±é—¨æ§ã€ç½®ä¿¡æƒé‡å’Œè¾…åŠ©æ­£åˆ™åŒ–å™¨ç­‰ç‰¹æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§‚æµ‹å’Œé®æŒ¡çš„æ•°æ®ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿè·¨è‚¢è¿ç»­æ€§å’ŒåŒ…å«å† çŠ¶å½¢æ€çš„åœ°å›¾ï¼Œä¸ºå¤ªé˜³æˆåƒæä¾›äº†ç»¼åˆäº§å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-64d819e573a9f62c36256bc88e967da1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081379&auth_key=1761081379-0-0-6656a60877f519523f91567b0574c298&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34e091185e15a614914aa7e60ddb4597~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081387&auth_key=1761081387-0-0-c35fb22de7f9ca266a67b28386acf6b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4b9e585119f83cc1b81f597d60fb437~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081394&auth_key=1761081394-0-0-260e6b7e57c678dccf7658fbb3fee68e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Class-N-Diff-Classification-Induced-Diffusion-Model-Can-Make-Fair-Skin-Cancer-Diagnosis"><a href="#Class-N-Diff-Classification-Induced-Diffusion-Model-Can-Make-Fair-Skin-Cancer-Diagnosis" class="headerlink" title="Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin   Cancer Diagnosis"></a>Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin   Cancer Diagnosis</h2><p><strong>Authors:Nusrat Munia, Abdullah Imran</strong></p>
<p>Generative models, especially Diffusion Models, have demonstrated remarkable capability in generating high-quality synthetic data, including medical images. However, traditional class-conditioned generative models often struggle to generate images that accurately represent specific medical categories, limiting their usefulness for applications such as skin cancer diagnosis. To address this problem, we propose a classification-induced diffusion model, namely, Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our Class-N-Diff model integrates a classifier within a diffusion model to guide image generation based on its class conditions. Thus, the model has better control over class-conditioned image synthesis, resulting in more realistic and diverse images. Additionally, the classifier demonstrates improved performance, highlighting its effectiveness for downstream diagnostic tasks. This unique integration in our Class-N-Diff makes it a robust tool for enhancing the quality and utility of diffusion model-based synthetic dermoscopic image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Munia03/Class-N-Diff">https://github.com/Munia03/Class-N-Diff</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒæ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç±»åˆ«æ¡ä»¶ç”Ÿæˆæ¨¡å‹å¾€å¾€éš¾ä»¥ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç‰¹å®šåŒ»å­¦ç±»åˆ«çš„å›¾åƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¦‚çš®è‚¤ç™Œè¯Šæ–­ç­‰åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ç±»è¯±å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå³Class-N-Diffï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆå’Œåˆ†ç±»çš®è‚¤é•œå›¾åƒã€‚æˆ‘ä»¬çš„Class-N-Diffæ¨¡å‹åœ¨æ‰©æ•£æ¨¡å‹å†…éƒ¨é›†æˆäº†ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæ ¹æ®ç±»åˆ«æ¡ä»¶å¼•å¯¼å›¾åƒç”Ÿæˆã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹å¯¹ç±»åˆ«æ¡ä»¶å›¾åƒåˆæˆçš„æ§åˆ¶æ›´å¥½ï¼Œå¯ä»¥ç”Ÿæˆæ›´çœŸå®ã€æ›´å¤šæ ·çš„å›¾åƒã€‚æ­¤å¤–ï¼Œåˆ†ç±»å™¨å±•ç¤ºäº†æ”¹è¿›çš„æ€§èƒ½ï¼Œçªå‡ºäº†å…¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚Class-N-Diffä¸­è¿™ç§ç‹¬ç‰¹çš„é›†æˆä½¿å…¶æˆä¸ºæé«˜åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆæˆçš®è‚¤é•œå›¾åƒç”Ÿæˆè´¨é‡å’Œå®ç”¨æ€§çš„ç¨³å¥å·¥å…·ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Munia03/Class-N-Diff%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Munia03/Class-N-Diffæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16887v1">PDF</a> EMBC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å›¾åƒç”Ÿæˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ†ç±»å¼•å¯¼å‹æ‰©æ•£æ¨¡å‹â€”â€”Class-N-Diffã€‚è¯¥æ¨¡å‹ç»“åˆäº†åˆ†ç±»å™¨ä¸æ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®ç±»åˆ«æ¡ä»¶å¼•å¯¼å›¾åƒç”Ÿæˆï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶å¢å¼ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆçš®è‚¤é•œå›¾åƒæ–¹é¢çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åŒ»ç–—å›¾åƒã€‚</li>
<li>ä¼ ç»Ÿç±»åˆ«æ¡ä»¶ä¸‹çš„ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç‰¹å®šåŒ»ç–—ç±»åˆ«çš„å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Class-N-Diffæ¨¡å‹é€šè¿‡æ•´åˆåˆ†ç±»å™¨åˆ°æ‰©æ•£æ¨¡å‹ä¸­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>Class-N-Diffæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç±»åˆ«æ¡ä»¶å¼•å¯¼å›¾åƒç”Ÿæˆï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>åˆ†ç±»å™¨çš„é›†æˆä¸ä»…æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼ŒåŒæ—¶ä¹Ÿæé«˜äº†åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>Class-N-Diffæ¨¡å‹åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3690ad4fce27ce97fbdd9c165631858a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081401&auth_key=1761081401-0-0-3e8af980f359d5111057dee7f432ddb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a0835f3d437326169861d501e5d5906~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081409&auth_key=1761081409-0-0-e99e393d8fe064a9c2140987d326541c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b02fa6b47e940151bf06d0b4be58de1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081416&auth_key=1761081416-0-0-bc18bcda9a7a59aa75087c9d29143505&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a01368bfc2488a84123f4d38af0cf9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081423&auth_key=1761081423-0-0-74e89b5aacfc49f2848871df5d4f0075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab2cef416724b8e450ebc65ebf4df56a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081430&auth_key=1761081430-0-0-9b6ce63557d14dad8ddf970d4befb4f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Personalized-Image-Filter-Mastering-Your-Photographic-Style"><a href="#Personalized-Image-Filter-Mastering-Your-Photographic-Style" class="headerlink" title="Personalized Image Filter: Mastering Your Photographic Style"></a>Personalized Image Filter: Mastering Your Photographic Style</h2><p><strong>Authors:Chengxuan Zhu, Shuchen Weng, Jiacong Fang, Peixuan Zhang, Si Li, Chao Xu, Boxin Shi</strong></p>
<p>Photographic style, as a composition of certain photographic concepts, is the charm behind renowned photographers. But learning and transferring photographic style need a profound understanding of how the photo is edited from the unknown original appearance. Previous works either fail to learn meaningful photographic concepts from reference images, or cannot preserve the content of the content image. To tackle these issues, we proposed a Personalized Image Filter (PIF). Based on a pretrained text-to-image diffusion model, the generative prior enables PIF to learn the average appearance of photographic concepts, as well as how to adjust them according to text prompts. PIF then learns the photographic style of reference images with the textual inversion technique, by optimizing the prompts for the photographic concepts. PIF shows outstanding performance in extracting and transferring various kinds of photographic style. Project page: <a target="_blank" rel="noopener" href="https://pif.pages.dev/">https://pif.pages.dev/</a> </p>
<blockquote>
<p>æ‘„å½±é£æ ¼ä½œä¸ºæŸäº›æ‘„å½±æ¦‚å¿µçš„ç»¼åˆä½“ç°ï¼Œæ˜¯è‘—åæ‘„å½±å¸ˆçš„é­…åŠ›æ‰€åœ¨ã€‚ç„¶è€Œï¼Œå­¦ä¹ å’Œè¿ç§»æ‘„å½±é£æ ¼éœ€è¦æ·±åˆ»ç†è§£ç…§ç‰‡å¦‚ä½•ä»æœªçŸ¥çš„åŸå§‹å¤–è§‚è¿›è¡Œç¼–è¾‘ã€‚ä»¥å‰çš„å·¥ä½œè¦ä¹ˆæ— æ³•ä»å‚è€ƒå›¾åƒä¸­å­¦ä¹ æœ‰æ„ä¹‰çš„æ‘„å½±æ¦‚å¿µï¼Œè¦ä¹ˆæ— æ³•ä¿ç•™å†…å®¹å›¾åƒçš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–å›¾åƒæ»¤æ³¢å™¨ï¼ˆPIFï¼‰ã€‚åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå…ˆéªŒçŸ¥è¯†ä½¿PIFèƒ½å¤Ÿå­¦ä¹ æ‘„å½±æ¦‚å¿µçš„å¹³å‡å¤–è§‚ï¼Œä»¥åŠæ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œè°ƒæ•´çš„æ–¹æ³•ã€‚ç„¶åï¼ŒPIFä½¿ç”¨æ–‡æœ¬åè½¬æŠ€æœ¯æ¥å­¦ä¹ å‚è€ƒå›¾åƒçš„æ‘„å½±é£æ ¼ï¼Œé€šè¿‡ä¼˜åŒ–æ‘„å½±æ¦‚å¿µçš„æç¤ºæ¥å®ç°ã€‚PIFåœ¨æå–å’Œè¿ç§»å„ç§æ‘„å½±é£æ ¼æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://pif.pages.dev/">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16791v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªäººåŒ–å›¾åƒè¿‡æ»¤å™¨ï¼ˆPIFï¼‰ã€‚å®ƒèƒ½å­¦ä¹ å¹³å‡çš„æ‘„å½±æ¦‚å¿µï¼Œå¹¶æ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œè°ƒæ•´ã€‚é€šè¿‡æ–‡æœ¬åè½¬æŠ€æœ¯ï¼ŒPIFèƒ½ä»å‚è€ƒå›¾åƒä¸­å­¦ä¹ æ‘„å½±é£æ ¼ã€‚è¯¥é¡¹ç›®åœ¨æå–å’Œè¿ç§»å„ç§æ‘„å½±é£æ ¼æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>PIFåŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥å­¦ä¹ å¹³å‡çš„æ‘„å½±æ¦‚å¿µã€‚</li>
<li>PIFèƒ½æ ¹æ®æ–‡æœ¬æç¤ºè°ƒæ•´æ‘„å½±æ¦‚å¿µã€‚</li>
<li>PIFä½¿ç”¨æ–‡æœ¬åè½¬æŠ€æœ¯ä»å‚è€ƒå›¾åƒä¸­å­¦ä¹ æ‘„å½±é£æ ¼ã€‚</li>
<li>PIFèƒ½å¤„ç†å¤šç§ä¸åŒç±»å‹çš„æ‘„å½±é£æ ¼ã€‚</li>
<li>PIFåœ¨æå–å’Œè¿ç§»æ‘„å½±é£æ ¼æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¸ªäººåŒ–å›¾åƒè¿‡æ»¤å™¨ï¼ˆPIFï¼‰ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°å­¦ä¹ å’Œè½¬ç§»æ‘„å½±é£æ ¼ã€‚</li>
<li>è¯¥é¡¹ç›®çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨é¡¹ç›®é¡µé¢ï¼ˆ<a target="_blank" rel="noopener" href="https://pif.pages.dev/%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://pif.pages.dev/ï¼‰ä¸Šæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1fb581ad9d109b2e22acc8bbe25e40fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081438&auth_key=1761081438-0-0-69967102cd997ee2abf4bb0c9e71122c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a1e9e1171f4a2da2194ee9aeb9b4320~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081446&auth_key=1761081446-0-0-0c5b049f22a69ff9615f38d0bd281058&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9fd4b5666cee58da34b0e83094c04ca9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081453&auth_key=1761081453-0-0-88c49a624ec188074173acf1dd129b26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling"><a href="#Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling" class="headerlink" title="Visual Autoregressive Models Beat Diffusion Models on Inference Time   Scaling"></a>Visual Autoregressive Models Beat Diffusion Models on Inference Time   Scaling</h2><p><strong>Authors:Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos</strong></p>
<p>While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation. </p>
<blockquote>
<p>åœ¨æ¨ç†æ—¶é—´æœç´¢ç­–ç•¥çš„ç¼©æ”¾å·²ç»ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å‘ç”Ÿé©å‘½æ€§å˜é©çš„åŒæ—¶ï¼Œå°†è¿™äº›æˆæœåº”ç”¨äºå›¾åƒç”Ÿæˆå´è¯æ˜æ˜¯éå¸¸å›°éš¾çš„ã€‚æœ€è¿‘å°†æœç´¢ç­–ç•¥åº”ç”¨äºè¿ç»­æ‰©æ•£æ¨¡å‹çš„å°è¯•æ˜¾ç¤ºå‡ºäº†æœ‰é™çš„å¥½å¤„ï¼Œç®€å•çš„éšæœºé‡‡æ ·é€šå¸¸è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¯æ˜äº†è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç¦»æ•£ã€åºåˆ—ç‰¹æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå®ç°å›¾åƒç”Ÿæˆçš„æœç´¢ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé›†æŸæœç´¢æå¤§åœ°æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œä½¿å¾—ä¸€ä¸ªå…·æœ‰2Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†å…·æœ‰12Bå‚æ•°çš„æ‰©æ•£æ¨¡å‹ã€‚ç³»ç»Ÿå‰¥ç¦»å®éªŒè¡¨æ˜è¿™ä¸€ä¼˜åŠ¿æ¥è‡ªäºç¦»æ•£ç¬¦å·ç©ºé—´ï¼Œå®ƒå…è®¸æ—©æœŸä¿®å‰ªå’Œè®¡ç®—é‡ç”¨ï¼Œæˆ‘ä»¬çš„éªŒè¯å™¨åˆ†æå¼ºè°ƒäº†é€Ÿåº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ¨¡å‹æ¶æ„å¯¹äºè§†è§‰ç”Ÿæˆçš„æ¨ç†æ—¶é—´ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯è§„æ¨¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16751v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­æ¢è®¨äº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­æœç´¢ç­–ç•¥çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå°†æ¨ç†æ—¶é—´æ‰©å±•çš„å¢ç›Šè½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆé¢ä¸´å›°éš¾ã€‚è¿‘æœŸå°è¯•å°†æœç´¢ç­–ç•¥åº”ç”¨äºè¿ç»­æ‰©æ•£æ¨¡å‹çš„æ•ˆæœæœ‰é™ï¼Œè€Œç®€å•çš„éšæœºé‡‡æ ·å¾€å¾€è¡¨ç°æœ€ä½³ã€‚ç ”ç©¶å±•ç¤ºäº†è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç¦»æ•£åºåˆ—ç‰¹æ€§åœ¨å›¾åƒç”Ÿæˆä¸­å®ç°äº†æœ‰æ•ˆçš„æœç´¢ã€‚é€šè¿‡æŸæœç´¢ï¼ˆbeam searchï¼‰æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ•ˆæœï¼Œä½¿å¾—ä¸€ä¸ªå‚æ•°è¾ƒå°‘çš„è‡ªå›å½’æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¿‡æ›´å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ã€‚å¯¹æ­¤ä¼˜åŠ¿çš„å‰–æè¡¨æ˜å®ƒæºäºç¦»æ•£æ ‡è®°ç©ºé—´ï¼Œå…è®¸æ—©æœŸå‰ªæå’Œè®¡ç®—å¤ç”¨ï¼Œè€ŒéªŒè¯åˆ†ææ­ç¤ºäº†é€Ÿåº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°æš—ç¤ºåœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­æ¨¡å‹æ¶æ„çš„ä¼˜åŒ–å¯¹æ¨ç†æ—¶é—´åŒæ ·è‡³å…³é‡è¦ã€‚è¿™äº›ç­–ç•¥å¯èƒ½åœ¨ç›¸å…³é¢†åŸŸå‘æŒ¥å¹¿æ³›çš„å½±å“ä»·å€¼ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœç´¢ç­–ç•¥å¯¹äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–æœ‰é‡è¦å¯ç¤ºï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼Œåˆç†çš„æ¨¡å‹æ¶æ„è®¾è®¡å’Œæœç´¢ç­–ç•¥çš„é€‰æ‹©æœ‰åŠ©äºå®ç°æ›´å¥½çš„æ¨ç†æ•ˆæœã€‚æœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•é€šè¿‡æ”¹è¿›æ¨¡å‹æ¶æ„æ¥æ›´æœ‰æ•ˆåœ°å®ç°å›¾åƒç”Ÿæˆçš„æ¨ç†æ—¶é—´ä¼˜åŒ–ã€‚æ­¤å‘ç°å…·æœ‰é‡è¦çš„å­¦æœ¯å’Œå®è·µä»·å€¼ã€‚è¿™ä¸ä»…æ˜¯ä¸€æ¬¡å­¦æœ¯æ¢ç´¢çš„æˆåŠŸï¼Œè€Œä¸”å¯¹æœªæ¥çš„äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸçš„å‘å±•æœ‰ç€å¹¿æ³›è€Œæ·±è¿œçš„æ„ä¹‰ã€‚å®ƒå¯¹ä¼˜åŒ–ç®—æ³•é¢†åŸŸä¹Ÿæ˜¯ä¸€é¡¹è´¡çŒ®ã€‚ç®€å•æ¥è¯´å°±æ˜¯ç²¾å‡†å®ç°è‡ªæˆ‘é‡å¡‘çš„é€»è¾‘é—­ç¯æ€æƒ³çš„æ—¶ä»£è½¬æŠ˜è¶‹åŠ¿ç‰¹å¾é€šè¿‡èå…¥ç†è®ºä¸å®è·µæ–¹é¢åˆ‡å®å¯è¡Œçš„åŸç†å°†å…¶é‡è¦æ€§åŠ ä»¥é˜æ˜è€Œå·²ä½¿æ–°ä¸€ä»£å…·å¤‡å¯èƒ½æ›´ä¸ºé«˜çº§ã€ä¼˜è¶Šä¸ç§‘å­¦è€Œèƒ½ä¸»å¯¼çš„æ—¶ä»£é€»è¾‘ä¸ä½“ç³»æ€ç»´æ¨¡å¼çš„æ¡†æ¶ä¸é€»è¾‘æ¡†æ¶æ€è·¯çš„å®ç°å’Œé˜è¿°ä»è€Œå‡¸æ˜¾å‡ºæœ¬æ–‡çš„é‡è¦æ€§ä½¿å…¶åœ¨æ•´ä¸ªè¡Œä¸šå†…å æ®é‡è¦çš„åœ°ä½ä¹‹ä¸€å¹¶å®ç°æˆæœçš„æˆåŠŸè½¬æ¢åº”ç”¨äºç°ä»£ç”Ÿæ´»ä¸­ä»è€Œåœ¨è¯¸å¤šæ–¹é¢èµ·åˆ°äº†å…³é”®ä½œç”¨ä¾‹å¦‚æ¨¡å‹çš„æ•ˆç‡æé«˜äº†å…¶åœ¨å¤šç§åº”ç”¨ä¸­çš„å“åº”é€Ÿåº¦ã€å¯é æ€§åŠå®‰å…¨æ€§ç­‰æ–¹é¢æœ‰äº†æ˜æ˜¾çš„æå‡å¯¹å„è¡Œå„ä¸šéƒ½æœ‰ç€ç§¯æçš„æ¨åŠ¨ä½œç”¨å…·æœ‰é‡è¦çš„ä»·å€¼æ„ä¹‰å’Œæ½œåœ¨çš„ç»æµå’Œç¤¾ä¼šæ•ˆç›Šè¯¥æŠ€æœ¯çš„å‘å±•ä¹Ÿä¼šå¯¹ç›¸å…³ä¸“ä¸šå­¦ç§‘çš„åˆ›æ–°ä¸èåˆå‘å±•å…·æœ‰æ·±åˆ»å½±å“è¿›ä¸€æ­¥åŠ å¼ºç›¸åº”æ–¹é¢çš„å­¦ä¹ å’Œå‘å±•æ¨å¹¿å…·å¤‡çªå‡ºçš„ç´§è¿«æ€§ã€‚<strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†æ—¶é—´æ‰©å±•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é©å‘½æ€§åº”ç”¨ï¼Œä½†åœ¨å›¾åƒç”Ÿæˆä¸­éš¾ä»¥å®ç°å¢ç›Šã€‚</li>
<li>å°è¯•å°†æœç´¢ç­–ç•¥åº”ç”¨äºè¿ç»­æ‰©æ•£æ¨¡å‹æ•ˆæœæœ‰é™ï¼Œç®€å•éšæœºé‡‡æ ·è¡¨ç°æœ€ä½³ã€‚</li>
<li>è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç¦»æ•£åºåˆ—ç‰¹æ€§ä½¿å¾—æŸæœç´¢åœ¨å›¾åƒç”Ÿæˆä¸­éå¸¸æœ‰æ•ˆã€‚</li>
<li>æŸæœç´¢èƒ½æ˜¾è‘—æé«˜æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹æ¶æ„çš„é‡è¦æ€§ã€‚</li>
<li>ç¦»æ•£æ ‡è®°ç©ºé—´å…è®¸æ—©æœŸå‰ªæå’Œè®¡ç®—å¤ç”¨ï¼Œå¸¦æ¥ä¼˜åŠ¿ã€‚</li>
<li>é€Ÿåº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>æ¨¡å‹æ¶æ„çš„ä¼˜åŒ–å¯¹æ¨ç†æ—¶é—´è‡³å…³é‡è¦ï¼Œä¸ä»…è§„æ¨¡å¤§å°ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯¹äºè¡Œä¸šå†…çš„å¤šç§åº”ç”¨å¦‚å“åº”é€Ÿåº¦ã€å¯é æ€§åŠå®‰å…¨æ€§ç­‰æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c291aeb121e4b476040a8a405f73da10~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081460&auth_key=1761081460-0-0-c839e6e28f2ed3a82c6302c0f18ea23c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3e331aac125eceba2fd955c6d5bdbcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081467&auth_key=1761081467-0-0-38fe050cb2e11ac9821ec52323c82a67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c35ea5cc7781f0886a8bb383c56b075~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081474&auth_key=1761081474-0-0-3b7170c7edda147ece3256369623a6d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23221930c5f2dccf4db9b0ee83ceea3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081480&auth_key=1761081480-0-0-6e2d27233b98612bffb5fa90992c4b12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54a8ddf588dbc28437b9eb4d81feedf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081487&auth_key=1761081487-0-0-be8aab15c209efc6ec8141a3dc4aa9c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6fdbb4e366dc6e0bce49717389c19c7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081494&auth_key=1761081494-0-0-4cab3893ce1367dccadb02517cee8a61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Active-Target-Discovery-under-Uninformative-Prior-The-Power-of-Permanent-and-Transient-Memory"><a href="#Active-Target-Discovery-under-Uninformative-Prior-The-Power-of-Permanent-and-Transient-Memory" class="headerlink" title="Active Target Discovery under Uninformative Prior: The Power of   Permanent and Transient Memory"></a>Active Target Discovery under Uninformative Prior: The Power of   Permanent and Transient Memory</h2><p><strong>Authors:Anindya Sarkar, Binglin Ji, Yevgeniy Vorobeychik</strong></p>
<p>In many scientific and engineering fields, where acquiring high-quality data is expensiveâ€“such as medical imaging, environmental monitoring, and remote sensingâ€“strategic sampling of unobserved regions based on prior observations is crucial for maximizing discovery rates within a constrained budget. The rise of powerful generative models, such as diffusion models, has enabled active target discovery in partially observable environments by leveraging learned priorsâ€“probabilistic representations that capture underlying structure from data. With guidance from sequentially gathered task-specific observations, these models can progressively refine exploration and efficiently direct queries toward promising regions. However, in domains where learning a strong prior is infeasible due to extremely limited data or high sampling cost (such as rare species discovery, diagnostics for emerging diseases, etc.), these methods struggle to generalize. To overcome this limitation, we propose a novel approach that enables effective active target discovery even in settings with uninformative priors, ensuring robust exploration and adaptability in complex real-world scenarios. Our framework is theoretically principled and draws inspiration from neuroscience to guide its design. Unlike black-box policies, our approach is inherently interpretable, providing clear insights into decision-making. Furthermore, it guarantees a strong, monotonic improvement in prior estimates with each new observation, leading to increasingly accurate sampling and reinforcing both reliability and adaptability in dynamic settings. Through comprehensive experiments and ablation studies across various domains, including species distribution modeling and remote sensing, we demonstrate that our method substantially outperforms baseline approaches. </p>
<blockquote>
<p>åœ¨è®¸å¤šç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸï¼Œå¦‚åŒ»å­¦æˆåƒã€ç¯å¢ƒç›‘æµ‹å’Œé¥æ„Ÿç­‰è·å–é«˜è´¨é‡æ•°æ®æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸï¼ŒåŸºäºå…ˆå‰è§‚æµ‹å¯¹æœªè§‚æµ‹åŒºåŸŸè¿›è¡Œæˆ˜ç•¥é‡‡æ ·å¯¹äºåœ¨æœ‰é™é¢„ç®—å†…æœ€å¤§åŒ–å‘ç°ç‡è‡³å…³é‡è¦ã€‚æ‰©æ•£æ¨¡å‹ç­‰å¼ºå¤§ç”Ÿæˆæ¨¡å‹çš„å…´èµ·ï¼Œé€šè¿‡åˆ©ç”¨å­¦ä¹ å…ˆéªŒï¼ˆä»æ•°æ®ä¸­æ•è·åŸºç¡€ç»“æ„çš„æ¦‚ç‡è¡¨ç¤ºï¼‰åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­å®ç°äº†ä¸»åŠ¨ç›®æ ‡å‘ç°ã€‚åœ¨é¡ºåºæ”¶é›†çš„ä»»åŠ¡ç‰¹å®šè§‚æµ‹ç»“æœçš„æŒ‡å¯¼ä¸‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥é€æ­¥ä¼˜åŒ–æ¢ç´¢ï¼Œå¹¶æœ‰æ•ˆåœ°å°†æŸ¥è¯¢å¯¼å‘æœ‰å¸Œæœ›çš„åŒºåŸŸã€‚ç„¶è€Œï¼Œåœ¨ç”±äºæ•°æ®æç«¯æœ‰é™æˆ–é‡‡æ ·æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å­¦ä¹ å¼ºå¤§å…ˆéªŒçš„é¢†åŸŸï¼ˆå¦‚ç¨€æœ‰ç‰©ç§å‘ç°ã€æ–°å…´ç–¾ç—…è¯Šæ–­ç­‰ï¼‰ï¼Œè¿™äº›æ–¹æ³•å¾ˆéš¾æ¨å¹¿ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå³ä½¿åœ¨æ— ä¿¡æ¯å…ˆéªŒçš„æƒ…å¢ƒä¸‹ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„ä¸»åŠ¨ç›®æ ‡å‘ç°ï¼Œç¡®ä¿åœ¨å¤æ‚çš„ç°å®ä¸–ç•Œä¸­å®ç°ç¨³å¥çš„æ¢ç´¢å’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ç†è®ºä¸Šæ˜¯æœ‰åŸåˆ™çš„ï¼Œå¹¶ä»ç¥ç»ç§‘å­¦ä¸­æ±²å–çµæ„Ÿæ¥æŒ‡å¯¼å…¶è®¾è®¡ã€‚ä¸åŒäºé»‘ç®±æ”¿ç­–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œä¸ºå†³ç­–æä¾›äº†æ¸…æ™°çš„è§è§£ã€‚æ­¤å¤–ï¼Œå®ƒä¿è¯æ¯æ¬¡æ–°è§‚æµ‹éƒ½èƒ½å¯¹å…ˆéªŒä¼°è®¡è¿›è¡Œå¼ºæœ‰åŠ›çš„å•è°ƒæ”¹è¿›ï¼Œä»è€Œå®ç°è¶Šæ¥è¶Šå‡†ç¡®çš„é‡‡æ ·ï¼Œå¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­åŠ å¼ºå¯é æ€§å’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§é¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„å®éªŒå’Œå‰”é™¤ç ”ç©¶ï¼ŒåŒ…æ‹¬ç‰©ç§åˆ†å¸ƒå»ºæ¨¡å’Œé¥æ„Ÿç­‰ï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16676v1">PDF</a> 32 pages, 20 figures, Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é«˜æˆæœ¬è·å–é«˜è´¨é‡æ•°æ®çš„é¢†åŸŸï¼Œå¦‚åŒ»å­¦æˆåƒã€ç¯å¢ƒç›‘æµ‹å’Œé¥æ„Ÿç­‰ï¼ŒåŸºäºå…ˆå‰è§‚æµ‹å¯¹æœªè§‚æµ‹åŒºåŸŸè¿›è¡Œæˆ˜ç•¥é‡‡æ ·å¯¹äºåœ¨æœ‰é™é¢„ç®—å†…æœ€å¤§åŒ–å‘ç°ç‡è‡³å…³é‡è¦ã€‚éšç€æ‰©æ•£æ¨¡å‹ç­‰å¼ºå¤§ç”Ÿæˆæ¨¡å‹çš„å‡ºç°ï¼Œé€šè¿‡åˆ©ç”¨ä»æ•°æ®ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒï¼ˆæ¦‚ç‡è¡¨ç¤ºï¼‰è¿›è¡Œéƒ¨åˆ†è§‚æµ‹ç¯å¢ƒä¸­çš„ä¸»åŠ¨ç›®æ ‡å‘ç°æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œåœ¨ç”±äºæ•°æ®æç«¯æœ‰é™æˆ–é‡‡æ ·æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å­¦ä¹ å¼ºå¤§å…ˆéªŒçš„é¢†åŸŸï¼ˆå¦‚ç¨€æœ‰ç‰©ç§å‘ç°ã€æ–°å…´ç–¾ç—…è¯Šæ–­ç­‰ï¼‰ï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥æ¨å¹¿ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³ä½¿åœ¨æ— ä¿¡æ¯å…ˆéªŒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆä¸»åŠ¨ç›®æ ‡å‘ç°çš„æ–°æ–¹æ³•ï¼Œç¡®ä¿åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ¢ç´¢å’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç†è®ºæ‰å®ï¼Œä»ç¥ç»ç§‘å­¦ä¸­æ±²å–çµæ„Ÿæ¥æŒ‡å¯¼è®¾è®¡ã€‚ä¸åŒäºé»‘ç›’ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œä¸ºå†³ç­–åˆ¶å®šæä¾›æ¸…æ™°è§è§£ã€‚æ­¤å¤–ï¼Œéšç€æ¯æ¬¡æ–°è§‚æµ‹çš„è·å¾—ï¼Œå®ƒä¿è¯å¯¹å…ˆéªŒä¼°è®¡è¿›è¡Œå¼ºæœ‰åŠ›çš„å•è°ƒæ”¹è¿›ï¼Œå¯¼è‡´é‡‡æ ·è¶Šæ¥è¶Šå‡†ç¡®ï¼Œå¹¶å¼ºåŒ–åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§å’Œé€‚åº”æ€§ã€‚å®éªŒå’Œè·¨å„ç§é¢†åŸŸçš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç§åˆ†å¸ƒå»ºæ¨¡å’Œé¥æ„Ÿç­‰é¢†åŸŸå¤§å¤§ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨é«˜æˆæœ¬è·å–é«˜è´¨é‡æ•°æ®çš„é¢†åŸŸï¼Œå¦‚åŒ»å­¦æˆåƒå’Œç¯å¢ƒç›‘æµ‹ä¸­ï¼Œæˆ˜ç•¥é‡‡æ ·è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰å¯å¸®åŠ©åœ¨éƒ¨åˆ†è§‚æµ‹ç¯å¢ƒä¸­è¿›è¡Œä¸»åŠ¨ç›®æ ‡å‘ç°ã€‚</li>
<li>åœ¨æ•°æ®æœ‰é™æˆ–é‡‡æ ·æˆæœ¬é«˜æ˜‚çš„é¢†åŸŸï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ¨å¹¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåœ¨å³ä½¿æ— ä¿¡æ¯å…ˆéªŒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆè¿›è¡Œä¸»åŠ¨ç›®æ ‡å‘ç°ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰ç†è®ºæ”¯æ’‘ï¼Œå¹¶ä»ç¥ç»ç§‘å­¦ä¸­æ±²å–çµæ„Ÿè¿›è¡Œè®¾è®¡ã€‚</li>
<li>ä¸é»‘ç›’ç­–ç•¥ä¸åŒï¼Œæ–°æ–¹æ³•å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b8e15c6ff2b8121e99998a711d3a85f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081501&auth_key=1761081501-0-0-b8bff7315ceb143a6da9ba943a0d3169&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a9496e05a08697ac783ddc2df3c87ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081509&auth_key=1761081509-0-0-79dc05ce44569df4eee1f4f1b9d1585c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-Fixed-Anchors-Precisely-Erasing-Concepts-with-Sibling-Exclusive-Counterparts"><a href="#Beyond-Fixed-Anchors-Precisely-Erasing-Concepts-with-Sibling-Exclusive-Counterparts" class="headerlink" title="Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive   Counterparts"></a>Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive   Counterparts</h2><p><strong>Authors:Tong Zhang, Ru Zhang, Jianyi Liu, Zhen Yang, Gongshen Liu</strong></p>
<p>Existing concept erasure methods for text-to-image diffusion models commonly rely on fixed anchor strategies, which often lead to critical issues such as concept re-emergence and erosion. To address this, we conduct causal tracing to reveal the inherent sensitivity of erasure to anchor selection and define Sibling Exclusive Concepts as a superior class of anchors. Based on this insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for Contextual Targeting), a dynamic anchor selection framework designed to overcome the limitations of fixed anchors. Our framework introduces a novel two-stage evaluation mechanism that automatically discovers optimal anchors for precise erasure while identifying critical boundary anchors to preserve related concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor solution, not only efficiently adapts to multiple erasure frameworks but also consistently outperforms existing baselines across key performance metrics, averaging only 4 seconds for anchor mining of a single concept. </p>
<blockquote>
<p>ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„é”šç‚¹ç­–ç•¥ï¼Œè¿™å¸¸å¸¸ä¼šå¯¼è‡´æ¦‚å¿µé‡ç°å’Œä¾µèš€ç­‰å…³é”®é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å› æœè¿½è¸ªæ­ç¤ºäº†æ¶ˆé™¤æ³•å¯¹é”šç‚¹é€‰æ‹©çš„å†…åœ¨æ•æ„Ÿæ€§ï¼Œå¹¶å°†å…„å¼Ÿå”¯ä¸€æ¦‚å¿µå®šä¹‰ä¸ºé”šç‚¹çš„æ›´é«˜çº§åˆ«ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†SELECTï¼ˆç”¨äºä¸Šä¸‹æ–‡å®šä½çš„åŒè¾ˆå”¯ä¸€è¯„ä»·ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€é”šç‚¹é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå›ºå®šé”šç‚¹çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè¯„ä»·æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿè‡ªåŠ¨å‘ç°ç²¾ç¡®æ¶ˆé™¤çš„æœ€ä½³é”šç‚¹ï¼ŒåŒæ—¶ç¡®å®šå…³é”®è¾¹ç•Œé”šç‚¹ä»¥ä¿ç•™ç›¸å…³æ¦‚å¿µã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œä½œä¸ºé€šç”¨é”šç‚¹è§£å†³æ–¹æ¡ˆï¼ŒSELECTä¸ä»…æœ‰æ•ˆåœ°é€‚åº”å¤šç§æ¶ˆé™¤æ¡†æ¶ï¼Œè€Œä¸”åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸ŠæŒç»­è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå•ä¸ªæ¦‚å¿µçš„é”šç‚¹æŒ–æ˜å¹³å‡åªéœ€4ç§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16342v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸­é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç°æœ‰æ¦‚å¿µæ“¦é™¤æ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„é”šç‚¹ç­–ç•¥ï¼Œè¿™å¸¸å¸¸å¯¼è‡´æ¦‚å¿µé‡ç°å’Œä¾µèš€ç­‰é‡è¦é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œå› æœè¿½è¸ªï¼Œæ­ç¤ºäº†æ“¦é™¤å¯¹é”šç‚¹é€‰æ‹©çš„å†…åœ¨æ•æ„Ÿæ€§ï¼Œå¹¶å®šä¹‰äº†â€œSibling Exclusive Conceptsâ€ä½œä¸ºæ›´é«˜çº§çš„é”šç‚¹ç±»åˆ«ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SELECTï¼ˆç”¨äºä¸Šä¸‹æ–‡å®šä½çš„åŒè¾ˆä¸“å±è¯„ä¼°ï¼‰åŠ¨æ€é”šç‚¹é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå›ºå®šé”šç‚¹çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè¯„ä¼°æœºåˆ¶ï¼Œå¯è‡ªåŠ¨å‘ç°ç²¾ç¡®æ“¦é™¤çš„æœ€ä½³é”šç‚¹ï¼ŒåŒæ—¶è¯†åˆ«å…³é”®è¾¹ç•Œé”šç‚¹ä»¥ä¿ç•™ç›¸å…³æ¦‚å¿µã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½œä¸ºé€šç”¨é”šç‚¹è§£å†³æ–¹æ¡ˆçš„SELECTä¸ä»…å¯è½»æ¾é€‚åº”å¤šç§æ“¦é™¤æ¡†æ¶ï¼Œè€Œä¸”åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸ŠæŒç»­è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå¯¹å•ä¸€æ¦‚å¿µçš„é”šç‚¹æŒ–æ˜å¹³å‡ä»…éœ€4ç§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ“¦é™¤æ–¹æ³•ä¸»è¦ä¾èµ–å›ºå®šé”šç‚¹ç­–ç•¥ï¼Œå­˜åœ¨æ¦‚å¿µé‡ç°å’Œä¾µèš€çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å› æœè¿½è¸ªå‘ç°æ“¦é™¤å¯¹é”šç‚¹é€‰æ‹©çš„æ•æ„Ÿæ€§ï¼Œæå‡ºâ€œSibling Exclusive Conceptsâ€ä½œä¸ºæ›´é«˜çº§çš„é”šç‚¹ç±»åˆ«ã€‚</li>
<li>æå‡ºäº†SELECTåŠ¨æ€é”šç‚¹é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå›ºå®šé”šç‚¹çš„å±€é™æ€§ã€‚</li>
<li>SELECTå¼•å…¥ä¸¤é˜¶æ®µè¯„ä¼°æœºåˆ¶ï¼Œè‡ªåŠ¨å‘ç°æœ€ä½³æ“¦é™¤é”šç‚¹å¹¶è¯†åˆ«å…³é”®è¾¹ç•Œé”šç‚¹ä»¥ä¿ç•™ç›¸å…³æ¦‚å¿µã€‚</li>
<li>SELECTä½œä¸ºé€šç”¨é”šç‚¹è§£å†³æ–¹æ¡ˆï¼Œå¯é€‚åº”å¤šç§æ“¦é™¤æ¡†æ¶ã€‚</li>
<li>åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šï¼ŒSELECTæŒç»­è¶…è¶Šç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4d71cb7c99b6468bca209cdf41080b0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081516&auth_key=1761081516-0-0-04f077fedec6a6a938ebe9f9d7f748cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7e41301ed16a90e0a0c0cf7a292334f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081523&auth_key=1761081523-0-0-3a8297c85dfb94b0db4a9f260980df78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7c4180182000fdadc7bb64c7dc1b57f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081529&auth_key=1761081529-0-0-ff68f9ceee333855f4b773ff186927fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d26171ff390e44bc3bc4142cc9f0133~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081536&auth_key=1761081536-0-0-54d5bd753032f0d8eaed92f4d513d13b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b25c071a3e8d1c74aa47f3fbbae7f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081543&auth_key=1761081543-0-0-304619b706e46e0c282926b2a89e48c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b4640e66b3e55996d9033dd092b4217~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081549&auth_key=1761081549-0-0-7deff8664cd98e264bef1b0c2d2678cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d432778b15b3698a0a3818b57779cd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081556&auth_key=1761081556-0-0-507ea3c56e263ac29e83b5363bce4f35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scale-DiT-Ultra-High-Resolution-Image-Generation-with-Hierarchical-Local-Attention"><a href="#Scale-DiT-Ultra-High-Resolution-Image-Generation-with-Hierarchical-Local-Attention" class="headerlink" title="Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical   Local Attention"></a>Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical   Local Attention</h2><p><strong>Authors:Yuyao Zhang, Yu-Wing Tai</strong></p>
<p>Ultra-high-resolution text-to-image generation demands both fine-grained texture synthesis and globally coherent structure, yet current diffusion models remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive quadratic complexity of attention and the scarcity of native $4K$ training data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces hierarchical local attention with low-resolution global guidance, enabling efficient, scalable, and semantically coherent image synthesis at ultra-high resolutions. Specifically, high-resolution latents are divided into fixed-size local windows to reduce attention complexity from quadratic to near-linear, while a low-resolution latent equipped with scaled positional anchors injects global semantics. A lightweight LoRA adaptation bridges global and local pathways during denoising, ensuring consistency across structure and detail. To maximize inference efficiency, we repermute token sequence in Hilbert curve order and implement a fused-kernel for skipping masked operations, resulting in a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT achieves more than $2\times$ faster inference and lower memory usage compared to dense attention baselines, while reliably scaling to $4K \times 4K$ resolution without requiring additional high-resolution training data. On both quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons, Scale-DiT delivers superior global coherence and sharper local detail, matching or outperforming state-of-the-art methods that rely on native 4K training. Taken together, these results highlight hierarchical local attention with guided low-resolution anchors as a promising and effective approach for advancing ultra-high-resolution image generation. </p>
<blockquote>
<p>è¶…é«˜åˆ†è¾¨ç‡çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆéœ€è¦ç²¾ç»†çš„çº¹ç†åˆæˆå’Œå…¨å±€ä¸€è‡´çš„ç»“æ„ï¼Œç„¶è€Œï¼Œç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§è¿‡é«˜å’ŒåŸç”Ÿ4Kè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œå½“å‰çš„æ‰©æ•£æ¨¡å‹ä»ç„¶å—é™äºä½äº1KÃ—1Kçš„åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ‰©æ•£æ¡†æ¶<strong>Scale-DiT</strong>ï¼Œå®ƒå¼•å…¥äº†åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›ä¸ä½åˆ†è¾¨ç‡å…¨å±€å¼•å¯¼ï¼Œèƒ½å¤Ÿåœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹å®ç°é«˜æ•ˆã€å¯æ‰©å±•å’Œè¯­ä¹‰è¿è´¯çš„å›¾åƒåˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œé«˜åˆ†è¾¨ç‡æ½œåœ¨ç©ºé—´è¢«åˆ†å‰²æˆå›ºå®šå¤§å°çš„å±€éƒ¨çª—å£ï¼Œå°†æ³¨æ„åŠ›çš„å¤æ‚æ€§ä»äºŒæ¬¡é™ä½åˆ°æ¥è¿‘çº¿æ€§ï¼Œè€Œé…å¤‡ç¼©æ”¾ä½ç½®é”šç‚¹çš„ä½åˆ†è¾¨ç‡æ½œåœ¨ç©ºé—´æ³¨å…¥äº†å…¨å±€è¯­ä¹‰ã€‚åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªè½»é‡çº§çš„LoRAé€‚é…å™¨æ¡¥æ¥äº†å…¨å±€å’Œå±€éƒ¨è·¯å¾„ï¼Œç¡®ä¿äº†ç»“æ„å’Œç»†èŠ‚ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–æ¨ç†æ•ˆç‡ï¼Œæˆ‘ä»¬æŒ‰ç…§Hilbertæ›²çº¿çš„é¡ºåºé‡æ–°æ’åˆ—äº†ä»¤ç‰Œåºåˆ—ï¼Œå¹¶å®ç°äº†èåˆå†…æ ¸ä»¥è·³è¿‡å±è”½æ“ä½œï¼Œä»è€Œå½¢æˆäº†å¯¹GPUå‹å¥½çš„è®¾è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å¯†é›†æ³¨æ„åŠ›åŸºçº¿ç›¸æ¯”ï¼ŒScale-DiTå®ç°äº†è¶…è¿‡2å€çš„æ¨ç†é€Ÿåº¦æå‡å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼Œå¹¶ä¸”å¯é åœ°æ‰©å±•åˆ°4KÃ—4Kåˆ†è¾¨ç‡ï¼Œè€Œæ— éœ€é¢å¤–çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®ã€‚åœ¨å®šé‡åŸºå‡†æµ‹è¯•ï¼ˆFIDã€ISã€CLIPåˆ†æ•°ï¼‰å’Œå®šæ€§æ¯”è¾ƒä¸­ï¼ŒScale-DiTå±•ç°å‡ºå“è¶Šçš„å…¨å±€è¿è´¯æ€§å’Œé”åˆ©çš„å±€éƒ¨ç»†èŠ‚ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šä¾èµ–åŸç”Ÿ4Kè®­ç»ƒæ•°æ®çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™äº›ç»“æœå…±åŒçªæ˜¾äº†åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›ä¸å¼•å¯¼çš„ä½åˆ†è¾¨ç‡é”šç‚¹ä½œä¸ºæ¨è¿›è¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ä¸€ç§æœ‰å‰é€”ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16325v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è¶…é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„éœ€æ±‚ï¼Œç°æœ‰æ‰©æ•£æ¨¡å‹å­˜åœ¨è®¡ç®—å¤æ‚åº¦é«˜ä¸ç¼ºä¹é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºScale-DiTæ¨¡å‹ï¼Œé‡‡ç”¨åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›ä¸ä½åˆ†è¾¨ç‡å…¨å±€å¼•å¯¼ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆã€å¯ä¼¸ç¼©å’Œè¯­ä¹‰è¿è´¯çš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆã€‚é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ä¸å¼•å…¥LoRAé€‚åº”ç­–ç•¥ï¼ŒScale-DiTåœ¨æ¨ç†é€Ÿåº¦ä¸å†…å­˜ä½¿ç”¨ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸæ‰©å±•åˆ°$4K \times 4K$åˆ†è¾¨ç‡ï¼Œä¸”æ— éœ€é¢å¤–çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®ã€‚åœ¨è¯„ä¼°æŒ‡æ ‡å’Œå®é™…å¯¹æ¯”ä¸­ï¼ŒScale-DiTå±•ç°å‡ºè‰²çš„å…¨å±€è¿è´¯æ€§ä¸å±€éƒ¨ç»†èŠ‚æ¸…æ™°åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ‰©æ•£æ¨¡å‹å—é™äºå­$1K \times 1K$åˆ†è¾¨ç‡ï¼Œé¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦å’Œç¼ºä¹é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®æŒ‘æˆ˜ã€‚</li>
<li>Scale-DiTæ¨¡å‹å¼•å…¥åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å°†é«˜åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºåˆ†æˆå›ºå®šå¤§å°çš„å±€éƒ¨çª—å£ï¼Œé™ä½æ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚æ€§ã€‚</li>
<li>ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºç»“åˆç¼©æ”¾ä½ç½®é”šç‚¹ï¼Œæ³¨å…¥å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>LoRAé€‚åº”ç­–ç•¥ç¡®ä¿å…¨å±€å’Œå±€éƒ¨è·¯å¾„åœ¨é™å™ªè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡Hilbertæ›²çº¿é¡ºåºé‡æ–°æ’åˆ—ä»¤ç‰Œåºåˆ—ï¼Œå¹¶å®æ–½èåˆå†…æ ¸è·³è¿‡æ©ç æ“ä½œï¼Œæœ€å¤§åŒ–æ¨ç†æ•ˆç‡ã€‚</li>
<li>Scale-DiTå®ç°äº†è¶…è¿‡ä¸¤å€äºå¯†é›†æ³¨æ„åŠ›åŸºå‡†çº¿çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚</li>
<li>Scale-DiTèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æ‰©å±•åˆ°$4K \times 4K$åˆ†è¾¨ç‡ï¼Œå¹¶åœ¨å®šé‡è¯„ä¼°å’Œå®šæ€§å¯¹æ¯”ä¸­è¡¨ç°å‡ºå“è¶Šçš„å…¨å±€è¿è´¯æ€§å’Œå±€éƒ¨ç»†èŠ‚æ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-72dc0e86ea31f55f7b202d322df48127~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081563&auth_key=1761081563-0-0-ac62f9dc36fe1e06464de709a288f5f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d2ee20f77b3d20ac080651b48d2e228~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081570&auth_key=1761081570-0-0-99436c02d7a89c60b896d252ef58f381&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a01edb250b5e08be37ae028afb23810~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081577&auth_key=1761081577-0-0-c414ad7665df8b4233ccae2b2f740b81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Time-Embedded-Algorithm-Unrolling-for-Computational-MRI"><a href="#Time-Embedded-Algorithm-Unrolling-for-Computational-MRI" class="headerlink" title="Time-Embedded Algorithm Unrolling for Computational MRI"></a>Time-Embedded Algorithm Unrolling for Computational MRI</h2><p><strong>Authors:Junno Yun, YaÅŸar Utku AlÃ§alar, Mehmet AkÃ§akaya</strong></p>
<p>Algorithm unrolling methods have proven powerful for solving the regularized least squares problem in computational magnetic resonance imaging (MRI). These approaches unfold an iterative algorithm with a fixed number of iterations, typically alternating between a neural network-based proximal operator for regularization, a data fidelity operation and auxiliary updates with learnable parameters. While the connection to optimization methods dictate that the proximal operator network should be shared across unrolls, this can introduce artifacts or blurring. Heuristically, practitioners have shown that using distinct networks may be beneficial, but this significantly increases the number of learnable parameters, making it challenging to prevent overfitting. To address these shortcomings, by taking inspirations from proximal operators with varying thresholds in approximate message passing (AMP) and the success of time-embedding in diffusion models, we propose a time-embedded algorithm unrolling scheme for inverse problems. Specifically, we introduce a novel perspective on the iteration-dependent proximal operation in vector AMP (VAMP) and the subsequent Onsager correction in the context of algorithm unrolling, framing them as a time-embedded neural network. Similarly, the scalar weights in the data fidelity operation and its associated Onsager correction are cast as time-dependent learnable parameters. Our extensive experiments on the fastMRI dataset, spanning various acceleration rates and datasets, demonstrate that our method effectively reduces aliasing artifacts and mitigates noise amplification, achieving state-of-the-art performance. Furthermore, we show that our time-embedding strategy extends to existing algorithm unrolling approaches, enhancing reconstruction quality without increasing the computational complexity significantly. </p>
<blockquote>
<p>åœ¨æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„è®¡ç®—ä¸­ï¼Œç®—æ³•å±•å¼€æ–¹æ³•å·²è¢«è¯æ˜åœ¨è§£å†³æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ä¸Šéå¸¸æœ‰æ•ˆã€‚è¿™äº›æ–¹æ³•å°†ä¸€ä¸ªè¿­ä»£ç®—æ³•å±•å¼€ä¸ºä¸€ä¸ªå›ºå®šçš„è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸åœ¨åŸºäºç¥ç»ç½‘ç»œçš„æ­£åˆ™åŒ–è¿‘ç«¯ç®—å­ã€æ•°æ®ä¿çœŸæ“ä½œå’Œå¸¦æœ‰å¯å­¦ä¹ å‚æ•°çš„è¾…åŠ©æ›´æ–°ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚è™½ç„¶ä¸ä¼˜åŒ–æ–¹æ³•çš„è”ç³»è¡¨æ˜è¿‘ç«¯ç®—å­ç½‘ç»œåº”è¯¥åœ¨å±•å¼€è¿‡ç¨‹ä¸­ä¿æŒå…±äº«ï¼Œä½†è¿™æ ·åšå¯èƒ½ä¼šå¼•å…¥ä¼ªå½±æˆ–æ¨¡ç³Šã€‚å¯å‘å¼åœ°ï¼Œä»ä¸šè€…å·²ç»è¯æ˜ä½¿ç”¨ä¸åŒçš„ç½‘ç»œå¯èƒ½æœ‰ç›Šï¼Œä½†è¿™ä¼šå¤§å¤§å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ï¼Œä»è€Œéš¾ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬ä»è¿‘ä¼¼æ¶ˆæ¯ä¼ é€’ï¼ˆAMPï¼‰ä¸­çš„ä¸åŒé˜ˆå€¼çš„è¿‘ç«¯ç®—å­å’Œæ‰©æ•£æ¨¡å‹ä¸­æ—¶é—´åµŒå…¥çš„æˆåŠŸä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹é€†é—®é¢˜çš„æ—¶é—´åµŒå…¥ç®—æ³•å±•å¼€æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‘é‡AMPï¼ˆVAMPï¼‰ä¸­è¿­ä»£ç›¸å…³çš„è¿‘ç«¯æ“ä½œå’Œç®—æ³•å±•å¼€ä¸Šä¸‹æ–‡ä¸­éšåçš„Onsageræ ¡æ­£çš„æ–°è§†è§’ï¼Œå°†å…¶æ„å»ºä¸ºæ—¶é—´åµŒå…¥ç¥ç»ç½‘ç»œã€‚åŒæ ·ï¼Œæ•°æ®ä¿çœŸæ“ä½œä¸­çš„æ ‡é‡æƒé‡åŠå…¶ç›¸å…³çš„Onsageræ ¡æ­£è¢«è½¬æ¢ä¸ºæ—¶é—´ç›¸å…³çš„å¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å„ç§åŠ é€Ÿç‡å’Œæ•°æ®é›†çš„fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†æ··å ä¼ªå½±å¹¶å‡è½»äº†å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ—¶é—´åµŒå…¥ç­–ç•¥å¯ä»¥æ‰©å±•åˆ°ç°æœ‰çš„ç®—æ³•å±•å¼€æ–¹æ³•ï¼Œåœ¨æé«˜é‡å»ºè´¨é‡çš„åŒæ—¶ä¸ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16321v1">PDF</a> Neural Information Processing Systems (NeurIPS), 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è®¡ç®—ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ï¼Œç®—æ³•å±•å¼€æ–¹æ³•å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æ—¶é—´åµŒå…¥çš„ç®—æ³•å±•å¼€æ–¹æ¡ˆï¼Œè§£å†³äº†ä½¿ç”¨å•ä¸€ç½‘ç»œå¸¦æ¥çš„ä¼ªå½±æˆ–æ¨¡ç³Šé—®é¢˜ï¼Œä»¥åŠä½¿ç”¨å¤šä¸ªç½‘ç»œå¸¦æ¥çš„å‚æ•°è¿‡å¤šã€æ˜“è¿‡æ‹Ÿåˆçš„æŒ‘æˆ˜ã€‚å—è¿‘ä¼¼æ¶ˆæ¯ä¼ é€’ï¼ˆAMPï¼‰ä¸­çš„ä¸åŒé˜ˆå€¼è¿‘ç«¯ç®—ç¬¦å’Œæ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´åµŒå…¥æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ—¶é—´åµŒå…¥ç®—æ³•å±•å¼€æ–¹æ¡ˆï¼Œç”¨äºè§£å†³åé—®é¢˜ã€‚åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†ä¼ªå½±å’Œå™ªå£°æ”¾å¤§ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ—¶é—´åµŒå…¥ç­–ç•¥å¯ä»¥æ‰©å±•åˆ°ç°æœ‰çš„ç®—æ³•å±•å¼€æ–¹æ³•ï¼Œåœ¨æé«˜é‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œä¸ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç®—æ³•å±•å¼€æ–¹æ³•åœ¨è§£å†³è®¡ç®—ç£å…±æŒ¯æˆåƒä¸­çš„æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ä¸Šå…·æœ‰æœ‰æ•ˆæ€§ã€‚</li>
<li>ä½¿ç”¨å•ä¸€ç½‘ç»œåœ¨ç®—æ³•å±•å¼€ä¸­å¯èƒ½å¯¼è‡´ä¼ªå½±æˆ–æ¨¡ç³Šï¼Œè€Œä½¿ç”¨å¤šä¸ªç½‘ç»œåˆ™é¢ä¸´å‚æ•°è¿‡å¤šå’Œæ˜“è¿‡æ‹Ÿåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ—¶é—´åµŒå…¥çš„ç®—æ³•å±•å¼€æ–¹æ¡ˆï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å—è¿‘ä¼¼æ¶ˆæ¯ä¼ é€’å’Œæ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´åµŒå…¥æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ—¶é—´åµŒå…¥ç®—æ³•å±•å¼€æ–¹æ¡ˆç”¨äºè§£å†³åé—®é¢˜ã€‚</li>
<li>åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†ä¼ªå½±å’Œå™ªå£°æ”¾å¤§ï¼Œå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ—¶é—´åµŒå…¥ç­–ç•¥å¯ä»¥æ‰©å±•åˆ°ç°æœ‰çš„ç®—æ³•å±•å¼€æ–¹æ³•ï¼Œæé«˜é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä¸å¢åŠ æ˜¾è‘—çš„è®¡ç®—å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-27b41f99fe5e25d250af2614eb2b6eea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081585&auth_key=1761081585-0-0-d4e61c41dbd4fef721dceed766d03278&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7018024704ef3e5f8d432280fe07de5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081592&auth_key=1761081592-0-0-e50fbf514db73d09158c375901a82e6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0f1390004971f8418e954a1d38194f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081599&auth_key=1761081599-0-0-722e6eca4a9576468f9fe823a9bf971a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db9c7674bdce2b7b2ee686d8e3c20db4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081605&auth_key=1761081605-0-0-89bc56fd4d6fb0c4f316c5d292981948&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4ed6f1317151d545219e9470a36534c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081612&auth_key=1761081612-0-0-f7f374c4349d8071239a2d4a4e06f5f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer"><a href="#GuideFlow3D-Optimization-Guided-Rectified-Flow-For-Appearance-Transfer" class="headerlink" title="GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer"></a>GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer</h2><p><strong>Authors:Sayan Deb Sarkar, Sinisa Stekovic, Vincent Lepetit, Iro Armeni</strong></p>
<p>Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions. </p>
<blockquote>
<p>ä½¿ç”¨ä¸åŒè¡¨ç°å½¢å¼ï¼ˆå¦‚å›¾åƒæˆ–æ–‡æœ¬ï¼‰å°†å¤–è§‚è½¬ç§»åˆ°3Dèµ„äº§ä¸Šï¼Œå› å…¶åœ¨æ¸¸æˆã€å¢å¼ºç°å®å’Œæ•°å­—å†…å®¹åˆ›å»ºç­‰è¡Œä¸šçš„å¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆè¿›æŠ€æœ¯ä»éš¾ä»¥å¤„ç†è¾“å…¥å¯¹è±¡ä¸å¤–è§‚å¯¹è±¡ä¹‹é—´å‡ ä½•å½¢çŠ¶å·®å¼‚è¾ƒå¤§çš„æƒ…å†µã€‚ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯ç›´æ¥åº”ç”¨3Dç”Ÿæˆæ¨¡å‹ï¼Œä½†æˆ‘ä»¬è¯æ˜è¿™æœ€ç»ˆæ— æ³•äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—é€šç”¨æŒ‡å¯¼å¯å‘çš„æœ‰åŸåˆ™çš„æ–¹æ³•ã€‚ç»™å®šä»¥å›¾åƒæˆ–æ–‡æœ¬ä¸ºæ¡ä»¶çš„é¢„è®­ç»ƒæ ¡æ­£æµæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•ä¸é‡‡æ ·è¿‡ç¨‹è¿›è¡Œäº¤äº’ï¼Œå®šæœŸæ·»åŠ æŒ‡å¯¼ã€‚è¿™ç§æŒ‡å¯¼å¯ä»¥å»ºæ¨¡ä¸ºå¯å¾®åˆ†çš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å°è¯•äº†ä¸¤ç§ä¸åŒçš„æŒ‡å¯¼ç±»å‹ï¼ŒåŒ…æ‹¬ç”¨äºå¤–è§‚çš„éƒ¨åˆ†æ„ŸçŸ¥æŸå¤±å’Œè‡ªç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†çº¹ç†å’Œå‡ ä½•ç»†èŠ‚è½¬ç§»åˆ°è¾“å…¥çš„3Dèµ„äº§ä¸Šï¼Œåœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œç”±äºä¼ ç»ŸæŒ‡æ ‡æ— æ³•å…³æ³¨å±€éƒ¨ç»†èŠ‚å¹¶åœ¨æ²¡æœ‰çœŸå®æ•°æ®çš„æƒ…å†µä¸‹æ¯”è¾ƒä¸ç›¸ä¼¼çš„è¾“å…¥ï¼Œå› æ­¤å®ƒä»¬ä¸é€‚åˆç”¨äºè¯„ä¼°æ­¤ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºGPTçš„ç³»ç»Ÿå®¢è§‚åœ°è¯„ä¼°å¤–è§‚è½¬ç§»è´¨é‡ï¼Œå¯¹è¾“å‡ºè¿›è¡Œæ’åï¼Œç¡®ä¿è¯„ä¼°å…·æœ‰ç¨³å¥æ€§å’Œäººç±»ç‰¹å¾ï¼Œæˆ‘ä»¬çš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†è¿™ä¸€ç‚¹ã€‚é™¤äº†å±•ç¤ºçš„åœºæ™¯å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯æ‰©å±•åˆ°ä¸åŒç±»å‹çš„æ‰©æ•£æ¨¡å‹å’Œå¼•å¯¼åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16136v1">PDF</a> NeurIPS 2025. Project Page: <a target="_blank" rel="noopener" href="https://sayands.github.io/guideflow3d/">https://sayands.github.io/guideflow3d/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé€šç”¨æŒ‡å¯¼åŸç†çš„æ–¹æ³•ï¼Œç”¨äºå°†å›¾åƒæˆ–æ–‡æœ¬ç­‰ä¸åŒè¡¨ç°å½¢å¼ä¸‹çš„å¤–è§‚å¯¹è±¡è½¬ç§»åˆ°3Dèµ„äº§ä¸Šã€‚æ­¤æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œé€šè¿‡åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å®šæœŸæ·»åŠ æŒ‡å¯¼æ¥ä¸é¢„è®­ç»ƒçš„ä¿®æ­£æµæ¨¡å‹äº¤äº’ã€‚é€šè¿‡éƒ¨åˆ†æ„ŸçŸ¥æŸå¤±å’Œè‡ªç›¸ä¼¼æ€§æŒ‡å¯¼ç­‰æ–¹æ³•ï¼Œå®ç°äº†çº¹ç†å’Œå‡ ä½•ç»†èŠ‚çš„è½¬ç§»ï¼Œåœ¨3Dèµ„äº§ä¸Šå–å¾—äº†è¶…è¶ŠåŸºå‡†çº¿çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œç”±äºä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡æ— æ³•ä¸“æ³¨äºå±€éƒ¨ç»†èŠ‚å’Œä¸ä¾èµ–äºçœŸå®æ•°æ®çš„ç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œæœ¬æ–‡é‡‡ç”¨GPTç³»ç»Ÿå®¢è§‚è¯„ä¼°å¤–è§‚è½¬ç§»è´¨é‡ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯å…¶ç¨³å¥æ€§å’Œäººæ€§åŒ–è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯æ‰©å±•åˆ°ä¸åŒç±»å‹çš„æ‰©æ•£æ¨¡å‹å’ŒæŒ‡å¯¼å‡½æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºé€šç”¨æŒ‡å¯¼åŸç†çš„æ–¹æ³•ï¼Œç”¨äºå°†å¤–è§‚å¯¹è±¡è½¬ç§»åˆ°3Dèµ„äº§ä¸Šã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„ä¿®æ­£æµæ¨¡å‹å®ç°è¿™ä¸€è½¬ç§»ï¼Œæ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒå½¢å¼çš„å¤–è§‚å¯¹è±¡è¡¨ç¤ºï¼Œå¦‚å›¾åƒæˆ–æ–‡æœ¬ã€‚</li>
<li>é‡‡ç”¨æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å®šæœŸæ·»åŠ æŒ‡å¯¼ä¸é‡‡æ ·è¿‡ç¨‹è¿›è¡Œäº¤äº’ã€‚</li>
<li>å®ç°çº¹ç†å’Œå‡ ä½•ç»†èŠ‚çš„è½¬ç§»ï¼Œå¹¶åœ¨å®éªŒä¸Šè¯æ˜è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰åŸºå‡†çº¿ã€‚</li>
<li>æŒ‡å‡ºä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡åœ¨è¯„ä¼°å¤–è§‚è½¬ç§»ä»»åŠ¡æ—¶çš„ä¸è¶³ï¼Œå¹¶å¼•å…¥GPTç³»ç»Ÿä½œä¸ºå®¢è§‚è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†è¯„ä¼°æ–¹æ³•çš„ç¨³å¥æ€§å’Œäººæ€§åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7b0b158fec0cbe509b84830a83b481e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081619&auth_key=1761081619-0-0-0a980d73287ce8aa4af4940b4af3ceb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f89c0d951fa4785130a7bcabec2eee67~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081627&auth_key=1761081627-0-0-8a5c67e344932a4469bf001759e175b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e7737c3b4b7b3bb7babf3fd182335e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081633&auth_key=1761081633-0-0-4408673f7edafaf40d2ba6fffe7b0af6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58c6285970b308dbbd2b981bb3bd016a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081640&auth_key=1761081640-0-0-2507e975ef36e33bcbb46eaf69b3434f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00b77257710f45ba22f98d8a91825c2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081647&auth_key=1761081647-0-0-2144022e78d18c1cce65705707d03ae8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BokehDiff-Neural-Lens-Blur-with-One-Step-Diffusion"><a href="#BokehDiff-Neural-Lens-Blur-with-One-Step-Diffusion" class="headerlink" title="BokehDiff: Neural Lens Blur with One-Step Diffusion"></a>BokehDiff: Neural Lens Blur with One-Step Diffusion</h2><p><strong>Authors:Chengxuan Zhu, Qingnan Fan, Qi Zhang, Jinwei Chen, Huaqi Zhang, Chao Xu, Boxin Shi</strong></p>
<p>We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†BokehDiffï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é•œå¤´æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ï¼Œå€ŸåŠ©ç”Ÿæˆæ‰©æ•£å…ˆéªŒå®ç°ç‰©ç†ä¸Šå‡†ç¡®å’Œè§†è§‰ä¸Šå¸å¼•äººçš„æ•ˆæœã€‚ä¹‹å‰çš„æ–¹æ³•å—é™äºæ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œä¼šåœ¨æ·±åº¦ä¸è¿ç»­çš„åœ°æ–¹äº§ç”Ÿä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç‰©ç†å¯å‘å¼çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä¸å›¾åƒå½¢æˆè¿‡ç¨‹ç›¸ä¸€è‡´ï¼Œèå…¥æ·±åº¦ç›¸å…³çš„åœ†åœˆæ¨¡ç³Šçº¦æŸå’Œè‡ªé®æŒ¡æ•ˆåº”ã€‚æˆ‘ä»¬è®©æ‰©æ•£æ¨¡å‹é€‚åº”ä¸€æ­¥æ¨ç†æ–¹æ¡ˆï¼Œæ²¡æœ‰å¼•å…¥é¢å¤–çš„å™ªå£°ï¼Œå®ç°äº†é«˜è´¨é‡å’Œé«˜ä¿çœŸåº¦çš„ç»“æœã€‚ä¸ºäº†è§£å†³ç¼ºä¹å¯æ‰©å±•çš„é…å¯¹æ•°æ®é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆå…·æœ‰é€æ˜åº¦çš„é€¼çœŸå‰æ™¯ï¼Œåœ¨çœŸå®æ€§å’Œåœºæ™¯å¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18060v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BokehDiffï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é•œå¤´æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ï¼Œå€ŸåŠ©ç”Ÿæˆæ‰©æ•£å…ˆéªŒå®ç°ç‰©ç†å‡†ç¡®å’Œè§†è§‰å¸å¼•äººçš„æ•ˆæœã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç‰©ç†å¯å‘å¼çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä¸å›¾åƒå½¢æˆè¿‡ç¨‹ç›¸ä¸€è‡´ï¼Œèå…¥æ·±åº¦ç›¸å…³çš„åœ†æ¨¡ç³Šçº¦æŸå’Œè‡ªé®æŒ¡æ•ˆåº”ã€‚é€‚åº”æ‰©æ•£æ¨¡å‹è‡³ä¸€æ­¥æ¨æ–­æ–¹æ¡ˆï¼Œæ— éœ€å¼•å…¥é¢å¤–å™ªå£°ï¼Œè¾¾æˆé«˜è´¨é‡å’Œä¿çœŸåº¦çš„ç»“æœã€‚ä¸ºè§£å†³ç¼ºä¹å¯æ‰©å±•é…å¯¹æ•°æ®çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºç”¨æ‰©æ•£æ¨¡å‹åˆæˆå…·æœ‰çœŸå®æ„Ÿå’Œé€æ˜åº¦çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BokehDiffæ˜¯ä¸€ç§æ–°çš„é•œå¤´æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ï¼Œå€ŸåŠ©ç”Ÿæˆæ‰©æ•£å…ˆéªŒå®ç°ç‰©ç†å‡†ç¡®å’Œè§†è§‰å¸å¼•äººçš„æ•ˆæœã€‚</li>
<li>ä¹‹å‰çš„æ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡çš„å‡†ç¡®åº¦ä¸Šæœ‰æ‰€é™åˆ¶ï¼Œä¼šåœ¨æ·±åº¦ä¸è¿ç»­å¤„äº§ç”Ÿä¼ªå½±ã€‚</li>
<li>BokehDiffé‡‡ç”¨ç‰©ç†å¯å‘å¼çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å›¾åƒå½¢æˆè¿‡ç¨‹ç›¸ä¸€è‡´çš„æ–¹å¼å·¥ä½œã€‚</li>
<li>è¯¥æ–¹æ³•èå…¥äº†æ·±åº¦ç›¸å…³çš„åœ†æ¨¡ç³Šçº¦æŸå’Œè‡ªé®æŒ¡æ•ˆåº”ã€‚</li>
<li>BokehDiffé€‚åº”äº†æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†ä¸€æ­¥æ¨æ–­ï¼Œæé«˜äº†ç»“æœçš„è´¨é‡å’Œä¿çœŸåº¦ã€‚</li>
<li>é’ˆå¯¹ç¼ºä¹å¯æ‰©å±•é…å¯¹æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†åˆæˆå…·æœ‰çœŸå®æ„Ÿå’Œé€æ˜åº¦çš„å‰æ™¯çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e4bd6b26b33cce0bc5bb13f4bca96226~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081654&auth_key=1761081654-0-0-5bad0d57aa986eea434e93e0309bf7c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85fda9dfe17231f991a0db48f1898378~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081662&auth_key=1761081662-0-0-5308e1fb6febcaa4e9f0de97e0346aae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa2aebf2c5ac947cb4a4f201e79175e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081668&auth_key=1761081668-0-0-5776a8c6ad2f42046db04c1e1d6d82c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17ef8c40a9c360fa4c5d63997699203f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081675&auth_key=1761081675-0-0-661c319c02d31a8fae7bcdfb6c211850&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model"><a href="#SV-DRR-High-Fidelity-Novel-View-X-Ray-Synthesis-Using-Diffusion-Model" class="headerlink" title="SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model"></a>SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</h2><p><strong>Authors:Chun Xie, Yuichi Yoshii, Itaru Kitahara</strong></p>
<p>X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/xiechun298/SV-DRR">https://github.com/xiechun298/SV-DRR</a>. </p>
<blockquote>
<p>Xå°„çº¿æˆåƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”æˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·ï¼Œç”¨äºå¯è§†åŒ–äººä½“å†…éƒ¨è§£å‰–ç»“æ„ã€‚è™½ç„¶å¤šè§†è§’Xå°„çº¿æˆåƒæä¾›äº†å¢å¼ºè¯Šæ–­ã€å¹²é¢„å’Œæ•™è‚²çš„è¡¥å……ä¿¡æ¯ï¼Œä½†ä»å¤šä¸ªè§’åº¦è·å–å›¾åƒä¼šå¢åŠ è¾å°„æš´éœ²å¹¶ä½¿ä¸´åºŠå·¥ä½œæµç¨‹å¤æ‚åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è§†å›¾è°ƒèŠ‚æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»å•ä¸ªè§†è§’åˆæˆå¤šè§†è§’Xå°„çº¿å›¾åƒã€‚ä¸åŒäºåœ¨è§’åº¦èŒƒå›´ã€åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ä¸Šæœ‰æ‰€é™åˆ¶çš„å‰æœŸæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜å‹å™¨æ¥ä¿ç•™ç»†èŠ‚ï¼Œå¹¶é‡‡ç”¨ä»å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥æ¥è¿›è¡Œç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å‡ºï¼Œå¯¹è§‚å¯Ÿè§’åº¦æœ‰æ›´å¥½çš„æ§åˆ¶ã€‚è¿™é¡¹èƒ½åŠ›ä¸ä»…å¯¹äºä¸´åºŠåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ï¼Œè€Œä¸”å¯¹äºåŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•ä¹Ÿå…·æœ‰é‡è¦å½±å“ï¼Œèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œç”¨äºåŸ¹è®­å’Œåˆ†æã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiechun298/SV-DRR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiechun298/SV-DRRæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05148v3">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•è§†è§’X-rayå›¾åƒçš„è§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ç ”ç©¶èƒ½æœ‰æ•ˆè§£å†³å¤šè§†è§’X-rayæˆåƒå¸¦æ¥çš„è¾å°„æš´éœ²å’Œä¸´åºŠå·¥ä½œæµç¨‹å¤æ‚åŒ–çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£å˜å‹å™¨åˆæˆå¤šè§†è§’å›¾åƒï¼Œæ—¢ä¿ç•™äº†ç²¾ç»†çš„ç»†èŠ‚ï¼Œåˆé‡‡ç”¨å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥å®ç°äº†ç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨ä¸´åºŠåº”ç”¨ä¸­æœ‰é‡è¦æ„ä¹‰ï¼Œè¿˜åœ¨åŒ»å­¦æ•™è‚²å’Œæ•°æ®æ‰©å±•æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œæ•°æ®åˆ†æã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-rayæˆåƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”æˆæœ¬æ•ˆç›Šé«˜çš„å†…éƒ¨äººä½“ç»“æ„å¯è§†åŒ–å·¥å…·ã€‚</li>
<li>å¤šè§†è§’X-rayæˆåƒæä¾›äº’è¡¥ä¿¡æ¯ï¼Œæœ‰åŠ©äºè¯Šæ–­ã€æ²»ç–—å’ŒåŒ»å­¦æ•™è‚²ã€‚</li>
<li>å¤šè§’åº¦æˆåƒå¢åŠ äº†è¾å°„æš´éœ²å¹¶å¤æ‚åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§†å›¾æ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»å•ä¸€è§†è§’åˆæˆå¤šè§†è§’X-rayå›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£å˜å‹å™¨è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆï¼Œä¿ç•™äº†ç²¾ç»†çš„ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨å¼±åˆ°å¼ºçš„è®­ç»ƒç­–ç•¥å®ç°äº†ç¨³å®šçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-48d4a87c3735ab9bceacacbfd00644f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081682&auth_key=1761081682-0-0-48d059c04cbd16bce90027ca8dda7d38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-614ab69cf84f28775636f043a20bb99a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081689&auth_key=1761081689-0-0-1139f61712b1bcf0b2c217a45347ae97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5498db6f6b0f7ff65919caac9131018e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081696&auth_key=1761081696-0-0-cdcd75f409a1acfe184c75702f19c6c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Cradle-to-Cane-A-Two-Pass-Framework-for-High-Fidelity-Lifespan-Face-Aging"><a href="#From-Cradle-to-Cane-A-Two-Pass-Framework-for-High-Fidelity-Lifespan-Face-Aging" class="headerlink" title="From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan   Face Aging"></a>From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan   Face Aging</h2><p><strong>Authors:Tao Liu, Dafeng Zhang, Gengchen Li, Shizhuo Liu, Yongqi Song, Senmao Li, Shiqi Yang, Boqian Li, Kai Wang, Yaxing Wang</strong></p>
<p>Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservationâ€“what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency. Code is available at <a target="_blank" rel="noopener" href="https://github.com/byliutao/Cradle2Cane">https://github.com/byliutao/Cradle2Cane</a>. </p>
<blockquote>
<p>é¢éƒ¨è¡°è€å·²æˆä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå…¶åº”ç”¨ä»å¨±ä¹åˆ°åŒ»ç–—ä¿å¥éƒ½æœ‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®ç°æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„çœŸå®æ— ç¼è½¬æ¢æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¾ƒå¤§çš„å¹´é¾„å·®è·æˆ–æç«¯å¤´éƒ¨å§¿åŠ¿æ—¶ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¹³è¡¡å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¿ç•™ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºAge-IDæƒè¡¡ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•è¦ä¹ˆä¼˜å…ˆè€ƒè™‘å¹´é¾„è½¬æ¢è€Œç‰ºç‰²èº«ä»½ä¸€è‡´æ€§ï¼Œè¦ä¹ˆåä¹‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„é¢éƒ¨è¡°è€æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡†æ¶åŸºäºå°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œåä¸ºCradle2Caneã€‚ç¬¬ä¸€é˜¶æ®µä¾§é‡äºè§£å†³å¹´é¾„å‡†ç¡®æ€§é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥è‡ªé€‚åº”å™ªå£°æ³¨å…¥ï¼ˆAdaNIï¼‰æœºåˆ¶ã€‚è¯¥æœºåˆ¶å—ç»™å®šäººç‰©çš„å¹´é¾„å’Œæ€§åˆ«æç¤ºæè¿°çš„å¼•å¯¼ï¼Œä½œä¸ºæ–‡æœ¬æ¡ä»¶ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´å™ªå£°æ°´å¹³ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶è¡°è€çš„å¼ºåº¦ï¼ŒåŒæ—¶ä½¿é¢éƒ¨è½¬æ¢æ›´åŠ çµæ´»ã€‚ä¸è¿‡ï¼Œè¿™é‡Œçš„èº«ä»½ä¿ç•™æ˜¯å¼±ä¿è¯ï¼Œä»¥ä¿ƒè¿›æ›´å¼ºçš„å¹´é¾„è½¬æ¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä½¿æ¨¡å‹ä¾èµ–äºä¸¤ä¸ªèº«ä»½æ„ŸçŸ¥åµŒå…¥ï¼ˆIDEmbï¼‰ï¼šSVR-ArcFaceå’ŒRotate-CLIPï¼Œå¢å¼ºèº«ä»½ä¿ç•™çš„åŒæ—¶ä¿æŒå¹´é¾„ç‰¹å®šç‰¹å¾ã€‚è¿™ä¸€é˜¶æ®µå…è®¸å¯¹ç¬¬ä¸€é˜¶æ®µçš„è½¬æ¢å›¾åƒè¿›è¡Œå»å™ªå¤„ç†ï¼Œç¡®ä¿åœ¨ä¿æŒå¹´é¾„å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºèº«ä»½ä¸€è‡´æ€§ã€‚ä¸¤ä¸ªé˜¶æ®µä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè”åˆè®­ç»ƒã€‚åœ¨CelebA-HQæµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œé€šè¿‡Face++å’ŒQwen-VLåè®®è¿›è¡Œè¯„ä¼°ï¼Œè¡¨æ˜æˆ‘ä»¬çš„Cradle2Caneåœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„é¢éƒ¨è¡°è€æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/byliutao/Cradle2Cane%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/byliutao/Cradle2Caneè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20977v2">PDF</a> 32 pages, 12 figures, NeurIPS 2025 Poster</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å…³æ³¨äººè„¸è¡°è€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤§å¹´é¾„å·®è·æˆ–æç«¯å¤´éƒ¨å§¿æ€ä¸‹çš„è½¬æ¢æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºä¸€ç§åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åŒé˜¶æ®µäººè„¸è¡°è€æ¡†æ¶ï¼ˆCradle2Caneï¼‰ã€‚ç¬¬ä¸€é˜¶æ®µçš„è‡ªé€‚åº”å™ªå£°æ³¨å…¥æœºåˆ¶ç€é‡è§£å†³å¹´é¾„å‡†ç¡®æ€§é—®é¢˜ï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™æ³¨é‡èº«ä»½ä¿ç•™å¹¶ç»´æŠ¤å¹´é¾„ç‰¹å¾ã€‚æœ€ç»ˆå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨CelebA-HQæµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒCradle2Caneåœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººè„¸è¡°è€åœ¨è®¡ç®—æœºè§†è§‰ä¸­æˆä¸ºé‡è¦ä»»åŠ¡ï¼Œæ¶µç›–å¨±ä¹å’ŒåŒ»ç–—ä¿å¥ç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´åœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…å®ç°çœŸå®æ— ç¼è½¬æ¢çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤§å¹´é¾„å·®è·æˆ–æç«¯å¤´éƒ¨å§¿æ€æ—¶ã€‚</li>
<li>æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¹³è¡¡å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¿ç•™ï¼ˆAge-IDæƒè¡¡ï¼‰ã€‚</li>
<li>å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•è¦ä¹ˆä¼˜å…ˆè€ƒè™‘å¹´é¾„è½¬æ¢è€Œå¿½è§†èº«ä»½ä¸€è‡´æ€§ï¼Œåä¹‹äº¦ç„¶ã€‚</li>
<li>æå‡ºçš„Cradle2CaneåŒé˜¶æ®µäººè„¸è¡°è€æ¡†æ¶åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡è‡ªé€‚åº”å™ªå£°æ³¨å…¥æœºåˆ¶è§£å†³å¹´é¾„å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡èº«ä»½æ„ŸçŸ¥åµŒå…¥å¢å¼ºèº«ä»½ä¿ç•™å¹¶ç»´æŠ¤å¹´é¾„ç‰¹å¾ã€‚ç»è¿‡ç«¯å¯¹ç«¯è”åˆè®­ç»ƒçš„å¹¿æ³›å®éªŒéªŒè¯ï¼ŒCradle2Caneåœ¨å¹´é¾„å‡†ç¡®æ€§å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d0964c4610eeac8c9522b2bfe67e0ebd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081704&auth_key=1761081704-0-0-7982600e074e51883fb5edec1cdfd2ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed62239351db7696ff4afc7ec60e6f4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081711&auth_key=1761081711-0-0-439f15aa36ec32d03576b5f26ae4ccdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bbaeafaffca959bf1b6b38f1fd47670~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081718&auth_key=1761081718-0-0-491ba11df3ff22c82ed46f8a5a113fcc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01b883434d03900ea84928adf5be6bd9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081724&auth_key=1761081724-0-0-6b6cd955bf6b7172b6ef0db24a6e5639&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates"><a href="#OSCAR-One-Step-Diffusion-Codec-Across-Multiple-Bit-rates" class="headerlink" title="OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates"></a>OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates</h2><p><strong>Authors:Jinpei Guo, Yifei Ji, Zheng Chen, Kai Liu, Min Liu, Wang Rao, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR">https://github.com/jp-guo/OSCAR</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æœ‰æŸå›¾åƒå‹ç¼©æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œè¿™å¾—ç›Šäºå…¶å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒï¼Œç”±å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºæ‰€å¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•è¾¾åˆ°äº†è¾ƒé«˜çš„é‡å»ºè´¨é‡ï¼Œä½†å®ƒä»¬çš„å¤šæ­¥é‡‡æ ·è¿‡ç¨‹äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ä¸åŒçš„å‹ç¼©æ¯”ç‰¹ç‡è®­ç»ƒä¸åŒçš„æ¨¡å‹ï¼Œå¯¼è‡´åŸ¹è®­å’Œå­˜å‚¨æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å¤šä¸ªæ¯”ç‰¹ç‡çš„æ‰©æ•£ç¼–è§£ç å™¨çš„ä¸€æ­¥å¼ç¼–ç è§£ç ç³»ç»Ÿï¼ˆOSCARï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å‹ç¼©çš„æ½œåœ¨å€¼è§†ä¸ºåŸå§‹æ½œåœ¨å€¼çš„å™ªå£°ç‰ˆæœ¬ï¼Œå…¶å¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ã€‚è¿™ä¸ªè§’åº¦å…è®¸å°†å®ƒä»¬å»ºæ¨¡ä¸ºæ²¿æ‰©æ•£è½¨è¿¹çš„ä¸­é—´çŠ¶æ€ã€‚é€šè¿‡å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥é•¿çš„æ˜ å°„ï¼Œæˆ‘ä»¬ä½¿å•ä¸ªç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šä¸ªæ¯”ç‰¹ç‡çš„é‡å»ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºå‹ç¼©çš„æ½œåœ¨å€¼ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œä½¿ä¸€æ­¥å»å™ªæˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼ŒOSCARç”¨ä¸€æ¬¡å»å™ªè¿‡ç¨‹å–ä»£äº†è¿­ä»£é‡‡æ ·ï¼Œå¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jp-guo/OSCAR%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jp-guo/OSCARä¸­è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16091v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ½œä¼æ‰©æ•£æ¨¡å‹åœ¨æœ‰æŸå›¾åƒå‹ç¼©æ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œå…¶åŸºäºå¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚ç°æœ‰çš„æ‰©æ•£æ–¹æ³•å¤§å¤šé€šè¿‡è¿­ä»£å»å™ªä»éšæœºå™ªå£°ä¸­é‡å»ºå›¾åƒï¼Œç”±å‹ç¼©æ½œä¼è¡¨ç¤ºä¸ºå¼•å¯¼ã€‚è™½ç„¶è¿™äº›æ–¹æ³•é‡å»ºè´¨é‡é«˜ï¼Œä½†å…¶å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å¸¦æ¥äº†è¾ƒå¤§çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”é€šå¸¸éœ€é’ˆå¯¹ä¸åŒå‹ç¼©æ¯”ç‰¹ç‡è®­ç»ƒå•ç‹¬æ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒå’Œå­˜å‚¨æˆæœ¬æ˜¾è‘—ä¸Šå‡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºè·¨å¤šæ¯”ç‰¹ç‡çš„å•æ­¥æ‰©æ•£ç¼–è§£ç å™¨OSCARã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å‹ç¼©æ½œä¼è§†ä¸ºåŸå§‹æ½œä¼çš„å™ªå£°å˜ä½“ï¼Œå…¶å¤±çœŸç¨‹åº¦å–å†³äºæ¯”ç‰¹ç‡ï¼Œå°†å…¶å»ºæ¨¡ä¸ºæ‰©æ•£è½¨è¿¹çš„ä¸­é—´çŠ¶æ€ã€‚é€šè¿‡å»ºç«‹ä»å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥çš„æ˜ å°„ï¼Œæˆ‘ä»¬ä½¿å•ä¸€ç”Ÿæˆæ¨¡å‹æ”¯æŒå¤šæ¯”ç‰¹ç‡çš„é‡å»ºã€‚æˆ‘ä»¬è®¤ä¸ºå‹ç¼©æ½œä¼ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä½¿å¾—ä¸€æ­¥å»å™ªæˆä¸ºå¯èƒ½ã€‚å› æ­¤ï¼ŒOSCARç”¨å•ä¸ªå»å™ªé€šé“å–ä»£äº†è¿­ä»£é‡‡æ ·ï¼Œå¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒè¯æ˜OSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡å®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ½œä¼æ‰©æ•£æ¨¡å‹åœ¨æœ‰æŸå›¾åƒå‹ç¼©æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•è™½ç„¶é‡å»ºè´¨é‡é«˜ï¼Œä½†å­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€éœ€é’ˆå¯¹å¤šç§æ¯”ç‰¹ç‡è®­ç»ƒå•ç‹¬æ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>OSCARæå‡ºè·¨å¤šæ¯”ç‰¹ç‡çš„å•æ­¥æ‰©æ•£ç¼–è§£ç å™¨æ–¹æ³•ï¼Œç®€åŒ–äº†è®¡ç®—è¿‡ç¨‹ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>OSCARå°†å‹ç¼©æ½œä¼è§†ä¸ºå¸¦æœ‰å™ªå£°çš„åŸå§‹æ½œä¼çŠ¶æ€ï¼Œå¹¶é€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œå»å™ªå¤„ç†ã€‚</li>
<li>OSCARé€šè¿‡æ˜ å°„å‹ç¼©æ¯”ç‰¹ç‡åˆ°ä¼ªæ‰©æ•£æ—¶é—´æ­¥ï¼Œå®ç°å•ä¸€æ¨¡å‹æ”¯æŒå¤šç§æ¯”ç‰¹ç‡çš„å›¾åƒé‡å»ºã€‚</li>
<li>å‹ç¼©æ½œä¼ä¿ç•™äº†ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä½¿å¾—ä¸€æ­¥å»å™ªæˆä¸ºå¯èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜OSCARåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a23ea585fe12b088743c602d1ec941f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081732&auth_key=1761081732-0-0-3c6a32b85b1ffadce6293c5fbd87dcb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9dffba3bc5b34dbe3db1e6404f0b3d3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081739&auth_key=1761081739-0-0-a0cef74c2a5ac10efa5ed22d3b3c0acc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e173338844d6637d082fbd91b65e1088~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081746&auth_key=1761081746-0-0-fec38681f87efd0968111c1c44f0ae63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4482dbb65fb8920d513a03d6ba8f0ccf~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081752&auth_key=1761081752-0-0-6b54e67049a3fee81ef422c67719f5a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1dbe29fa0cbdd02f0a42bd930d50a1b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081759&auth_key=1761081759-0-0-009c7f351095b24538e24999744019ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Is-Artificial-Intelligence-Generated-Image-Detection-a-Solved-Problem"><a href="#Is-Artificial-Intelligence-Generated-Image-Detection-a-Solved-Problem" class="headerlink" title="Is Artificial Intelligence Generated Image Detection a Solved Problem?"></a>Is Artificial Intelligence Generated Image Detection a Solved Problem?</h2><p><strong>Authors:Ziqiang Li, Jiazhen Yan, Ziwen He, Kai Zeng, Weiwei Jiang, Lizhi Xiong, Zhangjie Fu</strong></p>
<p>The rapid advancement of generative models, such as GANs and Diffusion models, has enabled the creation of highly realistic synthetic images, raising serious concerns about misinformation, deepfakes, and copyright infringement. Although numerous Artificial Intelligence Generated Image (AIGI) detectors have been proposed, often reporting high accuracy, their effectiveness in real-world scenarios remains questionable. To bridge this gap, we introduce AIGIBench, a comprehensive benchmark designed to rigorously evaluate the robustness and generalization capabilities of state-of-the-art AIGI detectors. AIGIBench simulates real-world challenges through four core tasks: multi-source generalization, robustness to image degradation, sensitivity to data augmentation, and impact of test-time pre-processing. It includes 23 diverse fake image subsets that span both advanced and widely adopted image generation techniques, along with real-world samples collected from social media and AI art platforms. Extensive experiments on 11 advanced detectors demonstrate that, despite their high reported accuracy in controlled settings, these detectors suffer significant performance drops on real-world data, limited benefits from common augmentations, and nuanced effects of pre-processing, highlighting the need for more robust detection strategies. By providing a unified and realistic evaluation framework, AIGIBench offers valuable insights to guide future research toward dependable and generalizable AIGI detection.Data and code are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HorizonTEL/AIGIBench">https://github.com/HorizonTEL/AIGIBench</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANå’ŒDiffusionæ¨¡å‹ï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆ›å»ºé«˜åº¦é€¼çœŸçš„åˆæˆå›¾åƒæˆä¸ºå¯èƒ½ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹è™šå‡ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’Œç‰ˆæƒä¾µçŠ¯çš„ä¸¥é‡å…³æ³¨ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šäººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨ï¼Œå¹¶ä¸”ç»å¸¸æŠ¥å‘Šå…¶é«˜å‡†ç¡®ç‡ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶ä»¤äººè´¨ç–‘ã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†AIGIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°æœ€å…ˆè¿›AIGIæ£€æµ‹å™¨ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚AIGIBenché€šè¿‡å››ä¸ªæ ¸å¿ƒä»»åŠ¡æ¨¡æ‹Ÿç°å®ä¸–ç•ŒæŒ‘æˆ˜ï¼šå¤šæºæ³›åŒ–ã€å¯¹å›¾åƒé€€åŒ–çš„ç¨³å¥æ€§ã€å¯¹æ•°æ®å¢å¼ºçš„æ•æ„Ÿæ€§å’Œæµ‹è¯•æ—¶é¢„å¤„ç†çš„å½±å“ã€‚å®ƒåŒ…æ‹¬23ä¸ªå¤šæ ·åŒ–çš„è™šå‡å›¾åƒå­é›†ï¼Œè¿™äº›å­é›†æ¶µç›–äº†å…ˆè¿›å’Œå¹¿æ³›é‡‡ç”¨çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œä»¥åŠä»ç¤¾äº¤åª’ä½“å’Œäººå·¥æ™ºèƒ½è‰ºæœ¯å¹³å°æ”¶é›†çš„çœŸå®ä¸–ç•Œæ ·æœ¬ã€‚å¯¹11ä¸ªå…ˆè¿›æ£€æµ‹å™¨çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å®ƒä»¬åœ¨å—æ§ç¯å¢ƒä¸­çš„æŠ¥å‘Šå‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†è¿™äº›æ£€æµ‹å™¨åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ€§èƒ½å´å‡ºç°æ˜¾è‘—ä¸‹é™ï¼Œä»å¸¸è§çš„æ•°æ®å¢å¼ºä¸­è·ç›Šæœ‰é™ï¼Œä»¥åŠé¢„å¤„ç†çš„å¾®å¦™å½±å“ï¼Œè¿™çªæ˜¾äº†éœ€è¦æ›´ç¨³å¥çš„æ£€æµ‹ç­–ç•¥ã€‚é€šè¿‡æä¾›ç»Ÿä¸€å’Œç°å®çš„è¯„ä¼°æ¡†æ¶ï¼ŒAIGIBenchä¸ºå¯é çš„AIGIæ£€æµ‹ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶æœç€é€šç”¨åŒ–æ£€æµ‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚æ•°æ®å’Œä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/HorizonTEL/AIGIBench">https://github.com/HorizonTEL/AIGIBench</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12335v2">PDF</a> Accepted by NeurIPS 2025 Datasets and Benchmarks Track</p>
<p><strong>Summary</strong></p>
<p>ç”Ÿæˆæ¨¡å‹å¦‚GANå’ŒDiffusionæ¨¡å‹çš„å¿«é€Ÿå‘å±•èƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„åˆæˆå›¾åƒï¼Œå¼•å‘äº†å…³äºé”™è¯¯ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’Œç‰ˆæƒä¾µçŠ¯çš„ä¸¥é‡å…³æ³¨ã€‚å°½ç®¡å·²æœ‰è®¸å¤šäººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨è¢«æå‡ºï¼Œå¹¶å£°ç§°å…·æœ‰é«˜ç²¾åº¦ï¼Œä½†å®ƒä»¬åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä»æœ‰å¾…éªŒè¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†AIGIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°æœ€æ–°AIGIæ£€æµ‹å™¨çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚AIGIBenché€šè¿‡å››ä¸ªæ ¸å¿ƒä»»åŠ¡æ¨¡æ‹Ÿç°å®æŒ‘æˆ˜ï¼šå¤šæºæ³›åŒ–ã€å¯¹å›¾åƒé€€åŒ–çš„ç¨³å¥æ€§ã€å¯¹æ•°æ®å¢å¼ºçš„æ•æ„Ÿæ€§ä»¥åŠæµ‹è¯•æ—¶é¢„å¤„ç†çš„å½±å“ã€‚å®ƒåŒ…æ‹¬23ä¸ªå¤šæ ·åŒ–çš„è™šå‡å›¾åƒå­é›†ï¼Œæ¶µç›–äº†å…ˆè¿›å’Œå¹¿æ³›é‡‡ç”¨çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œä»¥åŠä»ç¤¾äº¤åª’ä½“å’ŒAIè‰ºæœ¯å¹³å°æ”¶é›†çš„çœŸå®ä¸–ç•Œæ ·æœ¬ã€‚å¯¹11ç§å…ˆè¿›æ£€æµ‹å™¨çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ£€æµ‹å™¨åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä»å¸¸è§å¢å¼ºä¸­è·ç›Šæœ‰é™ï¼Œä»¥åŠé¢„å¤„ç†çš„å½±å“å¾®å¦™ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„æ£€æµ‹ç­–ç•¥ã€‚é€šè¿‡æä¾›ç»Ÿä¸€å’Œç°å®çš„è¯„ä¼°æ¡†æ¶ï¼ŒAIGIBenchä¸ºå¯é çš„AIGIæ£€æµ‹ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HorizonTEL/AIGIBench%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/HorizonTEL/AIGIBenchå…¬å¼€è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¦‚GANå’ŒDiffusionæ¨¡å‹èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„åˆæˆå›¾åƒï¼Œå¼•å‘å…³äºé”™è¯¯ä¿¡æ¯ã€æ·±åº¦ä¼ªé€ å’Œç‰ˆæƒä¾µçŠ¯çš„å…³æ³¨ã€‚</li>
<li>ç°æœ‰çš„äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨åœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§æœ‰å¾…æé«˜ã€‚</li>
<li>AIGIBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°AIGIæ£€æµ‹å™¨çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>AIGIBenchæ¨¡æ‹Ÿç°å®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤šæºæ³›åŒ–ã€å›¾åƒé€€åŒ–ã€æ•°æ®å¢å¼ºå’Œæµ‹è¯•æ—¶é¢„å¤„ç†çš„å½±å“ã€‚</li>
<li>AIGIBenchåŒ…å«å¤šæ ·åŒ–çš„è™šå‡å›¾åƒå­é›†å’ŒçœŸå®ä¸–ç•Œæ ·æœ¬ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ£€æµ‹å™¨åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œéœ€è¦æ›´ç¨³å¥çš„æ£€æµ‹ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-833d8d03dde19319369eb5da8e4f817e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081766&auth_key=1761081766-0-0-6234d2b54b82f6d48a4400ca7d977ac8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ea42f771adc0d31c9c0acb231e29408~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081773&auth_key=1761081773-0-0-e9e0abddb6678331626d8127439d0663&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5ac4ccf9b38493a3c47e4fc5d559dd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081780&auth_key=1761081780-0-0-18b1f215d336302ce971c12e48e16ecd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-362f8c87bf7cc6ef0bafbd01fe613c36~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081786&auth_key=1761081786-0-0-794bb7821b319b4d5db8e8ee1cf3b743&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-FranÃ§ois Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: <a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºè®®ä½¿ç”¨HDRé«˜æ–¯å¹³æ¿ä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨3Dæ¸²æŸ“ä¸­ä½¿ç”¨å¸¸è§„å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯å¹³æ¿å¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢çš„åº”ç”¨æ–¹é¢äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æˆæœã€‚ä¸ºäº†ä¿ƒè¿›å°†å›¾åƒä½œä¸ºå…‰æºçš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†æ¥è¯„ä¼°ä¸é¥±å’ŒHDRå›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ç»“åˆä½¿ç”¨è¿™ä¸€æ–°æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/%EF%BC%88%E6%B3%A8%EF%BC%9A%E8%AF%A5%E7%BD%91%E5%9D%80%E4%B8%BA%E8%99%9A%E6%9E%84%E9%93%BE%E6%8E%A5%EF%BC%8C%E5%AE%9E%E9%99%85%E7%BF%BB%E8%AF%91%E4%B8%AD%E5%BA%94%E6%9B%BF%E6%8D%A2%E4%B8%BA%E7%9C%9F%E5%AE%9E%E7%9A%84%E9%A1%B9%E7%9B%AE%E7%BD%91%E5%9D%80%E3%80%82%EF%BC%89">https://lvsn.github.io/gaslight/ï¼ˆæ³¨ï¼šè¯¥ç½‘å€ä¸ºè™šæ„é“¾æ¥ï¼Œå®é™…ç¿»è¯‘ä¸­åº”æ›¿æ¢ä¸ºçœŸå®çš„é¡¹ç›®ç½‘å€ã€‚ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v4">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œé¦–æ¬¡å®ç°äº†å¸¸è§„å›¾åƒå¯ä»¥ä½œä¸º3Dæ¸²æŸ“å™¨çš„å…‰æºã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–ç…§æ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶åº”ç”¨äºè™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯çš„ç…§æ˜æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä¸ºäº†å¯¹å›¾åƒä½œä¸ºå…‰æºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ–°æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLightæ–¹æ³•èƒ½å¤Ÿä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚</li>
<li>HDRé«˜æ–¯Splatsè¢«ç”¨ä½œå…‰æºè¡¨ç¤ºï¼Œä½¿å¸¸è§„å›¾åƒèƒ½åœ¨3Dæ¸²æŸ“ä¸­ä½œä¸ºå…‰æºã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå¢å¼ºå›¾åƒåŠ¨æ€èŒƒå›´ï¼Œç„¶ååˆ©ç”¨é«˜æ–¯Splatsè¿›è¡Œ3Dç…§æ˜å»ºæ¨¡ã€‚</li>
<li>æ–¹æ³•åœ¨HDRä¼°è®¡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚</li>
<li>æ–¹æ³•çš„åº”ç”¨åŒ…æ‹¬è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯çš„ç…§æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fb3e08fbb0f607279c3f8e25fe7f8c78~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081793&auth_key=1761081793-0-0-14948111e0a36114792fdd180f845f40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a028120200249e6dba615e0d198cd76~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081800&auth_key=1761081800-0-0-991accbc725f024e1bf7aeae95db7500&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16a1d1f424ea7d80a1e1fed27f8ee113~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081807&auth_key=1761081807-0-0-d625ddb834cd76ce9a7e77aed0d90fe2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fc275b15901ac8a138e60dcd346fba6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081813&auth_key=1761081813-0-0-3cbce721aef96198e2e5c0e5ceed8739&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-476e41ece61613c691b2732f2336080d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081820&auth_key=1761081820-0-0-c41bbc69ae458a0c078e70de875b32d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c43cbcda2e86a8d3b84ce3998e24666f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081827&auth_key=1761081827-0-0-6b515c978b934f81d73cf3dcf2c49ab3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8e804a7d388d24d702eb3c379815863~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081854&auth_key=1761081854-0-0-279907e8636e354b232382e285a1e915&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96e72cff83cd25f041b02c81d7573fbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081861&auth_key=1761081861-0-0-3d3a19d10d6eb8f8d4b4b9626dce151b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation"><a href="#Jasmine-Harnessing-Diffusion-Prior-for-Self-supervised-Depth-Estimation" class="headerlink" title="Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation"></a>Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, Yao Zhao</strong></p>
<p>In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based self-supervised framework for monocular depth estimation, which effectively harnesses SDâ€™s visual priors to enhance the sharpness and generalization of unsupervised prediction. Previous SD-based methods are all supervised since adapting diffusion models for dense prediction requires high-precision supervision. In contrast, self-supervised reprojection suffers from inherent challenges (e.g., occlusions, texture-less regions, illumination variance), and the predictions exhibit blurs and artifacts that severely compromise SDâ€™s latent priors. To resolve this, we construct a novel surrogate task of hybrid image reconstruction. Without any additional supervision, it preserves the detail priors of SD models by reconstructing the images themselves while preventing depth estimation from degradation. Furthermore, to address the inherent misalignment between SDâ€™s scale and shift invariant estimation and self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU. It not only bridges this distribution gap but also isolates the fine-grained texture of SD output against the interference of reprojection loss. Extensive experiments demonstrate that Jasmine achieves SoTA performance on the KITTI benchmark and exhibits superior zero-shot generalization across multiple datasets. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Jasmineï¼Œè¿™æ˜¯åŸºäºStable Diffusionï¼ˆSDï¼‰çš„é¦–ä¸ªè‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¹‹å‰çš„åŸºäºSDçš„æ–¹æ³•éƒ½æ˜¯æœ‰ç›‘ç£çš„ï¼Œå› ä¸ºå°†æ‰©æ•£æ¨¡å‹ç”¨äºå¯†é›†é¢„æµ‹éœ€è¦é«˜ç²¾åº¦ç›‘ç£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªç›‘ç£é‡æŠ•å½±é¢ä¸´ç€å›ºæœ‰çš„æŒ‘æˆ˜ï¼ˆä¾‹å¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ï¼‰ï¼Œå¹¶ä¸”é¢„æµ‹ç»“æœå‡ºç°æ¨¡ç³Šå’Œä¼ªå½±ï¼Œä¸¥é‡æŸå®³SDçš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ··åˆå›¾åƒé‡å»ºçš„æ›¿ä»£ä»»åŠ¡ã€‚å®ƒåœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡é‡å»ºå›¾åƒæœ¬èº«æ¥ä¿ç•™SDæ¨¡å‹çš„ç»†èŠ‚å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶é˜²æ­¢æ·±åº¦ä¼°è®¡é€€åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³SDçš„è§„æ¨¡å’Œç§»ä½ä¸å˜ä¼°è®¡ä¸è‡ªç›‘ç£è§„æ¨¡ä¸å˜æ·±åº¦ä¼°è®¡ä¹‹é—´çš„å›ºæœ‰ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†Scale-Shift GRUã€‚å®ƒä¸ä»…å¼¥è¡¥äº†åˆ†å¸ƒå·®è·ï¼Œè¿˜éš”ç¦»äº†SDè¾“å‡ºä¸­çš„ç²¾ç»†çº¹ç†ï¼Œä¸å—é‡æŠ•å½±æŸå¤±çš„å¹²æ‰°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15905v3">PDF</a> Accepted to NeurIPS 2025. 23 pages, with the appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºJasmineï¼Œé¦–ä¸ªåŸºäºStable Diffusionï¼ˆSDï¼‰çš„è‡ªæˆ‘ç›‘ç£æ¡†æ¶ï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æ— ç›‘ç£é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ä»¥å¾€SDæ–¹æ³•ä¸åŒï¼ŒJasmineé‡‡ç”¨è‡ªæˆ‘ç›‘ç£æ–¹å¼ï¼Œè§£å†³å¯†é›†é¢„æµ‹ä¸­å¯¹é«˜ç²¾åº¦ç›‘ç£çš„éœ€æ±‚ã€‚ä¸ºè§£å†³è‡ªæˆ‘ç›‘ç£é‡æŠ•å½±çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œå¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸã€å…‰ç…§å˜åŒ–ç­‰ï¼Œæå‡ºæ··åˆå›¾åƒé‡å»ºçš„æ–°æ›¿ä»£ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³SDå°ºåº¦ä¸ç§»ä½ä¸å˜ä¼°è®¡ä¸è‡ªæˆ‘ç›‘ç£å°ºåº¦ä¸å˜æ·±åº¦ä¼°è®¡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæ„å»ºScale-Shift GRUã€‚å®éªŒè¡¨æ˜ï¼ŒJasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°SOTAæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Jasmineæ˜¯é¦–ä¸ªåŸºäºStable Diffusionçš„è‡ªæˆ‘ç›‘ç£æ¡†æ¶ï¼Œç”¨äºå•ç›®æ·±åº¦ä¼°è®¡ã€‚</li>
<li>Jasmineåˆ©ç”¨SDçš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜é¢„æµ‹çš„æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–SDæ–¹æ³•ä¸åŒï¼ŒJasmineé‡‡ç”¨è‡ªæˆ‘ç›‘ç£æ–¹å¼ï¼Œæ— éœ€é«˜ç²¾åº¦ç›‘ç£ã€‚</li>
<li>å›ºæœ‰æŒ‘æˆ˜å¦‚é®æŒ¡ã€æ— çº¹ç†åŒºåŸŸå’Œå…‰ç…§å˜åŒ–é€šè¿‡æ··åˆå›¾åƒé‡å»ºä»»åŠ¡å¾—åˆ°è§£å†³ã€‚</li>
<li>ä¸ºè§£å†³SDå°ºåº¦ä¸ç§»ä½ä¸å˜ä¼°è®¡ä¸è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œå¼•å…¥Scale-Shift GRUã€‚</li>
<li>Jasmineåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-db2799dab1cc468518320dccefd58040~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081869&auth_key=1761081869-0-0-2561f98ec52169b260c084f2bc897ec0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57ac9dce7c85472e0932817a00e262bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081876&auth_key=1761081876-0-0-399107179bdd05d823fa746f14595b05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6358a4e090a41e3ebb50e6c1991d8154~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081904&auth_key=1761081904-0-0-ec38bbf3355b09650272a6aa52a01f2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42fb4dfe50642e077f38951cdbdfc7b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081911&auth_key=1761081911-0-0-7847fb8cf104ee32bea90c731affc143&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Geodesic-Diffusion-Models-for-Efficient-Medical-Image-Enhancement"><a href="#Geodesic-Diffusion-Models-for-Efficient-Medical-Image-Enhancement" class="headerlink" title="Geodesic Diffusion Models for Efficient Medical Image Enhancement"></a>Geodesic Diffusion Models for Efficient Medical Image Enhancement</h2><p><strong>Authors:Teng Zhang, Hongxu Jiang, Kuang Gong, Wei Shao</strong></p>
<p>Diffusion models generate data by learning to reverse a forward process, where samples are progressively perturbed with Gaussian noise according to a predefined noise schedule. From a geometric perspective, each noise schedule corresponds to a unique trajectory in probability space from the data distribution to a Gaussian prior. However, prior diffusion models rely on empirically chosen schedules that may not be optimal. This inefficiency necessitates many intermediate time steps, resulting in high computational costs during both training and sampling. To address this, we derive a family of geodesic noise schedules corresponding to the shortest paths in probability space under the Fisher-Rao metric. Based on these schedules, we propose Geodesic Diffusion Models (GDMs), which significantly improve training and sampling efficiency by minimizing the energy required to transform between probability distributions. This efficiency further enables sampling to start from an intermediate distribution in conditional image generation, achieving state-of-the-art results with as few as 6 steps. We evaluated GDM on two medical image enhancement tasks: CT image denoising and MRI image super-resolution. Experimental results show that GDM achieved state-of-the-art performance while reducing training time by 20- to 30-fold compared to Denoising Diffusion Probabilistic Models (DDPMs) and 4- to 6-fold compared to Fast-DDPM, and accelerating sampling by 160- to 170-fold and 1.6-fold, respectively. These gains support the use of GDM for efficient model development and real-time clinical applications. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡å­¦ä¹ åè½¬ä¸€ä¸ªæ­£å‘è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®ï¼Œåœ¨è¯¥è¿‡ç¨‹ä¸­ï¼Œæ ·æœ¬æ ¹æ®é¢„å…ˆå®šä¹‰çš„å™ªå£°æ—¶é—´è¡¨é€æ­¥å—åˆ°é«˜æ–¯å™ªå£°çš„å¹²æ‰°ã€‚ä»å‡ ä½•å­¦çš„è§’åº¦æ¥çœ‹ï¼Œæ¯ä¸ªå™ªå£°æ—¶é—´è¡¨å¯¹åº”äºæ¦‚ç‡ç©ºé—´ä¸­ä»æ•°æ®åˆ†å¸ƒåˆ°é«˜æ–¯å…ˆéªŒçš„ç‹¬ç‰¹è½¨è¿¹ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ‰©æ•£æ¨¡å‹ä¾èµ–äºç»éªŒé€‰æ‹©çš„æ—¶é—´è¡¨ï¼Œè¿™äº›æ—¶é—´è¡¨å¯èƒ½å¹¶ä¸æœ€ä¼˜ã€‚è¿™ç§ä½æ•ˆæ€§éœ€è¦å¤§é‡çš„ä¸­é—´æ—¶é—´æ­¥éª¤ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·æœŸé—´çš„è®¡ç®—æˆæœ¬éƒ½å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºå¯¹åº”äºFisher-Raoåº¦é‡ä¸‹æ¦‚ç‡ç©ºé—´ä¸­æœ€çŸ­è·¯å¾„çš„æµ‹åœ°çº¿å™ªå£°æ—¶é—´è¡¨ã€‚åŸºäºè¿™äº›æ—¶é—´è¡¨ï¼Œæˆ‘ä»¬æå‡ºäº†Geodesic Diffusion Modelsï¼ˆGDMsï¼‰ã€‚é€šè¿‡æœ€å°åŒ–æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è½¬æ¢æ‰€éœ€çš„èƒ½é‡ï¼ŒGDMsæ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œé‡‡æ ·çš„æ•ˆç‡ã€‚è¿™ç§æ•ˆç‡è¿˜ä½¿å¾—èƒ½å¤Ÿä»æ¡ä»¶å›¾åƒç”Ÿæˆçš„ä¸­é—´åˆ†å¸ƒå¼€å§‹é‡‡æ ·ï¼Œä»…ç”¨6æ­¥å°±å®ç°äº†æœ€æ–°ç»“æœã€‚æˆ‘ä»¬åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒå¢å¼ºä»»åŠ¡ä¸Šè¯„ä¼°äº†GDMï¼šCTå›¾åƒå»å™ªå’ŒMRIå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†20è‡³30å€ï¼Œä¸Fast-DDPMç›¸æ¯”å‡å°‘äº†4è‡³6å€ï¼›é‡‡æ ·é€Ÿåº¦åˆ†åˆ«åŠ å¿«äº†160è‡³170å€å’Œ1.6å€ã€‚è¿™äº›ä¼˜åŠ¿æ”¯æŒåœ¨é«˜æ•ˆæ¨¡å‹å¼€å‘å’Œå®æ—¶ä¸´åºŠåº”ç”¨ä¸­ä½¿ç”¨GDMã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/mirthAI/GDM-VEä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00745v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡å­¦ä¹ åè½¬ä¸€ä¸ªå°†æ ·æœ¬æ ¹æ®é¢„å®šä¹‰å™ªå£°æ—¶é—´è¡¨é€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°çš„è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®ã€‚æœ¬æ–‡ä»å‡ ä½•è§’åº¦ç ”ç©¶æ‰©æ•£æ¨¡å‹ï¼Œæ¯ä¸ªå™ªå£°æ—¶é—´è¡¨å¯¹åº”äºæ¦‚ç‡ç©ºé—´ä»æ•°æ®åˆ†å¸ƒåˆ°é«˜æ–¯å…ˆéªŒçš„ç‹¬ç‰¹è½¨è¿¹ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ‰©æ•£æ¨¡å‹ä¾èµ–äºç»éªŒé€‰æ‹©çš„æ—¶é—´è¡¨ï¼Œå¯èƒ½å¹¶éæœ€ä¼˜ã€‚è¿™ç§ä½æ•ˆéœ€è¦è®¸å¤šä¸­é—´æ­¥éª¤ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·æ—¶çš„è®¡ç®—æˆæœ¬éƒ½å¾ˆé«˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æ¨å¯¼å‡ºå¯¹åº”äºFisher-Raoåº¦é‡ä¸‹æ¦‚ç‡ç©ºé—´ä¸­æœ€çŸ­è·¯å¾„çš„æµ‹åœ°çº¿å™ªå£°æ—¶é—´è¡¨ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Geodesic Diffusion Modelsï¼ˆGDMsï¼‰ï¼Œé€šè¿‡æœ€å°åŒ–æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è½¬æ¢æ‰€éœ€çš„èƒ½é‡ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œé‡‡æ ·çš„æ•ˆç‡ã€‚è¿™ç§æ•ˆç‡è¿˜ä½¿å¾—å¯ä»¥ä»æ¡ä»¶å›¾åƒç”Ÿæˆçš„ä¸­é—´åˆ†å¸ƒå¼€å§‹é‡‡æ ·ï¼Œä»…ä»¥6æ­¥è¾¾åˆ°ä¸šç•Œæœ€ä½³ç»“æœã€‚æœ¬æ–‡åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒå¢å¼ºä»»åŠ¡ä¸Šè¯„ä¼°äº†GDMï¼šCTå›¾åƒå»å™ªå’ŒMRIå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDMåœ¨å–å¾—æœ€ä½³æ€§èƒ½çš„åŒæ—¶ï¼Œä¸DDPMç›¸æ¯”å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†20è‡³30å€ï¼Œä¸Fast-DDPMç›¸æ¯”ç¼©çŸ­äº†4è‡³6å€ï¼›é‡‡æ ·é€Ÿåº¦åˆ†åˆ«åŠ å¿«äº†160è‡³170å€å’Œ1.6å€ã€‚è¿™äº›æˆæœæ”¯æŒGDMåœ¨é«˜æ•ˆæ¨¡å‹å¼€å‘å’Œå®æ—¶ä¸´åºŠåº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE%E3%80%82">https://github.com/mirthAI/GDM-VEã€‚</a> </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡åè½¬åŒ…å«å™ªå£°çš„è¿‡ç¨‹ç”Ÿæˆæ•°æ®ï¼Œå™ªå£°æ˜¯æŒ‰ç…§é¢„å®šçš„æ—¶é—´è¡¨æ·»åŠ çš„ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¦‚ç‡ç©ºé—´ä¸­çš„è·¯å¾„é€‰æ‹©æ˜¯å…³é”®çš„ï¼Œå®ƒå½±å“äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ä¾èµ–äºç»éªŒé€‰æ‹©çš„æ—¶é—´è¡¨ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·çš„é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å™ªå£°æ—¶é—´è¡¨â€”â€”åŸºäºæµ‹åœ°çº¿çš„æ—¶é—´è¡¨ï¼Œå¯¹åº”äºæ¦‚ç‡ç©ºé—´ä¸­çš„æœ€çŸ­è·¯å¾„ã€‚</li>
<li>åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†Geodesic Diffusion Modelsï¼ˆGDMsï¼‰ï¼Œå®ƒé€šè¿‡æœ€å°åŒ–æ¦‚ç‡åˆ†å¸ƒé—´çš„è½¬æ¢èƒ½é‡æ¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>GDMèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„æ¡ä»¶å›¾åƒç”Ÿæˆï¼Œä»…éœ€è¦å¾ˆå°‘çš„æ­¥éª¤å°±èƒ½è¾¾åˆ°ä¸šç•Œæœ€ä½³ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e425e1c7c67681cfe7148337fb5dcd4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081918&auth_key=1761081918-0-0-fb3d1925a3e4fbdbce09c8e540e5121d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a64229edf413c1ff5787a601efbadf08~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081925&auth_key=1761081925-0-0-20fbf3f0ffd9e249357d100b27c325ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e40db0000336e4641a83dfa726e5eab~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081932&auth_key=1761081932-0-0-1cdfcc9e339f95afef0ff8692d6ed5f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f2c20fef5c59b2bc0e8e9de30e55619~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081938&auth_key=1761081938-0-0-4651ef37be87f9ff02f1f822b194f612&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79c022186e45a7eec38a54a417c3f4f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081945&auth_key=1761081945-0-0-5b3c04c4fd45ee2f3cf95b571534d5ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95241cb1b677bf8bfbc6c342d6654a06~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081952&auth_key=1761081952-0-0-5005fd27c2efeb6d3d8f367403a635b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FairGen-Enhancing-Fairness-in-Text-to-Image-Diffusion-Models-via-Self-Discovering-Latent-Directions"><a href="#FairGen-Enhancing-Fairness-in-Text-to-Image-Diffusion-Models-via-Self-Discovering-Latent-Directions" class="headerlink" title="FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via   Self-Discovering Latent Directions"></a>FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via   Self-Discovering Latent Directions</h2><p><strong>Authors:Yilei Jiang, Weihong Li, Yiyuan Zhang, Minghong Cai, Xiangyu Yue</strong></p>
<p>While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model retraining with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose FairGen, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, FairGen consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process. Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for retraining. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶åæ˜ å‡ºè®­ç»ƒé›†æ‰€å‘ˆç°çš„å›ºæœ‰åè§ã€‚ç”±äºæ‰©æ•£æ¨¡å‹ç°åœ¨å¹¿æ³›åº”ç”¨äºç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œè¿™äº›åè§å¯èƒ½ä¼šæŒç»­æ‰­æ›²ä¸–ç•Œè§‚å¹¶é˜»ç¢å°‘æ•°ç¾¤ä½“çš„æœºä¼šã€‚ç°æœ‰çš„å…³äºå»åæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°ä½¿ç”¨äººå·¥å‚è€ƒæ•°æ®é›†æˆ–é¢å¤–çš„åˆ†ç±»å™¨è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ”¶é›†å‚è€ƒæ•°æ®é›†ä¼šå¯¼è‡´æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬ï¼›ï¼ˆ2ï¼‰å»åæ€§èƒ½ä¸¥é‡ä¾èµ–äºå‚è€ƒæ•°æ®é›†æˆ–é™„åŠ åˆ†ç±»å™¨çš„è´¨é‡ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FairGenï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å‚è€ƒæ•°æ®é›†çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼ŒFairGenç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€ç»„å±æ€§é€‚é…å™¨å’Œä¸€ä¸ªåˆ†å¸ƒæŒ‡æ ‡ã€‚é›†åˆä¸­çš„æ¯ä¸ªé€‚é…å™¨æ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå±æ€§æ½œåœ¨æ–¹å‘ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘å‘ç°è¿‡ç¨‹è¿›è¡Œä¼˜åŒ–å™ªå£°ç»„åˆã€‚ç„¶åï¼Œé€šè¿‡å°†åˆ†å¸ƒæŒ‡æ ‡ä¸é€‚é…å™¨é›†åˆç›¸ä¹˜ï¼Œå¯ä»¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœå‘è§„å®šçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸåŒæ—¶å»é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šä¸ªå±æ€§åè§ï¼ŒåŒæ—¶ä¿æŒè½»é‡åŒ–ï¼Œå¹¶æ˜“äºä¸å…¶ä»–æ‰©æ•£æ¨¡å‹é›†æˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨æ¶ˆé™¤æ€§åˆ«ã€ç§æ—åŠå…¶äº¤å‰åè§çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä¼˜æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Diffusion Modelsï¼ˆDMï¼‰åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å…¶å­˜åœ¨çš„å›ºæœ‰åè§é—®é¢˜ã€‚ç°æœ‰å»åæ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä½¿ç”¨äººå·¥æ„å»ºçš„å‚è€ƒæ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”å—æ•°æ®é›†è´¨é‡é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFairGençš„å³æ’å³ç”¨æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œæ— éœ€ä¾èµ–å‚è€ƒæ•°æ®é›†ã€‚FairGenåŒ…æ‹¬å±æ€§é€‚é…å™¨é›†å’Œåˆ†å¸ƒæŒ‡æ ‡ä¸¤éƒ¨åˆ†ï¼Œæ—¨åœ¨åŒæ—¶å»é™¤éå•ä¸€å±æ€§çš„åè§ï¼Œå…·æœ‰è½»é‡çº§ã€æ˜“äºä¸å…¶ä»–DMé›†æˆç­‰ä¼˜ç‚¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤æ€§åˆ«ã€ç§æ—åŠäº¤å‰åè§æ–¹é¢å¤§å¹…è¶…è¶Šäº†å…ˆå‰æœ€ä½³æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelsåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å›ºæœ‰åè§é—®é¢˜ã€‚</li>
<li>ç°æœ‰å»åæ–¹æ³•éœ€é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–ä½¿ç”¨å‚è€ƒæ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚å¹¶å—é™äºæ•°æ®é›†è´¨é‡ã€‚</li>
<li>FairGenæ–¹æ³•é€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œæ— éœ€ä¾èµ–å‚è€ƒæ•°æ®é›†ã€‚</li>
<li>FairGenç”±å±æ€§é€‚é…å™¨é›†å’Œåˆ†å¸ƒæŒ‡æ ‡ä¸¤éƒ¨åˆ†ç»„æˆï¼Œå¯åŒæ­¥å»é™¤éå•ä¸€å±æ€§çš„åè§ã€‚</li>
<li>FairGenæ–¹æ³•å…·æœ‰è½»é‡çº§ã€æ˜“äºé›†æˆç­‰ç‰¹ç‚¹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒFairGenåœ¨å»é™¤æ€§åˆ«ã€ç§æ—åŠäº¤å‰åè§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…ˆå‰æœ€ä½³æ–¹æ³•ã€‚</li>
<li>FairGençš„æå‡ºä¸ºè§£å†³DMçš„åè§é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-65befc409c197e6b3eb4a8b3c95423ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081959&auth_key=1761081959-0-0-fc9adcbc82b2dc96bd96b746dff9909f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9841f5553e02dbf308b0041cfeaf600a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081967&auth_key=1761081967-0-0-9b818dfc0a717bb46c4383847f1abcae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f10001d4cb6f7b5bea27599e6f762f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081973&auth_key=1761081973-0-0-ea384a30509b0c14184554734cfda563&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77b09910e9aefa4c0d522f31dd014ee5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081980&auth_key=1761081980-0-0-aa2850abb7ec9ec657c9865373ca4376&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-264b6b6ce2ec1be2b97ff5dbc48dc863~resize:0:q75.jpg?source=1f5c5e47&expiration=1761081987&auth_key=1761081987-0-0-25c9d9585915e42bcb8665b7d7f1249f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c12977203201bb5db1de6665ae782b3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761082657&auth_key=1761082657-0-0-0f0c524a805fe8559bf62ebbbb98b5a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-be0133811283de2cf3e569d824e00137~resize:0:q75.jpg?source=1f5c5e47&expiration=1761080081&auth_key=1761080081-0-0-a3f4eee1f4252ad21706804a790b50f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Latent Spaces Beyond Synthesis From GANs to Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
