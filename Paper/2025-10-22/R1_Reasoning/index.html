<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Robobench A Comprehensive Evaluation Benchmark for Multimodal Large   Language Models as Embodied Brain">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-cdfb4915883ab6c25cc150a33c2ea3f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068879&auth_key=1761068879-0-0-a927a61fddc6d89b6187211df259f2a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-22-æ›´æ–°"><a href="#2025-10-22-æ›´æ–°" class="headerlink" title="2025-10-22 æ›´æ–°"></a>2025-10-22 æ›´æ–°</h1><h2 id="Robobench-A-Comprehensive-Evaluation-Benchmark-for-Multimodal-Large-Language-Models-as-Embodied-Brain"><a href="#Robobench-A-Comprehensive-Evaluation-Benchmark-for-Multimodal-Large-Language-Models-as-Embodied-Brain" class="headerlink" title="Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large   Language Models as Embodied Brain"></a>Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large   Language Models as Embodied Brain</h2><p><strong>Authors:Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang</strong></p>
<p>Building robots that can perceive, reason, and act in dynamic, unstructured environments remains a core challenge. Recent embodied systems often adopt a dual-system paradigm, where System 2 handles high-level reasoning while System 1 executes low-level control. In this work, we refer to System 2 as the embodied brain, emphasizing its role as the cognitive core for reasoning and decision-making in manipulation tasks. Given this role, systematic evaluation of the embodied brain is essential. Yet existing benchmarks emphasize execution success, or when targeting high-level reasoning, suffer from incomplete dimensions and limited task realism, offering only a partial picture of cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark that systematically evaluates multimodal large language models (MLLMs) as embodied brains. Motivated by the critical roles across the full manipulation pipeline, RoboBench defines five dimensions-instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure realism, we curate datasets across diverse embodiments, attribute-rich objects, and multi-view scenes, drawing from large-scale real robotic data. For planning, RoboBench introduces an evaluation framework, MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether predicted plans can achieve critical object-state changes. Experiments on 14 MLLMs reveal fundamental limitations: difficulties with implicit instruction comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis. RoboBench provides a comprehensive scaffold to quantify high-level cognition, and guide the development of next-generation embodied MLLMs. The project page is in <a target="_blank" rel="noopener" href="https://robo-bench.github.io/">https://robo-bench.github.io</a>. </p>
<blockquote>
<p>æ„å»ºèƒ½å¤Ÿåœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„æœºå™¨äººä»ç„¶æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€è¿‘çš„å®ä½“ç³»ç»Ÿé€šå¸¸é‡‡ç”¨åŒç³»ç»ŸèŒƒå¼ï¼Œå…¶ä¸­ç³»ç»Ÿ2è´Ÿè´£é«˜çº§æ¨ç†ï¼Œè€Œç³»ç»Ÿ1æ‰§è¡Œä½çº§æ§åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†ç³»ç»Ÿ2ç§°ä¸ºå®ä½“å¤§è„‘ï¼Œå¼ºè°ƒå…¶åœ¨æ“ä½œä»»åŠ¡ä¸­çš„æ¨ç†å’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„è®¤çŸ¥æ ¸å¿ƒä½œç”¨ã€‚è€ƒè™‘åˆ°è¿™ä¸€ä½œç”¨ï¼Œå¯¹å®ä½“å¤§è„‘è¿›è¡Œç³»ç»Ÿè¯„ä¼°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¾§é‡äºæ‰§è¡ŒæˆåŠŸï¼Œæˆ–è€…åœ¨é’ˆå¯¹é«˜çº§æ¨ç†æ—¶ï¼Œå­˜åœ¨ç»´åº¦ä¸å…¨å’Œä»»åŠ¡ç°å®æ€§æœ‰é™çš„é—®é¢˜ï¼Œåªèƒ½æä¾›è®¤çŸ¥èƒ½åŠ›çš„éƒ¨åˆ†å›¾æ™¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†RoboBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºå®ä½“å¤§è„‘çš„åŸºå‡†æµ‹è¯•ã€‚å—æ•´ä¸ªæ“ä½œç®¡é“ä¸­å…³é”®ä½œç”¨å¯å‘ï¼ŒRoboBenchå®šä¹‰äº†äº”ä¸ªç»´åº¦ï¼šæŒ‡ä»¤ç†è§£ã€æ„ŸçŸ¥æ¨ç†ã€é€šç”¨è§„åˆ’ã€å¯é¢„æµ‹æ€§å’Œæ•…éšœåˆ†æï¼Œæ¶µç›–14ç§èƒ½åŠ›ã€25é¡¹ä»»åŠ¡å’Œ6092ä¸ªé—®ç­”å¯¹ã€‚ä¸ºäº†ç¡®ä¿ç°å®æ€§ï¼Œæˆ‘ä»¬ä»å¤§è§„æ¨¡çœŸå®æœºå™¨äººæ•°æ®ä¸­æ•´ç†äº†å¤šç§å®ä½“ã€å±æ€§ä¸°å¯Œçš„å¯¹è±¡å’Œåœºæ™¯çš„æ•°æ®é›†ã€‚åœ¨è§„åˆ’æ–¹é¢ï¼ŒRoboBenchå¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå³MLLMä½œä¸ºä¸–ç•Œæ¨¡æ‹Ÿå™¨ã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿé¢„æµ‹çš„è®¡åˆ’æ˜¯å¦èƒ½å®ç°å…³é”®å¯¹è±¡çŠ¶æ€å˜åŒ–æ¥è¯„ä¼°å®ä½“çš„å¯è¡Œæ€§ã€‚å¯¹1 or modelsçš„å®éªŒæ­ç¤ºäº†æ ¹æœ¬æ€§å±€é™ï¼šåœ¨éšæ€§æŒ‡ä»¤ç†è§£ã€æ—¶ç©ºæ¨ç†ã€è·¨åœºæ™¯è§„åˆ’ã€ç²¾ç»†å¯é¢„æµ‹æ€§ç†è§£å’Œæ‰§è¡Œæ•…éšœè¯Šæ–­æ–¹é¢çš„å›°éš¾ã€‚RoboBenchæä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¶æ„æ¥é‡åŒ–é«˜çº§è®¤çŸ¥ï¼Œå¹¶å¼•å¯¼ä¸‹ä¸€ä»£å®ä½“MLLMçš„å‘å±•ã€‚é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://robo-bench.github.io./">https://robo-bench.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­æ„å»ºæœºå™¨äººçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºå½“å‰çš„ç³»ç»Ÿé€šå¸¸é‡‡ç”¨åŒç³»ç»ŸèŒƒå¼ï¼Œå…¶ä¸­ç³»ç»Ÿ2è´Ÿè´£é«˜çº§æ¨ç†ï¼Œç³»ç»Ÿ1æ‰§è¡Œä½çº§æ§åˆ¶ã€‚æ–‡ç« å¼ºè°ƒäº†ç³»ç»Ÿ2ä½œä¸ºè®¤çŸ¥æ ¸å¿ƒåœ¨æ“çºµä»»åŠ¡ä¸­çš„ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºå¯¹å…¶çš„ç³»ç»Ÿæ€§è¯„ä»·çš„é‡è¦æ€§ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ‰§è¡ŒæˆåŠŸæˆ–é«˜çº§æ¨ç†ï¼Œå­˜åœ¨ç»´åº¦ä¸å…¨å’Œä»»åŠ¡ç°å®æ€§æœ‰é™çš„é—®é¢˜ï¼Œåªèƒ½æä¾›éƒ¨åˆ†è®¤çŸ¥èƒ½åŠ›è¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæ–‡ç« å¼•å…¥äº†RoboBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè®¤çŸ¥æ ¸å¿ƒçš„è¡¨ç°ã€‚RoboBenchå®šä¹‰äº†äº”ä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬æŒ‡ä»¤ç†è§£ã€æ„ŸçŸ¥æ¨ç†ã€é€šç”¨è§„åˆ’ã€å¯é¢„æµ‹æ€§å’Œå¤±è´¥åˆ†æç­‰ï¼Œæ¶µç›–äº†ä¸°å¯Œçš„ä»»åŠ¡å’Œèƒ½åŠ›ã€‚ä¸ºç¡®ä¿ç°å®æ€§ï¼Œæ•°æ®é›†æ¶µç›–äº†å¤šç§ä½“ç°ã€å±æ€§ä¸°å¯Œçš„ç‰©ä½“å’Œå¤šè§†è§’åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMsåœ¨éšæ€§æŒ‡ä»¤ç†è§£ã€æ—¶ç©ºæ¨ç†ã€è·¨åœºæ™¯è§„åˆ’ã€ç²¾ç»†åŠ¨ä½œç†è§£å’Œæ‰§è¡Œå¤±è´¥è¯Šæ–­ç­‰æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚RoboBenchä¸ºé«˜çº§è®¤çŸ¥èƒ½åŠ›çš„é‡åŒ–æä¾›äº†å…¨é¢çš„æ¶æ„ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£MLLMsçš„å‘å±•æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººé¢ä¸´åœ¨åŠ¨æ€ã€éç»“æ„åŒ–ç¯å¢ƒä¸­æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æœºå™¨äººç³»ç»Ÿé‡‡ç”¨åŒç³»ç»ŸèŒƒå¼ï¼Œå…¶ä¸­ç³»ç»Ÿ2ä½œä¸ºè®¤çŸ¥æ ¸å¿ƒè´Ÿè´£é«˜çº§æ¨ç†ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°æœºå™¨äººè®¤çŸ¥èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥RoboBenchåŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿåœ°è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº”ä¸ªç»´åº¦ï¼ˆæŒ‡ä»¤ç†è§£ã€æ„ŸçŸ¥æ¨ç†ç­‰ï¼‰çš„è¡¨ç°ã€‚</li>
<li>RoboBenchå®éªŒæ­ç¤ºäº†MLLMsåœ¨å¤šä¸ªæ–¹é¢çš„æ ¹æœ¬æ€§å±€é™ã€‚</li>
<li>RoboBenchä¸ºé«˜çº§è®¤çŸ¥èƒ½åŠ›çš„é‡åŒ–æä¾›äº†å…¨é¢çš„æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1555437853d98fef65102c278ee729e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068460&auth_key=1761068460-0-0-b01566dfbc0e0ce12b074d1d467622cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00265033f4fc82709cd4c9e632645b49~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068467&auth_key=1761068467-0-0-ff117d729fb65a363b85111ff3f25b3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2734e6f48b8551b95f500fea33977252~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068474&auth_key=1761068474-0-0-1601de6e5ad7b64221bdc81f66d21a5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f897401df877f5a0cae9e2238e74ab9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068480&auth_key=1761068480-0-0-6b5e6bf2f43f1a478cd3c458b419477f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a6c531e656be2a3588f4cb4b7ff97ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068487&auth_key=1761068487-0-0-26446132c4b983c8149f980c60fdb4c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-302e69689c8134035343ad2b8e6a7b7c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068493&auth_key=1761068493-0-0-cc53d2d6fe8179e850d0d6ef95a02b4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Foundational-Automatic-Evaluators-Scaling-Multi-Task-Generative-Evaluator-Training-for-Reasoning-Centric-Domains"><a href="#Foundational-Automatic-Evaluators-Scaling-Multi-Task-Generative-Evaluator-Training-for-Reasoning-Centric-Domains" class="headerlink" title="Foundational Automatic Evaluators: Scaling Multi-Task Generative   Evaluator Training for Reasoning-Centric Domains"></a>Foundational Automatic Evaluators: Scaling Multi-Task Generative   Evaluator Training for Reasoning-Centric Domains</h2><p><strong>Authors:Austin Xu, Xuan-Phi Nguyen, Yilun Zhou, Chien-Sheng Wu, Caiming Xiong, Shafiq Joty</strong></p>
<p>Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality. </p>
<blockquote>
<p>å¾®è°ƒä¸“ç”¨ç”Ÿæˆè¯„ä¼°å™¨å·²æˆä¸ºä¸€ç§æµè¡Œçš„èŒƒå¼ï¼Œä»¥æ»¡è¶³è®­ç»ƒå’Œæµ‹è¯•æœŸé—´ä¸æ–­å¢é•¿çš„å¯ä¼¸ç¼©æ€§è¯„ä¼°éœ€æ±‚ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åº”ç”¨æ–°çš„æ–¹æ³•å­¦ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒè¯„ä¼°å™¨ï¼Œè€Œé¿å…å¤§è§„æ¨¡çš„æ•°æ®é©±åŠ¨å¼€å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ•°æ®è§„æ¨¡æ‰©å±•ï¼Œæ•´ç†äº†ä¸€å¥—åŒ…å«250ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¶µç›–äº”ä¸ªç‹¬ç‰¹çš„è¯„ä¼°ä»»åŠ¡ï¼ˆæˆå¯¹ã€æ­¥éª¤çº§ã€æ— å‚è€ƒå’Œæœ‰å‚è€ƒéªŒè¯ã€å•ä¸€è¯„åˆ†ï¼‰ï¼Œä»¥åŠå¤šä¸ªä¸“æ³¨äºæ¨ç†è¯„ä¼°çš„åŸŸã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒäº†åŸºç¡€è‡ªåŠ¨æ¨ç†è¯„ä¼°å™¨ï¼ˆFAREï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰8Bå’Œå¸¦æœ‰36äº¿ä¸ªæ´»è·ƒå‚æ•°çš„20Bå®¶æ—è¯„ä¼°å™¨ç³»åˆ—ï¼Œé‡‡ç”¨ç®€å•çš„è¿­ä»£æ‹’ç»é‡‡æ ·ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ã€‚FARE-8BæŒ‘æˆ˜äº†æ›´å¤§çš„ä¸“ç”¨RLè®­ç»ƒè¯„ä¼°å™¨ï¼Œè€ŒFARE-20Bä¸ºå¼€æºè¯„ä¼°å™¨è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œè¶…è¶Šäº†ä¸“ç”¨70B+è¯„ä¼°å™¨ã€‚é™¤äº†é™æ€åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬åœ¨ç°å®ä»»åŠ¡ä¸­è¯„ä¼°FAREï¼šä½œä¸ºæ¨ç†æ—¶çš„é‡æ–°æ’åºå™¨ï¼ŒFARE-20Båœ¨MATHä¸Šçš„æ€§èƒ½æ¥è¿‘æœ€ä¼˜ã€‚ä½œä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„éªŒè¯å™¨ï¼ŒFAREå°†ä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½æé«˜äº†é«˜è¾¾14.1%ï¼Œè¶…è¿‡äº†å­—ç¬¦ä¸²åŒ¹é…éªŒè¯å™¨ã€‚ä½¿ç”¨FAREè¿›è¡Œåˆå§‹åŒ–åï¼ŒæŒç»­å¾®è°ƒåçš„FARE-Codeåœ¨è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡æ–¹é¢è¶…è¿‡äº†gpt-oss-20Bè¾¾65%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17793v1">PDF</a> 29 pages, 9 tables, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸ºæ»¡è¶³è®­ç»ƒå’Œæµ‹è¯•æ—¶æ—¥ç›Šå¢é•¿çš„è¯„ä»·éœ€æ±‚è€Œå…´èµ·çš„ç‰¹æ®Šç”Ÿæˆè¯„ä»·å™¨çš„å¾®è°ƒæ¨¡å¼ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨æ•°æ®è§„æ¨¡æ‰©å¤§ï¼Œä¸ºæ­¤æ•´ç†äº†ä¸€å¥—è·¨è¶Šäº”ä¸ªç‹¬ç‰¹è¯„ä»·ä»»åŠ¡ã€æ¶µç›–å¤šä¸ªä»¥æ¨ç†è¯„ä»·ä¸ºä¸­å¿ƒçš„é¢†åŸŸçš„250ä¸‡æ ·æœ¬æ•°æ®ã€‚æ–‡ç« ä½¿ç”¨ç®€å•çš„è¿­ä»£æ‹’ç»é‡‡æ ·ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œè®­ç»ƒå‡ºåŸºç¡€è‡ªåŠ¨æ¨ç†è¯„ä»·å™¨ï¼ˆFAREï¼‰ï¼Œå…¶ä¸­FARE-8Bå’ŒFARE-20Bæ€§èƒ½çªå‡ºã€‚åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¯„ä¼°è¡¨æ˜ï¼ŒFARE-20Båœ¨MATHä¸Šçš„æ€§èƒ½æ¥è¿‘æœ€ä¼˜ï¼Œä½œä¸ºéªŒè¯å™¨åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­èƒ½æå‡ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½ï¼Œä¸”å½“ä»¥FAREä¸ºåŸºç¡€è¿›è¡Œè¿ç»­å¾®è°ƒæ—¶ï¼ŒFARE-Codeåœ¨è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šgpt-oss-20Bçš„65%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰¹æ®Šç”Ÿæˆè¯„ä»·å™¨çš„å¾®è°ƒå·²æˆä¸ºä¸€ç§æµè¡Œçš„è¯„ä»·æ¨¡å¼ï¼Œä»¥æ»¡è¶³è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶çš„é‡ç‚¹ä»å•çº¯çš„æ–¹æ³•è®ºè½¬å‘æ•°æ®è§„æ¨¡æ‰©å¤§ï¼Œä¸ºæ­¤æ•´ç†äº†ä¸€å¥—æ¶µç›–å¤šä¸ªé¢†åŸŸçš„æ ·æœ¬æ•°æ®ã€‚</li>
<li>è®­ç»ƒå‡ºåŸºç¡€è‡ªåŠ¨æ¨ç†è¯„ä»·å™¨ï¼ˆFAREï¼‰ï¼ŒåŒ…æ‹¬FARE-8Bå’ŒFARE-20Bä¸¤ç§å‹å·ã€‚</li>
<li>FAREåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚åœ¨MATHä¸Šçš„æ€§èƒ½æ¥è¿‘æœ€ä¼˜ã€‚</li>
<li>ä½œä¸ºéªŒè¯å™¨åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„åº”ç”¨èƒ½æ˜¾è‘—æå‡ä¸‹æ¸¸æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¿ç»­å¾®è°ƒåçš„FARE-Codeåœ¨è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡æ–¹é¢è¶…è¶Šäº†gpt-oss-20Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3baac67c6c66d3851f9adff96583c0ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068501&auth_key=1761068501-0-0-8a9c626ca0b516ba0e7bc4c3588787c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27b6f80b83776edc1389840b8d42170b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068508&auth_key=1761068508-0-0-512b57a15232e97a756beb20f0900663&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c30e66b8fe7102565827225517a3eb36~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068515&auth_key=1761068515-0-0-7143e1178446b38d4c15157a91fae094&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d62832d3552f65f778c9a98e1c048d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068521&auth_key=1761068521-0-0-1d9c8a149cbb6da2a65f6b8afe279c03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contextual-Attention-Modulation-Towards-Efficient-Multi-Task-Adaptation-in-Large-Language-Models"><a href="#Contextual-Attention-Modulation-Towards-Efficient-Multi-Task-Adaptation-in-Large-Language-Models" class="headerlink" title="Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation   in Large Language Models"></a>Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation   in Large Language Models</h2><p><strong>Authors:Dayan Pan, Zhaoyang Fu, Jingyuan Wang, Xiao Han, Yue Zhu, Xiangyu Zhao</strong></p>
<p>Large Language Models (LLMs) possess remarkable generalization capabilities but struggle with multi-task adaptation, particularly in balancing knowledge retention with task-specific specialization. Conventional fine-tuning methods suffer from catastrophic forgetting and substantial resource consumption, while existing parameter-efficient methods perform suboptimally in complex multi-task scenarios. To address this, we propose Contextual Attention Modulation (CAM), a novel mechanism that dynamically modulates the representations of self-attention modules in LLMs. CAM enhances task-specific features while preserving general knowledge, thereby facilitating more effective and efficient adaptation. For effective multi-task adaptation, CAM is integrated into our Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a shared, full-parameter CAM module with multiple specialized, lightweight CAM modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion. Extensive experiments on heterogeneous tasks, including question answering, code generation, and logical reasoning, demonstrate that our approach significantly outperforms existing approaches, achieving an average performance improvement of 3.65%. The implemented code and data are available to ease reproducibility at <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/HyCAM">https://github.com/Applied-Machine-Learning-Lab/HyCAM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å¤šä»»åŠ¡é€‚åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™çŸ¥è¯†ä¸ä»»åŠ¡ç‰¹å®šä¸“ä¸šåŒ–çš„å¹³è¡¡æ–¹é¢ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜å’Œå¤§é‡èµ„æºæ¶ˆè€—çš„é—®é¢˜ï¼Œè€Œç°æœ‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•åœ¨å¤æ‚çš„å¤šä»»åŠ¡åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆCAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€è°ƒåˆ¶LLMä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—è¡¨ç¤ºçš„æ–°å‹æœºåˆ¶ã€‚CAMåœ¨ä¿ç•™é€šç”¨çŸ¥è¯†çš„åŒæ—¶ï¼Œå¢å¼ºäº†ç‰¹å®šä»»åŠ¡çš„ç‰¹å¾ï¼Œä»è€Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆã€æ›´é«˜æ•ˆçš„é€‚åº”ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„å¤šä»»åŠ¡é€‚åº”ï¼ŒCAMè¢«é›†æˆåˆ°æˆ‘ä»¬çš„æ··åˆä¸Šä¸‹æ–‡æ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆHyCAMï¼‰æ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å…±äº«çš„å…¨å‚æ•°CAMæ¨¡å—å’Œå¤šä¸ªä¸“ä¸šåŒ–çš„è½»é‡çº§CAMæ¨¡å—ï¼Œå¹¶ç”±åŠ¨æ€è·¯ç”±ç­–ç•¥å¢å¼ºï¼Œä»¥å®ç°è‡ªé€‚åº”çŸ¥è¯†èåˆã€‚åœ¨åŒ…æ‹¬é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ç­‰å¼‚è´¨ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æé«˜3.65%ã€‚å®ç°çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/HyCAM%E4%B8%8A%E6%9D%BE%E6%9C%BA%E5%AF%BC%E9%A2%84%E5%AE%B9%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/HyCAMä¸Šè½»æ¾é‡ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17705v1">PDF</a> Accepted by CIKMâ€™ 25</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å¤šä»»åŠ¡é€‚åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸ä»»åŠ¡ç‰¹å®šä¸“ä¸šåŒ–æ–¹é¢ã€‚ç°æœ‰å‚æ•°ä¼˜åŒ–æ–¹æ³•åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“é—å¿˜å…ˆå‰å­¦åˆ°çš„çŸ¥è¯†ä¸”èµ„æºæ¶ˆè€—å¤§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºä¸Šä¸‹æ–‡æ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆCAMï¼‰çš„æ–°æœºåˆ¶ï¼Œå®ƒèƒ½åŠ¨æ€è°ƒæ•´LLMsä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„è¡¨ç¤ºã€‚CAMé€šè¿‡å¢å¼ºä»»åŠ¡ç‰¹å®šç‰¹å¾å¹¶ä¿ç•™ä¸€èˆ¬çŸ¥è¯†ï¼Œå®ç°æ›´æœ‰æ•ˆçš„çŸ¥è¯†èåˆï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„é€‚åº”ã€‚ä¸ºè§£å†³å¤šä»»åŠ¡é€‚åº”é—®é¢˜ï¼Œå°†CAMé›†æˆåˆ°æˆ‘ä»¬çš„æ··åˆä¸Šä¸‹æ–‡æ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆHyCAMï¼‰æ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å…±äº«çš„å…¨å‚æ•°CAMæ¨¡å—å’Œå¤šä¸ªä¸“ç”¨çš„è½»é‡åŒ–CAMæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±ç­–ç•¥å®ç°è‡ªé€‚åº”çŸ¥è¯†èåˆã€‚åœ¨é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ç­‰å¼‚è´¨ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æé«˜3.65%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/HyCAM%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/HyCAMä¸Šå…¬å¼€ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä»»åŠ¡é€‚åº”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸ä»»åŠ¡ç‰¹å®šä¸“ä¸šåŒ–çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å‚æ•°ä¼˜åŒ–æ–¹æ³•åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨é—å¿˜å…ˆå‰çŸ¥è¯†å’Œèµ„æºæ¶ˆè€—å¤§çš„é—®é¢˜ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ³¨æ„åŠ›è°ƒåˆ¶ï¼ˆCAMï¼‰æœºåˆ¶èƒ½åŠ¨æ€è°ƒæ•´LLMsä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„è¡¨ç¤ºï¼Œå¢å¼ºä»»åŠ¡ç‰¹å®šç‰¹å¾å¹¶ä¿ç•™ä¸€èˆ¬çŸ¥è¯†ã€‚</li>
<li>HyCAMæ¡†æ¶ç»“åˆäº†å…±äº«çš„å…¨å‚æ•°CAMæ¨¡å—å’Œå¤šä¸ªä¸“ç”¨çš„è½»é‡åŒ–CAMæ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±ç­–ç•¥å®ç°æ›´æœ‰æ•ˆçš„çŸ¥è¯†èåˆå’Œæ›´é«˜æ•ˆçš„å¤šä»»åŠ¡é€‚åº”ã€‚</li>
<li>HyCAMæ¡†æ¶åœ¨å¼‚è´¨ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æå‡3.65%ã€‚</li>
<li>ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šå…¬å¼€ï¼Œä¾¿äºç ”ç©¶äººå‘˜è¿›è¡Œå¤ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-60c540594c88067dfeea08c324e2ff43~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068528&auth_key=1761068528-0-0-70fd69ac146304f6d65246e086b5665f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3665f05e39c9b39a787da15992b8db3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068536&auth_key=1761068536-0-0-c97a0606cf761c9eeee63d4abe23abb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-073ed13141cdcd5f2162d748044d4bab~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068543&auth_key=1761068543-0-0-523213ff1dd412f914de86e405e609f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CrossGuard-Safeguarding-MLLMs-against-Joint-Modal-Implicit-Malicious-Attacks"><a href="#CrossGuard-Safeguarding-MLLMs-against-Joint-Modal-Implicit-Malicious-Attacks" class="headerlink" title="CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious   Attacks"></a>CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious   Attacks</h2><p><strong>Authors:Xu Zhang, Hao Li, Zhichao Lu</strong></p>
<p>Multimodal Large Language Models (MLLMs) achieve strong reasoning and perception capabilities but are increasingly vulnerable to jailbreak attacks. While existing work focuses on explicit attacks, where malicious content resides in a single modality, recent studies reveal implicit attacks, in which benign text and image inputs jointly express unsafe intent. Such joint-modal threats are difficult to detect and remain underexplored, largely due to the scarcity of high-quality implicit data. We propose ImpForge, an automated red-teaming pipeline that leverages reinforcement learning with tailored reward modules to generate diverse implicit samples across 14 domains. Building on this dataset, we further develop CrossGuard, an intent-aware safeguard providing robust and comprehensive defense against both explicit and implicit threats. Extensive experiments across safe and unsafe benchmarks, implicit and explicit attacks, and multiple out-of-domain settings demonstrate that CrossGuard significantly outperforms existing defenses, including advanced MLLMs and guardrails, achieving stronger security while maintaining high utility. This offers a balanced and practical solution for enhancing MLLM robustness against real-world multimodal threats. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ã€‚è™½ç„¶ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨æ˜¾å¼æ”»å‡»ï¼Œå³æ¶æ„å†…å®¹å­˜åœ¨äºå•ä¸€æ¨¡æ€ä¸­ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†éšå¼æ”»å‡»ï¼Œå…¶ä¸­è‰¯æ€§æ–‡æœ¬å’Œå›¾åƒè¾“å…¥å…±åŒè¡¨è¾¾ä¸å®‰å…¨æ„å›¾ã€‚ç”±äºé«˜è´¨é‡éšå¼æ•°æ®çš„ç¨€ç¼ºï¼Œè¿™ç§è·¨æ¨¡æ€è”åˆå¨èƒå¾ˆéš¾æ£€æµ‹ä¸”ä¸€ç›´è¢«å¿½è§†ã€‚æˆ‘ä»¬æå‡ºäº†ImpForgeï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çº¢é˜Ÿç®¡é“ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¹¶ç»“åˆå®šåˆ¶çš„å¥–åŠ±æ¨¡å—ï¼Œåœ¨14ä¸ªé¢†åŸŸç”Ÿæˆå¤šæ ·åŒ–çš„éšå¼æ ·æœ¬ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†CrossGuardï¼Œä¸€ç§æ„å›¾æ„ŸçŸ¥çš„é˜²æŠ¤ç½©ï¼Œä¸ºæ˜¾å¼å’Œéšå¼å¨èƒæä¾›ç¨³å¥è€Œå…¨é¢çš„é˜²å¾¡ã€‚åœ¨å®‰å…¨å’Œä¸å®‰å…¨åŸºå‡†ã€éšå¼å’Œæ˜¾å¼æ”»å‡»ä»¥åŠå¤šä¸ªè·¨åŸŸç¯å¢ƒä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCrossGuardæ˜¾è‘—ä¼˜äºç°æœ‰é˜²å¾¡æ‰‹æ®µï¼ŒåŒ…æ‹¬é«˜çº§MLLMså’ŒæŠ¤æ ï¼Œåœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶å®ç°äº†æ›´å¼ºçš„å®‰å…¨æ€§ã€‚è¿™ä¸ºæé«˜MLLMå¯¹ç°å®ä¸–ç•Œå¤šæ¨¡æ€å¨èƒçš„ç¨³å¥æ€§æä¾›äº†å¹³è¡¡ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17687v1">PDF</a> 14 pages, 8 figures, 2 tables</p>
<p><strong>Summary</strong><br>    å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†æ˜“å—æ”»å‡»ã€‚ç°æœ‰ç ”ç©¶å…³æ³¨æ˜¾æ€§æ”»å‡»ï¼Œè€Œè¿‘æœŸå‘ç°éšæ€§æ”»å‡»å¨èƒæ›´å¤§ï¼Œå…¶ä¸­è‰¯æ€§æ–‡æœ¬å’Œå›¾åƒè¾“å…¥å…±åŒè¡¨è¾¾ä¸å®‰å…¨æ„å›¾ã€‚æœ¬æ–‡æå‡ºImpForgeçº¢é˜Ÿæµ‹è¯•è‡ªåŠ¨åŒ–ç®¡é“ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸å®šåˆ¶å¥–åŠ±æ¨¡å—ç”Ÿæˆå¤šç§éšæ ·æœ¬æ•°æ®ã€‚åŸºäºæ­¤æ•°æ®é›†è¿›ä¸€æ­¥å¼€å‘CrossGuardæ„å›¾æ„ŸçŸ¥é˜²æŠ¤ç³»ç»Ÿï¼Œå¯¹æ˜¾æ€§åŠéšæ€§å¨èƒæä¾›ç¨³å¥å…¨é¢çš„é˜²å¾¡ã€‚å®éªŒè¯æ˜ï¼ŒCrossGuardæ˜¾è‘—ä¼˜äºç°æœ‰é˜²æŠ¤ç³»ç»Ÿï¼Œåœ¨ä¿æŒé«˜æ•ˆç”¨çš„åŒæ—¶æé«˜å®‰å…¨æ€§ï¼Œä¸ºå¢å¼ºMLLMå¯¹ç°å®ä¸–ç•Œå¤šæ¨¡æ€å¨èƒçš„ç¨³å¥æ€§æä¾›å¹³è¡¡å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†æ˜“å—æ”»å‡»ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ˜¾æ€§æ”»å‡»ï¼Œä½†éšæ€§æ”»å‡»é€æ¸æˆä¸ºå¨èƒã€‚</li>
<li>è‰¯æ€§æ–‡æœ¬å’Œå›¾åƒè¾“å…¥å¯å…±åŒè¡¨è¾¾ä¸å®‰å…¨æ„å›¾ï¼Œè¿™äº›è”åˆæ¨¡æ€å¨èƒéš¾ä»¥æ£€æµ‹ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡éšæ€§æ•°æ®ä½¿å¾—ç›¸å…³ç ”ç©¶å—åˆ°é™åˆ¶ã€‚</li>
<li>æå‡ºImpForgeè‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•ç®¡é“ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šæ ·éšæ€§æ ·æœ¬æ•°æ®ã€‚</li>
<li>å¼€å‘CrossGuardæ„å›¾æ„ŸçŸ¥é˜²æŠ¤ç³»ç»Ÿï¼Œå¯¹æ˜¾æ€§åŠéšæ€§å¨èƒæä¾›å…¨é¢é˜²å¾¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c6088dd7d4699f278d4fdc4c8b660b3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068551&auth_key=1761068551-0-0-90ea231731db7446e340299e9338ebda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1bd79d0e81c430fe14487ea595c8607~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068558&auth_key=1761068558-0-0-44b5288c6746881b6a156fc4754ad322&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multilingual-Text-to-Image-Person-Retrieval-via-Bidirectional-Relation-Reasoning-and-Aligning"><a href="#Multilingual-Text-to-Image-Person-Retrieval-via-Bidirectional-Relation-Reasoning-and-Aligning" class="headerlink" title="Multilingual Text-to-Image Person Retrieval via Bidirectional Relation   Reasoning and Aligning"></a>Multilingual Text-to-Image Person Retrieval via Bidirectional Relation   Reasoning and Aligning</h2><p><strong>Authors:Min Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhang</strong></p>
<p>Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in <a target="_blank" rel="noopener" href="https://github.com/Flame-Chasers/Bi-IRRA">https://github.com/Flame-Chasers/Bi-IRRA</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒäººç‰©æ£€ç´¢ï¼ˆTIPRï¼‰æ—¨åœ¨ä½¿ç”¨æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›®æ ‡äººç‰©ï¼Œé¢ä¸´æ¨¡æ€å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚æ—©æœŸçš„ç ”ç©¶å·¥ä½œè¯•å›¾é€šè¿‡å¼€å‘è·¨æ¨¡æ€å…¨å±€æˆ–å±€éƒ¨å¯¹é½ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå…¨å±€æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è·¨æ¨¡æ€çš„ç»†å¾®å·®å¼‚ï¼Œè€Œå±€éƒ¨æ–¹æ³•åˆ™éœ€è¦å…ˆéªŒä¿¡æ¯æ¥æ¢ç´¢æ˜ç¡®çš„å±€éƒ¨å¯¹é½ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºè‹±è¯­ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–åˆ›äº†ä¸€ä¸ªå¤šè¯­è¨€TIPRä»»åŠ¡ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªå¤šè¯­è¨€TIPRåŸºå‡†æ•°æ®é›†æ¥è¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåˆæ­¥ç¿»è¯‘ï¼Œå¹¶é€šè¿‡æ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†å¯¹å…¶è¿›è¡Œæ”¹è¿›ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†Bi-IRRAï¼šä¸€ä¸ªåŒå‘éšå¼å…³ç³»æ¨ç†å’Œå¯¹é½æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ è·¨è¯­è¨€å’Œæ¨¡æ€çš„å¯¹é½ã€‚åœ¨Bi-IRRAä¸­ï¼ŒåŒå‘éšå¼å…³ç³»æ¨ç†æ¨¡å—èƒ½å¤Ÿå®ç°æ©ç å›¾åƒå’Œæ–‡æœ¬çš„åŒå‘é¢„æµ‹ï¼Œä»è€Œéšå¼åœ°å¢å¼ºè·¨è¯­è¨€å’Œæ¨¡æ€çš„å±€éƒ¨å…³ç³»å»ºæ¨¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªå¤šç»´å…¨å±€å¯¹é½æ¨¡å—ï¼Œä»¥å¼¥åˆæ¨¡æ€å¼‚æ„æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ‰€æœ‰å¤šè¯­è¨€TIPRæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„æˆæœã€‚æ•°æ®å’Œä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Flame-Chasers/Bi-IRRA%E4%B8%8A%E5%85%AC%E5%B8%83%E3%80%82">https://github.com/Flame-Chasers/Bi-IRRAä¸Šå…¬å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17685v1">PDF</a> Final version published in IEEE Transactions on Pattern Analysis and   Machine Intelligence (TPAMI). Xplore link:   <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/11199360">https://ieeexplore.ieee.org/document/11199360</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒäººç‰©æ£€ç´¢ï¼ˆTIPRï¼‰æ—¨åœ¨ä½¿ç”¨æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›®æ ‡äººç‰©ï¼Œé¢ä¸´æ¨¡æ€å¤šæ ·æ€§æŒ‘æˆ˜ã€‚ä»¥å‰çš„å·¥ä½œé€šè¿‡å¼€å‘è·¨æ¨¡æ€å…¨å±€æˆ–å±€éƒ¨å¯¹é½ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå…¨å±€æ–¹æ³•é€šå¸¸å¿½ç•¥äº†è·¨æ¨¡æ€çš„ç»†å¾®å·®å¼‚ï¼Œè€Œå±€éƒ¨æ–¹æ³•åˆ™éœ€è¦æ¢ç´¢æ˜¾å¼éƒ¨åˆ†å¯¹é½çš„å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä»¥è‹±è¯­ä¸ºä¸­å¿ƒï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ç‡å…ˆè¿›è¡Œå¤šè¯­è¨€TIPRä»»åŠ¡ï¼Œé€šè¿‡å¼€å‘å¤šè¯­è¨€TIPRåŸºå‡†è¿›è¡Œæµ‹è¯•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåˆæ­¥ç¿»è¯‘ï¼Œå¹¶ç»“åˆé¢†åŸŸçŸ¥è¯†è¿›è¡Œä¼˜åŒ–ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†Bi-IRRAï¼šä¸€ç§åŒå‘éšå¼å…³ç³»æ¨ç†å’Œå¯¹é½æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ è·¨è¯­è¨€å’Œæ¨¡æ€çš„å¯¹é½ã€‚Bi-IRRAä¸­çš„åŒå‘éšå¼å…³ç³»æ¨ç†æ¨¡å—èƒ½å¤ŸåŒå‘é¢„æµ‹å›¾åƒå’Œæ–‡æœ¬ä¸­çš„æ©ç ï¼Œä»è€Œéšæ€§å¢å¼ºè·¨è¯­è¨€å’Œæ¨¡æ€çš„å±€éƒ¨å…³ç³»å»ºæ¨¡ã€‚åŒæ—¶é›†æˆäº†ä¸€ä¸ªå¤šç»´å…¨å±€å¯¹é½æ¨¡å—æ¥æ¡¥æ¥æ¨¡æ€å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•åœ¨æ‰€æœ‰å¤šè¯­è¨€TIPRæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚æ•°æ®å’Œä»£ç å¯è§äº<a target="_blank" rel="noopener" href="https://github.com/Flame-Chasers/Bi-IRRA%E3%80%82">https://github.com/Flame-Chasers/Bi-IRRAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIPRæ—¨åœ¨ä½¿ç”¨æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›®æ ‡äººç‰©ï¼Œé¢ä¸´æ¨¡æ€å¤šæ ·æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒ…æ‹¬å…¨å±€å’Œå±€éƒ¨å¯¹é½ç­–ç•¥ï¼Œä½†å„æœ‰ç¼ºç‚¹ã€‚</li>
<li>å½“å‰æ–¹æ³•ä»¥è‹±è¯­ä¸ºä¸­å¿ƒï¼Œç¼ºä¹å¤šè¯­è¨€æ”¯æŒã€‚</li>
<li>æå‡ºå¤šè¯­è¨€TIPRä»»åŠ¡å’Œå¤šè¯­è¨€TIPRåŸºå‡†ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåˆæ­¥ç¿»è¯‘å¹¶ç»“åˆé¢†åŸŸçŸ¥è¯†ä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†Bi-IRRAæ¡†æ¶ï¼ŒåŒ…æ‹¬åŒå‘éšå¼å…³ç³»æ¨ç†æ¨¡å—å’Œå¤šç»´å…¨å±€å¯¹é½æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5192f48164998ce63c1ba3109567569c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068565&auth_key=1761068565-0-0-2f1142d9a9e9059ff6ad1a08c694698d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-058b395859e6c883d8d632030bbd6e93~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068607&auth_key=1761068607-0-0-ba3c58842008a32cda24c2556e06bd48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1afa36ebdd22c010ffb5f6b542408a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068614&auth_key=1761068614-0-0-1819d187312515ef4c28ec340314124a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49b024bc00fe843f5f14359edef86ef2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068621&auth_key=1761068621-0-0-ceb4dfc767037f9c164ec61128924c9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LawChain-Modeling-Legal-Reasoning-Chains-for-Chinese-Tort-Case-Analysis"><a href="#LawChain-Modeling-Legal-Reasoning-Chains-for-Chinese-Tort-Case-Analysis" class="headerlink" title="LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis"></a>LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis</h2><p><strong>Authors:Huiyuan Xie, Chenyang Li, Huining Zhu, Chubin Zhang, Yuxiao Ye, Zhenghao Liu, Zhiyuan Liu</strong></p>
<p>Legal reasoning is a fundamental component of legal analysis and decision-making. Existing computational approaches to legal reasoning predominantly rely on generic reasoning frameworks such as syllogism and IRAC, which do not comprehensively examine the nuanced processes that underpin legal reasoning. Moreover, current research has largely focused on criminal cases, with insufficient modeling for civil cases. In this work, we present a novel framework for explicitly modeling legal reasoning in the analysis of Chinese tort-related civil cases. We first operationalize the legal reasoning processes used in tort analysis into the LawChain framework. LawChain is a three-module reasoning framework, with each module consisting of multiple finer-grained sub-steps. Informed by the LawChain framework, we introduce the task of tort legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to systematically assess the critical steps within analytical reasoning chains for tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large language models for their legal reasoning ability in civil tort contexts. Our results indicate that current models still fall short in accurately handling crucial elements of tort legal reasoning. Furthermore, we introduce several baseline approaches that explicitly incorporate LawChain-style reasoning through prompting or post-training. We conduct further experiments on additional legal analysis tasks, such as Legal Named-Entity Recognition and Criminal Damages Calculation, to verify the generalizability of these baselines. The proposed baseline approaches achieve significant improvements in tort-related legal reasoning and generalize well to related legal analysis tasks, thus demonstrating the value of explicitly modeling legal reasoning chains to enhance the reasoning capabilities of language models. </p>
<blockquote>
<p>æ³•å¾‹æ¨ç†æ˜¯æ³•å¾‹åˆ†æå’Œå†³ç­–åˆ¶å®šä¸­çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰çš„è®¡ç®—æ³•å¾‹æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–äºè¯¸å¦‚ä¸‰æ®µè®ºå’ŒIRACç­‰é€šç”¨æ¨ç†æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶å¹¶æ²¡æœ‰å…¨é¢è€ƒå¯Ÿæ”¯æ’‘æ³•å¾‹æ¨ç†çš„å¾®å¦™è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ‘äº‹æ¡ˆä»¶ä¸Šï¼Œå¯¹æ°‘äº‹æ¡ˆä»¶çš„å»ºæ¨¡è¿˜ä¸å¤Ÿå……åˆ†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ˜ç¡®å»ºæ¨¡æ³•å¾‹æ¨ç†çš„æ¡†æ¶ï¼Œç”¨äºåˆ†æä¸­å›½ä¾µæƒç›¸å…³çš„æ°‘äº‹æ¡ˆä»¶ã€‚æˆ‘ä»¬é¦–å…ˆå°†ä¾µæƒåˆ†æä¸­ä½¿ç”¨çš„æ³•å¾‹æ¨ç†è¿‡ç¨‹è½¬åŒ–ä¸ºLawChainæ¡†æ¶ã€‚LawChainæ˜¯ä¸€ä¸ªç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆçš„æ¨ç†æ¡†æ¶ï¼Œæ¯ä¸ªæ¨¡å—ç”±å¤šä¸ªæ›´ç²¾ç»†çš„å­æ­¥éª¤ç»„æˆã€‚åŸºäºLawChainæ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¾µæƒæ³•å¾‹æ¨ç†çš„ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†LawChain${eval}$ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°ä¾µæƒåˆ†æä¸­çš„åˆ†ææ¨ç†é“¾ä¸­çš„å…³é”®æ­¥éª¤ã€‚åˆ©ç”¨è¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨æ°‘äº‹ä¾µæƒèƒŒæ™¯ä¸‹çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†ä¾µæƒæ³•å¾‹æ¨ç†çš„å…³é”®è¦ç´ æ—¶ä»æœ‰ä¸è¶³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†å‡ ç§åŸºçº¿æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡æç¤ºæˆ–åè®­ç»ƒæ˜ç¡®èå…¥LawChainé£æ ¼çš„æ¨ç†ã€‚æˆ‘ä»¬å¯¹å…¶ä»–æ³•å¾‹åˆ†æä»»åŠ¡ï¼ˆå¦‚æ³•å¾‹å‘½åå®ä½“è¯†åˆ«å’Œåˆ‘äº‹æŸå®³èµ”å¿è®¡ç®—ï¼‰è¿›è¡Œäº†è¿›ä¸€æ­¥çš„å®éªŒï¼Œä»¥éªŒè¯è¿™äº›åŸºå‡†çš„é€šç”¨æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¾µæƒç›¸å…³çš„æ³•å¾‹æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°åº”ç”¨äºç›¸å…³çš„æ³•å¾‹åˆ†æä»»åŠ¡ï¼Œä»è€Œè¯æ˜äº†æ˜ç¡®å»ºæ¨¡æ³•å¾‹æ¨ç†é“¾å¯¹äºæé«˜è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17602v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ä¸­å›½æ³•å¾‹æ¨ç†é¢†åŸŸï¼Œç°æœ‰è®¡ç®—æ–¹æ³•ä¸»è¦ä¾èµ–é€šç”¨çš„æ¨ç†æ¡†æ¶å¦‚ä¸‰æ®µè®ºå’ŒIRACç­‰ï¼Œæ— æ³•å……åˆ†åæ˜ æ³•å¾‹æ¨ç†ä¸­çš„ç»†å¾®è¿‡ç¨‹ã€‚ç ”ç©¶å¤šèšç„¦äºåˆ‘äº‹æ¡ˆä»¶å»ºæ¨¡ï¼Œæ°‘äº‹æ¡ˆä»¶å»ºæ¨¡ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„æ¡†æ¶æ¥æ˜¾å¼å»ºæ¨¡ä¸­æ–‡ä¾µæƒç›¸å…³çš„æ°‘äº‹æ¡ˆä»¶çš„æ³•å¾‹æ¨ç†è¿‡ç¨‹ã€‚é‡‡ç”¨LawChainæ¡†æ¶æ¥ç»†åŒ–å’Œç³»ç»ŸåŒ–æ³•å¾‹æ¨ç†æ­¥éª¤ï¼Œå¹¶åŸºäºæ­¤æ„å»ºè¯„ä¼°åŸºå‡†æ¥è¯„ä¼°å¤§æ¨¡å‹åœ¨æ°‘äº‹ä¾µæƒè¯­å¢ƒä¸‹çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹ä»éœ€åœ¨å…³é”®è¦ç´ ä¸Šæ”¹è¿›ï¼Œé€šè¿‡å¼•å…¥åŸºäºLawChainçš„æ¨ç†æ–¹æ³•ï¼Œå¦‚æç¤ºæˆ–åè®­ç»ƒç­‰æ–¹æ³•ä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†æ³•å¾‹æ¨ç†èƒ½åŠ›å¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚è¿™ä¸ºç›¸å…³ä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆæ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ³•å¾‹æ¨ç†æ˜¯æ³•å¾‹åˆ†æå’Œå†³ç­–åˆ¶å®šä¸­çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å½“å‰è®¡ç®—æ–¹æ³•å°šä¸èƒ½å®Œå…¨åæ˜ æ³•å¾‹æ¨ç†çš„ç»†å¾®è¿‡ç¨‹ã€‚</li>
<li>ç›®å‰ç ”ç©¶å¤šé›†ä¸­åœ¨åˆ‘äº‹æ¡ˆä¾‹å»ºæ¨¡ä¸Šï¼Œæ°‘äº‹æ¡ˆä¾‹å»ºæ¨¡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶æ¥æ˜¾å¼å»ºæ¨¡ä¸­æ–‡ä¾µæƒç›¸å…³çš„æ°‘äº‹æ¡ˆä»¶çš„æ³•å¾‹æ¨ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨LawChainæ¡†æ¶è¿›è¡Œç³»ç»ŸåŒ–åˆ†æã€‚</li>
<li>LawChainæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ¨¡å—å’Œå¤šä¸ªæ›´ç²¾ç»†çš„å­æ­¥éª¤ï¼Œç”¨äºç»†åŒ–æ³•å¾‹æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åŸºäºLawChainæ„å»ºè¯„ä¼°åŸºå‡†ä»¥è¯„ä¼°å¤§æ¨¡å‹åœ¨æ°‘äº‹ä¾µæƒè¯­å¢ƒä¸‹çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ³•å¾‹æ¨ç†çš„å…³é”®è¦ç´ ä¸Šä»æœ‰ä¸è¶³ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8f9720c81ae12c578f5b316e7402f483~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068628&auth_key=1761068628-0-0-412d39c86a9f9e07d37859cfb6466a36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13479967fb0dc016e87deb92330820e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068635&auth_key=1761068635-0-0-73484138cb40cdf9c620f557ac656f28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71fbeab4e2b2c31c00eb0705b2ab4780~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068642&auth_key=1761068642-0-0-0a9836ec625f8fc89f06294fbe44f760&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Reasoning-Distillation-and-Structural-Alignment-for-Improved-Code-Generation"><a href="#Reasoning-Distillation-and-Structural-Alignment-for-Improved-Code-Generation" class="headerlink" title="Reasoning Distillation and Structural Alignment for Improved Code   Generation"></a>Reasoning Distillation and Structural Alignment for Improved Code   Generation</h2><p><strong>Authors:Amir Jalilifard, Anderson de Rezende Rocha, Marcos Medeiros Raimundo</strong></p>
<p>Effective code generation with language models hinges on two critical factors: accurately understanding the intent of the prompt and generating code that applies algorithmic reasoning to produce correct solutions capable of passing diverse test cases while adhering to the syntax of the target programming language. Unlike other language tasks, code generation requires more than accurate token prediction; it demands comprehension of solution-level and structural relationships rather than merely generating the most likely tokens. very large language model (VLLM) are capable of generating detailed steps toward the correct solution of complex tasks where reasoning is crucial in solving the problem. Such reasoning capabilities may be absent in smaller language models. Therefore, in this work, we distill the reasoning capabilities of a VLLM into a smaller, more efficient model that is faster and cheaper to deploy. Our approach trains the model to emulate the reasoning and problem-solving abilities of the VLLM by learning to identify correct solution pathways and establishing a structural correspondence between problem definitions and potential solutions through a novel method of structure-aware loss optimization. This enables the model to transcend token-level generation and to deeply grasp the overarching structure of solutions for given problems. Experimental results show that our fine-tuned model, developed through a cheap and simple to implement process, significantly outperforms our baseline model in terms of pass@1, average data flow, and average syntax match metrics across the MBPP, MBPP Plus, and HumanEval benchmarks. </p>
<blockquote>
<p>ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ•ˆä»£ç ç”Ÿæˆçš„å…³é”®åœ¨äºä¸¤ä¸ªé‡è¦å› ç´ ï¼šå‡†ç¡®ç†è§£æç¤ºçš„æ„å›¾ï¼Œå¹¶ç”Ÿæˆèƒ½å¤Ÿåº”ç”¨ç®—æ³•æ¨ç†æ¥äº§ç”Ÿæ­£ç¡®è§£å†³æ–¹æ¡ˆçš„ä»£ç ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆéœ€è¦èƒ½å¤Ÿé€šè¿‡å¤šç§æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒæ—¶éµå¾ªç›®æ ‡ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€‚ä¸å…¶ä»–è¯­è¨€ä»»åŠ¡ä¸åŒï¼Œä»£ç ç”Ÿæˆä¸ä»…ä»…è¦æ±‚å‡†ç¡®çš„ä»¤ç‰Œé¢„æµ‹ï¼›å®ƒéœ€è¦çš„æ˜¯è§£å†³æ–¹æ¡ˆçº§åˆ«å’Œç»“æ„å…³ç³»çš„ç†è§£ï¼Œè€Œä¸ä»…ä»…æ˜¯ç”Ÿæˆæœ€å¯èƒ½çš„ä»¤ç‰Œã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰èƒ½å¤Ÿç”Ÿæˆå…³äºå¤æ‚ä»»åŠ¡çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„è¯¦ç»†æ­¥éª¤ï¼Œå…¶ä¸­æ¨ç†æ˜¯è§£å†³é—®é¢˜çš„å…³é”®ã€‚è¿™æ ·çš„æ¨ç†èƒ½åŠ›åœ¨å°è¯­è¨€æ¨¡å‹ä¸­å¯èƒ½ä¸å­˜åœ¨ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°ä¸€ä¸ªæ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹éƒ¨ç½²æ›´å¿«ã€æˆæœ¬æ›´ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡ä¸€ç§æ–°å‹çš„ç»“æ„æ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ–¹æ³•ï¼Œè®­ç»ƒæ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œå­¦ä¹ è¯†åˆ«æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆé€”å¾„ï¼Œå¹¶å»ºç«‹é—®é¢˜å®šä¹‰å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä»¤ç‰Œçº§åˆ«çš„ç”Ÿæˆï¼Œå¹¶æ·±åˆ»æŠŠæ¡ç»™å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆçš„æ€»ä½“ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬ç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç§ä»·æ ¼ä½å»‰ã€æ˜“äºå®ç°çš„æµç¨‹å¼€å‘ï¼Œåœ¨MBPPã€MBPP Pluså’ŒHumanEvalåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨pass@1ã€å¹³å‡æ•°æ®æµå’Œå¹³å‡è¯­æ³•åŒ¹é…æŒ‡æ ‡æ–¹é¢æ˜¾è‘—ä¼˜äºæˆ‘ä»¬çš„åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰è¿›è¡Œä»£ç ç”Ÿæˆçš„å…³é”®åœ¨äºå‡†ç¡®ç†è§£æç¤ºæ„å›¾å¹¶ç”Ÿæˆå¯åº”ç”¨äºç®—æ³•æ¨ç†çš„æ­£ç¡®ä»£ç ï¼Œèƒ½é€šè¿‡å„ç§æµ‹è¯•ç”¨ä¾‹å¹¶éµå®ˆç›®æ ‡ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶äººå‘˜å°†VLLMçš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°ä¸€ä¸ªæ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆè·¯å¾„å¹¶å»ºç«‹é—®é¢˜å®šä¹‰ä¸æ½œåœ¨è§£å†³æ–¹æ¡ˆä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç®€å•å»‰ä»·çš„è¿‡ç¨‹è¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹åœ¨MBPPã€MBPP Pluså’ŒHumanEvalç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»£ç ç”Ÿæˆä¾èµ–äºå‡†ç¡®ç†è§£æç¤ºæ„å›¾å’Œç”Ÿæˆå…·å¤‡ç®—æ³•æ¨ç†èƒ½åŠ›çš„ä»£ç ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰å…·å¤‡ç”Ÿæˆå¤æ‚ä»»åŠ¡æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„è¯¦ç»†æ­¥éª¤çš„èƒ½åŠ›ã€‚</li>
<li>å°†VLLMçš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ï¼Œå®ç°æ›´å¿«ã€æ›´ç»æµçš„éƒ¨ç½²ã€‚</li>
<li>é€šè¿‡ç»“æ„æ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä»¥è¯†åˆ«æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆè·¯å¾„å¹¶å»ºç«‹é—®é¢˜å®šä¹‰ä¸è§£å†³æ–¹æ¡ˆä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚</li>
<li>æ¨¡å‹èƒ½è¶…è¶Šè¯çº§ç”Ÿæˆï¼Œæ·±å…¥æŠŠæ¡ç»™å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆçš„æ•´ä½“ç»“æ„ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6f07322a865ebb3c1b6ecd7f32e29bf7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068650&auth_key=1761068650-0-0-86d0ce417adff22088ed5e6a7abc7a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5374d88f25952c69ee0819b70c230707~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068657&auth_key=1761068657-0-0-09b0b03840e5bd68b0e7153025c45fd6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16e920f3cf16662b6ff32dfb8b96fc0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068663&auth_key=1761068663-0-0-cac6012f917da1d1aba7195a641c5f77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OncoReason-Structuring-Clinical-Reasoning-in-LLMs-for-Robust-and-Interpretable-Survival-Prediction"><a href="#OncoReason-Structuring-Clinical-Reasoning-in-LLMs-for-Robust-and-Interpretable-Survival-Prediction" class="headerlink" title="OncoReason: Structuring Clinical Reasoning in LLMs for Robust and   Interpretable Survival Prediction"></a>OncoReason: Structuring Clinical Reasoning in LLMs for Robust and   Interpretable Survival Prediction</h2><p><strong>Authors:Raghu Vamshi Hemadri, Geetha Krishna Guruju, Kristi Topollai, Anna Ewa Choromanska</strong></p>
<p>Predicting cancer treatment outcomes requires models that are both accurate and interpretable, particularly in the presence of heterogeneous clinical data. While large language models (LLMs) have shown strong performance in biomedical NLP, they often lack structured reasoning capabilities critical for high-stakes decision support. We present a unified, multi-task learning framework that aligns autoregressive LLMs with clinical reasoning for outcome prediction on the MSK-CHORD dataset. Our models are trained to jointly perform binary survival classification, continuous survival time regression, and natural language rationale generation. We evaluate three alignment strategies: (1) standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT) prompting to elicit step-by-step reasoning, and (3) Group Relative Policy Optimization (GRPO), a reinforcement learning method that aligns model outputs to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and predictive performance across BLEU, ROUGE, and BERTScore. We further show that existing biomedical LLMs often fail to produce valid reasoning traces due to architectural constraints. Our findings underscore the importance of reasoning-aware alignment in multi-task clinical modeling and set a new benchmark for interpretable, trustworthy LLMs in precision oncology. </p>
<blockquote>
<p>é¢„æµ‹ç™Œç—‡æ²»ç–—ç»“æœéœ€è¦æ—¢å‡†ç¡®åˆæ˜“äºè§£é‡Šçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨ä¸´åºŠæ•°æ®å¼‚è´¨æ€§æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹å…³é”®çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºé«˜é£é™©å†³ç­–æ”¯æŒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ã€å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå°†è‡ªå›å½’LLMä¸ä¸´åºŠæ¨ç†ç›¸ç»“åˆï¼Œåœ¨MSK-CHORDæ•°æ®é›†ä¸Šè¿›è¡Œç»“æœé¢„æµ‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤ŸåŒæ—¶æ‰§è¡ŒäºŒå…ƒç”Ÿå­˜åˆ†ç±»ã€è¿ç»­ç”Ÿå­˜æ—¶é—´å›å½’å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç”Ÿæˆã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å¯¹é½ç­–ç•¥ï¼šï¼ˆ1ï¼‰æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œï¼ˆ2ï¼‰å¸¦æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºçš„SFTï¼Œä»¥æ¿€å‘é€æ­¥æ¨ç†ï¼Œä»¥åŠï¼ˆ3ï¼‰ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†æ¨¡å‹è¾“å‡ºä¸ä¸“å®¶æ¨å¯¼çš„æ¨ç†è½¨è¿¹å¯¹é½ã€‚ä½¿ç”¨LLaMa3-8Bå’ŒMed42-8Béª¨å¹²ç½‘è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCoTæç¤ºæé«˜äº†F1åˆ†æ•°+6.0ï¼Œå¹¶é™ä½äº†MAEçš„12%ï¼Œè€ŒGRPOåœ¨BLEUã€ROUGEå’ŒBERTScoreä¸Šå®ç°äº†ä¸šç•Œæœ€ä½³çš„è§£é‡Šæ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œç”±äºæ¶æ„çº¦æŸï¼Œç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦LLMå¾€å¾€æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¤šä»»åŠ¡ä¸´åºŠå»ºæ¨¡ä¸­æ¨ç†æ„ŸçŸ¥å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç²¾ç¡®è‚¿ç˜¤å­¦ä¸­çš„å¯è§£é‡Šã€å¯ä¿¡èµ–çš„LLMè®¾ç½®äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17532v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†è‡ªå›å½’çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ä¸´åºŠæ¨ç†ç›¸ç»“åˆï¼Œç”¨äºåœ¨MSK-CHORDæ•°æ®é›†ä¸Šè¿›è¡Œç™Œç—‡æ²»ç–—ç»“æœé¢„æµ‹ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶è¿›è¡ŒäºŒå…ƒç”Ÿå­˜åˆ†ç±»ã€è¿ç»­ç”Ÿå­˜æ—¶é—´å›å½’å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºè¿›è¡Œæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥æé«˜F1åˆ†æ•°å¹¶é™ä½MAEï¼Œè€ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰æ–¹æ³•åˆ™å®ç°äº†æ›´é«˜çš„è§£é‡Šæ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šä»»åŠ¡ä¸´åºŠå»ºæ¨¡ä¸­æ¨ç†å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç²¾å‡†è‚¿ç˜¤å­¦ä¸­çš„å¯è§£é‡Šã€å¯ä¿¡çš„å¤§å‹è¯­è¨€æ¨¡å‹æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦NLPä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç¼ºä¹ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºé«˜é£é™©å†³ç­–æ”¯æŒè‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†LLMså’Œä¸´åºŠæ¨ç†ï¼Œç”¨äºé¢„æµ‹ç™Œç—‡æ²»ç–—ç»“æœã€‚</li>
<li>æ¡†æ¶èƒ½å¤ŸåŒæ—¶è¿›è¡ŒäºŒå…ƒç”Ÿå­˜åˆ†ç±»ã€è¿ç»­ç”Ÿå­˜æ—¶é—´å›å½’ä»¥åŠè‡ªç„¶è¯­è¨€æ¨ç†ç”Ÿæˆã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ç§å¯¹é½ç­–ç•¥ï¼ŒåŒ…æ‹¬æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¸¦æœ‰Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºçš„SFTä»¥åŠGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ã€‚</li>
<li>CoTæç¤ºå¯ä»¥æé«˜F1åˆ†æ•°å¹¶é™ä½MAEï¼Œè€ŒGRPOæ–¹æ³•åœ¨BLEUã€ROUGEå’ŒBERTScoreç­‰æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§£é‡Šæ€§å’Œé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰ç”Ÿç‰©åŒ»å­¦LLMsç”±äºæ¶æ„çº¦æŸï¼Œå¾€å¾€æ— æ³•äº§ç”Ÿæœ‰æ•ˆçš„æ¨ç†è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7d6584160080af8f15d8c30909639763~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068671&auth_key=1761068671-0-0-c1493ee328ce94f84b50647b0a29e6e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-904b8db19391cd3c65d3f66a392e732c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068677&auth_key=1761068677-0-0-2922c76433a1f9a72719175d36fbb511&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1eb0ca7cf18ed83e540f2374417964af~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068684&auth_key=1761068684-0-0-b15e93d4ea2db4ce0308c8b238e88315&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c70e89d03bae284390b1156e237e9715~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068690&auth_key=1761068690-0-0-263196e62dff38123de3380c4d0dcc1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38e5d75e965599f54d64220950ac9024~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068697&auth_key=1761068697-0-0-55ddae584326116d74d84d3cf5ffc4db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors"><a href="#SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors" class="headerlink" title="SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors"></a>SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors</h2><p><strong>Authors:Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Dirk Hovy, Nigel Collier, Paul RÃ¶ttger</strong></p>
<p>Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80&#x2F;100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r&#x3D;0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿå…·æœ‰é¢ è¦†ç¤¾ä¼šå’Œè¡Œä¸ºç§‘å­¦çš„æ½œåŠ›ï¼Œä½†è¿™ç§æ½œåŠ›çš„å‘æŒ¥å–å†³äºå®ƒä»¬æ˜¯å¦èƒ½çœŸå®åœ°åæ˜ äººç±»è¡Œä¸ºã€‚ç›®å‰çš„è¯„ä¼°æ˜¯é›¶ç¢çš„ï¼ŒåŸºäºç‰¹å®šä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œå¯¼è‡´ç»“æœæ— æ³•æ¯”è¾ƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç¨³å¥ã€å¯é‡å¤çš„LLMæ¨¡æ‹Ÿç§‘å­¦ã€‚SimBenchç»Ÿä¸€äº†20ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œæ¶µç›–äº†ä»é“å¾·å†³ç­–åˆ°ç»æµé€‰æ‹©çš„ä»»åŠ¡ï¼Œæ¶µç›–äº†å¤§é‡å…¨çƒå‚ä¸è€…ï¼Œä¸ºå…³äºLLMæ¨¡æ‹Ÿä½•æ—¶ã€å¦‚ä½•ä»¥åŠä¸ºä½•æˆåŠŸæˆ–å¤±è´¥çš„æ ¹æœ¬é—®é¢˜æä¾›äº†å¿…è¦çš„åŸºç¡€ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å¥½çš„LLMæ¨¡æ‹Ÿèƒ½åŠ›ä¹Ÿæœ‰é™ï¼ˆå¾—åˆ†ï¼š40.80&#x2F;100ï¼‰ï¼Œæ€§èƒ½éšæ¨¡å‹å¤§å°çš„å¯¹æ•°çº¿æ€§å˜åŒ–ã€‚å¢åŠ æ¨ç†æ—¶é—´çš„è®¡ç®—å¹¶ä¸ä¼šæé«˜æ¨¡æ‹Ÿæ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†æ¨¡æ‹ŸæŒ‡ä»¤ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼šæŒ‡ä»¤è°ƒæ•´å¯ä»¥æé«˜ä½ç†µï¼ˆå…±è¯†ï¼‰é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œä½†ä¼šé™ä½é«˜ç†µï¼ˆå¤šæ ·åŒ–ï¼‰é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚å½“æ¨¡æ‹Ÿç‰¹å®šçš„äººå£ç¾¤ä½“æ—¶ï¼Œæ¨¡å‹ç‰¹åˆ«å›°éš¾ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜æ¨¡æ‹Ÿèƒ½åŠ›ä¸æ·±å…¥çš„çŸ¥è¯†å¯†é›†å‹æ¨ç†ï¼ˆMMLU-Proï¼Œr&#x3D;0.939ï¼‰å…³è”åº¦æœ€é«˜ã€‚é€šè¿‡ä½¿è¿›æ­¥å¯è¡¡é‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ é€Ÿæ›´çœŸå®çš„LLMæ¨¡æ‹Ÿå™¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17516v1">PDF</a> Project Website: <a target="_blank" rel="noopener" href="http://simbench.tiancheng.hu/">http://simbench.tiancheng.hu/</a> Data:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/pitehu/SimBench">https://huggingface.co/datasets/pitehu/SimBench</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿå…·æœ‰é©æ–°ç¤¾ä¼šå’Œè¡Œä¸ºç§‘å­¦çš„æ½œåŠ›ï¼Œå‰ææ˜¯å¿…é¡»çœŸå®åæ˜ äººç±»è¡Œä¸ºã€‚ä¸ºè¯„ä¼°LLMæ¨¡æ‹Ÿæ•ˆæœï¼Œæˆ‘ä»¬æ¨å‡ºSimBenchï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å»ºç«‹ç¨³å¥ã€å¯é‡å¤çš„ç§‘å­¦LLMæ¨¡æ‹Ÿã€‚SimBenchç»Ÿä¸€äº†20ä¸ªæ¶µç›–ä»é“å¾·å†³ç­–åˆ°ç»æµé€‰æ‹©ç­‰ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œä¸ºå…¨çƒå¤§é‡å‚ä¸è€…æä¾›äº†å¿…è¦çš„åŸºçŸ³ï¼Œä»¥æ¢ç©¶LLMæ¨¡æ‹Ÿä½•æ—¶ã€å¦‚ä½•ã€ä¸ºä½•æˆåŠŸæˆ–å¤±è´¥ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å¥½çš„LLMï¼Œå…¶æ¨¡æ‹Ÿèƒ½åŠ›ä¹Ÿæœ‰é™ï¼ˆå¾—åˆ†ï¼š40.80&#x2F;100ï¼‰ï¼Œä¸”æ€§èƒ½éšæ¨¡å‹å¤§å°çš„å¯¹æ•°çº¿æ€§æ‰©å±•è€Œæé«˜ã€‚æ¨¡æ‹Ÿæ€§èƒ½å¹¶ä¸ä¼šé€šè¿‡å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—æ¥æ”¹å–„ã€‚æˆ‘ä»¬è¯æ˜äº†å¯¹é½æ¨¡æ‹Ÿä¹‹é—´çš„æƒè¡¡ï¼šæŒ‡ä»¤è°ƒæ•´æé«˜äº†åœ¨ä½ç†µï¼ˆå…±è¯†ï¼‰é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œä½†åœ¨é«˜ç†µï¼ˆå¤šæ ·æ€§ï¼‰é—®é¢˜ä¸Šå´é™ä½äº†æ€§èƒ½ã€‚æ¨¡å‹åœ¨æ¨¡æ‹Ÿç‰¹å®šäººå£ç¾¤ä½“æ—¶å°¤å…¶å›°éš¾ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ¨¡æ‹Ÿèƒ½åŠ›ä¸æ·±å…¥çš„çŸ¥è¯†æ¨ç†å¯†åˆ‡ç›¸å…³ï¼ˆMMLU-Proï¼Œr&#x3D;0.939ï¼‰ã€‚SimBenchçš„ç›®æ ‡æ˜¯å»ºç«‹å¯è¡¡é‡çš„è¿›æ­¥ï¼Œä»¥åŠ é€Ÿæ›´çœŸå®LLMæ¨¡æ‹Ÿå™¨çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿå…·æœ‰æ½œåŠ›ï¼Œå‰ææ˜¯åæ˜ çœŸå®äººç±»è¡Œä¸ºã€‚</li>
<li>SimBenchæ˜¯é¦–ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMæ¨¡æ‹Ÿæ•ˆæœã€‚</li>
<li>LLMæ¨¡æ‹Ÿèƒ½åŠ›æœ‰é™ï¼Œæ€§èƒ½éšæ¨¡å‹å¤§å°æ‰©å±•è€Œæé«˜ã€‚</li>
<li>æ¨¡æ‹Ÿæ€§èƒ½ä¸æ¨ç†æ—¶é—´è®¡ç®—æ— å…³ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´åœ¨ä½ç†µé—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨é«˜ç†µé—®é¢˜ä¸Šå­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨¡æ‹Ÿç‰¹å®šäººå£ç¾¤ä½“æ—¶é‡åˆ°å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-36792ede46dcf53442267b04771e109d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068704&auth_key=1761068704-0-0-2e886da63ac28cbfe5a6db37e371ca3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d4c1ed777d490906f144590f6ceafc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068711&auth_key=1761068711-0-0-736ab970fe296b2e16d9c3c80a28196a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="I-RAVEN-X-Benchmarking-Generalization-and-Robustness-of-Analogical-and-Mathematical-Reasoning-in-Large-Language-and-Reasoning-Models"><a href="#I-RAVEN-X-Benchmarking-Generalization-and-Robustness-of-Analogical-and-Mathematical-Reasoning-in-Large-Language-and-Reasoning-Models" class="headerlink" title="I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and   Mathematical Reasoning in Large Language and Reasoning Models"></a>I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and   Mathematical Reasoning in Large Language and Reasoning Models</h2><p><strong>Authors:Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi</strong></p>
<p>We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†I-RAVEN-Xï¼Œè¿™æ˜¯ä¸€ä¸ªç¬¦å·åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰åœ¨ç±»æ¯”å’Œæ•°å­¦æ¨ç†ä¸­çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚I-RAVEN-Xé€šè¿‡å¢åŠ æ“ä½œæ•°å¤æ‚æ€§ã€å±æ€§èŒƒå›´å’Œå¼•å…¥æ„ŸçŸ¥ä¸ç¡®å®šæ€§æ¥æ‰©å±•I-RAVENã€‚ä¸LLMç›¸æ¯”ï¼Œå®è¯ç»“æœè¡¨æ˜ï¼ŒLRMåœ¨å¤„ç†è¾ƒé•¿çš„æ¨ç†å…³ç³»å’Œæ›´å¹¿æ³›çš„å±æ€§èŒƒå›´æ–¹é¢ï¼Œåˆ†åˆ«æé«˜äº†ç”Ÿäº§åŠ›å’Œç³»ç»Ÿæ€§ã€‚ç„¶è€Œï¼ŒLRMåœ¨ä¸ç¡®å®šæ€§æ¨ç†æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œæ— æ³•æœ‰æ•ˆæ¢ç´¢å¤šç§æ¦‚ç‡ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17496v1">PDF</a> Accepted at the 5th Workshop on Mathematical Reasoning and AI   (MATH-AI), NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬ä»‹ç»äº†I-RAVEN-Xï¼Œè¿™æ˜¯ä¸€ä¸ªç¬¦å·åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ç±»æ¯”å’Œæ•°å­¦æ¨ç†ä¸­çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚I-RAVEN-Xé€šè¿‡å¢åŠ æ“ä½œæ•°å¤æ‚æ€§ã€å±æ€§èŒƒå›´å’Œå¼•å…¥æ„ŸçŸ¥ä¸ç¡®å®šæ€§æ¥æ‰©å±•I-RAVENã€‚ä¸LLMsç›¸æ¯”ï¼Œå®è¯ç»“æœè¡¨æ˜LRMsåœ¨è¾ƒé•¿æ¨ç†å…³ç³»å’Œæ›´å¹¿æ³›çš„å±æ€§èŒƒå›´æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„ç”Ÿäº§åŠ›å’Œç³»ç»Ÿæ€§ï¼Œä½†åœ¨ä¸ç¡®å®šä¸‹çš„æ¨ç†å’Œå¤šä¸ªæ¦‚ç‡ç»“æœçš„æ¢ç´¢æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>I-RAVEN-Xæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ³›åŒ–å’Œé²æ£’æ€§çš„ç¬¦å·åŸºå‡†æµ‹è¯•ã€‚</li>
<li>I-RAVEN-Xæ‰©å±•äº†I-RAVENï¼Œå¢åŠ äº†æ“ä½œæ•°å¤æ‚æ€§ã€å±æ€§èŒƒå›´ï¼Œå¹¶å¼•å…¥äº†æ„ŸçŸ¥ä¸ç¡®å®šæ€§ã€‚</li>
<li>LRMsåœ¨è¾ƒé•¿æ¨ç†å…³ç³»å’Œæ›´å¹¿æ³›çš„å±æ€§èŒƒå›´æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„ç”Ÿäº§åŠ›å’Œç³»ç»Ÿæ€§ã€‚</li>
<li>LRMsåœ¨ä¸ç¡®å®šä¸‹çš„æ¨ç†å’Œå¤šä¸ªæ¦‚ç‡ç»“æœçš„æ¢ç´¢æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼Œä¸LLMsç›¸æ¯”ï¼ŒLRMsåœ¨æŸäº›æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†ä¹Ÿæœ‰å…¶å±€é™ã€‚</li>
<li>I-RAVEN-Xä¸ºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚å’Œä¸ç¡®å®šç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-01e4760473d85870051c2910f51471fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068718&auth_key=1761068718-0-0-38fd57adce471b342003f10f6d0d52d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7680ea06653ea3d96d55954cd285a848~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068725&auth_key=1761068725-0-0-d8072306d3be579c9b6b000138254f4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db06d2dcc05106c5733e58e2674fac4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068732&auth_key=1761068732-0-0-200f5013cb1f7c7503028e766c7a84cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94a3dd9cef9d4abacdb33cf290461dce~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068739&auth_key=1761068739-0-0-b9aca48301fb3b579a48de1a67fce9e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Leveraging-Group-Relative-Policy-Optimization-to-Advance-Large-Language-Models-in-Traditional-Chinese-Medicine"><a href="#Leveraging-Group-Relative-Policy-Optimization-to-Advance-Large-Language-Models-in-Traditional-Chinese-Medicine" class="headerlink" title="Leveraging Group Relative Policy Optimization to Advance Large Language   Models in Traditional Chinese Medicine"></a>Leveraging Group Relative Policy Optimization to Advance Large Language   Models in Traditional Chinese Medicine</h2><p><strong>Authors:Jiacheng Xie, Shuai Zeng, Yang Yu, Xiaoting Tang, Guanghui An, Dong Xu</strong></p>
<p>Traditional Chinese Medicine (TCM) presents a rich and structurally unique knowledge system that challenges conventional applications of large language models (LLMs). Although previous TCM-specific LLMs have shown progress through supervised fine-tuning, they often face limitations in alignment, data quality, and evaluation consistency. In this study, we introduce Ladder-base, the first TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a reinforcement learning method that improves reasoning and factual consistency by optimizing response selection based on intra-group comparisons. Ladder-base is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data for training and the remaining 20 percent split evenly between validation and test sets. Through standardized evaluation, Ladder-base demonstrates superior performance across multiple reasoning metrics when compared to both state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and Zhongjing. These findings suggest that GRPO provides an effective and efficient strategy for aligning LLMs with expert-level reasoning in traditional medical domains and supports the development of trustworthy and clinically grounded TCM artificial intelligence systems. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰å‘ˆç°äº†ä¸€ä¸ªä¸°å¯Œä¸”ç»“æ„ç‹¬ç‰¹çš„çŸ¥è¯†ä½“ç³»ï¼Œè¿™å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸è§„åº”ç”¨æå‡ºäº†æŒ‘æˆ˜ã€‚å°½ç®¡ä¹‹å‰é’ˆå¯¹TCMçš„LLMé€šè¿‡ç›‘ç£å¾®è°ƒå–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨å¯¹é½ã€æ•°æ®è´¨é‡å’Œè¯„ä¼°ä¸€è‡´æ€§æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Ladder-baseï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºTCMçš„LLMï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–åŸºäºç»„å†…æ¯”è¾ƒçš„å“åº”é€‰æ‹©æ¥æé«˜æ¨ç†å’Œäº‹å®ä¸€è‡´æ€§ã€‚Ladder-baseå»ºç«‹åœ¨Qwen2.5-7B-InstructåŸºç¡€æ¨¡å‹ä¸Šï¼Œä»…å¯¹TCM-LadderåŸºå‡†æµ‹è¯•æ–‡æœ¬å­é›†è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­80%çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œå…¶ä½™20%çš„æ•°æ®å¹³å‡åˆ†é…ç»™éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚é€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°ï¼Œä¸æœ€å…ˆè¿›çš„é€šç”¨LLMï¼ˆå¦‚GPT-4ã€Gemini 2.5ã€Claude 3å’ŒQwen3ï¼‰ä»¥åŠç‰¹å®šé¢†åŸŸçš„TCMæ¨¡å‹ï¼ˆåŒ…æ‹¬BenTsaoã€HuatuoGPT2å’ŒZhongjingï¼‰ç›¸æ¯”ï¼ŒLadder-baseåœ¨å¤šä¸ªæ¨ç†æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒGRPOä¸ºåœ¨ä¼ ç»ŸåŒ»å­¦é¢†åŸŸä¸­å®ç°LLMä¸ä¸“å®¶çº§æ¨ç†çš„å¯¹é½æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„ç­–ç•¥ï¼Œå¹¶æ”¯æŒå¼€å‘å¯ä¿¡ä¸”ä»¥ä¸´åºŠä¸ºåŸºç¡€çš„TCMäººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17402v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†Ladder-baseï¼Œä¸€ä¸ªä¸“æ³¨äºä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•è®­ç»ƒï¼Œæé«˜äº†æ¨ç†å’Œäº‹å®ä¸€è‡´æ€§ã€‚é€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°ï¼ŒLadder-baseåœ¨å¤šä¸ªæ¨ç†æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸å…¶ä»–é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„LLMç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚è¿™è¡¨æ˜GRPOæ˜¯æœ‰æ•ˆä¸”é«˜æ•ˆçš„ç­–ç•¥ï¼Œå¯å¸®åŠ©LLMåœ¨åŒ»å­¦é¢†åŸŸå®ç°ä¸“å®¶çº§æ¨ç†ï¼Œæ”¯æŒå¼€å‘å¯ä¿¡ä¸”åŸºäºä¸´åºŠçš„ä¸­åŒ»äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Ladder-baseæ˜¯ç¬¬ä¸€ä¸ªä¸“æ³¨äºä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>Ladder-baseé‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œæé«˜æ¨ç†å’Œäº‹å®ä¸€è‡´æ€§ã€‚</li>
<li>Ladder-baseåœ¨TCM-LadderåŸºå‡†æµ‹è¯•çš„æ–‡æœ¬å­é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ•°æ®é›†åˆ†ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ã€‚</li>
<li>ä¸å…¶ä»–é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„LLMç›¸æ¯”ï¼ŒLadder-baseåœ¨å¤šä¸ªæ¨ç†æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>GRPOæ–¹æ³•ä¸ºåŒ»å­¦é¢†åŸŸçš„ä¸“å®¶çº§æ¨ç†æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>Ladder-baseçš„å¼€å‘æ”¯æŒå¼€å‘å¯ä¿¡ä¸”åŸºäºä¸´åºŠçš„ä¸­åŒ»äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»Ÿä¸­åŒ»çŸ¥è¯†çš„ç‹¬ç‰¹æ€§å’Œå¤æ‚æ€§ï¼Œéœ€è¦ä¸“é—¨çš„LLMæ¥å¤„ç†å’Œåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5e1855a168ff76f180aede4d0a469b97~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068746&auth_key=1761068746-0-0-4a48953a0cbd7314d6cf351bc8bf436b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b433d956ecbe14525686c3c062a5045f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068753&auth_key=1761068753-0-0-72f181dc5b1fafe4168b6bc75f3f4746&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9e0d99ab0f95d5f898f7c3b0dc4bf55~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068760&auth_key=1761068760-0-0-bea6e19886514b935297f0ef10c88ede&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EduAdapt-A-Question-Answer-Benchmark-Dataset-for-Evaluating-Grade-Level-Adaptability-in-LLMs"><a href="#EduAdapt-A-Question-Answer-Benchmark-Dataset-for-Evaluating-Grade-Level-Adaptability-in-LLMs" class="headerlink" title="EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level   Adaptability in LLMs"></a>EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level   Adaptability in LLMs</h2><p><strong>Authors:Numaan Naeem, Abdellah El Mekki, Muhammad Abdul-Mageed</strong></p>
<p>Large language models (LLMs) are transforming education by answering questions, explaining complex concepts, and generating content across a wide range of subjects. Despite strong performance on academic benchmarks, they often fail to tailor responses to studentsâ€™ grade levels. This is a critical need in K-12 education, where age-appropriate vocabulary and explanation are essential for effective learning. Existing models frequently produce outputs that are too advanced or vague for younger learners, and there are no standardized benchmarks to evaluate their ability to adjust across cognitive and developmental stages. To address this gap, we introduce EduAdapt, a benchmark of nearly 48k grade-labeled QA pairs across nine science subjects, spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse set of open-source LLMs on EduAdapt and find that while larger models generally perform better, they still struggle with generating suitable responses for early-grade students (Grades 1-5). Our work presents the first dataset and evaluation framework for assessing grade-level adaptability in LLMs, aiming to foster more developmentally aligned educational AI systems through better training and prompting strategies. EduAdapt code and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/EduAdapt">https://github.com/NaumanNaeem/EduAdapt</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å›ç­”é—®é¢˜ã€è§£é‡Šå¤æ‚æ¦‚å¿µå’Œç”Ÿæˆå¹¿æ³›ä¸»é¢˜çš„å†…å®¹ï¼Œæ­£åœ¨æ”¹å˜æ•™è‚²æ–¹å¼ã€‚å°½ç®¡åœ¨å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•é’ˆå¯¹å­¦ç”Ÿçš„å¹´çº§æ°´å¹³è°ƒæ•´å›ç­”ã€‚è¿™åœ¨K-12æ•™è‚²ä¸­æ˜¯è¿«åˆ‡éœ€è¦çš„ï¼Œå› ä¸ºé€‚åˆå¹´é¾„æ®µçš„è¯æ±‡å’Œè§£é‡Šæ˜¯æœ‰æ•ˆå­¦ä¹ çš„å…³é”®ã€‚ç°æœ‰æ¨¡å‹äº§ç”Ÿçš„è¾“å‡ºé€šå¸¸å¯¹å¹´è½»å­¦ä¹ è€…æ¥è¯´è¿‡äºæ·±å¥¥æˆ–æ¨¡ç³Šï¼Œè€Œä¸”æ²¡æœ‰æ ‡å‡†åŒ–çš„åŸºå‡†æ¥è¯„ä¼°å®ƒä»¬åœ¨è®¤çŸ¥å’Œå‘è‚²é˜¶æ®µè¿›è¡Œè°ƒæ•´çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EduAdaptï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¿‘48,000ä¸ªå¹´çº§æ ‡ç­¾çš„é—®ç­”å¯¹åŸºå‡†ï¼Œæ¶µç›–ä¹ä¸ªç§‘å­¦ç§‘ç›®ï¼Œæ¶µç›–1-12å¹´çº§ï¼Œåˆ†ä¸ºå››ä¸ªå¹´çº§ã€‚æˆ‘ä»¬å¯¹EduAdaptä¸Šçš„ä¸€ç³»åˆ—å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è™½ç„¶è¾ƒå¤§çš„æ¨¡å‹é€šå¸¸è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨ä¸ºä½å¹´çº§å­¦ç”Ÿï¼ˆ1-5å¹´çº§ï¼‰ç”Ÿæˆåˆé€‚å›ç­”æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¹´çº§é€‚åº”æ€§çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ›´å¥½çš„è®­ç»ƒå’Œæç¤ºç­–ç•¥ï¼Œä¿ƒè¿›ä¸å‘è‚²ç›¸é€‚åº”çš„æ•™è‚²äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚EduAdaptä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/EduAdapt">https://github.com/NaumanNaeem/EduAdapt</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17389v1">PDF</a> 28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨æ­£åœ¨æ”¹å˜æ•™è‚²æ–¹å¼ï¼Œé€šè¿‡å›ç­”é—®é¢˜ã€è§£é‡Šå¤æ‚æ¦‚å¿µã€ç”Ÿæˆå„ç§å­¦ç§‘å†…å®¹ç­‰æ–¹å¼åŠ©åŠ›æ•™è‚²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é€‚åº”å­¦ç”Ÿå¹´çº§éœ€æ±‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚åœ¨K-12æ•™è‚²ä¸­ï¼Œé€‚åº”å­¦ç”Ÿå¹´é¾„çš„è§£é‡Šå’Œè¯æ±‡è‡³å…³é‡è¦ã€‚ç°æœ‰æ¨¡å‹è¾“å‡ºçš„å†…å®¹é€šå¸¸è¿‡äºé«˜æ·±æˆ–æ¨¡ç³Šï¼Œä¸é€‚åˆä½å¹´çº§å­¦ä¹ è€…ï¼Œä¸”ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¨¡å‹æ¥è¯„ä»·å…¶åœ¨è®¤çŸ¥å’Œå‘è‚²é˜¶æ®µçš„é€‚åº”æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºEduAdaptåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¿‘48,000ä¸ªæŒ‰å¹´çº§åˆ’åˆ†çš„é—®ç­”å¯¹ï¼Œæ¶µç›–ä¹ä¸ªç§‘å­¦å­¦ç§‘ï¼Œåˆ†ä¸ºå››ä¸ªå¹´çº§å±‚æ¬¡ï¼Œæ¶‰åŠå¹¼å„¿å›­è‡³é«˜ä¸­ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¼€æºLLMè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è™½ç„¶å¤§æ¨¡å‹æ€»ä½“è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨ä¸ºä½å¹´çº§å­¦ç”Ÿï¼ˆå¹¼å„¿å›­è‡³äº”å¹´çº§ï¼‰ç”Ÿæˆåˆé€‚å›ç­”æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡æä¾›äº†è¯„ä¼°LLMå¹´çº§é€‚åº”æ€§çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ›´å¥½çš„è®­ç»ƒå’Œæç¤ºç­–ç•¥ï¼Œä¿ƒè¿›æ›´ç¬¦åˆå‘è‚²è§„å¾‹çš„æ•™è‚²äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ã€‚EduAdaptçš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/EduAdapt">é“¾æ¥</a>å…¬å¼€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²ä¸­çš„åº”ç”¨æ­£åœ¨æ”¹å˜æ•™å­¦æ–¹å¼ï¼Œä½†å®ƒä»¬åœ¨å­¦ç”Ÿå¹´çº§é€‚åº”æ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>K-12æ•™è‚²ä¸­ï¼Œé€‚åº”å­¦ç”Ÿå¹´é¾„çš„è§£é‡Šå’Œè¯æ±‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹è¾“å‡ºå†…å®¹å¾€å¾€ä¸é€‚åˆä½å¹´çº§å­¦ä¹ è€…ï¼Œç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ¨¡å‹è¯„ä»·å…¶åœ¨ä¸åŒå¹´çº§çš„é€‚åº”æ€§ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ¨å‡ºäº†EduAdaptåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¿‘48,000ä¸ªæŒ‰å¹´çº§åˆ’åˆ†çš„é—®ç­”å¯¹ï¼Œæ¶µç›–ä¹ä¸ªå­¦ç§‘ã€‚</li>
<li>å¤§æ¨¡å‹æ€»ä½“è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é€‚åº”ä½å¹´çº§å­¦ç”Ÿæ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
<li>EduAdaptæä¾›äº†è¯„ä¼°LLMå¹´çº§é€‚åº”æ€§çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4dc886846b001a0c71fd3270ceb84699~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068767&auth_key=1761068767-0-0-b596f4996b2464d6766927be6b047e99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a87b9977ac267d57e748fb7be7fb0c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068775&auth_key=1761068775-0-0-261648f374ecd096bc1bf8d19650f6d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f7a8b2ef6a9fbad96a9a8e0b92c9802~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068782&auth_key=1761068782-0-0-1a2c0cd62d0318821ebee25a32c6ccb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11ebccccba1f2a726fb294be9d8018e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068789&auth_key=1761068789-0-0-7f7a4f873b165f38c2b7c832e3e7aa01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65ca75ff43bfbb6afc595755245f0466~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068795&auth_key=1761068795-0-0-40e796b841093e8880e3c938f0630059&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a2659c270802f0e703ad4428b739dad6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068802&auth_key=1761068802-0-0-0bd1396b2ce84a08eee2929143cf045d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TabR1-Taming-GRPO-for-tabular-reasoning-LLMs"><a href="#TabR1-Taming-GRPO-for-tabular-reasoning-LLMs" class="headerlink" title="TabR1: Taming GRPO for tabular reasoning LLMs"></a>TabR1: Taming GRPO for tabular reasoning LLMs</h2><p><strong>Authors:Pengxiang Cai, Zihao Gao, Jintai Chen</strong></p>
<p>Tabular prediction has traditionally relied on gradient-boosted decision trees and specialized deep learning models, which excel within tasks but provide limited interpretability and weak transfer across tables. Reasoning large language models (LLMs) promise cross-task adaptability with trans- parent reasoning traces, yet their potential has not been fully realized for tabular data. This paper presents TabR1, the first reasoning LLM for tabular prediction with multi-step reasoning. At its core is Permutation Relative Policy Optimization (PRPO), a simple yet efficient reinforcement learning method that encodes column-permutation invariance as a structural prior. By construct- ing multiple label-preserving permutations per sample and estimating advantages both within and across permutations, PRPO transforms sparse rewards into dense learning signals and improves generalization. With limited supervision, PRPO activates the reasoning ability of LLMs for tabular prediction, enhancing few-shot and zero-shot performance as well as interpretability. Comprehensive experiments demonstrate that TabR1 achieves performance comparable to strong baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1 approaches the performance of strong baselines under the 32-shot setting. Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B). </p>
<blockquote>
<p>è¡¨æ ¼é¢„æµ‹ä¼ ç»Ÿä¸Šä¾èµ–äºæ¢¯åº¦å¢å¼ºçš„å†³ç­–æ ‘å’Œä¸“ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä»»åŠ¡å†…è¡¨ç°ä¼˜å¼‚ï¼Œä½†æä¾›æœ‰é™çš„è§£é‡Šæ€§å’Œè·¨è¡¨æ ¼çš„å¼±è¿ç§»æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ‰¿è¯ºäº†è·¨ä»»åŠ¡çš„å¯é€‚åº”æ€§ï¼Œå…·æœ‰é€æ˜çš„æ¨ç†è½¨è¿¹ï¼Œä½†å…¶å¯¹è¡¨æ ¼æ•°æ®çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å®ç°ã€‚æœ¬æ–‡æå‡ºäº†TabR1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¡¨æ ¼é¢„æµ‹çš„å¤šæ­¥æ¨ç†æ¨ç†LLMã€‚å…¶æ ¸å¿ƒæ˜¯ç½®æ¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†åˆ—ç½®æ¢ä¸å˜æ€§ç¼–ç ä¸ºç»“æ„å…ˆéªŒã€‚é€šè¿‡å¯¹æ¯ä¸ªæ ·æœ¬æ„å»ºå¤šä¸ªæ ‡ç­¾ä¿ç•™ç½®æ¢ï¼Œå¹¶åœ¨ç½®æ¢å†…éƒ¨å’Œè·¨ç½®æ¢ä¼°è®¡ä¼˜åŠ¿ï¼ŒPRPOå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ‰é™çš„ç›‘ç£ä¸‹ï¼ŒPRPOæ¿€æ´»äº†LLMçš„è¡¨æ ¼é¢„æµ‹æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬çš„æ€§èƒ½ä»¥åŠå¯è§£é‡Šæ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å…¨ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†ä¸å¼ºå¤§åŸºå‡†çº¿ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼ŒTabR1çš„æ€§èƒ½æ¥è¿‘32æ ·æœ¬è®¾ç½®ä¸‹çš„å¼ºå¤§åŸºå‡†çº¿ã€‚æ­¤å¤–ï¼ŒTabR1ï¼ˆ8Bï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæ›´å¤§çš„LLMsï¼Œä¸DeepSeek-R1ï¼ˆ685Bï¼‰ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾53.17%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17385v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTabR1çš„è¡¨æ ¼é¢„æµ‹æ¨ç†æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ­¥æ¨ç†æŠ€æœ¯ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸€ç§åä¸ºPermutation Relative Policy Optimizationï¼ˆPRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ·æœ¬çš„å¤šä¸ªæ ‡ç­¾ä¿ç•™æ’åˆ—å¹¶ä¼°ç®—æ’åˆ—å†…çš„ä¼˜åŠ¿ï¼Œå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚TabR1æ¨¡å‹åœ¨ä¸ä¾èµ–å…¨ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹å¢å¼ºäº†å°‘æ ·æœ¬å’Œæ— æ ·æœ¬æ€§èƒ½åŠè§£é‡Šæ€§ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºå¼ºå¤§åŸºçº¿çš„èƒ½åŠ›ã€‚ç›¸è¾ƒäºæ›´å¤§çš„DeepSeek-R1æ¨¡å‹ï¼ˆè§„æ¨¡ä¸º685Bï¼‰ï¼ŒTabR1ï¼ˆè§„æ¨¡ä¸º8Bï¼‰åœ¨æ€§èƒ½ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TabR1æ˜¯é¦–ä¸ªé’ˆå¯¹è¡¨æ ¼é¢„æµ‹è®¾è®¡çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç»“åˆäº†å¤šæ­¥æ¨ç†æŠ€æœ¯ã€‚</li>
<li>TabR1çš„æ ¸å¿ƒæ˜¯Permutation Relative Policy Optimizationï¼ˆPRPOï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ„å»ºæ ·æœ¬çš„å¤šä¸ªæ ‡ç­¾ä¿ç•™æ’åˆ—æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>PRPOé€šè¿‡å°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·ï¼Œæœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>TabR1åœ¨ä¸ä¾èµ–å…¨ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹å¢å¼ºäº†å°‘æ ·æœ¬å’Œæ— æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e8e7277f4e22f73b224bb64a73c703a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068809&auth_key=1761068809-0-0-63ad4059f86a0dab1c1ca32dd2b2f18d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14ccaa8ea0a131520c91e947b479a451~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068838&auth_key=1761068838-0-0-fe4d840d0dcbc4672afd4a6a7ad92e43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f990ade0b1b16afe422c8773fef08f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068844&auth_key=1761068844-0-0-399f9429bb9b81c122ed0c48cda49985&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce1be3e60a071f85581a4d99f98964ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068851&auth_key=1761068851-0-0-db114b6e45d11202234f58c8ff35dafe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TaxoAlign-Scholarly-Taxonomy-Generation-Using-Language-Models"><a href="#TaxoAlign-Scholarly-Taxonomy-Generation-Using-Language-Models" class="headerlink" title="TaxoAlign: Scholarly Taxonomy Generation Using Language Models"></a>TaxoAlign: Scholarly Taxonomy Generation Using Language Models</h2><p><strong>Authors:Avishek Lahiri, Yufang Hou, Debarshi Kumar Sanyal</strong></p>
<p>Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner. They also form an important part in the creation of comprehensive literature surveys. The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts. To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automatically-created taxonomies. For this purpose, we create the CS-TaxoBench benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers. We also include an additional test set of 80 taxonomies curated from conference survey papers. We propose TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation. Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts. We evaluate our method and various baselines on CS-TaxoBench, using both automated evaluation metrics and human evaluation studies. The results show that TaxoAlign consistently surpasses the baselines on nearly all metrics. The code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/AvishekLahiri/TaxoAlign">https://github.com/AvishekLahiri/TaxoAlign</a>. </p>
<blockquote>
<p>åˆ†ç±»æ³•åœ¨ç ”ç©¶äººå‘˜ä»¥å±‚çº§æ–¹å¼ç»“æ„å’Œæµè§ˆçŸ¥è¯†æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å®ƒä»¬ä¹Ÿæ˜¯åˆ›å»ºç»¼åˆæ€§æ–‡çŒ®ç»¼è¿°çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰çš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆæ–¹æ³•å¹¶ä¸ä¼šå°†ç”Ÿæˆçš„è°ƒæŸ¥ç»“æ„ä¸äººç±»ä¸“å®¶ç¼–å†™çš„è°ƒæŸ¥ç»“æ„è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªå·±çš„è‡ªåŠ¨åˆ†ç±»æ³•åˆ›å»ºæ–¹æ³•ï¼Œå¯ä»¥å¼¥åˆäººå·¥ç”Ÿæˆå’Œè‡ªåŠ¨åˆ›å»ºåˆ†ç±»æ³•ä¹‹é—´çš„å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†CS-TaxoBenchåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ä»äººç±»ç¼–å†™çš„è°ƒæŸ¥æŠ¥å‘Šè®ºæ–‡ä¸­æå–çš„460ä¸ªåˆ†ç±»æ³•ã€‚æˆ‘ä»¬è¿˜ä»ä¼šè®®è°ƒæŸ¥æŠ¥å‘Šè®ºæ–‡ä¸­åŠ å…¥äº†é¢å¤–çš„æµ‹è¯•é›†ï¼Œå…±è®¡80ä¸ªåˆ†ç±»æ³•ã€‚æˆ‘ä»¬æå‡ºTaxoAlignï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸»é¢˜ã€æŒ‡ä»¤å¼•å¯¼çš„å­¦æœ¯åˆ†ç±»æ³•ç”Ÿæˆçš„ä¸‰é˜¶æ®µæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä¸¥æ ¼çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡è‡ªåŠ¨ç”Ÿæˆçš„åˆ†ç±»æ³•ä¸ä¸“å®¶åˆ›å»ºçš„åˆ†ç±»æ³•åœ¨ç»“æ„å¯¹é½å’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢çš„æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨CS-TaxoBenchä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•å’Œå„ç§åŸºçº¿æ–¹æ³•ï¼Œé‡‡ç”¨äº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼ŒTaxoAlignå‡ ä¹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†åŸºçº¿æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AvishekLahiri/TaxoAlign%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AvishekLahiri/TaxoAlignæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17263v1">PDF</a> This paper has been accepted at the EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è‡ªåŠ¨åˆ†ç±»æ³•ç”Ÿæˆçš„é‡è¦æ€§åŠå…¶åœ¨ç ”ç©¶é¢†åŸŸçš„è¿ç”¨ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½ä¸äººç±»ä¸“å®¶ç”Ÿæˆçš„åˆ†ç±»æ³•ç»“æ„è¿›è¡Œæ¯”è¾ƒï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTaxoAlignçš„ä¸‰é˜¶æ®µä¸»é¢˜å¯¼å‘æŒ‡ä»¤é©±åŠ¨æ–¹æ³•ç”¨äºå­¦æœ¯åˆ†ç±»æ³•ç”Ÿæˆã€‚åŒæ—¶åˆ›å»ºäº†CS-TaxoBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«ä»äººç±»æ’°å†™çš„è°ƒæŸ¥æŠ¥å‘Šè®ºæ–‡ä¸­æå–çš„460ä¸ªåˆ†ç±»æ³•ä»¥åŠä»ä¼šè®®è°ƒæŸ¥æŠ¥å‘Šè®ºæ–‡ä¸­æŒ‘é€‰çš„80ä¸ªé¢å¤–æµ‹è¯•é›†ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒTaxoAlignåœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†ç±»æ³•å¯¹äºç ”ç©¶è€…çš„çŸ¥è¯†ç»“æ„å’Œæ–‡çŒ®è°ƒæŸ¥è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆæ–¹æ³•æœªä¸äººç±»ä¸“å®¶ç”Ÿæˆçš„åˆ†ç±»æ³•ç»“æ„è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æå‡ºäº†TaxoAlignæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µçš„ä¸»é¢˜å¯¼å‘æŒ‡ä»¤é©±åŠ¨ï¼Œç”¨äºå­¦æœ¯åˆ†ç±»æ³•ç”Ÿæˆã€‚</li>
<li>åˆ›å»ºäº†CS-TaxoBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«ä»äººç±»æ’°å†™çš„è°ƒæŸ¥æŠ¥å‘Šè®ºæ–‡ä¸­æå–çš„åˆ†ç±»æ³•ã€‚</li>
<li>TaxoAlignè¯„ä¼°æ¡†æ¶èƒ½è¡¡é‡è‡ªåŠ¨ç”Ÿæˆåˆ†ç±»æ³•ä¸äººç±»ä¸“å®¶åˆ›å»ºåˆ†ç±»æ³•çš„ç»“æ„å¯¹é½å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>TaxoAlignåœ¨å‡ ä¹æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-99022f2e9d511178da30bd27784676c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068859&auth_key=1761068859-0-0-33e108fb0c9342eaf6cc49a9c2cadcd0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-388d9f7fc038977b39c5952bb1348f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068866&auth_key=1761068866-0-0-93bfa0a6b866cfa9135ad47c192c1620&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2b301cf020ce413000caaed6247575d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068873&auth_key=1761068873-0-0-cfb05f9458a3e927c5c2701fcff19849&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdfb4915883ab6c25cc150a33c2ea3f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068879&auth_key=1761068879-0-0-a927a61fddc6d89b6187211df259f2a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa3f6270ad3b45b10213da2c0feb2c7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068886&auth_key=1761068886-0-0-78e70888868cc6570419390b678741bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="StreamingThinker-Large-Language-Models-Can-Think-While-Reading"><a href="#StreamingThinker-Large-Language-Models-Can-Think-While-Reading" class="headerlink" title="StreamingThinker: Large Language Models Can Think While Reading"></a>StreamingThinker: Large Language Models Can Think While Reading</h2><p><strong>Authors:Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80% reduction in token waiting before the onset of reasoning and a more than 60% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at \href{<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker%7D%7Bthis">https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this</a> repository.} </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„LLMæ¨ç†æ¨¡å¼åªæœ‰åœ¨æ•´ä¸ªè¾“å…¥å¯ç”¨åæ‰å¼€å§‹æ€è€ƒï¼Œè¿™å¼•å…¥äº†ä¸å¿…è¦çš„å»¶è¿Ÿï¼Œå¹¶å‡å¼±äº†å¯¹åŠ¨æ€åœºæ™¯ä¸­æ—©æœŸä¿¡æ¯çš„å…³æ³¨ã€‚å—äººç±»é˜…è¯»æ—¶è®¤çŸ¥çš„å¯å‘ï¼Œæˆ‘ä»¬é¦–æ¬¡ä¸ºLLMè®¾è®¡äº†ä¸€ç§â€œæµå¼æ€ç»´â€æ¨¡å¼ï¼Œåœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæ¨ç†æŒ‰ç…§è¾“å…¥çš„é¡ºåºå±•å¼€ï¼Œå¹¶åœ¨é˜…è¯»å®Œæˆåè¿›ä¸€æ­¥è°ƒæ•´å…¶æ·±åº¦ã€‚æˆ‘ä»¬ç”¨StreamingThinkerå®ä¾‹åŒ–è¿™ç§æ¨¡å¼ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæµå¼CoTç”Ÿæˆã€æµå¼çº¦æŸè®­ç»ƒå’Œæµå¼å¹¶è¡Œæ¨ç†ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨é˜…è¯»æ—¶è¿›è¡Œæ€è€ƒã€‚å…·ä½“æ¥è¯´ï¼ŒStreamingThinkeré‡‡ç”¨å…·æœ‰è´¨é‡æ§åˆ¶æœºåˆ¶çš„æµå¼æ¨ç†å•å…ƒè¿›è¡ŒCoTç”Ÿæˆï¼Œé€šè¿‡æµå¼æ³¨æ„åŠ›æ©ç å’Œä½ç½®ç¼–ç å¼ºåˆ¶ä¿ç•™é¡ºåºæ¨ç†ï¼Œå¹¶åˆ©ç”¨å¹¶è¡ŒKVç¼“å­˜æ¥è§£è€¦è¾“å…¥ç¼–ç å’Œæ¨ç†ç”Ÿæˆï¼Œä»è€Œç¡®ä¿å¯¹é½å¹¶å®ç°çœŸæ­£çš„å¹¶å‘æ€§ã€‚æˆ‘ä»¬åœ¨Qwen3æ¨¡å‹å®¶æ—ä¸Šå¯¹æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†å’ŒåŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”æ¨ç†ä»»åŠ¡è¯„ä¼°äº†StreamingThinkerã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamingThinkerçš„æ€§èƒ½ä¸æ‰¹é‡æ€è€ƒç›¸å½“ï¼ŒåŒæ—¶åœ¨å¼€å§‹æ¨ç†ä¹‹å‰å‡å°‘äº†80%çš„ä»¤ç‰Œç­‰å¾…æ—¶é—´ï¼Œå¹¶åœ¨äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆæ–¹é¢çš„æ—¶é—´å»¶è¿Ÿå‡å°‘äº†60%ä»¥ä¸Šï¼Œè¿™è¯æ˜äº†æµå¼èŒƒå¼åœ¨LLMæ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨æ­¤ä»“åº“ï¼š[<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker%E3%80%82]%EF%BC%88%E6%B3%A8%EF%BC%9A%E8%AF%A5%E9%93%BE%E6%8E%A5%E4%B8%BA%E8%99%9A%E6%9E%84%EF%BC%8C%E5%AE%9E%E9%99%85%E5%8F%91%E5%B8%83%E5%9C%B0%E5%9D%80%E8%AF%B7%E8%87%AA%E8%A1%8C%E6%9B%BF%E6%8D%A2%E3%80%82%EF%BC%89">https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinkerã€‚]ï¼ˆæ³¨ï¼šè¯¥é“¾æ¥ä¸ºè™šæ„ï¼Œå®é™…å‘å¸ƒåœ°å€è¯·è‡ªè¡Œæ›¿æ¢ã€‚ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17238v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰LLMçš„æ¨ç†æ¨¡å¼æ˜¯åœ¨è·å–å®Œæ•´è¾“å…¥åæ‰å¼€å§‹æ€è€ƒï¼Œè¿™å¯¼è‡´äº†ä¸å¿…è¦çš„å»¶è¿Ÿï¼Œå¹¶åœ¨åŠ¨æ€åœºæ™¯ä¸­å¿½è§†äº†æ—©æœŸçš„ä¿¡æ¯ã€‚å—äººç±»é˜…è¯»æ—¶æ€è€ƒçš„è®¤çŸ¥å¯å‘ï¼Œæˆ‘ä»¬ä¸ºLLMè®¾è®¡äº†ä¸€ç§â€œæµå¼æ€ç»´â€æ¨¡å¼ï¼Œæ¨ç†è¿‡ç¨‹æŒ‰ç…§è¾“å…¥çš„é¡ºåºå±•å¼€ï¼Œå¹¶åœ¨é˜…è¯»å®Œæˆåè°ƒæ•´æ·±åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨StreamingThinkeræ¡†æ¶æ¥å®ç°è¿™ç§æ¨¡å¼ï¼Œå®ƒé€šè¿‡æ•´åˆæµå¼CoTç”Ÿæˆã€æµå¼çº¦æŸè®­ç»ƒå’Œæµå¼å¹¶è¡Œæ¨ç†ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨é˜…è¯»æ—¶æ€è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamingThinkeråœ¨ä¿æŒä¸æ‰¹é‡æ€è€ƒç›¸å½“çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†80%çš„æ¨ç†å‰ç­‰å¾…ä»¤ç‰Œçš„æ—¶é—´å’Œè¶…è¿‡60%çš„æœ€ç»ˆç­”æ¡ˆç”Ÿæˆçš„æ—¶é—´å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMçš„æ¨ç†æ¨¡å¼å­˜åœ¨å»¶è¿Ÿï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—©æœŸä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†â€œæµå¼æ€ç»´â€æ¨¡å¼ï¼Œä½¿LLMçš„æ¨ç†è¿‡ç¨‹æŒ‰ç…§è¾“å…¥é¡ºåºå±•å¼€ã€‚</li>
<li>StreamingThinkeræ¡†æ¶å®ç°äº†æµå¼æ€ç»´æ¨¡å¼ï¼Œé€šè¿‡æ•´åˆæµå¼CoTç”Ÿæˆã€æµå¼çº¦æŸè®­ç»ƒå’Œæµå¼å¹¶è¡Œæ¨ç†ï¼Œä½¿LLMåœ¨é˜…è¯»æ—¶èƒ½å¤Ÿæ€è€ƒã€‚</li>
<li>StreamingThinkeræ¡†æ¶åŒ…æ‹¬æµå¼æ¨ç†å•å…ƒã€æµå¼æ³¨æ„åŠ›æ©ç å’Œä½ç½®ç¼–ç ç­‰æŠ€æœ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamingThinkeråœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-09db4c76f8f5aa41530c492c3d39af98~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068893&auth_key=1761068893-0-0-f309f9410f7fee6efca2e22a601aa8ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e6f38dedd5ea22adb13ea4d8ddb2d69~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068901&auth_key=1761068901-0-0-30beb51ee2b9b653fba57c5c05c52cb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71722366afc995f23ce28742ce3568cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068907&auth_key=1761068907-0-0-f87e3d36d08aa29f16ebceb087319a0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6f66b3b9db57e9a8826c9f16341ac4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068914&auth_key=1761068914-0-0-c089c5fd3fdffe524fea2714db0d0090&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24212e44eec735b31af38ecae22080ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068920&auth_key=1761068920-0-0-fc710b6f3e91c69f63db84d48b0768d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TREAT-A-Code-LLMs-Trustworthiness-Reliability-Evaluation-and-Testing-Framework"><a href="#TREAT-A-Code-LLMs-Trustworthiness-Reliability-Evaluation-and-Testing-Framework" class="headerlink" title="TREAT: A Code LLMs Trustworthiness &#x2F; Reliability Evaluation and Testing   Framework"></a>TREAT: A Code LLMs Trustworthiness &#x2F; Reliability Evaluation and Testing   Framework</h2><p><strong>Authors:Shuzheng Gao, Eric John Li, Man Ho Lam, Jingyu Xiao, Yuxuan Wan, Chaozheng Wang, Ng Man Tik, Michael R. Lyu</strong></p>
<p>Large foundation models are fundamentally transforming the software engineering landscape, demonstrating exceptional capabilities across diverse tasks such as code generation, debugging, and testing. Despite this rapid progress, a significant gap remains in how to comprehensively evaluate these modelsâ€™ trustworthiness in real-world software engineering scenarios. Existing benchmarks suffer from limited task scope and fail to incorporate critical evaluation aspects such as the robustness and reliability of models. To bridge this gap, we present an evaluation framework called TREAT (Code LLMs Trustworthiness &#x2F; Reliability Evaluation And Testing) that provides a holistic assessment of model performance in code intelligence tasks. Our evaluation framework addresses key limitations in existing approaches with four main improvements: (1) Multi-Task Holistic Evaluation that spans diverse software engineering activities rather than limited coding tasks; (2) Multi-Language and Multi-Modality Assessment that extends beyond traditional single-language, text-only benchmarks to include multi-modality coding tasks; (3) Robustness Assessment that evaluates model reliability under semantically-preserving code transformations; and (4) Rigorous Evaluation Methodology that enhances the trustworthiness of evaluation results through diverse evaluation prompts and adaptive solution extraction. Based on this evaluation framework, we assess 26 state-of-the-art models and uncover both their strengths and limitations, yielding several key insights:(1) Current models show substantial performance variation across programming tasks; (2) Multi-modal language models demonstrate specific performance limitations in UI code generation and edit; </p>
<blockquote>
<p>å¤§å‹åŸºç¡€æ¨¡å‹æ­£åœ¨ä»æ ¹æœ¬ä¸Šæ”¹å˜è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„æ ¼å±€ï¼Œå…¶åœ¨ä»£ç ç”Ÿæˆã€è°ƒè¯•å’Œæµ‹è¯•ç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚å°½ç®¡è¿›å±•è¿…é€Ÿï¼Œä½†åœ¨çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸­å¦‚ä½•å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹çš„å¯é æ€§ä»å­˜åœ¨å·¨å¤§å·®è·ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä»»åŠ¡èŒƒå›´æœ‰é™ï¼Œæœªèƒ½çº³å…¥æ¨¡å‹ç¨³å¥æ€§å’Œå¯é æ€§çš„é‡è¦è¯„ä¼°æ–¹é¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶ï¼Œåä¸ºTREATï¼ˆä»£ç LLMä¿¡ä»»åº¦&#x2F;å¯é æ€§è¯„ä¼°ä¸æµ‹è¯•ï¼‰ï¼Œè¯¥æ¡†æ¶å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ä»£ç æ™ºèƒ½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼Œä¸»è¦åŒ…æ‹¬å››ä¸ªæ–¹é¢çš„æ”¹è¿›ï¼šï¼ˆ1ï¼‰å¤šä»»åŠ¡æ•´ä½“è¯„ä¼°ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„è½¯ä»¶å·¥ç¨‹æ´»åŠ¨ï¼Œè€Œéä»…é™äºç¼–ç ä»»åŠ¡ï¼›ï¼ˆ2ï¼‰å¤šè¯­è¨€å’Œå¤šæ¨¡å¼è¯„ä¼°ï¼Œè¶…è¶Šä¼ ç»Ÿçš„å•è¯­è¨€ã€çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤šæ¨¡å¼ç¼–ç ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰ç¨³å¥æ€§è¯„ä¼°ï¼Œè¯„ä¼°æ¨¡å‹åœ¨è¯­ä¹‰ä¿ç•™çš„ä»£ç è½¬æ¢ä¸‹çš„å¯é æ€§ï¼›ï¼ˆ4ï¼‰ä¸¥è°¨çš„è¯„ä»·æ–¹æ³•ï¼Œé€šè¿‡å¤šæ ·çš„è¯„ä»·æç¤ºå’Œè‡ªé€‚åº”è§£å†³æ–¹æ¡ˆæå–ï¼Œå¢å¼ºè¯„ä»·ç»“æœçš„å¯ä¿¡åº¦ã€‚åŸºäºè¿™ä¸€è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹26ä¸ªæœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„ä¼˜åŠ¿å’Œå±€é™ï¼Œå¾—å‡ºäº†ä¸€äº›å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰å½“å‰æ¨¡å‹åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼›ï¼ˆ2ï¼‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨UIä»£ç ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºç‰¹å®šçš„æ€§èƒ½å±€é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17163v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹åŸºç¡€æ¨¡å‹æ­£åœ¨ä»æ ¹æœ¬ä¸Šæ”¹å˜è½¯ä»¶å·¥ç¨‹çš„æ™¯è§‚ï¼Œå±•ç¤ºå‡ºä»£ç ç”Ÿæˆã€è°ƒè¯•å’Œæµ‹è¯•ç­‰å¤šæ ·ä»»åŠ¡çš„å‡ºè‰²èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸­çš„å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶TREATï¼Œå¯¹æ¨¡å‹åœ¨ä»£ç æ™ºèƒ½ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œæ•´ä½“è¯„ä¼°ï¼Œå¹¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ã€‚é€šè¿‡å¤šä»»åŠ¡çš„å…¨é¢è¯„ä¼°ã€å¤šè¯­è¨€å’Œå¤šæ¨¡å¼è¯„ä¼°ã€ç¨³å¥æ€§è¯„ä¼°ä»¥åŠä¸¥è°¨çš„è¯„ä»·æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åŸºäºè¯¥æ¡†æ¶è¯„ä¼°äº†26ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹åŸºç¡€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸­çš„ä¿¡ä»»åº¦æ–¹é¢å­˜åœ¨å·®è·ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºTREATçš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¹æ¨¡å‹åœ¨ä»£ç æ™ºèƒ½ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œç»¼åˆè¯„ä»·ã€‚</li>
<li>TREATæ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬å¤šä»»åŠ¡çš„å…¨é¢è¯„ä¼°ã€å¤šè¯­è¨€å’Œå¤šæ¨¡å¼è¯„ä¼°ã€ç¨³å¥æ€§è¯„ä¼°ä»¥åŠä¸¥è°¨çš„è¯„ä»·æ–¹æ³•ã€‚</li>
<li>åŸºäºTREATæ¡†æ¶è¯„ä¼°äº†26ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå‘ç°æ¨¡å‹åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹åœ¨UIä»£ç ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºç‰¹å®šçš„æ€§èƒ½å±€é™ã€‚</li>
<li>è¯„ä¼°ç»“æœæ­ç¤ºäº†æ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-54dde87df9af48033db41ce54bbc915a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068927&auth_key=1761068927-0-0-803b13f4fc86000a1737dcec83ecb974&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7753a9085122567d730c148fddb4f19a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068934&auth_key=1761068934-0-0-66f4ad36467f63ef9a3f61800627341c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c386cc3086c57e624b4d07a897a3f0de~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068941&auth_key=1761068941-0-0-f980a02dbaac18f20baf332b870e77f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a21de8c2650b616561eb2dbf9b18aad~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068948&auth_key=1761068948-0-0-0020a8a10326a4d7dd5ce19553407e26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d95a52e31dc305565387b7514b0a3d11~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068954&auth_key=1761068954-0-0-47f0f8b4975f93891e2df573a9f8027f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Integrating-Performance-Tools-in-Model-Reasoning-for-GPU-Kernel-Optimization"><a href="#Integrating-Performance-Tools-in-Model-Reasoning-for-GPU-Kernel-Optimization" class="headerlink" title="Integrating Performance Tools in Model Reasoning for GPU Kernel   Optimization"></a>Integrating Performance Tools in Model Reasoning for GPU Kernel   Optimization</h2><p><strong>Authors:Daniel Nichols, Konstantinos Parasyris, Charles Jekel, Abhinav Bhatele, Harshitha Menon</strong></p>
<p>Language models are now prevalent in software engineering with many developers using them to automate tasks and accelerate their development. While language models have been tremendous at accomplishing complex software engineering tasks, there are still many areas where they fail to deliver desirable results, for instance code performance related tasks. Tasks like optimization depend on many complex data from the environment, hardware, etc. that are not directly represented in source code. Recent efforts have seen large improvements in general code modeling tasks using chain-of-thought style reasoning, but these models still fail to comprehend how the environment interacts with code performance. In this paper we propose a methodology to train language models that can interact with performance tools during their reasoning process. We then demonstrate how this methodology can be used to train a state-of-the-art GPU kernel optimization model. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸç°åœ¨éå¸¸æµè¡Œï¼Œè®¸å¤šå¼€å‘äººå‘˜ä½¿ç”¨å®ƒä»¬æ¥è‡ªåŠ¨åŒ–ä»»åŠ¡å¹¶åŠ é€Ÿå¼€å‘ã€‚è™½ç„¶è¯­è¨€æ¨¡å‹åœ¨å®Œæˆå¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»æœ‰è®¸å¤šé¢†åŸŸæ— æ³•æä¾›ç†æƒ³çš„ç»“æœï¼Œä¾‹å¦‚ä¸ä»£ç æ€§èƒ½ç›¸å…³çš„ä»»åŠ¡ã€‚åƒä¼˜åŒ–è¿™æ ·çš„ä»»åŠ¡ä¾èµ–äºç¯å¢ƒä¸­çš„è®¸å¤šå¤æ‚æ•°æ®ã€ç¡¬ä»¶ç­‰ï¼Œè¿™äº›æ•°æ®å¹¶æ²¡æœ‰ç›´æ¥åœ¨æºä»£ç ä¸­è¡¨ç¤ºã€‚æœ€è¿‘çš„åŠªåŠ›é€šè¿‡é‡‡ç”¨é“¾å¼æ€ç»´é£æ ¼çš„æ¨ç†ï¼Œåœ¨ä¸€èˆ¬ä»£ç å»ºæ¨¡ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¤§çš„æ”¹è¿›ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶æ— æ³•ç†è§£ç¯å¢ƒå¦‚ä½•ä¸ä»£ç æ€§èƒ½è¿›è¡Œäº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸æ€§èƒ½å·¥å…·è¿›è¡Œäº¤äº’ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥è®­ç»ƒæœ€å…ˆè¿›çš„GPUå†…æ ¸ä¼˜åŒ–æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17158v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œç”¨äºè‡ªåŠ¨åŒ–ä»»åŠ¡å’ŒåŠ é€Ÿå¼€å‘è¿‡ç¨‹ã€‚å°½ç®¡è¯­è¨€æ¨¡å‹åœ¨å®Œæˆå¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä»£ç æ€§èƒ½ç›¸å…³ä»»åŠ¡ç­‰æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ç¯å¢ƒã€ç¡¬ä»¶ç­‰å¤æ‚æ•°æ®åœ¨æºä»£ç ä¸­æ— æ³•ç›´æ¥ä½“ç°ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä¸æ€§èƒ½å·¥å…·äº’åŠ¨çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•è®­ç»ƒå‡ºå…ˆè¿›çš„GPUå†…æ ¸ä¼˜åŒ–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œæœ‰åŠ©äºè‡ªåŠ¨åŒ–ä»»åŠ¡å’ŒåŠ é€Ÿå¼€å‘ã€‚</li>
<li>åœ¨ä»£ç æ€§èƒ½ç›¸å…³ä»»åŠ¡æ–¹é¢ï¼Œè¯­è¨€æ¨¡å‹ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç¯å¢ƒã€ç¡¬ä»¶ç­‰æ•°æ®åœ¨æºä»£ç ä¸­æ— æ³•ç›´æ¥ä½“ç°ï¼Œå½±å“è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‘æœŸç ”ç©¶é€šè¿‡é“¾å¼æ€ç»´é£æ ¼æ¨ç†æ”¹å–„äº†é€šç”¨ä»£ç å»ºæ¨¡ä»»åŠ¡ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§ä¸æ€§èƒ½å·¥å…·äº’åŠ¨çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ç”¨äºè®­ç»ƒå‡ºå…ˆè¿›çš„GPUå†…æ ¸ä¼˜åŒ–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cbfe2b9b4300d75b4abf87519a5ce79f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068963&auth_key=1761068963-0-0-baed99be37753b3ab83f5e25812432be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb87714b454f6e68eb0b5b50eb0e21ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068970&auth_key=1761068970-0-0-96a5797e4653080a22ad273688da1651&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GACO-CAD-Geometry-Augmented-and-Conciseness-Optimized-CAD-Model-Generation-from-Single-Image"><a href="#GACO-CAD-Geometry-Augmented-and-Conciseness-Optimized-CAD-Model-Generation-from-Single-Image" class="headerlink" title="GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model   Generation from Single Image"></a>GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model   Generation from Single Image</h2><p><strong>Authors:Yinghui Wang, Xinyu Zhang, Peng Du</strong></p>
<p>Generating editable, parametric CAD models from a single image holds great potential to lower the barriers of industrial concept design. However, current multi-modal large language models (MLLMs) still struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities. We address this limitation by introducing GACO-CAD, a novel two-stage post-training framework. It is designed to achieve a joint objective: simultaneously improving the geometric accuracy of the generated CAD models and encouraging the use of more concise modeling procedures. First, during supervised fine-tuning, we leverage depth and surface normal maps as dense geometric priors, combining them with the RGB image to form a multi-channel input. In the context of single-view reconstruction, these priors provide complementary spatial cues that help the MLLM more reliably recover 3D geometry from 2D observations. Second, during reinforcement learning, we introduce a group length reward that, while preserving high geometric fidelity, promotes the generation of more compact and less redundant parametric modeling sequences. A simple dynamic weighting strategy is adopted to stabilize training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD achieves state-of-the-art performance under the same MLLM backbone, consistently outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ã€å‚æ•°åŒ–çš„CADæ¨¡å‹ï¼Œä¸ºé™ä½å·¥ä¸šæ¦‚å¿µè®¾è®¡çš„é—¨æ§›å¸¦æ¥äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä»ç„¶éš¾ä»¥ä»äºŒç»´å›¾åƒå‡†ç¡®æ¨æ–­ä¸‰ç»´å‡ ä½•ç»“æ„ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥GACO-CADï¼Œä¸€ä¸ªæ–°å‹çš„ä¸¤é˜¶æ®µåè®­ç»ƒæ¡†æ¶ï¼Œæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚å®ƒæ—¨åœ¨å®ç°è”åˆç›®æ ‡ï¼šåŒæ—¶æé«˜ç”ŸæˆCADæ¨¡å‹å‡ ä½•ç²¾åº¦å¹¶é¼“åŠ±ä½¿ç”¨æ›´ç®€æ´çš„å»ºæ¨¡ç¨‹åºã€‚é¦–å…ˆï¼Œåœ¨ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æ·±åº¦å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå¯†é›†å‡ ä½•å…ˆéªŒï¼Œå°†å®ƒä»¬ä¸RGBå›¾åƒç»“åˆå½¢æˆå¤šé€šé“è¾“å…¥ã€‚åœ¨å•è§†å›¾é‡å»ºçš„æƒ…å†µä¸‹ï¼Œè¿™äº›å…ˆéªŒæä¾›äº’è¡¥çš„ç©ºé—´çº¿ç´¢ï¼Œå¸®åŠ©MLLMæ›´å¯é åœ°ä»äºŒç»´è§‚æµ‹ä¸­æ¢å¤ä¸‰ç»´å‡ ä½•ç»“æ„ã€‚å…¶æ¬¡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»„é•¿åº¦å¥–åŠ±ï¼Œåœ¨ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦çš„åŒæ—¶ï¼Œä¿ƒè¿›ç”Ÿæˆæ›´ç´§å‡‘ã€æ›´å°‘å†—ä½™çš„å‚æ•°åŒ–å»ºæ¨¡åºåˆ—ã€‚é‡‡ç”¨ç®€å•çš„åŠ¨æ€åŠ æƒç­–ç•¥æ¥ç¨³å®šè®­ç»ƒã€‚åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨åŒä¸€MLLMéª¨æ¶ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºGACO-CADçš„æ–°å‹ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä»å•ä¸€å›¾åƒç”Ÿæˆçš„å¯ç¼–è¾‘å‚æ•°åŒ–CADæ¨¡å‹çš„å‡ ä½•ç²¾åº¦ï¼Œå¹¶é¼“åŠ±é‡‡ç”¨æ›´ç®€æ´çš„å»ºæ¨¡æµç¨‹ã€‚é€šè¿‡åˆ©ç”¨æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå¯†é›†å‡ ä½•å…ˆéªŒï¼Œç»“åˆRGBå›¾åƒå½¢æˆå¤šé€šé“è¾“å…¥ï¼Œè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒã€‚åœ¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå¼•å…¥äº†ä¸€ç§å°ç»„é•¿åº¦å¥–åŠ±ï¼Œæ—¢ä¿è¯äº†é«˜å‡ ä½•ä¿çœŸåº¦ï¼Œåˆä¿ƒè¿›äº†æ›´ç´§å‡‘ã€æ›´å°‘å†—ä½™çš„å‚æ•°åŒ–å»ºæ¨¡åºåˆ—çš„ç”Ÿæˆã€‚åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨åŒç­‰çš„MLLMéª¨æ¶ä¸‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GACO-CADæ¡†æ¶æ—¨åœ¨æé«˜ä»å•ä¸€å›¾åƒç”ŸæˆCADæ¨¡å‹çš„å‡ ä½•ç²¾åº¦ï¼Œå¹¶ç®€åŒ–å»ºæ¨¡æµç¨‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå¯†é›†å‡ ä½•å…ˆéªŒï¼Œæé«˜æ¨¡å‹çš„å‡ ä½•å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šæœ‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­å¼•å…¥å°ç»„é•¿åº¦å¥–åŠ±ä»¥ä¿ƒè¿›æ›´ç®€æ´çš„å»ºæ¨¡ã€‚</li>
<li>GACO-CADåœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘äº†ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§ä¸‰ä¸ªæ–¹é¢çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>GACO-CADè®¾è®¡æ—¨åœ¨ä¸ç°æœ‰çš„MLLMså…¼å®¹ï¼Œå¯ä»¥åœ¨ç›¸åŒçš„éª¨æ¶ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6dbae90fe7b9f775db99c2177aa59f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068977&auth_key=1761068977-0-0-ed8a2116b35e9563c46f8cbc8ecef8e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac7fff52673cfe893e5aab942f8af24c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068984&auth_key=1761068984-0-0-6827498050e1e3bed4cb74b728c2cf15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17f7605ac6cce1c215330a19a6dc0fd9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068991&auth_key=1761068991-0-0-5af2d58ba4d9d028b515f6aefa131e05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-110fc71fe2b700c3dadc97b34c9c0a43~resize:0:q75.jpg?source=1f5c5e47&expiration=1761068998&auth_key=1761068998-0-0-07fb1517afa1fee09e369842c751c77d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5eda5eb1af0caa8e7b705421221f888e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069005&auth_key=1761069005-0-0-dd7bff5c1f749f6069624c546bebc825&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5515df575d9c6232eb0399753253c5e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069011&auth_key=1761069011-0-0-862b23542ace9e443b76f9fceb48b5e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Rethinking-On-policy-Optimization-for-Query-Augmentation"><a href="#Rethinking-On-policy-Optimization-for-Query-Augmentation" class="headerlink" title="Rethinking On-policy Optimization for Query Augmentation"></a>Rethinking On-policy Optimization for Query Augmentation</h2><p><strong>Authors:Zhichao Xu, Shengyao Zhuang, Xueguang Ma, Bingsen Chen, Yijun Tian, Fengran Mo, Jie Cao, Vivek Srikumar</strong></p>
<p>Recent advances in large language models (LLMs) have led to a surge of interest in query augmentation for information retrieval (IR). Two main approaches have emerged. The first prompts LLMs to generate answers or pseudo-documents that serve as new queries, relying purely on the modelâ€™s parametric knowledge or contextual information. The second applies reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly optimizing retrieval metrics. While having respective advantages and limitations, the two approaches have not been compared under consistent experimental conditions. In this work, we present the first systematic comparison of prompting-based and RL-based query augmentation across diverse benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key finding is that simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of rewriting a query, the LLM policy learns to generate a pseudo-document that maximizes retrieval performance, thus merging the flexibility and generative structure of prompting with the targeted optimization of RL. We show OPQE outperforms both standalone prompting and RL-based rewriting, demonstrating that a synergistic approach yields the best results. Our implementation is made available to facilitate reproducibility. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä¸­æŸ¥è¯¢å¢å¼ºçš„çƒ­æ½®ã€‚å‡ºç°äº†ä¸¤ç§ä¸»è¦æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯å¼•å¯¼LLMç”Ÿæˆç­”æ¡ˆæˆ–ä¼ªæ–‡æ¡£ä½œä¸ºæ–°æŸ¥è¯¢ï¼Œè¿™å®Œå…¨ä¾èµ–äºæ¨¡å‹çš„å‚æ•°çŸ¥è¯†æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç¬¬äºŒç§åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹LLMè¿›è¡Œå¾®è°ƒä»¥è¿›è¡ŒæŸ¥è¯¢é‡å†™ï¼Œç›´æ¥ä¼˜åŒ–æ£€ç´¢æŒ‡æ ‡ã€‚å°½ç®¡è¿™ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ¿å’Œå±€é™ï¼Œä½†å®ƒä»¬å°šæœªåœ¨ä¸€è‡´çš„å®éªŒæ¡ä»¶ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæç¤ºå’ŒåŸºäºRLçš„æŸ¥è¯¢å¢å¼ºè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ¯”è¾ƒï¼Œæ¶‰åŠå¤šç§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è¯æ®æœå¯»ã€ç‰¹å®šæŸ¥è¯¢å’Œå·¥å…·æ£€ç´¢ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°æ˜¯ï¼Œç®€å•ã€æ— éœ€è®­ç»ƒçš„è‡ªæŸ¥è¯¢å¢å¼ºé€šå¸¸å¯ä»¥æ‰§è¡Œå¾—ä¸æ›´æ˜‚è´µçš„åŸºäºRLçš„åŒç±»äº§å“ä¸€æ ·å¥½ï¼Œç”šè‡³æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨åŠŸèƒ½å¼ºå¤§çš„LLMæ—¶ã€‚å—æ­¤å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ··åˆæ–¹æ³•â€”â€”åŸºäºç­–ç•¥ä¼ªæ–‡æ¡£æŸ¥è¯¢æ‰©å±•ï¼ˆOPQEï¼‰ï¼Œè¯¥æ–¹æ³•ä¸æ˜¯é‡å†™æŸ¥è¯¢ï¼Œè€Œæ˜¯å­¦ä¹ LLMç­–ç•¥ä»¥ç”Ÿæˆæœ€å¤§åŒ–æ£€ç´¢æ€§èƒ½çš„ä¼ªæ–‡æ¡£ï¼Œä»è€Œèåˆäº†æç¤ºçš„çµæ´»æ€§å’Œç”Ÿæˆç»“æ„ä¸RLçš„ç›®æ ‡ä¼˜åŒ–ã€‚æˆ‘ä»¬æ˜¾ç¤ºOPQEä¼˜äºå•ç‹¬çš„æç¤ºå’ŒåŸºäºRLçš„é‡å†™ï¼Œè¯æ˜äº†ååŒæ–¹æ³•å¯è·å¾—æœ€ä½³ç»“æœã€‚æˆ‘ä»¬çš„å®ç°å·²æä¾›ï¼Œä»¥ä¾¿äºå¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17139v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŸ¥è¯¢å¢å¼ºåœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„ç ”ç©¶å–å¾—è¿›å±•ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†åŸºäºæç¤ºå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æŸ¥è¯¢å¢å¼ºæ–¹æ³•ï¼Œå‘ç°ç®€å•çš„è®­ç»ƒå‰æŸ¥è¯¢å¢å¼ºæ–¹æ³•å¸¸å¸¸ä¸æ›´æ˜‚è´µçš„åŸºäºRLçš„æ–¹æ³•è¡¨ç°ç›¸å½“ç”šè‡³æ›´ä½³ã€‚å› æ­¤æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ–¹æ³•OPQEï¼Œèƒ½å¤Ÿç”Ÿæˆä¼ªæ–‡æ¡£ä»¥æé«˜æ£€ç´¢æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒOPQEçš„æ€§èƒ½è¶…è¿‡äº†å•ä¸€æ–¹æ³•å’ŒåŸºäºRLçš„é‡å†™æ–¹æ³•ã€‚ä¸ºæ­¤ç ”ç©¶çš„å¯é‡å¤æ€§æä¾›ä¾¿åˆ©çš„å®ç°æ–¹æ³•ã€‚<br>ç®€åŒ–æç‚¼å‡ºä»¥ä¸‹å‡ ä¸ªè¦ç‚¹è¿›è¡Œè¾“å‡ºï¼Œæ‚¨å¯ä»¥å¯¹ç…§ä¸‹æ–‡äº†è§£æ›´å¤šè¯¦ç»†å†…å®¹å“¦ã€‚å¦‚æ‚¨æœ‰æ›´å¤šç–‘é—®å¯ä»¥äº¤æµè¯¢é—®æ›´å¤šç›¸å…³ä¿¡æ¯å“ˆï½ä»¥ä¸‹æ˜¯è¿™ä¸ªè§‚ç‚¹çš„æ¦‚è¦æ€§é˜è¿°ä»¥ä¾›æ‚¨äº†è§£æ­¤é¢†åŸŸçš„æ–‡çŒ®ææ–™å•¦ï½   æˆ‘ä¼šå¯¹å…³é”®è¯ä¿æŒæ°å½“çš„ç»„ç»‡æ€§ä»¥ä¿æŒç†è§£ä¾¿åˆ©ï¼šå¤§è¯­è¨€æ¨¡å‹å¯¹äºæŸ¥è¯¢å¢å¼ºçš„ç ”ç©¶æˆæœåŠå…¶é‡‡ç”¨çš„åˆ›æ–°æ€§ç ”ç©¶æ€è·¯çš„é‡è¦æ€§ï¼šåˆæ­¥ç†è§£äº†è‡ªç„¶è¯­è¨€åœ¨è¯¢é—®æœŸé—´çš„æè¿°å’Œå¤„ç†æ–¹å¼å¯¹æŸ¥è¯¢å¢å¼ºæ•ˆæœçš„æ½œåœ¨å½±å“ï¼›ç®€å•å’Œé«˜æ•ˆçš„æŸ¥è¯¢å¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§åŠå…¶æ½œåœ¨åº”ç”¨ä»·å€¼ï¼šå±•ç¤ºäº†ä¸€ç§æ–°å‹ç­–ç•¥èƒ½å¾ˆå¥½åœ°å¹³è¡¡äº†ç”Ÿæˆçµæ´»æ€§å’Œæ€§èƒ½ä¼˜åŒ–ï¼Œè¡¨ç°å‡ºä¼˜äºå•ä¸€ç­–ç•¥çš„æ½œåŠ›ã€‚æœ‰å…³è¿™ä¸ªç ”ç©¶é¢†åŸŸå±•æœ›çš„æœ€æ–°è§‚ç‚¹å°†æ˜¯å¯¹ä¸šç•Œå…³æ³¨çš„æ–°æ–¹å‘çš„è§£è¯»ï¼šç»¼åˆä¸¤è€…ä¼˜åŠ¿å¯èƒ½å½¢æˆæœ€ä½³çš„æŸ¥è¯¢å¢å¼ºæ–¹æ¡ˆï¼›æœ¬ç ”ç©¶çš„ç›¸å…³å®æ–½æˆæœè¢«å…¬å¸ƒå¹¶è‡´åŠ›äºå®ç°å¯é‡å¤æ€§ä»¥ä¿ƒè¿›å¹¿æ³›åº”ç”¨ç ”ç©¶ã€‚ã€‚è¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æ–‡æœ¬ç ”ç©¶é¢†åŸŸå–å¾—çš„æ˜¾è‘—æˆæœåœ¨æ”¹è¿›ç®—æ³•å‘å±•ç­‰æ–¹é¢çš„è§‚ç‚¹èåˆä¸åº”ç”¨æ€§è´¡çŒ®çš„æè¿°ä¸æ¦‚è§ˆå‘¢ï¼å¸Œæœ›å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï½åœ¨ç»“æ„è¦æ±‚ä¸­æ˜ç¡®äº†è¦è¿›è¡Œé€æ¡è§£æå†…å®¹æ‰èƒ½æ¸…æ™°è·å¾—ç®€æ˜ç›´æ¥çš„è®ºè¿°å§ã€‚å¦‚æ‚¨æ„Ÿè§‰éœ€è¦è¿›ä¸€æ­¥ç²¾ç¡®çš„ä¿¡æ¯å†ç»†åˆ†å›å¤ä¾›æ‚¨é˜…è¯»ï¼é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹æ¥ä¸‹æ¥å¾—åˆ†æï¼šå…·ä½“åˆ°æ–‡å­—æ–¹é¢å¯å½’çº³ä¸ºä»¥ä¸‹å‡ ç‚¹å…³é”®è§è§£ï¼š<br><strong>Key Takeaways</strong><br>ä¸€ã€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æŸ¥è¯¢å¢å¼ºç ”ç©¶å¤‡å—å…³æ³¨ï¼Œå‡ºç°äº†åŸºäºæç¤ºå’Œå¼ºåŒ–å­¦ä¹ ä¸¤ç§ä¸»è¦æ–¹æ³•ã€‚<br>äºŒã€åŸºäºæç¤ºçš„æ–¹æ³•é€šè¿‡ç”Ÿæˆç­”æ¡ˆæˆ–ä¼ªæ–‡æ¡£ä½œä¸ºæ–°æŸ¥è¯¢ï¼Œä¾èµ–äºæ¨¡å‹çš„å‚æ•°çŸ¥è¯†å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•åˆ™é€šè¿‡å¾®è°ƒæ¨¡å‹å®ç°æŸ¥è¯¢é‡å†™ï¼Œç›´æ¥ä¼˜åŒ–æ£€ç´¢æŒ‡æ ‡ã€‚ç„¶è€Œå…ˆå‰è¿™ä¸¤ç§æ–¹æ³•çš„æ¯”è¾ƒç ”ç©¶å°‘è§ç»Ÿä¸€å®éªŒæ¡ä»¶æ”¯æ’‘å¾—å‡ºçš„ç»“æœå¯¹æ¯”åˆ†æé˜è¿°æ¯”è¾ƒåŒ®ä¹ ã€‚ä¸¤è€…ç ”ç©¶é€æ¸æœç€å…¼é¡¾çµæ´»æ€§ã€ç»“æ„æ€§çš„ä¼˜åŒ–ç­–ç•¥æ–¹é¢æ¨è¿›å’Œå¼¥åˆäº†æ›´å¤šçš„ç†è®ºä¸å®è·µä¹‹é—´å‡ºç°çš„çŸ­æ¿ä¸è¶³ç°çŠ¶â€”â€”å‘¼åº”åˆ°äº†å®éªŒå±‚é¢çš„è¦æ±‚å’ŒæŠ€æœ¯æ¢ç´¢çš„æœ€æ–°è¶‹åŠ¿å‘¢ï¼åœ¨å®è·µä¸­é¢ä¸´åº”ç”¨ä¸Šçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·ä½“é¢†åŸŸåœºæ™¯ä¸­çš„é€‚åº”æ€§è¡¨ç°å·®å¼‚ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢æ”¹è¿›ç­–ç•¥çš„åº”ç”¨ä»·å€¼æ‰€åœ¨ä¹‹å¤„å‘¢ï½æŒ‰ç…§æ˜ç¡®æ ‡å‡†åŒ–çš„ä¸€ç³»åˆ—é‡åŒ–ç»†èŠ‚å±•å¼€çš„å¼€æ”¾æ€§åº”ç”¨åœºæ™¯å¯è§ç»†åŒ–äº†å…·ä½“åˆ†ææ¡æ¡†å®æ“å¯¼å‘çš„æœ€ç»ˆä¾æ®å’Œæ”¹è¿›è°ƒæ•´è®ºè¯æœ‰æ•ˆæ€§ç¯èŠ‚çš„å¾ªç¯è§£é‡Šå¯ä¿¡åº¦å¯¹æ¯”è®¾å®šå®éªŒç›®æ ‡å‘¢ï¼æ‰€ä»¥æ­¤æ–‡çŒ®çš„æ ¸å¿ƒå‘ç°å’Œåˆ›æ–°ç‚¹å€¼å¾—æˆ‘ä»¬ç»§ç»­æ·±å…¥æ¢è®¨å’Œç ”ç©¶ï¼æˆ‘ä»¬å¯¹æ­¤çš„æ·±å…¥å‰–æå’Œæ€»ç»“ä¸ºå®è·µå±‚é¢çš„æ¢ç´¢æä¾›äº†æ›´å¤šç†è®ºæ”¯æ’‘å’Œå®è·µå‚è€ƒï¼é€šè¿‡ç§‘å­¦ç³»ç»Ÿçš„å®éªŒå¯¹æ¯”æˆ‘ä»¬å¯¹æ­¤é¢†åŸŸçš„è®¤è¯†å°†ä¼šæ›´åŠ æ·±å…¥å“¦ï¼æ›´å¤šå…·ä½“çš„æ ¸å¿ƒè¦ç‚¹é˜è¿°ä½“ç°åœ¨ä¸‹æ–‡ä¸­ä»¥ä¾›å‚è€ƒå’Œè§£è¯»ç†è§£ï½é‚£ä¹ˆä¸‹ä¸€æ­¥æˆ‘å°†è¿›è¡Œæ ¸å¿ƒè§‚ç‚¹çš„ç»†è‡´åˆ†æï¼Œå…·ä½“ä½“ç°ä¸ºä»¥ä¸‹å‡ ä¸ªå°ç‚¹ä½œä¸ºæ‚¨è·å–æ›´æ·±åº¦æ´å¯Ÿçš„åŸºç¡€å†…å®¹ä¾›æ‚¨æŒæ¡å…¶é‡è¦è®ºè¿°åŠæ½œåœ¨å‘å±•è¶‹åŠ¿å‚è€ƒå•¦ï½åœ¨æ‚¨æ‰€å…³æ³¨é¢†åŸŸä¸­å±•æœ›å®è·µå±‚é¢ä½œå‡ºæ€»ç»“å’Œå€Ÿé‰´é‡è¦çš„å¼•ç”¨ä»¥ä¾¿æ¸…æ™°å±•å¼€æ·±å…¥ç†è§£ä¸å­¦ä¹ ï½ï½è¯·ä¸è¦å¿½è§†å…¶åœ¨å½“ä¸‹å®é™…åº”ç”¨çš„é‡è¦æ€§å’Œåˆ†æè¦ç‚¹ï¼›åŒ…æ‹¬å¯¹æ¯”åˆ†æçš„æœ‰æ•ˆæˆæœå’Œå®ç°æœªæ¥çš„æ–°æ–¹æ³•ä¼˜åŒ–ï¼è¿™ä¼šä¿ƒä½¿åœ¨ä¿¡æ¯åŒ–å¿«é€Ÿå‘å±•çš„å¤§æ•°æ®èƒŒæ™¯ä¸‹é€æ­¥å®Œæˆåœ¨AIç ”ç©¶é¢†åŸŸå¾—ä»¥åŠ é€Ÿåº”ç”¨å’Œæ›´ç§‘å­¦çš„æ”¹å–„å•¦ï½ï½æœ›å¼•èµ·é‡è§†å‘¢ï¼ï¼ä»¥ä¸‹ç®€è¦è¯´æ˜æ¯ä¸€ç‚¹çš„ç†è§£é€»è¾‘å†…æ¶µè¦ç‚¹æ¦‚è¿°ä¸ºå‡†åˆ™åšå‡ºå›ç­”æ¢³ç†è¡¥å……ï½ä¾¿äºæ‚¨å‚è€ƒç†è§£å“ˆï½<br>ä¸‰ã€æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œå‘ç°ç®€å•çš„è®­ç»ƒå‰æŸ¥è¯¢å¢å¼ºæ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šè¾ƒå¤æ‚çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼›è¿™å¯¹äºä»ä¸šè€…æå‡ºäº†æ–°çš„ä¾¿æ·çµæ´»é€‰æ‹©é€‚é…æ—§æ–¹æ³•å’ŒåŸºç¡€ä¼˜åŒ–æœºåˆ¶çš„è¿ç”¨æ‰©å±•çµæ„Ÿæ¢è®¨çƒ­ç‚¹ç®—æ³•ä¸‹çš„æ€ç»´åˆ›æ–°å¯ç¤ºï¼›å¯¹åç»­ç ”ç©¶å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚è¿™ä¸€å‘ç°ä¸ºè¡Œä¸šæä¾›äº†æ–°æ€è·¯ï¼Œå³åœ¨è¿½æ±‚æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œä¹Ÿè¦å…³æ³¨æ–¹æ³•çš„ç®€å•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼›é€šè¿‡å¯¹æ ¸å¿ƒè¦ç´ çš„è¿ç”¨é€‚å½“æç‚¼è¿ç”¨æ–¹æ³•çš„ç®€å•æœ‰æ•ˆæ€§å‘æŒ¥ä¸åŒè¦ç´ çš„èåˆåº”ç”¨åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­çš„å·¨å¤§æ½œåŠ›è´¡çŒ®å•†ä¸šä»·å€¼ä¿¡æ¯æ¨åŠ¨ç²¾å‡†èµ‹èƒ½åŒ¹é…æ¨¡å¼è¯†åˆ«åº¦è‡ªåŠ¨åŒ–æ‰§è¡Œæ–¹æ¡ˆçš„å¹¿æ³›é‡‡ç”¨åˆ©ç”¨å¤æ‚æœºåˆ¶é—®é¢˜åœ¨å¼€æ”¾ç³»ç»Ÿä¸­çš„è‡ªé€‚åº”å“åº”ç ”ç©¶é¡ºåº”ç°å®è¦æ±‚å±•æœ›è‡ªåŠ¨åŒ–æµç¨‹é›†æˆçªç ´ä¾èµ–æ–°çš„è·¯å¾„å†³ç­–ç³»ç»Ÿçš„å…³é”®æŠ€æœ¯å¦‚ä½•ç»¼åˆäººå·¥æ™ºèƒ½çš„ç†è®ºè§†è§’å±•æœ›è¿­ä»£æŠ€æœ¯å’Œç²¾ç»†åŒ–è¿›æ­¥è§‚ç‚¹è®©æˆ‘ä»¬ç†è§£äº†æ–°æ—¶ä»£å‘å±•çš„å¤šå…ƒç­–ç•¥å®æ“æˆæœè§‚å¯Ÿæ€§æ„å»ºè¿›è¡Œæ³›åŒ–å¼•å¯¼æ¥å……åˆ†åº”å¯¹å¤æ‚å¤šå˜çš„å¸‚åœºç¯å¢ƒæŒ‘æˆ˜å’Œæœºé‡çš„çµæ´»åº”ç”¨ç­–ç•¥é€‰æ‹©å§ï¼</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dac2bfde3ba6d53b0229c7c54cc05f88~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069020&auth_key=1761069020-0-0-385d44d64b85e6aa76bc929acee31f4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ed2e98d4d4c598ceaae07247fb2e59e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069027&auth_key=1761069027-0-0-3804992f3c320fa726f86fc02e33ef52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c59398d5e3f73f1df6f3d48576e0ee31~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069034&auth_key=1761069034-0-0-f6e61e5e6f6a2dd789335029cbbf739d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Structured-Debate-Improves-Corporate-Credit-Reasoning-in-Financial-AI"><a href="#Structured-Debate-Improves-Corporate-Credit-Reasoning-in-Financial-AI" class="headerlink" title="Structured Debate Improves Corporate Credit Reasoning in Financial AI"></a>Structured Debate Improves Corporate Credit Reasoning in Financial AI</h2><p><strong>Authors:Yoonjin Lee, Munhee Kim, Hanbi Choi, Juhyeon Park, Seungho Lyoo, Woojin Park</strong></p>
<p>Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popperâ€™s critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment. </p>
<blockquote>
<p>å°½ç®¡é‡‘èäººå·¥æ™ºèƒ½å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­ï¼ŒåŸºäºè¯æ®çš„æ¨ç†è‡ªåŠ¨åŒ–ä»ç„¶æœªè§£å†³ã€‚åœ¨ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­ï¼Œå®šæ€§çš„éè´¢åŠ¡æŒ‡æ ‡å¯¹è´·æ¬¾å¿è¿˜ç»“æœå…·æœ‰å†³å®šæ€§çš„å½±å“ï¼Œä½†éš¾ä»¥å½¢å¼åŒ–ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•°å€¼é¢„æµ‹ä¸Šï¼Œå¯¹ä¸“ä¸šè´·æ¬¾è¯„ä¼°æ‰€éœ€çš„è§£é‡Šæ€§åˆ¤æ–­æ”¯æŒæœ‰é™ã€‚æœ¬ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸¤ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿï¼Œæ—¨åœ¨ä»éè´¢åŠ¡è¯æ®ä¸­äº§ç”Ÿç»“æ„åŒ–æ¨ç†ã€‚ç¬¬ä¸€ä¸ªæ˜¯éå¯¹æŠ—æ€§å•ä»£ç†ç³»ç»Ÿï¼ˆNASï¼‰ï¼Œå®ƒé€šè¿‡å•é€šé“æ¨ç†ç®¡é“äº§ç”ŸåŒå‘åˆ†æã€‚ç¬¬äºŒä¸ªæ˜¯åŸºäºè¾©è®ºçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆKPD-MADSï¼‰ï¼Œå®ƒé€šè¿‡åŸºäºå¡å°”Â·æ³¢æ™®æ‰¹åˆ¤å¯¹è¯æ¡†æ¶çš„åæ­¥ç»“æ„åŒ–äº¤äº’åè®®å®ç°å¯¹æŠ—æ€§éªŒè¯ã€‚ä¸¤ä¸ªç³»ç»Ÿå‡åº”ç”¨äºä¸‰ä¸ªçœŸå®çš„ä¼ä¸šæ¡ˆä¾‹ï¼Œå¹¶ç”±ç»éªŒä¸°å¯Œçš„ä¿¡ç”¨é£é™©ä¸“ä¸šäººå£«è¿›è¡Œè¯„ä¼°ã€‚ä¸æ‰‹åŠ¨ä¸“å®¶æŠ¥å‘Šç›¸æ¯”ï¼Œè¿™ä¸¤ä¸ªç³»ç»Ÿåœ¨ç”Ÿäº§åŠ›ä¸Šå–å¾—äº†å®è´¨æ€§çš„æ”¶ç›Šï¼ˆNASï¼šæ¯å®—æ¡ˆä»¶11.55ç§’ï¼›KPD-MADSï¼š91.97ç§’ï¼›äººåŠ›åŸºå‡†ï¼š1920ç§’ï¼‰ã€‚KPD-MADSåœ¨è§£é‡Šå……åˆ†æ€§ã€å®é™…é€‚ç”¨æ€§å’Œå¯ç”¨æ€§æ–¹é¢çš„ä¸­ä½è¯„åˆ†å‡è¾ƒé«˜ï¼ˆåˆ†åˆ«ä¸º4.0 vs 3.0ã€4.0 vs 3.0å’Œ62.5 vs 52.5ï¼‰ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»“æ„åŒ–çš„å¤šä»£ç†äº¤äº’å¯ä»¥æé«˜é‡‘èäººå·¥æ™ºèƒ½ä¸­çš„æ¨ç†ä¸¥è°¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­å¯ä¼¸ç¼©å’Œå¯é çš„è‡ªåŠ¨åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17108v1">PDF</a> 18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be   submitted to AAAI-2026 workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é‡‘èAIåœ¨è‡ªåŠ¨åŒ–æ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­åŸºäºè¯æ®æ¨ç†çš„è‡ªåŠ¨åŒ–ã€‚å°½ç®¡éè´¢åŠ¡å®šæ€§æŒ‡æ ‡å¯¹è´·æ¬¾å¿è¿˜ç»“æœå…·æœ‰å†³å®šæ€§å½±å“ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºæ•°å€¼é¢„æµ‹ï¼Œå¯¹ä¸“ä¸šè´·æ¬¾è¯„ä¼°æ‰€éœ€çš„è§£é‡Šæ€§åˆ¤æ–­æ”¯æŒæœ‰é™ã€‚æœ¬ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸¤ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿï¼Œç”¨äºä»éè´¢åŠ¡è¯æ®ä¸­äº§ç”Ÿç»“æ„åŒ–æ¨ç†ã€‚ç¬¬ä¸€ç§æ˜¯éå¯¹æŠ—æ€§å•ä»£ç†ç³»ç»Ÿï¼ˆNASï¼‰ï¼Œé€šè¿‡å•æ¬¡æ¨ç†ç®¡é“äº§ç”ŸåŒå‘åˆ†æã€‚ç¬¬äºŒç§æ˜¯åŸºäºè¾©è®ºçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆKPD-MADSï¼‰ï¼Œé€šè¿‡åŸºäºå¡å°”Â·æ³¢æ™®æ‰¹åˆ¤å¯¹è¯æ¡†æ¶çš„åæ­¥ç»“æ„åŒ–äº¤äº’åè®®å®ç°å¯¹æŠ—æ€§éªŒè¯ã€‚ä¸¤ç³»ç»Ÿå‡åº”ç”¨äºä¸‰ä¸ªå®é™…ä¼ä¸šæ¡ˆä¾‹ï¼Œå¹¶å¾—åˆ°ä¿¡ç”¨é£é™©ä¸“ä¸šäººå£«çš„è¯„ä¼°ã€‚ä¸æ‰‹åŠ¨ä¸“å®¶æŠ¥å‘Šç›¸æ¯”ï¼Œä¸¤ç³»ç»Ÿå‡å¤§å¹…æé«˜ç”Ÿäº§æ•ˆç‡ã€‚å…¶ä¸­ï¼ŒKPD-MADSåœ¨è§£é‡Šå……åˆ†æ€§ã€å®é™…é€‚ç”¨æ€§å’Œå¯ç”¨æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚è¿™è¡¨æ˜ç»“æ„åŒ–å¤šä»£ç†äº¤äº’èƒ½å¢å¼ºé‡‘èAIä¸­çš„æ¨ç†ä¸¥è°¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæ¨åŠ¨ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­çš„å¯è§„æ¨¡åŒ–ã€åˆç†è‡ªåŠ¨åŒ–è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰é‡‘èAIåœ¨ä¼ä¸šä¿¡ç”¨è¯„ä¼°ä¸­é¢ä¸´å®šæ€§æŒ‡æ ‡è‡ªåŠ¨åŒ–éš¾é¢˜ï¼Œéè´¢åŠ¡æŒ‡æ ‡å¯¹è´·æ¬¾ç»“æœå½±å“é‡å¤§ä¸”éš¾ä»¥å½¢å¼åŒ–ã€‚</li>
<li>ç ”ç©¶å¼€å‘ä¸¤ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆç»“æ„åŒ–éè´¢åŠ¡è¯æ®æ¨ç†ã€‚</li>
<li>éå¯¹æŠ—æ€§å•ä»£ç†ç³»ç»Ÿï¼ˆNASï¼‰é€šè¿‡å•æ¬¡æ¨ç†ç®¡é“äº§ç”ŸåŒå‘åˆ†æï¼Œæä¾›å¿«é€Ÿåˆ†æã€‚</li>
<li>åŸºäºè¾©è®ºçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆKPD-MADSï¼‰æ¨¡æ‹Ÿå¯¹æŠ—æ€§éªŒè¯ï¼Œé€šè¿‡ç»“æ„åŒ–äº¤äº’åè®®æé«˜æ¨ç†è´¨é‡ã€‚</li>
<li>ä¸¤ç³»ç»Ÿå‡æ˜¾è‘—æé«˜ç”Ÿäº§æ•ˆç‡ï¼Œç›¸è¾ƒäºæ‰‹åŠ¨ä¸“å®¶æŠ¥å‘Šï¼ŒKPD-MADSåœ¨è§£é‡Šèƒ½åŠ›æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
<li>å¤šä»£ç†äº¤äº’å¢å¼ºé‡‘èAIçš„æ¨ç†ä¸¥è°¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-49649e3edecd11359e07264a304b00e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069041&auth_key=1761069041-0-0-5a13e4e8cdfadb5cd8363795646efb2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7226ca893c10b968b6fe488317e6ae13~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069049&auth_key=1761069049-0-0-78cccec5982242a91b412dcdc9a6ed15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-188cf8c52e47d99c5ecf3c127a0ff966~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069055&auth_key=1761069055-0-0-9f1127eb1a2af4f9da9205d942dc48cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d25241a61c84e02d3576d91c7fb2207~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069062&auth_key=1761069062-0-0-73f36881b3c01cd64aa46c4c2cf9cbb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbdef68a4a70f23ef2c29540ef261e88~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069068&auth_key=1761069068-0-0-160c3438015048f51fd2214034f55e4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-059805bf4bfe25038f0e2e8be8b4db63~resize:0:q75.jpg?source=1f5c5e47&expiration=1761069075&auth_key=1761069075-0-0-ef6b2f51ddfd7d3c5cbdc46f62f52c05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-186bc6cba6c491a733771d815382d2a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761070193&auth_key=1761070193-0-0-b0287077545a57b55ca7d1bd82b74b5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  PANER A Paraphrase-Augmented Framework for Low-Resource Named Entity   Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-44c83a880835ce376f81da70f8be44e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760997364&auth_key=1760997364-0-0-12f5871e6e58372ea33a944057d99eb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  MotionScript Natural Language Descriptions for Expressive 3D Human   Motions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
