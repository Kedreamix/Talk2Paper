<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  VERA-V Variational Inference Framework for Jailbreaking Vision-Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-e3da1ebb40b68de5ed774057cdb6d124~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074940&auth_key=1761074940-0-0-78807e1ef79b812c39fd63a87a237d05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-22-æ›´æ–°"><a href="#2025-10-22-æ›´æ–°" class="headerlink" title="2025-10-22 æ›´æ–°"></a>2025-10-22 æ›´æ–°</h1><h2 id="VERA-V-Variational-Inference-Framework-for-Jailbreaking-Vision-Language-Models"><a href="#VERA-V-Variational-Inference-Framework-for-Jailbreaking-Vision-Language-Models" class="headerlink" title="VERA-V: Variational Inference Framework for Jailbreaking Vision-Language   Models"></a>VERA-V: Variational Inference Framework for Jailbreaking Vision-Language   Models</h2><p><strong>Authors:Qilin Liao, Anamika Lochab, Ruqi Zhang</strong></p>
<p>Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°äº†è§†è§‰æ¨ç†é¢†åŸŸï¼Œä½†å…¶å¤šæ¨¡æ€è®¾è®¡ä¹Ÿå¼•å…¥äº†æ–°çš„ã€å°šæœªå……åˆ†æ¢ç´¢çš„æ¼æ´ã€‚ç°æœ‰çš„å¤šæ¨¡æ€çº¢é˜Ÿæ–¹æ³•å¤§å¤šä¾èµ–äºè„†å¼±çš„æ¨¡æ¿ï¼Œä¾§é‡äºå•ä¸€æ”»å‡»ç¯å¢ƒï¼Œå¹¶ä¸”åªæš´éœ²å‡ºå°‘é‡çš„æ¼æ´ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†VERA-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå˜åˆ†æ¨ç†æ¡†æ¶ï¼Œå®ƒå°†å¤šæ¨¡æ€è¶Šç‹±å‘ç°é‡æ–°å®šä¹‰ä¸ºå­¦ä¹ é…å¯¹æ–‡æœ¬å›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒã€‚è¿™ç§æ¦‚ç‡è®ºè§†è§’èƒ½å¤Ÿç”Ÿæˆéšè”½çš„ã€è€¦åˆçš„å¯¹æŠ—è¾“å…¥ï¼Œä»è€Œç»•è¿‡æ¨¡å‹é˜²çº¿ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„æ”»å‡»è€…æ¥è¿‘ä¼¼åéªŒæ¦‚ç‡ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹å„ç§è¶Šç‹±è¿›è¡Œé‡‡æ ·ï¼Œå¹¶æä¾›å¯¹æ¼æ´çš„åˆ†å¸ƒè§è§£ã€‚VERA-Vè¿˜é›†æˆäº†ä¸‰ç§äº’è¡¥çš„ç­–ç•¥ï¼šï¼ˆiï¼‰åŸºäºå­—ä½“çš„æ–‡æœ¬æç¤ºï¼ŒåµŒå…¥æœ‰å®³çº¿ç´¢ï¼›ï¼ˆiiï¼‰åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆï¼Œå¼•å…¥å¯¹æŠ—ä¿¡å·ï¼›ï¼ˆiiiï¼‰ç»“æ„åŒ–å¹²æ‰°é¡¹æ¥åˆ†æ•£VLMæ³¨æ„åŠ›ã€‚åœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVERA-Våœ¨å¼€æºå’Œå‰æ²¿çš„VLMsä¸Šå‡ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåœ¨GPT-4oä¸Šçš„æœ€ä½³åŸºçº¿æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰æé«˜äº†é«˜è¾¾53.75%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17759v1">PDF</a> 18 pages, 7 Figures,</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èåˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨ç†ï¼Œä½†å…¶å¤šæ¨¡æ€è®¾è®¡ä¹Ÿå¼•å…¥äº†æ–°çš„ã€å°šæœªå……åˆ†æ¢ç´¢çš„æ¼æ´ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶çš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºVERA-Vçš„å˜åˆ†æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ–‡æœ¬å’Œå›¾åƒæç¤ºä¹‹é—´çš„è”åˆåéªŒåˆ†å¸ƒæ¥é‡æ–°å‘ç°å¤šæ¨¡æ€æ¼æ´ã€‚è¿™ä¸€æ¦‚ç‡è§†è§’å¯ä»¥ç”Ÿæˆç»•è¿‡æ¨¡å‹è­¦æˆ’ç³»ç»Ÿçš„éšè”½æ€§è€¦åˆå¯¹æŠ—è¾“å…¥ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ”»å‡»è€…æ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œä»¥ä¾¿æœ‰æ•ˆé‡‡æ ·å¤šæ ·åŒ–çš„æ¼æ´ï¼Œå¹¶æä¾›å…³äºè„†å¼±æ€§çš„åˆ†å¸ƒè§è§£ã€‚VERA-Vç»“åˆäº†ä¸‰ç§äº’è¡¥ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºå­—ä½“çš„æ–‡æœ¬æç¤ºã€åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆä»¥åŠç»“æ„åŒ–å¹²æ‰°ä¿¡æ¯æ¥åˆ†æ•£VLMæ³¨æ„åŠ›ã€‚åœ¨HarmBenchå’ŒHADESåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVERA-Våœ¨å¼€æºå’Œå‰æ²¿çš„VLMsä¸Šå‡ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œåœ¨GPT-4oä¸Šçš„æ”»å‡»æˆåŠŸç‡æé«˜äº†é«˜è¾¾53.75%ã€‚æ­¤æˆæœæ­ç¤ºäº†é’ˆå¯¹VLMçš„æ–°æŒ‘æˆ˜å’Œæ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsèåˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¼•å…¥æ–°çš„æ¼æ´æŒ‘æˆ˜ã€‚</li>
<li>VERA-Væ¡†æ¶é‡‡ç”¨å˜åˆ†æ¨ç†æ¥æ¨¡æ‹Ÿæ–‡æœ¬å’Œå›¾åƒæç¤ºçš„è”åˆåéªŒåˆ†å¸ƒï¼Œçªç ´ç°æœ‰å¤šæ¨¡æ€æ£€æµ‹é™åˆ¶ã€‚</li>
<li>VERA-Væ¡†æ¶èƒ½ç”Ÿæˆéšè”½çš„å¯¹æŠ—è¾“å…¥ä»¥ç»•è¿‡æ¨¡å‹çš„è­¦æˆ’ç³»ç»Ÿã€‚</li>
<li>è½»é‡çº§æ”»å‡»è€…è¢«è®­ç»ƒæ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œä»¥å®ç°é«˜æ•ˆçš„æ¼æ´é‡‡æ ·å’Œå¯¹è„†å¼±æ€§çš„æ·±å…¥æ´å¯Ÿã€‚</li>
<li>VERA-Vç»“åˆäº†å¤šç§ç­–ç•¥ï¼šåŸºäºå­—ä½“çš„æ–‡æœ¬æç¤ºã€åŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆå’Œç»“æ„åŒ–å¹²æ‰°ä¿¡æ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-146d67b71510263242b370087d719e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074821&auth_key=1761074821-0-0-6310db8fca9955e0a221542fa8066410&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-194950c36083470464e4344f62c47659~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074827&auth_key=1761074827-0-0-26ddebdbd2fbcf5f691b26643c7aba1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5ae7dffb5a1ae4955bba6437d18554f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074835&auth_key=1761074835-0-0-833aa824b044aa7fdf4d5c3d42ad5154&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac48a0d9600db03e7fe525540fddd379~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074841&auth_key=1761074841-0-0-e2831b5624c58ac97f812f9a55a1c277&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automatic-Classification-of-Circulating-Blood-Cell-Clusters-based-on-Multi-channel-Flow-Cytometry-Imaging"><a href="#Automatic-Classification-of-Circulating-Blood-Cell-Clusters-based-on-Multi-channel-Flow-Cytometry-Imaging" class="headerlink" title="Automatic Classification of Circulating Blood Cell Clusters based on   Multi-channel Flow Cytometry Imaging"></a>Automatic Classification of Circulating Blood Cell Clusters based on   Multi-channel Flow Cytometry Imaging</h2><p><strong>Authors:Suqiang Ma, Subhadeep Sengupta, Yao Lee, Beikang Gu, Xianyan Chen, Xianqiao Wang, Yang Liu, Mengjia Xu, Galit H. Frydman, He Li</strong></p>
<p>Circulating blood cell clusters (CCCs) containing red blood cells (RBCs), white blood cells(WBCs), and platelets are significant biomarkers linked to conditions like thrombosis, infection, and inflammation. Flow cytometry, paired with fluorescence staining, is commonly used to analyze these cell clusters, revealing cell morphology and protein profiles. While computational approaches based on machine learning have advanced the automatic analysis of single-cell flow cytometry images, there is a lack of effort to build tools to automatically analyze images containing CCCs. Unlike single cells, cell clusters often exhibit irregular shapes and sizes. In addition, these cell clusters often consist of heterogeneous cell types, which require multi-channel staining to identify the specific cell types within the clusters. This study introduces a new computational framework for analyzing CCC images and identifying cell types within clusters. Our framework uses a two-step analysis strategy. First, it categorizes images into cell cluster and non-cluster groups by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms traditional convolutional neural networks (CNNs), Vision Transformers (ViT). Then, it identifies cell types by overlaying cluster contours with regions from multi-channel fluorescence stains, enhancing accuracy despite cell debris and staining artifacts. This approach achieved over 95% accuracy in both cluster classification and phenotype identification. In summary, our automated framework effectively analyzes CCC images from flow cytometry, leveraging both bright-field and fluorescence data. Initially tested on blood cells, it holds potential for broader applications, such as analyzing immune and tumor cell clusters, supporting cellular research across various diseases. </p>
<blockquote>
<p>å¾ªç¯è¡€ç»†èƒå›¢ï¼ˆCCCsï¼‰åŒ…å«çº¢ç»†èƒï¼ˆRBCsï¼‰ã€ç™½ç»†èƒï¼ˆWBCsï¼‰å’Œè¡€å°æ¿ï¼Œæ˜¯ä¸è¡€æ “ã€æ„ŸæŸ“å’Œç‚ç—‡ç­‰ç–¾ç—…ç›¸å…³çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æµå¼ç»†èƒæœ¯ç»“åˆè§å…‰æŸ“è‰²å¸¸ç”¨äºåˆ†æè¿™äº›ç»†èƒå›¢ï¼Œæ­ç¤ºç»†èƒå½¢æ€å’Œè›‹ç™½è´¨è°±ã€‚è™½ç„¶åŸºäºæœºå™¨å­¦ä¹ çš„è®¡ç®—æ–¹æ³•åœ¨å•ç»†èƒæµå¼ç»†èƒæœ¯å›¾åƒè‡ªåŠ¨åˆ†ææ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¯¹äºè‡ªåŠ¨åˆ†æåŒ…å«CCCçš„å›¾åƒçš„å·¥å…·å¼€å‘ä»ç¼ºä¹åŠªåŠ›ã€‚ä¸å•ä¸ªç»†èƒä¸åŒï¼Œç»†èƒå›¢é€šå¸¸è¡¨ç°å‡ºä¸è§„åˆ™çš„å½¢çŠ¶å’Œå¤§å°ã€‚æ­¤å¤–ï¼Œè¿™äº›ç»†èƒå›¢å¾€å¾€ç”±å¤šç§ç»†èƒç±»å‹ç»„æˆï¼Œéœ€è¦å¤šé€šé“æŸ“è‰²æ¥è¯†åˆ«å›¢å†…çš„ç‰¹å®šç»†èƒç±»å‹ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œç”¨äºåˆ†æCCCå›¾åƒå¹¶è¯†åˆ«å›¢å†…çš„ç»†èƒç±»å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸¤æ­¥åˆ†æç­–ç•¥ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡å¾®è°ƒYou Only Look Onceï¼ˆYOLOv11ï¼‰æ¨¡å‹å°†å›¾åƒåˆ†ä¸ºç»†èƒå›¢å’Œéç»†èƒå›¢ä¸¤ç»„ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚ç„¶åï¼Œå®ƒé€šè¿‡å åŠ ç¾¤é›†è½®å»“å’Œå¤šé€šé“è§å…‰æŸ“æ–™çš„åŒºåŸŸæ¥è¯†åˆ«ç»†èƒç±»å‹ï¼Œå³ä½¿åœ¨ç»†èƒç¢ç‰‡å’ŒæŸ“è‰²ä¼ªå½±çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨é›†ç¾¤åˆ†ç±»å’Œè¡¨å‹è¯†åˆ«æ–¹é¢çš„å‡†ç¡®ç‡å‡è¶…è¿‡95%ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ¡†æ¶æœ‰æ•ˆåœ°åˆ†æäº†æµå¼ç»†èƒä»ªçš„CCCå›¾åƒï¼Œç»“åˆäº†æ˜åœºå’Œè§å…‰æ•°æ®ã€‚æœ€åˆåœ¨è¡€ç»†èƒä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå®ƒåœ¨åˆ†æå…ç–«å’Œè‚¿ç˜¤ç»†èƒå›¢ç­‰æ›´å¹¿æ³›åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œæ”¯æŒå„ç§ç–¾ç—…çš„ç»†èƒç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17716v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¾ªç¯è¡€ç»†èƒå›¢ï¼ˆCCCsï¼‰åŒ…å«çº¢ç»†èƒï¼ˆRBCsï¼‰ã€ç™½ç»†èƒï¼ˆWBCsï¼‰å’Œè¡€å°æ¿ï¼Œæ˜¯è¡€æ “ã€æ„ŸæŸ“å’Œç‚ç—‡ç­‰ç–¾ç—…çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æµå¼ç»†èƒæœ¯ç»“åˆè§å…‰æŸ“è‰²å¸¸ç”¨äºåˆ†æè¿™äº›ç»†èƒå›¢ï¼Œå¯æ­ç¤ºç»†èƒå½¢æ€å’Œè›‹ç™½è´¨è°±ã€‚è™½ç„¶åŸºäºæœºå™¨å­¦ä¹ çš„è®¡ç®—æ–¹æ³•åœ¨å•ç»†èƒæµå¼ç»†èƒæœ¯å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å°šæœªæœ‰å·¥å…·è‡ªåŠ¨åˆ†æåŒ…å«CCCsçš„å›¾åƒã€‚ä¸å•ä¸€ç»†èƒä¸åŒï¼Œç»†èƒå›¢å¸¸è¡¨ç°å‡ºä¸è§„åˆ™çš„å½¢æ€å’Œå¤§å°ã€‚æ­¤å¤–ï¼Œè¿™äº›ç»†èƒå›¢é€šå¸¸ç”±å¤šç§ç»†èƒç±»å‹ç»„æˆï¼Œéœ€è¦å¤šé€šé“æŸ“è‰²æ¥è¯†åˆ«å›¢å†…çš„ç‰¹å®šç»†èƒç±»å‹ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œç”¨äºåˆ†æCCCå›¾åƒå¹¶è¯†åˆ«å›¢å†…çš„ç»†èƒç±»å‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤æ­¥åˆ†æç­–ç•¥ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡å¾®è°ƒYou Only Look Onceï¼ˆYOLOv11ï¼‰æ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºç»†èƒå›¢å’Œéç»†èƒå›¢ç»„ï¼Œè¡¨ç°å‡ºæ¯”ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚ç„¶åï¼Œé€šè¿‡å åŠ ç¾¤è½®å»“ä¸å¤šé€šé“è§å…‰æŸ“è‰²çš„åŒºåŸŸæ¥è¯†åˆ«ç»†èƒç±»å‹ï¼Œæé«˜äº†åœ¨ç»†èƒç¢ç‰‡å’ŒæŸ“è‰²ä¼ªå½±å­˜åœ¨ä¸‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨é›†ç¾¤åˆ†ç±»å’Œè¡¨å‹è¯†åˆ«æ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡95%ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ¡†æ¶æœ‰æ•ˆåœ°åˆ†æäº†æµå¼ç»†èƒæœ¯ä¸­çš„CCCå›¾åƒï¼Œåˆ©ç”¨æ˜åœºå’Œè§å…‰æ•°æ®ã€‚æœ€åˆåœ¨è¡€ç»†èƒä¸Šçš„æµ‹è¯•è¡¨æ˜å…¶åœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä¾‹å¦‚åˆ†æå…ç–«å’Œè‚¿ç˜¤ç»†èƒå›¢ï¼Œæ”¯æŒå„ç§ç–¾ç—…çš„ç»†èƒç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¾ªç¯è¡€ç»†èƒå›¢ï¼ˆCCCsï¼‰æ˜¯å¤šç§ç–¾ç—…çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>æµå¼ç»†èƒæœ¯ç»“åˆè§å…‰æŸ“è‰²æ˜¯åˆ†æCCCsçš„å¸¸ç”¨æ–¹æ³•ã€‚</li>
<li>å½“å‰ç¼ºä¹è‡ªåŠ¨åˆ†æCCCså›¾åƒçš„å·¥å…·ã€‚</li>
<li>ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤æ­¥åˆ†æç­–ç•¥è¿›è¡ŒCCCså›¾åƒåˆ†æã€‚</li>
<li>æ¡†æ¶é¦–å…ˆé€šè¿‡æ”¹è¿›çš„YOLOv11æ¨¡å‹åŒºåˆ†ç»†èƒå›¢å’Œéç»†èƒå›¢ã€‚</li>
<li>æ¡†æ¶é€šè¿‡å åŠ å¤šé€šé“è§å…‰æŸ“è‰²ä¸ç¾¤è½®å»“ï¼Œå‡†ç¡®è¯†åˆ«ç»†èƒç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3f103d60b7350da818f055d6feed5ca9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074849&auth_key=1761074849-0-0-0f46bbc47b2554eb88d5bb77b8d96a1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-060a63a0a82186ad786288395ed3475a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074856&auth_key=1761074856-0-0-000ac94cb0e8d7aaaa2747f5b218b156&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-48957ca00b70f4e30d24feecafd6821d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074862&auth_key=1761074862-0-0-33ba69e8877eea47642dd88df93adff9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96fb513c9a8899536b17880d9025a1d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074869&auth_key=1761074869-0-0-7dc07bc69d83b5f3eb63926950063bf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ZACH-ViT-A-Zero-Token-Vision-Transformer-with-ShuffleStrides-Data-Augmentation-for-Robust-Lung-Ultrasound-Classification"><a href="#ZACH-ViT-A-Zero-Token-Vision-Transformer-with-ShuffleStrides-Data-Augmentation-for-Robust-Lung-Ultrasound-Classification" class="headerlink" title="ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data   Augmentation for Robust Lung Ultrasound Classification"></a>ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data   Augmentation for Robust Lung Ultrasound Classification</h2><p><strong>Authors:Athanasios Angelakis, Amne Mousa, Micah L. A. Heldeweg, Laurens A. Biesheuvel, Mark A. Haaksma, Jasper M. Smit, Pieter R. Tuinman, Paul W. G. Elbers</strong></p>
<p>Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and structurally normal lungs in lung ultrasound (LUS) videos remains challenging due to the high visual variability of non-cardiogenic inflammatory patterns (NCIP&#x2F;ARDS-like), interstitial lung disease, and healthy lungs. This heterogeneity complicates automated classification as overlapping B-lines and pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer variant that removes both positional embeddings and the [CLS] token, making it fully permutation-invariant and suitable for unordered medical image data. To enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA), which permutes probe-view sequences and frame orders while preserving anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95 critically ill patients against nine state-of-the-art baselines. Despite the heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60) and specificity (0.91), while all competing models collapsed to trivial classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with 2.5x fewer parameters, supporting real-time clinical deployment. These results show that aligning architectural design with data structure can outperform scale in small-data medical imaging. </p>
<blockquote>
<p>åœ¨è‚ºéƒ¨è¶…å£°ï¼ˆLUSï¼‰è§†é¢‘ä¸­ï¼Œä»å¿ƒæºæ€§è‚ºæ°´è‚¿ï¼ˆCPEï¼‰åŒºåˆ†éå¿ƒæºæ€§å’Œç»“æ„æ­£å¸¸çš„è‚ºéƒ¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéå¿ƒæºæ€§ç‚ç—‡æ¨¡å¼ï¼ˆNCIP&#x2F;ARDSç±»ä¼¼ï¼‰ã€é—´è´¨æ€§è‚ºç–¾ç—…å’Œå¥åº·è‚ºéƒ¨çš„è§†è§‰å˜åŒ–å¾ˆå¤§ã€‚è¿™ç§å¼‚è´¨æ€§ä½¿å¾—è‡ªåŠ¨åŒ–åˆ†ç±»å˜å¾—å¤æ‚ï¼Œå› ä¸ºBçº¿é‡å å’Œèƒ¸è†œä¼ªå½±å¾ˆå¸¸è§ã€‚æˆ‘ä»¬å¼•å…¥äº†ZACH-ViTï¼ˆé›¶ä»¤ç‰Œè‡ªé€‚åº”ç´§å‡‘åˆ†å±‚è§†è§‰è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰0.25Må‚æ•°çš„è§†è§‰è½¬æ¢å™¨å˜ä½“ï¼Œå®ƒç§»é™¤äº†ä½ç½®åµŒå…¥å’Œ[CLS]ä»¤ç‰Œï¼Œä½¿å…¶å®Œå…¨å…·æœ‰æ’åˆ—ä¸å˜æ€§ï¼Œé€‚ç”¨äºæ— åºçš„åŒ»å­¦å›¾åƒæ•°æ®ã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ShuffleStridesæ•°æ®å¢å¼ºï¼ˆSSDAï¼‰ï¼Œå®ƒé€šè¿‡æ‰“ä¹±æ¢å¤´è§†å›¾åºåˆ—å’Œå¸§é¡ºåºï¼ŒåŒæ—¶ä¿ç•™è§£å‰–æœ‰æ•ˆæ€§ã€‚ZACH-ViTåœ¨æ¥è‡ª95åå±é‡æ‚£è€…çš„380ä¸ªLUSè§†é¢‘ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä¹ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ã€‚å°½ç®¡éå¿ƒæºæ€§ç»„çš„å¼‚è´¨æ€§ï¼ŒZACH-ViTä»è·å¾—äº†æœ€é«˜çš„éªŒè¯å’Œæµ‹è¯•ROC-AUCï¼ˆåˆ†åˆ«ä¸º0.80å’Œ0.79ï¼‰ï¼Œå¹¶ä¸”å…·æœ‰å¹³è¡¡çš„æ•æ„Ÿæ€§ï¼ˆ0.60ï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆ0.91ï¼‰ï¼Œè€Œæ‰€æœ‰å…¶ä»–æ¨¡å‹åˆ™æ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†ç±»ã€‚å®ƒçš„è®­ç»ƒé€Ÿåº¦æ¯”å‚æ•°è¾ƒå°‘çš„Minimal ViTå¿«1.35å€ï¼Œå¹¶ä¸”å‚æ•°æ›´å°‘ï¼Œæ”¯æŒå®æ—¶ä¸´åºŠéƒ¨ç½²ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†æ¶æ„è®¾è®¡ä¸æ•°æ®ç»“æ„ç›¸åŒ¹é…å¯ä»¥åœ¨å°å‹åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸­è¶…è¶Šè§„æ¨¡ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17650v1">PDF</a> 14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine   Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),   eess.IV (Image and Video Processing). Code available at:   <a target="_blank" rel="noopener" href="https://github.com/Bluesman79/ZACH-ViT">https://github.com/Bluesman79/ZACH-ViT</a> Installation: pip install zachvit   Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License</p>
<p><strong>æ‘˜è¦</strong><br>    é›¶æ ‡è®°è‡ªé€‚åº”ç´§å‡‘åˆ†å±‚è§†è§‰Transformerï¼ˆZACH-ViTï¼‰èƒ½æœ‰æ•ˆè§£å†³å¿ƒè„æ€§è‚ºæ°´è‚¿ä¸éå¿ƒè„æ€§åŠæ­£å¸¸è‚ºç»“æ„çš„é‰´åˆ«é—®é¢˜ã€‚å»é™¤ä½ç½®åµŒå…¥å’Œå…¨å±€ç±»åˆ«ä¿¡æ¯tokenå®ç°äº†æ•°æ®çš„å…¨æ’åˆ—ä¸å˜æ€§ã€‚ä½¿ç”¨ShuffleStridesæ•°æ®å¢å¼ºæé«˜äº†æ¨¡å‹é€šç”¨æ€§ã€‚åœ¨è‚ºéƒ¨è¶…å£°è§†é¢‘è¯„ä¼°ä¸­ï¼ŒZACH-ViTå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°äº†æœ€é«˜éªŒè¯å’Œæµ‹è¯•ROC-AUCå€¼ï¼Œå¹¶è®­ç»ƒé€Ÿåº¦å¿«äºå…¶ä»–æ¨¡å‹ï¼Œæ”¯æŒå®æ—¶ä¸´åºŠåº”ç”¨ã€‚ç»“æœè¯æ˜ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒçš„å°æ•°æ®é›†ï¼Œé€‚é…æ¶æ„è®¾è®¡ä¼˜äºè§„æ¨¡ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ZACH-ViTæ¨¡å‹è§£å†³äº†å¿ƒè„æ€§è‚ºæ°´è‚¿ä¸éå¿ƒè„æ€§è‚ºç–¾ç—…çš„é‰´åˆ«éš¾é¢˜ã€‚</li>
<li>é€šè¿‡å»é™¤ä½ç½®åµŒå…¥å’Œå…¨å±€ç±»åˆ«ä¿¡æ¯tokenï¼Œæ¨¡å‹å®ç°å…¨æ’åˆ—ä¸å˜æ€§ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>ShuffleStridesæ•°æ®å¢å¼ºæŠ€æœ¯æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æ‰“ä¹±è§†å›¾åºåˆ—å’Œå¸§é¡ºåºæ¥ä¿ç•™è§£å‰–ç»“æ„çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨è‚ºéƒ¨è¶…å£°è§†é¢‘è¯„ä¼°ä¸­ï¼ŒZACH-ViTè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå…·æœ‰é«˜éªŒè¯å’Œæµ‹è¯•ROC-AUCå€¼ã€‚</li>
<li>ZACH-ViTä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
<li>ZACH-ViTè®­ç»ƒé€Ÿåº¦å¿«ï¼Œå‚æ•°å°‘ï¼Œé€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4371c8edd367ce36b8b8176b7eef99f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074876&auth_key=1761074876-0-0-dcc8989e16bd3d1bfa78754695d1b3d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb21c8236f33c9cd1938bdec22daff73~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074883&auth_key=1761074883-0-0-7c086ca55780b5ab1b1796c946ffda46&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94bdaf38f5ee054378c7a0ca6cadce4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074890&auth_key=1761074890-0-0-93b7ecc67f8aa0b5c7f0d0641bb93a2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e25e55d79640e38f0ab0e3923bb7ab40~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074897&auth_key=1761074897-0-0-e665ab2842da9f037c9b6177e5ca8977&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M2H-Multi-Task-Learning-with-Efficient-Window-Based-Cross-Task-Attention-for-Monocular-Spatial-Perception"><a href="#M2H-Multi-Task-Learning-with-Efficient-Window-Based-Cross-Task-Attention-for-Monocular-Spatial-Perception" class="headerlink" title="M2H: Multi-Task Learning with Efficient Window-Based Cross-Task   Attention for Monocular Spatial Perception"></a>M2H: Multi-Task Learning with Efficient Window-Based Cross-Task   Attention for Monocular Spatial Perception</h2><p><strong>Authors:U. V. B. L Udugama, George Vosselman, Francesco Nex</strong></p>
<p>Deploying real-time spatial perception on edge devices requires efficient multi-task models that leverage complementary task information while minimizing computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel multi-task learning framework designed for semantic segmentation and depth, edge, and surface normal estimation from a single monocular image. Unlike conventional approaches that rely on independent single-task models or shared encoder-decoder architectures, M2H introduces a Window-Based Cross-Task Attention Module that enables structured feature exchange while preserving task-specific details, improving prediction consistency across tasks. Built on a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time deployment and serves as the foundation for monocular spatial perception systems supporting 3D scene graph construction in dynamic environments. Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task models on NYUDv2, surpasses single-task depth and semantic baselines on Hypersim, and achieves superior performance on the Cityscapes dataset, all while maintaining computational efficiency on laptop hardware. Beyond benchmarks, M2H is validated on real-world data, demonstrating its practicality in spatial perception tasks. </p>
<blockquote>
<p>éƒ¨ç½²è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶ç©ºé—´æ„ŸçŸ¥éœ€è¦é«˜æ•ˆçš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œè¿™ç§æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨äº’è¡¥ä»»åŠ¡ä¿¡æ¯ï¼ŒåŒæ—¶æœ€å°åŒ–è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡ä»‹ç»äº†Multi-Mono-Hydraï¼ˆM2Hï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºè¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ã€è¾¹ç¼˜å’Œè¡¨é¢æ³•çº¿ä¼°è®¡è®¾è®¡çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œè¿™äº›å‡åŸºäºå•ç›®å›¾åƒã€‚ä¸åŒäºä¼ ç»Ÿä¾èµ–äºç‹¬ç«‹å•ä»»åŠ¡æ¨¡å‹æˆ–å…±äº«ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ–¹æ³•ï¼ŒM2Hå¼•å…¥äº†åŸºäºçª—å£çš„è·¨ä»»åŠ¡æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¿ç•™ä»»åŠ¡ç‰¹å®šç»†èŠ‚çš„åŒæ—¶ï¼Œå®ç°ç»“æ„åŒ–ç‰¹å¾äº¤æ¢ï¼Œæé«˜ä»»åŠ¡é—´é¢„æµ‹çš„ä¸€è‡´æ€§ã€‚M2Hå»ºç«‹åœ¨è½»é‡çº§çš„åŸºäºViTçš„DINOv2ä¸»å¹²ç½‘ä¸Šï¼Œä¼˜åŒ–äº†å®æ—¶éƒ¨ç½²ï¼Œå¹¶æˆä¸ºæ”¯æŒåŠ¨æ€ç¯å¢ƒä¸­3Dåœºæ™¯å›¾æ„å»ºçš„å•ç›®ç©ºé—´æ„ŸçŸ¥ç³»ç»Ÿçš„åŸºç¡€ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒM2Håœ¨NYUDv2ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œåœ¨Hypersimä¸Šè¶…è¶Šäº†å•ä»»åŠ¡æ·±åº¦å’Œè¯­ä¹‰åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨Cityscapesæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç¬”è®°æœ¬ç”µè„‘ç¡¬ä»¶ä¸Šä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚é™¤äº†åŸºå‡†æµ‹è¯•ä¹‹å¤–ï¼ŒM2Hè¿˜åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨ç©ºé—´æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17363v1">PDF</a> Accepted to the IEEE&#x2F;RSJ International Conference on Intelligent   Robots and Systems (IROS 2025). 8 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMulti-Mono-Hydraï¼ˆM2Hï¼‰çš„æ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºä»å•ä¸€å•ç›®å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ã€è¾¹ç¼˜å’Œè¡¨é¢æ³•çº¿ä¼°ç®—ã€‚M2Hé‡‡ç”¨åŸºäºçª—å£çš„è·¨ä»»åŠ¡æ³¨æ„æ¨¡å—ï¼Œä¿ƒè¿›ç»“æ„åŒ–ç‰¹å¾äº¤æ¢åŒæ—¶ä¿ç•™ä»»åŠ¡ç‰¹å®šç»†èŠ‚ï¼Œæ”¹è¿›äº†è·¨ä»»åŠ¡çš„é¢„æµ‹ä¸€è‡´æ€§ã€‚å®ƒå»ºç«‹åœ¨è½»é‡çº§çš„ViT-based DINOv2 backboneä¸Šï¼Œç»è¿‡ä¼˜åŒ–ï¼Œé€‚åˆåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶éƒ¨ç½²çš„å•ç›®ç©ºé—´æ„ŸçŸ¥ç³»ç»Ÿæ”¯æŒ3Dåœºæ™¯å›¾æ„å»ºã€‚è¯„ä¼°è¡¨æ˜ï¼ŒM2Håœ¨å¤šä»»åŠ¡æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¼˜äºNYUDv2ä¸Šçš„æœ€æ–°æŠ€æœ¯ï¼Œå¹¶åœ¨Hypersimä¸Šè¶…è¿‡äº†å•ä»»åŠ¡æ·±åº¦å’Œè¯­ä¹‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨Cityscapesæ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç¬”è®°æœ¬ç”µè„‘ç¡¬ä»¶ä¸Šä¿æŒè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M2Hæ˜¯ä¸€ä¸ªç”¨äºå®æ—¶ç©ºé—´æ„ŸçŸ¥çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚</li>
<li>å®ƒé‡‡ç”¨åŸºäºçª—å£çš„è·¨ä»»åŠ¡æ³¨æ„æ¨¡å—ï¼Œä¿ƒè¿›ç‰¹å¾äº¤æ¢å¹¶æ”¹è¿›é¢„æµ‹ä¸€è‡´æ€§ã€‚</li>
<li>M2Hå»ºç«‹åœ¨è½»é‡çº§çš„ViT-based DINOv2 backboneä¸Šï¼Œä¼˜åŒ–ç”¨äºå®æ—¶éƒ¨ç½²ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒä»å•ä¸€å•ç›®å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ã€è¾¹ç¼˜å’Œè¡¨é¢æ³•çº¿ä¼°ç®—ã€‚</li>
<li>M2Håœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ï¼ŒåŒ…æ‹¬NYUDv2ã€Hypersimå’ŒCityscapesã€‚</li>
<li>M2Håœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†å…¶å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-70cb32a06d7d9b26591a026d7c4ac599~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074905&auth_key=1761074905-0-0-79441e550523d4082df0c9c7a31b3984&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4075f6d5639e09c3e39f49f2479aa0d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074912&auth_key=1761074912-0-0-b2f630cbb33a8da71431dd45971338a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e0254a43b3f63718141131ee47c3af9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074920&auth_key=1761074920-0-0-ca615be8e56ffef03588e4c0474a0dce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13c4819f8b502203bba5a7a889244adb~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074927&auth_key=1761074927-0-0-556d8c203f412320129d70b439cac3d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4e2ed960584a2139439cc90e366466cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074933&auth_key=1761074933-0-0-04d5af9872eb8929be9ebfb42da4d466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3da1ebb40b68de5ed774057cdb6d124~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074940&auth_key=1761074940-0-0-78807e1ef79b812c39fd63a87a237d05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73af2c5bfa704110985c3164428dece0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074947&auth_key=1761074947-0-0-b73013e4f3ae13fc44bd84c2827a276a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36d1909f1b0668bb2e0b1f42fdf3ca54~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074953&auth_key=1761074953-0-0-b87404ceca7d84d520018f016d2efe52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OmniVIC-A-Self-Improving-Variable-Impedance-Controller-with-Vision-Language-In-Context-Learning-for-Safe-Robotic-Manipulation"><a href="#OmniVIC-A-Self-Improving-Variable-Impedance-Controller-with-Vision-Language-In-Context-Learning-for-Safe-Robotic-Manipulation" class="headerlink" title="OmniVIC: A Self-Improving Variable Impedance Controller with   Vision-Language In-Context Learning for Safe Robotic Manipulation"></a>OmniVIC: A Self-Improving Variable Impedance Controller with   Vision-Language In-Context Learning for Safe Robotic Manipulation</h2><p><strong>Authors:Heng Zhang, Wei-Hsing Huang, Gokhan Solak, Arash Ajoudani</strong></p>
<p>We present OmniVIC, a universal variable impedance controller (VIC) enhanced by a vision language model (VLM), which improves safety and adaptation in any contact-rich robotic manipulation task to enhance safe physical interaction. Traditional VIC have shown advantages when the robot physically interacts with the environment, but lack generalization in unseen, complex, and unstructured safe interactions in universal task scenarios involving contact or uncertainty. To this end, the proposed OmniVIC interprets task context derived reasoning from images and natural language and generates adaptive impedance parameters for a VIC controller. Specifically, the core of OmniVIC is a self-improving Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG retrieves relevant prior experiences from a structured memory bank to inform the controller about similar past tasks, and ICL leverages these retrieved examples and the prompt of current task to query the VLM for generating context-aware and adaptive impedance parameters for the current manipulation scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in universal task scenarios. The impedance parameter regulation is further informed by real-time force&#x2F;torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms baselines on a suite of complex contact-rich tasks, both in simulation and on real-world robotic tasks, with improved success rates and reduced force violations. OmniVIC takes a step towards bridging high-level semantic reasoning and low-level compliant control, enabling safer and more generalizable manipulation. Overall, the average success rate increases from 27% (baseline) to 61.4% (OmniVIC). </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†OmniVICï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¢å¼ºçš„é€šç”¨å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œå®ƒæé«˜äº†ä»»ä½•æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ï¼Œä»è€Œå¢å¼ºäº†å®‰å…¨ç‰©ç†äº¤äº’ã€‚ä¼ ç»ŸVICåœ¨æœºå™¨äººä¸ç¯å¢ƒè¿›è¡Œç‰©ç†äº¤äº’æ—¶æ˜¾ç¤ºå‡ºä¼˜åŠ¿ï¼Œä½†åœ¨æ¶‰åŠæ¥è§¦æˆ–ä¸ç¡®å®šæ€§çš„é€šç”¨ä»»åŠ¡åœºæ™¯ä¸­ç¼ºä¹åº”å¯¹æœªè§ã€å¤æ‚å’Œéç»“æ„åŒ–å®‰å…¨äº¤äº’çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ‰€æå‡ºçš„OmniVICè§£é‡Šä»å›¾åƒå’Œè‡ªç„¶è¯­è¨€ä¸­å¾—å‡ºçš„ä»»åŠ¡ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶ä¸ºVICæ§åˆ¶å™¨ç”Ÿæˆè‡ªé€‚åº”é˜»æŠ—å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒOmniVICçš„æ ¸å¿ƒæ˜¯è‡ªæˆ‘å®Œå–„çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå…¶ä¸­RAGä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆå‰ç»éªŒï¼Œä»¥å‘æ§åˆ¶å™¨å‘ŠçŸ¥ç±»ä¼¼è¿‡å»ä»»åŠ¡çš„ä¿¡æ¯ï¼Œè€ŒICLåˆ™åˆ©ç”¨è¿™äº›æ£€ç´¢åˆ°çš„ç¤ºä¾‹å’Œå½“å‰ä»»åŠ¡çš„æç¤ºæ¥æŸ¥è¯¢VLMï¼Œä»¥ç”Ÿæˆå½“å‰æ“ä½œåœºæ™¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè‡ªé€‚åº”é˜»æŠ—å‚æ•°ã€‚å› æ­¤ï¼Œè‡ªæˆ‘å®Œå–„çš„RAGå’ŒICLä¿è¯äº†OmniVICåœ¨é€šç”¨ä»»åŠ¡åœºæ™¯ä¸­çš„è¿è¡Œã€‚é˜»æŠ—å‚æ•°è°ƒèŠ‚è¿˜å—åˆ°å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆçš„å¯å‘ï¼Œä»¥ç¡®ä¿äº¤äº’åŠ›ä¿æŒåœ¨å®‰å…¨é˜ˆå€¼å†…ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ç³»åˆ—å¤æ‚çš„æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºå‡†çº¿ï¼Œæ— è®ºæ˜¯åœ¨æ¨¡æ‹Ÿè¿˜æ˜¯åœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œéƒ½æé«˜äº†æˆåŠŸç‡å¹¶å‡å°‘äº†åŠ›åº¦è¿è§„ã€‚OmniVICæœç€å¼¥åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§é¡ºåº”æ§åˆ¶çš„æ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œä½¿æ“ä½œæ›´å®‰å…¨ã€æ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒæˆåŠŸç‡ä»åŸºå‡†çº¿çš„27%æé«˜åˆ°OmniVICçš„61.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17150v1">PDF</a> Code, video and RAG dataset are available at   \url{<a target="_blank" rel="noopener" href="https://sites.google.com/view/omni-vic%7D">https://sites.google.com/view/omni-vic}</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>OmniVICæ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¢å¼ºçš„é€šç”¨å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œæ—¨åœ¨æé«˜æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ï¼Œå¢å¼ºå®‰å…¨ç‰©ç†äº¤äº’ã€‚OmniVICç»“åˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€çš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œç”Ÿæˆé€‚åº”æ€§çš„é˜»æŠ—å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ¥è§¦å¼ä»»åŠ¡çš„æœºå™¨äººã€‚å…¶æ ¸å¿ƒæ˜¯è‡ªæ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚RAGä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆéªŒç»éªŒï¼Œä¸ºæ§åˆ¶å™¨æä¾›ç±»ä¼¼è¿‡å»ä»»åŠ¡çš„ä¿¡æ¯ï¼›è€ŒICLåˆ™åˆ©ç”¨è¿™äº›æ£€ç´¢åˆ°çš„ç¤ºä¾‹å’Œå½“å‰ä»»åŠ¡çš„æç¤ºï¼ŒæŸ¥è¯¢VLMä»¥ç”Ÿæˆå½“å‰æ“ä½œåœºæ™¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œé€‚åº”æ€§é˜»æŠ—å‚æ•°ã€‚æ­¤å¤–ï¼Œé˜»æŠ—å‚æ•°è°ƒèŠ‚è¿˜å—åˆ°å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆçš„å¯å‘ï¼Œä»¥ç¡®ä¿äº¤äº’åŠ›ä¿æŒåœ¨å®‰å…¨é˜ˆå€¼å†…ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡çš„ä¸€ç³»åˆ—å¤æ‚æ¥è§¦å¼ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºä¼˜äºåŸºå‡†çº¿çš„æ€§èƒ½ï¼ŒæˆåŠŸç‡æé«˜ï¼ŒåŠ›è¿è§„å‡å°‘ã€‚OmniVICæœç€å¼¥åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§é¡ºåº”æ§åˆ¶ä¹‹é—´çš„é¸¿æ²Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä½¿æœºå™¨äººæ“ä½œæ›´å®‰å…¨ã€æ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒæˆåŠŸç‡ä»åŸºå‡†çš„27%æé«˜åˆ°OmniVICçš„61.4%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OmniVICæ˜¯ä¸€ä¸ªç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é€šç”¨å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œæ—¨åœ¨æé«˜æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>OmniVICé€šè¿‡ç»“åˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆé€‚åº”æ€§çš„é˜»æŠ—å‚æ•°ã€‚</li>
<li>æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬è‡ªæ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚</li>
<li>RAGä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆéªŒç»éªŒï¼Œä¸ºæ§åˆ¶å™¨æä¾›ç±»ä¼¼è¿‡å»ä»»åŠ¡çš„ä¿¡æ¯ã€‚</li>
<li>ICLåˆ©ç”¨æ£€ç´¢åˆ°çš„ç¤ºä¾‹å’Œå½“å‰ä»»åŠ¡æç¤ºæ¥æŸ¥è¯¢VLMï¼Œç”Ÿæˆé€‚åº”å½“å‰æ“ä½œåœºæ™¯çš„é˜»æŠ—å‚æ•°ã€‚</li>
<li>OmniVICå—åˆ°å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆçš„å¯å‘æ¥è¿›è¡Œé˜»æŠ—å‚æ•°è°ƒèŠ‚ï¼Œç¡®ä¿äº¤äº’åŠ›ä¿æŒåœ¨å®‰å…¨èŒƒå›´å†…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9fae0cbad7271277fa8d8acb10d111b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074960&auth_key=1761074960-0-0-476698b1d7c0969905d2d86f1a868b02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cfdc497754b4fd1f1156549706affe72~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074967&auth_key=1761074967-0-0-cd8affa18ecf064d0da9d2d91317dbe6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49ae2bf6b89b10774e65cf1bf2ad9310~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074974&auth_key=1761074974-0-0-847f23088a623b40c3a18e20483cfbeb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40df29ef8cc1c82011b2ff966f663598~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074981&auth_key=1761074981-0-0-37b9ccb018b109bf74aaa4afa75f507d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05e457b9ac2daf846498483d7ea15d68~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074987&auth_key=1761074987-0-0-2d68dd5b84f6621d5cb142d7b8164b36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-RGB-Leveraging-Vision-Transformers-for-Thermal-Weapon-Segmentation"><a href="#Beyond-RGB-Leveraging-Vision-Transformers-for-Thermal-Weapon-Segmentation" class="headerlink" title="Beyond RGB: Leveraging Vision Transformers for Thermal Weapon   Segmentation"></a>Beyond RGB: Leveraging Vision Transformers for Thermal Weapon   Segmentation</h2><p><strong>Authors:Akhila Kambhatla, Ahmed R Khaled</strong></p>
<p>Thermal weapon segmentation is crucial for surveillance and security applications, enabling robust detection under lowlight and visually obscured conditions where RGB-based systems fail. While convolutional neural networks (CNNs) dominate thermal segmentation literature, their ability to capture long-range dependencies and fine structural details is limited. Vision Transformers (ViTs), with their global context modeling capabilities, have achieved state-of-the-art results in RGB segmentation tasks, yet their potential in thermal weapon segmentation remains underexplored. This work adapts and evaluates four transformer-based architectures SegFormer, DeepLabV3+, SegNeXt, and Swin Transformer for binary weapon segmentation on a custom thermal dataset comprising 9,711 images collected from real world surveillance videos and automatically annotated using SAM2. We employ standard augmentation strategies within the MMSegmentation framework to ensure robust model training and fair architectural comparison. Experimental results demonstrate significant improvements in segmentation performance: SegFormer-b5 achieves the highest mIoU (94.15%) and Pixel Accuracy (97.04%), while SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive mIoU (90.84%). SegNeXt-mscans offers balanced performance with 85.12 FPS and 92.24% mIoU, and DeepLabV3+ R101-D8 reaches 92.76% mIoU at 29.86 FPS. The transformer architectures demonstrate robust generalization capabilities for weapon detection in low-light and occluded thermal environments, with flexible accuracy-speed trade-offs suitable for diverse real-time security applications. </p>
<blockquote>
<p>çƒ­æ­¦å™¨åˆ†å‰²å¯¹äºç›‘æ§å’Œå®‰å…¨åº”ç”¨è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿåœ¨ä½å…‰å’Œè§†è§‰é®è”½æ¡ä»¶ä¸‹å®ç°ç¨³å¥æ£€æµ‹ï¼Œè€ŒåŸºäºRGBçš„ç³»ç»Ÿåˆ™æ— æ³•åšåˆ°ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨çƒ­åˆ†å‰²é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å…¶æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œç²¾ç»†ç»“æ„ç»†èŠ‚çš„èƒ½åŠ›æœ‰é™ã€‚è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å‡­å€Ÿå…¶å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨RGBåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼Œä½†å…¶åœ¨çƒ­æ­¦å™¨åˆ†å‰²ä¸­çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é¡¹å·¥ä½œé€‚åº”äº†å››ç§åŸºäºå˜å‹å™¨çš„æ¶æ„SegFormerã€DeepLabV3+ã€SegNeXtå’ŒSwin Transformerï¼Œç”¨äºè‡ªå®šä¹‰çƒ­æ•°æ®é›†ä¸Šçš„äºŒè¿›æ­¦å™¨åˆ†å‰²ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»ç°å®ä¸–ç•Œç›‘æ§è§†é¢‘ä¸­æ”¶é›†çš„9711å¼ å›¾åƒï¼Œå¹¶ä½¿ç”¨SAM2è‡ªåŠ¨æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨MMSegmentationæ¡†æ¶å†…é‡‡ç”¨äº†æ ‡å‡†å¢å¼ºç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å¥è®­ç»ƒå’Œå…¬å¹³çš„æ¶æ„æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†å‰²æ€§èƒ½å¾—åˆ°æ˜¾ç€æé«˜ï¼šSegFormer-b5è¾¾åˆ°æœ€é«˜mIoUï¼ˆ94.15%ï¼‰å’Œåƒç´ å‡†ç¡®ç‡ï¼ˆ97.04%ï¼‰ï¼Œè€ŒSegFormer-b0æä¾›æœ€å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆ98.32 FPSï¼‰ï¼Œå…·æœ‰ç«äº‰åŠ›çš„mIoUï¼ˆ90.84%ï¼‰ã€‚SegNeXt-mscansæä¾›å‡è¡¡çš„æ€§èƒ½ï¼Œå…·æœ‰85.12 FPSå’Œ92.24%çš„mIoUï¼Œè€ŒDeepLabV3+ R101-D8è¾¾åˆ°92.76%çš„mIoUï¼Œå¸§é€Ÿç‡ä¸ºæ¯ç§’29.86å¸§ã€‚è¿™äº›åŸºäºå˜å‹å™¨çš„æ¶æ„æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºä½å…‰å’Œé®æŒ¡çš„çƒ­ç¯å¢ƒä¸­æ­¦å™¨æ£€æµ‹çš„å¤šæ ·åŒ–å®æ—¶å®‰å…¨åº”ç”¨ï¼Œå…·æœ‰çµæ´»çš„ç²¾åº¦é€Ÿåº¦æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16913v1">PDF</a> 9 Images with 1 figure and 3 Tables. This is a preprint submitted to   arXiv</p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ç ”ç©¶äº†çƒ­æˆåƒæ­¦å™¨åˆ†å‰²åœ¨ç›‘æ§å’Œå®‰å…¨åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œé’ˆå¯¹ä½å…‰ç…§å’Œé®è”½æ¡ä»¶ä¸‹çš„æ­¦å™¨æ£€æµ‹ï¼Œè¯„ä¼°äº†å››ç§åŸºäºVision Transformerçš„æ¶æ„ï¼ˆSegFormerã€DeepLabV3+ã€SegNeXtå’ŒSwin Transformerï¼‰åœ¨è‡ªå®šä¹‰çƒ­æˆåƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›Transformeræ¶æ„åœ¨çƒ­æ­¦å™¨åˆ†å‰²æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®é€Ÿåº¦æŠ˜è¡·ï¼Œé€‚ç”¨äºå®æ—¶å®‰å…¨åº”ç”¨ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çƒ­æ­¦å™¨åˆ†å‰²å¯¹äºç›‘æ§å’Œå®‰å…¨åº”ç”¨è‡³å…³é‡è¦ï¼Œåœ¨ä½å…‰ç…§å’Œè§†è§‰é®è”½æ¡ä»¶ä¸‹ï¼ŒRGB-basedç³»ç»Ÿå¤±æ•ˆæ—¶ä»èƒ½è¿›è¡Œç¨³å¥æ£€æµ‹ã€‚</li>
<li>è™½ç„¶CNNåœ¨çƒ­åˆ†å‰²æ–‡çŒ®ä¸­å ä¸»å¯¼åœ°ä½ï¼Œä½†å…¶åœ¨æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œç²¾ç»†ç»“æ„ç»†èŠ‚æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>Vision Transformersï¼ˆViTsï¼‰å‡­å€Ÿå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œå·²åœ¨RGBåˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—æœ€ä½³ç»“æœã€‚</li>
<li>æœ¬æ–‡è¯„ä¼°äº†å››ç§Transformeræ¶æ„ï¼ˆSegFormerã€DeepLabV3+ã€SegNeXtå’ŒSwin Transformerï¼‰åœ¨è‡ªå®šä¹‰çƒ­æˆåƒæ•°æ®é›†ä¸Šçš„çƒ­æ­¦å™¨åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>SegFormer-b5è·å¾—æœ€é«˜mIoUï¼ˆ94.15%ï¼‰å’Œåƒç´ å‡†ç¡®ç‡ï¼ˆ97.04%ï¼‰ï¼Œè€ŒSegFormer-b0æä¾›æœ€å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆ98.32 FPSï¼‰å¹¶è¡¨ç°å‡ºç«äº‰åŠ›mIoUï¼ˆ90.84%ï¼‰ã€‚</li>
<li>SegNeXt-msansæä¾›äº†å¹³è¡¡çš„ç»©æ•ˆï¼Œå…·æœ‰85.12 FPSå’Œ92.24% mIoUï¼Œè€ŒDeepLabV3+ R101-D8è¾¾åˆ°äº†92.76% mIoUã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1769b1d3ce28da01ff85b2d0438f3316~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074995&auth_key=1761074995-0-0-d699fb3afa23f1298527c4eda8f816c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cd49be9837063c2576dcd93a0f8b544~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075002&auth_key=1761075002-0-0-523a49f815281b4002d09954008307b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4deee22e73f1e5b4524a8cd467d8c0db~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075010&auth_key=1761075010-0-0-ffe08046cf0f67e3e6e74141f2772b43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ELIP-Enhanced-Visual-Language-Foundation-Models-for-Image-Retrieval"><a href="#ELIP-Enhanced-Visual-Language-Foundation-Models-for-Image-Retrieval" class="headerlink" title="ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval"></a>ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval</h2><p><strong>Authors:Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman</strong></p>
<p>The objective in this paper is to improve the performance of text-to-image retrieval. To this end, we introduce a new framework that can boost the performance of large-scale pre-trained vision-language models, so that they can be used for text-to-image re-ranking. The approach, Enhanced Language-Image Pre-training (ELIP), uses the text query, via a simple MLP mapping network, to predict a set of visual prompts to condition the ViT image encoding. ELIP can easily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. To train the architecture with limited computing resources, we develop a â€˜student friendlyâ€™ best practice, involving global hard sample mining, and curation of a large-scale dataset. On the evaluation side, we set up two new out-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assess the zero-shot generalisation of the models to different domains. The results demonstrate that ELIP significantly boosts CLIP&#x2F;SigLIP&#x2F;SigLIP-2 text-to-image retrieval performance and outperforms BLIP-2 on several benchmarks, as well as providing an easy means to adapt to OOD datasets. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯æé«˜æ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æå‡å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶å¯ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„é‡æ–°æ’åºã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºå¢å¼ºè¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆELIPï¼‰ã€‚ELIPé€šè¿‡ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢å’Œç®€å•çš„MLPæ˜ å°„ç½‘ç»œæ¥é¢„æµ‹ä¸€ç³»åˆ—è§†è§‰æç¤ºï¼Œä»¥è°ƒèŠ‚ViTå›¾åƒç¼–ç ã€‚ELIPå¯ä»¥è½»æ¾åº”ç”¨äºå¸¸ç”¨çš„CLIPã€SigLIPå’ŒBLIP-2ç½‘ç»œã€‚ä¸ºäº†ä½¿ç”¨æœ‰é™çš„è®¡ç®—èµ„æºè¿›è¡Œæ¶æ„è®­ç»ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§â€œå­¦ç”Ÿå‹å¥½å‹â€çš„æœ€ä½³å®è·µæ–¹æ³•ï¼ŒåŒ…æ‹¬å…¨å±€ç¡¬æ ·æœ¬æŒ–æ˜å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„åˆ¶ä½œã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªæ–°çš„è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰åŸºå‡†æµ‹è¯•ï¼Œå³è¢«é®æŒ¡çš„COCOå’ŒImageNet-Rï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒELIPæ˜¾è‘—æé«˜äº†CLIP&#x2F;SigLIP&#x2F;SigLIP-2çš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºBLIP-2ï¼ŒåŒæ—¶æä¾›äº†ä¸€ç§é€‚åº”OODæ•°æ®é›†çš„ç®€å•æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15682v3">PDF</a> Accepted by CBMI 2025 (IEEE International Conference on Content-Based   Multimedia Indexing)</p>
<p><strong>Summary</strong>:<br>æœ¬æ–‡æ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ç§æ–°æ¡†æ¶ELIPï¼Œèƒ½å¤Ÿå¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„é‡æ–°æ’åºã€‚ELIPé€šè¿‡ç®€å•çš„MLPæ˜ å°„ç½‘ç»œä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢æ¥é¢„æµ‹ä¸€ç³»åˆ—è§†è§‰æç¤ºï¼Œä»¥è°ƒèŠ‚ViTå›¾åƒç¼–ç ã€‚å¯åœ¨CLIPã€SigLIPå’ŒBLIP-2ç½‘ç»œä¸­ä½¿ç”¨ã€‚ä¸ºåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹è®­ç»ƒæ¶æ„ï¼Œæˆ‘ä»¬åˆ¶å®šäº†â€œå­¦ç”Ÿå‹å¥½â€çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬å…¨å±€ç¡¬æ ·æœ¬æŒ–æ˜å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„åˆ¶ä½œã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªæ–°çš„åŸŸå¤–åˆ†å¸ƒï¼ˆOODï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é®æŒ¡COCOå’ŒImageNet-Rï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼ŒELIPåœ¨CLIP&#x2F;SigLIP&#x2F;SigLIP-2çš„æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶åœ¨å‡ ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºBLIP-2ï¼Œä¸”æ˜“äºé€‚åº”OODæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼•å…¥æ–°æ¡†æ¶ELIPä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ã€‚</li>
<li>ELIPæ¡†æ¶èƒ½å¤Ÿå¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„é‡æ–°æ’åºã€‚</li>
<li>ELIPé€šè¿‡MLPæ˜ å°„ç½‘ç»œä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢é¢„æµ‹è§†è§‰æç¤ºï¼Œä»¥è°ƒèŠ‚å›¾åƒç¼–ç ã€‚</li>
<li>ELIPå¯è½»æ¾åº”ç”¨äºCLIPã€SigLIPå’ŒBLIP-2ç½‘ç»œã€‚</li>
<li>é‡‡ç”¨å…¨å±€ç¡¬æ ·æœ¬æŒ–æ˜å’Œå¤§è§„æ¨¡æ•°æ®é›†åˆ¶ä½œçš„æœ€ä½³å®è·µæ¥è®­ç»ƒæ¶æ„ã€‚</li>
<li>å»ºç«‹äº†ä¸¤ä¸ªæ–°çš„OODåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ELIPæ˜¾è‘—æé«˜æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºBLIP-2ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-30b98c7c70eebcf0212117a0c1411db1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075017&auth_key=1761075017-0-0-d470daebec5539e5e9f2bfbda18d328f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0da30e8e101df989ab7aec16b92bfcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075025&auth_key=1761075025-0-0-5ba3e4f0f895ddaeaebeeb29f34dbc42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be158f9bee8722123eb109de9e24501c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075031&auth_key=1761075031-0-0-993609a0970fcb573c1fa98b39df8a22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d5c5a9bda21b76267d50b5a7fce1bf0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075038&auth_key=1761075038-0-0-470db55e3890275efcca77bf87521e42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39db947e8eab6a03b6b94b6c8173993d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075045&auth_key=1761075045-0-0-9ac1f9ec20ca1dd55bbb4c2639318f16&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a5b3bcf5dcac60028b6790d3b3f459b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075052&auth_key=1761075052-0-0-89e6f2ddcbee9d795fb01745e82b4659&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Free-2-Guide-Training-Free-Text-to-Video-Alignment-using-Image-LVLM"><a href="#Free-2-Guide-Training-Free-Text-to-Video-Alignment-using-Image-LVLM" class="headerlink" title="Free$^2$Guide: Training-Free Text-to-Video Alignment using Image LVLM"></a>Free$^2$Guide: Training-Free Text-to-Video Alignment using Image LVLM</h2><p><strong>Authors:Jaemin Kim, Bryan Sangwoo Kim, Jong Chul Ye</strong></p>
<p>Diffusion models have achieved impressive results in generative tasks for text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependencies across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions trained for videos, hindering their scalability and applicability. In this paper, we propose \textbf{Free$^2$Guide}, a novel gradient-free and training-free framework for aligning generated videos with text prompts. Specifically, leveraging principles from path integral control, Free$^2$Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward models. To enable image-trained LVLMs to assess text-to-video alignment, we leverage \textit{stitching} between video frames and use system prompts to capture sequential attributions. Our framework supports the flexible ensembling of multiple reward models to synergistically enhance alignment without significant computational overhead. Experimental results confirm that Free$^2$Guide using image-trained LVLMs significantly improves text-to-video alignment, thereby enhancing the overall video quality. Our results and code are available at <a target="_blank" rel="noopener" href="https://kjm981995.github.io/free2guide/">https://kjm981995.github.io/free2guide/</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰åˆæˆçš„ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºå¸§ä¹‹é—´çš„å¤æ‚æ—¶é—´ä¾èµ–æ€§ï¼ŒT2Vç”Ÿæˆä¸­å®ç°æ–‡æœ¬å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•æ¥æé«˜æ–‡æœ¬å¯¹é½é€šå¸¸éœ€è¦å¯¹è§†é¢‘è¿›è¡Œå¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°è®­ç»ƒï¼Œè¿™é˜»ç¢äº†å…¶å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Free$^2$Guideï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ— éœ€æ¢¯åº¦å’Œæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºå°†ç”Ÿæˆçš„è§†é¢‘ä¸æ–‡æœ¬æç¤ºå¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œå€ŸåŠ©è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†ï¼ŒFree$^2$Guideä½¿ç”¨ä¸å¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°æ¥è¿‘ä¼¼æ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼ï¼Œä»è€Œèƒ½å¤Ÿæ•´åˆå¼ºå¤§çš„é»‘ç®±å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹ã€‚ä¸ºäº†å…è®¸å›¾åƒè®­ç»ƒçš„LVLMsè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘çš„å¯¹é½ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘å¸§ä¹‹é—´çš„â€œæ‹¼æ¥â€å’Œç³»ç»Ÿæç¤ºæ¥æ•è·åºåˆ—å½’å±ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„çµæ´»é›†æˆï¼Œä»¥ååŒå¢å¼ºå¯¹é½åº¦ï¼Œè€Œä¸ä¼šé€ æˆé‡å¤§è®¡ç®—è´Ÿæ‹…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å›¾åƒè®­ç»ƒçš„LVLMsçš„Free$^2$Guideæ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°è§†é¢‘çš„å¯¹é½ï¼Œä»è€Œæé«˜äº†æ•´ä½“è§†é¢‘è´¨é‡ã€‚æˆ‘ä»¬çš„ç»“æœå’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://kjm981995.github.io/free2guide/">https://kjm981995.github.io/free2guide/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17041v2">PDF</a> ICCV 2025 accepted</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬ä»‹ç»äº†Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æˆæœï¼Œä½†å‡†ç¡®å¯¹é½æ–‡æœ¬ä»æ˜¯æŒ‘æˆ˜ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦ä¸ºè§†é¢‘è®­ç»ƒå¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºFree$^2$Guideæ¡†æ¶ï¼Œæ— éœ€æ¢¯åº¦å’Œè®­ç»ƒï¼Œä½¿ç”¨éå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œå¹¶åˆ©ç”¨è·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†å®ç°ã€‚é€šè¿‡è§†é¢‘å¸§çš„æ‹¼æ¥å’Œç³»ç»Ÿæç¤ºæ¥è¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘çš„å¯¹é½æƒ…å†µã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„çµæ´»é›†æˆï¼ŒååŒå¢å¼ºå¯¹é½æ•ˆæœï¼Œä¸”è®¡ç®—å¼€é”€ä¸å¤§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å›¾åƒè®­ç»ƒçš„LVLMsçš„Free$^2$Guideæ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°è§†é¢‘çš„å¯¹é½æ•ˆæœï¼Œå¢å¼ºäº†è§†é¢‘æ•´ä½“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†æ–‡æœ¬å¯¹é½ä»æ˜¯éš¾é¢˜ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•éœ€è¦å¯å¾®åˆ†çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè§†é¢‘è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>Free$^2$Guideæ¡†æ¶é‡‡ç”¨éå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼ŒåŸºäºè·¯å¾„ç§¯åˆ†æ§åˆ¶åŸç†å®ç°æ–‡æœ¬ä¸è§†é¢‘å¯¹é½ã€‚</li>
<li>é€šè¿‡è§†é¢‘å¸§æ‹¼æ¥å’Œç³»ç»Ÿæç¤ºè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘çš„å¯¹é½æƒ…å†µã€‚</li>
<li>Free$^2$Guideæ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„é›†æˆï¼Œèƒ½ååŒå¢å¼ºå¯¹é½æ•ˆæœï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œä½¿ç”¨å›¾åƒè®­ç»ƒçš„LVLMsçš„Free$^2$Guideåœ¨æ–‡æœ¬åˆ°è§†é¢‘å¯¹é½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ab2f6649fef76eb8ea7903ae66ef03b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075059&auth_key=1761075059-0-0-bbc89fd43ebcb7cb12c955a63cb83193&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fb46850a8e40a84930c927169fee859~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075067&auth_key=1761075067-0-0-eaf9ad66cb6b2157475077d1da839f9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99d8e46a1367b9c63b7ddf6e40593c5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075074&auth_key=1761075074-0-0-f3e8a6bb4dced517da2e3d9a1ebd0bda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d958c3f63b321633c8eb56475ad5393a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761075675&auth_key=1761075675-0-0-d64388cc356206c2334bbe5e1d52c558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Expose Camouflage in the Water Underwater Camouflaged Instance   Segmentation and Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-8d68d20643cf9a6832863fb3062a2d9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761074163&auth_key=1761074163-0-0-2bdf8c76834d7504846c4cec6225fb48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  MT-Video-Bench A Holistic Video Understanding Benchmark for Evaluating   Multimodal LLMs in Multi-Turn Dialogues
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
