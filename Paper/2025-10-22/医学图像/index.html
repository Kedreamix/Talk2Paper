<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Towards Explainable Skin Cancer Classification A Dual-Network Attention   Model with Lesion Segmentation and Clinical Metadata Fusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-18ba0cb389248a5137569658961fd2ab')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-22-æ›´æ–°"><a href="#2025-10-22-æ›´æ–°" class="headerlink" title="2025-10-22 æ›´æ–°"></a>2025-10-22 æ›´æ–°</h1><h2 id="Towards-Explainable-Skin-Cancer-Classification-A-Dual-Network-Attention-Model-with-Lesion-Segmentation-and-Clinical-Metadata-Fusion"><a href="#Towards-Explainable-Skin-Cancer-Classification-A-Dual-Network-Attention-Model-with-Lesion-Segmentation-and-Clinical-Metadata-Fusion" class="headerlink" title="Towards Explainable Skin Cancer Classification: A Dual-Network Attention   Model with Lesion Segmentation and Clinical Metadata Fusion"></a>Towards Explainable Skin Cancer Classification: A Dual-Network Attention   Model with Lesion Segmentation and Clinical Metadata Fusion</h2><p><strong>Authors:Md. Enamul Atiq, Shaikh Anowarul Fattah</strong></p>
<p>Skin cancer is a life-threatening disease where early detection significantly improves patient outcomes. Automated diagnosis from dermoscopic images is challenging due to high intra-class variability and subtle inter-class differences. Many deep learning models operate as â€œblack boxes,â€ limiting clinical trust. In this work, we propose a dual-encoder attention-based framework that leverages both segmented lesions and clinical metadata to enhance skin lesion classification in terms of both accuracy and interpretability. A novel Deep-UNet architecture with Dual Attention Gates (DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment lesions. The classification stage uses two DenseNet201 encoders-one on the original image and another on the segmented lesion whose features are fused via multi-head cross-attention. This dual-input design guides the model to focus on salient pathological regions. In addition, a transformer-based module incorporates patient metadata (age, sex, lesion site) into the prediction. We evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019 challenges. The proposed method achieves state-of-the-art segmentation performance and significantly improves classification accuracy and average AUC compared to baseline models. To validate our modelâ€™s reliability, we use Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps. These visualizations confirm that our modelâ€™s predictions are based on the lesion area, unlike models that rely on spurious background features. These results demonstrate that integrating precise lesion segmentation and clinical data with attention-based fusion leads to a more accurate and interpretable skin cancer classification model. </p>
<blockquote>
<p>çš®è‚¤ç™Œæ˜¯ä¸€ç§å¨èƒç”Ÿå‘½çš„ç–¾ç—…ï¼Œæ—©æœŸå‘ç°èƒ½æ˜¾è‘—æ”¹å–„æ‚£è€…é¢„åã€‚ç”±äºç±»å†…å˜åŒ–é«˜åº¦å¤šæ ·å’Œç±»é—´å·®å¼‚ç»†å¾®ï¼Œä½¿å¾—ä»çš®è‚¤é•œå›¾åƒä¸­è¿›è¡Œè‡ªåŠ¨åŒ–è¯Šæ–­å……æ»¡æŒ‘æˆ˜ã€‚è®¸å¤šæ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚åŒâ€œé»‘ç®±â€ï¼Œé™åˆ¶äº†ä¸´åºŠä¿¡ä»»åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒç¼–ç å™¨æ³¨æ„åŠ›çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åˆ†å‰²çš„ç—…å˜å’Œä¸´åºŠå…ƒæ•°æ®æ¥æé«˜çš®è‚¤ç—…å˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚é¦–å…ˆï¼Œé‡‡ç”¨å…·æœ‰åŒæ³¨æ„åŠ›é—¨ï¼ˆDAGï¼‰å’Œå¤§å­”ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆASPPï¼‰çš„æ–°å‹Deep-UNetæ¶æ„è¿›è¡Œç—…å˜åˆ†å‰²ã€‚åˆ†ç±»é˜¶æ®µä½¿ç”¨ä¸¤ä¸ªDenseNet201ç¼–ç å™¨â€”â€”ä¸€ä¸ªç”¨äºåŸå§‹å›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºåˆ†å‰²çš„ç—…å˜ï¼Œå…¶ç‰¹å¾é€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›èåˆã€‚è¿™ç§åŒè¾“å…¥è®¾è®¡å¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—çš„ç—…ç†åŒºåŸŸã€‚æ­¤å¤–ï¼Œä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ¨¡å—å°†æ‚£è€…å…ƒæ•°æ®ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€ç—…å˜éƒ¨ä½ï¼‰çº³å…¥é¢„æµ‹ä¸­ã€‚æˆ‘ä»¬åœ¨HAM10000æ•°æ®é›†ä»¥åŠISIC 2018å’Œ2019æŒ‘æˆ˜èµ›ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åˆ†ç±»ç²¾åº¦å’Œå¹³å‡AUCã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¨¡å‹çš„å¯é æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰ç”Ÿæˆçƒ­å›¾ã€‚è¿™äº›å¯è§†åŒ–ç¡®è®¤äº†æˆ‘ä»¬æ¨¡å‹çš„é¢„æµ‹æ˜¯åŸºäºç—…å˜åŒºåŸŸçš„ï¼Œä¸åŒäºé‚£äº›ä¾èµ–äºè™šå‡èƒŒæ™¯ç‰¹å¾çš„æ¨¡å‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†ç²¾ç¡®çš„ç—…å˜åˆ†å‰²å’Œä¸´åºŠæ•°æ®ä¸åŸºäºæ³¨æ„åŠ›çš„èåˆç›¸ç»“åˆï¼Œå¯ä»¥æ„å»ºå‡ºæ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„çš®è‚¤ç™Œåˆ†ç±»æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17773v1">PDF</a> 15 pages, 7 Figures, 3 Tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒç¼–ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„æ¡†æ¶ï¼Œç»“åˆåˆ†å‰²ç—…ç¶å’Œä¸´åºŠå…ƒæ•°æ®ï¼Œä»¥æé«˜çš®è‚¤ç—…å˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚é‡‡ç”¨å¸¦æœ‰åŒæ³¨æ„åŠ›é—¨ï¼ˆDAGï¼‰å’Œé˜¿ç‰¹ç½—ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆASPPï¼‰çš„æ–°å‹Deep-UNetæ¶æ„è¿›è¡Œç—…ç¶åˆ†å‰²ã€‚åˆ†ç±»é˜¶æ®µä½¿ç”¨ä¸¤ä¸ªDenseNet201ç¼–ç å™¨ï¼Œä¸€ä¸ªå¤„ç†åŸå§‹å›¾åƒï¼Œå¦ä¸€ä¸ªå¤„ç†åˆ†å‰²åçš„ç—…ç¶ï¼Œé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›èåˆç‰¹å¾ã€‚è¿™ç§åŒè¾“å…¥è®¾è®¡å¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—çš„ç—…ç†åŒºåŸŸã€‚æ­¤å¤–ï¼Œä¸€ä¸ªåŸºäºå˜å‹å™¨æ¨¡å—çš„æ‚£è€…å…ƒæ•°æ®ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€ç—…ç¶éƒ¨ä½ï¼‰è¢«çº³å…¥é¢„æµ‹ä¸­ã€‚åœ¨HAM10000æ•°æ®é›†å’ŒISIC 2018åŠ2019æŒ‘æˆ˜èµ›ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•å®ç°äº†å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†åˆ†ç±»å‡†ç¡®ç‡å’Œå¹³å‡AUCå€¼ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ã€‚é€šè¿‡æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰éªŒè¯äº†æˆ‘ä»¬æ¨¡å‹çš„å¯é æ€§ï¼Œç”Ÿæˆçš„çƒ­å›¾ç¡®è®¤æˆ‘ä»¬çš„é¢„æµ‹æ˜¯åŸºäºç—…ç¶åŒºåŸŸï¼Œä¸åŒäºä¾èµ–èƒŒæ™¯ç‰¹å¾çš„æ¨¡å‹ã€‚ç»“æœè¯æ˜ï¼Œå°†ç²¾ç¡®ç—…ç¶åˆ†å‰²å’Œä¸´åºŠæ•°æ®ä¸æ³¨æ„åŠ›èåˆç›¸ç»“åˆï¼Œå¯æ„å»ºæ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„çš®è‚¤ç™Œåˆ†ç±»æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çš®è‚¤ç™Œæ—©æœŸæ£€æµ‹å¯¹æ‚£è€…é¢„åè‡³å…³é‡è¦ï¼Œè€Œè‡ªåŠ¨åŒ–è¯Šæ–­é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŒç¼–ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„æ¡†æ¶ï¼Œèåˆåˆ†å‰²ç—…ç¶å’Œä¸´åºŠå…ƒæ•°æ®ï¼Œä»¥æé«˜çš®è‚¤ç—…å˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å¼•å…¥æ–°å‹Deep-UNetæ¶æ„è¿›è¡Œç²¾ç¡®ç—…ç¶åˆ†å‰²ã€‚</li>
<li>åˆ†ç±»é˜¶æ®µé‡‡ç”¨åŒè¾“å…¥è®¾è®¡ï¼Œé€šè¿‡å¤šå¤´äº¤å‰æ³¨æ„åŠ›èåˆç‰¹å¾ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—ç—…ç†åŒºåŸŸã€‚</li>
<li>é¦–æ¬¡å°†æ‚£è€…å…ƒæ•°æ®çº³å…¥é¢„æµ‹ä¸­ï¼Œå¢å¼ºæ¨¡å‹çš„å…¨é¢æ€§å’Œå¯é æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†æ‰€ææ–¹æ³•çš„é«˜åº¦å‡†ç¡®æ€§å’Œå…ˆè¿›æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ee421b73a44cdbbe7afbe6656de146c" align="middle">
<img src="https://picx.zhimg.com/v2-4a544427e737cb6b1c5f24dfca7ca87a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Cross-Patient-Generalization-in-Parkinsonâ€™s-Disease-Detection-through-Chunk-Based-Analysis-of-Hand-Drawn-Patterns"><a href="#Improving-Cross-Patient-Generalization-in-Parkinsonâ€™s-Disease-Detection-through-Chunk-Based-Analysis-of-Hand-Drawn-Patterns" class="headerlink" title="Improving Cross-Patient Generalization in Parkinsonâ€™s Disease Detection   through Chunk-Based Analysis of Hand-Drawn Patterns"></a>Improving Cross-Patient Generalization in Parkinsonâ€™s Disease Detection   through Chunk-Based Analysis of Hand-Drawn Patterns</h2><p><strong>Authors:Mhd Adnan Albani, Riad Sonbol</strong></p>
<p>Parkinsonâ€™s disease (PD) is a neurodegenerative disease affecting about 1% of people over the age of 60, causing motor impairments that impede hand coordination activities such as writing and drawing. Many approaches have tried to support early detection of Parkinsonâ€™s disease based on hand-drawn images; however, we identified two major limitations in the related works: (1) the lack of sufficient datasets, (2) the robustness when dealing with unseen patient data. In this paper, we propose a new approach to detect Parkinsonâ€™s disease that consists of two stages: The first stage classifies based on their drawing type(circle, meander, spiral), and the second stage extracts the required features from the images and detects Parkinsonâ€™s disease. We overcame the previous two limitations by applying a chunking strategy where we divide each image into 2x2 chunks. Each chunk is processed separately when extracting features and recognizing Parkinsonâ€™s disease indicators. To make the final classification, an ensemble method is used to merge the decisions made from each chunk. Our evaluation shows that our proposed approach outperforms the top performing state-of-the-art approaches, in particular on unseen patients. On the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen patients and 94.91% for unseen patients, our proposed approach maintained a gap of only 2.17 percentage points, compared to the 4.76-point drop observed in prior work. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ˜¯ä¸€ç§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œå½±å“çº¦60å²ä»¥ä¸Šäººå£çš„1%ï¼Œå¯¼è‡´è¿åŠ¨éšœç¢ï¼Œå½±å“æ‰‹å†™å’Œç»˜å›¾ç­‰æ‰‹éƒ¨åè°ƒæ´»åŠ¨ã€‚è®¸å¤šæ–¹æ³•éƒ½è¯•å›¾åŸºäºæ‰‹ç»˜å›¾åƒè¿›è¡Œå¸•é‡‘æ£®ç—…çš„æ—©æœŸæ£€æµ‹ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨ç›¸å…³å·¥ä½œä¸­å‘ç°äº†ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ•°æ®é›†ä¸è¶³ï¼Œï¼ˆ2ï¼‰åœ¨å¤„ç†æœªè§æ‚£è€…æ•°æ®æ—¶çš„ç¨³å¥æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¸•é‡‘æ£®ç—…æ£€æµ‹æ–¹æ³•ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ ¹æ®ç»˜ç”»ç±»å‹ï¼ˆåœ†å½¢ã€è›‡å½¢ã€èºæ—‹å½¢ï¼‰è¿›è¡Œåˆ†ç±»ï¼Œç¬¬äºŒé˜¶æ®µä»å›¾åƒä¸­æå–æ‰€éœ€ç‰¹å¾å¹¶è¿›è¡Œå¸•é‡‘æ£®ç—…æ£€æµ‹ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨åˆ†å—ç­–ç•¥å…‹æœäº†å‰ä¸¤ä¸ªå±€é™æ€§ï¼Œå°†æ¯å¼ å›¾åƒåˆ†æˆ2x2çš„å—ã€‚åœ¨æå–ç‰¹å¾å’Œè¯†åˆ«å¸•é‡‘æ£®ç—…æŒ‡æ ‡æ—¶ï¼Œæ¯ä¸ªå—ä¼šå•ç‹¬å¤„ç†ã€‚ä¸ºäº†åšå‡ºæœ€ç»ˆåˆ†ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨é›†æˆæ–¹æ³•èåˆäº†æ¯ä¸ªå—çš„å†³ç­–ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æœªè§æ‚£è€…æ–¹é¢è¡¨ç°å‡ºä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯çš„æ•ˆæœã€‚åœ¨æ–°HandPDæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ºå·²çŸ¥æ‚£è€…å’ŒæœªçŸ¥æ‚£è€…çš„æ£€æµ‹æ–¹é¢åˆ†åˆ«è¾¾åˆ°äº†97.08%å’Œ94.91%çš„å‡†ç¡®ç‡ã€‚ä¸ä¹‹å‰å·¥ä½œä¸­è§‚å¯Ÿåˆ°çš„4.76%çš„ä¸‹é™ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä¿æŒäº†2.17%çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17703v1">PDF</a> 19 pages, 2 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¸•é‡‘æ£®ç—…æ—©æœŸæ£€æµ‹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ ¹æ®ç»˜ç”»ç±»å‹ï¼ˆåœ†åœˆã€æ›²æŠ˜ã€èºæ—‹ï¼‰è¿›è¡Œåˆ†ç±»ï¼Œæ¥ç€ä»å›¾åƒä¸­æå–ç‰¹å¾å¹¶æ£€æµ‹å¸•é‡‘æ£®ç—…ã€‚ä¸ºå…‹æœç°æœ‰ç ”ç©¶ä¸­çš„æ•°æ®é›†ä¸è¶³ä»¥åŠå¤„ç†æœªçŸ¥æ‚£è€…æ•°æ®ä¸é²æ£’çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†åˆ†å—ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æå‡ºçš„æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´ä½³è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªçŸ¥æ‚£è€…ä¸Šçš„è¡¨ç°ï¼ŒNewHandPDæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†97.08%ï¼ˆå·²çŸ¥æ‚£è€…ï¼‰å’Œ94.91%ï¼ˆæœªçŸ¥æ‚£è€…ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸•é‡‘æ£®ç—…å½±å“è¶…è¿‡60å²äººç¾¤çš„1%ï¼Œå¯¼è‡´æ‰‹éƒ¨åè°ƒæ´»åŠ¨å¦‚ä¹¦å†™å’Œç»˜ç”»å‡ºç°éšœç¢ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼šæ•°æ®é›†ä¸è¶³ä»¥åŠå¤„ç†æœªçŸ¥æ‚£è€…æ•°æ®çš„é²æ£’æ€§ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¸•é‡‘æ£®ç—…æ£€æµ‹æ³•ï¼Œåˆ†ä¸ºåŸºäºç»˜ç”»ç±»å‹çš„åˆ†ç±»å’Œä»å›¾åƒä¸­æå–ç‰¹å¾ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>é‡‡ç”¨åˆ†å—ç­–ç•¥å…‹æœä¸Šè¿°æŒ‘æˆ˜ï¼Œå°†å›¾åƒåˆ†ä¸ºå¤šä¸ªå°å—è¿›è¡Œå¤„ç†ã€‚</li>
<li>ä½¿ç”¨é›†æˆæ–¹æ³•ç»“åˆå„å°å—çš„å†³ç­–è¿›è¡Œæœ€ç»ˆåˆ†ç±»ã€‚</li>
<li>åœ¨NewHandPDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•æœ‰æ›´ä½³è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af60f599c7988cc3b8a0cf0120c11395" align="middle">
<img src="https://picx.zhimg.com/v2-d02dbb40daa4fca6472c718332c10b71" align="middle">
<img src="https://picx.zhimg.com/v2-f1d19fd0be9ac7ca3ee9e04f28215b74" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Intelligent-Communication-Mixture-of-Experts-Boosted-Medical-Image-Segmentation-Foundation-Model"><a href="#Intelligent-Communication-Mixture-of-Experts-Boosted-Medical-Image-Segmentation-Foundation-Model" class="headerlink" title="Intelligent Communication Mixture-of-Experts Boosted-Medical Image   Segmentation Foundation Model"></a>Intelligent Communication Mixture-of-Experts Boosted-Medical Image   Segmentation Foundation Model</h2><p><strong>Authors:Xinwei Zhang, Hu Chen, Zhe Yuan, Sukun Tian, Peng Feng</strong></p>
<p>Foundation models for medical image segmentation have achieved remarkable performance. Adaptive fine-tuning of natural image segmentation foundation models is crucial for medical image segmentation tasks. However, some limitations exist in existing fine-tuning methods: 1) insufficient representation of high-level features and 2) the fine-tuning process disrupts the structural integrity of pretrained weights. Inspired by these critical problems, we propose an intelligent communication mixture-of-experts boosted-medical image segmentation foundation model, named IC-MoE, with twofold ideas: 1) We construct basic experts, semantic experts, and adaptive experts. Moreover, we implement a pixel probability adaptive voting strategy, which enables expert selection and fusion through label consistency and load balancing. This approach preliminarily enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. 2) We propose a semantic-guided contrastive learning method to address the issue of weak supervision in contrastive learning. This method further enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. Extensive experiments across three public medical image segmentation datasets demonstrate that the IC-MoE outperforms other SOTA models. Consequently, the proposed IC-MoE effectively supplements foundational medical image segmentation models with high-level features and pretrained structural integrity. We also validate the superior generalizability of the IC-MoE across diverse medical image segmentation scenarios. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†å‰²çš„åŸºç¡€æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è‡ªç„¶å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹çš„è‡ªé€‚åº”å¾®è°ƒå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰å¾®è°ƒæ–¹æ³•å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼š1ï¼‰é«˜çº§ç‰¹å¾è¡¨ç¤ºä¸è¶³ï¼›2ï¼‰å¾®è°ƒè¿‡ç¨‹ç ´åäº†é¢„è®­ç»ƒæƒé‡çš„ç»“æ„å®Œæ•´æ€§ã€‚é’ˆå¯¹è¿™äº›å…³é”®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ™ºèƒ½é€šä¿¡æ··åˆä¸“å®¶å¢å¼ºåŒ»ç–—å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œç®€ç§°IC-MoEï¼Œè¯¥æ¨¡å‹æœ‰ä¸¤æ–¹é¢çš„æ€æƒ³ï¼š1ï¼‰æˆ‘ä»¬æ„å»ºäº†åŸºç¡€ä¸“å®¶ã€è¯­ä¹‰ä¸“å®¶å’Œè‡ªé€‚åº”ä¸“å®¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†åƒç´ æ¦‚ç‡è‡ªé€‚åº”æŠ•ç¥¨ç­–ç•¥ï¼Œé€šè¿‡æ ‡ç­¾ä¸€è‡´æ€§å’Œè´Ÿè½½å‡è¡¡è¿›è¡Œä¸“å®¶é€‰æ‹©å’Œèåˆã€‚è¿™ç§æ–¹æ³•åˆæ­¥å¢å¼ºäº†é«˜çº§ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæƒé‡çš„ç»“æ„å®Œæ•´æ€§ã€‚2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³å¯¹æ¯”å­¦ä¹ ä¸­ç›‘ç£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥å¢å¼ºäº†é«˜çº§ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæƒé‡çš„ç»“æ„å®Œæ•´æ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIC-MoEä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ã€‚å› æ­¤ï¼Œæ‰€æå‡ºçš„IC-MoEæœ‰æ•ˆåœ°è¡¥å……äº†åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹çš„é«˜çº§ç‰¹å¾å’Œé¢„è®­ç»ƒçš„ç»“æ„å®Œæ•´æ€§ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†IC-MoEåœ¨ä¸åŒåŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯ä¸­çš„ä¼˜ç§€æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17684v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹çš„å‡ºè‰²è¡¨ç°ï¼Œè‡ªé€‚åº”å¾®è°ƒè‡ªç„¶å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹å¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚é«˜å±‚æ¬¡ç‰¹å¾è¡¨ç¤ºä¸è¶³å’Œé¢„è®­ç»ƒæƒé‡ç»“æ„å®Œæ•´æ€§çš„ç ´åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ™ºèƒ½é€šä¿¡æ··åˆä¸“å®¶å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼ˆIC-MoEï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºåŸºæœ¬ä¸“å®¶ã€è¯­ä¹‰ä¸“å®¶å’Œè‡ªé€‚åº”ä¸“å®¶ï¼Œå¹¶é‡‡ç”¨åƒç´ æ¦‚ç‡è‡ªé€‚åº”æŠ•ç¥¨ç­–ç•¥å®ç°ä¸“å®¶é€‰æ‹©å’Œèåˆï¼Œæé«˜äº†é«˜å±‚æ¬¡ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæƒé‡çš„ç»“æ„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»¥è§£å†³å¯¹æ¯”å­¦ä¹ ä¸­çš„å¼±ç›‘ç£é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒIC-MoEåœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œæœ‰æ•ˆè¡¥å……äº†åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„é«˜å±‚æ¬¡ç‰¹å¾å’Œé¢„è®­ç»ƒç»“æ„å®Œæ•´æ€§ï¼Œå¹¶åœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è‡ªé€‚åº”å¾®è°ƒè‡ªç„¶å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹å¯¹äºåŒ»å­¦å›¾åƒåˆ†å‰²è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å¾®è°ƒæ–¹æ³•å­˜åœ¨é«˜å±‚æ¬¡ç‰¹å¾è¡¨ç¤ºä¸è¶³å’Œé¢„è®­ç»ƒæƒé‡ç»“æ„ç ´åçš„é—®é¢˜ã€‚</li>
<li>IC-MoEæ¨¡å‹é€šè¿‡æ„å»ºä¸“å®¶å¹¶é‡‡ç”¨åƒç´ æ¦‚ç‡è‡ªé€‚åº”æŠ•ç¥¨ç­–ç•¥ï¼Œæé«˜äº†é«˜å±‚æ¬¡çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>IC-MoEæ¨¡å‹é€šè¿‡è¯­ä¹‰å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ–¹æ³•è§£å†³äº†å¯¹æ¯”å­¦ä¹ ä¸­çš„å¼±ç›‘ç£é—®é¢˜ã€‚</li>
<li>IC-MoEåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f57547fec822cf32e648a68d5a29079" align="middle">
<img src="https://picx.zhimg.com/v2-05560a8edc583372dc2fb19c8b15021d" align="middle">
<img src="https://picx.zhimg.com/v2-14eec69c29592437c99c14bd990f95a2" align="middle">
<img src="https://picx.zhimg.com/v2-b4f1c0212fe0b83ddfa6a27f03beeab5" align="middle">
<img src="https://picx.zhimg.com/v2-f74db267d89ecae73228e8042bfa4f0b" align="middle">
<img src="https://picx.zhimg.com/v2-c19f5fa4c5895c27a803f36de154f9ec" align="middle">
<img src="https://picx.zhimg.com/v2-1a250a9e8b9d717513fb2fe791f78153" align="middle">
<img src="https://picx.zhimg.com/v2-d4c7d927eabc7144d433a66eea9c84c9" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ZACH-ViT-A-Zero-Token-Vision-Transformer-with-ShuffleStrides-Data-Augmentation-for-Robust-Lung-Ultrasound-Classification"><a href="#ZACH-ViT-A-Zero-Token-Vision-Transformer-with-ShuffleStrides-Data-Augmentation-for-Robust-Lung-Ultrasound-Classification" class="headerlink" title="ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data   Augmentation for Robust Lung Ultrasound Classification"></a>ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data   Augmentation for Robust Lung Ultrasound Classification</h2><p><strong>Authors:Athanasios Angelakis, Amne Mousa, Micah L. A. Heldeweg, Laurens A. Biesheuvel, Mark A. Haaksma, Jasper M. Smit, Pieter R. Tuinman, Paul W. G. Elbers</strong></p>
<p>Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and structurally normal lungs in lung ultrasound (LUS) videos remains challenging due to the high visual variability of non-cardiogenic inflammatory patterns (NCIP&#x2F;ARDS-like), interstitial lung disease, and healthy lungs. This heterogeneity complicates automated classification as overlapping B-lines and pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer variant that removes both positional embeddings and the [CLS] token, making it fully permutation-invariant and suitable for unordered medical image data. To enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA), which permutes probe-view sequences and frame orders while preserving anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95 critically ill patients against nine state-of-the-art baselines. Despite the heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60) and specificity (0.91), while all competing models collapsed to trivial classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with 2.5x fewer parameters, supporting real-time clinical deployment. These results show that aligning architectural design with data structure can outperform scale in small-data medical imaging. </p>
<blockquote>
<p>åœ¨è‚ºéƒ¨è¶…å£°ï¼ˆLUSï¼‰è§†é¢‘ä¸­ï¼Œå°†å¿ƒæºæ€§è‚ºæ°´è‚¿ï¼ˆCPEï¼‰ä¸ç»“æ„æ­£å¸¸çš„éå¿ƒæºæ€§è‚ºåŒºåˆ†å¼€æ¥ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºéå¿ƒæºæ€§ç‚ç—‡æ¨¡å¼ï¼ˆNCIP&#x2F;ARDSç±»ä¼¼ï¼‰ã€é—´è´¨æ€§è‚ºç–¾ç—…å’Œå¥åº·è‚ºéƒ¨çš„è§†è§‰å˜åŒ–éå¸¸å¤§ã€‚è¿™ç§å¼‚è´¨æ€§ä½¿å¾—è‡ªåŠ¨åŒ–åˆ†ç±»å˜å¾—å¤æ‚ï¼Œå› ä¸ºBçº¿é‡å å’Œèƒ¸è†œä¼ªå½±å¾ˆå¸¸è§ã€‚æˆ‘ä»¬å¼•å…¥äº†ZACH-ViTï¼ˆé›¶ä»¤ç‰Œè‡ªé€‚åº”ç´§å‡‘åˆ†å±‚è§†è§‰è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰0.25Må‚æ•°çš„è§†è§‰è½¬æ¢å™¨å˜ä½“ï¼Œå®ƒç§»é™¤äº†ä½ç½®åµŒå…¥å’Œ[CLS]ä»¤ç‰Œï¼Œä½¿å…¶å®Œå…¨å…·æœ‰æ’åˆ—ä¸å˜æ€§ï¼Œé€‚ç”¨äºæ— åºçš„åŒ»å­¦å›¾åƒæ•°æ®ã€‚ä¸ºäº†å¢å¼ºé€šç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ShuffleStridesæ•°æ®å¢å¼ºï¼ˆSSDAï¼‰ï¼Œå®ƒé€šè¿‡æ”¹å˜æ¢å¤´è§†å›¾åºåˆ—å’Œå¸§é¡ºåºæ¥ä¿ç•™è§£å‰–æœ‰æ•ˆæ€§ã€‚ZACH-ViTåœ¨æ¥è‡ª95åå±é‡æ‚£è€…çš„380ä¸ªLUSè§†é¢‘ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸9ç§æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œå°½ç®¡éå¿ƒæºæ€§ç»„çš„å¼‚è´¨æ€§å¾ˆå¼ºï¼Œä½†ZACH-ViTè¾¾åˆ°äº†æœ€é«˜çš„éªŒè¯å’Œæµ‹è¯•ROC-AUCï¼ˆåˆ†åˆ«ä¸º0.80å’Œ0.79ï¼‰ï¼Œå…·æœ‰å¹³è¡¡çš„æ•æ„Ÿæ€§ï¼ˆ0.60ï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆ0.91ï¼‰ï¼Œè€Œæ‰€æœ‰ç«äº‰æ¨¡å‹éƒ½å½’ç±»ä¸ºå¾®ä¸è¶³é“ã€‚å®ƒæ¯”å…·æœ‰0.62Må‚æ•°çš„æœ€å°ViTè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ˆå¿«1.35å€ï¼‰ï¼ŒåŒæ—¶å‚æ•°æ›´å°‘ï¼ˆå°‘2.5å€ï¼‰ï¼Œæ”¯æŒå®æ—¶ä¸´åºŠéƒ¨ç½²ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†æ¶æ„è®¾è®¡ä¸æ•°æ®ç»“æ„ç›¸åŒ¹é…å¯ä»¥åœ¨å°å‹åŒ»ç–—å›¾åƒæ•°æ®ä¸­è·å¾—è¶…è¶Šè§„æ¨¡çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17650v1">PDF</a> 14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine   Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),   eess.IV (Image and Video Processing). Code available at:   <a target="_blank" rel="noopener" href="https://github.com/Bluesman79/ZACH-ViT">https://github.com/Bluesman79/ZACH-ViT</a> Installation: pip install zachvit   Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è‚ºè¶…å£°è§†é¢‘ä¸­å¿ƒæºæ€§è‚ºæ°´è‚¿ï¼ˆCPEï¼‰ä¸éå¿ƒæºæ€§åŠç»“æ„æ­£å¸¸è‚ºéƒ¨çš„åŒºåˆ†ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚æå‡ºä¸€ç§åä¸ºZACH-ViTçš„æ–°å‹è§†è§‰è½¬æ¢å™¨ï¼Œå¯åœ¨æ— åºåŒ»å­¦å›¾åƒæ•°æ®ä¸Šå®ç°å…¨è‡ªåŠ¨åˆ†ç±»ã€‚é€šè¿‡ShuffleStridesæ•°æ®å¢å¼ºæ–¹æ³•æé«˜é€šç”¨æ€§ï¼Œå¹¶åœ¨380ä¸ªè‚ºè¶…å£°è§†é¢‘ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºZACH-ViTåœ¨å¼‚è´¨éå¿ƒæºæ€§ç–¾ç—…ç»„ä¸­å…·æœ‰æœ€ä½³éªŒè¯å’Œæµ‹è¯•ROC-AUCå€¼ï¼Œä¸”è®­ç»ƒé€Ÿåº¦å¿«ã€å‚æ•°å°‘ï¼Œé€‚åˆå®æ—¶ä¸´åºŠéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> * åŒºåˆ†è‚ºè¶…å£°è§†é¢‘ä¸­CPEä¸éå¿ƒæºæ€§åŠæ­£å¸¸è‚ºéƒ¨å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
 
 * ZACH-ViTæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è½¬æ¢å™¨ï¼Œå¯å¤„ç†æ— åºåŒ»å­¦å›¾åƒæ•°æ®ã€‚
 
 * æå‡ºçš„ShuffleStridesæ•°æ®å¢å¼ºæ–¹æ³•èƒ½æé«˜æ¨¡å‹çš„é€šç”¨æ€§ã€‚
 
 * ZACH-ViTåœ¨éå¿ƒæºæ€§è‚ºç—…ç»„ä¸­å…·æœ‰æœ€ä½³ROC-AUCå€¼ã€‚
 
 * ZACH-ViTå…·æœ‰å¿«é€Ÿè®­ç»ƒä¸è¾ƒå°‘çš„å‚æ•°ï¼Œé€‚åˆå®æ—¶ä¸´åºŠéƒ¨ç½²ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4371c8edd367ce36b8b8176b7eef99f" align="middle">
<img src="https://picx.zhimg.com/v2-fb21c8236f33c9cd1938bdec22daff73" align="middle">
<img src="https://picx.zhimg.com/v2-94bdaf38f5ee054378c7a0ca6cadce4e" align="middle">
<img src="https://picx.zhimg.com/v2-e25e55d79640e38f0ab0e3923bb7ab40" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CEPerFed-Communication-Efficient-Personalized-Federated-Learning-for-Multi-Pulse-MRI-Classification"><a href="#CEPerFed-Communication-Efficient-Personalized-Federated-Learning-for-Multi-Pulse-MRI-Classification" class="headerlink" title="CEPerFed: Communication-Efficient Personalized Federated Learning for   Multi-Pulse MRI Classification"></a>CEPerFed: Communication-Efficient Personalized Federated Learning for   Multi-Pulse MRI Classification</h2><p><strong>Authors:Ludi Li, Junbin Mao, Hanhe Lin, Xu Tian, Fang-Xiang Wu, Jin Liu</strong></p>
<p>Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical practice such as Alzheimerâ€™s disease diagnosis. To train a robust model for multi-pulse MRI classification, it requires large and diverse data from various medical institutions while protecting privacy by preventing raw data sharing across institutions. Although federated learning (FL) is a feasible solution to address this issue, it poses challenges of model convergence due to the effect of data heterogeneity and substantial communication overhead due to large numbers of parameters transmitted within the model. To address these challenges, we propose CEPerFed, a communication-efficient personalized FL method. It mitigates the effect of data heterogeneity by incorporating client-side historical risk gradients and historical mean gradients to coordinate local and global optimization. The former is used to weight the contributions from other clients, enhancing the reliability of local updates, while the latter enforces consistency between local updates and the global optimization direction to ensure stable convergence across heterogeneous data distributions. To address the high communication overhead, we propose a hierarchical SVD (HSVD) strategy that transmits only the most critical information required for model updates. Experiments on five classification tasks demonstrate the effectiveness of the CEPerFed method. The code will be released upon acceptance at <a target="_blank" rel="noopener" href="https://github.com/LD0416/CEPerFed">https://github.com/LD0416/CEPerFed</a>. </p>
<blockquote>
<p>å¤šè„‰å†²ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¹¿æ³›åº”ç”¨äºä¸´åºŠå®è·µï¼Œå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­ã€‚ä¸ºäº†è®­ç»ƒç”¨äºå¤šè„‰å†²MRIåˆ†ç±»çš„ç¨³å¥æ¨¡å‹ï¼Œéœ€è¦æ¥è‡ªä¸åŒåŒ»ç–—æœºæ„çš„å¤§é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ï¼ŒåŒæ—¶è¦é€šè¿‡é˜²æ­¢åŸå§‹æ•°æ®åœ¨æœºæ„ä¹‹é—´çš„å…±äº«æ¥ä¿æŠ¤éšç§ã€‚è™½ç„¶è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºæ•°æ®å¼‚è´¨æ€§çš„å½±å“å’Œæ¨¡å‹å†…ä¼ è¾“çš„å¤§é‡å‚æ•°å¯¼è‡´çš„é€šä¿¡å¼€é”€è¾ƒå¤§ï¼Œå®ƒç»™æ¨¡å‹çš„æ”¶æ•›å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CEPerFedï¼Œä¸€ç§é€šä¿¡é«˜æ•ˆçš„ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚å®ƒé€šè¿‡èå…¥å®¢æˆ·ç«¯ä¾§çš„å†å²é£é™©æ¢¯åº¦å’Œå†å²å¹³å‡æ¢¯åº¦æ¥åè°ƒå±€éƒ¨å’Œå…¨å±€ä¼˜åŒ–ï¼Œä»è€Œå‡è½»äº†æ•°æ®å¼‚è´¨æ€§çš„å½±å“ã€‚å‰è€…ç”¨äºæƒè¡¡å…¶ä»–å®¢æˆ·ç«¯çš„è´¡çŒ®ï¼Œæé«˜æœ¬åœ°æ›´æ–°çš„å¯é æ€§ï¼Œè€Œåè€…åˆ™å¼ºåˆ¶æœ¬åœ°æ›´æ–°ä¸å…¨å±€ä¼˜åŒ–æ–¹å‘ä¹‹é—´ä¿æŒä¸€è‡´ï¼Œä»¥ç¡®ä¿åœ¨å¼‚è´¨æ•°æ®åˆ†å¸ƒä¹‹é—´å®ç°ç¨³å®šçš„æ”¶æ•›ã€‚ä¸ºè§£å†³é«˜é€šä¿¡å¼€é”€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚SVDï¼ˆHSVDï¼‰ç­–ç•¥ï¼Œåªä¼ è¾“æ¨¡å‹æ›´æ–°æ‰€éœ€çš„æœ€å…³é”®ä¿¡æ¯ã€‚åœ¨äº”ä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†CEPerFedæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LD0416/CEPerFed%E3%80%82">https://github.com/LD0416/CEPerFedã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šè„‰å†²ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠå®è·µä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­ã€‚ä¸ºè§£å†³å¤šè„‰å†²MRIåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¤§é‡è·¨æœºæ„çš„æ•°æ®ä¸”è¦ä¿è¯æ•°æ®éšç§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ•°æ®å¼‚è´¨æ€§å’Œå¤§é‡å‚æ•°ä¼ è¾“å¸¦æ¥çš„æ¨¡å‹æ”¶æ•›å’Œé€šä¿¡æ•ˆç‡æŒ‘æˆ˜ä¹Ÿå¤‡å—å…³æ³¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†CEPerFedï¼Œä¸€ç§é€šä¿¡é«˜æ•ˆçš„ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚å®ƒé€šè¿‡èå…¥å®¢æˆ·ç«¯çš„å†å²é£é™©æ¢¯åº¦å’Œå†å²å¹³å‡æ¢¯åº¦æ¥åè°ƒæœ¬åœ°å’Œå…¨å±€ä¼˜åŒ–ï¼Œä»¥ç¼“è§£æ•°æ®å¼‚è´¨æ€§çš„å½±å“ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åˆ†å±‚SVDï¼ˆHSVDï¼‰ç­–ç•¥ï¼Œä»…ä¼ è¾“æ¨¡å‹æ›´æ–°æ‰€éœ€çš„æœ€å…³é”®ä¿¡æ¯ï¼Œä»¥è§£å†³é«˜é€šä¿¡å¼€é”€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCEPerFedæ–¹æ³•åœ¨äº”ä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè„‰å†²MRIåœ¨ä¸´åºŠå®è·µä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–¾ç—…çš„è¯Šæ–­ä¸Šï¼Œå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ã€‚</li>
<li>è§£å†³å¤šè„‰å†²MRIåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¤„ç†è·¨æœºæ„å¤§æ•°æ®çš„åŒæ—¶ä¿æŠ¤éšç§ã€‚</li>
<li>è”é‚¦å­¦ä¹ æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„å¯è¡Œæ–¹æ¡ˆï¼Œä½†é¢ä¸´æ•°æ®å¼‚è´¨æ€§å’Œé€šä¿¡æ•ˆç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>CEPerFedæ–¹æ³•é€šè¿‡èå…¥å†å²é£é™©æ¢¯åº¦å’Œå†å²å¹³å‡æ¢¯åº¦æ¥ç¼“è§£æ•°æ®å¼‚è´¨æ€§çš„å½±å“ã€‚</li>
<li>CEPerFedæ–¹æ³•é‡‡ç”¨åˆ†å±‚SVDç­–ç•¥ï¼Œå‡å°‘é€šä¿¡å¼€é”€ï¼Œä»…ä¼ è¾“æ¨¡å‹æ›´æ–°æ‰€éœ€çš„æœ€å…³é”®ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜CEPerFedæ–¹æ³•åœ¨äº”ä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7466c9ae404989516d2f645786b90502" align="middle">
<img src="https://picx.zhimg.com/v2-aa68e4c6253f71762dc3442ed9bf516c" align="middle">
<img src="https://picx.zhimg.com/v2-6fc62cfb6ddef5d696acc3122ec77b62" align="middle">
<img src="https://picx.zhimg.com/v2-d453768d3c716f01e2dbaa4a177d6f32" align="middle">
<img src="https://picx.zhimg.com/v2-e196c88c9cecbd31c0b52e1964024c3a" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MambaX-Net-Dual-Input-Mamba-Enhanced-Cross-Attention-Network-for-Longitudinal-MRI-Segmentation"><a href="#MambaX-Net-Dual-Input-Mamba-Enhanced-Cross-Attention-Network-for-Longitudinal-MRI-Segmentation" class="headerlink" title="MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for   Longitudinal MRI Segmentation"></a>MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for   Longitudinal MRI Segmentation</h2><p><strong>Authors:Yovin Yahathugoda, Davide Prezzi, Piyalitt Ittichaiwong, Vicky Goh, Sebastien Ourselin, Michela Antonelli</strong></p>
<p>Active Surveillance (AS) is a treatment option for managing low and intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while monitoring disease progression through serial MRI and clinical follow-up. Accurate prostate segmentation is an important preliminary step for automating this process, enabling automated detection and diagnosis of PCa. However, existing deep-learning segmentation models are often trained on single-time-point and expertly annotated datasets, making them unsuitable for longitudinal AS analysis, where multiple time points and a scarcity of expert labels hinder their effective fine-tuning. To address these challenges, we propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation architecture that computes the segmentation for time point t by leveraging the MRI and the corresponding segmentation mask from the previous time point. We introduce two new components: (i) a Mamba-enhanced Cross-Attention Module, which integrates the Mamba block into cross attention to efficiently capture temporal evolution and long-range spatial dependencies, and (ii) a Shape Extractor Module that encodes the previous segmentation mask into a latent anatomical representation for refined zone delination. Moreover, we introduce a semi-supervised self-training strategy that leverages pseudo-labels generated from a pre-trained nnU-Net, enabling effective learning without expert annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results showed that it significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even when trained on limited and noisy data. </p>
<blockquote>
<p>æ´»è·ƒç›‘æ§ï¼ˆASï¼‰æ˜¯ç®¡ç†å’Œæ²»ç–—ä½å±å’Œä¸­å±å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰çš„ä¸€ç§æ²»ç–—æ–¹å¼ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—MRIå’Œä¸´åºŠéšè®¿ç›‘æµ‹ç–¾ç—…è¿›å±•ï¼Œé¿å…è¿‡åº¦æ²»ç–—ã€‚å‡†ç¡®çš„å‰åˆ—è…ºåˆ†å‰²æ˜¯è‡ªåŠ¨åŒ–æ­¤è¿‡ç¨‹çš„é‡è¦åˆæ­¥æ­¥éª¤ï¼Œå¯å®ç°å‰åˆ—è…ºç™Œçš„è‡ªåŠ¨æ£€æµ‹å’Œè¯Šæ–­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹é€šå¸¸åŸºäºå•æ—¶é—´ç‚¹å’Œä¸“ä¸šæ³¨é‡Šçš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤ä¸é€‚ç”¨äºçºµå‘ASåˆ†æã€‚åœ¨çºµå‘ASåˆ†æä¸­ï¼Œå¤šä¸ªæ—¶é—´ç‚¹å’Œä¸“å®¶æ ‡ç­¾çš„ç¨€ç¼ºæ€§é˜»ç¢äº†å…¶æœ‰æ•ˆçš„å¾®è°ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MambaX-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åŠç›‘ç£ã€åŒæ‰«æ3Dåˆ†å‰²æ¶æ„ã€‚å®ƒé€šè¿‡åˆ©ç”¨MRIå’Œä¸Šä¸€ä¸ªæ—¶é—´ç‚¹çš„ç›¸åº”åˆ†å‰²æ©è†œæ¥è®¡ç®—æ—¶é—´ç‚¹tçš„åˆ†å‰²ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ä¸ªæ–°ç»„ä»¶ï¼šï¼ˆiï¼‰Mambaå¢å¼ºå‹äº¤å‰æ³¨æ„æ¨¡å—ï¼Œå®ƒå°†Mambaå—é›†æˆåˆ°äº¤å‰æ³¨æ„ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰æ—¶é—´æ¼”å˜å’Œè¿œç¨‹ç©ºé—´ä¾èµ–æ€§ï¼›ï¼ˆiiï¼‰å½¢çŠ¶æå–æ¨¡å—ï¼Œå®ƒå°†ä¸Šä¸€ä¸ªåˆ†å‰²æ©è†œç¼–ç ä¸ºæ½œåœ¨è§£å‰–è¡¨ç¤ºï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„åŒºåŸŸåˆ’åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠç›‘ç£è‡ªè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨æ¥è‡ªé¢„è®­ç»ƒnnU-Netç”Ÿæˆçš„ä¼ªæ ‡ç­¾ï¼Œå®ç°æ— éœ€ä¸“å®¶æ³¨é‡Šçš„æœ‰æ•ˆå­¦ä¹ ã€‚MambaX-Netåœ¨çºµå‘ASæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œå®ƒæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›U-Netå’ŒåŸºäºTransformerçš„æ¨¡å‹ï¼Œå³ä½¿åœ¨æœ‰é™å’Œå™ªå£°æ•°æ®ä¸­ä¹Ÿèƒ½å®ç°å“è¶Šçš„å‰åˆ—è…ºåŒºåŸŸåˆ†å‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17529v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä½å±å’Œä¸­é—´é£é™©çš„å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ²»ç–—è¿‡ç¨‹ä¸­çš„ä¸€ç§é‡è¦çš„é¢„å¤„ç†æ­¥éª¤â€”â€”å‡†ç¡®çš„å‰åˆ—è…ºåˆ†å‰²æŠ€æœ¯ã€‚ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸èƒ½æ»¡è¶³é•¿æœŸçš„æ´»è·ƒç›‘æµ‹åˆ†æéœ€æ±‚ï¼Œä¸ºæ­¤æå‡ºäº†åä¸ºMambaX-Netçš„åŠç›‘ç£åŒæ‰«æä¸‰ç»´åˆ†å‰²æ¶æ„ã€‚MambaX-NetåŒ…æ‹¬ä¸¤ç§æ–°ç»„ä»¶ï¼šä¸€ç§é©¬è¿ˆå·´å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰æ—¶é—´åºåˆ—æ¼”å˜å’Œè¿œç¨‹ç©ºé—´ä¾èµ–å…³ç³»ï¼›ä¸€ç§å½¢çŠ¶æå–æ¨¡å—ï¼Œç”¨äºå°†å…ˆå‰çš„åˆ†å‰²æ©è†œç¼–ç ä¸ºæ½œåœ¨è§£å‰–ç»“æ„è¡¨ç¤ºä»¥ç»†åŒ–åŒºåŸŸç•Œé™ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åŠç›‘ç£è‡ªè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å®ç°æ— éœ€ä¸“å®¶æ ‡æ³¨çš„æœ‰æ•ˆå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çºµå‘æ´»è·ƒç›‘æµ‹æ•°æ®é›†ä¸Šï¼ŒMambaX-Netç›¸è¾ƒäºç›®å‰æœ€å…ˆè¿›çš„U-Netå’ŒTransformeræ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™ä¸”å¸¦å™ªå£°çš„æ•°æ®é›†ä¸Šå®ç°ä¼˜å¼‚çš„å‰åˆ—è…ºåŒºåŸŸåˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ´»è·ƒç›‘æµ‹ï¼ˆASï¼‰æ˜¯æ²»ç–—ä½å±å’Œä¸­é—´é£é™©å‰åˆ—è…ºç™Œçš„æ–¹æ³•ä¹‹ä¸€ï¼Œå…¶ä¸­å‡†ç¡®çš„å‰åˆ—è…ºåˆ†å‰²æ˜¯é‡è¦æ­¥éª¤ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çºµå‘æ´»è·ƒç›‘æµ‹åˆ†æä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¤šä¸ªæ—¶é—´ç‚¹åŠä¸“å®¶æ ‡ç­¾ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>MambaX-Netæ˜¯ä¸€ç§åŠç›‘ç£åŒæ‰«æä¸‰ç»´åˆ†å‰²æ¶æ„ï¼Œç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚å®ƒä½¿ç”¨ä¸¤ç§æ–°ç»„ä»¶æ¥æé«˜æ€§èƒ½ï¼šé©¬è¿ˆå·´å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œå½¢çŠ¶æå–æ¨¡å—ã€‚</li>
<li>MambaX-Netèƒ½æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—æ¼”å˜å’Œè¿œç¨‹ç©ºé—´ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨å…ˆå‰çš„åˆ†å‰²æ©è†œè¿›è¡Œç²¾ç»†åŒºåŸŸåˆ’åˆ†ã€‚</li>
<li>MambaX-Neté€šè¿‡å¼•å…¥åŠç›‘ç£è‡ªè®­ç»ƒç­–ç•¥ï¼Œèƒ½åœ¨æ— ä¸“å®¶æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆå­¦ä¹ ã€‚</li>
<li>MambaX-Netåœ¨çºµå‘æ´»è·ƒç›‘æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œå¦‚U-Netå’ŒTransformeræ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fbcbc7533215165b0d6437c062c420b" align="middle">
<img src="https://picx.zhimg.com/v2-18ba0cb389248a5137569658961fd2ab" align="middle">
<img src="https://picx.zhimg.com/v2-fb6f2a2ae320695e18cb7767ba0c980e" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Segmenting-infant-brains-across-magnetic-fields-Domain-randomization-and-annotation-curation-in-ultra-low-field-MRI"><a href="#Segmenting-infant-brains-across-magnetic-fields-Domain-randomization-and-annotation-curation-in-ultra-low-field-MRI" class="headerlink" title="Segmenting infant brains across magnetic fields: Domain randomization   and annotation curation in ultra-low field MRI"></a>Segmenting infant brains across magnetic fields: Domain randomization   and annotation curation in ultra-low field MRI</h2><p><strong>Authors:Vladyslav Zalevskyi, Dondu-Busra Bulut, Thomas Sanchez, Meritxell Bach Cuadra</strong></p>
<p>Early identification of neurodevelopmental disorders relies on accurate segmentation of brain structures in infancy, a task complicated by rapid brain growth, poor tissue contrast, and motion artifacts in pediatric MRI. These challenges are further exacerbated in ultra-low-field (ULF, 0.064~T) MRI, which, despite its lower image quality, offers an affordable, portable, and sedation-free alternative for use in low-resource settings. In this work, we propose a domain randomization (DR) framework to bridge the domain gap between high-field (HF) and ULF MRI in the context of the hippocampi and basal ganglia segmentation in the LISA challenge. We show that pre-training on whole-brain HF segmentations using DR significantly improves generalization to ULF data, and that careful curation of training labels, by removing misregistered HF-to-ULF annotations from training, further boosts performance. By fusing the predictions of several models through majority voting, we are able to achieve competitive performance. Our results demonstrate that combining robust augmentation with annotation quality control can enable accurate segmentation in ULF data. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/lisasegm">https://github.com/Medical-Image-Analysis-Laboratory/lisasegm</a> </p>
<blockquote>
<p>æ—©æœŸç¥ç»å‘è‚²éšœç¢çš„è¯†åˆ«ä¾èµ–äºå©´å¹¼å„¿æœŸè„‘ç»“æ„çš„å‡†ç¡®åˆ†å‰²ï¼Œè¿™ä¸€ä»»åŠ¡å› å„¿ç«¥å¤§è„‘çš„å¿«é€Ÿç”Ÿé•¿ã€ç»„ç»‡å¯¹æ¯”åº¦å·®ä»¥åŠè¿åŠ¨ä¼ªå½±è€Œå˜å¾—å¤æ‚ã€‚è¿™äº›æŒ‘æˆ˜åœ¨è¶…ä½åœºï¼ˆULFï¼Œ0.064~Tï¼‰MRIä¸­æ›´ä¸ºä¸¥é‡ï¼Œå°½ç®¡å…¶å›¾åƒè´¨é‡è¾ƒä½ï¼Œä½†å®ƒæä¾›äº†ç»æµå®æƒ ã€ä¾¿æºä¸”æ— éœ€é•‡é™çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºèµ„æºåŒ®ä¹çš„ç¯å¢ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸŸéšæœºåŒ–ï¼ˆDRï¼‰æ¡†æ¶ï¼Œä»¥å¼¥è¡¥é«˜åœºï¼ˆHFï¼‰å’ŒULF MRIä¹‹é—´çš„åŸŸé—´éš™å·®å¼‚ï¼Œé’ˆå¯¹LisaæŒ‘æˆ˜ä¸­çš„æµ·é©¬ä½“å’ŒåŸºåº•ç¥ç»èŠ‚åˆ†å‰²è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨å…¨è„‘HFåˆ†å‰²ä¸Šä½¿ç”¨DRè¿›è¡Œé¢„è®­ç»ƒå¯æ˜¾è‘—æé«˜å¯¹ULFæ•°æ®çš„é€šç”¨æ€§ï¼Œå¹¶ä¸”é€šè¿‡ä»”ç»†ç­›é€‰è®­ç»ƒæ ‡ç­¾ï¼ˆä»è®­ç»ƒä¸­å‰”é™¤è¯¯æ³¨å†Œçš„é«˜é¢‘åˆ°è¶…ä½åœºçš„æ³¨é‡Šï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚é€šè¿‡æŠ•ç¥¨æ–¹å¼èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬èƒ½å¤Ÿå–å¾—ä»¤äººç©ç›®çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆé²æ£’çš„å¢å¼ºåŠŸèƒ½å’Œæ³¨é‡Šè´¨é‡æ§åˆ¶èƒ½å¤Ÿåœ¨ULFæ•°æ®ä¸­å®ç°å‡†ç¡®çš„åˆ†å‰²ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/lisasegm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/lisasegmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17436v1">PDF</a> 1st place (hippocampus) and 3rd place (basal ganglia) in the Low   field pediatric brain magnetic resonance Image Segmentation and quality   Assurance Challenge (LISA) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ—©æœŸç¥ç»å‘è‚²éšœç¢çš„å‡†ç¡®è¯Šæ–­ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸­ã€‚ç ”ç©¶ä¸­æå‡ºä¸€ç§åŸºäºé¢†åŸŸéšæœºåŒ–ï¼ˆDRï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°é«˜åœºï¼ˆHFï¼‰å’Œè¶…ä½é¢‘ï¼ˆULFï¼‰MRIåœ¨è„‘ç»“æ„åˆ†å‰²æ–¹é¢çš„å·®è·ã€‚é€šè¿‡é¢„è®­ç»ƒå…¨è„‘HFåˆ†å‰²å¹¶é‡‡ç”¨DRæŠ€æœ¯ï¼Œæ¨¡å‹åœ¨ULFæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚åŒæ—¶ï¼Œå¯¹è®­ç»ƒæ ‡ç­¾è¿›è¡Œä»”ç»†ç­›é€‰ï¼Œå»é™¤è¯¯æ³¨å†Œçš„é«˜é¢‘è‡³ä½é¢‘æ ‡æ³¨ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚é€šè¿‡èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå®ç°å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶ç»“åˆäº†é²æ£’å¢å¼ºä¸æ ‡æ³¨è´¨é‡æ§åˆ¶ï¼Œä½¿å¾—ULFæ•°æ®çš„ç²¾å‡†åˆ†å‰²æˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—©æœŸç¥ç»å‘è‚²éšœç¢è¯Šæ–­ä¾èµ–äºå©´å„¿æœŸè„‘ç»“æ„çš„å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>è¶…ä½é¢‘MRIï¼ˆULFï¼‰è™½å›¾åƒè´¨é‡è¾ƒä½ï¼Œä½†ä½œä¸ºä¸€ç§ç»æµå®æƒ ã€ä¾¿æºä¸”æ— éœ€é•‡é™çš„é€‰æ‹©ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­å…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>é¢†åŸŸéšæœºåŒ–ï¼ˆDRï¼‰æ¡†æ¶è¢«ç”¨äºç¼©å°é«˜åœºï¼ˆHFï¼‰MRIä¸ULF MRIåœ¨è„‘ç»“æ„åˆ†å‰²ä¸Šçš„å·®è·ã€‚</li>
<li>é¢„è®­ç»ƒå…¨è„‘HFåˆ†å‰²å¹¶é‡‡ç”¨DRæŠ€æœ¯æ˜¾è‘—æé«˜æ¨¡å‹åœ¨ULFæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒæ ‡ç­¾çš„ä»”ç»†ç­›é€‰å’Œè¯¯æ³¨å†Œæ ‡æ³¨çš„å»é™¤å¯¹æå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå®ç°äº†åœ¨ULFæ•°æ®ä¸Šçš„ç²¾å‡†åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f784b3ab94229cffdb4aa380c49f331" align="middle">
<img src="https://picx.zhimg.com/v2-0be5ef725cc9d4ddf636a5485f5c5dd5" align="middle">
<img src="https://picx.zhimg.com/v2-5ce4b811da38071321baca370255fae7" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GACO-CAD-Geometry-Augmented-and-Conciseness-Optimized-CAD-Model-Generation-from-Single-Image"><a href="#GACO-CAD-Geometry-Augmented-and-Conciseness-Optimized-CAD-Model-Generation-from-Single-Image" class="headerlink" title="GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model   Generation from Single Image"></a>GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model   Generation from Single Image</h2><p><strong>Authors:Yinghui Wang, Xinyu Zhang, Peng Du</strong></p>
<p>Generating editable, parametric CAD models from a single image holds great potential to lower the barriers of industrial concept design. However, current multi-modal large language models (MLLMs) still struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities. We address this limitation by introducing GACO-CAD, a novel two-stage post-training framework. It is designed to achieve a joint objective: simultaneously improving the geometric accuracy of the generated CAD models and encouraging the use of more concise modeling procedures. First, during supervised fine-tuning, we leverage depth and surface normal maps as dense geometric priors, combining them with the RGB image to form a multi-channel input. In the context of single-view reconstruction, these priors provide complementary spatial cues that help the MLLM more reliably recover 3D geometry from 2D observations. Second, during reinforcement learning, we introduce a group length reward that, while preserving high geometric fidelity, promotes the generation of more compact and less redundant parametric modeling sequences. A simple dynamic weighting strategy is adopted to stabilize training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD achieves state-of-the-art performance under the same MLLM backbone, consistently outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„å‚æ•°åŒ–CADæ¨¡å‹ï¼Œä¸ºé™ä½å·¥ä¸šæ¦‚å¿µè®¾è®¡çš„å£å’å¸¦æ¥äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç©ºé—´æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä»äºŒç»´å›¾åƒå‡†ç¡®æ¨æ–­ä¸‰ç»´å‡ ä½•ç»“æ„æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥GACO-CADè¿™ä¸€æ–°å‹ä¸¤é˜¶æ®µåè®­ç»ƒæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®ƒçš„è®¾è®¡æ—¨åœ¨å®ç°å…±åŒçš„ç›®æ ‡ï¼šåŒæ—¶æé«˜ç”Ÿæˆçš„CADæ¨¡å‹çš„ç©ºé—´å‡†ç¡®æ€§å¹¶é¼“åŠ±ä½¿ç”¨æ›´ç®€æ´çš„å»ºæ¨¡ç¨‹åºã€‚é¦–å…ˆï¼Œåœ¨ç›‘ç£å¾®è°ƒæœŸé—´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå¯†é›†å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œå°†å®ƒä»¬ä¸RGBå›¾åƒç»“åˆå½¢æˆå¤šé€šé“è¾“å…¥ã€‚åœ¨å•è§†å›¾é‡å»ºçš„æƒ…å¢ƒä¸­ï¼Œè¿™äº›å…ˆéªŒçŸ¥è¯†æä¾›äº†è¡¥å……çš„ç©ºé—´çº¿ç´¢ï¼Œæœ‰åŠ©äºMLLMæ›´å¯é åœ°ä»äºŒç»´è§‚æµ‹ä¸­æ¢å¤ä¸‰ç»´å‡ ä½•ç»“æ„ã€‚å…¶æ¬¡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ æœŸé—´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»„é•¿åº¦å¥–åŠ±ï¼Œå®ƒåœ¨ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦çš„åŒæ—¶ï¼Œä¿ƒè¿›äº†æ›´ç´§å‡‘ã€æ›´å°‘å†—ä½™çš„å‚æ•°åŒ–å»ºæ¨¡åºåˆ—çš„ç”Ÿæˆã€‚é‡‡ç”¨ç®€å•çš„åŠ¨æ€åŠ æƒç­–ç•¥æ¥ç¨³å®šè®­ç»ƒã€‚åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨ç›¸åŒçš„MLLMä¸»å¹²ç½‘ç»œä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•å‡†ç¡®æ€§å’Œå»ºæ¨¡ç®€æ´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17157v1">PDF</a> </p>
<p><strong>Summary</strong><br>    GACO-CADæ¡†æ¶é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µåè®­ç»ƒæŠ€æœ¯ï¼Œæå‡äº†ä»å•å›¾ç”Ÿæˆå¯ç¼–è¾‘å‚æ•°åŒ–CADæ¨¡å‹çš„å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç¨‹åºçš„ç®€æ´æ€§ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œä¸RGBå›¾åƒç»“åˆå½¢æˆå¤šé€šé“è¾“å…¥ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œå¼•å…¥ç»„é•¿åº¦å¥–åŠ±ï¼Œä¿ƒè¿›ç”Ÿæˆæ›´ç®€æ´çš„æ¨¡å‹åºåˆ—ã€‚å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨MLLMæ¶æ„ä¸‹å®ç°æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•åœ¨ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§æ–¹é¢çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GACO-CADæ¡†æ¶æ—¨åœ¨æé«˜ä»å•ä¸€å›¾åƒç”Ÿæˆçš„å¯ç¼–è¾‘å‚æ•°åŒ–CADæ¨¡å‹çš„å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸¤é˜¶æ®µåè®­ç»ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬åˆ©ç”¨æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå‡ ä½•å…ˆéªŒçš„ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œå¤šé€šé“è¾“å…¥æé«˜äº†æ¨¡å‹ä»2Då›¾åƒä¸­å‡†ç¡®æ¨æ–­3Då‡ ä½•çš„èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µå¼•å…¥çš„ç»„é•¿åº¦å¥–åŠ±ï¼Œåœ¨ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦çš„åŒæ—¶ï¼Œä¿ƒè¿›äº†æ›´ç®€æ´çš„å‚æ•°åŒ–å»ºæ¨¡åºåˆ—çš„ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨ç®€å•çš„åŠ¨æ€åŠ æƒç­–ç•¥æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨ç›¸åŒMLLMæ¶æ„ä¸‹å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6dbae90fe7b9f775db99c2177aa59f2b" align="middle">
<img src="https://picx.zhimg.com/v2-ac7fff52673cfe893e5aab942f8af24c" align="middle">
<img src="https://picx.zhimg.com/v2-17f7605ac6cce1c215330a19a6dc0fd9" align="middle">
<img src="https://picx.zhimg.com/v2-110fc71fe2b700c3dadc97b34c9c0a43" align="middle">
<img src="https://picx.zhimg.com/v2-5eda5eb1af0caa8e7b705421221f888e" align="middle">
<img src="https://picx.zhimg.com/v2-5515df575d9c6232eb0399753253c5e2" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PorousGen-An-Efficient-Algorithm-for-Generating-Porous-Structures-with-Accurate-Porosity-and-Uniform-Density-Distribution"><a href="#PorousGen-An-Efficient-Algorithm-for-Generating-Porous-Structures-with-Accurate-Porosity-and-Uniform-Density-Distribution" class="headerlink" title="PorousGen: An Efficient Algorithm for Generating Porous Structures with   Accurate Porosity and Uniform Density Distribution"></a>PorousGen: An Efficient Algorithm for Generating Porous Structures with   Accurate Porosity and Uniform Density Distribution</h2><p><strong>Authors:Shota Arai, Takashi Yoshidome</strong></p>
<p>This work presents a novel algorithm for generating porous structures as an alternative to the PoreSpy program suite. Unlike PoreSpy, which often produces structures whose porosity deviates from the target value, our proposed algorithm generates structures whose porosity closely matches the specified input, within a defined error margin. Furthermore, parallel computation enables efficient generation of large-scale structures, while memory usage is reduced compared to PoreSpy. To evaluate performance, structures were generated using both PoreSpy and the proposed method with parameters corresponding to X-ray ptychography experiments. The porosity mismatch in PoreSpy led to a relative error exceeding 20% in the computed gas diffusion coefficients, whereas our method reproduced the experimental values within 5%. These results demonstrate that the proposed method provides an efficient, high-precision approach for generating porous structures and supports reliable prediction of material properties. The program called PorousGen is publicly available under the MIT License from <a target="_blank" rel="noopener" href="https://github.com/YoshidomeGroup-Hydration/PorousGen">https://github.com/YoshidomeGroup-Hydration/PorousGen</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆå¤šå­”ç»“æ„çš„æ–°ç®—æ³•ï¼Œä½œä¸ºPoreSpyç¨‹åºå¥—ä»¶çš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ç»å¸¸äº§ç”Ÿå­”éš™åº¦åç¦»ç›®æ ‡å€¼çš„PoreSpyä¸åŒï¼Œæˆ‘ä»¬æå‡ºçš„ç®—æ³•èƒ½å¤Ÿç”Ÿæˆå­”éš™åº¦åœ¨æŒ‡å®šè¾“å…¥å€¼é™„è¿‘ã€åœ¨å®šä¹‰çš„è¯¯å·®èŒƒå›´å†…ç´§å¯†åŒ¹é…çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œå¹¶è¡Œè®¡ç®—ä½¿å¾—èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå¤§è§„æ¨¡ç»“æ„ï¼Œä¸PoreSpyç›¸æ¯”ï¼Œå†…å­˜ä½¿ç”¨ä¹Ÿæœ‰æ‰€å‡å°‘ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œä½¿ç”¨ä¸Xå°„çº¿ptychographyå®éªŒç›¸å¯¹åº”çš„å‚æ•°ï¼Œä½¿ç”¨PoreSpyå’Œæ‰€æå‡ºçš„æ–¹æ³•ç”Ÿæˆäº†ç»“æ„ã€‚PoreSpyä¸­çš„å­”éš™åº¦ä¸åŒ¹é…å¯¼è‡´è®¡ç®—å‡ºçš„æ°”ä½“æ‰©æ•£ç³»æ•°ç›¸å¯¹è¯¯å·®è¶…è¿‡20%ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨5%ä»¥å†…å†ç°äº†å®éªŒå€¼ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ç§é«˜æ•ˆã€é«˜ç²¾åº¦çš„ç”Ÿæˆå¤šå­”ç»“æ„çš„æ–¹æ³•ï¼Œå¹¶æ”¯æŒå¯¹ææ–™å±æ€§è¿›è¡Œå¯é çš„é¢„æµ‹ã€‚è¯¥ç¨‹åºåä¸ºPorousGenï¼Œå¯åœ¨MITè®¸å¯è¯ä¸‹ä»<a target="_blank" rel="noopener" href="https://github.com/YoshidomeGroup-Hydration/PorousGen%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YoshidomeGroup-Hydration/PorousGenå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17133v1">PDF</a> 15 pages, 5 figures</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆå¤šå­”ç»“æ„çš„æ–°ç®—æ³•ï¼Œä½œä¸ºPoreSpyç¨‹åºå¥—ä»¶çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ–°ç®—æ³•è§£å†³äº†PoreSpyäº§ç”Ÿçš„ç»“æ„å­”éš™ç‡ä¸ç›®æ ‡å€¼åå·®çš„é—®é¢˜ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸æŒ‡å®šè¾“å…¥ç´§å¯†åŒ¹é…çš„å­”éš™ç‡ç»“æ„ï¼Œå¹¶åœ¨ä¸€å®šè¯¯å·®èŒƒå›´å†…è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œæ–°ç®—æ³•é‡‡ç”¨å¹¶è¡Œè®¡ç®—ï¼Œæé«˜äº†å¤§è§„æ¨¡ç»“æ„çš„ç”Ÿæˆæ•ˆç‡ï¼Œå¹¶é™ä½äº†å†…å­˜ä½¿ç”¨é‡ã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œæ–°ç®—æ³•åœ¨æ°”ä½“æ‰©æ•£ç³»æ•°è®¡ç®—æ–¹é¢çš„ç²¾åº¦é«˜äºPoreSpyï¼Œèƒ½å¤Ÿå¯é é¢„æµ‹ææ–™å±æ€§ã€‚ç›¸å…³ç¨‹åºPorousGenå·²åœ¨MITè®¸å¯è¯ä¸‹å…¬å¼€ï¼Œå¯ä»<a target="_blank" rel="noopener" href="https://github.com/YoshidomeGroup-Hydration/PorousGen%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YoshidomeGroup-Hydration/PorousGenè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°ç®—æ³•è§£å†³äº†PoreSpyç”Ÿæˆçš„å¤šå­”ç»“æ„å­”éš™ç‡ä¸ç›®æ ‡å€¼åå·®çš„é—®é¢˜ã€‚</li>
<li>æ–°ç®—æ³•èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå¤§è§„æ¨¡ç»“æ„ï¼Œé‡‡ç”¨å¹¶è¡Œè®¡ç®—å¹¶é™ä½å†…å­˜ä½¿ç”¨ã€‚</li>
<li>æ–°ç®—æ³•ç”Ÿæˆçš„å­”éš™ç‡ç»“æ„åœ¨è¯¯å·®èŒƒå›´å†…ä¸æŒ‡å®šè¾“å…¥ç´§å¯†åŒ¹é…ã€‚</li>
<li>å¯¹æ¯”å®éªŒè¡¨æ˜æ–°ç®—æ³•åœ¨æ°”ä½“æ‰©æ•£ç³»æ•°è®¡ç®—æ–¹é¢è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦ã€‚</li>
<li>æ–°ç®—æ³•æ”¯æŒå¯é é¢„æµ‹ææ–™å±æ€§ã€‚</li>
<li>æ–°ç®—æ³•çš„ç¨‹åºPorousGenå·²å…¬å¼€ï¼Œå¯åœ¨GitHubä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4eb7a8ae749dd537e4e7f3b3f082f79" align="middle">
<img src="https://picx.zhimg.com/v2-25eec825383696b115800fcd01323216" align="middle">
<img src="https://picx.zhimg.com/v2-28ababdeb8976f964ed810a82711ac7c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Click-Predict-Trust-Clinician-in-the-Loop-AI-Segmentation-for-Lung-Cancer-CT-Based-Prognosis-within-the-Knowledge-to-Action-Framework"><a href="#Click-Predict-Trust-Clinician-in-the-Loop-AI-Segmentation-for-Lung-Cancer-CT-Based-Prognosis-within-the-Knowledge-to-Action-Framework" class="headerlink" title="Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung   Cancer CT-Based Prognosis within the Knowledge-to-Action Framework"></a>Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung   Cancer CT-Based Prognosis within the Knowledge-to-Action Framework</h2><p><strong>Authors:Mohammad R. Salmanpour, Sonya Falahati, Amir Hossein Pouria, Amin Mousavi, Somayeh Sadat Mehrnia, Morteza Alizadeh, Arman Gorji, Zeinab Farsangi, Alireza Safarian, Mehdi Maghsudi, Carlos Uribe, Arman Rahmim, Ren Yuan</strong></p>
<p>Lung cancer remains the leading cause of cancer mortality, with CT imaging central to screening, prognosis, and treatment. Manual segmentation is variable and time-intensive, while deep learning (DL) offers automation but faces barriers to clinical adoption. Guided by the Knowledge-to-Action framework, this study develops a clinician-in-the-loop DL pipeline to enhance reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data from 999 patients across 12 public datasets were analyzed using five DL models (3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against expert contours on whole and click-point cropped images. Segmentation reproducibility was assessed using 497 PySERA-extracted radiomic features via Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic modeling compared supervised (SL) and semi-supervised learning (SSL) across 38 dimensionality reduction strategies and 24 classifiers. Six physicians qualitatively evaluated masks across seven domains, including clinical meaningfulness, boundary quality, prognostic value, trust, and workflow integration. VNet achieved the best performance (Dice &#x3D; 0.83, IoU &#x3D; 0.71), radiomic stability (mean correlation &#x3D; 0.76, ICC &#x3D; 0.65), and predictive accuracy under SSL (accuracy &#x3D; 0.88, F1 &#x3D; 0.83). SSL consistently outperformed SL across models. Radiologists favored VNet for peritumoral representation and smoother boundaries, preferring AI-generated initial masks for refinement rather than replacement. These results demonstrate that integrating VNet with SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer prognosis, highlighting a feasible path toward physician-centered AI translation. </p>
<blockquote>
<p>è‚ºç™Œä»ç„¶æ˜¯å¯¼è‡´ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨ç­›æŸ¥ã€é¢„åå’Œæ²»ç–—ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚æ‰‹åŠ¨åˆ†å‰²å…·æœ‰å¤šå˜æ€§å’Œè€—æ—¶æ€§ï¼Œè€Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰è™½ç„¶å¯ä»¥å®ç°è‡ªåŠ¨åŒ–ï¼Œä½†ä¸´åºŠé‡‡ç”¨ä»é¢ä¸´éšœç¢ã€‚æœ¬ç ”ç©¶éµå¾ªçŸ¥è¯†åˆ°è¡ŒåŠ¨æ¡†æ¶ï¼Œå¼€å‘äº†ä¸€ç§ä¸´åºŠåŒ»ç”Ÿå‚ä¸çš„æ·±åº¦å­¦ä¹ ç®¡é“ï¼Œä»¥æé«˜å¯é‡å¤æ€§ã€é¢„åå‡†ç¡®æ€§å’Œä¸´åºŠä¿¡ä»»åº¦ã€‚ä½¿ç”¨äº”ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆ3D Attention U-Netã€ResUNetã€VNetã€ReconNetã€SAM-Med3Dï¼‰å¯¹æ¥è‡ª12ä¸ªå…¬å…±æ•°æ®é›†çš„999åæ‚£è€…çš„å¤šä¸­å¿ƒCTæ•°æ®è¿›è¡Œäº†åˆ†æï¼Œä»¥ä¸“å®¶è½®å»“ä¸ºåŸºå‡†ï¼Œå¯¹æ•´ä½“å’Œç‚¹å‡»ç‚¹è£å‰ªå›¾åƒè¿›è¡Œäº†è¯„ä¼°ã€‚ä½¿ç”¨PySERAæå–çš„497ä¸ªæ”¾å°„å­¦ç‰¹å¾é€šè¿‡æ–¯çš®å°”æ›¼ç›¸å…³æ€§ã€ICCã€å¨å°”ç§‘å…‹æ£®æ£€éªŒå’ŒMANOVAè¯„ä¼°äº†åˆ†å‰²çš„å¯é‡å¤æ€§ã€‚é¢„åæ¨¡å‹åœ¨38ç§é™ç»´ç­–ç•¥å’Œ2dç§åˆ†ç±»å™¨ä¸­æ¯”è¾ƒäº†ç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰å’ŒåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ã€‚å…­ä½åŒ»ç”Ÿåœ¨ä¸ƒä¸ªé¢†åŸŸå¯¹é®ç½©è¿›è¡Œäº†å®šæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸´åºŠæ„ä¹‰ã€è¾¹ç•Œè´¨é‡ã€é¢„åä»·å€¼ã€ä¿¡ä»»å’Œå·¥ä½œæµç¨‹é›†æˆã€‚VNetå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ˆDice &#x3D; 0.83ï¼ŒIoU &#x3D; 0.71ï¼‰ï¼Œæ”¾å°„ç¨³å®šæ€§ï¼ˆå¹³å‡ç›¸å…³æ€§&#x3D; 0.76ï¼ŒICC &#x3D; 0.65ï¼‰ï¼Œå¹¶åœ¨SSLä¸‹é¢„æµ‹å‡†ç¡®åº¦è¾ƒé«˜ï¼ˆå‡†ç¡®åº¦&#x3D; 0.88ï¼ŒF1 &#x3D; 0.83ï¼‰ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼ŒSSLå§‹ç»ˆä¼˜äºSLã€‚æ”¾å°„ç§‘åŒ»ç”Ÿé’çäºVNetçš„è‚¿ç˜¤å‘¨å›´è¡¨ç¤ºå’Œæ›´å¹³æ»‘çš„è¾¹ç•Œï¼Œæ›´å–œæ¬¢ä½¿ç”¨AIç”Ÿæˆçš„åˆå§‹é®ç½©è¿›è¡Œç»†åŒ–è€Œä¸æ˜¯æ›¿æ¢ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†VNetä¸SSLç›¸ç»“åˆï¼Œå¯å®ç°å‡†ç¡®ã€å¯å¤åˆ¶å’Œä¸´åºŠä¿¡èµ–çš„åŸºäºCTçš„è‚ºç™Œé¢„åé¢„æµ‹ï¼Œä¸ºä»¥åŒ»ç”Ÿä¸ºä¸­å¿ƒçš„AIç¿»è¯‘æä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17039v1">PDF</a> 13 pages, 2 figures, and 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‚ºç™ŒCTå½±åƒçš„æ·±åº¦å­¦ä¹ åˆ†å‰²ä¸é¢„åæ¨¡å‹ã€‚ç ”ç©¶é‡‡ç”¨å¤šä¸­å¿ƒCTæ•°æ®ï¼Œå¯¹æ¯”äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶é€šè¿‡åŒ»ç”Ÿè¯„ä»·éªŒè¯äº†æ¨¡å‹çš„å®ç”¨æ€§å’Œå¯é æ€§ã€‚æœ€ç»ˆå‘ç°ï¼ŒVNetç»“åˆåŠç›‘ç£å­¦ä¹ åœ¨è‚ºç™ŒCTå½±åƒåˆ†å‰²å’Œé¢„åé¢„æµ‹ä¸­å…·æœ‰æœ€ä½³æ€§èƒ½ï¼Œå¾—åˆ°äº†åŒ»ç”Ÿçš„è®¤å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œä»æ˜¯å¯¼è‡´ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ï¼ŒCTæˆåƒåœ¨ç­›æŸ¥ã€é¢„åå’Œæ²»ç–—ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ä¸ºè‚ºéƒ¨CTå½±åƒçš„è‡ªåŠ¨åˆ†å‰²æä¾›äº†æ–¹æ³•ï¼Œä½†ä¸´åºŠé‡‡çº³å­˜åœ¨éšœç¢ã€‚</li>
<li>ç ”ç©¶é€šè¿‡çŸ¥è¯†è¡ŒåŠ¨æ¡†æ¶å¼•å¯¼ï¼Œå»ºç«‹äº†åŒ»ç”Ÿå‚ä¸çš„æ·±åº¦å­¦ä¹ æµç¨‹ï¼Œæé«˜äº†é‡å¤æ€§ã€é¢„åå‡†ç¡®æ€§å’Œä¸´åºŠä¿¡ä»»åº¦ã€‚</li>
<li>å¤šä¸­å¿ƒCTæ•°æ®åˆ†æå’Œäº”ç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æ¯”è¡¨æ˜ï¼ŒVNetåœ¨å½±åƒåˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>è¯„ä¼°åˆ†å‰²é‡ç°æ€§çš„ç»“æœæ˜¾ç¤ºï¼ŒVNetçš„æ”¾å°„ç»„å­¦ç‰¹å¾ç¨³å®šï¼Œå…·æœ‰è¾ƒé«˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ åœ¨é¢„åå»ºæ¨¡ä¸­è¡¨ç°ä¼˜äºç›‘ç£å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20b67b256fd8ea64531c18f8815a8aa2" align="middle">
<img src="https://picx.zhimg.com/v2-006f6e4fa616bab70c64cea62b7f6525" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ReclAIm-A-multi-agent-framework-for-degradation-aware-performance-tuning-of-medical-imaging-AI"><a href="#ReclAIm-A-multi-agent-framework-for-degradation-aware-performance-tuning-of-medical-imaging-AI" class="headerlink" title="ReclAIm: A multi-agent framework for degradation-aware performance   tuning of medical imaging AI"></a>ReclAIm: A multi-agent framework for degradation-aware performance   tuning of medical imaging AI</h2><p><strong>Authors:Eleftherios Tzanis, Michail E. Klontzas</strong></p>
<p>Ensuring the long-term reliability of AI models in clinical practice requires continuous performance monitoring and corrective actions when degradation occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent framework capable of autonomously monitoring, evaluating, and fine-tuning medical image classification models. The system, built on a large language model core, operates entirely through natural language interaction, eliminating the need for programming expertise. ReclAIm successfully trains, evaluates, and maintains consistent performance of models across MRI, CT, and X-ray datasets. Once ReclAIm detects significant performance degradation, it autonomously executes state-of-the-art fine-tuning procedures that substantially reduce the performance gap. In cases with performance drops of up to -41.1% (MRI InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of the initial model results. ReclAIm enables automated, continuous maintenance of medical imaging AI models in a user-friendly and adaptable manner that facilitates broader adoption in both research and clinical environments. </p>
<blockquote>
<p>ç¡®ä¿äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä¸´åºŠå®è·µä¸­çš„é•¿æœŸå¯é æ€§ï¼Œéœ€è¦è¿›è¡ŒæŒç»­çš„æ€§èƒ½ç›‘æ§ï¼Œå¹¶åœ¨æ€§èƒ½ä¸‹é™æ—¶é‡‡å–çº æ­£æªæ–½ã€‚ä¸ºæ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ReclAImï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªä¸»ç›‘æ§ã€è¯„ä¼°å’Œå¾®è°ƒåŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚è¯¥ç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ ¸å¿ƒæ„å»ºï¼Œå®Œå…¨é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿è¡Œï¼Œæ— éœ€ç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†ã€‚ReclAImæˆåŠŸåœ°åœ¨MRIã€CTå’ŒXå°„çº¿æ•°æ®é›†ä¸Šè®­ç»ƒã€è¯„ä¼°å’Œç»´æŠ¤æ¨¡å‹çš„æ€§èƒ½ä¸€è‡´æ€§ã€‚ä¸€æ—¦ReclAImæ£€æµ‹åˆ°æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå®ƒå°†è‡ªä¸»æ‰§è¡Œæœ€å…ˆè¿›çš„å¾®è°ƒç¨‹åºï¼Œä»è€Œå¤§å¹…åº¦ç¼©å°æ€§èƒ½å·®è·ã€‚åœ¨æ€§èƒ½ä¸‹é™é«˜è¾¾-41.1%ï¼ˆMRI InceptionV3ï¼‰çš„æƒ…å†µä¸‹ï¼ŒReclAImèƒ½å¤Ÿåœ¨åˆå§‹æ¨¡å‹ç»“æœçš„1.5%èŒƒå›´å†…è°ƒæ•´æ€§èƒ½æŒ‡æ ‡ã€‚ReclAImä»¥ç”¨æˆ·å‹å¥½å’Œå¯é€‚åº”çš„æ–¹å¼å®ç°äº†åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½æ¨¡å‹çš„è‡ªåŠ¨åŒ–å’ŒæŒç»­ç»´æŠ¤ï¼Œæœ‰åŠ©äºåœ¨ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­æ›´å¹¿æ³›åœ°é‡‡ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17004v1">PDF</a> 25 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>ReclAImæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªä¸»ç›‘æ§ã€è¯„ä¼°å’Œå¾®è°ƒåŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œä»è€Œæå‡AIæ¨¡å‹åœ¨ä¸´åºŠå®è·µä¸­çš„é•¿æœŸå¯é æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿ä½œï¼Œæ— éœ€ç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†ã€‚å®ƒå¯ä»¥åœ¨MRIã€CTå’ŒX-rayæ•°æ®é›†ä¸ŠæˆåŠŸè®­ç»ƒã€è¯„ä¼°å’Œä¿æŒæ¨¡å‹çš„æ€§èƒ½ä¸€è‡´æ€§ã€‚å½“æ£€æµ‹åˆ°æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™æ—¶ï¼ŒReclAImå¯è‡ªä¸»æ‰§è¡Œå…ˆè¿›çš„å¾®è°ƒç¨‹åºï¼Œå¤§å¹…ç¼©å°æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReclAImæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªä¸»ç›‘æ§ã€è¯„ä¼°å’Œå¾®è°ƒåŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæå‡AIæ¨¡å‹åœ¨ä¸´åºŠå®è·µä¸­çš„é•¿æœŸå¯é æ€§ã€‚</li>
<li>ReclAImé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿ä½œï¼Œæ¶ˆé™¤äº†å¯¹ç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†çš„è¦æ±‚ã€‚</li>
<li>ReclAImå¯ä»¥åœ¨ä¸åŒçš„åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼ˆå¦‚MRIã€CTå’ŒX-rayï¼‰ä¸ŠæˆåŠŸè®­ç»ƒã€è¯„ä¼°å’Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å½“æ£€æµ‹åˆ°æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™æ—¶ï¼ŒReclAImèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¾®è°ƒç¨‹åºã€‚</li>
<li>ReclAImåœ¨æ€§èƒ½ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿç¼©å°æ€§èƒ½å·®è·ã€‚</li>
<li>ReclAImä¿ƒè¿›äº†åŒ»å­¦æˆåƒAIæ¨¡å‹çš„è‡ªåŠ¨åŒ–ã€æŒç»­ç»´æŠ¤ï¼Œä¸ºç”¨æˆ·æä¾›äº†å‹å¥½ã€å¯é€‚åº”çš„æ–¹å¼ï¼Œæœ‰åŠ©äºåœ¨ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­æ›´å¹¿æ³›åœ°é‡‡ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f694d1acaf485fe518b1ebd1964a275" align="middle">
<img src="https://picx.zhimg.com/v2-39634e5f54a5f2d1f53012f7d0123e0d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Class-N-Diff-Classification-Induced-Diffusion-Model-Can-Make-Fair-Skin-Cancer-Diagnosis"><a href="#Class-N-Diff-Classification-Induced-Diffusion-Model-Can-Make-Fair-Skin-Cancer-Diagnosis" class="headerlink" title="Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin   Cancer Diagnosis"></a>Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin   Cancer Diagnosis</h2><p><strong>Authors:Nusrat Munia, Abdullah Imran</strong></p>
<p>Generative models, especially Diffusion Models, have demonstrated remarkable capability in generating high-quality synthetic data, including medical images. However, traditional class-conditioned generative models often struggle to generate images that accurately represent specific medical categories, limiting their usefulness for applications such as skin cancer diagnosis. To address this problem, we propose a classification-induced diffusion model, namely, Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our Class-N-Diff model integrates a classifier within a diffusion model to guide image generation based on its class conditions. Thus, the model has better control over class-conditioned image synthesis, resulting in more realistic and diverse images. Additionally, the classifier demonstrates improved performance, highlighting its effectiveness for downstream diagnostic tasks. This unique integration in our Class-N-Diff makes it a robust tool for enhancing the quality and utility of diffusion model-based synthetic dermoscopic image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Munia03/Class-N-Diff">https://github.com/Munia03/Class-N-Diff</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼ˆåŒ…æ‹¬åŒ»å­¦å›¾åƒï¼‰æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç±»åˆ«æ¡ä»¶ç”Ÿæˆæ¨¡å‹å¾€å¾€éš¾ä»¥ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç‰¹å®šåŒ»å­¦ç±»åˆ«çš„å›¾åƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨çš®è‚¤ç™Œè¯Šæ–­ç­‰åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ç±»è¯±å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå³Class-N-Diffæ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆå’Œåˆ†ç±»çš®è‚¤é•œå›¾åƒã€‚æˆ‘ä»¬çš„Class-N-Diffæ¨¡å‹åœ¨æ‰©æ•£æ¨¡å‹å†…éƒ¨é›†æˆäº†ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæ ¹æ®ç±»åˆ«æ¡ä»¶æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹å¯¹ç±»åˆ«æ¡ä»¶å›¾åƒåˆæˆçš„æ§åˆ¶æ›´å¥½ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´çœŸå®ã€æ›´å¤šæ ·çš„å›¾åƒã€‚æ­¤å¤–ï¼Œåˆ†ç±»å™¨è¿˜å±•ç¤ºäº†å…¶æ”¹è¿›çš„æ€§èƒ½ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§ç‹¬ç‰¹çš„é›†æˆåœ¨æˆ‘ä»¬çš„Class-N-Diffæ¨¡å‹ä¸­ä½¿å…¶æˆä¸ºæé«˜åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆæˆçš®è‚¤é•œå›¾åƒç”Ÿæˆè´¨é‡å’Œå®ç”¨æ€§çš„å¼ºå¤§å·¥å…·ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Munia03/Class-N-Diff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Munia03/Class-N-Diffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16887v1">PDF</a> EMBC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºClass-N-Diffçš„åˆ†ç±»å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åˆ†ç±»å™¨é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æ ¹æ®ç±»åˆ«æ¡ä»¶å¼•å¯¼åŒ»å­¦å›¾åƒç”Ÿæˆã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸä¸”å¤šæ ·åŒ–çš„çš®è‚¤é•œå›¾åƒæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶æé«˜äº†åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºæ‰©æ•£æ¨¡å‹åˆæˆçš„çš®è‚¤é•œå›¾åƒè´¨é‡å’Œå®ç”¨æ€§çš„å¼ºå¤§å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿç±»æ¡ä»¶ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®ä»£è¡¨ç‰¹å®šåŒ»å­¦ç±»åˆ«çš„å›¾åƒæ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>Class-N-Diffæ¨¡å‹é€šè¿‡é›†æˆåˆ†ç±»å™¨åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæ ¹æ®ç±»åˆ«æ¡ä»¶å¼•å¯¼å›¾åƒç”Ÿæˆï¼Œä»è€Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Class-N-Diffæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ§åˆ¶ç±»æ¡ä»¶å›¾åƒåˆæˆï¼Œç”Ÿæˆæ›´é€¼çœŸå’Œå¤šæ ·åŒ–çš„å›¾åƒã€‚</li>
<li>Class-N-Diffä¸­çš„åˆ†ç±»å™¨æ€§èƒ½å¾—åˆ°æå‡ï¼Œæ˜¾ç¤ºå…¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Class-N-Diffæ¨¡å‹çš„ç‹¬ç‰¹é›†æˆä½¿å…¶æˆä¸ºæé«˜åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆæˆçš®è‚¤é•œå›¾åƒè´¨é‡å’Œå®ç”¨æ€§çš„å¼ºå¤§å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3690ad4fce27ce97fbdd9c165631858a" align="middle">
<img src="https://picx.zhimg.com/v2-6a0835f3d437326169861d501e5d5906" align="middle">
<img src="https://picx.zhimg.com/v2-8b02fa6b47e940151bf06d0b4be58de1" align="middle">
<img src="https://picx.zhimg.com/v2-5a01368bfc2488a84123f4d38af0cf9d" align="middle">
<img src="https://picx.zhimg.com/v2-ab2cef416724b8e450ebc65ebf4df56a" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BARL-Bilateral-Alignment-in-Representation-and-Label-Spaces-for-Semi-Supervised-Volumetric-Medical-Image-Segmentation"><a href="#BARL-Bilateral-Alignment-in-Representation-and-Label-Spaces-for-Semi-Supervised-Volumetric-Medical-Image-Segmentation" class="headerlink" title="BARL: Bilateral Alignment in Representation and Label Spaces for   Semi-Supervised Volumetric Medical Image Segmentation"></a>BARL: Bilateral Alignment in Representation and Label Spaces for   Semi-Supervised Volumetric Medical Image Segmentation</h2><p><strong>Authors:Shujian Gao, Yuan Wang, Zekuan Yu</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) seeks to match fully supervised performance while sharply reducing annotation cost. Mainstream SSMIS methods rely on \emph{label-space consistency}, yet they overlook the equally critical \emph{representation-space alignment}. Without harmonizing latent features, models struggle to learn representations that are both discriminative and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment in Representation and Label spaces (BARL)}, a unified framework that couples two collaborative branches and enforces alignment in both spaces. For label-space alignment, inspired by co-training and multi-scale decoding, we devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch consistency while mitigating error accumulation from coarse to fine scales. For representation-space alignment, we conduct region-level and lesion-instance matching between branches, explicitly capturing the fragmented, complex pathological patterns common in medical imagery. Extensive experiments on four public benchmarks and a proprietary CBCT dataset demonstrate that BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies further validate the contribution of each component. Code will be released soon. </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰æ—¨åœ¨åŒ¹é…å…¨ç›‘ç£æ€§èƒ½çš„åŒæ—¶å¤§å¹…åº¦é™ä½æ ‡æ³¨æˆæœ¬ã€‚ä¸»æµSSMISæ–¹æ³•ä¾èµ–äºæ ‡ç­¾ç©ºé—´çš„ä¸€è‡´æ€§ï¼Œå´å¿½è§†äº†åŒæ ·å…³é”®çš„è¡¨ç¤ºç©ºé—´å¯¹é½ã€‚æ²¡æœ‰åè°ƒæ½œåœ¨ç‰¹å¾ï¼Œæ¨¡å‹åœ¨å­¦ä¹ æ—¢æœ‰è¾¨åˆ«åŠ›åˆç©ºé—´è¿è´¯çš„è¡¨ç¤ºæ—¶å€æ„Ÿå›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>è¡¨ç¤ºç©ºé—´å’Œæ ‡ç­¾ç©ºé—´çš„åŒå‘å¯¹é½ï¼ˆBARLï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªååŒåˆ†æ”¯å¹¶åœ¨ä¸¤ä¸ªç©ºé—´å†…å¼ºåˆ¶æ‰§è¡Œå¯¹é½ã€‚å¯¹äºæ ‡ç­¾ç©ºé—´å¯¹é½ï¼Œæˆ‘ä»¬å—åˆ°ååŒè®­ç»ƒå’Œå¤šå°ºåº¦è§£ç çš„å¯å‘ï¼Œè®¾è®¡äº†<strong>åŒè·¯å¾„æ­£åˆ™åŒ–ï¼ˆDPRï¼‰</strong>å’Œ<strong>æ¸è¿›è®¤çŸ¥åå·®æ ¡æ­£ï¼ˆPCBCï¼‰</strong>ï¼Œä»¥åœ¨åˆ†æ”¯ä¹‹é—´å®ç°ç²¾ç»†ç²’åº¦çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶å‡è½»ä»ç²—åˆ°ç»†å°ºåº¦çš„è¯¯å·®ç´¯ç§¯ã€‚å¯¹äºè¡¨ç¤ºç©ºé—´å¯¹é½ï¼Œæˆ‘ä»¬åœ¨åˆ†æ”¯ä¹‹é—´è¿›è¡ŒåŒºåŸŸçº§åˆ«å’Œç—…å˜å®ä¾‹åŒ¹é…ï¼Œæ˜ç¡®æ•æ‰åŒ»å­¦å›¾åƒä¸­å¸¸è§çš„ç¢ç‰‡åŒ–ã€å¤æ‚ç—…ç†æ¨¡å¼ã€‚åœ¨å››ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•å’Œä¸“æœ‰CBCTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBARLå§‹ç»ˆè¶…è¶Šæœ€æ–°SSMISæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚ä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16863v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰æ—¨åœ¨å®ç°ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½æ ‡æ³¨æˆæœ¬ã€‚å½“å‰ä¸»æµæ–¹æ³•ä¾èµ–äºæ ‡ç­¾ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†å¿½ç•¥äº†è¡¨ç¤ºç©ºé—´å¯¹é½åŒæ ·é‡è¦ã€‚æœ¬æ–‡æå‡ºBARLï¼ˆåŒè¾¹å¯¹é½è¡¨ç¤ºå’Œæ ‡ç­¾ç©ºé—´ï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªåä½œåˆ†æ”¯å®ç°ä¸¤ä¸ªç©ºé—´çš„ååŒå¯¹é½ã€‚å¯¹äºæ ‡ç­¾ç©ºé—´å¯¹é½ï¼Œæœ¬æ–‡å—åˆ°ååŒè®­ç»ƒå’Œå¤šå°ºåº¦è§£ç çš„å¯å‘ï¼Œè®¾è®¡äº†åŒè·¯å¾„æ­£åˆ™åŒ–å’Œæ¸è¿›è®¤çŸ¥åå·®æ ¡æ­£ï¼Œä»¥å®ç°ç²¾ç»†è·¨åˆ†æ”¯ä¸€è‡´æ€§å¹¶å‡å°‘ä»ç²—åˆ°ç»†å°ºåº¦çš„è¯¯å·®ç´¯ç§¯ã€‚å¯¹äºè¡¨ç¤ºç©ºé—´å¯¹é½ï¼Œæœ¬æ–‡åœ¨åˆ†æ”¯é—´è¿›è¡ŒåŒºåŸŸçº§åˆ«å’Œç—…å˜å®ä¾‹åŒ¹é…ï¼Œæ˜ç¡®æ•æ‰åŒ»å­¦å›¾åƒä¸­å¸¸è§çš„ç¢ç‰‡åŒ–ã€å¤æ‚ç—…ç†æ¨¡å¼ã€‚åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•å’Œä¸“æœ‰CBCTæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBARLæ€§èƒ½è¶…è¶Šç°æœ‰SSMISæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSMISæ—¨åœ¨å‡å°‘æ ‡æ³¨æˆæœ¬åŒæ—¶è¾¾åˆ°å…¨ç›‘ç£æ€§èƒ½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨æ ‡ç­¾ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†è¡¨ç¤ºç©ºé—´å¯¹é½åŒæ ·é‡è¦ã€‚</li>
<li>BARLæ¡†æ¶å®ç°è¡¨ç¤ºç©ºé—´å’Œæ ‡ç­¾ç©ºé—´çš„åŒè¾¹å¯¹é½ã€‚</li>
<li>å¯¹äºæ ‡ç­¾ç©ºé—´å¯¹é½ï¼Œè®¾è®¡äº†åŒè·¯å¾„æ­£åˆ™åŒ–å’Œæ¸è¿›è®¤çŸ¥åå·®æ ¡æ­£ã€‚</li>
<li>å¯¹äºè¡¨ç¤ºç©ºé—´å¯¹é½ï¼Œè¿›è¡ŒåŒºåŸŸå’Œç—…å˜å®ä¾‹çš„åˆ†æ”¯é—´åŒ¹é…ã€‚</li>
<li>BARLåœ¨å¤šä¸ªå®éªŒä¸­çš„æ€§èƒ½è¶…è¶Šç°æœ‰SSMISæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d8d25dc286025ce99ef77e1c25abcf0" align="middle">
<img src="https://picx.zhimg.com/v2-a65274fdf4b87fe197c85934e4c87f1d" align="middle">
<img src="https://picx.zhimg.com/v2-37d537bad604daa5b5a58529704a9c16" align="middle">
<img src="https://picx.zhimg.com/v2-04ced30c7cf34d02edc62e56afa97cd4" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="EMRRG-Efficient-Fine-Tuning-Pre-trained-X-ray-Mamba-Networks-for-Radiology-Report-Generation"><a href="#EMRRG-Efficient-Fine-Tuning-Pre-trained-X-ray-Mamba-Networks-for-Radiology-Report-Generation" class="headerlink" title="EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for   Radiology Report Generation"></a>EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for   Radiology Report Generation</h2><p><strong>Authors:Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang</strong></p>
<p>X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on <a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis">https://github.com/Event-AHU/Medical_Image_Analysis</a>. </p>
<blockquote>
<p>åŸºäºXå°„çº¿çš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMRGï¼‰æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„ä¸€ä¸ªå…³é”®é¢†åŸŸï¼Œå¯ä»¥æ˜¾è‘—é™ä½åŒ»ç”Ÿçš„è¯Šæ–­è´Ÿæ‹…å’Œæ‚£è€…çš„ç­‰å¾…æ—¶é—´ã€‚ç°æœ‰çš„MRGæ¨¡å‹ä¸»è¦ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¹å–„æŠ¥å‘Šç”Ÿæˆï¼Œå¯¹é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹å’Œé«˜çº§å¾®è°ƒæŠ€æœ¯çš„æ¢ç´¢æœ‰é™ã€‚ä¸»æµæ¡†æ¶è¦ä¹ˆé¿å…å¾®è°ƒï¼Œè¦ä¹ˆä½¿ç”¨ç®€å•çš„å¦‚LoRAç­‰æ–¹æ³•ï¼Œå¾€å¾€å¿½è§†äº†å¢å¼ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„æ½œåŠ›ã€‚è™½ç„¶åŸºäºTransformerçš„æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å ä¸»å¯¼åœ°ä½ï¼Œä½†å¯¹äºåŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¥è¯´ï¼ŒéTransformeræ¶æ„ï¼Œå¦‚Mambaç½‘ç»œï¼Œä»ç„¶è¢«å¿½è§†ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EMRRGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„Xå°„çº¿æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä½¿ç”¨å‚æ•°æœ‰æ•ˆçš„æ–¹æ³•å¯¹é¢„è®­ç»ƒçš„Mambaç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼ŒXå°„çº¿å›¾åƒè¢«åˆ†å‰²æˆè¡¥ä¸ï¼Œè¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶é€šè¿‡SSMä¸ºåŸºç¡€çš„è§†è§‰ä¸»å¹²è¿›è¡Œç‰¹å¾æå–ï¼Œå…¶ä¸­Partial LoRAå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚ä¸€ä¸ªå¸¦æœ‰æ··åˆè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ»å­¦æŠ¥å‘Šï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¼ºå¤§è¡¨ç°ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå……åˆ†éªŒè¯äº†æˆ‘ä»¬ä¸ºXå°„çº¿MRGæå‡ºçš„ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Event-AHU/Medical_Image_Analysisä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16776v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºXå°„çº¿å›¾åƒçš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMRGï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…³é”®æ–¹å‘ï¼Œå¯æ˜¾è‘—å‡å°‘åŒ»ç”Ÿçš„è¯Šæ–­è´Ÿæ‹…å’Œæ‚£è€…çš„ç­‰å¾…æ—¶é—´ã€‚ç°æœ‰MRGæ¨¡å‹ä¸»è¦ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒæŠ¥å‘Šç”Ÿæˆï¼Œå¾ˆå°‘æ¢ç´¢é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹æˆ–é«˜çº§å¾®è°ƒæŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºäº† EMRRGï¼Œä¸€ç§æ–°å‹çš„ X å°„çº¿æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä½¿ç”¨å‚æ•°é«˜æ•ˆæ–¹æ³•å¯¹é¢„è®­ç»ƒçš„ Mamba ç½‘ç»œè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡åˆ†å‰²Xå°„çº¿å›¾åƒæˆè¡¥ä¸ã€ä»¤ç‰ŒåŒ–ï¼Œå¹¶ä½¿ç”¨SSMä¸ºåŸºç¡€çš„è§†è§‰ä¸»å¹²è¿›è¡Œç‰¹å¾æå–ï¼Œç»“åˆéƒ¨åˆ†LoRAæŠ€æœ¯è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚é‡‡ç”¨å¸¦æœ‰æ··åˆè§£ç å™¨çš„LLMç”ŸæˆåŒ»å­¦æŠ¥å‘Šï¼Œå®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œå¼ºç»“æœè¡¨ç°ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯å……åˆ†è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ç­–ç•¥åœ¨Xå°„çº¿MRGä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p> <strong>Key Takeaways</strong></p>
<ol>
<li>Xå°„çº¿å›¾åƒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦åº”ç”¨ï¼Œæœ‰åŠ©äºå‡è½»åŒ»ç”Ÿè¯Šæ–­è´Ÿæ‹…å’Œç¼©çŸ­æ‚£è€…ç­‰å¾…æ—¶é—´ã€‚</li>
<li>å½“å‰MRGæ¨¡å‹ä¸»è¦ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè€Œé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹å’Œé«˜çº§å¾®è°ƒæŠ€æœ¯å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„Xå°„çº¿æŠ¥å‘Šç”Ÿæˆæ¡†æ¶â€”â€”EMRRGï¼Œç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å’ŒMambaç½‘ç»œè¿›è¡ŒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>EMRRGæ¡†æ¶é‡‡ç”¨åˆ†å‰²ã€ä»¤ç‰ŒåŒ–å’Œç‰¹å¾æå–ç­‰æŠ€æœ¯å¤„ç†Xå°„çº¿å›¾åƒï¼Œå¹¶ç»“åˆéƒ¨åˆ†LoRAæŠ€æœ¯è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨å¸¦æœ‰æ··åˆè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ»å­¦æŠ¥å‘Šï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒå’Œè‰¯å¥½çš„ç»“æœè¡¨ç°ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå……åˆ†éªŒè¯äº†EMRRGæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a9bec87506704c7ea3e9d826b7de167" align="middle">
<img src="https://picx.zhimg.com/v2-4915a40d021c76d4d25bd8256d8d7e3b" align="middle">
<img src="https://picx.zhimg.com/v2-5298640b9817da3293f7ea2015254f8a" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Seeing-Through-the-Brain-New-Insights-from-Decoding-Visual-Stimuli-with-fMRI"><a href="#Seeing-Through-the-Brain-New-Insights-from-Decoding-Visual-Stimuli-with-fMRI" class="headerlink" title="Seeing Through the Brain: New Insights from Decoding Visual Stimuli with   fMRI"></a>Seeing Through the Brain: New Insights from Decoding Visual Stimuli with   fMRI</h2><p><strong>Authors:Zheng Huang, Enpei Zhang, Yinghao Cai, Weikang Qiu, Carl Yang, Elynn Chen, Xiang Zhang, Rex Ying, Dawei Zhou, Yujun Yan</strong></p>
<p>Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli, essentially images, from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pretrained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively. We present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision based space or a joint text image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute relationship search module that automatically identifies key attributes and relationships that best align with the neural activity. Extensive experiments on real world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction. </p>
<blockquote>
<p>ç†è§£å¤§è„‘å¦‚ä½•ç¼–ç è§†è§‰ä¿¡æ¯æ˜¯ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•æ˜¯é€šè¿‡åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ä¿¡å·é‡å»ºè§†è§‰åˆºæ¿€ï¼Œæœ¬è´¨ä¸Šæ˜¯å›¾åƒã€‚è¿™æ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šå°†fMRIä¿¡å·è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´ï¼Œç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹æ¥é‡å»ºå›¾åƒã€‚é‡å»ºè´¨é‡å–å†³äºæ½œåœ¨ç©ºé—´ä¸ç¥ç»æ´»åŠ¨ç»“æ„ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠç”Ÿæˆæ¨¡å‹ä»è¯¥ç©ºé—´ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»ä¸æ¸…æ¥šå“ªç§ç±»å‹çš„æ½œåœ¨ç©ºé—´æœ€é€‚åˆæ”¯æŒè¿™ç§è½¬æ¢ï¼Œä»¥åŠå¦‚ä½•ç»„ç»‡å®ƒä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰åˆºæ¿€ã€‚æˆ‘ä»¬æå‡ºä¸¤ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼ŒfMRIä¿¡å·ä¸è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç©ºé—´æ›´ä¸ºç›¸ä¼¼ï¼Œè€Œä¸æ˜¯åŸºäºè§†è§‰çš„ç©ºé—´æˆ–è”åˆæ–‡æœ¬å›¾åƒç©ºé—´ã€‚å…¶æ¬¡ï¼Œæ–‡æœ¬è¡¨ç¤ºå’Œç”Ÿæˆæ¨¡å‹åº”é€‚åº”äºæ•æ‰è§†è§‰åˆºæ¿€çš„ç»„æˆæ€§è´¨ï¼ŒåŒ…æ‹¬ç‰©ä½“ã€å…¶è¯¦ç»†å±æ€§å’Œå…³ç³»ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†PRISMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†fMRIä¿¡å·æŠ•å½±åˆ°ç»“æ„åŒ–æ–‡æœ¬ç©ºé—´ä½œä¸ºè§†è§‰åˆºæ¿€é‡å»ºçš„ä¸­é—´è¡¨ç¤ºã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ‰©æ•£æ¨¡å—ï¼Œé€šè¿‡ç»„åˆå•ä¸ªå¯¹è±¡æ¥ç”Ÿæˆå›¾åƒï¼Œä»¥å‡å°‘å¯¹è±¡æ£€æµ‹é”™è¯¯ï¼›ä»¥åŠä¸€ä¸ªå±æ€§å…³ç³»æœç´¢æ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªåŠ¨è¯†åˆ«ä¸ç¥ç»æ´»åŠ¨æœ€ä½³å¯¹é½çš„å…³é”®å±æ€§å’Œå…³ç³»ã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ„ŸçŸ¥æŸå¤±å‡å°‘äº†é«˜è¾¾8%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä½¿ç”¨ç»“æ„åŒ–æ–‡æœ¬ä½œä¸ºè¿æ¥fMRIä¿¡å·å’Œå›¾åƒé‡å»ºçš„ä¸­é—´ç©ºé—´çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16196v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨å¦‚ä½•ä½¿ç”¨åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ä¿¡å·é‡æ„è§†è§‰åˆºæ¿€å›¾åƒï¼Œæå‡ºäº†ä¸¤é¡¹å…³é”®å‘ç°ã€‚é¦–å…ˆï¼ŒfMRIä¿¡å·ä¸è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç©ºé—´æ›´ä¸ºç›¸ä¼¼ï¼Œè€ŒéåŸºäºè§†è§‰çš„ç©ºé—´æˆ–è”åˆæ–‡æœ¬å›¾åƒç©ºé—´ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æ•æ‰è§†è§‰åˆºæ¿€çš„ç»„æˆæ€§è´¨ï¼ŒåŒ…æ‹¬ç‰©ä½“ã€è¯¦ç»†å±æ€§å’Œå…³ç³»ï¼Œæ–‡æœ¬è¡¨ç¤ºå’Œç”Ÿæˆæ¨¡å‹åº”è¿›è¡Œç›¸åº”çš„è°ƒæ•´ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†PRISMæ¨¡å‹ï¼Œå°†fMRIä¿¡å·æŠ•å½±åˆ°ç»“æ„åŒ–æ–‡æœ¬ç©ºé—´ä½œä¸ºè§†è§‰åˆºæ¿€é‡æ„çš„ä¸­é—´è¡¨ç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé™ä½äº†8%çš„æ„ŸçŸ¥æŸå¤±ã€‚è¿™è¡¨æ˜ä½¿ç”¨ç»“æ„åŒ–æ–‡æœ¬ä½œä¸ºè¿æ¥fMRIä¿¡å·å’Œå›¾åƒé‡æ„çš„ä¸­é—´ç©ºé—´è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰ä¿¡å·é‡æ„è§†è§‰åˆºæ¿€æ˜¯ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>fMRIä¿¡å·ä¸è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç©ºé—´æ›´ä¸ºç›¸ä¼¼ã€‚</li>
<li>æ–‡æœ¬è¡¨ç¤ºå’Œç”Ÿæˆæ¨¡å‹éœ€è¦é€‚åº”æ•æ‰è§†è§‰åˆºæ¿€çš„ç»„æˆæ€§è´¨ï¼ŒåŒ…æ‹¬ç‰©ä½“ã€è¯¦ç»†å±æ€§å’Œå…³ç³»ã€‚</li>
<li>PRISMæ¨¡å‹é€šè¿‡å°†fMRIä¿¡å·æŠ•å½±åˆ°ç»“æ„åŒ–æ–‡æœ¬ç©ºé—´æ¥ä½œä¸ºè§†è§‰åˆºæ¿€é‡æ„çš„ä¸­é—´è¡¨ç¤ºã€‚</li>
<li>PRISMæ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªå¯¹è±¡ä¸­å¿ƒæ‰©æ•£æ¨¡å—ï¼Œé€šè¿‡ç»„åˆå•ä¸ªç‰©ä½“æ¥ç”Ÿæˆå›¾åƒï¼Œä»¥å‡å°‘å¯¹è±¡æ£€æµ‹é”™è¯¯ã€‚</li>
<li>PRISMæ¨¡å‹è¿˜åŒ…å«ä¸€ä¸ªå±æ€§å…³ç³»æœç´¢æ¨¡å—ï¼Œå¯è‡ªåŠ¨è¯†åˆ«ä¸ç¥ç»æ´»åŠ¨æœ€ä½³å¯¹é½çš„å…³é”®å±æ€§å’Œå…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-619bc8621e2fad9d1e6520341e47d5fc" align="middle">
<img src="https://picx.zhimg.com/v2-cffd937069215eda45fce5fc61a74b08" align="middle">
<img src="https://picx.zhimg.com/v2-cdff9a81255d164b1b48093fed810988" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DuetMatch-Harmonizing-Semi-Supervised-Brain-MRI-Segmentation-via-Decoupled-Branch-Optimization"><a href="#DuetMatch-Harmonizing-Semi-Supervised-Brain-MRI-Segmentation-via-Decoupled-Branch-Optimization" class="headerlink" title="DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via   Decoupled Branch Optimization"></a>DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via   Decoupled Branch Optimization</h2><p><strong>Authors:Thanh-Huy Nguyen, Hoang-Thien Nguyen, Vi Vu, Ba-Thinh Lam, Phat Huynh, Tianyang Wang, Xingjian Li, Ulas Bagci, Min Xu</strong></p>
<p>The limited availability of annotated data in medical imaging makes semi-supervised learning increasingly appealing for its ability to learn from imperfect supervision. Recently, teacher-student frameworks have gained popularity for their training benefits and robust performance. However, jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios. To address this for medical image segmentation, we propose DuetMatch, a novel dual-branch semi-supervised framework with asynchronous optimization, where each branch optimizes either the encoder or decoder while keeping the other frozen. To improve consistency under noisy conditions, we introduce Decoupled Dropout Perturbation, enforcing regularization across branches. We also design Pair-wise CutMix Cross-Guidance to enhance model diversity by exchanging pseudo-labels through augmented input pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose Consistency Matching, refining labels using stable predictions from frozen teacher models. Extensive experiments on benchmark brain MRI segmentation datasets, including ISLES2022 and BraTS, show that DuetMatch consistently outperforms state-of-the-art methods, demonstrating its effectiveness and robustness across diverse semi-supervised segmentation scenarios. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œæ ‡æ³¨æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ä½¿å¾—åŠç›‘ç£å­¦ä¹ å› å…¶ä»éå®Œç¾ç›‘ç£ä¸­å­¦ä¹ çš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ€è¿‘ï¼Œå¸ˆå¾’æ¡†æ¶å› å…¶è®­ç»ƒæ•ˆç›Šå’Œç¨³å¥æ€§èƒ½è€Œå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼ŒåŒæ—¶ä¼˜åŒ–æ•´ä¸ªç½‘ç»œå¯èƒ½ä¼šé˜»ç¢æ”¶æ•›å’Œç¨³å®šæ€§ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DuetMatchï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å¼‚æ­¥ä¼˜åŒ–çš„æ–°å‹åŒåˆ†æ”¯åŠç›‘ç£æ¡†æ¶ï¼Œæ¯ä¸ªåˆ†æ”¯å¯ä»¥ä¼˜åŒ–ç¼–ç å™¨æˆ–è§£ç å™¨ï¼ŒåŒæ—¶ä¿æŒå¦ä¸€ä¸ªåˆ†æ”¯å†»ç»“ã€‚ä¸ºäº†æé«˜åœ¨æœ‰å™ªå£°æ¡ä»¶ä¸‹çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å»è€¦dropoutæ‰°åŠ¨ï¼Œåœ¨åˆ†æ”¯ä¹‹é—´å®æ–½æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†Pair-wise CutMix Cross-Guidanceï¼Œé€šè¿‡å¢å¼ºè¾“å…¥å¯¹äº¤æ¢ä¼ªæ ‡ç­¾æ¥å¢å¼ºæ¨¡å‹å¤šæ ·æ€§ã€‚ä¸ºäº†å‡å°‘æ¥è‡ªå™ªå£°ä¼ªæ ‡ç­¾çš„ç¡®è®¤åè§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€è‡´æ€§åŒ¹é…ï¼Œä½¿ç”¨ç¨³å®šçš„é¢„æµ‹ç»“æœå¯¹å†»ç»“çš„æ•™å¸ˆæ¨¡å‹è¿›è¡Œæ ‡ç­¾ç»†åŒ–ã€‚åœ¨ISLES2022å’ŒBraTSç­‰åŸºå‡†å¤§è„‘MRIåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDuetMatchå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§åŠç›‘ç£åˆ†å‰²åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16146v1">PDF</a> The paper is under review at CMIG</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼‚æ­¥ä¼˜åŒ–çš„åŒåˆ†æ”¯åŠç›‘ç£å­¦ä¹ æ¡†æ¶DuetMatchï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–ç¼–ç å™¨æˆ–è§£ç å™¨ä¸­çš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒåŒæ—¶å†»ç»“å¦ä¸€ä¸ªåˆ†æ”¯ï¼Œä»¥æé«˜æ¨¡å‹çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚ä¸ºæé«˜å™ªå£°æ¡ä»¶ä¸‹çš„æ¨¡å‹ä¸€è‡´æ€§ï¼Œå¼•å…¥äº†å»è€¦dropoutæ‰°åŠ¨æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡äº¤æ¢å¢å¼ºè¾“å…¥å¯¹çš„ä¼ªæ ‡ç­¾å¢å¼ºæ¨¡å‹å¤šæ ·æ€§ï¼Œå¹¶è®¾è®¡ä¸€è‡´æ€§åŒ¹é…æŠ€æœ¯ä»¥ç¼“è§£å™ªå£°ä¼ªæ ‡ç­¾å¸¦æ¥çš„ç¡®è®¤åè§ã€‚åœ¨ISLES2022å’ŒBraTSç­‰åŸºå‡†è„‘MRIåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDuetMatchåœ¨å¤šç§åŠç›‘ç£åˆ†å‰²åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒæ ‡æ³¨æ•°æ®æœ‰é™ï¼ŒåŠç›‘ç£å­¦ä¹ å› å…¶èƒ½ä»ä¸å®Œå…¨ç›‘ç£ä¸­å­¦ä¹ è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚</li>
<li>æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶å› å…¶è®­ç»ƒä¼˜åŠ¿å’Œç¨³å¥æ€§èƒ½è€Œå—åˆ°å…³æ³¨ï¼Œä½†æ•´ä½“ç½‘ç»œè”åˆä¼˜åŒ–å¯èƒ½é˜»ç¢æ”¶æ•›å’Œç¨³å®šæ€§ã€‚</li>
<li>å¼•å…¥DuetMatchï¼šä¸€ç§æ–°å‹åŒåˆ†æ”¯åŠç›‘ç£æ¡†æ¶ï¼Œé‡‡ç”¨å¼‚æ­¥ä¼˜åŒ–ï¼Œåˆ†åˆ«ä¼˜åŒ–ç¼–ç å™¨æˆ–è§£ç å™¨ã€‚</li>
<li>ä¸ºæé«˜å™ªå£°æ¡ä»¶ä¸‹çš„æ¨¡å‹ä¸€è‡´æ€§ï¼Œæå‡ºå»è€¦dropoutæ‰°åŠ¨æŠ€æœ¯ã€‚</li>
<li>è®¾è®¡äº†Pair-wise CutMix Cross-GuidanceæŠ€æœ¯ï¼Œé€šè¿‡äº¤æ¢ä¼ªæ ‡ç­¾å¢å¼ºæ¨¡å‹å¤šæ ·æ€§ã€‚</li>
<li>ä¸ºç¼“è§£å™ªå£°ä¼ªæ ‡ç­¾å¸¦æ¥çš„ç¡®è®¤åè§ï¼Œå¼•å…¥ä¸€è‡´æ€§åŒ¹é…æŠ€æœ¯ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDuetMatchåœ¨å¤šç§åŠç›‘ç£åˆ†å‰²åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0af1e10c59c8412ea82c2aceb28c98b" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Identifying-multi-omics-interactions-for-lung-cancer-drug-targets-discovery-using-Kernel-Machine-Regression"><a href="#Identifying-multi-omics-interactions-for-lung-cancer-drug-targets-discovery-using-Kernel-Machine-Regression" class="headerlink" title="Identifying multi-omics interactions for lung cancer drug targets   discovery using Kernel Machine Regression"></a>Identifying multi-omics interactions for lung cancer drug targets   discovery using Kernel Machine Regression</h2><p><strong>Authors:Md. Imtyaz Ahmed, Md. Delwar Hossain, Md Mostafizer Rahman, Md. Ahsan Habib, Md. Mamunur Rashid, Md. Selim Reza, Md Ashad Alam</strong></p>
<p>Cancer exhibits diverse and complex phenotypes driven by multifaceted molecular interactions. Recent biomedical research has emphasized the comprehensive study of such diseases by integrating multi-omics datasets (genome, proteome, transcriptome, epigenome). This approach provides an efficient method for identifying genetic variants associated with cancer and offers a deeper understanding of how the disease develops and spreads. However, it is challenging to comprehend complex interactions among the features of multi-omics datasets compared to single omics. In this paper, we analyze lung cancer multi-omics datasets from The Cancer Genome Atlas (TCGA). Using four statistical methods, LIMMA, the T test, Canonical Correlation Analysis (CCA), and the Wilcoxon test, we identified differentially expressed genes across gene expression, DNA methylation, and miRNA expression data. We then integrated these multi-omics data using the Kernel Machine Regression (KMR) approach. Our findings reveal significant interactions among the three omics: gene expression, miRNA expression, and DNA methylation in lung cancer. From our data analysis, we identified 38 genes significantly associated with lung cancer. From our data analysis, we identified 38 genes significantly associated with lung cancer. Among these, eight genes of highest ranking (PDGFRB, PDGFRA, SNAI1, ID1, FGF11, TNXB, ITGB1, ZIC1) were highlighted by rigorous statistical analysis. Furthermore, in silico studies identified three top-ranked potential candidate drugs (Selinexor, Orapred, and Capmatinib) that could play a crucial role in the treatment of lung cancer. These proposed drugs are also supported by the findings of other independent studies, which underscore their potential efficacy in the fight against lung cancer. </p>
<blockquote>
<p>ç™Œç—‡è¡¨ç°å‡ºå¤šæ ·ä¸”å¤æ‚çš„è¡¨ç°å‹ï¼Œå…¶èƒŒåç”±å¤šå±‚é¢çš„åˆ†å­äº¤äº’é©±åŠ¨ã€‚è¿‘æœŸçš„ç”Ÿç‰©åŒ»å­¦ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡æ•´åˆå¤šç»„å­¦æ•°æ®é›†ï¼ˆåŸºå› ç»„ã€è›‹ç™½è´¨ç»„ã€è½¬å½•ç»„å’Œè¡¨è§‚åŸºå› ç»„ï¼‰æ¥å¯¹è¿™ç±»ç–¾ç—…è¿›è¡Œå…¨é¢ç ”ç©¶çš„é‡è¦æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ºè¯†åˆ«ä¸ç™Œç—‡ç›¸å…³çš„é—ä¼ å˜å¼‚æä¾›äº†ä¸€ç§é«˜æ•ˆæ–¹æ³•ï¼Œå¹¶æä¾›äº†å¯¹ç–¾ç—…å‘å±•å’Œæ‰©æ•£çš„æ›´æ·±å±‚æ¬¡ç†è§£ã€‚ç„¶è€Œï¼Œä¸å•ç»„å­¦ç›¸æ¯”ï¼Œç†è§£å¤šç»„å­¦æ•°æ®ç‰¹å¾ä¹‹é—´çš„å¤æ‚äº¤äº’æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„è‚ºç™Œå¤šç»„å­¦æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨å››ç§ç»Ÿè®¡æ–¹æ³•ï¼ˆLIMMAã€Tæ£€éªŒã€å…¸å‹ç›¸å…³åˆ†æï¼ˆCCAï¼‰å’ŒWilcoxonæ£€éªŒï¼‰æ¥è¯†åˆ«åŸºå› è¡¨è¾¾ã€DNAç”²åŸºåŒ–å’ŒmiRNAè¡¨è¾¾æ•°æ®ä¸­çš„å·®å¼‚è¡¨è¾¾åŸºå› ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ ¸æœºå™¨å›å½’ï¼ˆKMRï¼‰æ–¹æ³•æ•´åˆè¿™äº›å¤šç»„å­¦æ•°æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŸºå› è¡¨è¾¾ã€miRNAè¡¨è¾¾å’ŒDNAç”²åŸºåŒ–ä¸‰è€…ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„äº¤äº’ä½œç”¨ã€‚é€šè¿‡æˆ‘ä»¬çš„æ•°æ®åˆ†æï¼Œæˆ‘ä»¬é‰´å®šäº†ä¸è‚ºç™Œæ˜¾è‘—ç›¸å…³çš„38ä¸ªåŸºå› ã€‚å…¶ä¸­ï¼Œæ’åæœ€é«˜çš„8ä¸ªåŸºå› ï¼ˆPDGFRBã€PDGFRAã€SNAI1ã€ID1ã€FGF11ã€TNXBã€ITGB1ã€ZIC1ï¼‰ç»è¿‡ä¸¥æ ¼çš„ç»Ÿè®¡åˆ†æè¢«çªå‡ºæ˜¾ç¤ºã€‚æ­¤å¤–ï¼Œè®¡ç®—æœºæ¨¡æ‹Ÿç ”ç©¶ç¡®å®šäº†ä¸‰ç§æ’åæœ€é«˜çš„æ½œåœ¨å€™é€‰è¯ç‰©ï¼ˆSelinexorã€Orapredå’ŒCapmatinibï¼‰ï¼Œè¿™äº›è¯ç‰©åœ¨è‚ºç™Œçš„æ²»ç–—ä¸­å¯èƒ½å‘æŒ¥å…³é”®ä½œç”¨ã€‚è¿™äº›æè®®çš„è¯ç‰©ä¹Ÿå¾—åˆ°äº†å…¶ä»–ç‹¬ç«‹ç ”ç©¶çš„æ”¯æŒï¼Œè¿™å¼ºè°ƒäº†å®ƒä»¬åœ¨æŠ—å‡»è‚ºç™Œä¸­çš„æ½œåœ¨ç–—æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16093v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é€šè¿‡åˆ†æè‚ºç™Œçš„å¤šç»„å­¦æ•°æ®é›†ï¼ŒåŒ…æ‹¬åŸºå› è¡¨è¾¾ã€DNAç”²åŸºåŒ–å’ŒmiRNAè¡¨è¾¾æ•°æ®ï¼Œæ­ç¤ºäº†ä¸è‚ºç™Œç›¸å…³çš„åŸºå› äº¤äº’å’Œå˜åŒ–ã€‚åˆ©ç”¨å››ç§ç»Ÿè®¡æ–¹æ³•è¯†åˆ«äº†å·®å¼‚è¡¨è¾¾çš„åŸºå› ï¼Œå¹¶åˆ©ç”¨æ ¸æœºå™¨å›å½’æ–¹æ³•å¯¹è¿™äº›æ•°æ®è¿›è¡Œäº†æ•´åˆã€‚ç ”ç©¶å‘ç°äº†38ä¸ªä¸è‚ºç™Œæ˜¾è‘—ç›¸å…³çš„åŸºå› ï¼Œå¹¶ç¡®å®šäº†å…¶ä¸­8ä¸ªé«˜æ’åçš„åŸºå› ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿç ”ç©¶å‘ç°äº†ä¸‰ç§å¯èƒ½æˆä¸ºè‚ºç™Œæ²»ç–—å€™é€‰è¯ç‰©çš„æ½œåœ¨è¯ç‰©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡å±•ç°å‡ºå¤šæ ·ä¸”å¤æ‚çš„è¡¨å‹ï¼Œè¿™ç”±å¤šé¢çš„åˆ†å­äº¤äº’é©±åŠ¨ã€‚</li>
<li>æ•´åˆå¤šç»„å­¦æ•°æ®é›†ï¼ˆåŸºå› ç»„ã€è›‹ç™½è´¨ç»„ã€è½¬å½•ç»„å’Œè¡¨è§‚åŸºå› ç»„ï¼‰å¯¹äºå…¨é¢ç ”ç©¶æ­¤ç±»ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡åˆ†æè‚ºç™Œçš„å¤šç»„å­¦æ•°æ®é›†ï¼Œå¯ä»¥è¯†åˆ«ä¸ç™Œç—‡ç›¸å…³çš„åŸºå› å˜å¼‚å¹¶æ·±å…¥äº†è§£ç–¾ç—…å‘å±•å’Œä¼ æ’­ã€‚</li>
<li>ä½¿ç”¨LIMMAã€Tæ£€éªŒã€å…¸å‹ç›¸å…³æ€§åˆ†æå’ŒWilcoxonæ£€éªŒç­‰ç»Ÿè®¡æ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«å·®å¼‚è¡¨è¾¾çš„åŸºå› ã€‚</li>
<li>æ ¸æœºå™¨å›å½’æ–¹æ³•æœ‰åŠ©äºæ•´åˆå¤šç»„å­¦æ•°æ®å¹¶æ­ç¤ºåŸºå› è¡¨è¾¾ã€miRNAè¡¨è¾¾å’ŒDNAç”²åŸºåŒ–ä¹‹é—´çš„æ˜¾è‘—äº¤äº’ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†38ä¸ªä¸è‚ºç™Œæ˜¾è‘—ç›¸å…³çš„åŸºå› ï¼Œå…¶ä¸­8ä¸ªåŸºå› è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a4dfab852abc4c3adb12b99aaf82c41" align="middle">
<img src="https://picx.zhimg.com/v2-766c0d564dd102a5184b3d05d0837c5b" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CARDIUM-Congenital-Anomaly-Recognition-with-Diagnostic-Images-and-Unified-Medical-records"><a href="#CARDIUM-Congenital-Anomaly-Recognition-with-Diagnostic-Images-and-Unified-Medical-records" class="headerlink" title="CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and   Unified Medical records"></a>CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and   Unified Medical records</h2><p><strong>Authors:Daniela Vega, Hannah V. Ceballos, Javier S. Vera, Santiago Rodriguez, Alejandra Perez, Angela Castillo, Maria Escobar, Dario LondoÃ±o, Luis A. Sarmiento, Camila I. Castro, Nadiezhda Rodriguez, Juan C. BriceÃ±o, Pablo ArbelÃ¡ez</strong></p>
<p>Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential for Artificial Intelligence (AI)-driven solutions. However, collecting high-quality diagnostic data remains difficult due to the rarity of these conditions, resulting in imbalanced and low-quality datasets that hinder model performance. Moreover, no public efforts have been made to integrate multiple sources of information, such as imaging and clinical data, further limiting the ability of AI models to support and enhance clinical decision-making. To overcome these challenges, we introduce the Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first publicly available multimodal dataset consolidating fetal ultrasound and echocardiographic images along with maternal clinical records for prenatal CHD detection. Furthermore, we propose a robust multimodal transformer architecture that incorporates a cross-attention mechanism to fuse feature representations from image and tabular data, improving CHD detection by 11% and 50% over image and tabular single-modality approaches, respectively, and achieving an F1 score of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset and code to encourage further research on this unexplored field. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/BCV-Uniandes/Cardium">https://github.com/BCV-Uniandes/Cardium</a>, and at the project website <a target="_blank" rel="noopener" href="https://bcv-uniandes.github.io/CardiumPage/">https://bcv-uniandes.github.io/CardiumPage/</a> </p>
<blockquote>
<p>å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…ï¼ˆCHDsï¼‰çš„äº§å‰è¯Šæ–­åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›ç–¾ç—…çš„ç½•è§æ€§ï¼Œæ”¶é›†é«˜è´¨é‡çš„è¯Šæ–­æ•°æ®ä»ç„¶å¾ˆå›°éš¾ï¼Œå¯¼è‡´æ•°æ®é›†ä¸å¹³è¡¡ä¸”è´¨é‡ä½ä¸‹ï¼Œä»è€Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°šæœªæœ‰å…¬å¼€çš„åŠªåŠ›å°è¯•æ•´åˆå¤šç§æ¥æºçš„ä¿¡æ¯ï¼Œå¦‚æˆåƒå’Œä¸´åºŠæ•°æ®ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†AIæ¨¡å‹åœ¨æ”¯æŒå’Œå¢å¼ºä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…ˆå¤©æ€§å¼‚å¸¸è¯†åˆ«ä¸è¯Šæ–­å›¾åƒå’Œç»Ÿä¸€åŒ»ç–—è®°å½•ï¼ˆCARDIUMï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæ•´åˆäº†èƒå„¿è¶…å£°å’Œè¶…å£°å¿ƒåŠ¨å›¾åƒä»¥åŠäº§å¦‡ä¸´åºŠè®°å½•ï¼Œç”¨äºäº§å‰CHDæ£€æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„å¤šæ¨¡å¼è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨äº†äº¤å‰æ³¨æ„æœºåˆ¶æ¥èåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œæé«˜äº†CHDæ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç›¸å¯¹äºå›¾åƒå’Œè¡¨æ ¼å•æ¨¡æ€æ–¹æ³•åˆ†åˆ«æé«˜äº†11%å’Œ50%ï¼Œåœ¨CARDIUMæ•°æ®é›†ä¸Šè¾¾åˆ°äº†79.8Â±4.8%çš„F1åˆ†æ•°ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç ï¼Œä»¥é¼“åŠ±å¯¹æ­¤æœªå¼€å‘é¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/BCV-Uniandes/Cardium%E4%BB%A5%E5%8F%8A%E9%A1%B9%E7%9B%AE%E7%BD%91%E7%AB%99https://bcv-uniandes.github.io/CardiumPage%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/BCV-Uniandes/Cardiumä»¥åŠé¡¹ç›®ç½‘ç«™https://bcv-uniandes.github.io/CardiumPageä¸Šæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15208v2">PDF</a> Accepted to CVAMD Workshop, ICCV 2025</p>
<p><strong>Summary</strong><br>     å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…ï¼ˆCHDï¼‰çš„äº§å‰è¯Šæ–­å…·æœ‰å·¨å¤§çš„åº”ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›ç–¾ç—…çš„ç½•è§æ€§ï¼Œæ”¶é›†é«˜è´¨é‡çš„è¯Šæ–­æ•°æ®ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯¼è‡´æ•°æ®é›†ä¸å¹³è¡¡ä¸”è´¨é‡ä½ä¸‹ï¼Œä»è€Œé˜»ç¢æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°šæœªæœ‰å…¬å¼€çš„åŠªåŠ›æ•´åˆæˆåƒå’Œä¸´åºŠæ•°æ®ç­‰å¤šå…ƒä¿¡æ¯ï¼Œé™åˆ¶äº†AIæ¨¡å‹åœ¨æ”¯æŒå’Œæé«˜ä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…ˆå¤©æ€§å¼‚å¸¸è¯†åˆ«ä¸è¯Šæ–­å›¾åƒå’Œç»Ÿä¸€åŒ»ç–—è®°å½•ï¼ˆCARDIUMï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæ•´åˆäº†èƒå„¿è¶…å£°å’Œè¶…å£°å¿ƒåŠ¨å›¾åƒä»¥åŠå­•å¦‡ä¸´åºŠè®°å½•ç”¨äºäº§å‰CHDæ£€æµ‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç¨³å¥çš„å¤šæ¨¡å¼è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œæé«˜äº†CHDæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…ï¼ˆCHDï¼‰çš„äº§å‰è¯Šæ–­åœ¨åˆ©ç”¨äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ”¶é›†é«˜è´¨é‡çš„è¯Šæ–­æ•°æ®å¯¹äºCHDsçš„AIæ¨¡å‹è®­ç»ƒæ˜¯å›°éš¾çš„ï¼Œå› ä¸ºè¿™äº›ç–¾ç—…çš„ç½•è§æ€§å¯¼è‡´æ•°æ®é›†ä¸å¹³è¡¡ä¸”è´¨é‡ä½ä¸‹ã€‚</li>
<li>ç›®å‰å°šæœªæœ‰å…¬å¼€çš„åŠªåŠ›æ•´åˆå¤šå…ƒä¿¡æ¯ï¼Œå¦‚æˆåƒå’Œä¸´åºŠæ•°æ®ï¼Œä»¥æ”¯æŒAIæ¨¡å‹åœ¨CHDsæ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æ¨å‡ºäº†CARDIUMæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç”¨äºäº§å‰CHDæ£€æµ‹ï¼Œå…¶ä¸­åŒ…æ‹¬èƒå„¿è¶…å£°å’Œè¶…å£°å¿ƒåŠ¨å›¾åƒä»¥åŠå­•å¦‡ä¸´åºŠè®°å½•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨³å¥çš„å¤šæ¨¡å¼è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡èåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºæ¥æé«˜CHDæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨CARDIUMæ•°æ®é›†ä¸Šå®ç°äº†F1åˆ†æ•°ä¸º79.8Â±4.8%çš„æ£€æµ‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-538543c27e38454868afc060741ee680" align="middle">
<img src="https://picx.zhimg.com/v2-50a13677fb759f7da81cb1665ea4e2d6" align="middle">
<img src="https://picx.zhimg.com/v2-79c50f63db9d56f14cf4c0c314a6ab42" align="middle">
<img src="https://picx.zhimg.com/v2-119d3bc2b214ae5dc42ea2c5d58931bf" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Dolphin-v1-0-Technical-Report"><a href="#Dolphin-v1-0-Technical-Report" class="headerlink" title="Dolphin v1.0 Technical Report"></a>Dolphin v1.0 Technical Report</h2><p><strong>Authors:Taohan Weng, Kaibing Hu, Henan Liu, Siya Liu, Xiaoyang Liu, Zhenyu Liu, Jiren Ren, Boyan Wang, Boyang Wang, Yiyu Wang, Yalun Wu, Chaoran Yan, Kaiwen Yan, Jinze Yu, Chi Zhang, Duo Zhang, Haoyun Zheng, Xiaoqing Guo, Jacques Souquet, Hongcheng Guo, Anjie Le</strong></p>
<p>Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasoundâ€™s complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language framework.To tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical adaptability.The Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI. </p>
<blockquote>
<p>è¶…å£°åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€æ“ä½œè€…ä¾èµ–ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ï¼Œé˜»ç¢äº†äººå·¥æ™ºèƒ½çš„æ•´åˆã€‚è™½ç„¶å¤§å‹å¤šæ¨¡å¼æ¨¡å‹åœ¨å…¶ä»–åŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹è¶…å£°çš„å¤æ‚æ€§æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Dolphin v1.0ï¼ˆV1ï¼‰åŠå…¶å¢å¼ºæ¨ç†ç‰ˆæœ¬Dolphin R1â€”â€”è¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€äº†å¤šç§ä¸´åºŠä»»åŠ¡çš„å¤§å‹è¶…å£°å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶å†…ã€‚ä¸ºäº†è§£å†³è¶…å£°å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€ä¸ªè§„æ¨¡è¾¾2ç™¾ä¸‡çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç»“åˆäº†æ•™ç§‘ä¹¦çŸ¥è¯†ã€å…¬å¼€æ•°æ®ã€åˆæˆæ ·æœ¬å’Œä¸€èˆ¬è¯­æ–™åº“ã€‚è¿™ç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚åº”æ€§ã€‚Dolphinç³»åˆ—é‡‡ç”¨äº†ä¸€ç§ä¸‰é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼šé¢†åŸŸä¸“ä¸šåŒ–é¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨å¯¹é½å’ŒåŸºäºå¼ºåŒ–çš„ç²¾ç‚¼ã€‚Dolphin v1.0åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¯é çš„æ€§èƒ½ã€‚Dolphin R1é€šè¿‡å¼ºåŒ–å­¦ä¹ åˆ©ç”¨è¶…å£°ç‰¹å®šå¥–åŠ±å¢å¼ºè¯Šæ–­æ¨ç†ã€æ¨ç†é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚åœ¨U2-Benchä¸Šè¯„ä¼°çš„å…«ä¸ªè¶…å£°ä»»åŠ¡ä¸­ï¼ŒDolphin R1çš„U2åˆ†æ•°ä¸º0.5835ï¼Œæ˜¯ç¬¬äºŒåæ¨¡å‹ï¼ˆ0.2968ï¼‰çš„ä¸¤å€å¤šï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯è®°å½•ã€‚Dolphin v1.0ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å¯¹æ¯”æ˜¾ç¤ºï¼Œå¢å¼ºæ¨ç†è®­ç»ƒæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œè§£é‡Šæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨é«˜é£é™©åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25748v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°æ³¢åœ¨ç°ä»£åŒ»å­¦ä¸­çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†æµ·è±šv1.0åŠå…¶å¢å¼ºç‰ˆæµ·è±šR1ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼è¶…å£°æ³¢åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶å†…èåˆäº†ä¸åŒçš„ä¸´åºŠä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæµ·è±šç³»åˆ—æ¨¡å‹åœ¨åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’ŒæŠ¥å‘Šç”Ÿæˆç­‰æ–¹é¢è¡¨ç°å‡ºå¯é æ€§èƒ½ã€‚å…¶ä¸­ï¼Œæµ·è±šR1åœ¨U2-Benchçš„å…«ä¸ªè¶…å£°æ³¢ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æ³¢åœ¨ç°ä»£åŒ»å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´æ“ä½œä¾èµ–æ€§ã€å›¾åƒå™ªå£°å’Œå®æ—¶æ‰«æç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼å¤§å‹æ¨¡å‹åœ¨å…¶å®ƒåŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹è¶…å£°æ³¢å¤æ‚æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æµ·è±šv1.0åŠå…¶å¢å¼ºç‰ˆæµ·è±šR1è¢«å¼•å…¥ï¼Œä½œä¸ºé¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡å¼è¶…å£°æ³¢åŸºç¡€æ¨¡å‹ï¼Œç»Ÿä¸€äº†ä¸åŒçš„ä¸´åºŠä»»åŠ¡åœ¨ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶å†…ã€‚</li>
<li>ä¸ºäº†è§£å†³è¶…å£°æ³¢çš„å˜æ€§å’Œå™ªå£°é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç¡®ä¿äº†ç¨³å¥çš„æ„ŸçŸ¥ã€æ¦‚æ‹¬å’Œä¸´åºŠé€‚åº”æ€§ã€‚</li>
<li>æµ·è±šç³»åˆ—é‡‡ç”¨äº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢†åŸŸä¸“ä¸šé¢„è®­ç»ƒã€æŒ‡ä»¤é©±åŠ¨å¯¹é½å’Œå¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸Šçš„ç²¾ç‚¼ã€‚</li>
<li>æµ·è±šR1åœ¨U2-Benchçš„å¤šä¸ªè¶…å£°æ³¢ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æˆç»©ï¼Œè¯Šæ–­æ¨æ–­ã€æ¨ç†é€æ˜åº¦å’Œè§£é‡Šæ€§å¾—åˆ°äº†å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05567f9a8552e29de1cca10f07892a23" align="middle">
<img src="https://picx.zhimg.com/v2-93c11d39e4387b5149caa4b31599b313" align="middle">
<img src="https://picx.zhimg.com/v2-df7978ef47d2e0bcfb3d83a16d88e8e9" align="middle">
<img src="https://picx.zhimg.com/v2-fecb80e624417a398dec32a539721780" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12"><a href="#Robust-Pan-Cancer-Mitotic-Figure-Detection-with-YOLOv12" class="headerlink" title="Robust Pan-Cancer Mitotic Figure Detection with YOLOv12"></a>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</h2><p><strong>Authors:RaphaÃ«l Bourgade, Guillaume Balezo, Hana Feki, Lily Monier, Matthieu Blons, Alice Blondel, Delphine Loussouarn, Anne Vincent-Salomon, Thomas Walter</strong></p>
<p>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data. </p>
<blockquote>
<p>æœ‰ä¸åˆ†è£‚å›¾åƒæ˜¯è‚¿ç˜¤ç—…ç†ä¸­çš„å…³é”®ç»„ç»‡é¢„åç‰¹å¾ï¼Œä¸ºç†è§£è‚¿ç˜¤çš„ä¾µè¢­æ€§å’Œå¢æ®–æä¾›äº†é‡è¦ä¾æ®ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿æ˜¯ç»éªŒä¸°å¯Œçš„ç—…ç†å­¦å®¶ä¹‹é—´ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ‰ä¸åˆ†è£‚åŸŸæ³›åŒ–ï¼ˆMIDOGï¼‰2025æŒ‘æˆ˜èµ›æ˜¯å›½é™…ç«èµ›çš„ç¬¬ä¸‰ç‰ˆï¼Œæ—¨åœ¨å¼€å‘ç¨³å¥çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ç®—æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„æœ‰ä¸åˆ†è£‚å›¾åƒæ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæ­¥æµ‹è¯•é›†ï¼ˆä»…çƒ­ç‚¹ï¼‰ä¸Šè¾¾åˆ°äº†0.801çš„F1åˆ†æ•°ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•æ’è¡Œæ¦œä¸Šä»¥0.7216çš„F1åˆ†æ•°æ’åç¬¬äºŒï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ï¼Œå³å¯å¤„ç†å¤æ‚ä¸”éå‡è´¨çš„æ•´å¼ å¹»ç¯ç‰‡åŒºåŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02593v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæœ€æ–°YOLOv12ç›®æ ‡æ£€æµ‹æ¶æ„çš„è‚¿ç˜¤åˆ†è£‚è±¡æ£€æµ‹æ–°æ–¹æ³•ï¼Œå¯¹äºé¢„æµ‹è‚¿ç˜¤ä¾µè¢­æ€§å’Œå¢æ®–ç­‰å…³é”®é¢„åå› ç´ å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ–‡ç« æåˆ°è™½ç„¶ç—…ç†å­¦ä¸­è¯†åˆ«åˆ†è£‚è±¡ä»æ˜¯éš¾ç‚¹ï¼Œä½†å…¶ä»æ˜¯è¯Šæ–­å…³é”®ä¿¡æ¯çš„é‡è¦ä¸€ç¯ã€‚å› æ­¤ï¼ŒMIDOGæŒ‘æˆ˜åº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨å¼€å‘ç¨³å¥çš„åˆ†è£‚è±¡æ£€æµ‹ç®—æ³•ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ¡ˆåœ¨åˆæ­¥æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º0.801ï¼ˆä»…é™äºçƒ­ç‚¹åŒºåŸŸï¼‰ï¼Œå¹¶åœ¨æœ€ç»ˆæµ‹è¯•æ’è¡Œæ¦œä¸Šæ’åç¬¬äºŒï¼Œå…¶å‡†ç¡®ç‡ä¸º0.7216ï¼Œå³ä½¿é¢å¯¹å¤æ‚ä¸”å¼‚è´¨çš„å…¨å¹»ç¯ç‰‡åŒºåŸŸä¹Ÿæ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚è¿™ä¸€å‘ç°æœ‰åŠ©äºæ”¹å–„ç—…ç†å­¦å®¶é—´å› è§‚å¯Ÿè€…å·®å¼‚é€ æˆçš„è¯Šæ–­ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ†è£‚è±¡æ˜¯è‚¿ç˜¤ç—…ç†ä¸­çš„å…³é”®æŒ‡æ ‡ï¼Œæœ‰åŠ©äºäº†è§£è‚¿ç˜¤ä¾µè¢­æ€§å’Œå¢æ®–ã€‚ä½†å…¶è¯†åˆ«éš¾åº¦å¤§ï¼Œä¸åŒè§‚å¯Ÿè€…ä¹‹é—´å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚å› æ­¤å‡ºç°MIDOGæŒ‘æˆ˜æ¨åŠ¨å¯¹ç¨³å¥åˆ†è£‚è±¡æ£€æµ‹ç®—æ³•çš„ç ”ç©¶å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51967db42906f9111ee1dca5b6bb6228" align="middle">
<img src="https://picx.zhimg.com/v2-40ceb8503e1c39aa24ef169cdec97f7f" align="middle">
<img src="https://picx.zhimg.com/v2-75ad294e38596d5fefcb51b365352675" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-435c6a81f9e475d2650eec5f899c026c" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  TrajSelector Harnessing Latent Representations for Efficient and   Effective Best-of-N in Large Reasoning Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-22/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c12977203201bb5db1de6665ae782b3b" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-22  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
