<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  The Impact of Quantization on Large Reasoning Model Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a2ac713a00229992a801ec0a97e2005c')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-21-æ›´æ–°"><a href="#2025-11-21-æ›´æ–°" class="headerlink" title="2025-11-21 æ›´æ–°"></a>2025-11-21 æ›´æ–°</h1><h2 id="The-Impact-of-Quantization-on-Large-Reasoning-Model-Reinforcement-Learning"><a href="#The-Impact-of-Quantization-on-Large-Reasoning-Model-Reinforcement-Learning" class="headerlink" title="The Impact of Quantization on Large Reasoning Model Reinforcement Learning"></a>The Impact of Quantization on Large Reasoning Model Reinforcement Learning</h2><p><strong>Authors:Medha Kumar, Zifei Xu, Xin Wang, Tristan Webb</strong></p>
<p>Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.</p>
<blockquote>
<p>ç°åœ¨ï¼Œé€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ— éœ€ä»»ä½•ç›‘ç£å¾®è°ƒå³å¯å®ç°å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åœ¨å¾®è°ƒé¢†åŸŸå·²ç»å¾—åˆ°äº†å¾ˆå¥½çš„ç ”ç©¶ï¼Œä½†é‡åŒ–å¯¹å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­çš„å¼ºåŒ–å­¦ä¹ çš„å½±å“ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®éªŒï¼Œå‘ç°åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ åçš„é‡åŒ–æ¨¡å‹ä¸å…¶é‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„å¯¹åº”æ¨¡å‹ä¹‹é—´çš„æ¨ç†æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å­¦ä¹ è¿‡ç¨‹äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œè€ŒPTQå’ŒQLoRAåˆ™å¸¦æ¥äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15694v1">PDF</a> Accepted to the NeurIPS 2025 Efficient Reasoning Workshop</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­é€šè¿‡æ— éœ€ç›‘ç£ç²¾ç»†è°ƒæ•´çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ç°åœ¨å¯ä»¥å®ç°ã€‚è™½ç„¶é’ˆå¯¹ç²¾ç»†è°ƒæ•´çš„é‡åŒ–ï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å·²æœ‰å¹¿æ³›ç ”ç©¶ï¼Œä½†é‡åŒ–å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ä»æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ åé‡åŒ–çš„æ¨¡å‹åœ¨æ•°ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†æ€§èƒ½ä¸å…¶ç»è¿‡é‡åŒ–æ„ŸçŸ¥ä¼˜åŒ–çš„æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ç ”ç©¶å‘ç°ï¼Œé‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å­¦ä¹ è¿‡ç¨‹äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œè€ŒPTQå’ŒQLoRAåˆ™å¸¦æ¥äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç°åœ¨å¯ä»¥åœ¨æ²¡æœ‰ç›‘ç£ç²¾ç»†è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹é‡åŒ–åœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨ä¸Šçš„å½±å“ï¼Œä»æœ‰è®¸å¤šæœªè§£é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”äº†å¼ºåŒ–å­¦ä¹ åé‡åŒ–çš„æ¨¡å‹ä¸ç»è¿‡é‡åŒ–æ„ŸçŸ¥ä¼˜åŒ–çš„æ¨¡å‹åœ¨æ•°ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>å‘ç°äº†é‡åŒ–æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹å­¦ä¹ è¿‡ç¨‹çš„è´Ÿé¢å½±å“ã€‚</li>
<li>PTQå’ŒQLoRAæ–¹æ³•èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cabc4655247f6c89c6b997001f28b41" align="middle">
<img src="https://picx.zhimg.com/v2-33dfcf2069fe5e06a885ddb4245dcda7" align="middle">
<img src="https://picx.zhimg.com/v2-877e14f569d71a5497b4f3489aee74a8" align="middle">
<img src="https://picx.zhimg.com/v2-7379419f3fb88b637555f6a503b009d8" align="middle">
<img src="https://picx.zhimg.com/v2-e8a127795c405709f4e15c384149ef8e" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MF-GCN-A-Multi-Frequency-Graph-Convolutional-Network-for-Tri-Modal-Depression-Detection-Using-Eye-Tracking-Facial-and-Acoustic-Features"><a href="#MF-GCN-A-Multi-Frequency-Graph-Convolutional-Network-for-Tri-Modal-Depression-Detection-Using-Eye-Tracking-Facial-and-Acoustic-Features" class="headerlink" title="MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features"></a>MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features</h2><p><strong>Authors:Sejuti Rahman, Swakshar Deb, MD. Sameer Iqbal Chowdhury, MD. Jubair Ahmed Sourov, Mohammad Shamsuddin</strong></p>
<p>Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.</p>
<blockquote>
<p>çœ¼åŠ¨æ•°æ®å¯ä»¥é‡åŒ–æŠ‘éƒç¾¤ä½“ç»å¸¸è§‚å¯Ÿåˆ°çš„å¯¹è´Ÿé¢åˆºæ¿€çš„æ³¨æ„åŠ›åå‘ã€‚éŸ³é¢‘å’Œè§†é¢‘æ•°æ®åˆ™æ•æ‰åˆ°äº†æŠ‘éƒçš„ç‰¹å¾ï¼Œå¦‚æƒ…æ„Ÿå¹³æ·¡å’Œå¿ƒç†è¿åŠ¨è¿Ÿç¼“ã€‚ç»Ÿè®¡éªŒè¯å·²è¯å®è¿™äº›æ•°æ®åœ¨åŒºåˆ†æŠ‘éƒå’ŒéæŠ‘éƒç¾¤ä½“æ—¶å…·æœ‰æ˜¾è‘—çš„è¾¨åˆ«åŠ›ã€‚æˆ‘ä»¬è§£å†³äº†ç°æœ‰åŸºäºå›¾çš„æ¨¡å‹çš„ä¸€ä¸ªå…³é”®é™åˆ¶ï¼Œè¿™äº›æ¨¡å‹ä¸»è¦å…³æ³¨ä½é¢‘ä¿¡æ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šé¢‘ç‡å›¾å·ç§¯ç½‘ç»œï¼ˆMF-GCNï¼‰ã€‚è¯¥æ¡†æ¶ç”±ä¸€ä¸ªæ–°é¢–çš„å¤šé¢‘ç‡æ»¤æ³¢å™¨æ¨¡å—ï¼ˆMFFBMï¼‰ç»„æˆï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåˆ©ç”¨ä½é¢‘å’Œé«˜é¢‘ä¿¡å·ã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶çš„è¯„ä¼°ç»“æœç›¸æ¯”ï¼ŒMF-GCNæŒç»­è¶…è¶ŠåŸºçº¿ã€‚åœ¨äºŒå…ƒï¼ˆæŠ‘éƒå’ŒéæŠ‘éƒï¼‰åˆ†ç±»ä¸­ï¼Œè¯¥æ¨¡å‹çš„æ•æ„Ÿæ€§è¾¾åˆ°0.96ï¼ŒF2åˆ†æ•°ä¸º0.94ã€‚å¯¹äºä¸‰ç±»ï¼ˆæ— æŠ‘éƒã€è½»åº¦è‡³ä¸­åº¦æŠ‘éƒå’Œé‡åº¦æŠ‘éƒï¼‰åˆ†ç±»ä»»åŠ¡ï¼Œæ‰€æå‡ºçš„æ–¹æ³•çš„æ•æ„Ÿæ€§è¾¾åˆ°0.79ï¼Œç‰¹å¼‚æ€§ä¸º0.87ï¼Œå¹¶æ˜¾è‘—è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚ä¸ºäº†éªŒè¯æ¨¡å‹çš„é€šç”¨æ€§ï¼Œè¯¥æ¨¡å‹è¿˜åœ¨ä¸­å›½å¤šæ¨¡å¼æŠ‘éƒè¯­æ–™åº“ï¼ˆCMDCï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ•æ„Ÿæ€§è¾¾åˆ°0.95ï¼ŒF2åˆ†æ•°ä¸º0.96ã€‚è¿™äº›ç»“æœè¯å®ï¼Œæˆ‘ä»¬çš„ä¸‰æ¨¡æ€å¤šé¢‘æ¡†æ¶æœ‰æ•ˆåœ°æ•æ‰äº†è·¨æ¨¡æ€äº¤äº’ï¼Œå¯å®ç°å‡†ç¡®çš„æŠ‘éƒæ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15675v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡åˆ©ç”¨çœ¼åŠ¨è¿½è¸ªæ•°æ®ã€éŸ³è§†é¢‘æ•°æ®ï¼Œé€šè¿‡ç»Ÿè®¡éªŒè¯ï¼Œå¯¹æŠ‘éƒç—‡æ‚£è€…å’Œéæ‚£è€…çš„åŒºåˆ†åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚é’ˆå¯¹ç°æœ‰å›¾æ¨¡å‹å±€é™äºä½é¢‘ä¿¡æ¯çš„ä¸è¶³ï¼Œæå‡ºäº†åŸºäºå¤šé¢‘ä¿¡æ¯çš„å›¾å·ç§¯ç½‘ç»œï¼ˆMF-GCNï¼‰ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬æ–°å‹çš„å¤šé¢‘æ»¤æ³¢å™¨æ¨¡å—ï¼ˆMFFBMï¼‰ï¼Œèƒ½åŒæ—¶åˆ©ç”¨é«˜ä½é¢‘ä¿¡å·ã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶ç›¸æ¯”ï¼ŒMF-GCNåœ¨äºŒå…ƒåˆ†ç±»å’Œä¸‰åˆ†ç±»ä»»åŠ¡ä¸­å‡è¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨ä¸­å›½å¤šæ¨¡æ€æŠ‘éƒè¯­æ–™åº“ï¼ˆCMDCï¼‰ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çœ¼åŠ¨è¿½è¸ªæ•°æ®å’ŒéŸ³è§†é¢‘æ•°æ®åœ¨æŠ‘éƒç—‡è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç»Ÿè®¡éªŒè¯åœ¨åŒºåˆ†æŠ‘éƒç—‡æ‚£è€…å’Œéæ‚£è€…æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰å›¾æ¨¡å‹å±€é™äºä½é¢‘ä¿¡æ¯çš„ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šé¢‘å›¾å·ç§¯ç½‘ç»œï¼ˆMF-GCNï¼‰ã€‚</li>
<li>MF-GCNåŒ…æ‹¬å¤šé¢‘æ»¤æ³¢å™¨æ¨¡å—ï¼ˆMFFBMï¼‰ï¼Œèƒ½åŒæ—¶å¤„ç†é«˜ä½é¢‘ä¿¡å·ã€‚</li>
<li>MF-GCNåœ¨äºŒå…ƒåˆ†ç±»å’Œä¸‰åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸­å›½å¤šæ¨¡æ€æŠ‘éƒè¯­æ–™åº“ï¼ˆCMDCï¼‰ä¸Šçš„éªŒè¯ç»“æœè‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccacc64f2d01486afe0c8b147df98be8" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VisPlay-Self-Evolving-Vision-Language-Models-from-Images"><a href="#VisPlay-Self-Evolving-Vision-Language-Models-from-Images" class="headerlink" title="VisPlay: Self-Evolving Vision-Language Models from Images"></a>VisPlay: Self-Evolving Vision-Language Models from Images</h2><p><strong>Authors:Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang</strong></p>
<p>Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at <a target="_blank" rel="noopener" href="https://bruno686.github.io/VisPlay/">https://bruno686.github.io/VisPlay/</a></p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šæä¾›äº†åŸåˆ™æ€§çš„æ¡†æ¶è¿›è¡Œæ”¹è¿›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ä»»åŠ¡ç‰¹å®šçš„å¯å‘å¼æ–¹æ³•æ¥å®šä¹‰å¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™ä¸¤è€…éƒ½æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬å¼•å…¥äº†VisPlayï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿VLMèƒ½å¤Ÿåˆ©ç”¨å¤§é‡çš„æ— æ ‡ç­¾å›¾åƒæ•°æ®è‡ªä¸»æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚ä»å•ä¸ªåŸºç¡€VLMå¼€å§‹ï¼ŒVisPlayå°†æ¨¡å‹åˆ†é…ç»™ä¸¤ä¸ªäº¤äº’è§’è‰²ï¼šä¸€ä¸ªå—å›¾åƒæ¡ä»¶åˆ¶çº¦çš„æé—®è€…ï¼Œå®ƒåˆ¶å®šå…·æœ‰æŒ‘æˆ˜æ€§ä½†å¯å›ç­”çš„è§†è§‰é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†è€…ï¼Œå®ƒç”Ÿæˆé“¶è´¨ç­”æ¡ˆã€‚è¿™äº›è§’è‰²é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè”åˆè®­ç»ƒï¼Œå®ƒç»“åˆäº†å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±ï¼Œä»¥å¹³è¡¡ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§ä¸é“¶è´¨ç­”æ¡ˆçš„è´¨é‡ã€‚VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¸­çš„æ•ˆç‡éƒ½å¾ˆé«˜ã€‚åœ¨Qwen2.5-VLå’ŒMiMo-VLä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒVisPlayåœ¨è§†è§‰æ¨ç†ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰å‡å°‘ç­‰å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ”¹è¿›ï¼Œå…¶ä¸­åŒ…æ‹¬MM-Vetå’ŒMMMUï¼Œå±•ç¤ºäº†å‘è‡ªæˆ‘è¿›åŒ–çš„å¤šæ¨¡æ€æ™ºèƒ½å‘å±•çš„å¯æ‰©å±•è·¯å¾„ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://bruno686.github.io/VisPlay/%E8%AE%BF%E9%97%AE%E3%80%82">https://bruno686.github.io/VisPlay/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15661v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æå‡æä¾›äº†ç†è®ºæ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLæ–¹æ³•å¸¸å¸¸ä¾èµ–äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ä»»åŠ¡ç‰¹å®šçš„å¯å‘å¼æ–¹æ³•æ¥å®šä¹‰å¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™ä¸¤è€…éƒ½æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬æ¨å‡ºäº†VisPlayï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„RLæ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿å¾—VLMè‡ªä¸»æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„å›¾åƒæ•°æ®ã€‚VisPlayå°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªäº¤äº’è§’è‰²ï¼šå›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…ã€‚é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè¿™ä¸¤ä¸ªè§’è‰²è”åˆè®­ç»ƒï¼Œé€šè¿‡èå…¥å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±æ¥å¹³è¡¡ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§ä¸é“¶ç­”æ¡ˆçš„è´¨é‡ã€‚VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¸­å®ç°äº†æœ‰æ•ˆæ‰©å±•ã€‚åœ¨Qwen2.5-VLå’ŒMiMo-VLä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒVisPlayåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è§†è§‰æ¨ç†ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰å‡å°‘æ–¹é¢çš„ä¸€è‡´æ”¹è¿›ï¼ŒåŒ…æ‹¬MM-Vetå’ŒMMMUï¼Œè¯æ˜äº†è‡ªæˆ‘è¿›åŒ–å¤šæ¨¡å¼æ™ºèƒ½çš„å¯è¡Œè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡æ”¹è¿›æä¾›äº†æ¡†æ¶ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¾èµ–äººå·¥æ ‡æ³¨å’Œå¯å‘å¼æ–¹æ³•ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚</li>
<li>VisPlayæ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°å›¾åƒæ•°æ®æé«˜VLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VisPlayå°†æ¨¡å‹åˆ†ä¸ºå›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…ä¸¤ä¸ªäº¤äº’è§’è‰²ã€‚</li>
<li>VisPlayé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è”åˆè®­ç»ƒè¿™ä¸¤ä¸ªè§’è‰²ï¼Œèå…¥å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±ã€‚</li>
<li>VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¸­å®ç°äº†æœ‰æ•ˆæ‰©å±•ï¼Œæ”¹è¿›äº†è§†è§‰æ¨ç†ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰å‡å°‘æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc45436b748bc6b04f2f4b86e848760c" align="middle">
<img src="https://picx.zhimg.com/v2-09318d2d6c3eb1405c923b633daa2abe" align="middle">
<img src="https://picx.zhimg.com/v2-6558fb071e7121f8b0c8bf6aada8cb2e" align="middle">
<img src="https://picx.zhimg.com/v2-01fe54a40811489f24df683ebc487d9d" align="middle">
<img src="https://picx.zhimg.com/v2-854d170801eba52b7e05b4c57ee34a9a" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-to-Think-and-When-to-Look-Uncertainty-Guided-Lookback"><a href="#When-to-Think-and-When-to-Look-Uncertainty-Guided-Lookback" class="headerlink" title="When to Think and When to Look: Uncertainty-Guided Lookback"></a>When to Think and When to Look: Uncertainty-Guided Lookback</h2><p><strong>Authors:Jing Bi, Filippos Bellos, Junjia Guo, Yayuan Li, Chao Huang,  Yunlong,  Tang, Luchuan Song, Susan Liang,  Zhongfei,  Zhang, Jason J. Corso, Chenliang Xu</strong></p>
<p>Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.</p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„æ€è€ƒï¼ˆå³äº§ç”Ÿæ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾ï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å·²çŸ¥å¯ä»¥æé«˜æ€§èƒ½ï¼Œå¹¶ä¸”æœ€è¿‘åœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¢ç›Šã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›ç»“æœå¾ˆæœ‰å¸Œæœ›ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰å…³äºæ€è€ƒå¦‚ä½•å®é™…å½±å“è§†è§‰æ¨ç†çš„ç³»ç»Ÿæ€§åˆ†æã€‚æˆ‘ä»¬é¦–æ¬¡è¿›è¡Œäº†è¿™æ ·çš„åˆ†æï¼Œå¯¹LVLMsçš„æ€è€ƒè¿›è¡Œäº†å¤§è§„æ¨¡ã€å—æ§çš„æ¯”è¾ƒï¼Œåœ¨å®½æ¾ä»¤ç‰Œé¢„ç®—å’Œå¤šéè§£ç ä¸‹ï¼Œè¯„ä¼°äº†InternVL3.5å’ŒQwen3-VLå®¶æ—ä¸­çš„åä¸ªå˜ä½“åœ¨MMMU-valä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°æ€è€ƒæ›´å¤šå¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼›é•¿é“¾é€šå¸¸ä¼šäº§ç”Ÿå¿½ç•¥å›¾åƒçš„é•¿é”™è¯¯è½¨è¿¹ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†æŒ‡ä»¤æ¨¡å¼ä¸‹è¿è¡Œç›¸åŒçš„æ¨¡å‹è¡¨ç°è¾ƒå·®ã€‚æ›´æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼ŒæŸäº›çŸ­å›é¡¾çŸ­è¯­ï¼ˆæ˜ç¡®æŒ‡å›å›¾åƒï¼‰åœ¨æˆåŠŸè½¨è¿¹ä¸­ä¸°å¯Œå­˜åœ¨ï¼Œå¹¶ä¸æ›´å¥½çš„è§†è§‰å®šä½ç›¸å…³ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ç¡®å®šæ€§å¼•å¯¼å›é¡¾ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯è§£ç çš„ç­–ç•¥ï¼Œå®ƒå°†ä¸ç¡®å®šæ€§ä¿¡å·ä¸è‡ªé€‚åº”å›é¡¾æç¤ºå’Œå¹¿åº¦æœç´¢ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ•´ä½“çš„MMMUæ€§èƒ½ï¼Œåœ¨æ ‡å‡†æ€è€ƒè¾ƒå¼±çš„ç±»åˆ«ä¸­å–å¾—äº†æœ€å¤§çš„æ”¶ç›Šï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªå¼ºå¤§çš„è§£ç åŸºçº¿ï¼Œåœ¨å›ºå®šçš„æ¨¡å‹å®¶æ—å’Œä»¤ç‰Œé¢„ç®—ä¸‹è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¿™ç§è§£ç ç­–ç•¥å…·æœ‰é€šç”¨æ€§ï¼Œåœ¨å¦å¤–äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æŒç»­æ€§çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå¹¿æ³›çš„å¤šæ¨¡å¼å¥—ä»¶å’Œæ•°å­¦å¯¼å‘çš„è§†è§‰æ¨ç†æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15613v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æµ‹è¯•æ—¶çš„æ€è€ƒï¼ˆå³ç”Ÿæˆæ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾ï¼‰å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å½±å“ã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œå‘ç°æ›´å¤šçš„æ€è€ƒå¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼Œé•¿é“¾æ¨ç†æœ‰æ—¶ä¼šå¯¼è‡´é”™è¯¯çš„è½¨è¿¹ï¼Œå¿½ç•¥å›¾åƒå¹¶ä½äºæ ‡å‡†æŒ‡ä»¤æ¨¡å¼ä¸‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¿˜å‘ç°æˆåŠŸçš„æ¨ç†è½¨è¿¹ä¸­å¯Œå«çŸ­å›é¡¾çŸ­è¯­ï¼Œè¿™äº›çŸ­è¯­èƒ½å¤Ÿæ˜ç¡®å¼•ç”¨å›¾åƒã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸ç¡®å®šæ€§å¼•å¯¼å›é¡¾æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç­–ç•¥ï¼Œç»“åˆäº†ä¸ç¡®å®šæ€§ä¿¡å·ä¸è‡ªé€‚åº”å›é¡¾æç¤ºå’Œå¹¿åº¦æœç´¢ã€‚è¯¥æ–¹æ³•æé«˜äº†æ•´ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡å‡†æ€è€ƒèƒ½åŠ›è¾ƒå¼±çš„ç±»åˆ«ä¸­å–å¾—äº†æœ€å¤§çš„æ”¶ç›Šï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªå¼ºå¤§çš„è§£ç åŸºçº¿ï¼Œåœ¨å›ºå®šæ¨¡å‹å®¶æ—å’Œä»¤ç‰Œé¢„ç®—ä¸‹è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥è§£ç ç­–ç•¥åœ¨äº”å¥—é¢å¤–çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸€è‡´ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå¹¿æ³›çš„å¤šæ¨¡å¼å¥—ä»¶å’Œæ•°å­¦è§†è§‰æ¨ç†æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶çš„æ€è€ƒï¼ˆç”Ÿæˆä¸­é—´æ¨ç†é“¾ï¼‰å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ€§èƒ½æœ‰æå‡ä½œç”¨ã€‚</li>
<li>æ›´å¤šçš„æ€è€ƒå¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼Œé•¿é“¾æ¨ç†å¯èƒ½å¯¼è‡´é”™è¯¯çš„è½¨è¿¹ã€‚</li>
<li>æˆåŠŸæ¨ç†è½¨è¿¹ä¸­åŒ…å«çŸ­å›é¡¾çŸ­è¯­ï¼Œè¿™äº›çŸ­è¯­èƒ½å¤Ÿæ˜ç¡®å¼•ç”¨å›¾åƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§£ç ç­–ç•¥â€”â€”ä¸ç¡®å®šæ€§å¼•å¯¼å›é¡¾æ³•ï¼Œç»“åˆäº†ä¸ç¡®å®šæ€§ä¿¡å·ä¸è‡ªé€‚åº”å›é¡¾æç¤ºå’Œå¹¿åº¦æœç´¢ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ•´ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡å‡†æ€è€ƒèƒ½åŠ›è¾ƒå¼±çš„ç±»åˆ«ä¸­å–å¾—äº†æ˜¾è‘—æ”¶ç›Šã€‚</li>
<li>åœ¨å›ºå®šæ¨¡å‹å®¶æ—å’Œä»¤ç‰Œé¢„ç®—ä¸‹ï¼Œè¯¥è§£ç ç­–ç•¥è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªå¼ºå¤§çš„è§£ç åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f521caf0d3ebb1fc81a9be662ad0b50" align="middle">
<img src="https://picx.zhimg.com/v2-51d8b7ded01c7df413fb9726af80ff6a" align="middle">
<img src="https://picx.zhimg.com/v2-3aa75668f0e3cc1845934efb80697cfa" align="middle">
<img src="https://picx.zhimg.com/v2-00e5cfbbff6fed003da11848ba6dba8c" align="middle">
<img src="https://picx.zhimg.com/v2-7f60bc54bf63ad129dd54457d784ec73" align="middle">
<img src="https://picx.zhimg.com/v2-b9be825bd16d6563a6f4aa24b3080d68" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models"><a href="#SRPO-Self-Referential-Policy-Optimization-for-Vision-Language-Action-Models" class="headerlink" title="SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models"></a>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</h2><p><strong>Authors:Senyu Fei, Siyin Wang, Li Ji, Ao Li, Shiduo Zhang, Liming Liu, Jinlong Hou, Jingjing Gong, Xianzhong Zhao, Xipeng Qiu</strong></p>
<p>Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the modelâ€™s own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world modelâ€™s latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPOâ€™s efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.</p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä¸¥é‡ä¾èµ–äºä¸“å®¶æ¼”ç¤ºï¼Œè¿™å¯¼è‡´äº†æ¼”ç¤ºåè§å¹¶é™åˆ¶äº†æ€§èƒ½ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§å…‹æœè¿™äº›é™åˆ¶çš„é‡è¦åè®­ç»ƒç­–ç•¥ï¼Œç„¶è€Œï¼Œå½“å‰çš„VLA-RLæ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼Œéƒ½å—åˆ°å¥–åŠ±ç¨€ç–æ€§çš„ä¸¥é‡å›°æ‰°ã€‚ä¾èµ–äºŒå…ƒæˆåŠŸæŒ‡æ ‡æµªè´¹äº†å¤±è´¥è½¨è¿¹ä¸­çš„å®è´µä¿¡æ¯ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘å‚ç…§ç­–ç•¥ä¼˜åŒ–ï¼ˆSRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„VLA-RLæ¡†æ¶ã€‚SRPOé€šè¿‡åˆ©ç”¨æ¨¡å‹åœ¨å½“å‰è®­ç»ƒæ‰¹æ¬¡å†…ç”Ÿæˆçš„è‡ªèº«æˆåŠŸè½¨è¿¹ä½œä¸ºè‡ªæˆ‘å‚ç…§ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ¼”ç¤ºæˆ–æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºå¤±è´¥çš„å°è¯•åˆ†é…è¿›åº¦å¥–åŠ±ã€‚ä¸€ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯é€šè¿‡ä½¿ç”¨æ½œåœ¨çš„ä¸–ç•Œè¡¨ç¤ºæ¥ç¨³å¥åœ°æµ‹é‡è¡Œä¸ºè¿›åº¦ï¼Œè€Œä¸æ˜¯ä¾èµ–åŸå§‹åƒç´ æˆ–éœ€è¦ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­çš„å‹ç¼©ã€å¯è½¬ç§»ç¼–ç ã€‚è¿™äº›è¡¨ç¤ºè‡ªç„¶åœ°æ•è·äº†ç¯å¢ƒä¸­çš„è¿›åº¦æ¨¡å¼ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®ã€é€šç”¨çš„è½¨è¿¹æ¯”è¾ƒã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°è¯æ˜äº†SRPOçš„é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ä»ç›‘ç£å­¦ä¹ çš„åŸºçº¿å¼€å§‹ï¼ŒæˆåŠŸç‡è¾¾åˆ°48.9%ï¼ŒSRPOä»…åœ¨200æ­¥RLä¸­ä¾¿è¾¾åˆ°äº†99.2%çš„æ–°é‡Œç¨‹ç¢‘å¼æˆåŠŸç‡ï¼Œç›¸å¯¹æé«˜äº†103%ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–çš„ç›‘ç£ã€‚æ­¤å¤–ï¼ŒSRPOæ˜¾ç¤ºå‡ºå·¨å¤§çš„ç¨³å¥æ€§ï¼Œåœ¨LIBERO-PlusåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†167%çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15605v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸï¼ŒVision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠè§£å†³æ–¹æ¡ˆã€‚å°½ç®¡VLAæ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºä¸“å®¶æ¼”ç¤ºï¼Œå¯¼è‡´æ¼”ç¤ºåè§å¹¶é™åˆ¶æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºé‡è¦çš„è®­ç»ƒç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLA-RLæ–¹æ³•ï¼Œå¦‚åŸºäºç¾¤ä½“çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå—åˆ°å¥–åŠ±ç¨€ç–æ€§çš„ä¸¥é‡åˆ¶çº¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„VLA-RLæ¡†æ¶â€”â€”Self-Referential Policy Optimizationï¼ˆSRPOï¼‰ã€‚SRPOé€šè¿‡åˆ©ç”¨æ¨¡å‹åœ¨å½“å‰è®­ç»ƒæ‰¹æ¬¡å†…ç”Ÿæˆçš„è‡ªèº«æˆåŠŸè½¨è¿¹ä½œä¸ºè‡ªæˆ‘å‚ç…§ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ¼”ç¤ºæˆ–æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹çš„éœ€æ±‚ã€‚è¿™ä¸ºå¤±è´¥çš„å°è¯•åˆ†é…äº†è¿›åº¦å¥–åŠ±ã€‚æ­¤å¤–ï¼ŒSRPOè¿˜åˆ©ç”¨ä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ç¼–ç æ¥ç¨³å¥åœ°è¡¡é‡è¡Œä¸ºè¿›åº¦ï¼Œè€Œä¸æ˜¯ä¾èµ–åŸå§‹åƒç´ æˆ–éœ€è¦ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSRPOè¡¨ç°å‡ºäº†é«˜æ•ˆå’Œæœ‰æ•ˆæ€§ï¼Œä»ç›‘ç£å­¦ä¹ çš„åŸºçº¿48.9%æˆåŠŸç‡è·ƒå‡è‡³99.2%ï¼Œä»…ç”¨äº†200æ­¥å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†ç›¸å¯¹103%çš„æå‡ï¼Œä¸”æ— éœ€é¢å¤–çš„ç›‘ç£ã€‚æ­¤å¤–ï¼ŒSRPOåœ¨LIBERO-PlusåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œæ€§èƒ½æå‡167%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸé¢ä¸´ä¾èµ–ä¸“å®¶æ¼”ç¤ºã€æ¼”ç¤ºåè§å’Œæ€§èƒ½é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å…‹æœè¿™äº›é™åˆ¶çš„é‡è¦ç­–ç•¥ï¼Œä½†ç°æœ‰æ–¹æ³•å—åˆ°å¥–åŠ±ç¨€ç–æ€§çš„å›°æ‰°ã€‚</li>
<li>SRPOæ¡†æ¶é€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æˆåŠŸè½¨è¿¹ä½œä¸ºè‡ªæˆ‘å‚ç…§ï¼Œè§£å†³äº†å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚</li>
<li>SRPOä¸ºå¤±è´¥çš„å°è¯•åˆ†é…äº†è¿›åº¦å¥–åŠ±ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>SRPOåˆ©ç”¨ä¸–ç•Œæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ç¼–ç æ¥è¡¡é‡è¡Œä¸ºè¿›åº¦ï¼Œé¿å…ä¾èµ–åŸå§‹åƒç´ æˆ–ç‰¹å®šé¢†åŸŸå¾®è°ƒã€‚</li>
<li>åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSRPOå®ç°äº†ä»åŸºçº¿åˆ°99.2%çš„é«˜æˆåŠŸç‡ï¼Œä»…ç”¨äº†200æ­¥å¼ºåŒ–å­¦ä¹ ï¼Œä¸”æ— éœ€é¢å¤–ç›‘ç£ï¼Œè¡¨ç°å‡ºé«˜æ•ˆå’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bff3671516d40df38afb142d7f1d4b2" align="middle">
<img src="https://picx.zhimg.com/v2-19466ecc598c0da62140c89fedbc240f" align="middle">
<img src="https://picx.zhimg.com/v2-1d1368f17a7330c2ce82df0ae92d295f" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning"><a href="#AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning" class="headerlink" title="AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning"></a>AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning</h2><p><strong>Authors:Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar</strong></p>
<p>With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAARâ€™s effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.</p>
<blockquote>
<p>éšç€è§†é¢‘å†…å®¹çš„æ™®åŠï¼Œå¯¹é•¿è§†é¢‘è¿›è¡Œæœ‰æ•ˆç†è§£å’Œå›ç­”é—®é¢˜å·²æˆä¸ºè®¸å¤šåº”ç”¨é¢†åŸŸçš„æ ¸å¿ƒéœ€æ±‚ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½å·²ç»æå‡ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€å¾®å¦™æŸ¥è¯¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŸ¥è¯¢éœ€è¦å…¨é¢ç†è§£å’Œè¯¦ç»†åˆ†æã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AVATAARï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä»¥åŠé¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ã€‚AVATAARåˆ›å»ºäº†ä¸€ä¸ªæŒä¹…çš„å…¨å±€æ‘˜è¦ï¼Œå¹¶åœ¨åæ€æ¨¡å—å’Œé¢„æ£€ç´¢æ€è€ƒä»£ç†ä¹‹é—´å»ºç«‹äº†åé¦ˆå¾ªç¯ï¼Œå…è®¸ç³»ç»Ÿæ ¹æ®éƒ¨åˆ†ç­”æ¡ˆä¼˜åŒ–å…¶æ£€ç´¢ç­–ç•¥ï¼Œå¹¶æ¨¡æ‹Ÿäººç±»ç±»ä¼¼çš„è¿­ä»£æ¨ç†ã€‚åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸å¯¹äºåŸºçº¿è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ—¶åºæ¨ç†æ–¹é¢å®ç°äº†+5.6%çš„ç›¸å¯¹å¢é•¿ï¼Œåœ¨æŠ€æœ¯æŸ¥è¯¢æ–¹é¢å®ç°äº†+5%çš„å¢é•¿ï¼Œåœ¨åŸºäºä¸»é¢˜çš„é—®é¢˜ä¸­å®ç°äº†+8%çš„å¢é•¿ï¼Œä»¥åŠåœ¨å™äº‹ç†è§£æ–¹é¢å®ç°äº†+8.2%çš„å¢é•¿ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯¹æ•´ä½“æ€§èƒ½åšå‡ºäº†ç§¯æè´¡çŒ®ï¼Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†AVATAARåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼ŒAVATAARä¸ºé•¿è§†é¢‘é—®ç­”ï¼ˆQAï¼‰æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œèåˆäº†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15578v1">PDF</a> Accepted in the 5th IEEE Big Data Workshop on Multimodal AI (MMAI 2025), Dec 8-11, Macau, China, 2025 (Preprint Copy)</p>
<p><strong>Summary</strong></p>
<p>AVATAARæ¡†æ¶ç»“åˆå…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡é¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ï¼Œæé«˜äº†å¯¹é•¿è§†é¢‘çš„ç†è§£ä¸é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå„æ¨¡å—å¯¹æ•´ä½“æ€§èƒ½æœ‰æ­£é¢è´¡çŒ®ï¼Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVATAARæ¡†æ¶æ—¨åœ¨æé«˜é•¿è§†é¢‘å†…å®¹çš„ç†è§£å’Œé—®ç­”èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨ã€‚</li>
<li>é¢ä¸´å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶çš„æŒ‘æˆ˜ï¼Œéœ€è¦å…¨é¢ç†è§£å’Œè¯¦ç»†åˆ†æã€‚</li>
<li>AVATAARç»“åˆå…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œæä¾›æ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥é¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ï¼Œå»ºç«‹åé¦ˆå¾ªç¯ï¼Œä¼˜åŒ–æ£€ç´¢ç­–ç•¥ã€‚</li>
<li>åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬æ—¶åºæ¨ç†ã€æŠ€æœ¯æŸ¥è¯¢ã€ä¸»é¢˜é—®é¢˜å’Œå™äº‹ç†è§£ç­‰æ–¹é¢ã€‚</li>
<li>åé¦ˆå¾ªç¯å¯¹é€‚åº”æ€§è‡³å…³é‡è¦ï¼Œå„æ¨¡å—å¯¹æ•´ä½“æ€§èƒ½æœ‰æ­£é¢è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-311a9875497052a1bd50ff1e71fed034" align="middle">
<img src="https://picx.zhimg.com/v2-b59d9d92b9970764f010808d113fb44c" align="middle">
<img src="https://picx.zhimg.com/v2-69ad59825ca3800326d02e86106aa17c" align="middle">
<img src="https://picx.zhimg.com/v2-bb6665078586548d26d14e0ecef97655" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HSKBenchmark-Modeling-and-Benchmarking-Chinese-Second-Language-Acquisition-in-Large-Language-Models-through-Curriculum-Tuning"><a href="#HSKBenchmark-Modeling-and-Benchmarking-Chinese-Second-Language-Acquisition-in-Large-Language-Models-through-Curriculum-Tuning" class="headerlink" title="HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning"></a>HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning</h2><p><strong>Authors:Qihao Yang, Xuelin Wang, Jiale Chen, Xuelian Dong, Yuxin Hao, Tianyong Hao</strong></p>
<p>Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learnersâ€™ language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/CharlesYang030/HSKB">https://github.com/CharlesYang030/HSKB</a>.</p>
<blockquote>
<p>è¯­è¨€ä¹ å¾—å¯¹äºæ­ç¤ºäººç±»è¯­è¨€æ™ºèƒ½çš„æœ¬è´¨è‡³å…³é‡è¦ï¼Œå¹¶æœ€è¿‘æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯è§£é‡Šæ€§çš„æœ‰å‰é€”çš„è§†è§’ã€‚ç„¶è€Œï¼Œè¿›è¡Œéœ€è¦æ§åˆ¶äººç±»å­¦ä¹ è€…è¯­è¨€è¾“å…¥çš„å®éªŒåœ¨ä¼¦ç†ä¸Šå’Œå®è·µä¸Šéƒ½æ˜¯ä¸å¯è¡Œçš„ã€‚è¿™ç»™è¯­è¨€ä¹ å¾—å»ºæ¨¡çš„éªŒè¯å’Œå¯æ‰©å±•æ€§å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ç¬¬äºŒè¯­è¨€ä¹ å¾—ï¼ˆSLAï¼‰æ–¹é¢ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§å¯æ§å’Œå¯å¤åˆ¶çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä»ç¼ºä¹ç³»ç»ŸåŸºå‡†æ¥æ”¯æŒåˆ†é˜¶æ®µçš„å»ºæ¨¡å’Œè¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HSKBenchmarkï¼Œå®ƒæ˜¯é’ˆå¯¹ä¸­æ–‡SLAçš„å¤§å‹è¯­è¨€æ¨¡å‹åˆ†é˜¶æ®µå»ºæ¨¡å’Œå†™ä½œè¯„ä¼°çš„é¦–ä¸ªåŸºå‡†ã€‚å®ƒæ¶µç›–äº†æ±‰è¯­æ°´å¹³è€ƒè¯•ï¼ˆHSKï¼‰çš„ä¸‰è‡³å…­çº§ï¼ŒåŒ…æ‹¬çœŸå®çš„æ•™ç§‘ä¹¦ã€676ä¸‡ä¸ªä»¤ç‰Œã€1ä¸‡å…­åƒä¸ªåˆæˆæŒ‡ä»¤æ ·æœ¬ã€ä¸‰åä¸ªæµ‹è¯•ä¸»é¢˜ä»¥åŠä¸€ä¸ªåŸºäºè¯­è¨€å­¦çš„è¯„ä¼°ç³»ç»Ÿã€‚ä¸ºäº†æ¨¡æ‹Ÿäººç±»å­¦ä¹ è½¨è¿¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯¾ç¨‹è°ƒæ•´æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»åˆçº§åˆ°é«˜çº§è®­ç»ƒæ¨¡å‹ã€‚è¯„ä¼°ç³»ç»Ÿç”¨äºæ£€æŸ¥åŸºäºçº§åˆ«çš„è¯­æ³•è¦†ç›–ç‡ã€å†™ä½œé”™è¯¯ã€è¯æ±‡å’Œå¥æ³•å¤æ‚æ€§ä»¥åŠæ•´ä½“è¯„åˆ†ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†HSKAgentï¼Œå¯¹1ä¸‡ç¯‡å­¦ä¹ è€…ä½œæ–‡è¿›è¡Œäº†å¾®è°ƒã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHSKBenchmarkä¸ä»…æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†ä¸­æ–‡SLAï¼Œè€Œä¸”ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åŠ¨æ€å†™ä½œè¯„ä¼°çš„å¯é åŸºå‡†ã€‚æˆ‘ä»¬å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ä¸äººç±»é«˜çº§å­¦ä¹ è€…ç›¸å½“çš„å†™ä½œæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºäººç±»èˆ¬çš„å­¦ä¹ ç‰¹æ€§ã€‚HSKBenchmarkã€HSKAgentå’Œæ£€æŸ¥ç‚¹ä½œä¸ºåŸºæœ¬å·¥å…·å’Œèµ„æºï¼Œæœ‰å¯èƒ½ä¸ºæœªæ¥å…³äºè¯­è¨€ä¹ å¾—å»ºæ¨¡å’Œå¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šæ€§çš„ç ”ç©¶é“ºå¹³é“è·¯ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/CharlesYang030/HSKB">https://github.com/CharlesYang030/HSKB</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15574v1">PDF</a> Accepted by AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå¹¶ä»‹ç»äº†HSKBenchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡ç¬¬äºŒè¯­è¨€ä¹ å¾—ï¼ˆSLAï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿè¦†ç›–äº†æ±‰è¯­æ°´å¹³è€ƒè¯•ï¼ˆHSKï¼‰çš„ä¸‰è‡³å…­çº§ï¼ŒåŒ…å«äº†çœŸå®çš„è¯¾æœ¬æ•°æ®ã€åˆæˆçš„æ•™å­¦æ ·æœ¬ã€æµ‹è¯•è¯é¢˜ä»¥åŠåŸºäºè¯­è¨€å­¦çš„è¯„ä¼°ä½“ç³»ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ¨¡æ‹Ÿäººç±»çš„å­¦ä¹ è½¨è¿¹ï¼Œè®ºæ–‡ä¸­è¿˜æå‡ºäº†ä¸€ä¸ªè¯¾ç¨‹è°ƒæ•´æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ ¹æ®ä»åˆçº§åˆ°é«˜çº§çš„ä¸åŒå±‚æ¬¡æ¥è®­ç»ƒæ¨¡å‹ã€‚è¯¥ç ”ç©¶è¿˜æ„å»ºäº†HSKAgentï¼Œä¸€ä¸ªåœ¨10Kå­¦ä¹ è€…ä½œæ–‡ä¸Šç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHSKBenchmarkä¸ä»…æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†ä¸­æ–‡SLAï¼Œè¿˜ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠ¨æ€å†™ä½œè¯„ä¼°æä¾›äº†å¯é çš„åŸºå‡†ã€‚è¯¥ç ”ç©¶çš„æ¨¡å‹å’Œèµ„æºä¸ºæœªæ¥è¯­è¨€ä¹ å¾—å»ºæ¨¡å’ŒLLMsè§£é‡Šæ€§ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HSKBenchmarkæ˜¯é¦–ä¸ªé’ˆå¯¹ä¸­æ–‡ç¬¬äºŒè¯­è¨€ä¹ å¾—ï¼ˆSLAï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é˜¶æ®µæ€§å»ºæ¨¡å’Œå†™ä½œè¯„ä¼°åŸºå‡†ã€‚</li>
<li>è¯¥ç³»ç»Ÿè¦†ç›–HSKä¸‰è‡³å…­çº§ï¼ŒåŒ…å«çœŸå®è¯¾æœ¬æ•°æ®ã€åˆæˆæ•™å­¦æ ·æœ¬ã€æµ‹è¯•è¯é¢˜ä»¥åŠè¯­è¨€åŸºç¡€çš„è¯„ä¼°ä½“ç³»ã€‚</li>
<li>ä¸ºæ¨¡æ‹Ÿäººç±»å­¦ä¹ è½¨è¿¹ï¼Œå¼•å…¥è¯¾ç¨‹è°ƒæ•´æ¡†æ¶ï¼ŒæŒ‰å±‚æ¬¡è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ„å»ºHSKAgentæ¨¡å‹ï¼Œåœ¨10Kå­¦ä¹ è€…ä½œæ–‡ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>HSKBenchmarkæœ‰æ•ˆæ¨¡æ‹Ÿä¸­æ–‡SLAï¼Œå¹¶æä¾›å¯é çš„LLMå†™ä½œè¯„ä¼°åŸºå‡†ã€‚</li>
<li>ç ”ç©¶çš„æ¨¡å‹å’Œèµ„æºä¸ºæœªæ¥è¯­è¨€ä¹ å¾—å»ºæ¨¡å’ŒLLMè§£é‡Šæ€§ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2ac713a00229992a801ec0a97e2005c" align="middle">
<img src="https://picx.zhimg.com/v2-5fb7a6096298202dd9c8bde20c2ec124" align="middle">
<img src="https://picx.zhimg.com/v2-25f259a1e6d718ff28554fe37912fb02" align="middle">
<img src="https://picx.zhimg.com/v2-35b8c6f3cf0a4e373a1f7679c81b9b95" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Computer-Use-Agents-as-Judges-for-Generative-User-Interface"><a href="#Computer-Use-Agents-as-Judges-for-Generative-User-Interface" class="headerlink" title="Computer-Use Agents as Judges for Generative User Interface"></a>Computer-Use Agents as Judges for Generative User Interface</h2><p><strong>Authors:Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou</strong></p>
<p>Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humansâ€“prioritizing aesthetics and usabilityâ€“forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/showlab/AUI">https://github.com/showlab/AUI</a>.</p>
<blockquote>
<p>ç”¨æˆ·ç•Œé¢ä»£ç†ï¼ˆCUAï¼‰æ­£è¶Šæ¥è¶Šèƒ½å¤Ÿé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªä¸»æ“ä½œæ•°å­—ç¯å¢ƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°GUIä»ç„¶ä¸»è¦é¢å‘äººç±»è®¾è®¡â€”â€”ä¼˜å…ˆè€ƒè™‘ç¾è§‚å’Œå¯ç”¨æ€§â€”â€”è¿«ä½¿ä»£ç†é‡‡ç”¨é¢å‘äººç±»çš„ã€å¯¹äºé«˜æ•ˆä»»åŠ¡æ‰§è¡Œä¸å¿…è¦çš„è¡Œä¸ºã€‚åŒæ—¶ï¼Œç¼–ç å¯¼å‘çš„è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å·²ç»æ”¹å˜äº†è‡ªåŠ¨GUIè®¾è®¡ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šä½œä¸ºæ³•å®˜çš„CUAèƒ½å¦ååŠ©Coderè¿›è¡Œè‡ªåŠ¨GUIè®¾è®¡ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AUI-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨è¶Š52ä¸ªåº”ç”¨ç¨‹åºçš„è‡ªåŠ¨GUIå¼€å‘åŸºå‡†æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬ä½¿ç”¨è¯­è¨€æ¨¡å‹åˆæˆäº†1560ä¸ªæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ä»»åŠ¡å¯é æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªéªŒè¯å™¨ï¼Œè¯¥éªŒè¯å™¨å¯ä»¥ç¼–ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ˜¯å¦èƒ½åœ¨å…¶ç¯å¢ƒä¸­æ‰§è¡Œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Coder-CUAåä½œæ¡†æ¶ï¼šCoderå……å½“è®¾è®¡å¸ˆï¼Œç”Ÿæˆå’Œä¿®æ”¹ç½‘ç«™ï¼Œè€ŒCUAå……å½“æ³•å®˜ï¼Œè¯„ä¼°åŠŸèƒ½å¹¶æ”¹è¿›è®¾è®¡ã€‚æˆåŠŸçš„è¡¡é‡æ ‡å‡†ä¸æ˜¯è§†è§‰æ•ˆæœï¼Œè€Œæ˜¯ä»»åŠ¡çš„å¯è§£å†³æ€§å’ŒCUAçš„å¯¼èˆªæˆåŠŸç‡ã€‚ä¸ºäº†å°†CUAçš„åé¦ˆè½¬åŒ–ä¸ºå¯ç”¨çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬è®¾è®¡äº†CUAä»ªè¡¨æ¿ï¼Œå®ƒå°†å¤šæ­¥å¯¼èˆªå†å²å‹ç¼©æˆç®€æ´çš„è§†è§‰æ‘˜è¦ï¼Œä¸ºè¿­ä»£è®¾è®¡æä¾›å¯è§£é‡Šçš„æŒ‡å¯¼ã€‚é€šè¿‡å°†ä»£ç†å®šä½ä¸ºè®¾è®¡å¸ˆå’Œæ³•å®˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°†ç•Œé¢è®¾è®¡è½¬å‘ä»£ç†æœ¬èº«çš„æ•ˆç‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæ ‡å¿—ç€ä»£ç†ä»è¢«åŠ¨ä½¿ç”¨è½¬å‘ä¸»åŠ¨å‚ä¸æ•°å­—ç¯å¢ƒçš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/AUI%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/showlab/AUIè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15567v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://showlab.github.io/AUI">https://showlab.github.io/AUI</a> Github: <a target="_blank" rel="noopener" href="https://github.com/showlab/AUI">https://github.com/showlab/AUI</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è®¾è®¡ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚ç”±äºç°æœ‰çš„GUIè®¾è®¡ä¸»è¦é¢å‘äººç±»ï¼Œå› æ­¤ä¸é€‚åº”CUAçš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œéœ€æ±‚ã€‚éšç€é¢å‘ç¼–ç çš„è¯­è¨€æ¨¡å‹ï¼ˆCoderï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªåŠ¨GUIè®¾è®¡å·²æˆä¸ºå¯èƒ½ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨GUIå¼€å‘åŸºå‡†æµ‹è¯•AUI-Gymï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªCoderä¸CUAååŒå·¥ä½œçš„æ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼ŒCoderè´Ÿè´£è®¾è®¡ç½‘ç«™å¹¶è¿›è¡Œä¿®è®¢ï¼Œè€ŒCUAåˆ™ä½œä¸ºè£åˆ¤è¯„ä¼°åŠŸèƒ½å¹¶æ”¹è¿›è®¾è®¡ã€‚æˆåŠŸè¡¡é‡æ ‡å‡†ä¸åœ¨äºè§†è§‰å¤–è§‚ï¼Œè€Œåœ¨äºä»»åŠ¡è§£å†³èƒ½åŠ›å’ŒCUAå¯¼èˆªæˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†CUAä»ªè¡¨æ¿ï¼Œå°†å¤šæ­¥å¯¼èˆªå†å²å‹ç¼©æˆç®€æ´çš„è§†è§‰æ‘˜è¦ï¼Œä¸ºè¿­ä»£è®¾è®¡æä¾›å¯è§£é‡Šçš„æŒ‡å¯¼ã€‚è¯¥ç ”ç©¶å°†ä»£ç†ä»è¢«åŠ¨ä½¿ç”¨è½¬å‘ç§¯æå‚ä¸æ•°å­—ç¯å¢ƒçš„è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CUAï¼ˆè®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼‰è¶Šæ¥è¶Šèƒ½è‡ªä¸»æ“ä½œæ•°å­—ç¯å¢ƒï¼Œé€šè¿‡GUIè¿›è¡Œäº¤äº’ã€‚</li>
<li>ç°æœ‰çš„GUIè®¾è®¡ä¸»è¦é¢å‘äººç±»ï¼Œå¯¹CUAæ•ˆç‡ä¸é«˜ã€‚</li>
<li>è‡ªåŠ¨GUIè®¾è®¡å·²æˆä¸ºå¯èƒ½ï¼Œå¾—ç›Šäºé¢å‘ç¼–ç çš„è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚</li>
<li>å¼•å…¥AUI-GymåŸºå‡†æµ‹è¯•ï¼Œä¸ºè‡ªåŠ¨GUIå¼€å‘æä¾›è¯„ä¼°å¹³å°ã€‚</li>
<li>æå‡ºCoderä¸CUAååŒå·¥ä½œçš„æ¡†æ¶ï¼Œå…¶ä¸­Coderè´Ÿè´£è®¾è®¡ç½‘ç«™ï¼ŒCUAåˆ™è¯„ä¼°åŠŸèƒ½ã€‚</li>
<li>æˆåŠŸè¡¡é‡æ ‡å‡†åœ¨äºä»»åŠ¡è§£å†³èƒ½åŠ›å’ŒCUAå¯¼èˆªæˆåŠŸç‡ï¼Œè€Œéè§†è§‰å¤–è§‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-613b99dc84dabbffefc0028deb06da60" align="middle">
<img src="https://picx.zhimg.com/v2-02d63a08a294b5ec89c2d9fed1065013" align="middle">
<img src="https://picx.zhimg.com/v2-93c2e434930c476fdc1e34d566290ffb" align="middle">
<img src="https://picx.zhimg.com/v2-b254b27fd076ccb89e304458c419af97" align="middle">
<img src="https://picx.zhimg.com/v2-ab9f6c65d2dfc3c1ec11e996ac1e7dbd" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Meta-Black-Box-Optimization-with-Bi-Space-Landscape-Analysis-and-Dual-Control-Mechanism-for-SAEA"><a href="#Meta-Black-Box-Optimization-with-Bi-Space-Landscape-Analysis-and-Dual-Control-Mechanism-for-SAEA" class="headerlink" title="Meta-Black-Box Optimization with Bi-Space Landscape Analysis and Dual-Control Mechanism for SAEA"></a>Meta-Black-Box Optimization with Bi-Space Landscape Analysis and Dual-Control Mechanism for SAEA</h2><p><strong>Authors:Yukun Du, Haiyue Yu, Xiaotong Xie, Yan Zheng, Lixin Zhan, Yudong Du, Chongshuang Hu, Boxuan Wang, Jiang Jiang</strong></p>
<p>Surrogate-Assisted Evolutionary Algorithms (SAEAs) are widely used for expensive Black-Box Optimization. However, their reliance on rigid, manually designed components such as infill criteria and evolutionary strategies during the search process limits their flexibility across tasks. To address these limitations, we propose Dual-Control Bi-Space Surrogate-Assisted Evolutionary Algorithm (DB-SAEA), a Meta-Black-Box Optimization (MetaBBO) framework tailored for multi-objective problems. DB-SAEA learns a meta-policy that jointly regulates candidate generation and infill criterion selection, enabling dual control. The bi-space Exploratory Landscape Analysis (ELA) module in DB-SAEA adopts an attention-based architecture to capture optimization states from both true and surrogate evaluation spaces, while ensuring scalability across problem dimensions, population sizes, and objectives. Additionally, we integrate TabPFN as the surrogate model for accurate and efficient prediction with uncertainty estimation. The framework is trained via reinforcement learning, leveraging parallel sampling and centralized training to enhance efficiency and transferability across tasks. Experimental results demonstrate that DB-SAEA not only outperforms state-of-the-art baselines across diverse benchmarks, but also exhibits strong zero-shot transfer to unseen tasks with higher-dimensional settings. This work introduces the first MetaBBO framework with dual-level control over SAEAs and a bi-space ELA that captures surrogate model information.</p>
<blockquote>
<p>ä»£ç†è¾…åŠ©è¿›åŒ–ç®—æ³•ï¼ˆSAEAsï¼‰å¹¿æ³›åº”ç”¨äºæ˜‚è´µçš„é»‘ç®±ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœç´¢è¿‡ç¨‹ä¸­ä¾èµ–åˆšæ€§ã€æ‰‹åŠ¨è®¾è®¡çš„ç»„ä»¶ï¼Œå¦‚å¡«å……æ ‡å‡†å’Œè¿›åŒ–ç­–ç•¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä»»åŠ¡ä¸­çš„çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ§åˆ¶åŒç©ºé—´ä»£ç†è¾…åŠ©è¿›åŒ–ç®—æ³•ï¼ˆDB-SAEAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šç›®æ ‡é—®é¢˜çš„å…ƒé»‘ç®±ä¼˜åŒ–ï¼ˆMetaBBOï¼‰æ¡†æ¶ã€‚DB-SAEAå­¦ä¹ ä¸€ç§å…ƒç­–ç•¥ï¼Œè”åˆè°ƒæ§å€™é€‰ç”Ÿæˆå’Œå¡«å……æ ‡å‡†é€‰æ‹©ï¼Œå®ç°åŒé‡æ§åˆ¶ã€‚DB-SAEAä¸­çš„åŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†æï¼ˆELAï¼‰æ¨¡å—é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ï¼Œä»çœŸå®å’Œä»£ç†è¯„ä¼°ç©ºé—´ä¸­æ•è·ä¼˜åŒ–çŠ¶æ€ï¼ŒåŒæ—¶ç¡®ä¿åœ¨é—®é¢˜ç»´åº¦ã€ç§ç¾¤å¤§å°å’Œç›®æ ‡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´åˆäº†TabPFNä½œä¸ºä»£ç†æ¨¡å‹ï¼Œè¿›è¡Œå‡†ç¡®é«˜æ•ˆçš„é¢„æµ‹ï¼ŒåŒæ—¶ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨å¹¶è¡Œé‡‡æ ·å’Œé›†ä¸­è®­ç»ƒæé«˜æ•ˆç‡å’Œè·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDB-SAEAä¸ä»…åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°åŸºçº¿ï¼Œè€Œä¸”åœ¨å¯¹é«˜ç»´åº¦è®¾ç½®çš„æœªè§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶é•œå¤´è¿ç§»èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†å…·æœ‰å¯¹SAEAsçš„åŒé‡çº§åˆ«æ§åˆ¶å’Œæ•è·ä»£ç†æ¨¡å‹ä¿¡æ¯çš„åŒç©ºé—´ELAçš„MetaBBOæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSurrogate-Assisted Evolutionary Algorithmsï¼ˆSAEAï¼‰åœ¨å¤„ç†æ˜‚è´µé»‘ç®±ä¼˜åŒ–ä¸­çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Dual-control Bi-Space Surrogate-Assisted Evolutionary Algorithmï¼ˆDB-SAEAï¼‰ã€‚DB-SAEAæ˜¯ä¸€ç§é’ˆå¯¹å¤šç›®æ ‡é—®é¢˜çš„Meta-Black-Box Optimizationï¼ˆMetaBBOï¼‰æ¡†æ¶ï¼Œé€šè¿‡è”åˆè°ƒæ§å€™é€‰ç”Ÿæˆå’Œå¡«å……æ ‡å‡†é€‰æ‹©æ¥å®ç°åŒé‡æ§åˆ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªåŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†ææ¨¡å—ï¼Œå¹¶é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„æ¥æ•æ‰çœŸå®å’Œæ›¿ä»£è¯„ä¼°ç©ºé—´ä¸­çš„ä¼˜åŒ–çŠ¶æ€ã€‚æ­¤å¤–ï¼Œè¿˜é›†æˆäº†TabPFNä½œä¸ºæ›¿ä»£æ¨¡å‹è¿›è¡Œå‡†ç¡®ä¸”é«˜æ•ˆçš„é¢„æµ‹ï¼Œå¹¶è¯„ä¼°ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDB-SAEAä¸ä»…åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨æ›´é«˜ç»´åº¦çš„æœªè§è¿‡ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶è½¬ç§»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DB-SAEAæ˜¯é¦–ä¸ªç»“åˆSurrogate-Assisted Evolutionary Algorithmsï¼ˆSAEAï¼‰è¿›è¡ŒåŒé‡æ§åˆ¶çš„Meta-Black-Box Optimizationï¼ˆMetaBBOï¼‰æ¡†æ¶ã€‚</li>
<li>DB-SAEAè§£å†³äº†ä¼ ç»ŸSAEAåœ¨ä»»åŠ¡çµæ´»æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†ææ¨¡å—ï¼Œç»“åˆçœŸå®å’Œæ›¿ä»£è¯„ä¼°ç©ºé—´è¿›è¡Œæ›´æœ‰æ•ˆçš„ä¼˜åŒ–çŠ¶æ€æ•æ‰ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ç¡®ä¿æ¨¡å—çš„å¯æ‰©å±•æ€§ï¼Œé€‚åº”ä¸åŒçš„é—®é¢˜ç»´åº¦ã€ç§ç¾¤å¤§å°å’Œç›®æ ‡å‡†åˆ™ã€‚</li>
<li>é›†æˆTabPFNä½œä¸ºæ›¿ä»£æ¨¡å‹ï¼Œå®ç°å‡†ç¡®ä¸”é«˜æ•ˆçš„é¢„æµ‹ï¼Œå¹¶åŒ…å«ä¸ç¡®å®šæ€§è¯„ä¼°ã€‚</li>
<li>DB-SAEAé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨å¹¶è¡Œé‡‡æ ·å’Œé›†ä¸­è®­ç»ƒæé«˜æ•ˆç‡å’Œè·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b478c5087cad57aeade596aa0e0ee8a" align="middle">
<img src="https://picx.zhimg.com/v2-6b88c7975f129b22d99393b0a35b25b5" align="middle">
<img src="https://picx.zhimg.com/v2-d4c8176e3ef501b573b945129fb1dbcb" align="middle">
<img src="https://picx.zhimg.com/v2-b9f035bafb8185adcff73066c2b07dbb" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLM-MemCluster-Empowering-Large-Language-Models-with-Dynamic-Memory-for-Text-Clustering"><a href="#LLM-MemCluster-Empowering-Large-Language-Models-with-Dynamic-Memory-for-Text-Clustering" class="headerlink" title="LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering"></a>LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering</h2><p><strong>Authors:Yuanjie Zhu, Liangwei Yang, Ke Xu, Weizhi Zhang, Zihe Song, Jindong Wang, Philip S. Yu</strong></p>
<p>Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åŸºäºæ·±åº¦è¯­ä¹‰ç†è§£çš„æ–‡æœ¬èšç±»èƒ½åŠ›ï¼Œæ­£åœ¨é‡å¡‘æ— ç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œå…¶ç›´æ¥åº”ç”¨å—åˆ°ç¼ºä¹è¿­ä»£ä¼˜åŒ–çš„çŠ¶æ€è®°å¿†å’Œéš¾ä»¥ç®¡ç†èšç±»ç²’åº¦ç­‰æ ¹æœ¬é™åˆ¶ã€‚å› æ­¤ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤–éƒ¨æ¨¡å—çš„å¤æ‚æµæ°´çº¿ï¼Œç‰ºç‰²äº†çœŸæ­£çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†LLM-MemClusterï¼Œè¿™æ˜¯ä¸€ä¸ªé‡æ–°å°†èšç±»å®šä¹‰ä¸ºå®Œå…¨ç”±LLMé©±åŠ¨çš„ä»»åŠ¡çš„æ–°æ¡†æ¶ã€‚å®ƒåˆ©ç”¨åŠ¨æ€å†…å­˜æ¥åŸ¹å…»çŠ¶æ€æ„è¯†ï¼Œå¹¶åˆ©ç”¨åŒæç¤ºç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¹¶ç¡®å®šèšç±»æ•°é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ— éœ€è°ƒæ•´å‚æ•°çš„æ¡†æ¶æ˜¾è‘—ä¸”æŒç»­ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚LLM-MemClusterä¸ºåŸºäºLLMçš„æ–‡æœ¬èšç±»æä¾›äº†ä¸€ç§æœ‰æ•ˆã€å¯è§£é‡Šå’ŒçœŸæ­£çš„ç«¯åˆ°ç«¯èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15424v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ·±åº¦è¯­ä¹‰ç†è§£è¿›è¡Œæ–‡æœ¬èšç±»ï¼Œé‡å¡‘äº†æ— ç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘è¿­ä»£ä¼˜åŒ–çš„çŠ¶æ€è®°å¿†å’Œéš¾ä»¥ç®¡ç†èšç±»ç²’åº¦ï¼Œå…¶ç›´æ¥åº”ç”¨å­˜åœ¨æ ¹æœ¬å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºLLM-MemClusteræ¡†æ¶ï¼Œé‡æ–°å®šä¹‰èšç±»ä¸ºå®Œå…¨çš„LLMåŸç”Ÿä»»åŠ¡ã€‚å®ƒåˆ©ç”¨åŠ¨æ€å†…å­˜å®ç°çŠ¶æ€æ„ŸçŸ¥ï¼Œå¹¶ç”¨åŒæç¤ºç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¹¶ç¡®å®šèšç±»æ•°é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ— è°ƒæ•´æ¡†æ¶æ˜¾è‘—ä¸”ä¸€è‡´åœ°ä¼˜äºå¼ºåŸºçº¿ã€‚LLM-MemClusterä¸ºåŸºäºLLMçš„æ–‡æœ¬èšç±»æä¾›äº†æœ‰æ•ˆã€å¯è§£é‡Šå’ŒçœŸæ­£çš„ç«¯åˆ°ç«¯èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡æ·±åº¦è¯­ä¹‰ç†è§£é‡å¡‘äº†æ— ç›‘ç£å­¦ä¹ ä¸­çš„æ–‡æœ¬èšç±»ã€‚</li>
<li>LLMsç›´æ¥åº”ç”¨äºæ–‡æœ¬èšç±»å­˜åœ¨æ ¹æœ¬å±€é™æ€§ï¼Œå¦‚ç¼ºä¹çŠ¶æ€è®°å¿†å’Œéš¾ä»¥ç®¡ç†èšç±»ç²’åº¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–å¤æ‚çš„ç®¡é“å’Œå¤–éƒ¨æ¨¡å—ï¼Œç‰ºç‰²äº†çœŸæ­£çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚</li>
<li>å¼•å…¥LLM-MemClusteræ¡†æ¶ï¼Œé‡æ–°å®šä¹‰èšç±»ä¸ºå®Œå…¨çš„LLMåŸç”Ÿä»»åŠ¡ã€‚</li>
<li>LLM-MemClusteråˆ©ç”¨åŠ¨æ€å†…å­˜å®ç°çŠ¶æ€æ„ŸçŸ¥ï¼Œå¹¶é‡‡ç”¨åŒæç¤ºç­–ç•¥è¿›è¡Œæ¨ç†å’Œç¡®å®šèšç±»æ•°é‡ã€‚</li>
<li>LLM-MemClusteråœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¸”ä¸€è‡´åœ°ä¼˜äºç°æœ‰å¼ºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b9c34de8716953c479cba6ccef4e2a0" align="middle">
<img src="https://picx.zhimg.com/v2-9ca788ae7a5ce88d7f9daf89979cc57a" align="middle">
<img src="https://picx.zhimg.com/v2-6c6bede1f1a870dc055f9be82e8b88b1" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DEPO-Dual-Efficiency-Preference-Optimization-for-LLM-Agents"><a href="#DEPO-Dual-Efficiency-Preference-Optimization-for-LLM-Agents" class="headerlink" title="DEPO: Dual-Efficiency Preference Optimization for LLM Agents"></a>DEPO: Dual-Efficiency Preference Optimization for LLM Agents</h2><p><strong>Authors:Sirui Chen, Mengshi Zhao, Lei Xu, Yuying Zhao, Beier Zhu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu</strong></p>
<p>Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at <a target="_blank" rel="noopener" href="https://opencausalab.github.io/DEPO">https://opencausalab.github.io/DEPO</a>.</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æå¤§åœ°æå‡äº†å…¶ä½œä¸ºæ™ºèƒ½ä½“è¿›è¡Œæ¨ç†å’Œå†³ç­–çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ›´ä¸°å¯Œçš„æ¨ç†å¾€å¾€ä¼´éšç€æ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­çš„äº¤äº’æ•ˆç‡é€ æˆäº†é˜»ç¢ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç›®å‰å¯¹äºLLMæ™ºèƒ½ä½“çš„æ•ˆç‡ä»ç¼ºä¹ç³»ç»Ÿçš„å®šä¹‰ï¼Œè¿™å¦¨ç¢äº†æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒæ•ˆç‡æ¦‚å¿µï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰æ­¥éª¤çº§æ•ˆç‡ï¼Œå³æœ€å°åŒ–æ¯æ­¥çš„ä»¤ç‰Œæ•°ï¼Œï¼ˆiiï¼‰è½¨è¿¹çº§æ•ˆç‡ï¼Œå³æœ€å°åŒ–å®Œæˆä¸€é¡¹ä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤æ•°ã€‚åŸºäºè¿™ä¸ªå®šä¹‰ï¼Œæˆ‘ä»¬æå‡ºäº†DEPOï¼Œä¸€ç§åŒæ•ˆç‡åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶å¥–åŠ±ç®€æ´çš„å›åº”å’Œè¾ƒå°‘çš„è¡ŒåŠ¨æ­¥éª¤ã€‚åœ¨WebShopå’ŒBabyAIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOå°†ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†é«˜è¾¾60.9%ï¼Œæ­¥éª¤å‡å°‘äº†é«˜è¾¾26.9%ï¼ŒåŒæ—¶æ€§èƒ½æé«˜äº†é«˜è¾¾29.3%ã€‚DEPOè¿˜é€‚ç”¨äºä¸‰ç§ç¦»åŸŸæ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨25%çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ä»èƒ½ä¿æŒå…¶æ•ˆç‡æå‡ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äº <a target="_blank" rel="noopener" href="https://opencausalab.github.io/DEPO%E3%80%82">https://opencausalab.github.io/DEPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15392v1">PDF</a> Accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ™ºèƒ½ä½“è¿›è¡Œéƒ¨ç½²æ—¶ï¼Œå…¶æ¨ç†å’Œå†³ç­–èƒ½åŠ›æœ‰äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œæ›´ä¸°å¯Œçš„æ¨ç†å¾€å¾€ä¼´éšç€æ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå½±å“äº†åœ¨çœŸå®åœºæ™¯ä¸­çš„äº¤äº’æ•ˆç‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŒæ•ˆç‡æ¦‚å¿µï¼ŒåŒ…æ‹¬æ­¥éª¤çº§æ•ˆç‡å’Œè½¨è¿¹çº§æ•ˆç‡ï¼Œå¹¶åŸºäºæ­¤å®šä¹‰æå‡ºäº†DEPOä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶å¥–åŠ±ç®€æ´å›åº”å’Œè¾ƒå°‘çš„è¡ŒåŠ¨æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨WebShopå’ŒBabyAIä»»åŠ¡ä¸Šèƒ½å‡å°‘60.9%çš„ä»¤ç‰Œä½¿ç”¨å’Œæœ€å¤šå‡å°‘26.9%çš„æ­¥éª¤ï¼ŒåŒæ—¶æ€§èƒ½æé«˜æœ€å¤šè¾¾29.3%ã€‚æ­¤å¤–ï¼ŒDEPOè¿˜èƒ½æ³›åŒ–åˆ°ä¸‰ä¸ªé¢†åŸŸå¤–çš„æ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨25%çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ä»èƒ½ä¿æŒæ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›æœ‰äº†æ˜¾è‘—æé«˜ï¼Œä½†æ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å½±å“äº†çœŸå®åœºæ™¯ä¸­çš„äº¤äº’æ•ˆç‡ã€‚</li>
<li>é¦–æ¬¡æå‡ºåŒæ•ˆç‡æ¦‚å¿µï¼ŒåŒ…æ‹¬æ­¥éª¤çº§æ•ˆç‡å’Œè½¨è¿¹çº§æ•ˆç‡ï¼Œä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡å®Œæˆè¿‡ç¨‹ä¸­çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºDEPOä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è”åˆå¥–åŠ±ç®€æ´å›åº”å’Œè¾ƒå°‘çš„è¡ŒåŠ¨æ­¥éª¤æ¥æé«˜è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨WebShopå’ŒBabyAIä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOèƒ½æœ‰æ•ˆå‡å°‘ä»¤ç‰Œä½¿ç”¨å’Œæ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶æé«˜æ€§èƒ½ã€‚</li>
<li>DEPOå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸‰ä¸ªé¢†åŸŸå¤–çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>DEPOåœ¨ä»…ä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ä»èƒ½ä¿æŒæ•ˆç‡æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„é€‚åº”æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6ccfdb7ff7d51e0d800456d7fa94c4f" align="middle">
<img src="https://picx.zhimg.com/v2-3df3a556ac43bd24fa93679f85b8a4c4" align="middle">
<img src="https://picx.zhimg.com/v2-8dcb994de6d8b775829ea80481b4893e" align="middle">
<img src="https://picx.zhimg.com/v2-aa6f87c5c62c8f0f65f2f145baa49f45" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Breaking-Expert-Knowledge-Limits-Self-Pruning-for-Large-Language-Models"><a href="#Breaking-Expert-Knowledge-Limits-Self-Pruning-for-Large-Language-Models" class="headerlink" title="Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models"></a>Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models</h2><p><strong>Authors:Haidong Kang, Lihong Lin, Enneng Yang, Hongning Dai, Hao Wang</strong></p>
<p>Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AutoPrune">https://anonymous.4open.science/r/AutoPrune</a>.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶åºå¤§çš„è§„æ¨¡ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²ã€‚ç°æœ‰çš„é’ˆå¯¹LLMçš„ä¿®å‰ªæ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‡è¾¾ï¼‰ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡ä¿®å‰ªç®—æ³•ï¼Œä»è€Œå¯¼è‡´å·¨å¤§çš„åŠ³åŠ¨åŠ›æˆæœ¬å¹¶è¦æ±‚ä¸“å®¶çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¯é¦–æ‰¹è¯†åˆ«å‡ºåœ¨è¾ƒé«˜ä¿®å‰ªæ¯”ç‡ä¸‹ç”±å‡åŒ€ç¨€ç–æ€§å¼•èµ·çš„æ€§èƒ½æ€¥å‰§ä¸‹é™èƒŒåå­˜åœ¨çš„ä¸¥é‡â€œç¦»ç¾¤å€¼é—®é¢˜â€çš„ç ”ç©¶äººå‘˜ï¼Œè¿™å¼•å‘äº†å…³äºå¦‚ä½•ä¸ºLLMè®¾è®¡è‡ªé€‚åº”ä¿®å‰ªç¨€ç–æ€§çš„ç†æƒ³æ–¹æ³•çš„é¢å¤–æ‹…å¿§ã€‚LLMèƒ½è‡ªæˆ‘ä¿®å‰ªå—ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°é¢–çš„è‡ªä¿®å‰ªæ–¹æ³•ç»™å‡ºäº†è‚¯å®šçš„å›ç­”ï¼Œè¯¥æ–¹æ³•é¦–æ¬¡å…‹æœäº†ä¸“å®¶çŸ¥è¯†çš„é™åˆ¶ï¼Œåˆ©ç”¨LLMè‡ªåŠ¨è®¾è®¡æœ€ä½³ä¿®å‰ªç®—æ³•è€Œæ— éœ€ä»»ä½•ä¸“å®¶çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å‡è½»LLMçš„é»‘ç›’æ€§è´¨ï¼Œæˆ‘ä»¬æå‡ºäº†Graphé©±åŠ¨çš„æ€è€ƒé“¾ï¼ˆGCoTï¼‰æ¥ä¼˜åŒ–æç¤ºï¼Œè¿™æ˜¾è‘—å¢å¼ºäº†å­¦ä¹ ä¿®å‰ªç®—æ³•è¿‡ç¨‹ä¸­çš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸‹ä¸€ä»£å…·æœ‰å“è¶Šæ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„ä¿®å‰ªç®—æ³•ã€‚æœ€åï¼ŒåŸºäºå¯¹ç¦»ç¾¤å€¼é—®é¢˜çš„è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†å€¾æ–œæ„ŸçŸ¥åŠ¨æ€ç¨€ç–åˆ†é…ï¼ˆSDSAï¼‰æ¥è§£å†³ç¦»ç¾¤å€¼é—®é¢˜ï¼Œç¼“è§£é«˜ä¿®å‰ªæ¯”ç‡ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬åœ¨ä¸»æµçš„LLMåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†AutoPruneçš„ä¼˜è¶Šæ€§ï¼Œå®ƒå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ç«äº‰å¯¹æ‰‹ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AutoPrune%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/r/AutoPruneè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15390v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºè‰²æ€§èƒ½åœ¨å¤šä¸ªä»»åŠ¡ä¸­å¾—åˆ°éªŒè¯ï¼Œä½†å…¶å¤§è§„æ¨¡éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¿®å‰ªæ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„ç®—æ³•ï¼Œå¯¼è‡´åŠ³åŠ¨æˆæœ¬é«˜æ˜‚ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoPruneçš„æ–°å‹ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨LLMsè‡ªåŠ¨è®¾è®¡æœ€ä½³ä¿®å‰ªç®—æ³•ï¼Œæ— éœ€ä¸“å®¶çŸ¥è¯†ã€‚é€šè¿‡å¼•å…¥Graphé©±åŠ¨çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰ä¼˜åŒ–æç¤ºï¼Œå¢å¼ºäº†å¯¹ä¿®å‰ªç®—æ³•çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶åœ¨ä¸‹ä¸€ä»£ä¸­ç”Ÿæˆå…·æœ‰ä¼˜è¶Šæ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„ä¿®å‰ªç®—æ³•ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç¦»ç¾¤å€¼é—®é¢˜å¼•å…¥Skewæ„ŸçŸ¥åŠ¨æ€ç¨€ç–åˆ†é…ï¼ˆSDSAï¼‰ï¼Œä»¥ç¼“è§£é«˜ä¿®å‰ªæ¯”ä¾‹ä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒAutoPruneåœ¨ä¸»æµLLMsåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†éƒ¨ç½²é¢ä¸´è§„æ¨¡æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ä¿®å‰ªæ–¹æ³•ä¾èµ–æ‰‹åŠ¨è®¾è®¡ç®—æ³•ï¼Œæˆæœ¬é«˜ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>é¦–æ¬¡è¯†åˆ«é«˜ä¿®å‰ªæ¯”ä¾‹ä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™èƒŒåçš„ç¦»ç¾¤å€¼é—®é¢˜ã€‚</li>
<li>æå‡ºåä¸ºAutoPruneçš„æ–°å‹è‡ªåŠ¨ä¿®å‰ªæ–¹æ³•ï¼Œæ— éœ€ä¸“å®¶çŸ¥è¯†å³å¯è®¾è®¡æœ€ä½³ä¿®å‰ªç®—æ³•ã€‚</li>
<li>åˆ©ç”¨Graphé©±åŠ¨çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰ä¼˜åŒ–æç¤ºï¼Œå¢å¼ºå¯¹ä¿®å‰ªç®—æ³•çš„å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥Skewæ„ŸçŸ¥åŠ¨æ€ç¨€ç–åˆ†é…ï¼ˆSDSAï¼‰è§£å†³ç¦»ç¾¤å€¼é—®é¢˜ï¼Œç¼“è§£é«˜ä¿®å‰ªæ¯”ä¾‹ä¸‹çš„æ€§èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad0ef439395ff46a3a6a435f1f67f255" align="middle">
<img src="https://picx.zhimg.com/v2-1b7b2637d75b1332655c2055d420d334" align="middle">
<img src="https://picx.zhimg.com/v2-f101b2d4154543c014b7a2452ce026c6" align="middle">
<img src="https://picx.zhimg.com/v2-220a4ef89a8972e2a0931df87da95e42" align="middle">
<img src="https://picx.zhimg.com/v2-d530e428fc5feec41ab941207602a9cc" align="middle">
<img src="https://picx.zhimg.com/v2-3c3e7d568c07f7bb00da4d0cb957467c" align="middle">
<img src="https://picx.zhimg.com/v2-9d99ac85e5f9a6ca67ae44a07cb028a3" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Parameter-Importance-Driven-Continual-Learning-for-Foundation-Models"><a href="#Parameter-Importance-Driven-Continual-Learning-for-Foundation-Models" class="headerlink" title="Parameter Importance-Driven Continual Learning for Foundation Models"></a>Parameter Importance-Driven Continual Learning for Foundation Models</h2><p><strong>Authors:Lingxiang Wang, Hainan Zhang, Zhiming Zheng</strong></p>
<p>Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.</p>
<blockquote>
<p>é¢†åŸŸç‰¹å®šçš„åè®­ç»ƒå¾€å¾€ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼Œä½¿å¾—åŸºç¡€æ¨¡å‹å¤±å»å…¶é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œå¹¶é™åˆ¶å…¶åœ¨åŠ¨æ€ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„é€‚åº”èƒ½åŠ›ã€‚åœ¨è·å–ä¸‹æ¸¸é¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿ç•™é€šç”¨èƒ½åŠ›æ˜¯å¤§å‹è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œå¦‚æ­£åˆ™åŒ–ã€å›æ”¾å’Œæ¶æ„éš”ç¦»ï¼Œå­˜åœ¨ä¸‹æ¸¸æ€§èƒ½å·®ã€ä¾èµ–ä¸å¯è®¿é—®çš„å†å²æ•°æ®æˆ–é¢å¤–çš„å‚æ•°å¼€é”€ç­‰é—®é¢˜ã€‚è™½ç„¶æœ€è¿‘çš„å‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ–¹æ³•å¯ä»¥å‡è½»é—å¿˜ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¼ºçƒˆä¾èµ–äºå‚æ•°å’Œæ›´æ–°ç­–ç•¥çš„é€‰æ‹©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºå‚æ•°é‡è¦æ€§ä¼°è®¡çš„æŒç»­å¢å¼ºæ–¹æ³•PIECEï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å­¦ä¹ é¢†åŸŸçŸ¥è¯†ï¼Œè€Œæ— éœ€è®¿é—®å…ˆå‰çš„è®­ç»ƒæ•°æ®æˆ–å¢åŠ æ¨¡å‹å‚æ•°ã€‚PIECEé€‰æ‹©æ€§åœ°æ›´æ–°ä¸æ–°ä»»åŠ¡æœ€ç›¸å…³çš„æ ¸å¿ƒå‚æ•°ï¼Œä»…å 0.1%ï¼Œç”±ä¸¤ä¸ªé‡è¦æ€§ä¼°è®¡å™¨å¼•å¯¼ï¼šåŸºäºFisherä¿¡æ¯çš„PIECE-Fï¼Œä»¥åŠç»“åˆæ¢¯åº¦å’Œæ›²ç‡ä¿¡æ¯çš„äºŒé˜¶å½’ä¸€åŒ–çš„PIECE-Sã€‚åœ¨ä¸‰ç§è¯­è¨€æ¨¡å‹å’Œä¸¤ç§å¤šåª’ä½“æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPIECEä¿æŒäº†é€šç”¨èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†ä¸€æ¡å®ç”¨çš„é“è·¯ï¼Œå³æ„å»ºå¯æ‰©å±•çš„ã€é€‚åº”é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15375v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šè®­ç»ƒåé¢ä¸´çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†PIECEæ–¹æ³•ï¼Œé€šè¿‡å‚æ•°é‡è¦æ€§ä¼°è®¡è¿›è¡ŒæŒç»­å¢å¼ºï¼Œä»¥ä¿ç•™é€šç”¨èƒ½åŠ›å¹¶é«˜æ•ˆå­¦ä¹ é¢†åŸŸçŸ¥è¯†ã€‚è¯¥æ–¹æ³•æ— éœ€è®¿é—®å…ˆå‰è®­ç»ƒæ•°æ®æˆ–å¢åŠ æ¨¡å‹å‚æ•°ï¼Œä»…é€‰æ‹©æ€§åœ°æ›´æ–°ä¸æ–°ä»»åŠ¡æœ€ç›¸å…³çš„0.1%çš„æ ¸å¿ƒå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒPIECEåœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†è·¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„æœ€æ–°æŒç»­å­¦ä¹ æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šè®­ç»ƒåä¼šé¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¯¼è‡´é€šç”¨æ¨ç†èƒ½åŠ›ä¸§å¤±å’Œå¯¹åŠ¨æ€ç°å®ç¯å¢ƒçš„é€‚åº”æ€§å—é™ã€‚</li>
<li>ä¼ ç»Ÿçš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œå¦‚æ­£åˆ™åŒ–ã€å›æ”¾å’Œæ¶æ„éš”ç¦»ï¼Œå­˜åœ¨ä¸‹æ¸¸æ€§èƒ½å·®ã€ä¾èµ–ä¸å¯è®¿é—®çš„å†å²æ•°æ®æˆ–é¢å¤–çš„å‚æ•°å¼€é”€ç­‰é—®é¢˜ã€‚</li>
<li>æœ€è¿‘çš„å‚æ•°æ•ˆç‡è°ƒæ•´ï¼ˆPETï¼‰æ–¹æ³•å¯ä»¥å‡è½»é—å¿˜ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºå‚æ•°å’Œæ›´æ–°ç­–ç•¥çš„é€‰æ‹©ã€‚</li>
<li>PIECEæ–¹æ³•åŸºäºå‚æ•°é‡è¦æ€§ä¼°è®¡è¿›è¡ŒæŒç»­å¢å¼ºï¼Œå¯ä¿ç•™é€šç”¨èƒ½åŠ›å¹¶é«˜æ•ˆå­¦ä¹ é¢†åŸŸçŸ¥è¯†ï¼Œæ— éœ€è®¿é—®å…ˆå‰è®­ç»ƒæ•°æ®æˆ–å¢åŠ æ¨¡å‹å‚æ•°ã€‚</li>
<li>PIECEé€šè¿‡ä¸¤ç§é‡è¦æ€§ä¼°è®¡å™¨ï¼šåŸºäºFisherä¿¡æ¯çš„PIECE-Få’Œç»“åˆæ¢¯åº¦ä¸æ›²ç‡ä¿¡æ¯çš„PIECE-Sï¼Œé€‰æ‹©æ€§åœ°æ›´æ–°ä¸æ–°ä»»åŠ¡æœ€ç›¸å…³çš„æ ¸å¿ƒå‚æ•°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPIECEåœ¨å¤šç§è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹ä¸Šå‡èƒ½ä¿æŒé€šç”¨èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65185f98415643390a19e5d688cb80c3" align="middle">
<img src="https://picx.zhimg.com/v2-0b073170f10a63a0e234ea0b1dade4c0" align="middle">
<img src="https://picx.zhimg.com/v2-bc88dde86b3633496d86f278d531712e" align="middle">
<img src="https://picx.zhimg.com/v2-4939975e8e31ca632b33c135239eca86" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Octopus-Agentic-Multimodal-Reasoning-with-Six-Capability-Orchestration"><a href="#Octopus-Agentic-Multimodal-Reasoning-with-Six-Capability-Orchestration" class="headerlink" title="Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration"></a>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</h2><p><strong>Authors:Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Yingchao Li, Huacan Wang, Ronghao Chen</strong></p>
<p>Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.</p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ä¸æ¡†æ¶å­˜åœ¨åŸºæœ¬æ¶æ„ä¸Šçš„å±€é™ï¼šå¤§å¤šæ•°æ¨¡å‹ç¼ºä¹äººç±»è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼Œæ— è®ºæ˜¯ç›´æ¥æ¨ç†ã€å·¥å…·é©±åŠ¨çš„è§†è§‰æ¢ç´¢ã€ç¨‹åºåŒ–çš„è§†è§‰æ“ä½œï¼Œè¿˜æ˜¯å†…åœ¨çš„è§†è§‰æƒ³è±¡ã€‚å› æ­¤ï¼Œå®ƒä»¬å¾ˆéš¾é€‚åº”ç°å®ä»»åŠ¡ä¸­åŠ¨æ€å˜åŒ–çš„èƒ½åŠ›è¦æ±‚ã€‚ä¸æ­¤åŒæ—¶ï¼Œäººç±»åœ¨è§£å†³æ­¤ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº’è¡¥çš„æ€è€ƒèƒ½åŠ›ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸åªæ¶µç›–è¿™äº›ç»´åº¦çš„ä¸€éƒ¨åˆ†ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Octopusï¼šå…·æœ‰å…­ç§èƒ½åŠ›ååŒçš„å¤šæ¨¡æ€è‡ªä¸»æ¨ç†æ–°èŒƒå¼ã€‚æˆ‘ä»¬å®šä¹‰äº†å¤šæ¨¡æ€æ¨ç†æ‰€å¿…éœ€çš„å…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶ç›¸åº”åœ°æ„å»ºäº†ç»¼åˆè¯„ä¼°åŸºå‡†Octopus-Benchã€‚Octopusèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»æ¢ç´¢ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOctopusåœ¨Octopus-Benchä¸­çš„ç»å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¿™çªå‡ºäº†èƒ½åŠ›åè°ƒåœ¨å¤šæ¨¡æ€è‡ªä¸»æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å’Œæ¡†æ¶å­˜åœ¨çš„å±€é™æ€§ï¼Œå®ƒä»¬ç¼ºä¹äººç±»è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„èƒ½åŠ›è¦æ±‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Octopusï¼šå…·æœ‰å…­ç§èƒ½åŠ›ååŒçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨ç†ï¼Œå¹¶å®šä¹‰äº†å…­ç§æ ¸å¿ƒèƒ½åŠ›å’Œç›¸åº”çš„ç»¼åˆè¯„ä¼°åŸºå‡†Octopus-Benchã€‚Octopusèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOctopusåœ¨Octopus-Benchçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå‡¸æ˜¾äº†èƒ½åŠ›åè°ƒåœ¨å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å’Œæ¡†æ¶å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ã€‚</li>
<li>äººç±»åœ¨è§£å†³ä»»åŠ¡æ—¶å±•ç°å‡ºçš„æ€ç»´èƒ½åŠ›æ˜¯å¤šç»´åº¦çš„ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸åªè¦†ç›–ä¸€éƒ¨åˆ†ã€‚</li>
<li>ä½œè€…æå‡ºäº†Octopusï¼šå…·æœ‰å…­ç§èƒ½åŠ›ååŒçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨ç†ï¼Œä»¥å¼¥è¡¥ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>å…­ç§æ ¸å¿ƒèƒ½åŠ›è¢«å®šä¹‰ï¼Œå¹¶ç›¸åº”åˆ¶å®šäº†ç»¼åˆè¯„ä¼°åŸºå‡†Octopus-Benchã€‚</li>
<li>Octopusèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOctopusåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­æ€§èƒ½æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c98136c7236aca77c8cf05411d86cc77" align="middle">
<img src="https://picx.zhimg.com/v2-e4e763f0493a6f2a45db0cf3bc4be222" align="middle">
<img src="https://picx.zhimg.com/v2-f01c8e5b2464fffcecc58fa642b42f5d" align="middle">
<img src="https://picx.zhimg.com/v2-2f539ee2e1ed72cd151dfe430c8cccc5" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="C2F-Space-Coarse-to-Fine-Space-Grounding-for-Spatial-Instructions-using-Vision-Language-Models"><a href="#C2F-Space-Coarse-to-Fine-Space-Grounding-for-Spatial-Instructions-using-Vision-Language-Models" class="headerlink" title="C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models"></a>C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models</h2><p><strong>Authors:Nayoung Oh, Dohyun Kim, Junhyeong Bang, Rohan Paul, Daehyung Park</strong></p>
<p>Space grounding refers to localizing a set of spatial references described in natural language instructions. Traditional methods often fail to account for complex reasoning â€“ such as distance, geometry, and inter-object relationships â€“ while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs. To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization. For the coarse estimation, we design a grid-based visual-grounding prompt with a propose-validate strategy, maximizing VLMâ€™s spatial understanding and yielding physically and semantically valid canonical region (i.e., ellipses). For the refinement, we locally adapt the region to surrounding environment without over-relaxed to free space. We construct a new space-grounding benchmark and compare C2F-Space with five state-of-the-art baselines using success rate and intersection-over-union. Our C2F-Space significantly outperforms all baselines. Our ablation study confirms the effectiveness of each module in the two-step process and their synergistic effect of the combined framework. We finally demonstrate the applicability of C2F-Space to simulated robotic pick-and-place tasks.</p>
<blockquote>
<p>ç©ºé—´å®šä½æ˜¯æŒ‡å®šä½è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­æè¿°çš„ä¸€ç»„ç©ºé—´å‚è€ƒã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€æ— æ³•å¤„ç†å¤æ‚çš„æ¨ç†ï¼Œä¾‹å¦‚è·ç¦»ã€å‡ ä½•å’Œå¯¹è±¡é—´å…³ç³»ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°½ç®¡å…·æœ‰å¾ˆå¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç»†è¾“å‡ºåŒºåŸŸæ—¶å´é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†C2F-Spaceï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç²—åˆ°ç»†çš„ç©ºé—´å®šä½æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰ä½¿ç”¨VLMä¼°è®¡ä¸€ä¸ªå¤§è‡´ä½†ç©ºé—´ä¸Šä¸€è‡´çš„åŒºåŸŸï¼Œç„¶åï¼ˆiiï¼‰é€šè¿‡è¶…åƒç´ åŒ–ä½¿è¯¥åŒºåŸŸä¸å±€éƒ¨ç¯å¢ƒç›¸é€‚åº”ã€‚å¯¹äºç²—ç•¥ä¼°è®¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºç½‘æ ¼çš„è§†è§‰å®šä½æç¤ºï¼Œé‡‡ç”¨æå‡º-éªŒè¯ç­–ç•¥ï¼Œæœ€å¤§é™åº¦åœ°å‘æŒ¥VLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶äº§ç”Ÿç‰©ç†å’Œè¯­ä¹‰ä¸Šæœ‰æ•ˆçš„è§„èŒƒåŒºåŸŸï¼ˆå³æ¤­åœ†ï¼‰ã€‚å¯¹äºç»†åŒ–ï¼Œæˆ‘ä»¬å±€éƒ¨é€‚åº”åŒºåŸŸå‘¨å›´ç¯å¢ƒï¼Œè€Œä¸ä¼šè¿‡äºæ”¾æ¾åˆ°è‡ªç”±ç©ºé—´ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„ç©ºé—´å®šä½åŸºå‡†ï¼Œç”¨æˆåŠŸç‡å’Œäº¤é›†æ¯”æ¯”è¾ƒC2F-Spaceä¸äº”ç§æœ€æ–°æŠ€æœ¯åŸºå‡†ã€‚æˆ‘ä»¬çš„C2F-Spaceæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºå‡†ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†ä¸¤æ­¥è¿‡ç¨‹ä¸­æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä»¥åŠå®ƒä»¬ååŒä½œç”¨çš„æ•´ä½“æ•ˆæœã€‚æˆ‘ä»¬æœ€åå±•ç¤ºäº†C2F-Spaceåœ¨æ¨¡æ‹Ÿçš„æœºå™¨äººæ‹¾å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­çš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15333v1">PDF</a> 16 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç©ºé—´å®šä½æ¡†æ¶C2F-Spaceï¼Œå®ƒèƒ½å…‹æœä¼ ç»Ÿæ–¹æ³•å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å®šä½ä¸Šçš„å±€é™ã€‚C2F-Spaceé¦–å…ˆä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç²—ç•¥ä¼°è®¡ï¼Œç„¶åé€šè¿‡è¶…åƒç´ åŒ–å¯¹åŒºåŸŸè¿›è¡Œç»†åŒ–ï¼Œä»¥ä¸å±€éƒ¨ç¯å¢ƒå¯¹é½ã€‚è¯¥æ¡†æ¶åœ¨æ–°å‹ç©ºé—´å®šä½åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æˆåŠŸåº”ç”¨äºæ¨¡æ‹Ÿçš„æœºå™¨äººæŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>C2F-Spaceæ˜¯ä¸€ç§æ–°çš„ç²—åˆ°ç»†çš„ç©ºé—´å®šä½æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å®šä½ä¸Šçš„å±€é™ã€‚</li>
<li>æ¡†æ¶é¦–å…ˆä½¿ç”¨åŸºäºç½‘æ ¼çš„è§†è§‰å®šä½æç¤ºå’Œæå‡ºéªŒè¯ç­–ç•¥è¿›è¡Œç²—ç•¥ä¼°è®¡ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨è¶…åƒç´ åŒ–å¯¹åˆæ­¥ä¼°è®¡çš„åŒºåŸŸè¿›è¡Œç»†åŒ–ï¼Œä»¥ä¸å±€éƒ¨ç¯å¢ƒå¯¹é½ã€‚</li>
<li>C2F-Spaceè®¾è®¡äº†ä¸€ç§ç‰©ç†å’Œè¯­ä¹‰ä¸Šæœ‰æ•ˆçš„è§„èŒƒåŒºåŸŸï¼ˆå³æ¤­åœ†ï¼‰ã€‚</li>
<li>æ–°å»ºç«‹çš„ç©ºé—´å®šä½åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒC2F-Spaceæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ¡†æ¶ä¸­æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä»¥åŠå®ƒä»¬ååŒä½œç”¨çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e469c78e22ac2752f7da7e590139ad1" align="middle">
<img src="https://picx.zhimg.com/v2-aca83d0fab68b528513dcc961410f778" align="middle">
<img src="https://picx.zhimg.com/v2-57601ceecf3771860db4c8089bc41b1f" align="middle">
<img src="https://picx.zhimg.com/v2-2f373b2027af1afe68303450197c2b26" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficiency-Will-Not-Lead-to-Sustainable-Reasoning-AI"><a href="#Efficiency-Will-Not-Lead-to-Sustainable-Reasoning-AI" class="headerlink" title="Efficiency Will Not Lead to Sustainable Reasoning AI"></a>Efficiency Will Not Lead to Sustainable Reasoning AI</h2><p><strong>Authors:Philipp Wiesner, Daniel W. Oâ€™Neill, Francesca Larosa, Odej Kao</strong></p>
<p>AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computingâ€™s global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç ”ç©¶æ­£æ—¥ç›Šè½¬å‘å¤æ‚é—®é¢˜è§£å†³ï¼Œæ¨¡å‹ä¼˜åŒ–ä¸ä»…é’ˆå¯¹æ¨¡å¼è¯†åˆ«ï¼Œè€Œä¸”é’ˆå¯¹å¤šæ­¥éª¤æ¨ç†ã€‚åœ¨è®¡ç®—è¡Œä¸šçš„å…¨çƒèƒ½æºè¶³è¿¹å†å²æ€§åœ°é€šè¿‡æŒç»­æ•ˆç‡æå‡å’Œå¸‚åœºéœ€æ±‚è‡ªç„¶é¥±å’Œé—¨æ§›å¾—ä»¥ç¨³å®šä¹‹åï¼Œç”±äºæ•ˆç‡æ”¹è¿›æ­£åœ¨æ¥è¿‘ç‰©ç†æé™ï¼Œæ–°å…´çš„äººå·¥æ™ºèƒ½æ¨ç†ç¼ºå°‘ç›¸åº”çš„é¥±å’Œç‚¹ï¼šæ€§èƒ½ä¸å†å—å¯ç”¨è®­ç»ƒæ•°æ®é‡é™åˆ¶ï¼Œè€Œæ˜¯éšç€è®­ç»ƒå’Œæ¨ç†æ–¹é¢çš„æŒ‡æ•°çº§è®¡ç®—æŠ•èµ„è€ŒæŒç»­å‘å±•ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå•é æ•ˆç‡æ— æ³•å®ç°å¯æŒç»­çš„äººå·¥æ™ºèƒ½æ¨ç†ï¼Œå¹¶è®¨è®ºäº†ç ”ç©¶å’Œæ”¿ç­–æ–¹å‘ï¼Œå°†æ˜ç¡®é™åˆ¶çº³å…¥æ­¤ç±»ç³»ç»Ÿçš„ä¼˜åŒ–å’Œæ²»ç†ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15259v1">PDF</a> Presented at the Rethinking AI Workshop @ EurIPSâ€™25</p>
<p><strong>Summary</strong>ï¼šäººå·¥æ™ºèƒ½ç ”ç©¶æ­£æœç€è§£å†³å¤æ‚é—®é¢˜æ–¹å‘å‘å±•ï¼Œæ¨¡å‹ä¼˜åŒ–ä¸ä»…é™äºæ¨¡å¼è¯†åˆ«ï¼Œè¿˜åŒ…æ‹¬å¤šæ­¥éª¤æ¨ç†ã€‚éšç€æ•ˆç‡æå‡æ¥è¿‘ç‰©ç†æé™ï¼Œæ–°å…´æ¨ç†äººå·¥æ™ºèƒ½ç¼ºä¹ç›¸åº”çš„é¥±å’Œç‚¹ï¼Œæ€§èƒ½ä¸å†å—é™äºå¯ç”¨è®­ç»ƒæ•°æ®é‡ï¼Œè€Œæ˜¯éšç€è®­ç»ƒå’Œæ¨ç†ä¸­çš„æŒ‡æ•°çº§è®¡ç®—æŠ•èµ„è€Œä¸æ–­æ‰©å±•ã€‚æœ¬æ–‡è®¤ä¸ºå•é æ•ˆç‡æ— æ³•å®ç°å¯æŒç»­çš„æ¨ç†äººå·¥æ™ºèƒ½ï¼Œå¹¶è®¨è®ºäº†å°†æ˜ç¡®é™åˆ¶åµŒå…¥æ­¤ç±»ç³»ç»Ÿçš„ä¼˜åŒ–å’Œæ²»ç†çš„ç ”ç©¶å’Œæ”¿ç­–æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIç ”ç©¶æ­£åœ¨å‘è§£å†³å¤æ‚é—®é¢˜æ–¹å‘å‘å±•ï¼Œæ¶µç›–å¤šæ­¥éª¤æ¨ç†ã€‚</li>
<li>æ•ˆç‡æå‡æ¥è¿‘ç‰©ç†æé™ï¼Œæ–°å…´æ¨ç†AIç¼ºä¹é¥±å’Œç‚¹ã€‚</li>
<li>æ¨ç†AIçš„æ€§èƒ½æ‰©å±•ä¸å†å—é™äºè®­ç»ƒæ•°æ®é‡ã€‚</li>
<li>æ€§èƒ½å’Œè®¡ç®—æŠ•èµ„å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚</li>
<li>å•çº¯ä¾é æ•ˆç‡æ— æ³•å®ç°å¯æŒç»­çš„æ¨ç†AIã€‚</li>
<li>éœ€è¦ç ”ç©¶å’Œæ”¿ç­–æ–¹å‘æ¥æ˜ç¡®é™åˆ¶æ¨ç†AIç³»ç»Ÿçš„ä¼˜åŒ–å’Œæ²»ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-733eedb4a8e6a506060429644ae44cd4" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EntroPIC-Towards-Stable-Long-Term-Training-of-LLMs-via-Entropy-Stabilization-with-Proportional-Integral-Control"><a href="#EntroPIC-Towards-Stable-Long-Term-Training-of-LLMs-via-Entropy-Stabilization-with-Proportional-Integral-Control" class="headerlink" title="EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control"></a>EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control</h2><p><strong>Authors:Kai Yang, Xin Xu, Yangkun Chen, Weijie Liu, Jiafei Lyu, Zichuan Lin, Deheng Ye, Saiyong Yang</strong></p>
<p>Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.</p>
<blockquote>
<p>é•¿æœŸè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦ä¿æŒç¨³å®šçš„æ¢ç´¢ï¼Œä»¥é˜²æ­¢æ¨¡å‹é™·å…¥æ¬¡ä¼˜è¡Œä¸ºã€‚åœ¨æ­¤æƒ…å†µä¸‹ï¼Œç†µè‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæ§åˆ¶æ¢ç´¢å¹¶æœ‰åŠ©äºé¿å…è¿‡æ—©åœ°æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ä¿æŒé€‚å½“çš„ç†µæ°´å¹³æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹æ¶‰åŠæ­£è´Ÿé¢æ ·æœ¬çš„æ··åˆï¼Œæ¯ä¸€æ­¥ä¸­æ¯ä¸ªæ ·æœ¬å¯¹ç†µçš„å½±å“å„ä¸ç›¸åŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡æ¯”ä¾‹ç§¯åˆ†æ§åˆ¶å®ç°ç†µç¨³å®šåŒ–ï¼ˆEntroPICï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ­£è´Ÿé¢æ ·æœ¬çš„æŸå¤±ç³»æ•°æ¥è‡ªé€‚åº”åœ°è°ƒæ•´å…¶å½±å“ã€‚è¿™ç§æ–¹æ³•åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šäº†ç†µï¼Œç¡®ä¿äº†é«˜æ•ˆçš„æ¢ç´¢å’Œç¨³å®šçš„è¿›æ­¥ã€‚æˆ‘ä»¬ä¸ºåŸºäºç­–ç•¥å’Œç¦»ç­–ç•¥å­¦ä¹ æƒ…å¢ƒæä¾›äº†å…¨é¢çš„ç†è®ºåˆ†æï¼Œè¯æ˜äº†EntroPICåœ¨å¤§è§„æ¨¡LLMè®­ç»ƒä¸­æ§åˆ¶ç†µçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°ç»´æŒäº†æ‰€éœ€çš„ç†µæ°´å¹³ï¼Œå®ç°äº†LLMçš„ç¨³å®šå’Œæœ€ä¼˜å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æœŸè®­ç»ƒéœ€è¦ä¿æŒç¨³å®šçš„æ¢ç´¢ï¼Œä»¥é˜²æ­¢æ¨¡å‹é™·å…¥æ¬¡ä¼˜è¡Œä¸ºã€‚ç†µåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­è‡³å…³é‡è¦ï¼Œå®ƒèƒ½æ§åˆ¶æ¢ç´¢å¹¶å¸®åŠ©é¿å…è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç»´æŒé€‚å½“çš„ç†µæ°´å¹³æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­æ¶‰åŠçš„æ­£è´Ÿæ ·æœ¬åœ¨ä¸åŒæ­¥éª¤ä¸­ä»¥ä¸åŒæ–¹å¼å½±å“ç†µã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æ¯”ä¾‹ç§¯åˆ†æ§åˆ¶å®ç°ç†µç¨³å®šï¼ˆEntroPICï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æŸå¤±ç³»æ•°æ¥è‡ªé€‚åº”åœ°è°ƒæ•´å…¶å½±å“ã€‚æ­¤æ–¹æ³•åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ç¨³å®šäº†ç†µï¼Œç¡®ä¿äº†é«˜æ•ˆçš„æ¢ç´¢å’Œç¨³å®šçš„è¿›å±•ã€‚æˆ‘ä»¬ä¸ºåŸºäºç­–ç•¥å’Œç¦»ç­–ç•¥å­¦ä¹ åœºæ™¯æä¾›äº†å…¨é¢çš„ç†è®ºåˆ†æï¼Œè¯æ˜EntroPICåœ¨æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ç†µæ—¶éå¸¸æœ‰æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°ç»´æŒäº†æœŸæœ›çš„ç†µæ°´å¹³ï¼Œå®ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šå’Œä¼˜åŒ–è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é•¿æœŸè®­ç»ƒéœ€ä¿æŒç¨³å®šçš„æ¢ç´¢ï¼Œé˜²æ­¢é™·å…¥æ¬¡ä¼˜è¡Œä¸ºã€‚</li>
<li>ç†µåœ¨æ§åˆ¶æ¨¡å‹æ¢ç´¢ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå½±å“æ¨¡å‹é¿å…è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç»´æŒè®­ç»ƒè¿‡ç¨‹ä¸­çš„é€‚å½“ç†µæ°´å¹³æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>EntroPICæ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„å½±å“æ¥ç¨³å®šç†µã€‚</li>
<li>EntroPICæ–¹æ³•ç¡®ä¿äº†é«˜æ•ˆçš„æ¢ç´¢å’Œç¨³å®šçš„è®­ç»ƒè¿›å±•ã€‚</li>
<li>ç†è®ºå’Œå®éªŒç»“æœè¡¨æ˜EntroPICåœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ç†µæ§åˆ¶éå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-872e01b7369dd0d31a30da4834e3c2c6" align="middle">
<img src="https://picx.zhimg.com/v2-093adbbc49cdcd3077b0ba9d36ac60fa" align="middle">
<img src="https://picx.zhimg.com/v2-9d627af8dc601552fd4b66d19a02b1d9" align="middle">
<img src="https://picx.zhimg.com/v2-a5fbeb04d15c6547026e4fa23306c445" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="BBox-DocVQA-A-Large-Scale-Bounding-Box-Grounded-Dataset-for-Enhancing-Reasoning-in-Document-Visual-Question-Answer"><a href="#BBox-DocVQA-A-Large-Scale-Bounding-Box-Grounded-Dataset-for-Enhancing-Reasoning-in-Document-Visual-Question-Answer" class="headerlink" title="BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer"></a>BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer</h2><p><strong>Authors:Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang</strong></p>
<p>Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.</p>
<blockquote>
<p>æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰æ˜¯å¤šåª’ä½“æ–‡æ¡£ç†è§£çš„åŸºæœ¬ä»»åŠ¡ï¼Œä¹Ÿæ˜¯è§†è§‰è¯­è¨€æ¨ç†çš„é‡è¦æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„DocVQAæ•°æ®é›†ä»…é™äºé¡µé¢çº§åˆ«ï¼Œç¼ºä¹ç²¾ç»†çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†BBox DocVQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€åŸºäºè¾¹ç•Œæ¡†çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜æ–‡æ¡£ä¸­çš„ç©ºé—´æ¨ç†å’Œè¯æ®å®šä½èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ„å»ºæµç¨‹ï¼Œå³â€œåˆ†æ®µåˆ¤æ–­å¹¶ç”Ÿæˆâ€ï¼Œè¯¥æµç¨‹é›†æˆäº†åŒºåŸŸåˆ†å‰²çš„æ®µæ¨¡å‹ã€è¯­ä¹‰åˆ¤æ–­çš„VLMä»¥åŠç”¨äºç”Ÿæˆé—®ç­”çš„å…ˆè¿›VLMï¼Œéšåè¿›è¡Œäººå·¥éªŒè¯ä»¥ä¿è¯è´¨é‡ã€‚ç”Ÿæˆçš„æ•°æ®é›†åŒ…å«3600å¤šç§ä¸åŒçš„æ–‡æ¡£å’Œ3ä¸‡å¤šä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–å•åŒºåŸŸå’Œå¤šåŒºåŸŸä»¥åŠå•é¡µå’Œå¤šé¡µåœºæ™¯ã€‚æ¯ä¸ªé—®ç­”å®ä¾‹éƒ½åŸºäºæ˜ç¡®çš„è¾¹ç•Œæ¡†ï¼Œå®ç°å¯¹ç©ºé—´è¯­ä¹‰å¯¹é½çš„ç²¾ç»†è¯„ä¼°ã€‚åœ¨BBox DocVQAä¸Šå¯¹å¤šä¸ªæœ€å…ˆè¿›çš„VLMï¼ˆä¾‹å¦‚GPT 5ã€Qwen 2.5 VLå’ŒInternVLï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œåœ¨ç©ºé—´å®šä½å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢ä»å­˜åœ¨æŒç»­æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒå¯æ˜¾è‘—æé«˜è¾¹ç•Œæ¡†å®šä½å’Œç­”æ¡ˆç”Ÿæˆèƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æé«˜VLMæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯è§£é‡Šå’ŒåŸºäºç©ºé—´å®šä½çš„è§†è§‰è¯­è¨€æ¨ç†ç ”ç©¶çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15090v1">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰ä¸­çš„ä¸€é¡¹æ–°æŒ‘æˆ˜ï¼šå¤§å¤šæ•°ç°æœ‰æ•°æ®é›†ä»…é™äºé¡µé¢çº§åˆ«ï¼Œç¼ºä¹ç²¾ç»†çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„è§£é‡Šå’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†BBox DocVQAæ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†å’Œè¯æ®å®šä½èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤§è§„æ¨¡è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œå¯ä¸ºè§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´è¯­ä¹‰å¯¹é½è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†è‡ªåŠ¨åŒ–æ„å»ºæµç¨‹Segment Judge and Generateï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¤šç§å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚æ•°æ®é›†ä¸­åŒ…å«äº†ä¸°å¯Œçš„æ–‡æ¡£ä¸é—®ç­”å¯¹ï¼Œå¹¶åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒèƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨ç©ºé—´å’Œè¯­ä¹‰æ¨ç†ä¸Šçš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æ•°æ®å’Œä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ¨åŠ¨å¯è§£é‡Šæ€§å’Œç©ºé—´å®šä½çš„è§†è§‰è¯­è¨€æ¨ç†ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DocVQAé¢ä¸´æŒ‘æˆ˜ï¼šç°æœ‰æ•°æ®é›†ç¼ºä¹ç²¾ç»†çš„ç©ºé—´å®šä½ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„è§£é‡Šå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>BBox DocVQAæ•°æ®é›†æ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†å’Œè¯æ®å®šä½èƒ½åŠ›ï¼ŒåŒ…å«å¤§è§„æ¨¡è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œç”¨äºè§†è§‰æ–‡æ¡£ä¸­çš„ç©ºé—´è¯­ä¹‰å¯¹é½çš„ç²¾ç»†è¯„ä¼°ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ„å»ºæµç¨‹Segment Judge and Generateé›†æˆäº†åŒºåŸŸåˆ†å‰²ã€è¯­ä¹‰åˆ¤æ–­å’Œé—®ç­”ç”Ÿæˆç­‰åŠŸèƒ½ï¼Œå¹¶è¿›è¡Œäººå·¥éªŒè¯ä»¥ç¡®ä¿è´¨é‡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§æ–‡æ¡£å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–å•åŒºåŸŸã€å¤šåŒºåŸŸã€å•é¡µå’Œå¤šé¡µåœºæ™¯ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨BBox DocVQAæ•°æ®é›†ä¸Šè¡¨ç°æœ‰æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨ç©ºé—´å®šä½å’Œæ¨ç†å‡†ç¡®æ€§æ–¹é¢ã€‚</li>
<li>åœ¨BBox DocVQAæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¯æœ‰æ•ˆæå‡æ¨¡å‹åœ¨ç©ºé—´å’Œè¯­ä¹‰æ¨ç†ä¸Šçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-719ca0fc7ce059be1d4b83d4cdad58aa" align="middle">
<img src="https://picx.zhimg.com/v2-6b23fc4145c6c6c0b1e32f3633ee31a6" align="middle">
<img src="https://picx.zhimg.com/v2-dea23bfb5cac4871bf1a6496433831d8" align="middle">
<img src="https://picx.zhimg.com/v2-8ca15c31e421c53a9ce9c516ed90ec5d" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Empowering-Multi-Turn-Tool-Integrated-Reasoning-with-Group-Turn-Policy-Optimization"><a href="#Empowering-Multi-Turn-Tool-Integrated-Reasoning-with-Group-Turn-Policy-Optimization" class="headerlink" title="Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization"></a>Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization</h2><p><strong>Authors:Yifeng Ding, Hung Le, Songyang Han, Kangrui Ruan, Zhenghui Jin, Varun Kumar, Zijian Wang, Anoop Deoras</strong></p>
<p>Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.</p>
<blockquote>
<p>å¯¹äºå¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡â€”â€”å…¶ä¸­æ¨¡å‹è¿›è¡Œè¿­ä»£æ¨ç†ã€ç”Ÿæˆä»£ç å¹¶é€šè¿‡æ‰§è¡Œè¿›è¡ŒéªŒè¯â€”â€”ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä»¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸ºä»£è¡¨çš„å½“å‰RLæ–¹æ³•ï¼Œé­å—äº†å¥–åŠ±ç²’åº¦ç²—å¤§ã€è½¨è¿¹çº§åˆ«å¥–åŠ±ä¸è¶³çš„å›°æ‰°ï¼Œæ— æ³•ä¸ºå¤æ‚çš„å¤šè½®äº¤äº’æä¾›è¶³å¤Ÿçš„å­¦ä¹ ä¿¡å·ï¼Œå¯¼è‡´è®­ç»ƒåœæ»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹å¤šè½®TIRä»»åŠ¡è®­ç»ƒLLMçš„ç‰¹å®šå¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç¾¤ä½“è½®æ¬¡ç­–ç•¥ä¼˜åŒ–ï¼ˆGTPOï¼‰ã€‚GTPOå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆ1ï¼‰è½®æ¬¡çº§åˆ«çš„å¥–åŠ±åˆ†é…ï¼Œä¸ºå•ä¸ªè½®æ¬¡æä¾›ç²¾ç»†çš„åé¦ˆï¼›ï¼ˆ2ï¼‰åŸºäºå›æŠ¥çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œå…¶ä¸­å°†å½’ä¸€åŒ–æŠ˜æ‰£å›æŠ¥è®¡ç®—ä¸ºä¼˜åŠ¿ï¼›ï¼ˆ3ï¼‰è‡ªæˆ‘ç›‘ç£å¥–åŠ±å¡‘å½¢ï¼Œåˆ©ç”¨ç”Ÿæˆä»£ç ä¸­çš„è‡ªæˆ‘ç›‘ç£ä¿¡å·æ¥ä¸°å¯Œç¨€ç–çš„äºŒå…ƒç»“æœå¥–åŠ±ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒGTPOåœ¨å¤šæ ·åŒ–çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡ä¼˜äºGRPO 3.0%ï¼Œè¿™è¯æ˜äº†å…¶åœ¨æ¨è¿›ç°å®ä¸–ç•Œå¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14846v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä»»åŠ¡ä¸Šçš„è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå¦‚ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå­˜åœ¨å¥–åŠ±ç²’åº¦ç²—ã€è½¨è¿¹çº§åˆ«å¥–åŠ±ä¸è¶³çš„é—®é¢˜ï¼Œæ— æ³•ä¸ºå¤æ‚çš„å¤šè½®äº¤äº’æä¾›è¶³å¤Ÿçš„å­¦ä¹ ä¿¡å·ï¼Œå¯¼è‡´è®­ç»ƒåœæ»ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é›†å›¢å›åˆç­–ç•¥ä¼˜åŒ–ï¼ˆGTPOï¼‰è¿™ä¸€æ–°å‹çš„RLç®—æ³•ï¼Œä¸“ä¸ºè®­ç»ƒLLMsè¿›è¡Œå¤šè½®TIRä»»åŠ¡è®¾è®¡ã€‚GTPOå¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1ï¼‰å›åˆçº§åˆ«çš„å¥–åŠ±åˆ†é…ï¼Œä¸ºå•ä¸ªå›åˆæä¾›ç²¾ç»†çš„åé¦ˆï¼›2ï¼‰åŸºäºå›æŠ¥çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œå°†å½’ä¸€åŒ–æŠ˜æ‰£å›æŠ¥è®¡ç®—ä¸ºä¼˜åŠ¿ï¼›3ï¼‰è‡ªæˆ‘ç›‘ç£å¥–åŠ±å¡‘å½¢ï¼Œåˆ©ç”¨ç”Ÿæˆä»£ç çš„è‡ªæˆ‘ç›‘ç£ä¿¡å·æ¥å¯†é›†ç¨€ç–çš„äºŒå…ƒç»“æœå¯¼å‘å¥–åŠ±ã€‚æˆ‘ä»¬çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒGTPOåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºGRPO 3.0%ï¼Œè¯æ˜äº†å…¶åœ¨æ¨åŠ¨ç°å®ä¸–ç•Œå¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä»»åŠ¡ä¸Šçš„è®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå¦‚ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå­˜åœ¨å¥–åŠ±ç²’åº¦ç²—çš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒåœæ»ã€‚</li>
<li>GTPOé€šè¿‡å¼•å…¥å›åˆçº§åˆ«çš„å¥–åŠ±åˆ†é…ã€åŸºäºå›æŠ¥çš„ä¼˜åŠ¿ä¼°è®¡å’Œè‡ªæˆ‘ç›‘ç£å¥–åŠ±å¡‘å½¢ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>GTPOåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºGRPOã€‚</li>
<li>GTPOå¯ä»¥æé«˜å¤æ‚æ•°å­¦æ¨ç†åœ¨ç°å®ä¸–ç•Œçš„è¡¨ç°ã€‚</li>
<li>GTPOçš„å›åˆçº§åˆ«å¥–åŠ±åˆ†é…å¯ä»¥æä¾›æ›´ç²¾ç»†çš„åé¦ˆï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0174be672f8c8566e4c6f7fced6908e" align="middle">
<img src="https://picx.zhimg.com/v2-c52662aa26324fbad5b3611d2c8262a4" align="middle">
<img src="https://picx.zhimg.com/v2-eb937876ecb1536031149cd991223c38" align="middle">
<img src="https://picx.zhimg.com/v2-bac50fcdff9b7aca087cde5b32f4f76d" align="middle">
<img src="https://picx.zhimg.com/v2-9ca8eaaa704004f1d93a4424410d2dbd" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LLM-Aligned-Geographic-Item-Tokenization-for-Local-Life-Recommendation"><a href="#LLM-Aligned-Geographic-Item-Tokenization-for-Local-Life-Recommendation" class="headerlink" title="LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation"></a>LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation</h2><p><strong>Authors:Hao Jiang, Guoquan Wang, Donglin Zhou, Sheng Yu, Yang Zeng, Wencong Zeng, Kun Gai, Guorui Zhou</strong></p>
<p>Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLMâ€™s geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥é€šè¿‡ä¸ºä¼ ç»Ÿçš„IDåŸºç±»æ–¹æ³•å¢åŠ è¯­ä¹‰æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºäº†åŸºäºæ–‡æœ¬çš„æ¨èã€‚åŸºäºæ–‡æœ¬çš„æ–¹æ³•é€šå¸¸é€šè¿‡æç¤ºè®¾è®¡å¯¹é¡¹ç›®çš„æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡é¡¹ç›®ç¬¦å·åŒ–ç”Ÿæˆç¦»æ•£çš„è¯­ä¹‰IDã€‚ç„¶è€Œï¼Œåœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸­ï¼Œå¦‚æœ¬åœ°ç”Ÿæ´»æœåŠ¡ï¼Œä»…ä»…é€šè¿‡æç¤ºæ³¨å…¥ä½ç½®ä¿¡æ¯æ— æ³•æ•æ‰ç²¾ç»†çš„æ—¶ç©ºç‰¹æ€§å’Œé¡¹ç›®ä¹‹é—´çš„ç°å®ä¸–ç•Œè·ç¦»æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGSIDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæœ¬åœ°ç”Ÿæ´»æ¨èçš„ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½çš„åœ°ç†é¡¹ç›®ç¬¦å·åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„åœ°ç†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ï¼Œï¼ˆ2ï¼‰åˆ†å±‚åœ°ç†é¡¹ç›®ç¬¦å·åŒ–ã€‚åœ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹é½æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªåˆ—è¡¨å¥–åŠ±æ¨¡å‹æ¥æ•æ‰é¡¹ç›®ä¹‹é—´çš„ç°å®ä¸–ç•Œç©ºé—´å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„G-DPOç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹å°†æ³›åŒ–çš„ç©ºé—´çŸ¥è¯†å’ŒååŒä¿¡å·æ³¨å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå…¶è¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚åœ°ç†é¡¹ç›®ç¬¦å·åŒ–ç­–ç•¥ï¼Œå…¶ä¸­ä¸»è¦ç¬¦å·æ˜¯ä»ç¦»æ•£çš„ç©ºé—´å’Œå†…å®¹å±æ€§ä¸­æ´¾ç”Ÿå‡ºæ¥çš„ï¼Œå‰©ä½™ç¬¦å·åˆ™é€šè¿‡ä½¿ç”¨å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åœ°ç†è¡¨ç¤ºå‘é‡è¿›è¡Œç»†åŒ–ã€‚åœ¨å¿«æ‰‹è¡Œä¸šçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLGSIDå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åˆ¤åˆ«å’Œç”Ÿæˆæ¨èæ¨¡å‹ã€‚æ¶ˆèç ”ç©¶ã€å¯è§†åŒ–å’Œæ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„æœ€æ–°è¿›å±•ï¼Œæ–‡æœ¬æ¨èæ–¹æ³•å¾—åˆ°äº†æå‡ã€‚ä¼ ç»ŸåŸºäºIDçš„æ–¹æ³•é€šè¿‡æç¤ºè®¾è®¡å’Œä»¤ç‰ŒåŒ–ç”Ÿæˆç¦»æ•£è¯­ä¹‰IDè¿›è¡Œç¼–ç ï¼Œä½†éš¾ä»¥å¤„ç†ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œå¦‚æœ¬åœ°ç”Ÿæ´»æœåŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LGSIDæ¡†æ¶ï¼ŒåŒ…æ‹¬RLåœ°ç†LLMå¯¹é½å’Œåˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ä¸¤éƒ¨åˆ†ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒLGSIDåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¶…è¶Šå½“å‰ä¸»æµæ¨èæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æœ€æ–°è¿›å±•å¢å¼ºäº†æ–‡æœ¬æ¨èçš„è¯­ä¹‰æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»ŸåŸºäºæ–‡æœ¬çš„æ¨èæ–¹æ³•é€šè¿‡æç¤ºè®¾è®¡å’Œä»¤ç‰ŒåŒ–ç”Ÿæˆç¦»æ•£è¯­ä¹‰IDè¿›è¡Œç¼–ç ã€‚</li>
<li>åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­ï¼Œå¦‚æœ¬åœ°ç”Ÿæ´»æœåŠ¡ï¼Œä»…æ³¨å…¥ä½ç½®ä¿¡æ¯åˆ°æç¤ºä¸­æ— æ³•æ•æ‰ç²¾ç»†çš„ç©ºé—´ç‰¹æ€§å’Œç°å®ä¸–ç•Œä¸­çš„ç‰©å“è·ç¦»æ„ŸçŸ¥ã€‚</li>
<li>LGSIDæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šRLåœ°ç†LLMå¯¹é½å’Œåˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„RLåœ°ç†LLMå¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬è®­ç»ƒåˆ—è¡¨å¥–åŠ±æ¨¡å‹å’Œå¼•å…¥G-DPOç®—æ³•æ¥æ³¨å…¥æ³›åŒ–çš„ç©ºé—´çŸ¥è¯†å’ŒååŒä¿¡å·åˆ°LLMsä¸­ã€‚</li>
<li>åˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ç­–ç•¥è¢«æå‡ºï¼Œå…¶ä¸­ä¸»è¦ä»¤ç‰Œæ¥è‡ªç¦»æ•£çš„ç©ºé—´å’Œå†…å®¹å±æ€§ï¼Œå‰©ä½™ä»¤ç‰Œåˆ™é€šè¿‡LLMå¯¹é½åçš„åœ°ç†è¡¨ç¤ºå‘é‡è¿›è¡Œç²¾ç‚¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-434d737fd2161c80ed46d103602118ec" align="middle">
<img src="https://picx.zhimg.com/v2-eea8378981cec48b985f7f83bffa3110" align="middle">
<img src="https://picx.zhimg.com/v2-dc1252e5818c4b3da60a3cddd47abf07" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a9f0dbda5c3b8c4a573f07af5b919009" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  RescueLens LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a4aaf4b1b8230e6b61efd9d87e0355cc" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  Talk, Snap, Complain Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
