<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  RescueLens LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a9f0dbda5c3b8c4a573f07af5b919009')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-21-æ›´æ–°"><a href="#2025-11-21-æ›´æ–°" class="headerlink" title="2025-11-21 æ›´æ–°"></a>2025-11-21 æ›´æ–°</h1><h2 id="RescueLens-LLM-Powered-Triage-and-Action-on-Volunteer-Feedback-for-Food-Rescue"><a href="#RescueLens-LLM-Powered-Triage-and-Action-on-Volunteer-Feedback-for-Food-Rescue" class="headerlink" title="RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue"></a>RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue</h2><p><strong>Authors:Naveen Raman, Jingwu Tang, Zhiyu Chen, Zheyuan Ryan Shi, Sean Hudson, Ameesh Kapoor, Fei Fang</strong></p>
<p>Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.</p>
<blockquote>
<p>é£Ÿå“æ•‘æ´ç»„ç»‡é€šè¿‡ä¸å¿—æ„¿è€…åˆä½œï¼Œå°†æèµ è€…å¤šä½™çš„é£Ÿç‰©é‡æ–°åˆ†é…ç»™éœ€è¦å®ƒçš„æ¥æ”¶è€…ï¼ŒåŒæ—¶è§£å†³ç²®é£Ÿä¸å®‰å…¨é—®é¢˜å’Œæµªè´¹é—®é¢˜ã€‚å¿—æ„¿è€…çš„åé¦ˆä½¿é£Ÿå“æ•‘æ´ç»„ç»‡èƒ½å¤Ÿæ—©æœŸå‘ç°é—®é¢˜å¹¶ç¡®ä¿å¿—æ„¿è€…æ»¡æ„åº¦ã€‚ç„¶è€Œï¼Œé£Ÿå“æ•‘æ´ç»„ç»‡ç›®å‰é‡‡ç”¨æ‰‹åŠ¨ç›‘æ§åé¦ˆçš„æ–¹å¼ï¼Œè¿™å¯èƒ½æ—¢ç¹çåˆè€—åŠ³åŠ›ï¼Œéš¾ä»¥ç¡®å®šå“ªäº›é—®é¢˜æ˜¯æœ€é‡è¦çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å¸®åŠ©é£Ÿå“æ•‘æ´ç»„ç»‡è€…åŸºäºå¿—æ„¿è€…ç»éªŒç†è§£å’Œé‡‡å–è¡ŒåŠ¨ã€‚æˆ‘ä»¬ä¸ä½äºå®¾å¤•æ³•å°¼äºšå·åŒ¹å…¹å ¡çš„å¤§å‹é£Ÿå“æ•‘æ´ç»„ç»‡412é£Ÿå“æ•‘æ´åˆä½œï¼Œè®¾è®¡äº†ä¸€ä¸ªåä¸ºRescueLensçš„LLMé©±åŠ¨å·¥å…·ï¼Œè¯¥å·¥å…·å¯è‡ªåŠ¨åˆ†ç±»å¿—æ„¿è€…åé¦ˆï¼Œå»ºè®®è·Ÿè¿›çš„æèµ è€…å’Œæ¥æ”¶è€…ï¼Œå¹¶æ ¹æ®åé¦ˆæ›´æ–°å¿—æ„¿è€…æŒ‡ç¤ºã€‚æˆ‘ä»¬åœ¨æ ‡æ³¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†RescueLensçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜å…¶å¯ä»¥æ¢å¤96%çš„å¿—æ„¿è€…é—®é¢˜ï¼Œç²¾åº¦è¾¾åˆ°71%ã€‚æ­¤å¤–ï¼Œæ ¹æ®å¿—æ„¿è€…é—®é¢˜çš„æ¯”ç‡å¯¹æèµ è€…å’Œæ¥æ”¶è€…è¿›è¡Œæ’åï¼ŒRescueLenså…è®¸ç»„ç»‡è€…é‡ç‚¹å…³æ³¨0.5%çš„æèµ è€…ï¼Œè¿™äº›æèµ è€…è´Ÿè´£è¶…è¿‡30%çš„å¿—æ„¿è€…é—®é¢˜ã€‚RescueLensç°åœ¨å·²éƒ¨ç½²åœ¨412é£Ÿå“æ•‘æ´ä¸­ï¼Œé€šè¿‡ä¸ç»„ç»‡è€…è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆï¼Œæˆ‘ä»¬å‘ç°RescueLensç®€åŒ–äº†åé¦ˆæµç¨‹ï¼Œä½¿ç»„ç»‡è€…èƒ½æ›´å¥½åœ°åˆ†é…æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15698v1">PDF</a> Accepted at IAAIâ€™26</p>
<p><strong>Summary</strong></p>
<p>é£Ÿç‰©æ•‘æ´ç»„ç»‡é€šè¿‡å¿—æ„¿è€…å°†å¤šä½™çš„é£Ÿç‰©ä»æèµ è€…é‡æ–°åˆ†é…ç»™éœ€è¦å®ƒçš„æ¥æ”¶è€…ï¼ŒåŒæ—¶è§£å†³ç²®é£Ÿä¸å®‰å…¨å’Œæµªè´¹é—®é¢˜ã€‚å¿—æ„¿è€…åé¦ˆä½¿é£Ÿç‰©æ•‘æ´ç»„ç»‡èƒ½å¤Ÿæ—©æœŸå‘ç°é—®é¢˜å¹¶ç¡®ä¿å¿—æ„¿è€…æ»¡æ„åº¦ã€‚ç„¶è€Œï¼Œé£Ÿç‰©æ•‘æ´ç»„ç»‡ç›®å‰ä¸»è¦é€šè¿‡æ‰‹åŠ¨æ–¹å¼ç›‘æ§åé¦ˆï¼Œè¿™æ—¢ç¹çåˆåŠ³åŠ›å¯†é›†ï¼Œéš¾ä»¥ç¡®å®šå“ªäº›é—®é¢˜æ˜¯æœ€é‡è¦çš„ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å¸®åŠ©é£Ÿç‰©æ•‘æ´ç»„ç»‡è€…æ ¹æ®å¿—æ„¿è€…çš„ç»éªŒè¿›è¡Œç†è§£å’Œè¡ŒåŠ¨ã€‚æˆ‘ä»¬ä¸ä½äºå®¾å¤•æ³•å°¼äºšå·åŒ¹å…¹å ¡çš„å¤§å‹é£Ÿç‰©æ•‘æ´ç»„ç»‡412 Food Rescueåˆä½œï¼Œè®¾è®¡äº†RescueLenså·¥å…·ï¼Œè¯¥å·¥å…·åˆ©ç”¨LLMè‡ªåŠ¨åˆ†ç±»å¿—æ„¿è€…åé¦ˆï¼Œå»ºè®®è·Ÿè¿›çš„æèµ è€…å’Œæ¥æ”¶è€…ï¼Œå¹¶æ ¹æ®åé¦ˆæ›´æ–°å¿—æ„¿è€…æŒ‡å—ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒRescueLensèƒ½å¤Ÿæ¢å¤96%çš„å¿—æ„¿è€…é—®é¢˜ï¼Œç²¾åº¦ä¸º71%ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ ¹æ®å¿—æ„¿è€…é—®é¢˜çš„é¢‘ç‡å¯¹æèµ è€…å’Œæ¥æ”¶è€…è¿›è¡Œæ’åï¼ŒRescueLenså¸®åŠ©ç»„ç»‡è€…ä¸“æ³¨äºè§£å†³ç”±ä»…å 0.5%çš„æèµ è€…å¼•èµ·çš„è¶…è¿‡30%çš„å¿—æ„¿è€…é—®é¢˜ã€‚RescueLensç°å·²åœ¨412 Food Rescueéƒ¨ç½²ï¼Œé€šè¿‡ä¸éç»“æ„åŒ–è®¿è°ˆå‘ç°ï¼Œå®ƒç®€åŒ–äº†åé¦ˆæµç¨‹ï¼Œä½¿ç»„ç»‡è€…èƒ½æ›´å¥½åœ°åˆ†é…æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£Ÿç‰©æ•‘æ´ç»„ç»‡è‡´åŠ›äºè§£å†³é£Ÿå“æµªè´¹å’Œé£Ÿå“ä¸å®‰å…¨çš„é—®é¢˜ã€‚</li>
<li>å¿—æ„¿è€…åé¦ˆåœ¨é£Ÿç‰©æ•‘æ´ç»„ç»‡ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œå…è®¸æ—©æœŸå‘ç°é—®é¢˜å¹¶ç¡®ä¿å¿—æ„¿è€…æ»¡æ„åº¦ã€‚</li>
<li>ç›®å‰é£Ÿç‰©æ•‘æ´ç»„ç»‡ä¸»è¦é€šè¿‡æ‰‹åŠ¨å¤„ç†å¤§é‡åé¦ˆæ•°æ®ï¼Œæ•ˆç‡è¾ƒä½ä¸”éš¾ä»¥ä¼˜å…ˆå¤„ç†å…³é”®é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é£Ÿç‰©æ•‘æ´ç»„ç»‡ä¸­å…·æœ‰åº”ç”¨ä»·å€¼ï¼Œå¯å¸®åŠ©å¤„ç†å’Œåˆ†æå¿—æ„¿è€…åé¦ˆã€‚</li>
<li>ç ”ç©¶ä¸­è®¾è®¡çš„RescueLenså·¥å…·èƒ½å¤Ÿè‡ªåŠ¨åˆ†ç±»å¿—æ„¿è€…åé¦ˆï¼Œå¹¶å»ºè®®è·Ÿè¿›çš„æèµ è€…å’Œæ¥æ”¶è€…ä»¥åŠæ›´æ–°å¿—æ„¿è€…æŒ‡å—ã€‚</li>
<li>RescueLenså…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿåœ¨ç²¾ç¡®æ€§è¾ƒé«˜çš„å‰æä¸‹æ¢å¤å¤§éƒ¨åˆ†å¿—æ„¿è€…é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-133664123e140595cd29c8f671a1a53d" align="middle">
<img src="https://picx.zhimg.com/v2-9fe53bb0f4d40de81a8b6eb024201e03" align="middle">
<img src="https://picx.zhimg.com/v2-f1c29bef29664da689e57a7f0c7891ee" align="middle">
<img src="https://picx.zhimg.com/v2-1e0d029071b8f9a3e600553e498a26b0" align="middle">
<img src="https://picx.zhimg.com/v2-e6b8d242ceacfc77968e1fbd55b3b82e" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MoDES-Accelerating-Mixture-of-Experts-Multimodal-Large-Language-Models-via-Dynamic-Expert-Skipping"><a href="#MoDES-Accelerating-Mixture-of-Experts-Multimodal-Large-Language-Models-via-Dynamic-Expert-Skipping" class="headerlink" title="MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping"></a>MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</h2><p><strong>Authors:Yushi Huang, Zining Wang, Zhihang Yuan, Yifu Ding, Ruihao Gong, Jinyang Guo, Xianglong Liu, Jun Zhang</strong></p>
<p>Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.</p>
<blockquote>
<p>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†å‡å°‘æ¨ç†å¼€é”€ï¼Œäººä»¬æå‡ºäº†ä¸“å®¶è·³è¿‡æ–¹æ³•ï¼Œæ ¹æ®å½“å‰çš„è¾“å…¥ä»¤ç‰Œæ¥åœç”¨å¤šä½™çš„ä¸“å®¶ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å°†è¿™äº›åŸæœ¬ä¸ºå•æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„æ–¹æ³•åº”ç”¨äºMLLMsä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºè¿™äº›æ–¹æ³•æœªèƒ½è€ƒè™‘åˆ°MoEå±‚ä¸­ä¸“å®¶ä¹‹é—´çš„å¼‚è´¨è´¡çŒ®ä»¥åŠè¿™äº›å±‚å†…ä»¤ç‰Œç‰¹æœ‰çš„è·¨æ¨¡æ€è¡Œä¸ºã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†MoDESï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯è‡ªé€‚åº”è·³è¿‡ä¸“å®¶çš„æ¡†æ¶ï¼Œå¯å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„MoE MLLMæ¨ç†ã€‚å®ƒç»“åˆäº†å…¨å±€è°ƒæ§çš„å±€éƒ¨é—¨æ§ï¼ˆGMLGï¼‰æœºåˆ¶ï¼Œå°†å…¨å±€å±‚é‡è¦æ€§èå…¥å±€éƒ¨è·¯ç”±æ¦‚ç‡ä¸­ï¼Œä»¥å‡†ç¡®ä¼°è®¡æ¯ä¸ªä»¤ç‰Œä¸“å®¶çš„é‡è¦æ€§ã€‚ç„¶ååº”ç”¨åŒæ¨¡æ€é˜ˆå€¼ï¼ˆDMTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«å¤„ç†æ¥è‡ªæ¯ä¸ªæ¨¡æ€çš„ä»¤ç‰Œï¼Œä»¥å¾—å‡ºè·³è¿‡æ—¶é—´è¡¨ã€‚ä¸ºäº†è®¾ç½®æœ€ä½³é˜ˆå€¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‰æ²¿æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å•è°ƒæ€§å±æ€§ï¼Œå°†æ”¶æ•›æ—¶é—´ä»å‡ å¤©ç¼©çŸ­åˆ°å‡ å°æ—¶ã€‚å¯¹ä¸‰ä¸ªæ¨¡å‹ç³»åˆ—è¿›è¡Œå¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè·¨è¶Š13ä¸ªåŸºå‡†æµ‹è¯•é›†è¡¨æ˜ï¼ŒMoDESå¤§å¤§ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸ºQwen3-VL-MoE-30B-A3B-Instructè·³è¿‡88%çš„ä¸“å®¶æ—¶ï¼Œæ€§èƒ½æå‡äº†é«˜è¾¾10.67%ï¼ˆä»åŸæ¥çš„æå‡åˆ°ï¼‰ã€‚æ­¤å¤–ï¼ŒMoDESè¿˜æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŠ å¿«å¡«å……æ—¶é—´é«˜è¾¾2.16å€ï¼Œè§£ç æ—¶é—´é«˜è¾¾1.26å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15690v1">PDF</a> Code will be released upon acceptance</p>
<p><strong>æ‘˜è¦</strong></p>
<p>MoEï¼ˆMixture-of-Expertsï¼‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºå‡å°‘æ¨ç†å¼€é”€ï¼Œæå‡ºäº†åŸºäºå½“å‰è¾“å…¥ä»¤ç‰Œåœç”¨å†—ä½™ä¸“å®¶çš„ä¸“å®¶è·³è¿‡æ–¹æ³•ã€‚ç„¶è€Œï¼Œå°†åŸæœ¬ä¸ºå•æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„æ–¹æ³•åº”ç”¨äºMLLMsä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸»è¦æ˜¯å› ä¸ºè¿™äº›æ–¹æ³•å¿½ç•¥äº†MoEå±‚ä¸­ä¸“å®¶çš„å¼‚è´¨è´¡çŒ®å’Œè¿™äº›å±‚å†…ä»£å¸çš„æ¨¡æ€ç‰¹å®šè¡Œä¸ºã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMoDESï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è‡ªé€‚åº”è·³è¿‡ä¸“å®¶æ¡†æ¶ï¼Œå¯å®ç°MoE MLLMçš„é«˜æ•ˆä¸”å‡†ç¡®æ¨ç†ã€‚å®ƒç»“åˆäº†å…¨å±€è°ƒæ§çš„æœ¬åœ°é—¨æ§ï¼ˆGMLGï¼‰æœºåˆ¶ï¼Œå°†å…¨å±€å±‚é‡è¦æ€§èå…¥æœ¬åœ°è·¯ç”±æ¦‚ç‡ï¼Œä»¥å‡†ç¡®ä¼°è®¡æ¯ä¸ªä»£å¸çš„ä¸“å®¶é‡è¦æ€§ã€‚ç„¶ååº”ç”¨åŒæ¨¡æ€é˜ˆå€¼ï¼ˆDMTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«å¤„ç†æ¯ç§æ¨¡æ€çš„ä»£å¸ï¼Œä»¥å¾—å‡ºè·³è¿‡æ—¶é—´è¡¨ã€‚ä¸ºäº†è®¾ç½®æœ€ä½³é˜ˆå€¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‰æ²¿æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å•è°ƒæ€§å±æ€§ï¼Œå°†æ”¶æ•›æ—¶é—´ä»å‡ å¤©å‡å°‘åˆ°å‡ å°æ—¶ã€‚å¹¿æ³›å®éªŒè¯æ˜MoDESè¿œè¶…å…ˆå‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå¯¹äºQwen3-VL-MoE-30B-A3B-Instructæ¨¡å‹è·³è¿‡88%çš„ä¸“å®¶æ—¶ï¼Œæ€§èƒ½æå‡é«˜è¾¾10.67%ï¼ˆ97.33%å¯¹æ¯”86.66%ï¼‰ã€‚æ­¤å¤–ï¼ŒMoDESæ˜¾ç€æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŠ å¿«å¡«å……æ—¶é—´2.16å€å’Œç¼–ç æ—¶é—´1.26å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MoE MLLMsåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ä½†è®¡ç®—æ•ˆç‡ä½ã€‚</li>
<li>ç›´æ¥å°†é’ˆå¯¹LLMsè®¾è®¡çš„ä¸“å®¶è·³è¿‡æ–¹æ³•åº”ç”¨äºMLLMsä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºMoDESæ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ä¸“å®¶è‡ªé€‚åº”è·³è¿‡æ–¹æ³•ï¼Œä»¥æé«˜MoE MLLMçš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MoDESç»“åˆGMLGæœºåˆ¶ï¼Œé€šè¿‡è€ƒè™‘å…¨å±€å±‚é‡è¦æ€§æ¥ä¼°è®¡æ¯ä¸ªä»£å¸çš„ä¸“å®¶é‡è¦æ€§ã€‚</li>
<li>MoDESé‡‡ç”¨DMTæ–¹æ³•å¤„ç†æ¯ç§æ¨¡æ€çš„ä»£å¸ï¼Œå¹¶è®¾ç½®æœ€ä½³é˜ˆå€¼ä»¥å†³å®šä¸“å®¶è·³è¿‡ã€‚</li>
<li>åˆ©ç”¨å‰æ²¿æœç´¢ç®—æ³•å¿«é€Ÿè®¾ç½®é˜ˆå€¼ï¼Œæé«˜æ”¶æ•›é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e7efd59a848d61a679a02e3bbc2d000" align="middle">
<img src="https://picx.zhimg.com/v2-fe6963dcbf978e10744879562582b972" align="middle">
<img src="https://picx.zhimg.com/v2-4e1f866f3b3baa3ea32929d6a1460519" align="middle">
<img src="https://picx.zhimg.com/v2-49a02d453f8abd624a17381e5cdfb81e" align="middle">
<img src="https://picx.zhimg.com/v2-52dbef05165bcfa3e5acb65409c1473e" align="middle">
<img src="https://picx.zhimg.com/v2-fbf98593051ff4ba1e68e4ac6b8b2b85" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VisPlay-Self-Evolving-Vision-Language-Models-from-Images"><a href="#VisPlay-Self-Evolving-Vision-Language-Models-from-Images" class="headerlink" title="VisPlay: Self-Evolving Vision-Language Models from Images"></a>VisPlay: Self-Evolving Vision-Language Models from Images</h2><p><strong>Authors:Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang</strong></p>
<p>Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at <a target="_blank" rel="noopener" href="https://bruno686.github.io/VisPlay/">https://bruno686.github.io/VisPlay/</a></p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ä»»åŠ¡ç‰¹å®šçš„å¯å‘å¼æ–¹æ³•æ¥å®šä¹‰å¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™ä¸¤è€…éƒ½æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬æ¨å‡ºäº†VisPlayï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè®©VLMä½¿ç”¨å¤§é‡çš„æœªæ ‡è®°å›¾åƒæ•°æ®è‡ªä¸»åœ°æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚ä»å•ä¸ªåŸºç¡€VLMå‡ºå‘ï¼ŒVisPlayå°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªäº¤äº’è§’è‰²ï¼šå›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…ã€‚å›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…ä¼šåˆ¶å®šå…·æœ‰æŒ‘æˆ˜æ€§ä½†å¯å›ç­”çš„è§†è§‰é—®é¢˜ï¼Œè€Œå¤šæ¨¡æ€æ¨ç†è€…åˆ™ä¼šäº§ç”Ÿé“¶è´¨å›åº”ã€‚è¿™ä¸¤ä¸ªè§’è‰²é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè”åˆè®­ç»ƒï¼ŒGRPOç»“åˆäº†å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±ï¼Œä»¥å¹³è¡¡ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§ä¸é“¶è´¨ç­”æ¡ˆçš„è´¨é‡ã€‚VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¹‹é—´å®ç°äº†æœ‰æ•ˆæ‰©å±•ã€‚åœ¨Qwen2.5-VLå’ŒMiMo-VLä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒVisPlayåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è§†è§‰æ¨ç†ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰å‡å°‘æ–¹é¢çš„ä¸€è‡´æ”¹è¿›ï¼ŒåŒ…æ‹¬MM-Vetå’ŒMMMUï¼Œå±•ç¤ºäº†è‡ªæˆ‘è¿›åŒ–å¤šæ¨¡æ€æ™ºèƒ½çš„å¯æ‰©å±•è·¯å¾„ã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://bruno686.github.io/VisPlay/%E8%AE%BF%E9%97%AE%E3%80%82">https://bruno686.github.io/VisPlay/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15661v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æå‡æä¾›äº†åŸåˆ™æ€§çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLæ–¹æ³•å¸¸å¸¸ä¾èµ–äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ä»»åŠ¡ç‰¹å®šçš„å¯å‘å¼æ–¹æ³•æ¥å®šä¹‰å¯éªŒè¯çš„å¥–åŠ±ï¼Œè¿™äº›æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚æˆ‘ä»¬æ¨å‡ºäº†VisPlayï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„RLæ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿VLMè‡ªä¸»åœ°æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨å¤§é‡çš„æœªæ ‡è®°å›¾åƒæ•°æ®ã€‚VisPlayé€šè¿‡ä¸¤ä¸ªäº¤äº’è§’è‰²â€”â€”å›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…â€”â€”è¿›è¡Œè®­ç»ƒã€‚æé—®è€…åˆ¶å®šå…·æœ‰æŒ‘æˆ˜æ€§ä½†å¯å›ç­”çš„è§†è§‰é—®é¢˜ï¼Œè€Œæ¨ç†è€…ç”Ÿæˆé“¶è´¨ç­”æ¡ˆã€‚è¿™ä¸¤ä¸ªè§’è‰²é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è”åˆè®­ç»ƒï¼Œä»¥èå…¥å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±æ¥å¹³è¡¡ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§ä¸é“¶è´¨ç­”æ¡ˆçš„è´¨é‡ã€‚VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¹‹é—´å®ç°é«˜æ•ˆæ‰©å±•ã€‚åœ¨Qwen2.5-VLå’ŒMiMo-VLä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒVisPlayåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è§†è§‰æ¨ç†ã€ç»„åˆæ³›åŒ–å’Œå¹»è§‰å‡å°‘æ–¹é¢çš„ä¸€è‡´æ”¹è¿›ï¼ŒåŒ…æ‹¬MM-Vetå’ŒMMMUç­‰ã€‚è¯¥é¡¹ç›®çš„ç½‘é¡µåœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://bruno686.github.io/VisPlay/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¸¸å¸¸ä¾èµ–äººå·¥æ ‡æ³¨çš„æ ‡ç­¾å’Œä»»åŠ¡ç‰¹å®šå¯å‘å¼æ–¹æ³•ï¼Œæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚</li>
<li>VisPlayæ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„RLæ¡†æ¶ï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°å›¾åƒæ•°æ®ï¼Œä½¿VLMè‡ªä¸»æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VisPlayé€šè¿‡å›¾åƒæ¡ä»¶ä¸‹çš„æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…çš„ä¸¤ä¸ªäº¤äº’è§’è‰²è¿›è¡Œè®­ç»ƒã€‚</li>
<li>VisPlayé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œèå…¥å¤šæ ·æ€§å’Œéš¾åº¦å¥–åŠ±ï¼Œå¹³è¡¡ç”Ÿæˆé—®é¢˜çš„å¤æ‚æ€§å’Œé“¶è´¨ç­”æ¡ˆçš„è´¨é‡ã€‚</li>
<li>VisPlayåœ¨ä¸¤ä¸ªæ¨¡å‹å®¶æ—ä¹‹é—´å®ç°é«˜æ•ˆæ‰©å±•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°è§†è§‰æ¨ç†æ€§èƒ½çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc45436b748bc6b04f2f4b86e848760c" align="middle">
<img src="https://picx.zhimg.com/v2-09318d2d6c3eb1405c923b633daa2abe" align="middle">
<img src="https://picx.zhimg.com/v2-6558fb071e7121f8b0c8bf6aada8cb2e" align="middle">
<img src="https://picx.zhimg.com/v2-01fe54a40811489f24df683ebc487d9d" align="middle">
<img src="https://picx.zhimg.com/v2-854d170801eba52b7e05b4c57ee34a9a" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-SA-FARI-Dataset-Segment-Anything-in-Footage-of-Animals-for-Recognition-and-Identification"><a href="#The-SA-FARI-Dataset-Segment-Anything-in-Footage-of-Animals-for-Recognition-and-Identification" class="headerlink" title="The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification"></a>The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</h2><p><strong>Authors:Dante Francisco Wasmuht, Otto Brookes, Maximillian Schall, Pablo Palencia, Chris Beirne, Tilo Burghardt, Majid Mirmehdi, Hjalmar KÃ¼hl, Mimi Arandjelovic, Sam Pottie, Peter Bermant, Brandon Asheim, Yi Jin Toh, Adam Elzinga, Jason Holmberg, Andrew Whitworth, Eleanor Flatt, Laura Gustafson, Chaitanya Ryali, Yuan-Ting Hu, Baishan Guo, Andrew Westbury, Kate Saenko, Didac Suris</strong></p>
<p>Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\href{<a target="_blank" rel="noopener" href="https://www.conservationxlabs.com/sa-fari%7D%7B/text%7Bconservationxlabs.com/SA-FARI%7D%7D$">https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}$</a>.</p>
<blockquote>
<p>è‡ªåŠ¨åŒ–è§†é¢‘åˆ†æåœ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªåŸºç¡€ä»»åŠ¡æ˜¯å¤šç§åŠ¨ç‰©è·Ÿè¸ªï¼ˆMATï¼‰ï¼Œå®ƒæ˜¯ä¸ªä½“å†è¯†åˆ«å’Œè¡Œä¸ºè¯†åˆ«ç­‰åº”ç”¨çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ä¸Šæœ‰é™åˆ¶ï¼Œä»…é™äºå°‘æ•°ç‰©ç§ï¼Œæˆ–è€…ç¼ºä¹è¶³å¤Ÿçš„æ—¶é—´å’Œåœ°ç†å¤šæ ·æ€§ï¼Œå› æ­¤æ²¡æœ‰åˆé€‚çš„åŸºå‡†æ¥è®­ç»ƒé€‚ç”¨äºé‡ç”ŸåŠ¨ç‰©ç¾¤ä½“çš„é€šç”¨MATæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SA-FARIï¼Œè¿™æ˜¯æœ€å¤§çš„é‡ç”ŸåŠ¨ç‰©å¼€æ”¾æºä»£ç MATæ•°æ®é›†ã€‚å®ƒåŒ…å«äº†å¤§çº¦åå¹´ï¼ˆ2014å¹´è‡³2024å¹´ï¼‰å†…ä»å››å¤§æ´²çš„741ä¸ªåœ°ç‚¹æ”¶é›†çš„å…±åŒ…å«çº¦æ”¶é›†å…±åŒ…å«å…±åŒ…å«åŒ…å«åŠ¨ç‰©è§†é¢‘çš„æ‘„åƒæœºé™·é˜±å½•åƒã€‚æ•°æ®é›†å…±åŒ…æ‹¬æœ‰æ¥è‡ªå››å¤§æ´²è¶…è¿‡æ¨ªè·¨æ¶‰åŠè¦†ç›–å››å¤§æ´²æ¨ªè·¨å…±æ¶µç›–ä¸ªè·¨è¶Šä¸åŒåœ°ç†ä½ç½®çš„åœ°ç‚¹ï¼Œè·¨è¶Šç‰©ç§ç±»åˆ«ã€‚æ¯ä¸ªè§†é¢‘éƒ½è¿›è¡Œäº†è¯¦å°½çš„æ ‡æ³¨ï¼Œæœ€ç»ˆå½¢æˆäº†çº¦å¯†é›†æ ‡æ³¨çš„è§†é¢‘ç‰‡æ®µå°æ—¶å¯†é›†æ ‡æ³¨çš„è§†é¢‘ç‰‡æ®µï¼ŒåŒ…å«ä¸ªåŠ¨ç‰©èº«ä»½å’Œä¸ªä¸ªä½“è¾¹ç•Œæ¡†ç­‰ç‰©ç§æ ‡ç­¾å’Œè¾¹ç•Œæ¡†åˆ†å‰²æ©ç ä»¥åŠç‰©ç§æ ‡ç­¾ã€‚é™¤äº†é’ˆå¯¹ä»»åŠ¡çš„æ ‡æ³¨å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºæ¯æ¡è§†é¢‘å‘å¸ƒäº†åŒ¿åæ‘„åƒæœºé™·é˜±ä½ç½®ã€‚æœ€åæˆ‘ä»¬åœ¨SA-FARIæ•°æ®é›†ä¸Šæä¾›äº†å…³äºå…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹å’Œæœ€æ–°çš„è‡ªç„¶è¯­è¨€ç†è§£æŠ€æœ¯æ„å»ºçš„æ–¹æ³•å’Œå‰æ²¿çš„æŠ€æœ¯åŸºå‡†è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…æ‹¬ä½¿ç”¨ç‰¹å®šç‰©ç§å’Œé€šç”¨åŠ¨ç‰©æç¤ºçš„SAM 3æ£€æµ‹ä¸è¿½è¸ªæ€§èƒ½è¯„ä¼°æŠ¥å‘Šã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†ä¸“ä¸ºé‡ç”ŸåŠ¨ç‰©åˆ†æå¼€å‘çš„è§†è§‰æ–¹æ³•ã€‚SA-FARIæ˜¯é¦–ä¸ªç»“åˆé«˜ç‰©ç§å¤šæ ·æ€§ã€å¤šåŒºåŸŸè¦†ç›–å’Œé«˜è´¨é‡æ—¶ç©ºæ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸ºæ¨è¿›é‡å¤–é€šç”¨å¤šåŠ¨ç‰©è·Ÿè¸ªæä¾›äº†æ–°çš„åŸºç¡€ã€‚æ•°æ®é›†å¯åœ¨ç½‘å€ä¸Šè¿›è¡Œè®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://www.conservationxlabs.com/sa-fari">conservationxlabs.com&#x2F;SA-FARI</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15622v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è‡ªåŠ¨åŒ–è§†é¢‘åˆ†æåœ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å…¶ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡æ˜¯è¿›è¡Œå¤šåŠ¨ç‰©è¿½è¸ªï¼ˆMATï¼‰ï¼Œå®ƒä¸ºä¸ªä½“å†è¯†åˆ«å’Œè¡Œä¸ºè¯†åˆ«ç­‰åº”ç”¨æä¾›äº†æ”¯æŒã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†è§„æ¨¡æœ‰é™ï¼Œä»…é™äºå°‘æ•°ç‰©ç§ï¼Œç¼ºä¹è¶³å¤Ÿçš„æ—¶é—´å’Œåœ°ç†å¤šæ ·æ€§ï¼Œæ²¡æœ‰åˆé€‚çš„åŸºå‡†æ¥è®­ç»ƒé€‚ç”¨äºé‡ç”ŸåŠ¨ç‰©ç§ç¾¤çš„å¤šç”¨é€”MATæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SA-FARIæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç”¨äºé‡ç”ŸåŠ¨ç‰©çš„å¼€æ”¾å¤šåŠ¨ç‰©è¿½è¸ªæ•°æ®é›†ã€‚å®ƒç”±ä»å››å¤§æ´²çš„741ä¸ªåœ°ç‚¹æ”¶é›†çš„è¶…è¿‡å¤§çº¦åå¹´çš„ï¼ˆå³æˆªæ­¢è‡³ç°åœ¨åå¢åŠ çš„åŠ ä¸Šè¿‡å¾€çš„å››äº”å¹´å†å²æ—¶é—´æ®µæ˜¯ç›¸å½“äºæ•´ä½“çš„çº¦åå¹´æ”¶é›†ï¼‰æ¶‰åŠåŒ…æ‹¬ç¨€æœ‰å¤§ç‰©ç§å…±è®¡çš„å°†è¿‘ä¸€ä¸‡ä¸¤åƒä¸ªç‰©ç§çš„è§†é¢‘ç»„æˆã€‚æ¯ä¸ªè§†é¢‘éƒ½è¿›è¡Œäº†è¯¦å°½çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬å¤§çº¦æ ‡æ³¨äº†çº¦ä¸€ä¸‡å…­åƒå¤šä¸ªé¢å…·èº«ä»½å’Œä¹åå››ä¸‡å¤šä¸ªä¸ªä½“è¾¹ç•Œæ¡†ï¼Œåˆ†å‰²æ©æ¨¡å’Œç‰©ç§æ ‡ç­¾çš„æ—¶é—´åºåˆ—ç‰‡æ®µè§†é¢‘ä¿¡æ¯è¿‘è¾¾å››åå…­å°æ—¶æ—¶é•¿ï¼Œä»è€Œå½¢æˆäº†å¯†é›†æ ‡æ³¨çš„å½±åƒèµ„æ–™ã€‚é™¤äº†ç‰¹å®šä»»åŠ¡çš„æ³¨é‡Šå¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†æ¯ä¸ªè§†é¢‘çš„åŒ¿åç›¸æœºé™·é˜±ä½ç½®ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å¯¹SA-FARIè¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ä»…ä¸“æ³¨äºé‡ç”ŸåŠ¨ç‰©åˆ†æçš„è§†è§‰æ–¹æ³•çš„å¯¹æ¯”åˆ†æè¯æ˜ï¼ŒSA-FARIæ•°æ®é›†æ˜¯é¦–ä¸ªç»“åˆé«˜ç‰©ç§å¤šæ ·æ€§ã€å¤šåŒºåŸŸè¦†ç›–å’Œé«˜è´¨é‡æ—¶ç©ºæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸ºæ¨è¿›é€šç”¨çš„å¤šåŠ¨ç‰©é‡å¤–è¿½è¸ªæä¾›äº†åšå®çš„åŸºç¡€ã€‚è¯¥æ•°æ®é›†å¯åœ¨ <a target="_blank" rel="noopener" href="https://www.conservationxlabs.com/sa-fari">conservationxlabs.com&#x2F;SA-FARI</a> è·å¾—ã€‚è¯¥æ•°æ®é›†å°†æˆä¸ºæ¨åŠ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤ç ”ç©¶çš„é‡è¦å·¥å…·ã€‚å®ƒä¸ä»…ä¿ƒè¿›äº†æŠ€æœ¯çš„å¼€å‘å’Œåº”ç”¨ï¼Œè¿˜åŠ å¼ºäº†ç§‘ç ”äººå‘˜å¯¹é‡ç”ŸåŠ¨ç‰©çš„äº†è§£å’Œä¿æŠ¤æ„è¯†ã€‚æœªæ¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨è¯¥æ•°æ®é›†è¿›è¡Œæ›´æ·±å…¥çš„ç‰©ç§è¯†åˆ«å’Œå¤šæ¨¡æ€æ•°æ®èåˆåˆ†æï¼Œä»¥æ¨åŠ¨é‡ç”ŸåŠ¨ç‰©ä¿æŠ¤å·¥ä½œçš„è¿›æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>SA-FARIæ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç”¨äºé‡ç”ŸåŠ¨ç‰©çš„å¼€æ”¾å¤šåŠ¨ç‰©è¿½è¸ªæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è·¨è¶Šå››å¤§æ´²ä¸ƒä¸ªåœ°ç‚¹çš„è¿‘ä¸€ä¸‡ä¸¤åƒä¸ªç‰©ç§çš„è§†é¢‘ï¼Œé•¿è¾¾åå¹´çš„æ”¶é›†æœŸå¸¦æ¥ä¸°å¯Œçš„æ—¶é—´ä¸åœ°ç†å¤šæ ·æ€§ã€‚</li>
<li>æ•°æ®é›†ä¸­åŒ…å«å¤§é‡çš„å¯†é›†æ ‡æ³¨å½±åƒèµ„æ–™ï¼ŒåŒ…å«é¢éƒ¨èº«ä»½æ ‡æ³¨ã€è¾¹ç•Œæ¡†ã€åˆ†å‰²æ©æ¨¡å’Œç‰©ç§æ ‡ç­¾ç­‰è¯¦ç»†ä¿¡æ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf3b08132120c1d1b52b16503df57e4c" align="middle">
<img src="https://picx.zhimg.com/v2-90a9184e2c48b17580333b27447c9397" align="middle">
<img src="https://picx.zhimg.com/v2-550772c4120614e982c256b0b41235a9" align="middle">
<img src="https://picx.zhimg.com/v2-12b5f3ed61c45e520f7e3012a2d86682" align="middle">
<img src="https://picx.zhimg.com/v2-88453cb61d6b3c2a8c310cbc97eff3c4" align="middle">
<img src="https://picx.zhimg.com/v2-5942f0dc6b43c3e36593dedb5433d335" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="When-to-Think-and-When-to-Look-Uncertainty-Guided-Lookback"><a href="#When-to-Think-and-When-to-Look-Uncertainty-Guided-Lookback" class="headerlink" title="When to Think and When to Look: Uncertainty-Guided Lookback"></a>When to Think and When to Look: Uncertainty-Guided Lookback</h2><p><strong>Authors:Jing Bi, Filippos Bellos, Junjia Guo, Yayuan Li, Chao Huang,  Yunlong,  Tang, Luchuan Song, Susan Liang,  Zhongfei,  Zhang, Jason J. Corso, Chenliang Xu</strong></p>
<p>Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.</p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„æ€è€ƒï¼ˆå³ç”Ÿæˆæ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾ï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å·²çŸ¥å¯ä»¥æé«˜æ€§èƒ½ï¼Œæœ€è¿‘ä¹Ÿæ˜¾ç¤ºå‡ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¼ºåŠ²å¢é•¿ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›ç»“æœå¾ˆæœ‰å¸Œæœ›ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰å…³äºæ€è€ƒå¦‚ä½•å®é™…å½±å“è§†è§‰æ¨ç†çš„ç³»ç»Ÿæ€§åˆ†æã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹LVLMsçš„æ€è€ƒè¿›è¡Œäº†å¤§è§„æ¨¡ã€å—æ§çš„æ¯”è¾ƒåˆ†æï¼Œè¯„ä¼°äº†InternVL 3.5å’ŒQwen3-VLå®¶æ—ä¸­çš„åä¸ªå˜ä½“åœ¨MMMU-valä¸‹çš„é€šç”¨ç¬¦å·é¢„ç®—å’Œå¤šé€šé“è§£ç ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ›´å¤šçš„æ€è€ƒå¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼›é•¿é“¾é€šå¸¸ä¼šå¯¼è‡´å¿½ç•¥å›¾åƒçš„é•¿é”™è¯¯è½¨è¿¹ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†æŒ‡ä»¤æ¨¡å¼ä¸‹è¿è¡Œç›¸åŒçš„æ¨¡å‹è¡¨ç°è¾ƒå·®ã€‚æ›´æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼ŒæŸäº›çŸ­å›é¡¾çŸ­è¯­ï¼ˆæ˜ç¡®æŒ‡å›å›¾åƒï¼‰åœ¨æˆåŠŸçš„è½¨è¿¹ä¸­ä¸°å¯Œä¸”ä¸æ›´å¥½çš„è§†è§‰å®šä½ç›¸å…³ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ç¡®å®šæ€§å¼•å¯¼å›é¡¾ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç­–ç•¥ï¼Œç»“åˆäº†ä¸ç¡®å®šæ€§ä¿¡å·ä¸è‡ªé€‚åº”å›é¡¾æç¤ºå’Œå¹¿åº¦æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ•´ä½“çš„MMMUæ€§èƒ½ï¼Œåœ¨æ ‡å‡†æ€è€ƒè¾ƒå¼±çš„ç±»åˆ«ä¸­å–å¾—äº†æœ€å¤§çš„æ”¶ç›Šï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªå¼ºå¤§çš„è§£ç åŸºçº¿ï¼Œåœ¨å›ºå®šçš„æ¨¡å‹å®¶æ—å’Œç¬¦å·é¢„ç®—ä¸‹è¾¾åˆ°äº†æœ€æ–°çš„å…ˆè¿›æ°´å¹³ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¿™ç§è§£ç ç­–ç•¥å¯ä»¥æ¨å¹¿ï¼Œåœ¨å¦å¤–äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿäº†æŒç»­ä¸€è‡´çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå¹¿æ³›çš„å¤šæ¨¡å¼å¥—ä»¶å’Œæ•°å­¦èšç„¦çš„è§†è§‰æ¨ç†æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15613v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æµ‹è¯•æ—¶çš„æ€è€ƒï¼ˆå³ç”Ÿæˆæ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾ï¼‰å·²çŸ¥å¯ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æœ€è¿‘æ˜¾ç¤ºå‡ºå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¼ºçƒˆå¢ç›Šã€‚ç„¶è€Œï¼Œå°½ç®¡ç»“æœä»¤äººé¼“èˆï¼Œä½†ç›®å‰ä»æ²¡æœ‰ç³»ç»Ÿåˆ†ææ€è€ƒå¦‚ä½•å½±å“è§†è§‰æ¨ç†ã€‚æˆ‘ä»¬é¦–æ¬¡è¿›è¡Œäº†è¿™æ ·çš„åˆ†æï¼Œå¯¹LVLMsçš„æ€è€ƒè¿›è¡Œäº†å¤§è§„æ¨¡ã€å—æ§çš„æ¯”è¾ƒï¼Œè¯„ä¼°äº†InternVL3.5å’ŒQwen3-VLå®¶æ—çš„åä¸ªå˜ç§åœ¨MMMU-valä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨æ…·æ…¨çš„ä»¤ç‰Œé¢„ç®—å’Œå¤šé€šé“è§£ç ä¸‹ã€‚æˆ‘ä»¬å‘ç°æ›´å¤šçš„æ€è€ƒå¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼›é•¿é“¾å¸¸å¸¸å¯¼è‡´å¿½è§†å›¾åƒå¹¶äº§ç”Ÿé”™è¯¯çš„è½¨è¿¹ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†æŒ‡ä»¤æ¨¡å¼ä¸‹è¡¨ç°è¾ƒå·®ã€‚æ›´æ·±å…¥çš„åˆ†æè¡¨æ˜ï¼ŒæŸäº›çŸ­å›çœ‹çŸ­è¯­ï¼ˆæ˜ç¡®æŒ‡ä»£å›¾åƒï¼‰åœ¨æˆåŠŸçš„è½¨è¿¹ä¸­ä¸°å¯Œå­˜åœ¨ï¼Œå¹¶ä¸æ›´å¥½çš„è§†è§‰å®šä½ç›¸å…³ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ç¡®å®šæ€§å¼•å¯¼å›çœ‹ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯è§£ç çš„ç­–ç•¥ï¼Œç»“åˆäº†ä¸ç¡®å®šæ€§ä¿¡å·ä¸è‡ªé€‚åº”å›çœ‹æç¤ºå’Œå¹¿åº¦æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†MMMUçš„æ•´ä½“æ€§èƒ½ï¼Œåœ¨æ ‡å‡†æ€è€ƒè¾ƒå¼±çš„ç±»åˆ«ä¸­å–å¾—äº†æœ€å¤§çš„æ”¶ç›Šï¼Œå¹¶è¶…è¶Šäº†å¤šä¸ªå¼ºå¤§çš„è§£ç åŸºçº¿ï¼Œåœ¨å›ºå®šçš„æ¨¡å‹å®¶æ—å’Œä»¤ç‰Œé¢„ç®—ä¸‹åˆ›ä¸‹äº†æœ€æ–°è®°å½•ã€‚æ­¤å¤–ï¼Œè¯¥è§£ç ç­–ç•¥è¿˜æ˜¾ç¤ºå‡ºä¸€èˆ¬æ€§ï¼Œåœ¨äº”ä¸ªé¢å¤–çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æŒç»­è¿›æ­¥ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé€šç”¨çš„å¤šæ¨¡å¼å¥—ä»¶å’Œæ•°å­¦è§†è§‰æ¨ç†æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶çš„æ€è€ƒå¯ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼Œæ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾æœ‰åŠ©äºå¢å¼ºæ€§èƒ½ã€‚</li>
<li>æ›´å¤šçš„æ€è€ƒå¹¶ä¸æ€»æ˜¯å¸¦æ¥æ›´å¥½çš„è§†è§‰æ¨ç†æ•ˆæœï¼Œé•¿é“¾å¯èƒ½å¯¼è‡´å¿½è§†å›¾åƒå¹¶äº§ç”Ÿé”™è¯¯çš„è½¨è¿¹ã€‚</li>
<li>çŸ­å›çœ‹çŸ­è¯­åœ¨æˆåŠŸçš„è§†è§‰æ¨ç†è½¨è¿¹ä¸­é¢‘ç¹å‡ºç°ï¼Œå¹¶ä¸æ›´å¥½çš„è§†è§‰å®šä½ç›¸å…³ã€‚</li>
<li>ä¸ç¡®å®šæ€§å¼•å¯¼å›çœ‹ç­–ç•¥æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£ç æ–¹æ³•ï¼Œç»“åˆäº†ä¸ç¡®å®šæ€§ä¿¡å·ã€è‡ªé€‚åº”å›çœ‹æç¤ºå’Œå¹¿åº¦æœç´¢ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†MMMUçš„æ•´ä½“æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡å‡†æ€è€ƒè¾ƒå¼±çš„ç±»åˆ«ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f521caf0d3ebb1fc81a9be662ad0b50" align="middle">
<img src="https://picx.zhimg.com/v2-51d8b7ded01c7df413fb9726af80ff6a" align="middle">
<img src="https://picx.zhimg.com/v2-3aa75668f0e3cc1845934efb80697cfa" align="middle">
<img src="https://picx.zhimg.com/v2-00e5cfbbff6fed003da11848ba6dba8c" align="middle">
<img src="https://picx.zhimg.com/v2-7f60bc54bf63ad129dd54457d784ec73" align="middle">
<img src="https://picx.zhimg.com/v2-b9be825bd16d6563a6f4aa24b3080d68" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning"><a href="#AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning" class="headerlink" title="AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning"></a>AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning</h2><p><strong>Authors:Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar</strong></p>
<p>With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAARâ€™s effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.</p>
<blockquote>
<p>éšç€è§†é¢‘å†…å®¹çš„æ—¥ç›Šæ™®åŠï¼Œå¯¹äºé•¿è§†é¢‘çš„æœ‰æ•ˆç†è§£å’Œé—®ç­”å·²æˆä¸ºä¼—å¤šåº”ç”¨çš„æ ¸å¿ƒéœ€æ±‚ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½å·²ç»æå‡ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€å¾®å¦™æŸ¥è¯¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŸ¥è¯¢éœ€è¦å…¨é¢ç†è§£å’Œè¯¦ç»†åˆ†æã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AVATAARï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä»¥åŠé¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ã€‚AVATAARåˆ›å»ºäº†ä¸€ä¸ªæŒä¹…çš„å…¨å±€æ‘˜è¦ï¼Œå¹¶åœ¨åæ€æ¨¡å—å’Œé¢„æ£€ç´¢æ€è€ƒä»£ç†ä¹‹é—´å»ºç«‹äº†ä¸€ä¸ªåé¦ˆå¾ªç¯ï¼Œå…è®¸ç³»ç»Ÿæ ¹æ®éƒ¨åˆ†ç­”æ¡ˆå¯¹æ£€ç´¢ç­–ç•¥è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ï¼Œå¹¶å®ç°ç±»ä¼¼äººç±»çš„è¿­ä»£æ¨ç†ã€‚åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸è¾ƒäºåŸºçº¿æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ—¶åºæ¨ç†æ–¹é¢å®ç°äº†+5.6%çš„ç›¸å¯¹å¢é•¿ï¼Œåœ¨æŠ€æœ¯æŸ¥è¯¢æ–¹é¢å®ç°äº†+5%çš„å¢é•¿ï¼Œåœ¨åŸºäºä¸»é¢˜çš„æé—®ä¸­å®ç°äº†+8%çš„æå‡ï¼Œä»¥åŠåœ¨å™äº‹ç†è§£æ–¹é¢å®ç°äº†+8.2%çš„æå‡ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯¹æ•´ä½“æ€§èƒ½åšå‡ºäº†ç§¯æè´¡çŒ®ï¼Œè€Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†AVATAARåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼ŒAVATAARä¸ºé•¿è§†é¢‘é—®ç­”ï¼ˆQAï¼‰æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œèåˆäº†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15578v1">PDF</a> Accepted in the 5th IEEE Big Data Workshop on Multimodal AI (MMAI 2025), Dec 8-11, Macau, China, 2025 (Preprint Copy)</p>
<p><strong>Summary</strong></p>
<p>éšç€è§†é¢‘å†…å®¹çš„æ™®åŠï¼Œå¯¹é•¿è§†é¢‘è¿›è¡Œæœ‰æ•ˆç†è§£å’Œé—®ç­”åœ¨å¤šä¸ªé¢†åŸŸå˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æå‡äº†æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»é¢ä¸´å¤„ç†å¤æ‚æŸ¥è¯¢çš„æŒ‘æˆ˜ï¼Œéœ€è¦å…¨é¢ç†è§£å’Œè¯¦ç»†åˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºAVATAARï¼Œä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œç»“åˆå…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä»¥åŠé¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ã€‚AVATAARåˆ›å»ºæŒä¹…å…¨å±€æ‘˜è¦ï¼Œå»ºç«‹åæ€æ¨¡å—å’Œé¢„æ£€ç´¢æ€è€ƒä»£ç†ä¹‹é—´çš„åé¦ˆå¾ªç¯ï¼Œå…è®¸ç³»ç»Ÿæ ¹æ®éƒ¨åˆ†ç­”æ¡ˆä¼˜åŒ–æ£€ç´¢ç­–ç•¥ï¼Œæ¨¡æ‹Ÿäººç±»è¿­ä»£æ¨ç†ã€‚åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æ—¶åºæ¨ç†ã€æŠ€æœ¯æŸ¥è¯¢ã€ä¸»é¢˜é—®é¢˜å’Œå™äº‹ç†è§£æ–¹é¢åˆ†åˆ«æé«˜äº†+5.6%ã€+5%ã€+8%å’Œ+8.2%ã€‚å®éªŒè¯æ˜ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿç§¯æå½±å“ï¼Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚AVATAARåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¸ºé•¿è§†é¢‘é—®ç­”æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œèåˆäº†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€è§†é¢‘å†…å®¹çš„æ™®åŠï¼Œå¯¹é•¿è§†é¢‘è¿›è¡Œé—®ç­”çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>AVATAARæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ã€‚</li>
<li>AVATAARé€šè¿‡å»ºç«‹åé¦ˆå¾ªç¯ï¼Œæ¨¡æ‹Ÿäººç±»è¿­ä»£æ¨ç†ã€‚</li>
<li>åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸å¯¹äºåŸºçº¿æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>AVATAARçš„æ¯ä¸ªæ¨¡å—éƒ½å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿç§¯æå½±å“ï¼Œåé¦ˆå¾ªç¯æ˜¯å…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-311a9875497052a1bd50ff1e71fed034" align="middle">
<img src="https://picx.zhimg.com/v2-b59d9d92b9970764f010808d113fb44c" align="middle">
<img src="https://picx.zhimg.com/v2-69ad59825ca3800326d02e86106aa17c" align="middle">
<img src="https://picx.zhimg.com/v2-bb6665078586548d26d14e0ecef97655" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HSKBenchmark-Modeling-and-Benchmarking-Chinese-Second-Language-Acquisition-in-Large-Language-Models-through-Curriculum-Tuning"><a href="#HSKBenchmark-Modeling-and-Benchmarking-Chinese-Second-Language-Acquisition-in-Large-Language-Models-through-Curriculum-Tuning" class="headerlink" title="HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning"></a>HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning</h2><p><strong>Authors:Qihao Yang, Xuelin Wang, Jiale Chen, Xuelian Dong, Yuxin Hao, Tianyong Hao</strong></p>
<p>Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learnersâ€™ language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/CharlesYang030/HSKB">https://github.com/CharlesYang030/HSKB</a>.</p>
<blockquote>
<p>è¯­è¨€ä¹ å¾—å¯¹äºæ­ç¤ºäººç±»è¯­è¨€æ™ºèƒ½çš„æœ¬è´¨è‡³å…³é‡è¦ï¼Œå¹¶æœ€è¿‘æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯è§£é‡Šæ€§çš„æœ‰å‰é€”çš„è§†è§’ã€‚ç„¶è€Œï¼Œè¿›è¡Œéœ€è¦æ§åˆ¶äººç±»å­¦ä¹ è€…è¯­è¨€è¾“å…¥çš„å®éªŒåœ¨ä¼¦ç†ä¸Šå’Œå®è·µä¸Šéƒ½æ˜¯ä¸å¯è¡Œçš„ã€‚è¿™ç»™è¯­è¨€ä¹ å¾—å»ºæ¨¡çš„éªŒè¯å’Œå¯æ‰©å±•æ€§å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ç¬¬äºŒè¯­è¨€ä¹ å¾—ï¼ˆSLAï¼‰æ–¹é¢ã€‚è™½ç„¶LLMæä¾›äº†å¯æ§å’Œå¯å¤åˆ¶çš„é€‰æ‹©ï¼Œä½†ä»ç¼ºä¹æ”¯æŒåˆ†é˜¶æ®µå»ºæ¨¡å’Œè¯„ä¼°çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HSKBenchmarkï¼Œè¿™æ˜¯ä¸­æ–‡SLAçš„LLMåˆ†é˜¶æ®µå»ºæ¨¡å’Œå†™ä½œè¯„ä¼°çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ã€‚å®ƒæ¶µç›–HSK 3è‡³6çº§ï¼ŒåŒ…å«çœŸå®çš„æ•™ç§‘ä¹¦ã€676ä¸‡ä¸ªä»¤ç‰Œã€1ä¸‡å…­åƒä¸ªåˆæˆæŒ‡ä»¤æ ·æœ¬ã€ä¸‰åä¸ªæµ‹è¯•ä¸»é¢˜ä»¥åŠåŸºäºè¯­è¨€å­¦çš„è¯„ä¼°ç³»ç»Ÿã€‚ä¸ºäº†æ¨¡æ‹Ÿäººç±»å­¦ä¹ è½¨è¿¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯¾ç¨‹è°ƒæ•´æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»åˆçº§åˆ°é«˜çº§è®­ç»ƒæ¨¡å‹ã€‚è¯„ä¼°ç³»ç»Ÿç”¨äºæ£€æŸ¥åŸºäºçº§åˆ«çš„è¯­æ³•è¦†ç›–ç‡ã€å†™ä½œé”™è¯¯ã€è¯æ±‡å’Œå¥æ³•å¤æ‚æ€§ä»¥åŠæ•´ä½“è¯„åˆ†ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†åŸºäº1ä¸‡ç¯‡å­¦ä¹ è€…ä½œæ–‡çš„HSKAgentã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHSKBenchmarkä¸ä»…æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†ä¸­æ–‡SLAï¼Œè€Œä¸”ä½œä¸ºLLMä¸­åŠ¨æ€å†™ä½œè¯„ä¼°çš„å¯é åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å¾®è°ƒåçš„LLMçš„å†™ä½œæ€§èƒ½ä¸é«˜çº§äººç±»å­¦ä¹ è€…ç›¸å½“ï¼Œå¹¶è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„ä¹ å¾—ç‰¹å¾ã€‚HSKBenchmarkã€HSKAgentå’Œæ£€æŸ¥ç‚¹ä½œä¸ºåŸºç¡€å·¥å…·å’Œèµ„æºï¼Œä¸ºæœªæ¥çš„è¯­è¨€ä¹ å¾—å»ºæ¨¡å’ŒLLMå¯è§£é‡Šæ€§ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CharlesYang030/HSKB%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/CharlesYang030/HSKBå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15574v1">PDF</a> Accepted by AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¯­è¨€è·å–å¯¹äºæ­ç¤ºäººç±»è¯­è¨€æ™ºèƒ½çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ±‰è¯­ç¬¬äºŒè¯­è¨€è·å–ï¼ˆSLAï¼‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HSKBenchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºLLMçš„æ±‰è¯­SLAå»ºæ¨¡å’Œå†™ä½œè¯„ä¼°æä¾›çš„é˜¶æ®µæ€§åŸºå‡†æµ‹è¯•ã€‚HSKBenchmarkåŒ…æ‹¬HSK3è‡³HSK6çº§åˆ«çš„å†…å®¹ï¼Œå¹¶åŒ…æ‹¬çœŸå®çš„æ•™ç§‘ä¹¦ã€åˆæˆæŒ‡ä»¤æ ·æœ¬ã€æµ‹è¯•ä¸»é¢˜ä»¥åŠåŸºäºè¯­è¨€å­¦çš„è¯„ä¼°ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ¨¡æ‹Ÿäººç±»å­¦ä¹ è½¨è¿¹çš„è¯¾ç¨‹è°ƒæ•´æ¡†æ¶ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªHSKAgentæ¥è¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚æ€»ä¹‹ï¼ŒHSKBenchmarkä¸ä»…æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†æ±‰è¯­SLAï¼Œè€Œä¸”ä¸ºåŠ¨æ€å†™ä½œè¯„ä¼°æä¾›äº†ä¸€ä¸ªå¯é çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€è·å–å¯¹äºæ­ç¤ºäººç±»è¯­è¨€æ™ºèƒ½è‡³å…³é‡è¦ï¼ŒLLMåœ¨æ±‰è¯­SLAå»ºæ¨¡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>HSKBenchmarkæ˜¯é¦–ä¸ªé’ˆå¯¹LLMåœ¨æ±‰è¯­SLAå»ºæ¨¡å’Œå†™ä½œè¯„ä¼°çš„é˜¶æ®µæ€§åŸºå‡†æµ‹è¯•ã€‚</li>
<li>HSKBenchmarkåŒ…æ‹¬çœŸå®çš„æ•™ç§‘ä¹¦ã€åˆæˆæŒ‡ä»¤æ ·æœ¬ã€æµ‹è¯•ä¸»é¢˜å’ŒåŸºäºè¯­è¨€å­¦çš„è¯„ä¼°ç³»ç»Ÿã€‚</li>
<li>å¼•å…¥è¯¾ç¨‹è°ƒæ•´æ¡†æ¶æ¥æ¨¡æ‹Ÿäººç±»å­¦ä¹ è½¨è¿¹ã€‚</li>
<li>HSKAgentç”¨äºè¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œå…¶ç²¾ç»†è°ƒæ•´åçš„æ€§èƒ½ä¸é«˜çº§äººç±»å­¦ä¹ è€…ç›¸å½“ã€‚</li>
<li>HSKBenchmarkä¸ä»…æœ‰æ•ˆæ¨¡æ‹Ÿæ±‰è¯­SLAï¼Œè¿˜ä¸ºåŠ¨æ€å†™ä½œè¯„ä¼°æä¾›å¯é åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2ac713a00229992a801ec0a97e2005c" align="middle">
<img src="https://picx.zhimg.com/v2-5fb7a6096298202dd9c8bde20c2ec124" align="middle">
<img src="https://picx.zhimg.com/v2-25f259a1e6d718ff28554fe37912fb02" align="middle">
<img src="https://picx.zhimg.com/v2-35b8c6f3cf0a4e373a1f7679c81b9b95" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining"><a href="#Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining" class="headerlink" title="Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining"></a>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining</h2><p><strong>Authors:Qianâ€™ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan</strong></p>
<p>As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-&#x2F;off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on&#x2F;off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.</p>
<blockquote>
<p>éšç€å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰çš„å‘å±•ï¼Œç†è§£DeFiäº¤æ˜“èƒŒåçš„ç”¨æˆ·æ„å›¾è‡³å…³é‡è¦ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒåŸå› åŒ…æ‹¬å¤æ‚çš„æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šæ–¹é¢çš„é“¾ä¸Šé“¾ä¸‹å› ç´ ä»¥åŠä¸é€æ˜çš„hexæ—¥å¿—ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ·±åº¦è¯­ä¹‰æ´å¯Ÿã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†äº¤æ˜“æ„å›¾æŒ–æ˜ï¼ˆTIMï¼‰æ¡†æ¶ã€‚TIMåˆ©ç”¨åŸºäºæ‰æ ¹ç†è®ºå’Œå¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿæ„å»ºçš„DeFiæ„å›¾åˆ†ç±»æ³•æ¥ç¨³å¥åœ°æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚å…ƒçº§è§„åˆ’å™¨åŠ¨æ€åè°ƒé¢†åŸŸä¸“å®¶ï¼Œå°†å¤šä¸ªè§†è§’ç‰¹å®šçš„æ„å›¾åˆ†æåˆ†è§£ä¸ºå¯è§£å†³çš„å­ä»»åŠ¡ã€‚é—®é¢˜æ±‚è§£å™¨å¤„ç†è¿™äº›ä»»åŠ¡ï¼Œé‡‡ç”¨å¤šæ¨¡æ€é“¾ä¸Šé“¾ä¸‹æ•°æ®ã€‚è®¤çŸ¥è¯„ä¼°å™¨åˆ™å‡è½»LLMçš„å¹»è§‰æ•ˆåº”å¹¶ç¡®ä¿å¯éªŒè¯æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTIMåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ„å›¾æ¨æ–­ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºæ›´å¯é åœ°ç†è§£ç”¨æˆ·åœ¨DeFiä¸­çš„åŠ¨æœºï¼Œä¸ºå¤æ‚çš„åŒºå—é“¾æ´»åŠ¨æä¾›æƒ…å¢ƒæ„ŸçŸ¥è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15456v1">PDF</a> Written in 2025 Q1</p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§åä¸ºTransaction Intent Miningï¼ˆTIMï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ·±å…¥è§£æå»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰äº¤æ˜“ä¸­çš„ç”¨æˆ·æ„å›¾ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºå®è¯ç†è®ºçš„DeFiæ„å›¾åˆ†ç±»æ³•ã€å¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿä»¥åŠä¸€ç³»åˆ—ç­–ç•¥ï¼Œå¦‚å…ƒçº§è§„åˆ’å™¨ã€é—®é¢˜æ±‚è§£å™¨å’Œè®¤çŸ¥è¯„ä¼°å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒTIMæ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ã€‚å®ƒæœ‰åŠ©äºæ›´å¯é åœ°ç†è§£ç”¨æˆ·åœ¨DeFiä¸­çš„åŠ¨æœºï¼Œå¹¶ä¸ºå¤æ‚çš„åŒºå—é“¾æ´»åŠ¨æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIMæ¡†æ¶è¢«æå‡ºä»¥è§£å†³å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰äº¤æ˜“ä¸­ç†è§£ç”¨æˆ·æ„å›¾çš„æŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜æºäºå¤æ‚çš„æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šæ–¹é¢çš„é“¾å†…å¤–å› ç´ ä»¥åŠä¸é€æ˜çš„hexæ—¥å¿—ã€‚</li>
<li>TIMåˆ©ç”¨åŸºäºå®è¯ç†è®ºçš„DeFiæ„å›¾åˆ†ç±»æ³•ä»¥åŠå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿæ¥æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚</li>
<li>TIMæ¡†æ¶åŒ…æ‹¬å…ƒçº§è§„åˆ’å™¨ï¼Œç”¨äºåè°ƒé¢†åŸŸä¸“å®¶åˆ†è§£ç‰¹å®šè§†è§’çš„æ„å›¾åˆ†æä¸ºå¯è§£å†³çš„ä»»åŠ¡ã€‚</li>
<li>é—®é¢˜æ±‚è§£å™¨åˆ©ç”¨å¤šæ¨¡æ€çš„é“¾ä¸Š&#x2F;é“¾ä¸‹æ•°æ®æ¥å¤„ç†è¿™äº›ä»»åŠ¡ã€‚</li>
<li>è®¤çŸ¥è¯„ä¼°å™¨ç”¨äºå‡è½»LLMçš„å¹»è§‰å¹¶ç¡®ä¿å¯éªŒè¯æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTIMæ¡†æ¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afcd004e8456e76e348cad4f60561050" align="middle">
<img src="https://picx.zhimg.com/v2-93aad0807c8425dba60e1828d46c7017" align="middle">
<img src="https://picx.zhimg.com/v2-fc19a33165095d1e8362ae1161f79fa7" align="middle">
<img src="https://picx.zhimg.com/v2-89605be3fb3f9bc7cc921fd690ca5e8f" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CroPS-Improving-Dense-Retrieval-with-Cross-Perspective-Positive-Samples-in-Short-Video-Search"><a href="#CroPS-Improving-Dense-Retrieval-with-Cross-Perspective-Positive-Samples-in-Short-Video-Search" class="headerlink" title="CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search"></a>CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search</h2><p><strong>Authors:Ao Xie, Jiahui Chen, Quanzhi Zhu, Xiaoze Jiang, Zhiheng Qin, Enyun Yu, Han Li</strong></p>
<p>Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A&#x2F;B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.</p>
<blockquote>
<p>å¯†é›†æ£€ç´¢å·²æˆä¸ºç°ä»£æœç´¢ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­è§†é¢‘å¹³å°ä¸Šçš„åŸºç¡€èŒƒå¼ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å·¥ä¸šç³»ç»Ÿé‡‡ç”¨è‡ªæˆ‘åŠ å¼ºçš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“ä¾èµ–äºå†å²æš´éœ²çš„ç”¨æˆ·äº¤äº’è¿›è¡Œç›‘ç®¡ã€‚è¿™ç§èŒƒå¼ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´è¿‡æ»¤æ³¡æ•ˆåº”ï¼Œå°†æ½œåœ¨çš„ç›¸å…³ä½†æœªæ›¾è§è¿‡çš„å†…å®¹æ’é™¤åœ¨è®­ç»ƒä¿¡å·ä¹‹å¤–ï¼Œä½¿æ¨¡å‹åå‘äºç‹­çª„å’Œä¿å®ˆçš„æ£€ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CroPSï¼ˆè·¨è§†è§’æ­£æ ·æœ¬ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ£€ç´¢æ•°æ®å¼•æ“ï¼Œæ—¨åœ¨é€šè¿‡ä»å¤šä¸ªè§’åº¦å¼•å…¥å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸Šæ„ä¹‰é‡å¤§çš„æ­£ä¾‹æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚CroPSé€šè¿‡ç”¨æˆ·æŸ¥è¯¢é‡æ–°è¡¨è¿°è¡Œä¸ºï¼ˆæŸ¥è¯¢çº§åˆ«ï¼‰ã€æ¨èæµä¸­çš„å‚ä¸æ•°æ®ï¼ˆç³»ç»Ÿçº§åˆ«ï¼‰ä»¥åŠç”±å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆçš„å·¥ä½œçŸ¥è¯†ï¼ˆçŸ¥è¯†çº§åˆ«ï¼‰æ¥å¢å¼ºè®­ç»ƒã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨è¿™äº›å¼‚è´¨ä¿¡å·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚æ ‡ç­¾åˆ†é…ï¼ˆHLAï¼‰ç­–ç•¥ä»¥åŠç›¸åº”çš„H-InfoNCEæŸå¤±ï¼Œå®ƒä»¬å…±åŒå®ç°äº†ç²¾ç»†ç²’åº¦çš„ã€åŸºäºç›¸å…³æ€§çš„ä¼˜åŒ–ã€‚åœ¨å¿«æ‰‹æœç´¢ç­‰å¤§è§„æ¨¡å•†ä¸šçŸ­è§†é¢‘æœç´¢å¹³å°ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCroPSåœ¨ç¦»çº¿æµ‹è¯•å’Œå®æ—¶A&#x2F;Bæµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå®ç°äº†å‡ºè‰²çš„æ£€ç´¢æ€§èƒ½å’Œé™ä½äº†æŸ¥è¯¢é‡æ–°è¡¨è¿°ç‡ã€‚CroPSç°å·²åœ¨å¿«æ‰‹æœç´¢ä¸­å…¨é¢éƒ¨ç½²ï¼Œæ¯å¤©ä¸ºæ•°ç™¾ä¸‡ç”¨æˆ·æä¾›æœåŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15443v1">PDF</a> AAAI-2026, Oral</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£æœç´¢ç³»ç»Ÿå°¤å…¶æ˜¯çŸ­è§†é¢‘å¹³å°ä¸­çš„å¯†é›†æ£€ç´¢èŒƒå¼å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚é‡‡ç”¨ä¾èµ–å†å²ç”¨æˆ·äº¤äº’æ•°æ®çš„è‡ªå¼ºåŒ–è®­ç»ƒæµç¨‹å¯èƒ½å¯¼è‡´è¿‡æ»¤æ°”æ³¡æ•ˆåº”ï¼Œä½¿æ¨¡å‹åå‘ç‹­çª„ä¸”ä¿å®ˆçš„æ£€ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CroPSï¼ˆè·¨è§†è§’æ­£æ ·æœ¬ï¼‰æ£€ç´¢æ•°æ®å¼•æ“ï¼Œé€šè¿‡å¼•å…¥æ¥è‡ªå¤šä¸ªè§†è§’çš„å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„æ­£æ ·æœ¬ï¼Œå¢å¼ºè®­ç»ƒæ•ˆæœã€‚CroPSé€šè¿‡åˆ©ç”¨ç”¨æˆ·æŸ¥è¯¢é‡æ„è¡Œä¸ºã€æ¨èæµä¸­çš„å‚ä¸æ•°æ®ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹åˆæˆçš„ä¸–ç•ŒçŸ¥è¯†ç­‰å¼‚è´¨ä¿¡å·ï¼Œå®ç°äº†ç²¾ç»†ç²’åº¦çš„ç›¸å…³æ„ŸçŸ¥ä¼˜åŒ–ã€‚åœ¨å¿«æ‰‹æœç´¢ç­‰å¤§å‹å•†ä¸šçŸ­è§†é¢‘æœç´¢å¹³å°ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCroPSæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œæé«˜äº†æ£€ç´¢æ€§èƒ½å’Œé™ä½äº†æŸ¥è¯¢é‡æ„ç‡ã€‚ç°å·²å…¨é¢éƒ¨ç½²åœ¨å¿«æ‰‹æœç´¢ä¸­ï¼Œæ¯æ—¥æœåŠ¡æ•°äº¿ç”¨æˆ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯†é›†æ£€ç´¢å·²æˆä¸ºç°ä»£æœç´¢ç³»ç»Ÿçš„æ ¸å¿ƒèŒƒå¼ï¼Œå°¤å…¶åœ¨çŸ­è§†é¢‘å¹³å°ä¸Šã€‚</li>
<li>è‡ªå¼ºåŒ–è®­ç»ƒæµç¨‹ä¾èµ–å†å²ç”¨æˆ·äº¤äº’æ•°æ®ï¼Œå¯èƒ½å¯¼è‡´è¿‡æ»¤æ°”æ³¡æ•ˆåº”ã€‚</li>
<li>CroPSé€šè¿‡å¼•å…¥è·¨è§†è§’æ­£æ ·æœ¬è§£å†³æ­¤é—®é¢˜ï¼Œå¢å¼ºè®­ç»ƒæ•ˆæœã€‚</li>
<li>CroPSåˆ©ç”¨ç”¨æˆ·æŸ¥è¯¢é‡æ„è¡Œä¸ºã€æ¨èæµå‚ä¸æ•°æ®ä»¥åŠä¸–ç•ŒçŸ¥è¯†ç­‰å¼‚è´¨ä¿¡å·ã€‚</li>
<li>æå‡ºäº†åˆ†å±‚æ ‡ç­¾åˆ†é…ï¼ˆHLAï¼‰ç­–ç•¥å’Œç›¸åº”çš„H-InfoNCEæŸå¤±ï¼Œå®ç°ç²¾ç»†ç²’åº¦çš„ç›¸å…³æ„ŸçŸ¥ä¼˜åŒ–ã€‚</li>
<li>åœ¨å¿«æ‰‹æœç´¢å¹³å°ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCroPSæ˜¾è‘—æé«˜äº†æ£€ç´¢æ€§èƒ½å’Œé™ä½äº†æŸ¥è¯¢é‡æ„ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f91d823bf1f04f7ac493f344bd7ec24" align="middle">
<img src="https://picx.zhimg.com/v2-be18432a1c531e41b184e11138d4a0da" align="middle">
<img src="https://picx.zhimg.com/v2-70763c36571920a615b11fd382d22799" align="middle">
<img src="https://picx.zhimg.com/v2-f1a48c8d921c1a3c5bc63004fa8fa0fb" align="middle">
<img src="https://picx.zhimg.com/v2-35d6f1c0f2e888b483f437e1962a7554" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Small-Language-Models-for-Phishing-Website-Detection-Cost-Performance-and-Privacy-Trade-Offs"><a href="#Small-Language-Models-for-Phishing-Website-Detection-Cost-Performance-and-Privacy-Trade-Offs" class="headerlink" title="Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs"></a>Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs</h2><p><strong>Authors:Georg Goldenits, Philip Koenig, Sebastian Raubitzek, Andreas Ekelhart</strong></p>
<p>Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.</p>
<blockquote>
<p>é’“é±¼ç½‘ç«™å¯¹ç½‘ç»œå®‰å…¨æ„æˆé‡å¤§å¨èƒï¼Œå®ƒä»¬æ¬ºéª—æ¯«æ— é˜²å¤‡çš„ç”¨æˆ·ï¼Œå¹¶ç»™è´¢åŠ¡å’Œç»„ç»‡å¸¦æ¥é‡å¤§æŸå®³ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ£€æµ‹é’“é±¼ç½‘ç«™æ—¶ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„ç‰¹å¾å·¥ç¨‹ã€æŒç»­çš„å†è®­ç»ƒå’Œæ˜‚è´µçš„åŸºç¡€è®¾æ–½ç»´æŠ¤ã€‚åŒæ—¶ï¼Œç§æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é’“é±¼ç›¸å…³çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶è¿è¥æˆæœ¬å’Œå¯¹å¤–éƒ¨æä¾›å•†çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨è®¸å¤šå•†ä¸šç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚æœ¬æ–‡ç ”ç©¶äº†ä»…ä½¿ç”¨åŸå§‹HTMLä»£ç æ£€æµ‹é’“é±¼ç½‘ç«™çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å¯è¡Œæ€§ã€‚è¿™äº›æ¨¡å‹çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯å®ƒä»¬å¯ä»¥éƒ¨ç½²åœ¨æœ¬åœ°åŸºç¡€è®¾æ–½ä¸Šï¼Œä½¿ç»„ç»‡èƒ½å¤Ÿæ›´å¤§åœ°æ§åˆ¶æ•°æ®å’Œæ“ä½œã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†15ç§å¸¸ç”¨çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œå‚æ•°èŒƒå›´ä»1äº¿åˆ°70äº¿ï¼Œå¯¹å…¶åˆ†ç±»å‡†ç¡®æ€§ã€è®¡ç®—è¦æ±‚å’Œæˆæœ¬æ•ˆç›Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†æ£€æµ‹æ€§èƒ½ä¸èµ„æºæ¶ˆè€—ä¹‹é—´çš„æƒè¡¡ï¼Œè¡¨æ˜è™½ç„¶SLMçš„æ€§èƒ½ä¸åŠæœ€æ–°çš„ç§æœ‰LLMï¼Œä½†å®ƒä»¬ä»ç„¶å¯ä»¥ä¸ºå¤–éƒ¨LLMæœåŠ¡æä¾›å¯è¡Œå’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å¯¹æˆæœ¬å’Œæ•ˆç›Šçš„æ¯”è¾ƒåˆ†æï¼Œè¿™é¡¹å·¥ä½œä¸ºæœªæ¥ç ”ç©¶åœ¨é’“é±¼æ£€æµ‹ç³»ç»Ÿä¸­å¯¹SLMçš„é€‚åº”ã€å¾®è°ƒå’Œéƒ¨ç½²å¥ å®šäº†åŸºç¡€ï¼Œæ—¨åœ¨å¹³è¡¡å®‰å…¨æœ‰æ•ˆæ€§å’Œç»æµå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15434v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ç”¨äºæ£€æµ‹é’“é±¼ç½‘ç«™å…·æœ‰å¯è¡Œæ€§ï¼Œåˆ©ç”¨åŸå§‹HTMLä»£ç å³å¯å®ç°ã€‚ç›¸è¾ƒäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒSLMséƒ¨ç½²äºæœ¬åœ°åŸºç¡€è®¾æ–½ï¼Œå…·æœ‰æ›´å¥½çš„æ•°æ®å’Œæ§åˆ¶æƒä¼˜åŠ¿ã€‚æœ¬æ–‡è¯„ä¼°äº†å¤šä¸ªå¸¸ç”¨SLMsçš„åˆ†ç±»å‡†ç¡®æ€§ã€è®¡ç®—éœ€æ±‚å’Œæˆæœ¬æ•ˆç›Šï¼Œå‘ç°å®ƒä»¬åœ¨èµ„æºæ¶ˆè€—å’Œæ£€æµ‹æ€§èƒ½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚è™½ç„¶SLMsçš„æ€§èƒ½ç•¥ä½äºæœ€å…ˆè¿›çš„LLMsï¼Œä½†å®ƒä»¬ä»ä¸ºé’“é±¼ç½‘ç«™æ£€æµ‹æä¾›äº†å¯è¡Œä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é’“é±¼ç½‘ç«™æ˜¯ç½‘ç»œå®‰å…¨çš„ä¸»è¦å¨èƒä¹‹ä¸€ï¼Œé€ æˆè´¢åŠ¡å’Œç»„ç»‡ä¸Šçš„é‡å¤§æŸå¤±ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨é’“é±¼ç½‘ç«™æ£€æµ‹ä¸­å­˜åœ¨ç‰¹å¾å·¥ç¨‹ç¹çã€éœ€è¦æŒç»­å†è®­ç»ƒå’Œæ˜‚è´µçš„åŸºç¡€è®¾æ–½ç»´æŠ¤ç­‰é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é’“é±¼ç›¸å…³åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†æ“ä½œæˆæœ¬å’Œå¯¹å¤–éƒ¨æä¾›å•†çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨å•†ä¸šç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¯ä»¥ä»…ä½¿ç”¨åŸå§‹HTMLä»£ç æ£€æµ‹é’“é±¼ç½‘ç«™ã€‚</li>
<li>SLMséƒ¨ç½²åœ¨æœ¬åœ°åŸºç¡€è®¾æ–½ä¸Šï¼Œæä¾›ç»„ç»‡å’Œæ•°æ®æ§åˆ¶çš„ä¼˜åŠ¿ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†å¤šç§SLMsçš„åˆ†ç±»å‡†ç¡®æ€§ã€è®¡ç®—éœ€æ±‚å’Œæˆæœ¬æ•ˆç›Šï¼Œå‘ç°å®ƒä»¬åœ¨æ€§èƒ½å’Œèµ„æºæ¶ˆè€—ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb732da31108f71127747ce814e94714" align="middle">
<img src="https://picx.zhimg.com/v2-9c5b52d96879c4d26e9d9127a184e9ce" align="middle">
<img src="https://picx.zhimg.com/v2-46862a6b5c207b35d9d5fdc1c90b221f" align="middle">
<img src="https://picx.zhimg.com/v2-ce60227de37213e047ca9edc661dd688" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Open-Vocabulary-Human-Motion-Grounding-with-Test-Time-Training"><a href="#Zero-Shot-Open-Vocabulary-Human-Motion-Grounding-with-Test-Time-Training" class="headerlink" title="Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training"></a>Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training</h2><p><strong>Authors:Yunjiao Zhou, Xinyan Chen, Junlang Qian, Lihua Xie, Jianfei Yang</strong></p>
<p>Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.</p>
<blockquote>
<p>ç†è§£å¤æ‚çš„äººç±»æ´»åŠ¨éœ€è¦èƒ½å¤Ÿå°†è¿åŠ¨åˆ†è§£ä¸ºç²¾ç»†ä¸”è¯­ä¹‰å¯¹é½çš„å­åŠ¨ä½œçš„èƒ½åŠ›ã€‚è¿™ä¸€è¿åŠ¨å®šä½è¿‡ç¨‹å¯¹è¡Œä¸ºåˆ†æã€å®ä½“äººå·¥æ™ºèƒ½å’Œè™šæ‹Ÿç°å®è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½ä¾èµ–äºé¢„å®šä¹‰åŠ¨ä½œç±»åˆ«çš„å¯†é›†ç›‘ç£ï¼Œè¿™åœ¨å¼€æ”¾è¯æ±‡çš„ç°å®ä¸–ç•Œç¯å¢ƒä¸­å¹¶ä¸å¯è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ZOMGï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ã€å¼€æ”¾è¯æ±‡çš„æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿå°†è¿åŠ¨åºåˆ—åˆ†è§£ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å­åŠ¨ä½œï¼Œè€Œæ— éœ€ä»»ä½•æ³¨é‡Šæˆ–å¾®è°ƒã€‚ä»æŠ€æœ¯ä¸Šè¯´ï¼ŒZOMGé›†æˆäº†ï¼ˆ1ï¼‰è¯­è¨€è¯­ä¹‰åˆ†åŒºï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†æŒ‡ä»¤åˆ†è§£ä¸ºæœ‰åºçš„å­åŠ¨ä½œå•å…ƒï¼›ï¼ˆ2ï¼‰è½¯æ©æ¨¡ä¼˜åŒ–ï¼Œå®ƒå­¦ä¹ å®ä¾‹ç‰¹å®šçš„æ—¶é—´æ©æ¨¡ï¼Œä»¥å…³æ³¨å¯¹å­åŠ¨ä½œè‡³å…³é‡è¦çš„å¸§ï¼ŒåŒæ—¶ä¿æŒæ®µå†…è¿ç»­æ€§å¹¶å¼ºåˆ¶åˆ†æ®µåˆ†ç¦»ï¼Œè€Œæ— éœ€æ›´æ”¹é¢„è®­ç»ƒç¼–ç å™¨ã€‚åœ¨ä¸‰ä¸ªè¿åŠ¨è¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¿åŠ¨å®šä½æ€§èƒ½çš„æœ€å…ˆè¿›çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œåœ¨HumanML3DåŸºå‡†æµ‹è¯•ä¸Šæ¯”ç°æœ‰æ–¹æ³•é«˜å‡º+8.7% mAPã€‚åŒæ—¶ï¼Œä¸‹æ¸¸æ£€ç´¢ä¹Ÿæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºæ— æ³¨é‡Šè¿åŠ¨ç†è§£å»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å¼€æ”¾è¯æ±‡æ¡†æ¶ï¼ˆZOMGï¼‰å¯å®ç°åŠ¨ä½œåºåˆ—çš„è¯­ä¹‰åˆ†æ®µã€‚å®ƒé€šè¿‡åˆ©ç”¨è¯­è¨€è¯­ä¹‰åˆ’åˆ†å’Œè½¯æ©æ¨¡ä¼˜åŒ–æŠ€æœ¯ï¼Œæ— éœ€ä»»ä½•æ ‡æ³¨æˆ–å¾®è°ƒå³å¯å°†æŒ‡ä»¤åˆ†è§£ä¸ºæœ‰åºçš„å­åŠ¨ä½œå•å…ƒï¼Œå…³æ³¨å…³é”®å¸§ï¼Œç»´æŒæ®µå†…è¿ç»­æ€§å¹¶å¼ºåŒ–æ®µé—´åˆ†ç¦»ã€‚åœ¨ä¸‰ä¸ªè¿åŠ¨è¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶åœ¨åŠ¨ä½œå®šä½ä»»åŠ¡ä¸Šçš„å…ˆè¿›æ•ˆæœå’Œæ•ˆç‡ï¼Œå¹¶åœ¨HumanML3DåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†8.7%çš„mAPï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸æ£€ç´¢ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™ä¸ºæ— æ ‡æ³¨åŠ¨ä½œç†è§£æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZOMGæ¡†æ¶å¯ä»¥å®ç°åŠ¨ä½œåºåˆ—çš„è¯­ä¹‰åˆ†æ®µï¼Œæ— éœ€ä»»ä½•æ ‡æ³¨æˆ–å¾®è°ƒã€‚</li>
<li>ZOMGé›†æˆäº†è¯­è¨€è¯­ä¹‰åˆ’åˆ†å’Œè½¯æ©æ¨¡ä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>è¯­è¨€è¯­ä¹‰åˆ’åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†æŒ‡ä»¤åˆ†è§£ä¸ºæœ‰åºçš„å­åŠ¨ä½œå•å…ƒã€‚</li>
<li>è½¯æ©æ¨¡ä¼˜åŒ–å­¦ä¹ ç‰¹å®šå®ä¾‹çš„ä¸´æ—¶æ©æ¨¡ï¼Œä»¥å…³æ³¨å¯¹å­åŠ¨ä½œè‡³å…³é‡è¦çš„å¸§ã€‚</li>
<li>ZOMGåœ¨ç»´æŒæ®µå†…è¿ç»­æ€§çš„åŒæ—¶ï¼Œå¼ºåŒ–äº†æ®µé—´çš„åˆ†ç¦»ã€‚</li>
<li>åœ¨ä¸‰ä¸ªè¿åŠ¨è¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†ZOMGåœ¨åŠ¨ä½œå®šä½ä»»åŠ¡ä¸Šçš„å…ˆè¿›æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b801d4f25664161efffd11dd41f03e80" align="middle">
<img src="https://picx.zhimg.com/v2-881e4021aa6819a41dbd40dfe7efad59" align="middle">
<img src="https://picx.zhimg.com/v2-4492b5602c14af11a3b3db689bf36745" align="middle">
<img src="https://picx.zhimg.com/v2-4b1e560c534580bfcadfe57a995a263c" align="middle">
<img src="https://picx.zhimg.com/v2-d60836ec5a0990171cb3df5a51e10ffe" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Continual-Instruction-Tuning-with-Dynamic-Gradient-Guidance"><a href="#Multimodal-Continual-Instruction-Tuning-with-Dynamic-Gradient-Guidance" class="headerlink" title="Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance"></a>Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance</h2><p><strong>Authors:Songze Li, Mingyu Gao, Tonghua Su, Xu-Yao Zhang, Zhongjie Wang</strong></p>
<p>Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.</p>
<blockquote>
<p>å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¾æ¬¡é€‚åº”æ–°ä»»åŠ¡ï¼ŒåŒæ—¶å»ºç«‹åœ¨å·²è·å–çš„çŸ¥è¯†ä¹‹ä¸Šã€‚ç„¶è€Œï¼Œè¿™ç§æŒç»­å­¦ä¹ èŒƒå¼é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜çš„é‡å¤§æŒ‘æˆ˜ï¼Œå³å­¦ä¹ æ–°ä»»åŠ¡ä¼šå¯¼è‡´å¯¹å…ˆå‰ä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶æ¦‚å¿µåŒ–ä¸ºåœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­ç¼ºå°‘æ¥è‡ªæ—§ä»»åŠ¡çš„æ¢¯åº¦çš„é—®é¢˜ï¼Œä»è€Œå¼•å…¥äº†å…³äºç¾éš¾æ€§é—å¿˜çš„æ–°è§è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å‚æ•°ç©ºé—´çš„å‡ ä½•å±æ€§æ¥è¿‘ä¼¼è¿™äº›ç¼ºå¤±çš„æ¢¯åº¦ï¼Œå…·ä½“æ˜¯ä½¿ç”¨å½“å‰å‚æ•°ä¸å…ˆå‰æœ€ä¼˜å‚æ•°ä¹‹é—´çš„æ–¹å‘å‘é‡ä½œä¸ºæ¢¯åº¦æŒ‡å¯¼ã€‚è¿™ä¸ªè¿‘ä¼¼çš„æ¢¯åº¦å¯ä»¥è¿›ä¸€æ­¥ä¸æ¥è‡ªæœ‰é™å›æ”¾ç¼“å†²åŒºçš„çœŸå®æ¢¯åº¦ç›¸ç»“åˆï¼Œå¹¶ç”±åŠ¨æ€å¹³è¡¡æ¨¡å‹ç¨³å®šæ€§å’Œå¯å¡‘æ€§çš„ä¼¯åŠªåˆ©é‡‡æ ·ç­–ç•¥è¿›è¡Œè°ƒæ§ã€‚åœ¨å…³äºå¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸è¿›è¡Œæ¨¡å‹æ‰©å±•çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†ç´§å‡‘çš„æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒæ•´ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« é€šè¿‡æŠŠç¾éš¾æ€§é—å¿˜é—®é¢˜æ¦‚å¿µåŒ–ä¸ºæ—§ä»»åŠ¡åœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­çš„ç¼ºå¤±æ¢¯åº¦ï¼Œæå‡ºäº†åŸºäºå‚æ•°ç©ºé—´å‡ ä½•æ€§è´¨çš„è¿‘ä¼¼æ¢¯åº¦æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•å¯ä»¥ä¸æœ‰é™çš„å›æ”¾ç¼“å†²åŒºä¸­çš„çœŸå®æ¢¯åº¦ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡åŠ¨æ€å¹³è¡¡æ¨¡å‹ç¨³å®šæ€§å’Œå¯å¡‘æ€§çš„ä¼¯åŠªåˆ©é‡‡æ ·ç­–ç•¥è¿›è¡Œè°ƒæ§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ‰©å¤§æ¨¡å‹è§„æ¨¡çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒæ•´é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç¾éš¾æ€§é—å¿˜é—®é¢˜è¢«æ¦‚å¿µåŒ–ä¸ºæ—§ä»»åŠ¡åœ¨æ–°ä»»åŠ¡å­¦ä¹ ä¸­çš„ç¼ºå¤±æ¢¯åº¦ã€‚</li>
<li>æå‡ºåŸºäºå‚æ•°ç©ºé—´å‡ ä½•æ€§è´¨çš„è¿‘ä¼¼æ¢¯åº¦æ–¹æ³•æ¥è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>è¿‘ä¼¼æ¢¯åº¦å¯ä»¥ä¸æœ‰é™çš„å›æ”¾ç¼“å†²åŒºä¸­çš„çœŸå®æ¢¯åº¦ç›¸ç»“åˆã€‚</li>
<li>é‡‡ç”¨ä¼¯åŠªåˆ©é‡‡æ ·ç­–ç•¥å¹³è¡¡æ¨¡å‹ç¨³å®šæ€§å’Œå¯å¡‘æ€§ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç¼“è§£ç¾éš¾æ€§é—å¿˜æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e32e5e8b202c6b5a2603fa3ff632464" align="middle">
<img src="https://picx.zhimg.com/v2-dc5dba84e39297894b7594daa5dfa041" align="middle">
<img src="https://picx.zhimg.com/v2-99fb8afe69ff975d80c5c9f77da65ffa" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-Large-Language-Models-LLMs-Understand-Chronology"><a href="#Do-Large-Language-Models-LLMs-Understand-Chronology" class="headerlink" title="Do Large Language Models (LLMs) Understand Chronology?"></a>Do Large Language Models (LLMs) Understand Chronology?</h2><p><strong>Authors:Pattaraphon Kenny Wongchamcharoen, Paul Glasserman</strong></p>
<p>Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium&#x2F;high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low&#x2F;minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡‘èå’Œç»æµå­¦é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå…¶ä¸­åŸºäºæç¤ºçš„å¯¹æŠ—å‰ç»åå·®çš„å°è¯•éšå«åœ°å‡è®¾æ¨¡å‹èƒ½å¤Ÿç†è§£æ—¶é—´é¡ºåºã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ—¶é—´é¡ºåºä»»åŠ¡æµ‹è¯•è¿™ä¸€åŸºæœ¬é—®é¢˜ï¼Œè¿™äº›ä»»åŠ¡çš„å¤æ‚åº¦é€æ¸å¢åŠ ï¼Œæ¶‰åŠæ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå·²ç»çŸ¥é“çš„äº‹å®ã€‚æˆ‘ä»¬çš„ä»»åŠ¡åŒ…æ‹¬ï¼ˆ1ï¼‰æ—¶é—´é¡ºåºæ’åˆ—ï¼Œï¼ˆ2ï¼‰æ¡ä»¶æ’åºï¼ˆå…ˆè¿‡æ»¤ï¼Œç„¶åæ’åºï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰å¹´ä»£é”™è¯¯æ£€æµ‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†GPT-4.1ã€Claude-3.7 Sonnetï¼ˆå¸¦æ‰©å±•æ€è€ƒå’Œä¸å¸¦æ‰©å±•æ€è€ƒï¼‰ä»¥åŠGPT-5åœ¨å¤šç§æ¨ç†éš¾åº¦è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å³ä½¿åœ¨åºåˆ—é•¿åº¦å¢åŠ æ—¶ï¼Œç²¾ç¡®åŒ¹é…ç‡ä¼šæ€¥å‰§ä¸‹é™ï¼Œä½†æ’åç›¸å…³æ€§ä»ç„¶ä¿æŒè¾ƒé«˜æ°´å¹³ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åœ¨ä¿æŒå±€éƒ¨é¡ºåºæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»´æŒå•ä¸€å…¨å±€ä¸€è‡´çš„æ—¶é—´çº¿æ–¹é¢é‡åˆ°å›°éš¾ã€‚åœ¨æ¡ä»¶æ’åºä¸­ï¼Œå¤§å¤šæ•°å¤±è´¥æºäºè¿‡æ»¤æ­¥éª¤ï¼Œè€Œéæ’åºæ­¥éª¤ï¼Œä½†GPT-5å’ŒClaude-3.7 Sonnetæ­é…æ‰©å±•æ€è€ƒåŠŸèƒ½æ˜¾è‘—è¶…è¶Šäº†æ™®é€šæ¨¡å‹ã€‚æœ€åï¼Œå‘ç°å¹´ä»£é”™è¯¯æ£€æµ‹æ˜¯LLMsæœ€å®¹æ˜“çš„ä»»åŠ¡ï¼Œä½†éšç€æ—¶é—´çº¿æˆ–å®ä½“çš„é‡å ç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œæ€§èƒ½ä»ç„¶ä¼šä¸‹é™ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯è¡¨æ˜åˆ†é…æ˜ç¡®çš„æ¨ç†é¢„ç®—æœ‰åŠ©äºæ—¶é—´é¡ºåºæ’åˆ—ã€‚GPT-5åœ¨ä¸­&#x2F;é«˜æ¨ç†åŠ›åº¦ä¸‹èƒ½åœ¨æ‰€æœ‰é•¿åº¦ä¸Šå®ç°å®Œç¾æ’åºå’Œæ¡ä»¶æ’åºï¼ˆè‡ªæˆ‘è¿‡æ»¤å’Œç»™å®šå­é›†ï¼‰ï¼Œè€Œä½&#x2F;æœ€å°åŠ›åº¦åœ¨é¢å¯¹æ›´é•¿çš„åˆ—è¡¨æ—¶ä¼šè¡¨ç°é€€åŒ–ï¼Œç±»ä¼¼äºæ—©æœŸçš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç•Œå®šäº†å½“å‰LLMåœ¨æ—¶é—´é¡ºåºä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œä¸ºä»»åŠ¡å¤æ‚åº¦æä¾›äº†è§è§£ï¼Œå¹¶å±•ç¤ºäº†æ¨ç†æœ‰åŠ©äºçš„åœºæ™¯ã€‚è¿™äº›æ¨¡å¼å¯¹äºLLMåœ¨é‡‘èé¢†åŸŸçš„å®æ—¶åº”ç”¨éå¸¸é‡è¦ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰ä»£ç å’Œè¯„ä¼°æ¨¡æ¿ä»¥æ”¯æŒå®Œå…¨å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14214v2">PDF</a> Version 2: corrected footnote and added code repository link. Extended version of our work presented at the AAAI-26 AI4TS Workshop (poster) and AAAI-26 Student Abstract Program (oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡‘èä¸ç»æµé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åŸºäºæ—¶é—´é¡ºåºçš„ç†è§£å‡è®¾åœ¨è¿›è¡ŒåŸºäºæç¤ºçš„ä»»åŠ¡æ—¶éšæ€§åœ°å­˜åœ¨ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰é€’è¿›å¤æ‚æ€§çš„æ—¶é—´é¡ºåºä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹å¯¹å·²é¢„è®­ç»ƒçŸ¥è¯†çš„æŒæ¡æƒ…å†µã€‚åŒ…æ‹¬ï¼ˆ1ï¼‰æ—¶é—´é¡ºåºæ’åºï¼Œï¼ˆ2ï¼‰æ¡ä»¶ç­›é€‰ï¼ˆå…ˆè¿‡æ»¤å†æ’åºï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰å¹´ä»£é”™è¯¯æ£€æµ‹ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨åºåˆ—é•¿åº¦å¢åŠ çš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®åŒ¹é…ç‡æ€¥å‰§ä¸‹é™ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿æŒå±€éƒ¨é¡ºåºæ–¹é¢å¤§ä½“ä¿æŒå¾—å¾ˆå¥½ï¼Œéš¾ä»¥ç»´æŒå…¨å±€ä¸€è‡´çš„æ—¶é—´çº¿ã€‚åœ¨æ¡ä»¶ç­›é€‰ä¸­ï¼Œå¤§å¤šæ•°å¤±è´¥æºäºç­›é€‰æ­¥éª¤è€Œéæ’åºæ­¥éª¤ï¼Œä½†GPT-5å’Œå¸¦æœ‰æ‰©å±•æ€ç»´ï¼ˆExtended Thinkingï¼‰çš„Claude-3.7 Sonnetè¡¨ç°æ˜¾è‘—ä¼˜äºå¸¸è§„æ¨¡å‹ã€‚å¹´ä»£é”™è¯¯æ£€æµ‹è¢«è®¤ä¸ºæ˜¯LLMæœ€å®¹æ˜“å®Œæˆçš„ä»»åŠ¡ï¼Œä½†éšç€æ—¶é—´çº¿æˆ–å®ä½“çš„é‡å ç¨‹åº¦å¢åŠ ï¼Œæ€§èƒ½ä»ç„¶ä¼šä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºæ˜¾ç¤ºåˆ†é…æ˜ç¡®çš„æ¨ç†é¢„ç®—æœ‰åŠ©äºæ”¹å–„æ—¶é—´é¡ºåºæ’åºä»»åŠ¡ï¼ŒGPT-5åœ¨ä¸­ç­‰&#x2F;é«˜æ¨ç†åŠªåŠ›çš„æƒ…å†µä¸‹ï¼Œåœ¨å„ç§é•¿åº¦ä¸Šå®ç°äº†å®Œç¾çš„æ’åºå’Œæ¡ä»¶ç­›é€‰ï¼ˆåŒ…æ‹¬è‡ªæˆ‘è¿‡æ»¤å’Œç»™å®šå­é›†ï¼‰ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰LLMåœ¨æ—¶åºä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œä¸ºä»»åŠ¡å¤æ‚æ€§æä¾›äº†è§è§£ï¼Œå¹¶å±•ç¤ºäº†æ¨ç†æœ‰åŠ©äºçš„åœºæ™¯ã€‚è¿™äº›æ¨¡å¼å¯¹äºLLMåœ¨é‡‘èé¢†åŸŸçš„å®æ—¶åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡‘èå’Œç»æµé¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œå¯¹æ—¶é—´é¡ºåºçš„ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—ä»»åŠ¡å‘ç°ï¼ŒLLMåœ¨ä¿æŒå…¨å±€ä¸€è‡´çš„æ—¶é—´çº¿æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†åœ¨ä¿æŒå±€éƒ¨é¡ºåºæ–¹é¢è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>åœ¨æ¡ä»¶ç­›é€‰ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å¤±è´¥ä¸»è¦æºäºç­›é€‰æ­¥éª¤ã€‚</li>
<li>GPT-5å’Œå¸¦æœ‰æ‰©å±•æ€ç»´çš„æ¨¡å‹ï¼ˆå¦‚Claude-3.7 Sonnetï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¹´ä»£é”™è¯¯æ£€æµ‹æ˜¯LLMæœ€å®¹æ˜“å®Œæˆçš„ä»»åŠ¡ï¼Œä½†éšç€å®ä½“å’Œæ—¶é—´çº¿çš„é‡å ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>åˆ†é…æ˜ç¡®çš„æ¨ç†é¢„ç®—å¯ä»¥æ”¹å–„æ—¶é—´é¡ºåºæ’åºä»»åŠ¡çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2aad6073229f6792dad089134b0be588" align="middle">
<img src="https://picx.zhimg.com/v2-6ea1a6cd6f74619c9a9f184c8c3a95c7" align="middle">
<img src="https://picx.zhimg.com/v2-ba537e383a16a94eb9e8276e4a47bb5f" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Knowledge-Grounded-Agentic-Large-Language-Models-for-Multi-Hazard-Understanding-from-Reconnaissance-Reports"><a href="#Knowledge-Grounded-Agentic-Large-Language-Models-for-Multi-Hazard-Understanding-from-Reconnaissance-Reports" class="headerlink" title="Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports"></a>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</h2><p><strong>Authors:Chenchen Kuai, Zihao Li, Braden Rosen, Stephanie Paal, Navid Jafari, Jean-Louis Briaud, Yunlong Zhang, Youssef M. A. Hashash, Yang Zhou</strong></p>
<p>Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.</p>
<blockquote>
<p>ç¾åä¾¦å¯ŸæŠ¥å‘Šå¯¹äºç†è§£å¤šç¾ç§ç›¸äº’ä½œç”¨å«æœ‰é‡è¦è¯æ®ï¼Œä½†å…¶éç»“æ„åŒ–çš„å™è¿°ä½¿å¾—çŸ¥è¯†çš„ç³»ç»Ÿä¼ é€’å˜å¾—å›°éš¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåˆ†æè¿™äº›æŠ¥å‘Šæä¾›äº†æ–°çš„æ½œåŠ›ï¼Œä½†åœ¨ç¼ºä¹é¢†åŸŸåŸºç¡€çš„æƒ…å†µä¸‹ï¼Œé€šå¸¸ä¼šç”Ÿæˆä¸å¯é æˆ–è™šæ„çš„è¾“å‡ºã€‚æœ¬ç ”ç©¶å¼•å…¥äº†æ··åˆæ£€ç´¢ä»£ç†RAGï¼ˆMoRA-RAGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥çŸ¥è¯†ä¸ºåŸºç¡€çš„è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå¯å°†ä¾¦å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºå¤šç¾ç§æ¨ç†çš„ç»“æ„åŒ–åŸºç¡€ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤ŸåŠ¨æ€åœ°è·¨ç‰¹å®šç¾å®³æ•°æ®åº“è·¯ç”±æŸ¥è¯¢ï¼ŒåŒæ—¶ä½¿ç”¨ä»£ç†åˆ†å—åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚å®ƒè¿˜åŒ…å«ä¸€ä¸ªéªŒè¯å¾ªç¯ï¼Œè¯¥å¾ªç¯è¯„ä¼°è¯æ®å……åˆ†æ€§ã€æ”¹è¿›æŸ¥è¯¢ï¼Œå¹¶åœ¨ä¿¡æ¯ä¸å®Œæ•´æ—¶å¯åŠ¨æœ‰é’ˆå¯¹æ€§çš„æœç´¢ã€‚æˆ‘ä»¬é€šè¿‡ä»GEERä¾¦å¯ŸæŠ¥å‘Šä¸­æ´¾ç”Ÿé—®ç­”å¯¹æ¥æ„å»ºHazardRecQAï¼Œè¿™äº›æŠ¥å‘Šè®°å½•äº†å…¨çƒ90ä¸ªäº‹ä»¶ä¸­çš„ä¸ƒç§ä¸»è¦ç¾å®³ç±»å‹ã€‚MoRA-RAGçš„å‡†ç¡®ç‡é«˜è¾¾94.5%ï¼Œæ¯”é›¶æ ·æœ¬LLMé«˜å‡º30%ï¼Œæ¯”æœ€å…ˆè¿›çš„RAGç³»ç»Ÿé«˜å‡º10%ï¼ŒåŒæ—¶å‡å°‘äº†åœ¨å„ç§LLMæ¶æ„ä¸­çš„è™šæ„æƒ…å†µã€‚MoRA-RAGè¿˜ä½¿å¼€æ”¾å¼æƒé‡LLMèƒ½å¤Ÿå®ç°ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚å®ƒä¸ºå°†ç¾åæ–‡æ¡£è½¬åŒ–ä¸ºå¯ç”¨äºå®é™…è¡ŒåŠ¨çš„å¯é æƒ…æŠ¥ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„èŒƒå¼ï¼Œä»¥ç”¨äºç¾å®³éŸ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14010v2">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Mixture-of-Retrieval Agentic RAGï¼ˆMoRA-RAGï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºçŸ¥è¯†çš„LLMæ¡†æ¶ï¼Œå¯å°†ç¾åä¾¦å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºå¤šç¾ç§æ¨ç†çš„ç»“æ„åŒ–åŸºç¡€ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œå¯åœ¨ä¸åŒç¾ç§æ•°æ®åº“ä¸­åŠ¨æ€è·¯ç”±æŸ¥è¯¢ï¼ŒåŒæ—¶åˆ©ç”¨agentic chunkingæŠ€æœ¯ä¿æŒæ£€ç´¢è¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªéªŒè¯å¾ªç¯ï¼Œç”¨äºè¯„ä¼°è¯æ®å……è¶³æ€§ã€å®Œå–„æŸ¥è¯¢å¹¶å¯åŠ¨æœ‰é’ˆå¯¹æ€§çš„æœç´¢ã€‚MoRA-RAGå®ç°äº†é«˜è¾¾94.5%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºé›¶æ¬¡LLMå’Œæœ€æ–°çš„RAGç³»ç»Ÿåˆ†åˆ«æé«˜äº†30%å’Œ10%ï¼ŒåŒæ—¶å‡å°‘äº†ä¸åŒLLMæ¶æ„çš„è™šæ„æƒ…å†µã€‚è¯¥æ¡†æ¶è¿˜ä¸ºå¼€æ”¾æƒé‡LLMæä¾›äº†ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºç¾å®³åçš„æƒ…æŠ¥è½¬åŒ–æä¾›äº†å¯é ã€å¯è¡ŒåŠ¨çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨åˆ†æç¾åä¾¦å¯ŸæŠ¥å‘Šæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹é¢†åŸŸçŸ¥è¯†ä¼šå¯¼è‡´ç»“æœä¸å¯é æˆ–è™šæ„ã€‚</li>
<li>MoRA-RAGæ¡†æ¶æ˜¯ä¸€ç§åŸºäºçŸ¥è¯†çš„LLMæ¡†æ¶ï¼Œç”¨äºå°†ä¾¦å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ï¼Œä¾¿äºå¤šç¾ç§æ¨ç†ã€‚</li>
<li>MoRA-RAGç»“åˆäº†æ··åˆæ£€ç´¢æœºåˆ¶å’Œagentic chunkingæŠ€æœ¯ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡è¿è´¯æ€§å’ŒæŸ¥è¯¢çš„ç²¾å‡†åº¦ã€‚</li>
<li>éªŒè¯å¾ªç¯çš„å­˜åœ¨ä½¿MoRA-RAGèƒ½å¤Ÿè¯„ä¼°è¯æ®å……è¶³æ€§å¹¶è¿›è¡Œé’ˆå¯¹æ€§çš„æœç´¢ã€‚</li>
<li>MoRA-RAGå®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶å‡å°‘äº†è™šæ„æƒ…å†µã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºå¤šç§LLMæ¶æ„ï¼Œå¹¶ä¸ºå¼€æ”¾æƒé‡LLMæä¾›äº†ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-628b76da8563d9ac7fdf0d2088de2a7b" align="middle">
<img src="https://picx.zhimg.com/v2-e281c399547978927ed6d5d7c1a78a96" align="middle">
<img src="https://picx.zhimg.com/v2-726dc335a07c7659b454ec520d6df499" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AdamX-An-Adam-improvement-algorithm-based-on-a-novel-exponential-decay-mechanism-for-the-second-order-moment-estimate"><a href="#AdamX-An-Adam-improvement-algorithm-based-on-a-novel-exponential-decay-mechanism-for-the-second-order-moment-estimate" class="headerlink" title="AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate"></a>AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate</h2><p><strong>Authors:Meng Zhu, Quan Xiao, Weidong Min</strong></p>
<p>Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/mengzhu0308/AdamX">https://github.com/mengzhu0308/AdamX</a>.</p>
<blockquote>
<p>è‡ª21ä¸–çºªä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½å¼•é¢†äº†æ–°ä¸€è½®å·¥ä¸šé©å‘½ã€‚åœ¨è®­ç»ƒæ¡†æ¶ä¸‹ï¼Œä¼˜åŒ–ç®—æ³•æ—¨åœ¨ç¨³å®šåœ°å°†é«˜ç»´ä¼˜åŒ–æ”¶æ•›åˆ°å±€éƒ¨ç”šè‡³æ˜¯å…¨å±€æœ€å°å€¼ã€‚è¿›å…¥å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œå°½ç®¡æ¨¡å‹å‚æ•°å’Œæ•°æ®è§„æ¨¡æœ‰æ‰€å¢åŠ ï¼ŒAdamä»ç„¶æ˜¯ä¸»æµä¼˜åŒ–ç®—æ³•ã€‚ç„¶è€Œï¼Œä¸åŸºäºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„ä¼˜åŒ–ç®—æ³•ç›¸æ¯”ï¼ŒAdamæ›´å®¹æ˜“æ”¶æ•›åˆ°éå¹³å¦æœ€å°å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†AdamXç®—æ³•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°å‹çš„äºŒé˜¶çŸ©ä¼°è®¡æŒ‡æ•°è¡°å‡ç‡ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸å‡å¼±å­¦ä¹ æ­¥é•¿æ ¡æ­£å¼ºåº¦ï¼Œåœ¨ç¨³å®šè®­ç»ƒæœŸé€€åŒ–è‡³SGDï¼Œä»è€Œæé«˜äº†ç¨³å®šæœŸè®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶å¯èƒ½æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„äºŒé˜¶çŸ©ä¼°è®¡æŒ‡æ•°è¡°å‡ç‡ä¼˜äºå½“å‰äºŒé˜¶çŸ©ä¼°è®¡æŒ‡æ•°è¡°å‡ç‡ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šï¼ŒAdamXèƒ½å¤Ÿç¨³å®šä¼˜äºAdamåŠå…¶å˜ä½“ã€‚æˆ‘ä»¬çš„ä»£ç å·²å¼€æºåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mengzhu0308/AdamX%E3%80%82">https://github.com/mengzhu0308/AdamXã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13465v2">PDF</a> 25 pages, 6 figures, 12 tables. Version 2: (1) Clarified i.i.d. assumption on gradient and noise components (implicitly used in v1). See Hypothesis 1 for details. (2) Refined abstract terminology: explicitly states degradation to momentum SGD. The theoretical results and conclusions remain unchanged</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½å¼•é¢†æ–°ä¸€è½®å·¥ä¸šé©å‘½ï¼Œä¼˜åŒ–ç®—æ³•ç¨³å®šæ”¶æ•›è‡³å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ã€‚è¿›å…¥å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œè™½ç„¶æ¨¡å‹å‚æ•°å’Œæ•°æ®è§„æ¨¡å¢åŠ ï¼Œä½†Adamä»æ˜¯ä¸»æµä¼˜åŒ–ç®—æ³•ã€‚é’ˆå¯¹Adamæ˜“æ”¶æ•›è‡³éå¹³å¦æœ€å°å€¼çš„é—®é¢˜ï¼Œæå‡ºAdamXç®—æ³•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºæ–°å‹äºŒé˜¶çŸ©ä¼°è®¡æŒ‡æ•°è¡°å‡ç‡ï¼Œéšç€è®­ç»ƒè¿›å±•é€æ­¥å‡å¼±å­¦ä¹ æ­¥é•¿æ ¡æ­£å¼ºåº¦ï¼Œåœ¨ç¨³å®šè®­ç»ƒæœŸé€€åŒ–è‡³SGDï¼Œæé«˜ç¨³å®šæœŸè®­ç»ƒç¨³å®šæ€§å¹¶å¯èƒ½æå‡æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdamXæ€§èƒ½ç¨³å®šä¼˜äºAdamåŠå…¶å˜ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨æ–°å·¥ä¸šé©å‘½ä¸­èµ·å¼•é¢†ä½œç”¨ï¼Œä¼˜åŒ–ç®—æ³•çš„ç¨³å®šæ”¶æ•›è‡³å…³é‡è¦ã€‚</li>
<li>å°½ç®¡æ¨¡å‹å‚æ•°å’Œæ•°æ®è§„æ¨¡å¢å¤§ï¼ŒAdamä¼˜åŒ–ç®—æ³•ä¾ç„¶ä¸»æµã€‚</li>
<li>Adamä¼˜åŒ–ç®—æ³•æ˜“æ”¶æ•›è‡³éå¹³å¦æœ€å°å€¼ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
<li>AdamXç®—æ³•æå‡ºæ–°å‹äºŒé˜¶çŸ©ä¼°è®¡æŒ‡æ•°è¡°å‡ç‡ï¼Œé€æ­¥å‡å¼±å­¦ä¹ æ­¥é•¿æ ¡æ­£å¼ºåº¦ã€‚</li>
<li>AdamXç®—æ³•åœ¨ç¨³å®šè®­ç»ƒæœŸé€€åŒ–è‡³SGDï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>AdamXå¯èƒ½æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a5ed4d502cfe69fbffbf49bd6e1e297" align="middle">
<img src="https://picx.zhimg.com/v2-82f2b807afbb199b1eb8d52f785f8eae" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Model-Merging-Improves-Zero-Shot-Generalization-in-Bioacoustic-Foundation-Models"><a href="#Model-Merging-Improves-Zero-Shot-Generalization-in-Bioacoustic-Foundation-Models" class="headerlink" title="Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models"></a>Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models</h2><p><strong>Authors:Davide Marincione, Donato Crisostomi, Roberto Dessi, Emanuele RodolÃ , Emanuele Rossi</strong></p>
<p>Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.</p>
<blockquote>
<p>åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸï¼Œèƒ½å¤Ÿè·¨ç‰©ç§å’Œä»»åŠ¡è¿›è¡Œæ¦‚æ‹¬çš„åŸºç¡€æ¨¡å‹ä»£è¡¨äº†ä¸€ä¸ªå……æ»¡å¸Œæœ›çš„æ–°å‰æ²¿ï¼ŒNatureLMæ˜¯æœ€çªå‡ºçš„ä¾‹å­ä¹‹ä¸€ã€‚è™½ç„¶å…¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒåœ¨ç”Ÿç‰©å£°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ä¹Ÿå¸¦æ¥äº†æŒ‡ä»¤éµå¾ªçµæ´»æ€§çš„æƒè¡¡ã€‚ä¾‹å¦‚ï¼Œå½“åˆ†åˆ«æç¤ºé€šç”¨åæˆ–å­¦åæ—¶ï¼ŒNatureLMçš„å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†å½“ä¸¤è€…éƒ½åœ¨ä¸€ä¸ªæç¤ºä¸­åŒæ—¶è¦æ±‚æ—¶ï¼Œå…¶å‡†ç¡®ç‡ä¼šæ˜¾è‘—é™ä½ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹åˆå¹¶ç­–ç•¥æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥ç­–ç•¥å°†NatureLMä¸å…¶åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ’å€¼ï¼Œä»¥æ¢å¤æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ŒåŒæ—¶æœ€å°åŒ–é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„æŸå¤±ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜åˆå¹¶åçš„æ¨¡å‹å±•ç°å‡ºæ›´å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†è¶…è¿‡200%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨æœªè§ç‰©ç§çš„å°é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä¸­è¾¾åˆ°æœ€æ–°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05171v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªç„¶LMæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸå…·æœ‰é€šç”¨åŒ–çš„æ½œåŠ›ï¼Œä¾‹å¦‚NatureLMæ¨¡å‹å°±æ˜¯ä¸€ä¸ªæ˜¾è‘—å®ä¾‹ã€‚å°½ç®¡å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒèƒ½åœ¨ç”Ÿç‰©å£°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—å‡ºè‰²è¡¨ç°ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥æŒ‡ä»¤éµå¾ªçµæ´»æ€§çš„æƒè¡¡é—®é¢˜ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œå½“è¦æ±‚åŒæ—¶æä¾›å¸¸ç”¨åå’Œå­¦åæ—¶ï¼ŒNatureLMçš„å‡†ç¡®æ€§æ˜¾è‘—é™ä½ã€‚æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨ç®€å•çš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œå³ç»“åˆNatureLMä¸å…¶åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œè§£å†³è¯¥é—®é¢˜ï¼Œä¸ä»…æ¢å¤äº†å…¶æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸”ä»…è½»å¾®æŸå¤±ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€‚æœ€åç ”ç©¶æ˜¾ç¤ºï¼Œåˆå¹¶åçš„æ¨¡å‹é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°è¶…è¿‡å½“å‰æ°´å¹³çš„ä¸¤å€ä»¥ä¸Šæ”¹è¿›æ•ˆæœï¼Œä¸ºå°é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä¸­çš„æœªçŸ¥ç‰©ç§åˆ†ç±»è®¾å®šäº†æ–°çš„åŸºå‡†çº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç„¶LMæ¨¡å‹ï¼ˆå¦‚NatureLMï¼‰åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸå±•ç°é€šç”¨æ€§æ½œåŠ›ã€‚</li>
<li>è‡ªç„¶LMæ¨¡å‹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒèƒ½åœ¨ç”Ÿç‰©å£°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å½“åŒæ—¶è¦æ±‚æä¾›å¸¸ç”¨åå’Œå­¦åæ—¶ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ä¼šé™ä½ã€‚</li>
<li>é€šè¿‡ç»“åˆåŸºç¡€è¯­è¨€æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼Œå¯è§£å†³å‡†ç¡®æ€§é™ä½çš„é—®é¢˜å¹¶æ¢å¤æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åˆå¹¶ç­–ç•¥å¸¦æ¥äº†é€‚åº¦çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†æŸå¤±ã€‚</li>
<li>åˆå¹¶åçš„æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—æé«˜çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab6b1426902eca3b2a36eecda270db4e" align="middle">
<img src="https://picx.zhimg.com/v2-e391a157a032862ef0fc78067f0cf373" align="middle">
<img src="https://picx.zhimg.com/v2-3ff28dde9e3de38198ef66237cc3d4b1" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Automating-Android-Build-Repair-Bridging-the-Reasoning-Execution-Gap-in-LLM-Agents-with-Domain-Specific-Tools"><a href="#Automating-Android-Build-Repair-Bridging-the-Reasoning-Execution-Gap-in-LLM-Agents-with-Domain-Specific-Tools" class="headerlink" title="Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools"></a>Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools</h2><p><strong>Authors:Ha Min Son, Huan Ren, Xin Liu, Zhe Zhao</strong></p>
<p>Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixerâ€™s success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the modelâ€™s high-level reasoning and effective low-level execution.</p>
<blockquote>
<p>Androidä½œä¸ºæœ€å¤§çš„ç§»åŠ¨å¹³å°ï¼Œå…¶åº”ç”¨çš„è‡ªåŠ¨æ„å»ºä»ç„¶æ˜¯ä¸€ä¸ªå®é™…æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è§£å†³Androidæ„å»ºé”™è¯¯æ–¹é¢çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆæ¨å‡ºäº†AndroidBuildBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1019ä¸ªæ„å»ºå¤±è´¥çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›å¤±è´¥æ¡ˆä¾‹æ˜¯ä»43ä¸ªå¼€æºAndroidé¡¹ç›®çš„æäº¤å†å²ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ã€‚æ¯ä¸ªé—®é¢˜éƒ½ä¸åç»­æäº¤ä¸­çš„ç»è¿‡éªŒè¯çš„è§£å†³æ–¹æ¡ˆé…å¯¹ï¼Œä»¥ç¡®ä¿ä¿®å¤æ–¹æ¡ˆæ˜¯å¯è¡Œçš„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†GradleFixerï¼Œè¿™æ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œæ‹¥æœ‰ç”¨äºæ£€æŸ¥å’Œæ“ä½œGradleæ„å»ºç¯å¢ƒçš„é¢†åŸŸä¸“ç”¨å·¥å…·ã€‚GradleFixerçš„è§£å†³ç‡é«˜è¾¾81.4%ï¼ˆpass@1ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–é€šç”¨shellçš„æœ€æ–°ç¼–ç ä»£ç†ã€‚GradleFixerçš„æˆåŠŸè¡¨æ˜ï¼Œè™½ç„¶LLMå…·å¤‡è§£å†³è¿™äº›å¤±è´¥çš„é«˜çº§çŸ¥è¯†ï¼Œä½†å®ƒä»¬åœ¨ä½¿ç”¨é€šç”¨shellå°†çŸ¥è¯†è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ä½çº§æ“ä½œæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå·¥å…·æ¡¥æ¥â€çš„ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯¥ç­–ç•¥ç”¨é¢†åŸŸæ„ŸçŸ¥çš„æŠ½è±¡æ›¿æ¢äº†é€šç”¨shellå‘½ä»¤ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§æ–¹æ³•é€šè¿‡ä¸¤ç§æœºåˆ¶èµ·ä½œç”¨ï¼š1ï¼‰å®ƒæä¾›LLMæ›´å¯é ä½¿ç”¨çš„APIæ ¼å¼å·¥å…·ï¼›2ï¼‰å®ƒå°†åŠ¨ä½œç©ºé—´é™åˆ¶åœ¨ç›¸å…³æ“ä½œä¸Šã€‚è¿™ç§æ–¹æ³•å¼¥åˆäº†æ¨¡å‹é«˜çº§æ¨ç†å’Œæœ‰æ•ˆä½çº§æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08640v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Androidå¹³å°åœ¨è‡ªåŠ¨æ„å»ºåº”ç”¨ç¨‹åºæ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¿®å¤Androidæ„å»ºé”™è¯¯æ–¹é¢çš„æ½œåŠ›ã€‚æ–‡ä¸­é¦–å…ˆæ¨å‡ºäº†AndroidBuildBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1019ä¸ªæ„å»ºå¤±è´¥çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¿™äº›å¤±è´¥æ¡ˆä¾‹å‡æ¥è‡ª43ä¸ªå¼€æºAndroidé¡¹ç›®çš„æäº¤å†å²ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰åç»­æäº¤çš„éªŒè¯è§£å†³æ–¹æ¡ˆã€‚æ¥ç€ï¼Œæå‡ºäº†GradleFixerï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é¢†åŸŸç‰¹å®šå·¥å…·çš„LLMä»£ç†ï¼Œç”¨äºæ£€æŸ¥å’Œæ“ä½œGradleæ„å»ºç¯å¢ƒã€‚GradleFixerçš„è§£å†³ç‡é«˜è¾¾81.4%ï¼ˆpass@1ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–é€šç”¨å¤–å£³çš„å…ˆè¿›ç¼–ç ä»£ç†ã€‚æ–‡ç« è¿˜æ¢è®¨äº†LLMåœ¨å°†é«˜çº§çŸ¥è¯†è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ä½çº§è¡ŒåŠ¨æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶å±•ç¤ºäº†æ‰€è°“çš„â€œå·¥å…·æ¡¥æ¥â€ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç”¨é¢†åŸŸæ„ŸçŸ¥çš„æŠ½è±¡æ›¿æ¢é€šç”¨å¤–å£³å‘½ä»¤æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æ­¤ä¸¾è®©æ¨¡å‹çš„é«˜çº§æ¨ç†ä¸æœ‰æ•ˆçš„ä½çº§æ‰§è¡Œå¾—ä»¥ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Androidæ˜¯æœ€å¤§çš„ç§»åŠ¨å¹³å°ï¼Œä½†è‡ªåŠ¨æ„å»ºåº”ç”¨ç¨‹åºä»ç„¶æ˜¯ä¸€ä¸ªå®é™…æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ä¿®å¤Androidæ„å»ºé”™è¯¯æ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚</li>
<li>æ¨å‡ºäº†AndroidBuildBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«çœŸå®æ„å»ºå¤±è´¥æ¡ˆä¾‹çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°ä¿®å¤æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºäº†GradleFixerï¼Œä¸€ä¸ªå…·æœ‰é¢†åŸŸç‰¹å®šå·¥å…·çš„LLMä»£ç†ï¼Œç”¨äºæ£€æŸ¥å’Œæ“ä½œGradleæ„å»ºç¯å¢ƒï¼Œè§£å†³ç‡è¾ƒé«˜ã€‚</li>
<li>LLMåœ¨å°†é«˜çº§çŸ¥è¯†è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ä½çº§è¡ŒåŠ¨æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ç­–ç•¥æ¥æ¡¥æ¥è¿™ä¸€å·®è·ã€‚</li>
<li>â€œå·¥å…·æ¡¥æ¥â€ç­–ç•¥é€šè¿‡ç”¨é¢†åŸŸæ„ŸçŸ¥çš„æŠ½è±¡æ›¿æ¢é€šç”¨å¤–å£³å‘½ä»¤ï¼Œæé«˜äº†LLMä»£ç†çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b273f254ca01a7760046e880d2d3a27b" align="middle">
<img src="https://picx.zhimg.com/v2-8f59ce054ec0c30910dc966af72a42b6" align="middle">
<img src="https://picx.zhimg.com/v2-f9cfba4be32fd1da9a62c3dd1cb27119" align="middle">
<img src="https://picx.zhimg.com/v2-a9f0dbda5c3b8c4a573f07af5b919009" align="middle">
<img src="https://picx.zhimg.com/v2-64295e42067975066e05bb4a241b61fb" align="middle">
<img src="https://picx.zhimg.com/v2-f29a6c1a3565170f939ca434fae28ece" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Euclidâ€™s-Gift-Enhancing-Spatial-Perception-and-Reasoning-in-Vision-Language-Models-via-Geometric-Surrogate-Tasks"><a href="#Euclidâ€™s-Gift-Enhancing-Spatial-Perception-and-Reasoning-in-Vision-Language-Models-via-Geometric-Surrogate-Tasks" class="headerlink" title="Euclidâ€™s Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks"></a>Euclidâ€™s Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</h2><p><strong>Authors:Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen</strong></p>
<p>Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3â€“72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6% to 41.8% (+5.2%), and the mean MindCube accuracy rose from 31.4% to 38.1% (+6.7%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{<a target="_blank" rel="noopener" href="https://zgca-ai4edu.github.io/Euclids_Gift%7D%7Bthis%7D">https://zgca-ai4edu.github.io/Euclids_Gift}{this}</a>.</p>
<blockquote>
<p>ç©ºé—´æ™ºèƒ½æ¶µç›–äº†ä¸€ç³»åˆ—ä¸°å¯Œçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯è§†åŒ–å¹¶è½¬æ¢å½¢çŠ¶ã€å¿ƒç†æ—‹è½¬ç‰©ä½“ã€åˆ¤æ–­å…³ç³»ä½ç½®å’ŒåŒ…å«å…³ç³»ä»¥åŠä¼°è®¡æ•°é‡ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºå°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜è§£å†³ä½œä¸ºæ›¿ä»£ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦30000ä¸ªå¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†è®©æ¨¡å‹ä»è¿™äº›å‡ ä½•é—®é¢˜ä¸­å­¦ä¹ å¹¶åº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ï¼Œæˆ‘ä»¬ä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æ¥è‡ªQwen2.5VLã€Qwen3VLå’ŒRoboBrain2.0å®¶æ—çš„ä¸ƒä¸ªæ¨¡å‹å˜ä½“ï¼ˆæ¶µç›–3-72Bå‚æ•°ï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œæ¿€å‘æ¨¡å‹è¯†åˆ«å½¢çŠ¶ã€è®¡æ•°å’Œå…³è”å®ä½“ï¼Œå¹¶ä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸç†è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆSuper-CLEVRã€Omni3DBenchã€VSI-Benchå’ŒMindCubeï¼‰ä¸Šï¼Œæœªç»ä»»ä½•ä»»åŠ¡ç‰¹å®šé€‚åº”çš„æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„é›¶æ ·æœ¬å¢ç›Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡Euclid30Kè®­ç»ƒåï¼ŒVSI-Benchçš„å¹³å‡å‡†ç¡®ç‡ä»36.6%æé«˜åˆ°41.8%ï¼ˆ+5.2%ï¼‰ï¼ŒMindCubeçš„å¹³å‡å‡†ç¡®ç‡ä»31.4%æé«˜åˆ°38.1%ï¼ˆ+6.7%ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç³»ç»Ÿç ”ç©¶ï¼Œè¡¨æ˜ä»¥å‡ ä½•ä¸ºä¸­å¿ƒçš„å¾®è°ƒå¯ä»¥èµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹å¹¿æ³›å¯è½¬ç§»çš„ç©ºé—´æŠ€èƒ½ã€‚ä»£ç å’ŒEuclid30Kæ•°æ®é›†å¯ä»¥åœ¨æ­¤æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://zgca-ai4edu.github.io/Euclids_Gift">https://zgca-ai4edu.github.io/Euclids_Gift</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24473v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´æ™ºèƒ½æ¶µç›–äº†ä¸€ç³»åˆ—èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯è§†åŒ–ä¸è½¬æ¢å½¢çŠ¶ã€åœ¨å¤§è„‘ä¸­æ—‹è½¬ç‰©ä½“ã€åˆ¤æ–­ç›¸å¯¹ä½ç½®å’ŒåŒ…å«å…³ç³»ä»¥åŠä¼°ç®—æ•°é‡ç­‰ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ï¼Œç©ºé—´æ™ºèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºæ›¿ä»£ä»»åŠ¡ã€‚æˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«çº¦3ä¸‡é“å¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜ã€‚ä¸ºäº†ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»è¿™äº›é—®é¢˜ä¸­å­¦ä¹ å¹¶åº”ç”¨æ¬§å‡ é‡Œå¾—åŸç†ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹ä¸ƒä¸ªæ¨¡å‹å˜ä½“ï¼ˆå‚æ•°èŒƒå›´ä»3Båˆ°72Bï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œæ¿€å‘æ¨¡å‹è¯†åˆ«å½¢çŠ¶ã€è®¡æ•°å’Œå…³è”å®ä½“ä»¥åŠä½¿ç”¨æ¬§å‡ é‡Œå¾—åŸç†è¿›è¡Œå¤šæ­¥éª¤æ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å››ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆSuper-CLEVRã€Omni3DBenchã€VSI-Benchå’ŒMindCubeï¼‰ä¸Šï¼Œè¿™äº›æ¨¡å‹æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡é€‚åº”å³å¯å®ç°æ˜¾è‘—çš„é›¶æ ·æœ¬å¢ç›Šã€‚åœ¨Euclid30Kè®­ç»ƒåï¼ŒVSI-Benchçš„å¹³å‡å‡†ç¡®ç‡ä»36.6%æé«˜åˆ°41.8%ï¼ˆ+5.2%ï¼‰ï¼ŒMindCubeçš„å¹³å‡å‡†ç¡®ç‡ä»31.4%æé«˜åˆ°38.1%ï¼ˆ+6.7%ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æœ‰ç³»ç»Ÿçš„ç ”ç©¶è¡¨æ˜ï¼Œä»¥å‡ ä½•ä¸ºä¸­å¿ƒçš„å¾®è°ƒå¯ä»¥ä½¿è§†è§‰è¯­è¨€æ¨¡å‹è·å¾—å¹¿æ³›å¯è½¬ç§»çš„ç©ºé—´æŠ€èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´æ™ºèƒ½æ¶µç›–å¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬å½¢çŠ¶å¯è§†åŒ–ã€ç‰©ä½“æ—‹è½¬ã€ä½ç½®å…³ç³»åˆ¤æ–­åŠæ•°é‡ä¼°ç®—ç­‰ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºå°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜æ±‚è§£ä½œä¸ºæå‡ç©ºé—´æ™ºèƒ½çš„æ›¿ä»£ä»»åŠ¡ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºEuclid30Kçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡å‡ ä½•é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæå‡æ¨¡å‹å¯¹æ¬§å‡ é‡Œå¾—åŸç†çš„åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤šä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5624537b1c33f4c01bf6d84f2af071a5" align="middle">
<img src="https://picx.zhimg.com/v2-dc65c27d2a5f2908a2aa6597d20045c5" align="middle">
<img src="https://picx.zhimg.com/v2-70f8089248505e12544ee6458c9129cf" align="middle">
<img src="https://picx.zhimg.com/v2-dd91aba254448ae3f6adf57836597a35" align="middle">
<img src="https://picx.zhimg.com/v2-e7c95b0d1be3ff56bfe3bb99f76f16d4" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-In-Context-Learning-Framework-for-Large-Language-Models"><a href="#Privacy-Preserving-In-Context-Learning-Framework-for-Large-Language-Models" class="headerlink" title="Privacy Preserving In-Context-Learning Framework for Large Language Models"></a>Privacy Preserving In-Context-Learning Framework for Large Language Models</h2><p><strong>Authors:Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha</strong></p>
<p>Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models. The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/bhusalb/privacy-preserving-icl">https://github.com/bhusalb/privacy-preserving-icl</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾è‘—åœ°æ”¹å˜äº†è‡ªç„¶è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆï¼Œä½†å®ƒä»¬å¼•å‘äº†å…³äºæ½œåœ¨æš´éœ²æ•æ„Ÿä¿¡æ¯çš„éšç§æ‹…å¿§ã€‚ç ”ç©¶å·²ç»å¼ºè°ƒäº†ä¿¡æ¯æ³„éœ²çš„é£é™©ï¼Œå¯¹æ‰‹å¯ä»¥æå–åµŒå…¥åœ¨æç¤ºä¸­çš„æ•æ„Ÿä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ç§æœ‰é¢„æµ‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å¼ºå¤§éšç§ä¿è¯çš„é«˜è´¨é‡åˆæˆæ–‡æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶æ¥ç¡®ä¿åœ¨æœ€åæƒ…å†µä¸‹ä¿¡æ¯æ³„éœ²çš„ç†è®ºç•Œé™ï¼Œè€Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä»»ä½•å¾®è°ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•å¯¹ç§æœ‰è®°å½•è¿›è¡Œæ¨ç†ï¼Œå¹¶èšåˆå¾—åˆ°çš„æ¯ä¸ªæ ‡è®°è¾“å‡ºåˆ†å¸ƒã€‚è¿™èƒ½å¤Ÿåœ¨ä¿æŒéšç§ä¿è¯çš„åŒæ—¶ç”Ÿæˆæ›´é•¿å’Œè¿è´¯çš„åˆæˆæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ··åˆæ“ä½œï¼Œç»“åˆäº†ç§æœ‰å’Œå…¬å…±æ¨ç†ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå®ç”¨æ€§ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¿™ä¸ºåœ¨ä¿æŒé«˜å®ç”¨æ€§çš„åŒæ—¶å®ç°éšç§ä¿æŠ¤æ–‡æœ¬ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/bhusalb/privacy-preserving-icl%E3%80%82">https://github.com/bhusalb/privacy-preserving-iclã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13625v4">PDF</a> Git repo: <a target="_blank" rel="noopener" href="https://github.com/bhusalb/privacy-preserving-icl">https://github.com/bhusalb/privacy-preserving-icl</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢æœ‰ç€æ˜¾è‘—å˜é©ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†éšç§æ³„éœ²çš„æ‹…å¿§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¯¹æ‰‹å¯ä»¥é€šè¿‡æç¤ºæ¥æå–åµŒå…¥çš„æ•æ„Ÿä¿¡æ¯ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„éšç§é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½ç”Ÿæˆé«˜è´¨é‡åˆæˆæ–‡æœ¬å¹¶å…·å¤‡å¼ºå¤§çš„éšç§ä¿éšœã€‚è¯¥ç ”ç©¶åˆ©ç”¨å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶ç¡®ä¿ä¿¡æ¯æ³„éœ²çš„æœ€åæƒ…å†µç†è®ºç•Œé™ï¼Œä¸”æ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•å¯¹ç§å¯†è®°å½•è¿›è¡Œæ¨ç†ï¼Œå¹¶èšåˆç»“æœä¸­çš„æ¯ç¬¦å·è¾“å‡ºåˆ†å¸ƒï¼Œä»¥æ­¤ç”Ÿæˆé•¿ä¸”è¿è´¯çš„åˆæˆæ–‡æœ¬ï¼ŒåŒæ—¶ä¿æŒéšç§ä¿éšœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„æ··åˆæ“ä½œï¼Œå°†ç§å¯†æ¨ç†å’Œå…¬å…±æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜äº†å®ç”¨æ€§ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºéšç§ä¿æŠ¤æ–‡æœ¬ç”Ÿæˆçš„åŒæ—¶ä¿æŒé«˜å®ç”¨æ€§æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢çš„åº”ç”¨æ˜¾è‘—ï¼Œä½†å­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚</li>
<li>å¯¹æ‰‹å¯èƒ½é€šè¿‡æç¤ºæå–åµŒå…¥çš„æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„éšç§é¢„æµ‹æ¡†æ¶ï¼Œç”Ÿæˆåˆæˆæ–‡æœ¬æ—¶å…·æœ‰å¼ºéšç§ä¿éšœã€‚</li>
<li>åˆ©ç”¨å·®åˆ†éšç§ï¼ˆDPï¼‰æ¡†æ¶ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹å³å¯ç¡®ä¿ä¿¡æ¯æ³„éœ²çš„ç†è®ºç•Œé™ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹ç§å¯†è®°å½•è¿›è¡Œæ¨ç†ï¼Œèšåˆç¬¦å·è¾“å‡ºåˆ†å¸ƒä»¥ç”Ÿæˆè¿è´¯æ–‡æœ¬ã€‚</li>
<li>æå‡ºæ··åˆæ“ä½œï¼Œç»“åˆç§å¯†å’Œå…¬å…±æ¨ç†ï¼Œæé«˜å®ç”¨æ€§ã€‚</li>
<li>åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸ºéšç§ä¿æŠ¤æ–‡æœ¬ç”Ÿæˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-016cb272baa4526294ba56cd515460b2" align="middle">
<img src="https://picx.zhimg.com/v2-89d6ca3792caf616352893b845a06522" align="middle">
<img src="https://picx.zhimg.com/v2-7e2fd8c2a025047852c22cfecfad8ad4" align="middle">
<img src="https://picx.zhimg.com/v2-489a173730aab8716c89dec553c69233" align="middle">
<img src="https://picx.zhimg.com/v2-02c52118967e12d3c5b62c4fe139e7db" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="On-the-Alignment-of-Large-Language-Models-with-Global-Human-Opinion"><a href="#On-the-Alignment-of-Large-Language-Models-with-Global-Human-Opinion" class="headerlink" title="On the Alignment of Large Language Models with Global Human Opinion"></a>On the Alignment of Large Language Models with Global Human Opinion</h2><p><strong>Authors:Yang Liu, Masahiro Kaneko, Chenhui Chu</strong></p>
<p>Todayâ€™s large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMsâ€™ opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/ku-nlp/global-opinion-alignment">https://github.com/ku-nlp/global-opinion-alignment</a> and <a target="_blank" rel="noopener" href="https://github.com/nlply/global-opinion-alignment">https://github.com/nlply/global-opinion-alignment</a>.</p>
<blockquote>
<p>å½“ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ”¯æŒå¤šè¯­è¨€åœºæ™¯ï¼Œå…è®¸ç”¨æˆ·ç”¨æ¯è¯­ä¸LLMè¿›è¡Œäº¤äº’ã€‚å½“LLMå›åº”ç”¨æˆ·æå‡ºçš„ä¸»è§‚é—®é¢˜æ—¶ï¼Œå®ƒä»¬åº”ä¸ç”¨æˆ·ä¸æ¨¡å‹äº¤äº’æ—¶ä½¿ç”¨çš„è¯­è¨€çš„ç‰¹å®šç¾¤ä½“æˆ–å†å²æ—¶æœŸçš„è§‚ç‚¹ä¿æŒä¸€è‡´ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨LLMåœ¨ç¾å›½æˆ–å°‘æ•°å›½å®¶ç¾¤ä½“ä¸­ä»£è¡¨çš„è§‚ç‚¹ï¼Œç¼ºä¹å…¨çƒå„å›½æ ·æœ¬å’Œä¸åŒå†å²æ—¶æœŸçš„äººç±»è§‚ç‚¹ç ”ç©¶ï¼Œä¹Ÿç¼ºä¹å¦‚ä½•åˆ©ç”¨è¯­è¨€æ¥å¼•å¯¼LLMçš„è®¨è®ºã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¿½è§†äº†æç¤ºè¯­è¨€å¯¹LLMè§‚ç‚¹ä¸€è‡´æ€§çš„æ½œåœ¨å½±å“ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™äº›ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å…¨çƒä¸åŒå›½å®¶ã€è¯­è¨€å’Œå†å²æ—¶æœŸçš„è§‚ç‚¹ä¸äººç±»è§‚ç‚¹çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°LLMé€‚å½“æˆ–è¿‡åº¦ç¬¦åˆå°‘æ•°å›½å®¶çš„è§‚ç‚¹ï¼Œè€Œä¸å…¶ä»–å¤§å¤šæ•°å›½å®¶çš„è§‚ç‚¹ä¸å¤ªä¸€è‡´ã€‚æ­¤å¤–ï¼Œå°†æç¤ºçš„è¯­è¨€æ›´æ”¹ä¸ºä¸é—®å·ä¸­ä½¿ç”¨çš„è¯­è¨€ç›¸åŒ¹é…ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼LLMæ›´æœ‰æ•ˆåœ°ä¸ç›¸åº”å›½å®¶çš„è§‚ç‚¹ä¿æŒä¸€è‡´ï¼Œè¿™æ¯”ç°æœ‰çš„å¼•å¯¼æ–¹æ³•æ›´æœ‰æ•ˆã€‚åŒæ—¶ï¼ŒLLMä¸ç°ä»£äººå£çš„è§‚ç‚¹æ›´ä¸ºä¸€è‡´ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬ç ”ç©¶æ˜¯å…¨çƒã€è¯­è¨€å’Œæ—¶é—´ç»´åº¦ä¸Šå…³äºLLMè§‚ç‚¹ä¸€è‡´æ€§çš„é¦–æ¬¡å…¨é¢è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/ku-nlp/global-opinion-alignment%E5%92%8Chttps://github.com/nlply/global-opinion-alignment%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/ku-nlp/global-opinion-alignmentå’Œhttps://github.com/nlply/global-opinion-alignmentå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01418v2">PDF</a> 28 pages, 26 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ”¯æŒå¤šè¯­è¨€åœºæ™¯ï¼Œå…è®¸ç”¨æˆ·ç”¨æ¯è¯­ä¸æ¨¡å‹äº¤äº’ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨LLMåœ¨ç¾å›½ç­‰å›½å®¶äººå£ç¾¤ä½“ä¸­çš„æ„è§ä»£è¡¨æ€§ï¼Œç¼ºä¹å…¨çƒèŒƒå›´å†…çš„å›½å®¶æ ·æœ¬å’Œä¸åŒå†å²æ—¶æœŸçš„æ„è§ç ”ç©¶ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨è¯­è¨€å¼•å¯¼LLMçš„è®¨è®ºã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™äº›ç©ºç™½ï¼ŒåŸºäºä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥æ„å»ºè¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿç ”ç©¶LLMä¸äººç±»åœ¨ä¸åŒå›½å®¶ã€è¯­è¨€å’Œå†å²æ—¶æœŸçš„æ„è§ä¸€è‡´æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMå¯¹æŸäº›å›½å®¶çš„æ„è§é€‚å½“æˆ–è¿‡åº¦å¯¹é½ï¼Œè€Œå¯¹å¤§å¤šæ•°å›½å®¶çš„æ„è§ä¸è¶³å¯¹é½ã€‚æ­¤å¤–ï¼Œå°†æç¤ºçš„è¯­è¨€åŒ¹é…åˆ°é—®å·æ‰€ä½¿ç”¨çš„è¯­è¨€å¯ä»¥æœ‰æ•ˆå¼•å¯¼LLMæ›´åŠ è´´è¿‘å¯¹åº”å›½å®¶çš„æ„è§ï¼Œä½†ç›¸æ¯”äºå½“ä»£äººå£æ„è§å­˜åœ¨è¾ƒå¤šè¯¯å·®ã€‚æœ¬ç ”ç©¶æ˜¯å…¨çƒèŒƒå›´å†…é¦–ä¸ªå…¨é¢è°ƒæŸ¥LLMåœ¨ä¸åŒå›½å®¶ã€è¯­è¨€å’Œæ—¶é—´çš„æ„è§ä¸€è‡´æ€§çš„ç ”ç©¶ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€å¯è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMèƒ½å¤Ÿæ”¯æŒå¤šè¯­è¨€äº¤äº’ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç‰¹å®šå›½å®¶äººå£ç¾¤ä½“çš„æ„è§ä»£è¡¨æ€§ã€‚</li>
<li>ç¼ºä¹å…¨çƒèŒƒå›´å†…çš„å›½å®¶æ ·æœ¬å’Œä¸åŒå†å²æ—¶æœŸçš„æ„è§ç ”ç©¶ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨è¯­è¨€å¼•å¯¼LLMçš„è®¨è®ºã€‚</li>
<li>åŸºäºä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥æ„å»ºè¯„ä¼°æ¡†æ¶ï¼Œå‘ç°LLMå¯¹æŸäº›å›½å®¶çš„æ„è§å¯¹é½ç¨‹åº¦è¾ƒé«˜ï¼Œä½†å¯¹å¤šæ•°å›½å®¶ä¸è¶³å¯¹é½ã€‚</li>
<li>å°†æç¤ºçš„è¯­è¨€åŒ¹é…é—®å·è¯­è¨€å¯æœ‰æ•ˆå¼•å¯¼LLMè´´è¿‘å¯¹åº”å›½å®¶çš„æ„è§ã€‚ä½†ç›¸æ¯”äºå½“ä»£äººå£æ„è§å­˜åœ¨è¾ƒå¤šè¯¯å·®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ccab1b83ae0678fc466759b2394f516" align="middle">
<img src="https://picx.zhimg.com/v2-37623d217dd33f9f68001d79f356fca9" align="middle">
<img src="https://picx.zhimg.com/v2-9fd74daee4ee67bb1f071733313b5e50" align="middle">
<img src="https://picx.zhimg.com/v2-8d8ffdd861b927f5107e03a653ba9fba" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89605be3fb3f9bc7cc921fd690ca5e8f" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  Navigating Quantum Missteps in Agent-Based Modeling A Schelling Model Case Study
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2ac713a00229992a801ec0a97e2005c" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  The Impact of Quantization on Large Reasoning Model Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
