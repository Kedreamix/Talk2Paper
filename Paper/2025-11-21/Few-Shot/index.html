<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  In-N-On Scaling Egocentric Manipulation with in-the-wild and on-task Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-745a20c6638b9da27e60405d7625069c')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-21-æ›´æ–°"><a href="#2025-11-21-æ›´æ–°" class="headerlink" title="2025-11-21 æ›´æ–°"></a>2025-11-21 æ›´æ–°</h1><h2 id="In-N-On-Scaling-Egocentric-Manipulation-with-in-the-wild-and-on-task-Data"><a href="#In-N-On-Scaling-Egocentric-Manipulation-with-in-the-wild-and-on-task-Data" class="headerlink" title="In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data"></a>In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</h2><p><strong>Authors:Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang</strong></p>
<p>Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: <a target="_blank" rel="noopener" href="https://xiongyicai.github.io/In-N-On/">https://xiongyicai.github.io/In-N-On/</a></p>
<blockquote>
<p>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ˜¯å­¦ä¹ æ“ä½œç­–ç•¥çš„æœ‰ä»·å€¼ä¸”å¯æ‰©å±•çš„æ•°æ®æ¥æºã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®å­˜åœ¨å¤§é‡å¼‚è´¨æ€§ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…ä½¿ç”¨äººç±»æ•°æ®è¿›è¡Œç®€å•é¢„è®­ç»ƒï¼Œè¿™å¹¶æ²¡æœ‰å……åˆ†å‘æŒ¥å…¶å…¨éƒ¨æ½œåŠ›ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡å°†äººæ•°æ®åˆ†ä¸ºä¸¤å¤§ç±»ï¼šé‡ç”Ÿæ•°æ®å’Œä»»åŠ¡å†…æ•°æ®ï¼Œæä¾›äº†ä¸€å¥—æ”¶é›†å’Œä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒæ•°æ®çš„å¯æ‰©å±•æ–¹æ¡ˆï¼Œå¹¶è¿›è¡Œäº†å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œçš„ç³»ç»Ÿåˆ†æã€‚æˆ‘ä»¬é¦–å…ˆæ•´ç†äº†ä¸€ä¸ªæ•°æ®é›†PHSDï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡1000å°æ—¶çš„å„ç§é‡ç”Ÿä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ•°æ®å’Œè¶…è¿‡20å°æ—¶ç›´æ¥ä¸ç›®æ ‡æ“ä½œä»»åŠ¡å¯¹é½çš„ä»»åŠ¡å†…æ•°æ®ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå­¦ä¹ ä¸€é¡¹å¤§å‹çš„è¯­è¨€æ¡ä»¶æµåŒ¹é…ç­–ç•¥Human0ã€‚å€ŸåŠ©åŸŸé€‚åº”æŠ€æœ¯ï¼ŒHuman0ç¼©å°äº†äººç±»ä¸äººå½¢æœºå™¨äººçš„å·®è·ã€‚ä»å®è¯æ¥çœ‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Human0é€šè¿‡æ‰©å±•äººç±»æ•°æ®å®ç°äº†è‹¥å¹²æ–°é¢–çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬ä»…ä»äººç±»æ•°æ®ä¸­éµå¾ªæŒ‡ä»¤çš„è¯­è¨€è·Ÿéšèƒ½åŠ›ã€å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›å’Œä½¿ç”¨ä»»åŠ¡å†…æ•°æ®æé«˜çš„ç¨³å¥æ€§ã€‚é¡¹ç›®ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://xiongyicai.github.io/In-N-On/">https://xiongyicai.github.io/In-N-On/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15704v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://xiongyicai.github.io/In-N-On/">https://xiongyicai.github.io/In-N-On/</a></p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡äººç±»è§†è§’è§†é¢‘æ•°æ®å¯¹äºå­¦ä¹ æ“æ§ç­–ç•¥å…·æœ‰é‡è¦ä»·å€¼ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®å­˜åœ¨å¤§é‡å¼‚è´¨æ€§ï¼Œç°æœ‰æ–¹æ³•ä»…åˆ©ç”¨äººç±»æ•°æ®è¿›è¡Œç®€å•é¢„è®­ç»ƒï¼Œæœªèƒ½å……åˆ†å‘æŒ¥å…¶ä»·å€¼ã€‚æœ¬æ–‡æä¾›äº†ä¸€å¥—æ”¶é›†å’Œåˆ©ç”¨å¤§è§„æ¨¡äººç±»è§†è§’æ•°æ®çš„æ–¹æ¡ˆï¼Œå°†äººç±»æ•°æ®åˆ†ä¸ºä¸¤ç±»ï¼šè‡ªç„¶åœºæ™¯æ•°æ®å’Œä»»åŠ¡ç›¸å…³æ•°æ®ï¼Œå¹¶è¿›è¡Œäº†å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®çš„ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡æ„å»ºåŒ…å«è¶…è¿‡ä¸€åƒå°æ—¶çš„è‡ªç„¶åœºæ™¯æ•°æ®å’Œè¶…è¿‡äºŒåå°æ—¶çš„ä»»åŠ¡ç›¸å…³æ•°æ®é›†PHSDï¼Œå®ç°äº†å¤§è§„æ¨¡çš„äººç±»è§†è§’è¯­è¨€æ¡ä»¶ä¸‹çš„æµé‡åŒ¹é…ç­–ç•¥Human0ã€‚åˆ©ç”¨åŸŸé€‚åº”æŠ€æœ¯ï¼ŒHuman0ç¼©å°äº†äººç±»ä¸æœºå™¨äººä¹‹é—´çš„å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒHuman0ä»æ‰©å±•çš„äººç±»æ•°æ®ä¸­å®ç°äº†å¤šé¡¹æ–°é¢–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä»…ä¾èµ–äººç±»æ•°æ®çš„è¯­è¨€æŒ‡ä»¤éµå¾ªã€å°æ ·ä¾‹å­¦ä¹ å’Œåˆ©ç”¨ä»»åŠ¡ç›¸å…³æ•°æ®æé«˜é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤§è§„æ¨¡äººç±»è§†è§’è§†é¢‘æ•°æ®å­¦ä¹ æ“æ§ç­–ç•¥å…·æœ‰ä»·å€¼ã€‚</li>
<li>æ•°æ®å­˜åœ¨å¤§é‡å¼‚è´¨æ€§ï¼Œç®€å•é¢„è®­ç»ƒæ— æ³•å……åˆ†å‘æŒ¥å…¶ä»·å€¼ã€‚</li>
<li>å°†äººç±»æ•°æ®åˆ†ä¸ºè‡ªç„¶åœºæ™¯æ•°æ®å’Œä»»åŠ¡ç›¸å…³æ•°æ®ä¸¤ç±»ã€‚</li>
<li>æ„å»ºæ•°æ®é›†PHSDåŒ…å«è¶…è¿‡ä¸€åƒå°æ—¶çš„è‡ªç„¶åœºæ™¯æ•°æ®å’Œè¶…è¿‡äºŒåå°æ—¶çš„ä»»åŠ¡ç›¸å…³æ•°æ®ã€‚</li>
<li>å®ç°äº†å¤§è§„æ¨¡çš„äººç±»è§†è§’è¯­è¨€æ¡ä»¶ä¸‹çš„æµé‡åŒ¹é…ç­–ç•¥Human0ã€‚</li>
<li>åˆ©ç”¨åŸŸé€‚åº”æŠ€æœ¯ç¼©å°äº†äººç±»ä¸æœºå™¨äººä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2eedee461d9a01eca17ddfaf48298a17" align="middle">
<img src="https://picx.zhimg.com/v2-4ce5f0982491b235334df7e2a64548f7" align="middle">
<img src="https://picx.zhimg.com/v2-c325f491a649402e08acc84ab7e68481" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Qubits-to-Couplings-A-Hybrid-Quantum-Machine-Learning-Framework-for-LHC-Physics"><a href="#From-Qubits-to-Couplings-A-Hybrid-Quantum-Machine-Learning-Framework-for-LHC-Physics" class="headerlink" title="From Qubits to Couplings: A Hybrid Quantum Machine Learning Framework for LHC Physics"></a>From Qubits to Couplings: A Hybrid Quantum Machine Learning Framework for LHC Physics</h2><p><strong>Authors:Marwan Ait Haddou, Mohamed Belfkir, Salah Eddine El Harrauss</strong></p>
<p>In this paper, we propose a new Hybrid Quantum Machine Learning (HyQML) framework to improve the sensitivity of double Higgs boson searches in the $HH \to b\bar{b}Î³Î³$ final state at $\sqrt{s}$ &#x3D; 13.6 TeV. The proposed model combines parameterized quantum circuits with a classical neural network meta-model, enabling event-level features to be embedded in a quantum feature space while maintaining the optimization stability of classical learning. The hybrid model outperforms both a state-of-the-art XGBoost model and a purely quantum implementation by a factor of two, achieving an expected 95% CL upper limit on the non-resonant double Higgs boson production cross-section of $1.9\timesÏƒ_{\text{SM}}$ and $2.1\timesÏƒ_{\text{SM}}$ under background normalization uncertainties of 10% and 50%, respectively. In addition, expected constraints on the Higgs boson self-coupling $Îº_Î»$ and quartic vector-boson-Higgs coupling $Îº_{2V}$ are found to be improved compared to the classical and purely quantum models.</p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆé‡å­æœºå™¨å­¦ä¹ ï¼ˆHyQMLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åœ¨$\sqrt{s}$ &#x3D; 13.6 TeVæ¡ä»¶ä¸‹ï¼ŒåŒå¸Œæ ¼æ–¯ç»è‰²å­æœç´¢åœ¨$HH \to b\bar{b}Î³Î³$æœ€ç»ˆæ€çš„çµæ•åº¦ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å‚æ•°åŒ–é‡å­ç”µè·¯å’Œç»å…¸ç¥ç»ç½‘ç»œå…ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨é‡å­ç‰¹å¾ç©ºé—´ä¸­åµŒå…¥äº‹ä»¶çº§ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒç»å…¸å­¦ä¹ çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚æ··åˆæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„XGBoostæ¨¡å‹å’Œçº¯é‡å­å®ç°æ¨¡å‹ï¼Œå®ç°éå…±æŒ¯åŒå¸Œæ ¼æ–¯ç»è‰²å­äº§ç”Ÿæˆªé¢95% CLä¸Šé™çš„é¢„æœŸå€¼ï¼Œåœ¨èƒŒæ™¯å½’ä¸€åŒ–ä¸ç¡®å®šæ€§åˆ†åˆ«ä¸º1 ç»“ åˆæ—¶ä¸‹ è æ··åˆ æ¨¡ çš„ åœ¨ $Î» å…† sé—´ g ä¸” å¿« æ˜ æœ ä¸­è¾ƒ ä¼˜äº ä¼ ç»Ÿ æ¨¡ å‹ å’Œ çº¯ é‡ å­ æ¨¡ å‹ ã€‚ æ¨¡ å‹ èƒ½åœ¨$Ïƒ_{\text{SM}}$çš„1.9å€å’Œ$Ïƒ_{\text{SM}}$çš„2.1å€ä¹‹é—´ï¼Œåœ¨èƒŒæ™¯å½’ä¸€åŒ–ä¸ç¡®å®šæ€§åˆ†åˆ«ä¸º10%å’Œ50%çš„æƒ…å†µä¸‹å–å¾—è‰¯å¥½è¡¨ç°ã€‚æ­¤å¤–ï¼Œä¸ç»å…¸å’Œçº¯é‡å­æ¨¡å‹ç›¸æ¯”ï¼Œå¯¹å¸Œæ ¼æ–¯ç»è‰²å­è‡ªè€¦åˆ$Îº_Î»$å’Œå››æ¬¡çŸ¢é‡ç»è‰²å­-å¸Œæ ¼æ–¯è€¦åˆ$Îº_{2V}$çš„é¢„æœŸçº¦æŸä¹Ÿæœ‰æ‰€æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15672v1">PDF</a> 30 pages, 10 figures</p>
<p><strong>Summary</strong><br>é«˜æ€§èƒ½çš„æ··åˆå¼é‡å­æœºå™¨å­¦ä¹ ï¼ˆHyQMLï¼‰æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºæå‡åœ¨åŒå¸Œæ ¼æ–¯ç»è‰²å­æœç´¢ä¸­çš„çµæ•åº¦ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å‚æ•°åŒ–é‡å­ç”µè·¯å’Œç»å…¸ç¥ç»ç½‘ç»œå…ƒæ¨¡å‹ï¼Œå°†äº‹ä»¶çº§ç‰¹å¾åµŒå…¥é‡å­ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒç»å…¸å­¦ä¹ çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚HyQMLæ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€æ–°çš„XGBoostæ¨¡å‹å’Œçº¯é‡å­å®ç°ï¼Œåœ¨èƒŒæ™¯æ ‡å‡†åŒ–ä¸ç¡®å®šæ€§ä¸º10%å’Œ50%çš„æƒ…å†µä¸‹ï¼Œé¢„è®¡çš„éè°æŒ¯åŒå¸Œæ ¼æ–¯ç»è‰²å­ç”Ÿäº§æˆªé¢çš„95% CLä¸Šé™åˆ†åˆ«ä¸º1.9å€å’Œ2.1å€çš„æ ‡å‡†æ¨¡å‹é¢„æœŸå€¼ã€‚æ­¤å¤–ï¼Œä¸ç»å…¸å’Œçº¯é‡å­æ¨¡å‹ç›¸æ¯”ï¼Œå¯¹å¸Œæ ¼æ–¯ç»è‰²å­è‡ªè€¦åˆå’Œå››æ¬¡å‘é‡ç»è‰²å­-å¸Œæ ¼æ–¯è€¦åˆçš„é¢„æœŸçº¦æŸæœ‰æ‰€æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„Hybrid Quantum Machine Learning (HyQML) æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åœ¨åŒå¸Œæ ¼æ–¯ç»è‰²å­æœç´¢ä¸­çš„çµæ•åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å‚æ•°åŒ–é‡å­ç”µè·¯å’Œç»å…¸ç¥ç»ç½‘ç»œå…ƒæ¨¡å‹ï¼Œå®ç°äº†äº‹ä»¶çº§ç‰¹å¾åˆ°é‡å­ç‰¹å¾ç©ºé—´çš„åµŒå…¥ã€‚</li>
<li>HyQMLæ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„XGBoostæ¨¡å‹å’Œçº¯é‡å­å®ç°ã€‚</li>
<li>åœ¨ä¸åŒçš„èƒŒæ™¯æ ‡å‡†åŒ–ä¸ç¡®å®šæ€§ä¸‹ï¼ŒHyQMLæ¡†æ¶å¯¹éè°æŒ¯åŒå¸Œæ ¼æ–¯ç»è‰²å­ç”Ÿäº§æˆªé¢çš„é¢„æµ‹æœ‰ç€ä¸¥æ ¼çš„95% CLä¸Šé™ã€‚</li>
<li>ä¸ç»å…¸å’Œçº¯é‡å­æ¨¡å‹ç›¸æ¯”ï¼ŒHyQMLæ¡†æ¶åœ¨å¯¹å¸Œæ ¼æ–¯ç»è‰²å­è‡ªè€¦åˆå’Œå››æ¬¡å‘é‡ç»è‰²å­-å¸Œæ ¼æ–¯è€¦åˆçš„é¢„æµ‹çº¦æŸæ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿç»´æŒç»å…¸å­¦ä¹ çš„ä¼˜åŒ–ç¨³å®šæ€§ï¼ŒåŒæ—¶åˆ©ç”¨é‡å­è®¡ç®—çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e715344fb6c73f5910f91f0de34e6643" align="middle">
<img src="https://picx.zhimg.com/v2-cb2e181ef20fa046ad6e58cb462554e1" align="middle">
<img src="https://picx.zhimg.com/v2-753d666b2352fec0772b156ad9bbda22" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Meta-Black-Box-Optimization-with-Bi-Space-Landscape-Analysis-and-Dual-Control-Mechanism-for-SAEA"><a href="#Meta-Black-Box-Optimization-with-Bi-Space-Landscape-Analysis-and-Dual-Control-Mechanism-for-SAEA" class="headerlink" title="Meta-Black-Box Optimization with Bi-Space Landscape Analysis and Dual-Control Mechanism for SAEA"></a>Meta-Black-Box Optimization with Bi-Space Landscape Analysis and Dual-Control Mechanism for SAEA</h2><p><strong>Authors:Yukun Du, Haiyue Yu, Xiaotong Xie, Yan Zheng, Lixin Zhan, Yudong Du, Chongshuang Hu, Boxuan Wang, Jiang Jiang</strong></p>
<p>Surrogate-Assisted Evolutionary Algorithms (SAEAs) are widely used for expensive Black-Box Optimization. However, their reliance on rigid, manually designed components such as infill criteria and evolutionary strategies during the search process limits their flexibility across tasks. To address these limitations, we propose Dual-Control Bi-Space Surrogate-Assisted Evolutionary Algorithm (DB-SAEA), a Meta-Black-Box Optimization (MetaBBO) framework tailored for multi-objective problems. DB-SAEA learns a meta-policy that jointly regulates candidate generation and infill criterion selection, enabling dual control. The bi-space Exploratory Landscape Analysis (ELA) module in DB-SAEA adopts an attention-based architecture to capture optimization states from both true and surrogate evaluation spaces, while ensuring scalability across problem dimensions, population sizes, and objectives. Additionally, we integrate TabPFN as the surrogate model for accurate and efficient prediction with uncertainty estimation. The framework is trained via reinforcement learning, leveraging parallel sampling and centralized training to enhance efficiency and transferability across tasks. Experimental results demonstrate that DB-SAEA not only outperforms state-of-the-art baselines across diverse benchmarks, but also exhibits strong zero-shot transfer to unseen tasks with higher-dimensional settings. This work introduces the first MetaBBO framework with dual-level control over SAEAs and a bi-space ELA that captures surrogate model information.</p>
<blockquote>
<p>ä»£ç†è¾…åŠ©è¿›åŒ–ç®—æ³•ï¼ˆSAEAsï¼‰å¹¿æ³›åº”ç”¨äºæ˜‚è´µçš„é»‘ç®±ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœç´¢è¿‡ç¨‹ä¸­ä¾èµ–äºåˆšæ€§ã€æ‰‹åŠ¨è®¾è®¡çš„ç»„ä»¶ï¼Œå¦‚å¡«å……æ ‡å‡†å’Œè¿›åŒ–ç­–ç•¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä»»åŠ¡ä¸­çš„çµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ§åˆ¶åŒç©ºé—´ä»£ç†è¾…åŠ©è¿›åŒ–ç®—æ³•ï¼ˆDB-SAEAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šç›®æ ‡é—®é¢˜çš„MetaBBOæ¡†æ¶ã€‚DB-SAEAå­¦ä¹ ä¸€ç§å…ƒç­–ç•¥ï¼Œè”åˆè°ƒæ§å€™é€‰ç”Ÿæˆå’Œå¡«å……æ ‡å‡†é€‰æ‹©ï¼Œå®ç°åŒé‡æ§åˆ¶ã€‚DB-SAEAä¸­çš„åŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†æï¼ˆELAï¼‰æ¨¡å—é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ï¼Œä»çœŸå®å’Œä»£ç†è¯„ä¼°ç©ºé—´ä¸­æ•è·ä¼˜åŒ–çŠ¶æ€ï¼ŒåŒæ—¶ç¡®ä¿è·¨é—®é¢˜ç»´åº¦ã€ç¾¤ä½“å¤§å°å’Œç›®æ ‡çš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´åˆTabPFNä½œä¸ºä»£ç†æ¨¡å‹ï¼Œè¿›è¡Œå‡†ç¡®é«˜æ•ˆçš„é¢„æµ‹ï¼ŒåŒæ—¶ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨å¹¶è¡Œé‡‡æ ·å’Œé›†ä¸­è®­ç»ƒæ¥æé«˜æ•ˆç‡å’Œè·¨ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDB-SAEAä¸ä»…åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œè€Œä¸”åœ¨æ›´é«˜ç»´åº¦è®¾ç½®ä¸­çš„æœªçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶é•œå¤´è¿ç§»èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæ¨å‡ºäº†é¦–ä¸ªå…·æœ‰å¯¹SAEAsè¿›è¡ŒåŒé‡æ§åˆ¶å’Œæ•è·ä»£ç†æ¨¡å‹ä¿¡æ¯çš„åŒç©ºé—´ELAçš„MetaBBOæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†æçš„äºŒå…ƒæ§åˆ¶ä»£è¿›åŒ–ç®—æ³•æ¡†æ¶DB-SAEAï¼ˆSurrogate-Assisted Evolutionary Algorithmï¼‰ï¼Œå®ƒé€šè¿‡meta-policyæ§åˆ¶å€™é€‰ç”Ÿæˆå’Œå¡«å‘å‡†åˆ™é€‰æ‹©æ¥çªç ´ç°æœ‰ç®—æ³•çš„å±€é™æ€§ã€‚å¼•å…¥å¹¶è¡Œé‡‡æ ·å’Œé›†ä¸­è®­ç»ƒç­–ç•¥æ¥æé«˜æ•ˆç‡ä¸ä»»åŠ¡è¿ç§»èƒ½åŠ›ï¼Œå¯¹å¤šç»´é—®é¢˜çš„è¡¨ç°ä¼˜ç§€ï¼Œå¯çµæ´»åº”å¯¹å„ç§ä»»åŠ¡å’Œå…·æœ‰å¼ºå¤§çš„é›¶é¢„è®¾ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DB-SAEAæ˜¯é¦–ä¸ªå®ç°SAEAsåŒé‡æ§åˆ¶çš„MetaBBOæ¡†æ¶ã€‚å®ƒåˆ©ç”¨meta-policyå¯¹å€™é€‰ç”Ÿæˆå’Œå¡«å‘å‡†åˆ™è¿›è¡Œé€‰æ‹©ï¼Œå¢å¼ºäº†ç®—æ³•çš„çµæ´»æ€§ã€‚</li>
<li>DB-SAEAä¸­çš„åŒç©ºé—´æ¢ç´¢æ™¯è§‚åˆ†ææ¨¡å—ç»“åˆäº†çœŸå®å’Œä»£ç†è¯„ä¼°ç©ºé—´çš„ä¿¡æ¯ï¼Œé‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿è·¨é—®é¢˜ç»´åº¦ã€ç§ç¾¤è§„æ¨¡å’Œç›®æ ‡çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>é‡‡ç”¨TabPFNä½œä¸ºä»£ç†æ¨¡å‹ï¼Œå®ç°å‡†ç¡®ã€é«˜æ•ˆçš„é¢„æµ‹ä¸ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºè®­ç»ƒDB-SAEAæ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œé‡‡æ ·å’Œé›†ä¸­è®­ç»ƒæé«˜æ•ˆç‡å’Œä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b478c5087cad57aeade596aa0e0ee8a" align="middle">
<img src="https://picx.zhimg.com/v2-6b88c7975f129b22d99393b0a35b25b5" align="middle">
<img src="https://picx.zhimg.com/v2-d4c8176e3ef501b573b945129fb1dbcb" align="middle">
<img src="https://picx.zhimg.com/v2-b9f035bafb8185adcff73066c2b07dbb" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Convergence-and-Sketching-Based-Efficient-Computation-of-Neural-Tangent-Kernel-Weights-in-Physics-Based-Loss"><a href="#Convergence-and-Sketching-Based-Efficient-Computation-of-Neural-Tangent-Kernel-Weights-in-Physics-Based-Loss" class="headerlink" title="Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss"></a>Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss</h2><p><strong>Authors:Max Hirsch, Federico Pichi</strong></p>
<p>In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the networkâ€™s generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: <a target="_blank" rel="noopener" href="https://github.com/maxhirsch/Efficient-NTK">https://github.com/maxhirsch/Efficient-NTK</a></p>
<blockquote>
<p>åœ¨å¤šç›®æ ‡ä¼˜åŒ–ä¸­ï¼Œå¤šä¸ªæŸå¤±é¡¹ä¼šè¢«åŠ æƒå¹¶ç›¸åŠ ï¼Œä»¥å½¢æˆä¸€ä¸ªå•ä¸€çš„ç›®æ ‡ã€‚è¿™äº›æƒé‡æ˜¯æ ¹æ®æŸäº›å…ƒç›®æ ‡é€‰æ‹©æ¥é€‚å½“å¹³è¡¡ç›¸äº’ç«äº‰æŸå¤±ã€‚ä¾‹å¦‚ï¼Œåœ¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ä¸­ï¼Œè¿™äº›æƒé‡é€šå¸¸ä¼šè¢«è‡ªé€‚åº”é€‰æ‹©ï¼Œä»¥æé«˜ç½‘ç»œçš„æ³›åŒ–è¯¯å·®ã€‚ä¸€ç§æµè¡Œçš„è‡ªé€‚åº”æƒé‡é€‰æ‹©æ–¹æ³•åŸºäºPINNçš„ç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰ï¼Œå®ƒæè¿°äº†è®­ç»ƒè¿‡ç¨‹ä¸­ç½‘ç»œåœ¨é¢„æµ‹ç©ºé—´ä¸­çš„æ¼”å˜ã€‚è¿™ç§è‡ªé€‚åº”åŠ æƒç®—æ³•çš„æ”¶æ•›æ€§åœ¨äº‹å…ˆå¹¶ä¸æ¸…æ¥šã€‚æ­¤å¤–ï¼Œè¿™äº›åŸºäºNTKçš„æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé¢‘ç¹æ›´æ–°ï¼Œè¿›ä¸€æ­¥å¢åŠ å­¦ä¹ è¿‡ç¨‹è®¡ç®—è´Ÿæ‹…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼Œé€šè¿‡å¢å¼ºè‡ªé€‚åº”NTKåŸºæƒé‡çš„æ¢¯åº¦ä¸‹é™åœ¨åˆé€‚æ„ä¹‰ä¸Šå…·æœ‰æ”¶æ•›æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå—é¢„æµ‹æ ¡æ­£æ–¹æ³•å’ŒçŸ©é˜µè‰å›¾å¯å‘çš„éšæœºç®—æ³•æ¥è§£å†³è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œè¯¥ç®—æ³•å¯ä»¥äº§ç”ŸNTKçš„æ— åä¼°è®¡ï¼Œå…¶ç¦»æ•£è¯¯å·®å¯ä»¥ä»»æ„å°ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ•°å€¼å®éªŒæ¥æ”¯æŒæˆ‘ä»¬çš„ç†è®ºå‘ç°å¹¶å±•ç¤ºæˆ‘ä»¬çš„éšæœºç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å…¬å¼€äºï¼š[<a target="_blank" rel="noopener" href="https://github.com/maxhirsch/Efficient-NTK">https://github.com/maxhirsch/Efficient-NTK</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15530v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šç›®æ ‡ä¼˜åŒ–ä¸­ï¼Œé€šè¿‡åŠ æƒå¤šä¸ªæŸå¤±é¡¹å¹¶ç›¸åŠ å½¢æˆå•ä¸€ç›®æ ‡ã€‚åœ¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰ä¸­ï¼Œå¸¸é‡‡ç”¨åŸºäºç¥ç»ç½‘ç»œå¦åˆ‡æ ¸ï¼ˆNTKï¼‰çš„è‡ªé€‚åº”æƒé‡æ¥æ”¹å–„ç½‘ç»œæ³›åŒ–è¯¯å·®ã€‚æœ¬æ–‡è¯æ˜äº†åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼Œç»“åˆè‡ªé€‚åº”NTKæƒé‡çš„æ¢¯åº¦ä¸‹é™æ³•æ˜¯æ”¶æ•›çš„ã€‚ä¸ºè§£å†³è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§åŸºäºé¢„æµ‹æ ¡æ­£æ–¹æ³•å’ŒçŸ©é˜µç´ æçš„éšæœºç®—æ³•ï¼Œå¯å¾—åˆ°NTKçš„æ— åä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç›®æ ‡ä¼˜åŒ–ä¸­ï¼Œé€šè¿‡åŠ æƒå¤šä¸ªæŸå¤±é¡¹æ¥å®ç°å•ä¸€ç›®æ ‡ä¼˜åŒ–ï¼Œæƒé‡ç”¨äºå¹³è¡¡ç«äº‰æŸå¤±ä»¥å®ç°å…ƒç›®æ ‡ã€‚</li>
<li>ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰ä¸­ï¼Œå¸¸ä½¿ç”¨åŸºäºç¥ç»ç½‘ç»œå¦åˆ‡æ ¸ï¼ˆNTKï¼‰çš„è‡ªé€‚åº”æƒé‡æ¥æ”¹å–„ç½‘ç»œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡è¯æ˜äº†æ¢¯åº¦ä¸‹é™æ³•ç»“åˆè‡ªé€‚åº”NTKæƒé‡çš„æ”¶æ•›æ€§ã€‚</li>
<li>ä¸ºæé«˜è®¡ç®—æ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé¢„æµ‹æ ¡æ­£æ–¹æ³•å’ŒçŸ©é˜µç´ æçš„éšæœºç®—æ³•æ¥ä¼°è®¡NTKã€‚</li>
<li>è¯¥éšæœºç®—æ³•å¯ä»¥äº§ç”Ÿå¯¹NTKçš„æ— åä¼°è®¡ï¼Œä¸”å­˜åœ¨ä»»æ„å°çš„ç¦»æ•£è¯¯å·®ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†æ•°å€¼å®éªŒæ¥æ”¯æŒå…¶ç†è®ºå‘ç°ï¼Œå¹¶å±•ç¤ºäº†éšæœºç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-593097f54e54b6aeaf5a9a1b0a675eb5" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining"><a href="#Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining" class="headerlink" title="Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining"></a>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining</h2><p><strong>Authors:Qianâ€™ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan</strong></p>
<p>As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-&#x2F;off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on&#x2F;off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.</p>
<blockquote>
<p>éšç€å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰çš„å‘å±•ï¼Œç†è§£DeFiäº¤æ˜“èƒŒåçš„ç”¨æˆ·æ„å›¾è‡³å…³é‡è¦ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå…¶ä¸­åŒ…æ‹¬å¤æ‚çš„æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šå±‚é¢çš„é“¾ä¸Šé“¾ä¸‹å› ç´ ä»¥åŠä¸é€æ˜çš„hexæ—¥å¿—ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ·±åº¦è¯­ä¹‰æ´å¯Ÿã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº¤æ˜“æ„å›¾æŒ–æ˜ï¼ˆTIMï¼‰æ¡†æ¶ã€‚TIMåˆ©ç”¨åŸºäºæ‰å®ç†è®ºå’Œå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªDeFiæ„å›¾åˆ†ç±»ä½“ç³»ï¼Œä»¥ç¨³å¥åœ°æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚å…ƒçº§è§„åˆ’å™¨åŠ¨æ€åè°ƒé¢†åŸŸä¸“å®¶å°†å¤šä¸ªç‰¹å®šè§’åº¦çš„æ„å›¾åˆ†æåˆ†è§£ä¸ºå¯è§£å†³çš„å­ä»»åŠ¡ã€‚é—®é¢˜æ±‚è§£å™¨åˆ©ç”¨å¤šæ¨¡æ€é“¾ä¸Šé“¾ä¸‹æ•°æ®å®Œæˆä»»åŠ¡ã€‚è€Œè®¤çŸ¥è¯„ä¼°å™¨åˆ™å‡è½»LLMçš„å¹»è§‰æ•ˆåº”å¹¶ç¡®ä¿å¯éªŒè¯æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTIMåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ„å›¾æ¨æ–­ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºæ›´å¯é åœ°ç†è§£ç”¨æˆ·åœ¨DeFiä¸­çš„åŠ¨æœºï¼Œä¸ºå¤æ‚çš„åŒºå—é“¾æ´»åŠ¨æä¾›æƒ…å¢ƒæ„ŸçŸ¥çš„è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15456v1">PDF</a> Written in 2025 Q1</p>
<p><strong>Summary</strong><br>     æå‡ºçš„Transaction Intent Miningï¼ˆTIMï¼‰æ¡†æ¶åˆ©ç”¨åŸºäºæ‰æ ¹ç†è®ºå’Œå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿï¼Œæ„å»ºäº†ä¸€ä¸ªDeFiæ„å›¾åˆ†ç±»æ³•ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚æ¡†æ¶åŒ…æ‹¬å…ƒçº§è§„åˆ’å™¨ã€é—®é¢˜æ±‚è§£å™¨å’Œè®¤çŸ¥è¯„ä¼°å™¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£åˆ†è§£ä»»åŠ¡ã€å¤„ç†ä»»åŠ¡å’ŒéªŒè¯è¯„ä¼°ã€‚TIMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰åŠ©äºæ›´å¯é åœ°ç†è§£ç”¨æˆ·åœ¨DeFiä¸­çš„åŠ¨æœºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIMæ¡†æ¶è§£å†³äº†DeFiäº¤æ˜“ä¸­ç†è§£ç”¨æˆ·æ„å›¾çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šæ–¹é¢çš„é“¾å†…å¤–å› ç´ å’Œæ¨¡ç³Šçš„æ—¥å¿—ä¿¡æ¯ã€‚</li>
<li>TIMåˆ©ç”¨åŸºäºæ‰æ ¹ç†è®ºçš„DeFiæ„å›¾åˆ†ç±»æ³•æ¥æ„å»ºç”¨æˆ·æ„å›¾æ¨¡å‹ã€‚</li>
<li>TIMä½¿ç”¨å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿæ¥æ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œå¢å¼ºäº†æ¡†æ¶çš„ç¨³å¥æ€§ã€‚</li>
<li>å…ƒçº§è§„åˆ’å™¨è´Ÿè´£åˆ†è§£ä»»åŠ¡ï¼Œé—®é¢˜æ±‚è§£å™¨å¤„ç†ä»»åŠ¡ï¼Œè®¤çŸ¥è¯„ä¼°å™¨åˆ™éªŒè¯è¯„ä¼°ç»“æœï¼Œä¸‰è€…ååŒå·¥ä½œã€‚</li>
<li>TIMæ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€AgentåŸºå‡†æµ‹è¯•ã€‚</li>
<li>TIMä¸ºç†è§£ç”¨æˆ·åœ¨DeFiä¸­çš„åŠ¨æœºæä¾›äº†æ›´å¯é çš„æ–¹å¼ï¼Œæœ‰åŠ©äºè§£é‡Šå¤æ‚çš„åŒºå—é“¾æ´»åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afcd004e8456e76e348cad4f60561050" align="middle">
<img src="https://picx.zhimg.com/v2-93aad0807c8425dba60e1828d46c7017" align="middle">
<img src="https://picx.zhimg.com/v2-fc19a33165095d1e8362ae1161f79fa7" align="middle">
<img src="https://picx.zhimg.com/v2-89605be3fb3f9bc7cc921fd690ca5e8f" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-MemCluster-Empowering-Large-Language-Models-with-Dynamic-Memory-for-Text-Clustering"><a href="#LLM-MemCluster-Empowering-Large-Language-Models-with-Dynamic-Memory-for-Text-Clustering" class="headerlink" title="LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering"></a>LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering</h2><p><strong>Authors:Yuanjie Zhu, Liangwei Yang, Ke Xu, Weizhi Zhang, Zihe Song, Jindong Wang, Philip S. Yu</strong></p>
<p>Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åŸºäºæ·±åº¦è¯­ä¹‰ç†è§£çš„æ–‡æœ¬èšç±»èƒ½åŠ›ï¼Œæ­£åœ¨é‡å¡‘æ— ç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œå…¶ç›´æ¥åº”ç”¨å—åˆ°ç¼ºä¹è¿­ä»£ä¼˜åŒ–çš„çŠ¶æ€è®°å¿†å’Œéš¾ä»¥ç®¡ç†èšç±»ç²’åº¦çš„æ ¹æœ¬é™åˆ¶ã€‚å› æ­¤ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…·æœ‰å¤–éƒ¨æ¨¡å—çš„å¤æ‚ç®¡é“ï¼Œç‰ºç‰²äº†çœŸæ­£çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†LLM-MemClusterï¼Œè¿™æ˜¯ä¸€ä¸ªé‡æ–°æ„å»ºèšç±»çš„å…¨æ–°æ¡†æ¶ï¼Œä½œä¸ºå®Œå…¨åŸºäºLLMçš„ä»»åŠ¡ã€‚å®ƒåˆ©ç”¨åŠ¨æ€å†…å­˜æ¥çŒè¾“çŠ¶æ€æ„è¯†ï¼Œå¹¶ä½¿ç”¨åŒæç¤ºç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¹¶ç¡®å®šèšç±»çš„æ•°é‡ã€‚åœ¨å‡ ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ— éœ€è°ƒæ•´å‚æ•°çš„æ¡†æ¶æ˜¾è‘—ä¸”æŒç»­åœ°è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚LLM-MemClusterä¸ºåŸºäºLLMçš„æ–‡æœ¬èšç±»æä¾›äº†ä¸€ç§æœ‰æ•ˆã€å¯è§£é‡Šå’ŒçœŸæ­£çš„ç«¯åˆ°ç«¯èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15424v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ·±åº¦è¯­ä¹‰ç†è§£è¿›è¡Œæ–‡æœ¬èšç±»ï¼Œé‡å¡‘äº†æ— ç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘è¿­ä»£æ”¹è¿›çš„çŠ¶æ€è®°å¿†å’Œç®¡ç†é›†ç¾¤ç²’åº¦çš„å›°éš¾ï¼Œå…¶ç›´æ¥åº”ç”¨å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºLLM-MemClusteræ¡†æ¶ï¼Œé‡æ–°å®šä¹‰é›†ç¾¤ä¸ºä¸€ä¸ªå®Œå…¨ç”±LLMä¸»å¯¼çš„ä»»åŠ¡ã€‚å®ƒåˆ©ç”¨åŠ¨æ€è®°å¿†å®ç°çŠ¶æ€æ„ŸçŸ¥ï¼Œå¹¶ç”¨åŒæç¤ºç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¹¶ç¡®å®šé›†ç¾¤æ•°é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ— éœ€è°ƒæ•´å‚æ•°çš„æ¡†æ¶æŒç»­ä¸”æ˜¾è‘—ä¼˜äºå¼ºå¤§åŸºçº¿ã€‚LLM-MemClusterä¸ºåŸºäºLLMçš„æ–‡æœ¬èšç±»æä¾›äº†æœ‰æ•ˆã€å¯è§£é‡Šå’ŒçœŸæ­£çš„ç«¯åˆ°ç«¯èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ·±åº¦è¯­ä¹‰ç†è§£è¿›è¡Œæ–‡æœ¬èšç±»ï¼Œé‡å¡‘æ— ç›‘ç£å­¦ä¹ ã€‚</li>
<li>LLMsç›´æ¥åº”ç”¨äºæ–‡æœ¬èšç±»å­˜åœ¨çŠ¶æ€è®°å¿†å’Œç®¡ç†é›†ç¾¤ç²’åº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>LLM-MemClusteræ¡†æ¶å°†æ–‡æœ¬èšç±»å®šä¹‰ä¸ºå®Œå…¨ç”±LLMä¸»å¯¼çš„ä»»åŠ¡ã€‚</li>
<li>LLM-MemClusteråˆ©ç”¨åŠ¨æ€è®°å¿†å®ç°çŠ¶æ€æ„ŸçŸ¥ï¼Œå¹¶ç”¨åŒæç¤ºç­–ç•¥ç¡®å®šé›†ç¾¤æ•°é‡ã€‚</li>
<li>LLM-MemClusteræ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒæŒç»­ä¸”æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>LLM-MemClusteræä¾›äº†åŸºäºLLMçš„æ–‡æœ¬èšç±»çš„æœ‰æ•ˆã€å¯è§£é‡Šå’ŒçœŸæ­£çš„ç«¯åˆ°ç«¯èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b9c34de8716953c479cba6ccef4e2a0" align="middle">
<img src="https://picx.zhimg.com/v2-9ca788ae7a5ce88d7f9daf89979cc57a" align="middle">
<img src="https://picx.zhimg.com/v2-6c6bede1f1a870dc055f9be82e8b88b1" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="C2F-Space-Coarse-to-Fine-Space-Grounding-for-Spatial-Instructions-using-Vision-Language-Models"><a href="#C2F-Space-Coarse-to-Fine-Space-Grounding-for-Spatial-Instructions-using-Vision-Language-Models" class="headerlink" title="C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models"></a>C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models</h2><p><strong>Authors:Nayoung Oh, Dohyun Kim, Junhyeong Bang, Rohan Paul, Daehyung Park</strong></p>
<p>Space grounding refers to localizing a set of spatial references described in natural language instructions. Traditional methods often fail to account for complex reasoning â€“ such as distance, geometry, and inter-object relationships â€“ while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs. To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization. For the coarse estimation, we design a grid-based visual-grounding prompt with a propose-validate strategy, maximizing VLMâ€™s spatial understanding and yielding physically and semantically valid canonical region (i.e., ellipses). For the refinement, we locally adapt the region to surrounding environment without over-relaxed to free space. We construct a new space-grounding benchmark and compare C2F-Space with five state-of-the-art baselines using success rate and intersection-over-union. Our C2F-Space significantly outperforms all baselines. Our ablation study confirms the effectiveness of each module in the two-step process and their synergistic effect of the combined framework. We finally demonstrate the applicability of C2F-Space to simulated robotic pick-and-place tasks.</p>
<blockquote>
<p>ç©ºé—´å®šä½æ˜¯æŒ‡å®šä½è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­æè¿°çš„ä¸€ç»„ç©ºé—´å‚è€ƒã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€æ— æ³•å¤„ç†å¤æ‚çš„æ¨ç†ï¼Œä¾‹å¦‚è·ç¦»ã€å‡ ä½•å’Œå¯¹è±¡ä¹‹é—´çš„å…³ç³»ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°½ç®¡å…·æœ‰å¾ˆå¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨äº§ç”Ÿç²¾ç»†è¾“å‡ºåŒºåŸŸæ—¶å´é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†C2F-Spaceï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç²—åˆ°ç»†çš„ç©ºé—´å®šä½æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰ä½¿ç”¨VLMä¼°è®¡ä¸€ä¸ªç²—ç•¥ä½†ç©ºé—´ä¸Šä¸€è‡´çš„åŒºåŸŸï¼Œç„¶åï¼ˆiiï¼‰é€šè¿‡è¶…åƒç´ åŒ–ä½¿è¯¥åŒºåŸŸä¸å±€éƒ¨ç¯å¢ƒå¯¹é½ã€‚å¯¹äºç²—ç•¥ä¼°è®¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºç½‘æ ¼çš„è§†è§‰å®šä½æç¤ºï¼Œé‡‡ç”¨æå‡º-éªŒè¯ç­–ç•¥ï¼Œæœ€å¤§é™åº¦åœ°æé«˜VLMçš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¹¶äº§ç”Ÿç‰©ç†å’Œè¯­ä¹‰ä¸Šæœ‰æ•ˆçš„è§„èŒƒåŒºåŸŸï¼ˆå³æ¤­åœ†ï¼‰ã€‚å¯¹äºç»†åŒ–ï¼Œæˆ‘ä»¬å±€éƒ¨è°ƒæ•´åŒºåŸŸä»¥é€‚åº”å‘¨å›´ç¯å¢ƒï¼Œè€Œä¸ä¼šè¿‡äºæ”¾æ¾åˆ°è‡ªç”±ç©ºé—´ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„ç©ºé—´å®šä½åŸºå‡†ï¼Œå¹¶ä½¿ç”¨æˆåŠŸç‡å’Œäº¤é›†æ¯”æ¯”è¾ƒäº†C2F-Spaceä¸äº”ç§æœ€æ–°æŠ€æœ¯åŸºçº¿ã€‚æˆ‘ä»¬çš„C2F-Spaceæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†ä¸¤æ­¥è¿‡ç¨‹ä¸­æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä»¥åŠå®ƒä»¬ååŒä½œç”¨çš„ç»¼åˆæ¡†æ¶çš„ååŒä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†C2F-Spaceåœ¨æ¨¡æ‹Ÿçš„æœºå™¨äººæ‹¾å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15333v1">PDF</a> 16 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºC2F-Spaceçš„æ–°å‹ç²—åˆ°ç»†çš„ç©ºé—´å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­çš„ç©ºé—´å‚è€ƒå®šä½é—®é¢˜ã€‚å®ƒé€šè¿‡ä¸¤æ­¥è¿‡ç¨‹å®ç°ï¼šé¦–å…ˆä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç²—ç•¥ä¼°ç®—ï¼Œç„¶åé€šè¿‡è¶…åƒç´ åŒ–å¯¹åŒºåŸŸè¿›è¡Œç»†åŒ–ï¼Œä»¥ä¸å±€éƒ¨ç¯å¢ƒå¯¹é½ã€‚è¯¥æ¡†æ¶åœ¨æ–°å‹ç©ºé—´å®šä½åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–äº”ä¸ªæœ€å…ˆè¿›çš„åŸºç¡€çº¿æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†æ¨¡æ‹Ÿæœºå™¨äººæ‹¾å–å’Œæ”¾ç½®ä»»åŠ¡ä»¥éªŒè¯å…¶å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>C2F-Spaceæ˜¯ä¸€ç§ç²—åˆ°ç»†çš„ç©ºé—´å®šä½æ¡†æ¶ï¼Œç”¨äºè§£å†³è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­çš„ç©ºé—´å‚è€ƒå®šä½é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç²—ç•¥ä¼°ç®—ï¼Œç„¶åé€šè¿‡è¶…åƒç´ åŒ–å¯¹åŒºåŸŸè¿›è¡Œç»†åŒ–ã€‚</li>
<li>æ¡†æ¶è®¾è®¡äº†ä¸€ç§åŸºäºç½‘æ ¼çš„è§†è§‰å®šä½æç¤ºï¼Œé‡‡ç”¨æå‡º-éªŒè¯ç­–ç•¥ï¼Œæœ€å¤§åŒ–VLMçš„ç©ºé—´ç†è§£ï¼Œå¹¶äº§ç”Ÿç‰©ç†å’Œè¯­ä¹‰ä¸Šæœ‰æ•ˆçš„è§„èŒƒåŒºåŸŸï¼ˆå³æ¤­åœ†ï¼‰ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿå±€éƒ¨é€‚åº”å‘¨å›´ç¯å¢ƒï¼Œè€Œä¸ä¼šè¿‡åº¦æ”¾æ¾åˆ°è‡ªç”±ç©ºé—´ã€‚</li>
<li>C2F-Spaceåœ¨æ–°å‹ç©ºé—´å®šä½åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–äº”ä¸ªæœ€å…ˆè¿›çš„åŸºç¡€çº¿æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶çš„æˆåŠŸç‡é€šè¿‡æ¶ˆèç ”ç©¶å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†æ¯ä¸ªæ¨¡å—åœ¨ä¸¤æ­¥è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ååŒä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e469c78e22ac2752f7da7e590139ad1" align="middle">
<img src="https://picx.zhimg.com/v2-aca83d0fab68b528513dcc961410f778" align="middle">
<img src="https://picx.zhimg.com/v2-57601ceecf3771860db4c8089bc41b1f" align="middle">
<img src="https://picx.zhimg.com/v2-2f373b2027af1afe68303450197c2b26" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IndicGEC-Powerful-Models-or-a-Measurement-Mirage"><a href="#IndicGEC-Powerful-Models-or-a-Measurement-Mirage" class="headerlink" title="IndicGEC: Powerful Models, or a Measurement Mirage?"></a>IndicGEC: Powerful Models, or a Measurement Mirage?</h2><p><strong>Authors:Sowmya Vajjala</strong></p>
<p>In this paper, we report the results of the TeamNRCâ€™s participation in the BHASHA-Task 1 Grammatical Error Correction shared task <a target="_blank" rel="noopener" href="https://github.com/BHASHA-Workshop/IndicGEC2025/">https://github.com/BHASHA-Workshop/IndicGEC2025/</a> for 5 Indian languages. Our approach, focusing on zero&#x2F;few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.</p>
<blockquote>
<p>æœ¬æ–‡æŠ¥é“äº†TeamNRCå‚ä¸BHASHA-Task 1è¯­æ³•é”™è¯¯ä¿®æ­£å…±äº«ä»»åŠ¡çš„ç»“æœã€‚æˆ‘ä»¬é‡‡ç”¨çš„æ–¹æ³•ä¾§é‡äºå¯¹å„ç§è§„æ¨¡çš„æ¨¡å‹ï¼ˆä»4Båˆ°å¤§å‹ä¸“æœ‰æ¨¡å‹ï¼‰è¿›è¡Œé›¶æ¬¡æˆ–å¤šæ¬¡æç¤ºï¼Œåœ¨æ³°å¢å›ºè¯­å’Œå°åœ°è¯­ä¸­åˆ†åˆ«ä»¥GLEUå¾—åˆ†83.78å’Œ84.31çš„æˆç»©æ’åç¬¬4å’Œç¬¬2ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å®éªŒæ‰©å±•åˆ°å…±äº«ä»»åŠ¡ä¸­çš„å…¶ä»–ä¸‰ç§è¯­è¨€â€”â€”æ³°ç±³å°”è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­å’Œå­ŸåŠ æ‹‰è¯­ï¼Œå¹¶ä»”ç»†è§‚å¯Ÿäº†æ•°æ®è´¨é‡å’Œæ‰€ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç»“æœä¸»è¦å¼ºè°ƒäº†å°å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œå¹¶æ€»ç»“äº†åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†å’Œé€‚åˆå°åº¦è¯­è¨€è„šæœ¬çš„ç›¸å…³ä»»åŠ¡çš„é€‚å½“æŒ‡æ ‡æ‰€å­˜åœ¨çš„ç›¸å…³é—®é¢˜å’Œæ‹…å¿§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15260v1">PDF</a> Technical report</p>
<p><strong>Summary</strong><br>å›¢é˜ŸNRCå‚ä¸äº†BHASHA-Task 1çš„äº”ç§å°åº¦è¯­è¨€çš„è¯­æ³•é”™è¯¯ä¿®æ­£å…±äº«ä»»åŠ¡ã€‚é€šè¿‡ä¾§é‡äºé›¶&#x2F;å°‘æ ·æœ¬æç¤ºä¸åŒå¤§å°çš„è¯­è¨€æ¨¡å‹ï¼ˆä»4Båˆ°å¤§å‹ä¸“æœ‰æ¨¡å‹ï¼‰ï¼Œåœ¨æ³°å¢å›ºè¯­å’Œå°åœ°è¯­ä¸­åˆ†åˆ«æ’åç¬¬4å’Œç¬¬2ï¼ŒGLEUè¯„åˆ†åˆ†åˆ«ä¸º83.78å’Œ84.31ã€‚è¯¥è®ºæ–‡è¿˜æ‰©å±•äº†å…¶ä»–ä¸‰ç§å…±äº«è¯­è¨€å®éªŒï¼Œæ·±å…¥æ¢è®¨äº†æ•°æ®è´¨é‡å’Œè¯„ä¼°æŒ‡æ ‡çš„ä½¿ç”¨æƒ…å†µã€‚ä¸»è¦å¼ºè°ƒäº†å°å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œå¹¶æ€»ç»“äº†åˆ›å»ºé€‚åˆå°åº¦è¯­è¨€çš„é«˜è´¨é‡æ•°æ®é›†å’Œé€‚å½“çš„è¯„ä¼°æŒ‡æ ‡çš„å…³æ³¨ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TeamNRCå‚ä¸äº†åœ¨äº”ç§å°åº¦è¯­è¨€ä¸Šçš„è¯­æ³•é”™è¯¯ä¿®æ­£ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é›¶&#x2F;å°‘æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œåœ¨ä¸åŒå¤§å°çš„è¯­è¨€æ¨¡å‹ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</li>
<li>åœ¨æ³°å¢å›ºè¯­å’Œå°åœ°è¯­ä¸­åˆ†åˆ«æ’åç¬¬4å’Œç¬¬2ï¼ŒGLEUè¯„åˆ†è¾ƒé«˜ã€‚</li>
<li>è®ºæ–‡æ‰©å±•äº†å…¶ä»–ä¸‰ç§è¯­è¨€çš„å®éªŒï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ•°æ®è´¨é‡å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>ç»“æœä¸»è¦çªå‡ºäº†å°å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†åˆ›å»ºé€‚åˆå°åº¦è¯­è¨€çš„é«˜è´¨é‡æ•°æ®é›†çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5be89c4f769338340e3cec93adf53590" align="middle">
<img src="https://picx.zhimg.com/v2-814a417a59d7638a2146051cfc99cd2f" align="middle">
<img src="https://picx.zhimg.com/v2-82a688accf3d4b577c5585022e1dad3f" align="middle">
<img src="https://picx.zhimg.com/v2-e54b949c9b6106cfad67c92a469ea802" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Learning-Where-What-and-How-to-Transfer-A-Multi-Role-Reinforcement-Learning-Approach-for-Evolutionary-Multitasking"><a href="#Learning-Where-What-and-How-to-Transfer-A-Multi-Role-Reinforcement-Learning-Approach-for-Evolutionary-Multitasking" class="headerlink" title="Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking"></a>Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking</h2><p><strong>Authors:Jiajun Zhan, Zeyuan Ma, Yue-Jiao Gong, Kay Chen Tan</strong></p>
<p>Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.</p>
<blockquote>
<p>è¿›åŒ–å¤šä»»åŠ¡ï¼ˆEMTï¼‰ç®—æ³•é€šå¸¸éœ€è¦é’ˆå¯¹çŸ¥è¯†è½¬ç§»è¿›è¡Œå®šåˆ¶è®¾è®¡ï¼Œä»¥ç¡®ä¿å¤šä»»åŠ¡ä¼˜åŒ–ä¸­çš„æ”¶æ•›æ€§å’Œæœ€ä¼˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢é€šè¿‡å¼ºåŒ–å­¦ä¹ è®¾è®¡ç³»ç»ŸåŒ–ä¸”å¯æ¨å¹¿çš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šäº†ä¸‰å¤§æŒ‘æˆ˜ï¼šç¡®å®šè¦è½¬ç§»çš„ä»»åŠ¡ï¼ˆå“ªé‡Œï¼‰ã€è¦è½¬ç§»çš„çŸ¥è¯†ï¼ˆä»€ä¹ˆï¼‰å’Œè½¬ç§»çš„æœºåˆ¶ï¼ˆå¦‚ä½•ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªå¤šè§’è‰²å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå…¶ä¸­ä¸‰ä¸ªï¼ˆç»„ï¼‰æ”¿ç­–ç½‘ç»œå……å½“ä¸“ä¸šä»£ç†ï¼šä»»åŠ¡è·¯ç”±ä»£ç†é‡‡ç”¨åŸºäºæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§è¯†åˆ«æ¨¡å—ï¼Œé€šè¿‡æ³¨æ„åŠ›åˆ†æ•°ç¡®å®šæº-ç›®æ ‡è½¬ç§»å¯¹ï¼›çŸ¥è¯†æ§åˆ¶ä»£ç†ç¡®å®šè¦è½¬ç§»çš„ç²¾è‹±è§£å†³æ–¹æ¡ˆçš„æ¯”ä¾‹ï¼›ä¸€ç»„ç­–ç•¥é€‚åº”ä»£ç†é€šè¿‡åŠ¨æ€æ§åˆ¶EMTæ¡†æ¶ä¸­çš„è¶…å‚æ•°æ¥æ§åˆ¶è½¬ç§»å¼ºåº¦ã€‚é€šè¿‡ç«¯åˆ°ç«¯åœ°å¯¹å¢å¼ºçš„å¤šä»»åŠ¡é—®é¢˜åˆ†å¸ƒè¿›è¡Œæ‰€æœ‰ç½‘ç»œæ¨¡å—çš„é¢„è®­ç»ƒï¼Œè·å¾—äº†ä¸€ä¸ªé€šç”¨çš„å…ƒç­–ç•¥ã€‚ç»¼åˆéªŒè¯å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„æ·±å…¥åˆ†æä¸ä»…æ­ç¤ºäº†æˆ‘ä»¬çš„ææ¡ˆèƒŒåçš„åŸç†ï¼Œè€Œä¸”æä¾›äº†å…³äºç³»ç»Ÿæ‰€å­¦å†…å®¹çš„æ·±åˆ»è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®¾è®¡ç³»ç»ŸåŒ–ã€å¯æ¨å¹¿çš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ï¼Œä»¥è§£å†³è¿›åŒ–å¤šä»»åŠ¡ï¼ˆEMTï¼‰ç®—æ³•ä¸­çš„çŸ¥è¯†è½¬ç§»é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸‰å¤§æŒ‘æˆ˜ï¼Œå³ç¡®å®šè½¬ç§»ä»»åŠ¡ï¼ˆå“ªé‡Œï¼‰ã€è¦è½¬ç§»çš„çŸ¥è¯†ï¼ˆä»€ä¹ˆï¼‰å’Œè½¬ç§»æœºåˆ¶ï¼ˆå¦‚ä½•ï¼‰ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªå¤šè§’è‰²å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªï¼ˆç»„ï¼‰ç­–ç•¥ç½‘ç»œï¼Œåˆ†åˆ«ä½œä¸ºä¸“é—¨ä»£ç†æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå¢å¼ºçš„å¤šä»»åŠ¡é—®é¢˜åˆ†å¸ƒä¸Šå¯¹æ‰€æœ‰ç½‘ç»œæ¨¡å—è¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒï¼Œè·å¾—äº†ä¸€ä¸ªå¯æ¨å¹¿çš„å…ƒç­–ç•¥ã€‚ç»¼åˆéªŒè¯å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»£è¡¨æ€§åŸºçº¿æ–¹æ³•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥æ·±å…¥çš„åˆ†æä¸ä»…æ­ç¤ºäº†æœ¬æ–‡ææ¡ˆçš„åˆç†æ€§ï¼Œè¿˜æä¾›äº†å…³äºç³»ç»Ÿå­¦ä¹ å†…å®¹çš„æ·±åˆ»è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ åœ¨è¿›åŒ–å¤šä»»åŠ¡ï¼ˆEMTï¼‰ç®—æ³•ä¸­çš„çŸ¥è¯†è½¬ç§»ç­–ç•¥è®¾è®¡ã€‚</li>
<li>æå‡ºäº†ç¡®å®šè½¬ç§»ä»»åŠ¡ã€è¦è½¬ç§»çš„çŸ¥è¯†å’Œè½¬ç§»æœºåˆ¶çš„ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªå¤šè§’è‰²å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼ŒåŒ…å«ä»»åŠ¡è·¯ç”±ä»£ç†ã€çŸ¥è¯†æ§åˆ¶ä»£ç†å’Œç­–ç•¥é€‚åº”ä»£ç†ä¸‰ä¸ªä¸“é—¨ç½‘ç»œã€‚</li>
<li>é€šè¿‡ç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒï¼Œè·å¾—äº†ä¸€ä¸ªå¯æ¨å¹¿çš„å…ƒç­–ç•¥ã€‚</li>
<li>ç»¼åˆéªŒè¯å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä»£è¡¨æ€§åŸºçº¿ã€‚</li>
<li>æ·±å…¥çš„åˆ†ææ­ç¤ºäº†ææ¡ˆçš„åˆç†æ€§ï¼Œå¹¶å¯¹ç³»ç»Ÿå­¦ä¹ å†…å®¹æä¾›äº†æ·±åˆ»è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d46f8010937335864d4bd4f4bc215c6e" align="middle">
<img src="https://picx.zhimg.com/v2-03c877d8d40db2d97912a0e59052c46c" align="middle">
<img src="https://picx.zhimg.com/v2-0fcce5391a074028b10c04c59fe6aea5" align="middle">
<img src="https://picx.zhimg.com/v2-bbae92c65a2048cb1a3b171698c3ae73" align="middle">
<img src="https://picx.zhimg.com/v2-5ff3623879ef856a1928824d416e4a77" align="middle">
<img src="https://picx.zhimg.com/v2-3748bef4ccd5d9e20660771ec2c0bdc0" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Insert-In-Style-A-Zero-Shot-Generative-Framework-for-Harmonious-Cross-Domain-Object-Composition"><a href="#Insert-In-Style-A-Zero-Shot-Generative-Framework-for-Harmonious-Cross-Domain-Object-Composition" class="headerlink" title="Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition"></a>Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition</h2><p><strong>Authors:Raghu Vamsi Chittersu, Yuvraj Singh Rathore, Pranav Adlinge, Kunal Swami</strong></p>
<p>Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical â€œblendersâ€ that lack generative fidelity and â€œgeneratorsâ€ that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.</p>
<blockquote>
<p>åŸºäºå‚è€ƒçš„å¯¹è±¡ç»„åˆæ–¹æ³•åœ¨å°†ç°å®ä¸–ç•Œå¯¹è±¡æ’å…¥åˆ°é£æ ¼åŒ–é¢†åŸŸæ—¶ä¼šå‡ºç°é—®é¢˜ã€‚è¿™ä¸ªå°šæœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ç›®å‰åˆ†ä¸ºå®ç”¨çš„â€œæ··åˆå™¨â€ç¼ºä¹ç”Ÿæˆä¿çœŸåº¦ä»¥åŠéœ€è¦ä¸åˆ‡å®é™…ã€é’ˆå¯¹æ¯ä¸ªä¸»é¢˜çš„åœ¨çº¿ç²¾ç»†è°ƒæ•´çš„â€œç”Ÿæˆå™¨â€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Insert In Styleï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¢å®ç”¨åˆé«˜åº¦é€¼çœŸçš„é›¶æ ·æœ¬ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå…·æœ‰ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šé¦–å…ˆæ˜¯æ–°é¢–çš„å¤šé˜¶æ®µè®­ç»ƒåè®®ï¼Œå®ƒè§£å¼€èº«ä»½ã€é£æ ¼å’Œç»„åˆçš„è¡¨ç¤ºï¼›å…¶æ¬¡æ˜¯ä¸“ç”¨çš„æ©æ¨¡æ³¨æ„åŠ›æ¶æ„ï¼Œå®ƒåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼ºåˆ¶å®æ–½è¿™ç§åˆ†ç¦»ã€‚è¿™ç§æ–¹æ³•é¿å…äº†é€šç”¨ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹ä¸­å¸¸è§çš„æ¦‚å¿µå¹²æ‰°ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜¯åœ¨ä¸€ä¸ªæ–°çš„10ä¸‡æ ·æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†æ˜¯é€šè¿‡æ–°çš„æ•°æ®ç®¡é“ç²¾å¿ƒæŒ‘é€‰çš„ã€‚è¯¥ç®¡é“å°†å¤§è§„æ¨¡ç”Ÿæˆä¸ä¸¥æ ¼çš„ä¸¤ä¸ªé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹ç›¸ç»“åˆï¼Œä»¥ç¡®ä¿é«˜ä¿çœŸè¯­ä¹‰èº«ä»½å’Œé£æ ¼è¿è´¯æ€§ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„æ¨¡å‹çœŸæ­£å®ç°äº†é›¶æ ·æœ¬ï¼Œæ— éœ€æ–‡æœ¬æç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å…¬å…±é£æ ¼åŒ–ç»„åˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨èº«ä»½å’Œé£æ ¼æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™ä¸€ç»“æœå¾—åˆ°äº†ç”¨æˆ·ç ”ç©¶çš„å¼ºçƒˆæ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15197v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡è§£å†³äº†æ’å…¥ç°å®ä¸–ç•Œå¯¹è±¡åˆ°é£æ ¼åŒ–é¢†åŸŸæ—¶çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºInsert In Styleçš„é›¶æ ·æœ¬ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰å®ç”¨æ€§å’Œé«˜ä¿çœŸåº¦ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬èº«ä»½ã€é£æ ¼å’Œç»„æˆè¡¨ç¤ºçš„åˆ†ç¦»ä»¥åŠä¸“ç”¨çš„æ©è†œæ³¨æ„åŠ›æ¶æ„æ¥å®ç°æ¦‚å¿µå¹²æ‰°çš„é¢„é˜²ã€‚ä½¿ç”¨æ–°å‹æ•°æ®é›†å’Œä¸¥æ ¼çš„ä¸¤é˜¶æ®µè¿‡æ»¤æµç¨‹è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿é«˜ä¿çœŸè¯­ä¹‰èº«ä»½å’Œé£æ ¼ä¸€è‡´æ€§ã€‚æ— éœ€æ–‡æœ¬æç¤ºå³å¯å®ç°çœŸæ­£çš„é›¶æ ·æœ¬ç”Ÿæˆï¼Œå¹¶å¼•å…¥æ–°çš„é£æ ¼åŒ–ç»„åˆå…¬å…±åŸºå‡†æµ‹è¯•ï¼Œåœ¨èº«ä»½å’Œé£æ ¼æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Insert In Styleæ˜¯é¦–ä¸ªè§£å†³æ’å…¥ç°å®ä¸–ç•Œå¯¹è±¡åˆ°é£æ ¼åŒ–é¢†åŸŸçš„é›¶æ ·æœ¬ç”Ÿæˆæ¡†æ¶ï¼Œå…¼å…·å®ç”¨æ€§å’Œé«˜ä¿çœŸåº¦ã€‚</li>
<li>æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒåè®®ï¼Œå®ç°äº†èº«ä»½ã€é£æ ¼å’Œç»„æˆè¡¨ç¤ºçš„åˆ†ç¦»ã€‚</li>
<li>é‡‡ç”¨æ©è†œæ³¨æ„åŠ›æ¶æ„æ¥é¢„é˜²æ¦‚å¿µå¹²æ‰°ï¼Œé€‚ç”¨äºé€šç”¨é›†æˆæ³¨æ„åŠ›æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æ–°å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†é€šè¿‡ä¸¤é˜¶æ®µè¿‡æ»¤æµç¨‹ç¡®ä¿é«˜ä¿çœŸè¯­ä¹‰èº«ä»½å’Œé£æ ¼ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡å‹æ— éœ€æ–‡æœ¬æç¤ºå³å¯å®ç°çœŸæ­£çš„é›¶æ ·æœ¬ç”Ÿæˆã€‚</li>
<li>å¼•å…¥æ–°çš„å…¬å…±åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°é£æ ¼åŒ–ç»„åˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-017465463fe15a8283152576cb55cb3e" align="middle">
<img src="https://picx.zhimg.com/v2-caf24fcb12e44e0144456d3d57a51c25" align="middle">
<img src="https://picx.zhimg.com/v2-648e80fa018898fe6a2bab45c8656703" align="middle">
<img src="https://picx.zhimg.com/v2-c016115974e5a153bf4d18096e3d2973" align="middle">
<img src="https://picx.zhimg.com/v2-473a0e64549ee9f24d543faf990aed39" align="middle">
<img src="https://picx.zhimg.com/v2-75626dab6e2b99f19973b45177e42d52" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HISE-KT-Synergizing-Heterogeneous-Information-Networks-and-LLMs-for-Explainable-Knowledge-Tracing-with-Meta-Path-Optimization"><a href="#HISE-KT-Synergizing-Heterogeneous-Information-Networks-and-LLMs-for-Explainable-Knowledge-Tracing-with-Meta-Path-Optimization" class="headerlink" title="HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization"></a>HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization</h2><p><strong>Authors:Zhiyi Duan, Zixing Shi, Hongyu Yuan, Qi Wang</strong></p>
<p>Knowledge Tracing (KT) aims to mine studentsâ€™ evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target studentâ€™s history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.</p>
<blockquote>
<p>çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰æ—¨åœ¨æŒ–æ˜å­¦ç”Ÿä¸æ–­å˜åŒ–çš„çŸ¥è¯†çŠ¶æ€ï¼Œå¹¶é¢„æµ‹ä»–ä»¬æœªæ¥çš„é—®ç­”è¡¨ç°ã€‚ç°æœ‰åŸºäºå¼‚è´¨ä¿¡æ¯ç½‘ç»œï¼ˆHINsï¼‰çš„æ–¹æ³•å®¹æ˜“å› æ‰‹åŠ¨æˆ–éšæœºé€‰æ‹©å…ƒè·¯å¾„è€Œå¼•å…¥å™ªå£°ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹å…ƒè·¯å¾„å®ä¾‹çš„å¿…è¦è´¨é‡è¯„ä¼°ã€‚ç›¸åï¼Œæœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•å¿½ç•¥äº†å­¦ç”Ÿä¹‹é—´çš„ä¸°å¯Œä¿¡æ¯ï¼Œè¿™ä¸¤ç§èŒƒå¼éƒ½éš¾ä»¥æä¾›ä¸€è‡´å‡†ç¡®ã€æœ‰è¯æ®æ”¯æŒçš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°æ¡†æ¶ï¼Œå³HIN-LLMååŒå¢å¼ºçŸ¥è¯†è¿½è¸ªï¼ˆHISE-KTï¼‰ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†HINså’ŒLLMsã€‚HISE-KTé¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§èŠ‚ç‚¹ç±»å‹çš„å¤šå…³ç³»HINï¼Œé€šè¿‡å¤šä¸ªå…ƒè·¯å¾„æ•æ‰ç»“æ„å…³ç³»ã€‚ç„¶åé‡‡ç”¨LLMæ™ºèƒ½è¯„åˆ†å’Œè¿‡æ»¤å…ƒè·¯å¾„å®ä¾‹ï¼Œä¿ç•™é«˜è´¨é‡è·¯å¾„ï¼Œç‡å…ˆå®ç°è‡ªåŠ¨åŒ–å…ƒè·¯å¾„è´¨é‡è¯„ä¼°ã€‚å—æ•™è‚²å¿ƒç†å­¦åŸç†çš„å¯å‘ï¼ŒåŸºäºå…ƒè·¯å¾„è®¾è®¡äº†ä¸€ç§ç›¸ä¼¼å­¦ç”Ÿæ£€ç´¢æœºåˆ¶ï¼Œä¸ºé¢„æµ‹æä¾›æ›´æœ‰ä»·å€¼çš„ä¸Šä¸‹æ–‡ã€‚æœ€åï¼ŒHISE-KTä½¿ç”¨ç»“æ„åŒ–æç¤ºæ¥æ•´åˆç›®æ ‡å­¦ç”Ÿçš„å†å²è®°å½•ä¸æ£€ç´¢åˆ°çš„ç›¸ä¼¼è½¨è¿¹ï¼Œä½¿LLMä¸ä»…èƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„é¢„æµ‹ï¼Œè€Œä¸”èƒ½å¤Ÿç”Ÿæˆæœ‰è¯æ®æ”¯æŒçš„å¯è§£é‡Šåˆ†ææŠ¥å‘Šã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHISE-KTåœ¨é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„KTåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15191v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰çš„ç›®æ ‡å’Œæ–¹æ³•ã€‚ç°æœ‰çš„åŸºäºå¼‚è´¨ä¿¡æ¯ç½‘ç»œï¼ˆHINsï¼‰çš„æ–¹æ³•å®¹æ˜“å—åˆ°æ‰‹åŠ¨æˆ–éšæœºé€‰æ‹©å…ƒè·¯å¾„è€Œå¼•å…¥å™ªå£°çš„å½±å“ï¼Œç¼ºä¹å…ƒè·¯å¾„å®ä¾‹çš„è´¨é‡è¯„ä¼°ã€‚è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•å¿½è§†äº†å­¦ç”Ÿä¹‹é—´çš„ä¸°å¯Œä¿¡æ¯ï¼Œä¸¤ç§èŒƒå¼åœ¨æä¾›å‡†ç¡®å’ŒåŸºäºè¯æ®çš„è§£é‡Šæ–¹é¢éƒ½é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„æ¡†æ¶HISE-KTï¼Œæ— ç¼é›†æˆHINså’ŒLLMsã€‚HISE-KTé¦–å…ˆæ„å»ºåŒ…å«å¤šç§èŠ‚ç‚¹ç±»å‹çš„å¤šå…³ç³»HINï¼Œæ•æ‰é€šè¿‡å¤šä¸ªå…ƒè·¯å¾„çš„ç»“æ„å…³ç³»ã€‚ç„¶åé‡‡ç”¨LLMæ™ºèƒ½è¯„åˆ†å’Œè¿‡æ»¤å…ƒè·¯å¾„å®ä¾‹ï¼Œä¿ç•™é«˜è´¨é‡è·¯å¾„ï¼Œå¼€åˆ›æ€§åœ°å®ç°è‡ªåŠ¨å…ƒè·¯å¾„è´¨é‡è¯„ä¼°ã€‚æœ€åï¼ŒHISE-KTä½¿ç”¨ç»“æ„åŒ–æç¤ºæ•´åˆç›®æ ‡å­¦ç”Ÿçš„å†å²ä¸ç›¸ä¼¼çš„è½¨è¿¹ï¼Œä½¿LLMèƒ½å¤Ÿç”Ÿæˆæ—¢å‡†ç¡®é¢„æµ‹åˆæä¾›åŸºäºè¯æ®çš„è§£é‡Šåˆ†ææŠ¥å‘Šã€‚å®éªŒè¡¨æ˜ï¼ŒHISE-KTåœ¨é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„KTåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†è¿½è¸ªï¼ˆKTï¼‰æ—¨åœ¨æŒ–æ˜å­¦ç”Ÿçš„çŸ¥è¯†çŠ¶æ€å¹¶é¢„æµ‹å…¶æœªæ¥çš„é—®ç­”è¡¨ç°ã€‚</li>
<li>ç°æœ‰åŸºäºå¼‚è´¨ä¿¡æ¯ç½‘ç»œï¼ˆHINsï¼‰çš„æ–¹æ³•æ˜“å¼•å…¥å™ªå£°ï¼Œç¼ºä¹å…ƒè·¯å¾„å®ä¾‹çš„è´¨é‡è¯„ä¼°ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•å¿½è§†äº†å­¦ç”Ÿé—´çš„ä¸°å¯Œä¿¡æ¯ï¼Œéš¾ä»¥æä¾›å‡†ç¡®å’ŒåŸºäºè¯æ®çš„è§£é‡Šã€‚</li>
<li>HISE-KTæ¡†æ¶æ— ç¼é›†æˆHINså’ŒLLMsï¼Œæé«˜çŸ¥è¯†è¿½è¸ªçš„é¢„æµ‹æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</li>
<li>HISE-KTæ„å»ºå¤šå…³ç³»HINï¼Œæ•æ‰å¤šä¸ªå…ƒè·¯å¾„çš„ç»“æ„å…³ç³»ã€‚</li>
<li>LLMæ™ºèƒ½è¯„åˆ†å’Œè¿‡æ»¤å…ƒè·¯å¾„å®ä¾‹ï¼Œä¿ç•™é«˜è´¨é‡è·¯å¾„ï¼Œå®ç°è‡ªåŠ¨å…ƒè·¯å¾„è´¨é‡è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f4457334c98101fecbd3608d3a4e39f" align="middle">
<img src="https://picx.zhimg.com/v2-2b429348d536fabd7ecb41bc6181655d" align="middle">
<img src="https://picx.zhimg.com/v2-745a20c6638b9da27e60405d7625069c" align="middle">
<img src="https://picx.zhimg.com/v2-b94fe69368fadd0e8dd9ec3dc640eed0" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FaultDiffusion-Few-Shot-Fault-Time-Series-Generation-with-Diffusion-Model"><a href="#FaultDiffusion-Few-Shot-Fault-Time-Series-Generation-with-Diffusion-Model" class="headerlink" title="FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model"></a>FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model</h2><p><strong>Authors:Yi Xu, Zhigang Chen, Rui Wang, Yangfan Li, Fengxiao Tang, Ming Zhao, Jiaqi Liu</strong></p>
<p>In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.</p>
<blockquote>
<p>åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–è®¾å¤‡çš„ç›‘æ§ä¸­ï¼Œæ•…éšœè¯Šæ–­å¯¹ç¡®ä¿ç³»ç»Ÿå¯é æ€§å’Œå®ç°é¢„æµ‹æ€§ç»´æŠ¤è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•…éšœäº‹ä»¶ç¨€å°‘å’Œæ•°æ®æ ‡æ³¨çš„é«˜æˆæœ¬ï¼Œæ•…éšœæ•°æ®çš„ç¨€ç¼ºæ€§ä¸¥é‡é˜»ç¢äº†æ•°æ®é©±åŠ¨çš„æ–¹æ³•çš„åº”ç”¨ã€‚ç°æœ‰çš„æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹ï¼Œé’ˆå¯¹å¤§é‡æ­£å¸¸æ•°æ®è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å°‘æ•°åœºæ™¯ä¸‹çš„æ•…éšœåˆ†å¸ƒæ•æ‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç”±äºæ•…éšœé¢†åŸŸçš„å·¨å¤§å·®å¼‚å’Œæ•…éšœçš„é«˜ç±»å†…å˜åŒ–ï¼Œè¿™äº›æ¨¡å‹äº§ç”Ÿçš„æ ·æœ¬ç¼ºä¹çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å°‘æ•°æ•…éšœæ—¶é—´åºåˆ—ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ­£è´Ÿå·®å¼‚é€‚é…å™¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ­£å¸¸æ•°æ®åˆ†å¸ƒæ¥å»ºæ¨¡æ­£å¸¸å’Œæ•…éšœé¢†åŸŸä¹‹é—´çš„å·®å¼‚ï¼Œä»¥å®ç°å‡†ç¡®çš„æ•…éšœåˆæˆã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šæ ·æ€§æŸå¤±æ¥é˜²æ­¢æ¨¡å¼å´©æºƒï¼Œé€šè¿‡æ ·æœ¬é—´å·®å¼‚æ­£åˆ™åŒ–é¼“åŠ±ç”Ÿæˆå¤šæ ·åŒ–çš„æ•…éšœæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15174v1">PDF</a> 4 figures, 5 tables ,8 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å°‘æ ·æœ¬æ•…éšœæ—¶é—´åºåˆ—ç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³äº†å·¥ä¸šè®¾å¤‡ç›‘æµ‹ä¸­æ•…éšœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ­£è´Ÿå·®å¼‚é€‚é…å™¨åˆ©ç”¨é¢„è®­ç»ƒçš„æ­£å¸¸æ•°æ®åˆ†å¸ƒæ¥å»ºæ¨¡æ­£å¸¸å’Œæ•…éšœåŸŸä¹‹é—´çš„å·®å¼‚ï¼Œè¿›è¡Œå‡†ç¡®çš„æ•…éšœåˆæˆã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šæ ·æ€§æŸå¤±æ¥é˜²æ­¢æ¨¡å¼å´©æºƒï¼Œå¹¶é€šè¿‡æ ·æœ¬é—´å·®å¼‚æ­£åˆ™åŒ–é¼“åŠ±ç”Ÿæˆå¤šæ ·åŒ–çš„æ•…éšœæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹éš¾ä»¥æ•æ‰æ•…éšœåˆ†å¸ƒã€‚</li>
<li>æå‡ºçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶åˆ©ç”¨æ­£å¸¸æ•°æ®åˆ†å¸ƒå»ºæ¨¡æ­£å¸¸ä¸æ•…éšœåŸŸçš„å·®å¼‚ã€‚</li>
<li>æ­£è´Ÿå·®å¼‚é€‚é…å™¨ç”¨äºå‡†ç¡®åˆæˆæ•…éšœæ ·æœ¬ã€‚</li>
<li>å¼•å…¥å¤šæ ·æ€§æŸå¤±æ¥é˜²æ­¢æ¨¡å¼å´©æºƒå¹¶é¼“åŠ±ç”Ÿæˆå¤šæ ·åŒ–æ•…éšœæ ·æœ¬ã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºç¡®ä¿ç³»ç»Ÿå¯é æ€§å’Œé¢„æµ‹æ€§ç»´æŠ¤è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccecd4c9c4dc05d32092984977a3be5e" align="middle">
<img src="https://picx.zhimg.com/v2-6089ee70de215034d9f571015ff53b7f" align="middle">
<img src="https://picx.zhimg.com/v2-17ef0dbad83c1ee8158392e69359ae13" align="middle">
<img src="https://picx.zhimg.com/v2-b828f02d571a34a9905e69487b7da46d" align="middle">
<img src="https://picx.zhimg.com/v2-8df94dda35ee1fbf2439d79fd79b16b1" align="middle">
<img src="https://picx.zhimg.com/v2-fa5c540ad7376da3e2ecb3e442488538" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="X-WIN-Building-Chest-Radiograph-World-Model-via-Predictive-Sensing"><a href="#X-WIN-Building-Chest-Radiograph-World-Model-via-Predictive-Sensing" class="headerlink" title="X-WIN: Building Chest Radiograph World Model via Predictive Sensing"></a>X-WIN: Building Chest Radiograph World Model via Predictive Sensing</h2><p><strong>Authors:Zefan Yang, Ge Wang, James Hendler, Mannudeep K. Kalra, Pingkun Yan</strong></p>
<p>Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.</p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿æ‘„å½±ï¼ˆCXRï¼‰æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡è¦åŒ»å­¦æˆåƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œä½œä¸ºäºŒç»´æŠ•å½±å›¾åƒï¼ŒCXRå—åˆ°ç»“æ„å åŠ çš„é™åˆ¶ï¼Œæ— æ³•æ•æ‰ä¸‰ç»´ç»“æ„ã€‚è¿™ä¸€å±€é™æ€§ä½¿å¾—è¡¨ç¤ºå­¦ä¹ å’Œç–¾ç—…è¯Šæ–­å……æ»¡æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CXRä¸–ç•Œæ¨¡å‹X-WINï¼Œå®ƒé€šè¿‡å­¦ä¹ é¢„æµ‹æ½œåœ¨ç©ºé—´ä¸­èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„äºŒç»´æŠ•å½±æ¥æç‚¼ä½“ç§¯çŸ¥è¯†ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä¸€ä¸ªå…·æœ‰å†…éƒ¨åŒ–çš„ä¸‰ç»´è§£å‰–ç»“æ„çŸ¥è¯†çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹åœ¨ä¸åŒä¸‰ç»´ç©ºé—´å˜æ¢ä¸‹çš„CXRsã€‚åœ¨æŠ•å½±é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºäº²å’Œåº¦çš„å¯¹æ¯”å¯¹é½æŸå¤±ï¼Œåˆ©ç”¨ç›¸äº’ç›¸ä¼¼æ€§æ¥æ•è·æ¥è‡ªåŒä¸€ä½“ç§¯çš„æŠ•å½±ä¹‹é—´çš„ä¸°å¯Œç›¸å…³ä¿¡æ¯ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„é€‚åº”æ€§ï¼Œæˆ‘ä»¬é€šè¿‡é®ç½©å›¾åƒå»ºæ¨¡å°†çœŸå®CXRçº³å…¥è®­ç»ƒï¼Œå¹¶ä½¿ç”¨åŸŸåˆ†ç±»å™¨æ¥é¼“åŠ±çœŸå®å’Œæ¨¡æ‹ŸCXRçš„ç»Ÿè®¡è¡¨ç¤ºç›¸ä¼¼ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒX-WINåœ¨é‡‡ç”¨çº¿æ€§æ¢æµ‹å’Œå°‘é‡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒX-WINè¿˜å±•ç¤ºäº†ç”Ÿæˆç”¨äºé‡å»ºä¸‰ç»´CTä½“ç§¯çš„äºŒç»´æŠ•å½±çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14918v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CXRï¼ˆèƒ¸éƒ¨Xå°„çº¿æ”¾å°„æ‘„å½±ï¼‰æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡è¦åŒ»å­¦æˆåƒæŠ€æœ¯ï¼Œä½†å› å…¶ä¸ºäºŒç»´æŠ•å½±å›¾åƒï¼Œå­˜åœ¨ç»“æ„å åŠ çš„é™åˆ¶ï¼Œéš¾ä»¥æ•æ‰ä¸‰ç»´è§£å‰–ç»“æ„ï¼Œä½¿å¾—è¡¨å¾å­¦ä¹ å’Œç–¾ç—…è¯Šæ–­é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºX-WINçš„æ–°å‹CXRä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ é¢„æµ‹ä¸‰ç»´èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„äºŒç»´æŠ•å½±æ¥æç‚¼ä½“ç§¯çŸ¥è¯†ã€‚æ¨¡å‹æ ¸å¿ƒæ€æƒ³æ˜¯å…·æœ‰å†…éƒ¨åŒ–ä¸‰ç»´è§£å‰–ç»“æ„çŸ¥è¯†çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹åœ¨å„ç§ä¸‰ç»´ç©ºé—´å˜æ¢ä¸‹çš„CXRsã€‚åœ¨æŠ•å½±é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œå¼•å…¥åŸºäºäº²å’Œåº¦çš„å¯¹æ¯”å¯¹é½æŸå¤±ï¼Œåˆ©ç”¨ç›¸äº’ç›¸ä¼¼æ€§æ¥æ•è·æ¥è‡ªåŒä¸€ä½“ç§¯çš„æŠ•å½±ä¹‹é—´çš„ä¸°å¯Œå…³è”ä¿¡æ¯ã€‚ä¸ºæé«˜æ¨¡å‹é€‚åº”æ€§ï¼Œé€šè¿‡æ©ç›–å›¾åƒå»ºæ¨¡å°†çœŸå®CXRçº³å…¥è®­ç»ƒï¼Œå¹¶é‡‡ç”¨åŸŸåˆ†ç±»å™¨é¼“åŠ±çœŸå®å’Œæ¨¡æ‹ŸCXRsçš„ç»Ÿè®¡è¡¨ç¤ºç›¸ä¼¼ã€‚å®éªŒè¡¨æ˜ï¼ŒX-WINåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºç¡€æ¨¡å‹ï¼Œå…·å¤‡é€šè¿‡çº¿æ€§æ¢æµ‹å’Œå°‘é‡æ ·æœ¬å¾®è°ƒè¿›è¡Œæ¸²æŸ“çš„èƒ½åŠ›ï¼Œè¿˜èƒ½é‡å»ºä¸‰ç»´CTä½“ç§¯çš„äºŒç»´æŠ•å½±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CXRä½œä¸ºäºŒç»´æŠ•å½±å›¾åƒï¼Œå­˜åœ¨ç»“æ„å åŠ é™åˆ¶ï¼Œéš¾ä»¥æ•æ‰ä¸‰ç»´è§£å‰–ç»“æ„ï¼Œå½±å“ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„CXRä¸–ç•Œæ¨¡å‹X-WINï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ é¢„æµ‹ä¸‰ç»´CTçš„äºŒç»´æŠ•å½±æ¥æç‚¼ä½“ç§¯çŸ¥è¯†ã€‚</li>
<li>X-WINæ¨¡å‹æ ¸å¿ƒåœ¨äºé¢„æµ‹ä¸åŒä¸‰ç»´ç©ºé—´å˜æ¢ä¸‹çš„CXRsï¼Œå…·æœ‰æ›´å¼ºçš„è¡¨å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥åŸºäºäº²å’Œåº¦çš„å¯¹æ¯”å¯¹é½æŸå¤±ï¼Œæé«˜æ¨¡å‹åœ¨æŠ•å½±é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ©ç›–å›¾åƒå»ºæ¨¡å’ŒåŸŸåˆ†ç±»å™¨çš„æ–¹æ³•ï¼Œæé«˜æ¨¡å‹çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜X-WINåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·å¤‡çº¿æ€§æ¢æµ‹å’Œå°‘é‡æ ·æœ¬å¾®è°ƒçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc9c6a9cab245346dcf37d1014c1fe59" align="middle">
<img src="https://picx.zhimg.com/v2-8f36cf3c1cd4bf091facfa81db982753" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Do-Large-Language-Models-LLMs-Understand-Chronology"><a href="#Do-Large-Language-Models-LLMs-Understand-Chronology" class="headerlink" title="Do Large Language Models (LLMs) Understand Chronology?"></a>Do Large Language Models (LLMs) Understand Chronology?</h2><p><strong>Authors:Pattaraphon Kenny Wongchamcharoen, Paul Glasserman</strong></p>
<p>Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium&#x2F;high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low&#x2F;minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡‘èå’Œç»æµé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå…¶ä¸­åŸºäºæç¤ºçš„å¯¹æŠ—å‰ç»åå·®çš„å°è¯•éšå«åœ°å‡è®¾æ¨¡å‹èƒ½å¤Ÿç†è§£æ—¶é—´é¡ºåºã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ—¶é—´é¡ºåºä»»åŠ¡æ¥æµ‹è¯•è¿™ä¸ªåŸºæœ¬é—®é¢˜ï¼Œè¿™äº›ä»»åŠ¡çš„å¤æ‚åº¦é€æ¸å¢åŠ ï¼ŒåŸºäºæ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶å·²ç»çŸ¥é“çš„äº‹å®ã€‚æˆ‘ä»¬çš„ä»»åŠ¡åŒ…æ‹¬ï¼ˆ1ï¼‰æ—¶é—´é¡ºåºæ’åˆ—ï¼Œï¼ˆ2ï¼‰æ¡ä»¶æ’åºï¼ˆå…ˆè¿‡æ»¤ï¼Œç„¶åæ’åºï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰å¹´ä»£é”™è¯¯æ£€æµ‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†GPT-4.1ã€Claude-3.7 Sonnetåœ¨å¯ç”¨å’Œæœªå¯ç”¨æ‰©å±•æ€è€ƒï¼ˆETï¼‰çš„æƒ…å†µä¸‹ï¼Œä»¥åŠGPT-5åœ¨ä¸åŒæ¨ç†åŠªåŠ›ç¨‹åº¦ä¸‹çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14214v2">PDF</a> Version 2: corrected footnote and added code repository link. Extended version of our work presented at the AAAI-26 AI4TS Workshop (poster) and AAAI-26 Student Abstract Program (oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èç»æµå­¦é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œå­˜åœ¨å¯¹æ—¶é—´é¡ºåºç†è§£çš„å‡è®¾ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—æŒ‰éš¾åº¦é€’å¢çš„æ—¶é—´é¡ºåºä»»åŠ¡æµ‹è¯•æ¨¡å‹å¯¹æ­¤ç†è§£çš„èƒ½åŠ›ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬ï¼ˆ1ï¼‰æ—¶é—´é¡ºåºæ’åºï¼Œï¼ˆ2ï¼‰æ¡ä»¶æ’åºï¼ˆå…ˆç­›é€‰åæ’åºï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰å¹´ä»£é”™è¯¯æ£€æµ‹ã€‚è¯„ä¼°äº†GPT-4.1ã€Claude-3.7 Sonnetï¼ˆå¸¦æ‰©å±•æ€è€ƒï¼‰ä»¥åŠGPT-5åœ¨ä¸åŒæ¨ç†éš¾åº¦ä¸‹çš„è¡¨ç°ã€‚å‘ç°å³ä½¿åºåˆ—é•¿åº¦å¢åŠ ï¼Œç²¾ç¡®åŒ¹é…ç‡æ€¥å‰§ä¸‹é™ï¼Œä½†æ’åç›¸å…³æ€§ä»ç„¶ä¿æŒé«˜ä½ã€‚æ¡ä»¶æ’åºä¸­çš„å¤§å¤šæ•°é”™è¯¯æºäºç­›é€‰æ­¥éª¤è€Œéæ’åºæ­¥éª¤ï¼Œä½†GPT-5å’Œå¸¦æ‰©å±•æ€è€ƒçš„Claude-3.7 Sonnetè¡¨ç°æ˜¾è‘—ä¼˜äºå¸¸è§„æ¨¡å‹ã€‚å¹´ä»£é”™è¯¯æ£€æµ‹æ˜¯LLMsæœ€å®¹æ˜“å®Œæˆçš„ä»»åŠ¡ï¼Œä½†éšç€æ—¶é—´çº¿æˆ–å®ä½“çš„é‡å ç¨‹åº¦å¢åŠ ï¼Œæ€§èƒ½ä»ç„¶ä¼šä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯æ˜¾ç¤ºåˆ†é…æ˜ç¡®çš„æ¨ç†é¢„ç®—æœ‰åŠ©äºæ—¶é—´é¡ºåºæ’åºï¼ŒGPT-5åœ¨ä¸­ç­‰&#x2F;é«˜æ¨ç†åŠªåŠ›ä¸‹å¯åœ¨æ‰€æœ‰é•¿åº¦ä¸Šå®ç°å®Œç¾æ’åºå’Œæ¡ä»¶ç­›é€‰ï¼ˆåŒ…æ‹¬è‡ªæˆ‘ç­›é€‰å’Œç»™å®šå­é›†ï¼‰ï¼Œè€Œä½&#x2F;æœ€å°åŠªåŠ›ç¨‹åº¦åœ¨è¾ƒé•¿åˆ—è¡¨ä¸Šä¼šå‡ºç°é€€åŒ–ç°è±¡ã€‚æœ¬ç ”ç©¶çš„ç»“æœæ­ç¤ºäº†å½“å‰LLMåœ¨æ—¶é—´é¡ºåºä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä¸ºä»»åŠ¡å¤æ‚æ€§æä¾›äº†è§è§£ï¼Œå±•ç¤ºäº†æ¨ç†æœ‰åŠ©äºçš„åœºæ™¯ã€‚è¿™äº›æ¨¡å¼å¯¹äºLLMåœ¨é‡‘èé¢†åŸŸçš„å®æ—¶åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èç»æµå­¦é¢†åŸŸçš„åº”ç”¨éœ€è¦ç†è§£æ—¶é—´é¡ºåºã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—æ—¶é—´é¡ºåºä»»åŠ¡è¯„ä¼°äº†ä¸åŒLLMæ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>éšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œæ¨¡å‹çš„ç²¾ç¡®åŒ¹é…ç‡ä¸‹é™ï¼Œä½†æ’åç›¸å…³æ€§ä¿æŒé«˜ä½ã€‚</li>
<li>æ¡ä»¶æ’åºä¸­çš„é”™è¯¯ä¸»è¦æºäºç­›é€‰æ­¥éª¤ã€‚</li>
<li>GPT-5å’Œå¸¦æ‰©å±•æ€è€ƒçš„Claude-3.7 Sonnetåœ¨æ¡ä»¶æ’åºä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¹´ä»£é”™è¯¯æ£€æµ‹æ˜¯LLMsæœ€å®¹æ˜“å®Œæˆçš„ä»»åŠ¡ï¼Œä½†éšç€æ—¶é—´çº¿æˆ–å®ä½“çš„é‡å ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>åˆ†é…æ˜ç¡®çš„æ¨ç†é¢„ç®—æœ‰åŠ©äºæ—¶é—´é¡ºåºæ’åºä»»åŠ¡ï¼ŒGPT-5åœ¨ä¸­ç­‰æ¨ç†åŠªåŠ›ä¸‹è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2aad6073229f6792dad089134b0be588" align="middle">
<img src="https://picx.zhimg.com/v2-6ea1a6cd6f74619c9a9f184c8c3a95c7" align="middle">
<img src="https://picx.zhimg.com/v2-ba537e383a16a94eb9e8276e4a47bb5f" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Point-Cloud-Quantization-through-Multimodal-Prompting-for-3D-Understanding"><a href="#Point-Cloud-Quantization-through-Multimodal-Prompting-for-3D-Understanding" class="headerlink" title="Point Cloud Quantization through Multimodal Prompting for 3D Understanding"></a>Point Cloud Quantization through Multimodal Prompting for 3D Understanding</h2><p><strong>Authors:Hongxuan Li, Wencheng Zhu, Huiying Xu, Xinzhong Zhu, Pengfei Zhu</strong></p>
<p>Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.</p>
<blockquote>
<p>å‘é‡é‡åŒ–åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ä¸­å·²æˆä¸ºä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œé€šè¿‡ç¦»æ•£ä»¤ç‰Œç¼–ç ç»Ÿä¸€äº†å¼‚è´¨è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºç¨³å¥çš„ä»£ç æœ¬è®¾è®¡ã€‚å½“å‰åŸºäºåŸå‹çš„æ–¹æ³•ä¾èµ–äºå¯è®­ç»ƒå‘é‡æˆ–èšç±»è´¨å¿ƒï¼Œåœ¨ä»£è¡¨æ€§å’Œè§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œå¤šæ¨¡æ€å¯¹é½åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç‚¹äº‘åˆ†æçš„å¤šæ¨¡æ€æç¤ºé©±åŠ¨é‡åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ä¸¤ä¸ªæ ¸å¿ƒè§è§£ä¹‹ä¸Šï¼š1ï¼‰æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬åµŒå…¥é€šè¿‡å¤šå¯¹ä¸€å¯¹æ¯”å¯¹é½å›ºæœ‰åœ°ç¼–ç è§†è§‰è¯­ä¹‰ï¼Œè‡ªç„¶åœ°ä½œä¸ºç¨³å¥çš„åŸå‹å…ˆéªŒï¼›2ï¼‰å¤šæ¨¡æ€æç¤ºèƒ½å¤Ÿè‡ªé€‚åº”åœ°ç»†åŒ–è¿™äº›åŸå‹ï¼Œæœ‰æ•ˆåœ°ç¼“è§£è§†è§‰è¯­è¨€è¯­ä¹‰å·®è·ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªåŒçº¦æŸé‡åŒ–ç©ºé—´ï¼Œé€šè¿‡ç´§å‡‘æ€§å’Œåˆ†ç¦»æ­£åˆ™åŒ–æ¥å¼ºåˆ¶å®æ–½ï¼Œæ— ç¼é›†æˆäº†è§†è§‰å’ŒåŸå‹ç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿæ··åˆè¡¨ç¤ºï¼Œè”åˆç¼–ç å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨Gumbel-Softmaxæ¾å¼›æŠ€æœ¯å®ç°å¯å¾®åˆ†çš„ç¦»æ•£åŒ–ï¼ŒåŒæ—¶ä¿æŒé‡åŒ–çš„ç¨€ç–æ€§ã€‚åœ¨ModelNet40å’ŒScanObjectNNæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒæ¸…æ¥šåœ°è¯æ˜äº†æ‰€ææ–¹æ³•çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12079v2">PDF</a> Accepted by AAAI 2026. 11 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å‘é‡é‡åŒ–åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­å±•ç°å‡ºå¼ºå¤§çš„å®åŠ›ï¼Œé€šè¿‡ç¦»æ•£ä»¤ç‰Œç¼–ç ç»Ÿä¸€äº†å¼‚è´¨è¡¨ç¤ºã€‚æœ¬æ–‡æå‡ºä¸€ç§ç®€å•ã€å¤šæ¨¡æ€æç¤ºé©±åŠ¨çš„ç‚¹äº‘åˆ†æé‡åŒ–æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰åŸå‹åŸºç¡€æ–¹æ³•ä»£è¡¨æ€§ä¸è¶³å’Œè§£é‡Šæ€§ä¸å¼ºçš„é—®é¢˜ã€‚æ¡†æ¶åŸºäºä¸¤å¤§æ ¸å¿ƒè§‚ç‚¹ï¼Œå³æ–‡æœ¬åµŒå…¥è‡ªç„¶ç¼–ç è§†è§‰è¯­ä¹‰ä½œä¸ºç¨³å¥åŸå‹å…ˆéªŒå’Œå¤šæ¨¡æ€æç¤ºè‡ªé€‚åº”ä¼˜åŒ–è¿™äº›åŸå‹ã€‚é€šè¿‡å¼•å…¥åŒçº¦æŸé‡åŒ–ç©ºé—´ï¼Œç»“åˆè§†è§‰å’ŒåŸå‹ç‰¹å¾ï¼Œå½¢æˆæ··åˆè¡¨ç¤ºï¼ŒåŒæ—¶é‡‡ç”¨Gumbel-Softmaxæ¾å¼›å®ç°å¯å¾®åˆ†çš„ç¦»æ•£åŒ–å¹¶ä¿æŒé‡åŒ–ç¨€ç–æ€§ã€‚åœ¨ModelNet40å’ŒScanObjectNNæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘é‡é‡åŒ–å·²æˆä¸ºå¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æœ‰åŠ›å·¥å…·ï¼Œé€šè¿‡ç¦»æ•£ä»¤ç‰Œç¼–ç ç»Ÿä¸€å¼‚è´¨è¡¨ç¤ºã€‚</li>
<li>ç°æœ‰åŸå‹åŸºç¡€æ–¹æ³•åœ¨ä»£è¡¨æ€§å’Œè§£é‡Šæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§ç®€å•ã€å¤šæ¨¡æ€æç¤ºé©±åŠ¨çš„é‡åŒ–æ¡†æ¶ï¼Œç”¨äºç‚¹äº‘åˆ†æã€‚</li>
<li>æ–‡æœ¬åµŒå…¥è‡ªç„¶ç¼–ç è§†è§‰è¯­ä¹‰ä½œä¸ºç¨³å¥åŸå‹å…ˆéªŒã€‚</li>
<li>å¤šæ¨¡æ€æç¤ºè‡ªé€‚åº”ä¼˜åŒ–åŸå‹ï¼Œæœ‰æ•ˆç¼“è§£è§†è§‰è¯­è¨€è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>å¼•å…¥åŒçº¦æŸé‡åŒ–ç©ºé—´ï¼Œç»“åˆè§†è§‰å’ŒåŸå‹ç‰¹å¾ï¼Œå½¢æˆæ··åˆè¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36d02e7fc4929084834b7522a0d6f423" align="middle">
<img src="https://picx.zhimg.com/v2-deb2d9c2c858d72fbae4136edccac137" align="middle">
<img src="https://picx.zhimg.com/v2-daad4a6bf40b0df23d0a0d64f91ed9df" align="middle">
<img src="https://picx.zhimg.com/v2-57490639966231be2006a2cbea6fa24a" align="middle">
<img src="https://picx.zhimg.com/v2-92513d1cf240a226d55967cfa239dc95" align="middle">
<img src="https://picx.zhimg.com/v2-4d1a5ac610e314da91f516d65e745a0a" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="EPSegFZ-Efficient-Point-Cloud-Semantic-Segmentation-for-Few-and-Zero-Shot-Scenarios-with-Language-Guidance"><a href="#EPSegFZ-Efficient-Point-Cloud-Semantic-Segmentation-for-Few-and-Zero-Shot-Scenarios-with-Language-Guidance" class="headerlink" title="EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance"></a>EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance</h2><p><strong>Authors:Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee</strong></p>
<p>Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.</p>
<blockquote>
<p>é’ˆå¯¹å°‘æ ·æœ¬3Dç‚¹äº‘è¯­ä¹‰åˆ†å‰²çš„æœ€æ–°æ–¹æ³•é€šå¸¸éœ€è¦ä¸¤é˜¶æ®µå­¦ä¹ è¿‡ç¨‹ï¼Œå³é¢„è®­ç»ƒé˜¶æ®µåè·Ÿå°‘æ•°æ ·æœ¬è®­ç»ƒé˜¶æ®µã€‚è™½ç„¶è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒä»¬è¿‡äºä¾èµ–é¢„è®­ç»ƒï¼Œé˜»ç¢äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚ä¸€äº›æ¨¡å‹è¯•å›¾é¿å…é¢„è®­ç»ƒï¼Œä½†æœªèƒ½æ•è·å……è¶³çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨æ”¯æŒé›†ä¸­çš„è§†è§‰ä¿¡æ¯ï¼Œè€Œå¿½è§†æˆ–æœªå……åˆ†åˆ©ç”¨å…¶ä»–æœ‰ç”¨æ•°æ®ï¼Œå¦‚æ–‡æœ¬æ³¨é‡Šã€‚è¿™ç§å¯¹æ”¯æŒä¿¡æ¯çš„ä¸å……åˆ†åˆ©ç”¨å½±å“äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é™åˆ¶äº†å…¶é›¶æ ·æœ¬èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ— é¢„è®­ç»ƒç½‘ç»œï¼Œåä¸ºâ€œEfficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenariosâ€ï¼ˆé«˜æ•ˆç‚¹äº‘è¯­ä¹‰åˆ†å‰²ç”¨äºå°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ï¼‰ã€‚æˆ‘ä»¬çš„EPSegFZåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆæ˜¯Prototype-Enhanced Registers Attentionï¼ˆProERAï¼‰æ¨¡å—å’ŒåŸºäºDual Relative Positional Encodingï¼ˆDRPEï¼‰çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæ”¹è¿›ç‰¹å¾æå–å’Œå‡†ç¡®çš„æŸ¥è¯¢åŸå‹å¯¹åº”å…³ç³»çš„æ„å»ºï¼Œæ— éœ€é¢„è®­ç»ƒã€‚å…¶æ¬¡æ˜¯Language-Guided Prototype Embeddingï¼ˆLGPEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆåˆ©ç”¨æ”¯æŒé›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥æé«˜å°‘æ ·æœ¬æ€§èƒ½å¹¶å®ç°é›¶æ ·æœ¬æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨S3DISå’ŒScanNetåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†5.68%å’Œ3.82%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11700v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€é¢„è®­ç»ƒçš„é«˜æ•ˆç‚¹äº‘è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œåä¸ºEPSegFZã€‚è¯¥ç½‘ç»œé€šè¿‡å¼•å…¥ProERAæ¨¡å—å’ŒDRPEäº¤å‰æ³¨æ„åŠ›æœºåˆ¶æé«˜ç‰¹å¾æå–èƒ½åŠ›ï¼Œæ„å»ºå‡†ç¡®çš„æŸ¥è¯¢åŸå‹å¯¹åº”å…³ç³»ï¼ŒåŒæ—¶åˆ©ç”¨æ”¯æŒé›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæé«˜å°‘æ ·æœ¬æ€§èƒ½å¹¶å®ç°é›¶æ ·æœ¬æ¨ç†ã€‚åœ¨S3DISå’ŒScanNetåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåˆ†åˆ«æé«˜äº†5.68%å’Œ3.82%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•éœ€è¦å¤æ‚çš„ä¸¤é˜¶æ®µå­¦ä¹ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œå°‘é‡æ ·æœ¬è®­ç»ƒï¼Œç¼ºä¹çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>ä¸€äº›æ¨¡å‹è¯•å›¾é¿å…é¢„è®­ç»ƒï¼Œä½†æœªèƒ½æ•è·è¶³å¤Ÿçš„ä¿¡æ¯ã€‚</li>
<li>å½“å‰æ–¹æ³•è¿‡äºä¾èµ–è§†è§‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†å…¶ä»–æœ‰ç”¨çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬æ³¨é‡Šã€‚</li>
<li>æå‡ºçš„EPSegFZç½‘ç»œé€šè¿‡å¼•å…¥ProERAæ¨¡å—å’ŒDRPEäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾æå–èƒ½åŠ›ï¼Œæ— éœ€é¢„è®­ç»ƒã€‚</li>
<li>EPSegFZåˆ©ç”¨è¯­è¨€å¼•å¯¼åŸå‹åµŒå…¥ï¼ˆLGPEï¼‰æ¨¡å—ï¼Œæœ‰æ•ˆç»“åˆæ”¯æŒé›†ä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œæå‡å°‘æ ·æœ¬æ€§èƒ½å¹¶å®ç°é›¶æ ·æœ¬æ¨ç†ã€‚</li>
<li>EPSegFZåœ¨S3DISå’ŒScanNetåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0276c1bf535a645665c4f31725f23923" align="middle">
<img src="https://picx.zhimg.com/v2-265382953809ec9d38a8417c23bda251" align="middle">
<img src="https://picx.zhimg.com/v2-cc89d7de2054271472481608edb22f5e" align="middle">
<img src="https://picx.zhimg.com/v2-1b0aa42b4aad732a705a04b6fc4b9e11" align="middle">
<img src="https://picx.zhimg.com/v2-6aafb90ec62aae8fd938276f6a2e47c7" align="middle">
<img src="https://picx.zhimg.com/v2-20e6d41ca8cd8d0c7c542b292f378b4f" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AGENet-Adaptive-Edge-aware-Geodesic-Distance-Learning-for-Few-Shot-Medical-Image-Segmentation"><a href="#AGENet-Adaptive-Edge-aware-Geodesic-Distance-Learning-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation"></a>AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation</h2><p><strong>Authors:Ziyuan Gao</strong></p>
<p>Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.</p>
<blockquote>
<p>åŒ»å­¦å½±åƒåˆ†å‰²éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™æˆä¸ºä¸´åºŠåº”ç”¨çš„ä¸€ä¸ªé‡å¤§ç“¶é¢ˆã€‚å°½ç®¡å°‘æ•°é•œå¤´åˆ†å‰²æ–¹æ³•å¯ä»¥ä»æå°‘é‡çš„æ ·æœ¬ä¸­å­¦ä¹ ï¼Œä½†ç°æœ‰çš„æ–¹æ³•åœ¨åŒ»å­¦å½±åƒçš„ç²¾ç¡®è¾¹ç•Œåˆ’åˆ†ä¸Šè¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ºç°è§£å‰–ç›¸ä¼¼åŒºåŸŸè€Œæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†AGENetï¼ˆè‡ªé€‚åº”æµ‹åœ°çº¿è¾¹ç¼˜æ„ŸçŸ¥ç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œé€šè¿‡è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ èå…¥ç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼ŒåŒ»å­¦ç»“æ„éµå¾ªå¯é¢„æµ‹çš„å‡ ä½•æ¨¡å¼ï¼Œå³ä½¿åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œä¹Ÿèƒ½å¼•å¯¼åŸå‹æå–ã€‚ä¸å…¶ä»–ä¾èµ–å¤æ‚ç»„ä»¶æˆ–é‡å‹ç¥ç»ç½‘ç»œçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è®¡ç®—è½»é‡å‹çš„å‡ ä½•å»ºæ¨¡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šï¼ˆ1ï¼‰è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ æ¨¡å—ï¼Œé€šè¿‡è¿­ä»£å¿«é€Ÿè¡Œè¿›ç»†åŒ–æ¥å°Šé‡è§£å‰–è¾¹ç•Œï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”åŸå‹æå–ï¼Œé€šè¿‡ç©ºé—´åŠ æƒèšåˆæ•æ‰å…¨å±€ç»“æ„å’Œå±€éƒ¨è¾¹ç•Œç»†èŠ‚ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”å‚æ•°å­¦ä¹ ï¼Œå¯è‡ªåŠ¨é€‚åº”ä¸åŒçš„å™¨å®˜ç‰¹å¾ã€‚åœ¨å¤šç§åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾ƒæœ€æ–°çš„æ–¹æ³•æœ‰æ‰€æ”¹å–„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘è¾¹ç•Œè¯¯å·®çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œä½¿å…¶æˆä¸ºåœ¨éœ€è¦ç²¾ç¡®åˆ†å‰²ä¸”æ ‡æ³¨æ•°æ®æœ‰é™çš„ä¸´åºŠåº”ç”¨ä¸­éå¸¸åˆé€‚çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11662v1">PDF</a> Accepted for publication in WACV 2026 (Round 2)</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒåˆ†å‰²éœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™æˆä¸ºä¸´åºŠåº”ç”¨ä¸­çš„ç“¶é¢ˆã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶AGENetï¼ˆè‡ªé€‚åº”æµ‹åœ°çº¿è¾¹ç¼˜æ„ŸçŸ¥ç½‘ç»œï¼‰ï¼Œå®ƒé€šè¿‡è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ æŠ€æœ¯æ¥èåˆç©ºé—´å…³ç³»ã€‚æ–‡ç« ä¸»è¦è§‚ç‚¹æ˜¯åŒ»å­¦ç»“æ„éµå¾ªå¯é¢„æµ‹çš„å‡ ä½•æ¨¡å¼ï¼Œå³ä½¿è®­ç»ƒæ•°æ®æœ‰é™ï¼Œä¹Ÿèƒ½å¼•å¯¼åŸå‹æå–ã€‚ä¸åŒå…¶ä»–ä¾èµ–å¤æ‚æ¶æ„æˆ–å¤§å‹ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼ŒAGENeté‡‡ç”¨è®¡ç®—è½»é‡çº§çš„å‡ ä½•å»ºæ¨¡ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå°Šé‡è§£å‰–è¾¹ç•Œçš„è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ æ¨¡å—ã€é€šè¿‡ç©ºé—´åŠ æƒèšåˆæ•æ‰å…¨å±€ç»“æ„å’Œå±€éƒ¨è¾¹ç•Œç»†èŠ‚çš„è‡ªé€‚åº”åŸå‹æå–ä»¥åŠè‡ªåŠ¨é€‚åº”ä¸åŒå™¨å®˜ç‰¹æ€§çš„è‡ªé€‚åº”å‚æ•°å­¦ä¹ ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å‡å°‘è¾¹ç•Œè¯¯å·®çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç²¾ç¡®åˆ†å‰²å’Œæœ‰é™æ ‡æ³¨æ•°æ®çš„ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²é¢ä¸´å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ç“¶é¢ˆã€‚</li>
<li>AGENetæ¡†æ¶é€šè¿‡è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ æŠ€æœ¯èåˆç©ºé—´å…³ç³»ã€‚</li>
<li>åŒ»å­¦ç»“æ„éµå¾ªå¯é¢„æµ‹çš„å‡ ä½•æ¨¡å¼ï¼Œå¼•å¯¼åŸå‹æå–ã€‚</li>
<li>AGENeté‡‡ç”¨è®¡ç®—è½»é‡çº§çš„å‡ ä½•å»ºæ¨¡ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ã€‚</li>
<li>å°Šé‡è§£å‰–è¾¹ç•Œçš„è¾¹ç¼˜æ„ŸçŸ¥æµ‹åœ°çº¿è·ç¦»å­¦ä¹ æ¨¡å—ã€‚</li>
<li>è‡ªé€‚åº”åŸå‹æå–æ•æ‰å…¨å±€ç»“æ„å’Œå±€éƒ¨è¾¹ç•Œç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d5d511626d5ace9713057551c27a11d" align="middle">
<img src="https://picx.zhimg.com/v2-b8ad49db88be1e28c99e9f1efa129157" align="middle">
<img src="https://picx.zhimg.com/v2-5fe7e0d7254935e973f8c6f45edbdf7e" align="middle">
<img src="https://picx.zhimg.com/v2-e832673fb7996545c7904e9b08ac4d7e" align="middle">
<img src="https://picx.zhimg.com/v2-1e49fadf9974e0792017c75692be6222" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Boosting-In-Silicon-Directed-Evolution-with-Fine-Tuned-Protein-Language-Model-and-Tree-Search"><a href="#Boosting-In-Silicon-Directed-Evolution-with-Fine-Tuned-Protein-Language-Model-and-Tree-Search" class="headerlink" title="Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search"></a>Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</h2><p><strong>Authors:Yaodong Yang, Yang Wang, Jinpeng Li, Pei Guo, Da Han, Guangyong Chen, Pheng-Ann Heng</strong></p>
<p>Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms largely focus on designing heuristic search strategies, they overlook how to integrate the transformative protein language models, which encode rich evolutionary patterns, with reinforcement learning to learn to directly evolve proteins. To bridge this gap, we propose AlphaDE, a novel framework to optimize protein sequences by harnessing the innovative paradigms of large language models such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A further case study demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.</p>
<blockquote>
<p>è›‹ç™½è´¨é€šè¿‡æ°¨åŸºé…¸åºåˆ—çªå˜è¿›è¡Œè¿›åŒ–æ˜¯ç”Ÿå‘½ç§‘å­¦çš„æ ¸å¿ƒã€‚å½“å‰ï¼Œç¡…åŸºå®šå‘è¿›åŒ–ç®—æ³•ä¸»è¦å…³æ³¨å¯å‘å¼æœç´¢ç­–ç•¥çš„è®¾è®¡ï¼Œå´å¿½ç•¥äº†å¦‚ä½•ç»“åˆåŒ…å«ä¸°å¯Œè¿›åŒ–æ¨¡å¼çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæ¥å­¦ä¹ ç›´æ¥è¿›åŒ–è›‹ç™½è´¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaDEï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚å¾®è°ƒä¸æµ‹è¯•æ—¶æ¨ç†ï¼‰çš„åˆ›æ–°èŒƒå¼æ¥ä¼˜åŒ–è›‹ç™½è´¨åºåˆ—çš„æ–°å‹æ¡†æ¶ã€‚é¦–å…ˆï¼ŒAlphaDEä½¿ç”¨åŒæºè›‹ç™½è´¨åºåˆ—çš„æ©ç è¯­è¨€å»ºæ¨¡å¯¹é¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¿€æ´»æ‰€å…³æ³¨è›‹ç™½è´¨ç±»åˆ«çš„è¿›åŒ–å¯èƒ½æ€§ã€‚å…¶æ¬¡ï¼ŒAlphaDEå¼•å…¥äº†åŸºäºè’™ç‰¹å¡ç½—æ ‘æœç´¢çš„æµ‹è¯•æ—¶æ¨ç†ï¼Œæœ‰æ•ˆåœ°åœ¨æ¥è‡ªå¾®è°ƒåçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„è¿›åŒ–æŒ‡å¯¼ä¸‹è¿›åŒ–è›‹ç™½è´¨ã€‚å¤§é‡çš„åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å°‘æ•°é•œå¤´å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒAlphaDEä¹Ÿæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒAlphaDEæ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–å‡èšavGFPçš„è›‹ç™½è´¨åºåˆ—ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09900v2">PDF</a> working in progress, 26 pages, 6 figures, 16 tables, updated with more baselines and related works</p>
<p><strong>Summary</strong></p>
<p>è›‹ç™½è´¨åºåˆ—é€šè¿‡æ°¨åŸºé…¸çªå˜è¿›åŒ–æ˜¯ç”Ÿå‘½ç§‘å­¦çš„æ ¸å¿ƒé¢†åŸŸä¹‹ä¸€ã€‚å½“å‰åŸºäºèŠ¯ç‰‡çš„å®šå‘è¿›åŒ–ç®—æ³•ä¸»è¦å…³æ³¨å¯å‘å¼æœç´¢ç­–ç•¥çš„è®¾è®¡ï¼Œå¿½è§†äº†å¦‚ä½•æ•´åˆå¯Œå«è¿›åŒ–æ¨¡å¼çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç›´æ¥è¿›åŒ–è›‹ç™½è´¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaDEè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ä¸æµ‹è¯•æ—¶é—´æ¨æ–­ç­‰åˆ›æ–°ç†å¿µæ¥ä¼˜åŒ–è›‹ç™½è´¨åºåˆ—ã€‚AlphaDEé¦–å…ˆé€šè¿‡åŒæºè›‹ç™½è´¨åºåˆ—çš„æ©ç è¯­è¨€å»ºæ¨¡å¯¹é¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¿€æ´»ç›®æ ‡è›‹ç™½è´¨ç±»åˆ«çš„è¿›åŒ–å¯èƒ½æ€§ã€‚ç„¶åï¼ŒAlphaDEå¼•å…¥åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æµ‹è¯•æ—¶é—´æ¨æ–­ï¼Œæœ‰æ•ˆåœ°åœ¨è›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„æŒ‡å¯¼ä¸‹è¿›åŒ–è›‹ç™½è´¨ã€‚åŸºå‡†æµ‹è¯•å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¾®è°ƒæ ·æœ¬é‡å¾ˆå°‘çš„æƒ…å†µä¸‹ï¼ŒAlphaDEä¹Ÿèƒ½æ˜¾è‘—è¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚è¿›ä¸€æ­¥çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒAlphaDEæ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–å‡èšavGFPçš„è›‹ç™½è´¨åºåˆ—ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaDEæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è›‹ç™½è´¨åºåˆ—è¿›åŒ–ã€‚</li>
<li>AlphaDEé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œæ¿€æ´»ç›®æ ‡è›‹ç™½è´¨ç±»åˆ«çš„è¿›åŒ–å¯èƒ½æ€§ã€‚</li>
<li>AlphaDEå¼•å…¥æµ‹è¯•æ—¶é—´æ¨æ–­ï¼ŒåŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢æœ‰æ•ˆè¿›åŒ–è›‹ç™½è´¨ã€‚</li>
<li>AlphaDEåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå³ä½¿å°‘é‡æ ·æœ¬ä¹Ÿèƒ½è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚</li>
<li>AlphaDEæ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–å‡èšè›‹ç™½è´¨åºåˆ—ç©ºé—´ï¼Œä»¥avGFPä¸ºä¾‹è¿›è¡Œäº†å±•ç¤ºã€‚</li>
<li>å½“å‰å®šå‘è¿›åŒ–ç®—æ³•ä¸»è¦å…³æ³¨å¯å‘å¼æœç´¢ç­–ç•¥ï¼Œè€ŒAlphaDEåˆ™å¼ºè°ƒäº†è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2735d5745a0aea29be42f971908cc832" align="middle">
<img src="https://picx.zhimg.com/v2-2bf516209c67db72679586b340828e11" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Commonality-in-Few-Few-Shot-Multimodal-Anomaly-Detection-via-Hypergraph-Enhanced-Memory"><a href="#Commonality-in-Few-Few-Shot-Multimodal-Anomaly-Detection-via-Hypergraph-Enhanced-Memory" class="headerlink" title="Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory"></a>Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory</h2><p><strong>Authors:Yuxuan Lin, Hanjing Yan, Xuan Tong, Yang Chang, Huanzhen Wang, Ziheng Zhou, Shuyong Gao, Yan Wang, Wenqiang Zhang</strong></p>
<p>Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Sunny5250/CIF">https://github.com/Sunny5250/CIF</a>.</p>
<blockquote>
<p>å°‘æ ·æœ¬å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä½†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ä»»åŠ¡ï¼Œå®ƒå…·å¤‡å¿«é€Ÿé€‚åº”å¤æ‚å·¥ä¸šåœºæ™¯çš„èƒ½åŠ›ã€‚åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè®­ç»ƒæ ·æœ¬çš„ä¸è¶³å¾€å¾€æ— æ³•è¦†ç›–æµ‹è¯•æ ·æœ¬ä¸­å­˜åœ¨çš„å„ç§æ¨¡å¼ã€‚é€šè¿‡ä»å°‘é‡è®­ç»ƒæ ·æœ¬ä¸­æå–ç»“æ„å…±æ€§ï¼Œå¯ä»¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»“æ„å…±æ€§ã€åä¸ºCIFï¼ˆå°‘æ ·æœ¬å…±æ€§ï¼‰çš„æ–°å‹å°‘æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚ä¸ºäº†æå–ç±»å†…ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨è¶…å›¾æ¥å»ºæ¨¡é«˜é˜¶å…³è”ï¼Œä»¥æ•æ‰è®­ç»ƒæ ·æœ¬ä¸­çš„ç»“æ„å…±æ€§ï¼Œå¹¶ä½¿ç”¨è®°å¿†åº“æ¥å­˜å‚¨æ­¤ç±»ç±»å†…ç»“æ„å…ˆéªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé’ˆå¯¹å•è¯­ä¹‰å·¥ä¸šå›¾åƒçš„è¯­ä¹‰æ„ŸçŸ¥è¶…å›¾æ„å»ºæ¨¡å—ï¼Œä»ä¸­æå–å…¬å…±ç»“æ„æ¥å¼•å¯¼è®°å¿†åº“çš„æ„å»ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨æ— éœ€è®­ç»ƒçš„è¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ¥æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ï¼Œç¼©å°æµ‹è¯•ç‰¹å¾ä¸è®°å¿†åº“ä¸­ç‰¹å¾ä¹‹é—´çš„åˆ†å¸ƒå·®è·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è¶…è¾¹å¼•å¯¼çš„è®°å¿†æœç´¢æ¨¡å—ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯æ¥è¾…åŠ©è®°å¿†æœç´¢è¿‡ç¨‹å¹¶é™ä½è¯¯æŠ¥ç‡ã€‚åœ¨MVTec 3D-ADæ•°æ®é›†å’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sunny5250/CIF%E5%A4%96%E9%87%8F%E8%8F%BF%E6%8A%BD%E3%80%82">https://github.com/Sunny5250/CIFå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05966v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç»“æ„å…±æ€§ï¼ˆCIFï¼‰çš„å°‘æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¶…å›¾æ•æ‰è®­ç»ƒæ ·æœ¬çš„ç»“æ„å…±æ€§ï¼Œå¹¶ç”¨å†…å­˜åº“å­˜å‚¨æ­¤ç±»ç»“æ„å…ˆéªŒã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨æ— è®­ç»ƒè¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶å€ŸåŠ©è¶…è¾¹å¼•å¯¼å†…å­˜æœç´¢æ¨¡å—æ¥ç¼©å°åˆ†å¸ƒå·®è·ï¼Œé™ä½è¯¯æŠ¥ç‡ã€‚åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºç»“æ„å…±æ€§çš„å°‘æ ·æœ¬æ— ç›‘ç£å¤šæ¨¡æ€å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨è¶…å›¾å»ºæ¨¡è®­ç»ƒæ ·æœ¬çš„ç»“æ„å…±æ€§å¹¶å­˜å‚¨ç»“æ„å…ˆéªŒã€‚</li>
<li>é€šè¿‡æ— è®­ç»ƒè¶…å›¾æ¶ˆæ¯ä¼ é€’æ¨¡å—æ›´æ–°æµ‹è¯•æ ·æœ¬çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨è¶…è¾¹å¼•å¯¼å†…å­˜æœç´¢æ¨¡å—ç¼©å°åˆ†å¸ƒå·®è·å¹¶é™ä½è¯¯æŠ¥ç‡ã€‚</li>
<li>åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æå–ç»“æ„å…±æ€§ï¼ŒæˆåŠŸè§£å†³äº†å°‘æ ·æœ¬ç¯å¢ƒä¸‹è®­ç»ƒæ ·æœ¬æ— æ³•è¦†ç›–æµ‹è¯•æ ·æœ¬å¤šæ ·æ€§çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-154d07c8ae31bfb40a1aa54c9119ba25" align="middle">
<img src="https://picx.zhimg.com/v2-f8e2ff6ae20b726931e6dfbec906173a" align="middle">
<img src="https://picx.zhimg.com/v2-bbccd0b9ff813f081404c693c48a54c3" align="middle">
<img src="https://picx.zhimg.com/v2-43973f1e50fc573e486e53c5e9073f9d" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Model-Merging-Improves-Zero-Shot-Generalization-in-Bioacoustic-Foundation-Models"><a href="#Model-Merging-Improves-Zero-Shot-Generalization-in-Bioacoustic-Foundation-Models" class="headerlink" title="Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models"></a>Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models</h2><p><strong>Authors:Davide Marincione, Donato Crisostomi, Roberto Dessi, Emanuele RodolÃ , Emanuele Rossi</strong></p>
<p>Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.</p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨è·¨ç‰©ç§å’Œä»»åŠ¡ä¸­è¿›è¡Œæ¨å¹¿ï¼Œæ˜¯ç”Ÿç‰©å£°å­¦é¢†åŸŸä¸€ä¸ªå……æ»¡å¸Œæœ›çš„æ–°å‰æ²¿ï¼ŒNatureLMæ˜¯æœ€çªå‡ºçš„ä¾‹å­ä¹‹ä¸€ã€‚è™½ç„¶å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒåœ¨ç”Ÿç‰©å£°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä¹Ÿäº§ç”Ÿäº†æŒ‡ä»¤éµå¾ªçµæ´»æ€§çš„æƒè¡¡ã€‚ä¾‹å¦‚ï¼Œå½“è¢«æç¤ºä½¿ç”¨é€šç”¨åæˆ–å­¦åæ—¶ï¼ŒNatureLMå¯ä»¥å•ç‹¬å®ç°é«˜å‡†ç¡®æ€§ï¼Œä½†å½“ä¸¤è€…éƒ½åœ¨ä¸€ä¸ªæç¤ºä¸­è¯·æ±‚æ—¶ï¼Œå…¶å‡†ç¡®æ€§ä¼šæ˜¾è‘—é™ä½ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨ç®€å•çš„æ¨¡å‹åˆå¹¶ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç­–ç•¥å°†NatureLMä¸å…¶åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ’å€¼ï¼Œä»¥æœ€å°çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†æŸå¤±æ¢å¤æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆå¹¶æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†è¶…è¿‡200%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨æœªè§ç‰©ç§çš„å°é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä¸­åˆ›é€ äº†æ–°çš„æŠ€æœ¯çŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05171v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªç„¶LMï¼ˆNatureLMï¼‰æ˜¯ç”Ÿç‰©å£°å­¦é¢†åŸŸé€šç”¨æ¨¡å‹çš„ä¸€ä¸ªçªå‡ºä¾‹å­ï¼Œå®ƒåœ¨ç‰¹å®šé¢†åŸŸå¾®è°ƒåèƒ½åœ¨ç”Ÿç‰©å£°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨éµå¾ªæŒ‡ä»¤çš„çµæ´»æ€§æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚å½“è¢«è¦æ±‚åŒæ—¶æä¾›å¸¸è§åç§°å’Œç§‘å­¦åç§°æ—¶ï¼Œå…¶å‡†ç¡®æ€§ä¼šæ˜¾è‘—ä¸‹é™ã€‚é€šè¿‡é‡‡ç”¨ç®€å•çš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œå³å°†è‡ªç„¶LMä¸å…¶åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œæ’å€¼ï¼Œæˆ‘ä»¬åœ¨ä¿æŒé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„åŒæ—¶æ¢å¤äº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚åˆå¹¶åçš„æ¨¡å‹å±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§ç‰©ç§çš„å°é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡200%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½è®°å½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶LMæ˜¯ç”Ÿç‰©å£°å­¦é¢†åŸŸçš„ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒä½¿å…¶èƒ½å¤Ÿåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>è‡ªç„¶LMåœ¨åŒæ—¶å¤„ç†å¸¸è§åç§°å’Œç§‘å­¦åç§°æ—¶å­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ç®€å•çš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼Œå¯ä»¥æ¢å¤æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>åˆå¹¶åçš„æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆå¹¶åçš„æ¨¡å‹åœ¨æœªè§ç‰©ç§çš„å°é—­é›†é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>è‡ªç„¶LMçš„æ”¹è¿›ä¸ºç”Ÿç‰©å£°å­¦é¢†åŸŸå¸¦æ¥äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½è®°å½•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab6b1426902eca3b2a36eecda270db4e" align="middle">
<img src="https://picx.zhimg.com/v2-e391a157a032862ef0fc78067f0cf373" align="middle">
<img src="https://picx.zhimg.com/v2-3ff28dde9e3de38198ef66237cc3d4b1" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-709b3f305b195dec59a7579194a53d37" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  US-X Complete A Multi-Modal Approach to Anatomical 3D Shape Recovery
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91d5d151243c2ca383ce8af602fe6102" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  A Viable Paradigm of Software Automation Iterative End-to-End Automated Software Development
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
