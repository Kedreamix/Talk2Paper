<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  Navigating Quantum Missteps in Agent-Based Modeling A Schelling Model Case Study">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-89605be3fb3f9bc7cc921fd690ca5e8f')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-21-æ›´æ–°"><a href="#2025-11-21-æ›´æ–°" class="headerlink" title="2025-11-21 æ›´æ–°"></a>2025-11-21 æ›´æ–°</h1><h2 id="Navigating-Quantum-Missteps-in-Agent-Based-Modeling-A-Schelling-Model-Case-Study"><a href="#Navigating-Quantum-Missteps-in-Agent-Based-Modeling-A-Schelling-Model-Case-Study" class="headerlink" title="Navigating Quantum Missteps in Agent-Based Modeling: A Schelling Model Case Study"></a>Navigating Quantum Missteps in Agent-Based Modeling: A Schelling Model Case Study</h2><p><strong>Authors:C. Nico Barati, Arie Croitoru, Ross Gore, Michael Jarret, William Kennedy, Andrew Maciejunes, Maxim A. Malikov, Samuel S. Mendelson</strong></p>
<p>Quantum computing promises transformative advances, but remains constrained by recurring misconceptions and methodological pitfalls. This paper demonstrates a fundamental incompatibility between traditional agent-based modeling (ABM) implementations and quantum optimization frameworks like Quadratic Unconstrained Binary Optimization (QUBO). Using Schellingâ€™s segregation model as a case study, we show that the standard practice of directly translating ABM state observations into QUBO formulations not only fails to deliver quantum advantage, but actively undermines computational efficiency. The fundamental issue is architectural. Traditional ABM implementations entail observing the state of the system at each iteration, systematically destroying the quantum superposition required for computational advantage. Through analysis of Schellingâ€™s segregation dynamics on lollipop networks, we demonstrate how abandoning the QUBO reduction paradigm and instead reconceptualizing the research question, from â€œsimulate agent dynamics iteratively until convergenceâ€ to â€œcompute minimum of agent moves required for global satisfactionâ€, enables a faster classical solution. This structural reconceptualization yields an algorithm that exploits network symmetries obscured in traditional ABM simulations and QUBO formulations. It establishes a new lower bound which quantum approaches must outperform to achieve advantage. Our work emphasizes that progress in quantum agent-based modeling does not require forcing classical ABM implementations into quantum frameworks. Instead, it should focus on clarifying when quantum advantage is structurally possible, developing best-in-class classical baselines through problem analysis, and fundamentally reformulating research questions rather than preserving classical iterative state change observation paradigms.</p>
<blockquote>
<p>é‡å­è®¡ç®—è™½ç„¶å¸¦æ¥äº†çªç ´æ€§çš„æ½œåŠ›ï¼Œä½†å…¶å‘å±•ä»ç„¶å—åˆ°ä¸æ–­å‡ºç°çš„è¯¯è§£å’Œæ–¹æ³•ä¸Šçš„é™·é˜±çš„åˆ¶çº¦ã€‚æœ¬æ–‡é€šè¿‡èˆæ—çš„éš”ç¦»æ¨¡å‹è¿™ä¸€æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†ä¼ ç»ŸåŸºäºä¸»ä½“çš„å»ºæ¨¡ï¼ˆABMï¼‰å®ç°ä¸è¯¸å¦‚äºŒæ¬¡æ— çº¦æŸäºŒè¿›åˆ¶ä¼˜åŒ–ï¼ˆQUBOï¼‰çš„é‡å­ä¼˜åŒ–æ¡†æ¶ä¹‹é—´å­˜åœ¨æ ¹æœ¬çš„ä¸å…¼å®¹æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå°†ABMçŠ¶æ€è§‚å¯Ÿç›´æ¥è½¬æ¢ä¸ºQUBOå…¬å¼æ ‡å‡†çš„åšæ³•ä¸ä»…æ— æ³•å®ç°é‡å­ä¼˜åŠ¿ï¼Œè€Œä¸”ä¼šç ´åè®¡ç®—æ•ˆç‡ã€‚æ ¹æœ¬é—®é¢˜åœ¨äºæ¶æ„ã€‚ä¼ ç»Ÿçš„ABMå®ç°éœ€è¦åœ¨æ¯æ¬¡è¿­ä»£æ—¶è§‚å¯Ÿç³»ç»Ÿçš„çŠ¶æ€ï¼Œè¿™ç ´åäº†è®¡ç®—ä¼˜åŠ¿æ‰€éœ€çš„é‡å­å åŠ æ€§ã€‚é€šè¿‡å¯¹èˆæ—éš”ç¦»åŠ¨åŠ›å­¦åœ¨æ£’æ£’ç³–ç½‘ç»œä¸Šçš„åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ”¾å¼ƒQUBOç¼©å‡èŒƒå¼ï¼Œè½¬è€Œé‡æ–°æ€è€ƒç ”ç©¶é—®é¢˜çš„æ–¹å¼ï¼Œå³ä»â€œè¿­ä»£æ¨¡æ‹Ÿä»£ç†åŠ¨æ€ç›´è‡³æ”¶æ•›â€è½¬å˜ä¸ºâ€œè®¡ç®—å®ç°å…¨å±€æ»¡æ„åº¦æ‰€éœ€çš„æœ€å°ä»£ç†ç§»åŠ¨æ¬¡æ•°â€ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿«çš„ç»å…¸è§£å†³æ–¹æ¡ˆã€‚è¿™ç§ç»“æ„æ€§çš„é‡æ–°æ€è€ƒæ­ç¤ºäº†ä¸€ç§ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨ä¼ ç»ŸABMæ¨¡æ‹Ÿå’ŒQUBOå…¬å¼ä¸­éšè—çš„ç½‘ç»œå®‰å…¨å¯¹ç§°æ€§ã€‚è¿™ä¸ºé‡å­æ–¹æ³•å»ºç«‹äº†æ–°çš„ä¸‹é™ï¼Œå¿…é¡»è¶…è¶Šæ­¤ä¸‹é™æ‰èƒ½å®ç°ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒï¼Œåœ¨é‡å­åŸºäºä¸»ä½“çš„å»ºæ¨¡æ–¹é¢çš„è¿›å±•å¹¶ä¸éœ€è¦å¼ºè¿«ä¼ ç»Ÿçš„ABMå®ç°è¿›å…¥é‡å­æ¡†æ¶ã€‚ç›¸åï¼Œå®ƒåº”ä¸“æ³¨äºæ¾„æ¸…ä½•æ—¶ç»“æ„ä¸Šå¯èƒ½å®ç°é‡å­ä¼˜åŠ¿ï¼Œé€šè¿‡å¯¹é—®é¢˜çš„åˆ†ææ¥å¼€å‘æœ€ä½³çš„ç»å…¸åŸºå‡†çº¿ï¼Œå¹¶ä»æ ¹æœ¬ä¸Šé‡æ–°æ„å»ºç ”ç©¶é—®é¢˜ï¼Œè€Œä¸æ˜¯ç»´æŒä¼ ç»Ÿçš„è¿­ä»£çŠ¶æ€å˜æ›´è§‚å¯ŸèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15642v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¼ ç»ŸåŸºäºä»£ç†çš„å»ºæ¨¡ï¼ˆABMï¼‰ä¸é‡å­ä¼˜åŒ–æ¡†æ¶ï¼ˆå¦‚äºŒæ¬¡æ— çº¦æŸäºŒè¿›åˆ¶ä¼˜åŒ–ï¼ˆQUBOï¼‰ï¼‰ä¹‹é—´çš„æ ¹æœ¬æ€§ä¸å…¼å®¹ã€‚é€šè¿‡è°¢æ—éš”ç¦»æ¨¡å‹è¿™ä¸€æ¡ˆä¾‹ç ”ç©¶ï¼Œå‘ç°ç›´æ¥å°†ABMçŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºQUBOå…¬å¼ä¸ä»…æ— æ³•å®ç°é‡å­ä¼˜åŠ¿ï¼Œåè€Œä¼šé™ä½è®¡ç®—æ•ˆç‡ã€‚æœ¬æ–‡æŒ‡å‡ºé—®é¢˜å…³é”®åœ¨äºæ¶æ„ï¼Œä¼ ç»Ÿçš„ABMå®æ–½æ–¹æ³•é€šè¿‡ç³»ç»Ÿè§‚å¯ŸçŠ¶æ€ç ´åäº†é‡å­å åŠ æ‰€éœ€çš„æ¡ä»¶ï¼Œæ— æ³•å®ç°è®¡ç®—ä¼˜åŠ¿ã€‚é€šè¿‡é‡æ–°æ¦‚å¿µåŒ–ç ”ç©¶é—®é¢˜ï¼Œä»æ¨¡æ‹Ÿä»£ç†åŠ¨æ€ç›´è‡³æ”¶æ•›è½¬å‘è®¡ç®—è¾¾åˆ°å…¨å±€æ»¡æ„åº¦æ‰€éœ€çš„æœ€å°ä»£ç†ç§»åŠ¨æ¬¡æ•°ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°å®ç°ç»å…¸è§£å†³æ–¹æ¡ˆã€‚è¿™å¯ç¤ºæˆ‘ä»¬è¿›å±•åœ¨äºå¯¹é‡å­ä»£ç†å»ºæ¨¡çš„ç†è§£å’Œåº”ç”¨ï¼Œè€Œä¸æ˜¯å°†ç»å…¸ABMå¼ºåˆ¶èå…¥é‡å­æ¡†æ¶ã€‚æˆ‘ä»¬åº”èšç„¦äºæ˜ç¡®é‡å­ä¼˜åŠ¿çš„ç»“æ„å¯èƒ½æ€§ã€é€šè¿‡é—®é¢˜åˆ†æåˆ¶å®šæœ€ä½³ç»å…¸åŸºå‡†çº¿ï¼Œå¹¶ä»æ ¹æœ¬ä¸Šé‡æ–°åˆ¶å®šé—®é¢˜è€Œéä¿ç•™ç»å…¸è¿­ä»£çŠ¶æ€å˜åŒ–è§‚å¯Ÿæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸåŸºäºä»£ç†çš„å»ºæ¨¡ï¼ˆABMï¼‰ä¸é‡å­ä¼˜åŒ–æ¡†æ¶ï¼ˆå¦‚QUBOï¼‰å­˜åœ¨æ ¹æœ¬æ€§ä¸å…¼å®¹é—®é¢˜ã€‚</li>
<li>ç›´æ¥å°†ABMçŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºQUBOå…¬å¼æ— æ³•å®ç°é‡å­ä¼˜åŠ¿ï¼Œå¹¶é™ä½è®¡ç®—æ•ˆç‡ã€‚</li>
<li>é—®é¢˜å…³é”®åœ¨äºä¼ ç»ŸABMå®æ–½æ–¹æ³•é€šè¿‡ç³»ç»Ÿè§‚å¯ŸçŠ¶æ€ç ´åäº†é‡å­å åŠ æ‰€éœ€çš„æ¡ä»¶ã€‚</li>
<li>é€šè¿‡é‡æ–°æ¦‚å¿µåŒ–ç ”ç©¶é—®é¢˜ï¼Œèƒ½æ›´å¿«åœ°å®ç°ç»å…¸è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é‡æ–°æ¦‚å¿µåŒ–ç ”ç©¶é—®é¢˜åŒ…æ‹¬ä»æ¨¡æ‹Ÿä»£ç†åŠ¨æ€ç›´è‡³æ”¶æ•›è½¬å‘è®¡ç®—è¾¾åˆ°å…¨å±€æ»¡æ„åº¦æ‰€éœ€çš„æœ€å°ä»£ç†ç§»åŠ¨æ¬¡æ•°ã€‚</li>
<li>å¯¹é‡å­ä»£ç†å»ºæ¨¡çš„ç†è§£å’Œåº”ç”¨æ˜¯å…³é”®è¿›å±•ç‚¹ï¼Œè€Œéå°†ç»å…¸ABMå¼ºåˆ¶èå…¥é‡å­æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3adaecef633058100b16fdffd4653800" align="middle">
<img src="https://picx.zhimg.com/v2-bb6009109f8ed9a2b1b2d927501f99c8" align="middle">
<img src="https://picx.zhimg.com/v2-fd446e4912b643e87cf9305bf17d6167" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning"><a href="#AVATAAR-Agentic-Video-Answering-via-Temporal-Adaptive-Alignment-and-Reasoning" class="headerlink" title="AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning"></a>AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning</h2><p><strong>Authors:Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar</strong></p>
<p>With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAARâ€™s effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.</p>
<blockquote>
<p>éšç€è§†é¢‘å†…å®¹çš„æ—¥ç›Šæ™®åŠï¼Œå¯¹äºé•¿è§†é¢‘çš„æœ‰æ•ˆç†è§£å’Œé—®ç­”å·²ç»æˆä¸ºè®¸å¤šåº”ç”¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ€§èƒ½å·²ç»æå‡ï¼Œä½†å®ƒä»¬ä»ç„¶ç»å¸¸é¢ä¸´å¾®å¦™æŸ¥è¯¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŸ¥è¯¢éœ€è¦å…¨é¢ç†è§£å’Œè¯¦ç»†åˆ†æã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†AVATAARï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä»¥åŠé¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ã€‚AVATAARåˆ›å»ºäº†ä¸€ä¸ªæŒä¹…çš„å…¨å±€æ‘˜è¦ï¼Œå¹¶åœ¨åæ€æ¨¡å—å’Œé¢„æ£€ç´¢æ€è€ƒä»£ç†ä¹‹é—´å»ºç«‹äº†åé¦ˆå¾ªç¯ï¼Œå…è®¸ç³»ç»Ÿæ ¹æ®éƒ¨åˆ†ç­”æ¡ˆä¼˜åŒ–å…¶æ£€ç´¢ç­–ç•¥ï¼Œå¹¶æ¨¡æ‹Ÿäººç±»ç±»ä¼¼çš„è¿­ä»£æ¨ç†ã€‚åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸å¯¹äºåŸºçº¿è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ—¶åºæ¨ç†ã€æŠ€æœ¯æŸ¥è¯¢ã€ä¸»é¢˜ç›¸å…³é—®é¢˜å’Œå™äº‹ç†è§£æ–¹é¢åˆ†åˆ«å®ç°äº†+5.6%ã€+5%ã€+8%å’Œ+8.2%çš„ç›¸å¯¹å¢é•¿ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯¹æ•´ä½“æ€§èƒ½åšå‡ºäº†ç§¯æè´¡çŒ®ï¼Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†AVATAARåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼ŒAVATAARä¸ºé•¿è§†é¢‘é—®ç­”ï¼ˆQAï¼‰æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œèåˆäº†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15578v1">PDF</a> Accepted in the 5th IEEE Big Data Workshop on Multimodal AI (MMAI 2025), Dec 8-11, Macau, China, 2025 (Preprint Copy)</p>
<p><strong>Summary</strong></p>
<p>AVATAARæ¡†æ¶ç»“åˆå…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡é¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ï¼Œæé«˜é•¿è§†é¢‘ç†è§£é—®ç­”æ€§èƒ½ã€‚AVATAARåœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå„æ¨¡å—å¯¹æ•´ä½“æ€§èƒ½æœ‰ç§¯æå½±å“ï¼Œåé¦ˆå¾ªç¯å¯¹äºé€‚åº”æ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVATAARæ¡†æ¶æœ‰æ•ˆæé«˜è§†é¢‘ç†è§£é—®ç­”æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶ç»“åˆå…¨å±€å’Œå±€éƒ¨è§†é¢‘ä¸Šä¸‹æ–‡ã€‚</li>
<li>å¼•å…¥é¢„æ£€ç´¢æ€è€ƒä»£ç†å’Œåæ€æ¨¡å—ã€‚</li>
<li>åœ¨CinePileåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAVATAARç›¸å¯¹åŸºçº¿æœ‰+5.6%çš„æ—¶é—´æ¨ç†ã€+5%çš„æŠ€æœ¯æŸ¥è¯¢ã€+8%çš„ä¸»é¢˜é—®é¢˜å’Œ+8.2%çš„å™äº‹ç†è§£æ”¹è¿›ã€‚</li>
<li>åé¦ˆå¾ªç¯å¯¹é€‚åº”æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å„æ¨¡å—å¯¹æ•´ä½“æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-311a9875497052a1bd50ff1e71fed034" align="middle">
<img src="https://picx.zhimg.com/v2-b59d9d92b9970764f010808d113fb44c" align="middle">
<img src="https://picx.zhimg.com/v2-69ad59825ca3800326d02e86106aa17c" align="middle">
<img src="https://picx.zhimg.com/v2-bb6665078586548d26d14e0ecef97655" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Computer-Use-Agents-as-Judges-for-Generative-User-Interface"><a href="#Computer-Use-Agents-as-Judges-for-Generative-User-Interface" class="headerlink" title="Computer-Use Agents as Judges for Generative User Interface"></a>Computer-Use Agents as Judges for Generative User Interface</h2><p><strong>Authors:Kevin Qinghong Lin, Siyuan Hu, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Zheng Shou</strong></p>
<p>Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humansâ€“prioritizing aesthetics and usabilityâ€“forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/showlab/AUI">https://github.com/showlab/AUI</a>.</p>
<blockquote>
<p>ç”¨æˆ·ç•Œé¢ä»£ç†ï¼ˆCUAï¼‰æ­£è¶Šæ¥è¶Šèƒ½å¤Ÿé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªä¸»æ“ä½œæ•°å­—ç¯å¢ƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°GUIä»ç„¶ä¸»è¦é¢å‘äººç±»è®¾è®¡ï¼Œä¼˜å…ˆè€ƒè™‘ç¾è§‚æ€§å’Œæ˜“ç”¨æ€§ï¼Œè¿™ä½¿å¾—ä»£ç†å¿…é¡»é‡‡ç”¨é¢å‘äººç±»çš„ã€å¯¹äºé«˜æ•ˆä»»åŠ¡æ‰§è¡Œä¸å¿…è¦çš„è¡Œä¸ºã€‚ä¸æ­¤åŒæ—¶ï¼Œé¢å‘ç¼–ç çš„è¯­è¨€æ¨¡å‹ï¼ˆCoderï¼‰çš„å¿«é€Ÿå‘å±•å·²ç»æ”¹å˜äº†è‡ªåŠ¨GUIè®¾è®¡ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šä½œä¸ºæ³•å®˜çš„CUAèƒ½å¦ååŠ©Coderè¿›è¡Œè‡ªåŠ¨GUIè®¾è®¡ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AUI-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨è¶Š52ä¸ªåº”ç”¨ç¨‹åºçš„è‡ªåŠ¨GUIå¼€å‘åŸºå‡†æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬åˆ©ç”¨è¯­è¨€æ¨¡å‹åˆæˆäº†1560ä¸ªæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ä»»åŠ¡å¯é æ€§ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªéªŒè¯å™¨ï¼Œè¯¥ç¨‹åºä¼šæ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ˜¯å¦èƒ½åœ¨å…¶ç¯å¢ƒä¸­æ‰§è¡Œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Coderå’ŒCUAçš„åˆä½œæ¡†æ¶ï¼šCoderä½œä¸ºè®¾è®¡å¸ˆï¼Œç”Ÿæˆå’Œä¿®æ”¹ç½‘ç«™ï¼Œè€ŒCUAä½œä¸ºæ³•å®˜ï¼Œè¯„ä¼°åŠŸèƒ½å¹¶æ”¹è¿›è®¾è®¡ã€‚æˆåŠŸçš„è¡¡é‡æ ‡å‡†ä¸æ˜¯è§†è§‰æ•ˆæœï¼Œè€Œæ˜¯ä»»åŠ¡çš„è§£å†³èƒ½åŠ›å’ŒCUAçš„å¯¼èˆªæˆåŠŸç‡ã€‚ä¸ºäº†å°†CUAçš„åé¦ˆè½¬åŒ–ä¸ºå¯ç”¨çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬è®¾è®¡äº†CUAä»ªè¡¨æ¿ï¼Œå®ƒå°†å¤šæ­¥å¯¼èˆªå†å²å‹ç¼©æˆç®€æ´çš„è§†è§‰æ‘˜è¦ï¼Œä¸ºè¿­ä»£è®¾è®¡æä¾›å¯è§£é‡Šçš„æŒ‡å¯¼ã€‚é€šè¿‡å°†ä»£ç†å®šä½ä¸ºè®¾è®¡å¸ˆå’Œæ³•å®˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°†ç•Œé¢è®¾è®¡è½¬å‘ä»£ç†æœ¬èº«çš„æ•ˆç‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œè¿ˆå‡ºäº†ä»è¢«åŠ¨ä½¿ç”¨è½¬å‘æ•°å­—ç¯å¢ƒä¸­çš„ç§¯æå‚ä¸çš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/AUI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/showlab/AUIä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15567v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://showlab.github.io/AUI">https://showlab.github.io/AUI</a> Github: <a target="_blank" rel="noopener" href="https://github.com/showlab/AUI">https://github.com/showlab/AUI</a></p>
<p><strong>Summary</strong><br>     è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰èƒ½è‡ªä¸»æ“ä½œæ•°å­—ç¯å¢ƒï¼Œä½†ç›®å‰å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è®¾è®¡å¤§å¤šä»¥äººç±»ä¸ºä¸­å¿ƒï¼Œè¿™å¯¼è‡´ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶é¢ä¸´ä¸å¿…è¦çš„éšœç¢ã€‚åŒæ—¶ï¼Œç¼–ç å¯¼å‘çš„è¯­è¨€æ¨¡å‹ï¼ˆCoderï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è‡ªåŠ¨GUIè®¾è®¡çš„å‘å±•ã€‚ç ”ç©¶å¼•å…¥AUI-GymåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå¯¹è‡ªåŠ¨GUIå¼€å‘è¿›è¡Œè·¨åŸŸè¯„ä¼°ï¼Œå¹¶åˆ©ç”¨è¯­è¨€æ¨¡å‹åˆæˆæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæ„å»ºCoderä¸CUAååŒå·¥ä½œçš„æ¡†æ¶ï¼Œå…¶ä¸­Coderè´Ÿè´£è®¾è®¡å’Œä¿®è®¢ç½‘ç«™ï¼ŒCUAåˆ™è´Ÿè´£è¯„ä¼°å’Œç²¾ç‚¼è®¾è®¡ã€‚æ­¤æ¡†æ¶é‡è§†ä»»åŠ¡è§£å†³å’ŒCUAå¯¼èˆªæˆåŠŸç‡è€Œéè§†è§‰å¤–è§‚ã€‚æ­¤å¤–ï¼Œè®¾è®¡CUAä»ªè¡¨æ¿ä»¥æä¾›ç®€æ´çš„è§†è§‰æ‘˜è¦ï¼ŒæŒ‡å¯¼è¿­ä»£è®¾è®¡ã€‚æ­¤æ¡†æ¶æ¨åŠ¨ç•Œé¢è®¾è®¡å‘ä»£ç†åŸç”Ÿæ•ˆç‡å’Œå¯é æ€§è½¬å˜ï¼Œä½¿ä»£ç†ä»è¢«åŠ¨ä½¿ç”¨è€…è½¬å˜ä¸ºæ•°å­—ç¯å¢ƒçš„ç§¯æå‚ä¸è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰èƒ½å¤Ÿè‡ªä¸»æ“ä½œæ•°å­—ç¯å¢ƒï¼Œä½†å½“å‰GUIè®¾è®¡ä¸»è¦é¢å‘äººç±»ï¼Œé™åˆ¶äº†æ•ˆç‡ã€‚</li>
<li>è‡ªåŠ¨GUIè®¾è®¡çš„å‘å±•æ¨åŠ¨äº†CUAåœ¨å…¶ä¸­çš„ä½œç”¨å˜åŒ–ã€‚</li>
<li>AUI-GymåŸºå‡†æµ‹è¯•å¹³å°ç”¨äºè¯„ä¼°è‡ªåŠ¨GUIå¼€å‘çš„è·¨åŸŸè¡¨ç°ï¼Œåˆæˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ä»»åŠ¡ã€‚</li>
<li>æ„å»ºCoderä¸CUAååŒå·¥ä½œçš„æ¡†æ¶ï¼Œå…¶ä¸­Coderè´Ÿè´£è®¾è®¡ï¼ŒCUAè´Ÿè´£è¯„ä¼°ã€‚</li>
<li>è¯„ä¼°é‡ç‚¹ä»è§†è§‰å¤–è§‚è½¬å‘ä»»åŠ¡è§£å†³å’ŒCUAå¯¼èˆªæˆåŠŸç‡ã€‚</li>
<li>CUAä»ªè¡¨æ¿æä¾›ç®€æ´çš„è§†è§‰æ‘˜è¦ï¼ŒæŒ‡å¯¼è¿­ä»£è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-613b99dc84dabbffefc0028deb06da60" align="middle">
<img src="https://picx.zhimg.com/v2-02d63a08a294b5ec89c2d9fed1065013" align="middle">
<img src="https://picx.zhimg.com/v2-93c2e434930c476fdc1e34d566290ffb" align="middle">
<img src="https://picx.zhimg.com/v2-b254b27fd076ccb89e304458c419af97" align="middle">
<img src="https://picx.zhimg.com/v2-ab9f6c65d2dfc3c1ec11e996ac1e7dbd" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining"><a href="#Know-Your-Intent-An-Autonomous-Multi-Perspective-LLM-Agent-Framework-for-DeFi-User-Transaction-Intent-Mining" class="headerlink" title="Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining"></a>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining</h2><p><strong>Authors:Qianâ€™ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan</strong></p>
<p>As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-&#x2F;off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on&#x2F;off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.</p>
<blockquote>
<p>éšç€å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰çš„å‘å±•ï¼Œç†è§£DeFiäº¤æ˜“èƒŒåçš„ç”¨æˆ·æ„å›¾è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¤æ‚çš„æ™ºèƒ½åˆçº¦äº¤äº’ã€å¤šæ–¹é¢çš„é“¾ä¸Šé“¾ä¸‹å› ç´ ä»¥åŠä¸é€æ˜çš„hexæ—¥å¿—ï¼Œè¿™æå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ·±åº¦è¯­ä¹‰æ´å¯Ÿã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†äº¤æ˜“æ„å›¾æŒ–æ˜ï¼ˆTIMï¼‰æ¡†æ¶ã€‚TIMåˆ©ç”¨åŸºäºæ‰æ ¹ç†è®ºå’Œå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿæ„å»ºäº†DeFiæ„å›¾åˆ†ç±»æ³•ï¼Œä»¥ç¨³å¥åœ°æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚å…ƒçº§è§„åˆ’å™¨åŠ¨æ€åè°ƒé¢†åŸŸä¸“å®¶ï¼Œå°†å¤šä¸ªç‰¹å®šè§†è§’çš„æ„å›¾åˆ†æåˆ†è§£ä¸ºå¯è§£å†³çš„å­ä»»åŠ¡ã€‚é—®é¢˜æ±‚è§£å™¨ä½¿ç”¨å¤šæ¨¡æ€çš„é“¾ä¸Šé“¾ä¸‹æ•°æ®æ¥å¤„ç†ä»»åŠ¡ã€‚è®¤çŸ¥è¯„ä¼°å™¨åˆ™ç¼“è§£LLMçš„å¹»è§‰æ•ˆåº”å¹¶ç¡®ä¿å¯éªŒè¯æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTIMåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ„å›¾æ¨æ–­ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºæä¾›æ›´å¯é çš„å…³äºDeFiç”¨æˆ·åŠ¨æœºçš„ç†è§£ï¼Œä¸ºå¤æ‚çš„åŒºå—é“¾æ´»åŠ¨æä¾›æƒ…å¢ƒæ„ŸçŸ¥çš„è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15456v1">PDF</a> Written in 2025 Q1</p>
<p><strong>Summary</strong><br>    DeFiäº¤æ˜“ç”¨æˆ·æ„å›¾ç†è§£è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºTransaction Intent Miningï¼ˆTIMï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ‰æ ¹ç†è®ºå’Œå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿæ¨æ–­ç”¨æˆ·æ„å›¾ï¼Œå¹¶é€šè¿‡å…ƒçº§è§„åˆ’å™¨åè°ƒä¸“å®¶å¤„ç†å¤šä»»åŠ¡ï¼Œå®ç°å¯é çš„ç”¨æˆ·æ„å›¾æ¨æ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeFiäº¤æ˜“ç”¨æˆ·æ„å›¾ç†è§£çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>TIMæ¡†æ¶åˆ©ç”¨DeFiæ„å›¾åˆ†ç±»å’Œå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿè¿›è¡Œç”¨æˆ·æ„å›¾æ¨æ–­ã€‚</li>
<li>å…ƒçº§è§„åˆ’å™¨åŠ¨æ€åè°ƒä¸“å®¶å¤„ç†å¤šä»»åŠ¡ã€‚</li>
<li>é—®é¢˜æ±‚è§£å™¨åˆ©ç”¨å¤šæ¨¡æ€é“¾ä¸Šé“¾ä¸‹æ•°æ®å®Œæˆä»»åŠ¡ã€‚</li>
<li>è®¤çŸ¥è¯„ä¼°å™¨å‡å°‘LLMå¹»æƒ³å¹¶ä¿éšœå¯éªŒè¯æ€§ã€‚</li>
<li>TIMæ¡†æ¶åœ¨å®éªŒä¸­æ˜¾è‘—ä¼˜äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€å•ä¸€LLMå’Œå•ä¸€AgentåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afcd004e8456e76e348cad4f60561050" align="middle">
<img src="https://picx.zhimg.com/v2-93aad0807c8425dba60e1828d46c7017" align="middle">
<img src="https://picx.zhimg.com/v2-fc19a33165095d1e8362ae1161f79fa7" align="middle">
<img src="https://picx.zhimg.com/v2-89605be3fb3f9bc7cc921fd690ca5e8f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DEPO-Dual-Efficiency-Preference-Optimization-for-LLM-Agents"><a href="#DEPO-Dual-Efficiency-Preference-Optimization-for-LLM-Agents" class="headerlink" title="DEPO: Dual-Efficiency Preference Optimization for LLM Agents"></a>DEPO: Dual-Efficiency Preference Optimization for LLM Agents</h2><p><strong>Authors:Sirui Chen, Mengshi Zhao, Lei Xu, Yuying Zhao, Beier Zhu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu</strong></p>
<p>Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at <a target="_blank" rel="noopener" href="https://opencausalab.github.io/DEPO">https://opencausalab.github.io/DEPO</a>.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æå¤§åœ°æé«˜äº†å®ƒä»¬ä½œä¸ºä»£ç†éƒ¨ç½²æ—¶çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ›´ä¸°å¯Œçš„æ¨ç†å¾€å¾€ä»¥æ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ä¸ºä»£ä»·ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­é˜»ç¢äº†äº¤äº’æ•ˆç‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå…³äºLLMä»£ç†æ•ˆç‡çš„ç³»ç»Ÿæ€§å®šä¹‰ä»ç„¶ç¼ºå¤±ï¼Œé˜»ç¢äº†æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡æ•ˆç‡çš„æ¦‚å¿µï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰æ­¥éª¤çº§æ•ˆç‡ï¼Œå³æœ€å°åŒ–æ¯æ­¥çš„ä»¤ç‰Œæ•°ï¼Œï¼ˆiiï¼‰è½¨è¿¹çº§æ•ˆç‡ï¼Œå³æœ€å°åŒ–å®Œæˆä»»åŠ¡çš„æ­¥éª¤æ•°ã€‚åŸºäºè¿™ä¸ªå®šä¹‰ï¼Œæˆ‘ä»¬æå‡ºäº†DEPOï¼Œä¸€ç§åŒé‡æ•ˆç‡åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶å¥–åŠ±ç®€æ´çš„å›åº”å’Œè¾ƒå°‘çš„è¡ŒåŠ¨æ­¥éª¤ã€‚åœ¨WebShopå’ŒBabyAIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOå¯ä»¥å°†ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘é«˜è¾¾60.9%ï¼Œæ­¥éª¤å‡å°‘é«˜è¾¾26.9%ï¼ŒåŒæ—¶æ€§èƒ½æé«˜é«˜è¾¾29.3%ã€‚DEPOè¿˜å¯ä»¥æ¨å¹¿åˆ°ä¸‰ç§ç¦»åŸŸæ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨25%çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ä»èƒ½ä¿æŒæ•ˆç‡æå‡ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://opencausalab.github.io/DEPO%E3%80%82">https://opencausalab.github.io/DEPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15392v1">PDF</a> Accepted to AAAI 2026</p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºä»£ç†éƒ¨ç½²æ—¶çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›å¾—åˆ°æå¤§æå‡ï¼Œä½†æ›´ä¸°å¯Œçš„æ¨ç†å¾€å¾€ä»¥æ›´é•¿çš„æ€è€ƒé“¾ï¼ˆCoTï¼‰ä¸ºä»£ä»·ï¼Œå½±å“ç°å®åœºæ™¯ä¸­çš„äº¤äº’æ•ˆç‡ã€‚é’ˆå¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºåŒæ•ˆç‡æ¦‚å¿µï¼ŒåŒ…æ‹¬æ­¥éª¤çº§æ•ˆç‡å’Œè½¨è¿¹çº§æ•ˆç‡ï¼Œå¹¶åŸºäºæ­¤å®šä¹‰æå‡ºäº†DEPOæ–¹æ³•ï¼Œä¸€ç§åŒæ•ˆç‡åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨ç®€æ´å›åº”å¹¶å‡å°‘è¡ŒåŠ¨æ­¥éª¤ã€‚å®éªŒæ˜¾ç¤ºï¼ŒDEPOåœ¨WebShopå’ŒBabyAIä»»åŠ¡ä¸Šåˆ†åˆ«å‡å°‘äº†é«˜è¾¾60.9%çš„ä»¤ç‰Œä½¿ç”¨é‡å’Œé«˜è¾¾26.9%çš„æ­¥éª¤ï¼ŒåŒæ—¶æ€§èƒ½æé«˜äº†é«˜è¾¾29.3%ã€‚DEPOè¿˜é€‚ç”¨äºä¸‰ç§è¶…å‡ºèŒƒå›´çš„æ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨25%çš„æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ä»èƒ½ä¿æŒæ•ˆç‡æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMä»£ç†çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›å¾—åˆ°å¢å¼ºï¼Œä½†æ¨ç†ä¸°å¯Œæ€§å¸¸ä¼´éšæ›´é•¿çš„æ€è€ƒé“¾ï¼Œå½±å“ç°å®äº¤äº’æ•ˆç‡ã€‚</li>
<li>æå‡ºåŒæ•ˆç‡æ¦‚å¿µï¼ŒåŒ…æ‹¬æ­¥éª¤çº§å’Œè½¨è¿¹çº§æ•ˆç‡ï¼Œä»¥è¡¡é‡LLMä»£ç†çš„æ•ˆç‡ã€‚</li>
<li>å¼•å…¥DEPOæ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–åŒæ•ˆç‡æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨WebShopå’ŒBabyAIä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œæ­¥éª¤æ•°é‡ã€‚</li>
<li>DEPOåœ¨ä¸‰ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¹¶åœ¨æœ‰é™æ•°æ®è®­ç»ƒä¸‹ä»èƒ½ä¿æŒæ•ˆç‡æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6ccfdb7ff7d51e0d800456d7fa94c4f" align="middle">
<img src="https://picx.zhimg.com/v2-3df3a556ac43bd24fa93679f85b8a4c4" align="middle">
<img src="https://picx.zhimg.com/v2-8dcb994de6d8b775829ea80481b4893e" align="middle">
<img src="https://picx.zhimg.com/v2-aa6f87c5c62c8f0f65f2f145baa49f45" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Octopus-Agentic-Multimodal-Reasoning-with-Six-Capability-Orchestration"><a href="#Octopus-Agentic-Multimodal-Reasoning-with-Six-Capability-Orchestration" class="headerlink" title="Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration"></a>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</h2><p><strong>Authors:Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Yingchao Li, Huacan Wang, Ronghao Chen</strong></p>
<p>Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.</p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ä¸æ¡†æ¶å­˜åœ¨åŸºæœ¬æ¶æ„ä¸Šçš„å±€é™æ€§ï¼šå¤§å¤šæ•°æ¨¡å‹ç¼ºä¹äººç±»è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼Œæ— è®ºæ˜¯åœ¨ç›´æ¥æ¨ç†ã€å·¥å…·é©±åŠ¨çš„è§†è§‰æ¢ç´¢ã€ç¨‹åºåŒ–çš„è§†è§‰æ“ä½œï¼Œè¿˜æ˜¯å†…åœ¨çš„è§†è§‰æƒ³è±¡åŠ›æ–¹é¢ã€‚å› æ­¤ï¼Œå®ƒä»¬å¾ˆéš¾é€‚åº”çœŸå®ä»»åŠ¡ä¸­åŠ¨æ€å˜åŒ–çš„èƒ½åŠ›è¦æ±‚ã€‚ä¸æ­¤åŒæ—¶ï¼Œäººç±»åœ¨è§£å†³æ­¤ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†ä¸€ç³»åˆ—äº’è¡¥çš„æ€è€ƒèƒ½åŠ›ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸åªæ¶µç›–è¿™äº›ç»´åº¦çš„ä¸€éƒ¨åˆ†ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Octopusï¼šå…·æœ‰å…­ç§èƒ½åŠ›ååŒçš„å¤šæ¨¡æ€è‡ªä¸»æ¨ç†æ–°å‹èŒƒå¼ã€‚æˆ‘ä»¬å®šä¹‰äº†å¤šæ¨¡æ€æ¨ç†æ‰€å¿…éœ€çš„å…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶ç›¸åº”åœ°æ„å»ºäº†ç»¼åˆè¯„ä¼°åŸºå‡†Octopus-Benchã€‚Octopusèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»æ¢ç´¢ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOctopusåœ¨Octopus-Benchä¸­çš„ç»å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¿™çªå‡ºäº†èƒ½åŠ›åè°ƒåœ¨å¤šæ¨¡æ€è‡ªä¸»æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å’Œæ¡†æ¶å­˜åœ¨åŸºæœ¬æ¶æ„ä¸Šçš„å±€é™ï¼Œç¼ºä¹äººç±»è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„çš„èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„èƒ½åŠ›è¦æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºOctopusï¼šå…·æœ‰å…­ç§èƒ½åŠ›ååŒçš„å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†ï¼Œå®šä¹‰å…­ç§æ ¸å¿ƒèƒ½åŠ›å¹¶æ„å»ºç›¸åº”çš„è¯„ä¼°åŸºå‡†Octopus-Benchã€‚Octopusèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å’ŒåŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOctopusåœ¨Octopus-Benchä¸­çš„å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­˜åœ¨æ¶æ„ä¸Šçš„å±€é™æ€§ï¼Œæ— æ³•åƒäººç±»ä¸€æ ·è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>äººç±»åœ¨è§£å†³ç°å®ä»»åŠ¡æ—¶å±•ç°å‡ºå¤šç§äº’è¡¥çš„æ€è€ƒèƒ½åŠ›ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸åªè¦†ç›–å…¶ä¸­ä¸€éƒ¨åˆ†ã€‚</li>
<li>Octopusæå‡ºä¸€ç§æ–°çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†èŒƒå¼ï¼Œå…·å¤‡å…­ç§æ ¸å¿ƒèƒ½åŠ›ååŒå·¥ä½œã€‚</li>
<li>Octopusèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦çš„èƒ½åŠ›ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚</li>
<li>Octopusåœ¨Octopus-Benchè¯„ä¼°åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å®éªŒä¸­å¼ºè°ƒäº†èƒ½åŠ›ååŒåœ¨å¤šæ¨¡æ€æ™ºèƒ½æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c98136c7236aca77c8cf05411d86cc77" align="middle">
<img src="https://picx.zhimg.com/v2-e4e763f0493a6f2a45db0cf3bc4be222" align="middle">
<img src="https://picx.zhimg.com/v2-f01c8e5b2464fffcecc58fa642b42f5d" align="middle">
<img src="https://picx.zhimg.com/v2-2f539ee2e1ed72cd151dfe430c8cccc5" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adversarial-Attack-on-Black-Box-Multi-Agent-by-Adaptive-Perturbation"><a href="#Adversarial-Attack-on-Black-Box-Multi-Agent-by-Adaptive-Perturbation" class="headerlink" title="Adversarial Attack on Black-Box Multi-Agent by Adaptive Perturbation"></a>Adversarial Attack on Black-Box Multi-Agent by Adaptive Perturbation</h2><p><strong>Authors:Jianming Chen, Yawen Wang, Junjie Wang, Xiaofei Xie, Yuanzhe Hu, Qing Wang, Fanjiang Xu</strong></p>
<p>Evaluating security and reliability for multi-agent systems (MAS) is urgent as they become increasingly prevalent in various applications. As an evaluation technique, existing adversarial attack frameworks face certain limitations, e.g., impracticality due to the requirement of white-box information or high control authority, and a lack of stealthiness or effectiveness as they often target all agents or specific fixed agents. To address these issues, we propose AdapAM, a novel framework for adversarial attacks on black-box MAS. AdapAM incorporates two key components: (1) Adaptive Selection Policy simultaneously selects the victim and determines the anticipated malicious action (the action would lead to the worst impact on MAS), balancing effectiveness and stealthiness. (2) Proxy-based Perturbation to Induce Malicious Action utilizes generative adversarial imitation learning to approximate the target MAS, allowing AdapAM to generate perturbed observations using white-box information and thus induce victims to execute malicious action in black-box settings. We evaluate AdapAM across eight multi-agent environments and compare it with four state-of-the-art and commonly-used baselines. Results demonstrate that AdapAM achieves the best attack performance in different perturbation rates. Besides, AdapAM-generated perturbations are the least noisy and hardest to detect, emphasizing the stealthiness.</p>
<blockquote>
<p>è¯„ä¼°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„å®‰å…¨æ€§å’Œå¯é æ€§éšç€å…¶åœ¨å„ç§åº”ç”¨ä¸­çš„æ—¥ç›Šæ™®åŠè€Œå˜å¾—è¶Šæ¥è¶Šç´§è¿«ã€‚ä½œä¸ºä¸€ç§è¯„ä¼°æŠ€æœ¯ï¼Œç°æœ‰çš„å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶é¢ä¸´ä¸€å®šçš„å±€é™æ€§ï¼Œä¾‹å¦‚ç”±äºéœ€è¦ç™½ç›’ä¿¡æ¯æˆ–é«˜æ§åˆ¶æƒé™è€Œå¯¼è‡´çš„ä¸å®ç”¨æ€§ï¼Œä»¥åŠé’ˆå¯¹æ‰€æœ‰æ™ºèƒ½ä½“æˆ–ç‰¹å®šå›ºå®šæ™ºèƒ½ä½“çš„æ”»å‡»ç¼ºä¹éšè”½æ€§æˆ–æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdapAMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é»‘ç®±MASçš„æ–°å‹å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶ã€‚AdapAMåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥åŒæ—¶é€‰æ‹©å—å®³æ™ºèƒ½ä½“å¹¶ç¡®å®šé¢„æœŸçš„æ¶æ„è¡ŒåŠ¨ï¼ˆä¼šå¯¹MASäº§ç”Ÿæœ€ç³Ÿç³•å½±å“çš„è¡ŒåŠ¨ï¼‰ï¼Œä»¥å¹³è¡¡æœ‰æ•ˆæ€§å’Œéšè”½æ€§ã€‚ï¼ˆ2ï¼‰åŸºäºä»£ç†çš„æ‰°åŠ¨ä»¥è¯±å¯¼æ¶æ„è¡ŒåŠ¨åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ æ¥é€¼è¿‘ç›®æ ‡MASï¼Œä»è€Œä½¿AdapAMèƒ½å¤Ÿä½¿ç”¨ç™½ç›’ä¿¡æ¯ç”Ÿæˆæ‰°åŠ¨è§‚å¯Ÿï¼Œå¹¶åœ¨é»‘ç®±è®¾ç½®ä¸­è¯±å¯¼å—å®³æ™ºèƒ½ä½“æ‰§è¡Œæ¶æ„è¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªå¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¯„ä¼°äº†AdapAMï¼Œå¹¶å°†å…¶ä¸å››ç§æœ€æ–°ä¸”å¸¸ç”¨çš„åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒçš„æ‰°åŠ¨ç‡ä¸‹ï¼ŒAdapAMè¾¾åˆ°äº†æœ€ä½³çš„æ”»å‡»æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAdapAMç”Ÿæˆçš„æ‰°åŠ¨å™ªå£°æœ€å°‘ï¼Œæœ€éš¾æ£€æµ‹ï¼Œçªå‡ºäº†å…¶éšè”½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15292v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„å®‰å…¨ä¸å¯é æ€§è¯„ä¼°æå‡ºäº†ç´§è¿«éœ€æ±‚ã€‚ç°æœ‰å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚éœ€è¦ç™½ç›’ä¿¡æ¯å’Œé«˜æ§åˆ¶æƒé™çš„ä¸å®ç”¨æ€§ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹æ‰€æœ‰æ™ºèƒ½ä½“æˆ–ç‰¹å®šå›ºå®šæ™ºèƒ½ä½“çš„éšå½¢å’Œæœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶AdapAMï¼Œé€‚ç”¨äºé»‘ç›’MASã€‚å…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥å’ŒåŸºäºä»£ç†çš„æ‰°åŠ¨è¯±å¯¼æ¶æ„è¡ŒåŠ¨ã€‚è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥å¯å¹³è¡¡æœ‰æ•ˆæ€§å’Œéšè”½æ€§ï¼ŒåŒæ—¶é€‰æ‹©å—å®³è€…å’Œé¢„æœŸçš„æ¶æ„è¡ŒåŠ¨ã€‚åŸºäºä»£ç†çš„æ‰°åŠ¨è¯±å¯¼æ¶æ„è¡ŒåŠ¨åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ æ¥é€¼è¿‘ç›®æ ‡MASï¼Œä½¿AdapAMèƒ½å¤Ÿåœ¨é»‘ç®±ç¯å¢ƒä¸­åˆ©ç”¨ç™½ç®±ä¿¡æ¯ç”Ÿæˆæ‰°åŠ¨è§‚å¯Ÿå¹¶è¯±å¯¼å—å®³è€…æ‰§è¡Œæ¶æ„è¡ŒåŠ¨ã€‚åœ¨å¤šä¸ªç¯å¢ƒä¸­è¯„ä¼°AdapAMï¼Œç»“æœè¡¨æ˜å…¶åœ¨ä¸åŒæ‰°åŠ¨ç‡ä¸‹è¾¾åˆ°æœ€ä½³æ”»å‡»æ€§èƒ½ï¼Œä¸”ç”Ÿæˆçš„æ‰°åŠ¨æœ€ä¸æ˜“è¢«æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„å®‰å…¨ä¸å¯é æ€§è¯„ä¼°å˜å¾—æ—¥ç›Šé‡è¦ã€‚</li>
<li>ç°æœ‰å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚éœ€è¦ç™½ç›’ä¿¡æ¯å’Œé«˜æ§åˆ¶æƒé™ã€‚</li>
<li>AdapAMæ˜¯ä¸€ç§æ–°å‹å¯¹æŠ—æ€§æ”»å‡»æ¡†æ¶ï¼Œé€‚ç”¨äºé»‘ç›’MASã€‚</li>
<li>AdapAMåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè‡ªé€‚åº”é€‰æ‹©æ”¿ç­–å’ŒåŸºäºä»£ç†çš„æ‰°åŠ¨è¯±å¯¼æ¶æ„è¡ŒåŠ¨ã€‚</li>
<li>è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥èƒ½å¤Ÿå¹³è¡¡æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œéšè”½æ€§ã€‚</li>
<li>åŸºäºä»£ç†çš„æ‰°åŠ¨æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ æ¥é€¼è¿‘ç›®æ ‡MASã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1eb736024e59acf9f8d50d54ee1ca9b" align="middle">
<img src="https://picx.zhimg.com/v2-4b61cca2fdd4e8be0a817590c04249fe" align="middle">
<img src="https://picx.zhimg.com/v2-c235bb268a79a3beb64149f189be4193" align="middle">
<img src="https://picx.zhimg.com/v2-63f607fd10efbe6976a877b28d60a4e8" align="middle">
<img src="https://picx.zhimg.com/v2-15bbdbd1195133af2f15d687db2e49dc" align="middle">
<img src="https://picx.zhimg.com/v2-c35d21cb351c6ed5a24e0bdc03e04f84" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Symmetry-Breaking-in-Multi-Agent-Navigation-Winding-Number-Aware-MPC-with-a-Learned-Topological-Strategy"><a href="#Symmetry-Breaking-in-Multi-Agent-Navigation-Winding-Number-Aware-MPC-with-a-Learned-Topological-Strategy" class="headerlink" title="Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy"></a>Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy</h2><p><strong>Authors:Tomoki Nakao, Kazumi Kasaura, Tadashi Kozuno</strong></p>
<p>We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at <a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/WNumMPC">https://github.com/omron-sinicx/WNumMPC</a>.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å±‚å¯¼èˆªæ–¹æ³•ï¼Œä»¥è§£å†³åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¯¼èˆªä¸­ç”±å¯¹ç§°æ€§å¼•èµ·çš„æ­»é”è¿™ä¸€åŸºæœ¬æŒ‘æˆ˜ã€‚å½“å¤šä¸ªæ™ºèƒ½ä½“ç›¸äº’ä½œç”¨æ—¶ï¼Œå®ƒä»¬è‡ªä¸»æ‰“ç ´å¯¹ç§°æ€§çš„å†³ç­–æ–¹å¼æœ¬èº«å°±æ˜¯å›°éš¾çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨ç§°ä¸ºç¼ ç»•æ•°çš„æ‹“æ‰‘ä¸å˜é‡æ¥é‡åŒ–åˆä½œå¯¹ç§°ç ´ç¼ºç­–ç•¥ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ è¿™äº›ç­–ç•¥æœ¬èº«ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰åˆ†å±‚ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºå­¦ä¹ çš„è§„åˆ’å™¨ï¼Œç”¨äºè§„åˆ’æ‹“æ‰‘åˆä½œç­–ç•¥ï¼Œä»¥åŠåŸºäºæ¨¡å‹çš„æ§åˆ¶å™¨ï¼Œç”¨äºæ‰§è¡Œè¿™äº›ç­–ç•¥ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œè§„åˆ’å™¨å­¦ä¹ ä¸ºæ§åˆ¶å™¨ç”Ÿæˆä¸¤ç§ç±»å‹çš„å‚æ•°ï¼šä¸€ç§æ˜¯ç”±ç¼ ç»•æ•°è¡¨ç¤ºçš„æ‹“æ‰‘åˆä½œç­–ç•¥ï¼Œå¦ä¸€ç»„æ˜¯åŠ¨æ€æƒé‡ï¼Œç”¨äºç¡®å®šåœ¨å¤šä¸ªæ™ºèƒ½ä½“åŒæ—¶äº¤å‰çš„å¯†é›†åœºæ™¯ä¸­åº”ä¼˜å…ˆä¸å“ªä¸ªæ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚ç„¶åï¼Œæ§åˆ¶å™¨æ ¹æ®è§„åˆ’å™¨æä¾›çš„ç­–ç•¥å’Œæƒé‡ç”Ÿæˆæ— ç¢°æ’å’Œé«˜æ•ˆçš„åŠ¨ä½œã€‚è¿™ç§åˆ†å±‚ç»“æ„ç»“åˆäº†åŸºäºå­¦ä¹ æ–¹æ³•çš„çµæ´»å†³ç­–èƒ½åŠ›ä¸åŸºäºæ¨¡å‹çš„å¯é æ€§ã€‚ä»¿çœŸå’ŒçœŸå®æœºå™¨äººå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯†é›†ç¯å¢ƒä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿé«˜æ•ˆé¿å…ç¢°æ’å’Œæ­»é”ï¼ŒåŒæ—¶å®ç°å‡ºè‰²çš„å¯¼èˆªæ€§èƒ½ã€‚å®éªŒçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/WNumMPC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/omron-sinicx/WNumMPCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15239v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§è§£å†³åˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å¯¼èˆªä¸­å¯¹ç§°æ­»é”é—®é¢˜çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥é‡åŒ–åˆä½œå¯¹ç§°ç ´ç¼ºç­–ç•¥å¹¶ä½¿ç”¨æ‹“æ‰‘ä¸å˜é‡â€”â€”ç»•æ•°ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ ç­–ç•¥ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚ç­–ç•¥ï¼ŒåŸºäºå­¦ä¹ çš„è§„åˆ’å™¨åˆ¶å®šæ‹“æ‰‘åˆä½œç­–ç•¥ï¼ŒåŸºäºæ¨¡å‹çš„æ§åˆ¶å™¨æ‰§è¡Œç­–ç•¥ã€‚è§„åˆ’å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ äº§ç”Ÿä¸¤ç±»å‚æ•°ä¾›æ§åˆ¶å™¨ä½¿ç”¨ï¼šä¸€ç±»æ˜¯ç”±ç»•æ•°è¡¨ç¤ºçš„æ‹“æ‰‘åˆä½œç­–ç•¥ï¼Œå¦ä¸€ç±»æ˜¯åŠ¨æ€æƒé‡é›†ï¼Œç”¨äºç¡®å®šåœ¨å¯†é›†åœºæ™¯ä¸­åŒæ—¶äº¤å‰çš„å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„äº¤äº’ä¼˜å…ˆçº§ã€‚æ§åˆ¶å™¨æ ¹æ®è§„åˆ’å™¨æä¾›çš„ç­–ç•¥å’Œæƒé‡ç”Ÿæˆæ— ç¢°æ’ã€é«˜æ•ˆçš„è¿åŠ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å­¦ä¹ æ–¹æ³•çš„çµæ´»å†³ç­–èƒ½åŠ›å’Œæ¨¡å‹æ–¹æ³•çš„å¯é æ€§ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯†é›†ç¯å¢ƒä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥ç»•æ•°æ¥è§£å†³å¤šæ™ºèƒ½ä½“å¯¼èˆªä¸­çš„å¯¹ç§°æ€§é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ åˆä½œå¯¹ç§°ç ´ç¼ºç­–ç•¥ã€‚</li>
<li>æå‡ºåˆ†å±‚å¯¼èˆªæ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå­¦ä¹ çš„è§„åˆ’å™¨å’ŒåŸºäºæ¨¡å‹çš„æ§åˆ¶å™¨ã€‚</li>
<li>è§„åˆ’å™¨ä¸ºæ§åˆ¶å™¨æä¾›æ‹“æ‰‘åˆä½œç­–ç•¥å’ŒåŠ¨æ€æƒé‡é›†ã€‚</li>
<li>æ§åˆ¶å™¨æ ¹æ®ç­–ç•¥å’Œæƒé‡ç”Ÿæˆæ— ç¢°æ’ã€é«˜æ•ˆçš„è¿åŠ¨ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å­¦ä¹ æ–¹æ³•çš„çµæ´»æ€§å’Œæ¨¡å‹æ–¹æ³•çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c755a8d9fbca6940423c6a5af876d998" align="middle">
<img src="https://picx.zhimg.com/v2-0fc073314ded40ebb2ed9d8a4adc8dbb" align="middle">
<img src="https://picx.zhimg.com/v2-f1ce6b1bc1fcc549080c36e987d60a24" align="middle">
<img src="https://picx.zhimg.com/v2-3b47ca5f94e6f6fc08e114d589685d61" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OEMA-Ontology-Enhanced-Multi-Agent-Collaboration-Framework-for-Zero-Shot-Clinical-Named-Entity-Recognition"><a href="#OEMA-Ontology-Enhanced-Multi-Agent-Collaboration-Framework-for-Zero-Shot-Clinical-Named-Entity-Recognition" class="headerlink" title="OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition"></a>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition</h2><p><strong>Authors:Xinli Tao, Xin Dong, Xuezhong Zhou</strong></p>
<p>Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMAâ€™s three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.</p>
<blockquote>
<p>ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯ä»ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­æå–ä¿¡æ¯çš„å…³é”®æŠ€æœ¯ã€‚ä½†CRFå’ŒBioClinicalBERTç­‰ç›‘ç£æ¨¡å‹éœ€è¦è€—è´¹å¤§é‡æˆæœ¬çš„æ ‡æ³¨æ•°æ®ã€‚è™½ç„¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬NERé™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä½†åœ¨ç¤ºä¾‹é€‰æ‹©ç²’åº¦ä»¥åŠä¸è‡ªæˆ‘æ”¹è¿›é›†æˆæç¤ºæ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OEMAï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œçš„é›¶æ ·æœ¬ä¸´åºŠNERæ¡†æ¶ã€‚OEMAçš„ä¸‰ä¸ªç»„ä»¶åŒ…æ‹¬ï¼šç”Ÿæˆç¤ºä¾‹çš„è‡ªæˆ‘æ³¨é‡Šå™¨ã€é€šè¿‡SNOMED CTè¿›è¡Œè¿‡æ»¤çš„é‰´åˆ«å™¨ï¼Œä»¥åŠä½¿ç”¨å®ä½“æè¿°è¿›è¡Œå‡†ç¡®æ¨æ–­çš„é¢„æµ‹å™¨ã€‚åœ¨MTSampleå’ŒVAERSæ•°æ®é›†ä¸Šï¼ŒOEMAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾ç¡®åŒ¹é…æ€§èƒ½ã€‚åœ¨ç›¸å…³åŒ¹é…æ–¹é¢ï¼Œå®ƒä¸ç›‘ç£çš„BioClinicalBERTç›¸åŒ¹é…å¹¶è¶…è¿‡äº†CRFã€‚OEMAé€šè¿‡æœ¬ä½“å¼•å¯¼æ¨ç†å’Œå¤šæ™ºèƒ½ä½“åä½œè§£å†³äº†å…³é”®çš„é›¶æ ·æœ¬NERæŒ‘æˆ˜ï¼Œå®ç°äº†æ¥è¿‘ç›‘ç£çš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå‡ºåœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15211v1">PDF</a> 12 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ¡†æ¶OEMAï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå®ç°è‡ªæˆ‘æ ‡æ³¨ã€ç­›é€‰é¢„æµ‹ï¼Œåœ¨MTSampleå’ŒVAERSæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åŒ¹é…æ€§èƒ½ã€‚OEMAè§£å†³äº†é›¶æ ·æœ¬NERçš„å…³é”®æŒ‘æˆ˜ï¼Œå±•ç°å‡ºè¿‘ç›‘ç£æ€§èƒ½ï¼Œä¸ºä¸´åºŠNLPåº”ç”¨æä¾›äº†å¸Œæœ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¿¡æ¯æå–çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>ç›‘ç£æ¨¡å‹å¦‚CRFå’ŒBioClinicalBERTéœ€è¦æ˜‚è´µçš„æ ‡æ³¨æ•°æ®ã€‚</li>
<li>é›¶æ ·æœ¬NERä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>OEMAæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ä¸´åºŠNERæ¡†æ¶ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œè¿›è¡Œè‡ªæˆ‘æ ‡æ³¨ã€ç­›é€‰å’Œé¢„æµ‹ã€‚</li>
<li>OEMAåœ¨MTSampleå’ŒVAERSæ•°æ®é›†ä¸Šè¾¾åˆ°äº†å…ˆè¿›çš„åŒ¹é…æ€§èƒ½ã€‚</li>
<li>OEMAè§£å†³äº†é›¶æ ·æœ¬NERçš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç¤ºä¾‹é€‰æ‹©ç²’åº¦å’Œæç¤ºè‡ªæˆ‘æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8907d9afc570a489fd392909d8459cd" align="middle">
<img src="https://picx.zhimg.com/v2-21998fb829da9054396bca4faf6e73bb" align="middle">
<img src="https://picx.zhimg.com/v2-245f4f945e3d4eefa9f3ed083f5a6929" align="middle">
<img src="https://picx.zhimg.com/v2-357c162c2adfa8077523bce7d9893516" align="middle">
<img src="https://picx.zhimg.com/v2-2ea71b4b75d0bd90f035c2af0a34a6dd" align="middle">
<img src="https://picx.zhimg.com/v2-e390865491f39bde304a679c11b18015" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MedBench-v4-A-Robust-and-Scalable-Benchmark-for-Evaluating-Chinese-Medical-Language-Models-Multimodal-Models-and-Intelligent-Agents"><a href="#MedBench-v4-A-Robust-and-Scalable-Benchmark-for-Evaluating-Chinese-Medical-Language-Models-Multimodal-Models-and-Intelligent-Agents" class="headerlink" title="MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents"></a>MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</h2><p><strong>Authors:Jinru Ding, Lu Lu, Chao Ding, Mouxiao Bian, Jiayuan Chen, Wenrao Pang, Ruiyao Chen, Xinwei Peng, Renjie Lu, Sijie Ren, Guanxu Zhu, Xiaoqin Wu, Zhiqiang Liu, Rongzhao Zhang, Luyi Jiang, Bing Han, Yunqiu Wang, Jie Xu</strong></p>
<p>Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1&#x2F;100 (best: Claude Sonnet 4.5, 62.5&#x2F;100), but safety and ethics remain low (18.4&#x2F;100). Multimodal models perform worse overall (mean 47.5&#x2F;100; best: GPT-5, 54.9&#x2F;100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8&#x2F;100), with Claude Sonnet 4.5-based agents achieving up to 85.3&#x2F;100 overall and 88.9&#x2F;100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.</p>
<blockquote>
<p>è¿‘æœŸåŒ»å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä»£ç†çš„å‘å±•ï¼Œéœ€è¦è¯„ä¼°æ¡†æ¶èƒ½åæ˜ çœŸå®çš„ä¸´åºŠå·¥ä½œæµç¨‹å’Œå®‰å…¨çº¦æŸã€‚æˆ‘ä»¬æ¨å‡ºMedBench v4ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨å›½æ€§çš„äº‘åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½ï¼ŒåŒ…å«è¶…è¿‡70ä¸‡é¡¹ä¸“å®¶ç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–24ä¸ªä¸»è¦ä¸“ä¸šå’Œ91ä¸ªæ¬¡è¦ä¸“ä¸šï¼Œå¹¶ä¸ºLLMsã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä»£ç†è®¾æœ‰ä¸“é—¨èµ›é“ã€‚å„é¡¹ä»»åŠ¡ç»è¿‡æ¥è‡ª500å¤šå®¶æœºæ„çš„ä¸´åºŠåŒ»ç”Ÿçš„å¤šé˜¶æ®µå®Œå–„å’Œå¤šè½®å®¡æŸ¥ï¼Œå¼€æ”¾æ€§å›ç­”ç”±æ ¡å‡†ä¸ºäººç±»è¯„åˆ†æ ‡å‡†çš„LLMè¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä»¬è¯„ä¼°äº†15æ¬¾å‰æ²¿æ¨¡å‹ã€‚åŸºç¡€LLMçš„å¹³å‡æ•´ä½“å¾—åˆ†ä¸º54.1&#x2F;100ï¼ˆæœ€ä½³ï¼šClaude Sonnet 4.5ï¼Œå¾—åˆ†62.5&#x2F;100ï¼‰ï¼Œä½†å®‰å…¨å’Œé“å¾·æ–¹é¢å¾—åˆ†è¾ƒä½ï¼ˆ18.4&#x2F;100ï¼‰ã€‚å¤šæ¨¡æ€æ¨¡å‹çš„æ€»ä½“è¡¨ç°è¾ƒå·®ï¼ˆå¹³å‡å¾—åˆ†47.5&#x2F;100ï¼›æœ€ä½³ï¼šGPT-5ï¼Œå¾—åˆ†54.9&#x2F;100ï¼‰ï¼Œå…·æœ‰è‰¯å¥½çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†åœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢è¾ƒå¼±ã€‚åŸºäºç›¸åŒæ¶æ„çš„æ™ºèƒ½ä»£ç†å¤§å¹…æé«˜äº†ç«¯åˆ°ç«¯çš„æ€§èƒ½ï¼ˆå¹³å‡å¾—åˆ†79.8&#x2F;100ï¼‰ï¼Œå…¶ä¸­Claude Sonnet 4.5æ™ºèƒ½ä»£ç†çš„æ€»ä½“å¾—åˆ†é«˜è¾¾85.3&#x2F;100ï¼Œå®‰å…¨ä»»åŠ¡çš„å¾—åˆ†ä¸º88.9&#x2F;100ã€‚å› æ­¤ï¼ŒMedBench v4æ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†å’Œå®‰å…¨æ–¹é¢å­˜åœ¨çš„æŒç»­å·®è·ï¼ŒåŒæ—¶è¡¨æ˜æ²»ç†æ„è¯†æ™ºèƒ½åè°ƒå¯ä»¥åœ¨ä¸ç‰ºç‰²èƒ½åŠ›çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜ä¸´åºŠå°±ç»ªçš„åŸºå‡†æµ‹è¯•æ°´å¹³ã€‚é€šè¿‡ä¸ä¸­å›½ä¸´åºŠæŒ‡å—å’Œç›‘ç®¡ä¼˜å…ˆäº‹é¡¹ç›¸åè°ƒçš„ä»»åŠ¡ï¼Œè¯¥å¹³å°ä¸ºåŒ»é™¢ã€å¼€å‘äººå‘˜å’Œæ”¿ç­–åˆ¶å®šè€…è¯„ä¼°åŒ»ç–—äººå·¥æ™ºèƒ½æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å‚è€ƒä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14439v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°çš„åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä½“çš„è¿›å±•ï¼Œéœ€è¦åæ˜ çœŸå®ä¸´åºŠæµç¨‹å’Œå®‰å…¨æ€§çº¦æŸçš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬æ¨å‡ºMedBench v4ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨å›½æ€§çš„äº‘åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«è¶…è¿‡70ä¸‡ä¸“å®¶ç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–24ä¸ªä¸»è¦å’Œ91ä¸ªæ¬¡è¦ä¸“ä¸šé¢†åŸŸï¼Œå¹¶ä¸ºLLMã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä½“è®¾æœ‰ä¸“é—¨èµ›é“ã€‚é€šè¿‡å¯¹æ¥è‡ª500å¤šå®¶æœºæ„çš„åŒ»ç”Ÿçš„å¤šæ¬¡é˜¶æ®µç²¾ç»†åŒ–å’Œå¤šè½®å®¡æŸ¥ï¼Œä»¥åŠç”±æ ¡å‡†ä¸ºäººç±»è¯„åˆ†çš„LLMä½œä¸ºæ³•å®˜å¯¹å¼€æ”¾å¼å›åº”è¿›è¡Œè¯„åˆ†ï¼Œæˆ‘ä»¬å¯¹15ä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚åŸºç¡€LLMçš„å¹³å‡æ€»ä½“å¾—åˆ†ä¸º54.1åˆ†ï¼ˆæœ€ä½³ï¼šClaude Sonnet 4.5ï¼Œ62.5åˆ†ï¼‰ï¼Œä½†å®‰å…¨æ€§å’Œé“å¾·æ€§è¾ƒä½ï¼ˆ18.4åˆ†ï¼‰ã€‚å¤šæ¨¡æ€æ¨¡å‹æ€»ä½“è¡¨ç°ç¨å·®ï¼ˆå¹³å‡å¾—åˆ†47.5åˆ†ï¼›æœ€ä½³ï¼šGPT-5ï¼Œ54.9åˆ†ï¼‰ï¼Œå…·æœ‰å¼ºå¤§çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†è·¨æ¨¡æ€æ¨ç†è¾ƒå¼±ã€‚åŸºäºç›¸åŒèƒŒæ™¯çš„æ™ºèƒ½ä½“å®è´¨ä¸Šæé«˜äº†ç«¯åˆ°ç«¯çš„æ€§èƒ½ï¼ˆå¹³å‡å¾—åˆ†79.8åˆ†ï¼‰ï¼Œå…¶ä¸­Claude Sonnet 4.5æ™ºèƒ½ä½“æ€»ä½“å¾—åˆ†é«˜è¾¾85.3åˆ†ï¼Œå®‰å…¨ä»»åŠ¡å¾—åˆ†88.9åˆ†ã€‚å› æ­¤ï¼ŒMedBench v4æ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†å’Œå®‰å…¨æ€§æ–¹é¢å­˜åœ¨çš„æŒç»­å·®è·ï¼ŒåŒæ—¶è¡¨æ˜æ²»ç†æ„è¯†æ™ºèƒ½ä½“çš„ååŒå¯ä»¥æ˜¾è‘—æé«˜ä¸´åºŠå‡†å¤‡æ€§è€Œä¸ä¼šç‰ºç‰²èƒ½åŠ›ã€‚è¯¥å¹³å°é€šè¿‡ä¸ä¸­å›½çš„ä¸´åºŠæŒ‡å—å’Œç›‘ç®¡ä¼˜å…ˆäº‹é¡¹ç›¸ä¸€è‡´çš„ä»»åŠ¡ï¼Œä¸ºåŒ»é™¢ã€å¼€å‘è€…å’Œå†³ç­–è€…æä¾›äº†ä¸€ä¸ªå®ç”¨çš„åŒ»ç–—äººå·¥æ™ºèƒ½å®¡è®¡å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedBench v4æ˜¯ä¸€ä¸ªå…¨å›½æ€§çš„äº‘åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–å¤šä¸ªåŒ»ç–—ä¸“ä¸šé¢†åŸŸï¼ŒåŒ…æ‹¬LLMã€å¤šæ¨¡æ€æ¨¡å‹å’Œæ™ºèƒ½ä½“çš„ä¸“é—¨èµ›é“ã€‚</li>
<li>åŸºç¡€LLMåœ¨è¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®‰å…¨æ€§å’Œé“å¾·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¤šæ¨¡æ€æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›å¼ºï¼Œä½†åœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢æœ‰å¾…æå‡ã€‚</li>
<li>æ™ºèƒ½ä½“åœ¨æé«˜ç«¯åˆ°ç«¯æ€§èƒ½æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå°¤å…¶æ˜¯åŸºäºClaude Sonnet 4.5çš„æ™ºèƒ½ä½“è¡¨ç°çªå‡ºã€‚</li>
<li>MedBench v4æ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†å’Œå®‰å…¨æ€§æ–¹é¢çš„å·®è·ã€‚</li>
<li>æ²»ç†æ„è¯†æ™ºèƒ½ä½“çš„ååŒå¯ä»¥æ˜¾è‘—æé«˜ä¸´åºŠå‡†å¤‡æ€§ï¼Œè€Œä¸ä¼šç‰ºç‰²èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b9e98b18fcf2bd7d95fd865b2b3a732" align="middle">
<img src="https://picx.zhimg.com/v2-cc1964fb04de8afd81dadafa30f918c9" align="middle">
<img src="https://picx.zhimg.com/v2-132c0301b67506ca97b575639e032922" align="middle">
<img src="https://picx.zhimg.com/v2-fb1982d0475a12ff724525511a1fcce4" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Knowledge-Grounded-Agentic-Large-Language-Models-for-Multi-Hazard-Understanding-from-Reconnaissance-Reports"><a href="#Knowledge-Grounded-Agentic-Large-Language-Models-for-Multi-Hazard-Understanding-from-Reconnaissance-Reports" class="headerlink" title="Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports"></a>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</h2><p><strong>Authors:Chenchen Kuai, Zihao Li, Braden Rosen, Stephanie Paal, Navid Jafari, Jean-Louis Briaud, Yunlong Zhang, Youssef M. A. Hashash, Yang Zhou</strong></p>
<p>Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.</p>
<blockquote>
<p>ç¾åä¾¦å¯ŸæŠ¥å‘Šå¯¹äºç†è§£å¤šç¾ç§äº¤äº’ä½œç”¨è‡³å…³é‡è¦ï¼Œä½†å…¶éç»“æ„åŒ–çš„å™è¿°ä½¿å¾—çŸ¥è¯†çš„ç³»ç»Ÿä¼ é€’å˜å¾—å›°éš¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåˆ†æè¿™äº›æŠ¥å‘Šæä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†åœ¨ç¼ºä¹é¢†åŸŸä¾æ®çš„æƒ…å†µä¸‹å¸¸å¸¸äº§ç”Ÿä¸å¯é æˆ–è™šæ„çš„è¾“å‡ºã€‚æœ¬ç ”ç©¶å¼•å…¥äº†æ··åˆæ£€ç´¢ä»£ç†RAGï¼ˆMoRA-RAGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥çŸ¥è¯†ä¸ºåŸºç¡€çš„è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå°†ä¾¦å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºå¤šç¾ç§æ¨ç†çš„ç»“æ„åŒ–åŸºç¡€ã€‚è¯¥æ¡†æ¶èåˆäº†æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°è·¨ç‰¹å®šç¾å®³æ•°æ®åº“è¿›è¡Œè·¯ç”±æŸ¥è¯¢ï¼ŒåŒæ—¶ä½¿ç”¨ä»£ç†åˆ†å—æ¥åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªéªŒè¯å¾ªç¯ï¼Œç”¨äºè¯„ä¼°è¯æ®å……åˆ†æ€§ã€æ”¹è¿›æŸ¥è¯¢ä»¥åŠåœ¨ä¿¡æ¯ä¸å®Œæ•´æ—¶å¯åŠ¨é’ˆå¯¹æ€§æœç´¢ã€‚æˆ‘ä»¬é€šè¿‡ä»GEERç¾åä¾¦å¯ŸæŠ¥å‘Šä¸­æå–é—®ç­”å¯¹æ„å»ºäº†HazardRecQAæ•°æ®é›†ï¼Œè¿™äº›æŠ¥å‘Šè®°å½•äº†å…¨çƒ90ä¸ªäº‹ä»¶ä¸­çš„ä¸ƒç§ä¸»è¦ç¾å®³ç±»å‹ã€‚MoRA-RAGå‡†ç¡®ç‡é«˜è¾¾94.5%ï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬LLMå’Œç°æœ‰æœ€å…ˆè¿›çš„RAGç³»ç»Ÿåˆ†åˆ«é«˜å‡º30%å’Œ10%ï¼Œå¹¶åœ¨å¤šç§LLMæ¶æ„ä¸­å‡å°‘äº†è™šæ„ç°è±¡ã€‚MoRA-RAGè¿˜ä½¿å¾—è½»é‡çº§çš„LLMæ€§èƒ½å¯ä»¥ä¸ä¸“æœ‰æ¨¡å‹ç›¸åª²ç¾ã€‚å®ƒä¸ºå°†ç¾åæ–‡æ¡£è½¬åŒ–ä¸ºå¯é ä¸”å¯æ“ä½œçš„æƒ…æŠ¥ä»¥åº”å¯¹ç¾å®³éŸ§æ€§å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14010v2">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMoRA-RAGçš„çŸ¥è¯†æ¥åœ°å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯å°†ä¾¦å¯ŸæŠ¥å‘Šè½¬åŒ–ä¸ºå¤šç¾ç§æ¨ç†çš„ç»“æ„åŒ–åŸºç¡€ã€‚MoRA-RAGç»“åˆäº†æ··åˆæ£€ç´¢æœºåˆ¶å’Œä»£ç†å—ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€è·¯ç”±æŸ¥è¯¢çš„åŒæ—¶ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼Œå¹¶å…·æœ‰éªŒè¯å¾ªç¯æ¥è¯„ä¼°è¯æ®å……åˆ†æ€§ã€ç»†åŒ–æŸ¥è¯¢å’Œå¯åŠ¨ç›®æ ‡æœç´¢ã€‚è¯¥æ¡†æ¶åœ¨ç¾éš¾åçš„ä¾¦å¯ŸæŠ¥å‘Šä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå®ç°äº†é«˜å‡†ç¡®æ€§å¹¶é™ä½äº†è™šæ„ç°è±¡çš„å‡ºç°ã€‚å®ƒä¸ºå°†ç¾åæ–‡æ¡£è½¬åŒ–ä¸ºå¯ä¿¡çš„æ™ºèƒ½æƒ…æŠ¥æä¾›äº†å…¨æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†æç¾éš¾ä¾¦å¯ŸæŠ¥å‘Šæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>MoRA-RAGæ˜¯ä¸€ä¸ªçŸ¥è¯†æ¥åœ°çš„è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤šç¾ç§çš„ä¾¦å¯ŸæŠ¥å‘Šã€‚</li>
<li>MoRA-RAGç»“åˆäº†æ··åˆæ£€ç´¢æœºåˆ¶å’Œä»£ç†å—æŠ€æœ¯ï¼Œæé«˜äº†æŸ¥è¯¢æ•ˆç‡å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚</li>
<li>MoRA-RAGåŒ…å«ä¸€ä¸ªéªŒè¯å¾ªç¯ï¼Œèƒ½å¤Ÿè¯„ä¼°è¯æ®å……åˆ†æ€§å¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æœç´¢ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨MoRA-RAGæ¡†æ¶ï¼Œç¾éš¾ä¾¦å¯ŸæŠ¥å‘Šçš„å‡†ç¡®æ€§å¯è¾¾åˆ°94.5%ã€‚</li>
<li>MoRA-RAGç›¸è¾ƒäºé›¶åŸºç¡€å’Œç°æœ‰æœ€å…ˆè¿›çš„RAGç³»ç»Ÿè¡¨ç°æ›´ä¼˜ï¼Œä¸”é™ä½äº†è™šæ„ä¿¡æ¯çš„å‡ºç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-628b76da8563d9ac7fdf0d2088de2a7b" align="middle">
<img src="https://picx.zhimg.com/v2-e281c399547978927ed6d5d7c1a78a96" align="middle">
<img src="https://picx.zhimg.com/v2-726dc335a07c7659b454ec520d6df499" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="S-DAG-A-Subject-Based-Directed-Acyclic-Graph-for-Multi-Agent-Heterogeneous-Reasoning"><a href="#S-DAG-A-Subject-Based-Directed-Acyclic-Graph-for-Multi-Agent-Heterogeneous-Reasoning" class="headerlink" title="S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning"></a>S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning</h2><p><strong>Authors:Jiangwen Dong, Zehui Lin, Wanyu Lin, Mingjin Zhang</strong></p>
<p>Large Language Models (LLMs) have achieved impressive performance in complex reasoning problems. Their effectiveness highly depends on the specific nature of the task, especially the required domain knowledge. Existing approaches, such as mixture-of-experts, typically operate at the task level; they are too coarse to effectively solve the heterogeneous problems involving multiple subjects. This work proposes a novel framework that performs fine-grained analysis at subject level equipped with a designated multi-agent collaboration strategy for addressing heterogeneous problem reasoning. Specifically, given an input query, we first employ a Graph Neural Network to identify the relevant subjects and infer their interdependencies to generate an \textit{Subject-based Directed Acyclic Graph} (S-DAG), where nodes represent subjects and edges encode information flow. Then we profile the LLM models by assigning each model a subject-specific expertise score, and select the top-performing one for matching corresponding subject of the S-DAG. Such subject-model matching enables graph-structured multi-agent collaboration where information flows from the starting model to the ending model over S-DAG. We curate and release multi-subject subsets of standard benchmarks (MMLU-Pro, GPQA, MedMCQA) to better reflect complex, real-world reasoning tasks. Extensive experiments show that our approach significantly outperforms existing task-level model selection and multi-agent collaboration baselines in accuracy and efficiency. These results highlight the effectiveness of subject-aware reasoning and structured collaboration in addressing complex and multi-subject problems.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„æ¨ç†é—®é¢˜ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ã€‚å®ƒä»¬çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºä»»åŠ¡çš„å…·ä½“æ€§è´¨ï¼Œå°¤å…¶æ˜¯æ‰€éœ€çš„é¢†åŸŸçŸ¥è¯†ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ä¸“å®¶æ··åˆæ³•ï¼Œé€šå¸¸æ˜¯åœ¨ä»»åŠ¡å±‚é¢è¿›è¡Œæ“ä½œï¼›å®ƒä»¬è¿‡äºç²—ç•¥ï¼Œæ— æ³•æœ‰æ•ˆè§£å†³æ¶‰åŠå¤šä¸ªä¸»é¢˜çš„å¼‚è´¨æ€§é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œä»¥ä¸»é¢˜å±‚é¢è¿›è¡Œç²¾ç»†åˆ†æï¼Œå¹¶é…å¤‡ä¸“é—¨çš„å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥ï¼Œä»¥è§£å†³å¼‚è´¨æ€§é—®é¢˜æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šè¾“å…¥æŸ¥è¯¢ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨å›¾ç¥ç»ç½‘ç»œæ¥è¯†åˆ«ç›¸å…³ä¸»é¢˜å¹¶æ¨æ–­å®ƒä»¬ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»¥ç”Ÿæˆä¸€ä¸ªåŸºäºä¸»é¢˜çš„æœ‰å‘æ— ç¯å›¾ï¼ˆS-DAGï¼‰ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨ä¸»é¢˜ï¼Œè¾¹ç¼–ç ä¿¡æ¯æµã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºæ¯ä¸ªæ¨¡å‹åˆ†é…ä¸»é¢˜ç‰¹å®šä¸“ä¸šåˆ†æ•°æ¥è¯„ä¼°LLMæ¨¡å‹ï¼Œå¹¶é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ¥åŒ¹é…S-DAGä¸­çš„ç›¸åº”ä¸»é¢˜ã€‚è¿™ç§ä¸»é¢˜ä¸æ¨¡å‹çš„åŒ¹é…ä½¿å¾—ä¿¡æ¯å¯ä»¥ä»èµ·å§‹æ¨¡å‹æµå‘S-DAGä¸Šçš„ç»“æŸæ¨¡å‹ï¼Œä»è€Œå®ç°å›¾ç»“æ„åŒ–çš„å¤šæ™ºèƒ½ä½“åä½œã€‚æˆ‘ä»¬æ•´ç†å’Œå‘å¸ƒäº†å¤šä¸»é¢˜å­é›†çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆMMLU-Proã€GPQAã€MedMCQAï¼‰ï¼Œä»¥æ›´å¥½åœ°åæ˜ å¤æ‚ã€ç°å®ä¸–ç•Œçš„æ¨ç†ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä»»åŠ¡çº§æ¨¡å‹é€‰æ‹©å’ŒåŸºäºå¤šæ™ºèƒ½ä½“çš„åä½œåŸºçº¿ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ä¸»é¢˜æ„ŸçŸ¥æ¨ç†å’Œç»“æ„åŒ–åä½œåœ¨è§£å†³å¤æ‚å’Œå¤šä¸»é¢˜é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06727v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†é—®é¢˜æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶æ•ˆæœé«˜åº¦ä¾èµ–äºä»»åŠ¡çš„å…·ä½“æ€§è´¨å’Œæ‰€éœ€çš„é¢†åŸŸçŸ¥è¯†ã€‚ç°æœ‰æ–¹æ³•å¦‚æ··åˆä¸“å®¶é€šå¸¸åœ¨ä»»åŠ¡å±‚é¢æ“ä½œï¼Œéš¾ä»¥è§£å†³æ¶‰åŠå¤šä¸ªä¸»é¢˜çš„ä¸åŒè´¨é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåœ¨ä¸»é¢˜å±‚é¢è¿›è¡Œç²¾ç»†åˆ†æï¼Œå¹¶é…å¤‡ä¸“é—¨çš„å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥æ¥è§£å†³å¼‚è´¨æ€§é—®é¢˜æ¨ç†ã€‚é€šè¿‡è¾“å…¥æŸ¥è¯¢ï¼Œä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¯†åˆ«ç›¸å…³ä¸»é¢˜å¹¶æ¨æ–­å…¶ç›¸äº’ä¾èµ–æ€§ï¼Œç”Ÿæˆä¸»é¢˜å¯¼å‘çš„æ— ç¯å›¾ï¼ˆS-DAGï¼‰ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨ä¸»é¢˜ï¼Œè¾¹ç¼˜ç¼–ç ä¿¡æ¯æµã€‚é€šè¿‡ä¸ºæ¯ç§æ¨¡å‹åˆ†é…ä¸»é¢˜ç‰¹å®šä¸“ä¸šåˆ†æ•°æ¥è¯„ä¼°LLMæ¨¡å‹ï¼Œå¹¶é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¸S-DAGä¸­çš„ç›¸åº”ä¸»é¢˜ç›¸åŒ¹é…ã€‚è¿™ç§ä¸»é¢˜æ¨¡å‹åŒ¹é…å¯å®ç°å›¾å½¢ç»“æ„çš„å¤šæ™ºèƒ½ä½“åä½œï¼Œä¿¡æ¯ä»èµ·å§‹æ¨¡å‹æµç»S-DAGæµå‘æœ€ç»ˆæ¨¡å‹ã€‚ç ”ç©¶æ•´ç†å¹¶å‘å¸ƒäº†å¤šä¸»é¢˜å­é›†çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ˆMMLU-Proã€GPQAã€MedMCQAï¼‰ï¼Œä»¥æ›´å¥½åœ°åæ˜ å¤æ‚ã€ç°å®ä¸–ç•Œçš„æ¨ç†ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä»»åŠ¡çº§åˆ«æ¨¡å‹é€‰æ‹©å’Œå¤šå…ƒæ™ºèƒ½åä½œåŸºå‡†æµ‹è¯•ã€‚ç»“æœè¯æ˜äº†ä¸»é¢˜æ„ŸçŸ¥æ¨ç†å’Œç»“æ„åä½œåœ¨è§£å†³å¤æ‚å¤šä¸»é¢˜é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶æ•ˆæœå—ä»»åŠ¡ç‰¹æ€§å’Œé¢†åŸŸçŸ¥è¯†çš„å½±å“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚æ··åˆä¸“å®¶åœ¨è§£å†³æ¶‰åŠå¤šä¸ªä¸»é¢˜çš„é—®é¢˜æ—¶è¡¨ç°ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåœ¨ä¸»é¢˜å±‚é¢è¿›è¡Œç²¾ç»†åˆ†æï¼Œå¹¶é…å¤‡å¤šæ™ºèƒ½ä½“åä½œç­–ç•¥ã€‚</li>
<li>ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¯†åˆ«ç›¸å…³ä¸»é¢˜å¹¶ç”Ÿæˆä¸»é¢˜å¯¼å‘çš„æ— ç¯å›¾ï¼ˆS-DAGï¼‰ã€‚</li>
<li>é€šè¿‡è¯„ä¼°LLMæ¨¡å‹çš„ä¸»é¢˜ç‰¹å®šä¸“ä¸šåˆ†æ•°ï¼Œå®ç°æ›´ç²¾å‡†çš„æ¨¡å‹é€‰æ‹©ã€‚</li>
<li>æå‡ºä¸€ç§ä¸»é¢˜æ¨¡å‹åŒ¹é…æ–¹æ³•ï¼Œå®ç°å›¾å½¢ç»“æ„çš„å¤šæ™ºèƒ½ä½“åä½œã€‚</li>
<li>æ•´ç†å’Œå‘å¸ƒå¤šä¸»é¢˜å­é›†çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ‰€ææ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61d2d8945862039a039386a900ca4e68" align="middle">
<img src="https://picx.zhimg.com/v2-19cc6d3560c835a8a7f487781770a835" align="middle">
<img src="https://picx.zhimg.com/v2-9d99f00b6ae48b540c2c43f51e3702a9" align="middle">
<img src="https://picx.zhimg.com/v2-2ebd007d44ccf5d5c9149f7956142f3a" align="middle">
<img src="https://picx.zhimg.com/v2-fc301101e29d3c88598f0fb77145bda4" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Automating-Android-Build-Repair-Bridging-the-Reasoning-Execution-Gap-in-LLM-Agents-with-Domain-Specific-Tools"><a href="#Automating-Android-Build-Repair-Bridging-the-Reasoning-Execution-Gap-in-LLM-Agents-with-Domain-Specific-Tools" class="headerlink" title="Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools"></a>Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools</h2><p><strong>Authors:Ha Min Son, Huan Ren, Xin Liu, Zhe Zhao</strong></p>
<p>Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixerâ€™s success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the modelâ€™s high-level reasoning and effective low-level execution.</p>
<blockquote>
<p>Androidæ˜¯ç›®å‰æœ€å¤§çš„ç§»åŠ¨å¹³å°ï¼Œä½†è‡ªåŠ¨æ„å»ºåº”ç”¨ç¨‹åºä»ç„¶æ˜¯ä¸€ä¸ªå®é™…æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è§£å†³Androidæ„å»ºé”™è¯¯æ–¹é¢çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆæ¨å‡ºäº†AndroidBuildBenchï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«1019ä¸ªæ„å»ºå¤±è´¥çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›å¤±è´¥æ¡ˆä¾‹æ˜¯ä»43ä¸ªå¼€æºAndroidé¡¹ç›®çš„æäº¤å†å²ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ã€‚æ¯ä¸ªé—®é¢˜éƒ½ä¸åç»­æäº¤ä¸­çš„ç»è¿‡éªŒè¯çš„è§£å†³æ–¹æ¡ˆé…å¯¹ï¼Œç¡®ä¿ä¿®å¤æ–¹æ¡ˆæ˜¯å¯è¡Œçš„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†GradleFixerï¼Œè¿™æ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œæ‹¥æœ‰ç”¨äºæ£€æŸ¥å’Œæ“ä½œGradleæ„å»ºç¯å¢ƒçš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚GradleFixerçš„è§£å†³ç‡é«˜è¾¾81.4%ï¼ˆpass@1ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¾èµ–é€šç”¨å¤–å£³çš„æœ€æ–°ç¼–ç ä»£ç†ã€‚GradleFixerçš„æˆåŠŸè¡¨æ˜ï¼Œè™½ç„¶LLMå…·å¤‡è§£å†³è¿™äº›æ•…éšœçš„é«˜çº§çŸ¥è¯†ï¼Œä½†å®ƒä»¬åœ¨å°†çŸ¥è¯†è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ä½çº§è¡ŒåŠ¨æ—¶ä½¿ç”¨é€šç”¨å¤–å£³ä¼šé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå·¥å…·æ¡¥æ¥â€çš„ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯¥ç­–ç•¥ç”¨é¢†åŸŸæ„ŸçŸ¥çš„æŠ½è±¡æ›¿æ¢é€šç”¨å¤–å£³å‘½ä»¤ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§æ–¹æ³•é€šè¿‡ä¸¤ç§æœºåˆ¶èµ·ä½œç”¨ï¼š1ï¼‰å®ƒæä¾›APIç­‰æ ¼å¼çš„å·¥å…·ï¼ŒLLMä½¿ç”¨å¾—æ›´å¯é ï¼›2ï¼‰å®ƒå°†è¡ŒåŠ¨ç©ºé—´é™åˆ¶åœ¨ç›¸å…³æ“ä½œä¸Šã€‚è¿™ç§æ–¹æ³•å¼¥åˆäº†æ¨¡å‹çš„é«˜çº§æ¨ç†å’Œæœ‰æ•ˆçš„ä½çº§æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08640v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Androidå¹³å°æ„å»ºåº”ç”¨æ—¶é‡åˆ°çš„æŒ‘æˆ˜ï¼Œæå‡ºAndroidBuildBenchä½œä¸ºåŒ…å«1019ä¸ªæ„å»ºå¤±è´¥çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶ä»43ä¸ªå¼€æºAndroidé¡¹ç›®çš„æäº¤å†å²ä¸­ç­›é€‰å‡ºé—®é¢˜åŠå…¶å¯¹åº”çš„è§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºGradleFixerçš„LLMä»£ç†ï¼Œé€šè¿‡å·¥å…·æ¡¥æ¥ç­–ç•¥å®ç°äº†å¯¹Gradleæ„å»ºç¯å¢ƒçš„æ£€æŸ¥å’Œæ“ä½œï¼Œå¹¶è¾¾åˆ°äº†è¾ƒé«˜çš„è§£å†³ç‡ã€‚æ–‡ç« å¼ºè°ƒäº†LLMåœ¨è§£å†³æ„å»ºå¤±è´¥é—®é¢˜ä¸Šçš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å·¥å…·æ¡¥æ¥ç­–ç•¥åœ¨æé«˜LLMä»£ç†æ‰§è¡Œæ•ˆç‡æ–¹é¢çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Androidæ˜¯ç§»åŠ¨å¹³å°ä¸­æœ€å¤§çš„ï¼Œä½†è‡ªåŠ¨æ„å»ºåº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>Large Language Modelsï¼ˆLLMsï¼‰åœ¨ä»£ç ä¿®å¤æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨è§£å†³Androidæ„å»ºé”™è¯¯æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>å¼•å…¥äº†AndroidBuildBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1019ä¸ªæ„å»ºå¤±è´¥çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>æå‡ºäº†GradleFixerï¼Œä¸€ä¸ªé’ˆå¯¹Gradleæ„å»ºç¯å¢ƒçš„LLMä»£ç†ï¼Œé€šè¿‡å·¥å…·æ¡¥æ¥ç­–ç•¥å®ç°äº†é«˜æ•ˆçš„è§£å†³ç‡ã€‚</li>
<li>GradleFixerçš„æˆåŠŸè¡¨æ˜LLMå…·å¤‡è§£å†³è¿™äº›å¤±è´¥çš„é«˜çº§çŸ¥è¯†ï¼Œä½†åœ¨å°†çŸ¥è¯†è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ä½çº§æ“ä½œä¸Šé‡åˆ°å›°éš¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b273f254ca01a7760046e880d2d3a27b" align="middle">
<img src="https://picx.zhimg.com/v2-8f59ce054ec0c30910dc966af72a42b6" align="middle">
<img src="https://picx.zhimg.com/v2-f9cfba4be32fd1da9a62c3dd1cb27119" align="middle">
<img src="https://picx.zhimg.com/v2-a9f0dbda5c3b8c4a573f07af5b919009" align="middle">
<img src="https://picx.zhimg.com/v2-64295e42067975066e05bb4a241b61fb" align="middle">
<img src="https://picx.zhimg.com/v2-f29a6c1a3565170f939ca434fae28ece" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MedLA-A-Logic-Driven-Multi-Agent-Framework-for-Complex-Medical-Reasoning-with-Large-Language-Models"><a href="#MedLA-A-Logic-Driven-Multi-Agent-Framework-for-Complex-Medical-Reasoning-with-Large-Language-Models" class="headerlink" title="MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models"></a>MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models</h2><p><strong>Authors:Siqi Ma, Jiajie Huang, Fan Zhang, Jinlin Wu, Yue Shen, Guohui Fan, Zhu Zhang, Zelin Zang</strong></p>
<p>Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.</p>
<blockquote>
<p>å›ç­”å¤æ‚çš„åŒ»å­¦é—®é¢˜ä¸ä»…éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†å’Œæ‚£è€…ç‰¹å®šä¿¡æ¯ï¼Œè¿˜éœ€è¦ç»“æ„åŒ–å’Œå¤šè§’åº¦çš„æ¨ç†ã€‚ç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šçš„è§’è‰²æˆ–æµ…å±‚æ¬¡çš„äº¤äº’æç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ£€æµ‹å’Œè§£å†³ç»†å¾®é€»è¾‘ä¸ä¸€è‡´æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘é©±åŠ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶â€”â€”MedLAã€‚æ¯ä¸ªæ™ºèƒ½ä½“å°†å…¶æ¨ç†è¿‡ç¨‹ç»„ç»‡æˆåŸºäºä¸‰æ®µè®ºï¼ˆå¤§å‰æã€å°å‰æå’Œç»“è®ºï¼‰çš„æ˜ç¡®é€»è¾‘æ ‘ï¼Œä»è€Œå®ç°é€æ˜çš„æ¨ç†å’Œå‰æå±‚é¢çš„å¯¹é½ã€‚æ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œå¤šè½®ã€å›¾è¡¨å¼•å¯¼çš„è®¨è®ºï¼Œä»¥æ¯”è¾ƒå’Œè¿­ä»£åœ°ä¼˜åŒ–ä»–ä»¬çš„é€»è¾‘æ ‘ï¼Œé€šè¿‡é”™è¯¯ä¿®æ­£å’Œè§£å†³çŸ›ç›¾è¾¾æˆå…±è¯†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MedDDxå’Œæ ‡å‡†åŒ»å­¦é—®ç­”ä»»åŠ¡ï¼‰ä¸­ï¼ŒMedLAå§‹ç»ˆä¼˜äºåŸºäºé™æ€è§’è‰²çš„ç³»ç»Ÿå’Œå•æ™ºèƒ½ä½“åŸºçº¿ã€‚æ­¤å¤–ï¼ŒMedLAåœ¨å¼€æºå’Œå•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹ä¸»å¹²ä¸Šå‡æœ‰æ•ˆæ‰©å±•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸ºå¯ä¿¡åŒ»å­¦æ¨ç†æä¾›äº†å¯æ¨å¹¿çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23725v2">PDF</a> accepted by AAAI-26 (ORAL)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªé€»è¾‘é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶MedLAï¼Œç”¨äºè§£å†³å¤æ‚çš„åŒ»ç–—é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é€šè¿‡é€»è¾‘æ ‘è¿›è¡Œæ¨ç†ï¼Œé€»è¾‘æ ‘åŸºäºä¸‰æ®µè®ºï¼ˆå¤§å‰æã€å°å‰æå’Œç»“è®ºï¼‰è¿›è¡Œç»„ç»‡ï¼Œå®ç°é€æ˜æ¨ç†å’Œå‰æçº§å¯¹é½ã€‚æ™ºèƒ½ä½“é€šè¿‡å¤šè½®å›¾å½¢å¼•å¯¼çš„è®¨è®ºæ¥æ¯”è¾ƒå’Œè¿­ä»£ä¼˜åŒ–å…¶é€»è¾‘æ ‘ï¼Œé€šè¿‡çº é”™å’ŒçŸ›ç›¾è§£å†³è¾¾æˆå…±è­˜ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMedLAè¡¨ç°ä¼˜å¼‚ï¼Œå¯è·¨å¼€æºå’Œå•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ•ˆæ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MedLAæ˜¯ä¸€ä¸ªé€»è¾‘é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤æ‚çš„åŒ»ç–—é—®é¢˜ã€‚</li>
<li>å®ƒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºï¼Œèƒ½å¤Ÿæ•´åˆä¸åŒé¢†åŸŸçŸ¥è¯†å’Œæ‚£è€…ä¿¡æ¯ã€‚</li>
<li>æ™ºèƒ½ä½“é€šè¿‡é€»è¾‘æ ‘è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œæé«˜æ¨ç†çš„é€æ˜åº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ä¹‹é—´çš„å¤šè½®å›¾å½¢å¼•å¯¼è®¨è®ºå¯ä»¥æ£€æµ‹å’Œè§£å†³é€»è¾‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>MedLAåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºé™æ€è§’è‰²ç³»ç»Ÿå’Œå•æ™ºèƒ½ä½“åŸºçº¿ã€‚</li>
<li>å®ƒå¯å¹¿æ³›åº”ç”¨äºåŒ»ç–—é—®ç­”ç³»ç»Ÿå’Œå…¶ä»–ç›¸å…³ä»»åŠ¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad7f88be737051fc7b2888b9eccfbabc" align="middle">
<img src="https://picx.zhimg.com/v2-c1dcf6a7113f12c2bafd81221d4de6b5" align="middle">
<img src="https://picx.zhimg.com/v2-e5e5a6f5b5c149f5c7d6ae0d68f1a600" align="middle">
<img src="https://picx.zhimg.com/v2-d43345bd1a17d621ff56bce358ba97fd" align="middle">
<img src="https://picx.zhimg.com/v2-e5b3fdbb35f12fbc7e294c31207e372b" align="middle">
<img src="https://picx.zhimg.com/v2-96c4707d614128e46831a55b6b58906b" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Core-Safety-Values-for-Provably-Corrigible-Agents"><a href="#Core-Safety-Values-for-Provably-Corrigible-Agents" class="headerlink" title="Core Safety Values for Provably Corrigible Agents"></a>Core Safety Values for Provably Corrigible Agents</h2><p><strong>Authors:Aran Nayebi</strong></p>
<p>We introduce the first complete formal solution to corrigibility in the off-switch game, with provable guarantees in multi-step, partially observed environments. Our framework consists of five <em>structurally separate</em> utility heads â€“ deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward â€“ combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is <em>learned</em> to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating <em>any</em> safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF&#x2F;RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits provably dominate even when incentives conflict. For settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon â€œdecidable islandâ€ where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åœ¨å…³é—­å¼€å…³æ¸¸æˆä¸­å¯¹å¯çº æ­£æ€§çš„é¦–ä¸ªå®Œæ•´å½¢å¼è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¤šæ­¥ã€éƒ¨åˆ†è§‚å¯Ÿç¯å¢ƒä¸­æä¾›äº†å¯è¯æ˜ä¿è¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±äº”ä¸ªç»“æ„ä¸Šåˆ†ç¦»çš„æ•ˆç”¨å¤´ç»„æˆâ€”â€”è°¦è®©ã€å¼€å…³è®¿é—®ä¿ç•™ã€çœŸå®æ€§ã€åŸºäºä¿¡ä»°çš„æ‰©å±•ä»¥è¾¾æˆæ•ˆç”¨ä¿ç•™çš„ä½å½±å“è¡Œä¸ºï¼Œä»¥åŠæœ‰ç•Œä»»åŠ¡å¥–åŠ±â€”â€”æŒ‰ä¸¥æ ¼æƒé‡å·®è·è¯å…¸æ’åˆ—ç»„åˆã€‚å®šç†1è¯æ˜äº†éƒ¨åˆ†å¯è§‚å¯Ÿå…³é—­å¼€å…³æ¸¸æˆä¸­çš„ç²¾ç¡®å•è½®å¯çº æ­£æ€§ï¼›å®šç†3å°†ä¿è¯èŒƒå›´æ‰©å±•åˆ°å¤šæ­¥è‡ªç¹æ®–æ™ºèƒ½ä½“ï¼Œè¡¨æ˜å³ä½¿æ¯ä¸ªæ™ºèƒ½ä½“å¤´éƒ½æ˜¯ä»¥å‡æ–¹è¯¯å·®Îµè¿›è¡Œå­¦ä¹ çš„ï¼Œè§„åˆ’å™¨çš„æœ€ä¼˜è§£è¯¯å·®ä¹Ÿæ˜¯Îµï¼Œä½†ä»èƒ½ä¿è¯ä¸è¿åä»»ä½•å®‰å…¨å±æ€§çš„æƒ…å†µä¸‹ï¼ŒåŒæ—¶ç¡®ä¿äººç±»å‡€æ”¶ç›Šã€‚ä¸åˆå¹¶æ‰€æœ‰è§„èŒƒä¸ºä¸€ä¸ªå­¦ä¹ æ ‡é‡çš„å®ªæ³•äººå·¥æ™ºèƒ½æˆ–RLHF&#x2F;RLAIFç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åˆ†ç¦»ä½¿å¾—å³ä½¿åœ¨æ¿€åŠ±å‘ç”Ÿå†²çªçš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è¯æ˜æœä»å’Œå½±å“åŠ›é™åˆ¶å ä¸»å¯¼åœ°ä½ã€‚å¯¹äºå¯ä»¥ä¿®æ”¹æ™ºèƒ½ä½“çš„å¯¹æ‰‹ç¯å¢ƒè®¾ç½®ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶å½’ç»“ä¸ºåœæœºé—®é¢˜æ¥è¯æ˜åˆ¤æ–­ä»»æ„è¢«é»‘å®¢æ”»å‡»åçš„æ™ºèƒ½ä½“æ˜¯å¦è¿åäº†å¯çº æ­£æ€§æ˜¯ä¸å¯åˆ¤å®šçš„ï¼Œç„¶ååˆ’å®šä¸€ä¸ªæœ‰é™çš„â€œå¯åˆ¤å®šå²›å±¿â€ï¼Œåœ¨è¿™é‡Œå®‰å…¨å¯ä»¥åœ¨éšæœºå¤šé¡¹å¼æ—¶é—´å†…å¾—åˆ°è®¤è¯ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¿æŠ¤éšç§ã€å¸¸è§„å®šæ—¶å½’é›¶è¯æ˜æ¥éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20964v2">PDF</a> 14 pages. To appear in AAAI 2026 Machine Ethics Workshop (W37) Proceedings</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é¦–ä¸ªå®Œæ•´è§£å†³åœ¨ç¦»çº¿å¼€å…³æ¸¸æˆä¸­å¯çº æ­£æ€§çš„æ­£å¼æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šæ­¥ã€éƒ¨åˆ†è§‚å¯Ÿç¯å¢ƒä¸­å…·æœ‰å¯è¯æ˜çš„ä¿è¯ã€‚æ¡†æ¶ç”±äº”ä¸ªç»“æ„ç‹¬ç«‹çš„æ•ˆç”¨å¤´ç»„æˆï¼ŒåŒ…æ‹¬è°¦è®©ã€å¼€å…³è®¿é—®ä¿ç•™ã€çœŸå®æ€§ã€åŸºäºä¿¡å¿µæ‰©å±•çš„å¯å®ç°æ•ˆç”¨ä¿ç•™çš„ä½å½±å“è¡Œä¸ºå’Œæœ‰é™ä»»åŠ¡å¥–åŠ±ï¼ŒæŒ‰ä¸¥æ ¼æƒé‡å·®è·å­—å…¸åºç»„åˆã€‚å®šç†1è¯æ˜äº†éƒ¨åˆ†å¯è§‚å¯Ÿç¦»çº¿å¼€å…³æ¸¸æˆä¸­çš„ç²¾ç¡®å•è½®å¯çº æ­£æ€§ï¼›å®šç†3å°†ä¿è¯æ‰©å±•åˆ°å¤šæ­¥è‡ªè¡ç”Ÿä»£ç†ï¼Œæ˜¾ç¤ºå³ä½¿æ¯ä¸ªå¤´ä»¥å‡æ–¹è¯¯å·®Îµå­¦ä¹ ï¼Œä¸”è§„åˆ’å™¨æ˜¯Îµæ¬¡ä¼˜çš„ï¼Œè¿åä»»ä½•å®‰å…¨å±æ€§çš„æ¦‚ç‡ä»ç„¶æœ‰ç•Œï¼ŒåŒæ—¶ç¡®ä¿äººç±»å‡€æ”¶ç›Šã€‚ä¸åˆå¹¶æ‰€æœ‰è§„èŒƒä¸ºä¸€ä¸ªå­¦ä¹ æ ‡é‡çš„å®ªæ³•äººå·¥æ™ºèƒ½æˆ–RLHF&#x2F;RLAIFç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åˆ†ç¦»ä½¿å¾—æœä»å’Œå½±å“é™åˆ¶åœ¨æ¿€åŠ±å†²çªæ—¶ä¹Ÿèƒ½å æ®ä¸»å¯¼åœ°ä½ã€‚åœ¨å¯¹æ‰‹å¯ä»¥ä¿®æ”¹ä»£ç†çš„åœºåˆä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ¤å®šä»»æ„åé»‘å®¢æ”»å‡»ä»£ç†æ˜¯å¦æ°¸è¿œè¿åå¯çº æ­£æ€§æ˜¯åœé—®é¢˜çš„ä¸å¯åˆ¤å®šç»“æœï¼Œç„¶ååˆ’å®šäº†ä¸€ä¸ªæœ‰é™çš„â€œå¯åˆ¤å®šå²›â€ï¼Œåœ¨æ­¤åŒºåŸŸå†…å®‰å…¨å¯ä»¥å¾—åˆ°éšæœºå¤šé¡¹å¼æ—¶é—´å†…çš„è®¤è¯å’Œå…·æœ‰éšç§ä¿æŠ¤æ€§çš„æ’å®šè½®æ¬¡é›¶çŸ¥è¯†è¯æ˜è¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é¦–ä¸ªå®Œæ•´è§£å†³ç¦»çº¿å¼€å…³æ¸¸æˆä¸­å¯çº æ­£æ€§çš„æ­£å¼æ–¹æ¡ˆï¼Œå¹¶åœ¨å¤šæ­¥ã€éƒ¨åˆ†è§‚å¯Ÿç¯å¢ƒä¸­å…·æœ‰å¯è¯æ˜çš„ä¿è¯ã€‚</li>
<li>é€šè¿‡äº”ä¸ªç»“æ„ç‹¬ç«‹çš„æ•ˆç”¨å¤´ç»„æˆæ¡†æ¶ï¼ŒåŒ…æ‹¬è°¦è®©ã€å¼€å…³è®¿é—®ä¿ç•™ç­‰ã€‚</li>
<li>é€šè¿‡å®šç†1å’Œå®šç†3ç¡®ä¿äº†ç²¾ç¡®å•è½®å’Œå¤šæ­¥çš„å¯çº æ­£æ€§ï¼Œå³ä½¿ä»£ç†è¢«å­¦ä¹ æˆ–è§„åˆ’å™¨å­˜åœ¨è¯¯å·®ï¼Œè¿åå®‰å…¨å±æ€§çš„æ¦‚ç‡ä»ç„¶æœ‰ç•Œã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ¿€åŠ±å†²çªæ—¶ä»èƒ½ä¿è¯æœä»å’Œå½±å“é™åˆ¶ã€‚</li>
<li>åœ¨å¯¹æ‰‹å¯ä»¥ä¿®æ”¹ä»£ç†çš„åœºåˆä¸‹ï¼Œåˆ¤å®šä»£ç†çš„å¯çº æ­£æ€§æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œç±»ä¼¼äºåœæœºé—®é¢˜ã€‚</li>
<li>åˆ’å®šäº†ä¸€ä¸ªæœ‰é™çš„â€œå¯åˆ¤å®šå²›â€ï¼Œåœ¨æ­¤åŒºåŸŸå†…å®‰å…¨çš„è®¤è¯å’ŒéªŒè¯æ˜¯å¯è¡Œçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c646a980350e449e0abc50b32c80a37" align="middle">
<img src="https://picx.zhimg.com/v2-69e3563d4a8893211670eab40f628588" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Assemble-Your-Crew-Automatic-Multi-agent-Communication-Topology-Design-via-Autoregressive-Graph-Generation"><a href="#Assemble-Your-Crew-Automatic-Multi-agent-Communication-Topology-Design-via-Autoregressive-Graph-Generation" class="headerlink" title="Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation"></a>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</h2><p><strong>Authors:Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan</strong></p>
<p>Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at <a target="_blank" rel="noopener" href="https://github.com/Shiy-Li/ARG-Designer">https://github.com/Shiy-Li/ARG-Designer</a>.</p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å·²ä½œä¸ºå¤„ç†è·¨ä¸åŒé¢†åŸŸçš„å¤æ‚é—®é¢˜çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚MASçš„æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºå…¶åä½œæ‹“æ‰‘ç»“æ„ï¼Œè¿™å·²æˆä¸ºè‡ªåŠ¨è®¾è®¡ç ”ç©¶çš„ä¸€ä¸ªé‡ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»æ ¹æœ¬ä¸Šå—åˆ°å…¶ä¾èµ–äºæ¨¡æ¿å›¾ä¿®æ”¹èŒƒå¼çš„çº¦æŸï¼Œå…·æœ‰é¢„å®šä¹‰çš„æ™ºèƒ½ä½“é›†åˆå’Œç¡¬ç¼–ç çš„äº¤äº’ç»“æ„ï¼Œè¿™å¯¹äºç‰¹å®šä»»åŠ¡çš„ç‰¹å®šè¦æ±‚é€‚åº”èƒ½åŠ›è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†MASè®¾è®¡ä¸ºä¸€ç§æ¡ä»¶è‡ªå›å½’å›¾ç”Ÿæˆä»»åŠ¡ï¼Œå…¶ä¸­ç³»ç»Ÿç»„æˆå’Œç»“æ„æ˜¯å…±åŒè®¾è®¡çš„ã€‚æˆ‘ä»¬æå‡ºäº†ARG-Designerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä»é›¶å¼€å§‹æ„å»ºåä½œå›¾æ¥å®ç°è¿™ä¸€èŒƒå¼ã€‚åŸºäºè‡ªç„¶è¯­è¨€ä»»åŠ¡æŸ¥è¯¢ï¼ŒARG-Designerå¯ä»¥é¡ºåºåœ°åŠ¨æ€åœ°ç¡®å®šæ‰€éœ€æ™ºèƒ½ä½“çš„æ•°é‡ï¼Œä»å¯æ‰©å±•çš„æ™ºèƒ½ä½“æ± ä¸­ä¸ºå®ƒä»¬é€‰æ‹©é€‚å½“çš„è§’è‰²å¹¶å»ºç«‹æœ€ä½³çš„é€šä¿¡é“¾æ¥ã€‚è¿™ç§ç”Ÿæˆå¼æ–¹æ³•ä¸ºä¸åŒçš„ä»»åŠ¡åˆ›å»ºå®šåˆ¶çš„æ‹“æ‰‘ç»“æ„ï¼Œä»¥çµæ´»å’Œå¯æ‰©å±•çš„æ–¹å¼ç²¾ç¡®åŒ¹é…ä¸åŒçš„éœ€æ±‚ã€‚åœ¨å…­ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒARG-Designerä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨ä»¤ç‰Œæ•ˆç‡å’Œæ‰©å±•æ€§æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚ARG-Designerçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Shiy-Li/ARG-Designer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Shiy-Li/ARG-Designerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18224v4">PDF</a> Accepted as an oral presentation by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå·²æˆä¸ºå¤„ç†è·¨åŸŸå¤æ‚é—®é¢˜çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚ç³»ç»Ÿåä½œæ‹“æ‰‘æ˜¯æ™ºèƒ½ä½“ç³»ç»Ÿçš„å…³é”®ï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºå°†æ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡é‡æ–°å®šä½ä¸ºæ¡ä»¶è‡ªå›å½’å›¾ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ARG-Designeræ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½æ ¹æ®è‡ªç„¶è¯­è¨€ä»»åŠ¡æŸ¥è¯¢åŠ¨æ€æ„å»ºåä½œå›¾ï¼Œç¡®å®šæ™ºèƒ½ä½“æ•°é‡ã€é€‰æ‹©è§’è‰²å¹¶å»ºç«‹æœ€ä¼˜é€šä¿¡é“¾æ¥ã€‚è¿™ç§æ–¹æ³•å¯çµæ´»ã€å¯æ‰©å±•åœ°åˆ›å»ºå®šåˆ¶åŒ–æ‹“æ‰‘ï¼Œé€‚åº”ä¸åŒä»»åŠ¡çš„ç‹¬ç‰¹éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒARG-Designeråœ¨å…­ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶å…·æœ‰é«˜æ•ˆçš„æ ‡è®°æ•ˆç‡å’Œå‡ºè‰²çš„å¯æ‰©å±•æ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Shiy-Li/ARG-Designer%E3%80%82">https://github.com/Shiy-Li/ARG-Designerã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>æ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ‹“æ‰‘æ˜¯æœ‰æ•ˆæ€§çš„å…³é”®ï¼Œç°æœ‰è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ARG-Designeræ¨¡å‹å°†æ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡é‡æ–°å®šä½ä¸ºæ¡ä»¶è‡ªå›å½’å›¾ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ARG-Designerèƒ½æ ¹æ®è‡ªç„¶è¯­è¨€ä»»åŠ¡æŸ¥è¯¢åŠ¨æ€æ„å»ºåä½œå›¾ï¼ŒåŒ…æ‹¬ç¡®å®šæ™ºèƒ½ä½“æ•°é‡ã€è§’è‰²å’Œé€šä¿¡é“¾æ¥ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯é€‚åº”ä¸åŒä»»åŠ¡çš„ç‹¬ç‰¹éœ€æ±‚ã€‚</li>
<li>ARG-Designeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œä¸”æ ‡è®°æ•ˆç‡é«˜ã€å¯æ‰©å±•æ€§å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13ab640fe2faaebec7bb2d24919f52b8" align="middle">
<img src="https://picx.zhimg.com/v2-816144718901bea45dba97edb291d021" align="middle">
<img src="https://picx.zhimg.com/v2-4084acd43ec21c3b5adf02a26ba6f222" align="middle">
<img src="https://picx.zhimg.com/v2-5e9eff5e465336f5b7584a6d607fe168" align="middle">
<img src="https://picx.zhimg.com/v2-b7655f91111804f78c9ff92dfcb29524" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Agent-SAMA-State-Aware-Mobile-Assistant"><a href="#Agent-SAMA-State-Aware-Mobile-Assistant" class="headerlink" title="Agent-SAMA: State-Aware Mobile Assistant"></a>Agent-SAMA: State-Aware Mobile Assistant</h2><p><strong>Authors:Linqiang Guo, Wei Liu, Yi Wen Heng,  Tse-Hsun,  Chen, Yang Wang</strong></p>
<p>Mobile Graphical User Interface (GUI) agents aim to autonomously complete tasks within or across apps based on user instructions. While recent Multimodal Large Language Models (MLLMs) enable these agents to interpret UI screens and perform actions, existing agents remain fundamentally reactive. They reason over the current UI screen but lack a structured representation of the app navigation flow, limiting GUI agentsâ€™ ability to understand execution context, detect unexpected execution results, and recover from errors. We introduce Agent-SAMA, a state-aware multi-agent framework that models app execution as a Finite State Machine (FSM), treating UI screens as states and user actions as transitions. Agent-SAMA implements four specialized agents that collaboratively construct and use FSMs in real time to guide task planning, execution verification, and recovery. We evaluate Agent-SAMA on two types of benchmarks: cross-app (Mobile-Eval-E, SPA-Bench) and mostly single-app (AndroidWorld). On Mobile-Eval-E, Agent-SAMA achieves an 84.0% success rate and a 71.9% recovery rate. On SPA-Bench, it reaches an 80.0% success rate with a 66.7% recovery rate. Compared to prior methods, Agent-SAMA improves task success by up to 12% and recovery success by 13.8%. On AndroidWorld, Agent-SAMA achieves a 63.7% success rate, outperforming the baselines. Our results demonstrate that structured state modeling enhances robustness and can serve as a lightweight, model-agnostic memory layer for future GUI agents.</p>
<blockquote>
<p>ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡ä»¤åœ¨åº”ç”¨ç¨‹åºå†…éƒ¨æˆ–è·¨åº”ç”¨ç¨‹åºè‡ªä¸»å®Œæˆä»»åŠ¡ã€‚è™½ç„¶æœ€è¿‘çš„å¤šåª’ä½“è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…è®¸è¿™äº›ä»£ç†è§£é‡ŠUIå±å¹•å¹¶æ‰§è¡Œæ“ä½œï¼Œä½†ç°æœ‰ä»£ç†ä»ç„¶åŸºæœ¬ä¸Šæ˜¯ååº”å¼çš„ã€‚å®ƒä»¬åœ¨å½“å‰çš„UIå±å¹•ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†ç¼ºä¹åº”ç”¨ç¨‹åºå¯¼èˆªæµçš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†GUIä»£ç†ç†è§£æ‰§è¡Œä¸Šä¸‹æ–‡ã€æ£€æµ‹æ„å¤–çš„æ‰§è¡Œç»“æœå¹¶ä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†Agent-SAMAï¼Œä¸€ä¸ªçŠ¶æ€æ„ŸçŸ¥çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†åº”ç”¨ç¨‹åºçš„æ‰§è¡Œå»ºæ¨¡ä¸ºæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ï¼Œå°†UIå±å¹•è§†ä¸ºçŠ¶æ€ï¼Œç”¨æˆ·æ“ä½œè§†ä¸ºè½¬æ¢ã€‚Agent-SAMAå®ç°äº†å››ä¸ªä¸“ä¸šä»£ç†ï¼Œå®ƒä»¬ååŒå®æ—¶æ„å»ºå’Œä½¿ç”¨FSMï¼Œä»¥æŒ‡å¯¼ä»»åŠ¡è§„åˆ’ã€æ‰§è¡ŒéªŒè¯å’Œæ¢å¤ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§ç±»å‹çš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†Agent-SAMAï¼šè·¨åº”ç”¨ç¨‹åºï¼ˆMobile-Eval-Eï¼ŒSPA-Benchï¼‰å’Œä¸»è¦æ˜¯å•åº”ç”¨ç¨‹åºï¼ˆAndroidWorldï¼‰ã€‚åœ¨Mobile-Eval-Eä¸Šï¼ŒAgent-SAMAè¾¾åˆ°äº†84.0%çš„æˆåŠŸç‡å’Œ71.9%çš„æ¢å¤ç‡ã€‚åœ¨SPA-Benchä¸Šï¼Œå…¶æˆåŠŸç‡è¾¾åˆ°80.0%ï¼Œæ¢å¤ç‡ä¸º66.7%ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒAgent-SAMAçš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†é«˜è¾¾12%ï¼Œæ¢å¤æˆåŠŸç‡æé«˜äº†13.8%ã€‚åœ¨AndroidWorldä¸Šï¼ŒAgent-SAMAè¾¾åˆ°äº†63.7%çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†åŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡æé«˜äº†ç¨³å¥æ€§ï¼Œå¹¶å¯ä»¥ä½œä¸ºæœªæ¥GUIä»£ç†çš„è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è®°å¿†å±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23596v3">PDF</a> Accepted to AAAI-26 (Main Technical Track)</p>
<p><strong>Summary</strong></p>
<p>ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ—¨åœ¨åŸºäºç”¨æˆ·æŒ‡ä»¤åœ¨åº”ç”¨ç¨‹åºå†…éƒ¨æˆ–è·¨åº”ç”¨ç¨‹åºè‡ªä¸»å®Œæˆä»»åŠ¡ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½¿è¿™äº›ä»£ç†èƒ½å¤Ÿè§£é‡ŠUIå±å¹•å¹¶æ‰§è¡Œæ“ä½œï¼Œä½†ç°æœ‰ä»£ç†æœ¬è´¨ä¸Šæ˜¯ååº”å¼çš„ã€‚å®ƒä»¬å¯¹å½“å‰çš„UIå±å¹•è¿›è¡Œæ¨ç†ï¼Œä½†ç¼ºä¹åº”ç”¨ç¨‹åºå¯¼èˆªæµçš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†GUIä»£ç†ç†è§£æ‰§è¡Œä¸Šä¸‹æ–‡ã€æ£€æµ‹æ„å¤–æ‰§è¡Œç»“æœå¹¶ä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Agent-SAMAï¼Œä¸€ä¸ªçŠ¶æ€æ„ŸçŸ¥çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†åº”ç”¨ç¨‹åºæ‰§è¡Œå»ºæ¨¡ä¸ºæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ï¼Œå°†UIå±å¹•è§†ä¸ºçŠ¶æ€ï¼Œç”¨æˆ·æ“ä½œè§†ä¸ºè½¬æ¢ã€‚Agent-SAMAå®ç°äº†å››ä¸ªä¸“ä¸šä»£ç†ï¼Œå®ƒä»¬ååŒå®æ—¶æ„å»ºå’Œä½¿ç”¨FSMï¼Œä»¥æŒ‡å¯¼ä»»åŠ¡è§„åˆ’ã€æ‰§è¡ŒéªŒè¯å’Œæ¢å¤ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒAgent-SAMAåœ¨è·¨åº”ç”¨ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒé«˜çš„æˆåŠŸç‡å’Œæ¢å¤ç‡ï¼Œä¸”åœ¨å•åº”ç”¨ä»»åŠ¡ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ï¼Œè¯æ˜ç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡èƒ½æé«˜ç¨³å¥æ€§ï¼Œå¹¶å¯ä½œä¸ºæœªæ¥GUIä»£ç†çš„è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è®°å¿†å±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨GUIä»£ç†è™½èƒ½åŸºäºç”¨æˆ·æŒ‡ä»¤å®Œæˆä»»åŠ¡ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸‹ç¼ºä¹åº”ç”¨å¯¼èˆªæµçš„ç»“æ„åŒ–è¡¨ç¤ºã€‚</li>
<li>Agent-SAMAé¦–æ¬¡å°†åº”ç”¨æ‰§è¡Œå»ºæ¨¡ä¸ºFSMï¼Œåˆ›æ–°æ€§åœ°ç®¡ç†UIå±å¹•å’Œç”¨æˆ·æ“ä½œã€‚</li>
<li>Agent-SAMAåŒ…å«å››ä¸ªä¸“ä¸šä»£ç†ï¼Œå®æ—¶æ„å»ºå’Œä½¿ç”¨FSMï¼Œæé«˜ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡ŒéªŒè¯çš„æ•ˆæœã€‚</li>
<li>Agent-SAMAåœ¨è·¨åº”ç”¨ä»»åŠ¡å’Œå•åº”ç”¨ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡å’Œæ¢å¤ç‡ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAgent-SAMAåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œæ¢å¤ç‡ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>ç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡èƒ½æé«˜GUIä»£ç†çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e54e970afec7d6482b33094bd3d9ee8" align="middle">
<img src="https://picx.zhimg.com/v2-accf16e3599b090f18001f73cefe2e64" align="middle">
<img src="https://picx.zhimg.com/v2-5bfa9e141dda833e648bf0bf23fb8f79" align="middle">
<img src="https://picx.zhimg.com/v2-5f0be0f2daae925b723e53b241a753b2" align="middle">
<img src="https://picx.zhimg.com/v2-0dbdf1c12788ab271412d59eee5ec770" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TongUI-Building-Generalized-GUI-Agents-by-Learning-from-Multimodal-Web-Tutorials"><a href="#TongUI-Building-Generalized-GUI-Agents-by-Learning-from-Multimodal-Web-Tutorials" class="headerlink" title="TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials"></a>TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials</h2><p><strong>Authors:Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, Qing Li</strong></p>
<p>Building Graphical User Interface (GUI) agents is a promising research direction, which simulates human interaction with computers or mobile phones to perform diverse GUI tasks. However, a major challenge in developing generalized GUI agents is the lack of sufficient trajectory data across various operating systems and applications, mainly due to the high cost of manual annotations. In this paper, we propose the TongUI framework that builds generalized GUI agents by learning from rich multimodal web tutorials. Concretely, we crawl and process online GUI tutorials (such as videos and articles) into GUI agent trajectory data, through which we produce the GUI-Net dataset containing 143K trajectory data across five operating systems and more than 200 applications. We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B&#x2F;7B models on GUI-Net, which show remarkable performance improvements on commonly used grounding and navigation benchmarks, outperforming baseline agents about 10% on multiple benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring the significance of our TongUI framework. We will fully open-source the code, the GUI-Net dataset, and the trained models soon.</p>
<blockquote>
<p>æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ˜¯ä¸€ä¸ªå……æ»¡å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œå®ƒæ¨¡æ‹Ÿäººä¸è®¡ç®—æœºæˆ–æ‰‹æœºä¹‹é—´çš„äº¤äº’ï¼Œä»¥æ‰§è¡Œå„ç§GUIä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨å¼€å‘é€šç”¨GUIä»£ç†æ—¶é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹è·¨å„ç§æ“ä½œç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºçš„è¶³å¤Ÿè½¨è¿¹æ•°æ®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ‰‹åŠ¨æ³¨é‡Šçš„æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TongUIæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»ä¸°å¯Œçš„å¤šåª’ä½“ç½‘ç»œæ•™ç¨‹ä¸­å­¦ä¹ æ¥æ„å»ºé€šç”¨çš„GUIä»£ç†ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬åœ¨çº¿çˆ¬å–å¹¶å¤„ç†GUIæ•™ç¨‹ï¼ˆå¦‚è§†é¢‘å’Œæ–‡ç« ï¼‰ï¼Œå°†å…¶è½¬åŒ–ä¸ºGUIä»£ç†è½¨è¿¹æ•°æ®ï¼Œä»è€Œç”ŸæˆåŒ…å«è·¨äº”ä¸ªæ“ä½œç³»ç»Ÿå’Œè¶…è¿‡200ä¸ªåº”ç”¨ç¨‹åºçš„14.3ä¸‡æ¡è½¨è¿¹æ•°æ®çš„GUI-Netæ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨GUI-Netå¯¹Qwen2.5-VL-3B&#x2F;7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥å¼€å‘TongUIä»£ç†ï¼Œåœ¨å¸¸ç”¨çš„å®šä½å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿ä»£ç†çº¦10%ï¼Œè¿™æ˜¾ç¤ºäº†GUI-Netæ•°æ®é›†çš„æœ‰æ•ˆæ€§å¹¶çªå‡ºäº†æˆ‘ä»¬TongUIæ¡†æ¶çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å°†å¾ˆå¿«å…¬å¼€æºä»£ç ã€GUI-Netæ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12679v3">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç½‘ç»œæ•™ç¨‹çš„é€šç”¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ„å»ºæ¡†æ¶TongUIï¼Œè§£å†³äº†æ„å»ºGUIä»£ç†é¢ä¸´çš„æ•°æ®è½¨è¿¹ç¼ºä¹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ åœ¨çº¿ä¸°å¯Œçš„å¤šåª’ä½“æ•™ç¨‹ï¼ˆå¦‚è§†é¢‘å’Œæ–‡ç« ï¼‰ç”ŸæˆGUIä»£ç†è½¨è¿¹æ•°æ®ï¼Œåˆ›å»ºäº†è·¨äº”ä¸ªæ“ä½œç³»ç»Ÿå’Œè¶…è¿‡200ä¸ªåº”ç”¨ç¨‹åºçš„GUI-Netæ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨GUI-Netå¾®è°ƒQwen2.5-VL-3B&#x2F;7Bæ¨¡å‹ï¼ŒTongUIä»£ç†åœ¨å¸¸ç”¨çš„å®šä½ä¸å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºåŸºçº¿ä»£ç†åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æå‡äº†çº¦10%ï¼Œè¯æ˜äº†GUI-Netæ•°æ®é›†çš„æœ‰æ•ˆæ€§ä»¥åŠTongUIæ¡†æ¶çš„é‡è¦æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†å¾ˆå¿«å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TongUIæ¡†æ¶é€šè¿‡å­¦ä¹ åœ¨çº¿å¤šåª’ä½“æ•™ç¨‹æ¥æ„å»ºé€šç”¨çš„GUIä»£ç†ï¼Œè§£å†³äº†è½¨è¿¹æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>TongUIé€šè¿‡çˆ¬å–å’ŒåŠ å·¥åœ¨çº¿GUIæ•™ç¨‹ï¼Œåˆ›å»ºäº†GUI-Netæ•°æ®é›†ï¼ŒåŒ…å«143Kè½¨è¿¹æ•°æ®ï¼Œè¦†ç›–äº”ä¸ªæ“ä½œç³»ç»Ÿå’Œè¶…è¿‡200ä¸ªåº”ç”¨ç¨‹åºã€‚</li>
<li>TongUIä»£ç†é€šè¿‡å¾®è°ƒQwen2.5-VL-3B&#x2F;7Bæ¨¡å‹åœ¨GUI-Netä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>TongUIä»£ç†åœ¨å®šä½ä¸å¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºåŸºçº¿ä»£ç†æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>GUI-Netæ•°æ®é›†å¯¹TongUIä»£ç†çš„æ€§èƒ½æå‡èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>TongUIæ¡†æ¶çš„é‡è¦æ€§åœ¨äºå…¶åˆ©ç”¨ä¸°å¯Œçš„ç½‘ç»œèµ„æºæ¥è§£å†³GUIä»£ç†å¼€å‘ä¸­çš„æ•°æ®è½¨è¿¹é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-931bc6c533afe62dce25917496b02380" align="middle">
<img src="https://picx.zhimg.com/v2-715089dbaa0bcdeff579365a426c7144" align="middle">
<img src="https://picx.zhimg.com/v2-accf000421e44bba4ae476d93111033c" align="middle">
<img src="https://picx.zhimg.com/v2-f6404e3080148601ad0493f92a67686a" align="middle">
<img src="https://picx.zhimg.com/v2-3e0f023b08512b6838c1a7edb1610f1a" align="middle">
<img src="https://picx.zhimg.com/v2-50fad536bc99a0a432e4710c63fd3fab" align="middle">
<img src="https://picx.zhimg.com/v2-55baf2289a3430b0abc41479f2dfafe4" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Adversarial-Agents-Black-Box-Evasion-Attacks-with-Reinforcement-Learning"><a href="#Adversarial-Agents-Black-Box-Evasion-Attacks-with-Reinforcement-Learning" class="headerlink" title="Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning"></a>Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning</h2><p><strong>Authors:Kyle Domico, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Eric Pauley, Josiah Hanna, Patrick McDaniel</strong></p>
<p>Attacks on machine learning models have been extensively studied through stateless optimization. In this paper, we demonstrate how a reinforcement learning (RL) agent can learn a new class of attack algorithms that generate adversarial samples. Unlike traditional adversarial machine learning (AML) methods that craft adversarial samples independently, our RL-based approach retains and exploits past attack experience to improve the effectiveness and efficiency of future attacks. We formulate adversarial sample generation as a Markov Decision Process and evaluate RLâ€™s ability to (a) learn effective and efficient attack strategies and (b) compete with state-of-the-art AML. On two image classification benchmarks, our agent increases attack success rate by up to 13.2% and decreases the average number of victim model queries per attack by up to 16.9% from the start to the end of training. In a head-to-head comparison with state-of-the-art image attacks, our approach enables an adversary to generate adversarial samples with 17% more success on unseen inputs post-training. From a security perspective, this work demonstrates a powerful new attack vector that uses RL to train agents that attack ML models efficiently and at scale.</p>
<blockquote>
<p>å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ”»å‡»å·²ç»é€šè¿‡æ— çŠ¶æ€ä¼˜åŒ–è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†å¦‚ä½•å­¦ä¹ ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„æ–°ç±»æ”»å‡»ç®—æ³•ã€‚ä¸ä¼ ç»Ÿçš„å¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ï¼ˆAMLï¼‰æ–¹æ³•ç‹¬ç«‹åˆ¶ä½œå¯¹æŠ—æ ·æœ¬ä¸åŒï¼Œæˆ‘ä»¬çš„åŸºäºRLçš„æ–¹æ³•ä¿ç•™å¹¶åˆ©ç”¨è¿‡å»çš„æ”»å‡»ç»éªŒï¼Œä»¥æé«˜æœªæ¥æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å°†å¯¹æŠ—æ ·æœ¬ç”Ÿæˆåˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶è¯„ä¼°RLï¼ˆaï¼‰å­¦ä¹ æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ”»å‡»ç­–ç•¥çš„èƒ½åŠ›ï¼Œï¼ˆbï¼‰ä¸æœ€å…ˆè¿›çš„AMLç«äº‰çš„èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªå›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æé«˜äº†æ”»å‡»æˆåŠŸç‡ï¼Œé«˜è¾¾13.2%ï¼Œå¹¶å‡å°‘äº†æ¯æ¬¡æ”»å‡»çš„å—å®³è€…æ¨¡å‹æŸ¥è¯¢æ¬¡æ•°ï¼Œé«˜è¾¾16.9%ã€‚åœ¨ä¸æœ€å…ˆè¿›çš„å›¾åƒæ”»å‡»çš„ç›´æ¥æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¯¹æ‰‹èƒ½å¤Ÿåœ¨è®­ç»ƒåç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ—¶ï¼Œåœ¨æœªè§è¿‡è¾“å…¥çš„æƒ…å†µä¸‹æé«˜æˆåŠŸç‡ä¸ºè®­ç»ƒå‰çš„çº¦ä¸¤å€ã€‚ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ä¸ªå¼ºå¤§çš„æ–°æ”»å‡»å‘é‡ï¼Œè¯¥å‘é‡ä½¿ç”¨RLè®­ç»ƒä»£ç†ä»¥å¤§è§„æ¨¡æœ‰æ•ˆåœ°æ”»å‡»æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01734v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœºå™¨å­¦ä¹ æ¨¡å‹å—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä»£ç†ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„æ”»å‡»ç®—æ³•å°šå±æ–°é¢–ã€‚ä¸ä¼ ç»Ÿç‹¬ç«‹ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•èƒ½å¤Ÿä¿ç•™å¹¶åˆ©ç”¨è¿‡å»çš„æ”»å‡»ç»éªŒï¼Œæé«˜æœªæ¥æ”»å‡»çš„æ•ˆç‡å’Œæ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç†å°†å¯¹æŠ—æ ·æœ¬ç”Ÿæˆè¿‡ç¨‹å½¢å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ”»å‡»æˆåŠŸç‡æé«˜äº†é«˜è¾¾13.2%ï¼Œä¸”è®­ç»ƒæœŸé—´æ¯æ¬¡æ”»å‡»æ‰€éœ€æŸ¥è¯¢æ¨¡å‹çš„å¹³å‡æ¬¡æ•°å‡å°‘äº†é«˜è¾¾16.9%ã€‚åœ¨å®‰å…¨é¢†åŸŸï¼Œè¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†è¿›è¡Œå¤§è§„æ¨¡æ”»å‡»æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼ºå¤§æ–°æ”»å‡»é€”å¾„ã€‚é€šè¿‡ä»£ç†ä¸å›¾åƒæ”»å‡»çš„äº¤é”‹ç«äº‰è¯æ˜ï¼Œå¼ºåŒ–å­¦ä¹ å¯¹ä¸å¯è§è¾“å…¥çš„å¯¹æŠ—æ ·æœ¬ç”ŸæˆæˆåŠŸç‡æé«˜äº†é«˜è¾¾17%ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨æ”»å‡»æœºå™¨å­¦ä¹ æ¨¡å‹æ–¹é¢çš„æ½œåœ¨å½±å“ï¼Œæå‡ºäº†é˜²å¾¡æ–°çš„å®‰å…¨éšæ‚£éœ€è¦åŠ å¼ºå¯¹æœªæ¥å¯¹æŠ—æ ·æœ¬çš„ç ”ç©¶åŠç­–ç•¥çš„ä¼˜åŒ–ç­‰ç ”ç©¶ç‚¹ã€‚è¿™ä¹Ÿæš—ç¤ºäº†å¯¹ä½¿ç”¨æœºå™¨å­¦ä¹ çš„ç³»ç»Ÿå®‰å…¨æ€§çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ å¯ç”¨äºç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„æ–°æ”»å‡»ç®—æ³•ã€‚</li>
<li>ä¸ä¼ ç»Ÿç‹¬ç«‹ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ”»å‡»ä¿ç•™äº†è¿‡å»çš„æ”»å‡»ç»éªŒä»¥æé«˜æœªæ¥çš„æ”»å‡»æ•ˆæœã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ”»å‡»æˆåŠŸç‡æé«˜æ˜¾è‘—ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬å¯¹ä¸å¯è§è¾“å…¥çš„æ”»å‡»æˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚è¿™æš—ç¤ºäº†å¯¹ä½¿ç”¨æœºå™¨å­¦ä¹ çš„ç³»ç»Ÿå®‰å…¨æ€§çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f08c9888522f8ade4aa04b5a5b6325a" align="middle">
<img src="https://picx.zhimg.com/v2-2661788b196722c0ba34303f2150beed" align="middle">
<img src="https://picx.zhimg.com/v2-b3808f0edb8ace1d7c98cc8d3a3af867" align="middle">
<img src="https://picx.zhimg.com/v2-cd174343986b8e109df7abaa02aa07f8" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Harnessing-Diverse-Perspectives-A-Multi-Agent-Framework-for-Enhanced-Error-Detection-in-Knowledge-Graphs"><a href="#Harnessing-Diverse-Perspectives-A-Multi-Agent-Framework-for-Enhanced-Error-Detection-in-Knowledge-Graphs" class="headerlink" title="Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs"></a>Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs</h2><p><strong>Authors:Yu Li, Yi Huang, Guilin Qi, Junlan Feng, Nan Hu, Songlin Zhai, Haohan Xue, Yongrui Chen, Ruoyan Shen, Tongtong Wu</strong></p>
<p>Knowledge graphs are widely used in industrial applications, making error detection crucial for ensuring the reliability of downstream applications. Existing error detection methods often fail to effectively utilize fine-grained subgraph information and rely solely on fixed graph structures, while also lacking transparency in their decision-making processes, which results in suboptimal detection performance. In this paper, we propose a novel Multi-Agent framework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple large language models (LLMs) in a collaborative setting. By concatenating fine-grained, bidirectional subgraph embeddings with LLM-based query embeddings during training, our framework integrates these representations to produce four specialized agents. These agents utilize subgraph information from different dimensions to engage in multi-round discussions, thereby improving error detection accuracy and ensuring a transparent decision-making process. Extensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms state-of-the-art methods, enhancing the accuracy and robustness of KG evaluation. For specific industrial scenarios, our framework can facilitate the training of specialized agents using domain-specific knowledge graphs for error detection, which highlights the potential industrial application value of our framework. Our code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/kse-ElEvEn/MAKGED">https://github.com/kse-ElEvEn/MAKGED</a>.</p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±åœ¨å·¥ä¸šåº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå› æ­¤é”™è¯¯æ£€æµ‹å¯¹äºç¡®ä¿ä¸‹æ¸¸åº”ç”¨çš„å¯é æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰çš„é”™è¯¯æ£€æµ‹æ–¹æ³•å¾€å¾€ä¸èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨ç»†ç²’åº¦å­å›¾ä¿¡æ¯ï¼Œä»…ä¾èµ–äºå›ºå®šçš„å›¾ç»“æ„ï¼ŒåŒæ—¶å…¶å†³ç­–è¿‡ç¨‹ç¼ºä¹é€æ˜åº¦ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºçŸ¥è¯†å›¾è°±é”™è¯¯æ£€æµ‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMAKGEDï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨åä½œè®¾ç½®ä¸­ä½¿ç”¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­å°†ç»†ç²’åº¦åŒå‘å­å›¾åµŒå…¥ä¸åŸºäºLLMçš„æŸ¥è¯¢åµŒå…¥è¿›è¡Œæ‹¼æ¥ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°†è¿™äº›è¡¨ç¤ºé›†æˆåœ¨ä¸€èµ·ï¼Œäº§ç”Ÿå››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“åˆ©ç”¨æ¥è‡ªä¸åŒç»´åº¦çš„å­å›¾ä¿¡æ¯è¿›è¡Œå¤šè½®è®¨è®ºï¼Œä»è€Œæé«˜é”™è¯¯æ£€æµ‹ç²¾åº¦ï¼Œå¹¶ç¡®ä¿å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ã€‚åœ¨FB15Kå’ŒWN18RRä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMAKGEDä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œæé«˜äº†çŸ¥è¯†å›¾è°±è¯„ä¼°çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å¯¹äºç‰¹å®šçš„å·¥ä¸šåœºæ™¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥åˆ©ç”¨é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±è®­ç»ƒä¸“ä¸šæ™ºèƒ½ä½“è¿›è¡Œé”™è¯¯æ£€æµ‹ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬æ¡†æ¶æ½œåœ¨çš„å·¥ä¸šåº”ç”¨ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kse-ElEvEn/MAKGED%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kse-ElEvEn/MAKGEDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15791v5">PDF</a> This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025 (Oral)</p>
<p><strong>Summary</strong><br>çŸ¥è¯†å›¾è°±åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„é”™è¯¯æ£€æµ‹è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹åˆ©ç”¨ç²¾ç»†å­å›¾ä¿¡æ¯ï¼Œä¾èµ–å›ºå®šå›¾ç»“æ„ï¼Œä¸”å†³ç­–è¿‡ç¨‹ä¸é€æ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“çš„çŸ¥è¯†å›¾è°±é”™è¯¯æ£€æµ‹æ¡†æ¶ï¼ˆMAKGEDï¼‰ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèå…¥å¤šè½®è®¨è®ºæœºåˆ¶ä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§å¹¶ç¡®ä¿å†³ç­–é€æ˜ã€‚å®éªŒè¯æ˜ï¼ŒMAKGEDåœ¨FB15Kå’ŒWN18RRæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ½œåœ¨å·¥ä¸šåº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>çŸ¥è¯†å›¾è°±åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„é”™è¯¯æ£€æµ‹éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹åˆ©ç”¨ç²¾ç»†å­å›¾ä¿¡æ¯ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚</li>
<li>MAKGEDæ¡†æ¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šè½®è®¨è®ºæœºåˆ¶æé«˜é”™è¯¯æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>MAKGEDæ¡†æ¶èƒ½ç¡®ä¿å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ã€‚</li>
<li>MAKGEDåœ¨FB15Kå’ŒWN18RRæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>MAKGEDæ¡†æ¶å…·æœ‰æ½œåœ¨å·¥ä¸šåº”ç”¨ä»·å€¼ï¼Œå¯é’ˆå¯¹ç‰¹å®šåœºæ™¯è®­ç»ƒä¸“ä¸šæ™ºèƒ½ä½“è¿›è¡Œé”™è¯¯æ£€æµ‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3ac2717a8030b8ccdfc43b37a0e5e01" align="middle">
<img src="https://picx.zhimg.com/v2-dbc4550f83b33cbd1fd158c3074d9dcd" align="middle">
<img src="https://picx.zhimg.com/v2-181e5799444d5ee4f721e50225ed38bf" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91d5d151243c2ca383ce8af602fe6102" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  A Viable Paradigm of Software Automation Iterative End-to-End Automated Software Development
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a9f0dbda5c3b8c4a573f07af5b919009" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  RescueLens LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
