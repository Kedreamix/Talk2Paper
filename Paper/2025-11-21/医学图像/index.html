<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  Joint Semantic-Channel Coding and Modulation for Token Communications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-45f2355056cac697533b61eee3320427')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-21-æ›´æ–°"><a href="#2025-11-21-æ›´æ–°" class="headerlink" title="2025-11-21 æ›´æ–°"></a>2025-11-21 æ›´æ–°</h1><h2 id="Joint-Semantic-Channel-Coding-and-Modulation-for-Token-Communications"><a href="#Joint-Semantic-Channel-Coding-and-Modulation-for-Token-Communications" class="headerlink" title="Joint Semantic-Channel Coding and Modulation for Token Communications"></a>Joint Semantic-Channel Coding and Modulation for Token Communications</h2><p><strong>Authors:Jingkai Ying, Zhijin Qin, Yulong Feng, Liejun Wang, Xiaoming Tao</strong></p>
<p>In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒTransformeræ¶æ„åœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œæ¨¡æ€ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»¤ç‰Œæ˜¯Transformeræ¨¡å‹ä¸­çš„ç»Ÿä¸€è¾“å…¥è¾“å‡ºè¡¨ç¤ºï¼Œå·²æˆä¸ºåŸºæœ¬çš„ä¿¡æ¯å•å…ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ä»¤ç‰Œé€šä¿¡é—®é¢˜ï¼Œç ”ç©¶å¦‚ä½•é«˜æ•ˆå¯é åœ°ä¼ è¾“ä»¤ç‰Œã€‚ç‚¹äº‘ä½œä¸ºä¸€ç§æµè¡Œçš„ä¸‰ç»´æ ¼å¼ï¼Œä¸å›¾åƒæˆ–è§†é¢‘ç›¸æ¯”ï¼Œå±•ç°å‡ºæ›´å¤æ‚çš„ç©ºé—´ç»“æ„ï¼Œè¢«é€‰ä¸ºä¿¡æ¯æ¥æºã€‚æˆ‘ä»¬åˆ©ç”¨é›†åˆæŠ½è±¡æ–¹æ³•è·å–ç‚¹ä»¤ç‰Œã€‚ä¸ºäº†åŸºäºä»¤ç‰Œè·å¾—æ›´å…·ä¿¡æ¯æ€§å’Œä¼ è¾“å‹å¥½çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬ä¸ºä»¤ç‰Œç¼–ç å™¨æå‡ºäº†ä¸€ç§è”åˆè¯­ä¹‰é€šé“å’Œè°ƒåˆ¶ï¼ˆJSCCMï¼‰æ–¹æ¡ˆï¼Œå°†ç‚¹ä»¤ç‰Œæ˜ å°„åˆ°æ ‡å‡†æ•°å­—æ˜Ÿåº§ç‚¹ï¼ˆè°ƒåˆ¶ä»¤ç‰Œï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒJSCCMåŒ…å«ä¸¤ä¸ªå¹¶è¡Œçš„åŸºäºPoint Transformerçš„ç¼–ç å™¨å’Œä¸€ç§å·®åˆ†è°ƒåˆ¶å™¨ï¼Œè¯¥è°ƒåˆ¶å™¨ç»“åˆäº†Gumel-softmaxå’Œè½¯é‡åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†é€Ÿç‡åˆ†é…å™¨å’Œé€šé“é€‚é…å™¨ï¼Œä¾¿äºæ ¹æ®è¯­ä¹‰ä¿¡æ¯å’Œé€šé“æ¡ä»¶è‡ªé€‚åº”ç”Ÿæˆé«˜è´¨é‡è°ƒåˆ¶ä»¤ç‰Œã€‚å¤§é‡ä»¿çœŸè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é‡å»ºä¸Šå®ç°äº†è¶…è¿‡1dBçš„å¢ç›Šï¼Œåœ¨è°ƒåˆ¶ç¬¦å·çš„å‹ç¼©æ¯”ä¸Šä¹Ÿè¾¾åˆ°äº†6å€ä»¥ä¸Šï¼Œè¶…è¿‡äº†è”åˆè¯­ä¹‰é€šé“ç¼–ç å’Œä¼ ç»Ÿå•ç‹¬ç¼–ç çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15699v1">PDF</a> 14 pages, 14 figures, 2 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨Transformeræ¶æ„ç ”ç©¶ç‚¹äº‘æ•°æ®çš„æ ‡è®°ï¼ˆtokenï¼‰é€šä¿¡é—®é¢˜ã€‚é€šè¿‡é›†åˆæŠ½è±¡æ–¹æ³•è·å–ç‚¹æ ‡è®°ï¼Œå¹¶æå‡ºè”åˆè¯­ä¹‰é€šé“å’Œè°ƒåˆ¶ï¼ˆJSCCMï¼‰æ–¹æ¡ˆï¼Œå°†ç‚¹æ ‡è®°æ˜ å°„ä¸ºæ ‡å‡†æ•°å­—æ˜Ÿåº§ç‚¹ï¼ˆè°ƒåˆ¶æ ‡è®°ï¼‰ã€‚è¯¥æ–¹æ¡ˆåŒ…æ‹¬ä¸¤ä¸ªå¹¶è¡ŒPoint Transformerç¼–ç å™¨ã€å·®åˆ†è°ƒåˆ¶å™¨ä»¥åŠé€Ÿç‡åˆ†é…å™¨å’Œé€šé“é€‚é…å™¨ï¼Œå¯è‡ªé€‚åº”ç”Ÿæˆé«˜è´¨é‡è°ƒåˆ¶æ ‡è®°ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºä¸Šä¼˜äºè”åˆè¯­ä¹‰é€šé“ç¼–ç å’Œä¼ ç»Ÿåˆ†ç¦»ç¼–ç ï¼Œè°ƒåˆ¶ç¬¦å·çš„å‹ç¼©æ¯”è¾¾åˆ°6å€ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨Transformeræ¶æ„è§£å†³ç‚¹äº‘æ•°æ®çš„æ ‡è®°é€šä¿¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡é›†åˆæŠ½è±¡æ–¹æ³•è·å–ç‚¹æ ‡è®°ä½œä¸ºåŸºæœ¬çš„ä¿¡æ¯å•å…ƒã€‚</li>
<li>æå‡ºJSCCMæ–¹æ¡ˆï¼Œå°†ç‚¹æ ‡è®°æ˜ å°„åˆ°æ ‡å‡†æ•°å­—æ˜Ÿåº§ç‚¹ï¼Œå®ç°æ›´å‹å¥½ä¼ è¾“ã€‚</li>
<li>JSCCMæ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªå¹¶è¡ŒPoint Transformerç¼–ç å™¨ã€å·®åˆ†è°ƒåˆ¶å™¨ã€‚</li>
<li>å¼€å‘é€Ÿç‡åˆ†é…å™¨å’Œé€šé“é€‚é…å™¨ï¼Œå®ç°è‡ªé€‚åº”ç”Ÿæˆé«˜è´¨é‡è°ƒåˆ¶æ ‡è®°ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœè¡¨æ˜æ‰€ææ–¹æ³•ä¼˜äºä¼ ç»Ÿç¼–ç æ–¹å¼ï¼Œåœ¨é‡å»ºä¸Šè·å¾—è¶…è¿‡1dBçš„å¢ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cef1d9792ac5835947c25fb1a76d4142" align="middle">
<img src="https://picx.zhimg.com/v2-c8decd2187ae487daf30876ebc51f0d3" align="middle">
<img src="https://picx.zhimg.com/v2-71cb2526c322e85e408b4dceb58e8c8a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Stage-Residual-Aware-Unsupervised-Deep-Learning-Framework-for-Consistent-Ultrasound-Strain-Elastography"><a href="#Multi-Stage-Residual-Aware-Unsupervised-Deep-Learning-Framework-for-Consistent-Ultrasound-Strain-Elastography" class="headerlink" title="Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography"></a>Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography</h2><p><strong>Authors:Shourov Joarder, Tushar Talukder Showrav, Md. Kamrul Hasan</strong></p>
<p>Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Netâ€™s outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.</p>
<blockquote>
<p>è¶…å£°åº”å˜å¼¹æ€§æˆåƒï¼ˆUSEï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„éä¾µå…¥æ€§æˆåƒæŠ€æœ¯ï¼Œç”¨äºè¯„ä¼°ç»„ç»‡çš„æœºæ¢°ç‰¹æ€§ï¼Œåœ¨å¤šç§ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰é‡è¦çš„è¯Šæ–­ä»·å€¼ã€‚ç„¶è€Œï¼Œå…¶ä¸´åºŠåº”ç”¨ä»å—åˆ°ç»„ç»‡å»ç›¸å…³å™ªå£°ã€çœŸå®æ•°æ®ç¼ºä¹å’Œä¸åŒå˜å½¢æ¡ä»¶ä¸‹çš„åº”å˜ä¼°è®¡ä¸ä¸€è‡´ç­‰é—®é¢˜çš„é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬æå‡ºäº†MUSSE-Netï¼Œè¿™æ˜¯ä¸€ä¸ªå‰©ä½™æ„ŸçŸ¥ã€å¤šé˜¶æ®µæ— ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç¨³å¥ä¸”ä¸€è‡´çš„åº”å˜ä¼°è®¡ã€‚å…¶æ ¸å¿ƒæ˜¯æˆ‘ä»¬æå‡ºçš„USSE-Netï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šå…ƒæµç¼–ç è§£ç å™¨æ¶æ„ï¼Œå¹¶è¡Œå¤„ç†é¢„å˜å½¢å’Œåå˜å½¢å°„é¢‘åºåˆ—ä»¥ä¼°è®¡ä½ç§»åœºå’Œè½´å‘åº”å˜ã€‚è¯¥æ–°å‹æ¶æ„ç»“åˆäº†åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥äº’è¡¥ç‰¹å¾èåˆçš„ç¼–ç å™¨ä¸å¸¦æœ‰ä¸‰äº¤å‰æ³¨æ„åŠ›ï¼ˆTCAï¼‰ç“¶é¢ˆçš„äº¤å‰æ³¨æ„åŠ›èåˆï¼ˆCAFï¼‰åºè´¯è§£ç å™¨ã€‚ä¸ºäº†ç¡®ä¿ä¸åŒå˜å½¢æ°´å¹³çš„æ—¶ç©ºä¸€è‡´æ€§å’Œåº”å˜ç¨³å®šæ€§ï¼Œè¯¥æ¶æ„åˆ©ç”¨å®šåˆ¶çš„ä¸€è‡´æ€§æŸå¤±ã€‚æœ€åï¼Œé€šè¿‡MUSSE-Netæ¡†æ¶ï¼ŒäºŒæ¬¡æ®‹å·®ç»†åŒ–é˜¶æ®µè¿›ä¸€æ­¥æé«˜äº†å‡†ç¡®æ€§å¹¶æŠ‘åˆ¶äº†å™ªå£°ã€‚åœ¨æ¨¡æ‹Ÿã€ä½“å†…ä»¥åŠæ¥è‡ªå­ŸåŠ æ‹‰å›½å·¥ç¨‹æŠ€æœ¯å¤§å­¦çš„ç§äººä¸´åºŠæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒMUSSE-Netè¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šï¼ŒMUSSE-Netè¾¾åˆ°äº†ç›®æ ‡ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¸º24.54ã€èƒŒæ™¯ä¿¡å™ªæ¯”ä¸º132.76ã€å¯¹æ¯”åº¦å™ªå£°æ¯”ï¼ˆCNRï¼‰ä¸º59.81å’Œå¼¹æ€§æˆåƒä¿¡å™ªæ¯”ä¸º9.73çš„å…ˆè¿›æ°´å¹³æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨BUETæ•°æ®é›†ä¸Šï¼ŒMUSSE-Netç”Ÿæˆçš„åº”å˜å›¾å…·æœ‰å¢å¼ºçš„ç—…å˜ä¸èƒŒæ™¯å¯¹æ¯”åº¦ä»¥åŠæ˜¾è‘—å™ªå£°æŠ‘åˆ¶ï¼Œäº§ç”Ÿå¯ä¸´åºŠè§£é‡Šçš„åº”å˜æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15640v1">PDF</a> 13 pages, 9 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¶…å£°åº”å˜å¼¹æ€§æˆåƒï¼ˆUSEï¼‰æ˜¯ä¸€ç§è¯„ä¼°ç»„ç»‡æœºæ¢°ç‰¹æ€§çš„å¼ºå¤§æ— åˆ›æˆåƒæŠ€æœ¯ï¼Œåœ¨å¤šç§ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰é‡è¦è¯Šæ–­ä»·å€¼ã€‚ç„¶è€Œï¼Œå…¶ä¸´åºŠåº”ç”¨ä»å—é™äºç»„ç»‡å»ç›¸å…³å™ªå£°ã€åœ°é¢çœŸå®æƒ…å†µçš„ç¨€ç¼ºæ€§ä»¥åŠåœ¨ä¸åŒå˜å½¢æ¡ä»¶ä¸‹çš„åº”å˜ä¼°è®¡ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬æå‡ºäº†MUSSE-Netï¼Œè¿™æ˜¯ä¸€ä¸ªæ®‹ä½™æ„ŸçŸ¥ã€å¤šé˜¶æ®µçš„æ— äººç›‘ç£çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç¨³å¥ä¸”ä¸€è‡´çš„åº”å˜ä¼°è®¡ã€‚å…¶æ ¸å¿ƒåœ¨äºæˆ‘ä»¬æå‡ºçš„USSE-Netï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå®ƒå¹¶è¡Œå¤„ç†é¢„å˜å½¢å’Œåå˜å½¢çš„å°„é¢‘åºåˆ—ä»¥ä¼°è®¡ä½ç§»åœºå’Œè½´å‘åº”å˜ã€‚è¯¥æ–°é¢–æ¶æ„ç»“åˆäº†åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥äº’è¡¥ç‰¹å¾èåˆçš„ç¼–ç å™¨ä¸å¸¦æœ‰ä¸‰é‡äº¤å‰æ³¨æ„åŠ›ç“¶é¢ˆçš„äº¤å‰æ³¨æ„åŠ›èåˆåŸºäºåºåˆ—çš„è§£ç å™¨ã€‚ä¸ºäº†ç¡®ä¿åœ¨ä¸åŒå˜å½¢æ°´å¹³ä¸Šçš„æ—¶é—´è¿è´¯æ€§å’Œåº”å˜ç¨³å®šæ€§ï¼Œè¯¥æ¶æ„åˆ©ç”¨å®šåˆ¶çš„è¿è´¯æ€§æŸå¤±ã€‚æœ€åï¼Œé€šè¿‡MUSSE-Netæ¡†æ¶ï¼ŒäºŒæ¬¡æ®‹å·®ç»†åŒ–é˜¶æ®µè¿›ä¸€æ­¥æé«˜äº†å‡†ç¡®æ€§å¹¶æŠ‘åˆ¶äº†å™ªå£°ã€‚åœ¨æ¨¡æ‹Ÿã€ä½“å†…ä»¥åŠæ¥è‡ªå­ŸåŠ æ‹‰å›½å·¥ç¨‹æŠ€æœ¯å¤§å­¦çš„ç§äººä¸´åºŠæ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯è¡¨æ˜ï¼ŒMUSSE-Netè¶…è¶Šäº†ç°æœ‰çš„æ— äººç›‘ç£æ–¹æ³•ã€‚åœ¨ç›®æ ‡ä¿¡å™ªæ¯”ã€èƒŒæ™¯ä¿¡å™ªæ¯”ã€å¯¹æ¯”åº¦å™ªå£°æ¯”å’Œå¼¹æ€§å›¾åƒä¿¡å™ªæ¯”ç­‰æŒ‡æ ‡ä¸Šï¼ŒMUSSE-Netè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨BUETæ•°æ®é›†ä¸Šï¼ŒMUSSE-Netäº§ç”Ÿçš„åº”å˜å›¾å…·æœ‰å¢å¼ºçš„ç—…ç¶ä¸èƒŒæ™¯å¯¹æ¯”åº¦ï¼Œå¹¶æ˜¾è‘—æŠ‘åˆ¶äº†å™ªå£°ï¼Œäº§ç”Ÿäº†å¯ä¸´åºŠè§£é‡Šçš„åº”å˜æ¨¡å¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Ultrasound Strain Elastography (USE) æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°ç»„ç»‡æœºæ¢°ç‰¹æ€§çš„é‡è¦éä¾µå…¥æ€§æˆåƒæŠ€æœ¯ã€‚</li>
<li>USEåœ¨ä¸´åºŠåº”ç”¨ä¸Šé¢ä¸´å™ªå£°ã€ç¼ºä¹çœŸå®å‚ç…§å’Œåº”å˜ä¼°è®¡ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„MUSSE-Netæ¡†æ¶æ˜¯ä¸€ç§å¤šé˜¶æ®µã€åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°ç¨³å¥å’Œä¸€è‡´çš„åº”å˜ä¼°è®¡ã€‚</li>
<li>USSE-Netä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œé€šè¿‡å¹¶è¡Œå¤„ç†é¢„å˜å½¢å’Œåå˜å½¢çš„å°„é¢‘åºåˆ—æ¥ä¼°è®¡ä½ç§»åœºå’Œè½´å‘åº”å˜ã€‚</li>
<li>MUSSE-Netä½¿ç”¨å®šåˆ¶çš„è¿è´¯æ€§æŸå¤±ç¡®ä¿åœ¨ä¸åŒå˜å½¢æ°´å¹³ä¸Šçš„åº”å˜ç¨³å®šæ€§ã€‚</li>
<li>äºŒæ¬¡æ®‹å·®ç»†åŒ–é˜¶æ®µæé«˜äº†å‡†ç¡®æ€§å’Œå™ªå£°æŠ‘åˆ¶æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2de5f52744429e7b2afbb05069c384a8" align="middle">
<img src="https://picx.zhimg.com/v2-8d4a2af3df957732d60e2110299c10ae" align="middle">
<img src="https://picx.zhimg.com/v2-3199585a9ccd87ad7ebd8bfc6959e14d" align="middle">
<img src="https://picx.zhimg.com/v2-2862ee33d2f5c7ad3e20e3f080f2285d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MaskMed-Decoupled-Mask-and-Class-Prediction-for-Medical-Image-Segmentation"><a href="#MaskMed-Decoupled-Mask-and-Class-Prediction-for-Medical-Image-Segmentation" class="headerlink" title="MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation"></a>MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation</h2><p><strong>Authors:Bin Xie, Gady Agam</strong></p>
<p>Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é€šå¸¸é‡‡ç”¨é€ç‚¹å·ç§¯åˆ†å‰²å¤´æ¥é¢„æµ‹å¯†é›†æ ‡ç­¾ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å‡ºé€šé“éƒ½å¯å‘å¼åœ°ä¸ç‰¹å®šç±»åˆ«ç›¸å…³è”ã€‚è¿™ç§åˆšæ€§çš„è®¾è®¡æ—¢é™åˆ¶äº†ç‰¹å¾å…±äº«ä¹Ÿé™åˆ¶äº†è¯­ä¹‰æ³›åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§£è€¦åˆ†å‰²å¤´ï¼Œåˆ©ç”¨å…±äº«å¯¹è±¡æŸ¥è¯¢å°†å¤šç±»åˆ«é¢„æµ‹åˆ†ä¸ºä¸ç±»åˆ«æ— å…³çš„æ©è†œé¢„æµ‹å’Œç±»åˆ«æ ‡ç­¾é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨å°ºåº¦å¯å˜å½¢Transformeræ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ä½åˆ†è¾¨ç‡ç¼–ç å™¨ç‰¹å¾èƒ½å¤Ÿé€šè¿‡å¯å˜å½¢æ³¨æ„åŠ›å…³æ³¨å…¨åˆ†è¾¨ç‡ç¼–ç å™¨ç‰¹å¾ï¼Œå®ç°å†…å­˜é«˜æ•ˆä¸”ç©ºé—´å¯¹é½çš„å…¨å°ºåº¦èåˆã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åä¸ºMaskMedï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨AMOS 2022ä¸Šæ¯”nnUNeté«˜å‡º+2.0%çš„Diceç³»æ•°ï¼Œåœ¨BTCVä¸Šé«˜å‡º+6.9%çš„Diceç³»æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15603v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§£è€¦åˆ†å‰²å¤´ï¼Œé€šè¿‡å°†å¤šç±»é¢„æµ‹åˆ†ç¦»ä¸ºç±»æ— å…³çš„æ©è†œé¢„æµ‹å’Œç±»æ ‡ç­¾é¢„æµ‹ï¼Œåˆ©ç”¨å…±äº«å¯¹è±¡æŸ¥è¯¢æ¥å®ç°ã€‚åŒæ—¶å¼•å…¥äº†å…¨å°ºåº¦æ„ŸçŸ¥å¯å˜å½¢è½¬æ¢å™¨æ¨¡å—ï¼Œä½¿ä½åˆ†è¾¨ç‡ç¼–ç å™¨ç‰¹å¾èƒ½å¤Ÿé€šè¿‡å¯å˜å½¢æ³¨æ„åŠ›å…³æ³¨å…¨åˆ†è¾¨ç‡ç¼–ç å™¨ç‰¹å¾ï¼Œå®ç°å†…å­˜é«˜æ•ˆä¸”ç©ºé—´å¯¹é½çš„å…¨å°ºåº¦èåˆã€‚æ‰€æå‡ºçš„æ–¹æ³•MaskMedåœ¨AMOS 2022å’ŒBTCVä¸Šåˆ†åˆ«è¶…è¶Šäº†nnUNetï¼ŒDiceå¾—åˆ†æé«˜äº†+2.0%å’Œ+6.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿä¸€è§£è€¦åˆ†å‰²å¤´ï¼Œç”¨äºæ”¹è¿›ä¼ ç»Ÿçš„ç‚¹å·ç§¯åˆ†å‰²å¤´æ–¹æ³•ã€‚è¿™ç§æ–°æ–¹æ³•é€šè¿‡å°†å¤šç±»é¢„æµ‹åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤æ¥ä¼˜åŒ–ç‰¹å¾å…±äº«å’Œè¯­ä¹‰æ³›åŒ–ã€‚</li>
<li>é€šè¿‡å¼•å…¥å…±äº«å¯¹è±¡æŸ¥è¯¢ï¼Œå®ç°äº†ç±»æ— å…³çš„æ©è†œé¢„æµ‹å’Œç±»æ ‡ç­¾é¢„æµ‹çš„åˆ†ç¦»ã€‚è¿™æœ‰åŠ©äºç®€åŒ–æ¨¡å‹å¤æ‚æ€§å¹¶æé«˜å…¶çµæ´»æ€§ã€‚</li>
<li>å¼•å…¥äº†å…¨å°ºåº¦æ„ŸçŸ¥å¯å˜å½¢è½¬æ¢å™¨æ¨¡å—ï¼Œå®ç°äº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡ç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆèåˆã€‚è¿™æœ‰åŠ©äºæ•æ‰å›¾åƒä¸­çš„ç»†å¾®ç»†èŠ‚å¹¶ä¿æŒå†…å­˜é«˜æ•ˆã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•MaskMedåœ¨å…¨å°ºåº¦çš„ç©ºé—´å’Œå°ºåº¦ä¸Šçš„åŠ¨æ€ç‰¹æ€§å’Œäº¤äº’æ€§ä½¿å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c7f0ab8683672a16156529c61aec59e" align="middle">
<img src="https://picx.zhimg.com/v2-90bb6db371bc76992aa45246a61eaae5" align="middle">
<img src="https://picx.zhimg.com/v2-61c3738b8216048504698ec5785e0ada" align="middle">
<img src="https://picx.zhimg.com/v2-54ccc38dcfcd655cf3d1647c68422000" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="US-X-Complete-A-Multi-Modal-Approach-to-Anatomical-3D-Shape-Recovery"><a href="#US-X-Complete-A-Multi-Modal-Approach-to-Anatomical-3D-Shape-Recovery" class="headerlink" title="US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery"></a>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery</h2><p><strong>Authors:Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour</strong></p>
<p>Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p &lt; 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasoundâ€™s key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/miruna20/US-X-Complete">https://github.com/miruna20/US-X-Complete</a></p>
<blockquote>
<p>è¶…å£°æä¾›äº†ä¸€ç§æ— è¾å°„ã€æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå®æ—¶å¯è§†åŒ–è„ŠæŸ±æ ‡å¿—ã€æ—è„ŠæŸ±è½¯ç»„ç»‡å’Œç¥ç»è¡€ç®¡ç»“æ„ï¼Œå¯¹äºè„ŠæŸ±æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„æœ¯ä¸­æŒ‡å¯¼éå¸¸æœ‰ä»·å€¼ã€‚ç„¶è€Œï¼Œè¶…å£°åœ¨å¯è§†åŒ–å®Œæ•´æ¤ä½“è§£å‰–ç»“æ„æ–¹é¢å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ˜¾ç¤ºæ¤ä½“æ–¹é¢ï¼Œç”±äºéª¨éª¼å¼•èµ·çš„å£°å½±æ•ˆåº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å•å¼ Xå°„çº¿å›¾åƒçš„è¡¥å……ä¿¡æ¯æ¥å®Œæˆ3Dè¶…å£°ä¸­çš„é®æŒ¡è§£å‰–ç»“æ„ã€‚ä¸ºäº†è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é…å¯¹è®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ¨¡æ‹ŸXå°„çº¿æ‰«æçš„2Dä¾§ä½æ¤ä½“è§†å›¾ï¼›ï¼ˆ2ï¼‰æ¨¡æ‹Ÿè¶…å£°è„ŠæŸ±æˆåƒä¸­é‡åˆ°çš„æœ‰é™å¯è§æ€§å’Œé®æŒ¡çš„3Déƒ¨åˆ†æ¤ä½“è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†ä¸¤ç§æˆåƒæ¨¡å¼çš„å½¢æ€ä¿¡æ¯ï¼Œä¸3Dè¶…å£°æ¤ä½“å®Œæˆçš„å‰æ²¿æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨æ¤ä½“é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ˆp &lt; 0.001ï¼‰ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹»å½±ç ”ç©¶ï¼Œä½œä¸ºæœªæ¥ä¸´åºŠè½¬åŒ–çš„åˆæ­¥æ­¥éª¤ï¼Œå¹¶åœ¨æ— éœ€ä¸æœ¯å‰æ¨¡å¼ï¼ˆå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼‰é…å‡†çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ›´ç²¾ç¡®ã€å®Œæ•´çš„è…°æ¤è¶…å£°æ‰«æä¸Šçš„ä½“ç§¯å¯è§†åŒ–ã€‚è¿™è¯æ˜ï¼Œæ•´åˆå•å¼ Xå°„çº¿æŠ•å½±å¯ä»¥ç¼“è§£è¶…å£°çš„ä¸»è¦å±€é™æ€§ï¼ŒåŒæ—¶ä¿ç•™å…¶ä½œä¸ºä¸»è¦æˆåƒæ¨¡å¼çš„ä¼˜åŠ¿ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/miruna20/US-X-Complete%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/miruna20/US-X-Completeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15600v1">PDF</a> Accepted at the Workshop on Shape in Medical Imaging at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç»“åˆè¶…å£°å’ŒXå°„çº¿å›¾åƒè¿›è¡Œä¸‰ç»´æ¤ä½“å®Œæ•´åŒ–æ˜¾ç¤ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†è¶…å£°æˆåƒåœ¨æ˜¾ç¤ºå®Œæ•´æ¤ä½“è§£å‰–ç»“æ„æ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡åˆ©ç”¨Xå°„çº¿å›¾åƒçš„è¡¥å……ä¿¡æ¯ï¼Œå®ç°äº†å¯¹é®æŒ¡çš„è§£å‰–ç»“æ„çš„é‡å»ºã€‚é€šè¿‡ç”Ÿæˆé…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ¨¡æ‹ŸXå°„çº¿æ‰«æçš„äºŒç»´ä¾§é¢æ¤ä½“è§†å›¾å’Œæ¨¡æ‹Ÿè¶…å£°æˆåƒä¸­çš„æœ‰é™å¯è§æ€§å’Œé®æŒ¡é—®é¢˜çš„ä¸‰ç»´éƒ¨åˆ†æ¤ä½“è¡¨ç¤ºï¼Œæé«˜äº†æ¤ä½“é‡å»ºçš„å‡†ç¡®åº¦ã€‚æ­¤æ–¹æ³•èƒ½æ›´å‡†ç¡®ã€å®Œæ•´åœ°æ˜¾ç¤ºè…°æ¤è„ŠæŸ±çš„è¶…å£°æ‰«æå›¾åƒï¼Œæ— éœ€ä¸æœ¯å‰æ¨¡æ€ï¼ˆå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼‰è¿›è¡Œé…å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç»“åˆè¶…å£°å’ŒXå°„çº¿å›¾åƒè¿›è¡Œä¸‰ç»´æ¤ä½“å®Œæ•´åŒ–æ˜¾ç¤ºã€‚</li>
<li>é€šè¿‡ç”Ÿæˆé…å¯¹è®­ç»ƒæ•°æ®æ¨¡æ‹ŸXå°„çº¿æ‰«æçš„äºŒç»´ä¾§é¢æ¤ä½“è§†å›¾å’Œè¶…å£°æˆåƒä¸­çš„é®æŒ¡é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•æ•´åˆäº†ä¸¤ç§æˆåƒæ¨¡å¼çš„å½¢æ€ä¿¡æ¯ï¼Œæé«˜äº†æ¤ä½“é‡å»ºçš„å‡†ç¡®åº¦ã€‚</li>
<li>è¯¥æŠ€æœ¯å…‹æœäº†è¶…å£°æˆåƒåœ¨æ˜¾ç¤ºå®Œæ•´æ¤ä½“è§£å‰–ç»“æ„æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>æ–¹æ³•é€šè¿‡ç»“åˆå•å¼ Xå°„çº¿æŠ•å½±å®ç°äº†æ›´å‡†ç¡®ã€å®Œæ•´çš„è…°æ¤è„ŠæŸ±å¯è§†åŒ–ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¿ç•™äº†è¶…å£°æˆåƒä½œä¸ºä¸»è¦æˆåƒæ¨¡æ€çš„ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0faae9e0170e01711099ff70b6cb5b9" align="middle">
<img src="https://picx.zhimg.com/v2-db5e5830d9dce91cdc38b993d12a4769" align="middle">
<img src="https://picx.zhimg.com/v2-709b3f305b195dec59a7579194a53d37" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Excess-of-diffuse-gamma-ray-emission-detected-from-the-galaxy-cluster-Abell-119-from-14-year-Fermi-LAT-Data"><a href="#Excess-of-diffuse-gamma-ray-emission-detected-from-the-galaxy-cluster-Abell-119-from-14-year-Fermi-LAT-Data" class="headerlink" title="Excess of diffuse gamma-ray emission detected from the galaxy cluster Abell 119 from 14-year Fermi-LAT Data"></a>Excess of diffuse gamma-ray emission detected from the galaxy cluster Abell 119 from 14-year Fermi-LAT Data</h2><p><strong>Authors:Gajanan D Harale, Surajit Paul</strong></p>
<p>Galaxy clusters are among the most massive gravitationally bound systems in the Universe and are considered major reservoirs of high-energy cosmic rays, yet no conclusive $Î³$-ray detection from them has been achieved. This non-detection may stem from limited sensitivity and source localization of current $Î³$-ray instruments, as well as strong interactions of $Î³$-rays with intervening material that restrict detectable signals to only a few nearby and dynamically active clusters. Motivated by these constraints, we selected a sample of nearby ($z&lt;0.05$) merging clusters and analyzed 14 years of \textit{Fermi}-LAT data. In this work, we present a detailed study of Abell 119 (A119), a merging cluster with significant X-ray luminosity and complex dynamics. Using \textit{Fermipy} and \textit{Fermi} Science Tools, we modeled all potential $Î³$-ray sources and confirmed the 4FGL point sources 4FGL J0059.3$-$0152, 4FGL J0101.0$-$0059, and 4FGL J0059.2+0006 with significant TS values.s. It further reveals, a $\sim4Ïƒ$ excess of diffuse $Î³$-ray emission offset by $\sim0.25^\circ$ from the cluster center, plausibly associated with the cluster halo. An extended model provides the best fit, yielding luminosity bounds of $\sim 12.21^{+2.74}<em>{-3.95}\times10^{42},\mathrm{erg,s^{-1}}$ and a particle spectral index of $\sim2.25^{+0.38}</em>{-0.13}$, consistent with earlier expectations for cluster-scale non-thermal emission. These results suggest a hadronic origin for the detected signal. Although the $\sim4Ïƒ$ excess is compelling, uncertainties in localization and instrumental limitations prevent a definitive detection. Nonetheless, the results highlight the potential for deeper cluster studies, and the estimated neutrino flux $E^{2}Ï†_Î½\approx3\times10^{-10},\mathrm{GeV,cm^{-2},s^{-1},sr^{-1}}$ motivates future observations with upcoming neutrino telescopes.</p>
<blockquote>
<p>æ˜Ÿç³»å›¢æ˜¯å®‡å®™ä¸­æœ€å¤§çš„å¼•åŠ›æŸç¼šç³»ç»Ÿä¹‹ä¸€ï¼Œè¢«è®¤ä¸ºæ˜¯é«˜èƒ½å®‡å®™å°„çº¿çš„ä¸»è¦å‚¨å­˜åº“ï¼Œç„¶è€Œï¼Œè‡³ä»Šå°šæœªå®ç°å¯¹å®ƒä»¬çš„ç¡®å®šæ€§Î³å°„çº¿æ£€æµ‹ã€‚è¿™ç§éæ£€æµ‹å¯èƒ½æ˜¯ç”±äºå½“å‰Î³å°„çº¿ä»ªå™¨çš„çµæ•åº¦æœ‰é™å’Œæºå®šä½ä¸å‡†ç¡®ï¼Œä»¥åŠÎ³å°„çº¿ä¸ä¸­é—´ç‰©è´¨çš„å¼ºçƒˆç›¸äº’ä½œç”¨ï¼Œä½¿å¾—å¯æ£€æµ‹åˆ°çš„ä¿¡å·ä»…é™äºå°‘æ•°é™„è¿‘çš„åŠ¨æ€æ´»è·ƒæ˜Ÿç³»å›¢ã€‚åŸºäºè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªé‚»è¿‘ï¼ˆz&lt;0.05ï¼‰çš„åˆå¹¶æ˜Ÿç³»å›¢æ ·æœ¬ï¼Œå¹¶åˆ†æäº†é•¿è¾¾14å¹´çš„è´¹ç±³LATæ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å…·æœ‰æ˜¾è‘—Xå°„çº¿å‘å…‰åº¦å’Œå¤æ‚åŠ¨æ€çš„åˆå¹¶æ˜Ÿç³»å›¢é˜¿è´å°”119ï¼ˆA119ï¼‰è¿›è¡Œäº†è¯¦ç»†ç ”ç©¶ã€‚ä½¿ç”¨è´¹ç±³çš®å’Œè´¹ç±³ç§‘å­¦å·¥å…·ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ½œåœ¨çš„Î³å°„çº¿æºè¿›è¡Œäº†å»ºæ¨¡ï¼Œå¹¶ç¡®è®¤äº†å…·æœ‰æ˜¾è‘—TSå€¼çš„å››ä¸ªFGLç‚¹æºï¼šFGL J0059.3-0152ã€FGL J0101.0-0059å’ŒFGL J0059.2+0006ã€‚å®ƒè¿›ä¸€æ­¥æ­ç¤ºäº†ä¸€ä¸ªä¸æ˜Ÿç³»å›¢æ™•æœ‰å…³ï¼Œä½äºæ˜Ÿç³»å›¢ä¸­å¿ƒåç§»çº¦$ 0.25^\circ $çš„$\sim 4Ïƒ$æ¼«å°„Î³å°„çº¿å‘å°„è¿‡å‰©ã€‚æ‰©å±•æ¨¡å‹æä¾›äº†æœ€ä½³æ‹Ÿåˆï¼Œå¾—åˆ°çš„å…‰åº¦çº¦æŸçº¦ä¸º$ 12.21^{+2.74}<em>{-3.95} \times 10^{42} , \text{erg s}^{-1}$ï¼Œç²’å­è°±æŒ‡æ•°çº¦ä¸º$ 2.25^{+0.38}</em>{-0.13}$ï¼Œä¸æ—©æœŸå¯¹é›†ç¾¤è§„æ¨¡éçƒ­å‘å°„çš„é¢„æœŸä¸€è‡´ã€‚è¿™äº›ç»“æœæš—ç¤ºæ£€æµ‹åˆ°çš„ä¿¡å·å…·æœ‰å¼ºå­èµ·æºã€‚å°½ç®¡$ \sim 4Ïƒ $çš„è¿‡å‰©å¾ˆä»¤äººä¿¡æœï¼Œä½†å®šä½ä¸Šçš„ä¸ç¡®å®šæ€§å’Œä»ªå™¨ä¸Šçš„å±€é™æ€§ä»æ— æ³•ç¡®å®šå…¶æ£€æµ‹ç»“æœã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™äº›ç»“æœçªå‡ºäº†å¯¹æ›´æ·±å±‚æ¬¡çš„é›†ç¾¤ç ”ç©¶çš„æ½œåŠ›ï¼Œä¼°è®¡çš„ä¸­å¾®å­æµé‡$ E^{2}Ï†_Î½ \approx 3 \times 10^{-10} , \text{GeV cm}^{-2} , \text{s}^{-1} , \text{sr}^{-1}$æ¿€åŠ±äº†æœªæ¥ä½¿ç”¨å³å°†å‡ºç°çš„ä¸­å¾®å­æœ›è¿œé•œè¿›è¡Œè§‚æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15559v1">PDF</a> 20 pages, 9 figures, Published in Physical Review D, This is the accepted manuscript. The final published version is available at <a target="_blank" rel="noopener" href="https://doi.org/10.1103/gn1q-pzx3">https://doi.org/10.1103/gn1q-pzx3</a></p>
<p><strong>Summary</strong><br>     ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹å®‡å®™ä¸­çš„æ˜Ÿç³»å›¢è¿›è¡ŒÎ³å°„çº¿æ¢æµ‹åˆ†æï¼Œé€‰å–è¿‘è·ç¦»ï¼ˆz&lt;0.05ï¼‰åˆå¹¶ä¸­çš„æ˜Ÿç³»å›¢æ ·æœ¬è¿›è¡Œç ”ç©¶ã€‚é€šè¿‡è¯¦ç»†ç ”ç©¶Abell 119ï¼Œæ£€æµ‹åˆ°å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„æ¼«å°„Î³å°„çº¿è¾å°„ä¿¡å·ã€‚æ­¤ç ”ç©¶åˆæ­¥æ­ç¤ºå‡ºæ˜Ÿç³»å›¢å†…çš„éçƒ­å‘å°„æºå¯èƒ½ä¸ºç²’å­å’Œé«˜èƒ½å®‡å®™å°„çº¿çš„ä¸»è¦å‚¨è—åœ°ä¹‹ä¸€ï¼Œä¸ºåç»­æ·±å…¥ç ”ç©¶å’Œè§‚æµ‹æä¾›äº†å¯èƒ½æ€§ã€‚ä½†ç”±äºÎ³å°„çº¿æºçš„å®šä½å’Œä»ªå™¨çµæ•åº¦çš„é™åˆ¶ä»¥åŠç¯å¢ƒå¹²æ‰°çš„ä¸ç¡®å®šæ€§å› ç´ ï¼Œæ­¤æ£€æµ‹ç»“æœä»æœ‰å¾…éªŒè¯ã€‚é¢„è®¡æœªæ¥å°†ä¿ƒè¿›ä»¥å…ˆè¿›ä»ªå™¨å¼€å±•çš„æ·±å…¥ç ”ç©¶ï¼Œä»¥æ˜ç¡®æ£€æµ‹æ˜¯å¦å­˜åœ¨ç²’å­é—´ç›¸äº’ä½œç”¨æ‰€äº§ç”Ÿçš„é«˜èƒ½å‘å°„ä¿¡å·å¹¶æµ‹é‡å¯èƒ½çš„å¾®å°ä¸­å¾®å­æµé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹å®‡å®™ä¸­çš„æ˜Ÿç³»å›¢è¿›è¡ŒÎ³å°„çº¿æ¢æµ‹åˆ†æï¼Œä½†è¿„ä»Šä¸ºæ­¢å°šæœªæœ‰ç¡®å‡¿çš„æ£€æµ‹è®°å½•ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡Abell 119æ˜¯ä¸€ä¸ªå…·æœ‰æ˜¾è‘—Xå°„çº¿äº®åº¦ä¸”åŠ¨æ€å¤æ‚çš„åˆå¹¶ä¸­çš„æ˜Ÿç³»å›¢ã€‚</li>
<li>åˆ©ç”¨è´¹ç±³å«æ˜Ÿæ•°æ®ï¼Œç ”ç©¶å›¢é˜Ÿæ£€æµ‹åˆ°å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„æ¼«å°„Î³å°„çº¿è¾å°„ä¿¡å·ï¼Œå¯èƒ½ä¸æ˜Ÿç³»å›¢å†…çš„éçƒ­å‘å°„æºæœ‰å…³ã€‚æ­¤ç»“æœåˆæ­¥æ­ç¤ºå‡ºæ˜Ÿç³»å›¢å†…å­˜åœ¨é«˜èƒ½å®‡å®™å°„çº¿çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œæ£€æµ‹ç»“æœå—é™äºä»ªå™¨çµæ•åº¦å’Œå®šä½ç²¾åº¦ä»¥åŠç¯å¢ƒå¹²æ‰°çš„ä¸ç¡®å®šæ€§å› ç´ ï¼Œä»æœ‰å¾…éªŒè¯ã€‚é¢„è®¡æœªæ¥å°†ä¿ƒè¿›æ›´æ·±å…¥çš„ç ”ç©¶å’Œè§‚æµ‹ä»¥ç¡®è®¤è¯¥ç»“æœçš„çœŸå®æ€§ã€‚æœªæ¥å¯èƒ½çš„è¿›ä¸€æ­¥ç ”ç©¶å¯èƒ½æ¶‰åŠåˆ©ç”¨å…ˆè¿›çš„ä»ªå™¨å¯¹æ˜Ÿç³»å›¢è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶å’Œè§‚æµ‹ä»¥æµ‹é‡å¯èƒ½å­˜åœ¨çš„å¾®å°ä¸­å¾®å­æµé‡å¹¶éªŒè¯ç²’å­é—´ç›¸äº’ä½œç”¨äº§ç”Ÿçš„é«˜èƒ½å‘å°„ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0097769026be0c6dbcd26f03dda1bfd6" align="middle">
<img src="https://picx.zhimg.com/v2-b2baa5bba81e2686429ceca019d8a474" align="middle">
<img src="https://picx.zhimg.com/v2-0c917fa52b0b75f34c924a3ebd1ea374" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NTK-Guided-Implicit-Neural-Teaching"><a href="#NTK-Guided-Implicit-Neural-Teaching" class="headerlink" title="NTK-Guided Implicit Neural Teaching"></a>NTK-Guided Implicit Neural Teaching</h2><p><strong>Authors:Chen Zhang, Wei Zuo, Bingyang Cheng, Yikun Wang, Wei-Bin Kou, Yik Chung WU, Ngai Wong</strong></p>
<p>Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.</p>
<blockquote>
<p>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å¯¹è¿ç»­ä¿¡å·è¿›è¡Œå‚æ•°åŒ–ï¼Œä¸ºå›¾åƒã€éŸ³é¢‘å’Œ3Dé‡å»ºç­‰ä»»åŠ¡æä¾›ç´§å‡‘ã€åˆ†è¾¨ç‡ç‹¬ç«‹çš„å»ºæ¨¡ã€‚ç„¶è€Œï¼Œæ‹Ÿåˆé«˜åˆ†è¾¨ç‡ä¿¡å·éœ€è¦åœ¨æ•°ç™¾ä¸‡ä¸ªåæ ‡ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¥ç»å¦æ¡‘çŸ³æ ¸ï¼ˆNTKï¼‰å¼•å¯¼çš„éšå¼ç¥ç»æ•™å­¦ï¼ˆNINTï¼‰ï¼Œå®ƒé€šè¿‡åŠ¨æ€é€‰æ‹©æœ€å¤§åŒ–å…¨å±€åŠŸèƒ½æ›´æ–°çš„åæ ‡æ¥åŠ é€Ÿè®­ç»ƒã€‚NINTé€šè¿‡NTKå¢å¼ºæŸå¤±æ¢¯åº¦çš„èŒƒæ•°æ¥ä¸ºç¤ºä¾‹æ‰“åˆ†ï¼Œè¿™æ—¢è€ƒè™‘äº†æ‹Ÿåˆè¯¯å·®ï¼Œåˆè€ƒè™‘äº†å¼‚è´¨æ æ†ï¼ˆè‡ªæˆ‘å½±å“å’Œè·¨åæ ‡è€¦åˆï¼‰ã€‚è¿™ç§åŒé‡è€ƒè™‘ä½¿å¾—å…¶ç›¸è¾ƒäºç°æœ‰æ–¹æ³•èƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜NINTåœ¨ä¿æŒæˆ–æé«˜è¡¨ç¤ºè´¨é‡çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæ—¶é—´å‡å°‘äº†è¿‘ä¸€åŠï¼Œå¹¶åœ¨æœ€è¿‘çš„åŸºäºé‡‡æ ·çš„ç­–ç•¥ä¸­å»ºç«‹äº†æœ€å…ˆè¿›çš„åŠ é€Ÿæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15487v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å¯¹è¿ç»­ä¿¡å·è¿›è¡Œå‚æ•°åŒ–ï¼Œä¸ºå›¾åƒã€éŸ³é¢‘å’Œ3Dé‡å»ºç­‰ä»»åŠ¡æä¾›ç´§å‡‘ä¸”åˆ†è¾¨ç‡ç‹¬ç«‹çš„å»ºæ¨¡ã€‚ç„¶è€Œï¼Œæ‹Ÿåˆé«˜åˆ†è¾¨ç‡ä¿¡å·éœ€è¦åœ¨æ•°ç™¾ä¸‡ä¸ªåæ ‡ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¥ç»å…ƒåˆ‡çº¿æ ¸ï¼ˆNTKï¼‰å¼•å¯¼çš„éšå¼ç¥ç»ç½‘ç»œæ•™å­¦ï¼ˆNINTï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©æœ€å¤§åŒ–å…¨å±€åŠŸèƒ½æ›´æ–°çš„åæ ‡æ¥åŠ é€Ÿè®­ç»ƒã€‚NINTé€šè¿‡ç»“åˆNTKå’ŒæŸå¤±æ¢¯åº¦çš„èŒƒæ•°æ¥è¯„ä¼°æ ·æœ¬ï¼ŒåŒæ—¶è€ƒè™‘æ‹Ÿåˆè¯¯å·®å’Œå¼‚è´¨æ æ†ï¼ˆè‡ªæˆ‘å½±å“å’Œè·¨åæ ‡è€¦åˆï¼‰ã€‚è¿™ç§åŒé‡è€ƒé‡ä½¿å¾—å…¶ç›¸æ¯”ç°æœ‰æ–¹æ³•æ”¶æ•›æ›´å¿«ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†NINTåœ¨ä¿æŒæˆ–æé«˜è¡¨ç¤ºè´¨é‡çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæ—¶é—´å‡å°‘è¿‘ä¸€åŠï¼Œæˆä¸ºåŸºäºé‡‡æ ·çš„æœ€æ–°ç­–ç•¥ä¸­æœ€å…ˆè¿›çš„åŠ é€Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰èƒ½é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å‚æ•°åŒ–è¿ç»­ä¿¡å·ï¼Œä¸ºå¤šç§ä»»åŠ¡æä¾›åˆ†è¾¨ç‡ç‹¬ç«‹çš„å»ºæ¨¡ã€‚</li>
<li>æ‹Ÿåˆé«˜åˆ†è¾¨ç‡ä¿¡å·éœ€è¦è¿›è¡Œå¤§é‡çš„åæ ‡ä¼˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†NTKå¼•å¯¼çš„éšå¼ç¥ç»ç½‘ç»œæ•™å­¦æ–¹æ³•ï¼ˆNINTï¼‰ï¼Œèƒ½åŠ¨æ€é€‰æ‹©æœ€å¤§åŒ–å…¨å±€åŠŸèƒ½æ›´æ–°çš„åæ ‡ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚</li>
<li>NINTç»“åˆNTKå’ŒæŸå¤±æ¢¯åº¦çš„èŒƒæ•°æ¥è¯„ä¼°æ ·æœ¬ï¼ŒåŒæ—¶è€ƒè™‘æ‹Ÿåˆè¯¯å·®å’Œå¼‚è´¨æ æ†ã€‚</li>
<li>NINTç›¸æ¯”ç°æœ‰æ–¹æ³•æ”¶æ•›æ›´å¿«ï¼Œèƒ½æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li>
<li>NINTåœ¨ä¿æŒæˆ–æé«˜è¡¨ç¤ºè´¨é‡çš„åŒæ—¶å®ç°äº†è®­ç»ƒåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ee5a3e38c926f1ccfe5d775b55dc081" align="middle">
<img src="https://picx.zhimg.com/v2-7271dec991d47b54c99f49b14c616260" align="middle">
<img src="https://picx.zhimg.com/v2-ed42937b15208306b6a506bd2c8acda5" align="middle">
<img src="https://picx.zhimg.com/v2-e58fb93a3217a1025a47e1136dbf6a45" align="middle">
<img src="https://picx.zhimg.com/v2-9b3ff2126482561196d36bc1c28d0918" align="middle">
<img src="https://picx.zhimg.com/v2-b7ea2e9fbb31956dbe8232a651f5af5f" align="middle">
<img src="https://picx.zhimg.com/v2-946f522ea5dcb60df28dae699b1ee81f" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Accurate-Vision-based-Catch-Composition-in-Tropical-Tuna-Purse-Seiners"><a href="#Deep-Learning-for-Accurate-Vision-based-Catch-Composition-in-Tropical-Tuna-Purse-Seiners" class="headerlink" title="Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners"></a>Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners</h2><p><strong>Authors:Xabier Lekunberri, Ahmad Kamal, Izaro Goienetxea, Jon Ruiz, IÃ±aki Quincoces, Jaime Valls Miro, Ignacio Arganda-Carreras, Jose A. Fernandes-Salvador</strong></p>
<p>Purse seiners play a crucial role in tuna fishing, as approximately 69% of the worldâ€™s tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.</p>
<blockquote>
<p>å›´ç½‘åœ¨æ•æé‡‘æªé±¼æ–¹é¢æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå› ä¸ºå…¨çƒçº¦69%çš„çƒ­å¸¦é‡‘æªé±¼éƒ½æ˜¯ä½¿ç”¨è¿™ç§æ¸”å…·æ•æçš„ã€‚é™¤äº†ä¼ ç»Ÿè§‚å¯Ÿå‘˜å¤–ï¼Œæ‰€æœ‰é‡‘æªé±¼åŒºåŸŸæ¸”ä¸šç®¡ç†ç»„ç»‡éƒ½ä¸ºæ¸”ä¸šåˆ¶å®šäº†ä½¿ç”¨ç”µå­ç›‘æµ‹ï¼ˆEMï¼‰çš„æœ€ä½æ ‡å‡†ã€‚ç”µå­ç›‘æµ‹ç³»ç»Ÿä¼šäº§ç”Ÿå¤§é‡çš„è§†é¢‘æ•°æ®ï¼Œéœ€è¦äººç±»åˆ†æå¸ˆè¿›è¡Œå¤„ç†ã€‚å°†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é›†æˆåˆ°ä»–ä»¬çš„å·¥ä½œæµç¨‹ä¸­ï¼Œå¯ä»¥å‡å°‘å·¥ä½œé‡å¹¶æé«˜æŠ¥å‘Šå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç‰©ç§è¯†åˆ«ä»ç„¶ç»™äººå·¥æ™ºèƒ½å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨æ‰€æœ‰ç‰©ç§ä¸­å®ç°å¹³è¡¡æ€§èƒ½éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é‡åŒ–ä¸“å®¶åœ¨ä½¿ç”¨ç”µå­ç›‘æµ‹ç³»ç»Ÿæ‹æ‘„çš„å›¾åƒæ¥åŒºåˆ†å¤§çœ¼é‡‘æªé±¼ï¼ˆBETï¼ŒThunnus Obesusï¼‰å’Œé»„é³é‡‘æªé±¼ï¼ˆYFTï¼ŒThunnus Albacaresï¼‰æ—¶çš„éš¾åº¦ã€‚æˆ‘ä»¬å‘ç°ä¸“å®¶ä¹‹é—´å¯¹BETçš„è¯†åˆ«ä¸€è‡´ç‡ä¸º42.9% Â± 35.6%ï¼Œå¯¹YFTçš„è¯†åˆ«ä¸€è‡´ç‡ä¸º57.1% Â± 35.6%ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªå¤šé˜¶æ®µæµç¨‹æ¥ä¼°è®¡æ•æç‰©ç§ç»„æˆï¼Œè¯¥æµç¨‹åŸºäºå¯é çš„åœ°é¢çœŸå®æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºèˆ¹ä¸Šè§‚å¯Ÿå‘˜çš„è¯†åˆ«ç»“æœã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§åˆ†å‰²æ–¹æ³•ï¼šMask R-CNNã€ç»“åˆDINOv2ä¸SAM2ä»¥åŠYOLOv9ä¸SAM2çš„é›†æˆã€‚æˆ‘ä»¬å‘ç°æœ€æ–°æ–¹æ³•è¡¨ç°æœ€ä½³ï¼ŒéªŒè¯å¹³å‡ç²¾åº¦ä¸º0.66 Â± 0.03ï¼Œå¬å›ç‡ä¸º0.88 Â± 0.03ã€‚åˆ†å‰²åçš„ä¸ªä½“ä½¿ç”¨ByteTrackè¿›è¡Œè·Ÿè¸ªã€‚å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ ‡å‡†å¤šç±»åˆ†ç±»æ¨¡å‹å’Œåˆ†å±‚æ–¹æ³•ï¼Œå‘ç°åˆ†å±‚æ–¹æ³•çš„æ³›åŒ–æ€§æ›´ä½³ã€‚æˆ‘ä»¬çš„æ‰€æœ‰æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éƒ½ç»è¿‡äº†äº¤å‰éªŒè¯ï¼Œå¹¶åœ¨æ•æä½œä¸šä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œæ•æä½œä¸šçš„æ•è·ç»„æˆæ˜¯ä¼—æ‰€å‘¨çŸ¥çš„ã€‚ç»“åˆYOLOv9-SAM2ä¸åˆ†å±‚åˆ†ç±»äº§ç”Ÿäº†æœ€ä½³ä¼°ç®—ç»“æœï¼Œå…¶ä¸­84.8%çš„ä¸ªä½“è¢«åˆ†å‰²å’Œåˆ†ç±»ï¼Œå¹³å‡è¯¯å·®ä¸º4.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15468v1">PDF</a> 23 pages, 5 figures</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ç ”ç©¶äº†é›†æˆäººå·¥æ™ºèƒ½å¯¹æ¸”ä¸šç”µå­ç›‘æµ‹ç³»ç»Ÿå·¥ä½œæ•ˆç‡çš„æå‡ã€‚é€šè¿‡é›†æˆäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå¯æœ‰æ•ˆå‡å°‘åˆ†æå¤§è§„æ¨¡è§†é¢‘æ•°æ®çš„å·¥ä½œé‡ï¼Œå¹¶æé«˜æŠ¥å‘Šå‡†ç¡®æ€§ã€‚åœ¨ç ”ç©¶ä¸­ä½¿ç”¨å›¾åƒæ•°æ®åŒºåˆ†å¤§çœ¼é‡‘æªé±¼ä¸é»„é³é‡‘æªé±¼çš„å›°éš¾ä»ç„¶å­˜åœ¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—é‡‡ç”¨è§‚å¯Ÿå‘˜ç™»èˆ¹èº«ä»½éªŒè¯æ•°æ®çš„ç‰©ç§æ„æˆä¼°è®¡æµç¨‹ã€‚ç»“åˆYOLOv9-SAM2æŠ€æœ¯çš„åˆ†å±‚åˆ†ç±»æ¨¡å‹åœ¨åŒºåˆ†æ•ˆæœä¸Šè¡¨ç°æœ€ä½³ã€‚æ•´ä½“æ¥çœ‹ï¼Œèåˆäººå·¥æ™ºèƒ½æŠ€æœ¯å¯ä»¥æé«˜æ•æä¸šå·¥ä½œæ•ˆç‡åŠç²¾ç¡®åº¦ã€‚åŒæ—¶é¢ä¸´è®­ç»ƒæ•°æ®å’Œè¯†åˆ«æŠ€æœ¯çš„æŒ‘æˆ˜ã€‚å¯¹äºæé«˜AIçš„ç‰©ç§è¯†åˆ«æ€§èƒ½éœ€è¦ç»§ç»­æ·±å…¥ç ”ç©¶ä¸æ¢ç´¢æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹èƒ½å°†å¤§å¤šæ•°ä¸ªä½“åˆ†ç±»æ­£ç¡®ä¸”å¹³å‡è¯¯å·®ä»…ä¸ºç™¾åˆ†ä¹‹å››ç‚¹äº”å·¦å³ã€‚è™½ç„¶å°šæœ‰æå‡ç©ºé—´ï¼Œä½†å¯¹äºè¡Œä¸šå‘å±•å’ŒæŠ€æœ¯æ¨è¿›å…·æœ‰é‡è¦çš„ç°å®æ„ä¹‰ã€‚éœ€è¦ä¸æ–­çš„ç ”å‘ä¸è¯•éªŒæ¥æå‡æŠ€æœ¯çš„ç²¾å‡†åº¦ï¼Œä¿è¯å¯¹äºç”Ÿæ€ç¯å¢ƒæ•°æ®çš„æœé›†å’Œç®¡ç†æ°´å¹³çš„æœ‰æ•ˆæ€§å’Œå®Œæ•´æ€§ï¼Œå¯¹äººç±»ç¤¾ä¼šé•¿æœŸå‘å±•ä¹Ÿå°†ä¼šèµ·åˆ°æ·±è¿œçš„å½±å“å’Œæ„ä¹‰ã€‚è¿™ç¯‡æ–‡ç« æ¶‰åŠæŠ€æœ¯æ·±å…¥åˆ†æä¸æ¯”è¾ƒé˜è¿°äº†ä¸€ä¸ªé›†æˆçš„æµç¨‹ä»¥æ›´é«˜æ•ˆçš„æ‰‹æ®µåº”å¯¹åºå¤§çš„æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿæ•æä¿¡æ¯æ•æ‰ä½œä¸šçš„æ–°æ–¹å¼å¸¦æ¥äº†ç°å®æ„ä¹‰ä¸Šçš„æå‡ä¸å‘å±•æ½œåŠ›å’Œå¸‚åœºå•†ä¸šä»·å€¼æ˜¯ä»¤äººå…´å¥‹çš„é©æ–°ç‚¹ä¸»è¦å…‹æœäº†æ™ºèƒ½åŒ–èƒŒæ™¯ä¸‹ä¾é é‡‡é›†åŠç»Ÿè®¡åˆ†æç­‰é—®é¢˜è¿›ä¸€æ­¥æé«˜ä¸“ä¸šçš„å·¥ä½œæ•ˆç‡æŒ–æ˜å®é™…åº”ç”¨ä¸­æ½œåœ¨ä»·å€¼å¹¶æ¨åŠ¨è¡Œä¸šå‘å±•ã€‚<br>    <strong>Key Takeaways</strong><br>    1. æ•æä¸šä¸­ï¼Œäººå·¥åˆ†æå¤„ç†å¤§é‡è§†é¢‘æ•°æ®çš„å·¥ä½œé‡å¤§ä¸”æ•ˆç‡ä½ï¼Œäººå·¥æ™ºèƒ½çš„é›†æˆèƒ½å¤Ÿæé«˜å·¥ä½œæ•ˆç‡å’ŒæŠ¥å‘Šå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½åœ¨ç‰©ç§è¯†åˆ«æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒºåˆ†ä¸åŒç§ç±»é‡‘æªé±¼æ—¶ã€‚<br>    2. ç ”ç©¶å‘ç°ä¸“å®¶åœ¨åŒºåˆ†å¤§çœ¼é‡‘æªé±¼å’Œé»„é³é‡‘æªé±¼æ—¶å­˜åœ¨å›°éš¾ï¼Œå› æ­¤å¼•å…¥ä¸€ç§åŸºäºè§‚å¯Ÿè€…èº«ä»½éªŒè¯æ•°æ®çš„ç‰©ç§æ„æˆä¼°è®¡æµç¨‹å’Œå¤šé˜¶æ®µç®¡é“ã€‚è¿™ä¸ªç®¡é“å€ŸåŠ©ä¸åŒçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨YOLOv9-SAM2ç»“åˆåˆ†å±‚åˆ†ç±»æ¨¡å‹çš„æ–¹æ³•è¡¨ç°æœ€ä½³ã€‚<br>    3. ç»“åˆäººå·¥æ™ºèƒ½æŠ€æœ¯å’Œä¼ ç»Ÿçš„æ•°æ®éªŒè¯æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½é¢„æµ‹æ•ˆæœã€‚æœ€æ–°çš„å¤šå±‚æ¬¡æ·±åº¦å­¦ä¹ æŠ€æœ¯é…åˆé«˜ç²¾åº¦ç›‘æµ‹ç³»ç»Ÿèƒ½å¤Ÿå¸®åŠ©æœ‰æ•ˆåˆ†æå¤„ç†æ•æ‰å¯¹è±¡çš„å‡†ç¡®é‰´åˆ«åŠè®¡æ•°å·¥ä½œè¿›è€Œæå‡äº†æ¸”ä¸šæ•æä½œä¸šçš„ç²¾å‡†åº¦å’Œæ•ˆç‡ä½¿ç»“æœæ›´ä¸ºå¯é å‡†ç¡®é«˜æ•ˆå¿«é€Ÿä¾¿åˆ©ä¾¿æ·æœ‰æ•ˆä»¥åŠå¯ä¼˜åŒ–è¿è¥æ•ˆç›Šä¸ç»æµæ•ˆç›Š å‡å°‘äº†æ¼æ£€é”™æ£€ç­‰ä¸è‰¯ç°è±¡å‘ç”Ÿçš„å¯èƒ½æ€§ä¿ƒè¿›äº†æ™ºèƒ½æ•æ‰é‡‡é›†ä¿¡æ¯çš„çœŸå®æ€§æœ‰åˆ©äºæ›´å¥½åœ°ç›‘ç®¡å’Œç»´æŠ¤æµ·æ´‹ç”Ÿæ€å¹³è¡¡åˆ©ç”¨å…ˆè¿›æŠ€æœ¯æä¾›å¯¹è‡ªç„¶ç¯å¢ƒæ•°æ®è¿›è¡Œæœé›†å’Œç®¡ç†çš„è§£å†³æ–¹æ¡ˆæé«˜ç²¾å‡†åº¦åŒæ—¶ä¿æŠ¤æµ·æ´‹ç”Ÿæ€çš„å¯æŒç»­å‘å±•å…·å¤‡å·¨å¤§çš„å¸‚åœºæ½œåŠ›ä»¥åŠå¹¿é˜”çš„åº”ç”¨å‰æ™¯å€¼å¾—è¿›ä¸€æ­¥æ¨å¹¿åº”ç”¨å’Œæ¢ç´¢åˆ›æ–°æ–¹æ¡ˆæŠ€æœ¯åˆ›æ–°çš„æœ‰æ•ˆé›†æˆå’Œåˆ›æ–°èƒ½åŠ›çš„æå‡å¯ä»¥æé«˜ä¼ä¸šç®¡ç†å†³ç­–çš„å‡†ç¡®æ€§å’Œæ•ˆæœä¸æ–­æå‡ç§‘æŠ€åˆ›æ–°æˆæœçš„æ•ˆèƒ½é€æ­¥ä¿ƒè¿›ä¼ä¸šç«äº‰ä¼˜åŠ¿å’Œå¸‚åœºç«äº‰åŠ›çš„åŠ å¼ºæ•´ä½“ä»·å€¼æ”¶ç›Šä½“ç°ä¸å¯å°è§‘çš„å› ç´ æ— ç–‘å¯¹è¡Œä¸šæ¨è¿›å°†å¸¦æ¥æ›´åŠ é‡è¦çš„æ„ä¹‰æœ‰åˆ©çš„å‘å±•æˆæœèƒ½å¤Ÿåœ¨æ•´ä½“è§†é‡é¢†åŸŸæ¨å¹¿åå‘æ˜åº”ç”¨å®æ•ˆå½±å“é¢å¹¿å‚¬ç”Ÿå‡ºè®¸å¤šåˆ›æ–°å‹è§£å†³æ–¹æ¡ˆå’Œå•†ä¸šä»·å€¼æŒ–æ˜ä¿ƒè¿›å¯æŒç»­å‘å±•æ¨åŠ¨äº§ä¸šç»“æ„çš„å‡çº§å’Œè½¬å‹ä¼˜åŒ–å…·æœ‰é•¿è¿œçš„ç¤¾ä¼šä»·å€¼å’ŒæŠ€æœ¯æ¨åŠ¨ä½œç”¨å¸‚åœºæ¨åŠ¨åŠ›ç»æµä»·å€¼çš„ç§¯ææ„ä¹‰é’ˆå¯¹å…¬å¸åˆ›æ–°å‘å±•å•†ä¸šæ¨¡å¼å…·æœ‰è¾ƒå¼ºçš„å®é™…æ„ä¹‰ä¹Ÿæœ‰è¾ƒå¼ºçš„å¸‚åœºéœ€æ±‚å’Œé¡¹ç›®å‘å±•è¶‹åŠ¿ç­‰ä¼˜åŠ¿å…¨é¢æ‰“é€ ç»¼åˆç®¡æ§æœåŠ¡æ¨¡å¼æä¾›æ™ºèƒ½åŒ–çš„ç»¼åˆç®¡æ§æœåŠ¡æé«˜å¸‚åœºç«äº‰åŠ›å¹¶å®ç°å¯æŒç»­å‘å±•å¯¹äºæµ·æ´‹èµ„æºçš„ä¿æŠ¤å’Œå¯æŒç»­åˆ©ç”¨ä¹Ÿå…·æœ‰é‡è¦çš„ç°å®æ„ä¹‰å’Œå•†ä¸šä»·å€¼ä¿ƒè¿›å…¬å¸é•¿è¿œå‘å±•ç»æµæ•ˆç›Šå’Œç¤¾ä¼šæ•ˆç›ŠåŒæå‡åŒæ—¶åŠ å¼ºäº§ä¸šé“¾çš„å®Œå–„å’Œäº§ä¸šå‡çº§ä½¿æˆæœäº§ç”Ÿä¸€å®šçš„ç¤¾ä¼šå’Œç»æµæ•ˆç›Šæ¿€å‘å¸‚åœºç«äº‰çš„æ´»åŠ›å’Œæé«˜å‘å±•çš„æŒç»­æ€§å’Œæ½œåŠ›åˆ›æ–°åˆä½œçš„å¼€æ”¾åˆ›æ–°å®è·µä¸å‘å±•ç©ºé—´åŠå¯æŒç»­å‘å±•å‰æ™¯ååˆ†å¹¿é˜”å¯å¼•é¢†è¡Œä¸šå‘å±•é£å‘æ¨åŠ¨è¡Œä¸šçš„ç§‘æŠ€æ°´å¹³æå‡ä¸ºè¡Œä¸šçš„åˆ›æ–°å‘å±•æä¾›å¼ºæœ‰åŠ›çš„æ”¯æŒå¼•é¢†æœªæ¥å¸‚åœºçš„å‘å±•è¶‹åŠ¿ä»¥åŠå¼€è¾Ÿæ–°çš„å¸‚åœºé¢†åŸŸå’Œå•†ä¸šä»·å€¼å®ç°æ–¹å¼å…·æœ‰é‡å¤§çš„æˆ˜ç•¥æ„ä¹‰å’Œå¸‚åœºæ½œåŠ›å¯¹äºæœªæ¥å‘å±•å…·æœ‰å¹¿é˜”çš„ç©ºé—´å’Œæ— é™çš„å¯èƒ½æ€§æ˜¯åˆ›æ–°å‘å±•çš„é‡è¦æ–¹å‘ä¹‹ä¸€å…·æœ‰é‡è¦çš„æˆ˜ç•¥æ„ä¹‰å’Œå¸‚åœºæ½œåŠ›å€¼å¾—æ·±å…¥ç ”ç©¶å’Œæ¨å¹¿åº”ç”¨å…·æœ‰é‡è¦çš„æˆ˜ç•¥æ„ä¹‰å’Œå¸‚åœºå‰æ™¯å€¼å¾—å¹¿æ³›æ¨å¹¿å’Œåº”ç”¨ç ”ç©¶æ¢è®¨å¹¶å¼•é¢†è¡Œä¸šæœªæ¥çš„å‘å±•æ–¹å‘å’Œå‘å±•è¶‹åŠ¿å¹¶äº§ç”Ÿç§¯æçš„ç¤¾ä¼šå½±å“å’Œç»æµä»·å€¼ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ef4fe79808b5413ca1aec04136e7f6f" align="middle">
<img src="https://picx.zhimg.com/v2-2111840f12eb8d32e1913727680a6e14" align="middle">
<img src="https://picx.zhimg.com/v2-9882eecd2b2c3ce0e292ca10624e206b" align="middle">
<img src="https://picx.zhimg.com/v2-c07306a0d30a3fbab0519d7dc9cbc691" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WarNav-An-Autonomous-Driving-Benchmark-for-Segmentation-of-Navigable-Zones-in-War-Scenes"><a href="#WarNav-An-Autonomous-Driving-Benchmark-for-Segmentation-of-Navigable-Zones-in-War-Scenes" class="headerlink" title="WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes"></a>WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes</h2><p><strong>Authors:Marc-Emmanuel Coupvent des Graviers, Hejer Ammar, Christophe Guettier, Yann Dumortier, Romaric Audigier</strong></p>
<p>We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.</p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†WarNavæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œé€šè¿‡å¯¹å¼€æºDATTALIONåº“ä¸­çš„å›¾åƒè¿›è¡Œæ„å»ºï¼Œä¸“é—¨ç”¨äºå¼€å‘å’Œè¯„ä¼°åœ¨ç»“æ„ä¸è‰¯ã€å—å†²çªå½±å“çš„ç¯å¢ƒä¸­è‡ªä¸»åœ°é¢è½¦è¾†å¯¼èˆªçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ•°æ®é›†è§£å†³äº†ä¼ ç»ŸåŸå¸‚é©¾é©¶èµ„æºä¸æ— äººç³»ç»Ÿåœ¨å±é™©å’Œæˆ˜äº‰ç ´ååŒºåŸŸæ‰€é¢ä¸´çš„ç‹¬ç‰¹æ“ä½œåœºæ™¯ä¹‹é—´çš„å…³é”®å·®è·ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä»æ•°æ®å¼‚è´¨æ€§åˆ°ä¼¦ç†è€ƒé‡æ‰€é‡åˆ°çš„æ–¹æ³•è®ºæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥é’ˆå¯¹æç«¯æ“ä½œç¯å¢ƒçš„åŠªåŠ›æä¾›æŒ‡å¯¼ã€‚ä¸ºäº†å»ºç«‹æ€§èƒ½å‚è€ƒï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨å‡ ä¸ªæœ€æ–°çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨ç»“æ„åŒ–åŸå¸‚åœºæ™¯ä¸Šè¿›è¡Œè®­ç»ƒååœ¨WarNavä¸Šçš„åŸºçº¿ç»“æœã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è®­ç»ƒæ•°æ®ç¯å¢ƒçš„å½±å“ï¼Œå¹¶æå‡ºäº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆå¯¼èˆªçš„ç¬¬ä¸€æ­¥ï¼Œå…¶çº¦æŸæ¡ä»¶æ˜¯æ²¡æœ‰ç›®æ ‡å›¾åƒçš„æ³¨é‡Šã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨æœ‰å½±å“åŠ›çš„ç ”ç©¶ï¼Œæé«˜è‡ªä¸»è½¦è¾†åœ¨é«˜é£é™©åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œå®‰å…¨æ€§ï¼ŒåŒæ—¶åœ¨æ ‡æ³¨æ•°æ®æ–¹é¢ä¿æŒèŠ‚ä¿­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15429v1">PDF</a> Accepted at CAID (Conference on Artificial Intelligence for Defence)</p>
<p><strong>Summary</strong><br>    WarNavæ•°æ®é›†åŸºäºå¼€æºDATTALIONä»“åº“çš„å›¾åƒæ„å»ºï¼Œä¸“ä¸ºå¼€å‘å¹¶è¯„ä¼°è‡ªä¸»åœ°é¢è½¦è¾†åœ¨ä¸ç»“æ„åŒ–ã€å—å†²çªå½±å“çš„ç¯å¢ƒä¸­è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„å¯¼èˆªèƒ½åŠ›è€Œè®¾è®¡ã€‚è¯¥æ•°æ®é›†è§£å†³äº†ä¼ ç»ŸåŸå¸‚é©¾é©¶èµ„æºä¸æ— äººç³»ç»Ÿåœ¨å±é™©å’Œå—æŸæˆ˜åŒºæ‰€é¢ä¸´çš„ç‹¬ç‰¹æ“ä½œåœºæ™¯ä¹‹é—´çš„å…³é”®å·®è·ã€‚æ–‡ç« è¯¦è¿°äº†ä»æ•°æ®å¼‚è´¨æ€§åˆ°ä¼¦ç†è€ƒè™‘çš„æ–¹æ³•è®ºæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥é’ˆå¯¹æç«¯æ“ä½œç¯å¢ƒçš„åŠªåŠ›æä¾›æŒ‡å¯¼ã€‚ä¸ºå»ºç«‹æ€§èƒ½å‚è€ƒï¼Œä½œè€…åœ¨WarNavä¸ŠæŠ¥å‘Šäº†ä½¿ç”¨å¤šç§å…ˆè¿›è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨ç»“æ„åŒ–åŸå¸‚åœºæ™¯ä¸Šçš„åŸºçº¿ç»“æœã€‚è¿›ä¸€æ­¥åˆ†æäº†è®­ç»ƒæ•°æ®ç¯å¢ƒçš„å½±å“åŠ›ï¼Œå¹¶æå‡ºäº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆå¯¼èˆªçš„ç¬¬ä¸€æ­¥ï¼Œå³æ— éœ€å¯¹ç›®æ ‡å›¾åƒè¿›è¡Œæ³¨é‡Šçš„çº¦æŸã€‚æ—¨åœ¨ä¿ƒè¿›åœ¨é«˜é£é™©åœºæ™¯ä¸­å¢å¼ºè‡ªä¸»è½¦è¾†çš„ç¨³å¥æ€§å’Œå®‰å…¨æ€§çš„ç ”ç©¶ï¼ŒåŒæ—¶èŠ‚çº¦æ ‡æ³¨æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WarNavæ˜¯ä¸€ä¸ªåŸºäºDATTALIONä»“åº“å›¾åƒæ„å»ºçš„æ–°ç°å®ä¸–ç•Œæ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†ä¸“ä¸ºè¯„ä¼°è‡ªä¸»åœ°é¢è½¦è¾†åœ¨ä¸ç»“æ„åŒ–ç¯å¢ƒä¸­çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹å¯¼èˆªèƒ½åŠ›è€Œè®¾è®¡ã€‚</li>
<li>WarNavæ•°æ®é›†å¡«è¡¥äº†ä¼ ç»ŸåŸå¸‚é©¾é©¶èµ„æºä¸æ— äººç³»ç»Ÿåœ¨æˆ˜åŒºæ‰€é¢ä¸´çš„ç‹¬ç‰¹åœºæ™¯ä¹‹é—´çš„ç©ºç™½ã€‚</li>
<li>æ–‡ç« è¯¦è¿°äº†æ„å»ºæ•°æ®é›†è¿‡ç¨‹ä¸­é‡åˆ°çš„æ–¹æ³•è®ºæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®å¼‚è´¨æ€§å’Œä¼¦ç†è€ƒè™‘ã€‚</li>
<li>æŠ¥å‘Šäº†ä½¿ç”¨å…ˆè¿›è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨WarNavä¸Šçš„åŸºçº¿ç»“æœï¼Œä»¥å»ºç«‹æ€§èƒ½å‚è€ƒã€‚</li>
<li>åˆ†æäº†è®­ç»ƒæ•°æ®ç¯å¢ƒå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21a72ec47bb31b477b38d319ad5cfade" align="middle">
<img src="https://picx.zhimg.com/v2-2ff4868e5f7a6b11a4a7b35b565b4298" align="middle">
<img src="https://picx.zhimg.com/v2-c9ef4582cd605b342aa91c058bc34bd2" align="middle">
<img src="https://picx.zhimg.com/v2-4026b9a2c630f950dccd26da2e8dbd93" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Controlling-False-Positives-in-Image-Segmentation-via-Conformal-Prediction"><a href="#Controlling-False-Positives-in-Image-Segmentation-via-Conformal-Prediction" class="headerlink" title="Controlling False Positives in Image Segmentation via Conformal Prediction"></a>Controlling False Positives in Image Segmentation via Conformal Prediction</h2><p><strong>Authors:Luca Mossina, Corentin Friedrich</strong></p>
<p>Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at <a target="_blank" rel="noopener" href="https://github.com/deel-ai-papers/conseco">https://github.com/deel-ai-papers/conseco</a>.</p>
<blockquote>
<p>å¯é çš„è¯­ä¹‰åˆ†å‰²å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†æ·±åº¦æ¨¡å‹å¾ˆå°‘å¯¹å…¶é”™è¯¯æä¾›æ˜ç¡®çš„ç»Ÿè®¡ä¿è¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„åç»­å¤„ç†æ¡†æ¶ï¼Œé€šè¿‡éåˆ†å¸ƒæ€§ã€å›¾åƒçº§æ§åˆ¶å‡é˜³æ€§é¢„æµ‹æ¥æ„å»ºç½®ä¿¡æ©è†œã€‚å¯¹äºä»»ä½•é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡å¢åŠ å¾—åˆ†é˜ˆå€¼æˆ–åº”ç”¨å½¢æ€ä¾µèš€æ¥è·å¾—ç¼©å°æ©è†œçš„å¯åµŒå¥—å®¶æ—ã€‚ä½¿ç”¨æœ‰æ ‡ç­¾çš„æ ¡å‡†é›†ï¼Œé€šè¿‡ç¬¦åˆé¢„æµ‹é€‰æ‹©ä¸€ä¸ªå•ä¸€çš„æ”¶ç¼©å‚æ•°ï¼Œç¡®ä¿å¯¹äºä¸æ ¡å‡†æ•°æ®å¯äº¤æ¢çš„æ–°å›¾åƒï¼Œç½®ä¿¡æ©è†œä¸­ä¿ç•™çš„å‡é˜³æ€§æ¯”ä¾‹åœ¨ç”¨æˆ·æŒ‡å®šçš„å®¹å¿åº¦ä»¥ä¸‹ä¸”æ¦‚ç‡è¾ƒé«˜ã€‚è¯¥æ–¹æ³•ä¸æ¨¡å‹æ— å…³ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸ºåº•å±‚é¢„æµ‹å™¨æä¾›äº†æœ‰é™æ ·æœ¬ä¿è¯ã€‚åœ¨å¤šè¾¹å½¢åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†ç›®æ ‡çº§åˆ«çš„ç»éªŒæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨è¿‡åº¦åˆ†å‰²ä¼šäº§ç”Ÿä¸´åºŠåæœçš„ç¯å¢ƒä¸­å®ç°å®ç”¨çš„é£é™©æ„ŸçŸ¥åˆ†å‰²ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/deel-ai-papers/conseco">https://github.com/deel-ai-papers/conseco</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¯é è¯­ä¹‰åˆ†å‰²å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†æ·±åº¦æ¨¡å‹å¾ˆå°‘æä¾›æ˜ç¡®çš„ç»Ÿè®¡ä¿è¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„åå¤„ç†æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç½®ä¿¡æ©è†œï¼Œå®ç°å¯¹å‡é˜³æ€§é¢„æµ‹çš„å›¾åƒçº§æ§åˆ¶ï¼Œæ— éœ€ä¾èµ–ç‰¹å®šåˆ†å¸ƒã€‚å¯¹äºä»»ä½•é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡æé«˜åˆ†æ•°é˜ˆå€¼æˆ–åº”ç”¨å½¢æ€å­¦è…èš€æ¥è·å¾—ç¼©å°çš„æ©è†œã€‚ä½¿ç”¨æ ‡ç­¾æ ¡å‡†é›†å¹¶é€šè¿‡å…±é€‚é¢„æµ‹æ¥é€‰æ‹©å•ä¸ªæ”¶ç¼©å‚æ•°ï¼Œç¡®ä¿å¯¹äºä¸æ ¡å‡†æ•°æ®å¯äº¤æ¢çš„æ–°å›¾åƒï¼Œç½®ä¿¡æ©è†œä¸­ä¿ç•™çš„å‡é˜³æ€§æ¯”ä¾‹ä½äºç”¨æˆ·æŒ‡å®šçš„å®¹å¿åº¦ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”ä¸ºåº•å±‚é¢„æµ‹å™¨æä¾›äº†æœ‰é™æ ·æœ¬ä¿è¯ã€‚åœ¨æ¯è‚‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†ç›®æ ‡çº§åˆ«çš„ç»éªŒæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºè¿‡åº¦åˆ†å‰²å…·æœ‰ä¸´åºŠåæœçš„ç¯å¢ƒæä¾›äº†å®ç”¨çš„é£é™©æ„è¯†åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯é è¯­ä¹‰åˆ†å‰²å¯¹ä¸´åºŠå†³ç­–çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ·±åº¦æ¨¡å‹åœ¨ç»Ÿè®¡ä¿è¯æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åå¤„ç†æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç½®ä¿¡æ©è†œå®ç°å¯¹å‡é˜³æ€§é¢„æµ‹çš„å›¾åƒçº§æ§åˆ¶ã€‚</li>
<li>æ¡†æ¶é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æé«˜åˆ†æ•°é˜ˆå€¼æˆ–åº”ç”¨å½¢æ€å­¦è…èš€æ¥åˆ›å»ºç¼©å°çš„æ©è†œã€‚</li>
<li>ä½¿ç”¨æ ‡ç­¾æ ¡å‡†é›†å’Œå…±é€‚é¢„æµ‹æ¥é€‰æ‹©æ”¶ç¼©å‚æ•°ã€‚</li>
<li>æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œå¹¶æä¾›äº†æœ‰é™æ ·æœ¬ä¿è¯ã€‚åœ¨æ¯è‚‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›äº†é£é™©æ„è¯†åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8f9e0fbf991d2881bdb5c9e0619bcff" align="middle">
<img src="https://picx.zhimg.com/v2-e98aab0c4826d1d2505bf2b876a5b9c5" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection"><a href="#Taming-Generative-Synthetic-Data-for-X-ray-Prohibited-Item-Detection" class="headerlink" title="Taming Generative Synthetic Data for X-ray Prohibited Item Detection"></a>Taming Generative Synthetic Data for X-ray Prohibited Item Detection</h2><p><strong>Authors:Jialong Sun, Hongguang Zhu, Weizhe Liu, Yunda Sun, Renshuai Tao, Yunchao Wei</strong></p>
<p>Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at <a target="_blank" rel="noopener" href="https://github.com/pILLOW-1/Xsyn/">https://github.com/pILLOW-1/Xsyn/</a>.</p>
<blockquote>
<p>è®­ç»ƒç¦æ­¢ç‰©å“æ£€æµ‹æ¨¡å‹éœ€è¦å¤§é‡çš„Xå…‰å®‰æ£€å›¾åƒï¼Œä½†æ”¶é›†å’Œæ ‡æ³¨è¿™äº›å›¾åƒæ—¢è€—æ—¶åˆè´¹åŠ›ã€‚ä¸ºäº†è§£å†³æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼ŒXå…‰å®‰æ£€å›¾åƒåˆæˆæ–¹æ³•é€šè¿‡åˆæˆå›¾åƒæ¥æ‰©å±•æ•°æ®é›†ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•ä¸»è¦éµå¾ªä¸¤é˜¶æ®µç®¡é“ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µè¿›è¡ŒåŠ³åŠ¨å¯†é›†å‹çš„å‰æ™¯æå–ï¼Œç„¶ååœ¨ç¬¬äºŒé˜¶æ®µè¿›è¡Œå›¾åƒåˆæˆã€‚è¿™ç§ç®¡é“æµç¨‹å¸¦æ¥äº†é¢å¤–çš„ä¸å¯é¿å…çš„äººå·¥æˆæœ¬ä¸”æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸€é˜¶æ®µXå…‰å®‰æ£€å›¾åƒåˆæˆç®¡é“ï¼ˆXsynï¼‰ï¼Œå®ƒé‡‡ç”¨äº†ä¸¤ç§æœ‰æ•ˆçš„ç­–ç•¥æ¥æé«˜åˆæˆå›¾åƒçš„ä½¿ç”¨ä»·å€¼ã€‚Cross-Attention Refinementï¼ˆCARï¼‰ç­–ç•¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾æ¥å®Œå–„è¾¹ç•Œæ¡†æ³¨é‡Šã€‚Background Occlusion Modelingï¼ˆBOMï¼‰ç­–ç•¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ˜¾å¼åœ°å»ºç«‹èƒŒæ™¯é®æŒ¡æ¨¡å‹ï¼Œä»¥æé«˜æˆåƒçš„å¤æ‚æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒXsyné¦–æ¬¡å®ç°äº†æ— éœ€é¢å¤–äººå·¥æˆæœ¬çš„é«˜è´¨é‡Xå…‰å®‰æ£€å›¾åƒåˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨mAPä¸Šæé«˜äº†1.2%ï¼Œè¶…è¿‡äº†æ‰€æœ‰å…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶ä¸”æˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå¯¹æé«˜å„ç§Xå…‰å®‰æ£€æ•°æ®é›†å’Œæ£€æµ‹å™¨çš„ç¦æ­¢ç‰©å“æ£€æµ‹æ€§èƒ½æ˜¯æœ‰ç›Šçš„ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/pILLOW-1/Xsyn/%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/pILLOW-1/Xsyn/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15299v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§åŸºäºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸€ç«™å¼Xå°„çº¿å®‰æ£€å›¾åƒåˆæˆæ–¹æ³•ï¼ˆXsynï¼‰ï¼Œé‡‡ç”¨ä¸¤ç§ç­–ç•¥æé«˜åˆæˆå›¾åƒçš„ä½¿ç”¨æ€§ã€‚å…¶ä¸­Cross-Attention Refinementç­–ç•¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾ä¼˜åŒ–è¾¹ç•Œæ¡†æ ‡æ³¨ï¼ŒBackground Occlusion Modelingç­–ç•¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ˜¾å¼å»ºæ¨¡èƒŒæ™¯é®æŒ¡ä»¥å¢å¼ºå›¾åƒå¤æ‚æ€§ã€‚Xsynæ— éœ€é¢å¤–åŠ³åŠ¨åŠ›æˆæœ¬ï¼Œå®ç°é«˜è´¨é‡Xå°„çº¿å®‰æ£€å›¾åƒåˆæˆï¼Œæå‡æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹Xå°„çº¿å®‰æ£€å›¾åƒä¸­ç¦æ­¢ç‰©å“æ£€æµ‹æ¨¡å‹è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†Xsynå›¾åƒåˆæˆæ–¹æ³•ã€‚</li>
<li>Xsyné‡‡ç”¨åŸºäºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸€ç«™å¼æµç¨‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæµç¨‹æ›´åŠ é«˜æ•ˆï¼Œæ— éœ€é¢å¤–åŠ³åŠ¨åŠ›æˆæœ¬ã€‚</li>
<li>Xsyné€šè¿‡Cross-Attention Refinementç­–ç•¥ä¼˜åŒ–è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œæé«˜åˆæˆå›¾åƒçš„å‡†ç¡®æ€§ã€‚</li>
<li>Xsyné€šè¿‡Background Occlusion Modelingç­–ç•¥åœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡èƒŒæ™¯é®æŒ¡ï¼Œå¢å¼ºå›¾åƒå¤æ‚æ€§ã€‚</li>
<li>Xsynæ–¹æ³•å®ç°äº†é«˜è´¨é‡Xå°„çº¿å®‰æ£€å›¾åƒåˆæˆï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒXsynåˆæˆçš„å›¾åƒå¯¹æé«˜ç¦æ­¢ç‰©å“æ£€æµ‹æ€§èƒ½æœ‰ç›Šï¼Œé€‚ç”¨äºå¤šç§Xå°„çº¿å®‰æ£€æ•°æ®é›†å’Œæ£€æµ‹å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b6f1d37728266ef6b9f429e8996856d" align="middle">
<img src="https://picx.zhimg.com/v2-58668a08881ba7cf636abd59c3538ba6" align="middle">
<img src="https://picx.zhimg.com/v2-1b648bc86e5e172231dd7131b5448367" align="middle">
<img src="https://picx.zhimg.com/v2-bb30402b3b3ca6699deb293ce3107b96" align="middle">
<img src="https://picx.zhimg.com/v2-ca0a2e91f3788acd39262d173122e00f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OEMA-Ontology-Enhanced-Multi-Agent-Collaboration-Framework-for-Zero-Shot-Clinical-Named-Entity-Recognition"><a href="#OEMA-Ontology-Enhanced-Multi-Agent-Collaboration-Framework-for-Zero-Shot-Clinical-Named-Entity-Recognition" class="headerlink" title="OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition"></a>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition</h2><p><strong>Authors:Xinli Tao, Xin Dong, Xuezhong Zhou</strong></p>
<p>Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMAâ€™s three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.</p>
<blockquote>
<p>ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯ä»ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­æå–ä¿¡æ¯çš„å…³é”®æŠ€æœ¯ï¼Œä½†CRFå’ŒBioClinicalBERTç­‰ç›‘ç£æ¨¡å‹éœ€è¦æ˜‚è´µçš„æ ‡æ³¨æ•°æ®ã€‚è™½ç„¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬NERå‡å°‘äº†å¯¹æ­¤ç±»æ•°æ®çš„ä¾èµ–ï¼Œä½†åœ¨ç¤ºä¾‹é€‰æ‹©ç²’åº¦å’Œæ•´åˆæç¤ºä»¥å®ç°è‡ªæˆ‘æ”¹è¿›æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OEMAï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œçš„é›¶æ ·æœ¬ä¸´åºŠNERæ¡†æ¶ã€‚OEMAçš„ä¸‰ä¸ªç»„ä»¶åŒ…æ‹¬ï¼šç”Ÿæˆç¤ºä¾‹çš„è‡ªæˆ‘æ³¨é‡Šå™¨ã€é€šè¿‡SNOMED CTè¿›è¡Œè¿‡æ»¤çš„é‰´åˆ«å™¨ï¼Œä»¥åŠä½¿ç”¨å®ä½“æè¿°è¿›è¡Œå‡†ç¡®æ¨æ–­çš„é¢„æµ‹å™¨ã€‚åœ¨MTSampleå’ŒVAERSæ•°æ®é›†ä¸Šï¼ŒOEMAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾ç¡®åŒ¹é…æ€§èƒ½ã€‚åœ¨ç›¸å…³åŒ¹é…æ–¹é¢ï¼Œå®ƒä¸ç›‘ç£çš„BioClinicalBERTç›¸åŒ¹é…å¹¶è¶…è¶Šäº†CRFã€‚OEMAé€šè¿‡æœ¬ä½“å¼•å¯¼æ¨ç†å’Œå¤šæ™ºèƒ½ä½“åä½œè§£å†³äº†å…³é”®çš„é›¶æ ·æœ¬NERæŒ‘æˆ˜ï¼Œå®ç°äº†æ¥è¿‘ç›‘ç£çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15211v1">PDF</a> 12 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¿¡æ¯æå–ä¸­è‡³å…³é‡è¦ï¼Œä½†ç›‘ç£æ¨¡å‹å¦‚CRFå’ŒBioClinicalBERTéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ã€‚é›¶æ ·æœ¬NERä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡å°‘äº†å¯¹æ­¤ä¾èµ–ï¼Œä½†åœ¨ç¤ºä¾‹é€‰æ‹©ç²’åº¦ä»¥åŠä¸è‡ªæˆ‘æ”¹è¿›ç»“åˆæ–¹é¢å­˜åœ¨é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šæ™ºèƒ½ä½“åä½œçš„é›¶æ ·æœ¬ä¸´åºŠNERæ¡†æ¶OEMAã€‚OEMAåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šç”Ÿæˆä¾‹å­çš„è‡ªæˆ‘æ³¨é‡Šå™¨ã€é€šè¿‡SNOMED CTè¿‡æ»¤å®ƒä»¬çš„é‰´åˆ«å™¨ä»¥åŠä½¿ç”¨å®ä½“æè¿°è¿›è¡Œå‡†ç¡®æ¨æ–­çš„é¢„æµ‹å™¨ã€‚åœ¨MTSæ ·æœ¬å’ŒVAERSæ•°æ®é›†ä¸Šï¼ŒOEMAå®ç°äº†ç²¾ç¡®åŒ¹é…çš„æœ€æ–°æ€§èƒ½ã€‚åœ¨ç›¸å…³åŒ¹é…ä¸‹ï¼Œå®ƒä¸ç›‘ç£çš„BioClinicalBERTç›¸åŒ¹é…å¹¶è¶…è¿‡äº†CRFã€‚OEMAé€šè¿‡æœ¬ä½“å¼•å¯¼æ¨ç†å’Œå¤šæ™ºèƒ½ä½“åä½œè§£å†³äº†é›¶æ ·æœ¬NERçš„å…³é”®æŒ‘æˆ˜ï¼Œå®ç°äº†æ¥è¿‘ç›‘ç£çš„æ€§èƒ½ï¼Œä¸ºä¸´åºŠNLPåº”ç”¨ç¨‹åºæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åœ¨ç”µå­å¥åº·è®°å½•ä¿¡æ¯æå–ä¸­å¾ˆé‡è¦ã€‚</li>
<li>ç›‘ç£æ¨¡å‹å¦‚CRFå’ŒBioClinicalBERTéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>é›¶æ ·æœ¬NERä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>é›¶æ ·æœ¬ä¸´åºŠNERæ¡†æ¶OEMAé€šè¿‡ä½¿ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¥è§£å†³ç¤ºä¾‹é€‰æ‹©ç²’åº¦ç­‰æŒ‘æˆ˜ã€‚</li>
<li>OEMAåŒ…å«è‡ªæˆ‘æ³¨é‡Šå™¨ã€é‰´åˆ«å™¨å’Œé¢„æµ‹å™¨ä¸‰ä¸ªç»„ä»¶ã€‚</li>
<li>OEMAåœ¨MTSæ ·æœ¬å’ŒVAERSæ•°æ®é›†ä¸Šå®ç°äº†ç²¾ç¡®åŒ¹é…çš„æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8907d9afc570a489fd392909d8459cd" align="middle">
<img src="https://picx.zhimg.com/v2-21998fb829da9054396bca4faf6e73bb" align="middle">
<img src="https://picx.zhimg.com/v2-245f4f945e3d4eefa9f3ed083f5a6929" align="middle">
<img src="https://picx.zhimg.com/v2-357c162c2adfa8077523bce7d9893516" align="middle">
<img src="https://picx.zhimg.com/v2-2ea71b4b75d0bd90f035c2af0a34a6dd" align="middle">
<img src="https://picx.zhimg.com/v2-e390865491f39bde304a679c11b18015" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BrainRotViT-Transformer-ResNet-Hybrid-for-Explainable-Modeling-of-Brain-Aging-from-3D-sMRI"><a href="#BrainRotViT-Transformer-ResNet-Hybrid-for-Explainable-Modeling-of-Brain-Aging-from-3D-sMRI" class="headerlink" title="BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI"></a>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI</h2><p><strong>Authors:Wasif Jalal, Md Nafiu Rahman, M. Sohel Rahman</strong></p>
<p>Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r&#x3D;0.98$, Spearman $Ï&#x3D;0.97$, $R^2&#x3D;0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimerâ€™s disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.</p>
<blockquote>
<p>ä»ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å‡†ç¡®ä¼°è®¡å¤§è„‘å¹´é¾„æ˜¯ç ”ç©¶è¡°è€å’Œç¥ç»é€€åŒ–çš„å®è´µç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ä¼ ç»Ÿå›å½’å’ŒåŸºäºCNNçš„æ–¹æ³•é¢ä¸´è¯¸å¦‚æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ã€æ„Ÿå—é‡æœ‰é™ä»¥åŠå¯¹å¼‚è´¨æ•°æ®è¿‡åº¦æ‹Ÿåˆç­‰å±€é™æ€§ã€‚çº¯Transformeræ¨¡å‹è™½ç„¶æœ‰æ•ˆï¼Œä½†éœ€è¦å¤§é‡æ•°æ®é›†å’Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†Brain ResNet over trained Vision Transformerï¼ˆBrainRotViTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œç»“åˆäº†è§†è§‰Transformerï¼ˆViTï¼‰çš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ®‹å·®CNNçš„å±€éƒ¨ç²¾ç»†åŒ–ã€‚é¦–å…ˆï¼ŒViTç¼–ç å™¨åœ¨è¾…åŠ©çš„å¹´é¾„å’Œæ€§åˆ«åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ åˆ‡ç‰‡çº§åˆ«çš„ç‰¹å¾ã€‚ç„¶åå°†å†»ç»“çš„ç¼–ç å™¨åº”ç”¨äºæ‰€æœ‰çŸ¢çŠ¶åˆ‡ç‰‡ï¼Œä»¥ç”ŸæˆåµŒå…¥å‘é‡çš„äºŒç»´çŸ©é˜µï¼Œè¯¥çŸ©é˜µè¢«è¾“å…¥åˆ°æ®‹å·®CNNå›å½’å™¨ä¸­ï¼Œå¹¶åœ¨æœ€ç»ˆçš„å…¨è¿æ¥å±‚ä¸­ç»“åˆå—è¯•è€…æ€§åˆ«æ¥ä¼°è®¡è¿ç»­çš„å¤§è„‘å¹´é¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨11ä¸ªMRIæ•°æ®é›†ï¼ˆæ¶µç›–è¶…è¿‡130ä¸ªé‡‡é›†ç«™ç‚¹ï¼‰çš„éªŒè¯ä¸Šå®ç°äº†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º3.34å¹´çš„ç»“æœï¼ˆçš®å°”é€Šç›¸å…³ç³»æ•°r&#x3D;0.98ï¼Œæ–¯çš®å°”æ›¼Ï&#x3D;0.97ï¼ŒR^2&#x3D;0.95ï¼‰ï¼Œè¶…è¶Šäº†åŸºå‡†å’Œæœ€æ–°æ¨¡å‹ã€‚å®ƒåœ¨4ä¸ªç‹¬ç«‹é˜Ÿåˆ—ä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ï¼ŒMAEåœ¨3.77è‡³5.04å¹´ä¹‹é—´ã€‚å¯¹å¤§è„‘å¹´é¾„å·®è·ï¼ˆé¢„æµ‹å¹´é¾„ä¸å®é™…å¹´é¾„ä¹‹é—´çš„å·®å¼‚ï¼‰çš„åˆ†ææ˜¾ç¤ºï¼Œè¡°è€æ¨¡å¼ä¸é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è®¤çŸ¥éšœç¢å’Œè‡ªé—­ç—‡è°±ç³»éšœç¢æœ‰å…³ã€‚æ¨¡å‹æ³¨æ„åŠ›å›¾çªå‡ºäº†ä¸è¡°è€ç›¸å…³çš„å¤§è„‘åŒºåŸŸï¼Œå°¤å…¶æ˜¯å°è„‘ä¸­éƒ¨ã€ä¸­å¤®å‰åå›ã€é¢å¶å’Œå†…ä¾§å‰é¢å¶çš®å±‚ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å¯è§£é‡Šå’Œé€šç”¨çš„è„‘é¾„é¢„æµ‹æ¡†æ¶ï¼Œç¼©å°äº†åŸºäºCNNå’ŒTransformerçš„æ–¹æ³•ä¹‹é—´çš„å·®è·ï¼Œå¹¶ä¸ºè¡°è€å’Œç¥ç»é€€åŒ–ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆVision Transformerå’Œæ®‹å·®CNNçš„æ··åˆæ¶æ„ï¼ˆBrainResNet over trained Vision Transformerï¼Œç®€ç§°BrainRotViTï¼‰ï¼Œç”¨äºä»ç»“æ„MRIå‡†ç¡®ä¼°è®¡å¤§è„‘å¹´é¾„ã€‚è¯¥æ–¹æ³•é€šè¿‡è¾…åŠ©å¹´é¾„å’Œæ€§åˆ«åˆ†ç±»ä»»åŠ¡è®­ç»ƒViTç¼–ç å™¨ä»¥å­¦ä¹ åˆ‡ç‰‡çº§ç‰¹å¾ï¼Œå¹¶ç»“åˆæ®‹å·®CNNå›å½’å™¨æ¥ä¼°è®¡è¿ç»­çš„å¤§è„‘å¹´é¾„ã€‚åœ¨è·¨è¶Šå¤šä¸ªMRIæ•°æ®é›†å’Œç‹¬ç«‹é˜Ÿåˆ—çš„éªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†å¤§è„‘å¹´é¾„å·®å¼‚ä¸é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è®¤çŸ¥éšœç¢å’Œè‡ªé—­ç—‡è°±ç³»éšœç¢ä¹‹é—´çš„å…³è”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrainRotViTæ˜¯ä¸€ä¸ªæ··åˆæ¶æ„ï¼Œç»“åˆäº†Vision Transformerå’Œæ®‹å·®CNNçš„ä¼˜ç‚¹ï¼Œç”¨äºå¤§è„‘å¹´é¾„ä¼°è®¡ã€‚</li>
<li>ViTç¼–ç å™¨é€šè¿‡è¾…åŠ©ä»»åŠ¡å­¦ä¹ åˆ‡ç‰‡çº§ç‰¹å¾ï¼Œç„¶åä¸æ®‹å·®CNNå›å½’å™¨ç»“åˆï¼Œä»¥ä¼°è®¡è¿ç»­çš„å¤§è„‘å¹´é¾„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªMRIæ•°æ®é›†å’Œç‹¬ç«‹é˜Ÿåˆ—ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é«˜å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°å¤§è„‘å¹´é¾„å·®å¼‚ä¸æŸäº›ç–¾ç—…ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è®¤çŸ¥éšœç¢å’Œè‡ªé—­ç—‡è°±ç³»éšœç¢ï¼‰ä¹‹é—´å­˜åœ¨å…³è”ã€‚</li>
<li>æ¨¡å‹æ³¨æ„åŠ›å›¾çªå‡ºäº†ä¸è¡°è€ç›¸å…³çš„å¤§è„‘åŒºåŸŸï¼ŒåŒ…æ‹¬å°è„‘ã€å‰é¢å¶ã€é¢å¶ç­‰ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯è§£é‡Šå’Œé€šç”¨çš„æ¡†æ¶ï¼Œä¸ºå¤§è„‘å¹´é¾„é¢„æµ‹å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50654beac7a4cd71a34b50943bb9cdc2" align="middle">
<img src="https://picx.zhimg.com/v2-6fcb650d843d5eeb0a9ed6fb730197a9" align="middle">
<img src="https://picx.zhimg.com/v2-6171a960ca793fb45096786bfe6206bf" align="middle">
<img src="https://picx.zhimg.com/v2-b095e995a7b33088fd7ade0d4b37a2dd" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="WaveFuse-AL-Cyclical-and-Performance-Adaptive-Multi-Strategy-Active-Learning-for-Medical-Images"><a href="#WaveFuse-AL-Cyclical-and-Performance-Adaptive-Multi-Strategy-Active-Learning-for-Medical-Images" class="headerlink" title="WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images"></a>WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images</h2><p><strong>Authors:Nishchala Thakur, Swati Kochhar, Deepti R. Bathula, Sukrit Gupta</strong></p>
<p>Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.</p>
<blockquote>
<p>ä¸»åŠ¨å­¦ä¹ é€šè¿‡ç­–ç•¥æ€§åœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œé™ä½äº†åŒ»å­¦æˆåƒçš„æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œä¸åŒçš„ä¸»åŠ¨å­¦ä¹ å‘¨æœŸé˜¶æ®µï¼Œä¸ªä½“è·å–ç­–ç•¥å¾€å¾€è¡¨ç°å‡ºä¸ä¸€è‡´çš„è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†å¾ªç¯å’Œæ€§èƒ½è‡ªé€‚åº”å¤šç­–ç•¥ä¸»åŠ¨å­¦ä¹ ï¼ˆWaveFuse-ALï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°èåˆå¤šç§å·²å»ºç«‹çš„è·å–ç­–ç•¥ï¼ŒåŒ…æ‹¬BALDã€BADGEã€ç†µå’ŒCoreSetï¼Œè´¯ç©¿å­¦ä¹ è¿‡ç¨‹ã€‚WaveFuse-ALç»“åˆäº†å¾ªç¯ï¼ˆæ­£å¼¦ï¼‰æ—¶é—´å…ˆéªŒçŸ¥è¯†å’Œæ€§èƒ½é©±åŠ¨çš„è‡ªé€‚åº”ï¼Œä»¥éšæ—¶é—´åŠ¨æ€è°ƒæ•´ç­–ç•¥çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šå¯¹WaveFuse-ALè¿›è¡Œäº†è¯„ä¼°ï¼šAPTOS-2019ï¼ˆå¤šç±»åˆ†ç±»ï¼‰ã€RSNAè‚ºç‚æ£€æµ‹ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰å’ŒISIC-2018ï¼ˆçš®è‚¤ç—…å˜åˆ†å‰²ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaveFuse-ALå§‹ç»ˆä¼˜äºå•ç­–ç•¥å’Œäº¤æ›¿ç­–ç•¥çš„åŸºçº¿ï¼Œåœ¨12ä¸ªåº¦é‡æŒ‡æ ‡ä¸­çš„10ä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†ç»Ÿè®¡ä¸Šæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°æé«˜äº†æœ‰é™æ ‡æ³¨é¢„ç®—çš„æ•ˆç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15132v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­çš„æ ‡æ³¨æˆæœ¬é™ä½æ•ˆæœï¼Œæå‡ºä¸€ç§æ–°å‹æ¡†æ¶WaveFuse-ALï¼Œè¯¥æ¡†æ¶å¯è‡ªé€‚åº”èåˆå¤šç§é‡‡é›†ç­–ç•¥ï¼Œå¦‚BALDã€BADGEã€Entropyå’ŒCoreSetã€‚WaveFuse-ALé€šè¿‡ç»“åˆå¾ªç¯ï¼ˆæ­£å¼¦ï¼‰æ—¶é—´å…ˆéªŒçŸ¥è¯†å’Œæ€§èƒ½é©±åŠ¨çš„è‡ªé€‚åº”æœºåˆ¶ï¼Œåœ¨ä¸åŒå­¦ä¹ é˜¶æ®µåŠ¨æ€è°ƒæ•´ç­–ç•¥é‡è¦æ€§ã€‚åœ¨APTOS-2019ã€RSNAè‚ºç‚æ£€æµ‹å’ŒISIC-2018ä¸‰ä¸ªåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒWaveFuse-ALè¡¨ç°å‡ºè‰²ï¼Œä¸€è‡´ä¼˜äºå•ç­–ç•¥å’Œäº¤æ›¿ç­–ç•¥åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨åä¸ªåº¦é‡æŒ‡æ ‡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»åŠ¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­é€šè¿‡é€‰æ‹©æ€§æ ‡æ³¨æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œé™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>WaveFuse-ALæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå¯è‡ªé€‚åº”èåˆå¤šç§é‡‡é›†ç­–ç•¥ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆæœã€‚</li>
<li>WaveFuse-ALç»“åˆäº†å¾ªç¯ï¼ˆæ­£å¼¦ï¼‰æ—¶é—´å…ˆéªŒçŸ¥è¯†å’Œæ€§èƒ½é©±åŠ¨çš„è‡ªé€‚åº”æœºåˆ¶ï¼Œèƒ½åŠ¨æ€è°ƒæ•´ç­–ç•¥é‡è¦æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒWaveFuse-ALæ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å•ç­–ç•¥å’Œäº¤æ›¿ç­–ç•¥ã€‚</li>
<li>WaveFuse-ALæœ€å¤§åŒ–äº†æœ‰é™æ ‡æ³¨é¢„ç®—çš„æ•ˆç”¨ã€‚</li>
<li>WaveFuse-ALåœ¨åäºŒä¸ªåº¦é‡æŒ‡æ ‡ä¸­çš„åä¸ªå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-261a6d7202ca9c306f9bbb5fd020f4e4" align="middle">
<img src="https://picx.zhimg.com/v2-ccef9f4128180d6f72ddcf95b80a08c2" align="middle">
<img src="https://picx.zhimg.com/v2-5ae10900e19f76d5ba071d199362cd7b" align="middle">
<img src="https://picx.zhimg.com/v2-2e85f50e6170702a5de79296639ff0e7" align="middle">
<img src="https://picx.zhimg.com/v2-2794a450e8e0b657e546188484adda1b" align="middle">
<img src="https://picx.zhimg.com/v2-3b2f91875141d9594384c1b445d26243" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Novel-Pixel-Chip-Based-Region-of-Interest-Readout-Circuit-Design"><a href="#A-Novel-Pixel-Chip-Based-Region-of-Interest-Readout-Circuit-Design" class="headerlink" title="A Novel Pixel-Chip-Based Region-of-Interest Readout Circuit Design"></a>A Novel Pixel-Chip-Based Region-of-Interest Readout Circuit Design</h2><p><strong>Authors:Shi-Qiang Zhou, Li-Rong Xie, Dong Wang, Cheng Lian, Si-Ying Liu, Zi-Yi Zhang, Xiang-Ming Sun, Hong-Bang Liu, Chao-Song Gao, Jun Liu, Huan-Bo Feng, Di-Fan Yi</strong></p>
<p>This paper presents a novel pixel chip readout scheme: the Region-of-Interest Readout Circuit (ROIRC), which is designed for large area, large array pixel chips and Gas Pixel Detector (GPD). This design employs a sentinel pixel detection strategy, enabling rapid identification and prioritized readout of the pixel regions containing signal events. During the scanning readout of these signal events, ROIRC employs a Block-based readout approach, effectively minimizing the readout of non-signal pixels. The functionality of ROIRC has been successfully implemented on both the ASIC and FPGA platforms. In the tests of the ROIRC, the pixel chip embedded in the GPD is capable of detecting low-energy X-rays in the range of 2-10 keV and supports multiple event readouts, and the pixel chip can read out photo-electron signal events with the count rate up to 15k &#x2F; (cm2 x s).</p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åƒç´ èŠ¯ç‰‡è¯»å‡ºæ–¹æ¡ˆï¼šæ„Ÿå…´è¶£åŒºåŸŸè¯»å‡ºç”µè·¯ï¼ˆROIRCï¼‰ï¼Œè¯¥æ–¹æ¡ˆé’ˆå¯¹å¤§é¢ç§¯ã€å¤§é˜µåˆ—åƒç´ èŠ¯ç‰‡å’Œæ°”ä½“åƒç´ æ¢æµ‹å™¨ï¼ˆGPDï¼‰è€Œè®¾è®¡ã€‚è¯¥è®¾è®¡é‡‡ç”¨å“¨å…µåƒç´ æ£€æµ‹ç­–ç•¥ï¼Œèƒ½å¤Ÿè¿…é€Ÿè¯†åˆ«å’Œä¼˜å…ˆè¯»å‡ºå«æœ‰ä¿¡å·äº‹ä»¶çš„åƒç´ åŒºåŸŸã€‚åœ¨æ‰«æè¯»å‡ºè¿™äº›ä¿¡å·äº‹ä»¶æ—¶ï¼ŒROIRCé‡‡ç”¨åŸºäºåŒºå—çš„è¯»å‡ºæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†éä¿¡å·åƒç´ çš„è¯»å‡ºã€‚ROIRCçš„åŠŸèƒ½å·²åœ¨ASICå’ŒFPGAå¹³å°ä¸ŠæˆåŠŸå®ç°ã€‚åœ¨ROIRCæµ‹è¯•ä¸­ï¼ŒåµŒå…¥GPDä¸­çš„åƒç´ èŠ¯ç‰‡èƒ½å¤Ÿæ£€æµ‹2-10keVèŒƒå›´å†…çš„ä½èƒ½Xå°„çº¿ï¼Œæ”¯æŒå¤šæ¬¡äº‹ä»¶è¯»å‡ºï¼Œä¸”è¯¥åƒç´ èŠ¯ç‰‡èƒ½å¤Ÿè¯»å‡ºè®¡æ•°ç‡é«˜è¾¾15k&#x2F;ï¼ˆcm2 x sï¼‰çš„å…‰ç”µå­ä¿¡å·äº‹ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15130v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹åƒç´ èŠ¯ç‰‡è¯»å‡ºæ–¹æ¡ˆâ€”â€”æ„Ÿå…´è¶£åŒºåŸŸè¯»å‡ºç”µè·¯ï¼ˆROIRCï¼‰ï¼Œé€‚ç”¨äºå¤§é¢ç§¯ã€å¤§é˜µåˆ—åƒç´ èŠ¯ç‰‡å’Œæ°”ä½“åƒç´ æ¢æµ‹å™¨ï¼ˆGPDï¼‰ã€‚é‡‡ç”¨å“¨å…µåƒç´ æ£€æµ‹ç­–ç•¥ï¼Œå¯å¿«é€Ÿè¯†åˆ«å¹¶ä¼˜å…ˆè¯»å‡ºå«æœ‰ä¿¡å·äº‹ä»¶çš„åƒç´ åŒºåŸŸï¼Œé‡‡ç”¨åŸºäºåˆ†å—çš„è¯»å‡ºæ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘éä¿¡å·åƒç´ çš„è¯»å‡ºã€‚ROIRCå·²åœ¨ASICå’ŒFPGAå¹³å°ä¸ŠæˆåŠŸå®ç°åŠŸèƒ½ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒåµŒå…¥GPDçš„åƒç´ èŠ¯ç‰‡å¯æ£€æµ‹2-10keVä½èƒ½Xå°„çº¿ï¼Œæ”¯æŒå¤šæ¬¡äº‹ä»¶è¯»å‡ºï¼Œåƒç´ èŠ¯ç‰‡å¯è¯»å‡ºè®¡æ•°ç‡é«˜è¾¾15k&#x2F;(cmÂ²Â·s)çš„å…‰ç”µå­ä¿¡å·äº‹ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ROIRCæ˜¯ä¸€ç§é’ˆå¯¹å¤§é¢ç§¯ã€å¤§é˜µåˆ—åƒç´ èŠ¯ç‰‡å’Œæ°”ä½“åƒç´ æ¢æµ‹å™¨ï¼ˆGPDï¼‰çš„æ–°å‹åƒç´ èŠ¯ç‰‡è¯»å‡ºæ–¹æ¡ˆã€‚</li>
<li>é‡‡ç”¨å“¨å…µåƒç´ æ£€æµ‹ç­–ç•¥ï¼Œå¯å¿«é€Ÿè¯†åˆ«å’Œä¼˜å…ˆè¯»å‡ºå«æœ‰ä¿¡å·äº‹ä»¶çš„åƒç´ åŒºåŸŸã€‚</li>
<li>ROIRCé‡‡ç”¨åŸºäºåˆ†å—çš„è¯»å‡ºæ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘éä¿¡å·åƒç´ çš„è¯»å‡ºã€‚</li>
<li>ROIRCå·²åœ¨ASICå’ŒFPGAå¹³å°ä¸ŠæˆåŠŸå®ç°åŠŸèƒ½ã€‚</li>
<li>åƒç´ èŠ¯ç‰‡å¯æ£€æµ‹2-10keVä½èƒ½Xå°„çº¿ã€‚</li>
<li>åƒç´ èŠ¯ç‰‡æ”¯æŒå¤šæ¬¡äº‹ä»¶è¯»å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b10f5d7f08399c07fd9a5de79a81c140" align="middle">
<img src="https://picx.zhimg.com/v2-b2fc955ca7f4b3d6905a1c0e1fc43ea0" align="middle">
<img src="https://picx.zhimg.com/v2-59e3bc82f70dc3949cbf74cd7441f7ab" align="middle">
<img src="https://picx.zhimg.com/v2-8bb73fd36b0b1ab7339a6bc00d6e7207" align="middle">
<img src="https://picx.zhimg.com/v2-131b77bacb50c2afcf4b0fb4905cac0e" align="middle">
<img src="https://picx.zhimg.com/v2-a3249167a0ba1e84222af71987857b06" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Deep-Pathomic-Learning-Defines-Prognostic-Subtypes-and-Molecular-Drivers-in-Colorectal-Cancer"><a href="#Deep-Pathomic-Learning-Defines-Prognostic-Subtypes-and-Molecular-Drivers-in-Colorectal-Cancer" class="headerlink" title="Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer"></a>Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer</h2><p><strong>Authors:Zisong Wang, Xuanyu Wang, Hang Chen, Haizhou Wang, Yuxin Chen, Yihang Xu, Yunhe Yuan, Lihuan Luo, Xitong Ling, Xiaoping Liu</strong></p>
<p>Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n&#x3D;581), validated it in an independent external cohort (n&#x3D;1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.</p>
<blockquote>
<p>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰çš„ç²¾ç¡®é¢„ååˆ†å±‚å› å…¶é«˜åº¦çš„å¼‚è´¨æ€§ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„ä¸´åºŠæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„TNMåˆ†æœŸç³»ç»Ÿå¯¹äºä¸ªæ€§åŒ–åŒ»ç–—æ¥è¯´æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘å¹¶éªŒè¯ä¸€ç§æ–°å‹çš„å¤šå®ä¾‹å­¦ä¹ æ¨¡å‹TDAM-CRCï¼Œåˆ©ç”¨ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œå‡†ç¡®çš„é¢„åé¢„æµ‹ï¼Œå¹¶æ­ç¤ºå…¶æ½œåœ¨çš„åˆ†å­æœºåˆ¶ã€‚æˆ‘ä»¬åœ¨TCGAå‘ç°é˜Ÿåˆ—ï¼ˆn&#x3D;581ï¼‰ä¸­è®­ç»ƒäº†æ¨¡å‹ï¼Œåœ¨ç‹¬ç«‹çš„å¤–éƒ¨é˜Ÿåˆ—ï¼ˆn&#x3D;1031ï¼‰ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œå¹¶è¿›ä¸€æ­¥æ•´åˆäº†å¤šç»„å­¦æ•°æ®ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å¹¶è¯†åˆ«æ–°çš„é¢„åç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ç»“æœè¡¨æ˜ï¼ŒTDAM-CRCåœ¨ä¸¤ä¸ªé˜Ÿåˆ—ä¸­å‡å®ç°äº†ç¨³å¥çš„é£é™©åˆ†å±‚ã€‚å…¶é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¸´åºŠåˆ†æœŸç³»ç»Ÿå’Œå¤šç§æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚TDAM-CRCé£é™©è¯„åˆ†åœ¨å¤šå˜é‡åˆ†æä¸­è¢«ç¡®è®¤ä¸ºç‹¬ç«‹çš„é¢„åå› ç´ ã€‚å¤šç»„å­¦åˆ†æè¡¨æ˜ï¼Œé«˜é£é™©äºšå‹ä¸ä»£è°¢é‡ç¼–ç¨‹å’Œå…ç–«æŠ‘åˆ¶çš„è‚¿ç˜¤å¾®ç¯å¢ƒå¯†åˆ‡ç›¸å…³ã€‚é€šè¿‡ç›¸äº’ä½œç”¨ç½‘ç»œåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†çº¿ç²’ä½“æ ¸ç³–ä½“è›‹ç™½L37ï¼ˆMRPL37ï¼‰æ˜¯ä¸€ä¸ªå…³é”®çš„ä¸­å¿ƒåŸºå› ï¼Œå®ƒå°†æ·±åº¦ç—…ç†ç‰¹å¾è¿æ¥åˆ°ä¸´åºŠé¢„åã€‚æˆ‘ä»¬å‘ç°ï¼Œç”±å¯åŠ¨å­ä½ç”²åŸºåŒ–é©±åŠ¨çš„é«˜MRPL37è¡¨è¾¾æ˜¯é¢„åè‰¯å¥½çš„ç‹¬ç«‹ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»“åˆTDAM-CRCé£é™©è¯„åˆ†å’Œä¸´åºŠå› ç´ çš„åˆ—çº¿å›¾ï¼Œä¸ºCRCæ‚£è€…æä¾›ç²¾ç¡®å’Œå¯è§£é‡Šçš„ä¸´åºŠå†³ç­–å·¥å…·ã€‚æˆ‘ä»¬çš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç—…ç†æ¨¡å‹TDAM-CRCä¸ºæ”¹è¿›CRCé£é™©åˆ†å±‚ã€æ­ç¤ºæ–°çš„åˆ†å­é¶æ ‡å’Œä¿ƒä¸´åºŠå†³ç­–ä¸ªæ€§åŒ–æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15067v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰çš„ç²¾ç¡®é¢„ååˆ†å±‚æ˜¯ä¸€å¤§ä¸´åºŠæŒ‘æˆ˜ï¼Œå› è‚¿ç˜¤é«˜åº¦å¼‚è´¨æ€§å¯¼è‡´å¸¸è§„TNMåˆ†æœŸç³»ç»Ÿæ— æ³•æ»¡è¶³ä¸ªæ€§åŒ–åŒ»ç–—éœ€æ±‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘å¹¶éªŒè¯ä¸€ç§æ–°å‹å¤šé‡å®ä¾‹å­¦ä¹ æ¨¡å‹TDAM-CRCï¼Œåˆ©ç”¨ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œå‡†ç¡®çš„é¢„åé¢„æµ‹ï¼Œå¹¶æ­ç¤ºå…¶æ½œåœ¨çš„åˆ†å­æœºåˆ¶ã€‚æ¨¡å‹åœ¨TCGAå‘ç°é˜Ÿåˆ—ï¼ˆn&#x3D;581ï¼‰ä¸­è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ç‹¬ç«‹çš„å¤–éƒ¨é˜Ÿåˆ—ï¼ˆn&#x3D;1031ï¼‰ä¸­éªŒè¯ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶æ•´åˆäº†å¤šç»„å­¦æ•°æ®ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å¹¶è¯†åˆ«æ–°çš„é¢„åç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ç»“æœè¡¨æ˜ï¼ŒTDAM-CRCåœ¨ä¸¤ä¸ªé˜Ÿåˆ—ä¸­å‡å®ç°äº†ç¨³å¥çš„é£é™©åˆ†å±‚ï¼Œå…¶é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå¸¸è§„ä¸´åºŠåˆ†æœŸç³»ç»Ÿå’Œå¤šç§æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚TDAM-CRCé£é™©è¯„åˆ†åœ¨å¤šå˜é‡åˆ†æä¸­è¢«ç¡®è®¤ä¸ºç‹¬ç«‹çš„é¢„åå› ç´ ã€‚å¤šç»„å­¦åˆ†ææ˜¾ç¤ºï¼Œé«˜é£é™©äºšå‹çš„è‚¿ç˜¤ä¸ä»£è°¢é‡ç¼–ç¨‹å’Œå…ç–«æŠ‘åˆ¶çš„è‚¿ç˜¤å¾®ç¯å¢ƒå¯†åˆ‡ç›¸å…³ã€‚é€šè¿‡ç›¸äº’ä½œç”¨ç½‘ç»œåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†çº¿ç²’ä½“æ ¸ç³–ä½“è›‹ç™½L37ï¼ˆMRPL37ï¼‰æ˜¯ä¸€ä¸ªå…³é”®çš„ä¸­å¿ƒåŸºå› ï¼Œå®ƒå°†æ·±åº¦ç—…ç†å­¦ç‰¹å¾è”ç³»åˆ°ä¸´åºŠé¢„åã€‚æˆ‘ä»¬å‘ç°MRPL37çš„é«˜è¡¨è¾¾ç”±å¯åŠ¨å­ä½ç”²åŸºåŒ–é©±åŠ¨ï¼Œå¯ä½œä¸ºç‹¬ç«‹çš„é¢„åç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè¯ºæ¨¡å›¾ï¼Œå°†TDAM-CRCé£é™©è¯„åˆ†å’Œä¸´åºŠå› ç´ ç›¸ç»“åˆï¼Œä¸ºCRCæ‚£è€…æä¾›ç²¾ç¡®ä¸”å¯è§£é‡Šçš„ä¸´åºŠå†³ç­–å·¥å…·ã€‚æœ¬ç ”ç©¶å¼€å‘çš„AIé©±åŠ¨çš„ç—…ç†æ¨¡å‹TDAM-CRCä¸ºæ”¹å–„CRCé£é™©åˆ†å±‚æä¾›äº†å¼ºå¤§å·¥å…·ï¼Œæ­ç¤ºäº†æ–°çš„åˆ†å­é¶ç‚¹ï¼Œä¿ƒè¿›äº†ä¸ªæ€§åŒ–çš„ä¸´åºŠå†³ç­–åˆ¶å®šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç»“ç›´è‚ ç™Œçš„é¢„ååˆ†å±‚é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>TDAM-CRCæ¨¡å‹åˆ©ç”¨å…¨ç—…ç†åˆ‡ç‰‡å›¾åƒè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>TDAM-CRCæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„TNMåˆ†æœŸç³»ç»Ÿå’Œå…¶ä»–ç°æœ‰æ¨¡å‹ã€‚</li>
<li>å¤šç»„å­¦åˆ†ææ­ç¤ºäº†ä¸é«˜é£é™©äºšå‹ç›¸å…³çš„ä»£è°¢é‡ç¼–ç¨‹å’Œå…ç–«æŠ‘åˆ¶çš„è‚¿ç˜¤å¾®ç¯å¢ƒã€‚</li>
<li>MRPL37åŸºå› è¢«é‰´å®šä¸ºè”ç³»ä¸´åºŠé¢„åå’Œç—…ç†å­¦ç‰¹å¾çš„å…³é”®åŸºå› ã€‚</li>
<li>MRPL37çš„é«˜è¡¨è¾¾æ°´å¹³å¯ä½œä¸ºç»“ç›´è‚ ç™Œçš„ç‹¬ç«‹é¢„åç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>æ„å»ºäº†åŒ…å«TDAM-CRCé£é™©è¯„åˆ†å’Œä¸´åºŠå› ç´ çš„è¯ºæ¨¡å›¾ï¼Œä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›å†³ç­–æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de0540558f89c62fb6bca26b1da54470" align="middle">
<img src="https://picx.zhimg.com/v2-095f1f515036fe3e0f40a1063749ea09" align="middle">
<img src="https://picx.zhimg.com/v2-7a835867b30da998a80806727d60f8d8" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ProPL-Universal-Semi-Supervised-Ultrasound-Image-Segmentation-via-Prompt-Guided-Pseudo-Labeling"><a href="#ProPL-Universal-Semi-Supervised-Ultrasound-Image-Segmentation-via-Prompt-Guided-Pseudo-Labeling" class="headerlink" title="ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling"></a>ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling</h2><p><strong>Authors:Yaxiong Chen, Qicong Wang, Chunlei Li, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.</p>
<blockquote>
<p>é’ˆå¯¹è¶…å£°å›¾åƒåˆ†å‰²é—®é¢˜ï¼Œç°æœ‰çš„æ–¹æ³•ï¼Œæ— è®ºæ˜¯ç›‘ç£å­¦ä¹ è¿˜æ˜¯åŠç›‘ç£å­¦ä¹ ï¼Œé€šå¸¸éƒ½é’ˆå¯¹ç‰¹å®šçš„è§£å‰–ç»“æ„æˆ–ä»»åŠ¡ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–åˆ›äº†é€šç”¨åŠç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯ä»¥å¤„ç†å¤šä¸ªå™¨å®˜å’Œåˆ†å‰²ä»»åŠ¡çš„åŒæ—¶åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®çš„ProPLæ¡†æ¶ã€‚ProPLçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…±äº«çš„è§†è§‰ç¼–ç å™¨ï¼Œç»“åˆæç¤ºå¼•å¯¼çš„åŒè§£ç å™¨ï¼Œé€šè¿‡è§£ç æç¤ºæœºåˆ¶å’Œä¸ç¡®å®šé©±åŠ¨çš„ä¼ªæ ‡ç­¾æ ¡å‡†ï¼ˆUPLCï¼‰æ¨¡å—å®ç°çµæ´»çš„ä»»åŠ¡é€‚åº”æ€§å’Œå¯é çš„è‡ªæˆ‘è®­ç»ƒã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«5ä¸ªå™¨å®˜å’Œ8ä¸ªåˆ†å‰²ä»»åŠ¡çš„å…¨é¢è¶…å£°æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProPLåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œä¸ºé€šç”¨è¶…å£°å›¾åƒåˆ†å‰²å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15057v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŠç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•ProPLï¼Œèƒ½åŒæ—¶å¤„ç†å¤šå™¨å®˜å’Œå¤šä»»åŠ¡åˆ†å‰²é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ ‡æ³¨å’Œéæ ‡æ³¨æ•°æ®ã€‚è¯¥æ–¹æ³•é€šè¿‡å…±äº«è§†è§‰ç¼–ç å™¨ä¸æç¤ºå¼•å¯¼çš„åŒè§£ç å™¨å®ç°çµæ´»çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œå¹¶é‡‡ç”¨åŸºäºä¸ç¡®å®šæ€§é©±åŠ¨ä¼ªæ ‡ç­¾æ ¡å‡†çš„UPLCæ¨¡å—è¿›è¡Œå¯é è‡ªè®­ç»ƒã€‚åŒæ—¶å¼•å…¥æ¶µç›–5ä¸ªå™¨å®˜å’Œ8ä¸ªåˆ†å‰²ä»»åŠ¡çš„å…¨é¢è¶…å£°æ•°æ®é›†ï¼Œå®éªŒè¡¨æ˜ProPLåœ¨å¤šç§æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºé€šç”¨è¶…å£°å›¾åƒåˆ†å‰²å»ºç«‹äº†æ–°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProPLæ˜¯ä¸€ç§æ–°å‹åŠç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œèƒ½å¤„ç†å¤šå™¨å®˜å’Œå¤šä»»åŠ¡åˆ†å‰²ã€‚</li>
<li>ProPLé€šè¿‡å…±äº«è§†è§‰ç¼–ç å™¨ä¸æç¤ºå¼•å¯¼çš„åŒè§£ç å™¨å®ç°ä»»åŠ¡çµæ´»æ€§ã€‚</li>
<li>UPLCæ¨¡å—åŸºäºä¸ç¡®å®šæ€§é©±åŠ¨ä¼ªæ ‡ç­¾æ ¡å‡†ï¼Œå¢å¼ºæ¨¡å‹è‡ªè®­ç»ƒèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ¶µç›–å¤šä¸ªå™¨å®˜å’Œåˆ†å‰²ä»»åŠ¡çš„å…¨é¢è¶…å£°æ•°æ®é›†ã€‚</li>
<li>ProPLåœ¨å¤šç§æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºè¶…å£°å›¾åƒåˆ†å‰²å»ºç«‹æ–°åŸºå‡†ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ ‡æ³¨å’Œéæ ‡æ³¨æ•°æ®ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-298f547c6de24b89eccf985771dee32c" align="middle">
<img src="https://picx.zhimg.com/v2-f05cb854c171df27e1292e97ecb74989" align="middle">
<img src="https://picx.zhimg.com/v2-45f2355056cac697533b61eee3320427" align="middle">
<img src="https://picx.zhimg.com/v2-fa299807482b6623edbe0cab7df7bd0f" align="middle">
<img src="https://picx.zhimg.com/v2-0fc4eacab36dbc389f59bd6ca16bfcf8" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CellGenNet-A-Knowledge-Distilled-Framework-for-Robust-Cell-Segmentation-in-Cancer-Tissues"><a href="#CellGenNet-A-Knowledge-Distilled-Framework-for-Robust-Cell-Segmentation-in-Cancer-Tissues" class="headerlink" title="CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues"></a>CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues</h2><p><strong>Authors:Srijan Ray, Bikesh K. Nirala, Jason T. Yustein, Sundaresh Ram</strong></p>
<p>Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.</p>
<blockquote>
<p>æ˜¾å¾®é•œå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­çš„ç»†èƒæ ¸ç²¾ç¡®åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæŸ“è‰²ã€æˆåƒæ¡ä»¶å’Œç»„ç»‡å½¢æ€å­˜åœ¨å·®å¼‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†CellGenNetï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æœ‰é™ç›‘ç£ä¸‹è¿›è¡Œç¨³å¥è·¨ç»„ç»‡ç»†èƒåˆ†å‰²çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚CellGenNeté‡‡ç”¨äº†ä¸€ç§å­¦ç”Ÿ-æ•™å¸ˆæ¶æ„ï¼Œå…¶ä¸­æ•™å¸ˆæ¨¡å‹åœ¨ç¨€ç–æ³¨é‡Šä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸ºæœªæ ‡è®°åŒºåŸŸç”Ÿæˆè½¯ä¼ªæ ‡ç­¾ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡æ•´åˆçœŸå®æ ‡ç­¾ã€æ•™å¸ˆç”Ÿæˆçš„æ¦‚ç‡ç›®æ ‡å’Œç»“åˆäºŒå…ƒäº¤å‰ç†µå’ŒTverskyæŸå¤±çš„æ··åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œé‡‡ç”¨ä¸å¯¹ç§°æƒ©ç½šæ¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶æ›´å¥½åœ°ä¿ç•™å°‘æ•°ç»†èƒæ ¸ç»“æ„ã€‚ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œé€å±‚ä¸¢å¼ƒè¿›ä¸€æ­¥ç¨³å®šç‰¹å¾è¡¨ç¤ºï¼Œå¹¶ä¿ƒè¿›å¯é çš„ç‰¹å¾è¿ç§»ã€‚åœ¨ä¸åŒç™Œç—‡ç»„ç»‡WSIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCellGenNetçš„åˆ†å‰²å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›è¶…è¿‡äº†ç›‘ç£å’ŒåŠç›‘ç£åŸºçº¿ï¼Œæ”¯æŒå¯æ‰©å±•å’Œå¯é‡å¤çš„ç—…ç†ç»„ç»‡åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.15054v1">PDF</a> 4 pages, 3 figures, Submitted to IEEE SSIAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†CellGenNetï¼Œä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„è·¨ç»„ç»‡ç»†èƒåˆ†å‰²æ¡†æ¶ï¼Œç”¨äºåœ¨æœ‰é™çš„ç›‘ç£æ¡ä»¶ä¸‹å®ç°æ˜¾å¾®å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸­ç»†èƒæ ¸çš„ç²¾ç¡®åˆ†å‰²ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å­¦ç”Ÿ-æ•™å¸ˆæ¶æ„ï¼Œé€šè¿‡ç¨€ç–æ³¨é‡Šè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºæœªæ ‡è®°åŒºåŸŸç”Ÿæˆè½¯ä¼ªæ ‡ç­¾ã€‚å­¦ç”Ÿæ¨¡å‹ä½¿ç”¨è”åˆç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥ç›®æ ‡ç»“åˆäº†çœŸå®æ ‡ç­¾ã€æ•™å¸ˆç”Ÿæˆçš„æ¦‚ç‡ç›®æ ‡å’Œæ··åˆæŸå¤±å‡½æ•°ï¼ˆåŒ…æ‹¬äºŒå…ƒäº¤å‰ç†µå’ŒTverskyæŸå¤±ï¼‰ï¼Œä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶æ›´å¥½åœ°ä¿ç•™å°‘æ•°ç»†èƒæ ¸ç»“æ„ã€‚é€šè¿‡ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œé€å±‚ä¸¢å¼ƒç­–ç•¥è¿›ä¸€æ­¥ç¨³å®šç‰¹å¾è¡¨ç¤ºå¹¶ä¿ƒè¿›å¯é çš„ç‰¹å¾è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼ŒCellGenNetåœ¨å¤šæ ·åŒ–çš„ç™Œç—‡ç»„ç»‡WSIsä¸Šæé«˜äº†åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ”¯æŒå¯æ‰©å±•å’Œå¯é‡å¤çš„ç»„ç»‡ç—…ç†å­¦åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CellGenNetæ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†è’¸é¦çš„è·¨ç»„ç»‡ç»†èƒåˆ†å‰²æ¡†æ¶ï¼Œç”¨äºè§£å†³æ˜¾å¾®å…¨åˆ‡ç‰‡å›¾åƒä¸­ç»†èƒæ ¸åˆ†å‰²çš„æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨å­¦ç”Ÿ-æ•™å¸ˆæ¶æ„ï¼Œå…¶ä¸­æ•™å¸ˆæ¨¡å‹åœ¨ç¨€ç–æ³¨é‡Šä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆè½¯ä¼ªæ ‡ç­¾ä»¥è¾…åŠ©å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹ä½¿ç”¨è”åˆç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œç»“åˆäº†çœŸå®æ ‡ç­¾ã€æ•™å¸ˆæ¦‚ç‡ç›®æ ‡å’Œæ··åˆæŸå¤±å‡½æ•°ã€‚</li>
<li>æ··åˆæŸå¤±å‡½æ•°åŒ…æ‹¬äºŒå…ƒäº¤å‰ç†µå’ŒTverskyæŸå¤±ï¼Œä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶ä¿ç•™å°‘æ•°ç»†èƒæ ¸ç»“æ„ã€‚</li>
<li>é€šè¿‡ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œé€å±‚ä¸¢å¼ƒç­–ç•¥æ¥ç¨³å®šç‰¹å¾è¡¨ç¤ºå¹¶ä¿ƒè¿›ç‰¹å¾è½¬ç§»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCellGenNetåœ¨å¤šæ ·åŒ–çš„ç™Œç—‡ç»„ç»‡WSIsä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.15054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51919ff43391f4479c603b146191d18b" align="middle">
<img src="https://picx.zhimg.com/v2-e643a6b358632c7cfa59a64aea52ae39" align="middle">
<img src="https://picx.zhimg.com/v2-4b8aacfa17acd30b73a7f31e27e1c7b8" align="middle">
<img src="https://picx.zhimg.com/v2-6689ac674056b7e48c4457888c53dc98" align="middle">
<img src="https://picx.zhimg.com/v2-99f77e3e9cb0c4cb85fd6085db325992" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GloTok-Global-Perspective-Tokenizer-for-Image-Reconstruction-and-Generation"><a href="#GloTok-Global-Perspective-Tokenizer-for-Image-Reconstruction-and-Generation" class="headerlink" title="GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation"></a>GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation</h2><p><strong>Authors:Xuan Zhao, Zhongyu Zhang, Yuge Huang, Yuxi Mi, Guodong Mu, Shouhong Ding, Jun Wang, Rizen Guo, Shuigeng Zhou</strong></p>
<p>Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.</p>
<blockquote>
<p>ç°æœ‰æœ€å…ˆè¿›çš„å›¾åƒæ ‡è®°åŒ–æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„å¤šç§è¯­ä¹‰ç‰¹å¾è¿›è¡Œé¢å¤–çš„ç›‘ç£ï¼Œä»¥æ‰©å¤§æ½œåœ¨è¡¨ç¤ºçš„åˆ†å¸ƒï¼Œä»è€Œæé«˜å›¾åƒé‡å»ºå’Œç”Ÿæˆçš„è´¨é‡ã€‚è¿™äº›æ–¹æ³•é‡‡ç”¨å±€éƒ¨ç›‘ç£æ–¹æ³•è¿›è¡Œè¯­ä¹‰ç›‘ç£ï¼Œè¿™é™åˆ¶äº†è¯­ä¹‰åˆ†å¸ƒçš„å‡åŒ€æ€§ã€‚ç„¶è€Œï¼ŒVA-VAEè¯æ˜æ›´å‡åŒ€çš„åˆ†å¸ƒç‰¹å¾ä¼šäº§ç”Ÿæ›´å¥½çš„ç”Ÿæˆæ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…¨å±€è§†è§’æ ‡è®°å™¨ï¼ˆGloTokï¼‰ï¼Œå®ƒåˆ©ç”¨å…¨å±€å…³ç³»ä¿¡æ¯æ¥å»ºæ¨¡æ›´å‡åŒ€çš„æ ‡è®°åŒ–ç‰¹å¾è¯­ä¹‰åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç æœ¬ç›´æ–¹å›¾çš„å…³ç³»å­¦ä¹ æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå»ºæ¨¡çš„è¯­ä¹‰è½¬ç§»åˆ°è¯­ä¹‰ä»£ç æœ¬ä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ®‹å·®å­¦ä¹ æ¨¡å—ï¼Œä»¥æ¢å¤ç»†å¾®çš„ç»†èŠ‚ï¼Œä»¥æœ€å°åŒ–é‡åŒ–å¼•èµ·çš„é‡å»ºè¯¯å·®ã€‚é€šè¿‡ä»¥ä¸Šçš„è®¾è®¡ï¼ŒGloTokæä¾›äº†æ›´å‡åŒ€åˆ†å¸ƒçš„è¯­ä¹‰æ½œåœ¨è¡¨ç¤ºï¼Œè¿™æœ‰åŠ©äºè®­ç»ƒè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€ç›´æ¥è®¿é—®é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨æ ‡å‡†çš„ImageNet-1kåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ¸…æ¥šåœ°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½å’Œç”Ÿæˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14184v2">PDF</a> Accepted at AAAIâ€™26</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§å…¨æ–°çš„å…¨å±€è§†è§’ä»¤ç‰ŒåŒ–å™¨ï¼ˆGloTokï¼‰ï¼Œå®ƒåˆ©ç”¨å…¨å±€å…³ç³»ä¿¡æ¯æ¥å»ºæ¨¡æ›´å‡åŒ€çš„è¯­ä¹‰åˆ†å¸ƒç‰¹å¾ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç§åˆ›æ–°çš„ä»£ç æœ¬ç›¸å…³ç›´æ–¹å›¾å…³ç³»å­¦ä¹ æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå»ºæ¨¡çš„è¯­ä¹‰è½¬ç§»åˆ°è¯­ä¹‰ä»£ç æœ¬ä¸­ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ®‹å·®å­¦ä¹ æ¨¡å—ï¼Œä»¥æ¢å¤ç»†èŠ‚å¹¶æœ€å°åŒ–é‡åŒ–å¼•èµ·çš„é‡å»ºè¯¯å·®ã€‚å®éªŒè¯æ˜ï¼ŒGloTokèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆçš„é‡å»ºæ€§èƒ½å’Œç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GloTokåˆ©ç”¨å…¨å±€å…³ç³»ä¿¡æ¯å»ºæ¨¡æ›´å‡åŒ€çš„è¯­ä¹‰åˆ†å¸ƒç‰¹å¾ã€‚</li>
<li>åˆ›æ–°åœ°ä½¿ç”¨äº†ä»£ç æœ¬ç›¸å…³ç›´æ–¹å›¾å…³ç³»å­¦ä¹ æ–¹æ³•ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„è¯­ä¹‰è½¬ç§»åˆ°è¯­ä¹‰ä»£ç æœ¬ä¸­ã€‚</li>
<li>è®¾è®¡äº†æ®‹å·®å­¦ä¹ æ¨¡å—æ¥æ¢å¤ç»†èŠ‚ï¼Œå¹¶æœ€å°åŒ–é‡åŒ–å¼•èµ·çš„é‡å»ºè¯¯å·®ã€‚</li>
<li>GloTokå®ç°äº†åœ¨æ²¡æœ‰ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé«˜è´¨çš„å›¾åƒç”Ÿæˆå’Œé‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ImageNet-1kæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸šç•Œé¢†å…ˆçš„é‡å»ºæ€§èƒ½å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>GloToké€šè¿‡å…¨å±€è§†è§’å»ºæ¨¡æ”¹å–„äº†ä¹‹å‰å±€éƒ¨ç›‘ç£æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>GloTokæœ‰åŠ©äºè®­ç»ƒè‡ªå›å½’æ¨¡å‹ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e6034ecaa4630d233fe00473db687aa" align="middle">
<img src="https://picx.zhimg.com/v2-0507af980c48e6a77d6bcf9e33a1e28f" align="middle">
<img src="https://picx.zhimg.com/v2-dde4e5035fe9f180e6d59e77a7b6e6fa" align="middle">
<img src="https://picx.zhimg.com/v2-d453d502ac666fe8034c6535bab7e168" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Zero-Training-Task-Specific-Model-Synthesis-for-Few-Shot-Medical-Image-Classification"><a href="#Zero-Training-Task-Specific-Model-Synthesis-for-Few-Shot-Medical-Image-Classification" class="headerlink" title="Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification"></a>Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification</h2><p><strong>Authors:Yao Qin, Yangyang Yan, YuanChao Yang, Jinhua Pang, Huanyong Bi, Yuan Liu, HaiHua Wang</strong></p>
<p>Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on â€œbig dataâ€ is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.   The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.</p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä»æ ¹æœ¬ä¸Šå—åˆ°å¤§è§„æ¨¡ç²¾ç»†æ ‡æ³¨æ•°æ®é›†è¦æ±‚çš„é™åˆ¶ã€‚å¯¹â€œå¤§æ•°æ®â€çš„ä¾èµ–æ˜¯åŒ»å­¦é¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼ŒåŒ»å­¦é¢†åŸŸæ‚£è€…çš„æ•°æ®æœ¬è´¨ä¸Šæ˜¯éš¾ä»¥è·å–çš„ï¼Œä¸“å®¶æ³¨é‡Šä¹Ÿå¾ˆæ˜‚è´µï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®šä¹‰ä¸Šæ ·æœ¬ç¨€ç¼ºçš„ç½•è§ç–¾ç—…ã€‚ä¸ºäº†å…‹æœè¿™ä¸€åŸºæœ¬æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å¼ï¼šé›¶è®­ç»ƒä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆæˆï¼ˆZS-TMSï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯é€‚åº”ç°æœ‰çš„æ¨¡å‹æˆ–è®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„ç”Ÿæˆå¼•æ“ç›´æ¥åˆæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åˆ†ç±»å™¨çš„æ•´ä¸ªå‚æ•°é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¯­ä¹‰å¼•å¯¼å‚æ•°åˆæˆå™¨ï¼ˆSGPSï¼‰ï¼Œä»¥æœ€å°‘çš„å¤šæ¨¡å¼ä»»åŠ¡ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œåªéœ€è¦ä¸€ä¸ªç¤ºä¾‹å›¾åƒï¼ˆå•æ¬¡ï¼‰å’Œç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°ï¼Œå°±å¯ä»¥ç›´æ¥åˆæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åˆ†ç±»å™¨çš„æ•´ä¸ªå‚æ•°é›†ã€‚ç”Ÿæˆå¼•æ“è§£é‡Šè¿™äº›è¾“å…¥ä»¥ç”Ÿæˆè½»é‡çº§é«˜æ•ˆåˆ†ç±»å™¨çš„æƒé‡ï¼ˆä¾‹å¦‚EfficientNet-V2ï¼‰ï¼Œè¯¥åˆ†ç±»å™¨å¯ä»¥ç«‹å³éƒ¨ç½²è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬åœ¨ISIC 2018çš®è‚¤ç—…å˜æ•°æ®é›†å’Œè‡ªå®šä¹‰ç½•è§ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘é‡æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒSGPSå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…ˆè¿›çš„å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…ä½æ•°æ®çš„1ä¸ªæ ·æœ¬å’Œ5ä¸ªæ ·æœ¬åˆ†ç±»ä¸­ã€‚è¿™é¡¹å·¥ä½œä¸ºå¿«é€Ÿå¼€å‘å’Œéƒ¨ç½²AIé©±åŠ¨çš„è¯Šæ–­å·¥å…·é“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ•°æ®ä¸¥é‡å—é™çš„ç½•è§ç–¾ç—…çš„é•¿å°¾éƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹Zero-Training Task-Specific Model Synthesisï¼ˆZS-TMSï¼‰ã€‚ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§è§„æ¨¡ç²¾ç»†æ ‡æ³¨æ•°æ®é›†ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸï¼Œè·å–æ‚£è€…æ•°æ®ä»¥åŠä¸“å®¶æ ‡æ³¨éå¸¸å›°éš¾ä¸”æˆæœ¬é«˜ã€‚æ–‡ç« åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆå¼•æ“ç›´æ¥åˆæˆä»»åŠ¡ç‰¹å®šåˆ†ç±»å™¨çš„æ‰€æœ‰å‚æ•°ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä»…éœ€è¦å°‘é‡æ¨¡æ€ä»»åŠ¡ä¿¡æ¯å’Œå•ä¾‹å›¾åƒï¼ˆå•æ¬¡æ‹æ‘„ï¼‰åŠç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç”Ÿæˆå¼•æ“èƒ½å¤Ÿè§£é‡Šè¿™äº›è¾“å…¥ç”Ÿæˆè½»é‡çº§é«˜æ•ˆåˆ†ç±»å™¨çš„æƒé‡ï¼Œç”¨äºè¿›è¡Œæ¨ç†ï¼Œæ— éœ€ç‰¹å®šçš„ä»»åŠ¡è®­ç»ƒæˆ–å¾®è°ƒã€‚åœ¨ISIC 2018çš®è‚¤ç—…å˜æ•°æ®é›†å’Œè‡ªå®šä¹‰ç½•è§ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¶…ä½æ•°æ®ç¯å¢ƒä¸‹çš„å•æ¬¡å’Œäº”æ¬¡åˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸ºAIé©±åŠ¨çš„å¿«é€Ÿå¼€å‘å’Œéƒ¨ç½²è¯Šæ–­å·¥å…·é“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æåº¦å—é™çš„ç½•è§ç–¾ç—…é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸Šå–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†ä»å—é™äºå¤§è§„æ¨¡ç²¾ç»†æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>åŒ»å­¦é¢†åŸŸçš„æ•°æ®è·å–å’Œä¸“å®¶æ ‡æ³¨éå¸¸å›°éš¾ä¸”æˆæœ¬é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç½•è§ç–¾ç—…ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•Zero-Training Task-Specific Model Synthesisï¼ˆZS-TMSï¼‰ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆå¼•æ“ç›´æ¥åˆæˆä»»åŠ¡ç‰¹å®šåˆ†ç±»å™¨çš„å‚æ•°ã€‚</li>
<li>ä»…éœ€å°‘é‡æ¨¡æ€ä»»åŠ¡ä¿¡æ¯å’Œå•ä¾‹å›¾åƒï¼ˆå•æ¬¡æ‹æ‘„ï¼‰åŠç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ã€‚</li>
<li>ç”Ÿæˆå¼•æ“èƒ½å¤Ÿè§£é‡Šè¿™äº›è¾“å…¥ç”Ÿæˆè½»é‡çº§é«˜æ•ˆåˆ†ç±»å™¨çš„æƒé‡ï¼Œæ— éœ€ç‰¹å®šçš„ä»»åŠ¡è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>åœ¨è¶…ä½æ•°æ®ç¯å¢ƒä¸‹çš„å•æ¬¡å’Œäº”æ¬¡åˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97e9f108bde6e1be2fcec377ca561061" align="middle">
<img src="https://picx.zhimg.com/v2-0d70533e591c58d9de6625e70e25b1c6" align="middle">
<img src="https://picx.zhimg.com/v2-19e256eecfef319e3427048f2afc4e25" align="middle">
<img src="https://picx.zhimg.com/v2-b2eface14ac9ce48aabfcdcc2cb6a5c7" align="middle">
<img src="https://picx.zhimg.com/v2-14a3dc02c070a6cef17b73e72fdb66e4" align="middle">
<img src="https://picx.zhimg.com/v2-423db59cc494298f795618d71d4a6699" align="middle">
<img src="https://picx.zhimg.com/v2-047e79ea2a177ccc5f4023cef03fd614" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="H-CNN-ViT-A-Hierarchical-Gated-Attention-Multi-Branch-Model-for-Bladder-Cancer-Recurrence-Prediction"><a href="#H-CNN-ViT-A-Hierarchical-Gated-Attention-Multi-Branch-Model-for-Bladder-Cancer-Recurrence-Prediction" class="headerlink" title="H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction"></a>H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction</h2><p><strong>Authors:Xueyang Li, Zongren Wang, Yuliang Zhang, Zixuan Pan, Yu-Jen Chen, Nishchal Sapkota, Gelei Xu, Danny Z. Chen, Yiyu Shi</strong></p>
<p>Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at <a target="_blank" rel="noopener" href="https://github.com/XLIAaron/H-CNN-ViT">https://github.com/XLIAaron/H-CNN-ViT</a>.</p>
<blockquote>
<p>è†€èƒ±ç™Œæ˜¯ä¸–ç•Œä¸Šæœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ä¹‹ä¸€ï¼Œå¤å‘ç‡é«˜è¾¾78%ï¼Œå› æ­¤éœ€è¦å‡†ç¡®çš„æœ¯åç›‘æµ‹ä»¥å®ç°æœ‰æ•ˆçš„æ‚£è€…ç®¡ç†ã€‚å¤šåºåˆ—å¢å¼ºMRIé€šå¸¸ç”¨äºæ£€æµ‹å¤å‘ï¼›ç„¶è€Œï¼Œç”±äºæ‰‹æœ¯åçš„æ”¹å˜ï¼Œå¦‚ç–¤ç—•ã€è‚¿èƒ€å’Œç»„ç»‡é‡å¡‘ï¼Œå³ä½¿æ˜¯æœ‰ç»éªŒçš„æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨è§£è¯»è¿™äº›æ‰«ææ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­å·¥å…·åœ¨æ”¹å–„è†€èƒ±ç™Œå¤å‘é¢„æµ‹æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†è¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°ç¼ºä¹ä¸“é—¨ç”¨äºå¤å‘è¯„ä¼°ç ”ç©¶çš„å¤šåºåˆ—MRIæ•°æ®é›†çš„é˜»ç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„å¤šåºåˆ—ã€å¤šæ¨¡å¼MRIæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè†€èƒ±ç™Œå¤å‘é¢„æµ‹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å»ºç«‹äº†å®è´µçš„åŸºå‡†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†H-CNN-ViTï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åˆ†å±‚é—¨æ§æ³¨æ„åŠ›å¤šåˆ†æ”¯æ¨¡å‹ï¼Œå®ƒå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚é€‰æ‹©æ€§åœ°åŠ æƒå…¨å±€ï¼ˆViTï¼‰å’Œå±€éƒ¨ï¼ˆCNNï¼‰è·¯å¾„çš„ç‰¹å¾ï¼Œå®ç°å¹³è¡¡ä¸”æœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾èåˆã€‚æˆ‘ä»¬çš„å¤šåˆ†æ”¯æ¶æ„ç‹¬ç«‹å¤„ç†æ¯ç§æ¨¡æ€ï¼Œç¡®ä¿æ¯ç§æˆåƒé€šé“çš„ç‹¬ç‰¹å±æ€§å¾—åˆ°æœ€ä½³æ•è·å’Œé›†æˆã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¯„ä¼°çš„H-CNN-ViTè¾¾åˆ°äº†78.6%çš„AUCï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XLIAaron/H-CNN-ViT%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/XLIAaron/H-CNN-ViTå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13869v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è†€èƒ±ç™Œå¤å‘é¢„æµ‹çš„å¤šå…ƒMRIæ•°æ®é›†å’Œæ–°å‹åˆ†å±‚é—¨æ§æ³¨æ„åŠ›å¤šåˆ†æ”¯æ¨¡å‹H-CNN-ViTã€‚è¯¥æ¨¡å‹èƒ½é€‰æ‹©æ€§åŠ æƒå…¨å±€ï¼ˆViTï¼‰å’Œå±€éƒ¨ï¼ˆCNNï¼‰è·¯å¾„çš„ç‰¹å¾ï¼Œå®ç°å¹³è¡¡ä¸”æœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾èåˆï¼Œç‹¬ç«‹å¤„ç†æ¯ç§æ¨¡æ€ï¼Œç¡®ä¿æ¯ç§æˆåƒé€šé“çš„ç‹¬ç‰¹å±æ€§å¾—åˆ°æœ€ä½³æ•æ‰å’Œé›†æˆã€‚åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šï¼ŒH-CNN-ViTçš„AUCè¾¾åˆ°äº†78.6%ï¼Œè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è†€èƒ±ç™Œå¤å‘æ£€æµ‹å¯¹æœ‰æ•ˆæ‚£è€…ç®¡ç†è‡³å…³é‡è¦ï¼Œæœ¯åç›‘æµ‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>AIè¾…åŠ©è¯Šæ–­å·¥å…·åœ¨é¢„æµ‹è†€èƒ±ç™Œå¤å‘æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç¼ºä¹ä¸“é—¨ç”¨äºå¤å‘è¯„ä¼°çš„å¤šåºåˆ—MRIæ•°æ®é›†é™åˆ¶äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤šåºåˆ—ã€å¤šæ¨¡æ€çš„MRIæ•°æ®é›†ï¼Œä¸“ä¸ºè†€èƒ±ç™Œå¤å‘é¢„æµ‹è€Œè®¾è®¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å±‚é—¨æ§æ³¨æ„åŠ›å¤šåˆ†æ”¯æ¨¡å‹H-CNN-ViTã€‚</li>
<li>H-CNN-ViTæ¨¡å‹èƒ½å¹³è¡¡å¹¶é’ˆå¯¹æ€§åœ°èåˆç‰¹å¾ï¼Œç‹¬ç«‹å¤„ç†å„ç§æ¨¡æ€ï¼Œç¡®ä¿æœ€ä½³æ•æ‰å’Œé›†æˆæ¯ç§æˆåƒé€šé“çš„ç‹¬ç‰¹å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-babaae4637504e784a1c4e4ba6555a57" align="middle">
<img src="https://picx.zhimg.com/v2-a4e6277729969261ab684c9d8b4cbe52" align="middle">
<img src="https://picx.zhimg.com/v2-a9c5995a14336abbbc088a3477f313e2" align="middle">
<img src="https://picx.zhimg.com/v2-8eee25db384b06830605ff9dbcdd0612" align="middle">
<img src="https://picx.zhimg.com/v2-0ed001410005dd0635ae133af237c27c" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-670c6f1943cfe6518915191d87368c72" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  StreamingTalker Audio-driven 3D Facial Animation with Autoregressive Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-21/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6284d0e4f6e2ce7a2a445f0345fb29d" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-21  When CNNs Outperform Transformers and Mambas Revisiting Deep Architectures for Dental Caries Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
