<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  The cross-correlation between soft X-rays and galaxies A new benchmark   for galaxy evolution models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e4d62530a026b201a3a8adfeccfe9b0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="The-cross-correlation-between-soft-X-rays-and-galaxies-A-new-benchmark-for-galaxy-evolution-models"><a href="#The-cross-correlation-between-soft-X-rays-and-galaxies-A-new-benchmark-for-galaxy-evolution-models" class="headerlink" title="The cross-correlation between soft X-rays and galaxies A new benchmark   for galaxy evolution models"></a>The cross-correlation between soft X-rays and galaxies A new benchmark   for galaxy evolution models</h2><p><strong>Authors:Johan Comparat, Andrea Merloni, Gabriele Ponti, Soumya Shreeram, Yi Zhang, Thomas H. Reiprich, Ang Liu, Riccardo Seppi, Xiaoyuan Zhang, Nicolas Clerc, Andrina Nicola, Kirpal Nandra, Mara Salvato, Nicola Malavasi</strong></p>
<p>This article presents the construction and validation of complete stellar mass-selected, volume-limited galaxy samples using the Legacy Survey (data release 10) galaxy catalogs, covering $\sim16,800$ deg$^2$ of extra-galactic sky, and extending to redshift $z&lt;0.35$. We measure the correlation function of these galaxies with tiny statistical uncertainties at the percent level and systematic uncertainties up to 5%. A 4-parameter halo occupation distribution (HOD) model is fitted to retrieve the population of host halos, yielding results on the stellar to halo mass relation consistent with the current models of galaxy formation and evolution. Using these complete galaxy samples, we measure and analyze the cross-correlation (X-corr) between galaxies and all soft X-ray photons observed by SRG&#x2F;eROSITA in the 0.5-2 keV band over $\sim13,000$ deg$^2$. The cross correlation measurements have unprecedented sub-percent statistical uncertainty and ~5-10% systematic uncertainty.   An extension to the halo model is introduced to interpret the X-corr, decomposing contributions from X-ray point sources, hot gas (CGM), satellites, and the 2-halo term. For low stellar mass thresholds ($\log M^*&#x2F;M_{\odot}&gt;$ 10, 10.25, 10.5), we find that the point source emission dominates the X-corr at small separation ($r&lt;80$kpc). Then, in the range ($80&lt;r&lt;2$Mpc), the emission from large halos hosting satellite galaxies dominates. Finally, on scales beyond that considered here ($r&gt;2$Mpc), the 2-halo term becomes dominant. Interestingly, there is no scale at which the CGM dominates. In the range ($20&lt;r&lt;200$kpc), the CGM contributes to more than 10% of the signal. Progressively, with the minimum stellar mass increasing, the CGM emission increases. We constrain the $M_{500c}-L_X$ scaling relation slope, $1.629^{+0.091}_{-0.089}$, at the 5% level using the samples with the lowest mass threshold. </p>
<blockquote>
<p>æœ¬æ–‡åˆ©ç”¨Legacy Surveyï¼ˆç¬¬10æ¬¡æ•°æ®å‘å¸ƒï¼‰æ˜Ÿç³»ç›®å½•ï¼Œæ„å»ºäº†å®Œæ•´çš„æ’æ˜Ÿè´¨é‡é€‰å®šã€ä½“ç§¯é™åˆ¶çš„æ˜Ÿç³»æ ·æœ¬ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚è¯¥æ ·æœ¬è¦†ç›–äº†çº¦16,800å¹³æ–¹åº¦çš„å¤–æ˜Ÿé™…å¤©ç©ºï¼Œå¹¶æ‰©å±•è‡³çº¢ç§»z&lt;0.35ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ˜Ÿç³»çš„ç›¸å…³æ€§å‡½æ•°è¿›è¡Œäº†æµ‹é‡ï¼Œç»Ÿè®¡è¯¯å·®æå°ï¼Œä¸ºç™¾åˆ†çº§åˆ«ï¼Œç³»ç»Ÿè¯¯å·®é«˜è¾¾5%ã€‚é‡‡ç”¨4å‚æ•°æ˜Ÿç³»æ™•å æ®åˆ†å¸ƒï¼ˆHODï¼‰æ¨¡å‹æ¥æ£€ç´¢å®¿ä¸»æ™•çš„äººå£ï¼Œå¾—åˆ°æ’æ˜Ÿä¸æ™•è´¨é‡å…³ç³»çš„ç»“æœï¼Œä¸å½“å‰çš„æ˜Ÿç³»å½¢æˆå’Œæ¼”åŒ–æ¨¡å‹ä¸€è‡´ã€‚åˆ©ç”¨è¿™äº›å®Œæ•´çš„æ˜Ÿç³»æ ·æœ¬ï¼Œæˆ‘ä»¬æµ‹é‡å¹¶åˆ†æäº†æ˜Ÿç³»ä¸SRG&#x2F;eROSITAåœ¨0.5-2åƒç”µå­ä¼æ³¢æ®µè§‚å¯Ÿåˆ°çš„æ‰€æœ‰è½¯Xå°„çº¿çš„äº¤å‰ç›¸å…³æ€§ï¼ˆX-corrï¼‰ï¼Œè¦†ç›–çº¦13,000å¹³æ–¹åº¦ã€‚äº¤å‰ç›¸å…³æ€§æµ‹é‡çš„ç»Ÿè®¡è¯¯å·®å…·æœ‰å‰æ‰€æœªæœ‰çš„æ¬¡ç™¾åˆ†æ¯”ï¼Œç³»ç»Ÿè¯¯å·®çº¦ä¸º5-10%ã€‚å¼•å…¥å¯¹æ™•æ¨¡å‹çš„æ‰©å±•æ¥è§£é‡ŠX-corrï¼Œåˆ†è§£æ¥è‡ªXå°„çº¿ç‚¹æºã€çƒ­æ°”ä½“ï¼ˆæ™•å‘¨å›´ä»‹è´¨ï¼‰ã€å«æ˜Ÿå’Œä¸¤æ™•é¡¹çš„è´¡çŒ®ã€‚å¯¹äºè¾ƒä½çš„æ’æ˜Ÿè´¨é‡é˜ˆå€¼ï¼ˆlog M*&#x2F;MâŠ™&gt; 10ã€10.25ã€10.5ï¼‰ï¼Œæˆ‘ä»¬å‘ç°ç‚¹æºå‘å°„åœ¨å°åˆ†ç¦»åº¦ï¼ˆr&lt;80åƒç§’å·®è·ï¼‰æ—¶ä¸»å¯¼X-corrã€‚ç„¶åï¼Œåœ¨ï¼ˆ80&lt;r&lt;2ç™¾ä¸‡ç§’å·®è·ï¼‰èŒƒå›´å†…ï¼Œç”±å®¿ä¸»å«æ˜Ÿæ˜Ÿç³»çš„å¤§æ™•å‘å°„å ä¸»å¯¼åœ°ä½ã€‚æœ€åï¼Œåœ¨æœ¬æ–‡è€ƒè™‘çš„å°ºåº¦ä¹‹å¤–ï¼ˆr&gt;2ç™¾ä¸‡ç§’å·®è·ï¼‰ï¼Œä¸¤æ™•é¡¹æˆä¸ºä¸»å¯¼ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ²¡æœ‰å°ºåº¦ä½¿å¾—æ™•å‘¨å›´ä»‹è´¨å ä¸»å¯¼ã€‚åœ¨ï¼ˆ20&lt;r&lt;200åƒç§’å·®è·ï¼‰èŒƒå›´å†…ï¼Œæ™•å‘¨å›´ä»‹è´¨çš„è´¡çŒ®è¶…è¿‡ä¿¡å·çš„10%ã€‚éšç€æœ€å°æ’æ˜Ÿè´¨é‡çš„å¢åŠ ï¼Œæ™•å‘¨å›´ä»‹è´¨çš„å‘å°„é‡ä¹Ÿåœ¨å¢åŠ ã€‚æˆ‘ä»¬åˆ©ç”¨å…·æœ‰æœ€ä½è´¨é‡é˜ˆå€¼çš„æ ·æœ¬ï¼Œåœ¨5%çš„æ°´å¹³ä¸Šçº¦æŸäº†M500c-Lxçš„æ ‡åº¦å…³ç³»æ–œç‡ï¼Œä¸º1.629Â±0.091ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19796v1">PDF</a> Accepted in A&amp;A</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡åˆ©ç”¨Legacy Surveyï¼ˆæ•°æ®å‘å¸ƒ10ç‰ˆï¼‰çš„æ˜Ÿç³»ç›®å½•æ„å»ºäº†å®Œæ•´çš„æ’æ˜Ÿè´¨é‡é€‰å®šçš„ä½“ç§¯é™åˆ¶æ˜Ÿç³»æ ·æœ¬ï¼Œç ”ç©¶å…¶å…³è”å‡½æ•°ï¼Œå¹¶é‡‡ç”¨4å‚æ•°æ™•å æ®åˆ†å¸ƒï¼ˆHODï¼‰æ¨¡å‹è§£æç»“æœã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜åˆ©ç”¨è¿™äº›æ˜Ÿç³»æ ·æœ¬æµ‹é‡äº†ä¸SRG&#x2F;eROSITAè§‚å¯Ÿåˆ°çš„è½¯Xå°„çº¿å…‰å­çš„äº¤å‰å…³è”ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚é€šè¿‡å¼•å…¥æ™•æ¨¡å‹çš„æ‰©å±•æ¥è§£é‡Šäº¤å‰å…³è”ï¼Œåˆ†è§£äº†æ¥è‡ªXå°„çº¿ç‚¹æºã€çƒ­æ°”ä½“ï¼ˆæ™•å‘¨å›´ä»‹è´¨ï¼‰ã€å«æ˜Ÿæ˜Ÿç³»ä»¥åŠåŒæ™•æ•ˆåº”çš„è´¡çŒ®ã€‚ç ”ç©¶è¡¨æ˜ç‚¹æºåœ¨è¾ƒå°çš„ç©ºé—´å°ºåº¦ä¸Šè´¡çŒ®æœ€å¤§ï¼Œç„¶åæ˜¯å«æ˜Ÿæ˜Ÿç³»ä¸»å¯¼çš„èŒƒå›´ï¼Œæœ€ç»ˆæ˜¯åŒæ™•æ•ˆåº”çš„èŒƒå›´ã€‚åœ¨ç‰¹å®šå°ºåº¦ä¸Šï¼Œæ™•å‘¨å›´ä»‹è´¨çš„è´¡çŒ®ä¸æ˜æ˜¾ã€‚å¯¹äºæ’æ˜Ÿè´¨é‡æœ€å°çš„æ ·æœ¬ï¼Œæ™•å‘¨å›´ä»‹è´¨çš„è´¡çŒ®è¶…è¿‡äº†ä¿¡å·çš„ç™¾åˆ†ä¹‹åã€‚éšç€æœ€å°æ’æ˜Ÿè´¨é‡çš„å¢åŠ ï¼Œæ™•å‘¨å›´ä»‹è´¨çš„å‘å°„ä¹Ÿéšä¹‹å¢åŠ ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜çº¦æŸäº†M500cä¸Lxä¹‹é—´çš„æ¯”ä¾‹å…³ç³»æ–œç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨Legacy Surveyæ•°æ®æ„å»ºäº†å®Œæ•´çš„æ’æ˜Ÿè´¨é‡é€‰å®šä½“ç§¯é™åˆ¶æ˜Ÿç³»æ ·æœ¬ã€‚</li>
<li>é€šè¿‡æµ‹é‡æ˜Ÿç³»é—´çš„å…³è”å‡½æ•°å’Œä¸SRG&#x2F;eROSITAè§‚æµ‹æ•°æ®çš„äº¤å‰å…³è”æ¥ç ”ç©¶æ˜Ÿç³»æ€§è´¨ã€‚</li>
<li>é‡‡ç”¨4å‚æ•°HODæ¨¡å‹è§£æç»“æœï¼Œå¹¶ä¸å½“å‰æ˜Ÿç³»å½¢æˆæ¼”åŒ–æ¨¡å‹ä¸€è‡´ã€‚</li>
<li>äº¤å‰å…³è”åˆ†ææ­ç¤ºäº†ç‚¹æºã€å«æ˜Ÿæ˜Ÿç³»ã€æ™•å‘¨å›´ä»‹è´¨å’ŒåŒæ™•æ•ˆåº”çš„è´¡çŒ®ã€‚</li>
<li>åœ¨ç‰¹å®šå°ºåº¦èŒƒå›´å†…ï¼Œç‚¹æºå’Œå«æ˜Ÿæ˜Ÿç³»å¯¹äº¤å‰å…³è”çš„è´¡çŒ®æ˜¾è‘—ã€‚</li>
<li>æ™•å‘¨å›´ä»‹è´¨çš„è´¡çŒ®éšç€æœ€å°æ’æ˜Ÿè´¨é‡çš„å¢åŠ è€Œå¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc1428e4b923a9d59b752340d76f3fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d09b8ca01c7bdf6b19c0aeda2717de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461095b569510c65720a6e77e9de54ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6e8e8c226c1aff366572171df3399d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14b9a5312f598e5516517f9382318177.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0aba394aa0b00e03d6da8412c90097a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a41cfa338011bc96178f9018cf0715c4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BiPrompt-SAM-Enhancing-Image-Segmentation-via-Explicit-Selection-between-Point-and-Text-Prompts"><a href="#BiPrompt-SAM-Enhancing-Image-Segmentation-via-Explicit-Selection-between-Point-and-Text-Prompts" class="headerlink" title="BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection   between Point and Text Prompts"></a>BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection   between Point and Text Prompts</h2><p><strong>Authors:Suzhe Xu, Jialin Peng, Chengyuan Zhang</strong></p>
<p>Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The recent Segment Anything Model (SAM) has demonstrated powerful point-prompt segmentation capabilities, while text-based segmentation models offer rich semantic understanding. However, existing approaches rarely explore how to effectively combine these complementary modalities for optimal segmentation performance. This paper presents BiPrompt-SAM, a novel dual-modal prompt segmentation framework that fuses the advantages of point and text prompts through an explicit selection mechanism. Specifically, we leverage SAMâ€™s inherent ability to generate multiple mask candidates, combined with a semantic guidance mask from text prompts, and explicitly select the most suitable candidate based on similarity metrics. This approach can be viewed as a simplified Mixture of Experts (MoE) system, where the point and text modules act as distinct â€œexperts,â€ and the similarity scoring serves as a rudimentary â€œgating network.â€ We conducted extensive evaluations on both the Endovis17 medical dataset and RefCOCO series natural image datasets. On Endovis17, BiPrompt-SAM achieved 89.55% mDice and 81.46% mIoU, comparable to state-of-the-art specialized medical segmentation models. On the RefCOCO series datasets, our method attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments demonstrate that our explicit dual-selection method effectively combines the spatial precision of point prompts with the semantic richness of text prompts, particularly excelling in scenarios involving semantically complex objects, multiple similar objects, and partial occlusions. BiPrompt-SAM not only provides a simple yet effective implementation but also offers a new perspective on multi-modal prompt fusion. </p>
<blockquote>
<p>åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œéšç€çµæ´»æ€§çš„è¦æ±‚ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•é€æ¸å—åˆ°é‡è§†ã€‚æœ€è¿‘çš„Segment Anything Modelï¼ˆSAMï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç‚¹æç¤ºåˆ†å‰²èƒ½åŠ›ï¼Œè€ŒåŸºäºæ–‡æœ¬çš„åˆ†å‰²æ¨¡å‹æä¾›äº†ä¸°å¯Œçš„è¯­ä¹‰ç†è§£ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾ˆå°‘æ¢ç´¢å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆè¿™äº›äº’è¡¥æ¨¡å¼ä»¥è¾¾åˆ°æœ€ä½³åˆ†å‰²æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†BiPrompt-SAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒæ¨¡æ€æç¤ºåˆ†å‰²æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜ç¡®çš„é€‰æ‹©æœºåˆ¶èåˆäº†ç‚¹å’Œæ–‡æœ¬æç¤ºçš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨SAMç”Ÿæˆå¤šä¸ªè’™ç‰ˆå€™é€‰ç‰©çš„å›ºæœ‰èƒ½åŠ›ï¼Œç»“åˆæ–‡æœ¬æç¤ºçš„è¯­ä¹‰å¼•å¯¼è’™ç‰ˆï¼Œå¹¶åŸºäºç›¸ä¼¼åº¦æŒ‡æ ‡æ˜ç¡®é€‰æ‹©æœ€åˆé€‚çš„å€™é€‰ç‰©ã€‚è¿™ç§æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªç®€åŒ–çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç³»ç»Ÿï¼Œå…¶ä¸­ç‚¹å’Œæ–‡æœ¬æ¨¡å—å……å½“ä¸åŒçš„â€œä¸“å®¶â€ï¼Œç›¸ä¼¼åº¦è¯„åˆ†å……å½“åŸºæœ¬çš„â€œç½‘å…³ç½‘ç»œâ€ã€‚æˆ‘ä»¬åœ¨Endovis17åŒ»å­¦æ•°æ®é›†å’ŒRefCOCOç³»åˆ—è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚åœ¨Endovis17ä¸Šï¼ŒBiPrompt-SAMè¾¾åˆ°äº†89.55%çš„mDiceå’Œ81.46%çš„mIoUï¼Œä¸æœ€å…ˆè¿›çš„ä¸“ç”¨åŒ»å­¦åˆ†å‰²æ¨¡å‹ç›¸å½“ã€‚åœ¨RefCOCOç³»åˆ—æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†87.1%ã€86.5%å’Œ85.8%çš„IoUï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ˜¾å¼åŒé€‰æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†ç‚¹æç¤ºçš„ç©ºé—´ç²¾åº¦å’Œæ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠè¯­ä¹‰å¤æ‚å¯¹è±¡ã€å¤šä¸ªç›¸ä¼¼å¯¹è±¡å’Œå±€éƒ¨é®æŒ¡çš„åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚BiPrompt-SAMä¸ä»…æä¾›äº†ç®€å•æœ‰æ•ˆçš„å®ç°ï¼Œè¿˜ä¸ºå¤šæ¨¡å¼æç¤ºèåˆæä¾›äº†æ–°çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19769v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†BiPrompt-SAMï¼Œä¸€ç§ç»“åˆç‚¹æç¤ºå’Œæ–‡æœ¬æç¤ºçš„åŒæ¨¡æ€åˆ†å‰²æ¡†æ¶ã€‚å®ƒé€šè¿‡æ˜ç¡®çš„é€‰æ‹©æœºåˆ¶èåˆäº†ç‚¹æç¤ºå’Œæ–‡æœ¬æç¤ºçš„ä¼˜åŠ¿ï¼Œå®ç°äº†åœ¨åŒ»å­¦å’Œè‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šçš„é«˜æ€§èƒ½åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BiPrompt-SAMæ˜¯ä¸€ç§æ–°å‹çš„åŒæ¨¡æ€æç¤ºåˆ†å‰²æ¡†æ¶ï¼Œèåˆäº†ç‚¹æç¤ºå’Œæ–‡æœ¬æç¤ºçš„ä¼˜åŠ¿ã€‚</li>
<li>å®ƒåˆ©ç”¨SAMç”Ÿæˆå¤šä¸ªæ©è†œå€™é€‰ï¼Œç»“åˆæ–‡æœ¬æç¤ºçš„è¯­ä¹‰å¼•å¯¼æ©è†œï¼Œé€šè¿‡ç›¸ä¼¼æ€§åº¦é‡æ˜ç¡®é€‰æ‹©æœ€åˆé€‚çš„å€™é€‰ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªç®€åŒ–çš„Mixture of Expertsï¼ˆMoEï¼‰ç³»ç»Ÿï¼Œå…¶ä¸­ç‚¹å’Œæ–‡æœ¬æ¨¡å—å……å½“ä¸åŒçš„â€œä¸“å®¶â€ï¼Œç›¸ä¼¼æ€§è¯„åˆ†ä½œä¸ºåŸºæœ¬çš„â€œç½‘å…³ç½‘ç»œâ€ã€‚</li>
<li>åœ¨Endovis17åŒ»å­¦æ•°æ®é›†ä¸Šï¼ŒBiPrompt-SAMè¾¾åˆ°äº†89.55%çš„mDiceå’Œ81.46%çš„mIoUï¼Œä¸æœ€å…ˆè¿›çš„åŒ»ç–—åˆ†å‰²æ¨¡å‹ç›¸å½“ã€‚</li>
<li>åœ¨RefCOCOç³»åˆ—è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†87.1%ã€86.5%å’Œ85.8%çš„IoUï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æ˜ç¡®åŒé€‰ç­–ç•¥æœ‰æ•ˆåœ°ç»“åˆäº†ç‚¹æç¤ºçš„ç©ºé—´ç²¾åº¦å’Œæ–‡æœ¬æç¤ºçš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œå°¤å…¶åœ¨å¤„ç†è¯­ä¹‰å¤æ‚çš„å¯¹è±¡ã€å¤šä¸ªç›¸ä¼¼å¯¹è±¡å’Œéƒ¨åˆ†é®æŒ¡çš„åœºæ™¯æ—¶è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c085410b606497b2b4e6b4ec2db8459e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de52ca209d45cabe0504fd669e42592.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e4d62530a026b201a3a8adfeccfe9b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-Shot-AI-assisted-quantification-of-KI-67-proliferation-index-in-breast-cancer"><a href="#Single-Shot-AI-assisted-quantification-of-KI-67-proliferation-index-in-breast-cancer" class="headerlink" title="Single Shot AI-assisted quantification of KI-67 proliferation index in   breast cancer"></a>Single Shot AI-assisted quantification of KI-67 proliferation index in   breast cancer</h2><p><strong>Authors:Deepti Madurai Muthu, Priyanka S, Lalitha Rani N, P. G. Kubendran Amos</strong></p>
<p>Reliable quantification of Ki-67, a key proliferation marker in breast cancer, is essential for molecular subtyping and informed treatment planning. Conventional approaches, including visual estimation and manual counting, suffer from interobserver variability and limited reproducibility. This study introduces an AI-assisted method using the YOLOv8 object detection framework for automated Ki-67 scoring. High-resolution digital images (40x magnification) of immunohistochemically stained tumor sections were captured from Ki-67 hotspot regions and manually annotated by a domain expert to distinguish Ki-67-positive and negative tumor cells. The dataset was augmented and divided into training (80%), validation (10%), and testing (10%) subsets. Among the YOLOv8 variants tested, the Medium model achieved the highest performance, with a mean Average Precision at 50% Intersection over Union (mAP50) exceeding 85% for Ki-67-positive cells. The proposed approach offers an efficient, scalable, and objective alternative to conventional scoring methods, supporting greater consistency in Ki-67 evaluation. Future directions include developing user-friendly clinical interfaces and expanding to multi-institutional datasets to enhance generalizability and facilitate broader adoption in diagnostic practice. </p>
<blockquote>
<p>å¯¹ä¹³è…ºç™Œä¸­å…³é”®å¢æ®–æ ‡å¿—ç‰©Ki-67çš„å¯é é‡åŒ–å¯¹äºåˆ†å­äºšå‹å’ŒçŸ¥æƒ…æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›®æµ‹å’Œæ‰‹åŠ¨è®¡æ•°ï¼Œå­˜åœ¨è§‚å¯Ÿè€…é—´å·®å¼‚å’Œé‡ç°æ€§æœ‰é™çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ä½¿ç”¨YOLOv8ç›®æ ‡æ£€æµ‹æ¡†æ¶è¾…åŠ©çš„è‡ªåŠ¨åŒ–Ki-67è¯„åˆ†æ–¹æ³•ã€‚ä»Ki-67çƒ­ç‚¹åŒºåŸŸæ•è·å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²è‚¿ç˜¤ç»„ç»‡åˆ‡ç‰‡çš„é«˜åˆ†è¾¨ç‡æ•°å­—å›¾åƒï¼ˆæ”¾å¤§å€æ•°40å€ï¼‰ï¼Œå¹¶ç”±é¢†åŸŸä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šä»¥åŒºåˆ†Ki-67é˜³æ€§å’Œé˜´æ€§è‚¿ç˜¤ç»†èƒã€‚æ•°æ®é›†ç»è¿‡æ‰©å……ååˆ†ä¸ºè®­ç»ƒé›†ï¼ˆå 80%ï¼‰ã€éªŒè¯é›†ï¼ˆå 10%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆå 10%ï¼‰ã€‚åœ¨æµ‹è¯•çš„YOLOv8ç‰ˆæœ¬ä¸­ï¼Œä¸­å‹æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå¯¹äºKi-67é˜³æ€§ç»†èƒçš„å¹³å‡ç²¾åº¦ï¼ˆmAP50ï¼‰è¶…è¿‡85%ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ºä¼ ç»Ÿè¯„åˆ†æ–¹æ³•æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•å’Œå®¢è§‚çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ”¯æŒåœ¨Ki-67è¯„ä¼°ä¸­æ›´å¤§çš„ä¸€è‡´æ€§ã€‚æœªæ¥æ–¹å‘åŒ…æ‹¬å¼€å‘ç”¨æˆ·å‹å¥½çš„ä¸´åºŠç•Œé¢ï¼Œå¹¶æ‰©å±•åˆ°å¤šæœºæ„æ•°æ®é›†ï¼Œä»¥æé«˜é€šç”¨æ€§å’Œä¿ƒè¿›åœ¨è¯Šæ–­å®è·µä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19606v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨YOLOv8ç›®æ ‡æ£€æµ‹æ¡†æ¶è¾…åŠ©çš„Ki-67è¯„åˆ†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ä¹³è…ºç™Œä¸­å…³é”®å¢æ®–æ ‡å¿—ç‰©Ki-67çš„å¯é é‡åŒ–ã€‚é€šè¿‡é‡‡ç”¨é«˜åˆ†è¾¨ç‡æ•°å­—å›¾åƒï¼Œç»“åˆä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨ï¼Œå¯¹Ki-67é˜³æ€§ç»†èƒè¿›è¡Œè‡ªåŠ¨æ£€æµ‹ä¸è®¡æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOv8çš„Mediumæ¨¡å‹åœ¨Ki-67é˜³æ€§ç»†èƒæ£€æµ‹ä¸­å–å¾—äº†è¾ƒé«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAP50è¶…è¿‡85%ï¼‰ã€‚æ­¤æ–°æ–¹æ³•ä¸ºä¼ ç»Ÿçš„è¯„åˆ†æ–¹æ³•æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•å’Œå®¢è§‚çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæé«˜äº†Ki-67è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ki-67æ˜¯ä¹³è…ºç™Œä¸­é‡è¦çš„å¢æ®–æ ‡å¿—ç‰©ï¼Œå…¶å‡†ç¡®é‡åŒ–å¯¹äºåˆ†å­äºšå‹å’Œæ²»ç–—çš„è§„åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚è§†è§‰è¯„ä¼°å’Œæ‰‹åŠ¨è®¡æ•°å­˜åœ¨è§‚å¯Ÿè€…é—´å·®å¼‚å’Œå¯é‡å¤æ€§ä½çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨AIè¾…åŠ©çš„YOLOv8ç›®æ ‡æ£€æµ‹æ¡†æ¶è¿›è¡ŒKi-67è¯„åˆ†ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–æ£€æµ‹ä¸è®¡æ•°ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡å›¾åƒç»“åˆä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨ï¼Œæé«˜äº†Ki-67é˜³æ€§ç»†èƒçš„æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>YOLOv8çš„Mediumæ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ï¼ŒmAP50è¶…è¿‡85%ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•å’Œå®¢è§‚çš„è¯„ä¼°æ–¹å¼ï¼Œæé«˜äº†Ki-67è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c64a2e78e1994dc6aaabd244716eff0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6263f5c45865e50e61076fe379de6e0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SACB-Net-Spatial-awareness-Convolutions-for-Medical-Image-Registration"><a href="#SACB-Net-Spatial-awareness-Convolutions-for-Medical-Image-Registration" class="headerlink" title="SACB-Net: Spatial-awareness Convolutions for Medical Image Registration"></a>SACB-Net: Spatial-awareness Convolutions for Medical Image Registration</h2><p><strong>Authors:Xinxing Cheng, Tianyang Zhang, Wenqi Lu, Qingjie Meng, Alejandro F. Frangi, Jinming Duan</strong></p>
<p>Deep learning-based image registration methods have shown state-of-the-art performance and rapid inference speeds. Despite these advances, many existing approaches fall short in capturing spatially varying information in non-local regions of feature maps due to the reliance on spatially-shared convolution kernels. This limitation leads to suboptimal estimation of deformation fields. In this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to enhance the spatial information within feature representations. Our SACB estimates the spatial clusters within feature maps by leveraging feature similarity and subsequently parameterizes the adaptive convolution kernels across diverse regions. This adaptive mechanism generates the convolution kernels (weights and biases) tailored to spatial variations, thereby enabling the network to effectively capture spatially varying information. Building on SACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates SACBs to facilitate multi-scale flow composition, particularly addressing large deformations. Experimental results on the brain IXI and LPBA datasets as well as Abdomen CT datasets demonstrate the effectiveness of SACB and the superiority of SACB-Net over the state-of-the-art learning-based registration methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/x-xc/SACB_Net">https://github.com/x-xc/SACB_Net</a> . </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒé…å‡†æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„æ€§èƒ½å’Œå¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œä½†ç”±äºè®¸å¤šç°æœ‰æ–¹æ³•ä¾èµ–äºç©ºé—´å…±äº«å·ç§¯æ ¸ï¼Œå› æ­¤åœ¨ç‰¹å¾å›¾éå±€éƒ¨åŒºåŸŸæ•è·ç©ºé—´å˜åŒ–ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚è¿™ä¸€å±€é™æ€§å¯¼è‡´äº†å˜å½¢åœºä¼°è®¡ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§3Dç©ºé—´æ„ŸçŸ¥å·ç§¯å—ï¼ˆSACBï¼‰ä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºä¸­çš„ç©ºé—´ä¿¡æ¯ã€‚æˆ‘ä»¬çš„SACBé€šè¿‡åˆ©ç”¨ç‰¹å¾ç›¸ä¼¼æ€§æ¥ä¼°è®¡ç‰¹å¾å›¾ä¸­çš„ç©ºé—´èšç±»ï¼Œéšåå¯¹ä¸åŒåŒºåŸŸè¿›è¡Œè‡ªé€‚åº”å·ç§¯æ ¸çš„å‚æ•°åŒ–ã€‚è¿™ç§è‡ªé€‚åº”æœºåˆ¶ç”Ÿæˆäº†é’ˆå¯¹ç©ºé—´å˜åŒ–çš„å·ç§¯æ ¸ï¼ˆæƒé‡å’Œåå·®ï¼‰ï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·ç©ºé—´å˜åŒ–ä¿¡æ¯ã€‚åŸºäºSACBï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé‡‘å­—å¡”æµä¼°è®¡å™¨ï¼ˆå‘½åä¸ºSACB-Netï¼‰ï¼Œå®ƒé›†æˆäº†SACBï¼Œä»¥ä¿ƒè¿›å¤šå°ºåº¦æµç»„æˆï¼Œç‰¹åˆ«æ˜¯è§£å†³å¤§å˜å½¢é—®é¢˜ã€‚åœ¨å¤§è„‘IXIå’ŒLPBAæ•°æ®é›†ä»¥åŠè…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†SACBçš„æœ‰æ•ˆæ€§ä»¥åŠSACB-Netåœ¨æœ€æ–°å­¦ä¹ é…å‡†æ–¹æ³•ä¸­çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/x-xc/SACB_Net%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/x-xc/SACB_Netæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19592v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ å›¾åƒé…å‡†æ–¹æ³•å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å› ä¾èµ–ç©ºé—´å…±äº«å·ç§¯æ ¸è€Œéš¾ä»¥æ•æ‰ç‰¹å¾å›¾ä¸­éå±€éƒ¨åŒºåŸŸçš„ç©ºé—´å˜åŒ–ä¿¡æ¯ï¼Œå¯¼è‡´å˜å½¢åœºä¼°è®¡ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸€ç§3Dç©ºé—´æ„ŸçŸ¥å·ç§¯å—ï¼ˆSACBï¼‰ï¼Œé€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§ä¼°è®¡ç‰¹å¾å›¾å†…çš„ç©ºé—´èšç±»ï¼Œå¹¶å‚æ•°åŒ–ä¸åŒåŒºåŸŸçš„è‡ªé€‚åº”å·ç§¯æ ¸ã€‚è¿™ç§è‡ªé€‚åº”æœºåˆ¶ç”Ÿæˆé€‚åº”ç©ºé—´å˜åŒ–çš„å·ç§¯æ ¸ï¼Œä»è€Œæé«˜ç½‘ç»œæ•æ‰ç©ºé—´å˜åŒ–ä¿¡æ¯çš„èƒ½åŠ›ã€‚åŸºäºSACBï¼Œæˆ‘ä»¬å¼•å…¥é‡‘å­—å¡”æµä¼°è®¡å™¨ï¼ˆSACB-Netï¼‰ï¼Œé›†æˆSACBå®ç°å¤šå°ºåº¦æµç»„åˆï¼Œå°¤å…¶é€‚ç”¨äºå¤§å˜å½¢é—®é¢˜ã€‚åœ¨IXIè„‘ã€LPBAå’Œè…¹éƒ¨CTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜SACBçš„æœ‰æ•ˆæ€§ä»¥åŠSACB-Netç›¸è¾ƒäºå…¶ä»–å…ˆè¿›å­¦ä¹ é…å‡†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å›¾åƒé…å‡†æ–¹æ³•å…·æœ‰å“è¶Šæ€§èƒ½å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰ç‰¹å¾å›¾ä¸­éå±€éƒ¨åŒºåŸŸçš„ç©ºé—´å˜åŒ–ä¿¡æ¯ã€‚</li>
<li>3Dç©ºé—´æ„ŸçŸ¥å·ç§¯å—ï¼ˆSACBï¼‰é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§ä¼°è®¡ç©ºé—´èšç±»ï¼Œå¹¶å‚æ•°åŒ–è‡ªé€‚åº”å·ç§¯æ ¸ã€‚</li>
<li>SACBæé«˜äº†ç½‘ç»œæ•æ‰ç©ºé—´å˜åŒ–ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>SACB-Neté›†æˆSACBå®ç°å¤šå°ºåº¦æµç»„åˆï¼Œé€‚ç”¨äºå¤§å˜å½¢é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜SACBå’ŒSACB-Netçš„æœ‰æ•ˆæ€§åŠä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b69eaa46a23e925e5a88d182d04e2e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ff965a0dba4d2b4b6d930f7eadc7b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4f19dd583bb3822d77bab0404e772d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Dual-Path-UNet-with-Mamba-for-Medical-Image-Segmentation"><a href="#Prompt-Guided-Dual-Path-UNet-with-Mamba-for-Medical-Image-Segmentation" class="headerlink" title="Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation"></a>Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation</h2><p><strong>Authors:Shaolei Zhang, Jinyan Liu, Tianyi Qian, Xuesong Li</strong></p>
<p>Convolutional neural networks (CNNs) and transformers are widely employed in constructing UNet architectures for medical image segmentation tasks. However, CNNs struggle to model long-range dependencies, while transformers suffer from quadratic computational complexity. Recently, Mamba, a type of State Space Models, has gained attention for its exceptional ability to model long-range interactions while maintaining linear computational complexity. Despite the emergence of several Mamba-based methods, they still present the following limitations: first, their network designs generally lack perceptual capabilities for the original input data; second, they primarily focus on capturing global information, while often neglecting local details. To address these challenges, we propose a prompt-guided CNN-Mamba dual-path UNet, termed PGM-UNet, for medical image segmentation. Specifically, we introduce a prompt-guided residual Mamba module that adaptively extracts dynamic visual prompts from the original input data, effectively guiding Mamba in capturing global information. Additionally, we design a local-global information fusion network, comprising a local information extraction module, a prompt-guided residual Mamba module, and a multi-focus attention fusion module, which effectively integrates local and global information. Furthermore, inspired by Kolmogorov-Arnold Networks (KANs), we develop a multi-scale information extraction module to capture richer contextual information without altering the resolution. We conduct extensive experiments on the ISIC-2017, ISIC-2018, DIAS, and DRIVE. The results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in multiple medical image segmentation tasks. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œå˜å‹å™¨åœ¨ç½‘ç»œæ„å»ºä¸­å¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„UNetæ¶æ„ã€‚ç„¶è€Œï¼ŒCNNsåœ¨å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œå˜å‹å™¨åˆ™é¢ä¸´äºŒæ¬¡è®¡ç®—å¤æ‚æ€§ã€‚æœ€è¿‘ï¼ŒMambaä½œä¸ºä¸€ç§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå› å…¶å‡ºè‰²çš„é•¿è·ç¦»äº¤äº’å»ºæ¨¡èƒ½åŠ›åŒæ—¶ä¿æŒçº¿æ€§è®¡ç®—å¤æ‚æ€§è€Œå—åˆ°å…³æ³¨ã€‚å°½ç®¡å‡ºç°äº†å‡ ç§åŸºäºMambaçš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼šé¦–å…ˆï¼Œå®ƒä»¬çš„ç½‘ç»œè®¾è®¡é€šå¸¸ç¼ºä¹åŸå§‹è¾“å…¥æ•°æ®çš„æ„ŸçŸ¥èƒ½åŠ›ï¼›å…¶æ¬¡ï¼Œå®ƒä»¬ä¸»è¦å…³æ³¨æ•è·å…¨å±€ä¿¡æ¯ï¼Œå¾€å¾€å¿½ç•¥äº†å±€éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æç¤ºå¼•å¯¼çš„CNN-MambaåŒè·¯å¾„UNetï¼Œç§°ä¸ºPGM-UNetï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæç¤ºå¼•å¯¼çš„æ®‹å·®Mambaæ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°ä»åŸå§‹è¾“å…¥æ•°æ®ä¸­æå–åŠ¨æ€è§†è§‰æç¤ºï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼Mambaæ•è·å…¨å±€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå±€éƒ¨-å…¨å±€ä¿¡æ¯èåˆç½‘ç»œï¼ŒåŒ…æ‹¬å±€éƒ¨ä¿¡æ¯æå–æ¨¡å—ã€æç¤ºå¼•å¯¼çš„æ®‹å·®Mambaæ¨¡å—å’Œå¤šç„¦ç‚¹æ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œæœ‰æ•ˆåœ°èåˆäº†å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå—åˆ°Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šå°ºåº¦ä¿¡æ¯æå–æ¨¡å—ï¼Œä»¥æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œä¸æ”¹å˜åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬åœ¨ISIC-2017ã€ISIC-2018ã€DIASå’ŒDRIVEä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜å‹å™¨æ¨¡å‹çš„åº”ç”¨åŠå…¶å±€é™æ€§ã€‚ä¸ºå…‹æœè¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæç¤ºå¼•å¯¼çš„CNN-MambaåŒè·¯å¾„UNetï¼ˆPGM-UNetï¼‰çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æç¤ºå¼•å¯¼æ®‹å·®Mambaæ¨¡å—ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”æå–åŸå§‹è¾“å…¥æ•°æ®çš„åŠ¨æ€è§†è§‰æç¤ºï¼Œå¹¶æŒ‡å¯¼Mambaæ•è·å…¨å±€ä¿¡æ¯ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªå±€éƒ¨-å…¨å±€ä¿¡æ¯èåˆç½‘ç»œï¼Œå¹¶å—åˆ°Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰çš„å¯å‘ï¼Œå¼€å‘äº†å¤šå°ºåº¦ä¿¡æ¯æå–æ¨¡å—ä»¥æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CNNå’Œå˜å‹å™¨æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Mambaæ¨¡å‹èƒ½å¤Ÿå»ºæ¨¡é•¿ç¨‹ç›¸äº’ä½œç”¨å¹¶ä¿æŒçº¿æ€§è®¡ç®—å¤æ‚æ€§ï¼Œä½†ä»å­˜åœ¨ç½‘ç»œè®¾è®¡å’Œä¿¡æ¯å¤„ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæç¤ºå¼•å¯¼çš„CNN-MambaåŒè·¯å¾„UNetï¼ˆPGM-UNetï¼‰æ–¹æ³•ï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥æç¤ºå¼•å¯¼æ®‹å·®Mambaæ¨¡å—ï¼Œè‡ªé€‚åº”æå–åŠ¨æ€è§†è§‰æç¤ºï¼ŒæŒ‡å¯¼Mambaæ•è·å…¨å±€ä¿¡æ¯ã€‚</li>
<li>è®¾è®¡äº†å±€éƒ¨-å…¨å±€ä¿¡æ¯èåˆç½‘ç»œï¼ŒåŒ…æ‹¬å±€éƒ¨ä¿¡æ¯æå–æ¨¡å—ã€æç¤ºå¼•å¯¼æ®‹å·®Mambaæ¨¡å—å’Œå¤šç„¦ç‚¹æ³¨æ„åŠ›èåˆæ¨¡å—ã€‚</li>
<li>å—Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰å¯å‘çš„å¤šå°ºåº¦ä¿¡æ¯æå–æ¨¡å—ï¼Œç”¨äºæ•è·æ›´ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPGM-UNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f77ad4a3c3da7f99e8100b2ae38a90fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e35907b5d1c194e34b1d5b2e5b418c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-739581b5565e6515ae8e19ba2c6f4beb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49775232613f520a49c64a7658deabae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-154ad8700f342f735c8046f89977f9c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4e27d6255f814c58e210c8c7c077cb2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Noisier2Inverse-Self-Supervised-Learning-for-Image-Reconstruction-with-Correlated-Noise"><a href="#Noisier2Inverse-Self-Supervised-Learning-for-Image-Reconstruction-with-Correlated-Noise" class="headerlink" title="Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with   Correlated Noise"></a>Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with   Correlated Noise</h2><p><strong>Authors:Nadja Gruber, Johannes Schwab, Markus Haltmeier, Ander Biguri, Clemens Dlaska, Gyeongha Hwang</strong></p>
<p>We propose Noisier2Inverse, a correction-free self-supervised deep learning approach for general inverse prob- lems. The proposed method learns a reconstruction function without the need for ground truth samples and is ap- plicable in cases where measurement noise is statistically correlated. This includes computed tomography, where detector imperfections or photon scattering create correlated noise patterns, as well as microscopy and seismic imaging, where physical interactions during measurement introduce dependencies in the noise structure. Similar to Noisier2Noise, a key step in our approach is the generation of noisier data from which the reconstruction net- work learns. However, unlike Noisier2Noise, the proposed loss function operates in measurement space and is trained to recover an extrapolated image instead of the original noisy one. This eliminates the need for an extrap- olation step during inference, which would otherwise suffer from ill-posedness. We numerically demonstrate that our method clearly outperforms previous self-supervised approaches that account for correlated noise. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æ— æ ¡æ­£è‡ªç›‘ç£æ·±åº¦å­¦ä¹ é€šç”¨åé—®é¢˜è§£å†³æ–¹æ¡ˆâ€”â€”Noisier2Inverseã€‚è¯¥æ–¹æ³•å­¦ä¹ é‡å»ºå‡½æ•°è€Œæ— éœ€çœŸå®æ ·æœ¬ï¼Œé€‚ç”¨äºæµ‹é‡å™ªå£°ç»Ÿè®¡ç›¸å…³çš„æƒ…å†µã€‚è¿™åŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼Œå…¶ä¸­æ£€æµ‹å™¨çš„ä¸å®Œå–„æˆ–å…‰å­æ•£å°„ä¼šäº§ç”Ÿç›¸å…³çš„å™ªå£°æ¨¡å¼ï¼Œä»¥åŠæ˜¾å¾®é•œå’Œåœ°éœ‡æˆåƒï¼Œå…¶ä¸­æµ‹é‡è¿‡ç¨‹ä¸­çš„ç‰©ç†ç›¸äº’ä½œç”¨åœ¨å™ªå£°ç»“æ„ä¸­å¼•å…¥ä¾èµ–æ€§ã€‚ä¸Noisier2Noiseç±»ä¼¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®æ­¥éª¤æ˜¯ä»ä¸­ç”Ÿæˆå™ªå£°æ•°æ®ä»¥é‡å»ºç½‘ç»œå­¦ä¹ ã€‚ç„¶è€Œï¼Œä¸Noisier2Noiseä¸åŒçš„æ˜¯ï¼Œæ‰€æå‡ºçš„æŸå¤±å‡½æ•°åœ¨æµ‹é‡ç©ºé—´ä¸Šæ“ä½œï¼Œå¹¶ç»è¿‡è®­ç»ƒä»¥æ¢å¤æ¨æ–­å›¾åƒè€Œä¸æ˜¯åŸå§‹çš„å™ªå£°å›¾åƒã€‚è¿™æ¶ˆé™¤äº†æ¨æ–­è¿‡ç¨‹ä¸­éœ€è¦å¤–æ¨çš„ä¸€æ­¥ï¼Œå¦åˆ™ä¼šå—åˆ°ä¸é€‚å®šæ€§çš„å½±å“ã€‚æˆ‘ä»¬æ•°å€¼è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºä»¥å‰è€ƒè™‘ç›¸å…³å™ªå£°çš„è‡ªç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19468v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Noisier2Inverseæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ ¡æ­£çš„è‡ªç›‘ç£æ·±åº¦å­¦ä¹ é€šç”¨åé—®é¢˜è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ— éœ€çœŸå®æ ·æœ¬å³å¯å­¦ä¹ é‡å»ºå‡½æ•°ï¼Œé€‚ç”¨äºæµ‹é‡å™ªå£°ç»Ÿè®¡ç›¸å…³çš„æƒ…å†µï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æã€æ˜¾å¾®é•œå’Œåœ°éœ‡æˆåƒç­‰ã€‚è¯¥æ–¹æ³•ç±»ä¼¼äºNoisier2Noiseï¼Œå…³é”®æ­¥éª¤æ˜¯ä»å™ªå£°æ•°æ®ä¸­ç”Ÿæˆç”¨äºè®­ç»ƒé‡å»ºç½‘ç»œçš„æ•°æ®ã€‚ç„¶è€Œï¼Œä¸Noisier2Noiseä¸åŒçš„æ˜¯ï¼Œæ‰€æè®®çš„æŸå¤±å‡½æ•°åœ¨æµ‹é‡ç©ºé—´å†…è¿è¡Œï¼Œå¹¶ç»è¿‡è®­ç»ƒä»¥æ¢å¤å¤–æ¨å›¾åƒè€Œä¸æ˜¯åŸå§‹å™ªå£°å›¾åƒã€‚è¿™æ¶ˆé™¤äº†æ¨æ–­è¿‡ç¨‹ä¸­éœ€è¦çš„å¤–æ¨æ­¥éª¤ï¼Œä»è€Œé¿å…äº†ä¸é€‚å®šæ€§é—®é¢˜ã€‚æœ¬æ–‡æ•°å€¼è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜æ˜¾ä¼˜äºå¤„ç†ç›¸å…³å™ªå£°çš„å…ˆå‰è‡ªç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Noisier2Inverseæ˜¯ä¸€ç§æ— éœ€æ ¡æ­£çš„è‡ªç›‘ç£æ·±åº¦å­¦ä¹ é€šç”¨åé—®é¢˜è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨æ— éœ€çœŸå®æ ·æœ¬çš„æƒ…å†µä¸‹å­¦ä¹ é‡å»ºå‡½æ•°ã€‚</li>
<li>Noisier2Inverseé€‚ç”¨äºæµ‹é‡å™ªå£°ç»Ÿè®¡ç›¸å…³çš„æƒ…å†µï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æã€æ˜¾å¾®é•œå’Œåœ°éœ‡æˆåƒã€‚</li>
<li>Noisier2Inverseä¸Noisier2Noiseç±»ä¼¼ï¼Œå…³é”®æ­¥éª¤åœ¨äºä»å™ªå£°æ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒé‡å»ºç½‘ç»œæ‰€éœ€çš„æ•°æ®ã€‚</li>
<li>æ‰€æè®®çš„æŸå¤±å‡½æ•°åœ¨æµ‹é‡ç©ºé—´å†…è¿è¡Œï¼Œå¹¶ç»è¿‡è®­ç»ƒä»¥æ¢å¤å¤–æ¨å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•æ¶ˆé™¤äº†æ¨æ–­è¿‡ç¨‹ä¸­éœ€è¦çš„å¤–æ¨æ­¥éª¤ï¼Œé¿å…äº†ä¸é€‚å®šæ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-34c36467e2074ca13aeebf16700a3af1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ASP-VMUNet-Atrous-Shifted-Parallel-Vision-Mamba-U-Net-for-Skin-Lesion-Segmentation"><a href="#ASP-VMUNet-Atrous-Shifted-Parallel-Vision-Mamba-U-Net-for-Skin-Lesion-Segmentation" class="headerlink" title="ASP-VMUNet: Atrous Shifted Parallel Vision Mamba U-Net for Skin Lesion   Segmentation"></a>ASP-VMUNet: Atrous Shifted Parallel Vision Mamba U-Net for Skin Lesion   Segmentation</h2><p><strong>Authors:Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Qi Zhao, Changyu Zeng, Wenpei Bai, Guangliang Cheng</strong></p>
<p>Skin lesion segmentation is a critical challenge in computer vision, and it is essential to separate pathological features from healthy skin for diagnostics accurately. Traditional Convolutional Neural Networks (CNNs) are limited by narrow receptive fields, and Transformers face significant computational burdens. This paper presents a novel skin lesion segmentation framework, the Atrous Shifted Parallel Vision Mamba UNet (ASP-VMUNet), which integrates the efficient and scalable Mamba architecture to overcome limitations in traditional CNNs and computationally demanding Transformers. The framework introduces an atrous scan technique that minimizes background interference and expands the receptive field, enhancing Mambaâ€™s scanning capabilities. Additionally, the inclusion of a Parallel Vision Mamba (PVM) layer and a shift round operation optimizes feature segmentation and fosters rich inter-segment information exchange. A supplementary CNN branch with a Selective-Kernel (SK) Block further refines the segmentation by blending local and global contextual information. Tested on four benchmark datasets (ISIC16&#x2F;17&#x2F;18 and PH2), ASP-VMUNet demonstrates superior performance in skin lesion segmentation, validated by comprehensive ablation studies. This approach not only advances medical image segmentation but also highlights the benefits of hybrid architectures in medical imaging technology. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BaoBao0926/ASP-VMUNet/tree/main">https://github.com/BaoBao0926/ASP-VMUNet/tree/main</a>. </p>
<blockquote>
<p>çš®è‚¤ç—…å˜åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œä¸ºäº†å‡†ç¡®è¯Šæ–­ï¼Œåˆ†ç¦»ç—…ç†ç‰¹å¾ä¸æ­£å¸¸çš®è‚¤è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å—é™äºè¾ƒå°çš„æ„Ÿå—é‡ï¼Œè€ŒTransformeråˆ™é¢ä¸´å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çš®è‚¤ç—…å˜åˆ†å‰²æ¡†æ¶ï¼Œå³Atrous Shifted Parallel Vision Mamba UNetï¼ˆASP-VMUNetï¼‰ã€‚è¯¥æ¡†æ¶èåˆäº†é«˜æ•ˆå¯æ‰©å±•çš„Mambaæ¶æ„ï¼Œå…‹æœäº†ä¼ ç»ŸCNNå’Œè®¡ç®—å¯†é›†å‹Transformerçš„å±€é™æ€§ã€‚æ¡†æ¶å¼•å…¥äº†ä¸€ç§atrousæ‰«ææŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æœ€å¤§é™åº¦åœ°å‡å°‘äº†èƒŒæ™¯å¹²æ‰°å¹¶æ‰©å¤§äº†æ„Ÿå—é‡ï¼Œå¢å¼ºäº†Mambaçš„æ‰«æèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Parallel Vision Mambaï¼ˆPVMï¼‰å±‚å’Œç§»ä½æ“ä½œï¼Œä»¥ä¼˜åŒ–ç‰¹å¾åˆ†å‰²å¹¶ä¿ƒè¿›ä¸°å¯Œçš„åˆ†æ®µé—´ä¿¡æ¯äº¤æ¢ã€‚å¸¦æœ‰Selective-Kernelï¼ˆSKï¼‰å—çš„è¾…åŠ©CNNåˆ†æ”¯é€šè¿‡èåˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›ä¸€æ­¥æ”¹è¿›äº†åˆ†å‰²æ•ˆæœã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆISIC16&#x2F;17&#x2F;18å’ŒPH</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºçš®è‚¤ç—…å˜åˆ†å‰²çš„ç ”ç©¶è®ºæ–‡ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å‰²æ¡†æ¶ASP-VMUNetï¼Œç»“åˆäº†Mambaæ¶æ„çš„ä¼˜ç‚¹ï¼Œå…‹æœäº†ä¼ ç»ŸCNNå’Œè®¡ç®—é‡è¾ƒå¤§çš„Transformerçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§ç©ºæ´æ‰«ææŠ€æœ¯ï¼Œèƒ½å¤Ÿå‡å°‘èƒŒæ™¯å¹²æ‰°å¹¶æ‰©å¤§æ„Ÿå—é‡ï¼Œæé«˜äº†Mambaçš„æ‰«æèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜åŠ å…¥äº†PVMå±‚å’Œç§»ä½æ“ä½œï¼Œä¼˜åŒ–äº†ç‰¹å¾åˆ†å‰²å¹¶ä¿ƒè¿›äº†ä¸°å¯Œçš„è·¨æ®µä¿¡æ¯äº¤æ¢ã€‚è¯¥ç ”ç©¶åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯æ˜äº†ASP-VMUNetåœ¨çš®è‚¤ç—…å˜åˆ†å‰²ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çš®è‚¤ç—…å˜åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå¯¹äºå‡†ç¡®è¯Šæ–­éœ€è¦å°†ç—…ç†ç‰¹å¾ä»å¥åº·çš®è‚¤ä¸­åˆ†ç¦»å‡ºæ¥ã€‚</li>
<li>ä¼ ç»ŸCNNå’ŒTransformeråœ¨çš®è‚¤ç—…å˜åˆ†å‰²ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ASP-VMUNetæ¡†æ¶ç»“åˆäº†Mambaæ¶æ„ï¼Œæ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚</li>
<li>ç©ºæ´æ‰«ææŠ€æœ¯ç”¨äºå‡å°‘èƒŒæ™¯å¹²æ‰°å¹¶æ‰©å¤§æ„Ÿå—é‡ï¼Œæé«˜æ‰«æèƒ½åŠ›ã€‚</li>
<li>PVMå±‚å’Œç§»ä½æ“ä½œä¼˜åŒ–äº†ç‰¹å¾åˆ†å‰²ï¼Œä¿ƒè¿›äº†è·¨æ®µä¿¡æ¯äº¤æ¢ã€‚</li>
<li>ASP-VMUNetåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b2fa8db508ab412ddbff127a8adb6b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637d37bac687879831b94381c55e1608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64eb6b50d1b9e6b5b8a3acdb171b464e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd86f56e9ad8601c5f8eb74a28d69f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8316bb72542efe9a2ac36cc8cbee03d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-541e3b070498a3c5c58160565e1c079d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-modal-3D-Pose-and-Shape-Estimation-with-Computed-Tomography"><a href="#Multi-modal-3D-Pose-and-Shape-Estimation-with-Computed-Tomography" class="headerlink" title="Multi-modal 3D Pose and Shape Estimation with Computed Tomography"></a>Multi-modal 3D Pose and Shape Estimation with Computed Tomography</h2><p><strong>Authors:Mingxiao Tu, Hoijoon Jung, Alireza Moghadam, Jineel Raythatha, Lachlan Allan, Jeremy Hsu, Andre Kyme, Jinman Kim</strong></p>
<p>In perioperative care, precise in-bed 3D patient pose and shape estimation (PSE) can be vital in optimizing patient positioning in preoperative planning, enabling accurate overlay of medical images for augmented reality-based surgical navigation, and mitigating risks of prolonged immobility during recovery. Conventional PSE methods relying on modalities such as RGB-D, infrared, or pressure maps often struggle with occlusions caused by bedding and complex patient positioning, leading to inaccurate estimation that can affect clinical outcomes. To address these challenges, we present the first multi-modal in-bed patient 3D PSE network that fuses detailed geometric features extracted from routinely acquired computed tomography (CT) scans with depth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that utilizes probabilistic correspondence alignment, a pose estimation module with a refined neural network, and a final parameters mixing module. This multi-modal network robustly reconstructs occluded body regions and enhances the accuracy of the estimated 3D human mesh model. We validated mPSE-CT using proprietary whole-body rigid phantom and volunteer datasets in clinical scenarios. mPSE-CT outperformed the best-performing prior method by 23% and 49.16% in pose and shape estimation respectively, demonstrating its potential for improving clinical outcomes in challenging perioperative environments. </p>
<blockquote>
<p>åœ¨å›´æ‰‹æœ¯æœŸæŠ¤ç†ä¸­ï¼Œç²¾ç¡®çš„åºŠä¸Š3Dæ‚£è€…å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡ï¼ˆPSEï¼‰å¯¹äºä¼˜åŒ–æœ¯å‰è§„åˆ’ä¸­çš„æ‚£è€…å®šä½ã€å®ç°åŸºäºå¢å¼ºç°å®çš„æ‰‹æœ¯å¯¼èˆªçš„åŒ»å­¦å›¾åƒç²¾ç¡®å åŠ ï¼Œä»¥åŠå‡å°‘æ¢å¤æœŸé—´é•¿æœŸä¸åŠ¨çš„é£é™©è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„PSEæ–¹æ³•ä¾èµ–äºRGB-Dã€çº¢å¤–çº¿æˆ–å‹åŠ›å›¾ç­‰æ¨¡å¼ï¼Œé€šå¸¸éš¾ä»¥å¤„ç†ç”±åºŠé“ºå’Œå¤æ‚æ‚£è€…å®šä½é€ æˆçš„é®æŒ¡é—®é¢˜ï¼Œä»è€Œå¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®ï¼Œå¯èƒ½å½±å“ä¸´åºŠç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†å¤šæ¨¡å¼åºŠä¸Šæ‚£è€…3DPSEç½‘ç»œï¼ˆmPSE-CTï¼‰ï¼Œå®ƒå°†ä»å¸¸è§„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«æä¸­æå–çš„è¯¦ç»†å‡ ä½•ç‰¹å¾ä¸æ·±åº¦åœ°å›¾ç›¸ç»“åˆã€‚mPSE-CTåŒ…å«ä¸€ä¸ªåˆ©ç”¨æ¦‚ç‡å¯¹åº”å¯¹é½çš„å½¢çŠ¶ä¼°è®¡æ¨¡å—ï¼Œä¸€ä¸ªç»è¿‡ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„å§¿æ€ä¼°è®¡æ¨¡å—ï¼Œä»¥åŠä¸€ä¸ªæœ€ç»ˆå‚æ•°æ··åˆæ¨¡å—ã€‚è¿™ç§å¤šæ¨¡å¼ç½‘ç»œèƒ½å¤Ÿç¨³å¥åœ°é‡å»ºè¢«é®æŒ¡çš„èº¯ä½“åŒºåŸŸï¼Œæé«˜äº†ä¼°è®¡çš„3Däººä½“ç½‘æ ¼æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸“æœ‰å…¨èº«åˆšæ€§å¹»å½±å’Œå¿—æ„¿è€…æ•°æ®é›†åœ¨ä¸´åºŠåœºæ™¯ä¸­å¯¹mPSE-CTè¿›è¡Œäº†éªŒè¯ã€‚åœ¨å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡æ–¹é¢ï¼ŒmPSE-CTåˆ†åˆ«æ¯”æœ€ä½³å‰æœŸæ–¹æ³•é«˜å‡º23%å’Œ49.16%ï¼Œè¡¨æ˜å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›´æ‰‹æœ¯æœŸç¯å¢ƒä¸­æ”¹å–„ä¸´åºŠç»“æœçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å›´æœ¯æœŸæŠ¤ç†ä¸­ï¼Œç²¾ç¡®çš„åºŠä½3Dæ‚£è€…å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡ï¼ˆPSEï¼‰çš„é‡è¦æ€§ï¼Œå¯¹äºä¼˜åŒ–æ‚£è€…å®šä½ã€å¢å¼ºç°å®æ‰‹æœ¯å¯¼èˆªçš„åŒ»å­¦å›¾åƒå åŠ ä»¥åŠå‡å°‘æ¢å¤æœŸé—´çš„é•¿æœŸä¸åŠ¨é£é™©å…·æœ‰å…³é”®ä½œç”¨ã€‚ä¼ ç»ŸPSEæ–¹æ³•å¸¸å¸¸å—åˆ°åºŠä½é®æŒ¡å’Œå¤æ‚æ‚£è€…å®šä½çš„å½±å“ï¼Œå¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®ä»è€Œå½±å“ä¸´åºŠç»“æœã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤šæ¨¡å¼åºŠä½æ‚£è€…3D PSEç½‘ç»œmPSE-CTï¼Œå®ƒèåˆäº†ä»å¸¸è§„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«æä¸­æå–çš„è¯¦ç»†å‡ ä½•ç‰¹å¾ä¸æ·±åº¦å›¾ã€‚mPSE-CTåŒ…æ‹¬å½¢æ€ä¼°è®¡æ¨¡å—ã€å§¿åŠ¿ä¼°è®¡æ¨¡å—å’Œæœ€ç»ˆå‚æ•°æ··åˆæ¨¡å—ï¼Œå¯ç¨³å¥åœ°é‡å»ºé®æŒ¡çš„èº«ä½“åŒºåŸŸå¹¶æé«˜ä¼°è®¡çš„3Däººä½“ç½‘æ ¼æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡ä¸“æœ‰å…¨èº«åˆšæ€§å¹»å½±å’Œå¿—æ„¿è€…æ•°æ®é›†çš„ä¸´åºŠåœºæ™¯éªŒè¯ï¼ŒmPSE-CTåœ¨å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡æ–¹é¢åˆ†åˆ«ä¼˜äºæœ€ä½³ç°æœ‰æ–¹æ³•23%å’Œ49.16%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›´æ‰‹æœ¯æœŸç¯å¢ƒä¸­æ”¹å–„ä¸´åºŠç»“æœçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾ç¡®çš„åºŠä½3Dæ‚£è€…å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡ï¼ˆPSEï¼‰åœ¨å›´æœ¯æœŸæŠ¤ç†ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»ŸPSEæ–¹æ³•å—åˆ°åºŠä½é®æŒ¡å’Œå¤æ‚æ‚£è€…å®šä½çš„å½±å“ï¼Œå­˜åœ¨ä¼°è®¡ä¸å‡†ç¡®çš„å±€é™æ€§ã€‚</li>
<li>mPSE-CTæ˜¯å¤šæ¨¡å¼åºŠä½æ‚£è€…3D PSEç½‘ç»œï¼Œèåˆäº†CTæ‰«æçš„å‡ ä½•ç‰¹å¾ä¸æ·±åº¦å›¾ã€‚</li>
<li>mPSE-CTåŒ…æ‹¬å½¢æ€ã€å§¿åŠ¿ä¼°è®¡æ¨¡å—å’Œå‚æ•°æ··åˆæ¨¡å—ï¼Œèƒ½ç¨³å¥é‡å»ºé®æŒ¡åŒºåŸŸå¹¶æé«˜3Dæ¨¡å‹å‡†ç¡®æ€§ã€‚</li>
<li>mPSE-CTåœ¨å§¿åŠ¿å’Œå½¢æ€ä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>mPSE-CTå…·æœ‰æ”¹å–„å›´æ‰‹æœ¯æœŸä¸´åºŠç»“æœçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3f448b41c81a1a87b13e0030a54a358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25e7d305e610a74366ff3661b5c3b4cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb5e85857655e195c75574ec928c1efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-219ec2e4dd48d52c7a4b709902927c36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d1e4a6be0e3f8debc7a9c6072dd70f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27df441728069e8edc3d5d91a89b087.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction"><a href="#VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction" class="headerlink" title="VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction"></a>VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction</h2><p><strong>Authors:Zizhi Chen, Minghao Han, Xukun Zhang, Shuwei Ma, Tao Liu, Xing Wei, Lihua Zhang</strong></p>
<p>Multimodal learning combining pathology images and genomic sequences enhances cancer survival analysis but faces clinical implementation barriers due to limited access to genomic sequencing in under-resourced regions. To enable survival prediction using only whole-slide images (WSI), we propose the Visual-Genomic Answering-Guided Transformer (VGAT), a framework integrating Visual Question Answering (VQA) techniques for genomic modality reconstruction. By adapting VQAâ€™s text feature extraction approach, we derive stable genomic representations that circumvent dimensionality challenges in raw genomic data. Simultaneously, a cluster-based visual prompt module selectively enhances discriminative WSI patches, addressing noise from unfiltered image regions. Evaluated across five TCGA datasets, VGAT outperforms existing WSI-only methods, demonstrating the viability of genomic-informed inference without sequencing. This approach bridges multimodal research and clinical feasibility in resource-constrained settings. The code link is <a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT">https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT</a>. </p>
<blockquote>
<p>å°†ç—…ç†å›¾åƒä¸åŸºå› ç»„åºåˆ—ç›¸ç»“åˆçš„å¤šæ¨¡æ€å­¦ä¹ èƒ½æé«˜ç™Œç—‡ç”Ÿå­˜åˆ†æèƒ½åŠ›ï¼Œä½†ç”±äºèµ„æºåŒ®ä¹åœ°åŒºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®æ€§ï¼Œå…¶åœ¨ä¸´åºŠå®æ–½ä¸­é¢ä¸´éšœç¢ã€‚ä¸ºäº†ä»…ä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸºå› ç»„é—®ç­”å¼•å¯¼è½¬æ¢å™¨ï¼ˆVGATï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯ç”¨äºåŸºå› ç»„æ¨¡å¼é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œè¿™é¿å…äº†åŸå§‹åŸºå› ç»„æ•°æ®ä¸­çš„ç»´åº¦æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—ä¼šé€‰æ‹©æ€§å¢å¼ºåˆ¤åˆ«æ€§çš„WSIæ–‘å—ï¼Œè§£å†³äº†æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒVGATåœ¨ä»…ä½¿ç”¨WSIçš„æ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†åœ¨æ²¡æœ‰æµ‹åºçš„æƒ…å†µä¸‹è¿›è¡ŒåŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šæ¨¡æ€ç ”ç©¶å’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„ä¸´åºŠå¯è¡Œæ€§ä¹‹é—´æ­å»ºäº†æ¡¥æ¢ã€‚ä»£ç é“¾æ¥æ˜¯ï¼š[é“¾æ¥åœ°å€]ï¼ˆè¯·æ›¿æ¢ä¸ºçœŸå®çš„ä»£ç é“¾æ¥ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19367v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç—…ç†å›¾åƒå’ŒåŸºå› ç»„åºåˆ—çš„å¤šæ¨¡æ€å­¦ä¹ å¢å¼ºäº†ç™Œç—‡ç”Ÿå­˜åˆ†æï¼Œä½†ç”±äºèµ„æºåŒ®ä¹åœ°åŒºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®è€Œé¢ä¸´ä¸´åºŠå®æ–½éšœç¢ã€‚ä¸ºä½¿ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºè§†è§‰åŸºå› ç»„é—®ç­”å¼•å¯¼è½¬æ¢å™¨ï¼ˆVGATï¼‰æ¡†æ¶ï¼Œæ•´åˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯è¿›è¡ŒåŸºå› ç»„æ¨¡æ€é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬è·å¾—ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œé¿å…åŸå§‹åŸºå› ç»„æ•°æ®çš„ç»´æ•°æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—é€‰æ‹©æ€§å¢å¼ºé‰´åˆ«æ€§çš„WSIè¡¥ä¸ï¼Œè§£å†³æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVGATåœ¨æ— éœ€æµ‹åºçš„æƒ…å†µä¸‹è¿›è¡ŒåŸºå› ç»„ä¿¡æ¯æ¨æ–­å…·æœ‰å¯è¡Œæ€§ï¼Œæœ‰æ•ˆå¼¥åˆäº†å¤šæ¨¡æ€ç ”ç©¶ä¸èµ„æºå—é™ç¯å¢ƒä¸‹çš„ä¸´åºŠå¯è¡Œæ€§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ ç»“åˆäº†ç—…ç†å›¾åƒå’ŒåŸºå› ç»„åºåˆ—ï¼Œæå‡äº†ç™Œç—‡ç”Ÿå­˜åˆ†æèƒ½åŠ›ã€‚</li>
<li>åœ¨èµ„æºæœ‰é™åœ°åŒºï¼Œç”±äºåŸºå› ç»„æµ‹åºçš„é™åˆ¶ï¼Œä¸´åºŠå®æ–½é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†VGATæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰é—®ç­”æŠ€æœ¯å®ç°åŸºå› ç»„æ¨¡æ€é‡å»ºã€‚</li>
<li>VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ç”¨äºè·å–ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œè§£å†³åŸå§‹æ•°æ®çš„ç»´æ•°é—®é¢˜ã€‚</li>
<li>é›†ç¾¤è§†è§‰æç¤ºæ¨¡å—èƒ½å¢å¼ºé‰´åˆ«æ€§çš„å…¨åˆ‡ç‰‡å›¾åƒåŒºåŸŸä¿¡æ¯ã€‚</li>
<li>VGATé€šè¿‡é€‰æ‹©æ€§å¤„ç†æœ‰æ•ˆå¼¥åˆäº†å¤šæ¨¡æ€ç ”ç©¶ä¸ä¸´åºŠå¯è¡Œæ€§ä¹‹é—´çš„å·®è·ã€‚</li>
<li>VGATä»£ç é“¾æ¥å¯ä¾›è®¿é—®ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT%EF%BC%89%E3%80%82">https://github.com/CZZZZZZZZZZZZZZZZZ/VGATï¼‰ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19ed8808e4327643febd1fc6e5c485a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832917e08e6d9470f2d6d8137c783037.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09a3d4d5585b895dd0efdb66b1e0879f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e504c5c98b17cc8d9e22464901c320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ecea5958615700f6cb7868676a6405.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f8164d1d306337ce7ca27561231d59.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Show-and-Segment-Universal-Medical-Image-Segmentation-via-In-Context-Learning"><a href="#Show-and-Segment-Universal-Medical-Image-Segmentation-via-In-Context-Learning" class="headerlink" title="Show and Segment: Universal Medical Image Segmentation via In-Context   Learning"></a>Show and Segment: Universal Medical Image Segmentation via In-Context   Learning</h2><p><strong>Authors:Yunhe Gao, Di Liu, Zhuowei Li, Yunsheng Li, Dongdong Chen, Mu Zhou, Dimitris N. Metaxas</strong></p>
<p>Medical image segmentation remains challenging due to the vast diversity of anatomical structures, imaging modalities, and segmentation tasks. While deep learning has made significant advances, current approaches struggle to generalize as they require task-specific training or fine-tuning on unseen classes. We present Iris, a novel In-context Reference Image guided Segmentation framework that enables flexible adaptation to novel tasks through the use of reference examples without fine-tuning. At its core, Iris features a lightweight context task encoding module that distills task-specific information from reference context image-label pairs. This rich context embedding information is used to guide the segmentation of target objects. By decoupling task encoding from inference, Iris supports diverse strategies from one-shot inference and context example ensemble to object-level context example retrieval and in-context tuning. Through comprehensive evaluation across twelve datasets, we demonstrate that Iris performs strongly compared to task-specific models on in-distribution tasks. On seven held-out datasets, Iris shows superior generalization to out-of-distribution data and unseen classes. Further, Irisâ€™s task encoding module can automatically discover anatomical relationships across datasets and modalities, offering insights into medical objects without explicit anatomical supervision. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè§£å‰–ç»“æ„ã€æˆåƒæ–¹å¼å’Œåˆ†å‰²ä»»åŠ¡çš„å·¨å¤§å¤šæ ·æ€§ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨æ¨å¹¿æ—¶ä»é¢ä¸´å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¯¹æœªè§çš„ä»»åŠ¡è¿›è¡Œç‰¹å®šè®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†Irisï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡å‚è€ƒå›¾åƒå¼•å¯¼åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å‚è€ƒç¤ºä¾‹å®ç°çµæ´»é€‚åº”æ–°ä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒã€‚Irisçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè½»é‡çº§çš„ä¸Šä¸‹æ–‡ä»»åŠ¡ç¼–ç æ¨¡å—ï¼Œå®ƒä»å‚è€ƒä¸Šä¸‹æ–‡å›¾åƒ-æ ‡ç­¾å¯¹ä¸­æç‚¼å‡ºç‰¹å®šä»»åŠ¡ä¿¡æ¯ã€‚è¿™ç§ä¸°å¯Œçš„ä¸Šä¸‹æ–‡åµŒå…¥ä¿¡æ¯ç”¨äºå¼•å¯¼ç›®æ ‡å¯¹è±¡çš„åˆ†å‰²ã€‚é€šè¿‡å°†ä»»åŠ¡ç¼–ç ä¸æ¨ç†è§£è€¦ï¼ŒIrisæ”¯æŒä»ä¸€æ¬¡æ€§æ¨ç†å’Œä¸Šä¸‹æ–‡ç¤ºä¾‹é›†åˆåˆ°å¯¹è±¡çº§ä¸Šä¸‹æ–‡ç¤ºä¾‹æ£€ç´¢å’Œä¸Šä¸‹æ–‡è°ƒæ•´çš„å„ç§ç­–ç•¥ã€‚é€šè¿‡å¯¹åäºŒä¸ªæ•°æ®é›†çš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†Irisåœ¨å†…éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸Šä¸ç‰¹å®šä»»åŠ¡æ¨¡å‹ç›¸æ¯”è¡¨ç°å¼ºåŠ²ã€‚åœ¨ä¸ƒä¸ªä¿ç•™æ•°æ®é›†ä¸­ï¼ŒIrisæ˜¾ç¤ºå‡ºå¯¹å¤–éƒ¨æ•°æ®å’Œæœªè§ç±»åˆ«çš„å“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒIrisçš„ä»»åŠ¡ç¼–ç æ¨¡å—å¯ä»¥è‡ªåŠ¨å‘ç°è·¨æ•°æ®é›†å’Œæ¨¡æ€çš„è§£å‰–å…³ç³»ï¼Œä¸ºåŒ»å­¦å¯¹è±¡æä¾›è§è§£ï¼Œæ— éœ€æ˜ç¡®çš„è§£å‰–ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19359v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºIrisçš„æ–°å‹ä¸Šä¸‹æ–‡å‚è€ƒå›¾åƒå¼•å¯¼åˆ†å‰²æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹çµæ´»é€‚åº”æ–°ä»»åŠ¡ã€‚Irisé€šè¿‡å‚è€ƒå›¾åƒä¸æ ‡ç­¾å¯¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å¼•å¯¼ç›®æ ‡å¯¹è±¡çš„åˆ†å‰²ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒIrisè¿˜å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶è‡ªåŠ¨å‘ç°æ•°æ®é›†å’Œæ¨¡æ€ä¹‹é—´çš„è§£å‰–å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Irisæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡å‚è€ƒå›¾åƒå¼•å¯¼åˆ†å‰²æ¡†æ¶ï¼Œå¯ä»¥çµæ´»åœ°é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>Irisä½¿ç”¨å‚è€ƒå›¾åƒä¸æ ‡ç­¾å¯¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æŒ‡å¯¼ç›®æ ‡å¯¹è±¡çš„åˆ†å‰²ã€‚</li>
<li>Irisé€šè¿‡è§£è€¦ä»»åŠ¡ç¼–ç å’Œæ¨ç†ï¼Œæ”¯æŒå¤šç§ç­–ç•¥ï¼Œå¦‚ä¸€æ¬¡æ¨ç†ã€ä¸Šä¸‹æ–‡ç¤ºä¾‹é›†åˆã€å¯¹è±¡çº§ä¸Šä¸‹æ–‡ç¤ºä¾‹æ£€ç´¢å’Œä¸Šä¸‹æ–‡è°ƒæ•´ã€‚</li>
<li>Irisåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ç‰¹å®šä»»åŠ¡æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Irisçš„ä»»åŠ¡ç¼–ç æ¨¡å—èƒ½å¤Ÿè‡ªåŠ¨å‘ç°æ•°æ®é›†å’Œæ¨¡æ€ä¹‹é—´çš„è§£å‰–å…³ç³»ï¼Œä¸ºåŒ»å­¦å¯¹è±¡æä¾›è§è§£ã€‚</li>
<li>Irisæ¡†æ¶æœ‰åŠ©äºè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚è§£å‰–ç»“æ„ã€æˆåƒæ–¹å¼å’Œåˆ†å‰²ä»»åŠ¡çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47dccd2cecd0c986913956d132e2a3fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2fa0c4a32adcc1e86b6fce7c2809747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5332ccddbfe8a0a45381c42b23d95cab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc5941e5796245c63bb179ab96c9c760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e80037250b93bf0589a8280107bdc12e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Analysis-of-Mamba-for-3D-Volumetric-Medical-Image-Segmentation"><a href="#A-Comprehensive-Analysis-of-Mamba-for-3D-Volumetric-Medical-Image-Segmentation" class="headerlink" title="A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image   Segmentation"></a>A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image   Segmentation</h2><p><strong>Authors:Chaohan Wang, Yutong Xie, Qi Chen, Yuyin Zhou, Qi Wu</strong></p>
<p>Mamba, with its selective State Space Models (SSMs), offers a more computationally efficient solution than Transformers for long-range dependency modeling. However, there is still a debate about its effectiveness in high-resolution 3D medical image segmentation. In this study, we present a comprehensive investigation into Mambaâ€™s capabilities in 3D medical image segmentation by tackling three pivotal questions: Can Mamba replace Transformers? Can it elevate multi-scale representation learning? Is complex scanning necessary to unlock its full potential? We evaluate Mambaâ€™s performance across three large public benchmarks-AMOS, TotalSegmentator, and BraTS. Our findings reveal that UlikeMamba, a U-shape Mamba-based network, consistently surpasses UlikeTrans, a U-shape Transformer-based network, particularly when enhanced with custom-designed 3D depthwise convolutions, boosting accuracy and computational efficiency. Further, our proposed multi-scale Mamba block demonstrates superior performance in capturing both fine-grained details and global context, especially in complex segmentation tasks, surpassing Transformer-based counterparts. We also critically assess complex scanning strategies, finding that simpler methods often suffice, while our Tri-scan approach delivers notable advantages in the most challenging scenarios. By integrating these advancements, we introduce a new network for 3D medical image segmentation, positioning Mamba as a transformative force that outperforms leading models such as nnUNet, CoTr, and U-Mamba, offering competitive accuracy with superior computational efficiency. This study provides key insights into Mambaâ€™s unique advantages, paving the way for more efficient and accurate approaches to 3D medical imaging. </p>
<blockquote>
<p>Mambaå‡­å€Ÿé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰æä¾›äº†ä¸€ç§æ¯”Transformeræ›´é€‚ç”¨äºé•¿è·ç¦»ä¾èµ–å»ºæ¨¡çš„è®¡ç®—æ•ˆç‡æ›´é«˜çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…³äºå…¶åœ¨é«˜åˆ†è¾¨ç‡3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ä»å­˜åœ¨äº‰è®®ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼Œå¯¹Mambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼šMambaèƒ½å¦æ›¿ä»£Transformerï¼Ÿå®ƒæ˜¯å¦èƒ½æå‡å¤šå°ºåº¦è¡¨ç¤ºå­¦ä¹ ï¼Ÿæ˜¯å¦éœ€è¦é€šè¿‡å¤æ‚çš„æ‰«ææ¥å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼Ÿæˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤§å‹å…¬å…±åŸºå‡†æµ‹è¯•ï¼ˆAMOSã€TotalSegmentatorå’ŒBraTSï¼‰ä¸Šè¯„ä¼°äº†Mambaçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŸºäºU-shapeçš„Mambaç½‘ç»œUlikeMambaï¼Œå°¤å…¶æ˜¯ç»“åˆäº†å®šåˆ¶çš„3Dæ·±åº¦å·ç§¯åï¼Œå§‹ç»ˆè¶…è¶Šäº†åŸºäºU-shapeçš„Transformerç½‘ç»œUlikeTransï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„å¤šå°ºåº¦Mambaå—åœ¨æ•è·ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤æ‚çš„åˆ†å‰²ä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºäºTransformerçš„åŒç±»äº§å“ã€‚æˆ‘ä»¬è¿˜å¯¹å¤æ‚çš„æ‰«æç­–ç•¥è¿›è¡Œäº†æ‰¹åˆ¤æ€§è¯„ä¼°ï¼Œå‘ç°ç®€å•çš„æ–¹æ³•é€šå¸¸å°±è¶³å¤Ÿäº†ï¼Œè€Œæˆ‘ä»¬çš„Tri-scanæ–¹æ³•åœ¨æœ€å¤æ‚çš„åœºæ™¯ä¸­å¸¦æ¥äº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚é€šè¿‡æ•´åˆè¿™äº›è¿›æ­¥ï¼Œæˆ‘ä»¬ä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²å¼•å…¥äº†ä¸€ç§æ–°ç½‘ç»œï¼Œå°†Mambaå®šä½ä¸ºä¸€ç§å˜é©æ€§åŠ›é‡ï¼Œè¶…è¶Šäº†nnUNetã€CoTrå’ŒU-Mambaç­‰é¢†å…ˆæ¨¡å‹ï¼Œä»¥æ›´é«˜çš„è®¡ç®—æ•ˆç‡æä¾›å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶ä¸ºMambaçš„ç‹¬ç‰¹ä¼˜åŠ¿æä¾›äº†å…³é”®è§è§£ï¼Œä¸º3DåŒ»å­¦æˆåƒæä¾›æ›´é«˜æ•ˆå’Œæ›´å‡†ç¡®çš„æ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19308v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶å…¨é¢æ¢è®¨äº†Mambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„èƒ½åŠ›ï¼Œé€šè¿‡è§£å†³ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šMambaæ˜¯å¦èƒ½æ›¿ä»£Transformerã€æ˜¯å¦èƒ½æå‡å¤šå°ºåº¦è¡¨ç¤ºå­¦ä¹ ä»¥åŠæ˜¯å¦éœ€è¦è¿›è¡Œå¤æ‚çš„æ‰«æä»¥å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºMambaçš„UlikeMambaç½‘ç»œåœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºäºTransformerçš„UlikeTransï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆè‡ªå®šä¹‰è®¾è®¡çš„3Dæ·±åº¦å·ç§¯åï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚åŒæ—¶ï¼Œæå‡ºçš„å¤šå°ºåº¦Mambaå—åœ¨æ•æ‰ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åˆ†å‰²ä»»åŠ¡ä¸­ã€‚æ­¤å¤–ï¼Œè¯„ä¼°äº†å¤æ‚æ‰«æç­–ç•¥ï¼Œå‘ç°ç®€å•æ–¹æ³•å¾€å¾€è¶³å¤Ÿï¼Œè€ŒTri-scanæ–¹æ³•åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å› æ­¤ï¼ŒMambaä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„ç½‘ç»œï¼Œè¡¨ç°å‡ºä¼˜äºnnUNetã€CoTrå’ŒU-Mambaç­‰é¢†å…ˆæ¨¡å‹çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Mambaé€šè¿‡å…¶é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä¸ºé•¿è·ç¦»ä¾èµ–å»ºæ¨¡æä¾›äº†æ¯”Transformeræ›´é«˜æ•ˆçš„è®¡ç®—è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ï¼ŒUlikeMambaç½‘ç»œæ€§èƒ½è¶…è¶Šäº†åŸºäºTransformerçš„UlikeTransï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆè‡ªå®šä¹‰è®¾è®¡çš„3Dæ·±åº¦å·ç§¯åã€‚</li>
<li>å¤šå°ºåº¦Mambaå—åœ¨æ•æ‰ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤æ‚åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>å¤æ‚æ‰«æç­–ç•¥è¯„ä¼°æ˜¾ç¤ºï¼Œç®€å•æ–¹æ³•å¾€å¾€è¶³å¤Ÿï¼Œè€ŒTri-scanæ–¹æ³•åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>Mambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä½œä¸ºä¸€ç§å˜é©æ€§åŠ›é‡ã€‚</li>
<li>Mambaçš„è¡¨ç°ä¼˜äºå…¶ä»–é¢†å…ˆæ¨¡å‹ï¼Œå¦‚nnUNetã€CoTrå’ŒU-Mambaç­‰ï¼Œæä¾›äº†ç«äº‰æ€§çš„å‡†ç¡®æ€§å’Œå‡ºè‰²çš„è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99e34d3e3ba7e9b686a291aa91f8436e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1dc7f2c0d8e0a3a18530a999e9881cb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BIMII-Net-Brain-Inspired-Multi-Iterative-Interactive-Network-for-RGB-T-Road-Scene-Semantic-Segmentation"><a href="#BIMII-Net-Brain-Inspired-Multi-Iterative-Interactive-Network-for-RGB-T-Road-Scene-Semantic-Segmentation" class="headerlink" title="BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T   Road Scene Semantic Segmentation"></a>BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T   Road Scene Semantic Segmentation</h2><p><strong>Authors:Hanshuo Qiu, Jie Jiang, Ruoli Yang, Lixin Zhan, Jizhao Liu</strong></p>
<p>RGB-T road scene semantic segmentation enhances visual scene understanding in complex environments characterized by inadequate illumination or occlusion by fusing information from RGB and thermal images. Nevertheless, existing RGB-T semantic segmentation models typically depend on simple addition or concatenation strategies or ignore the differences between information at different levels. To address these issues, we proposed a novel RGB-T road scene semantic segmentation network called Brain-Inspired Multi-Iteration Interaction Network (BIMII-Net). First, to meet the requirements of accurate texture and local information extraction in road scenarios like autonomous driving, we proposed a deep continuous-coupled neural network (DCCNN) architecture based on a brain-inspired model. Second, to enhance the interaction and expression capabilities among multi-modal information, we designed a cross explicit attention-enhanced fusion module (CEAEF-Module) in the feature fusion stage of BIMII-Net to effectively integrate features at different levels. Finally, we constructed a complementary interactive multi-layer decoder structure, incorporating the shallow-level feature iteration module (SFI-Module), the deep-level feature iteration module (DFI-Module), and the multi-feature enhancement module (MFE-Module) to collaboratively extract texture details and global skeleton information, with multi-module joint supervision further optimizing the segmentation results. Experimental results demonstrate that BIMII-Net achieves state-of-the-art (SOTA) performance in the brain-inspired computing domain and outperforms most existing RGB-T semantic segmentation methods. It also exhibits strong generalization capabilities on multiple RGB-T datasets, proving the effectiveness of brain-inspired computer models in multi-modal image segmentation tasks. </p>
<blockquote>
<p>RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²é€šè¿‡èåˆRGBå’Œçº¢å¤–å›¾åƒçš„ä¿¡æ¯ï¼Œæé«˜äº†å¤æ‚ç¯å¢ƒä¸‹çš„è§†è§‰åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰çº¿ä¸è¶³æˆ–è¢«é®æŒ¡çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸ä¾èµ–äºç®€å•çš„åŠ æ³•æˆ–è¿æ¥ç­–ç•¥ï¼Œæˆ–è€…å¿½ç•¥äº†ä¸åŒçº§åˆ«ä¿¡æ¯ä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œåä¸ºBrain-Inspired Multi-Iteration Interaction Network (BIMII-Net)ã€‚é¦–å…ˆï¼Œä¸ºäº†æ»¡è¶³è‡ªåŠ¨é©¾é©¶ç­‰é“è·¯åœºæ™¯ä¸‹ç²¾ç¡®çº¹ç†å’Œå±€éƒ¨ä¿¡æ¯æå–çš„è¦æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè„‘å¯å‘æ¨¡å‹çš„æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å¢å¼ºå¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„äº¤äº’å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨BIMII-Netçš„ç‰¹å¾èåˆé˜¶æ®µè®¾è®¡äº†è·¨æ˜¾æ³¨æ„å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEFæ¨¡å—ï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆä¸åŒçº§åˆ«çš„ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªäº’è¡¥çš„å¤šå±‚è§£ç å™¨ç»“æ„ï¼Œç»“åˆäº†æµ…å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ï¼ˆSFIæ¨¡å—ï¼‰ã€æ·±å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ï¼ˆDFIæ¨¡å—ï¼‰å’Œå¤šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆMFEæ¨¡å—ï¼‰ï¼Œä»¥ååŒæå–çº¹ç†ç»†èŠ‚å’Œå…¨å±€éª¨æ¶ä¿¡æ¯ï¼Œå¤šæ¨¡å—è”åˆç›‘ç£è¿›ä¸€æ­¥ä¼˜åŒ–äº†åˆ†å‰²ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°ç°æœ‰çš„RGB-Tè¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®ƒåœ¨å¤šä¸ªRGB-Tæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†è„‘å¯å‘è®¡ç®—æœºæ¨¡å‹åœ¨å¤šæ¨¡æ€å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡èåˆRGBå’Œçº¢å¤–å›¾åƒä¿¡æ¯æé«˜å¤æ‚ç¯å¢ƒä¸‹çš„è§†è§‰åœºæ™¯ç†è§£ã€‚é’ˆå¯¹ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²ç½‘ç»œBIMII-Netã€‚è¯¥ç½‘ç»œé€šè¿‡æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„å’Œè·¨æ˜ç¡®æ³¨æ„åŠ›å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEF-Moduleï¼‰å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆå’Œè¡¨è¾¾ã€‚åŒæ—¶ï¼Œæ„å»ºäº†äº’è¡¥çš„å¤šå±‚è§£ç å™¨ç»“æ„ï¼ŒåŒ…æ‹¬æµ…å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ã€æ·±å±‚ç‰¹å¾è¿­ä»£æ¨¡å—å’Œå¤šç‰¹å¾å¢å¼ºæ¨¡å—ï¼Œä»¥ååŒæå–çº¹ç†ç»†èŠ‚å’Œå…¨å±€éª¨æ¶ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜BIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªRGB-Tæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²æŠ€æœ¯é€šè¿‡èåˆRGBå’Œçº¢å¤–å›¾åƒä¿¡æ¯æé«˜è§†è§‰åœºæ™¯ç†è§£ã€‚</li>
<li>ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹å­˜åœ¨ç®€å•èåˆç­–ç•¥æˆ–å¿½ç•¥ä¸åŒå±‚çº§ä¿¡æ¯å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>BIMII-Netç½‘ç»œé€šè¿‡æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„æå–ç²¾å‡†çº¹ç†å’Œå±€éƒ¨ä¿¡æ¯ã€‚</li>
<li>è·¨æ˜ç¡®æ³¨æ„åŠ›å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEF-Moduleï¼‰å¢å¼ºäº†å¤šæ¨¡æ€ä¿¡æ¯çš„äº¤äº’å’Œè¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>BIMII-Netæ„å»ºäº†å¤šå±‚è§£ç å™¨ç»“æ„ï¼ŒååŒæå–çº¹ç†ç»†èŠ‚å’Œå…¨å±€éª¨æ¶ä¿¡æ¯ã€‚</li>
<li>BIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f28645406850ca71a7d44b1410618348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-605c59d5c9d75d8e8c5f1ca084902ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ff7b1c7411577ec4c994887776c869.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Tumor-monitoring-and-detection-of-lymph-node-metastasis-using-quantitative-ultrasound-and-immune-cytokine-profiling-in-dogs-undergoing-radiation-therapy-a-pilot-study"><a href="#Tumor-monitoring-and-detection-of-lymph-node-metastasis-using-quantitative-ultrasound-and-immune-cytokine-profiling-in-dogs-undergoing-radiation-therapy-a-pilot-study" class="headerlink" title="Tumor monitoring and detection of lymph node metastasis using   quantitative ultrasound and immune cytokine profiling in dogs undergoing   radiation therapy: a pilot study"></a>Tumor monitoring and detection of lymph node metastasis using   quantitative ultrasound and immune cytokine profiling in dogs undergoing   radiation therapy: a pilot study</h2><p><strong>Authors:Mick Gardner, Audrey Billhymer, Rebecca Kamerer, Joanna Schmit, Trevor Park, Julie Nguyen-Edquilang, Rita Miller, Kim A Selting, Michael Oelze</strong></p>
<p>Quantitative ultrasound (QUS) characterizes the composition of cells to distinguish diseased from healthy tissue. QUS can reflect the complexity of the tumor and detect early lymph node (LN) metastasis ex vivo. The objective in this study was to gather preliminary QUS and cytokine data from dogs undergoing radiation therapy and correlate QUS data with both LN metastasis and tumor response. Spontaneous solid tumors were evaluated with QUS before and up to one year after receiving RT. Additionally, regional LNs were evaluated with QUS in vivo, then excised and examined with histopathology to detect metastasis. Paired t-tests were used to compare QUS data of metastatic and non-metastatic LNs within patients. Furthermore, paired t-tests compared pre- versus post-RT QUS data. Serum was collected at each time point for cytokine profiles. Most statistical tests were underpowered to produce significant p values, but interesting trends were observed. The lowest p values for LN tests were found with the envelope statistics K (p &#x3D; 0.142) and mu (p &#x3D; 0.181), which correspond to cell structure and number of scatterers. For tumor response, the lowest p values were found with K (p &#x3D; 0.115) and mu (p &#x3D; 0.127) when comparing baseline QUS data with QUS data 1 week after RT. Monocyte chemoattractant protein 1 (MCP-1) was significantly higher in dogs with cancer when compared to healthy controls (p &#x3D; 1.12e-4). A weak correlation was found between effective scatterer diameter (ESD) and Transforming growth factor beta 1 (TGFB-1). While statistical tests on the preliminary QUS data alone were underpowered to detect significant differences among groups, our methods create a basis for future studies. </p>
<blockquote>
<p>å®šé‡è¶…å£°ï¼ˆQUSï¼‰èƒ½å¤Ÿè¡¨å¾ç»†èƒçš„ç»„æˆï¼Œä»¥åŒºåˆ†ç—…å˜ç»„ç»‡å’Œå¥åº·ç»„ç»‡ã€‚QUSèƒ½å¤Ÿåæ˜ è‚¿ç˜¤çš„å¤æ‚æ€§ï¼Œå¹¶åœ¨ä½“å¤–æ£€æµ‹æ—©æœŸæ·‹å·´ç»“è½¬ç§»ã€‚æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æ”¶é›†æ­£åœ¨æ¥å—æ”¾å°„æ²»ç–—çš„ç‹—çš„åˆæ­¥QUSå’Œç»†èƒå› å­æ•°æ®ï¼Œå¹¶å°†QUSæ•°æ®ä¸æ·‹å·´ç»“è½¬ç§»å’Œè‚¿ç˜¤ååº”è¿›è¡Œå…³è”ã€‚åœ¨æ¥å—æ”¾å°„æ²»ç–—å‰åä»¥åŠé•¿è¾¾ä¸€å¹´çš„æ—¶é—´é‡Œï¼Œå¯¹è‡ªå‘æ€§å®ä½“ç˜¤è¿›è¡Œäº†QUSè¯„ä¼°ã€‚å¦å¤–ï¼Œè¿˜å¯¹åŒºåŸŸæ€§æ·‹å·´ç»“è¿›è¡Œäº†ä½“å†…QUSè¯„ä¼°ï¼Œç„¶ååˆ‡é™¤å¹¶è¿›è¡Œç»„ç»‡ç—…ç†å­¦æ£€æŸ¥ä»¥æ£€æµ‹è½¬ç§»æƒ…å†µã€‚ä½¿ç”¨é…å¯¹tæ£€éªŒæ¯”è¾ƒæ‚£è€…å†…éƒ¨è½¬ç§»æ€§å’Œéè½¬ç§»æ€§æ·‹å·´ç»“çš„QUSæ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜æ¯”è¾ƒäº†æ”¾ç–—å‰åçš„QUSæ•°æ®ã€‚åœ¨å„ä¸ªæ—¶é—´ç‚¹æ”¶é›†è¡€æ¸…è¿›è¡Œç»†èƒå› å­åˆ†æã€‚å¤§å¤šæ•°ç»Ÿè®¡æ£€éªŒçš„åŠ›åº¦ä¸è¶³ä»¥äº§ç”Ÿæ˜¾è‘—çš„på€¼ï¼Œä½†è§‚å¯Ÿåˆ°äº†ä¸€äº›æœ‰è¶£çš„è¶‹åŠ¿ã€‚åœ¨æ·‹å·´ç»“æµ‹è¯•ä¸­ï¼Œæœ€ä½çš„på€¼æ˜¯ç”±åŒ…ç»œç»Ÿè®¡Kï¼ˆp&#x3D;0.142ï¼‰å’Œmuï¼ˆp&#x3D;0.181ï¼‰å¾—åˆ°çš„ï¼Œå®ƒä»¬å¯¹åº”äºç»†èƒç»“æ„å’Œæ•£å°„ä½“æ•°é‡ã€‚å¯¹äºè‚¿ç˜¤ååº”ï¼Œæ¯”è¾ƒåŸºçº¿QUSæ•°æ®ä¸æ”¾ç–—åä¸€å‘¨çš„QUSæ•°æ®æ—¶ï¼ŒKå’Œmuçš„på€¼æœ€ä½ï¼ˆåˆ†åˆ«ä¸ºp&#x3D;0.115å’Œp&#x3D;0.127ï¼‰ã€‚ä¸å¥åº·çš„å¯¹ç…§ç»„ç›¸æ¯”ï¼Œæ‚£æœ‰ç™Œç—‡çš„ç‹—çš„å•æ ¸ç»†èƒè¶‹åŒ–å› å­è›‹ç™½1ï¼ˆMCP-1ï¼‰æ˜æ˜¾æ›´é«˜ï¼ˆp&#x3D;1.12e-4ï¼‰ã€‚æœ‰æ•ˆæ•£å°„ä½“ç›´å¾„ï¼ˆESDï¼‰ä¸è½¬åŒ–ç”Ÿé•¿å› å­Î²1ï¼ˆTGFB-1ï¼‰ä¹‹é—´å­˜åœ¨å¼±ç›¸å…³æ€§ã€‚è™½ç„¶ä»…å¯¹åˆæ­¥QUSæ•°æ®è¿›è¡Œç»Ÿè®¡æ£€éªŒçš„åŠ›åº¦ä¸è¶³ä»¥æ£€æµ‹å„ç»„ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19243v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong><br>     å®šé‡è¶…å£°ï¼ˆQUSï¼‰å¯è¡¨å¾ç»†èƒæˆåˆ†ä»¥åŒºåˆ†ç—…å˜ç»„ç»‡å’Œå¥åº·ç»„ç»‡ã€‚QUSèƒ½å¤Ÿåæ˜ è‚¿ç˜¤çš„å¤æ‚æ€§å¹¶æ£€æµ‹ä½“å¤–æ—©æœŸæ·‹å·´ç»“è½¬ç§»ï¼ˆLNï¼‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ”¶é›†æ¥å—æ”¾å°„ç–—æ³•æ²»ç–—çš„ç‹—çš„åˆæ­¥QUSå’Œç»†èƒå› å­æ•°æ®ï¼Œå¹¶å°†QUSæ•°æ®ä¸æ·‹å·´ç»“è½¬ç§»å’Œè‚¿ç˜¤ååº”è¿›è¡Œå…³è”ã€‚å¯¹è‡ªå‘å›ºä½“è‚¿ç˜¤åœ¨æ¥å—æ”¾ç–—å‰åè¿›è¡Œäº†QUSè¯„ä¼°ã€‚æ­¤å¤–ï¼Œå¯¹åŒºåŸŸæ·‹å·´ç»“è¿›è¡Œäº†ä½“å†…QUSè¯„ä¼°ï¼Œåˆ‡é™¤åè¿›è¡Œç»„ç»‡ç—…ç†å­¦æ£€æŸ¥ä»¥æ£€æµ‹è½¬ç§»æƒ…å†µã€‚ä½¿ç”¨é…å¯¹tæ£€éªŒæ¯”è¾ƒæ‚£è€…ä½“å†…è½¬ç§»æ€§å’Œéè½¬ç§»æ€§æ·‹å·´ç»“çš„QUSæ•°æ®ï¼Œä»¥åŠæ”¾ç–—å‰åçš„QUSæ•°æ®ã€‚æ”¶é›†è¡€æ¸…ä»¥æ£€æµ‹ç»†èƒå› å­ã€‚è™½ç„¶å¤§å¤šæ•°æµ‹è¯•æ— æ³•äº§ç”Ÿæ˜¾è‘—çš„på€¼ï¼Œä½†è§‚å¯Ÿåˆ°äº†ä¸€äº›æœ‰è¶£çš„è¶‹åŠ¿ã€‚åœ¨æ·‹å·´ç»“æµ‹è¯•ä¸­ï¼Œå…·æœ‰æœ€ä½på€¼çš„ä¸ºåŒ…ç»œç»Ÿè®¡Kï¼ˆp&#x3D;0.142ï¼‰å’ŒÎ¼ï¼ˆp&#x3D;0.181ï¼‰ï¼Œå®ƒä»¬å¯¹åº”äºç»†èƒç»“æ„å’Œæ•£å°„ä½“æ•°é‡ã€‚å¯¹äºè‚¿ç˜¤ååº”ï¼Œæ¯”è¾ƒåŸºçº¿QUSæ•°æ®ä¸æ”¾ç–—åä¸€å‘¨çš„QUSæ•°æ®æ—¶ï¼ŒKå’ŒÎ¼çš„på€¼æœ€ä½ã€‚ä¸å¥åº·çš„å¯¹ç…§ç»„ç›¸æ¯”ï¼Œå•æ ¸ç»†èƒè¶‹åŒ–å› å­è›‹ç™½-1ï¼ˆMCP-1ï¼‰åœ¨ç™Œç—‡æ‚£è€…ä¸­æ˜¾è‘—å‡é«˜ï¼ˆp&#x3D;1.12e-4ï¼‰ã€‚æœ‰æ•ˆæ•£å°„ä½“ç›´å¾„ï¼ˆESDï¼‰ä¸è½¬åŒ–ç”Ÿé•¿å› å­Î²-1ï¼ˆTGFB-1ï¼‰ä¹‹é—´å­˜åœ¨å¾®å¼±çš„ç›¸å…³æ€§ã€‚è™½ç„¶åˆæ­¥QUSæ•°æ®çš„ç»Ÿè®¡æµ‹è¯•åœ¨å•ç‹¬ä½¿ç”¨æ—¶æ— æ³•æ£€æµ‹åˆ°ç»„é—´æ˜¾è‘—å·®å¼‚ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®šé‡è¶…å£°ï¼ˆQUSï¼‰èƒ½å¤ŸåŒºåˆ†ç—…å˜ç»„ç»‡å’Œå¥åº·ç»„ç»‡ï¼Œåæ˜ è‚¿ç˜¤å¤æ‚æ€§å¹¶æ£€æµ‹æ—©æœŸæ·‹å·´ç»“è½¬ç§»ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨å…³è”QUSæ•°æ®ä¸æ·‹å·´ç»“è½¬ç§»åŠè‚¿ç˜¤ååº”ï¼Œå¹¶æ”¶é›†æ”¾ç–—å‰åç‹—çš„QUSå’Œç»†èƒå› å­æ•°æ®ã€‚</li>
<li>QUSè¯„ä¼°äº†è‡ªå‘å›ºä½“è‚¿ç˜¤å’ŒåŒºåŸŸæ·‹å·´ç»“ï¼Œå¹¶é€šè¿‡ç»„ç»‡ç—…ç†å­¦æ£€æŸ¥æ£€æµ‹è½¬ç§»æƒ…å†µã€‚</li>
<li>ä½¿ç”¨é…å¯¹tæ£€éªŒæ¯”è¾ƒè½¬ç§»æ€§å’Œéè½¬ç§»æ€§æ·‹å·´ç»“ä»¥åŠæ”¾ç–—å‰åçš„QUSæ•°æ®ã€‚</li>
<li>è§‚å¯Ÿåˆ°ä¸€äº›æœ‰è¶£çš„è¶‹åŠ¿ï¼Œå°½ç®¡å¤§å¤šæ•°æµ‹è¯•æœªèƒ½äº§ç”Ÿæ˜¾è‘—çš„på€¼ã€‚</li>
<li>åŒ…ç»œç»Ÿè®¡Kå’ŒÎ¼åœ¨æ·‹å·´ç»“æµ‹è¯•ä¸­è¡¨ç°å‡ºè¾ƒä½çš„på€¼ï¼Œåæ˜ ç»†èƒç»“æ„å’Œæ•£å°„ä½“æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f916a880bc3fe6fcb54d34cc036db829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5570cc0762ecebef01f156ccaf60bd95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1a754438ec55180bde4af68dff3376a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aee7aa90911a73b49da0cccce262caa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19109fc4185aa57ced8a191abc05387d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d660554313820eff4ef8e10ac643fa6b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy"><a href="#TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy" class="headerlink" title="TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy"></a>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy</h2><p><strong>Authors:Yiling Wang, Elia Lombardo, Adrian Thummerer, Tom BlÃ¶cker, Yu Fan, Yue Zhao, Christianna Iris Papadopoulou, Coen Hurkmans, Rob H. N. Tijssen, Pia A. W. GÃ¶rts, Shyama U. Tetar, Davide Cusumano, Martijn P. W. Intven, Pim Borman, Marco Riboldi, Denis DudÃ¡Å¡, Hilary Byrne, Lorenzo Placidi, Marco Fusella, Michael Jameson, Miguel Palacios, Paul Cobussen, Tobias Finazzi, Cornelis J. A. Haasbeek, Paul Keall, Christopher Kurz, Guillaume Landry, Matteo Maspero</strong></p>
<p>Purpose: Magnetic resonance imaging (MRI) to visualize anatomical motion is becoming increasingly important when treating cancer patients with radiotherapy. Hybrid MRI-linear accelerator (MRI-linac) systems allow real-time motion management during irradiation. This paper presents a multi-institutional real-time MRI time series dataset from different MRI-linac vendors. The dataset is designed to support developing and evaluating real-time tumor localization (tracking) algorithms for MRI-guided radiotherapy within the TrackRAD2025 challenge (<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/">https://trackrad2025.grand-challenge.org/</a>).   Acquisition and validation methods: The dataset consists of sagittal 2D cine MRIs in 585 patients from six centers (3 Dutch, 1 German, 1 Australian, and 1 Chinese). Tumors in the thorax, abdomen, and pelvis acquired on two commercially available MRI-linacs (0.35 T and 1.5 T) were included. For 108 cases, irradiation targets or tracking surrogates were manually segmented on each temporal frame. The dataset was randomly split into a public training set of 527 cases (477 unlabeled and 50 labeled) and a private testing set of 58 cases (all labeled).   Data Format and Usage Notes: The data is publicly available under the TrackRAD2025 collection: <a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539">https://doi.org/10.57967/hf/4539</a>. Both the images and segmentations for each patient are available in metadata format.   Potential Applications: This novel clinical dataset will enable the development and evaluation of real-time tumor localization algorithms for MRI-guided radiotherapy. By enabling more accurate motion management and adaptive treatment strategies, this dataset has the potential to advance the field of radiotherapy significantly. </p>
<blockquote>
<p>ç›®çš„ï¼šåœ¨æ²»ç–—ç™Œç—‡æ‚£è€…æ—¶è¿›è¡Œæ”¾å°„æ²»ç–—æ—¶ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ¥å¯è§†åŒ–è§£å‰–è¿åŠ¨å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ··åˆMRI-ç›´çº¿åŠ é€Ÿå™¨ï¼ˆMRI-linacï¼‰ç³»ç»Ÿå¯åœ¨ç…§å°„è¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶è¿åŠ¨ç®¡ç†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæœºæ„å®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªä¸åŒçš„MRI-linacä¾›åº”å•†ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒå¼€å‘å¹¶è¯„ä¼°MRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„å®æ—¶è‚¿ç˜¤å®šä½ï¼ˆè·Ÿè¸ªï¼‰ç®—æ³•ï¼Œä»¥åº”å¯¹TrackRAD2025æŒ‘æˆ˜ï¼ˆ<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/%EF%BC%89%E3%80%82">https://trackrad2025.grand-challenge.org/ï¼‰ã€‚</a></p>
</blockquote>
<p>é‡‡é›†å’ŒéªŒè¯æ–¹æ³•ï¼šæ•°æ®é›†åŒ…å«æ¥è‡ª6ä¸ªä¸­å¿ƒï¼ˆè·å…°3ä¸ªï¼Œå¾·å›½1ä¸ªï¼Œæ¾³å¤§åˆ©äºš1ä¸ªï¼Œä¸­å›½1ä¸ªï¼‰çš„585ä¾‹æ‚£è€…çš„çŸ¢çŠ¶é¢2Dç”µå½±MRIã€‚èƒ¸éƒ¨ã€è…¹éƒ¨å’Œéª¨ç›†éƒ¨ä½çš„è‚¿ç˜¤æ˜¯åœ¨ä¸¤å°å•†ç”¨MRI-linacï¼ˆ0.35Tå’Œ1.5Tï¼‰ä¸Šè·å¾—çš„ã€‚åœ¨108ä¸ªç—…ä¾‹ä¸­ï¼Œæ¯ä¸ªæ—¶é—´å¸§ä¸Šéƒ½æ‰‹åŠ¨åˆ†å‰²äº†ç…§å°„ç›®æ ‡æˆ–è·Ÿè¸ªä»£ç†ã€‚æ•°æ®é›†è¢«éšæœºåˆ†æˆ527ä¾‹å…¬å…±è®­ç»ƒé›†ï¼ˆå…¶ä¸­477ä¾‹æœªæ ‡è®°ï¼Œ50ä¾‹å·²æ ‡è®°ï¼‰å’Œ58ä¾‹ç§æœ‰æµ‹è¯•é›†ï¼ˆå‡å·²æ ‡è®°ï¼‰ã€‚</p>
<p>æ•°æ®æ ¼å¼å’Œä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼šæ•°æ®å¯åœ¨TrackRAD2025æ”¶è—ä¸­å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539%E3%80%82%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%84%E5%9B%BE%E5%83%8F%E5%92%8C%E5%88%86%E6%AE%B5%E5%9D%87%E4%BB%A5%E5%85%83%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E6%8F%90%E4%BE%9B%E3%80%82">https://doi.org/10.57967/hf/4539ã€‚æ¯ä¸ªæ‚£è€…çš„å›¾åƒå’Œåˆ†æ®µå‡ä»¥å…ƒæ•°æ®æ ¼å¼æä¾›ã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19119v1">PDF</a> 10 pages, 5 figures, 2 tables; submitted to Medical Physics</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæœºæ„å®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªä¸åŒçš„MRI-linacä¾›åº”å•†ï¼Œæ—¨åœ¨æ”¯æŒå¼€å‘å¹¶è¯„ä¼°ç”¨äºMRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„å®æ—¶è‚¿ç˜¤å®šä½ï¼ˆè·Ÿè¸ªï¼‰ç®—æ³•ã€‚æ•°æ®é›†åŒ…å«585åæ‚£è€…çš„çŸ¢çŠ¶é¢2Dç”µå½±MRIï¼Œæ¶µç›–äº†èƒ¸éƒ¨ã€è…¹éƒ¨å’Œéª¨ç›†çš„è‚¿ç˜¤ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œå¹¶å¯ç”¨äºç®—æ³•å¼€å‘å’Œè¯„ä¼°ï¼Œæœ‰æœ›æ¨åŠ¨æ”¾ç–—é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªå®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œä¸“ä¸ºæ”¯æŒMRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„è‚¿ç˜¤å®šä½ç®—æ³•çš„å¼€å‘å’Œè¯„ä¼°è€Œè®¾è®¡ã€‚</li>
<li>æ•°æ®é›†æ¥æºäºä¸åŒMRI-linacä¾›åº”å•†çš„å¤šä¸ªæœºæ„åˆä½œã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªå…­ä¸ªä¸­å¿ƒçš„585åæ‚£è€…çš„å½±åƒæ•°æ®ï¼Œæ¶µç›–äº†èƒ¸éƒ¨ã€è…¹éƒ¨å’Œéª¨ç›†çš„è‚¿ç˜¤ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å…¬å¼€è®­ç»ƒå’Œç§äººæµ‹è¯•ä¸¤ç»„æ•°æ®ï¼Œåˆ†åˆ«åŒ…å«527ä¾‹å’Œ58ä¾‹ç—…ä¾‹ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨å…ƒæ•°æ®æ ¼å¼å…¬å¼€å¯ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒå’Œæ‚£è€…ä¿¡æ¯ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºå¼€å‘å®æ—¶è‚¿ç˜¤å®šä½ç®—æ³•ï¼Œæœ‰æœ›æé«˜æ”¾ç–—çš„å‡†ç¡®æ€§å¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b919f061c46a248d7b200b249517bd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-059fe7726d74d1d019cfdf446ddd8aa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f428d2f83cf9447ea1c0ec7d2aef9772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e728732b1f7178d28770b1d34f6d92.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Your-ViT-is-Secretly-an-Image-Segmentation-Model"><a href="#Your-ViT-is-Secretly-an-Image-Segmentation-Model" class="headerlink" title="Your ViT is Secretly an Image Segmentation Model"></a>Your ViT is Secretly an Image Segmentation Model</h2><p><strong>Authors:Tommie Kerssies, NiccolÃ² Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, Daan de Geus</strong></p>
<p>Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: <a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/">https://www.tue-mps.org/eomt/</a>. </p>
<blockquote>
<p>Vision Transformersï¼ˆViTsï¼‰åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†å°†å•å°ºåº¦ViTsåº”ç”¨äºå›¾åƒåˆ†å‰²ï¼Œç°æœ‰æ–¹æ³•é‡‡ç”¨å·ç§¯é€‚é…å™¨ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ï¼Œåƒç´ è§£ç å™¨èåˆè¿™äº›ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨èåˆç‰¹å¾çš„Transformerè§£ç å™¨è¿›è¡Œé¢„æµ‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œç»™å®šè¶³å¤Ÿå¤§çš„æ¨¡å‹å’Œå¹¿æ³›çš„é¢„è®­ç»ƒï¼Œè¿™äº›ç‰¹å®šä»»åŠ¡ç»„ä»¶æ‰€å¼•å…¥çš„å½’çº³åè§ä¹Ÿå¯ä»¥ç”±ViTæœ¬èº«æ¥å­¦ä¹ ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»…ç¼–ç å™¨æ©è†œTransformerï¼ˆEoMTï¼‰ï¼Œå®ƒé‡æ–°åˆ©ç”¨æ™®é€šçš„ViTæ¶æ„æ¥è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚åˆ©ç”¨å¤§è§„æ¨¡æ¨¡å‹å’Œé¢„è®­ç»ƒï¼ŒEoMTçš„åˆ†å‰²ç²¾åº¦ä¸ä½¿ç”¨ç‰¹å®šä»»åŠ¡ç»„ä»¶çš„æœ€å…ˆè¿›æ¨¡å‹ç›¸ä¼¼ã€‚åŒæ—¶ï¼Œç”±äºæ¶æ„ç®€å•ï¼ŒEoMTçš„è¿™äº›æ–¹æ³•æ˜æ˜¾æ›´å¿«ï¼Œä¾‹å¦‚ViT-Lé«˜è¾¾4å€ã€‚åœ¨å¤šç§æ¨¡å‹å¤§å°ä¸­ï¼ŒEoMTåœ¨åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œè¿™è¡¨æ˜è®¡ç®—èµ„æºæ›´å¥½åœ°ç”¨äºæ‰©å±•ViTæœ¬èº«ï¼Œè€Œä¸æ˜¯å¢åŠ æ¶æ„çš„å¤æ‚æ€§ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/%E3%80%82">https://www.tue-mps.org/eomt/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19108v1">PDF</a> CVPR 2025. Code: <a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/">https://www.tue-mps.org/eomt/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEncoder-only Mask Transformer (EoMT)çš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨çº¯ViTæ¶æ„è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨å¤§å‹æ¨¡å‹å’Œé¢„è®­ç»ƒçš„åŸºç¡€ä¸Šï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡ç»„ä»¶ï¼Œå³å¯è¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„åˆ†å‰²ç²¾åº¦ã€‚åŒæ—¶ï¼Œç”±äºæ¶æ„ç®€æ´ï¼ŒEoMTçš„é¢„æµ‹é€Ÿåº¦æ˜¾è‘—å¿«äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚ViT-Lé€Ÿåº¦æå‡è¾¾4å€ã€‚åœ¨ä¸åŒæ¨¡å‹å¤§å°ä¸‹ï¼ŒEoMTåœ¨åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´è¾¾åˆ°æœ€ä¼˜å¹³è¡¡ï¼Œè¡¨æ˜è®¡ç®—èµ„æºæ›´å¥½ç”¨äºæ‰©å¤§ViTè§„æ¨¡ï¼Œè€Œéå¢åŠ æ¶æ„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) å¯ä»¥åº”ç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨å·ç§¯é€‚é…å™¨ã€åƒç´ è§£ç å™¨å’ŒTransformerè§£ç å™¨è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>æœ¬ç ”ç©¶è¯æ˜ï¼Œå¤§å‹æ¨¡å‹å’Œé¢„è®­ç»ƒä¸‹ï¼ŒViTæœ¬èº«å¯ä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„è¯±å¯¼åè§ã€‚</li>
<li>å¼•å…¥Encoder-only Mask Transformer (EoMT)ï¼Œé‡‡ç”¨çº¯ViTæ¶æ„è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>EoMTåœ¨å¤§å‹æ¨¡å‹å’Œé¢„è®­ç»ƒçš„åŸºç¡€ä¸Šï¼Œè¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>EoMTæ¶æ„ç®€æ´ï¼Œé¢„æµ‹é€Ÿåº¦æ˜¾è‘—å¿«äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86b2b340e2b610e4f43be47dd78084c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9cda0532167423bd1fa3aed662395fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97c99e31828579f14b77e7267fe19573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7510c5475e908fe3d7fbfcb55a851df4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis"><a href="#Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis" class="headerlink" title="Efficient Self-Supervised Adaptation for Medical Image Analysis"></a>Efficient Self-Supervised Adaptation for Medical Image Analysis</h2><p><strong>Authors:Moein Sorkhei, Emir Konuk, Jingyu Guo, Christos Matsoukas, Kevin Smith</strong></p>
<p>Self-supervised adaptation (SSA) improves foundation model transfer to medical domains but is computationally prohibitive. Although parameter efficient fine-tuning methods such as LoRA have been explored for supervised adaptation, their effectiveness for SSA remains unknown. In this work, we introduce efficient self-supervised adaptation (ESSA), a framework that applies parameter-efficient fine-tuning techniques to SSA with the aim of reducing computational cost and improving adaptation performance. Among the methods tested, Attention Projection Layer Adaptation (APLA) sets a new state-of-the-art, consistently surpassing full-parameter SSA and supervised fine-tuning across diverse medical tasks, while reducing GPU memory by up to 40.1% and increasing training throughput by 25.2%, all while maintaining inference efficiency. </p>
<blockquote>
<p>è‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆSSAï¼‰æ”¹å–„äº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»ï¼Œä½†è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚è™½ç„¶å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å·²è¢«æ¢ç´¢ç”¨äºç›‘ç£é€‚åº”ï¼Œä½†å®ƒä»¬åœ¨SSAä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ•ˆè‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯åº”ç”¨äºSSAï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æ”¹å–„é€‚åº”æ€§èƒ½ã€‚åœ¨æµ‹è¯•çš„æ–¹æ³•ä¸­ï¼Œæ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰è¡¨ç°å“è¶Šï¼Œå§‹ç»ˆè¶…è¶Šå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒï¼Œåœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°é¢†å…ˆï¼ŒåŒæ—¶è¿˜å°†GPUå†…å­˜å‡å°‘äº†é«˜è¾¾40.1%ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡è¾¾25.2%ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç›‘ç£é€‚åº”ï¼ˆSSAï¼‰æ”¹è¿›åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»åº”ç”¨ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬ç ”ç©¶å¼•å…¥é«˜æ•ˆè‡ªç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åº”ç”¨äºSSAï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§èƒ½ã€‚å…¶ä¸­ï¼Œæ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰æ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ä¸åŒåŒ»å­¦ä»»åŠ¡ä¸­æŒç»­è¶…è¶Šå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†GPUå†…å­˜æ¶ˆè€—å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£é€‚åº”ï¼ˆSSAï¼‰æ”¹å–„äº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»åº”ç”¨ã€‚</li>
<li>è®¡ç®—æˆæœ¬æ˜¯SSAé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ESSAæ¡†æ¶åˆ©ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æ”¹å–„SSAçš„é€‚åº”æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>æ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰æ˜¯ESSAä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>APLAåœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>APLAé™ä½äº†GPUå†…å­˜æ¶ˆè€—å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d8b4f599ff44268bb20cbf604dfb6d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-380606255f1db1f238e4fd5f66b6f0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6052497ba5251734b558ad775cd93c78.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Learning-to-segment-anatomy-and-lesions-from-disparately-labeled-sources-in-brain-MRI"><a href="#Learning-to-segment-anatomy-and-lesions-from-disparately-labeled-sources-in-brain-MRI" class="headerlink" title="Learning to segment anatomy and lesions from disparately labeled sources   in brain MRI"></a>Learning to segment anatomy and lesions from disparately labeled sources   in brain MRI</h2><p><strong>Authors:Meva Himmetoglu, Ilja Ciernik, Ender Konukoglu</strong></p>
<p>Segmenting healthy tissue structures alongside lesions in brain Magnetic Resonance Images (MRI) remains a challenge for todayâ€™s algorithms due to lesion-caused disruption of the anatomy and lack of jointly labeled training datasets, where both healthy tissues and lesions are labeled on the same images. In this paper, we propose a method that is robust to lesion-caused disruptions and can be trained from disparately labeled training sets, i.e., without requiring jointly labeled samples, to automatically segment both. In contrast to prior work, we decouple healthy tissue and lesion segmentation in two paths to leverage multi-sequence acquisitions and merge information with an attention mechanism. During inference, an image-specific adaptation reduces adverse influences of lesion regions on healthy tissue predictions. During training, the adaptation is taken into account through meta-learning and co-training is used to learn from disparately labeled training images. Our model shows an improved performance on several anatomical structures and lesions on a publicly available brain glioblastoma dataset compared to the state-of-the-art segmentation methods. </p>
<blockquote>
<p>åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œå¯¹è„‘éƒ¨çš„å¥åº·ç»„ç»‡ç»“æ„å’Œç—…å˜éƒ¨ä½è¿›è¡Œåˆ†å‰²ä»ç„¶æ˜¯å½“ä»Šç®—æ³•é¢ä¸´çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç—…å˜å¼•èµ·çš„è§£å‰–ç»“æ„ç ´åä»¥åŠç¼ºä¹åœ¨åŒä¸€å›¾åƒä¸ŠåŒæ—¶æ ‡æ³¨çš„å¥åº·ç»„ç»‡å’Œç—…å˜éƒ¨ä½è”åˆæ ‡è®°çš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½å¤Ÿåº”å¯¹ç—…å˜å¼•èµ·çš„å¹²æ‰°ã€å¯ä»åˆ†æ•£æ ‡è®°çš„è®­ç»ƒé›†ä¸­è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ï¼Œå³æ— éœ€è”åˆæ ‡è®°æ ·æœ¬å³å¯è‡ªåŠ¨å¯¹ä¸¤è€…è¿›è¡Œåˆ†å‰²ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬å°†å¥åº·ç»„ç»‡å’Œç—…å˜éƒ¨ä½çš„åˆ†å‰²è§£è€¦ä¸ºä¸¤æ¡è·¯å¾„ï¼Œä»¥åˆ©ç”¨å¤šåºåˆ—é‡‡é›†å¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åˆå¹¶ä¿¡æ¯ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå›¾åƒç‰¹å®šçš„é€‚åº”æ€§è°ƒæ•´å‡å°‘äº†ç—…å˜åŒºåŸŸå¯¹å¥åº·ç»„ç»‡é¢„æµ‹çš„ä¸åˆ©å½±å“ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å…ƒå­¦ä¹ è€ƒè™‘é€‚åº”æ€§è°ƒæ•´ï¼Œå¹¶ä½¿ç”¨ååŒè®­ç»ƒä»åˆ†æ•£æ ‡è®°çš„è®­ç»ƒå›¾åƒä¸­å­¦ä¹ ã€‚ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å…¬å¼€å¯ç”¨çš„è„‘èƒ¶è´¨æ¯ç»†èƒç˜¤æ•°æ®é›†ä¸Šçš„å¤šä¸ªè§£å‰–ç»“æ„å’Œç—…å˜éƒ¨ä½ä¸Šè¡¨ç°å‡ºæ”¹è¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18840v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€è”åˆæ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä»åˆ†æ•£æ ‡æ³¨çš„è®­ç»ƒé›†ä¸­è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨åˆ†å‰²å¥åº·å’Œç—…å˜ç»„ç»‡ã€‚è¯¥æ–¹æ³•é€šè¿‡è§£è€¦å¥åº·ç»„ç»‡å’Œç—…å˜çš„åˆ†å‰²è·¯å¾„ï¼Œåˆ©ç”¨å¤šåºåˆ—é‡‡é›†çš„ä¿¡æ¯å¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œåˆå¹¶ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å›¾åƒç‰¹å®šçš„é€‚åº”æ€§å‡å°‘ç—…å˜åŒºåŸŸå¯¹å¥åº·ç»„ç»‡é¢„æµ‹çš„ä¸åˆ©å½±å“ã€‚è¯¥ç ”ç©¶æ”¹è¿›äº†ç°æœ‰çš„åˆ†å‰²æ–¹æ³•ï¼Œå¹¶åœ¨å…¬å…±å¯ç”¨çš„è„‘èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåº”å¯¹ç—…å˜å¼•èµ·çš„ç»“æ„ç ´åï¼Œæ— éœ€è”åˆæ ‡æ³¨çš„è®­ç»ƒæ•°æ®é›†å³å¯è¿›è¡Œå¥åº·ç»„ç»‡å’Œç—…å˜çš„è‡ªåŠ¨åˆ†å‰²ã€‚</li>
<li>é€šè¿‡è§£è€¦å¥åº·ç»„ç»‡å’Œç—…å˜çš„åˆ†å‰²è·¯å¾„ï¼Œåˆ©ç”¨å¤šåºåˆ—é‡‡é›†ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åˆå¹¶è¿™äº›ä¿¡æ¯ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å›¾åƒç‰¹å®šçš„é€‚åº”æ€§ç­–ç•¥ï¼Œä»¥å‡å°‘ç—…å˜åŒºåŸŸå¯¹å¥åº·ç»„ç»‡é¢„æµ‹çš„ä¸åˆ©å½±å“ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å…ƒå­¦ä¹ å’ŒååŒè®­ç»ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è€ƒè™‘é€‚åº”æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å¼€å¯ç”¨çš„è„‘èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¦‚ç—…å˜å¼•èµ·çš„è§£å‰–ç»“æ„ç ´åå’Œç¼ºä¹è”åˆæ ‡æ³¨çš„è®­ç»ƒæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4457cf6ff66b7f71709de619ca7fe4a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b7f96f5b881a419856f0cdd0db5d66d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation"><a href="#Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation" class="headerlink" title="Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation"></a>Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation</h2><p><strong>Authors:Qin Wang, Benjamin Bruns, Hanno Scharr, Kai Krajsek</strong></p>
<p>The equivariant behaviour of features is essential in many computer vision tasks, yet popular self-supervised learning (SSL) methods tend to constrain equivariance by design. We propose a self-supervised learning approach where the system learns transformations independently by reconstructing images that have undergone previously unseen transformations. Specifically, the model is tasked to reconstruct intermediate transformed images, e.g. translated or rotated images, without prior knowledge of these transformations. This auxiliary task encourages the model to develop equivariance-coherent features without relying on predefined transformation rules. To this end, we apply transformations to the input image, generating an image pair, and then split the extracted features into two sets per image. One set is used with a usual SSL loss encouraging invariance, the other with our loss based on the auxiliary task to reconstruct the intermediate transformed images. Our loss and the SSL loss are linearly combined with weighted terms. Evaluating on synthetic tasks with natural images, our proposed method strongly outperforms all competitors, regardless of whether they are designed to learn equivariance. Furthermore, when trained alongside augmentation-based methods as the invariance tasks, such as iBOT or DINOv2, we successfully learn a balanced combination of invariant and equivariant features. Our approach performs strong on a rich set of realistic computer vision downstream tasks, almost always improving over all baselines. </p>
<blockquote>
<p>ç‰¹å¾çš„ä¸å˜æ€§åœ¨è®¸å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œæµè¡Œçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è®¾è®¡æ—¶å¾€å¾€é™åˆ¶å…¶ä¸å˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­ç³»ç»Ÿé€šè¿‡é‡å»ºç»è¿‡å…ˆå‰æœªè§å˜æ¢çš„å›¾åƒæ¥ç‹¬ç«‹åœ°å­¦ä¹ å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯é‡å»ºä¸­é—´å˜æ¢å›¾åƒï¼Œä¾‹å¦‚å¹³ç§»æˆ–æ—‹è½¬å›¾åƒï¼Œè€Œæ— éœ€äº‹å…ˆçŸ¥é“è¿™äº›å˜æ¢ã€‚æ­¤è¾…åŠ©ä»»åŠ¡é¼“åŠ±æ¨¡å‹å‘å±•ç¬¦åˆä¸å˜æ€§çš„ç‰¹å¾ï¼Œæ— éœ€ä¾èµ–é¢„å®šä¹‰çš„å˜æ¢è§„åˆ™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥å›¾åƒåº”ç”¨å˜æ¢ï¼Œç”Ÿæˆå›¾åƒå¯¹ï¼Œç„¶åå°†æå–çš„ç‰¹å¾ä¸ºæ¯ä¸ªå›¾åƒåˆ†æˆä¸¤ç»„ã€‚ä¸€ç»„ä½¿ç”¨é€šå¸¸çš„è‡ªç›‘ç£æŸå¤±æ¥é¼“åŠ±ä¸å˜æ€§ï¼Œå¦ä¸€ç»„ä½¿ç”¨åŸºäºé‡å»ºä¸­é—´å˜æ¢å›¾åƒçš„è¾…åŠ©ä»»åŠ¡çš„æŸå¤±ã€‚æˆ‘ä»¬çš„æŸå¤±å’Œè‡ªç›‘ç£æŸå¤±é€šè¿‡åŠ æƒé¡¹è¿›è¡Œçº¿æ€§ç»„åˆã€‚åœ¨å¯¹è‡ªç„¶å›¾åƒè¿›è¡Œåˆæˆä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œæ— è®ºæ˜¯å¦è®¾è®¡ä¸ºå­¦ä¹ ä¸å˜æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éƒ½å¤§å¤§ä¼˜äºæ‰€æœ‰ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œå½“ä¸åŸºäºå¢å¼ºçš„æ–¹æ³•ï¼ˆå¦‚iBOTæˆ–DINOv2ï¼‰ä½œä¸ºä¸å˜æ€§ä»»åŠ¡ä¸€èµ·è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æˆåŠŸå­¦ä¼šäº†ä¸å˜æ€§å’Œç­‰å˜æ€§çš„å¹³è¡¡ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸°å¯Œçš„å®é™…è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‡ ä¹æ€»æ˜¯ä¼˜äºæ‰€æœ‰åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18753v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é‡å»ºç»è¿‡æœªè§è½¬æ¢çš„å›¾åƒæ¥ç‹¬ç«‹åœ°å­¦ä¹ è½¬æ¢ï¼Œä½¿ç³»ç»Ÿå‘å±•å‡ºç¬¦åˆç­‰å˜çš„ç‰¹å¾ï¼Œè€Œæ— éœ€ä¾èµ–é¢„è®¾çš„è½¬æ¢è§„åˆ™ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆä»»åŠ¡ä¸Šä½¿ç”¨è‡ªç„¶å›¾åƒè¿›è¡Œè¯„ä¼°æ—¶ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŸºäºå¢å¼ºçš„æ–¹æ³•ä¸€èµ·è®­ç»ƒæ—¶ï¼Œèƒ½å¤ŸæˆåŠŸå­¦ä¹ ä¸å˜å’Œç­‰å˜ç‰¹å¾çš„å¹³è¡¡ç»„åˆã€‚æ­¤æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡å¼ºè°ƒç‰¹å¾åœ¨ç­‰å˜è¡Œä¸ºä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰SSLæ–¹æ³•å¯¹ç­‰å˜æ€§çš„çº¦æŸã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é‡å»ºç»è¿‡æœªè§è½¬æ¢çš„å›¾åƒæ¥è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚å¹³ç§»æˆ–æ—‹è½¬çš„å›¾åƒï¼Œä»è€Œé¼“åŠ±æ¨¡å‹å‘å±•å‡ºç¬¦åˆç­‰å˜çš„ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸éœ€è¦é¢„å…ˆçŸ¥é“å›¾åƒè½¬æ¢çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥è®ºæ–‡æå‡ºçš„æ–¹æ³•åœ¨åˆæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŸºäºå¢å¼ºçš„æ–¹æ³•ç»“åˆæ—¶ï¼Œèƒ½å¤Ÿå­¦ä¹ ä¸å˜å’Œç­‰å˜ç‰¹å¾çš„å¹³è¡¡ç»„åˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å®ç”¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸”å‡ ä¹æ€»æ˜¯ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4624e380e737448d093a3f8980418dc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a84161630958b82b2d090cd96b8cce4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-719fb08ef408a26365be538a9207d86d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b91a3e6831f344e4761ea4bd13483ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feff8439c69758e551d9d94b3261bb10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b667ed4a87e288129c7ae8edbcc9859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b041a509a2a5f5b42a308a78d2f200dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deda0d46342074f6b8bc70a3a9a09b41.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="NullSwap-Proactive-Identity-Cloaking-Against-Deepfake-Face-Swapping"><a href="#NullSwap-Proactive-Identity-Cloaking-Against-Deepfake-Face-Swapping" class="headerlink" title="NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping"></a>NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping</h2><p><strong>Authors:Tianyi Wang, Harry Cheng, Xiao Zhang, Yinglong Wang</strong></p>
<p>Suffering from performance bottlenecks in passively detecting high-quality Deepfake images due to the advancement of generative models, proactive perturbations offer a promising approach to disabling Deepfake manipulations by inserting signals into benign images. However, existing proactive perturbation approaches remain unsatisfactory in several aspects: 1) visual degradation due to direct element-wise addition; 2) limited effectiveness against face swapping manipulation; 3) unavoidable reliance on white- and grey-box settings to involve generative models during training. In this study, we analyze the essence of Deepfake face swapping and argue the necessity of protecting source identities rather than target images, and we propose NullSwap, a novel proactive defense approach that cloaks source image identities and nullifies face swapping under a pure black-box scenario. We design an Identity Extraction module to obtain facial identity features from the source image, while a Perturbation Block is then devised to generate identity-guided perturbations accordingly. Meanwhile, a Feature Block extracts shallow-level image features, which are then fused with the perturbation in the Cloaking Block for image reconstruction. Furthermore, to ensure adaptability across different identity extractors in face swapping algorithms, we propose Dynamic Loss Weighting to adaptively balance identity losses. Experiments demonstrate the outstanding ability of our approach to fool various identity recognition models, outperforming state-of-the-art proactive perturbations in preventing face swapping models from generating images with correct source identities. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œè¢«åŠ¨æ£€æµ‹é«˜è´¨é‡Deepfakeå›¾åƒæ—¶å‡ºç°äº†æ€§èƒ½ç“¶é¢ˆï¼Œä¸»åŠ¨æ‰°åŠ¨æ–¹æ³•é€šè¿‡åœ¨è‰¯æ€§å›¾åƒä¸­æ’å…¥ä¿¡å·æ¥ç¦ç”¨Deepfakeæ“ä½œï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸»åŠ¨æ‰°åŠ¨æ–¹æ³•åœ¨æŸäº›æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼š1ï¼‰ç”±äºç›´æ¥å…ƒç´ çº§æ·»åŠ å¯¼è‡´çš„è§†è§‰é€€åŒ–ï¼›2ï¼‰å¯¹é¢éƒ¨æ›¿æ¢æ“ä½œçš„æ•ˆç”¨æœ‰é™ï¼›3ï¼‰ä¸å¯é¿å…åœ°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–äºç™½ç›’å’Œç°ç›’è®¾ç½®ä»¥æ¶‰åŠç”Ÿæˆæ¨¡å‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†Deepfakeé¢éƒ¨æ›¿æ¢çš„æœ¬è´¨ï¼Œå¹¶è®¤ä¸ºä¿æŠ¤æºèº«ä»½è€Œä¸æ˜¯ç›®æ ‡å›¾åƒæ˜¯å¿…è¦çš„ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†NullSwapï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¸»åŠ¨é˜²å¾¡æ–¹æ³•ï¼Œå¯åœ¨çº¯é»‘ç®±åœºæ™¯ä¸‹æ©ç›–æºå›¾åƒèº«ä»½å¹¶æ¶ˆé™¤é¢éƒ¨æ›¿æ¢åŠŸèƒ½ã€‚æˆ‘ä»¬è®¾è®¡äº†èº«ä»½æå–æ¨¡å—ä»¥ä»æºå›¾åƒä¸­è·å–é¢éƒ¨èº«ä»½ç‰¹å¾ï¼Œç„¶åè®¾è®¡äº†æ‰°åŠ¨å—ä»¥ç›¸åº”åœ°ç”Ÿæˆèº«ä»½æŒ‡å¯¼æ‰°åŠ¨ã€‚åŒæ—¶ï¼Œç‰¹å¾å—æå–æµ…å±‚å›¾åƒç‰¹å¾ï¼Œç„¶åå°†å…¶ä¸é®è”½å—ä¸­çš„æ‰°åŠ¨ç›¸ç»“åˆè¿›è¡Œå›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿é€‚åº”é¢éƒ¨æ›¿æ¢ç®—æ³•ä¸­çš„ä¸åŒèº«ä»½æå–å™¨ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€æŸå¤±åŠ æƒä»¥è‡ªé€‚åº”åœ°å¹³è¡¡èº«ä»½æŸå¤±ã€‚å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ¬ºéª—å„ç§èº«ä»½è¯†åˆ«æ¨¡å‹æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›ï¼Œåœ¨é˜²æ­¢é¢éƒ¨æ›¿æ¢æ¨¡å‹ç”Ÿæˆå…·æœ‰æ­£ç¡®æºèº«ä»½çš„å›¾åƒæ–¹é¢ï¼Œä¼˜äºæœ€å…ˆè¿›çš„ä¸»åŠ¨æ‰°åŠ¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºNullSwapçš„æ–°å‹ä¸»åŠ¨é˜²å¾¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨çº¯é»‘ç®±åœºæ™¯ä¸‹éšè—æºå›¾åƒèº«ä»½å¹¶é˜»æ­¢é¢éƒ¨æ›¿æ¢ã€‚é€šè¿‡è®¾è®¡èº«ä»½æå–æ¨¡å—å’Œæ‰°åŠ¨å—ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç›¸åº”çš„èº«ä»½å¼•å¯¼æ‰°åŠ¨ï¼Œå¹¶é€šè¿‡ç‰¹å¾å—è¿›è¡Œæµ…å±‚å›¾åƒç‰¹å¾çš„æå–å’Œé‡å»ºã€‚åŒæ—¶é‡‡ç”¨åŠ¨æ€æŸå¤±æƒé‡æŠ€æœ¯ï¼Œç¡®ä¿å¯¹ä¸åŒèº«ä»½æå–å™¨çš„é€‚åº”æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é˜²æ­¢é¢éƒ¨æ›¿æ¢æ¨¡å‹ç”Ÿæˆå…·æœ‰æ­£ç¡®æºèº«ä»½å›¾åƒæ–¹é¢å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰ä¸»åŠ¨æ‰°åŠ¨æ–¹æ³•åœ¨å¤„ç†Deepfakeé¢éƒ¨æ›¿æ¢æ—¶å­˜åœ¨è§†è§‰é€€åŒ–ã€æ•ˆæœæœ‰é™ä»¥åŠå¯¹ç™½ç›’å’Œç°ç›’è®¾ç½®çš„ä¾èµ–ç­‰é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºNullSwapæ–¹æ³•ï¼Œæ—¨åœ¨éšè—æºå›¾åƒèº«ä»½å¹¶é˜»æ­¢é¢éƒ¨æ›¿æ¢ï¼Œé€‚ç”¨äºçº¯é»‘ç®±åœºæ™¯ã€‚</li>
<li>NullSwapé€šè¿‡è®¾è®¡èº«ä»½æå–æ¨¡å—å’Œæ‰°åŠ¨å—ç”Ÿæˆèº«ä»½å¼•å¯¼æ‰°åŠ¨ï¼Œé€šè¿‡ç‰¹å¾å—è¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€æŸå¤±æƒé‡æŠ€æœ¯ï¼Œä»¥æé«˜å¯¹ä¸åŒé¢éƒ¨æ›¿æ¢ç®—æ³•ä¸­èº«ä»½æå–å™¨çš„é€‚åº”æ€§ã€‚</li>
<li>å®éªŒè¯æ˜NullSwapåœ¨é˜²æ­¢é¢éƒ¨æ›¿æ¢æ¨¡å‹ç”Ÿæˆå…·æœ‰æ­£ç¡®æºèº«ä»½å›¾åƒæ–¹é¢çš„æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸»åŠ¨æ‰°åŠ¨æ–¹æ³•ã€‚</li>
<li>NullSwapæ–¹æ³•å¼ºè°ƒä¿æŠ¤æºå›¾åƒèº«ä»½çš„é‡è¦æ€§ï¼Œè€Œä¸æ˜¯ç›®æ ‡å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a507071af01bd7eca5f7aa0ec3d77fa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50d293e0bd909c97d7fa9a153c52e23a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eeb210201cd46f0c5a1fb1e9a7fa321.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e54ce83ae78f1ee87675d5718462a27a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2b752877c656c637f4afd3a3af12614f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Video-T1 Test-Time Scaling for Video Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3ba9c1323f6e44da60a300846902f26f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  PartRM Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
