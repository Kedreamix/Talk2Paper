<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Scaling Vision Pre-Training to 4K Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1b9aeb002c5dca7ac7509bd575c42d14.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="Scaling-Vision-Pre-Training-to-4K-Resolution"><a href="#Scaling-Vision-Pre-Training-to-4K-Resolution" class="headerlink" title="Scaling Vision Pre-Training to 4K Resolution"></a>Scaling Vision Pre-Training to 4K Resolution</h2><p><strong>Authors:Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, Hongxu Yin</strong></p>
<p>High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡è§†è§‰ç»†èŠ‚çš„æ„ŸçŸ¥å¯¹äºæ—¥å¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†è§‰é¢„è®­ç»ƒä»ç„¶å—é™äºä½åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ï¼Œ378 x 378åƒç´ ï¼‰ï¼Œå› ä¸ºå¤„ç†æ›´å¤§å›¾åƒçš„æˆæœ¬æ˜¯äºŒæ¬¡æ–¹çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†PS3ï¼Œå®ƒä»¥æ¥è¿‘æ’å®šçš„æˆæœ¬å°†CLIPé£æ ¼çš„è§†è§‰é¢„è®­ç»ƒæ‰©å±•åˆ°4Kåˆ†è¾¨ç‡ã€‚PS3ä¸åŒäºåŸºäºå…¨å±€å›¾åƒè¡¨ç¤ºçš„å¯¹æ¯”å­¦ä¹ ï¼Œå®ƒé€šè¿‡å¯¹å±€éƒ¨åŒºåŸŸè¿›è¡Œé€‰æ‹©æ€§å¤„ç†ï¼Œå¹¶ä¸å±€éƒ¨è¯¦ç»†å­—å¹•è¿›è¡Œå¯¹æ¯”æ¥é¢„å…ˆè®­ç»ƒï¼Œä»è€Œä»¥å¤§å¤§é™ä½çš„è®¡ç®—å¼€é”€å®ç°äº†é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ã€‚é¢„è®­ç»ƒçš„PS3èƒ½å¤Ÿåœ¨ä½åˆ†è¾¨ç‡ä¸‹å¯¹å…¨å±€å›¾åƒè¿›è¡Œç¼–ç ï¼Œå¹¶æ ¹æ®å±€éƒ¨é«˜åˆ†è¾¨ç‡åŒºåŸŸçš„æ˜¾è‘—æ€§æˆ–ä¸æ–‡æœ¬æç¤ºçš„ç›¸å…³æ€§å¯¹å…¶è¿›è¡Œé€‰æ‹©æ€§å¤„ç†ã€‚å°†PS3åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶ï¼Œæ‰€å¾—æ¨¡å‹VILA-HDçš„é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¾—åˆ°äº†æ˜¾ç€æé«˜ï¼Œç›¸è¾ƒäºæ²¡æœ‰é«˜åˆ†è¾¨ç‡è§†è§‰é¢„è®­ç»ƒçš„åŸºå‡†æ¨¡å‹ï¼ˆå¦‚AnyReså’ŒS^2ï¼‰ï¼Œå®ƒä½¿ç”¨çš„ä»¤ç‰Œæ•°è¦å°‘è¾¾4.3å€ã€‚PS3è¿˜è§£é”äº†VILA-HDçš„å¸å¼•äººçš„å¯æ‰©å±•å±æ€§ï¼ŒåŒ…æ‹¬å…è´¹æ‰©å±•åˆ†è¾¨ç‡å’Œæ‰©å±•æµ‹è¯•æ—¶é—´çš„è®¡ç®—ä»¥æé«˜æ€§èƒ½ã€‚ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒVILA-HDåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¹‹å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚NVILAå’ŒQwen2-VLï¼‰ï¼Œå¹¶ä¸”åœ¨æ–°çš„æè®®çš„4Kåˆ†è¾¨ç‡å›¾åƒé—®ç­”åŸºå‡†4KProä¸Šè¡¨ç°æ›´å‡ºè‰²ï¼Œåœ¨è¯¥åŸºå‡†ä¸Šï¼ŒVILA-HDä¼˜äºæ‰€æœ‰å…ˆå‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¯¹GPT-4oçš„14.5%æ”¹è¿›ï¼Œä»¥åŠå¯¹Qwen2-VLçš„3.2%æ”¹è¿›å’Œ2.96å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19903v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/PS3">https://nvlabs.github.io/PS3</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PS3ï¼Œä¸€ç§å°†CLIPé£æ ¼çš„è§†è§‰é¢„è®­ç»ƒæ‰©å±•åˆ°4Kåˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œå…·æœ‰è¿‘æ’å®šçš„æˆæœ¬ã€‚PS3é€šè¿‡é€‰æ‹©æ€§å¤„ç†å±€éƒ¨åŒºåŸŸå’Œä¸å±€éƒ¨è¯¦ç»†å­—å¹•è¿›è¡Œå¯¹æ¯”æ¥é¢„è®­ç»ƒï¼Œå®ç°äº†é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚é¢„è®­ç»ƒçš„PS3èƒ½å¤Ÿç¼–ç ä½åˆ†è¾¨ç‡çš„å…¨å±€å›¾åƒï¼Œå¹¶æ ¹æ®æ–‡æœ¬æç¤ºé€‰æ‹©æ€§å¤„ç†å±€éƒ¨é«˜åˆ†è¾¨ç‡åŒºåŸŸã€‚å°†PS3åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åï¼Œç”Ÿæˆçš„VILA-HDæ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥æ–¹é¢æ˜¾è‘—ä¼˜äºæœªè¿›è¡Œé«˜åˆ†è¾¨ç‡è§†è§‰é¢„è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚AnyReså’ŒS^2ã€‚åŒæ—¶ï¼ŒPS3è¿˜è§£é”äº†VILA-HDçš„å¸å¼•äººçš„å¯æ‰©å±•å±æ€§ï¼ŒåŒ…æ‹¬å…è´¹æ‰©å¤§åˆ†è¾¨ç‡å’Œå¢åŠ æµ‹è¯•æ—¶é—´çš„è®¡ç®—ä»¥æé«˜æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVILA-HDè¡¨ç°ä¼˜äºå…¶ä»–MLLMsï¼Œå¦‚NVILAå’ŒQwen2-VLã€‚æ­¤å¤–ï¼Œé’ˆå¯¹å½“å‰åŸºå‡†æµ‹è¯•ä¸éœ€è¦4Kåˆ†è¾¨ç‡æ„ŸçŸ¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å›¾åƒé—®ç­”åŸºå‡†æµ‹è¯•â€”â€”4KProï¼ŒVILA-HDåœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæ‰€æœ‰å…ˆå‰çš„MLLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PS3æ˜¯ä¸€ç§å°†CLIPé£æ ¼çš„è§†è§‰é¢„è®­ç»ƒæ‰©å±•åˆ°4Kåˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œèƒ½é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>PS3é€šè¿‡é€‰æ‹©æ€§å¤„ç†å±€éƒ¨åŒºåŸŸå’Œå¯¹æ¯”å±€éƒ¨è¯¦ç»†å­—å¹•è¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>é¢„è®­ç»ƒçš„PS3èƒ½ç¼–ç å…¨å±€ä½åˆ†è¾¨ç‡å›¾åƒå¹¶é€‰æ‹©æ€§å¤„ç†å±€éƒ¨é«˜åˆ†è¾¨ç‡åŒºåŸŸã€‚</li>
<li>VILA-HDæ¨¡å‹é€šè¿‡åº”ç”¨PS3æ˜¾è‘—æé«˜äº†é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹AnyReså’ŒS^2ç­‰ã€‚</li>
<li>VILA-HDæ¨¡å‹å±•ç°å‡ºå¸å¼•äººçš„å¯æ‰©å±•å±æ€§ï¼Œå¦‚æ”¯æŒæ‰©å¤§åˆ†è¾¨ç‡å’Œå¢åŠ æµ‹è¯•è®¡ç®—æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVILA-HDæ€§èƒ½ä¼˜äºå…¶ä»–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14a162c3cac4b6d19cb10b94506c3c2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51a7799afaf01f4f6789c01f87b76643.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ad6bb066f71e874a9b94e43b1be6914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad084558a5792e0d27295a1910b36fd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2a01655f8d3a12de59ed8095c2e88f7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="fine-CLIP-Enhancing-Zero-Shot-Fine-Grained-Surgical-Action-Recognition-with-Vision-Language-Models"><a href="#fine-CLIP-Enhancing-Zero-Shot-Fine-Grained-Surgical-Action-Recognition-with-Vision-Language-Models" class="headerlink" title="fine-CLIP: Enhancing Zero-Shot Fine-Grained Surgical Action Recognition   with Vision-Language Models"></a>fine-CLIP: Enhancing Zero-Shot Fine-Grained Surgical Action Recognition   with Vision-Language Models</h2><p><strong>Authors:Saurav Sharma, Didier Mutter, Nicolas Padoy</strong></p>
<p>While vision-language models like CLIP have advanced zero-shot surgical phase recognition, they struggle with fine-grained surgical activities, especially action triplets. This limitation arises because current CLIP formulations rely on global image features, which overlook the fine-grained semantics and contextual details crucial for complex tasks like zero-shot triplet recognition. Furthermore, these models do not explore the hierarchical structure inherent in triplets, reducing their ability to generalize to novel triplets. To address these challenges, we propose fine-CLIP, which learns object-centric features and lever- ages the hierarchy in triplet formulation. Our approach integrates three components: hierarchical prompt modeling to capture shared semantics, LoRA-based vision backbone adaptation for enhanced feature extraction, and a graph-based condensation strategy that groups similar patch features into meaningful object clusters. Since triplet classification is a challenging task, we introduce an alternative yet meaningful base-to-novel generalization benchmark with two settings on the CholecT50 dataset: Unseen-Target, assessing adaptability to triplets with novel anatomical structures, and Unseen-Instrument-Verb, where models need to generalize to novel instrument-verb interactions. fine-CLIP shows significant improvements in F1 and mAP, enhancing zero-shot recognition of novel surgical triplets. </p>
<blockquote>
<p>è™½ç„¶CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹å·²ç»æé«˜äº†é›¶æ ·æœ¬æ‰‹æœ¯é˜¶æ®µçš„è¯†åˆ«èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç²¾ç»†çš„æ‰‹æœ¯æ–¹å¼ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åŠ¨ä½œä¸‰å…ƒç»„ã€‚è¿™ä¸€å±€é™æ€§æ˜¯ç”±äºå½“å‰çš„CLIPæ¨¡å‹ä¾èµ–äºå…¨å±€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†ç²¾ç»†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œè¿™åœ¨é›¶æ ·æœ¬ä¸‰å…ƒç»„è¯†åˆ«ç­‰å¤æ‚ä»»åŠ¡ä¸­éå¸¸å…³é”®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹å¹¶æ²¡æœ‰æ¢ç´¢ä¸‰å…ƒç»„å›ºæœ‰çš„å±‚æ¬¡ç»“æ„ï¼Œé™ä½äº†å®ƒä»¬å¯¹æ–°å‹ä¸‰å…ƒç»„çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†fine-CLIPï¼Œå®ƒé€šè¿‡å­¦ä¹ ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç‰¹æ€§å’Œåˆ©ç”¨ä¸‰å…ƒç»„ä¸­çš„å±‚æ¬¡ç»“æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ä¸‰ä¸ªç»„ä»¶ï¼šå±‚æ¬¡æç¤ºå»ºæ¨¡ä»¥æ•è·å…±äº«è¯­ä¹‰ã€åŸºäºLoRAçš„è§†è§‰ä¸»å¹²é€‚åº”æ€§å¢å¼ºç‰¹å¾æå–ä»¥åŠåŸºäºå›¾çš„å‹ç¼©ç­–ç•¥ï¼Œå°†ç›¸ä¼¼çš„è¡¥ä¸ç‰¹å¾ç»„åˆæˆæœ‰æ„ä¹‰çš„å¯¹è±¡é›†ç¾¤ã€‚ç”±äºä¸‰å…ƒç»„åˆ†ç±»æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨CholecT50æ•°æ®é›†ä¸Šå¼•å…¥äº†ä¸¤ç§è®¾ç½®çš„æœ‰æ„ä¹‰å’Œæ›¿ä»£æ€§çš„åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–åŸºå‡†æµ‹è¯•ï¼šæœªè§è¿‡ç›®æ ‡æµ‹è¯•ï¼Œè¯„ä¼°å¯¹å…·æœ‰æ–°é¢–è§£å‰–ç»“æ„çš„ä¸‰å…ƒç»„çš„é€‚åº”æ€§ï¼›æœªè§è¿‡çš„ä»ªå™¨åŠ¨è¯æµ‹è¯•ï¼Œæ¨¡å‹éœ€è¦æ³›åŒ–åˆ°æ–°é¢–çš„ä»ªå™¨åŠ¨è¯äº¤äº’ã€‚fine-CLIPåœ¨F1å’ŒmAPä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œæé«˜äº†é›¶æ ·æœ¬æ–°å‹æ‰‹æœ¯ä¸‰å…ƒç»„çš„è¯†åˆ«èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19670v1">PDF</a> 6 pages, 3 tables, 3 figures</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨é›¶æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«æ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ç²¾ç»†çš„æ‰‹æœ¯æ´»åŠ¨ï¼Œå°¤å…¶æ˜¯åŠ¨ä½œä¸‰å…ƒç»„è¯†åˆ«ä¸Šä»æœ‰å›°éš¾ã€‚è¿™æ˜¯å› ä¸ºå½“å‰CLIPæ¨¡å‹ä¾èµ–äºå…¨å±€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†ç²¾ç»†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå¯¹äºé›¶æ ·æœ¬ä¸‰å…ƒç»„è¯†åˆ«ç­‰å¤æ‚ä»»åŠ¡è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹æ²¡æœ‰æ¢ç´¢ä¸‰å…ƒç»„ä¸­çš„å±‚æ¬¡ç»“æ„ï¼Œé™ä½äº†å¯¹æ–°å‹ä¸‰å…ƒç»„çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†fine-CLIPï¼Œå®ƒå­¦ä¹ å¯¹è±¡ä¸­å¿ƒçš„ç‰¹å¾å¹¶åˆ©ç”¨ä¸‰å…ƒç»„ä¸­çš„å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ä¸‰ä¸ªç»„ä»¶ï¼šåˆ†å±‚æç¤ºå»ºæ¨¡ä»¥æ•è·å…±äº«è¯­ä¹‰ï¼ŒåŸºäºLoRAçš„è§†è§‰ä¸»å¹²é€‚åº”ä»¥å¢å¼ºç‰¹å¾æå–ï¼Œä»¥åŠåŸºäºå›¾çš„å‡èšç­–ç•¥ï¼Œå°†ç›¸ä¼¼çš„è¡¥ä¸ç‰¹å¾åˆ†ç»„ä¸ºæœ‰æ„ä¹‰çš„å¯¹è±¡é›†ç¾¤ã€‚åœ¨CholecT50æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ›¿ä»£ä½†æœ‰æ„ä¹‰çš„åŸºç¡€åˆ°æ–°å‹çš„æ³›åŒ–åŸºå‡†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªè®¾ç½®ï¼šæœªè§ç›®æ ‡ï¼Œè¯„ä¼°å¯¹å…·æœ‰æ–°å‹è§£å‰–ç»“æ„çš„ä¸‰å…ƒç»„çš„é€‚åº”æ€§ï¼›æœªè§ä»ªå™¨åŠ¨è¯ï¼Œæ¨¡å‹éœ€è¦æ³›åŒ–åˆ°æ–°å‹çš„ä»ªå™¨åŠ¨è¯äº¤äº’ã€‚fine-CLIPåœ¨F1å’ŒmAPä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œæé«˜äº†é›¶æ ·æœ¬æ–°å‹æ‰‹æœ¯ä¸‰å…ƒç»„è¯†åˆ«çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨ç²¾ç»†æ‰‹æœ¯æ´»åŠ¨çš„è¯†åˆ«ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œä¸‰å…ƒç»„è¯†åˆ«æ–¹é¢ã€‚</li>
<li>è¿™ç§å±€é™æ€§æºäºæ¨¡å‹ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†å…³é”®ä¿¡æ¯çš„ç»†èŠ‚ã€‚</li>
<li>fine-CLIPé€šè¿‡ç»“åˆå¯¹è±¡ä¸­å¿ƒçš„ç‰¹å¾å’Œä¸‰å…ƒç»„ä¸­çš„å±‚æ¬¡ç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>fine-CLIPé›†æˆäº†åˆ†å±‚æç¤ºå»ºæ¨¡ã€è§†è§‰ä¸»å¹²é€‚åº”å’ŒåŸºäºå›¾çš„å‡èšç­–ç•¥ç­‰æ–¹æ³•æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨CholecT50æ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æœªè§ç›®æ ‡å’Œæœªè§ä»ªå™¨åŠ¨è¯ä¸¤ä¸ªè®¾ç½®ã€‚</li>
<li>fine-CLIPåœ¨F1å’ŒmAPæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜ï¼Œæ˜¾ç¤ºå‡ºå…¶å¯¹äºé›¶æ ·æœ¬æ–°å‹æ‰‹æœ¯ä¸‰å…ƒç»„è¯†åˆ«çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-216404ce9f80ed4443ea0e4202f773f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa49ba8a13603de4b8b42c92c184778f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878e0ebe2c4994e7b32048413b74bedc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-706abe3f02beee81aa27951cea7d1584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4da621424404f5b56263fabf371c981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c4134963106170a0b8cda92d52e16f5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ChA-MAEViT-Unifying-Channel-Aware-Masked-Autoencoders-and-Multi-Channel-Vision-Transformers-for-Improved-Cross-Channel-Learning"><a href="#ChA-MAEViT-Unifying-Channel-Aware-Masked-Autoencoders-and-Multi-Channel-Vision-Transformers-for-Improved-Cross-Channel-Learning" class="headerlink" title="ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel   Vision Transformers for Improved Cross-Channel Learning"></a>ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel   Vision Transformers for Improved Cross-Channel Learning</h2><p><strong>Authors:Chau Pham, Juan C. Caicedo, Bryan A. Plummer</strong></p>
<p>Prior work using Masked Autoencoders (MAEs) typically relies on random patch masking based on the assumption that images have significant redundancies across different channels, allowing for the reconstruction of masked content using cross-channel correlations. However, this assumption does not hold in Multi-Channel Imaging (MCI), where channels may provide complementary information with minimal feature overlap. Thus, these MAEs primarily learn local structures within individual channels from patch reconstruction, failing to fully leverage cross-channel interactions and limiting their MCI effectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that enhances feature learning across MCI channels via four key strategies: (1) dynamic channel-patch masking, which compels the model to reconstruct missing channels in addition to masked patches, thereby enhancing cross-channel dependencies and improving robustness to varying channel configurations; (2) memory tokens, which serve as long-term memory aids to promote information sharing across channels, addressing the challenges of reconstructing structurally diverse channels; (3) hybrid token fusion module, which merges fine-grained patch tokens with a global class token to capture richer representations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes channel tokens to effectively reconstruct image patches. Experiments on satellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that ChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%, highlighting the importance of cross-channel interactions in MCI. </p>
<blockquote>
<p>å…ˆå‰çš„å·¥ä½œä½¿ç”¨Masked Autoencodersï¼ˆMAEsï¼‰é€šå¸¸åŸºäºå›¾åƒåœ¨ä¸åŒé€šé“ä¹‹é—´å­˜åœ¨å¤§é‡å†—ä½™çš„å‡è®¾ï¼Œåˆ©ç”¨è·¨é€šé“ç›¸å…³æ€§é‡å»ºæ©ç å†…å®¹ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾åœ¨å¤šé€šé“æˆåƒï¼ˆMCIï¼‰ä¸­å¹¶ä¸æˆç«‹ï¼Œå› ä¸ºå„é€šé“å¯èƒ½æä¾›äº’è¡¥ä¿¡æ¯ï¼Œç‰¹å¾é‡å è¾ƒå°ã€‚å› æ­¤ï¼Œè¿™äº›MAEsä¸»è¦ä»è¡¥ä¸é‡å»ºä¸­å­¦ä¹ å•ä¸ªé€šé“å†…çš„å±€éƒ¨ç»“æ„ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è·¨é€šé“äº¤äº’ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨MCIä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMAEçš„æ–¹æ³•ChA-MAEViTï¼Œé€šè¿‡ä»¥ä¸‹å››ä¸ªå…³é”®ç­–ç•¥å¢å¼ºMCIé€šé“ä¹‹é—´çš„ç‰¹å¾å­¦ä¹ ï¼šï¼ˆ1ï¼‰åŠ¨æ€é€šé“è¡¥ä¸æ©è”½ï¼Œè¿«ä½¿æ¨¡å‹é™¤äº†æ©è”½çš„è¡¥ä¸å¤–è¿˜é‡å»ºç¼ºå¤±çš„é€šé“ï¼Œä»è€Œå¢å¼ºè·¨é€šé“ä¾èµ–æ€§å¹¶æé«˜å¯¹ä¸åŒé€šé“é…ç½®çš„é²æ£’æ€§ï¼›ï¼ˆ2ï¼‰å†…å­˜ä»¤ç‰Œå……å½“é•¿æœŸè®°å¿†è¾…åŠ©å·¥å…·ï¼Œä¿ƒè¿›è·¨é€šé“ä¿¡æ¯å…±äº«ï¼Œè§£å†³é‡å»ºç»“æ„å¤šæ ·é€šé“çš„æŒ‘æˆ˜ï¼›ï¼ˆ3ï¼‰æ··åˆä»¤ç‰Œèåˆæ¨¡å—ï¼Œåˆå¹¶ç»†ç²’åº¦è¡¥ä¸ä»¤ç‰Œä¸å…¨å±€ç±»åˆ«ä»¤ç‰Œä»¥æ•è·æ›´ä¸°å¯Œçš„è¡¨ç¤ºï¼›ï¼ˆ4ï¼‰é€šé“æ„ŸçŸ¥è§£ç å™¨æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨ï¼Œåˆ©ç”¨é€šé“ä»¤ç‰Œæœ‰æ•ˆåœ°é‡å»ºå›¾åƒè¡¥ä¸ã€‚åœ¨å«æ˜Ÿå’Œæ˜¾å¾®é•œæ•°æ®é›†CHAMMIã€JUMP-CPå’ŒSo2Satä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒChA-MAEViTæ˜¾è‘—ä¼˜äºæœ€æ–°çš„MCI-ViTsï¼Œæ€§èƒ½æå‡3.0-21.5%ï¼Œçªæ˜¾äº†MCIä¸­è·¨é€šé“äº¤äº’çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19331v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è·¨é€šé“æˆåƒï¼ˆMCIï¼‰ä¸­ï¼ŒåŸºäºMasked Autoencoderï¼ˆMAEï¼‰çš„æ–¹æ³•åœ¨ç‰¹å¾å­¦ä¹ ä¸Šçš„å±€é™æ€§ã€‚ç”±äºä¼ ç»ŸMAEä¾èµ–äºè·¨é€šé“å†—ä½™çš„å‡è®¾ï¼Œå› æ­¤åœ¨MCIä¸­è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ChA-MAEViTæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é€šé“å—æ©è”½ã€è®°å¿†ä»¤ç‰Œã€æ··åˆä»¤ç‰Œèåˆæ¨¡å—å’Œé€šé“æ„ŸçŸ¥è§£ç å™¨å››ä¸ªå…³é”®ç­–ç•¥ï¼Œå¢å¼ºè·¨MCIé€šé“çš„ç‰¹å¾å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒChA-MAEViTåœ¨å«æ˜Ÿå’Œæ˜¾å¾®é•œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰MCI-ViTsï¼Œè¯æ˜äº†è·¨é€šé“äº¤äº’åœ¨MCIä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»ŸMAEæ–¹æ³•åŸºäºå›¾åƒè·¨é€šé“å†—ä½™çš„å‡è®¾ï¼Œåœ¨MCIä¸­è¡¨ç°æœ‰é™ã€‚</li>
<li>ChA-MAEViTé€šè¿‡åŠ¨æ€é€šé“å—æ©è”½å¢å¼ºè·¨é€šé“ä¾èµ–æ€§ï¼Œæé«˜å¯¹ä¸åŒé€šé“é…ç½®çš„é²æ£’æ€§ã€‚</li>
<li>è®°å¿†ä»¤ç‰Œä¿ƒè¿›è·¨é€šé“ä¿¡æ¯å…±äº«ï¼Œè§£å†³ç»“æ„å¤šæ ·é€šé“é‡å»ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æ··åˆä»¤ç‰Œèåˆæ¨¡å—ç»“åˆç²¾ç»†å—ä»¤ç‰Œå’Œå…¨å±€ç±»åˆ«ä»¤ç‰Œï¼Œæ•æ‰æ›´ä¸°å¯Œè¡¨ç¤ºã€‚</li>
<li>é€šé“æ„ŸçŸ¥è§£ç å™¨æœ‰æ•ˆåˆ©ç”¨é€šé“ä»¤ç‰Œè¿›è¡Œå›¾åƒå—é‡å»ºã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºChA-MAEViTåœ¨å«æ˜Ÿå’Œæ˜¾å¾®é•œæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰MCI-ViTsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbea55c4473727833bacc8f3259d6e74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c354371a2ba22c9d1ea696e9793df14b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19512e65411b2396dc8b1c429caa1a7e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Face-Spoofing-Detection-using-Deep-Learning"><a href="#Face-Spoofing-Detection-using-Deep-Learning" class="headerlink" title="Face Spoofing Detection using Deep Learning"></a>Face Spoofing Detection using Deep Learning</h2><p><strong>Authors: Najeebullah, Maaz Salman, Zar Nawab Khan Swati</strong></p>
<p>Digital image spoofing has emerged as a significant security threat in biometric authentication systems, particularly those relying on facial recognition. This study evaluates the performance of three vision based models, MobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in image classification, utilizing a dataset of 150,986 images divided into training , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof detection is critical for enhancing the security of image recognition systems, and this research compares the models effectiveness through accuracy, precision, recall, and F1 score metrics. Results reveal that MobileNetV2 outperforms other architectures on the test dataset, achieving an accuracy of 91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared to ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation dataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17% accuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during training and superior generalization to unseen data, despite both models showing signs of overfitting. These findings highlight MobileNetV2 balanced performance and robustness, making it the preferred choice for spoof detection applications where reliability on new data is essential. The study underscores the importance of model selection in security sensitive contexts and suggests MobileNetV2 as a practical solution for real world deployment. </p>
<blockquote>
<p>æ•°å­—å›¾åƒæ¬ºéª—å·²æˆä¸ºç”Ÿç‰©ç‰¹å¾è¯†åˆ«ç³»ç»Ÿï¼ˆå°¤å…¶æ˜¯ä¾èµ–é¢éƒ¨è¯†åˆ«çš„ç³»ç»Ÿï¼‰çš„é‡è¦å®‰å…¨å¨èƒã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºè§†è§‰çš„æ¨¡å‹ï¼ˆMobileNetV2ã€ResNET50å’ŒVision Transformerï¼Œç®€ç§°ViTï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æ¬ºéª—æ£€æµ‹æ€§èƒ½ï¼Œä½¿ç”¨äº†ç”±150986å¼ å›¾åƒç»„æˆçš„æ•°æ®é›†ï¼Œåˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ140002å¼ ï¼‰ã€æµ‹è¯•é›†ï¼ˆ10984å¼ ï¼‰å’ŒéªŒè¯é›†ï¼ˆ39574å¼ ï¼‰ã€‚æ¬ºéª—æ£€æµ‹å¯¹äºæé«˜å›¾åƒè¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œæœ¬ç ”ç©¶é€šè¿‡å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰æŒ‡æ ‡æ¯”è¾ƒäº†å„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒMobileNetV2çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¶æ„ï¼Œå…¶å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º91.59%ã€91.72%ã€91.59%å’Œ91.58%ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒViTåˆ†åˆ«ä¸º86.54%ã€88.28%ã€86.54%å’Œ86.39%ã€‚åœ¨éªŒè¯æ•°æ®é›†ä¸Šï¼ŒMobileNetV2å’ŒViTè¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­MobileNetV2ä»¥97.17%çš„å‡†ç¡®æ€§ç•¥èƒœä¸€ç­¹ï¼ŒViTä¸º96.36%ã€‚å°½ç®¡ä¸¤è€…éƒ½æ˜¾ç¤ºå‡ºè¿‡æ‹Ÿåˆçš„è¿¹è±¡ï¼Œä½†MobileNetV2åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œå¯¹æ–°æ•°æ®çš„æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†MobileNetV2çš„å¹³è¡¡æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œä½¿å…¶æˆä¸ºå¯é æ€§å¯¹æ–°æ•°æ®è‡³å…³é‡è¦çš„æ¬ºéª—æ£€æµ‹åº”ç”¨çš„é¦–é€‰ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å®‰å…¨æ•æ„Ÿç¯å¢ƒä¸­æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®å°†MobileNetV2ä½œä¸ºå®é™…éƒ¨ç½²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19223v1">PDF</a> 26 pages, 9 figures,3 tables</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶è¯„ä¼°äº†MobileNetV2ã€ResNET50å’ŒVision Transformerï¼ˆViTï¼‰ä¸‰ç§è§†è§‰æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æŠ—æ¬ºéª—æ£€æµ‹æ€§èƒ½ã€‚ç ”ç©¶ä½¿ç”¨åŒ…å«150,986å¼ å›¾ç‰‡çš„æ•°æ®åº“ï¼Œç»“æœæ˜¾ç¤ºMobileNetV2åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡ä¼˜äºViTã€‚åœ¨éªŒè¯é›†ä¸Šï¼ŒMobileNetV2å’ŒViTè¡¨ç°ä¼˜ç§€ï¼Œä¸”MobileNetV2çš„å‡†ç¡®ç‡ç¨é«˜ã€‚ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©åœ¨å®‰å…¨æ•æ„Ÿç¯å¢ƒä¸­çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®åœ¨å®é™…éƒ¨ç½²ä¸­ä½¿ç”¨MobileNetV2ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§è§†è§‰æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æŠ—æ¬ºéª—æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>MobileNetV2åœ¨æµ‹è¯•é›†ä¸Šçš„å„é¡¹æŒ‡æ ‡å‡ä¼˜äºVision Transformerï¼ˆViTï¼‰ã€‚</li>
<li>åœ¨éªŒè¯é›†ä¸Šï¼ŒMobileNetV2çš„å‡†ç¡®ç‡ç•¥é«˜äºViTã€‚</li>
<li>MobileNetV2åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¶æ•›æ›´å¿«ï¼Œä¸”å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚</li>
<li>MobileNetV2çš„å¹³è¡¡æ€§èƒ½å’Œç¨³å¥æ€§ä½¿å…¶åœ¨æ¬ºéª—æ£€æµ‹åº”ç”¨ä¸­æˆä¸ºé¦–é€‰ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©åœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e35f7addefd3c109fcc75c57fdd3520d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Your-ViT-is-Secretly-an-Image-Segmentation-Model"><a href="#Your-ViT-is-Secretly-an-Image-Segmentation-Model" class="headerlink" title="Your ViT is Secretly an Image Segmentation Model"></a>Your ViT is Secretly an Image Segmentation Model</h2><p><strong>Authors:Tommie Kerssies, NiccolÃ² Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, Daan de Geus</strong></p>
<p>Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: <a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/">https://www.tue-mps.org/eomt/</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†å°†å•å°ºåº¦ViTsåº”ç”¨äºå›¾åƒåˆ†å‰²ï¼Œç°æœ‰æ–¹æ³•é‡‡ç”¨å·ç§¯é€‚é…å™¨ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ã€åƒç´ è§£ç å™¨èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥åŠä½¿ç”¨èåˆç‰¹å¾è¿›è¡Œé¢„æµ‹çš„Transformerè§£ç å™¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œç»™å®šè¶³å¤Ÿå¤§çš„æ¨¡å‹å’Œå¹¿æ³›çš„é¢„è®­ç»ƒï¼Œè¿™äº›é’ˆå¯¹ç‰¹å®šä»»åŠ¡ç»„ä»¶æ‰€å¼•å…¥çš„å½’çº³åè§ä¹Ÿå¯ä»¥ç”±ViTæœ¬èº«æ¥å­¦ä¹ ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»…ç¼–ç å™¨æ©ç Transformerï¼ˆEoMTï¼‰ï¼Œå®ƒé‡æ–°åˆ©ç”¨æ™®é€šçš„ViTæ¶æ„æ¥è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚å€ŸåŠ©å¤§è§„æ¨¡æ¨¡å‹å’Œé¢„è®­ç»ƒï¼ŒEoMTçš„åˆ†å‰²ç²¾åº¦ä¸ä½¿ç”¨ç‰¹å®šä»»åŠ¡ç»„ä»¶çš„æœ€å…ˆè¿›æ¨¡å‹ç›¸ä¼¼ã€‚åŒæ—¶ï¼Œç”±äºEoMTçš„æ¶æ„ç®€å•ï¼Œå®ƒçš„é€Ÿåº¦æ˜¾è‘—å¿«äºè¿™äº›æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨ViT-Læ—¶é€Ÿåº¦æœ€å¿«å¯è¾¾4å€ã€‚åœ¨å¤šç§æ¨¡å‹å¤§å°ä¸­ï¼ŒEoMTåœ¨åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œè¿™è¡¨æ˜è®¡ç®—èµ„æºæ›´å¥½åœ°ç”¨äºæ‰©å±•ViTæœ¬èº«ï¼Œè€Œä¸æ˜¯å¢åŠ æ¶æ„çš„å¤æ‚æ€§ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/%E3%80%82">https://www.tue-mps.org/eomt/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19108v1">PDF</a> CVPR 2025. Code: <a target="_blank" rel="noopener" href="https://www.tue-mps.org/eomt/">https://www.tue-mps.org/eomt/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Vision Transformerï¼ˆViTï¼‰åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¤§è§„æ¨¡æ¨¡å‹å’Œé¢„è®­ç»ƒï¼ŒViTæœ¬èº«å¯ä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡ç»„ä»¶å¼•å…¥çš„å½’çº³åè§ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEncoder-only Mask Transformerï¼ˆEoMTï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ™®é€šViTæ¶æ„é‡æ–°ç”¨äºå›¾åƒåˆ†å‰²ã€‚ä¸é‡‡ç”¨ç‰¹å®šä»»åŠ¡ç»„ä»¶çš„å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼ŒEoMTåœ¨å¤§å‹æ¨¡å‹å’Œé¢„è®­ç»ƒçš„æ”¯æŒä¸‹ï¼Œå¯ä»¥è·å¾—ç›¸ä¼¼çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶ä¸”ç”±äºæ¶æ„ç®€å•è€Œæ˜¾è‘—å¿«äºè¿™äº›æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒEoMTåœ¨å¤šç§æ¨¡å‹å¤§å°ä¹‹é—´å®ç°äº†åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œè¡¨æ˜è®¡ç®—èµ„æºæ›´å¥½ç”¨äºæ‰©å±•ViTæœ¬èº«ï¼Œè€Œä¸æ˜¯å¢åŠ æ¶æ„çš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨å·ç§¯é€‚é…å™¨ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾ã€åƒç´ è§£ç å™¨æ¥èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥åŠä½¿ç”¨èåˆç‰¹å¾çš„Transformerè§£ç å™¨è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æœ¬ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è¶³å¤Ÿå¤§çš„æ¨¡å‹å’Œé¢„è®­ç»ƒï¼ŒViTæœ¬èº«å¯ä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡ç»„ä»¶å¼•å…¥çš„å½’çº³åè§ã€‚</li>
<li>æå‡ºäº†Encoder-only Mask Transformer (EoMT) æ–¹æ³•ï¼Œé‡æ–°ä½¿ç”¨æ™®é€šViTæ¶æ„è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>EoMTåœ¨å¤§å‹æ¨¡å‹å’Œé¢„è®­ç»ƒçš„æ”¯æŒä¸‹ï¼Œåˆ†å‰²ç²¾åº¦ä¸å…ˆè¿›æ¨¡å‹ç›¸å½“ï¼Œä½†æ˜¾è‘—åŠ å¿«äº†é¢„æµ‹é€Ÿåº¦ã€‚</li>
<li>EoMTåœ¨å¤šç§æ¨¡å‹å¤§å°ä¹‹é—´å®ç°äº†åˆ†å‰²ç²¾åº¦å’Œé¢„æµ‹é€Ÿåº¦çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-86b2b340e2b610e4f43be47dd78084c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9cda0532167423bd1fa3aed662395fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97c99e31828579f14b77e7267fe19573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7510c5475e908fe3d7fbfcb55a851df4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhanced-OoD-Detection-through-Cross-Modal-Alignment-of-Multi-Modal-Representations"><a href="#Enhanced-OoD-Detection-through-Cross-Modal-Alignment-of-Multi-Modal-Representations" class="headerlink" title="Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal   Representations"></a>Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal   Representations</h2><p><strong>Authors:Jeonghyeon Kim, Sangheum Hwang</strong></p>
<p>Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na&quot;ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy. </p>
<blockquote>
<p>å…ˆå‰å…³äºåˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ¨¡æ€æ¨¡å‹ä¸Šã€‚æœ€è¿‘ï¼Œéšç€å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å‡ºç°ï¼Œåˆ©ç”¨è¿™ç§å¤šæ¨¡æ€è¡¨ç¤ºé€šè¿‡é›¶æ ·æœ¬å’Œæç¤ºå­¦ä¹ ç­–ç•¥çš„OoDDæ–¹æ³•å·²ç»å‡ºç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ¶‰åŠå†»ç»“é¢„è®­ç»ƒæƒé‡æˆ–ä»…å¯¹å…¶è¿›è¡Œéƒ¨åˆ†è°ƒæ•´ï¼Œè¿™å¯¹äºä¸‹æ¸¸æ•°æ®é›†å¯èƒ½æ˜¯æ¬¡ä¼˜çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒå¤šæ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰å¯ä»¥å®ç°æ˜¾è‘—çš„OoDDæ€§èƒ½ã€‚å°½ç®¡æœ€è¿‘çš„ä¸€äº›å·¥ä½œå±•ç¤ºäº†å¾®è°ƒæ–¹æ³•å¯¹OoDDçš„å½±å“ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ€§èƒ½æ”¹è¿›æ½œåŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†ç®€å•å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†çš„åŸå› ã€‚æˆ‘ä»¬çš„ç»éªŒåˆ†æè¡¨æ˜ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½æºäºå†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰åµŒå…¥ä¸­çš„æ¨¡æ€å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºè·¨æ¨¡æ€å¯¹é½çš„è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡æ­£åˆ™åŒ–IDæ•°æ®çš„å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·ç¦»æ¥å®ç°ã€‚è¿™ç§è°ƒæ•´æœ‰åŠ©äºæ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ä¿¡æ¯ï¼Œé€šè¿‡å°†ä¸åŒæ¨¡æ€ï¼ˆå³æ–‡æœ¬å’Œå›¾åƒï¼‰çš„ç›¸ä¼¼è¯­ä¹‰åœ¨è¶…çƒè¡¨ç¤ºç©ºé—´ä¸­æ›´ç´§å¯†åœ°å¯¹é½æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†æ‰€æå‡ºçš„æ­£åˆ™åŒ–å¯¹åº”äºè¶…çƒä¸ŠåŸºäºèƒ½é‡çš„æ¨¡å‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚åˆ©ç”¨ImageNet-1kçš„OoDåŸºå‡†æ•°æ®é›†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆå¦‚NegLabelï¼‰çš„åæœŸOoDDæ–¹æ³•ç›¸ç»“åˆï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„OoDDæ€§èƒ½å’Œé¢†å…ˆçš„IDå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18817v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè·¨æ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰åœ¨é¢å‘åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰ä¸­çš„ä¼˜åŠ¿ã€‚æ–‡ç« æŒ‡å‡ºï¼Œé€šè¿‡å¢å¼ºè·¨æ¨¡æ€å¯¹é½ï¼Œç¼©å°æ¨¡æ€å†…åˆ†å¸ƒï¼ˆIDï¼‰åµŒå…¥çš„è·ç¦»ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ã€‚ç»“åˆåå¤„ç†OoDDæ–¹æ³•ï¼Œå¦‚NegLabelï¼Œè¯¥æ–¹æ³•åœ¨ImageNet-1k OoDåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„OoDDæ€§èƒ½å’Œé¢†å…ˆçš„IDå‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨é¢å‘åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOoDDï¼‰ä¸­çš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ã€‚</li>
<li>è·¨æ¨¡æ€å¾®è°ƒï¼ˆMMFTï¼‰åœ¨OoDDä¸­è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†çš„åŸå› å¯èƒ½æºäºæ¨¡æ€é—´å·®è·å¯¼è‡´çš„IDåµŒå…¥é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¢å¼ºè·¨æ¨¡æ€å¯¹é½çš„è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡æ­£åˆ™åŒ–å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„è·ç¦»æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰ä¸­çš„ç›¸ä¼¼è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆåå¤„ç†OoDDæ–¹æ³•ï¼ˆå¦‚NegLabelï¼‰ï¼Œåœ¨ImageNet-1k OoDåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-170260ba1692fa1e580abd6ad568443c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aa140002da7e8ba56c276cee86c9ace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04298f6fa204f5b1920d6ec5288966a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SKDU-at-De-Factify-4-0-Vision-Transformer-with-Data-Augmentation-for-AI-Generated-Image-Detection"><a href="#SKDU-at-De-Factify-4-0-Vision-Transformer-with-Data-Augmentation-for-AI-Generated-Image-Detection" class="headerlink" title="SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for   AI-Generated Image Detection"></a>SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for   AI-Generated Image Detection</h2><p><strong>Authors:Shrikant Malviya, Neelanjan Bhowmik, Stamos Katsigiannis</strong></p>
<p>The aim of this work is to explore the potential of pre-trained vision-language models, e.g. Vision Transformers (ViT), enhanced with advanced data augmentation strategies for the detection of AI-generated images. Our approach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset, which includes images generated by state-of-the-art models such as Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and MidJourney. We employ perturbation techniques like flipping, rotation, Gaussian noise injection, and JPEG compression during training to improve model robustness and generalisation. The experimental results demonstrate that our ViT-based pipeline achieves state-of-the-art performance, significantly outperforming competing methods on both validation and test datasets. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨æ¢ç´¢é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œä¾‹å¦‚é‡‡ç”¨å…ˆè¿›æ•°æ®å¢å¼ºç­–ç•¥çš„Vision Transformersï¼ˆViTï¼‰ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åœ¨Defactify-4.0æ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ViTæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±æœ€å‰æ²¿æ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œå¦‚Stable Diffusion 2.1ã€Stable Diffusion XLã€Stable Diffusion 3ã€DALL-E 3å’ŒMidJourneyã€‚æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨æ‰°åŠ¨æŠ€æœ¯ï¼Œå¦‚ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åŸºäºViTçš„ç®¡é“å®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç«äº‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18812v1">PDF</a> De-Factify 4.0 workshop at the 39th Annual AAAI Conference on   Artificial Intelligence (AAAI 2025)</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚è§†ç•Œè½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œç»“åˆå…ˆè¿›çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯ç”¨äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚æœ¬ç ”ç©¶é‡‡ç”¨åœ¨Defactify-4.0æ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ViTæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±Stable Diffusion 2.1ã€Stable Diffusion XLã€Stable Diffusion 3ã€DALL-E 3å’ŒMidJourneyç­‰å…ˆè¿›æŠ€æœ¯ç”Ÿæˆçš„å›¾åƒã€‚é€šè¿‡ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ç­‰æ‰°åŠ¨æŠ€æœ¯æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºViTçš„ç®¡é“è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Vision Transformers, ViTï¼‰åœ¨æ£€æµ‹AIç”Ÿæˆå›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†Defactify-4.0æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±å¤šç§å…ˆè¿›æŠ€æœ¯ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¦‚ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ç­‰ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒéªŒè¯äº†åŸºäºViTçš„ç®¡é“åœ¨æ£€æµ‹AIç”Ÿæˆå›¾åƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ£€æµ‹æ–¹æ³•å’Œæ¨¡å‹ã€‚</li>
<li>æ­¤é¡¹ç ”ç©¶å±•ç¤ºäº†ç»“åˆå…ˆè¿›æ•°æ®å¢å¼ºç­–ç•¥çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸçš„å¼ºå¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b7da089d458f7765eedc93d9a9134a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0ba8b816af1da84b786f18bd158d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e532019f587a00554c84bb49e23123e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e996881c98831bff97fcc740073fbbcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9556c486bff7e77f49cb364683070cb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PathoHR-Breast-Cancer-Survival-Prediction-on-High-Resolution-Pathological-Images"><a href="#PathoHR-Breast-Cancer-Survival-Prediction-on-High-Resolution-Pathological-Images" class="headerlink" title="PathoHR: Breast Cancer Survival Prediction on High-Resolution   Pathological Images"></a>PathoHR: Breast Cancer Survival Prediction on High-Resolution   Pathological Images</h2><p><strong>Authors:Yang Luo, Shiru Wang, Jun Liu, Jiaxuan Xiao, Rundong Xue, Zeyu Zhang, Hao Zhang, Yu Lu, Yang Zhao, Yutong Xie</strong></p>
<p>Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumorâ€™s aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/PathoHR">https://github.com/AIGeeksGroup/PathoHR</a>. </p>
<blockquote>
<p>ä¹³è…ºç™Œåœ¨ç—…ç†è®¡ç®—ä¸­çš„å­˜æ´»é¢„æµ‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè‚¿ç˜¤çš„å¼‚è´¨æ€§ã€‚ä¾‹å¦‚ï¼Œç—…ç†å›¾åƒä¸­çš„åŒä¸€è‚¿ç˜¤çš„ä¸åŒåŒºåŸŸå¯èƒ½ä¼šæ˜¾ç¤ºå‡ºä¸åŒçš„å½¢æ€å’Œåˆ†å­ç‰¹å¾ã€‚è¿™ä½¿å¾—ä»å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­æå–çœŸæ­£åæ˜ è‚¿ç˜¤ä¾µè¢­æ½œåŠ›å’Œå¯èƒ½ç”Ÿå­˜ç»“æœçš„ä»£è¡¨æ€§ç‰¹å¾å˜å¾—å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PathoHRï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‡†ç¡®é¢„æµ‹ä¹³è…ºç™Œå­˜æ´»çš„æ–°å‹ç®¡é“ï¼Œå®ƒå¯ä»¥å¢å¼ºä»»ä½•å¤§å°çš„ç—…ç†å›¾åƒï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰èå…¥å³æ’å³ç”¨çš„é«˜åˆ†è¾¨ç‡Vision Transformerï¼ˆViTï¼‰ï¼Œä»¥å¢å¼ºé€å—çš„WSIè¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´è¯¦ç»†å’Œå…¨é¢çš„ç‰¹å¾æå–ï¼›ï¼ˆ2ï¼‰ç³»ç»Ÿè¯„ä¼°å¤šç§å…ˆè¿›çš„ç›¸ä¼¼æ€§åº¦é‡æŒ‡æ ‡ï¼Œä»¥æ¯”è¾ƒWSIæå–çš„ç‰¹å¾ï¼Œä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è‚¿ç˜¤ç‰¹å¾ï¼›ï¼ˆ3ï¼‰è¯æ˜é€šè¿‡éµå¾ªæ‰€æå‡ºç®¡é“å¢å¼ºçš„è¾ƒå°å›¾åƒè¡¥ä¸å¯ä»¥è¾¾åˆ°ä¸åŸå§‹è¾ƒå¤§è¡¥ä¸ç›¸å½“çš„é¢„æµ‹ç²¾åº¦ï¼Œç”šè‡³æ›´é«˜ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPathoHRé€šè¿‡å°†å¢å¼ºçš„å›¾åƒåˆ†è¾¨ç‡ä¸ä¼˜åŒ–åçš„ç‰¹å¾å­¦ä¹ ç›¸ç»“åˆï¼Œä¸ºè®¡ç®—ç—…ç†å­¦æä¾›äº†æ½œåœ¨çš„æ–¹æ³•ï¼Œä¸ºæ›´å‡†ç¡®ã€æ›´æœ‰æ•ˆçš„ä¹³è…ºç™Œå­˜æ´»é¢„æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/PathoHR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/PathoHRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17970v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹åœ¨ç—…ç†å­¦è®¡ç®—ä¸­æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè‚¿ç˜¤å­˜åœ¨å¼‚è´¨æ€§ã€‚åŒä¸€è‚¿ç˜¤çš„ä¸åŒåŒºåŸŸåœ¨ç—…ç†å›¾åƒä¸­å¯èƒ½è¡¨ç°å‡ºä¸åŒçš„å½¢æ€å­¦å’Œåˆ†å­ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºPathoHRï¼Œä¸€ç§ç”¨äºç²¾ç¡®é¢„æµ‹ä¹³è…ºç™Œç”Ÿå­˜çš„æ–°é¢–æµç¨‹ï¼Œå¯æé«˜å„ç§å°ºå¯¸çš„ç—…ç†å›¾åƒä»¥å¢å¼ºç‰¹å¾å­¦ä¹ æ•ˆæœã€‚PathoHRåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ–¹é¢ï¼šï¼ˆ1ï¼‰é‡‡ç”¨å¯æ’å…¥å¼é«˜åˆ†è¾¨ç‡Vision Transformerï¼ˆViTï¼‰å¢å¼ºå›¾åƒè¡¥ä¸è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰ç³»ç»Ÿè¯„ä¼°å¤šç§é«˜çº§ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ï¼Œä»¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼›ï¼ˆ3ï¼‰æ¼”ç¤ºè¾ƒå°çš„å›¾åƒè¡¥ä¸åœ¨éµå¾ªè¯¥æµç¨‹åèƒ½è¾¾åˆ°ç­‰æ•ˆæˆ–æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚æ­¤æ–¹æ¡ˆå±•ç°å‡ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œæœ‰æœ›é€šè¿‡æ•´åˆå¢å¼ºå›¾åƒåˆ†è¾¨ç‡å’Œä¼˜åŒ–ç‰¹å¾å­¦ä¹ æ¥æé«˜ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬ä¸­çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>ä¹³è…ºç™Œç”Ÿå­˜é¢„æµ‹åœ¨ç—…ç†å­¦è®¡ç®—ä¸­æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè‚¿ç˜¤å­˜åœ¨å¼‚è´¨æ€§ã€‚</li>
<li>PathoHRæ˜¯ä¸€ç§ç”¨äºç²¾ç¡®é¢„æµ‹ä¹³è…ºç™Œç”Ÿå­˜çš„æ–°é¢–æµç¨‹ã€‚</li>
<li>PathoHRé€šè¿‡é‡‡ç”¨é«˜åˆ†è¾¨ç‡Vision Transformerï¼ˆViTï¼‰å¢å¼ºå›¾åƒè¡¥ä¸è¡¨ç¤ºæ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>PathoHRç³»ç»Ÿè¯„ä¼°å¤šç§ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ä»¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>è¾ƒå°å›¾åƒè¡¥ä¸åœ¨éµå¾ªPathoHRæµç¨‹åå¯ä»¥è¾¾åˆ°ç­‰æ•ˆæˆ–æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨PathoHRæµç¨‹å¯æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4aef8202ff9ee47de44638c74173e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dba6759144e17df99c3c7a6c83a695e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e77e063677b3a553fd52e86ca30210a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Variational-Autoencoder-Prompt-Tuning"><a href="#Visual-Variational-Autoencoder-Prompt-Tuning" class="headerlink" title="Visual Variational Autoencoder Prompt Tuning"></a>Visual Variational Autoencoder Prompt Tuning</h2><p><strong>Authors:Xi Xiao, Yunbei Zhang, Yanshuh Li, Xingjian Li, Tianyang Wang, Jihun Hamm, Xiao Wang, Min Xu</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for adapting large vision transformers to downstream tasks without the prohibitive computational costs of full fine-tuning. While existing visual prompt tuning (VPT) methods have made significant strides, they predominantly rely on static, domain-specific prompts that fail to capture the rich visual diversity within individual instances. This paper introduces V$^2$APT (Visual Variational Autoencoder Prompt Tuning), a novel framework that generates dynamic, input-dependent prompts using a variational autoencoder architecture. By learning a latent representation of image-specific features and decoding them into customized prompts, V$^2$APT adapts to the unique visual characteristics of each input. Extensive experiments on FGVC, HTA, and VTAB-1k benchmarks demonstrate that our approach consistently outperforms state-of-the-art PEFT methods. Notably, V$^2$APT achieves +3.2% improvement over VPT-Deep on HTA, with an average performance gain of +2.0% across all three datasets. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å·²ç»æˆä¸ºä¸€ç§å…³é”®æ–¹æ³•ï¼Œå¯åœ¨ä¸äº§ç”Ÿå…¨é‡å¾®è°ƒå¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œä½¿å¤§å‹è§†è§‰å˜å‹å™¨é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚å°½ç®¡ç°æœ‰çš„è§†è§‰æç¤ºè°ƒæ•´ï¼ˆVPTï¼‰æ–¹æ³•å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å®ƒä»¬ä¸»è¦ä¾èµ–äºé™æ€çš„ç‰¹å®šé¢†åŸŸæç¤ºï¼Œæ— æ³•æ•è·å•ä¸ªå®ä¾‹å†…éƒ¨çš„ä¸°å¯Œè§†è§‰å¤šæ ·æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†V$^2$APTï¼ˆè§†è§‰å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æç¤ºè°ƒæ•´ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒä½¿ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¶æ„ç”ŸæˆåŠ¨æ€ã€è¾“å…¥ç›¸å…³çš„æç¤ºã€‚é€šè¿‡å­¦ä¹ å›¾åƒç‰¹å®šç‰¹å¾çš„æ½œåœ¨è¡¨ç¤ºå¹¶å°†å…¶è§£ç ä¸ºè‡ªå®šä¹‰æç¤ºï¼ŒV$^2$APTå¯ä»¥é€‚åº”æ¯ä¸ªè¾“å…¥çš„ç‹¬ç‰¹è§†è§‰ç‰¹å¾ã€‚åœ¨FGVCã€HTAå’ŒVTAB-1kåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€æ–°çš„PEFTæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒV$^2$APTåœ¨HTAä¸Šç›¸å¯¹äºVPT-Deepæé«˜äº†+3.2%ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­å¹³å‡æ€§èƒ½æå‡+2.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17650v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯¥è®ºæ–‡ä»‹ç»äº†V$^2$APTï¼ˆè§†è§‰å˜åˆ†è‡ªç¼–ç å™¨æç¤ºè°ƒæ•´ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ç”ŸæˆåŠ¨æ€ã€ä¾èµ–äºè¾“å…¥çš„æç¤ºçš„æ–°å‹æ–¹æ³•ã€‚é€šè¿‡å­¦ä¹ ä¸å›¾åƒç‰¹å®šç‰¹å¾ç›¸å…³çš„æ½œåœ¨è¡¨ç¤ºå¹¶å°†å…¶è§£ç ä¸ºè‡ªå®šä¹‰æç¤ºï¼ŒV$^2$APTèƒ½å¤Ÿè‡ªé€‚åº”äºæ¯ä¸ªè¾“å…¥çš„ç‹¬ç‰¹è§†è§‰ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FGVCã€HTAå’ŒVTAB-1kåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸€è‡´ä¼˜äºæœ€æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨HTAä¸Šï¼Œä¸VPT-Deepç›¸æ¯”ï¼ŒV$^2$APTå®ç°äº†+3.2%çš„æ”¹è¿›ï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†+2.0%ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>V$^2$APTåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ç”ŸæˆåŠ¨æ€ã€è¾“å…¥ä¾èµ–çš„æç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ å’Œè§£ç å›¾åƒç‰¹å®šç‰¹å¾çš„æ½œåœ¨è¡¨ç¤ºæ¥ç”Ÿæˆè‡ªå®šä¹‰æç¤ºã€‚</li>
<li>V$^2$APTèƒ½é€‚åº”æ¯ä¸ªè¾“å…¥çš„ç‹¬ç‰¹è§†è§‰ç‰¹å¾ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒV$^2$APTè¡¨ç°ä¼˜äºç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚</li>
<li>åœ¨HTAåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸VPT-Deepç›¸æ¯”ï¼ŒV$^2$APTå®ç°äº†æ˜¾è‘—çš„+3.2%çš„æ€§èƒ½æå‡ã€‚</li>
<li>V$^2$APTåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†+2.0%ã€‚</li>
<li>V$^2$APTæ¡†æ¶ä¸ºè§†è§‰ä»»åŠ¡ä¸­çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæä¾›äº†ä¸€ç§æ–°çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3fba0469e5ad92128a0cdd44963d162.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be0a5346ef60084512c866650e34c07f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb5bc1e58dcfdd3f43f6d1351714078c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4895143ee87eb892c005430185f8f15.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Vision-Transformer-Based-Semantic-Communications-for-Next-Generation-Wireless-Networks"><a href="#Vision-Transformer-Based-Semantic-Communications-for-Next-Generation-Wireless-Networks" class="headerlink" title="Vision Transformer Based Semantic Communications for Next Generation   Wireless Networks"></a>Vision Transformer Based Semantic Communications for Next Generation   Wireless Networks</h2><p><strong>Authors:Muhammad Ahmed Mohsin, Muhammad Jazib, Zeeshan Alam, Muhmmad Farhan Khan, Muhammad Saad, Muhammad Ali Jamshed</strong></p>
<p>In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications. </p>
<blockquote>
<p>åœ¨6Gç½‘ç»œä¸æ–­æ¼”å˜çš„èƒŒæ™¯ä¸‹ï¼Œè¯­ä¹‰é€šä¿¡å°†é€šè¿‡ä¼˜å…ˆä¼ è¾“è¯­ä¹‰æ„ä¹‰è€Œä¸æ˜¯åŸå§‹æ•°æ®å‡†ç¡®æ€§æ¥é©å‘½æ€§åœ°æ”¹å˜æ•°æ®ä¼ è¾“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºVision Transformerï¼ˆViTï¼‰çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œæ—¨åœ¨å®ç°å›¾åƒä¼ è¾“è¿‡ç¨‹ä¸­çš„é«˜è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹å¸¦å®½çš„éœ€æ±‚ã€‚é€šè¿‡å°†ViTä½œä¸ºç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œæ‰€æå‡ºçš„æ¶æ„èƒ½å¤Ÿç†Ÿç»ƒåœ°å°†åœ¨å‘å°„æœºç«¯çš„å›¾åƒç¼–ç æˆé«˜è¯­ä¹‰å†…å®¹ï¼Œå¹¶åœ¨è€ƒè™‘åˆ°æ¥æ”¶ç«¯çš„ç°å®ä¸–ç•Œè¡°è½å’Œå™ªå£°å› ç´ æ—¶ç²¾ç¡®åœ°é‡å»ºå›¾åƒã€‚åŸºäºViTå›ºæœ‰çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç”¨äºç”Ÿæˆæ­¤ç±»å›¾åƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚åŸºäºæ‰€æå‡ºçš„ViTç½‘ç»œçš„æ¶æ„å®ç°äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º38åˆ†è´ï¼Œè¿™åœ¨ç»´æŒä¸åŒé€šä¿¡ç¯å¢ƒä¸­çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ–¹é¢é«˜äºå…¶ä»–æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•ã€‚è¿™äº›å‘ç°è¯æ˜äº†æˆ‘ä»¬åŸºäºViTçš„æ–¹æ³•æ˜¯è¯­ä¹‰é€šä¿¡ä¸­çš„é‡å¤§çªç ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17275v1">PDF</a> Accepted @ ICC 2025</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£ç½‘ç»œå‘å±•è¶‹åŠ¿ä¸‹ï¼Œè¯­ä¹‰é€šä¿¡æ­£åœ¨æ”¹å˜æ•°æ®ä¼ è¾“æ–¹å¼ï¼Œæ›´ä¾§é‡äºè¯­ä¹‰å«ä¹‰çš„ä¼ è¾“è€ŒéåŸå§‹æ•°æ®çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œåœ¨å›¾åƒä¼ è¾“æ—¶è¾¾æˆé«˜è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹å¸¦å®½çš„éœ€æ±‚ã€‚è¿ç”¨ViTä½œä¸ºç¼–ç è§£ç æ¡†æ¶ï¼Œå¯åœ¨å‘é€ç«¯æœ‰æ•ˆç¼–ç å›¾åƒä¸ºé«˜åº¦è¯­ä¹‰å†…å®¹ï¼Œå¹¶åœ¨è€ƒè™‘ç°å®ä¸–ç•Œçš„è¡°è½å’Œå™ªå£°æƒ…å†µä¸‹äºæ¥æ”¶ç«¯ç²¾ç¡®é‡å»ºå›¾åƒã€‚åŸºäºViTçš„æ¨¡å‹è¡¨ç°ä¼˜äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œå…¶å›¾åƒç”Ÿæˆæ¶æ„è¾¾åˆ°å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰38åˆ†è´ï¼Œè¾ƒå…¶ä»–æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•æ›´èƒ½ç»´æŒä¸åŒé€šä¿¡ç¯å¢ƒä¸­çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æ­¤ç ”ç©¶ç¡®ç«‹ViTåœ¨è¯­ä¹‰é€šä¿¡ä¸­çš„çªç ´æ€§åœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰é€šä¿¡åœ¨æ–°ä¸€ä»£ç½‘ç»œä¸­è¶‹åŠ¿æ˜¾è‘—ï¼Œæ³¨é‡è¯­ä¹‰ä¼ è¾“è€Œéæ•°æ®å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºåŸºäºVision Transformerï¼ˆViTï¼‰çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œå®ç°å›¾åƒä¼ è¾“çš„é«˜è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚</li>
<li>ViTä½œä¸ºç¼–ç è§£ç æ¡†æ¶èƒ½æœ‰æ•ˆå¤„ç†å‘é€ç«¯çš„å›¾åƒç¼–ç åŠæ¥æ”¶ç«¯çš„å›¾åƒé‡å»ºã€‚</li>
<li>æ­¤æ¶æ„è€ƒè™‘äº†ç°å®ä¸–ç•Œçš„é€šä¿¡ç¯å¢ƒï¼Œå¦‚è¡°è½å’Œå™ªå£°ã€‚</li>
<li>åŸºäºViTçš„æ¨¡å‹è¡¨ç°ä¼˜äºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¦‚CNNså’ŒGANsã€‚</li>
<li>ViTæ¨¡å‹çš„å›¾åƒç”Ÿæˆæ¶æ„è¾¾åˆ°PSNR 38åˆ†è´ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç¡®ç«‹äº†ViTåœ¨è¯­ä¹‰é€šä¿¡é¢†åŸŸçš„çªç ´æ€§åœ°ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0df534ae5b470e64576af7600b755f3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbbb7e60332633c34c364b801488e4c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21209048b7e9606b9b6a28830f1830e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d270606b6b6e3e9d05c2c1607efd1556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffe64521aff8c50ce552d0e908684eb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29e567987f1d7b2f1df1e554f4da7309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b7f2666c41efb12f7db35f8ea712403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b93f024f9013a40f1e6a437c1faf0579.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Slide-Level-Prompt-Learning-with-Vision-Language-Models-for-Few-Shot-Multiple-Instance-Learning-in-Histopathology"><a href="#Slide-Level-Prompt-Learning-with-Vision-Language-Models-for-Few-Shot-Multiple-Instance-Learning-in-Histopathology" class="headerlink" title="Slide-Level Prompt Learning with Vision Language Models for Few-Shot   Multiple Instance Learning in Histopathology"></a>Slide-Level Prompt Learning with Vision Language Models for Few-Shot   Multiple Instance Learning in Histopathology</h2><p><strong>Authors:Devavrat Tomar, Guillaume Vray, Dwarikanath Mahapatra, Sudipta Roy, Jean-Philippe Thiran, Behzad Bozorgtabar</strong></p>
<p>In this paper, we address the challenge of few-shot classification in histopathology whole slide images (WSIs) by utilizing foundational vision-language models (VLMs) and slide-level prompt learning. Given the gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods rely on aggregation functions to derive slide-level (bag-level) predictions from patch representations, which require extensive bag-level labels for training. In contrast, VLM-based approaches excel at aligning visual embeddings of patches with candidate class text prompts but lack essential pathological prior knowledge. Our method distinguishes itself by utilizing pathological prior knowledge from language models to identify crucial local tissue types (patches) for WSI classification, integrating this within a VLM-based MIL framework. Our approach effectively aligns patch images with tissue types, and we fine-tune our model via prompt learning using only a few labeled WSIs per category. Experimentation on real-world pathological WSI datasets and ablation studies highlight our methodâ€™s superior performance over existing MIL- and VLM-based methods in few-shot WSI classification tasks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/LTS5/SLIP">https://github.com/LTS5/SLIP</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¹»ç¯ç‰‡çº§æç¤ºå­¦ä¹ æ¥è§£å†³ç—…ç†å­¦ä¸­å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰å°æ ·æœ¬åˆ†ç±»çš„æŒ‘æˆ˜ã€‚é‰´äºå…¨å¹»ç¯ç‰‡å›¾åƒè¾¾åˆ°åƒå…†åƒç´ è§„æ¨¡ï¼Œä¼ ç»Ÿçš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ä¾èµ–äºèšåˆå‡½æ•°ä»è¡¥ä¸è¡¨ç¤ºæ´¾ç”Ÿå‡ºå¹»ç¯ç‰‡çº§ï¼ˆåŒ…çº§ï¼‰é¢„æµ‹ï¼Œè¿™éœ€è¦å¤§é‡çš„åŒ…çº§æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºVLMçš„æ–¹æ³•æ“…é•¿å°†è¡¥ä¸çš„è§†è§‰åµŒå…¥ä¸å€™é€‰ç±»åˆ«æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹é½ï¼Œä½†å´ç¼ºä¹å…³é”®çš„ç—…ç†å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†è¯­è¨€æ¨¡å‹çš„ç—…ç†å…ˆéªŒçŸ¥è¯†ç”¨äºè¯†åˆ«å…³é”®å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ï¼Œåœ¨åŸºäºVLMçš„MILæ¡†æ¶å†…å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒè¿›è¡Œåˆ†ç±»æ¥åŒºåˆ†è‡ªå·±ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†è¡¥ä¸å›¾åƒä¸ç»„ç»‡ç±»å‹å¯¹é½ï¼Œå¹¶ä¸”æˆ‘ä»¬åªé€šè¿‡æ¯ç±»åˆ«å°‘é‡çš„æœ‰æ ‡ç­¾WSIså¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨çœŸå®ä¸–ç•Œçš„ç—…ç†å…¨å¹»ç¯ç‰‡å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒå’Œæ¶ˆèç ”ç©¶çªæ˜¾äº†æˆ‘ä»¬åœ¨å°æ ·æœ¬å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ç›¸å¯¹äºç°æœ‰çš„MILå’ŒVLMæ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/LTS5/SLIP%E3%80%82">https://github.com/LTS5/SLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17238v1">PDF</a> Accepted to ISBI 2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³ç—…ç†å­¦å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»æŒ‘æˆ˜ã€‚é’ˆå¯¹WSIçš„å¤§è§„æ¨¡åƒç´ é—®é¢˜ï¼Œä¼ ç»Ÿå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ä¾èµ–äºèšåˆå‡½æ•°ä»è¡¥ä¸è¡¨ç¤ºä¸­å¾—å‡ºå¹»ç¯ç‰‡çº§åˆ«çš„é¢„æµ‹ï¼Œè¿™éœ€è¦å¤§é‡çš„åŒ…çº§åˆ«æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒVLMæ–¹æ³•æ“…é•¿å°†è¡¥ä¸çš„è§†è§‰åµŒå…¥ä¸å€™é€‰ç±»åˆ«æ–‡æœ¬æç¤ºå¯¹é½ï¼Œä½†ç¼ºä¹å…³é”®çš„ç—…ç†å­¦å…ˆéªŒçŸ¥è¯†ã€‚æœ¬æ–‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ç—…ç†å­¦å…ˆéªŒçŸ¥è¯†ï¼Œè¯†åˆ«å¯¹WSIåˆ†ç±»è‡³å…³é‡è¦çš„å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ï¼Œå¹¶å°†å…¶é›†æˆåˆ°åŸºäºVLMçš„MILæ¡†æ¶ä¸­ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆåœ°å°†è¡¥ä¸å›¾åƒä¸ç»„ç»‡ç±»å‹å¯¹é½ï¼Œå¹¶é€šè¿‡ä»…ä½¿ç”¨æ¯ä¸ªç±»åˆ«çš„ä¸€äº›æ ‡è®°WSIè¿›è¡Œæç¤ºå­¦ä¹ æ¥å¾®è°ƒæ¨¡å‹ã€‚åœ¨çœŸå®ç—…ç†WSIæ•°æ®é›†ä¸Šçš„å®éªŒå’Œæ¶ˆèç ”ç©¶çªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬WSIåˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„MILå’ŒVLMæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡åƒç´ çš„WSIæ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚</li>
<li>VLMæ–¹æ³•è™½ç„¶æ“…é•¿å¯¹é½è¡¥ä¸çš„è§†è§‰åµŒå…¥ä¸ç±»åˆ«æ–‡æœ¬æç¤ºï¼Œä½†ç¼ºä¹ç—…ç†å­¦å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•ç»“åˆç—…ç†å­¦å…ˆéªŒçŸ¥è¯†ï¼Œè¯†åˆ«å¯¹WSIåˆ†ç±»è‡³å…³é‡è¦çš„å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ã€‚</li>
<li>æ–¹æ³•å°†è¡¥ä¸å›¾åƒä¸ç»„ç»‡ç±»å‹æœ‰æ•ˆå¯¹é½ï¼Œå¹¶é€šè¿‡å°‘é‡æ ‡è®°WSIè¿›è¡Œå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>åœ¨çœŸå®ç—…ç†WSIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬WSIåˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-038ae23b63c8e3ea3772760301971730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12522f905cb31eae6b0423bd359e1bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e34ce182618497fb68dd4f3673f12640.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6118cd028dd6333f11725858076997f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes"><a href="#Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes" class="headerlink" title="Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes"></a>Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes</h2><p><strong>Authors:Davide Antonio Mura, Michela Pinna, Lorenzo Putzu, Andrea Loddo, Alessandra Perniciano, Olga Mulas, Cecilia Di Ruberto</strong></p>
<p>The detection of blood disorders often hinges upon the quantification of specific blood cell types. Variations in cell counts may indicate the presence of pathological conditions. Thus, the significance of developing precise automatic systems for blood cell enumeration is underscored. The investigation focuses on a novel approach termed DE-ViT. This methodology is employed in a Few-Shot paradigm, wherein training relies on a limited number of images. Two distinct datasets are utilised for experimental purposes: the Raabin-WBC dataset for Leukocyte detection and a local dataset for Schistocyte identification. In addition to the DE-ViT model, two baseline models, Faster R-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being compared against those of the proposed model. While DE-ViT has demonstrated state-of-the-art performance on the COCO and LVIS datasets, both baseline models surpassed its performance on the Raabin-WBC dataset. Moreover, only Faster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed disparities in performance may possibly be attributed to domain shift phenomena. </p>
<blockquote>
<p>è¡€æ¶²ç–¾ç—…çš„æ£€æµ‹å¾€å¾€å–å†³äºç‰¹å®šè¡€ç»†èƒçš„é‡åŒ–ã€‚ç»†èƒè®¡æ•°çš„å˜åŒ–å¯èƒ½è¡¨æ˜å­˜åœ¨ç—…ç†çŠ¶å†µã€‚å› æ­¤ï¼Œå¼€å‘ç²¾ç¡®è‡ªåŠ¨è¡€ç»†èƒè®¡æ•°ç³»ç»Ÿçš„æ„ä¹‰å°¤ä¸ºé‡è¦ã€‚ç ”ç©¶é‡ç‚¹æ˜¯ä¸€ç§ç§°ä¸ºDE-ViTçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å°æ ·æœ¬èŒƒå¼ï¼Œè®­ç»ƒä¾èµ–äºå°‘é‡å›¾åƒã€‚ä¸ºå®éªŒç›®çš„ä½¿ç”¨äº†ä¸¤ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†ï¼šç”¨äºç™½ç»†èƒæ£€æµ‹çš„Raabin-WBCæ•°æ®é›†å’Œç”¨äºè£‚çº¢ç»†èƒè¯†åˆ«çš„æœ¬åœ°æ•°æ®é›†ã€‚é™¤äº†DE-ViTæ¨¡å‹å¤–ï¼Œè¿˜é‡‡ç”¨äº†Faster R-CNN 50å’ŒFaster R-CNN X 101ä¸¤ä¸ªåŸºå‡†æ¨¡å‹ï¼Œå¹¶å°†å…¶ç»“æœä¸æ‰€æå‡ºæ¨¡å‹çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚è™½ç„¶DE-ViTåœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ä¸¤ä¸ªåŸºå‡†æ¨¡å‹åœ¨Raabin-WBCæ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†å®ƒã€‚æ­¤å¤–ï¼Œåªæœ‰Faster R-CNN X 101åœ¨SC-IDBä¸Šäº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚è§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®å¼‚å¯èƒ½å½’å› äºåŸŸåç§»ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17107v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç™½ç»†èƒå’Œè¡€ç»†èƒçš„è®¡æ•°å˜åŒ–å¯èƒ½æŒ‡ç¤ºç—…ç†æ€§çŠ¶å†µçš„å­˜åœ¨ï¼Œå› æ­¤å¼€å‘ç²¾ç¡®çš„è‡ªåŠ¨è¡€ç»†èƒè®¡æ•°ç³»ç»Ÿçš„æ„ä¹‰é‡å¤§ã€‚æœ¬ç ”ç©¶å…³æ³¨ä¸€ç§åä¸ºDE-ViTçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å°æ ·æœ¬è®­ç»ƒæ¨¡å¼ã€‚ç ”ç©¶ä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†ï¼šç”¨äºç™½ç»†èƒæ£€æµ‹çš„Raabin-WBCæ•°æ®é›†å’Œç”¨äºè¯†åˆ«è£‚æ®–è¡€ç»†èƒçš„æœ¬åœ°æ•°æ®é›†ã€‚é™¤äº†DE-ViTæ¨¡å‹ï¼Œè¿˜é‡‡ç”¨äº†Faster R-CNN 50å’ŒFaster R-CNN X 101ä¸¤ç§åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸DE-ViTæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚åœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šï¼ŒDE-ViTè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨Raabin-WBCæ•°æ®é›†ä¸Šï¼ŒåŸºçº¿æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚ä»…Faster R-CNN X 101åœ¨SC-IDBä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚æ€§èƒ½å·®å¼‚å¯èƒ½æ˜¯ç”±äºé¢†åŸŸæ¼‚ç§»ç°è±¡é€ æˆçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€ç»†èƒè®¡æ•°çš„å˜åŒ–å¯èƒ½æŒ‡ç¤ºç—…ç†æ€§çŠ¶å†µã€‚</li>
<li>DE-ViTæ˜¯ä¸€ç§æ–°çš„è¡€ç»†èƒæ£€æµ‹æ–¹æ³•ï¼Œé‡‡ç”¨å°æ ·æœ¬è®­ç»ƒã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†Raabin-WBCæ•°æ®é›†å’Œæœ¬åœ°æ•°æ®é›†è¿›è¡Œè¯•éªŒã€‚</li>
<li>åœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šï¼ŒDE-ViTè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨Raabin-WBCæ•°æ®é›†ä¸Šï¼ŒåŸºçº¿æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>Faster R-CNN X 101åœ¨SC-IDBä¸Šå–å¾—äº†è‰¯å¥½ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e20ca7f60177aa8ec79c9f40acf0c782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe8e2ec1f311e54b959baac9c0821624.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8172ea314037f0aa69f8bfc90f4aca7a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions"><a href="#EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions" class="headerlink" title="EEG-CLIP : Learning EEG representations from natural language   descriptions"></a>EEG-CLIP : Learning EEG representations from natural language   descriptions</h2><p><strong>Authors:Tidiane Camaret Nâ€™dir, Robin Tibor Schirrmeister</strong></p>
<p>Deep networks for electroencephalogram (EEG) decoding are currently often trained to only solve a specific task like pathology or gender decoding. A more general approach leveraging the medical reports of clinical EEG recordings is to learn mappings between medical reports and EEG recordings. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework EEG-CLIP that aligns EEG time series and their corresponding clinical text descriptions in a shared embedding space. We investigate its potential for versatile EEG decoding, assessing performance on a range of few-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to nontrivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at <a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip">https://github.com/tidiane-camaret/EEGClip</a>. </p>
<blockquote>
<p>æ·±åº¦ç½‘ç»œé€šå¸¸è¢«ç”¨äºè„‘ç”µå›¾ï¼ˆEEGï¼‰è§£ç ï¼Œä»¥è§£å†³ç‰¹å®šä»»åŠ¡ï¼Œå¦‚ç—…ç†å­¦æˆ–æ€§åˆ«è§£ç ã€‚ä¸€ç§æ›´é€šç”¨çš„æ–¹æ³•æ˜¯é€šè¿‡ä¸´åºŠè„‘ç”µå›¾è®°å½•çš„åŒ»å­¦æŠ¥å‘Šæ¥å­¦ä¹ åŒ»å­¦æŠ¥å‘Šå’Œè„‘ç”µå›¾è®°å½•ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚è¿™ç§æ–¹æ³•æœ€åˆåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œç”¨äºåŒ¹é…å›¾åƒå’Œæ–‡æœ¬æ ‡é¢˜ï¼Œéšåé€šè¿‡ä½¿ç”¨æ–‡æœ¬ç±»æç¤ºæˆåŠŸå®ç°äº†é›¶æ ·æœ¬è§£ç ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬éµå¾ªè¿™ç§æ–¹æ³•ï¼Œå¼€å‘äº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶EEG-CLIPï¼Œè¯¥æ¡†æ¶å°†è„‘ç”µå›¾æ—¶é—´åºåˆ—ä¸å…¶ç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°å¯¹é½åˆ°å…±äº«åµŒå…¥ç©ºé—´ä¸­ã€‚æˆ‘ä»¬å¯¹å…¶åœ¨å„ç§å°æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„é€šç”¨è„‘ç”µå›¾è§£ç æ½œåŠ›è¿›è¡Œäº†è°ƒæŸ¥ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“æœè¡¨æ˜EEG-CLIPèƒ½å¤Ÿå®ç°å¯¹æ–‡æœ¬å’Œè„‘ç”µå›¾è¡¨ç¤ºçš„éå¹³å‡¡å¯¹é½ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€ç§å­¦ä¹ é€šç”¨è„‘ç”µå›¾è¡¨ç¤ºçš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œè¿™å¯èƒ½é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–ä½¿ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹æ¥æ›´å®¹æ˜“åœ°åˆ†æå„ç§è§£ç é—®é¢˜ã€‚é‡ç°æˆ‘ä»¬ç»“æœçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tidiane-camaret/EEGClipæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16531v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šEEGä¿¡å·è§£ç ä¸­çš„æ·±åº¦ç½‘ç»œç»å¸¸ä»…é™äºç‰¹å®šçš„ä»»åŠ¡å¦‚ç—…ç†ä¿¡æ¯æˆ–æ€§åˆ«è§£ç ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†å¦ä¸€ç§æ›´å…·æ™®éæ€§çš„æ–¹æ³•ï¼Œå€ŸåŠ©ä¸´åºŠEEGè®°å½•çš„åŒ»ç–—æŠ¥å‘Šå­¦ä¹ EEGè®°å½•å’ŒæŠ¥å‘Šä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ç‡å…ˆé€šè¿‡åŒ¹é…å›¾åƒåŠå…¶æ–‡æœ¬æ ‡ç­¾å¼€åˆ›äº†å…ˆæ²³ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬ç±»åˆ«æç¤ºå®ç°äº†æˆåŠŸçš„é›¶æ ·æœ¬è§£ç ã€‚è¯¥ç ”ç©¶åŸºäºæ­¤æå‡ºäº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶EEG-CLIPï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å®ç°å¯¹EEGæ—¶é—´åºåˆ—å’Œå¯¹åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°çš„åŒ¹é…ã€‚ç»è¿‡ä¸€ç³»åˆ—å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬çš„å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜EEG-CLIPèƒ½è‰¯å¥½åœ°åŒ¹é…æ–‡æœ¬å’ŒEEGè¡¨å¾ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§é€šç”¨åŒ–çš„EEGè¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œæœ‰æœ›é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬å³å¯è½»æ¾åˆ†æå„ç§è§£ç é—®é¢˜ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å½“å‰EEGä¿¡å·è§£ç çš„æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸å±€é™äºç‰¹å®šä»»åŠ¡ï¼Œå¦‚ç—…ç†ä¿¡æ¯æˆ–æ€§åˆ«è§£ç ã€‚</li>
<li>ä¸€ç§æ›´å…·æ™®éæ€§çš„æ–¹æ³•æ˜¯é€šè¿‡å­¦ä¹ EEGè®°å½•å’ŒåŒ»ç–—æŠ¥å‘Šä¹‹é—´çš„æ˜ å°„å…³ç³»æ¥è¿›è¡Œç ”ç©¶ã€‚</li>
<li>è¯¥æ–¹æ³•å€Ÿé‰´äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„å›¾åƒä¸æ–‡æœ¬åŒ¹é…æŠ€æœ¯ï¼Œå®ç°äº†é›¶æ ·æœ¬è§£ç ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†å¯¹æ¯”å­¦ä¹ æ¡†æ¶EEG-CLIPï¼Œè¯¥æ¡†æ¶èƒ½åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­åŒ¹é…EEGæ—¶é—´åºåˆ—å’Œå¯¹åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬çš„å®éªŒéªŒè¯ï¼ŒEEG-CLIPèƒ½æœ‰æ•ˆåœ°åŒ¹é…æ–‡æœ¬å’ŒEEGè¡¨å¾ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§é€šç”¨åŒ–çš„EEGè¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿç®€åŒ–å¯¹å¤šç§è§£ç é—®é¢˜çš„åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c28016350ea7c3379f55fe36088f04b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5380352982422170cf49dbe9f33a43d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176d89d2ecd6fe8f594af6f799918f37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130d0bbbf27d5ec78924de2024c3ccf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11f8219173346a8618cd8f54c25c7337.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="APLA-A-Simple-Adaptation-Method-for-Vision-Transformers"><a href="#APLA-A-Simple-Adaptation-Method-for-Vision-Transformers" class="headerlink" title="APLA: A Simple Adaptation Method for Vision Transformers"></a>APLA: A Simple Adaptation Method for Vision Transformers</h2><p><strong>Authors:Moein Sorkhei, Emir Konuk, Kevin Smith, Christos Matsoukas</strong></p>
<p>Existing adaptation techniques typically require architectural modifications or added parameters, leading to high computational costs and complexity. We introduce Attention Projection Layer Adaptation (APLA), a simple approach to adapt vision transformers (ViTs) without altering the architecture or adding parameters. Through a systematic analysis, we find that the layer immediately after the attention mechanism is crucial for adaptation. By updating only this projection layer, or even just a random subset of this layerâ€™s weights, APLA achieves state-of-the-art performance while reducing GPU memory usage by up to 52.63% and training time by up to 43.0%, with no extra cost at inference. Across 46 datasets covering a variety of tasks including scene classification, medical imaging, satellite imaging, and fine-grained classification, APLA consistently outperforms 17 other leading adaptation methods, including full fine-tuning, on classification, segmentation, and detection tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MoeinSorkhei/APLA">https://github.com/MoeinSorkhei/APLA</a>. </p>
<blockquote>
<p>ç°æœ‰çš„è‡ªé€‚åº”æŠ€æœ¯é€šå¸¸éœ€è¦ä¿®æ”¹æ¶æ„æˆ–å¢åŠ å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ä¸”å¤æ‚ã€‚æˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›æŠ•å½±å±‚è‡ªé€‚åº”ï¼ˆAPLAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸æ”¹å˜æ¶æ„æˆ–å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹é€‚åº”è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å‘ç°æ³¨æ„åŠ›æœºåˆ¶ä¹‹åçš„é‚£ä¸€å±‚å¯¹äºè‡ªé€‚åº”è‡³å…³é‡è¦ã€‚é€šè¿‡ä»…æ›´æ–°è¿™ä¸€æŠ•å½±å±‚ï¼Œæˆ–è€…ç”šè‡³åªæ›´æ–°è¯¥å±‚æƒé‡çš„éšæœºå­é›†ï¼ŒAPLAæ–¹æ³•åœ¨é™ä½GPUå†…å­˜ä½¿ç”¨ç‡é«˜è¾¾52.63%å’Œè®­ç»ƒæ—¶é—´é«˜è¾¾43.0%çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰ä»»ä½•é¢å¤–çš„æˆæœ¬ã€‚åœ¨æ¶µç›–åœºæ™¯åˆ†ç±»ã€åŒ»å­¦å½±åƒã€å«æ˜Ÿæˆåƒå’Œç²¾ç»†åˆ†ç±»ç­‰å¤šç§ä»»åŠ¡çš„46ä¸ªæ•°æ®é›†ä¸Šï¼ŒAPLAæ–¹æ³•åœ¨åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºå…¶ä»–17ç§é¢†å…ˆçš„è‡ªé€‚åº”æ–¹æ³•ï¼ŒåŒ…æ‹¬å®Œå…¨å¾®è°ƒæ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MoeinSorkhei/APLA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MoeinSorkhei/APLAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11335v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Attention Projection Layer Adaptationï¼ˆAPLAï¼‰æ–¹æ³•ï¼Œç”¨äºé€‚åº”è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰è€Œæ— éœ€æ”¹å˜æ¶æ„æˆ–å¢åŠ å‚æ•°ã€‚é€šè¿‡æ›´æ–°æ³¨æ„åŠ›æœºåˆ¶åçš„å…³é”®å±‚çš„æŠ•å½±å±‚æˆ–è¯¥å±‚æƒé‡çš„éšæœºå­é›†ï¼ŒAPLAå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡å’Œè®­ç»ƒæ—¶é—´ï¼Œä¸”æ¨ç†æˆæœ¬æ²¡æœ‰é¢å¤–å¢åŠ ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒAPLAåœ¨åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸ŠæŒç»­è¶…è¶Šäº†å…¶ä»–17ç§ä¸»æµçš„é€‚åº”æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APLAæ˜¯ä¸€ç§é€‚åº”è§†è§‰å˜å‹å™¨çš„æ–°æ–¹æ³•ï¼Œæ— éœ€æ”¹å˜æ¶æ„æˆ–å¢åŠ å‚æ•°ã€‚</li>
<li>é€šè¿‡å¯¹ç³»ç»Ÿçš„åˆ†æï¼Œå‘ç°æ³¨æ„åŠ›æœºåˆ¶åçš„å±‚å¯¹äºé€‚åº”è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡æ›´æ–°è¯¥æŠ•å½±å±‚æˆ–è¯¥å±‚æƒé‡çš„éšæœºå­é›†ï¼ŒAPLAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>APLAæ–¹æ³•æ˜¾è‘—é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>APLAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–é€‚åº”æ–¹æ³•ã€‚</li>
<li>APLAæ–¹æ³•å‡å°‘äº†è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-754a7f2d550d14d07300dbaa88b844bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea7108820f671bc0860b8cc37dc7461d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4539b129ebaa1457839527c8614859cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d839673fa04edbe0a58fcee79e522331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac0cf94e8f6e282dbb86327a6551b107.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection"><a href="#Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection"></a>Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding</strong></p>
<p>Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limits generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and improve the modelâ€™s generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-model attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our methodâ€™s superior performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL">https://github.com/xiaozhen228/Bayes-PFL</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½è¡¨ç°ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾…åŠ©æ•°æ®ï¼Œå¯ä»¥ç›´æ¥å¯¹ç›®æ ‡æ•°æ®é›†è¿›è¡Œè·¨ç±»åˆ«çš„å¼‚å¸¸æ£€æµ‹ï¼Œä¾‹å¦‚æ£€æµ‹å·¥ä¸šäº§å“è¡¨é¢çš„ç¼ºé™·æˆ–è¯†åˆ«ç»„ç»‡ä¸­çš„è‚¿ç˜¤ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡æ‰‹åŠ¨è®¾è®¡æˆ–ä¼˜åŒ–å¯å­¦ä¹ çš„æç¤ºå‘é‡æ¥æ„å»ºæ–‡æœ¬æç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰æ‰‹å·¥åˆ¶ä½œçš„æç¤ºéœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯•é”™ï¼›2ï¼‰å•ä¸€å½¢å¼çš„å¯å­¦ä¹ æç¤ºéš¾ä»¥æ•æ‰å¤æ‚çš„å¼‚å¸¸è¯­ä¹‰ï¼›3ï¼‰æ— çº¦æŸçš„æç¤ºç©ºé—´é™åˆ¶äº†æœªè§ç±»åˆ«çš„æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è´å¶æ–¯æç¤ºæµå­¦ä¹ ï¼ˆBayes-PFLï¼‰ï¼Œå®ƒä»è´å¶æ–¯çš„è§’åº¦å°†æç¤ºç©ºé—´å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªæç¤ºæµæ¨¡å—æ¥å­¦ä¹ å›¾åƒç‰¹å®šå’Œå›¾åƒé€šç”¨çš„åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒè¢«å…±åŒç”¨æ¥è§„èŒƒæ–‡æœ¬æç¤ºç©ºé—´ï¼Œæé«˜æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å­¦ä¹ åˆ°çš„åˆ†å¸ƒç„¶åè¢«é‡‡æ ·ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„æ–‡æœ¬æç¤ºï¼Œæœ‰æ•ˆåœ°è¦†ç›–äº†æç¤ºç©ºé—´ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ®‹å·®è·¨æ¨¡å‹æ³¨æ„åŠ›ï¼ˆRCAï¼‰æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å°†åŠ¨æ€æ–‡æœ¬åµŒå…¥ä¸ç²¾ç»†å›¾åƒç‰¹å¾å¯¹é½ã€‚åœ¨15ä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaozhen228/Bayes-PFLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10080v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ‰‹åŠ¨è®¾è®¡æ–‡æœ¬æç¤ºç¹çã€å­¦ä¹ å¼æç¤ºå‘é‡å½¢å¼å•ä¸€ä»¥åŠæç¤ºç©ºé—´æœªçº¦æŸç­‰é—®é¢˜ï¼Œæå‡ºäº†è´å¶æ–¯æç¤ºæµå­¦ä¹ ï¼ˆBayes-PFLï¼‰ã€‚è¯¥æ–¹æ³•ä»è´å¶æ–¯è§’åº¦å°†æç¤ºç©ºé—´å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè®¾è®¡æç¤ºæµæ¨¡å—å­¦ä¹ å›¾åƒç‰¹å®šå’Œé€šç”¨çš„åˆ†å¸ƒï¼Œä»¥è§„èŒƒæ–‡æœ¬æç¤ºç©ºé—´å¹¶æé«˜æœªè§ç±»åˆ«çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶å¼•å…¥æ®‹ä½™è·¨æ¨¡å‹æ³¨æ„åŠ›ï¼ˆRCAï¼‰æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å¯¹é½åŠ¨æ€æ–‡æœ¬åµŒå…¥å’Œç²¾ç»†å›¾åƒç‰¹å¾ã€‚åœ¨å¤šä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯é€šè¿‡åˆ©ç”¨è¾…åŠ©æ•°æ®è¿›è¡Œè·¨ç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ‰‹åŠ¨è®¾è®¡æˆ–ä¼˜åŒ–å­¦ä¹ å¼æç¤ºå‘é‡æ¥æ„å»ºæ–‡æœ¬æç¤ºã€‚</li>
<li>æ‰‹åŠ¨è®¾è®¡æç¤ºéœ€è¦ä¸“ä¸šçŸ¥è¯†ä¸”éœ€å¤§é‡è¯•é”™ï¼Œå­¦ä¹ å¼æç¤ºå‘é‡å½¢å¼å•ä¸€éš¾ä»¥æ•æ‰å¤æ‚å¼‚å¸¸è¯­ä¹‰ã€‚</li>
<li>æå‡ºè´å¶æ–¯æç¤ºæµå­¦ä¹ æ–¹æ³•ï¼Œå°†æç¤ºç©ºé—´å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>è®¾è®¡æç¤ºæµæ¨¡å—ä»¥å­¦ä¹ å›¾åƒç‰¹å®šå’Œé€šç”¨çš„åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ®‹ä½™è·¨æ¨¡å‹æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜æ–‡æœ¬åµŒå…¥ä¸å›¾åƒç‰¹å¾çš„åŒ¹é…åº¦ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜è¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œä»£ç å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b9aeb002c5dca7ac7509bd575c42d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c94fdcc90b6b6383389dc62ddbed2cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb006e8cc299475ed5d64fc6ce62fdd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Spectral-State-Space-Model-for-Rotation-Invariant-Visual-Representation-Learning"><a href="#Spectral-State-Space-Model-for-Rotation-Invariant-Visual-Representation-Learning" class="headerlink" title="Spectral State Space Model for Rotation-Invariant Visual Representation   Learning"></a>Spectral State Space Model for Rotation-Invariant Visual Representation   Learning</h2><p><strong>Authors:Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers</strong></p>
<p>State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation. To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”±äºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰å…·æœ‰ä»¥çº¿æ€§å¤æ‚åº¦å¯¹å…¨å±€å…³ç³»è¿›è¡Œå»ºæ¨¡çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œå®ƒä»¬ä½œä¸ºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ›¿ä»£å“è€Œå‡ºç°ã€‚SSMsä¸“é—¨è®¾è®¡ç”¨äºæ•è·å›¾åƒè¡¥ä¸çš„ç©ºé—´é‚»è¿‘å…³ç³»ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•è¯†åˆ«æ¦‚å¿µä¸Šç›¸å…³ä½†å¹¶éç›¸é‚»çš„è¡¥ä¸ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸€å±€é™æ€§æºäºå›¾åƒæ•°æ®çš„éå› æœæ€§è´¨ï¼Œå³ç¼ºä¹å›ºæœ‰çš„æ–¹å‘å…³ç³»ã€‚æ­¤å¤–ï¼Œå½“å‰çš„åŸºäºè§†è§‰çš„SSMså¯¹æ—‹è½¬ç­‰å˜æ¢é«˜åº¦æ•æ„Ÿã€‚å®ƒä»¬çš„é¢„è®¾æ‰«ææ–¹å‘å–å†³äºåŸå§‹å›¾åƒçš„æ–¹å‘ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æ—‹è½¬åäº§ç”Ÿä¸ä¸€è‡´çš„è¡¥ä¸å¤„ç†åºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Spectral VMambaè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä»å›¾åƒè¡¥ä¸çš„å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­å¾—å‡ºçš„å…‰è°±ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°æ•æ‰å›¾åƒå†…çš„å…¨å±€ç»“æ„ã€‚é€šè¿‡è°±åˆ†è§£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç‹¬ç«‹äºå›¾åƒæ–¹å‘ç¼–ç è¡¥ä¸å…³ç³»ï¼Œå€ŸåŠ©æˆ‘ä»¬çš„æ—‹è½¬ç‰¹å¾è§„èŒƒåŒ–å™¨ï¼ˆRFNï¼‰æ¨¡å—å®ç°æ—‹è½¬ä¸å˜æ€§ã€‚æˆ‘ä»¬åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpectral VMambaåœ¨è§†è§‰é¢†åŸŸè¶…è¶Šäº†é¢†å…ˆçš„SSMsæ¨¡å‹ï¼Œå¦‚VMambaç­‰ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—‹è½¬çš„ä¸å˜æ€§ï¼Œå¹¶æä¾›ç±»ä¼¼çš„è¿è¡Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06369v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰å¯¹å›¾åƒå—ä¹‹é—´å…¨å±€å…³ç³»çš„çº¿æ€§å¤æ‚åº¦å»ºæ¨¡èƒ½åŠ›ï¼Œå®ƒä½œä¸ºä¸€ç§æ–°å…´æŠ€æœ¯æˆä¸ºVision Transformersï¼ˆViTsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒæ•°æ®ç¼ºä¹å›ºæœ‰çš„æ–¹å‘æ€§å…³ç³»ï¼ŒSSMsæ— æ³•è¯†åˆ«æ¦‚å¿µä¸Šç›¸å…³ä½†ä¸ç›¸é‚»çš„å›¾åƒå—ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spectral VMambaæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒå—çš„å›¾æ‹‰æ™®æ‹‰æ–¯è°±ä¿¡æ¯ï¼Œé€šè¿‡è°±åˆ†è§£ç¼–ç å›¾åƒå—å…³ç³»ï¼Œç‹¬ç«‹äºå›¾åƒæ–¹å‘ï¼Œå®ç°äº†æ—‹è½¬ä¸å˜æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSpectral VMambaåœ¨è§†è§‰é¢†åŸŸé¢†å…ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¦‚VMambaç­‰ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—‹è½¬çš„ä¸å˜æ€§å¹¶ç»´æŒé«˜æ•ˆçš„è¿è¡Œæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰èƒ½å¤Ÿçº¿æ€§åœ°å»ºæ¨¡å›¾åƒå—ä¹‹é—´çš„å…¨å±€å…³ç³»ã€‚</li>
<li>SSMséš¾ä»¥è¯†åˆ«éç›¸é‚»ä½†æ¦‚å¿µç›¸å…³çš„å›¾åƒå—ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>Spectral VMambaæ–¹æ³•é€šè¿‡åˆ©ç”¨å›¾æ‹‰æ™®æ‹‰æ–¯è°±ä¿¡æ¯è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>Spectral VMambaé€šè¿‡è°±åˆ†è§£ç¼–ç å›¾åƒå—å…³ç³»ï¼Œå®ç°äº†æ—‹è½¬ä¸å˜æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSpectral VMambaåœ¨è§†è§‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¦‚VMambaç­‰é¢†å…ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>Spectral VMambaç»´æŒäº†å¯¹æ—‹è½¬çš„ä¸å˜æ€§å¹¶ä¸”å…·æœ‰é«˜æ•ˆçš„è¿è¡Œæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12fe6d523c6b6bba1c1baebea8f3c70a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-969e514de369ed035e4e5380f9159f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ae810ef23d77c168fee387a9d9c08b3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Sparse-autoencoders-reveal-selective-remapping-of-visual-concepts-during-adaptation"><a href="#Sparse-autoencoders-reveal-selective-remapping-of-visual-concepts-during-adaptation" class="headerlink" title="Sparse autoencoders reveal selective remapping of visual concepts during   adaptation"></a>Sparse autoencoders reveal selective remapping of visual concepts during   adaptation</h2><p><strong>Authors:Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider</strong></p>
<p>Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms. </p>
<blockquote>
<p>é€‚åº”åŸºç¡€æ¨¡å‹ç”¨äºç‰¹å®šç›®çš„å·²æˆä¸ºä¸ºä¸‹æ¸¸åº”ç”¨æ„å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œé€‚åº”è¿‡ç¨‹ä¸­å“ªäº›æœºåˆ¶èµ·ä½œç”¨è¿˜æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºCLIPè§†è§‰å˜å‹å™¨å¼€å‘äº†ä¸€ç§æ–°çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œå‘½åä¸ºPatchSAEï¼Œä»¥åœ¨ç»†ç²’åº¦çº§åˆ«æå–å¯è§£é‡Šçš„æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œå½¢çŠ¶ã€é¢œè‰²æˆ–å¯¹è±¡çš„è¯­ä¹‰ï¼‰åŠå…¶é€å—çš„ç©ºé—´å½’å±ã€‚æˆ‘ä»¬æ¢è®¨äº†è¿™äº›æ¦‚å¿µå¦‚ä½•å½±å“ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºï¼Œå¹¶ç ”ç©¶äº†æœ€æ–°çš„åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯å¦‚ä½•æ”¹å˜æ¨¡å‹è¾“å…¥ä¸è¿™äº›æ¦‚å¿µçš„è”ç³»ã€‚è™½ç„¶é€‚åº”æ¨¡å‹å’Œéé€‚åº”æ¨¡å‹ä¹‹é—´çš„æ¦‚å¿µæ¿€æ´»ç•¥æœ‰å˜åŒ–ï¼Œä½†æˆ‘ä»¬å‘ç°å¸¸è§é€‚åº”ä»»åŠ¡çš„å¤§éƒ¨åˆ†æ”¶ç›Šéƒ½å¯ä»¥ç”¨éé€‚åº”åŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„æ¦‚å¿µæ¥è§£é‡Šã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä½¿ç”¨è§†è§‰å˜å‹å™¨çš„SAEçš„å…·ä½“æ¡†æ¶ï¼Œå¹¶æä¾›äº†è§£é‡Šé€‚åº”æœºåˆ¶çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05276v2">PDF</a> Published as a conference paper at the Thirteenth International   Conference on Learning Representations (ICLR 2025)</p>
<p><strong>Summary</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºPatchSAEçš„æ–°æ–¹æ³•ï¼Œå®ƒæ˜¯ç”¨äºCLIPè§†è§‰å˜å‹å™¨çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿæå–å¯è§£é‡Šçš„æ¦‚å¿µï¼ˆå¦‚å½¢çŠ¶ã€é¢œè‰²æˆ–å¯¹è±¡çš„è¯­ä¹‰ï¼‰ï¼Œä»¥åŠå®ƒä»¬åœ¨å›¾åƒå—ä¸­çš„ç©ºé—´å½’å±ã€‚æ–‡ç« æ¢è®¨äº†è¿™äº›æ¦‚å¿µå¦‚ä½•å½±å“ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºï¼Œå¹¶ç ”ç©¶äº†åŸºäºæœ€æ–°å‰æ²¿æŠ€æœ¯çš„æç¤ºé€‚åº”æ€§æŠ€æœ¯å¦‚ä½•æ”¹å˜æ¨¡å‹è¾“å…¥ä¸è¿™äº›æ¦‚å¿µçš„è”ç³»ã€‚æ–‡ç« æä¾›äº†ç”¨äºè§†è§‰å˜å‹å™¨ä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨çš„å…·ä½“æ¡†æ¶ï¼Œå¹¶ä¸ºè§£é‡Šé€‚åº”æ€§æœºåˆ¶æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æ–¹æ³•PatchSAEç”¨äºCLIPè§†è§‰å˜å‹å™¨ï¼Œä»¥æå–å›¾åƒä¸­çš„å¯è§£é‡Šæ¦‚å¿µï¼Œå¦‚å½¢çŠ¶ã€é¢œè‰²å’Œå¯¹è±¡è¯­ä¹‰ã€‚</li>
<li>åœ¨ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¦‚å¿µä¼šå½±å“æ¨¡å‹è¾“å‡ºã€‚</li>
<li>è¿‘æœŸå‰æ²¿çš„æç¤ºé€‚åº”æ€§æŠ€æœ¯ä¼šæ”¹å˜æ¨¡å‹è¾“å…¥ä¸æ¦‚å¿µä¹‹é—´çš„è”ç³»ã€‚</li>
<li>é€‚åº”æ€§å’Œéé€‚åº”æ€§æ¨¡å‹çš„æ¦‚å¿µæ¿€æ´»ç•¥æœ‰å˜åŒ–ã€‚</li>
<li>å¤§å¤šæ•°å¸¸è§çš„é€‚åº”æ€§ä»»åŠ¡çš„æ”¶ç›Šå¯ä»¥é€šè¿‡éé€‚åº”æ€§åŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„æ¦‚å¿µæ¥è§£é‡Šã€‚</li>
<li>æ­¤å·¥ä½œæä¾›äº†è®­ç»ƒå’Œä½¿ç”¨è§†è§‰å˜å‹å™¨çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨çš„å…·ä½“æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a063ecde33fc52578f657cc4f5d027e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e2c416526095dc599482f467f82a67c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ba668415abea3a8d0da1fe158b0e35e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d73767112054016f47af56e01fc8a662.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Aberration-Correcting-Vision-Transformers-for-High-Fidelity-Metalens-Imaging"><a href="#Aberration-Correcting-Vision-Transformers-for-High-Fidelity-Metalens-Imaging" class="headerlink" title="Aberration Correcting Vision Transformers for High-Fidelity Metalens   Imaging"></a>Aberration Correcting Vision Transformers for High-Fidelity Metalens   Imaging</h2><p><strong>Authors:Byeonghyeon Lee, Youbin Kim, Yongjae Jo, Hyunsu Kim, Hyemi Park, Yangkyu Kim, Debabrata Mandal, Praneeth Chakravarthula, Inki Kim, Eunbyung Park</strong></p>
<p>Metalens is an emerging optical system with an irreplaceable merit in that it can be manufactured in ultra-thin and compact sizes, which shows great promise in various applications. Despite its advantage in miniaturization, its practicality is constrained by spatially varying aberrations and distortions, which significantly degrade the image quality. Several previous arts have attempted to address different types of aberrations, yet most of them are mainly designed for the traditional bulky lens and ineffective to remedy harsh aberrations of the metalens. While there have existed aberration correction methods specifically for metalens, they still fall short of restoration quality. In this work, we propose a novel aberration correction framework for metalens-captured images, harnessing Vision Transformers (ViT) that have the potential to restore metalens images with non-uniform aberrations. Specifically, we devise a Multiple Adaptive Filters Guidance (MAFG), where multiple Wiener filters enrich the degraded input images with various noise-detail balances and a cross-attention module reweights the features considering the different degrees of aberrations. In addition, we introduce a Spatial and Transposed self-Attention Fusion (STAF) module, which aggregates features from spatial self-attention and transposed self-attention modules to further ameliorate aberration correction. We conduct extensive experiments, including correcting aberrated images and videos, and clean 3D reconstruction. The proposed method outperforms the previous arts by a significant margin. We further fabricate a metalens and verify the practicality of our method by restoring the images captured with the manufactured metalens. Code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://benhenryl.github.io/Metalens-Transformer">https://benhenryl.github.io/Metalens-Transformer</a>. </p>
<blockquote>
<p>Metalensæ˜¯ä¸€ç§æ–°å…´çš„å…‰å­¦ç³»ç»Ÿï¼Œå…·æœ‰ä¸å¯æ›¿ä»£çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåˆ¶æˆè¶…è–„ã€ç´§å‡‘çš„å°ºå¯¸ï¼Œåœ¨å¤šç§åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å°½ç®¡å…¶åœ¨å°å‹åŒ–æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§å—åˆ°ç©ºé—´å˜åŒ–å¼•èµ·çš„åƒå·®å’Œå¤±çœŸçš„é™åˆ¶ï¼Œè¿™ä¼šä¸¥é‡é™ä½å›¾åƒè´¨é‡ã€‚è™½ç„¶ä¹‹å‰çš„ä¸€äº›ç ”ç©¶è¯•å›¾è§£å†³ä¸åŒç±»å‹çš„åƒå·®ï¼Œä½†å¤§å¤šæ•°ä¸»è¦æ˜¯ä¸ºä¼ ç»Ÿçš„ç¬¨é‡é•œå¤´è®¾è®¡çš„ï¼Œå¯¹äºé‡‘å±é€é•œçš„ä¸¥é‡åƒå·®æ ¡æ­£æ•ˆæœæœ‰é™ã€‚å°½ç®¡å­˜åœ¨ä¸“é—¨é’ˆå¯¹é‡‘å±é€é•œçš„åƒå·®æ ¡æ­£æ–¹æ³•ï¼Œä½†å®ƒä»¬ä»è¾¾ä¸åˆ°ç†æƒ³çš„ä¿®å¤è´¨é‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé‡‘å±é€é•œæ•è·å›¾åƒçš„æ–°å‹åƒå·®æ ¡æ­£æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„æ½œåŠ›æ¥æ¢å¤å…·æœ‰éå‡åŒ€åƒå·®çš„é‡‘å±é€é•œå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šé‡è‡ªé€‚åº”æ»¤æ³¢å™¨æŒ‡å¯¼ï¼ˆMAFGï¼‰æ–¹æ³•ï¼Œå…¶ä¸­å¤šä¸ªWieneræ»¤æ³¢å™¨ä»¥ä¸åŒçš„å™ªå£°ç»†èŠ‚å¹³è¡¡ä¸°å¯Œé™è´¨çš„è¾“å…¥å›¾åƒï¼Œä¸€ä¸ªäº¤å‰æ³¨æ„æ¨¡å—æ ¹æ®ä¸åŒç¨‹åº¦çš„åƒå·®é‡æ–°åŠ æƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç©ºé—´è½¬ç½®è‡ªæ³¨æ„åŠ›èåˆï¼ˆSTAFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èšåˆæ¥è‡ªç©ºé—´è‡ªæ³¨æ„åŠ›å’Œè½¬ç½®è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç‰¹å¾ï¼Œä»¥è¿›ä¸€æ­¥æ”¹å–„åƒå·®æ ¡æ­£ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬æ ¡æ­£åƒå·®å›¾åƒå’Œè§†é¢‘ä»¥åŠæ¸…æ´3Dé‡å»ºã€‚æ‰€æå‡ºçš„æ–¹æ³•å¤§å¤§ä¼˜äºä»¥å‰çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ¶é€ äº†ä¸€ä¸ªé‡‘å±é€é•œï¼Œå¹¶é€šè¿‡æ¢å¤ç”¨åˆ¶é€ çš„é‡‘å±é€é•œæ•è·çš„å›¾åƒæ¥éªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„å®ç”¨æ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://benhenryl.github.io/Metalens-Transformer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://benhenryl.github.io/Metalens-Transformerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04591v2">PDF</a> 22 pages, 22 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å…´çš„å…‰å­¦ç³»ç»Ÿâ€”â€”Metalensï¼Œå®ƒå…·æœ‰è¶…è–„ã€ç´§å‡‘çš„ä¼˜ç‚¹ï¼Œåœ¨å¤šç§åº”ç”¨ä¸­æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ç€ç©ºé—´å˜åŒ–å¯¼è‡´çš„åƒå·®å’Œå¤±çœŸé—®é¢˜ï¼Œè¿™ä¼šä¸¥é‡å½±å“å›¾åƒè´¨é‡ã€‚ä¸ºæ”¹å–„Metalensæˆåƒè´¨é‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ–°å‹åƒå·®æ ¡æ­£æ¡†æ¶ï¼Œé€šè¿‡å¤šé‡é€‚åº”æ€§æ»¤æ³¢å™¨å¼•å¯¼å’Œç©ºé—´ä¸è½¬ç½®è‡ªæ³¨æ„åŠ›èåˆæ¨¡å—ï¼Œå®ç°äº†å¯¹éå‡åŒ€åƒå·®çš„æ ¡æ­£ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿®æ­£åƒå·®å›¾åƒå’Œè§†é¢‘ä»¥åŠæ¸…æ´3Dé‡å»ºæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç›¸å…³ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://benhenryl.github.io/Metalens-Transformer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://benhenryl.github.io/Metalens-Transformerä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Metalensä½œä¸ºä¸€ç§æ–°å…´å…‰å­¦ç³»ç»Ÿï¼Œå…·æœ‰è¶…è–„ã€ç´§å‡‘çš„ç‰¹ç‚¹ï¼Œåœ¨å¤šç§åº”ç”¨ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>Metalensåœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ç©ºé—´å˜åŒ–çš„åƒå·®å’Œå¤±çœŸé—®é¢˜ï¼Œä¸¥é‡å½±å“å›¾åƒè´¨é‡ã€‚</li>
<li>ç°æœ‰åƒå·®æ ¡æ­£æ–¹æ³•ä¸»è¦é’ˆå¯¹ä¼ ç»Ÿé•œå¤´ï¼Œå¯¹Metalensçš„ä¸¥è‹›åƒå·®æ ¡æ­£æ•ˆæœä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerçš„åƒå·®æ ¡æ­£æ¡†æ¶ï¼Œé€šè¿‡å¤šé‡é€‚åº”æ€§æ»¤æ³¢å™¨å¼•å¯¼å’Œç©ºé—´ä¸è½¬ç½®è‡ªæ³¨æ„åŠ›èåˆæ¨¡å—è¿›è¡Œåƒå·®æ ¡æ­£ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¯¹Metalenséå‡åŒ€åƒå·®çš„æ ¡æ­£ï¼Œå¹¶åœ¨ä¿®æ­£åƒå·®å›¾åƒã€è§†é¢‘ä»¥åŠ3Dé‡å»ºæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>å®éªŒå®¤åˆ¶é€ äº†Metalensï¼Œå¹¶é€šè¿‡å®é™…å›¾åƒæ¢å¤éªŒè¯äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-defe161a2b217800b06fe9d878a7743c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b560301f63e92e05270d4196d5ddfb4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3cf6beb5592876292827e82de494eb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52cdbf5ab7888e17f9d0c646188819f5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Morphing-Tokens-Draw-Strong-Masked-Image-Models"><a href="#Morphing-Tokens-Draw-Strong-Masked-Image-Models" class="headerlink" title="Morphing Tokens Draw Strong Masked Image Models"></a>Morphing Tokens Draw Strong Masked Image Models</h2><p><strong>Authors:Taekyung Kim, Byeongho Heo, Dongyoon Han</strong></p>
<p>Masked image modeling (MIM) has emerged as a promising approach for pre-training Vision Transformers (ViTs). MIMs predict masked tokens token-wise to recover target signals that are tokenized from images or generated by pre-trained models like vision-language models. While using tokenizers or pre-trained models is viable, they often offer spatially inconsistent supervision even for neighboring tokens, hindering models from learning discriminative representations. Our pilot study identifies spatial inconsistency in supervisory signals and suggests that addressing it can improve representation learning. Building upon this insight, we introduce Dynamic Token Morphing (DTM), a novel method that dynamically aggregates tokens while preserving context to generate contextualized targets, thereby likely reducing spatial inconsistency. DTM is compatible with various SSL frameworks; we showcase significantly improved MIM results, barely introducing extra training costs. Our method facilitates MIM training by using more spatially consistent targets, resulting in improved training trends as evidenced by lower losses. Experiments on ImageNet-1K and ADE20K demonstrate DTMâ€™s superiority, which surpasses complex state-of-the-art MIM methods. Furthermore, the evaluation of transfer learning on downstream tasks like iNaturalist, along with extensive empirical studies, supports DTMâ€™s effectiveness. </p>
<blockquote>
<p>æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ä½œä¸ºä¸€ç§é¢„è®­ç»ƒè§†è§‰Transformerï¼ˆViTï¼‰çš„æ–¹æ³•å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚MIMé€šè¿‡é¢„æµ‹è¢«æ©ç çš„ä»¤ç‰Œæ¥é€ä¸ªæ¢å¤ç›®æ ‡ä¿¡å·ï¼Œè¿™äº›ä¿¡å·æ˜¯ä»å›¾åƒä¸­æ ‡è®°å‡ºæ¥çš„æˆ–ç”±é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ç”Ÿæˆçš„ã€‚è™½ç„¶ä½¿ç”¨æ ‡è®°å™¨æˆ–é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¯è¡Œçš„ï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹ç›¸é‚»ä»¤ç‰Œæä¾›ç©ºé—´ä¸ä¸€è‡´çš„ç›‘ç£ï¼Œé˜»ç¢äº†æ¨¡å‹å­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºã€‚æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶ç¡®å®šäº†ç›‘ç£ä¿¡å·ä¸­çš„ç©ºé—´ä¸ä¸€è‡´æ€§ï¼Œå¹¶è¡¨æ˜è§£å†³è¿™ä¸€é—®é¢˜å¯ä»¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€ä»¤ç‰Œå˜å½¢ï¼ˆDTMï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåœ¨ä¿ç•™ä¸Šä¸‹æ–‡çš„åŒæ—¶åŠ¨æ€èšåˆä»¤ç‰Œä»¥ç”Ÿæˆä¸Šä¸‹æ–‡ç›®æ ‡ï¼Œä»è€Œå¯èƒ½å‡å°‘ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚DTMä¸å„ç§SSLæ¡†æ¶å…¼å®¹ï¼›æˆ‘ä»¬å±•ç¤ºäº†æ˜¾è‘—æ”¹è¿›çš„MIMç»“æœï¼Œå‡ ä¹æ²¡æœ‰å¼•å…¥é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚é€šè¿‡ä½¿ç”¨æ›´ç©ºé—´ä¸€è‡´çš„ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ƒè¿›äº†MIMè®­ç»ƒï¼Œé€šè¿‡æ›´ä½çš„æŸå¤±è¯æ˜äº†æ”¹è¿›çš„è®­ç»ƒè¶‹åŠ¿ã€‚åœ¨ImageNet-1Kå’ŒADE20Kä¸Šçš„å®éªŒè¯æ˜äº†DTMçš„ä¼˜è¶Šæ€§ï¼Œè¶…è¿‡äº†å¤æ‚çš„æœ€æ–°MIMæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡å¦‚iNaturalistçš„è¿ç§»å­¦ä¹ çš„è¯„ä¼°ä»¥åŠå¹¿æ³›çš„å®è¯ç ”ç©¶æ”¯æŒäº†DTMçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.00254v4">PDF</a> 24 pages, 16 tables, 8 figures. To be presented at ICLRâ€™25</p>
<p><strong>Summary</strong></p>
<p>æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å·²æˆä¸ºé¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚æœ¬æ–‡è¯†åˆ«äº†ç›‘ç£ä¿¡å·ä¸­çš„ç©ºé—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†åŠ¨æ€ä»¤ç‰Œå½¢æ€ï¼ˆDTMï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€èšåˆä»¤ç‰ŒåŒæ—¶ä¿ç•™ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆä¸Šä¸‹æ–‡ç›®æ ‡ï¼Œä»è€Œå‡å°‘ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ä¸å„ç§SSLæ¡†æ¶å…¼å®¹ï¼Œå±•ç¤ºäº†æ”¹è¿›çš„MIMç»“æœï¼Œä¸”å‡ ä¹ä¸ä¼šå¢åŠ é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚åœ¨ImageNet-1Kå’ŒADE20Kä¸Šçš„å®éªŒè¡¨æ˜DTMçš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†å¤æ‚çš„æœ€æ–°MIMæ–¹æ³•ã€‚ä¸‹æ¸¸ä»»åŠ¡çš„è¿ç§»å­¦ä¹ è¯„ä¼°ï¼Œä»¥åŠå¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œéƒ½æ”¯æŒDTMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å·²æˆä¸ºé¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨ç©ºé—´ä¸ä¸€è‡´æ€§ç›‘ç£é—®é¢˜ï¼Œå½±å“æ¨¡å‹å­¦ä¹ åˆ¤åˆ«æ€§è¡¨ç¤ºã€‚</li>
<li>åŠ¨æ€ä»¤ç‰Œå½¢æ€ï¼ˆDTMï¼‰æ–¹æ³•é€šè¿‡åŠ¨æ€èšåˆä»¤ç‰Œç”Ÿæˆä¸Šä¸‹æ–‡ç›®æ ‡ï¼Œå‡å°‘ç©ºé—´ä¸ä¸€è‡´æ€§ã€‚</li>
<li>DTMä¸å¤šç§SSLæ¡†æ¶å…¼å®¹ï¼Œå±•ç¤ºæ˜¾è‘—æ”¹å–„çš„MIMç»“æœï¼Œä¸”å‡ ä¹ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬ã€‚</li>
<li>DTMåœ¨ImageNet-1Kå’ŒADE20Kç­‰å®éªŒä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šæœ€æ–°MIMæ–¹æ³•ã€‚</li>
<li>DTMåœ¨è¿ç§»å­¦ä¹ å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°è¯„ä¼°ï¼Œå¹¶ç»è¿‡å¹¿æ³›å®è¯ç ”ç©¶éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>DTMæ–¹æ³•å¯¹äºæ”¹è¿›è§†è§‰è½¬æ¢å™¨çš„æ€§èƒ½å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.00254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e34e2c6b25f7aa71f9cbad506cf9496.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48869f549e7d5a1d17c1cd978f64d77b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7130979d1012a897e2cd6a03617d49b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d33e7f0a4cc46afee074701f92504560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2697bafce21c4f9afe33d6468633d97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96049dd0222d25ddc19403b13aabdd98.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe8e2ec1f311e54b959baac9c0821624.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  LPOSS Label Propagation Over Patches and Pixels for Open-vocabulary   Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Exploring Hallucination of Large Multimodal Models in Video   Understanding Benchmark, Analysis and Mitigation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
