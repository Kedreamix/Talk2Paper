<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Show or Tell? Effectively prompting Vision-Language Models for semantic   segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b9a423886f9f0e59e16c6b1f850b2916.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="Show-or-Tell-Effectively-prompting-Vision-Language-Models-for-semantic-segmentation"><a href="#Show-or-Tell-Effectively-prompting-Vision-Language-Models-for-semantic-segmentation" class="headerlink" title="Show or Tell? Effectively prompting Vision-Language Models for semantic   segmentation"></a>Show or Tell? Effectively prompting Vision-Language Models for semantic   segmentation</h2><p><strong>Authors:Niccolo Avogaro, Thomas Frick, Mattia Rigotti, Andrea Bartezzaghi, Filip Janicki, Cristiano Malossi, Konrad Schindler, Roy Assaf</strong></p>
<p>Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training. We examine the seemingly obvious question: how to effectively prompt VLMs for semantic segmentation. To that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the out-of-distribution MESS dataset collection. We introduce a scalable prompting scheme, few-shot prompted semantic segmentation, inspired by open-vocabulary segmentation and few-shot learning. It turns out that VLMs lag far behind specialist models trained for a specific segmentation task, by about 30% on average on the Intersection-over-Union metric. Moreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve. Our analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11% improvement in performance. Motivated by our findings, we propose PromptMatcher, a remarkably simple training-free baseline that combines both text and visual prompts, achieving state-of-the-art results outperforming the best text-prompted VLM by 2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic segmentation. </p>
<blockquote>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ­£è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œè¢«è§†ä¸ºå¯ä»¥é€šè¿‡æç¤ºæŒ‡ä»¤è§£å†³å„ç§ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„é—®é¢˜ï¼šå¦‚ä½•æœ‰æ•ˆåœ°å¯¹VLMsè¿›è¡Œè¯­ä¹‰åˆ†å‰²æç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿè¯„ä¼°äº†ä½¿ç”¨æ–‡æœ¬æˆ–è§†è§‰æç¤ºå¼•å¯¼çš„å¤šä¸ªæœ€æ–°æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„MESSæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯æ‰©å±•çš„æç¤ºæ–¹æ¡ˆï¼Œå³å°‘é•œå¤´è¯­ä¹‰åˆ†å‰²æç¤ºï¼Œè¯¥æ–¹æ¡ˆå—åˆ°å¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå°‘é•œå¤´å­¦ä¹ çš„å¯å‘ã€‚ç»“æœè¡¨æ˜ï¼ŒVLMsåœ¨äº¤å¹¶æ¯”æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¿œè¿œè½åäºé’ˆå¯¹ç‰¹å®šåˆ†å‰²ä»»åŠ¡çš„ä¸“ä¸šæ¨¡å‹ï¼Œå¹³å‡è½åçº¦30%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ–‡æœ¬æç¤ºå’Œè§†è§‰æç¤ºæ˜¯äº’è¡¥çš„ï¼šä¸¤ç§æ¨¡å¼ä¸­çš„æ¯ä¸€ç§åœ¨è®¸å¤šä¾‹å­ä¸Šéƒ½ä¼šå¤±è´¥ï¼Œè€Œå¦ä¸€ç§å¯ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œèƒ½å¤Ÿé¢„æµ‹æœ€æœ‰æ•ˆçš„æç¤ºæ¨¡å¼å¯ä»¥å¯¼è‡´æ€§èƒ½æé«˜11%ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†PromptMatcherï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æ— éœ€è®­ç»ƒçš„åŸºç¡€çº¿æ¨¡å‹ï¼Œå®ƒç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨å°‘é•œå¤´è¯­ä¹‰åˆ†å‰²æç¤ºä¸Šè¶…è¶Šäº†æœ€ä½³æ–‡æœ¬æç¤ºVLM 2.5%ï¼Œä»¥åŠé¡¶çº§è§†è§‰æç¤ºVLM 3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æœ‰æ•ˆæç¤ºæ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä½¿ç”¨æ–‡æœ¬æˆ–è§†è§‰æç¤ºå¼•å¯¼çš„å¤šä¸ªæ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç§åŸºäºå°‘æ ·æœ¬å­¦ä¹ çš„å¯æ‰©å±•æç¤ºæ–¹æ¡ˆâ€”â€”å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ã€‚è™½ç„¶VLMsçš„è¡¨ç°è¾ƒä¸“ä¸šæ¨¡å‹é€Šè‰²ï¼Œå¹³å‡åœ¨Intersection-over-UnionæŒ‡æ ‡ä¸Šè½åçº¦30%ï¼Œä½†ç ”ç©¶å‘ç°æ–‡æœ¬æç¤ºå’Œè§†è§‰æç¤ºæ˜¯äº’è¡¥çš„ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†PromptMatcherè¿™ä¸€æ— éœ€è®­ç»ƒçš„åŸºçº¿æ–¹æ³•ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¢«çœ‹ä½œæ˜¯å¯ä»¥é€šè¿‡æç¤ºè§£å†³å„ç§ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>å¯¹äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæ–‡æœ¬æç¤ºå’Œè§†è§‰æç¤ºæ˜¯äº’è¡¥çš„ã€‚</li>
<li>VLMsåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾ƒä¸“ä¸šæ¨¡å‹å¹³å‡è½åçº¦30%ã€‚</li>
<li>æœ€æœ‰æ•ˆçš„æç¤ºæ¨¡æ€çš„é¢„æµ‹èƒ½åŠ›å¯ä»¥å¯¼è‡´æ€§èƒ½æå‡11%ã€‚</li>
<li>PromptMatcheræ–¹æ³•ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œå®ç°äº†å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æœ€ä½³æ•ˆæœã€‚</li>
<li>PromptMatcheræ— éœ€è®­ç»ƒï¼Œæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-06fd4240363d8a55ad6314cc8b69a18c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c3bd50c96c745073b7d71795fcdffbd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language"><a href="#HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language" class="headerlink" title="HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language"></a>HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language</h2><p><strong>Authors:Dimitrios Stamatios Bouras, Yihan Dai, Tairan Wang, Yingfei Xiong, Sergey Mechtaev</strong></p>
<p>While software requirements are often expressed in natural language, verifying the correctness of a program against natural language requirements is a hard and underexplored problem. Large language models (LLMs) are promising candidates for addressing this challenge, however our experience shows that they are ineffective in this task, often failing to detect even straightforward bugs. To address this gap, we introduce HoarePrompt, a novel approach that adapts fundamental ideas from program analysis and verification to natural language artifacts. Drawing inspiration from the strongest postcondition calculus, HoarePrompt employs a systematic, step-by-step process in which an LLM generates natural language descriptions of reachable program states at various points in the code. To manage loops, we propose few-shot-driven k-induction, an adaptation of the k-induction method widely used in model checking. Once program states are described, HoarePrompt leverages the LLM to assess whether the program, annotated with these state descriptions, conforms to the natural language requirements. For evaluating the quality of classifiers of program correctness with respect to natural language requirements, we constructed CoCoClaNeL, a challenging dataset of solutions to programming competition problems. Our experiments show that HoarePrompt improves the MCC by 62% compared to directly using Zero-shot-CoT prompts for correctness classification. Furthermore, HoarePrompt outperforms a classifier that assesses correctness via LLM-based test generation by increasing the MCC by 93%. The inductive reasoning mechanism contributes a 28% boost to MCC, underscoring its effectiveness in managing loops. </p>
<blockquote>
<p>è™½ç„¶è½¯ä»¶éœ€æ±‚é€šå¸¸ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥è¡¨è¾¾ï¼Œä½†é’ˆå¯¹è‡ªç„¶è¯­è¨€è¦æ±‚çš„ç¨‹åºæ­£ç¡®æ€§éªŒè¯æ˜¯ä¸€ä¸ªå›°éš¾ä¸”æœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„å€™é€‰è€…ï¼Œä½†æˆ‘ä»¬çš„ç»éªŒè¡¨æ˜ï¼Œå®ƒä»¬åœ¨æ­¤ä»»åŠ¡ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³æ— æ³•æ£€æµ‹åˆ°ç®€å•çš„é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†HoarePromptï¼Œè¿™æ˜¯ä¸€ç§å°†ç¨‹åºåˆ†æå’ŒéªŒè¯çš„åŸºæœ¬æ€æƒ³é€‚åº”åˆ°è‡ªç„¶è¯­è¨€å·¥ä»¶çš„æ–°æ–¹æ³•ã€‚ä»æœ€å¼ºçš„åæ¡ä»¶æ¼”ç®—ä¸­æ±²å–çµæ„Ÿï¼ŒHoarePrompté‡‡ç”¨ç³»ç»Ÿã€åˆ†æ­¥çš„è¿‡ç¨‹ï¼Œå…¶ä¸­LLMç”Ÿæˆä»£ç å„ç‚¹å¯è¾¾åˆ°ç¨‹åºçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚ä¸ºäº†ç®¡ç†å¾ªç¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°‘é‡æ ·æœ¬çš„kå½’çº³æ³•ï¼Œè¿™æ˜¯å¯¹æ¨¡å‹æ£€æŸ¥ä¸­å¹¿æ³›ä½¿ç”¨çš„kå½’çº³æ–¹æ³•çš„é€‚åº”ã€‚ä¸€æ—¦æè¿°äº†ç¨‹åºçŠ¶æ€ï¼ŒHoarePromptå°±åˆ©ç”¨LLMæ¥è¯„ä¼°å¸¦æœ‰è¿™äº›çŠ¶æ€æè¿°çš„ç¨‹åºæ˜¯å¦ç¬¦åˆè‡ªç„¶è¯­è¨€è¦æ±‚ã€‚ä¸ºäº†è¯„ä¼°åˆ†ç±»å™¨å¯¹ç¨‹åºæ­£ç¡®æ€§ç›¸å¯¹äºè‡ªç„¶è¯­è¨€è¦æ±‚çš„åˆ†ç±»è´¨é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†CoCoClaNeLæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€é¡¹ç«èµ›è§£å†³æ–¹æ¡ˆçš„æŒ‘æˆ˜æ€§æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ç›´æ¥ä½¿ç”¨é›¶æ ·æœ¬è®¤çŸ¥æç¤ºè¿›è¡Œæ­£ç¡®æ€§åˆ†ç±»ç›¸æ¯”ï¼ŒHoarePromptæé«˜äº†åŒ¹é…æ­£ç¡®ç‡ï¼ˆMCCï¼‰çš„62%ã€‚æ­¤å¤–ï¼Œä¸é€šè¿‡LLMç”Ÿæˆçš„æµ‹è¯•è¯„ä¼°æ­£ç¡®æ€§çš„åˆ†ç±»å™¨ç›¸æ¯”ï¼ŒHoarePromptæé«˜äº†åŒ¹é…æ­£ç¡®ç‡ï¼ˆMCCï¼‰çš„93%ã€‚å½’çº³æ¨ç†æœºåˆ¶å¯¹åŒ¹é…æ­£ç¡®ç‡çš„æå‡è¾¾åˆ°äº†28%ï¼Œè¯æ˜äº†å…¶åœ¨ç®¡ç†å¾ªç¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19599v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è½¯ä»¶éœ€æ±‚é€šå¸¸ä»¥è‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œä½†éªŒè¯ç¨‹åºæ˜¯å¦ç¬¦åˆè‡ªç„¶è¯­è¨€è¦æ±‚æ˜¯ä¸€é¡¹å›°éš¾ä¸”é²œæœ‰ç ”ç©¶çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½å…·æœ‰è§£å†³æ­¤é—®é¢˜çš„æ½œåŠ›ï¼Œä½†åœ¨è¯¥ä»»åŠ¡ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³æ— æ³•æ£€æµ‹åˆ°ç®€å•çš„é”™è¯¯ã€‚ä¸ºè§£å†³æ­¤å·®è·ï¼Œæˆ‘ä»¬æå‡ºHoarePromptï¼Œä¸€ç§ç»“åˆç¨‹åºåˆ†æä¸éªŒè¯åŸºæœ¬ç†å¿µçš„è‡ªç„¶è¯­è¨€å·¥ä»¶æ–°æ–¹æ³•ã€‚HoarePromptå—æœ€å¼ºåæ¡ä»¶æ¼”ç®—å¯å‘ï¼Œé‡‡ç”¨ç³»ç»Ÿã€é€æ­¥çš„è¿‡ç¨‹ï¼Œç”±LLMç”Ÿæˆä»£ç å„ç‚¹å¯è¾¾åˆ°çš„ç¨‹åºçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚ä¸ºå¤„ç†å¾ªç¯ï¼Œæˆ‘ä»¬æå‡ºåŸºäºå°‘é‡æ•°æ®çš„kå½’çº³æ³•ã€‚ä¸€æ—¦æè¿°äº†ç¨‹åºçŠ¶æ€ï¼ŒHoarePromptåˆ©ç”¨LLMè¯„ä¼°å¸¦æœ‰è¿™äº›çŠ¶æ€æè¿°çš„ç¨‹åºçš„è‡ªç„¶è¯­è¨€è¦æ±‚ç¬¦åˆç¨‹åº¦ã€‚ä¸ºè¯„ä¼°ç¨‹åºæ­£ç¡®æ€§åˆ†ç±»å™¨çš„è´¨é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†CoCoClaNeLæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„ç¼–ç¨‹ç«èµ›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒæ˜¾ç¤ºï¼Œä¸é›¶æ ·æœ¬CoTæç¤ºç›¸æ¯”ï¼ŒHoarePromptçš„MCCæé«˜äº†62%ã€‚æ­¤å¤–ï¼ŒHoarePromptä¼˜äºé€šè¿‡LLMæµ‹è¯•ç”Ÿæˆè¯„ä¼°æ­£ç¡®æ€§çš„åˆ†ç±»å™¨ï¼Œæé«˜äº†MCCçš„93%ã€‚å½’çº³æ¨ç†æœºåˆ¶å¯¹MCCçš„æå‡è´¡çŒ®ç‡ä¸º28%ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¾ªç¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éªŒè¯ç¨‹åºæ˜¯å¦ç¬¦åˆè‡ªç„¶è¯­è¨€è¦æ±‚æ˜¯ä¸€é¡¹å›°éš¾ä¸”ç ”ç©¶ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™ä¸€ä»»åŠ¡ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚</li>
<li>HoarePromptæ˜¯ä¸€ç§ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œç¨‹åºåˆ†æä¸éªŒè¯çš„æ–°æ–¹æ³•ã€‚</li>
<li>HoarePrompté‡‡ç”¨é€æ­¥ç”Ÿæˆç¨‹åºçŠ¶æ€è‡ªç„¶è¯­è¨€æè¿°çš„æ–¹å¼å¤„ç†ç¨‹åºã€‚</li>
<li>ä¸ºå¤„ç†å¾ªç¯ï¼ŒHoarePromptå¼•å…¥äº†åŸºäºå°‘é‡æ•°æ®çš„kå½’çº³æ³•ã€‚</li>
<li>è¯„ä¼°äº†HoarePromptåœ¨CoCoClaNeLæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9aac91c900d2b9664b95be1f1b5e1c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-483e45bef9eaa1c647c8a784d07e3ba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51be63da55cae30c60ebb0207831fe94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acb9ff9354c9a6291e2ec1bc9530388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30db18e69155615e68bc738105d9f4a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4210fc0b739d8df99f0cfa9f64c1cd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e88ae79be101f21e2b916b0c38d2145.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters"><a href="#Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters" class="headerlink" title="Flow to Learn: Flow Matching on Neural Network Parameters"></a>Flow to Learn: Flow Matching on Neural Network Parameters</h2><p><strong>Authors:Daniel Saragih, Deyu Cao, Tejas Balaji, Ashwin Santhosh</strong></p>
<p>Foundational language models show a remarkable ability to learn new concepts during inference via context data. However, similar work for images lag behind. To address this challenge, we introduce FLoWN, a flow matching model that learns to generate neural network parameters for different tasks. Our approach models the flow on latent space, while conditioning the process on context data. Experiments verify that FLoWN attains various desiderata for a meta-learning model. In addition, it matches or exceeds baselines on in-distribution tasks, provides better initializations for classifier training, and is performant on out-of-distribution few-shot tasks while having a fine-tuning mechanism to improve performance. </p>
<blockquote>
<p>åŸºç¡€è¯­è¨€æ¨¡å‹æ˜¾ç¤ºå‡ºåœ¨é€šè¿‡ä¸Šä¸‹æ–‡æ•°æ®æ¨ç†æ—¶å­¦ä¹ æ–°æ¦‚å¿µçš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå›¾åƒçš„ç±»ä¼¼å·¥ä½œæ»åäºè¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FLoWNï¼Œè¿™æ˜¯ä¸€ä¸ªæµåŒ¹é…æ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ ä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ½œåœ¨ç©ºé—´ä¸Šçš„æµå»ºæ¨¡ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œè¿‡ç¨‹æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒFLoWNæ»¡è¶³äº†å…ƒå­¦ä¹ æ¨¡å‹çš„å„ç§æœŸæœ›è¦æ±‚ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å†…éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŸºçº¿ç›¸å½“æˆ–è¶…è¿‡åŸºçº¿ï¼Œä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›äº†æ›´å¥½çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨å¤–éƒ¨åˆ†å¸ƒçš„å°‘æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶å…·æœ‰å¾®è°ƒæœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19371v1">PDF</a> Accepted at the ICLR Workshop on Neural Network Weights as a New Data   Modality 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFLoWNçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å­¦ä¹ åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„æµæ¥ç”Ÿæˆä¸åŒä»»åŠ¡çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼Œå¹¶åœ¨ä¸Šä¸‹æ–‡æ•°æ®çš„åŸºç¡€ä¸Šå¯¹æµç¨‹è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒè¯æ˜ï¼ŒFLoWNè¾¾åˆ°äº†å…ƒå­¦ä¹ æ¨¡å‹çš„å¤šç§ç†æƒ³è¦æ±‚ï¼Œå¹¶åœ¨å†…éƒ¨ä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¶Šäº†åŸºçº¿æ°´å¹³ï¼Œä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›äº†æ›´å¥½çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œå¹¶åœ¨å¤–éƒ¨å°‘é‡æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰å¾®è°ƒæœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLoWNæ¨¡å‹èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸Šå»ºæ¨¡æµï¼Œç”¨äºç”Ÿæˆä¸åŒä»»åŠ¡çš„ç¥ç»ç½‘ç»œå‚æ•°ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡æ•°æ®çš„åŸºç¡€ä¸Šå¯¹æµç¨‹è¿›è¡Œå»ºæ¨¡å’Œå­¦ä¹ æ–°æ¦‚å¿µã€‚</li>
<li>å®éªŒè¡¨æ˜FLoWNå…·å¤‡å¤šç§ç†æƒ³çš„å…ƒå­¦ä¹ æ¨¡å‹ç‰¹æ€§ã€‚</li>
<li>FLoWNåœ¨å†…éƒ¨ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†åŸºçº¿æ°´å¹³ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›äº†æ›´å¥½çš„åˆå§‹åŒ–æ–¹æ¡ˆã€‚</li>
<li>FLoWNåœ¨å¤–éƒ¨å°‘é‡æ ·æœ¬ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2b158347800b29121d12bf56b7440c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a298b0bd4ccabba9ec56434c9549e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125341decd46ab72a66b4d1e2ece85f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a604287f859f79a89a5a1c2701ffebf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0839fb81ce1617f3e1cadaa9383b320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff0b14f4cdf6fe914991acc36a6f7ddb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FRESA-Feedforward-Reconstruction-of-Personalized-Skinned-Avatars-from-Few-Images"><a href="#FRESA-Feedforward-Reconstruction-of-Personalized-Skinned-Avatars-from-Few-Images" class="headerlink" title="FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from   Few Images"></a>FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from   Few Images</h2><p><strong>Authors:Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh</strong></p>
<p>We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at <a target="_blank" rel="noopener" href="https://github.com/rongakowang/FRESA">https://github.com/rongakowang/FRESA</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»…é€šè¿‡å‡ å¼ å›¾ç‰‡é‡å»ºä¸ªæ€§åŒ–3Däººç±»è§’è‰²å¹¶ç”Ÿæˆé€¼çœŸåŠ¨ç”»çš„æ–°æ–¹æ³•ã€‚ç”±äºäººä½“å½¢çŠ¶ã€å§¿åŠ¿å’Œæœè£…ç±»å‹å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šéœ€è¦åœ¨æ¨ç†æœŸé—´å¯¹æ¯ä¸ªä¸»é¢˜è¿›è¡Œæ•°å°æ—¶çš„ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ä»æ•°åƒåç©¿è¡£è€…èº«ä¸Šå­¦ä¹ é€šç”¨å…ˆéªŒçŸ¥è¯†ï¼Œä»¥å®ç°å³æ—¶å‰é¦ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨é€šç”¨çš„è’™çš®æƒé‡æ¥å›ºå®šè§’è‰²ï¼Œè€Œæ˜¯è”åˆæ¨æ–­ä¸ªæ€§åŒ–çš„è§’è‰²å½¢çŠ¶ã€è’™çš®æƒé‡å’Œå§¿åŠ¿ç›¸å…³çš„å˜å½¢ï¼Œè¿™æœ‰æ•ˆåœ°æé«˜äº†æ•´ä½“å‡ ä½•é€¼çœŸåº¦å¹¶å‡å°‘äº†å˜å½¢ä¼ªå½±ã€‚æ­¤å¤–ï¼Œä¸ºäº†å½’ä¸€åŒ–å§¿åŠ¿å˜åŒ–å¹¶è§£å†³è§„èŒƒå½¢çŠ¶å’Œè’™çš®æƒé‡ä¹‹é—´çš„è€¦åˆæ¨¡ç³Šæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª3Dè§„èŒƒåŒ–è¿‡ç¨‹æ¥ç”Ÿæˆåƒç´ å¯¹é½çš„åˆå§‹æ¡ä»¶ï¼Œè¿™æœ‰åŠ©äºé‡å»ºç²¾ç»†çš„å‡ ä½•ç»†èŠ‚ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå¸§ç‰¹å¾èšåˆæ–¹æ³•ï¼Œä»¥ç¨³å¥åœ°å‡å°‘è§„èŒƒåŒ–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¼ªå½±ï¼Œå¹¶èåˆä¿ç•™ä¸ªäººç‰¹å¾çš„åˆç†è§’è‰²ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªå¤§è§„æ¨¡æ•è·æ•°æ®é›†ä¸Šï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸é«˜è´¨é‡3Dæ‰«æé…å¯¹çš„å¤šæ ·åŒ–äººç±»ä¸»é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„æŠ€æœ¯ç”Ÿæˆæ›´çœŸå®çš„ä¸‰ç»´é‡å»ºå’ŒåŠ¨ç”»ï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥æ¨å¹¿åˆ°æ¥è‡ªéšæ„æ‹æ‘„çš„æ‰‹æœºç…§ç‰‡è¾“å…¥ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/rongakowang/FRESA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/rongakowang/FRESAè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19207v1">PDF</a> Published in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»…é€šè¿‡å‡ å¼ å›¾ç‰‡é‡å»ºä¸ªæ€§åŒ–3Däººç±»è§’è‰²æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é€šç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°å³æ—¶å‰é¦ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡ä¼˜åŒ–æ—¶é—´çš„é—®é¢˜ã€‚é€šè¿‡è”åˆæ¨æ–­ä¸ªæ€§åŒ–è§’è‰²å½¢çŠ¶ã€è’™çš®æƒé‡å’Œå§¿æ€ç›¸å…³å˜å½¢ï¼Œæé«˜äº†æ•´ä½“å‡ ä½•é€¼çœŸåº¦ï¼Œå‡å°‘äº†å˜å½¢ä¼ªå½±ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡3Dè§„èŒƒåŒ–è¿‡ç¨‹å’Œå¤šå¸§ç‰¹å¾èšåˆï¼Œè§£å†³äº†å§¿æ€å˜åŒ–æ ‡å‡†åŒ–åŠæ¨¡ç³Šæ€§é—®é¢˜ï¼Œæé«˜äº†ç²¾ç»†å‡ ä½•ç»†èŠ‚çš„é‡å»ºæ•ˆæœã€‚æœ€åï¼Œåœ¨å¤§å‹æ•è·æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯æ¡†æ¶è®­ç»ƒï¼ŒåŒ…å«å¤šç§äººç±»ä¸»ä½“ä¸é«˜è´¨é‡3Dæ‰«æé…å¯¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æŠ€æœ¯æ›´èƒ½ç”ŸæˆçœŸå®çš„äººç‰©é‡å»ºå’ŒåŠ¨ç”»ï¼Œå¹¶èƒ½ç›´æ¥æ³›åŒ–åˆ°æ‰‹æœºç…§ç‰‡ç­‰è¾“å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ä»å°‘é‡å›¾ç‰‡é‡å»ºä¸ªæ€§åŒ–3Däººç±»è§’è‰²æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å­¦ä¹ é€šç”¨å…ˆéªŒçŸ¥è¯†å®ç°å³æ—¶å‰é¦ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–ï¼Œè§£å†³äº†ä¼˜åŒ–æ—¶é—´é•¿çš„é—®é¢˜ã€‚</li>
<li>è”åˆæ¨æ–­ä¸ªæ€§åŒ–è§’è‰²å½¢çŠ¶ã€è’™çš®æƒé‡å’Œå§¿æ€ç›¸å…³å˜å½¢ï¼Œæé«˜å‡ ä½•é€¼çœŸåº¦å’Œå‡å°‘å˜å½¢ä¼ªå½±ã€‚</li>
<li>è®¾è®¡äº†3Dè§„èŒƒåŒ–è¿‡ç¨‹å’Œå¤šå¸§ç‰¹å¾èšåˆï¼Œè§£å†³å§¿æ€å˜åŒ–æ ‡å‡†åŒ–åŠæ¨¡ç³Šæ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤§å‹æ•è·æ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯æ¡†æ¶è®­ç»ƒï¼ŒåŒ…å«å¤šç§äººç±»ä¸»ä½“ä¸é«˜è´¨é‡3Dæ‰«æé…å¯¹ã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•ç”Ÿæˆçš„äººç‰©é‡å»ºå’ŒåŠ¨ç”»æ›´ä¸ºçœŸå®ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6836aeb555f598f7167e011d4788b25d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b015e489b3d7e78163a2c17b2048d4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc991e939e499f783d3edf435c625b70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FACE-Few-shot-Adapter-with-Cross-view-Fusion-for-Cross-subject-EEG-Emotion-Recognition"><a href="#FACE-Few-shot-Adapter-with-Cross-view-Fusion-for-Cross-subject-EEG-Emotion-Recognition" class="headerlink" title="FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG   Emotion Recognition"></a>FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG   Emotion Recognition</h2><p><strong>Authors:Haiqi Liu, C. L. Philip Chen, Tong Zhang</strong></p>
<p>Cross-subject EEG emotion recognition is challenged by significant inter-subject variability and intricately entangled intra-subject variability. Existing works have primarily addressed these challenges through domain adaptation or generalization strategies. However, they typically require extensive target subject data or demonstrate limited generalization performance to unseen subjects. Recent few-shot learning paradigms attempt to address these limitations but often encounter catastrophic overfitting during subject-specific adaptation with limited samples. This article introduces the few-shot adapter with a cross-view fusion method called FACE for cross-subject EEG emotion recognition, which leverages dynamic multi-view fusion and effective subject-specific adaptation. Specifically, FACE incorporates a cross-view fusion module that dynamically integrates global brain connectivity with localized patterns via subject-specific fusion weights to provide complementary emotional information. Moreover, the few-shot adapter module is proposed to enable rapid adaptation for unseen subjects while reducing overfitting by enhancing adapter structures with meta-learning. Experimental results on three public EEG emotion recognition benchmarks demonstrate FACEâ€™s superior generalization performance over state-of-the-art methods. FACE provides a practical solution for cross-subject scenarios with limited labeled data. </p>
<blockquote>
<p>è·¨å­¦ç§‘çš„è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«é¢ä¸´ç€å—è¯•è€…é—´å’Œå—è¯•è€…å†…å­˜åœ¨æ˜¾è‘—å·®å¼‚çš„å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡é¢†åŸŸé€‚åº”æˆ–é€šç”¨åŒ–ç­–ç•¥æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„ç›®æ ‡å—è¯•è€…æ•°æ®ï¼Œæˆ–è€…åœ¨æœªè§è¿‡çš„å—è¯•è€…ä¸Šè¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–æ€§èƒ½ã€‚æœ€è¿‘çš„å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼è¯•å›¾è§£å†³è¿™äº›å±€é™æ€§ï¼Œä½†åœ¨å…·æœ‰æœ‰é™æ ·æœ¬çš„ç‰¹å®šä¸»ä½“é€‚åº”è¿‡ç¨‹ä¸­å¸¸å¸¸é­é‡ç¾éš¾æ€§çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºè·¨ä¸»ä½“è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«çš„å°‘æ ·æœ¬é€‚é…å™¨ä¸ä¸€ç§ç§°ä¸ºFACEçš„è·¨è§†å›¾èåˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŠ¨æ€å¤šè§†å›¾èåˆå’Œæœ‰æ•ˆçš„ä¸»ä½“ç‰¹å®šé€‚åº”ã€‚å…·ä½“æ¥è¯´ï¼ŒFACEç»“åˆäº†ä¸€ä¸ªè·¨è§†å›¾èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ä¸»ä½“ç‰¹å®šçš„èåˆæƒé‡åŠ¨æ€åœ°æ•´åˆå…¨å±€å¤§è„‘è¿é€šæ€§ä¸å±€éƒ¨æ¨¡å¼ï¼Œä»¥æä¾›äº’è¡¥çš„æƒ…æ„Ÿä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å°‘æ ·æœ¬é€‚é…å™¨æ¨¡å—ï¼Œä»¥å®ç°æœªè§ä¸»ä½“çš„å¿«é€Ÿé€‚åº”ï¼Œå¹¶é€šè¿‡å¢å¼ºå…ƒå­¦ä¹ çš„é€‚é…å™¨ç»“æ„æ¥å‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFACEåœ¨æœ€æ–°æŠ€æœ¯ä¸Šçš„æ³›åŒ–æ€§èƒ½è¡¨ç°ä¼˜è¶Šã€‚FACEä¸ºæœ‰é™æ ‡è®°æ•°æ®çš„è·¨ä¸»ä½“åœºæ™¯æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18998v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè·¨è§†å›¾èåˆæ–¹æ³•çš„å°‘æ ·æœ¬é€‚é…å™¨ï¼ˆFACEï¼‰åœ¨è·¨ä¸»ä½“è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«ä¸­çš„åº”ç”¨ã€‚å®ƒé‡‡ç”¨åŠ¨æ€å¤šè§†å›¾èåˆå’Œæœ‰æ•ˆçš„ä¸»ä½“ç‰¹å®šé€‚åº”ç­–ç•¥ï¼Œé€šè¿‡èåˆå…¨å±€è„‘è¿é€šæ€§å’Œå±€éƒ¨æ¨¡å¼æ¥æä¾›äº’è¡¥çš„æƒ…æ„Ÿä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå°‘æ ·æœ¬é€‚é…å™¨æ¨¡å—é€šè¿‡å…ƒå­¦ä¹ å¢å¼ºé€‚é…å™¨ç»“æ„ï¼Œä»¥å®ç°å¿«é€Ÿé€‚åº”æœªè§ä¸»ä½“å¹¶å‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚åœ¨ä¸‰ä¸ªå…¬å…±è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFACEåœ¨æ³›åŒ–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæœ‰é™æ ‡è®°æ•°æ®çš„è·¨ä¸»ä½“åœºæ™¯æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨ä¸»ä½“è„‘ç”µå›¾æƒ…æ„Ÿè¯†åˆ«é¢ä¸´æ˜¾è‘—çš„ä¸»ä½“é—´å˜å¼‚å’Œå¤æ‚çš„ä¸»ä½“å†…å˜å¼‚æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦é€šè¿‡é¢†åŸŸé€‚åº”æˆ–æ³›åŒ–ç­–ç•¥æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½†éœ€è¦å¤§é‡çš„ç›®æ ‡ä¸»ä½“æ•°æ®ï¼Œæˆ–å¯¹æœªè§ä¸»ä½“çš„æ³›åŒ–æ€§èƒ½æœ‰é™ã€‚</li>
<li>æœ€è¿‘å‡ºç°çš„å°‘æ ·æœ¬å­¦ä¹ èŒƒå¼è¯•å›¾è§£å†³è¿™äº›é™åˆ¶ï¼Œä½†åœ¨ä¸»ä½“ç‰¹å®šé€‚åº”æ—¶ï¼Œç”±äºæ ·æœ¬æœ‰é™ï¼Œå¸¸é­é‡ç¾éš¾æ€§çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>æœ¬æ–‡ä»‹ç»çš„å°‘æ ·æœ¬é€‚é…å™¨ï¼ˆFACEï¼‰é‡‡ç”¨åŠ¨æ€å¤šè§†å›¾èåˆç­–ç•¥ï¼Œç»“åˆå…¨å±€è„‘è¿é€šæ€§å’Œå±€éƒ¨æ¨¡å¼ï¼Œæä¾›äº’è¡¥æƒ…æ„Ÿä¿¡æ¯ã€‚<br>5.FACEé€šè¿‡å…ƒå­¦ä¹ å¢å¼ºé€‚é…å™¨ç»“æ„ï¼Œä»¥å®ç°å¿«é€Ÿé€‚åº”æœªè§ä¸»ä½“å¹¶å‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFACEåœ¨æ³›åŒ–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dbe02d900c030abf2254cd438228f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-426ca2078a78a65ff673d1870affaf4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2168448fe44003cb090964fce45e4e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting"><a href="#NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting" class="headerlink" title="NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting"></a>NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting</h2><p><strong>Authors:Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du</strong></p>
<p>Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably advanced photo-realistic novel view synthesis using images from densely spaced camera viewpoints. However, these methods struggle in few-shot scenarios due to limited supervision. In this paper, we present NexusGS, a 3DGS-based approach that enhances novel view synthesis from sparse-view images by directly embedding depth information into point clouds, without relying on complex manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our method introduces a novel point cloud densification strategy that initializes 3DGS with a dense point cloud, reducing randomness in point placement while preventing over-smoothing and overfitting. Specifically, NexusGS comprises three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses to compute accurate depth maps, while mitigating the inaccuracies often associated with optical flow. By incorporating epipolar depth priors, NexusGS ensures reliable dense point cloud coverage and supports stable 3DGS training under sparse-view conditions. Experiments demonstrate that NexusGS significantly enhances depth accuracy and rendering quality, surpassing state-of-the-art methods by a considerable margin. Furthermore, we validate the superiority of our generated point clouds by substantially boosting the performance of competing methods. Project page: <a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/">https://usmizuki.github.io/NexusGS/</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯è´´ç‰‡ï¼ˆ3DGSï¼‰åˆ©ç”¨å¯†é›†é—´éš”ç›¸æœºè§†è§’çš„å›¾åƒï¼Œæ˜¾è‘—åœ°æ¨è¿›äº†é€¼çœŸçš„æ–°å‹è§†å›¾åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬çš„åœºæ™¯ä¸­ç”±äºç›‘ç£æœ‰é™è€Œè¡¨ç°ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NexusGSï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç›´æ¥å°†æ·±åº¦ä¿¡æ¯åµŒå…¥ç‚¹äº‘ä¸­ï¼Œæé«˜äº†ä»ç¨€ç–è§†å›¾å›¾åƒè¿›è¡Œçš„æ–°å‹è§†å›¾åˆæˆçš„è´¨é‡ï¼Œè€Œæ— éœ€ä¾èµ–å¤æ‚çš„æ‰‹åŠ¨æ­£åˆ™åŒ–ã€‚åˆ©ç”¨3DGSçš„å›ºæœ‰æçº¿å‡ ä½•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„ç‚¹äº‘å¯†é›†åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨å¯†é›†çš„ç‚¹äº‘åˆå§‹åŒ–3DGSï¼Œå‡å°‘äº†ç‚¹æ”¾ç½®çš„éšæœºæ€§ï¼ŒåŒæ—¶é˜²æ­¢è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚å…·ä½“æ¥è¯´ï¼ŒNexusGSåŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šæçº¿æ·±åº¦Nexusã€æµé€‚åº”æ·±åº¦æ··åˆå’Œæµè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚è¿™äº›æ­¥éª¤åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€æ¥è®¡ç®—å‡†ç¡®çš„æ·±åº¦å›¾ï¼ŒåŒæ—¶å‡è½»ä¸å…‰å­¦æµé€šå¸¸ç›¸å…³çš„è¯¯å·®ã€‚é€šè¿‡å¼•å…¥æçº¿æ·±åº¦å…ˆéªŒï¼ŒNexusGSç¡®ä¿å¯é ä¸”å¯†é›†çš„ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹æ”¯æŒç¨³å®šçš„3DGSè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNexusGSæ˜¾è‘—æé«˜äº†æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œå¤§å¤§è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤§å¹…æå‡ç«äº‰å¯¹æ‰‹æ–¹æ³•çš„æ€§èƒ½æ¥éªŒè¯äº†æˆ‘ä»¬ç”Ÿæˆç‚¹äº‘çš„ä¼˜è¶Šæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/%E3%80%82">https://usmizuki.github.io/NexusGS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18794v1">PDF</a> This paper is accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>NeRFå’Œ3DGSåœ¨å¯†é›†ç›¸æœºè§†è§’çš„å›¾åƒä¸Šå®ç°äº†é€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œä½†åœ¨å°‘é‡å›¾åƒçš„æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºNexusGSï¼Œä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥åœ¨ç‚¹äº‘ä¸­åµŒå…¥æ·±åº¦ä¿¡æ¯ï¼Œæé«˜ç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨3DGSçš„å›ºæœ‰æå‡ ä½•ç‰¹æ€§ï¼Œå¼•å…¥æ–°å‹ç‚¹äº‘å¯†é›†åŒ–ç­–ç•¥ï¼Œä»¥å¯†é›†ç‚¹äº‘åˆå§‹åŒ–3DGSï¼Œå‡å°‘ç‚¹æ”¾ç½®çš„éšæœºæ€§ï¼Œé˜²æ­¢è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚NexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æŠ—æ¼‚ç§»æ·±åº¦æ··åˆå’Œæµè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚è¿™äº›æ–¹æ³•åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€è®¡ç®—å‡†ç¡®æ·±åº¦å›¾ï¼Œç¼“è§£ä¸å…‰å­¦æµç›¸å…³çš„ä¸å‡†ç¡®æ€§ã€‚é€šè¿‡ç»“åˆææ·±åº¦å…ˆéªŒï¼ŒNexusGSç¡®ä¿å¯é çš„å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–è§†è§’æ¡ä»¶ä¸‹å®ç°ç¨³å®šçš„3DGSè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNexusGSæ˜¾è‘—æé«˜æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œå¤§å¤§è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFå’Œ3DGSåœ¨å¯†é›†ç›¸æœºè§†è§’çš„å›¾åƒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å°‘é‡å›¾åƒçš„æƒ…å†µä¸‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>NexusGSæ˜¯ä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚</li>
<li>NexusGSåˆ©ç”¨3DGSçš„å›ºæœ‰æå‡ ä½•ç‰¹æ€§ï¼Œé€šè¿‡å¯†é›†ç‚¹äº‘åˆå§‹åŒ–ï¼Œå‡å°‘ç‚¹æ”¾ç½®çš„éšæœºæ€§ã€‚</li>
<li>NexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æŠ—æ¼‚ç§»æ·±åº¦æ··åˆå’Œæµè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚</li>
<li>NexusGSåˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€è®¡ç®—æ·±åº¦å›¾ï¼Œç¼“è§£ä¸å…‰å­¦æµç›¸å…³çš„ä¸å‡†ç¡®æ€§ã€‚</li>
<li>NexusGSé€šè¿‡ç»“åˆææ·±åº¦å…ˆéªŒï¼Œç¡®ä¿å¯é çš„å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå®ç°ç¨³å®šè®­ç»ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒNexusGSåœ¨æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d91262626222a26061a4d223c84fbca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93ed83d2b2f76ab2cb97bd4a5f9f65b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a47a15036cbec97db110b1320e4f846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf847bb4e3c6ffe7de25a2effa479071.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CFReID-Continual-Few-shot-Person-Re-Identification"><a href="#CFReID-Continual-Few-shot-Person-Re-Identification" class="headerlink" title="CFReID: Continual Few-shot Person Re-Identification"></a>CFReID: Continual Few-shot Person Re-Identification</h2><p><strong>Authors:Hao Ni, Lianli Gao, Pengpeng Zeng, Heng Tao Shen, Jingkuan Song</strong></p>
<p>Real-world surveillance systems are dynamically evolving, requiring a person Re-identification model to continuously handle newly incoming data from various domains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed to learn and accumulate knowledge across multiple domains incrementally. However, LReID models need to be trained on large-scale labeled data for each unseen domain, which are typically inaccessible due to privacy and cost concerns. In this paper, we propose a new paradigm called Continual Few-shot ReID (CFReID), which requires models to be incrementally trained using few-shot data and tested on all seen domains. Under few-shot conditions, CFREID faces two core challenges: 1) learning knowledge from few-shot data of unseen domain, and 2) avoiding catastrophic forgetting of seen domains. To tackle these two challenges, we propose a Stable Distribution Alignment (SDA) framework from feature distribution perspective. Specifically, our SDA is composed of two modules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot Adaptation (PFA). To support the study of CFReID, we establish an evaluation benchmark for CFReID on five publicly available ReID datasets. Extensive experiments demonstrate that our SDA can enhance the few-shot learning and anti-forgetting capabilities under few-shot conditions. Notably, our approach, using only 5% of the data, i.e., 32 IDs, significantly outperforms LReIDâ€™s state-of-the-art performance, which requires 700 to 1,000 IDs. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œç›‘æ§ç³»ç»Ÿæ­£åœ¨åŠ¨æ€æ¼”å˜ï¼Œéœ€è¦è¡Œäººé‡è¯†åˆ«æ¨¡å‹æŒç»­å¤„ç†æ¥è‡ªä¸åŒé¢†åŸŸçš„æ–°æ•°æ®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›åŠ¨æ€å˜åŒ–ï¼Œæå‡ºäº†ç»ˆèº«ReIDï¼ˆLReIDï¼‰ä»¥å¢é‡æ–¹å¼åœ¨ä¸åŒé¢†åŸŸå­¦ä¹ å’Œç§¯ç´¯çŸ¥è¯†ã€‚ç„¶è€Œï¼ŒLReIDæ¨¡å‹éœ€è¦åœ¨æ¯ä¸ªæœªè§é¢†åŸŸçš„å¤§è§„æ¨¡æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”±äºéšç§å’Œæˆæœ¬æ‹…å¿§ï¼Œè¿™äº›é€šå¸¸æ— æ³•è·å¾—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæŒç»­å°æ ·æœ¬ReIDï¼ˆCFReIDï¼‰ï¼Œéœ€è¦æ¨¡å‹ä½¿ç”¨å°æ ·æœ¬æ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒï¼Œå¹¶åœ¨æ‰€æœ‰å·²è§é¢†åŸŸä¸Šè¿›è¡Œæµ‹è¯•ã€‚åœ¨å°æ ·æœ¬æ¡ä»¶ä¸‹ï¼ŒCFReIDé¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š1ï¼‰ä»æœªè§é¢†åŸŸçš„å°æ ·æœ¬æ•°æ®ä¸­å­¦ä¹ çŸ¥è¯†ï¼Œ2ï¼‰é¿å…å·²è§é¢†åŸŸçš„ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»ç‰¹å¾åˆ†å¸ƒçš„è§’åº¦æå‡ºäº†ç¨³å®šåˆ†å¸ƒå¯¹é½ï¼ˆSDAï¼‰æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„SDAç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼Œå³å…ƒåˆ†å¸ƒå¯¹é½ï¼ˆMDAï¼‰å’ŒåŸºäºåŸå‹çš„å°‘æ ·æœ¬é€‚åº”ï¼ˆPFAï¼‰ã€‚ä¸ºäº†æ”¯æŒå¯¹CFReIDçš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨äº”ä¸ªå…¬å¼€çš„ReIDæ•°æ®é›†ä¸Šå»ºç«‹äº†CFReIDè¯„ä¼°åŸºå‡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SDAå¯ä»¥å¢å¼ºå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›å’ŒæŠ—é—å¿˜èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨5%çš„æ•°æ®ï¼Œå³32ä¸ªIDï¼Œå°±æ˜¾è‘—ä¼˜äºLReIDçš„æœ€æ–°æ€§èƒ½ï¼Œåè€…éœ€è¦700åˆ°1000ä¸ªIDã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18469v1">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒç»­å°‘æ ·æœ¬é‡è¯†åˆ«ï¼ˆCFReIDï¼‰æ–¹æ³•ï¼Œç”¨äºè§£å†³ç°å®ä¸–ç•Œç›‘æ§ç³»ç»Ÿä¸­çš„åŠ¨æ€å˜åŒ–é—®é¢˜ã€‚CFReIDè¦æ±‚æ¨¡å‹åœ¨å°‘é‡æ•°æ®ä¸Šå¢é‡å­¦ä¹ ï¼Œå¹¶æµ‹è¯•åœ¨æ‰€æœ‰å·²è§é¢†åŸŸä¸Šçš„è¡¨ç°ã€‚ä¸ºè§£å†³ä»æœªè§é¢†åŸŸçš„å°‘é‡æ•°æ®å­¦ä¹ å’Œé¿å…å·²è§é¢†åŸŸçš„ç¾éš¾æ€§é—å¿˜è¿™ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ç¨³å®šåˆ†å¸ƒå¯¹é½ï¼ˆSDAï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬å…ƒåˆ†å¸ƒå¯¹é½ï¼ˆMDAï¼‰å’ŒåŸºäºåŸå‹çš„å°‘é‡é€‚åº”ï¼ˆPFAï¼‰ä¸¤ä¸ªæ¨¡å—ã€‚åœ¨äº”ä¸ªå…¬å¼€å¯ç”¨çš„ReIDæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSDAå¢å¼ºäº†å°‘æ ·æœ¬å­¦ä¹ çš„æŠ—é—å¿˜èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†æŒç»­å°‘æ ·æœ¬é‡è¯†åˆ«ï¼ˆCFReIDï¼‰æ–¹æ³•ï¼Œé€‚åº”åŠ¨æ€å˜åŒ–çš„ç›‘æ§ç³»ç»Ÿéœ€æ±‚ã€‚</li>
<li>CFReIDé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä»æœªè§é¢†åŸŸçš„å°‘é‡æ•°æ®å­¦ä¹ å’Œé¿å…å·²è§é¢†åŸŸçš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ç¨³å®šåˆ†å¸ƒå¯¹é½ï¼ˆSDAï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬MDAå’ŒPFAä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>SDAæ¡†æ¶é€šè¿‡ç‰¹å¾åˆ†å¸ƒè§’åº¦å¢å¼ºå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡å»ºç«‹äº†CFReIDçš„è¯„ä¼°åŸºå‡†ï¼Œå¹¶åœ¨äº”ä¸ªå…¬å¼€ReIDæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSDAæ¡†æ¶æ˜¾è‘—æå‡äº†å°‘æ ·æœ¬å­¦ä¹ å’ŒæŠ—é—å¿˜èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-181efb3a6ffadb77c58fc43c18d46ccd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23d159b2212784dfac0aa101e9061ec6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b50cb77a7412c26a24d390d875f7443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae73ac3e93a37b4170c594a30395a4b9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graph-Enhanced-Generative-Multi-modal-Models-for-Class-Incremental-Learning"><a href="#Knowledge-Graph-Enhanced-Generative-Multi-modal-Models-for-Class-Incremental-Learning" class="headerlink" title="Knowledge Graph Enhanced Generative Multi-modal Models for   Class-Incremental Learning"></a>Knowledge Graph Enhanced Generative Multi-modal Models for   Class-Incremental Learning</h2><p><strong>Authors:Xusheng Cao, Haori Lu, Linlan Huang, Fei Yang, Xialei Liu, Ming-Ming Cheng</strong></p>
<p>Continual learning in computer vision faces the critical challenge of catastrophic forgetting, where models struggle to retain prior knowledge while adapting to new tasks. Although recent studies have attempted to leverage the generalization capabilities of pre-trained models to mitigate overfitting on current tasks, models still tend to forget details of previously learned categories as tasks progress, leading to misclassification. To address these limitations, we introduce a novel Knowledge Graph Enhanced Generative Multi-modal model (KG-GMM) that builds an evolving knowledge graph throughout the learning process. Our approach utilizes relationships within the knowledge graph to augment the class labels and assigns different relations to similar categories to enhance model differentiation. During testing, we propose a Knowledge Graph Augmented Inference method that locates specific categories by analyzing relationships within the generated text, thereby reducing the loss of detailed information about old classes when learning new knowledge and alleviating forgetting. Experiments demonstrate that our method effectively leverages relational information to help the model correct mispredictions, achieving state-of-the-art results in both conventional CIL and few-shot CIL settings, confirming the efficacy of knowledge graphs at preserving knowledge in the continual learning scenarios. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰ä¸­çš„æŒç»­å­¦ä¹ é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜çš„å…³é”®æŒ‘æˆ˜ï¼Œæ¨¡å‹åœ¨é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œå¾ˆéš¾ä¿ç•™å…ˆéªŒçŸ¥è¯†ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜è¯•å›¾åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥ç¼“è§£å¯¹å½“å‰ä»»åŠ¡çš„è¿‡åº¦æ‹Ÿåˆï¼Œä½†éšç€ä»»åŠ¡çš„è¿›å±•ï¼Œæ¨¡å‹ä»ç„¶å€¾å‘äºå¿˜è®°å…ˆå‰å­¦ä¹ ç±»åˆ«çš„ç»†èŠ‚ï¼Œä»è€Œå¯¼è‡´è¯¯åˆ†ç±»ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çŸ¥è¯†å›¾è°±å¢å¼ºç”Ÿæˆå¤šæ¨¡æ€æ¨¡å‹ï¼ˆKG-GMMï¼‰ï¼Œå®ƒåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ„å»ºäº†ä¸€ä¸ªä¸æ–­è¿›åŒ–çš„çŸ¥è¯†å›¾è°±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨çŸ¥è¯†å›¾è°±å†…çš„å…³ç³»æ¥å¢å¼ºç±»åˆ«æ ‡ç­¾ï¼Œå¹¶ä¸ºç›¸ä¼¼ç±»åˆ«åˆ†é…ä¸åŒçš„å…³ç³»ä»¥å¢å¼ºæ¨¡å‹çš„åŒºåˆ«èƒ½åŠ›ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åˆ†æç”Ÿæˆæ–‡æœ¬å†…çš„å…³ç³»æ¥å®šä½ç‰¹å®šç±»åˆ«ï¼Œä»è€Œå‡å°‘åœ¨å­¦ä¹ æ–°çŸ¥è¯†æ—¶ä¸¢å¤±æ—§ç±»çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ç¼“è§£é—å¿˜ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†å…³ç³»ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹çº æ­£è¯¯é¢„æµ‹ï¼Œåœ¨å¸¸è§„CILå’Œå°‘æ ·æœ¬CILè®¾ç½®ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œè¯å®äº†çŸ¥è¯†å›¾è°±åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ä¿ç•™çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18403v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹è®¡ç®—æœºè§†è§‰ä¸­çš„æŒç»­å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜â€”â€”ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæ–‡ä¸­æå‡ºä¸€ç§æ–°å‹çŸ¥è¯†å›¾è°±å¢å¼ºç”Ÿæˆå¤šæ¨¡æ€æ¨¡å‹ï¼ˆKG-GMMï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±æ¥åº”å¯¹å­¦ä¹ è¿‡ç¨‹ä¸­çš„é—å¿˜é—®é¢˜ã€‚KG-GMMåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å…³ç³»æ¥å¢å¼ºç±»æ ‡ç­¾ï¼Œå¹¶ä¸ºç›¸ä¼¼ç±»åˆ«åˆ†é…ä¸åŒçš„å…³ç³»ä»¥å¢å¼ºæ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†æ–¹æ³•ï¼Œåˆ†æç”Ÿæˆçš„æ–‡æœ¬ä¸­çš„å…³ç³»æ¥ç¡®å®šç‰¹å®šç±»åˆ«ï¼Œä»è€Œå‡å°‘å­¦ä¹ æ–°çŸ¥è¯†æ—¶æ—§ç±»è¯¦ç»†ä¿¡æ¯çš„ä¸¢å¤±ï¼Œå¹¶ç¼“è§£é—å¿˜é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨å…³ç³»ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹çº æ­£è¯¯é¢„æµ‹ï¼Œåœ¨å¸¸è§„å’Œå°‘æ ·æœ¬æŒç»­å­¦ä¹ åœºæ™¯ä¸­å‡å–å¾—æœ€æ–°æˆæœï¼Œè¯å®äº†çŸ¥è¯†å›¾è°±åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ä¿ç•™çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰ä¸­çš„æŒç»­å­¦ä¹ é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å°è¯•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥ç¼“è§£å¯¹å½“å‰ä»»åŠ¡çš„è¿‡åº¦æ‹Ÿåˆï¼Œä½†æ¨¡å‹ä»ç„¶å®¹æ˜“å¿˜è®°å…ˆå‰å­¦åˆ°çš„ç±»åˆ«çš„ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çŸ¥è¯†å›¾è°±å¢å¼ºç”Ÿæˆå¤šæ¨¡æ€æ¨¡å‹ï¼ˆKG-GMMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±æ¥åº”å¯¹å­¦ä¹ è¿‡ç¨‹ä¸­çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>KG-GMMåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å…³ç³»æ¥å¢å¼ºç±»æ ‡ç­¾ï¼Œå¹¶ä¸ºç›¸ä¼¼ç±»åˆ«åˆ†é…ä¸åŒçš„å…³ç³»ä»¥å¢å¼ºæ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§çŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†æ–¹æ³•ï¼Œé€šè¿‡è¯¥æ–¹æ³•å¯ä»¥åˆ†æç”Ÿæˆçš„æ–‡æœ¬ä¸­çš„å…³ç³»æ¥ç¡®å®šç‰¹å®šç±»åˆ«ï¼Œä»è€Œå‡å°‘æ—§ç±»è¯¦ç»†ä¿¡æ¯çš„ä¸¢å¤±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒKG-GMMåœ¨å¸¸è§„å’Œå°‘æ ·æœ¬æŒç»­å­¦ä¹ åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œåˆ©ç”¨å…³ç³»ä¿¡æ¯æœ‰æ•ˆçº æ­£è¯¯é¢„æµ‹ã€‚</li>
<li>çŸ¥è¯†å›¾è°±å¯¹äºåœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ä¿ç•™çŸ¥è¯†å…·æœ‰æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17cb644a0d7dfc9f91de7023c01d44cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86db6ed15c8fc57b5b2f2c00d2936cb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24dc41570cdc96c8fbaac9261f9c110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c4b81c742dad7088b5b9d4e556c5cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-046e2887ecd8c26f8577028a89f8f681.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PNN-A-Novel-Progressive-Neural-Network-for-Fault-Classification-in-Rotating-Machinery-under-Small-Dataset-Constraint"><a href="#PNN-A-Novel-Progressive-Neural-Network-for-Fault-Classification-in-Rotating-Machinery-under-Small-Dataset-Constraint" class="headerlink" title="PNN: A Novel Progressive Neural Network for Fault Classification in   Rotating Machinery under Small Dataset Constraint"></a>PNN: A Novel Progressive Neural Network for Fault Classification in   Rotating Machinery under Small Dataset Constraint</h2><p><strong>Authors:Praveen Chopra, Himanshu Kumar, Sandeep Yadav</strong></p>
<p>Fault detection in rotating machinery is a complex task, particularly in small and heterogeneous dataset scenarios. Variability in sensor placement, machinery configurations, and structural differences further increase the complexity of the problem. Conventional deep learning approaches often demand large, homogeneous datasets, limiting their applicability in data-scarce industrial environments. While transfer learning and few-shot learning have shown potential, however, they are often constrained by the need for extensive fault datasets. This research introduces a unified framework leveraging a novel progressive neural network (PNN) architecture designed to address these challenges. The PNN sequentially estimates the fixed-size refined features of the higher order with the help of all previously estimated features and appends them to the feature set. This fixed-size feature output at each layer controls the complexity of the PNN and makes it suitable for effective learning from small datasets. The frameworkâ€™s effectiveness is validated on eight datasets, including six open-source datasets, one in-house fault simulator, and one real-world industrial dataset. The PNN achieves state-of-the-art performance in fault detection across varying dataset sizes and machinery types, highlighting superior generalization and classification capabilities. </p>
<blockquote>
<p>æ—‹è½¬æœºæ¢°æ•…éšœè¯Šæ–­æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹å’Œå¼‚æ„æ•°æ®é›†åœºæ™¯ä¸­ã€‚ä¼ æ„Ÿå™¨æ”¾ç½®ã€æœºæ¢°é…ç½®å’Œç»“æ„å·®å¼‚çš„å¯å˜æ€§è¿›ä¸€æ­¥å¢åŠ äº†é—®é¢˜çš„å¤æ‚æ€§ã€‚ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§å‹ã€åŒè´¨çš„æ•°æ®é›†ï¼Œè¿™åœ¨æ•°æ®ç¨€ç¼ºçš„å·¥ä¸šç¯å¢ƒä¸­é™åˆ¶äº†å…¶åº”ç”¨ã€‚å°½ç®¡è¿ç§»å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ å·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€å—åˆ°éœ€è¦å¤§é‡æ•…éšœæ•°æ®é›†çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œåˆ©ç”¨äº†ä¸€ç§æ–°å‹æ¸è¿›ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰æ¶æ„æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚PNNå€ŸåŠ©æ‰€æœ‰å…ˆå‰ä¼°è®¡çš„ç‰¹å¾é¡ºåºåœ°ä¼°è®¡é«˜é˜¶çš„å›ºå®šå¤§å°ç²¾ç»†ç‰¹å¾ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç‰¹å¾é›†ä¸­ã€‚æ¯å±‚å›ºå®šçš„ç‰¹å¾è¾“å‡ºæ§åˆ¶PNNçš„å¤æ‚æ€§ï¼Œä½¿å…¶é€‚åˆä»å°è§„æ¨¡æ•°æ®é›†ä¸­è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼ŒåŒ…æ‹¬å…­ä¸ªå¼€æºæ•°æ®é›†ã€ä¸€ä¸ªå†…éƒ¨æ•…éšœæ¨¡æ‹Ÿå™¨å’Œä¸€ä¸ªçœŸå®å·¥ä¸šæ•°æ®é›†ã€‚PNNåœ¨ä¸åŒæ•°æ®é›†å¤§å°å’Œæœºæ¢°ç±»å‹ä¸Šå®ç°äº†æ•…éšœæ£€æµ‹çš„æœ€æ–°æ€§èƒ½ï¼Œçªå‡ºäº†å…¶å‡ºè‰²çš„æ³›åŒ–å’Œåˆ†ç±»èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18263v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—‹è½¬æœºæ¢°æ•…éšœæ£€æµ‹æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å°æ ·æœ¬å’Œå¼‚æ„å›¾æ•°æ®é›†ä¸­æ›´å…·æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ©ç”¨æ–°é¢–æ¸è¿›ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰æ¶æ„çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚PNNèƒ½å¤Ÿåˆ©ç”¨å…ˆå‰ä¼°è®¡çš„ç‰¹å¾æ¥ä¼°è®¡æ›´é«˜é˜¶çš„å›ºå®šå¤§å°ç²¾ç»†ç‰¹å¾ï¼Œå¹¶è¿½åŠ åˆ°ç‰¹å¾é›†ä¸­ã€‚æ¡†æ¶çš„æœ‰æ•ˆæ€§åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ï¼ŒåŒ…æ‹¬å…­ä¸ªå¼€æºæ•°æ®é›†ã€ä¸€ä¸ªå†…éƒ¨æ•…éšœæ¨¡æ‹Ÿå™¨å’Œä¸€ä¸ªçœŸå®çš„å·¥ä¸šæ•°æ®é›†ã€‚PNNåœ¨æ•…éšœæ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œçªæ˜¾äº†å…¶å‡ºè‰²çš„æ³›åŒ–å’Œåˆ†ç±»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—‹è½¬æœºæ¢°æ•…éšœæ£€æµ‹é¢ä¸´å¤æ‚æ€§å’Œæ•°æ®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„å·¥ä¸šç¯å¢ƒä¸­é€‚ç”¨æ€§æœ‰é™ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºæ–°é¢–æ¸è¿›ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰çš„ç»Ÿä¸€æ¡†æ¶æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>PNNèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨å…ˆå‰ä¼°è®¡çš„ç‰¹å¾æ¥ä¼°è®¡æ›´é«˜é˜¶çš„å›ºå®šå¤§å°ç²¾ç»†ç‰¹å¾ã€‚</li>
<li>PNNæ¶æ„é€‚åˆä»å°è§„æ¨¡æ•°æ®é›†ä¸­è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae059db59594581cfcd9b5a3d6c9cd73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84f0d3dc5d63a7d9004c24177d96279e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76a68eb9a0bbbd0cc8013a16964c1f4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f079af23281e76beeb07ebb62d2a44ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b4eecec766d3d42d7fd54f9f820f51a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e13b3174969853cbb34d22a7edeaa6c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ba5f644db0ac5aed2c086ab74db4c3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FS-SS-Few-Shot-Learning-for-Fast-and-Accurate-Spike-Sorting-of-High-channel-Count-Probes"><a href="#FS-SS-Few-Shot-Learning-for-Fast-and-Accurate-Spike-Sorting-of-High-channel-Count-Probes" class="headerlink" title="FS-SS: Few-Shot Learning for Fast and Accurate Spike Sorting of   High-channel Count Probes"></a>FS-SS: Few-Shot Learning for Fast and Accurate Spike Sorting of   High-channel Count Probes</h2><p><strong>Authors:Tao Fang, Majid Zamani</strong></p>
<p>There is a need for fast adaptation in spike sorting algorithms to implement brain-machine interface (BMIs) in different applications. Learning and adapting the functionality of the sorting process in real-time can significantly improve the performance. However, deep neural networks (DNNs) depend on large amounts of data for training models and their performance sustainability decreases when data is limited. Inspired by meta-learning, this paper proposes a few-shot spike sorting framework (FS-SS) with variable network model size that requires minimal learning training and supervision. The framework is not only compatible with few-shot adaptations, but also it uses attention mechanism and dilated convolutional neural networks. This allows scaling the network parameters to learn the important features of spike signals and to quickly generalize the learning ability to new spike waveforms in recording channels after few observations. The FS-SS was evaluated by using freely accessible datasets, also compared with the other state-of-the-art algorithms. The average classification accuracy of the proposed method is 99.28%, which shows extreme robustness to background noise and similarity of the spike waveforms. When the number of training samples is reduced by 90%, the parameter scale is reduced by 68.2%, while the accuracy only decreased by 0.55%. The paper also visualizes the modelâ€™s attention distribution under spike sorting tasks of different difficulty levels. The attention distribution results show that the proposed model has clear interpretability and high robustness. </p>
<blockquote>
<p>å¯¹äºåœ¨ä¸åŒåº”ç”¨ä¸­å®æ–½è„‘æœºæ¥å£ï¼ˆBMIsï¼‰çš„è„‰å†²æ’åºç®—æ³•ï¼Œéœ€è¦å¿«é€Ÿé€‚åº”ã€‚å®æ—¶å­¦ä¹ å’Œé€‚åº”æ’åºè¿‡ç¨‹çš„åŠŸèƒ½å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä¾èµ–äºå¤§é‡æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå½“æ•°æ®é‡æœ‰é™æ—¶ï¼Œå…¶æ€§èƒ½å¯æŒç»­æ€§ä¼šé™ä½ã€‚æœ¬æ–‡å—å…ƒå­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ ·æœ¬é‡å°çš„è„‰å†²æ’åºæ¡†æ¶ï¼ˆFS-SSï¼‰ï¼Œè¯¥æ¡†æ¶å…·æœ‰å¯å˜ç½‘ç»œæ¨¡å‹å¤§å°ï¼Œéœ€è¦çš„å­¦ä¹ è®­ç»ƒå’Œç›‘ç£æœ€å°‘ã€‚è¯¥æ¡†æ¶ä¸ä»…å…¼å®¹å°æ ·æœ¬é€‚åº”ï¼Œè¿˜é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œã€‚è¿™å…è®¸åœ¨å°‘é‡è§‚å¯Ÿåï¼Œæ ¹æ®ç½‘ç»œå‚æ•°è°ƒæ•´å­¦ä¹ é‡è¦ç‰¹å¾è„‰å†²ä¿¡å·çš„èƒ½åŠ›ï¼Œå¹¶è¿…é€Ÿå°†å­¦ä¹ èƒ½åŠ›æ¨å¹¿åˆ°æ–°çš„è„‰å†²æ³¢å½¢è®°å½•é€šé“ã€‚é€šè¿‡ä½¿ç”¨å¯è‡ªç”±è®¿é—®çš„æ•°æ®é›†å¯¹FS-SSè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸å…¶å®ƒæœ€æ–°ç®—æ³•è¿›è¡Œæ¯”è¾ƒã€‚æ‰€ææ–¹æ³•çš„å¹³å‡åˆ†ç±»å‡†ç¡®ç‡ä¸º99.28%ï¼Œæ˜¾ç¤ºå‡ºå¯¹èƒŒæ™¯å™ªå£°å’Œè„‰å†²æ³¢å½¢ç›¸ä¼¼æ€§çš„æé«˜ç¨³å¥æ€§ã€‚å½“è®­ç»ƒæ ·æœ¬æ•°é‡å‡å°‘90%æ—¶ï¼Œå‚æ•°è§„æ¨¡å‡å°‘äº†68.2%ï¼Œè€Œå‡†ç¡®ç‡ä»…ä¸‹é™äº†0.55%ã€‚è®ºæ–‡è¿˜å¯è§†åŒ–æ¨¡å‹åœ¨ä¸åŒéš¾åº¦çº§åˆ«è„‰å†²æ’åºä»»åŠ¡ä¸‹çš„æ³¨æ„åŠ›åˆ†å¸ƒç»“æœã€‚æ³¨æ„åŠ›åˆ†å¸ƒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å…·æœ‰æ˜ç¡®çš„è§£é‡Šæ€§å’Œé«˜ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18040v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå…ƒå­¦ä¹ çš„æ–°å‹å¿«é€Ÿé€‚åº”çš„ç¥ç»å…ƒæ”¾ç”µæ’åºç®—æ³•æ¡†æ¶FS-SSã€‚è¯¥æ¡†æ¶å…·å¤‡å¯å˜ç½‘ç»œæ¨¡å‹å¤§å°ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸Šè¿›è¡Œå­¦ä¹ å’Œé€‚åº”ï¼ŒåŒæ—¶é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œæ¥å¿«é€Ÿå­¦ä¹ ç¥ç»å…ƒæ”¾ç”µä¿¡å·çš„å…³é”®ç‰¹å¾ï¼Œå¹¶å¿«é€Ÿé€‚åº”æ–°çš„æ”¾ç”µæ³¢å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å…·æœ‰æé«˜çš„åˆ†ç±»å‡†ç¡®ç‡å’Œå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¯ä»¥åœ¨é™ä½è®­ç»ƒæ ·æœ¬æ•°é‡æ—¶ä»èƒ½ç»´æŒè¾ƒé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FS-SSæ¡†æ¶åˆ©ç”¨å…ƒå­¦ä¹ è¿›è¡Œå¿«é€Ÿé€‚åº”ï¼Œå¯åœ¨ä¸åŒåº”ç”¨ä¸­å®ç°å¤§è„‘-æœºå™¨æ¥å£ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å¯å˜ç½‘ç»œæ¨¡å‹å¤§å°è®¾è®¡ï¼Œéœ€è¦å°‘é‡æ•°æ®æ ·æœ¬å’Œæå°‘ç›‘ç£ã€‚</li>
<li>ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè†¨èƒ€å·ç§¯ç¥ç»ç½‘ç»œå®ç°ç²¾ç¡®å’Œå¿«é€Ÿå­¦ä¹ ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æŠ€æœ¯ï¼ŒFS-SSæ¡†æ¶åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¯¹èƒŒæ™¯å™ªå£°å’Œç›¸ä¼¼æ”¾ç”µæ³¢å½¢è¡¨ç°å‡ºæé«˜ç¨³å¥æ€§ã€‚</li>
<li>å½“å‡å°‘è®­ç»ƒæ ·æœ¬æ•°é‡æ—¶ï¼ŒFS-SSæ¡†æ¶çš„æ€§èƒ½é™ä½å¹…åº¦è¾ƒå°ã€‚</li>
<li>è®ºæ–‡å¯è§†åŒ–å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒéš¾åº¦çº§åˆ«ä¸‹çš„æ³¨æ„åŠ›åˆ†å¸ƒç»“æœï¼Œè¯æ˜äº†å…¶é«˜è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60cc7756f80ceeac6d76811a3e7a7395.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d7f317cc79663a437095a2d1ed39dec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42be4a6eb03e02a0883462dff51c5ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54ac5f01388b07b481042fd29fdff639.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b90a110d3b3fd58611b2d586f3f79aa7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PRIMAL-Physically-Reactive-and-Interactive-Motor-Model-for-Avatar-Learning"><a href="#PRIMAL-Physically-Reactive-and-Interactive-Motor-Model-for-Avatar-Learning" class="headerlink" title="PRIMAL: Physically Reactive and Interactive Motor Model for Avatar   Learning"></a>PRIMAL: Physically Reactive and Interactive Motor Model for Avatar   Learning</h2><p><strong>Authors:Yan Zhang, Yao Feng, AlpÃ¡r Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black</strong></p>
<p>To build a motor system of the interactive avatar, it is essential to develop a generative motion model drives the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although motion generation has been extensively studied, most methods do not support <code>embodied intelligence&#39;&#39; due to their offline setting, slow speed, limited motion lengths, or unnatural movements. To overcome these limitations, we propose PRIMAL, an autoregressive diffusion model that is learned with a two-stage paradigm, inspired by recent advances in foundation models. In the pretraining stage, the model learns motion dynamics from a large number of sub-second motion segments, providing </code>motor primitivesâ€™â€™ from which more complex motions are built. In the adaptation phase, we employ a ControlNet-like adaptor to fine-tune the motor control for semantic action generation and spatial target reaching. Experiments show that physics effects emerge from our training. Given a single-frame initial state, our model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In addition, we can effectively and efficiently adapt our base model to few-shot personalized actions and the task of spatial control. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that is highly responsive and natural. Code, models, and more results are available at: <a target="_blank" rel="noopener" href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL">https://yz-cnsdqz.github.io/eigenmotion/PRIMAL</a> </p>
<blockquote>
<p>ä¸ºäº†æ„å»ºäº¤äº’å¼è™šæ‹Ÿè§’è‰²çš„è¿åŠ¨ç³»ç»Ÿï¼Œå…³é”®åœ¨äºå¼€å‘ä¸€ç§ç”Ÿæˆæ€§è¿åŠ¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé©±åŠ¨è§’è‰²åœ¨ä¸‰ç»´ç©ºé—´ä¸­ä»¥æŒç»­ã€ç°å®ã€å¯æ§å’Œå“åº”è¿…é€Ÿçš„æ–¹å¼ç§»åŠ¨ã€‚å°½ç®¡è¿åŠ¨ç”Ÿæˆå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•å¹¶ä¸æ”¯æŒâ€œå†…åœ¨æ™ºèƒ½â€ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨ç¦»çº¿è®¾ç½®ä¸‹è¿è¡Œçš„ã€é€Ÿåº¦æ…¢ã€è¿åŠ¨é•¿åº¦æœ‰é™æˆ–ä¸è‡ªç„¶ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PRIMALï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„äºŒé˜¶å­¦ä¹ èŒƒå¼ï¼Œå—åˆ°æœ€æ–°åŸºç¡€æ¨¡å‹çš„å¯å‘ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹ä»å¤§é‡äºšç§’çº§è¿åŠ¨ç‰‡æ®µä¸­å­¦ä¹ è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œæä¾›â€œè¿åŠ¨åŸè¯­â€ï¼Œä»ä¸­æ„å»ºæ›´å¤æ‚çš„åŠ¨ä½œã€‚åœ¨é€‚åº”é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ç±»ä¼¼ControlNetçš„é€‚é…å™¨å¯¹è¿åŠ¨æ§åˆ¶è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°è¯­ä¹‰åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡å®šä½ã€‚å®éªŒè¡¨æ˜ï¼Œç‰©ç†æ•ˆæœåœ¨æˆ‘ä»¬çš„è®­ç»ƒä¸­æ˜¾ç°ã€‚ç»™å®šå•å¸§åˆå§‹çŠ¶æ€ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç”Ÿæˆæ— ç•Œã€ç°å®å’Œå¯æ§çš„è¿åŠ¨ï¼Œè¿˜èƒ½ä½¿è™šæ‹Ÿè§’è‰²å®æ—¶å“åº”å¼•èµ·çš„å†²åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆä¸”é«˜æ•ˆåœ°åŸºäºæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹è¿›è¡Œå°‘é‡ä¸ªæ€§åŒ–åŠ¨ä½œå’Œç©ºé—´æ§åˆ¶ä»»åŠ¡çš„é€‚åº”ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬åˆ©ç”¨è¯¥æ¨¡å‹åœ¨Unreal Engineä¸­åˆ›å»ºäº†ä¸€ä¸ªå®æ—¶è§’è‰²åŠ¨ç”»ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…·æœ‰é«˜åº¦å“åº”æ€§å’Œè‡ªç„¶æ€§ã€‚ä»£ç ã€æ¨¡å‹å’Œæ›´å¤šç»“æœå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL">URLé“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17544v1">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ„å»ºäº¤äº’å¼è™šæ‹Ÿè§’è‰²çš„è¿åŠ¨ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå¤§å¤šæ•°è¿åŠ¨ç”Ÿæˆæ–¹æ³•ç”±äºç¦»çº¿è®¾ç½®ã€é€Ÿåº¦æ…¢ã€è¿åŠ¨é•¿åº¦æœ‰é™æˆ–ä¸è‡ªç„¶è¿åŠ¨ç­‰åŸå› ï¼Œä¸æ”¯æŒâ€œå†…åœ¨æ™ºèƒ½â€ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åä¸ºPRIMALçš„ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ èŒƒå¼ï¼Œä»å¤§é‡äºšç§’çº§è¿åŠ¨ç‰‡æ®µä¸­å­¦ä¹ è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œå¹¶ä¸ºæ›´å¤æ‚çš„è¿åŠ¨æä¾›â€œè¿åŠ¨åŸè¯­â€ã€‚é€‚åº”é˜¶æ®µåˆ™é€šè¿‡ç±»ä¼¼ControlNetçš„é€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°è¯­ä¹‰åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡è¾¾åˆ°ã€‚å®éªŒè¡¨æ˜ï¼Œç‰©ç†æ•ˆæœå¯ä»¥ä»è®­ç»ƒä¸­æ¶Œç°å‡ºæ¥ã€‚ç»™å®šåˆå§‹çŠ¶æ€çš„å•å¸§å›¾åƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…å¯ä»¥ç”Ÿæˆæ— é™åˆ¶ã€ç°å®å’Œå¯æ§çš„è¿åŠ¨ï¼Œè¿˜å¯ä»¥ä½¿è™šæ‹Ÿè§’è‰²å¯¹å®æ—¶è¯±å¯¼çš„è„‰å†²ä½œå‡ºå“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”ä¸ªæ€§åŒ–åŠ¨ä½œå’Œç©ºé—´æ§åˆ¶ä»»åŠ¡ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹åˆ›å»ºäº†å“åº”è¿…é€Ÿã€åŠ¨ä½œè‡ªç„¶çš„Unreal Engineå®æ—¶è§’è‰²åŠ¨ç”»ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»æ„å»ºäº¤äº’å¼è™šæ‹Ÿè§’è‰²è¿åŠ¨ç³»ç»Ÿçš„é‡è¦æ€§åŠç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>æå‡ºåä¸ºPRIMALçš„ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µå­¦ä¹ èŒƒå¼ä»å¤§é‡äºšç§’çº§è¿åŠ¨ç‰‡æ®µä¸­å­¦ä¹ ã€‚</li>
<li>åœ¨é€‚åº”é˜¶æ®µä½¿ç”¨ç±»ä¼¼ControlNetçš„é€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œå®ç°è¯­ä¹‰åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡è¾¾åˆ°ã€‚</li>
<li>æ¨¡å‹å¯ç”Ÿæˆæ— é™åˆ¶ã€ç°å®å’Œå¯æ§çš„è¿åŠ¨ï¼Œä½¿è™šæ‹Ÿè§’è‰²å¯¹å®æ—¶è¯±å¯¼çš„è„‰å†²ä½œå‡ºå“åº”ã€‚</li>
<li>è¯¥æ¨¡å‹å¯æœ‰æ•ˆé€‚åº”ä¸ªæ€§åŒ–åŠ¨ä½œå’Œç©ºé—´æ§åˆ¶ä»»åŠ¡ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c8bdd85a621e48ac3dbb74013a19409.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eedcaca66fc55810b870d4060e05c87f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fcc07537694a1362f3d1e8dba4f583f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Slide-Level-Prompt-Learning-with-Vision-Language-Models-for-Few-Shot-Multiple-Instance-Learning-in-Histopathology"><a href="#Slide-Level-Prompt-Learning-with-Vision-Language-Models-for-Few-Shot-Multiple-Instance-Learning-in-Histopathology" class="headerlink" title="Slide-Level Prompt Learning with Vision Language Models for Few-Shot   Multiple Instance Learning in Histopathology"></a>Slide-Level Prompt Learning with Vision Language Models for Few-Shot   Multiple Instance Learning in Histopathology</h2><p><strong>Authors:Devavrat Tomar, Guillaume Vray, Dwarikanath Mahapatra, Sudipta Roy, Jean-Philippe Thiran, Behzad Bozorgtabar</strong></p>
<p>In this paper, we address the challenge of few-shot classification in histopathology whole slide images (WSIs) by utilizing foundational vision-language models (VLMs) and slide-level prompt learning. Given the gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods rely on aggregation functions to derive slide-level (bag-level) predictions from patch representations, which require extensive bag-level labels for training. In contrast, VLM-based approaches excel at aligning visual embeddings of patches with candidate class text prompts but lack essential pathological prior knowledge. Our method distinguishes itself by utilizing pathological prior knowledge from language models to identify crucial local tissue types (patches) for WSI classification, integrating this within a VLM-based MIL framework. Our approach effectively aligns patch images with tissue types, and we fine-tune our model via prompt learning using only a few labeled WSIs per category. Experimentation on real-world pathological WSI datasets and ablation studies highlight our methodâ€™s superior performance over existing MIL- and VLM-based methods in few-shot WSI classification tasks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/LTS5/SLIP">https://github.com/LTS5/SLIP</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¹»ç¯ç‰‡çº§åˆ«æç¤ºå­¦ä¹ æ¥è§£å†³ç»„ç»‡ç—…ç†å­¦å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­çš„å°æ ·æœ¬åˆ†ç±»æŒ‘æˆ˜ã€‚é‰´äºå…¨å¹»ç¯ç‰‡å›¾åƒçš„åƒå…†åƒç´ è§„æ¨¡ï¼Œä¼ ç»Ÿçš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ä¾èµ–äºèšåˆå‡½æ•°ä»è¡¥ä¸è¡¨ç¤ºæ´¾ç”Ÿå‡ºå¹»ç¯ç‰‡çº§åˆ«ï¼ˆåŒ…çº§åˆ«ï¼‰çš„é¢„æµ‹ç»“æœï¼Œè¿™éœ€è¦å¤§é‡çš„åŒ…çº§åˆ«æ ‡ç­¾æ¥è¿›è¡Œè®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºVLMçš„æ–¹æ³•æ“…é•¿å°†è¡¥ä¸çš„è§†è§‰åµŒå…¥ä¸å€™é€‰ç±»åˆ«æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹é½ï¼Œä½†ç¼ºä¹å¿…è¦çš„ç—…ç†å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†åŸºäºè¯­è¨€æ¨¡å‹çš„ç—…ç†å…ˆéªŒçŸ¥è¯†ç”¨äºè¯†åˆ«å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»è‡³å…³é‡è¦çš„å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ï¼Œå¹¶å°†è¿™ä¸€ç‚¹çº³å…¥åŸºäºVLMçš„MILæ¡†æ¶ä¸­è„±é¢–è€Œå‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†è¡¥ä¸å›¾åƒä¸ç»„ç»‡ç±»å‹å¯¹é½ï¼Œå¹¶é€šè¿‡ä»…ä½¿ç”¨æ¯ä¸ªç±»åˆ«å°‘é‡æ ‡è®°çš„å¹»ç¯ç‰‡å›¾åƒè¿›è¡Œæç¤ºå­¦ä¹ æ¥å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨ç°å®ä¸–ç•Œçš„ç—…ç†æ€§å…¨å¹»ç¯ç‰‡å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒå’Œæ¶ˆèç ”ç©¶çªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘æ ·æœ¬å…¨å¹»ç¯ç‰‡å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰çš„MILå’ŒVLMæ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LTS5/SLIP%E4%B8%AD%E6%89%BE%E5%88%B0%E5%B9%B6%E5%85%AC%E5%BC%BA%E6%B6%AF%E8%AE%BF%E3%80%82">https://github.com/LTS5/SLIPä¸­æ‰¾åˆ°å¹¶å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17238v1">PDF</a> Accepted to ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹ç—…ç†å­¦ä¸­å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå°‘æ ·æœ¬åˆ†ç±»çš„æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¹»ç¯ç‰‡çº§åˆ«çš„æç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œåˆ©ç”¨ç—…ç†å…ˆéªŒçŸ¥è¯†è¯†åˆ«å…³é”®å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ï¼Œå¹¶å°†å…¶çº³å…¥åŸºäºVLMçš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ç—…ç†WSIæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„å¤šå®ä¾‹å­¦ä¹ å’ŒåŸºäºVLMçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³äº†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„å°‘æ ·æœ¬åˆ†ç±»é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¹»ç¯ç‰‡çº§åˆ«çš„æç¤ºå­¦ä¹ æŠ€æœ¯æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ç—…ç†å…ˆéªŒçŸ¥è¯†è¯†åˆ«å…³é”®å±€éƒ¨ç»„ç»‡ç±»å‹ï¼ˆè¡¥ä¸ï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸€ä¸ªåŸºäºVLMçš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶ä¸­æ•´åˆäº†è¯†åˆ«å‡ºçš„å…³é”®å±€éƒ¨ç»„ç»‡ç±»å‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨çœŸå®ç—…ç†WSIæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œå¹¶æ˜¾ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-038ae23b63c8e3ea3772760301971730.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12522f905cb31eae6b0423bd359e1bbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e34ce182618497fb68dd4f3673f12640.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6118cd028dd6333f11725858076997f8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes"><a href="#Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes" class="headerlink" title="Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes"></a>Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes</h2><p><strong>Authors:Davide Antonio Mura, Michela Pinna, Lorenzo Putzu, Andrea Loddo, Alessandra Perniciano, Olga Mulas, Cecilia Di Ruberto</strong></p>
<p>The detection of blood disorders often hinges upon the quantification of specific blood cell types. Variations in cell counts may indicate the presence of pathological conditions. Thus, the significance of developing precise automatic systems for blood cell enumeration is underscored. The investigation focuses on a novel approach termed DE-ViT. This methodology is employed in a Few-Shot paradigm, wherein training relies on a limited number of images. Two distinct datasets are utilised for experimental purposes: the Raabin-WBC dataset for Leukocyte detection and a local dataset for Schistocyte identification. In addition to the DE-ViT model, two baseline models, Faster R-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being compared against those of the proposed model. While DE-ViT has demonstrated state-of-the-art performance on the COCO and LVIS datasets, both baseline models surpassed its performance on the Raabin-WBC dataset. Moreover, only Faster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed disparities in performance may possibly be attributed to domain shift phenomena. </p>
<blockquote>
<p>è¡€æ¶²ç–¾ç—…çš„æ£€æµ‹å¾€å¾€å–å†³äºç‰¹å®šè¡€ç»†èƒç±»å‹çš„é‡åŒ–ã€‚ç»†èƒè®¡æ•°çš„å˜åŒ–å¯èƒ½è¡¨æ˜å­˜åœ¨ç—…ç†çŠ¶å†µã€‚å› æ­¤ï¼Œå¼€å‘ç²¾ç¡®è‡ªåŠ¨è¡€ç»†èƒè®¡æ•°ç³»ç»Ÿçš„æ„ä¹‰å°¤ä¸ºé‡è¦ã€‚ç ”ç©¶é‡ç‚¹æ˜¯ä¸€ç§ç§°ä¸ºDE-ViTçš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨å°æ ·æœ¬ï¼ˆFew-Shotï¼‰æ¨¡å¼ï¼Œè®­ç»ƒä¾èµ–äºå°‘é‡å›¾åƒã€‚ä¸ºå®éªŒç›®çš„ä½¿ç”¨äº†ä¸¤ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†ï¼šç”¨äºç™½ç»†èƒæ£€æµ‹çš„Raabin-WBCæ•°æ®é›†å’Œç”¨äºè¯†åˆ«è£‚æ®–çº¢ç»†èƒï¼ˆSchistocyteï¼‰çš„æœ¬åœ°æ•°æ®é›†ã€‚é™¤äº†DE-ViTæ¨¡å‹å¤–ï¼Œè¿˜é‡‡ç”¨äº†Faster R-CNN 50å’ŒFaster R-CNN X 101ä¸¤ä¸ªåŸºå‡†æ¨¡å‹ï¼Œå¹¶å°†å…¶ç»“æœä¸æ‰€æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è™½ç„¶DE-ViTåœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ä¸¤ä¸ªåŸºå‡†æ¨¡å‹åœ¨Raabin-WBCæ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†å®ƒã€‚æ­¤å¤–ï¼Œåªæœ‰Faster R-CNN X 101åœ¨SC-IDBä¸Šäº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚è§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®å¼‚å¯èƒ½å½’å› äºé¢†åŸŸè¿ç§»ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17107v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è¡€æ¶²ç–¾ç—…æ£€æµ‹ä¸­ï¼Œç‰¹å®šè¡€ç»†èƒç±»å‹çš„é‡åŒ–è‡³å…³é‡è¦ã€‚ä¸€ç§åä¸ºDE-ViTçš„æ–°æ–¹æ³•è¢«åº”ç”¨äºFew-Shotæ¨¡å¼ä¸‹è¿›è¡Œè¡€ç»†èƒè®¡æ•°ï¼Œä½¿ç”¨æœ‰é™çš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚å°½ç®¡DE-ViTåœ¨æŸäº›æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨é’ˆå¯¹ç‰¹å®šæ•°æ®é›†ï¼ˆå¦‚Raabin-WBCã€SC-IDBï¼‰çš„æµ‹è¯•ä¸­ï¼Œå…¶ä»–æ¨¡å‹ï¼ˆå¦‚Faster R-CNN 50å’ŒFaster R-CNN X 101ï¼‰è¡¨ç°æ›´ä½³ã€‚æ€§èƒ½å·®å¼‚å¯èƒ½ä¸é¢†åŸŸè¿ç§»ç°è±¡æœ‰å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€ç»†èƒç±»å‹çš„é‡åŒ–åœ¨è¡€æ¶²ç–¾ç—…æ£€æµ‹ä¸­éå¸¸é‡è¦ã€‚</li>
<li>DE-ViTæ˜¯ä¸€ç§åº”ç”¨äºFew-Shotæ¨¡å¼ä¸‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºè¡€ç»†èƒè®¡æ•°ã€‚</li>
<li>DE-ViTåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„è¡¨ç°å“è¶Šã€‚</li>
<li>åœ¨ç‰¹å®šæ•°æ®é›†ï¼ˆå¦‚Raabin-WBCã€SC-IDBï¼‰çš„æµ‹è¯•ä¸­ï¼Œå…¶ä»–æ¨¡å‹ï¼ˆå¦‚Faster R-CNN 50å’ŒFaster R-CNN X 101ï¼‰çš„è¡¨ç°è¶…è¿‡äº†DE-ViTã€‚</li>
<li>æ€§èƒ½å·®å¼‚å¯èƒ½æºäºé¢†åŸŸè¿ç§»ç°è±¡ã€‚</li>
<li>ä½¿ç”¨æœ‰é™çš„å›¾åƒè¿›è¡Œè®­ç»ƒæ˜¯è¯¥ç ”ç©¶çš„ä¸€ä¸ªå…³é”®ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e20ca7f60177aa8ec79c9f40acf0c782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe8e2ec1f311e54b959baac9c0821624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8172ea314037f0aa69f8bfc90f4aca7a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FFaceNeRF-Few-shot-Face-Editing-in-Neural-Radiance-Fields"><a href="#FFaceNeRF-Few-shot-Face-Editing-in-Neural-Radiance-Fields" class="headerlink" title="FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields"></a>FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields</h2><p><strong>Authors:Kwan Yun, Chaelin Kim, Hangyeul Shin, Junyong Noh</strong></p>
<p>Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {\href{<a target="_blank" rel="noopener" href="https://kwanyun.github.io/FFaceNeRF_page/%7D%7Bproject-page%7D%7D">https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}</a>. </p>
<blockquote>
<p>æœ€è¿‘ä½¿ç”¨æ©è†œï¼ˆmasksï¼‰çš„3Dé¢éƒ¨ç¼–è¾‘æ–¹æ³•é€šè¿‡åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”Ÿæˆäº†é«˜è´¨é‡ç¼–è¾‘å›¾åƒã€‚å°½ç®¡å…¶æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†ç°æœ‰æ–¹æ³•ç”±äºä½¿ç”¨é¢„è®­ç»ƒåˆ†å‰²æ©è†œè€Œå¾€å¾€æä¾›æœ‰é™çš„ç”¨æˆ·æ§åˆ¶ã€‚ä¸ºäº†ä½¿ç”¨å…·æœ‰æ‰€éœ€å¸ƒå±€çš„æ©è†œï¼Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®é›†ï¼Œè¿™å¾ˆéš¾æ”¶é›†ã€‚æˆ‘ä»¬æå‡ºäº†FFaceNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºNeRFçš„é¢éƒ¨ç¼–è¾‘æŠ€æœ¯ï¼Œå¯ä»¥å…‹æœå› ä½¿ç”¨å›ºå®šæ©è†œå¸ƒå±€è€Œå¸¦æ¥çš„ç”¨æˆ·æ§åˆ¶å—é™çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¸¦æœ‰ç‰¹å¾æ³¨å…¥çš„å‡ ä½•é€‚é…å™¨ï¼Œå…è®¸å¯¹å‡ ä½•å±æ€§è¿›è¡Œæœ‰æ•ˆæ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨æ½œåœ¨æ··åˆè¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œè¿™å¯ä»¥åœ¨å°‘é‡æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™æœ‰åŠ©äºæ¨¡å‹è¿…é€Ÿé€‚åº”æ‰€éœ€çš„æ©è†œå¸ƒå±€ï¼Œå¯¹äºä¸ªæ€§åŒ–åŒ»å­¦å½±åƒæˆ–åˆ›æ„é¢éƒ¨ç¼–è¾‘ç­‰é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼ŒFFaceNeRFåœ¨çµæ´»æ€§ã€æ§åˆ¶å’Œç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„åŸºäºæ©è†œçš„é¢éƒ¨ç¼–è¾‘æ–¹æ³•ï¼Œä¸ºå®šåˆ¶å’Œé«˜ä¿çœŸ3Dé¢éƒ¨ç¼–è¾‘çš„æœªæ¥è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ä»£ç å·²åœ¨é¡¹ç›®é¡µé¢ï¼ˆ<a target="_blank" rel="noopener" href="https://kwanyun.github.io/FFaceNeRF_page/%EF%BC%89%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://kwanyun.github.io/FFaceNeRF_page/ï¼‰ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17095v1">PDF</a> CVPR2025, 11 pages, 14 figures</p>
<p><strong>Summary</strong><br>åŸºäºNeRFçš„3Däººè„¸ç¼–è¾‘æ–¹æ³•é€šè¿‡ä½¿ç”¨æ©è†œå®ç°äº†é«˜è´¨é‡çš„äººè„¸å›¾åƒç¼–è¾‘ã€‚FFaceNeRFå…‹æœäº†ç°æœ‰æ–¹æ³•å› ä½¿ç”¨å›ºå®šæ©è†œå¸ƒå±€å¯¼è‡´çš„ç”¨æˆ·æ§åˆ¶å—é™çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å‡ ä½•é€‚é…å™¨å’Œç‰¹å¾æ³¨å…¥æŠ€æœ¯æœ‰æ•ˆæ“æ§å‡ ä½•å±æ€§ï¼Œå¹¶é‡‡ç”¨æ½œåœ¨æ··åˆæŠ€æœ¯è¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¿«é€Ÿé€‚åº”æ‰€éœ€çš„æ©è†œå¸ƒå±€ã€‚æ­¤æ–¹æ³•åœ¨ä¸ªæ€§åŒ–åŒ»ç–—æˆåƒæˆ–åˆ›æ„äººè„¸ç¼–è¾‘ç­‰é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ï¼Œå¹¶åœ¨çµæ´»æ€§ã€æ§åˆ¶å’Œç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šç°æœ‰åŸºäºæ©è†œçš„äººè„¸ç¼–è¾‘æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FFaceNeRFä½¿ç”¨NeRFæŠ€æœ¯å®ç°é«˜è´¨é‡3Däººè„¸ç¼–è¾‘ã€‚</li>
<li>å…‹æœç°æœ‰æ–¹æ³•å› ä½¿ç”¨å›ºå®šæ©è†œå¸ƒå±€å¯¼è‡´çš„ç”¨æˆ·æ§åˆ¶å—é™çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å‡ ä½•é€‚é…å™¨å’Œç‰¹å¾æ³¨å…¥æŠ€æœ¯æ“æ§å‡ ä½•å±æ€§ã€‚</li>
<li>é‡‡ç”¨æ½œåœ¨æ··åˆæŠ€æœ¯è¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ–¹æ³•èƒ½å¿«é€Ÿé€‚åº”æ‰€éœ€çš„æ©è†œå¸ƒå±€ã€‚</li>
<li>åœ¨ä¸ªæ€§åŒ–åŒ»ç–—æˆåƒæˆ–åˆ›æ„äººè„¸ç¼–è¾‘ç­‰é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc159b82dbe17b48b3ef37e8157479d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce7a46159b8e2ff1bc339e0fcaaf7987.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5819230720ea1883f74fbd5f797383f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae26aac97fbeb6bfea7a1bc46523922b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97501c3dda47cc54e1c1efa600ac13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4ad5718adc27195c33254b7b834450.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generative-Compositor-for-Few-Shot-Visual-Information-Extraction"><a href="#Generative-Compositor-for-Few-Shot-Visual-Information-Extraction" class="headerlink" title="Generative Compositor for Few-Shot Visual Information Extraction"></a>Generative Compositor for Few-Shot Visual Information Extraction</h2><p><strong>Authors:Zhibo Yang, Wei Hua, Sibo Song, Cong Yao, Yingying Zhu, Wenqing Cheng, Xiang Bai</strong></p>
<p>Visual Information Extraction (VIE), aiming at extracting structured information from visually rich document images, plays a pivotal role in document processing. Considering various layouts, semantic scopes, and languages, VIE encompasses an extensive range of types, potentially numbering in the thousands. However, many of these types suffer from a lack of training data, which poses significant challenges. In this paper, we propose a novel generative model, named Generative Compositor, to address the challenge of few-shot VIE. The Generative Compositor is a hybrid pointer-generator network that emulates the operations of a compositor by retrieving words from the source text and assembling them based on the provided prompts. Furthermore, three pre-training strategies are employed to enhance the modelâ€™s perception of spatial context information. Besides, a prompt-aware resampler is specially designed to enable efficient matching by leveraging the entity-semantic prior contained in prompts. The introduction of the prompt-based retrieval mechanism and the pre-training strategies enable the model to acquire more effective spatial and semantic clues with limited training samples. Experiments demonstrate that the proposed method achieves highly competitive results in the full-sample training, while notably outperforms the baseline in the 1-shot, 5-shot, and 10-shot settings. </p>
<blockquote>
<p>è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰æ—¨åœ¨ä»ä¸°å¯Œçš„æ–‡æ¡£å›¾åƒä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œåœ¨æ–‡æ¡£å¤„ç†ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è€ƒè™‘åˆ°å„ç§å¸ƒå±€ã€è¯­ä¹‰èŒƒå›´å’Œè¯­è¨€ï¼ŒVIEæ¶µç›–äº†å¹¿æ³›çš„ç±»å‹ï¼Œæ•°é‡å¯èƒ½è¾¾åˆ°æ•°åƒç§ã€‚ç„¶è€Œï¼Œè¿™äº›ç±»å‹ä¸­çš„è®¸å¤šéƒ½ç¼ºä¹è®­ç»ƒæ•°æ®ï¼Œè¿™æ„æˆäº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œåä¸ºâ€œç”Ÿæˆå¼ç»„åˆå™¨â€ï¼ˆGenerative Compositorï¼‰ï¼Œä»¥è§£å†³æ ·æœ¬é‡ä¸è¶³çš„è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰é—®é¢˜ã€‚ç”Ÿæˆå¼ç»„åˆå™¨æ˜¯ä¸€ç§æ··åˆæŒ‡é’ˆç”Ÿæˆç½‘ç»œï¼Œå®ƒé€šè¿‡ä»æºæ–‡æœ¬ä¸­æ£€ç´¢å•è¯å¹¶æ ¹æ®æä¾›çš„æç¤ºè¿›è¡Œç»„åˆï¼Œæ¨¡æ‹Ÿç»„åˆå™¨çš„æ“ä½œã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ä¸‰ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹å¯¹ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜ç‰¹æ„è®¾è®¡äº†ä¸€ç§æç¤ºæ„ŸçŸ¥é‡é‡‡æ ·å™¨ï¼Œä»¥åˆ©ç”¨æç¤ºä¸­çš„å®ä½“è¯­ä¹‰å…ˆéªŒä¿¡æ¯è¿›è¡Œé«˜æ•ˆåŒ¹é…ã€‚åŸºäºæç¤ºçš„æ£€ç´¢æœºåˆ¶å’Œé¢„è®­ç»ƒç­–ç•¥çš„å¼•å…¥ï¼Œä½¿æ¨¡å‹åœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸‹èƒ½å¤Ÿè·å–æ›´æœ‰æ•ˆçš„ç©ºé—´å’Œè¯­ä¹‰çº¿ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨æ ·æœ¬è®­ç»ƒä¸­çš„è¡¨ç°æå…·ç«äº‰åŠ›ï¼Œè€Œåœ¨1æ¬¡ã€5æ¬¡å’Œ10æ¬¡æ‹æ‘„çš„è®¾ç½®ä¸­ï¼Œåˆ™æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ä¿¡æ¯æå–ï¼ˆVIEï¼‰æ—¨åœ¨ä»ä¸°å¯Œçš„æ–‡æ¡£å›¾åƒä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œåœ¨æ–‡æ¡£å¤„ç†ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è€ƒè™‘å„ç§å¸ƒå±€ã€è¯­ä¹‰èŒƒå›´å’Œè¯­è¨€ï¼ŒVIEåŒ…å«å¹¿æ³›çš„ç±»å‹ï¼Œæ•°é‡å¯èƒ½è¾¾åˆ°æ•°åƒç§ã€‚ç„¶è€Œï¼Œè®¸å¤šç±»å‹é¢ä¸´è®­ç»ƒæ•°æ®ç¼ºä¹çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹â€”â€”ç”Ÿæˆå¼ç»„åˆå™¨ï¼ˆGenerative Compositorï¼‰ï¼Œä»¥è§£å†³å°‘æ•°æ ·æœ¬VIEçš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ç§æ··åˆæŒ‡é’ˆç”Ÿæˆå™¨ç½‘ç»œï¼Œé€šè¿‡ä»æºæ–‡æœ¬ä¸­æ£€ç´¢å•è¯å¹¶æ ¹æ®æç¤ºè¿›è¡Œç»„åˆæ¥æ¨¡æ‹Ÿç»„åˆå™¨çš„æ“ä½œã€‚é€šè¿‡é‡‡ç”¨ä¸‰ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§æç¤ºæ„ŸçŸ¥é‡é‡‡æ ·å™¨ï¼Œé€šè¿‡åˆ©ç”¨æç¤ºä¸­çš„å®ä½“è¯­ä¹‰å…ˆéªŒæ¥å®ç°é«˜æ•ˆåŒ¹é…ã€‚åŸºäºæç¤ºçš„æ£€ç´¢æœºåˆ¶å’Œé¢„è®­ç»ƒç­–ç•¥çš„ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸­è·å¾—æ›´æœ‰æ•ˆçš„ç©ºé—´å’Œè¯­ä¹‰çº¿ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨æ ·æœ¬è®­ç»ƒä¸­å…·æœ‰é«˜åº¦ç«äº‰åŠ›ï¼Œåœ¨1æ¬¡ã€5æ¬¡å’Œ10æ¬¡å°„å‡»è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Visual Information Extraction (VIE)æ˜¯ä»æ–‡æ¡£å›¾åƒä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>VIEé¢ä¸´è®­ç»ƒæ•°æ®ç¼ºä¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ•°é‡å¯èƒ½è¾¾åˆ°æ•°åƒçš„å„ç±»å¸ƒå±€ã€è¯­ä¹‰èŒƒå›´å’Œè¯­è¨€ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„ç”Ÿæˆæ¨¡å‹â€”â€”ç”Ÿæˆå¼ç»„åˆå™¨ï¼ˆGenerative Compositorï¼‰ï¼Œç”¨äºè§£å†³å°‘æ•°æ ·æœ¬VIEé—®é¢˜ã€‚</li>
<li>ç”Ÿæˆå¼ç»„åˆå™¨æ˜¯ä¸€ä¸ªæ··åˆæŒ‡é’ˆç”Ÿæˆå™¨ç½‘ç»œï¼Œæ¨¡æ‹Ÿç»„åˆå™¨çš„æ“ä½œï¼Œä»æºæ–‡æœ¬ä¸­æ£€ç´¢å•è¯å¹¶æ ¹æ®æç¤ºè¿›è¡Œç»„åˆã€‚</li>
<li>é€šè¿‡ä¸‰ç§é¢„è®­ç»ƒç­–ç•¥å¢å¼ºæ¨¡å‹å¯¹ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†æç¤ºæ„ŸçŸ¥é‡é‡‡æ ·å™¨ï¼Œåˆ©ç”¨æç¤ºä¸­çš„å®ä½“è¯­ä¹‰å…ˆéªŒå®ç°é«˜æ•ˆåŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6751f02fe16eb05a28d2afe94902cb57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-509c8172287ea6be79a0efaffc7a2a7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a71f6972ed841b29e0dec7b5ecb00a59.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy"><a href="#Ensemble-Debiasing-Across-Class-and-Sample-Levels-for-Fairer-Prompting-Accuracy" class="headerlink" title="Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy"></a>Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting   Accuracy</h2><p><strong>Authors:Ruixi Lin, Ziqiao Wang, Yang You</strong></p>
<p>Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹æ˜¯å¼ºå¤§çš„å°æ ·æœ¬å­¦ä¹ è€…ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†è‰¯å¥½çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œæ©ç›–äº†å…¶ç»“æœå­˜åœ¨ä¸¥é‡çš„ç±»åˆ«å‡†ç¡®ç‡ä¸å¹³è¡¡çš„äº‹å®ã€‚æˆ‘ä»¬è®¤ä¸ºè¿½æ±‚æ€»ä½“å‡†ç¡®ç‡ä¸åº”é€šè¿‡ä¸°å¯Œå¼ºç±»åˆ«æ¥å®ç°ï¼Œè€Œåº”é€šè¿‡æå‡å¼±ç±»åˆ«æ¥å®ç°ã€‚ä¸ºäº†è§£å†³ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç±»å’Œæ ·æœ¬çº§åˆ«çµæ´»åœ°ä¿®æ­£ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ ç±»åˆ«æ¦‚ç‡ã€‚ä½¿ç”¨Llama-2-13Båœ¨ä¸ƒä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¹³è¡¡çš„ç±»åˆ«å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€»ä½“å‡†ç¡®ç‡å¢ç›Šã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹å¾—åˆ°çš„æ¦‚ç‡ä¿®æ­£æ–¹æ¡ˆè¿›è¡Œäº†åˆ†æï¼Œè¡¨æ˜æ ·æœ¬çº§çš„ä¿®æ­£å¯¹äºæå‡å¼±ç±»åˆ«æ˜¯å¿…è¦çš„ã€‚ç”±äºæœ‰æ•ˆåœ°çº æ­£äº†å¼±ç±»åˆ«ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜ä¸ºæ›´å¤§çš„æ¨¡å‹å˜ä½“Llama-2-70Bå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä»»åŠ¡ä¸­ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨ä¸¤çº§è¿›è¡Œé›†æˆå»åçš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05157v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­æ€»ä½“å‡†ç¡®åº¦è¾ƒé«˜ï¼Œä½†å­˜åœ¨ç±»åˆ«å‡†ç¡®åº¦å¤±è¡¡çš„é—®é¢˜ã€‚ä¸ºæé«˜å¼±ç±»åˆ«å‡†ç¡®åº¦ï¼Œæå‡ºä¸€ç§åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œå¯åœ¨ç±»å’Œæ ·æœ¬çº§åˆ«çµæ´»è°ƒæ•´ä¸Šä¸‹æ–‡å­¦ä¹ åˆ°çš„ç±»åˆ«æ¦‚ç‡ã€‚åœ¨ä¸ƒä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šå¯¹Llama-2-13Bæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€»ä½“ç²¾åº¦å¢ç›Šå’Œå¹³è¡¡çš„ç±»åˆ«ç²¾åº¦ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ ·æœ¬çº§ä¿®æ­£å¯¹äºæå‡å¼±ç±»åˆ«æ˜¯å¿…è¦çš„ã€‚è¯¥æ–¹æ³•åœ¨å¤§å‹æ¨¡å‹å˜ä½“Llama-2-70Bä¸Šä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä»»åŠ¡ä¸­è¿›ä¸€æ­¥è¯æ˜äº†é›†æˆå»åçš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å­˜åœ¨ç±»åˆ«å‡†ç¡®åº¦å¤±è¡¡çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæµ·ç»´èµ›å¾·é˜¶è·ƒå‡½æ•°çš„é›†æˆå»åæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¼±ç±»åˆ«çš„å‡†ç¡®åº¦ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›çš„æ€»ä½“ç²¾åº¦å’Œå¹³è¡¡çš„ç±»åˆ«ç²¾åº¦ã€‚</li>
<li>æ ·æœ¬çº§ä¿®æ­£å¯¹äºæå‡å¼±ç±»åˆ«çš„é‡è¦æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤§å‹æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­ï¼Œå¦‚ç”Ÿç‰©åŒ»å­¦é¢†åŸŸã€‚</li>
<li>é›†æˆå»åæ–¹æ³•å¯¹äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8dc7d6a8f7e81557fef796a1579e9485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e7abf837ae065b0f6b125e263f6ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-206aed54cacde9744d1d5f74312a649e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b10245ff8527ca89793162cfdbbc0b9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Spectral-Informed-Mamba-for-Robust-Point-Cloud-Processing"><a href="#Spectral-Informed-Mamba-for-Robust-Point-Cloud-Processing" class="headerlink" title="Spectral Informed Mamba for Robust Point Cloud Processing"></a>Spectral Informed Mamba for Robust Point Cloud Processing</h2><p><strong>Authors:Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</strong></p>
<p>State space models have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mambaâ€™s capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in Masked Autoencoder for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate the improvements of our approach in classification, segmentation, and few-shot tasks over state-of-the-art baselines. </p>
<blockquote>
<p>çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œæœ€è¿‘åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿå¤‡å—å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨Mambaå’ŒMasked Autoencoderç½‘ç»œè¿›è¡Œæœ‰ç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ ä¸­çš„ç‚¹äº‘æ•°æ®å¤„ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼Œä»¥å¢å¼ºMambaå¤„ç†å¤æ‚ç‚¹äº‘ç»“æ„çš„èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„è°±æ¥æ•æ‰æ–‘å—è¿é€šæ€§ï¼Œå®šä¹‰ä¸€ä¸ªç­‰è·ä¸å˜éå†é¡ºåºï¼Œè¯¥é¡ºåºå¯¹è§‚ç‚¹å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿçš„åŸºäº3Dç½‘æ ¼çš„éå†æ›´å¥½åœ°æ•æ‰å½¢çŠ¶æµå½¢ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ‹‰æ™®æ‹‰æ–¯è°±åˆ†é‡æ¥æŒ‡å¯¼é€’å½’æ–‘å—åˆ†åŒºç­–ç•¥ï¼Œå®ç°æ›´ç²¾ç»†çš„é›†æˆå’Œåˆ†æ®µåˆ†æã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è§£å†³äº†Masked Autoencoderä¸­çš„ä»¤ç‰Œæ”¾ç½®é—®é¢˜ï¼Œé€šè¿‡å°†ä»¤ç‰Œæ¢å¤åˆ°å…¶åŸå§‹ä½ç½®æ¥ä¿ç•™å¿…è¦çš„é¡ºåºå¹¶æ”¹è¿›å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†ç±»ã€åˆ†å‰²å’Œå°‘é‡ä»»åŠ¡æ–¹é¢çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04953v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨Mambaå’ŒMasked Autoencoderç½‘ç»œå¤„ç†ç‚¹äº‘æ•°æ®çš„æ–°æ–¹æ³•ï¼Œé€‚ç”¨äºç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚æ–‡ç« æå‡ºäº†ä¸‰é¡¹å…³é”®è´¡çŒ®ä»¥æå‡Mambaåœ¨å¤„ç†å¤æ‚ç‚¹äº‘ç»“æ„æ–¹é¢çš„èƒ½åŠ›ï¼šä¸€ã€åˆ©ç”¨å›¾æ‹‰æ™®æ‹‰æ–¯è°±æ•æ‰æ–‘å—è¿æ¥æ€§ï¼Œå®šä¹‰äº†ä¸€ä¸ªå¯¹è§†ç‚¹å…·æœ‰é²æ£’æ€§çš„ç­‰è·ä¸å˜éå†é¡ºåºï¼Œæ›´å¥½åœ°æ•æ‰å½¢çŠ¶æµå½¢ï¼›äºŒã€é€šè¿‡é€’å½’æ–‘å—åˆ†åŒºç­–ç•¥è¿›è¡Œåˆ†æ®µï¼Œä»¥æ‹‰æ™®æ‹‰æ–¯å…‰è°±æˆåˆ†ä¸ºæŒ‡å¯¼ï¼Œå®ç°æ›´ç²¾ç»†çš„é›†æˆå’Œåˆ†æ®µåˆ†æï¼›ä¸‰ã€è§£å†³Masked Autoencoderä¸­Mambaçš„ä»¤ç‰Œæ”¾ç½®é—®é¢˜ï¼Œå°†ä»¤ç‰Œæ¢å¤åˆ°å…¶åŸå§‹ä½ç½®ï¼Œä¿ç•™é‡è¦é¡ºåºå¹¶æ”¹è¿›å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ã€åˆ†å‰²å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨Mambaå’ŒMasked Autoencoderç½‘ç»œå¤„ç†ç‚¹äº‘æ•°æ®ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å›¾æ‹‰æ™®æ‹‰æ–¯è°±ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„éå†é¡ºåºå®šä¹‰ï¼Œä»¥å¢å¼ºå¯¹å¤æ‚ç‚¹äº‘ç»“æ„çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†Mambaå¯¹è§†ç‚¹å˜åŒ–çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å½¢çŠ¶æµå½¢ã€‚</li>
<li>é€šè¿‡é€’å½’æ–‘å—åˆ†åŒºç­–ç•¥è¿›è¡Œåˆ†æ®µï¼Œå®ç°äº†æ›´ç²¾ç»†çš„é›†æˆå’Œåˆ†æ®µåˆ†æã€‚</li>
<li>è§£å†³äº†Masked Autoencoderä¸­Mambaçš„ä»¤ç‰Œæ”¾ç½®é—®é¢˜ï¼Œæé«˜äº†å­¦ä¹ æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»ã€åˆ†å‰²å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc189e35d43ca54181320185fc36ea81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-271dd050b708634ac1be384cedceed73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88315570e76a3c8b17222da288b5e1eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edc20420bba5bbff6fb5121d8f7a002f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation"><a href="#3D-Prior-is-All-You-Need-Cross-Task-Few-shot-2D-Gaze-Estimation" class="headerlink" title="3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation"></a>3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation</h2><p><strong>Authors:Yihua Cheng, Hengfei Wang, Zhongqun Zhang, Yang Yue, Bo Eun Kim, Feng Lu, Hyung Jin Chang</strong></p>
<p>3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To address these challenges, we propose a novel framework that bridges the gap between 3D and 2D gaze. Our framework contains a physics-based differentiable projection module with learnable parameters to model screen poses and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Additionally, we introduce a dynamic pseudo-labelling strategy for flipped images, which is particularly challenging for 2D labels due to unknown screen poses. To overcome this, we reverse the projection process by converting 2D labels to 3D space, where flipping is performed. Notably, this 3D space is not aligned with the camera coordinate system, so we learn a dynamic transformation matrix to compensate for this misalignment. We evaluate our method on MPIIGaze, EVE, and GazeCapture datasets, collected respectively on laptops, desktop computers, and mobile devices. The superior performance highlights the effectiveness of our approach, and demonstrates its strong potential for real-world applications. </p>
<blockquote>
<p>3Då’Œ2Dæ³¨è§†ä¼°è®¡å…±äº«æ•æ‰çœ¼ç›è¿åŠ¨çš„åŸºæœ¬ç›®æ ‡ï¼Œä½†ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è·¨ä»»åŠ¡å°æ ·æœ¬2Dæ³¨è§†ä¼°è®¡æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨ä»…å°‘é‡è®­ç»ƒå›¾åƒï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„3Dæ³¨è§†ä¼°è®¡ç½‘ç»œå¯¹æœªè§è¿‡çš„è®¾å¤‡è¿›è¡Œ2Dæ³¨è§†é¢„æµ‹ã€‚ç”±äº3Då’Œ2Dæ³¨è§†ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€æœªçŸ¥å±å¹•å§¿åŠ¿å’Œæœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œæ­¤ä»»åŠ¡å…·æœ‰æé«˜æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ¡¥æ¢3Då’Œ2Dæ³¨è§†ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸€ä¸ªåŸºäºç‰©ç†çš„å¯å¾®æŠ•å½±æ¨¡å—ï¼Œå…·æœ‰å¯å­¦ä¹ å‚æ•°æ¥æ¨¡æ‹Ÿå±å¹•å§¿åŠ¿å¹¶å°†3Dæ³¨è§†æŠ•å½±åˆ°2Dæ³¨è§†ã€‚è¯¥æ¡†æ¶æ˜¯å®Œå…¨å¯å¾®åˆ†çš„ï¼Œå¹¶ä¸”å¯ä»¥é›†æˆåˆ°ç°æœ‰çš„3dæ³¨è§†ç½‘ç»œä¸­ï¼Œæ— éœ€ä¿®æ”¹å…¶åŸå§‹æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºç¿»è½¬å›¾åƒå¼•å…¥äº†ä¸€ç§åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œè¿™å¯¹äº2Dæ ‡ç­¾è€Œè¨€å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨æœªçŸ¥å±å¹•å§¿åŠ¿ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šè¿‡å°†2Dæ ‡ç­¾è½¬æ¢åˆ°3Dç©ºé—´æ¥åè½¬æŠ•å½±è¿‡ç¨‹ï¼Œå¹¶åœ¨è¯¥ç©ºé—´æ‰§è¡Œç¿»è½¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ª3Dç©ºé—´ä¸ç›¸æœºåæ ‡ç³»å¹¶ä¸å¯¹é½ï¼Œå› æ­¤æˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªåŠ¨æ€è½¬æ¢çŸ©é˜µæ¥è¡¥å¿è¿™ç§é”™ä½ã€‚æˆ‘ä»¬åœ¨MPIIGazeã€EVEå’ŒGazeCaptureæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›æ•°æ®é›†åˆ†åˆ«åœ¨ç¬”è®°æœ¬ç”µè„‘ã€å°å¼ç”µè„‘å’Œç§»åŠ¨è®¾å¤‡ä¸Šæ”¶é›†ã€‚ä¼˜è¶Šçš„æ€§èƒ½å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04074v3">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–ã€æŒ‘æˆ˜æ€§çš„è·¨ä»»åŠ¡å°‘é•œå¤´2Dçœ¼åŠ¨è¿½è¸ªæŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„3Dçœ¼åŠ¨è¿½è¸ªç½‘ç»œå¯¹æœªè§è®¾å¤‡ä»…é€šè¿‡å°‘é‡è®­ç»ƒå›¾åƒè¿›è¡Œ2Dçœ¼åŠ¨é¢„æµ‹ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œå¼•å…¥ä¸€ä¸ªæ–°é¢–æ¡†æ¶å¡«è¡¥ä¸¤è€…ä¹‹é—´çš„ç©ºç™½ï¼Œå¹¶é‡‡ç”¨ç‰©ç†åŸºç¡€çš„å¾®åˆ†æŠ•å½±æ¨¡å—è¿›è¡Œå±å¹•å§¿æ€å»ºæ¨¡å’Œå°†3Dçœ¼åŠ¨è½¬åŒ–ä¸º2Dçœ¼åŠ¨ã€‚è¯¥æ–¹æ³•å¯å®Œå…¨å¾®åˆ†å¹¶èå…¥ç°æœ‰3Dçœ¼åŠ¨ç½‘ç»œè€Œä¸æ”¹å˜å…¶åŸå§‹æ¶æ„ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥åº”å¯¹ç¿»è½¬å›¾åƒçš„æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡åè½¬æŠ•å½±è¿‡ç¨‹å°†2Dæ ‡ç­¾è½¬æ¢ä¸º3Dç©ºé—´è¿›è¡Œå¤„ç†ã€‚åœ¨MPIIGazeã€EVEå’ŒGazeCaptureæ•°æ®é›†ä¸Šçš„ä¼˜è¶Šè¡¨ç°è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è·¨ä»»åŠ¡å°‘é•œå¤´å­¦ä¹ æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„3Dçœ¼åŠ¨è¿½è¸ªç½‘ç»œè¿›è¡Œ2Dçœ¼åŠ¨é¢„æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶æ¥å¡«è¡¥3Då’Œ2Dçœ¼åŠ¨è¿½è¸ªä¹‹é—´çš„å·®è·ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŸºäºç‰©ç†çš„å¾®åˆ†æŠ•å½±æ¨¡å—ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥èå…¥ç°æœ‰çš„3Dçœ¼åŠ¨ç½‘ç»œæ¶æ„ä¸­ï¼Œæ— éœ€è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€ä¼ªæ ‡ç­¾ç­–ç•¥åº”å¯¹ç¿»è½¬å›¾åƒçš„æŒ‘æˆ˜ï¼Œé€šè¿‡è½¬æ¢ç©ºé—´å¤„ç†è¿™äº›å›¾åƒã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a1dc0d0889de01d2ba055c30e9914b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a423886f9f0e59e16c6b1f850b2916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8ef542cda50013ea21d98b66fc923a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7889d33c9089358d01601dc03b0178ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48d43692d316c59f8661f7371a6f979b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Model-Predictive-Task-Sampling-for-Efficient-and-Robust-Adaptation"><a href="#Model-Predictive-Task-Sampling-for-Efficient-and-Robust-Adaptation" class="headerlink" title="Model Predictive Task Sampling for Efficient and Robust Adaptation"></a>Model Predictive Task Sampling for Efficient and Robust Adaptation</h2><p><strong>Authors:Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</strong></p>
<p>Foundation models have revolutionized general-purpose problem-solving, offering rapid task adaptation through pretraining, meta-training, and finetuning. Recent crucial advances in these paradigms reveal the importance of challenging task prioritized sampling to enhance adaptation robustness under distribution shifts. However, ranking task difficulties over iteration as a preliminary step typically requires exhaustive task evaluation, which is practically unaffordable in computation and data-annotation. This study provides a novel perspective to illuminate the possibility of leveraging the dual importance of adaptation robustness and learning efficiency, particularly in scenarios where task evaluation is risky or costly, such as iterative agent-environment interactions for robotic policy evaluation or computationally intensive inference steps for finetuning foundation models. Firstly, we introduce Model Predictive Task Sampling (MPTS), a framework that bridges the task space and adaptation risk landscape, providing a theoretical foundation for robust active task sampling. MPTS employs a generative model to characterize the episodic optimization process and predicts task-specific adaptation risk via posterior inference. The resulting risk learner amortizes the costly evaluation of task adaptation performance and provably approximates task difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot, and supervised finetuning settings. Empirically, we conduct extensive experiments in pattern recognition using foundation models and sequential decision-making. Our results demonstrate that MPTS significantly enhances adaptation robustness for tail or out-of-distribution (OOD) tasks and improves learning efficiency compared to state-of-the-art (SOTA) methods. The code is available at the project site <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS">https://github.com/thu-rllab/MPTS</a>. </p>
<blockquote>
<p>æ¨¡å‹åŸºç¡€å·²ç»å½»åº•æ”¹å˜äº†é€šç”¨é—®é¢˜è§£å†³çš„èƒ½åŠ›ï¼Œé€šè¿‡é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒå®ç°äº†å¿«é€Ÿä»»åŠ¡é€‚åº”ã€‚è¿™äº›æ¨¡å¼çš„æœ€æ–°å…³é”®è¿›å±•æ­ç¤ºäº†ä¼˜å…ˆé‡‡æ ·æŒ‘æˆ˜æ€§ä»»åŠ¡åœ¨æé«˜åˆ†å¸ƒè½¬ç§»ä¸‹çš„é€‚åº”ç¨³å¥æ€§ä¸­çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œä½œä¸ºåˆæ­¥æ­¥éª¤ï¼Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­å¯¹ä»»åŠ¡éš¾åº¦è¿›è¡Œæ’åºé€šå¸¸éœ€è¦è¿›è¡Œå…¨é¢çš„ä»»åŠ¡è¯„ä¼°ï¼Œè¿™åœ¨è®¡ç®—å’Œæ•°æ®æ ‡æ³¨æ–¹é¢å®é™…ä¸Šæ˜¯æ— æ³•æ‰¿å—çš„ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œé˜æ˜äº†åœ¨ä»»åŠ¡è¯„ä¼°å­˜åœ¨é£é™©æˆ–æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡åŒé‡é‡è¦æ€§çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººç­–ç•¥è¯„ä¼°çš„è¿­ä»£ä»£ç†ç¯å¢ƒäº¤äº’æˆ–å¾®è°ƒåŸºç¡€æ¨¡å‹çš„è®¡ç®—å¯†é›†å‹æ¨ç†æ­¥éª¤ç­‰åœºæ™¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶èµ·äº†ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚çš„æ¡¥æ¢ï¼Œä¸ºç¨³å¥çš„æ´»åŠ¨ä»»åŠ¡é‡‡æ ·æä¾›äº†ç†è®ºåŸºç¡€ã€‚MPTSé‡‡ç”¨ç”Ÿæˆæ¨¡å‹æ¥åˆ»ç”»ç‰‡æ®µä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åéªŒæ¨ç†é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©ã€‚æ‰€å¾—çš„é£é™©å­¦ä¹ è€…æ‘Šè¿˜äº†æ˜‚è´µçš„ä»»åŠ¡é€‚åº”æ€§èƒ½è¯„ä¼°æˆæœ¬ï¼Œå¹¶å¯è¯æ˜è¿‘ä¼¼ä»»åŠ¡éš¾åº¦æ’åã€‚MPTSæ— ç¼é›†æˆåˆ°é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç›‘ç£å¾®è°ƒç¯å¢ƒã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„æ¨¡å¼è¯†åˆ«å’Œåºåˆ—å†³ç­–åˆ¶å®šæ–¹é¢è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒMPTSæ˜¾è‘—æé«˜äº†å¯¹å°¾ç«¯æˆ–è¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰çš„ä»»åŠ¡çš„é€‚åº”ç¨³å¥æ€§ï¼Œå¹¶æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-rllab/MPTSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11039v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºæ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰æ¡†æ¶ï¼Œç”¨äºå®ç°ç¨³å¥çš„æ´»åŠ¨ä»»åŠ¡é‡‡æ ·ã€‚å®ƒç»“åˆä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ï¼Œé€šè¿‡ç”Ÿæˆæ¨¡å‹åˆ»ç”»ä¼˜åŒ–è¿‡ç¨‹ï¼Œé¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©ï¼Œå¹¶æ®æ­¤è¯„ä¼°ä»»åŠ¡éš¾åº¦æ’åã€‚MPTSåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç›‘ç£å¾®è°ƒåœºæ™¯ä¸­æ— ç¼é›†æˆï¼Œèƒ½æ˜¾è‘—æé«˜å¯¹å°¾éƒ¨åˆ†å¸ƒæˆ–å¼‚å¸¸ä»»åŠ¡çš„é€‚åº”ç¨³å¥æ€§ï¼ŒåŒæ—¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ç»“åˆäº†ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ç”¨äºåˆ»ç”»ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©ã€‚</li>
<li>MPTSæ¡†æ¶èƒ½è¯„ä¼°ä»»åŠ¡éš¾åº¦æ’åï¼Œå‡å°‘æ˜‚è´µçš„ä»»åŠ¡è¯„ä¼°æˆæœ¬ã€‚</li>
<li>MPTSåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç›‘æŠ›å¾®è°ƒåœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>MPTSæ˜¾è‘—æé«˜å¯¹å°¾éƒ¨åˆ†å¸ƒæˆ–å¼‚å¸¸ä»»åŠ¡çš„é€‚åº”ç¨³å¥æ€§ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒMPTSæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fad1f6841be2afc5b013a6406332684c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-157d6764a7d4b888228915e7e0718669.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41ba778d656bfeb10682e88c5d9e2586.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a4a92c19f55d55744554e7718c5c70f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Self-Corrected-Flow-Distillation-for-Consistent-One-Step-and-Few-Step-Text-to-Image-Generation"><a href="#Self-Corrected-Flow-Distillation-for-Consistent-One-Step-and-Few-Step-Text-to-Image-Generation" class="headerlink" title="Self-Corrected Flow Distillation for Consistent One-Step and Few-Step   Text-to-Image Generation"></a>Self-Corrected Flow Distillation for Consistent One-Step and Few-Step   Text-to-Image Generation</h2><p><strong>Authors:Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, Anh Tran</strong></p>
<p>Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at <a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/SCFlow">https://github.com/VinAIResearch/SCFlow</a> </p>
<blockquote>
<p>æµåŒ¹é…å·²æˆä¸ºè®­ç»ƒç”Ÿæˆæ¨¡å‹çš„ä¸€ç§æœ‰å‰é€”çš„æ¡†æ¶ï¼Œå®ƒè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»éªŒæ€§èƒ½ï¼Œä¸åŸºäºæ‰©æ•£çš„æ¨¡å‹ç›¸æ¯”ï¼Œè®­ç»ƒç›¸å¯¹å®¹æ˜“ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä»ç„¶éœ€è¦å¤§é‡å‡½æ•°è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªæ ¡æ­£æµè’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°åœ¨æµåŒ¹é…æ¡†æ¶å†…é›†æˆäº†ä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒã€‚è¿™é¡¹å·¥ä½œåœ¨å°‘æ•°æ­¥éª¤å’Œä¸€æ­¥é‡‡æ ·ä¸­éƒ½å®ç°äº†ä¸€è‡´çš„ç”Ÿæˆè´¨é‡ï¼Œå…·æœ‰å¼€åˆ›æ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨CelebA-HQå’ŒCOCOæ•°æ®é›†çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å®šé‡å’Œå®šæ€§çš„ä¼˜è¶Šç»“æœã€‚æˆ‘ä»¬çš„å®ç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/SCFlow%E3%80%82">https://github.com/VinAIResearch/SCFlowã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16906v2">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æµåŒ¹é…æ¡†æ¶åœ¨è®­ç»ƒç”Ÿæˆæ¨¡å‹æ–¹é¢çš„æ½œåŠ›ï¼Œå…¶è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œç›¸è¾ƒäºåŸºäºæ‰©æ•£çš„æ¨¡å‹æ›´æ˜“è®­ç»ƒã€‚ç„¶è€Œï¼Œé‡‡æ ·è¿‡ç¨‹ä¸­ä»éœ€å¤§é‡å‡½æ•°è¯„ä¼°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§è‡ªæˆ‘æ ¡æ­£æµè’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†ä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒé›†æˆåˆ°æµåŒ¹é…æ¡†æ¶ä¸­ã€‚æ­¤é¡¹å·¥ä½œç‡å…ˆå®ç°äº†ä¸€æ­¥åŠå°‘æ­¥é‡‡æ ·çš„ç¨³å®šç”Ÿæˆè´¨é‡ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨CelebA-HQå’ŒCOCOæ•°æ®é›†çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å®šé‡å’Œå®šæ€§çš„ä¼˜è¶Šç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµåŒ¹é…æ¡†æ¶å·²æˆä¸ºè®­ç»ƒç”Ÿæˆæ¨¡å‹çš„æœ‰åŠ›å€™é€‰è€…ï¼Œè¡¨ç°ä¼˜å¼‚ä¸”ç›¸å¯¹æ˜“è®­ç»ƒã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹ä¸­ä»éœ€è¦ä¼—å¤šå‡½æ•°è¯„ä¼°ï¼Œè¿™æ˜¯å½“å‰æ–¹æ³•çš„ä¸€ä¸ªå±€é™ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘æ ¡æ­£æµè’¸é¦æ–¹æ³•ï¼Œé›†æˆä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒåˆ°æµåŒ¹é…æ¡†æ¶ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸€æ­¥å’Œå°‘æ­¥é‡‡æ ·ä¸­å®ç°äº†ç¨³å®šçš„ç”Ÿæˆè´¨é‡ï¼Œæ˜¯ä¸šå†…çš„å…ˆé©±ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•çš„å®æ–½ç»†èŠ‚å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/SCFlow%E3%80%82">https://github.com/VinAIResearch/SCFlowã€‚</a></li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºæ”¹è¿›ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f86b64efff6a795729668adcf50d0f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e030c58585d6ccff20e233808c89fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-630fe84a82965f963cede547162378dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c87164aed3923114db2b38fb740612fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b723a2e98e7195e7621df6a2b26f34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38145dae5f71c06a2bab034a40103336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69d657040ab61bc0dd2555b49724d79f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb2d3837ceb875cc8b1fa18ed3ab8601.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  A Multi-Agent Framework Integrating Large Language Models and Generative   AI for Accelerated Metamaterial Design
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
