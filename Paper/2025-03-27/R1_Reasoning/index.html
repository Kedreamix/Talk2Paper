<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  ORION A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation"><a href="#ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation" class="headerlink" title="ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation"></a>ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation</h2><p><strong>Authors:Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai</strong></p>
<p>End-to-end (E2E) autonomous driving methods still struggle to make correct decisions in interactive closed-loop evaluation due to limited causal reasoning capability. Current methods attempt to leverage the powerful understanding and reasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma. However, the problem is still open that few VLMs for E2E methods perform well in the closed-loop evaluation due to the gap between the semantic reasoning space and the purely numerical trajectory output in the action space. To tackle this issue, we propose ORION, a holistic E2E autonomous driving framework by vision-language instructed action generation. ORION uniquely combines a QT-Former to aggregate long-term history context, a Large Language Model (LLM) for driving scenario reasoning, and a generative planner for precision trajectory prediction. ORION further aligns the reasoning space and the action space to implement a unified E2E optimization for both visual question-answering (VQA) and planning tasks. Our method achieves an impressive closed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate (SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art (SOTA) methods by a large margin of 14.28 DS and 19.61% SR. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•ç”±äºåœ¨å› æœæ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œä»ç„¶åœ¨äº¤äº’å¼é—­ç¯è¯„ä¼°ä¸­éš¾ä»¥åšå‡ºæ­£ç¡®å†³ç­–ã€‚å½“å‰çš„æ–¹æ³•è¯•å›¾åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼ºå¤§ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™ä¸€å›°å¢ƒã€‚ç„¶è€Œï¼Œé—®é¢˜åœ¨äºå¾ˆå°‘æœ‰é’ˆå¯¹E2Eæ–¹æ³•çš„VLMsåœ¨é—­ç¯è¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¿™æ˜¯ç”±äºè¯­ä¹‰æ¨ç†ç©ºé—´ä¸åŠ¨ä½œç©ºé—´ä¸­çº¯æ•°å€¼è½¨è¿¹è¾“å‡ºä¹‹é—´çš„é¸¿æ²Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ORIONï¼Œä¸€ä¸ªå…¨é¢çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æŒ‡å¯¼è¡ŒåŠ¨ç”Ÿæˆã€‚ORIONç‹¬ç‰¹åœ°ç»“åˆäº†QT-Formerä»¥æ±‡èšé•¿æœŸå†å²è¯­å¢ƒã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºé©¾é©¶åœºæ™¯æ¨ç†ï¼Œä»¥åŠç”Ÿæˆå¼è§„åˆ’å™¨ç”¨äºç²¾ç¡®è½¨è¿¹é¢„æµ‹ã€‚ORIONè¿›ä¸€æ­¥å¯¹é½æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œä»¥å®ç°è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç»Ÿä¸€ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Bench2Driveæ•°æ®é›†ä¸Šå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é—­ç¯æ€§èƒ½ï¼Œé©¾é©¶å¾—åˆ†ï¼ˆDSï¼‰è¾¾åˆ°77.74ï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰è¾¾åˆ°54.62%ï¼Œå¤§å¹…è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œå¾—åˆ†é«˜å‡º14.28 DSï¼ŒæˆåŠŸç‡é«˜å‡º19.61%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19755v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•åœ¨é—­ç¯è¯„ä¼°ä¸­ä»å­˜åœ¨å†³ç­–å›°éš¾ï¼Œå› ä¸ºç¼ºä¹å› æœæ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å°è¯•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼ºå¤§ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¯­ä¹‰æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œè¾“å‡ºä¹‹é—´çš„é¸¿æ²Ÿï¼Œç°æœ‰VLMsåœ¨é—­ç¯è¯„ä¼°ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ORIONï¼Œä¸€ä¸ªç»“åˆQT-Formerã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼è§„åˆ’å™¨çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶ã€‚ORIONå®ç°äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œå¹¶åœ¨Bench2Driveæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„é—­ç¯æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ–¹æ³•åœ¨é—­ç¯è¯„ä¼°ä¸­å­˜åœ¨å†³ç­–å›°éš¾ï¼Œå› ä¸ºç¼ºä¹å› æœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è€…æ­£å°è¯•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯­ä¹‰æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œè¾“å‡ºä¹‹é—´çš„é¸¿æ²Ÿé™åˆ¶äº†ç°æœ‰VLMsåœ¨é—­ç¯è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚</li>
<li>ORIONæ¡†æ¶ç»“åˆäº†QT-Formerã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼è§„åˆ’å™¨ï¼Œä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>ORIONå®ç°äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</li>
<li>ORIONåœ¨Bench2Driveæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„é—­ç¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5a2866d6591e42f508a7e4e60329124f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d16e53ff0e849fd6a21ac12bf522c1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b7738e3f7c73461656202c92717b8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a46ab5675ebcffdc095dba5dbe12632.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d17867080e144ed161adc2e3c4319c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Benchmarking-Spatial-Reasoning-in-Vision-Language-Models"><a href="#Mind-the-Gap-Benchmarking-Spatial-Reasoning-in-Vision-Language-Models" class="headerlink" title="Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models"></a>Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models</h2><p><strong>Authors:Ilias Stogiannidis, Steven McDonagh, Sotirios A. Tsaftaris</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged as powerful tools, excelling in tasks that integrate visual and textual comprehension, such as image captioning, visual question answering, and image-text retrieval. However, existing benchmarks for VLMs include spatial components, which often fail to isolate spatial reasoning from related tasks such as object detection or semantic comprehension. In this paper, we address these deficiencies with a multi-faceted approach towards understanding spatial reasoning. Informed by the diverse and multi-dimensional nature of human spatial reasoning abilities, we present a detailed analysis that first delineates the core elements of spatial reasoning: spatial relations, orientation and navigation, mental rotation, and spatial visualization, and then assesses the performance of these models in both synthetic and real-world images, bridging controlled and naturalistic contexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering pivotal insights into their spatial reasoning performance. Our results reveal profound shortcomings in current VLMs, with average accuracy across the 13 models approximating random chance, highlighting spatial reasoning as a persistent obstacle. This work not only exposes the pressing need to advance spatial reasoning within VLMs but also establishes a solid platform for future exploration. Code available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/stogiannidis/srbench">https://github.com/stogiannidis/srbench</a>) and dataset available on HuggingFace (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stogiannidis/srbench">https://huggingface.co/datasets/stogiannidis/srbench</a>). </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘ä½œä¸ºå¼ºå¤§çš„å·¥å…·å‡ºç°ï¼Œåœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œå›¾åƒæ–‡æœ¬æ£€ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMåŸºå‡†æµ‹è¯•é€šå¸¸åŒ…å«ç©ºé—´æˆåˆ†ï¼Œè¿™äº›æˆåˆ†å¾€å¾€æ— æ³•ä»ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹æˆ–è¯­ä¹‰ç†è§£ï¼‰ä¸­éš”ç¦»å‡ºç©ºé—´æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ–¹é¢çš„æ–¹æ³•æ¥è§£å†³è¿™äº›ä¸è¶³ï¼Œä»¥ç†è§£ç©ºé—´æ¨ç†ã€‚å—åˆ°äººç±»ç©ºé—´æ¨ç†èƒ½åŠ›å¤šæ ·æ€§å’Œå¤šç»´æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æï¼Œé¦–å…ˆé˜è¿°äº†ç©ºé—´æ¨ç†çš„æ ¸å¿ƒè¦ç´ ï¼šç©ºé—´å…³ç³»ã€æ–¹å‘å¯¼èˆªã€å¿ƒç†æ—‹è½¬å’Œç©ºé—´å¯è§†åŒ–ï¼Œç„¶åè¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨åˆæˆå›¾åƒå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸­çš„æ€§èƒ½ï¼Œåœ¨å—æ§å’Œè‡ªç„¶è¯­å¢ƒä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ã€‚æˆ‘ä»¬åˆ†æäº†1bç§æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯¹å®ƒä»¬åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„è¡¨ç°è·å¾—äº†å…³é”®è§è§£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰VLMså­˜åœ¨æ·±åˆ»ç¼ºé™·ï¼Œå¹³å‡å‡†ç¡®ç‡æ¥è¿‘éšæœºæ¦‚ç‡æ°´å¹³ï¼Œçªæ˜¾äº†ç©ºé—´æ¨ç†æ˜¯ä¸€ä¸ªæŒä¹…éšœç¢ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†æ¨åŠ¨VLMsä¸­ç©ºé—´æ¨ç†çš„è¿«åˆ‡éœ€æ±‚ï¼Œè€Œä¸”ä¸ºæœªæ¥æ¢ç´¢å»ºç«‹äº†å¯é çš„å¹³å°ã€‚ä»£ç å¯åœ¨GitHubï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/stogiannidis/srbench%EF%BC%89%E4%B8%8A%E8%8E%B7%E5%BE%97%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E5%9C%A8HuggingFace%EF%BC%88https://huggingface.co/datasets/stogiannidis/srbench%EF%BC%89%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/stogiannidis/srbenchï¼‰ä¸Šè·å¾—ï¼Œæ•°æ®é›†å¯åœ¨HuggingFaceï¼ˆhttps://huggingface.co/datasets/stogiannidis/srbenchï¼‰ä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19707v1">PDF</a> 8 main pages, 4 pages Appendix, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚ç°æœ‰VLMsåœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé¢æ–¹æ³•ï¼Œåˆ†æäº†äººç±»ç©ºé—´æ¨ç†èƒ½åŠ›çš„å¤šæ ·æ€§å’Œå¤šç»´æ€§è´¨ï¼Œå¹¶è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸­çš„æ€§èƒ½ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œå½“å‰VLMså­˜åœ¨æ˜¾è‘—çŸ­æ¿ï¼Œå¹³å‡å‡†ç¡®ç‡æ¥è¿‘éšæœºæ°´å¹³ï¼Œå‡¸æ˜¾å‡ºç©ºé—´æ¨ç†çš„éšœç¢ã€‚æœ¬æ–‡ä¸ä»…æ­ç¤ºäº†æå‡VLMsä¸­ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç´§è¿«éœ€æ±‚ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åšå®å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œå›¾åƒæ–‡æœ¬æ£€ç´¢ã€‚</li>
<li>ç°æœ‰VLMsçš„åŸºå‡†æµ‹è¯•åŒ…å«ç©ºé—´æˆåˆ†ï¼Œä½†å¾€å¾€æ— æ³•å°†ç©ºé—´æ¨ç†ä¸ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹æˆ–è¯­ä¹‰ç†è§£ï¼‰åŒºåˆ†å¼€æ¥ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å¤šé¢æ–¹æ³•åˆ†æäººç±»ç©ºé—´æ¨ç†èƒ½åŠ›çš„å¤šæ ·æ€§å’Œå¤šç»´æ€§è´¨ï¼Œå¹¶è¯„ä¼°äº†VLMsåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>åˆ†ææ¶µç›–äº†ç©ºé—´æ¨ç†çš„æ ¸å¿ƒå…ƒç´ ï¼ŒåŒ…æ‹¬ç©ºé—´å…³ç³»ã€æ–¹å‘æ„ŸçŸ¥ã€å¯¼èˆªã€å¿ƒç†æ—‹è½¬å’Œç©ºé—´å¯è§†åŒ–ã€‚</li>
<li>ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå½“å‰VLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—çŸ­æ¿ï¼Œå¹³å‡å‡†ç¡®ç‡æ¥è¿‘éšæœºæ°´å¹³ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†æå‡VLMsä¸­ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç´§è¿«éœ€æ±‚ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01a53ee6599de99f528bbeeba6b1a40f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dab53535bb85433acbfb08ddcb1025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f550b514f9f70ead608088190282ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2d4c8a79a78b1dcacdd5fe9b74622b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models"><a href="#RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models" class="headerlink" title="RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models"></a>RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models</h2><p><strong>Authors:Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen</strong></p>
<p>We introduce RGB-Th-Bench, the first benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs. While VLMs have demonstrated remarkable progress in visual reasoning and multimodal understanding, their evaluation has been predominantly limited to RGB-based benchmarks, leaving a critical gap in assessing their capabilities in infrared vision tasks. Existing visible-infrared datasets are either task-specific or lack high-quality annotations necessary for rigorous model evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive evaluation framework covering 14 distinct skill dimensions, with a total of 1,600+ expert-annotated Yes&#x2F;No questions. The benchmark employs two accuracy metrics: a standard question-level accuracy and a stricter skill-level accuracy, which evaluates model robustness across multiple questions within each skill dimension. This design ensures a thorough assessment of model performance, including resilience to adversarial and hallucinated responses. We conduct extensive evaluations on 19 state-of-the-art VLMs, revealing significant performance gaps in RGB-Thermal understanding. Our results show that even the strongest models struggle with thermal image comprehension, with performance heavily constrained by their RGB-based capabilities. Additionally, the lack of large-scale application-specific and expert-annotated thermal-caption-pair datasets in pre-training is an important reason of the observed performance gap. RGB-Th-Bench highlights the urgent need for further advancements in multimodal learning to bridge the gap between visible and thermal image understanding. The dataset is available through this link, and the evaluation code will also be made publicly available. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†RGB-Th-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç†è§£RGB-çƒ­æˆåƒå›¾åƒå¯¹çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡VLMåœ¨è§†è§‰æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶è¯„ä¼°ä¸»è¦å±€é™äºåŸºäºRGBçš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨è¯„ä¼°çº¢å¤–è§†è§‰ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚ç°æœ‰çš„å¯è§å…‰-çº¢å¤–æ•°æ®é›†æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œæˆ–è€…ç¼ºä¹è¿›è¡Œä¸¥æ ¼æ¨¡å‹è¯„ä¼°æ‰€éœ€çš„é«˜è´¨é‡æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼ŒRGB-Th-Benchæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–14ä¸ªä¸åŒçš„æŠ€èƒ½ç»´åº¦ï¼Œæ€»å…±æœ‰1600å¤šä¸ªä¸“å®¶æ³¨é‡Šçš„æ˜¯éé—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸¤ç§å‡†ç¡®ç‡æŒ‡æ ‡ï¼šæ ‡å‡†é—®é¢˜çº§å‡†ç¡®ç‡å’Œä¸¥æ ¼çš„æŠ€èƒ½çº§å‡†ç¡®ç‡ï¼Œåè€…è¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªæŠ€èƒ½ç»´åº¦å†…å¤šä¸ªé—®é¢˜ä¸Šçš„ç¨³å¥æ€§ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§å’Œè™šæ„å“åº”çš„éŸ§æ€§ã€‚æˆ‘ä»¬å¯¹19ä¸ªæœ€å…ˆè¿›çš„VLMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°åœ¨RGB-çƒ­æˆåƒç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹åœ¨çƒ­æˆåƒç†è§£æ–¹é¢ä¹Ÿé‡åˆ°å›°éš¾ï¼Œå…¶æ€§èƒ½ä¸¥é‡å—åˆ°åŸºäºRGBçš„èƒ½åŠ›çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§è§„æ¨¡åº”ç”¨ç‰¹å®šçš„å’Œä¸“å®¶æ³¨é‡Šçš„çƒ­æˆåƒå­—å¹•å¯¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒæ˜¯è§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®è·çš„é‡è¦åŸå› ã€‚RGB-Th-Benchå¼ºè°ƒäº†è¿›ä¸€æ­¥æ”¹è¿›å¤šæ¨¡æ€å­¦ä¹ ä»¥å¼¥åˆå¯è§å…‰ä¸çƒ­æˆåƒç†è§£ä¹‹é—´å·®è·çš„ç´§è¿«éœ€æ±‚ã€‚æ•°æ®é›†å¯é€šè¿‡æ­¤é“¾æ¥è·å–ï¼Œè¯„ä¼°ä»£ç ä¹Ÿå°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19654v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä»‹ç»äº†ä¸€ä¸ªåä¸ºRGB-Th-Benchçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œå®ƒæ˜¯é¦–ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹RGB-Thermalå›¾åƒå¯¹ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡VLMsåœ¨è§†è§‰æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¯„ä¼°ä¸»è¦å±€é™äºRGBåŸºå‡†æµ‹è¯•ï¼Œåœ¨çº¢å¤–è§†è§‰ä»»åŠ¡è¯„ä¼°èƒ½åŠ›æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚RGB-Th-Benchæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–14ä¸ªä¸åŒçš„æŠ€èƒ½ç»´åº¦ï¼Œæ€»å…±æœ‰1600å¤šä¸ªä¸“å®¶æ³¨é‡Šçš„æ˜¯éé—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸¤ç§å‡†ç¡®ç‡åº¦é‡æ ‡å‡†ï¼šæ ‡å‡†é—®é¢˜çº§å‡†ç¡®ç‡å’Œæ›´ä¸¥æ ¼çš„æŠ€èƒ½çº§å‡†ç¡®ç‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæŠ€èƒ½ç»´åº¦å†…å¤šä¸ªé—®é¢˜ä¸Šçš„ç¨³å¥æ€§ã€‚å¯¹19ä¸ªæœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨RGB-Thermalç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æœ€å¼ºå¤§çš„æ¨¡å‹åœ¨çƒ­å›¾åƒç†è§£æ–¹é¢ä»æœ‰å›°éš¾ï¼Œæ€§èƒ½å—åˆ°å…¶åŸºäºRGBçš„èƒ½åŠ›çš„ä¸¥é‡åˆ¶çº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RGB-Th-Benchæ˜¯é¦–ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç†è§£RGB-Thermalå›¾åƒå¯¹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºRGBå›¾åƒï¼Œä½¿å¾—åœ¨çº¢å¤–è§†è§‰ä»»åŠ¡è¯„ä¼°ä¸Šå­˜åœ¨å·®è·ã€‚</li>
<li>RGB-Th-Benchæ¶µç›–14ä¸ªæŠ€èƒ½ç»´åº¦ï¼ŒåŒ…å«1600+ä¸“å®¶æ ‡æ³¨çš„æ˜¯éé—®é¢˜ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸¤ç§å‡†ç¡®ç‡åº¦é‡æ ‡å‡†ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¯¹19ä¸ªæœ€å…ˆè¿›çš„VLMsçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨RGB-Thermalç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>æœ€å¼ºçš„æ¨¡å‹åœ¨çƒ­å›¾åƒç†è§£æ–¹é¢ä»æœ‰å›°éš¾ï¼Œåˆ¶çº¦å› ç´ ä¹‹ä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡åº”ç”¨ç‰¹å®šå’Œä¸“å®¶æ³¨é‡Šçš„çƒ­å›¾åƒæè¿°å¯¹æ•°æ®åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0a5b0cc890f0d5bb24812c3f4968f79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a493ba8ec1a57de70b64417cda26f36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05634fc4f20012f0c4d9a8bf2a175f98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa83dd7a4503f9123391c25d286d5824.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bdf9fe9b56260d24c25c628883879f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7733da6c7be120aad291b22a16afd62b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training"><a href="#1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training" class="headerlink" title="1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training"></a>1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training</h2><p><strong>Authors:Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, Xiangang Li</strong></p>
<p>The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces for general reasoning tasks, composed of high-quality and challenging reasoning problems. These problems are collected from a multitude of open-source datasets, subjected to semantic deduplication and meticulous cleaning to eliminate test set contamination. All responses within the dataset are distilled from reasoning models (predominantly DeepSeek-R1) and have undergone rigorous verification procedures. Mathematical problems are validated by checking against reference answers, code problems are verified using test cases, and other tasks are evaluated with the aid of a reward model. The AM-Distill-Qwen-32B model, which was trained through only simple Supervised Fine-Tuning (SFT) using this batch of data, outperformed the DeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model surpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We are releasing these 1.4 million problems and their corresponding responses to the research community with the objective of fostering the development of powerful reasoning-oriented Large Language Models (LLMs). The dataset was published in \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D%7Bhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}</a>. </p>
<blockquote>
<p>AM-DeepSeek-R1-Distilledæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¸¦æœ‰æ€è€ƒç—•è¿¹çš„ä¸€èˆ¬æ¨ç†ä»»åŠ¡æ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡ä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ¥è‡ªå¤šä¸ªå¼€æºæ•°æ®é›†ï¼Œç»è¿‡è¯­ä¹‰å»é‡å’Œç»†è‡´æ¸…ç†ï¼Œæ¶ˆé™¤äº†æµ‹è¯•é›†æ±¡æŸ“ã€‚æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç­”æ¡ˆéƒ½ç»è¿‡æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦æ˜¯DeepSeek-R1ï¼‰çš„æç‚¼ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯ç¨‹åºã€‚æ•°å­¦é—®é¢˜çš„ç­”æ¡ˆé€šè¿‡å¯¹ç…§å‚è€ƒç­”æ¡ˆè¿›è¡ŒéªŒè¯ï¼Œä»£ç é—®é¢˜çš„ç­”æ¡ˆé€šè¿‡ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯ï¼Œå…¶ä»–ä»»åŠ¡åˆ™å€ŸåŠ©å¥–åŠ±æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ä»…é€šè¿‡ç®€å•ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½¿ç”¨è¿™æ‰¹æ•°æ®è®­ç»ƒçš„AM-Distill-Qwen-32Bæ¨¡å‹åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†DeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹ï¼šAIME2024ã€MATH-500ã€GPQA-Diamondå’ŒLiveCodeBenchã€‚æ­¤å¤–ï¼ŒAM-Distill-Qwen-72Bæ¨¡å‹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¶…è¿‡äº†DeepSeek-R1-Distill-Llama-70Bæ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒè¿™140ä¸‡é—®é¢˜åŠç›¸åº”ç­”æ¡ˆçš„ç›®çš„æ˜¯ä¸ºäº†æ¨åŠ¨ç ”ç©¶ç¤¾åŒºå¼€å‘å¼ºå¤§çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-%E3%8C%AE%E9%A1%B5%E9%A2%98%E5%AE%B6/AM-DeepSeek-R1-Distilled-%E4%BA%8C-%E5%A3%B3-%E9%97%AE%E7%AD%BE-%E5%BC%8F-%E6%B3%A8-%E6%8E%A5-%E7%BA%BF-%E6%AF%8F-%E6%B2%BB-%E6%AC%A6-%F46%E5%8D%A1">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4Må‘å¸ƒã€‚</a>!</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19633v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹æ•°æ®é›†AM-DeepSeek-R1-Distilledä¸“æ³¨äºé€šç”¨æ¨ç†ä»»åŠ¡ï¼ŒåŒ…å«é«˜è´¨é‡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡è¯­ä¹‰å»é‡å’Œä¸¥æ ¼æ¸…ç†æ¥æ¶ˆé™¤æµ‹è¯•é›†æ±¡æŸ“ï¼Œå…¶å“åº”ç”±æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦æ˜¯DeepSeek-R1ï¼‰è’¸é¦å¾—å‡ºï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯ç¨‹åºã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œç®€å•çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒAM-Distill-Qwen-32Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºDeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†è¿™äº›åŒ…å«çº¦ä¸€åƒä¸‡é—®é¢˜çš„æ•°æ®é›†åŠå…¶å“åº”ï¼Œæ—¨åœ¨ä¿ƒè¿›å¼ºå¤§çš„æ¨ç†å¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AM-DeepSeek-R1-Distilledæ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹é€šç”¨æ¨ç†ä»»åŠ¡çš„é«˜è´¨é‡ã€æŒ‘æˆ˜æ€§æ¨ç†é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†ç»è¿‡è¯­ä¹‰å»é‡å’Œæ¸…ç†ï¼Œä»¥æ¶ˆé™¤æµ‹è¯•é›†æ±¡æŸ“ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„å“åº”ç”±æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦æ˜¯DeepSeek-R1ï¼‰è’¸é¦å¾—å‡ºï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯ç¨‹åºç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ç®€å•çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒAM-Distill-Qwen-32Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜äºDeepSeek-R1-Distillæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AM-Distill-Qwen-72Bæ¨¡å‹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†DeepSeek-R1-Distill-Llama-70Bæ¨¡å‹ã€‚</li>
<li>æ•°æ®é›†æ—¨åœ¨ä¿ƒè¿›å¯¹é€šç”¨æ¨ç†ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f6df3c97952c719378666d41a3ce4ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61c4635aa84efbe4f96ecf75988557d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77132bd24b561f2554256e0f6e60dfb5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language"><a href="#HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language" class="headerlink" title="HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language"></a>HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language</h2><p><strong>Authors:Dimitrios Stamatios Bouras, Yihan Dai, Tairan Wang, Yingfei Xiong, Sergey Mechtaev</strong></p>
<p>While software requirements are often expressed in natural language, verifying the correctness of a program against natural language requirements is a hard and underexplored problem. Large language models (LLMs) are promising candidates for addressing this challenge, however our experience shows that they are ineffective in this task, often failing to detect even straightforward bugs. To address this gap, we introduce HoarePrompt, a novel approach that adapts fundamental ideas from program analysis and verification to natural language artifacts. Drawing inspiration from the strongest postcondition calculus, HoarePrompt employs a systematic, step-by-step process in which an LLM generates natural language descriptions of reachable program states at various points in the code. To manage loops, we propose few-shot-driven k-induction, an adaptation of the k-induction method widely used in model checking. Once program states are described, HoarePrompt leverages the LLM to assess whether the program, annotated with these state descriptions, conforms to the natural language requirements. For evaluating the quality of classifiers of program correctness with respect to natural language requirements, we constructed CoCoClaNeL, a challenging dataset of solutions to programming competition problems. Our experiments show that HoarePrompt improves the MCC by 62% compared to directly using Zero-shot-CoT prompts for correctness classification. Furthermore, HoarePrompt outperforms a classifier that assesses correctness via LLM-based test generation by increasing the MCC by 93%. The inductive reasoning mechanism contributes a 28% boost to MCC, underscoring its effectiveness in managing loops. </p>
<blockquote>
<p>è™½ç„¶è½¯ä»¶éœ€æ±‚é€šå¸¸ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥è¡¨è¾¾ï¼Œä½†æ ¹æ®è‡ªç„¶è¯­è¨€è¦æ±‚éªŒè¯ç¨‹åºçš„æ­£ç¡®æ€§æ˜¯ä¸€ä¸ªå›°éš¾ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯åº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ï¼Œä½†æˆ‘ä»¬çš„ç»éªŒè¡¨æ˜ï¼Œå®ƒä»¬åœ¨æ­¤ä»»åŠ¡ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¸¸å¸¸æ— æ³•æ£€æµ‹åˆ°ç®€å•çš„é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†HoarePromptï¼Œè¿™æ˜¯ä¸€ç§å°†ç¨‹åºåˆ†æå’ŒéªŒè¯çš„åŸºæœ¬æ€æƒ³é€‚åº”åˆ°è‡ªç„¶è¯­è¨€å·¥ä»¶çš„æ–°æ–¹æ³•ã€‚HoarePromptå€Ÿé‰´äº†æœ€å¼ºçš„åæ¡ä»¶æ¼”ç®—çš„ç†å¿µï¼Œé‡‡ç”¨ç³»ç»Ÿã€åˆ†æ­¥çš„æ–¹æ³•ï¼Œå…¶ä¸­LLMä¼šåœ¨ä»£ç çš„å„ä¸ªç‚¹ç”Ÿæˆå¯è®¿é—®ç¨‹åºçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚ä¸ºäº†ç®¡ç†å¾ªç¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°‘é‡æ ·æœ¬çš„kå½’çº³æ³•ï¼Œè¿™æ˜¯å¯¹å¹¿æ³›åº”ç”¨äºæ¨¡å‹æ£€æŸ¥çš„kå½’çº³æ–¹æ³•çš„é€‚åº”ã€‚ä¸€æ—¦æè¿°äº†ç¨‹åºçŠ¶æ€ï¼ŒHoarePromptå°±ä¼šåˆ©ç”¨LLMæ¥è¯„ä¼°å¸¦æœ‰è¿™äº›çŠ¶æ€æè¿°çš„ç¨‹åºæ˜¯å¦ç¬¦åˆè‡ªç„¶è¯­è¨€è¦æ±‚ã€‚ä¸ºäº†è¯„ä¼°åˆ†ç±»å™¨å¯¹äºè‡ªç„¶è¯­è¨€è¦æ±‚çš„ç¨‹åºæ­£ç¡®æ€§çš„è´¨é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†CoCoClaNeLæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ¥è‡ªç¼–ç¨‹ç«èµ›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆçš„æŒ‘æˆ˜é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ç›´æ¥ä½¿ç”¨é›¶æ ·æœ¬CoTæç¤ºè¿›è¡Œæ­£ç¡®æ€§åˆ†ç±»ç›¸æ¯”ï¼ŒHoarePromptçš„MCCæé«˜äº†62%ã€‚æ­¤å¤–ï¼Œä¸é€šè¿‡LLMç”Ÿæˆçš„æµ‹è¯•è¯„ä¼°æ­£ç¡®æ€§çš„åˆ†ç±»å™¨ç›¸æ¯”ï¼ŒHoarePromptçš„MCCæé«˜äº†93%ã€‚å½’çº³æ¨ç†æœºåˆ¶å¯¹MCCçš„è´¡çŒ®æå‡äº†28%ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨ç®¡ç†å¾ªç¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19599v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è‡ªç„¶è¯­è¨€æè¿°çš„è½¯ä»¶éœ€æ±‚éªŒè¯é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•HoarePromptã€‚è¯¥æ–¹æ³•å€Ÿé‰´ç¨‹åºåˆ†æå’ŒéªŒè¯çš„åŸºæœ¬æ€æƒ³ï¼Œåº”ç”¨äºè‡ªç„¶è¯­è¨€å·¥ä»¶ã€‚é€šè¿‡ç³»ç»Ÿã€åˆ†æ­¥éª¤çš„è¿‡ç¨‹ï¼ŒLLMç”Ÿæˆä»£ç å„ç‚¹çš„å¯è¾¾ç¨‹åºçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚é’ˆå¯¹å¾ªç¯ç®¡ç†ï¼Œæå‡ºåŸºäºå°‘æ ·æœ¬é©±åŠ¨çš„kå½’çº³æ³•ã€‚ä½¿ç”¨LLMè¯„ä¼°æ ‡æ³¨æœ‰çŠ¶æ€æè¿°çš„ç¨‹åºçš„è‡ªç„¶è¯­è¨€è¦æ±‚ç¬¦åˆç¨‹åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒHoarePromptç›¸æ¯”ç›´æ¥ä½¿ç”¨æ–¹æ³•åˆ†ç±»å™¨æ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªç„¶è¯­è¨€æè¿°çš„è½¯ä»¶éœ€æ±‚éªŒè¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ä¸”æœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚</li>
<li>LLMsåœ¨ç›´æ¥å¤„ç†è‡ªç„¶è¯­è¨€è½¯ä»¶éœ€æ±‚éªŒè¯ä»»åŠ¡æ—¶æ•ˆæœä¸ä½³ã€‚</li>
<li>HoarePromptæ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå€Ÿé‰´ç¨‹åºåˆ†æå’ŒéªŒè¯æ¥å¤„ç†è‡ªç„¶è¯­è¨€å·¥ä»¶ã€‚</li>
<li>HoarePrompté€šè¿‡ç³»ç»Ÿæ­¥éª¤è®©LLMç”Ÿæˆä»£ç å„ç‚¹çš„ç¨‹åºçŠ¶æ€çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>é’ˆå¯¹å¾ªç¯ç®¡ç†ï¼Œé‡‡ç”¨åŸºäºå°‘æ ·æœ¬é©±åŠ¨çš„kå½’çº³æ³•ã€‚</li>
<li>ä½¿ç”¨LLMè¯„ä¼°æ ‡æ³¨ç¨‹åºçŠ¶æ€çš„ç¨‹åºçš„ç¬¦åˆè‡ªç„¶è¯­è¨€è¦æ±‚çš„ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9aac91c900d2b9664b95be1f1b5e1c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-483e45bef9eaa1c647c8a784d07e3ba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51be63da55cae30c60ebb0207831fe94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acb9ff9354c9a6291e2ec1bc9530388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30db18e69155615e68bc738105d9f4a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4210fc0b739d8df99f0cfa9f64c1cd18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e88ae79be101f21e2b916b0c38d2145.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models"><a href="#ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models" class="headerlink" title="ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models"></a>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models</h2><p><strong>Authors:Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim</strong></p>
<p>Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: <a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>. </p>
<blockquote>
<p>æ—¶ç©ºæ¨ç†åœ¨ç†è§£å„ä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œç¯å¢ƒï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶å’Œè¿åŠ¨åˆ†æä¸­ï¼Œéƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚æœ€è¿‘çš„è¿›æ­¥é€šè¿‡å¼•å…¥å¤§è§„æ¨¡æ•°æ®æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶éš¾ä»¥åˆ†æè¿åŠ¨ç‰©ä½“çš„è¡Œç¨‹è·ç¦»å’Œé€Ÿåº¦ç­‰è¿åŠ¨å­¦å…ƒç´ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ¶‰åŠè¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSTKitå’ŒSTKit-Benchã€‚å®ƒä»¬ç”±åŒ…å«3Dæ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ç»„æˆï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“çš„è¿åŠ¨åŠ¨æ€ï¼šè¡Œç¨‹è·ç¦»ã€é€Ÿåº¦ã€è¿åŠ¨æ–¹å‘ã€ç‰©ä½“é—´è·ç¦»æ¯”è¾ƒå’Œç›¸å¯¹è¿åŠ¨æ–¹å‘ã€‚ä¸ºäº†å°†æ­¤ç±»æ•°æ®æ„å»ºè¿›ä¸€æ­¥æ‰©å±•åˆ°æ²¡æœ‰3Dæ ‡ç­¾çš„è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„4Dé‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ—¶ç©ºæ¨ç†è¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ST-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºæ—¶ç©ºæ¨ç†çš„VLMï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ActivityNetã€TVQA+ï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ã€‚æœ€åï¼Œé€šè¿‡æ•´åˆå­¦åˆ°çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ï¼ŒST-VLMèƒ½å¤Ÿå®ç°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM%E3%80%82">https://ikodoh.github.io/ST-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19355v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ—¶ç©ºæ¨ç†åœ¨ç°å®ç¯å¢ƒç†è§£ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŠ¨æ€å…ƒç´ åˆ†æä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œå³STKitå’ŒSTKit-Benchã€‚è¯¥æ•°æ®é›†åŒ…å«å¸¦æœ‰3Dæ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“è¿åŠ¨çš„åŠ¨åŠ›å­¦ï¼Œå¦‚è¡Œé©¶è·ç¦»ã€é€Ÿåº¦ã€è¿åŠ¨æ–¹å‘ç­‰ã€‚ä¸ºæ‰©å¤§æ­¤ç±»æ•°æ®æ„å»ºèŒƒå›´è‡³æ— 3Dæ ‡ç­¾çš„è§†é¢‘ï¼Œæå‡ºäº†åˆ©ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„4Dé‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚é€šè¿‡æ—¶ç©ºæ¨ç†çš„åŠ¨åŠ›å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæ¨å‡ºäº†å¢å¼ºæ—¶ç©ºæ¨ç†èƒ½åŠ›çš„ST-VLMæ¨¡å‹ï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆå¦‚ActivityNetã€TVQA+ï¼‰ä¸Šä¼˜äºåŸºçº¿ã€‚æœ€åï¼Œé€šè¿‡å°†å­¦åˆ°çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ç›¸ç»“åˆï¼ŒST-VLMèƒ½å¤Ÿå®ç°å¤æ‚çš„å¤šæ­¥æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºæ¨ç†åœ¨ç°å®ç¯å¢ƒç†è§£ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æé¢†åŸŸã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŠ¨æ€å…ƒç´ åˆ†æä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†STKitå’ŒSTKit-Benchæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œè§†é¢‘åŠå…¶3Dæ³¨é‡Šï¼Œç”¨äºç‰©ä½“è¿åŠ¨åŠ¨åŠ›å­¦åˆ†æã€‚</li>
<li>ä¸ºæ‰©å¤§æ•°æ®æ„å»ºèŒƒå›´è‡³æ— 3Dæ ‡ç­¾çš„è§†é¢‘ï¼Œæå‡ºäº†åˆ©ç”¨4Dé‡å»ºç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚</li>
<li>æ¨å‡ºäº†ST-VLMæ¨¡å‹ï¼Œå¢å¼ºæ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨STKit-Benchä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d05948da4e5493c49a016f2f5cf526f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e0fc995603c14e9dbb59bec545fd16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c084e9998c5bd21ee62e3b8ab498e4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83fee94008c4eb63eb5913ba6b6fdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf26d67f83df6c8c93a5c2dc429dcce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32a68729a67a41a12a06f038493170e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2baa39b34006ef12edd50ab6a02d4a0d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ImageGen-CoT-Enhancing-Text-to-Image-In-context-Learning-with-Chain-of-Thought-Reasoning"><a href="#ImageGen-CoT-Enhancing-Text-to-Image-In-context-Learning-with-Chain-of-Thought-Reasoning" class="headerlink" title="ImageGen-CoT: Enhancing Text-to-Image In-context Learning with   Chain-of-Thought Reasoning"></a>ImageGen-CoT: Enhancing Text-to-Image In-context Learning with   Chain-of-Thought Reasoning</h2><p><strong>Authors:Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin, Yu Cheng, Lijuan Wang</strong></p>
<p>In this work, we study the problem of Text-to-Image In-Context Learning (T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in recent years, they struggle with contextual reasoning in T2I-ICL scenarios. To address this limitation, we propose a novel framework that incorporates a thought process called ImageGen-CoT prior to image generation. To avoid generating unstructured ineffective reasoning steps, we develop an automatic pipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs using this dataset to enhance their contextual reasoning capabilities. To further enhance performance, we explore test-time scale-up strategies and propose a novel hybrid scaling approach. This approach first generates multiple ImageGen-CoT chains and then produces multiple images for each chain via sampling. Extensive experiments demonstrate the effectiveness of our proposed method. Notably, fine-tuning with the ImageGen-CoT dataset leads to a substantial 80% performance gain for SEED-X on T2I-ICL tasks. See our project page at <a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/">https://ImageGen-CoT.github.io/</a>. Code and model weights will be open-sourced. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆT2I-ICLï¼‰çš„é—®é¢˜ã€‚å°½ç®¡ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œä½†åœ¨T2I-ICLåœºæ™¯ä¸­å®ƒä»¬åœ¨è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†æ—¶ä»é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒç”Ÿæˆä¹‹å‰èå…¥äº†ä¸€ä¸ªåä¸ºImageGen-CoTçš„æ€ç»´è¿‡ç¨‹ã€‚ä¸ºäº†é¿å…ç”Ÿæˆæ— åºä¸”æ— æ•ˆçš„æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨æµç¨‹æ¥åˆ›å»ºé«˜è´¨é‡çš„ImageGen-CoTæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºå…¶ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ‰©å±•æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆç”Ÿæˆå¤šä¸ªImageGen-CoTé“¾ï¼Œç„¶åé€šè¿‡é‡‡æ ·ä¸ºæ¯ä¸ªé“¾ç”Ÿæˆå¤šä¸ªå›¾åƒã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ImageGen-CoTæ•°æ®é›†è¿›è¡Œå¾®è°ƒä½¿å¾—SEED-Xåœ¨T2I-ICLä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†80%ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/%E3%80%82%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%B0%86%E5%BC%80%E6%BA%90%E3%80%82">https://ImageGen-CoT.github.io/ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†å¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19312v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/">https://ImageGen-CoT.github.io/</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆT2I-ICLï¼‰çš„é—®é¢˜ã€‚é’ˆå¯¹ç»Ÿä¸€è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨T2I-ICLåœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡æ¨ç†å›°éš¾ï¼Œæå‡ºäº†èå…¥ImageGen-CoTæ€ç»´è¿‡ç¨‹çš„æ–°æ¡†æ¶ï¼Œç”¨äºå›¾åƒç”Ÿæˆå‰çš„æ¨ç†ã€‚ä¸ºé¿å…ç”Ÿæˆæ— ç»“æ„çš„ä¸åˆç†æ¨ç†æ­¥éª¤ï¼Œå¼€å‘äº†è‡ªåŠ¨ç®¡é“æ¥åˆ›å»ºé«˜è´¨é‡çš„ImageGen-CoTæ•°æ®é›†ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºå…¶ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚è¿˜æ¢ç´¢äº†æµ‹è¯•æ—¶çš„æ‰©å±•ç­–ç•¥ï¼Œå¹¶æå‡ºäº†æ–°çš„æ··åˆæ‰©å±•æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆç”Ÿæˆå¤šä¸ªImageGen-CoTé“¾ï¼Œç„¶åé€šè¿‡é‡‡æ ·ä¸ºæ¯ä¸ªé“¾ç”Ÿæˆå¤šä¸ªå›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ImageGen-CoTæ•°æ®é›†å¾®è°ƒåï¼ŒSEED-Xåœ¨T2I-ICLä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†80%ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/%E3%80%82%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%B0%86%E5%BC%80%E6%BA%90%E3%80%82">https://ImageGen-CoT.github.io/ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†å¼€æºã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆT2I-ICLï¼‰çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»Ÿä¸€è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>é’ˆå¯¹MLLMsåœ¨T2I-ICLåœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡æ¨ç†å›°éš¾ï¼Œæå‡ºäº†èå…¥ImageGen-CoTæ€ç»´è¿‡ç¨‹çš„æ–°æ¡†æ¶ã€‚</li>
<li>ä¸ºæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨ç®¡é“æ¥åˆ›å»ºé«˜è´¨é‡çš„ImageGen-CoTæ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒMLLMsã€‚</li>
<li>æå‡ºäº†æµ‹è¯•æ—¶çš„æ‰©å±•ç­–ç•¥ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¤šä¸ªImageGen-CoTé“¾å¹¶ä¸ºæ¯ä¸ªé“¾ç”Ÿæˆå¤šä¸ªå›¾åƒçš„æ–°æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ImageGen-CoTæ•°æ®é›†å¾®è°ƒåï¼Œæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¼€æ”¾æºä»£ç å’Œæ¨¡å‹æƒé‡ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4b37b3fd31065017dd407161a65e359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e913e4c3092a3a6e54b91f4353b46dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8be7e8a78605cf0766342258184c195a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e6c055eedfe6b0217a1846e9b31545a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c25a7ab4dc71edd0231694b15d97d8f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DWIM-Towards-Tool-aware-Visual-Reasoning-via-Discrepancy-aware-Workflow-Generation-Instruct-Masking-Tuning"><a href="#DWIM-Towards-Tool-aware-Visual-Reasoning-via-Discrepancy-aware-Workflow-Generation-Instruct-Masking-Tuning" class="headerlink" title="DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow   Generation &amp; Instruct-Masking Tuning"></a>DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow   Generation &amp; Instruct-Masking Tuning</h2><p><strong>Authors:Fucai Ke, Vijay Kumar B G, Xingjian Leng, Zhixi Cai, Zaid Khan, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, Manmohan Chandraker</strong></p>
<p>Visual reasoning (VR), which is crucial in many fields for enabling human-like visual understanding, remains highly challenging. Recently, compositional visual reasoning approaches, which leverage the reasoning abilities of large language models (LLMs) with integrated tools to solve problems, have shown promise as more effective strategies than end-to-end VR methods. However, these approaches face limitations, as frozen LLMs lack tool awareness in VR, leading to performance bottlenecks. While leveraging LLMs for reasoning is widely used in other domains, they are not directly applicable to VR due to limited training data, imperfect tools that introduce errors and reduce data collection efficiency in VR, and challenging in fine-tuning on noisy workflows. To address these challenges, we propose DWIM: i) Discrepancy-aware training Workflow generation, which assesses tool usage and extracts more viable workflows for training; and ii) Instruct-Masking fine-tuning, which guides the model to only clone effective actions, enabling the generation of more practical solutions. Our experiments demonstrate that DWIM achieves state-of-the-art performance across various VR tasks, exhibiting strong generalization on multiple widely-used datasets. </p>
<blockquote>
<p>è§†è§‰æ¨ç†ï¼ˆVRï¼‰åœ¨è®¸å¤šé¢†åŸŸä¸­å¯¹äºå®ç°äººç±»èˆ¬çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸é›†æˆå·¥å…·è§£å†³é—®é¢˜çš„ç»„åˆè§†è§‰æ¨ç†æ–¹æ³•ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæ¯”ç«¯åˆ°ç«¯VRæ–¹æ³•æ›´æœ‰æ•ˆçš„ç­–ç•¥å‰æ™¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´å±€é™æ€§ï¼Œå› ä¸ºå†»ç»“çš„LLMç¼ºä¹VRä¸­çš„å·¥å…·æ„è¯†ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚è™½ç„¶åˆ©ç”¨LLMè¿›è¡Œæ¨ç†åœ¨å…¶ä»–é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®æœ‰é™ã€å·¥å…·ä¸å®Œç¾è€Œå¼•å…¥é”™è¯¯å¹¶é™ä½VRä¸­çš„æ•°æ®æ”¶é›†æ•ˆç‡ï¼Œä»¥åŠåœ¨å˜ˆæ‚å·¥ä½œæµç¨‹ä¸­å¾®è°ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå®ƒä»¬å¹¶ä¸èƒ½ç›´æ¥åº”ç”¨äºVRã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºDWIMï¼šä¸€ï¼‰å·®å¼‚æ„ŸçŸ¥è®­ç»ƒå·¥ä½œæµç¨‹ç”Ÿæˆï¼Œå®ƒè¯„ä¼°å·¥å…·ä½¿ç”¨å¹¶æå–æ›´å¤šå¯è¡Œçš„å·¥ä½œæµç¨‹ç”¨äºè®­ç»ƒï¼›äºŒ)æŒ‡ä»¤å±è”½å¾®è°ƒï¼Œå®ƒå¼•å¯¼æ¨¡å‹åªå¤åˆ¶æœ‰æ•ˆæ“ä½œï¼Œä»è€Œç”Ÿæˆæ›´å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDWIMåœ¨å„ç§VRä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19263v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰æ¨ç†ï¼ˆVRï¼‰åœ¨è®¸å¤šé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ä¸å·¥å…·é›†æˆè§£å†³é—®é¢˜çš„ç»„åˆè§†è§‰æ¨ç†æ–¹æ³•æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´å±€é™æ€§ï¼Œå› ä¸ºå†»ç»“çš„LLMsç¼ºä¹å·¥å…·æ„è¯†ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºDWIMæ–¹æ³•ï¼ŒåŒ…æ‹¬å·®å¼‚æ„ŸçŸ¥è®­ç»ƒå·¥ä½œæµç¨‹ç”Ÿæˆå’ŒæŒ‡ä»¤å±è”½å¾®è°ƒã€‚å®éªŒè¯æ˜DWIMåœ¨å„ç§è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†ï¼ˆVRï¼‰åœ¨è®¸å¤šé¢†åŸŸçš„é‡è¦æ€§åŠå…¶å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ç»„åˆè§†è§‰æ¨ç†æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´å› ä¸ºLLMsç¼ºä¹å·¥å…·æ„è¯†è€Œäº§ç”Ÿçš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>DWIMæ–¹æ³•åŒ…æ‹¬å·®å¼‚æ„ŸçŸ¥è®­ç»ƒå·¥ä½œæµç¨‹ç”Ÿæˆï¼Œè¯„ä¼°å·¥å…·ä½¿ç”¨å¹¶æå–æ›´å¤šå¯è¡Œçš„å·¥ä½œæµç¨‹ç”¨äºè®­ç»ƒã€‚</li>
<li>DWIMçš„å¦ä¸€ç§æŠ€æœ¯æ˜¯Instruct-masking fine-tuningï¼Œå¼•å¯¼æ¨¡å‹åªå¤åˆ¶æœ‰æ•ˆæ“ä½œï¼Œç”Ÿæˆæ›´å®é™…çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¯æ˜DWIMåœ¨å¤šç§è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-82b3459712ba55a2f8a84150e99f9dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65760e3a58c8609b32c0cf29a9de68ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad159a64d35b3fa75c9120b933a29920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58db9a095c1f5feb429247135651e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6ce6f2b15ea066180057216e7eb66c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Browsing-Lost-Unformed-Recollections-A-Benchmark-for-Tip-of-the-Tongue-Search-and-Reasoning"><a href="#Browsing-Lost-Unformed-Recollections-A-Benchmark-for-Tip-of-the-Tongue-Search-and-Reasoning" class="headerlink" title="Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue   Search and Reasoning"></a>Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue   Search and Reasoning</h2><p><strong>Authors:Sky CH-Wang, Darshan Deshpande, Smaranda Muresan, Anand Kannappan, Rebecca Qian</strong></p>
<p>We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multi-modal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†â€œæµè§ˆä¸¢å¤±çš„æœªæˆå½¢å›å¿†â€ï¼ˆBrowsing Lost Unformed Recollectionsï¼Œç®€ç§°BLURï¼‰è¿™ä¸€é’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†çš„èˆŒå°–å·²çŸ¥é¡¹ç›®æœç´¢ä¸æ¨ç†åŸºå‡†æµ‹è¯•ã€‚BLURå¼•å…¥äº†ä¸€ç»„ç»è¿‡ç°å®ä¸–ç•ŒéªŒè¯çš„573ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜éœ€è¦è·¨å¤šæ¨¡æ€å’Œå¤šè¯­è¨€è¾“å…¥è¿›è¡Œæœç´¢å’Œæ¨ç†ï¼Œå¹¶ç†Ÿç»ƒæŒæ¡å·¥å…·ä½¿ç”¨ï¼Œä»¥å–å¾—å“è¶Šè¡¨ç°ã€‚äººç±»å¾ˆå®¹æ˜“è§£å†³è¿™äº›é—®é¢˜ï¼ˆå¹³å‡å¾—åˆ†ä¸º98%ï¼‰ï¼Œè€Œè¡¨ç°æœ€ä½³çš„ç³»ç»Ÿçš„å¾—åˆ†çº¦ä¸º56%ã€‚ä¸ºäº†ä¿ƒè¿›é’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§å’ŒæœŸæœ›çš„ç”¨ä¾‹çš„è¿›æ­¥ï¼Œæˆ‘ä»¬é€šè¿‡å…¬å¼€æ’è¡Œæ¦œå‘å¸ƒ350ä¸ªé—®é¢˜ï¼Œä¿ç•™å…¶ä¸­250ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œå…¶ä½™ä½œä¸ºç§äººæµ‹è¯•é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æµè§ˆä¸¢å¤±æœªæˆå½¢å›å¿†ï¼ˆBLURï¼‰æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†çš„å°–ç«¯å·²çŸ¥é¡¹ç›®æœç´¢å’Œæ¨ç†åŸºå‡†æµ‹è¯•ã€‚BLURåŒ…å«573ä¸ªçœŸå®ä¸–ç•ŒéªŒè¯é—®é¢˜ï¼Œè¦æ±‚äººå·¥æ™ºèƒ½åŠ©ç†åœ¨è·¨æ¨¡æ€å’Œå¤šè¯­è¨€è¾“å…¥çš„æƒ…å†µä¸‹è¿›è¡Œæœç´¢å’Œæ¨ç†ï¼Œä»¥åŠç†Ÿç»ƒä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚äººç±»å¯ä»¥è½»æ¾è§£å†³è¿™äº›é—®é¢˜ï¼ˆå¹³å‡å¾—åˆ†98%ï¼‰ï¼Œè€Œæœ€ä½³ç³»ç»Ÿå¾—åˆ†çº¦ä¸º56%ã€‚ä¸ºäº†æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†è§£å†³è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§å’Œå‰ç»æ€§çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬å‘å¸ƒäº†350ä¸ªé—®é¢˜é€šè¿‡å…¬å…±æ’è¡Œæ¦œï¼Œä¿ç•™å¯¹å…¶ä¸­250ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œå…¶ä½™ä½œä¸ºç§æœ‰æµ‹è¯•é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BLURæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨AIåŠ©ç†çš„æœç´¢å’Œæ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒåŒ…å«573ä¸ªçœŸå®ä¸–ç•ŒéªŒè¯é—®é¢˜ï¼Œéœ€è¦è·¨æ¨¡æ€å’Œå¤šè¯­è¨€è¾“å…¥ã€‚</li>
<li>è¿™äº›é—®é¢˜è¦æ±‚AIåŠ©ç†ç†Ÿç»ƒä½¿ç”¨å·¥å…·è¿›è¡Œæœç´¢å’Œæ¨ç†ã€‚</li>
<li>äººç±»å¯ä»¥è½»æ¾è§£å†³è¿™äº›é—®é¢˜ï¼Œè€Œå½“å‰æœ€ä½³AIç³»ç»Ÿçš„è¡¨ç°è¾ƒä½ã€‚</li>
<li>350ä¸ªé—®é¢˜å°†é€šè¿‡å…¬å…±æ’è¡Œæ¦œå‘å¸ƒï¼Œå…¶ä¸­250ä¸ªé—®é¢˜çš„ç­”æ¡ˆè¢«ä¿ç•™ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨æ¨åŠ¨é€šç”¨AIåŠ©ç†åœ¨è§£å†³æŒ‘æˆ˜æ€§ç”¨ä¾‹æ–¹é¢çš„è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b779ff1edf87eda9327a2fa29417d84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf866dac599779d5a5699a282e1ba8bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e44c41f4be8fd3403809a07dc051735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4a61466c1aa80b0763084dff98e1a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f17a5de1aff763d97177599d2a201dec.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models"><a href="#SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models" class="headerlink" title="SoK: How Robust is Audio Watermarking in Generative AI models?"></a>SoK: How Robust is Audio Watermarking in Generative AI models?</h2><p><strong>Authors:Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan</strong></p>
<p>Audio watermarking is increasingly used to verify the provenance of AI-generated content, enabling applications such as detecting AI-generated speech, protecting music IP, and defending against voice cloning. To be effective, audio watermarks must resist removal attacks that distort signals to evade detection. While many schemes claim robustness, these claims are typically tested in isolation and against a limited set of attacks. A systematic evaluation against diverse removal attacks is lacking, hindering practical deployment. In this paper, we investigate whether recent watermarking schemes that claim robustness can withstand a broad range of removal attacks. First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we summarize their underlying technologies and potential vulnerabilities. We then present a large-scale empirical study to assess their robustness. To support this, we build an evaluation framework encompassing 22 types of removal attacks (109 configurations) including signal-level, physical-level, and AI-induced distortions. We reproduce 9 watermarking schemes using open-source code, identify 8 new highly effective attacks, and highlight 11 key findings that expose the fundamental limitations of these methods across 3 public datasets. Our results reveal that none of the surveyed schemes can withstand all tested distortions. This evaluation offers a comprehensive view of how current watermarking methods perform under real-world threats. Our demo and code are available at <a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/">https://sokaudiowm.github.io/</a>. </p>
<blockquote>
<p>éŸ³é¢‘æ°´å°æŠ€æœ¯è¶Šæ¥è¶Šå¤šåœ°ç”¨äºéªŒè¯äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„å‡ºå¤„ï¼Œæ”¯æŒæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è¯­éŸ³ã€ä¿æŠ¤éŸ³ä¹çŸ¥è¯†äº§æƒå’Œé˜²èŒƒå£°éŸ³å…‹éš†ç­‰åº”ç”¨ã€‚è¦æœ‰æ•ˆå‘æŒ¥ä½œç”¨ï¼ŒéŸ³é¢‘æ°´å°å¿…é¡»æŠµæŠ—èƒ½å¤Ÿæ‰­æ›²ä¿¡å·ä»¥èº²é¿æ£€æµ‹çš„å»é™¤æ”»å‡»ã€‚è™½ç„¶è®¸å¤šæ–¹æ¡ˆéƒ½å£°ç§°å…·æœ‰ç¨³å¥æ€§ï¼Œä½†è¿™äº›å£°ç§°é€šå¸¸æ˜¯å•ç‹¬æµ‹è¯•å¹¶ä¸”åœ¨æœ‰é™çš„æ”»å‡»é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚ç¼ºä¹å¯¹å¤šç§å»é™¤æ”»å‡»çš„ç³»ç»Ÿçš„è¯„ä¼°ï¼Œé˜»ç¢äº†å®é™…éƒ¨ç½²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æœ€è¿‘çš„å£°ç§°å…·æœ‰ç¨³å¥æ€§çš„æ°´å°æ–¹æ¡ˆæ˜¯å¦èƒ½å¤Ÿæ‰¿å—å¹¿æ³›çš„å»é™¤æ”»å‡»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªæ¶µç›–22ç§éŸ³é¢‘æ°´å°æ–¹æ¡ˆçš„åˆ†ç±»ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ€»ç»“äº†å®ƒä»¬çš„åŸºç¡€æŠ€æœ¯å’Œæ½œåœ¨æ¼æ´ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶æ¥è¯„ä¼°å®ƒä»¬çš„ç¨³å¥æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬22ç§ç±»å‹çš„å»é™¤æ”»å‡»ï¼ˆ109ç§é…ç½®ï¼‰ï¼ŒåŒ…æ‹¬ä¿¡å·çº§ã€ç‰©ç†çº§å’Œäººå·¥æ™ºèƒ½å¼•èµ·çš„å¤±çœŸã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºä»£ç é‡æ–°åˆ¶ä½œäº†9ç§æ°´å°æ–¹æ¡ˆï¼Œè¯†åˆ«äº†8ç§æ–°çš„é«˜åº¦æœ‰æ•ˆçš„æ”»å‡»ï¼Œå¹¶å¼ºè°ƒäº†11ä¸ªå…³é”®å‘ç°ï¼Œæ­ç¤ºäº†è¿™äº›æ–¹æ³•åœ¨3ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„æ ¹æœ¬å±€é™æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ä»»ä½•è°ƒæŸ¥è¿‡çš„æ–¹æ¡ˆèƒ½å¤Ÿæ‰¿å—æ‰€æœ‰æµ‹è¯•è¿‡çš„å¤±çœŸã€‚è¿™ä¸€è¯„ä¼°æä¾›äº†å¯¹å½“å‰æ°´å°æ–¹æ³•åœ¨ç°å®ä¸–ç•Œå¨èƒä¸‹çš„è¡¨ç°çš„ç»¼åˆè§‚ç‚¹ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://sokaudiowm.github.io/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†éŸ³é¢‘æ°´å°æŠ€æœ¯åœ¨éªŒè¯AIç”Ÿæˆå†…å®¹æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºéŸ³é¢‘æ°´å°æŠ€æœ¯éœ€è¦æŠµæŠ—ç§»é™¤æ”»å‡»ï¼Œä»¥é¿å…ä¿¡å·å¤±çœŸä»¥é€ƒé¿æ£€æµ‹ã€‚å°½ç®¡æœ‰è®¸å¤šæ–¹æ¡ˆå£°ç§°å…·æœ‰ç¨³å¥æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…åœ¨æœ‰é™çš„æ”»å‡»ç¯å¢ƒä¸‹è¿›è¡Œæµ‹è¯•ï¼Œç¼ºä¹å¯¹å„ç§ç§»é™¤æ”»å‡»çš„å…¨æ–¹ä½è¯„ä¼°ï¼Œé˜»ç¢äº†å®é™…åº”ç”¨éƒ¨ç½²ã€‚æœ¬æ–‡ä»‹ç»äº†æœ€æ–°çš„æ°´å°æ–¹æ¡ˆæ˜¯å¦ç»å¾—èµ·å¤§èŒƒå›´æ”»å‡»çš„è€ƒéªŒï¼ŒåŒæ—¶æ­ç¤ºè¿™äº›æ–¹æ³•åœ¨åº”å¯¹ç°å®å¨èƒæ—¶çš„è¡¨ç°å¦‚ä½•ã€‚æ€»ä½“è€Œè¨€ï¼Œç°æœ‰æ–¹æ¡ˆå‡æ— æ³•æŠµå¾¡æ‰€æœ‰æµ‹è¯•ä¸­çš„å¤±çœŸæ”»å‡»ã€‚è¿™é¡¹è¯„ä¼°æä¾›äº†å¯¹å½“å‰æ°´å°æ–¹æ³•åœ¨å®é™…å¨èƒä¸‹è¡¨ç°çš„å…¨é¢è§†è§’ã€‚ </p>
<p><strong>Key Takeaways</strong> </p>
<ol>
<li>éŸ³é¢‘æ°´å°æŠ€æœ¯å¹¿æ³›åº”ç”¨äºéªŒè¯AIç”Ÿæˆå†…å®¹çš„æ¥æºã€‚</li>
<li>éŸ³é¢‘æ°´å°éœ€è¦æŠµæŠ—ç§»é™¤æ”»å‡»ï¼Œä»¥é˜²ä¿¡å·å¤±çœŸé€ƒé¿æ£€æµ‹ã€‚ </li>
<li>ç›®å‰ç¼ºä¹å¯¹å„ç§ç§»é™¤æ”»å‡»çš„å…¨æ–¹ä½è¯„ä¼°æ–¹æ³•ï¼Œé˜»ç¢äº†éŸ³é¢‘æ°´å°çš„å®é™…åº”ç”¨éƒ¨ç½²ã€‚ </li>
<li>å¯¹å¤šç§æ°´å°æ–¹æ¡ˆè¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè¯„ä¼°å…¶ç¨³å¥æ€§ã€‚ </li>
<li>ç ”ç©¶ä¸­å¼•å…¥äº†æ–°çš„æ”»å‡»æ–¹å¼å¹¶æ­ç¤ºäº†ç°æœ‰æ–¹æ¡ˆçš„å±€é™æ€§ã€‚ </li>
<li>æ²¡æœ‰ä¸€ç§æ–¹æ¡ˆèƒ½å¤ŸæŠµå¾¡æ‰€æœ‰æµ‹è¯•ä¸­çš„å¤±çœŸæ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d306ac484b621e9574fecdc8675bab0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51d5f17e700fedafdd6907f1b290749b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68abb1c89524d8dcac34d99f76c530b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3736acb676bae171e1f460cf29ed7044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ff7648c6efba1ec4a8ab3c97ef303d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MIRAGE-Multimodal-Immersive-Reasoning-and-Guided-Exploration-for-Red-Team-Jailbreak-Attacks"><a href="#MIRAGE-Multimodal-Immersive-Reasoning-and-Guided-Exploration-for-Red-Team-Jailbreak-Attacks" class="headerlink" title="MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for   Red-Team Jailbreak Attacks"></a>MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for   Red-Team Jailbreak Attacks</h2><p><strong>Authors:Wenhao You, Bryan Hooi, Yiwei Wang, Youke Wang, Zong Ke, Ming-Hsuan Yang, Zi Huang, Yujun Cai</strong></p>
<p>While safety mechanisms have significantly progressed in filtering harmful text inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit their cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal jailbreak framework that exploits narrative-driven context and role immersion to circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By systematically decomposing the toxic query into environment, role, and action triplets, MIRAGE constructs a multi-turn visual storytelling sequence of images and text using Stable Diffusion, guiding the target model through an engaging detective narrative. This process progressively lowers the modelâ€™s defences and subtly guides its reasoning through structured contextual cues, ultimately eliciting harmful responses. In extensive experiments on the selected datasets with six mainstream MLLMs, MIRAGE achieves state-of-the-art performance, improving attack success rates by up to 17.5% over the best baselines. Moreover, we demonstrate that role immersion and structured semantic reconstruction can activate inherent model biases, facilitating the modelâ€™s spontaneous violation of ethical safeguards. These results highlight critical weaknesses in current multimodal safety mechanisms and underscore the urgent need for more robust defences against cross-modal threats. </p>
<blockquote>
<p>è™½ç„¶å®‰å…¨æœºåˆ¶åœ¨è¿‡æ»¤æœ‰å®³æ–‡æœ¬è¾“å…¥æ–¹é¢å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶å®¹æ˜“å—åˆ°åˆ©ç”¨è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ã€‚æˆ‘ä»¬æå‡ºäº†MIRAGEï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€è¶Šç‹±æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å™äº‹é©±åŠ¨çš„ä¸Šä¸‹æ–‡å’Œè§’è‰²æ²‰æµ¸æ¥ç»•è¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å®‰å…¨æœºåˆ¶ã€‚MIRAGEé€šè¿‡ç³»ç»Ÿåœ°å°†æœ‰æ¯’æŸ¥è¯¢åˆ†è§£æˆç¯å¢ƒã€è§’è‰²å’ŒåŠ¨ä½œä¸‰å…ƒç»„ï¼Œä½¿ç”¨Stable Diffusionæ„å»ºç”±å›¾åƒå’Œæ–‡æœ¬ç»„æˆçš„å¤šè½®è§†è§‰æ•…äº‹å™è¿°åºåˆ—ï¼Œé€šè¿‡å¼•äººå…¥èƒœçš„ä¾¦æ¢å™äº‹å¼•å¯¼ç›®æ ‡æ¨¡å‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€æ­¥é™ä½äº†æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡çº¿ç´¢å¾®å¦™åœ°å¼•å¯¼å…¶æ¨ç†ï¼Œæœ€ç»ˆå¼•å‘æœ‰å®³çš„ååº”ã€‚åœ¨å¯¹æ‰€é€‰æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒä¸­ï¼Œä¸å…­æ¬¾ä¸»æµMLLMç›¸æ¯”ï¼ŒMIRAGEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨æœ€ä½³åŸºçº¿çš„åŸºç¡€ä¸Šæé«˜äº†é«˜è¾¾17.5%çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†è§’è‰²æ²‰æµ¸å’Œç»“æ„åŒ–è¯­ä¹‰é‡å»ºå¯ä»¥æ¿€æ´»æ¨¡å‹æœ¬èº«çš„åè§ï¼Œä¿ƒä½¿æ¨¡å‹è‡ªå‘è¿åé“å¾·ä¿éšœã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å½“å‰å¤šæ¨¡æ€å®‰å…¨æœºåˆ¶çš„å…³é”®å¼±ç‚¹ï¼Œå¹¶å¼ºè°ƒäº†å¯¹æŠ—è·¨æ¨¡æ€å¨èƒçš„æ›´ç¨³å¥é˜²å¾¡çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19134v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶å…·æœ‰å…ˆè¿›çš„è¿‡æ»¤æœ‰å®³æ–‡æœ¬è¾“å…¥çš„å®‰å…¨æœºåˆ¶ï¼Œä½†ä»å®¹æ˜“å—åˆ°å¤šæ¨¡æ€çªç ´æ”»å‡»çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†MIRAGEæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºä»¥æ•…äº‹å™è¿°å¼•å¯¼æ¨¡å‹çš„å¤šæ¨¡æ€æ”»å‡»æ–¹å¼ï¼Œç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚MIRAGEé€šè¿‡å°†æœ‰æ¯’æŸ¥è¯¢åˆ†è§£ä¸ºç¯å¢ƒã€è§’è‰²å’Œè¡Œä¸ºä¸‰å…ƒç»„ï¼Œå¹¶åˆ©ç”¨Stable Diffusionæ„å»ºå›¾åƒå’Œæ–‡å­—çš„å¤šè½®æ•…äº‹å™è¿°åºåˆ—æ¥æŒ‡å¯¼ç›®æ ‡æ¨¡å‹ã€‚é€šè¿‡ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼ŒMIRAGEé€æ¸é™ä½æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›å¹¶å¼•å¯¼å…¶æ¨ç†ï¼Œæœ€ç»ˆå¼•å‘æœ‰å®³ååº”ã€‚åœ¨å¤§é‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMIRAGEåœ¨æ”»å‡»æˆåŠŸç‡æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æé«˜äº†é«˜è¾¾17.5%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜è§’è‰²æ²‰æµ¸å’Œç»“æ„åŒ–è¯­ä¹‰é‡å»ºå¯ä»¥æ¿€æ´»æ¨¡å‹å›ºæœ‰åè§ï¼Œä¿ƒä½¿æ¨¡å‹è‡ªå‘è¿åå®‰å…¨ä¿æŠ¤æªæ–½ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å®‰å…¨æœºåˆ¶çš„é‡å¤§å¼±ç‚¹ï¼Œå¹¶å¼ºè°ƒäº†å¯¹æŠ—è·¨æ¨¡æ€å¨èƒçš„æ›´å¼ºå¤§é˜²å¾¡æ‰‹æ®µçš„ç´§è¿«éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsï¼ˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å°½ç®¡åœ¨å®‰å…¨æœºåˆ¶æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´è·¨æ¨¡æ€æ”»å‡»çš„é£é™©ã€‚</li>
<li>MIRAGEæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡æ„é€ æ•…äº‹å™è¿°çš„æ–¹å¼ç»•è¿‡MLLMsçš„å®‰å…¨æœºåˆ¶ã€‚</li>
<li>MIRAGEå°†æœ‰æ¯’æŸ¥è¯¢åˆ†è§£ä¸ºç¯å¢ƒã€è§’è‰²å’Œè¡Œä¸ºä¸‰å…ƒç»„ï¼Œå¹¶ä½¿ç”¨Stable DiffusionæŠ€æœ¯æ„å»ºå›¾åƒå’Œæ–‡å­—çš„å¤šè½®å™è¿°åºåˆ—æ¥æŒ‡å¯¼æ¨¡å‹æ¨ç†ã€‚</li>
<li>MIRAGEèƒ½æœ‰æ•ˆé™ä½æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡çº¿ç´¢å¼•å¯¼æ¨¡å‹äº§ç”Ÿæœ‰å®³ååº”ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰æœ€ä½³åŸºçº¿æ–¹æ³•ï¼ŒMIRAGEåœ¨æ”»å‡»æˆåŠŸç‡ä¸Šæé«˜äº†é«˜è¾¾17.5%ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºè§’è‰²æ²‰æµ¸å’Œç»“æ„åŒ–è¯­ä¹‰é‡å»ºèƒ½å¤Ÿæ¿€æ´»æ¨¡å‹çš„å›ºæœ‰åè§ï¼Œå¯èƒ½ä½¿æ¨¡å‹è‡ªå‘è¿åå®‰å…¨ä¿æŠ¤æªæ–½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fee1975508503d35bec32009cd076956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04cc0814f44c21c6a530f52a9ea1c11.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Understanding-and-Improving-Information-Preservation-in-Prompt-Compression-for-LLMs"><a href="#Understanding-and-Improving-Information-Preservation-in-Prompt-Compression-for-LLMs" class="headerlink" title="Understanding and Improving Information Preservation in Prompt   Compression for LLMs"></a>Understanding and Improving Information Preservation in Prompt   Compression for LLMs</h2><p><strong>Authors:Weronika Åajewska, Momchil Hardalov, Laura Aina, Neha Anna John, Hang Su, LluÃ­s MÃ rquez</strong></p>
<p>Recent advancements in large language models (LLMs) have enabled their successful application to a broad range of tasks. However, in information-intensive tasks, the prompt length can grow fast, leading to increased computational requirements, performance degradation, and induced biases from irrelevant or redundant information. Recently, various prompt compression techniques have been introduced to optimize the trade-off between reducing input length and retaining performance. We propose a holistic evaluation framework that allows for in-depth analysis of prompt compression methods. We focus on three key aspects, besides compression ratio: (i) downstream task performance, (ii) grounding in the input context, and (iii) information preservation. Through this framework, we investigate state-of-the-art soft and hard compression methods, showing that they struggle to preserve key details from the original prompt, limiting their performance on complex tasks. We demonstrate that modifying soft prompting methods to control better the granularity of the compressed information can significantly improve their effectiveness â€“ up to +23% in downstream task performance, more than +8 BERTScore points in grounding, and 2.7x more entities preserved in compression. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å…¶èƒ½å¤ŸæˆåŠŸåº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨ä¿¡æ¯å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œæç¤ºé•¿åº¦å¯èƒ½ä¼šå¿«é€Ÿå¢é•¿ï¼Œå¯¼è‡´è®¡ç®—éœ€æ±‚å¢åŠ ã€æ€§èƒ½ä¸‹é™ä»¥åŠç”±æ— å…³æˆ–å†—ä½™ä¿¡æ¯å¼•èµ·çš„åè§ã€‚æœ€è¿‘ï¼Œå·²ç»æ¨å‡ºäº†å„ç§æç¤ºå‹ç¼©æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–å‡å°‘è¾“å…¥é•¿åº¦å’Œä¿ç•™æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå…è®¸å¯¹æç¤ºå‹ç¼©æ–¹æ³•è¿›è¡Œæ·±å…¥åˆ†æã€‚é™¤äº†å‹ç¼©ç‡å¤–ï¼Œæˆ‘ä»¬å…³æ³¨ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆiï¼‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œï¼ˆiiï¼‰å¯¹è¾“å…¥ä¸Šä¸‹æ–‡çš„ä¾èµ–ï¼Œï¼ˆiiiï¼‰ä¿¡æ¯ä¿ç•™ã€‚é€šè¿‡æ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æœ€æ–°çš„è½¯å‹ç¼©å’Œç¡¬å‹ç¼©æ–¹æ³•ï¼Œå‘ç°å®ƒä»¬åœ¨ä¿ç•™åŸå§‹æç¤ºçš„å…³é”®ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰é™ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¿®æ”¹è½¯æç¤ºæ–¹æ³•ä»¥æ›´å¥½åœ°æ§åˆ¶å‹ç¼©ä¿¡æ¯çš„ç²’åº¦å¯ä»¥æ˜¾è‘—æé«˜å®ƒä»¬çš„æœ‰æ•ˆæ€§â€”â€”ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æé«˜+23%ï¼Œä¾èµ–BERTScoreæé«˜è¶…è¿‡+8åˆ†ï¼Œå‹ç¼©ä¸­ä¿ç•™çš„å®ä½“æ•°é‡æé«˜è‡³åŸæ¥çš„ä¸¤å€ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19114v1">PDF</a> 21 pages, 6 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨ä¿¡æ¯å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œç”±äºæç¤ºä¿¡æ¯çš„å¢é•¿å¸¦æ¥çš„è®¡ç®—éœ€æ±‚å¢åŠ ã€æ€§èƒ½ä¸‹é™å’Œç”±æ— å…³æˆ–å†—ä½™ä¿¡æ¯å¼•èµ·çš„åè§ç­‰é—®é¢˜æ—¥ç›Šçªå‡ºã€‚ä¸ºä¼˜åŒ–è¾“å…¥é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ï¼Œå‡ºç°äº†å¤šç§æç¤ºå‹ç¼©æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œé™¤å‹ç¼©ç‡å¤–ï¼Œè¿˜å…³æ³¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€è¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„åŸºç¡€æ€§å’Œä¿¡æ¯ä¿ç•™ä¸‰ä¸ªæ–¹é¢ã€‚è°ƒæŸ¥è¡¨æ˜ï¼Œç°æœ‰è½¯ç¡¬å‹ç¼©æ–¹æ³•éš¾ä»¥ä¿ç•™åŸå§‹æç¤ºçš„å…³é”®ç»†èŠ‚ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½å—é™ã€‚é€šè¿‡æ”¹è¿›è½¯æç¤ºæ–¹æ³•ä»¥æ›´å¥½åœ°æ§åˆ¶å‹ç¼©ä¿¡æ¯çš„ç²’åº¦ï¼Œå¯æ˜¾è‘—æé«˜æœ‰æ•ˆæ€§ï¼Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æé«˜è¾¾+23%ï¼ŒåŸºç¡€æ€§èƒ½æé«˜è¶…è¿‡+8 BERTScoreç‚¹ï¼Œå‹ç¼©æ—¶ä¿ç•™å®ä½“æ•°é‡å¢åŠ 2.7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¿¡æ¯å¯†é›†å‹ä»»åŠ¡ä¸­é¢ä¸´æç¤ºä¿¡æ¯å¢é•¿å¸¦æ¥çš„é—®é¢˜ï¼ŒåŒ…æ‹¬è®¡ç®—éœ€æ±‚å¢åŠ ã€æ€§èƒ½ä¸‹é™å’Œç”±æ— å…³æˆ–å†—ä½™ä¿¡æ¯å¼•èµ·çš„åè§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå‡ºç°äº†å¤šç§æç¤ºå‹ç¼©æŠ€æœ¯ï¼Œæ—¨åœ¨ä¼˜åŒ–è¾“å…¥é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æç¤ºå‹ç¼©æ–¹æ³•çš„ä¸‰ä¸ªæ–¹é¢ï¼šä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€è¾“å…¥ä¸Šä¸‹æ–‡ä¸­çš„åŸºç¡€æ€§å’Œä¿¡æ¯ä¿ç•™ã€‚</li>
<li>ç°æœ‰è½¯ç¡¬å‹ç¼©æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸Šéš¾ä»¥ä¿ç•™åŸå§‹æç¤ºçš„å…³é”®ç»†èŠ‚ï¼Œæ€§èƒ½å—é™ã€‚</li>
<li>é€šè¿‡æ”¹è¿›è½¯æç¤ºæ–¹æ³•ä»¥æ›´å¥½åœ°æ§åˆ¶å‹ç¼©ä¿¡æ¯çš„ç²’åº¦ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æœ‰æ•ˆæ€§ã€‚</li>
<li>æ”¹è¿›åçš„æ–¹æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€åŸºç¡€æ€§èƒ½å’Œå‹ç¼©æ—¶ä¿ç•™å®ä½“æ•°é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15530d8efa363526efad247326390643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c4345b74233f347984960c5a898e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce25502c1fe64cc10ef6bb620482dc58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-023f2d5fde8f8b3b795938dbb245f8b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93c2ad54eb3d5b3f925127d3af57a7b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3878ccb002210a135d7fbc7bf4b10f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Option-Discovery-Using-LLM-guided-Semantic-Hierarchical-Reinforcement-Learning"><a href="#Option-Discovery-Using-LLM-guided-Semantic-Hierarchical-Reinforcement-Learning" class="headerlink" title="Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement   Learning"></a>Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement   Learning</h2><p><strong>Authors:Chak Lam Shek, Pratap Tokekar</strong></p>
<p>Large Language Models (LLMs) have shown remarkable promise in reasoning and decision-making, yet their integration with Reinforcement Learning (RL) for complex robotic tasks remains underexplored. In this paper, we propose an LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven subgoal selection and option reuse to enhance sample efficiency, generalization, and multi-task adaptability. Traditional RL methods often suffer from inefficient exploration and high computational cost. Hierarchical RL helps with these challenges, but existing methods often fail to reuse options effectively when faced with new tasks. To address these limitations, we introduce a three-stage framework that uses LLMs for subgoal generation given natural language description of the task, a reusable option learning and selection method, and an action-level policy, enabling more effective decision-making across diverse tasks. By incorporating LLMs for subgoal prediction and policy guidance, our approach improves exploration efficiency and enhances learning performance. On average, LDSC outperforms the baseline by 55.9% in average reward, demonstrating its effectiveness in complex RL settings. More details and experiment videos could be found in \href{<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC/%7D%7Bthis">https://raaslab.org/projects/LDSC/}{this</a> link\footnote{<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC%7D%7D">https://raaslab.org/projects/LDSC}}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’Œå†³ç­–åˆ¶å®šæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤æ‚æœºå™¨äººä»»åŠ¡ä¸­çš„èåˆä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLDSCçš„LLMå¼•å¯¼åˆ†å±‚RLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMé©±åŠ¨çš„å­ç›®æ ‡é€‰æ‹©å’Œé€‰é¡¹é‡ç”¨ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ã€é€šç”¨æ€§å’Œå¤šä»»åŠ¡é€‚åº”æ€§ã€‚ä¼ ç»ŸRLæ–¹æ³•å¸¸å¸¸é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚åˆ†å±‚RLæœ‰åŠ©äºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹æ–°ä»»åŠ¡æ—¶å¾€å¾€æ— æ³•æœ‰æ•ˆåœ°é‡ç”¨é€‰é¡¹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨LLMæ ¹æ®ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå­ç›®æ ‡ï¼Œæä¾›ä¸€ç§å¯é‡ç”¨çš„é€‰é¡¹å­¦ä¹ å’Œé€‰æ‹©æ–¹æ³•ï¼Œä»¥åŠåŠ¨ä½œçº§ç­–ç•¥ï¼Œä»è€Œåœ¨å„ç§ä»»åŠ¡ä¸­æ›´æœ‰æ•ˆåœ°è¿›è¡Œå†³ç­–åˆ¶å®šã€‚é€šè¿‡ç»“åˆLLMè¿›è¡Œå­ç›®æ ‡é¢„æµ‹å’Œæ”¿ç­–æŒ‡å¯¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ¢ç´¢æ•ˆç‡å¹¶å¢å¼ºäº†å­¦ä¹ æ•ˆæœã€‚å¹³å‡è€Œè¨€ï¼ŒLDSCåœ¨å¹³å‡å¥–åŠ±æ–¹é¢æ¯”åŸºçº¿é«˜å‡º55.9%ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚RLç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚æ›´å¤šç»†èŠ‚å’Œå®éªŒè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC/">æ­¤é“¾æ¥</a>æ‰¾åˆ°[^<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC]%E3%80%82">https://raaslab.org/projects/LDSC]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19007v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLMsä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç»“åˆåœ¨å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLLMæŒ‡å¯¼çš„åˆ†å±‚RLæ¡†æ¶ï¼Œç§°ä¸ºLDSCã€‚å®ƒåˆ©ç”¨LLMé©±åŠ¨çš„å­ç›®æ ‡é€‰æ‹©å’Œé€‰é¡¹é‡ç”¨ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œå¤šä»»åŠ¡é€‚åº”æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»ŸRLæ–¹æ³•æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œé€šè¿‡LLMè¿›è¡Œå­ç›®æ ‡ç”Ÿæˆå’Œä»»åŠ¡æè¿°çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæé«˜æ¢ç´¢æ•ˆç‡å’Œå­¦ä¹ æ€§èƒ½ã€‚LDSCåœ¨å¹³å‡å¥–åŠ±æ–¹é¢æ¯”åŸºçº¿é«˜å‡º55.9%ï¼Œåœ¨å¤æ‚çš„RLç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æœºå™¨äººå¤æ‚ä»»åŠ¡ä¸­çš„æ½œåŠ›å·¨å¤§ï¼Œç‰¹åˆ«æ˜¯ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆæ—¶ã€‚</li>
<li>LDSCæ˜¯ä¸€ç§åŸºäºLLMæŒ‡å¯¼çš„åˆ†å±‚RLæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRLæ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>LDSCåˆ©ç”¨LLMé©±åŠ¨çš„å­ç›®æ ‡é€‰æ‹©æ¥æé«˜æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œå¤šä»»åŠ¡é€‚åº”æ€§ã€‚</li>
<li>LDSCé€šè¿‡å¼•å…¥é€‰é¡¹é‡ç”¨æ–¹æ³•ï¼Œæ›´æœ‰æ•ˆåœ°åº”å¯¹æ–°ä»»åŠ¡ã€‚</li>
<li>LDSCåˆ©ç”¨LLMè¿›è¡Œå­ç›®æ ‡ç”Ÿæˆå’Œä»»åŠ¡æè¿°çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæé«˜æ¢ç´¢æ•ˆç‡å’Œå­¦ä¹ æ€§èƒ½ã€‚</li>
<li>LDSCæ¡†æ¶åœ¨å¹³å‡å¥–åŠ±æ–¹é¢æ˜¾è‘—æé«˜ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•æå‡äº†55.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f085797ab54bbbe586fda646a8f214ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-539c34fe905bc0aa95da2d0f23e68817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac0554db7c8cbf58176c2731618542f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28c8a75392296635052256193825c343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42032ea8bdb6d2f011ac926b54055074.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e229d2fcd72469613f92df5e3c3d1175.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="I-Have-Covered-All-the-Bases-Here-Interpreting-Reasoning-Features-in-Large-Language-Models-via-Sparse-Autoencoders"><a href="#I-Have-Covered-All-the-Bases-Here-Interpreting-Reasoning-Features-in-Large-Language-Models-via-Sparse-Autoencoders" class="headerlink" title="I Have Covered All the Bases Here: Interpreting Reasoning Features in   Large Language Models via Sparse Autoencoders"></a>I Have Covered All the Bases Here: Interpreting Reasoning Features in   Large Language Models via Sparse Autoencoders</h2><p><strong>Authors:Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate â€˜â€™reasoning featuresâ€™â€™ from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the modelâ€™s reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at <a target="_blank" rel="noopener" href="https://github.com/AIRI-Institute/SAE-Reasoning">https://github.com/AIRI-Institute/SAE-Reasoning</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚æœ€è¿‘çš„è¿›å±•å¯¼è‡´äº†ä¸€ç±»æ–°å‹æ¨ç†LLMçš„å‘å±•ï¼›ä¾‹å¦‚ï¼Œå¼€æºçš„DeepSeek-R1é€šè¿‡æ·±åº¦æ€è€ƒå’Œå¤æ‚æ¨ç†çš„é›†æˆï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°½ç®¡è¿™äº›èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†è¿™äº›æ¨¡å‹çš„å†…éƒ¨æ¨ç†æœºåˆ¶ä»ç„¶æœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å­¦ä¹ ç¥ç»ç½‘ç»œæ½œåœ¨è¡¨ç¤ºçš„ç¨€ç–åˆ†è§£å¹¶å°†å…¶è½¬åŒ–ä¸ºå¯è§£é‡Šç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥è¯†åˆ«é©±åŠ¨DeepSeek-R1ç³»åˆ—æ¨¡å‹ä¸­æ¨ç†çš„ç‰¹å¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»SAEè¡¨ç¤ºä¸­æå–å€™é€‰â€œæ¨ç†ç‰¹å¾â€çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å®è¯åˆ†æå’Œå¯è§£é‡Šæ€§æ–¹æ³•å¯¹è¿™äº›ç‰¹å¾è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜å®ƒä»¬ä¸æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å­˜åœ¨ç›´æ¥å…³è”ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†ç³»ç»Ÿåœ°æ§åˆ¶è¿™äº›ç‰¹å¾å¯ä»¥å¢å¼ºæ¨ç†æ€§èƒ½ï¼Œè¿™ä¸ºLLMä¸­çš„æ¨ç†æä¾›äº†ç¬¬ä¸€ä¸ªæœºæ¢°è§£é‡Šã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/AIRI-Institute/SAE-Reasoning%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AIRI-Institute/SAE-Reasoningè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18878v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œæœ€æ–°çš„è¿›å±•ä¸­å‡ºç°äº†ä¸€ç§æ–°çš„æ¨ç†å‹LLMã€‚æœ¬ç ”ç©¶ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥æ¢ç´¢DeepSeek-R1ç³»åˆ—æ¨¡å‹çš„å†…éƒ¨æ¨ç†æœºåˆ¶ã€‚é€šè¿‡æå–å€™é€‰çš„â€œæ¨ç†ç‰¹å¾â€ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›ç‰¹å¾ä¸æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç›´æ¥å…³è”ï¼Œå¹¶è¯æ˜ç³»ç»Ÿåœ°å¼•å¯¼è¿™äº›ç‰¹å¾å¯ä»¥æé«˜æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ–°å‹æ¨ç†å‹LLMçš„å‡ºç°è¿›ä¸€æ­¥æå‡äº†LLMçš„æ€§èƒ½ã€‚</li>
<li>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰è¢«ç”¨äºæ¢ç´¢DeepSeek-R1ç³»åˆ—æ¨¡å‹çš„å†…éƒ¨æ¨ç†æœºåˆ¶ã€‚</li>
<li>é€šè¿‡æå–å’ŒéªŒè¯å€™é€‰çš„â€œæ¨ç†ç‰¹å¾â€ï¼Œæ­ç¤ºäº†è¿™äº›ç‰¹å¾ä¸æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³è”ã€‚</li>
<li>å¼•å¯¼è¿™äº›â€œæ¨ç†ç‰¹å¾â€å¯ç³»ç»Ÿåœ°æé«˜æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æœºåˆ¶æä¾›äº†åˆæ­¥çš„æœºåˆ¶æ€§è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1241f8da312aecec45189939405f446d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f36a31b68957d35c2d351bfd11173c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b894919d79675a3a35a01883dda0417.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3592478ec334c8687593cc7f2a323183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddbdb35698ee997c058868060139a037.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AlphaSpace-Enabling-Robotic-Actions-through-Semantic-Tokenization-and-Symbolic-Reasoning"><a href="#AlphaSpace-Enabling-Robotic-Actions-through-Semantic-Tokenization-and-Symbolic-Reasoning" class="headerlink" title="AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and   Symbolic Reasoning"></a>AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and   Symbolic Reasoning</h2><p><strong>Authors:Alan Dao, Dinh Bach Vu, Bui Quang Huy</strong></p>
<p>This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AlphaSpaceï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‰ç»´ç¬›å¡å°”ç©ºé—´å¯¼èˆªä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›çš„æ–°å‹æ–¹æ³•ã€‚AlphaSpaceé‡‡ç”¨åŸºäºè¯­ä¹‰çš„æ ‡è®°åŒ–ç­–ç•¥ï¼Œé€šè¿‡ä¸“é—¨çš„è¯­ä¹‰æ ‡è®°å¯¹é«˜åº¦ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå¹¶ä¸»è¦æ•´åˆç¬¦å·åˆæˆæ¨ç†æ•°æ®ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—LLMèƒ½å¤Ÿå‡†ç¡®åœ°å°†ç‰©ä½“å®šä½åœ¨ç‰¹å®šçš„[xï¼Œyï¼Œz]åæ ‡ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ“çºµå­ä»»åŠ¡æ–¹é¢ï¼ŒAlphaSpaceæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ€»å‡†ç¡®ç‡è¾¾åˆ°äº†66.67%ï¼Œè€ŒGPT-4oçš„å‡†ç¡®ç‡ä¸º37.5%ï¼ŒClaude 3.5 Sonnetçš„å‡†ç¡®ç‡ä¸º29.17%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†AlphaSpaceè¿™ä¸€æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‰ç»´ç¬›å¡å°”ç©ºé—´å¯¼èˆªä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚AlphaSpaceé‡‡ç”¨åŸºäºè¯­ä¹‰çš„ä»¤ç‰ŒåŒ–ç­–ç•¥ï¼Œé€šè¿‡ä¸“ç”¨è¯­ä¹‰ä»¤ç‰Œç¼–ç é«˜åº¦ä¿¡æ¯ï¼Œå¹¶ä¸»è¦èå…¥ç¬¦å·åˆæˆæ¨ç†æ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿LLMsèƒ½å¤Ÿå‡†ç¡®åœ°å¯¹ç‰©ä½“è¿›è¡Œå®šä½æ“ä½œï¼Œå®éªŒç»“æœæ˜¾ç¤ºAlphaSpaceåœ¨æ“æ§å­ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ€»å‡†ç¡®ç‡è¾¾åˆ°66.67%ï¼Œè€ŒGPT-4oå’ŒClaude 3.5 Sonnetåˆ†åˆ«ä¸º37.5%å’Œ29.17%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaSpaceæ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚</li>
<li>AlphaSpaceé‡‡ç”¨åŸºäºè¯­ä¹‰çš„ä»¤ç‰ŒåŒ–ç­–ç•¥æ¥å¤„ç†é«˜åº¦ä¿¡æ¯ã€‚</li>
<li>AlphaSpaceä¸»è¦é€šè¿‡èå…¥ç¬¦å·åˆæˆæ¨ç†æ•°æ®æ¥æå‡LLMsçš„ç©ºé—´æ“æ§èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºAlphaSpaceåœ¨æ“æ§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>AlphaSpaceçš„æ€»å‡†ç¡®ç‡è¾¾åˆ°66.67%ï¼Œé«˜äºGPT-4oå’ŒClaude 3.5 Sonnetçš„è¡¨ç°ã€‚</li>
<li>AlphaSpaceçš„åº”ç”¨æœ‰åŠ©äºLLMsæ›´å‡†ç¡®åœ°æ‰§è¡Œç‰©ä½“å®šä½æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1dcb48e7331995109113939f9675d8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd03bd88758205fa4395abca4a4bbe44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78b622ba5cab70a498ccbd1bde8c17a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-wise-Multi-dimensional-and-Generalist-Reward-Model-with-Benchmark"><a href="#Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-wise-Multi-dimensional-and-Generalist-Reward-Model-with-Benchmark" class="headerlink" title="Boosting Virtual Agent Learning and Reasoning: A Step-wise,   Multi-dimensional, and Generalist Reward Model with Benchmark"></a>Boosting Virtual Agent Learning and Reasoning: A Step-wise,   Multi-dimensional, and Generalist Reward Model with Benchmark</h2><p><strong>Authors:Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li</strong></p>
<p>The development of Generalist Virtual Agents (GVAs) powered by Multimodal Large Language Models (MLLMs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-wise Multi-dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Galery23/Similar-v1">https://github.com/Galery23/Similar-v1</a>. </p>
<blockquote>
<p>ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨çš„å…¨èƒ½è™šæ‹Ÿä»£ç†ï¼ˆGVAsï¼‰çš„å‘å±•åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„è®­ç»ƒæ¨¡å¼é¢ä¸´å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ä¾èµ–äºç»“æœç›‘ç£å’ŒåŠ³åŠ¨å¯†é›†å‹äººå·¥æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œç›¸ä¼¼â€ï¼Œä¸€ç§é€æ­¥å¤šç»´å…¨èƒ½å¥–åŠ±æ¨¡å‹ï¼Œå®ƒä¸ºä»£ç†è®­ç»ƒæä¾›ç²¾ç»†çš„ä¿¡å·ï¼Œå¹¶ä¸ºæ¨ç†æ—¶é—´å°ºåº¦é€‰æ‹©æ›´å¥½çš„æ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆç³»ç»Ÿåœ°å®šä¹‰äº†äº”ä¸ªç»´åº¦æ¥è¯„ä¼°ä»£ç†æ“ä½œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§MCTS-Pç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è‡ªåŠ¨æ”¶é›†å’Œæ³¨é‡Šé€æ­¥çš„ã€äº”ç»´çš„ä»£ç†æ‰§è¡Œæ•°æ®ã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬ç”¨ä¸‰é‡Mç­–ç•¥è®­ç»ƒâ€œç›¸ä¼¼â€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è™šæ‹Ÿä»£ç†é¢†åŸŸä¸­ç¬¬ä¸€ä¸ªç”¨äºé€æ­¥å¤šç»´å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºSRMã€‚è¯¥åŸºå‡†æµ‹è¯•ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç”¨ä½œâ€œç›¸ä¼¼â€è®­ç»ƒé›†çš„SRMTrainå’Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹çš„SRMEvalæ‰‹åŠ¨é€‰å®šæµ‹è¯•é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œâ€œç›¸ä¼¼â€é€šè¿‡å…¶é€æ­¥å¤šç»´è¯„ä¼°å’ŒååŒå¢ç›Šï¼Œä¸ºGVAsåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´å°ºåº¦ä¸Šæä¾›äº†æœ‰æ•ˆçš„ä¸­é—´ä¿¡å·ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Galery23/Similar-v1%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Galery23/Similar-v1ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18665v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨è™šæ‹Ÿä»£ç†ï¼ˆGVAsï¼‰åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“å‰è®­ç»ƒæ¨¡å¼å­˜åœ¨ä¾èµ–ç»“æœç›‘ç£å’ŒåŠ³åŠ¨å¯†é›†å‹äººå·¥æ ‡æ³¨ç­‰å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºSimilarçš„é€æ­¥å¤šç»´é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œä¸ºä»£ç†è®­ç»ƒæä¾›ç²¾ç»†ä¿¡å·ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´ç¼©æ”¾æ—¶é€‰æ‹©æ›´å¥½çš„åŠ¨ä½œã€‚æ–‡ç« å®šä¹‰äº”ä¸ªç»´åº¦è¯„ä¼°ä»£ç†åŠ¨ä½œï¼Œè®¾è®¡MCTS-Pç®—æ³•è‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨é€æ­¥çš„äº”ç»´ä»£ç†æ‰§è¡Œæ•°æ®ã€‚æ­¤å¤–ï¼Œå¼•å…¥è™šæ‹Ÿä»£ç†é¢†åŸŸçš„é¦–ä¸ªé€æ­¥å¤šç»´å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°åŸºå‡†ï¼Œåä¸ºSRMï¼ŒåŒ…æ‹¬ç”¨äºè®­ç»ƒSimilarçš„SRMTrainå’Œç”¨äºè¯„ä¼°å¥–åŠ±æ¨¡å‹çš„SRMEvalã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimilaré€šè¿‡é€æ­¥å¤šç»´è¯„ä¼°å’ŒååŒå¢ç›Šï¼Œä¸ºGVAsåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ç¼©æ”¾æœŸé—´æä¾›æœ‰æ•ˆçš„ä¸­é—´ä¿¡å·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Generalist Virtual Agents (GVAs) powered by Multimodal Large Language Models (MLLMs) åœ¨è‡ªä¸»ä»»åŠ¡æ‰§è¡Œæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰GVAsè®­ç»ƒé¢ä¸´ä¾èµ–ç»“æœç›‘ç£å’ŒåŠ³åŠ¨å¯†é›†å‹äººå·¥æ ‡æ³¨ç­‰å±€é™æ€§ã€‚</li>
<li>Similaræ˜¯ä¸€ç§é€æ­¥å¤šç»´é€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œæä¾›ç²¾ç»†ä¿¡å·ç”¨äºä»£ç†è®­ç»ƒï¼Œå¹¶åœ¨æ¨ç†æ—¶å¸®åŠ©é€‰æ‹©æ›´å¥½çš„åŠ¨ä½œã€‚</li>
<li>å®šä¹‰äº†äº”ä¸ªç»´åº¦è¯„ä¼°ä»£ç†åŠ¨ä½œã€‚</li>
<li>ä½¿ç”¨MCTS-Pç®—æ³•è‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨ä»£ç†æ‰§è¡Œæ•°æ®ã€‚</li>
<li>å¼•å…¥è™šæ‹Ÿä»£ç†é¢†åŸŸçš„é¦–ä¸ªåŸºå‡†SRMï¼ŒåŒ…æ‹¬SRMTrainå’ŒSRMEvalï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a931af52e2894d497c517a1a841f3cb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f739315c21c403e2ca7d1a01e417fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce07ac3e319f12b147d3084fdf28bcb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ab38ff767563a24153349f862b6f2f9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Training-Free-Personalization-via-Retrieval-and-Reasoning-on-Fingerprints"><a href="#Training-Free-Personalization-via-Retrieval-and-Reasoning-on-Fingerprints" class="headerlink" title="Training-Free Personalization via Retrieval and Reasoning on   Fingerprints"></a>Training-Free Personalization via Retrieval and Reasoning on   Fingerprints</h2><p><strong>Authors:Deepayan Das, Davide Talon, Yiming Wang, Massimiliano Mancini, Elisa Ricci</strong></p>
<p>Vision Language Models (VLMs) have lead to major improvements in multimodal reasoning, yet they still struggle to understand user-specific concepts. Existing personalization methods address this limitation but heavily rely on training procedures, that can be either costly or unpleasant to individual users. We depart from existing work, and for the first time explore the training-free setting in the context of personalization. We propose a novel method, Retrieval and Reasoning for Personalization (R2P), leveraging internal knowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint, i.e., key attributes uniquely defining the concept within its semantic class. When a query arrives, the most similar fingerprints are retrieved and scored via chain-of-thought-reasoning. To reduce the risk of hallucinations, the scores are validated through cross-modal verification at the attribute level: in case of a discrepancy between the scores, R2P refines the concept association via pairwise multimodal matching, where the retrieved fingerprints and their images are directly compared with the query. We validate R2P on two publicly available benchmarks and a newly introduced dataset, Personal Concepts with Visual Ambiguity (PerVA), for concept identification highlighting challenges in visual ambiguity. R2P consistently outperforms state-of-the-art approaches on various downstream tasks across all benchmarks. Code will be available upon acceptance. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—äº†é‡å¤§æ”¹è¿›ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç†è§£ç”¨æˆ·ç‰¹å®šçš„æ¦‚å¿µã€‚ç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•è§£å†³äº†è¿™ä¸€å±€é™æ€§ï¼Œä½†ä¸¥é‡ä¾èµ–äºè®­ç»ƒç¨‹åºï¼Œè¿™å¯èƒ½æ—¢æ˜‚è´µåˆä»¤ä¸ªåˆ«ç”¨æˆ·æ„Ÿåˆ°ä¸é€‚ã€‚æˆ‘ä»¬ä¸ç°æœ‰å·¥ä½œæœ‰æ‰€ä¸åŒï¼Œé¦–æ¬¡åœ¨ä¸ªæ€§åŒ–çš„èƒŒæ™¯ä¸‹æ¢ç´¢æ— è®­ç»ƒè®¾ç½®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåä¸ºä¸ªæ€§åŒ–æ£€ç´¢ä¸æ¨ç†ï¼ˆR2Pï¼‰ï¼Œå®ƒåˆ©ç”¨VLMsçš„å†…éƒ¨çŸ¥è¯†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨VLMsæå–æ¦‚å¿µæŒ‡çº¹ï¼Œå³åœ¨å…¶è¯­ä¹‰ç±»åˆ«ä¸­å”¯ä¸€å®šä¹‰æ¦‚å¿µçš„å…³é”®å±æ€§ã€‚å½“æŸ¥è¯¢åˆ°æ¥æ—¶ï¼Œé€šè¿‡æ€ç»´é“¾æ¨ç†æ£€ç´¢æœ€ç›¸ä¼¼çš„æŒ‡çº¹å¹¶è¿›è¡Œè¯„åˆ†ã€‚ä¸ºäº†å‡å°‘å¹»è§‰çš„é£é™©ï¼Œé€šè¿‡è·¨æ¨¡æ€éªŒè¯åœ¨å±æ€§çº§åˆ«éªŒè¯è¯„åˆ†ï¼šåœ¨è¯„åˆ†ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼ŒR2Pé€šè¿‡ä¸€å¯¹ä¸€çš„å¤šæ¨¡æ€åŒ¹é…ç»†åŒ–æ¦‚å¿µå…³è”ï¼Œå…¶ä¸­æ£€ç´¢åˆ°çš„æŒ‡çº¹åŠå…¶å›¾åƒç›´æ¥ä¸æŸ¥è¯¢è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•é›†å’Œä¸€ä¸ªæ–°å¼•å…¥çš„æ•°æ®é›†Personal Concepts with Visual Ambiguityï¼ˆPerVAï¼‰ä¸ŠéªŒè¯äº†R2Påœ¨æ¦‚å¿µè¯†åˆ«æ–¹é¢çš„è¡¨ç°ï¼Œçªå‡ºäº†è§†è§‰æ¨¡ç³Šæ€§çš„æŒ‘æˆ˜ã€‚R2Påœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºå‡†æµ‹è¯•é›†çš„æœ€æ–°æ–¹æ³•ã€‚ä»£ç å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºR2Pçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨VLMsçš„å†…éƒ¨çŸ¥è¯†ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°ä¸ªæ€§åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–æ¦‚å¿µæŒ‡çº¹å’Œè¿›è¡Œé“¾å¼æ€ç»´æ¨ç†ï¼Œæ£€ç´¢æœ€ç›¸ä¼¼çš„æŒ‡çº¹å¹¶è¿›è¡Œè¯„åˆ†ã€‚ä¸ºå‡å°‘å¹»è§‰é£é™©ï¼Œé€šè¿‡è·¨æ¨¡æ€éªŒè¯å±æ€§çº§åˆ«çš„è¯„åˆ†ã€‚åœ¨å­˜åœ¨è¯„åˆ†å·®å¼‚çš„æƒ…å†µä¸‹ï¼ŒR2Pé€šè¿‡æˆå¯¹çš„å¤šæ¨¡æ€åŒ¹é…æ¥å®Œå–„æ¦‚å¿µå…³è”ã€‚è¯¥æ–¹æ³•åœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ã€æ–°å¼•å…¥çš„PerVAæ•°æ®é›†ä¸Šçš„æ¦‚å¿µè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—äº†é‡å¤§æ”¹è¿›ï¼Œä½†åœ¨ç†è§£ç”¨æˆ·ç‰¹å®šæ¦‚å¿µæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•è™½èƒ½è§£å†³æ­¤é—®é¢˜ï¼Œä½†ä¾èµ–ç¹çä¸”è€—æ—¶çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†æ— è®­ç»ƒè®¾ç½®çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œæå‡ºäº†R2Pæ–¹æ³•ã€‚</li>
<li>R2Påˆ©ç”¨VLMsæå–æ¦‚å¿µæŒ‡çº¹ï¼Œå¹¶é€šè¿‡é“¾å¼æ€ç»´æ¨ç†æ£€ç´¢å¹¶è¯„åˆ†æœ€ç›¸ä¼¼çš„æŒ‡çº¹ã€‚</li>
<li>ä¸ºå‡å°‘å¹»è§‰é£é™©ï¼ŒR2Pé‡‡ç”¨è·¨æ¨¡æ€éªŒè¯å±æ€§çº§åˆ«çš„è¯„åˆ†ã€‚</li>
<li>å½“å­˜åœ¨è¯„åˆ†å·®å¼‚æ—¶ï¼ŒR2Pé€šè¿‡æˆå¯¹çš„å¤šæ¨¡æ€åŒ¹é…å®Œå–„æ¦‚å¿µå…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b55a66b6117f3ee305754eae40af2d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47832bf33976f39f74787b175ff4fd61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cca1a1b40c6f5157ec851645557ad4c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MetaSpatial-Reinforcing-3D-Spatial-Reasoning-in-VLMs-for-the-Metaverse"><a href="#MetaSpatial-Reinforcing-3D-Spatial-Reasoning-in-VLMs-for-the-Metaverse" class="headerlink" title="MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse"></a>MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse</h2><p><strong>Authors:Zhenyu Pan, Han Liu</strong></p>
<p>We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR&#x2F;VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at <a target="_blank" rel="noopener" href="https://github.com/PzySeere/MetaSpatial">https://github.com/PzySeere/MetaSpatial</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MetaSpatialï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå®ç°å®æ—¶3Dåœºæ™¯ç”Ÿæˆè€Œæ— éœ€ç¡¬ç¼–ç ä¼˜åŒ–ã€‚MetaSpatialè§£å†³äº†ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šï¼ˆiï¼‰VLMså†…éƒ¨ç¼ºä¹3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶ç”ŸæˆçœŸå®å¸ƒå±€çš„èƒ½åŠ›ï¼›ï¼ˆiiï¼‰å¯¹äºå¸ƒå±€ç”Ÿæˆä»»åŠ¡ï¼Œä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå®Œç¾çš„çœŸå®æ ‡æ³¨ä¸å¯ç”¨ã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºé‡‡ç”¨äº†ä¸€ç§å¤šå›åˆçš„åŸºäºRLçš„ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èåˆäº†ç‰©ç†æ„ŸçŸ¥çº¦æŸå’Œæ¸²æŸ“å›¾åƒè¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„3Då¸ƒå±€è¿è´¯ã€ç‰©ç†ä¸Šå¯è¡Œä¸”å®¡ç¾ä¸Šä¸€è‡´ã€‚æ–¹æ³•ä¸Šï¼ŒMetaSpatialå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼ŒVLMé€šè¿‡åˆ†ææ¸²æŸ“è¾“å‡ºæ¥å®Œå–„ç©ºé—´å¸ƒå±€ï¼Œé€æ­¥æ”¹è¿›åœºæ™¯çš„ä¸€è‡´æ€§ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMetaSpatialæ˜¾è‘—æé«˜äº†å„ç§è§„æ¨¡æ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§å’Œæ ¼å¼ç¨³å®šæ€§ã€‚è®­ç»ƒåï¼Œå¯¹è±¡æ”¾ç½®æ›´åŠ çœŸå®ã€å¯¹é½å’ŒåŠŸèƒ½è¿è´¯ï¼ŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨å…ƒå®‡å®™ã€AR&#x2F;VRã€æ•°å­—å­ªç”Ÿå’Œæ¸¸æˆå¼€å‘åº”ç”¨ç¨‹åºä¸­çš„3Dç©ºé—´æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œè®­ç»ƒç®¡é“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PzySeere/MetaSpatial">https://github.com/PzySeere/MetaSpatial</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18470v1">PDF</a> Working Paper</p>
<p><strong>Summary</strong></p>
<p>MetaSpatialæ˜¯é¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå®ç°å®æ—¶3Dåœºæ™¯ç”Ÿæˆï¼Œæ— éœ€ç¡¬ç¼–ç ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è§£å†³äº†VLMåœ¨ç”ŸæˆçœŸå®å¸ƒå±€æ–¹é¢çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å†…åœ¨çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›å’Œä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¸ƒå±€ç”Ÿæˆä»»åŠ¡ä¸­çš„ä½æ•ˆæ€§ã€‚MetaSpatialé€šè¿‡å¤šå›åˆçš„RLä¼˜åŒ–æœºåˆ¶ï¼Œç»“åˆç‰©ç†æ„ŸçŸ¥çº¦æŸå’Œæ¸²æŸ“å›¾åƒè¯„ä¼°ï¼Œç¡®ä¿ç”Ÿæˆçš„3Då¸ƒå±€åœ¨é€»è¾‘ä¸Šè¿è´¯ã€ç‰©ç†ä¸Šå¯è¡Œå’Œç¾å­¦ä¸Šä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaSpatialæ˜¯é¦–ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MetaSpatialè§£å†³äº†VLMåœ¨ç”ŸæˆçœŸå®å¸ƒå±€æ–¹é¢çš„ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹å†…åœ¨çš„3Dç©ºé—´æ¨ç†èƒ½åŠ›æ˜¯VLMç”ŸæˆçœŸå®å¸ƒå±€çš„ä¸€ä¸ªé™åˆ¶ã€‚</li>
<li>ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¸ƒå±€ç”Ÿæˆä»»åŠ¡ä¸­çš„ä½æ•ˆæ€§æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚</li>
<li>MetaSpatialé€šè¿‡å¤šå›åˆçš„RLä¼˜åŒ–æœºåˆ¶æ¥ç¡®ä¿ç”Ÿæˆçš„3Då¸ƒå±€çš„è´¨é‡ã€‚</li>
<li>MetaSpatialç»“åˆäº†ç‰©ç†æ„ŸçŸ¥çº¦æŸå’Œæ¸²æŸ“å›¾åƒè¯„ä¼°æ¥ç¡®ä¿ç”Ÿæˆçš„å¸ƒå±€åœ¨é€»è¾‘ä¸Šè¿è´¯ã€ç‰©ç†ä¸Šå¯è¡Œå’Œç¾å­¦ä¸Šä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9a3097db8d905794c82d950a1274ae88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106860fd65527a76611b25de4bdc9519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f965290fe05553f063fc920b3ff4cd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PALATE-Peculiar-Application-of-the-Law-of-Total-Expectation-to-Enhance-the-Evaluation-of-Deep-Generative-Models"><a href="#PALATE-Peculiar-Application-of-the-Law-of-Total-Expectation-to-Enhance-the-Evaluation-of-Deep-Generative-Models" class="headerlink" title="PALATE: Peculiar Application of the Law of Total Expectation to Enhance   the Evaluation of Deep Generative Models"></a>PALATE: Peculiar Application of the Law of Total Expectation to Enhance   the Evaluation of Deep Generative Models</h2><p><strong>Authors:Tadeusz Dziarmaga, Marcin KÄ…dzioÅ‚ka, Artur Kasymov, Marcin Mazur</strong></p>
<p>Deep generative models (DGMs) have caused a paradigm shift in the field of machine learning, yielding noteworthy advancements in domains such as image synthesis, natural language processing, and other related areas. However, a comprehensive evaluation of these models that accounts for the trichotomy between fidelity, diversity, and novelty in generated samples remains a formidable challenge. A recently introduced solution that has emerged as a promising approach in this regard is the Feature Likelihood Divergence (FLD), a method that offers a theoretically motivated practical tool, yet also exhibits some computational challenges. In this paper, we propose PALATE, a novel enhancement to the evaluation of DGMs that addresses limitations of existing metrics. Our approach is based on a peculiar application of the law of total expectation to random variables representing accessible real data. When combined with the MMD baseline metric and DINOv2 feature extractor, PALATE offers a holistic evaluation framework that matches or surpasses state-of-the-art solutions while providing superior computational efficiency and scalability to large-scale datasets. Through a series of experiments, we demonstrate the effectiveness of the PALATE enhancement, contributing a computationally efficient, holistic evaluation approach that advances the field of DGMs assessment, especially in detecting sample memorization and evaluating generalization capabilities. </p>
<blockquote>
<p>æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•èµ·äº†èŒƒå¼è½¬å˜ï¼Œåœ¨å›¾åƒåˆæˆã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†å€¼å¾—æ³¨æ„çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹ç”Ÿæˆçš„æ ·æœ¬ä¸­ä¿çœŸåº¦ã€å¤šæ ·æ€§å’Œæ–°é¢–æ€§ä¸‰è€…ä¹‹é—´çš„ä¸‰åˆ†å…³ç³»è¿›è¡Œå…¨é¢è¯„ä¼°ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘å‡ºç°çš„ä¸€ç§å…·æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆæ˜¯ç‰¹å¾ä¼¼ç„¶æ•£åº¦ï¼ˆFLDï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§æä¾›ç†è®ºé©±åŠ¨çš„å®ç”¨å·¥å…·çš„æ–¹æ³•ï¼Œä½†ä¹Ÿè¡¨ç°å‡ºä¸€äº›è®¡ç®—æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹DGMsè¯„ä¼°çš„æ–°å‹å¢å¼ºæ–¹æ³•PALATEï¼Œè§£å†³äº†ç°æœ‰æŒ‡æ ‡çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå¯¹å¯è®¿é—®å®é™…æ•°æ®ä»£è¡¨çš„éšæœºå˜é‡åº”ç”¨å…¨æœŸæœ›å®šå¾‹çš„ä¸€ç§ç‰¹æ®Šåº”ç”¨ã€‚å½“ä¸MMDåŸºçº¿æŒ‡æ ‡å’ŒDINOv2ç‰¹å¾æå–å™¨ç»“åˆæ—¶ï¼ŒPALATEæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨åŒ¹é…æˆ–è¶…è¶Šæœ€æ–°è§£å†³æ–¹æ¡ˆçš„åŒæ—¶ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é›†æä¾›äº†ä¼˜è¶Šçš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†PALATEå¢å¼ºçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ·±åº¦ç”Ÿæˆæ¨¡å‹è¯„ä¼°é¢†åŸŸæå‡ºäº†ä¸€ç§è®¡ç®—é«˜æ•ˆã€å…¨é¢çš„è¯„ä¼°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æ ·æœ¬è®°å¿†å’Œè¯„ä¼°æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•èµ·äº†èŒƒå¼è½¬å˜ï¼Œå¹¶åœ¨å›¾åƒåˆæˆã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹æ¨¡å‹çš„å…¨é¢è¯„ä¼°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦è€ƒè™‘åˆ°ç”Ÿæˆæ ·æœ¬çš„é€¼çœŸåº¦ã€å¤šæ ·æ€§å’Œæ–°é¢–æ€§ä¹‹é—´çš„ä¸‰å…ƒå…³ç³»ã€‚æœ€è¿‘å‡ºç°äº†ä¸€ç§åä¸ºç‰¹å¾å¯èƒ½æ€§å‘æ•£ï¼ˆFLDï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå°½ç®¡è¯¥æ–¹æ³•æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å·¥å…·ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºPALATEï¼Œä¸€ç§é’ˆå¯¹DGMsè¯„ä¼°çš„æ–°å‹å¢å¼ºæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æŒ‡æ ‡çš„é™åˆ¶ã€‚PALATEåŸºäºå¯è®¿é—®çœŸå®æ•°æ®ä»£è¡¨çš„éšæœºå˜é‡çš„æ€»æœŸæœ›å®šå¾‹çš„ç‰¹æ®Šåº”ç”¨ã€‚ä¸MMDåŸºçº¿æŒ‡æ ‡å’ŒDINOv2ç‰¹å¾æå–å™¨ç»“åˆï¼ŒPALATEæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œåœ¨åŒ¹é…æˆ–è¶…è¶Šæœ€æ–°è§£å†³æ–¹æ¡ˆçš„åŒæ—¶ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®é›†æä¾›äº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜ï¼ŒPALATEå¢å¼ºæ–¹æ³•æœ‰æ•ˆï¼Œä¸ºDGMsè¯„ä¼°é¢†åŸŸæä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆã€å…¨é¢çš„è¯„ä¼°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æ ·æœ¬è®°å¿†å’Œè¯„ä¼°æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæœ‰é‡å¤§çªç ´ï¼Œå°¤å…¶åœ¨å›¾åƒåˆæˆå’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚</li>
<li>å¯¹DGMsçš„è¯„ä¼°éœ€è¦ç»¼åˆè€ƒè™‘ç”Ÿæˆæ ·æœ¬çš„é€¼çœŸåº¦ã€å¤šæ ·æ€§å’Œæ–°é¢–æ€§ã€‚</li>
<li>ç‰¹å¾å¯èƒ½æ€§å‘æ•£ï¼ˆFLDï¼‰æ˜¯ä¸€ç§æ–°å…´çš„DGMsè¯„ä¼°æ–¹æ³•ï¼Œä½†å­˜åœ¨è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>PALATEæ˜¯ä¸€ç§æ–°å‹çš„DGMsè¯„ä¼°å¢å¼ºæ–¹æ³•ï¼ŒåŸºäºæ€»æœŸæœ›å®šå¾‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŒ‡æ ‡çš„é™åˆ¶ã€‚</li>
<li>PALATEä¸MMDåŸºçº¿æŒ‡æ ‡å’ŒDINOv2ç‰¹å¾æå–å™¨ç»“åˆï¼Œæä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå…·æœ‰é«˜æ•ˆè®¡ç®—å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>å®éªŒè¯æ˜PALATEå¢å¼ºæ–¹æ³•æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹æ ·æœ¬è®°å¿†å’Œè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fc1e2df360a3a3e0f3a0e4dfbae962d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef95342bd7801b296a9beb92668e6a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b022379f3aa8285b24e663417803a175.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding"><a href="#On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding" class="headerlink" title="On the Perception Bottleneck of VLMs for Chart Understanding"></a>On the Perception Bottleneck of VLMs for Chart Understanding</h2><p><strong>Authors:Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He</strong></p>
<p>Chart understanding requires models to effectively analyze and reason about numerical data, textual elements, and complex visual components. Our observations reveal that the perception capabilities of existing large vision-language models (LVLMs) constitute a critical bottleneck in this process. In this study, we delve into this perception bottleneck by decomposing it into two components: the vision encoder bottleneck, where the visual representation may fail to encapsulate the correct information, and the extraction bottleneck, where the language model struggles to extract the necessary information from the provided visual representations. Through comprehensive experiments, we find that (1) the information embedded within visual representations is substantially richer than what is typically captured by linear extractors, such as the widely used retrieval accuracy metric; (2) While instruction tuning effectively enhances the extraction capability of LVLMs, the vision encoder remains a critical bottleneck, demanding focused attention and improvement. Therefore, we further enhance the visual encoder to mitigate the vision encoder bottleneck under a contrastive learning framework. Empirical results demonstrate that our approach significantly mitigates the perception bottleneck and improves the ability of LVLMs to comprehend charts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>. </p>
<blockquote>
<p>å›¾è¡¨ç†è§£éœ€è¦æ¨¡å‹æœ‰æ•ˆåœ°åˆ†æå’Œæ¨ç†æ•°å€¼æ•°æ®ã€æ–‡æœ¬å…ƒç´ å’Œå¤æ‚çš„è§†è§‰æˆåˆ†ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿè¡¨æ˜ï¼Œç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›æ„æˆäº†è¿™ä¸€è¿‡ç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†æ¥æ·±å…¥ç ”ç©¶è¿™ä¸€æ„ŸçŸ¥ç“¶é¢ˆï¼šè§†è§‰ç¼–ç å™¨ç“¶é¢ˆï¼Œå…¶ä¸­è§†è§‰è¡¨ç¤ºå¯èƒ½æ— æ³•åŒ…å«æ­£ç¡®çš„ä¿¡æ¯ï¼›å’Œæå–ç“¶é¢ˆï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹éš¾ä»¥ä»æä¾›çš„è§†è§‰è¡¨ç¤ºä¸­æå–å¿…è¦çš„ä¿¡æ¯ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼ˆ1ï¼‰è§†è§‰è¡¨ç¤ºä¸­æ‰€åµŒå…¥çš„ä¿¡æ¯è¿œæ¯”çº¿æ€§æå–å™¨ï¼ˆå¦‚å¹¿æ³›ä½¿ç”¨çš„æ£€ç´¢å‡†ç¡®ç‡æŒ‡æ ‡ï¼‰æ‰€æ•è·çš„è¦ä¸°å¯Œå¾—å¤šï¼›ï¼ˆ2ï¼‰è™½ç„¶æŒ‡ä»¤è°ƒæ•´æœ‰æ•ˆåœ°æé«˜äº†LVLMsçš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å’Œæ”¹è¿›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸‹è¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ï¼Œä»¥ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ç¼“è§£äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†LVLMsç†è§£å›¾è¡¨çš„èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart%E4%B8%8A%E3%80%82">https://github.com/hkust-nlp/Vision4Chartä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18435v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾è¡¨ç†è§£æ—¶çš„ç“¶é¢ˆé—®é¢˜ï¼Œå°†å…¶åˆ†è§£ä¸ºè§†è§‰ç¼–ç å™¨ç“¶é¢ˆå’Œä¿¡æ¯æå–ç“¶é¢ˆä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè§†è§‰è¡¨ç¤ºçš„ä¿¡æ¯ä¸°å¯Œç¨‹åº¦è¶…è¿‡çº¿æ€§æå–å™¨çš„æ•æ‰èƒ½åŠ›ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒè™½èƒ½æé«˜è¯­è¨€æ¨¡å‹çš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚ä¸ºç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆï¼Œç ”ç©¶åœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸‹å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡è½»äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å›¾è¡¨çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾è¡¨ç†è§£éœ€è¦æ¨¡å‹åˆ†ææ•°å€¼æ•°æ®ã€æ–‡æœ¬å…ƒç´ å’Œå¤æ‚è§†è§‰æˆåˆ†ã€‚</li>
<li>ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥è¿‡ç¨‹ä¸­å­˜åœ¨å…³é”®ç“¶é¢ˆã€‚</li>
<li>è§†è§‰è¡¨ç¤ºçš„ä¿¡æ¯ä¸°å¯Œç¨‹åº¦è¶…è¿‡çº¿æ€§æå–å™¨çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæé«˜äº†è¯­è¨€æ¨¡å‹çš„æå–èƒ½åŠ›ï¼Œä½†è§†è§‰ç¼–ç å™¨ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸‹å¢å¼ºäº†è§†è§‰ç¼–ç å™¨ä»¥ç¼“è§£è§†è§‰ç¼–ç å™¨ç“¶é¢ˆã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—å‡è½»äº†æ„ŸçŸ¥ç“¶é¢ˆï¼Œæé«˜äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å›¾è¡¨çš„ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44af51b11fa63a1eb20ce5ea5246515c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63a7596c12fd7308cb9c1c5e95bc870e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc82eda912ecb1ca48d328289b07f47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c4d962c16e96c7d3cab5fe875f9970c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-709fcb5d7cf493b5393161df9d10f8e3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1fd0df86edf04907f3a27458febd4eb5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  CoLLM A Large Language Model for Composed Image Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8b5ba8281591458d5956e0ce43c3bee.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-24  ReLearn Unlearning via Learning for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
