<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-27  ORION A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-27-更新"><a href="#2025-03-27-更新" class="headerlink" title="2025-03-27 更新"></a>2025-03-27 更新</h1><h2 id="ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation"><a href="#ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation" class="headerlink" title="ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation"></a>ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation</h2><p><strong>Authors:Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai</strong></p>
<p>End-to-end (E2E) autonomous driving methods still struggle to make correct decisions in interactive closed-loop evaluation due to limited causal reasoning capability. Current methods attempt to leverage the powerful understanding and reasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma. However, the problem is still open that few VLMs for E2E methods perform well in the closed-loop evaluation due to the gap between the semantic reasoning space and the purely numerical trajectory output in the action space. To tackle this issue, we propose ORION, a holistic E2E autonomous driving framework by vision-language instructed action generation. ORION uniquely combines a QT-Former to aggregate long-term history context, a Large Language Model (LLM) for driving scenario reasoning, and a generative planner for precision trajectory prediction. ORION further aligns the reasoning space and the action space to implement a unified E2E optimization for both visual question-answering (VQA) and planning tasks. Our method achieves an impressive closed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate (SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art (SOTA) methods by a large margin of 14.28 DS and 19.61% SR. </p>
<blockquote>
<p>端到端（E2E）自动驾驶方法由于在因果推理能力上的局限性，仍然在交互式闭环评估中难以做出正确决策。当前的方法试图利用视觉语言模型（VLMs）的强大理解和推理能力来解决这一困境。然而，问题在于很少有针对E2E方法的VLMs在闭环评估中表现良好，这是由于语义推理空间与动作空间中纯数值轨迹输出之间的鸿沟。为了解决这个问题，我们提出了ORION，一个全面的端到端自动驾驶框架，通过视觉语言指导行动生成。ORION独特地结合了QT-Former以汇聚长期历史语境、大型语言模型（LLM）用于驾驶场景推理，以及生成式规划器用于精确轨迹预测。ORION进一步对齐推理空间和动作空间，以实现视觉问答（VQA）和规划任务的统一端到端优化。我们的方法在Bench2Drive数据集上实现了令人印象深刻的闭环性能，驾驶得分（DS）达到77.74，成功率（SR）达到54.62%，大幅超越了现有先进技术（SOTA）方法，得分高出14.28 DS，成功率高出19.61%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19755v1">PDF</a> </p>
<p><strong>Summary</strong>：端到端（E2E）自动驾驶方法在闭环评估中仍存在决策困难，因为缺乏因果推理能力。为解决这个问题，研究者尝试利用视觉语言模型（VLMs）的强大理解和推理能力。然而，由于语义推理空间和动作输出之间的鸿沟，现有VLMs在闭环评估中的表现并不理想。为解决这一问题，本文提出了ORION，一个结合QT-Former、大型语言模型（LLM）和生成式规划器的端到端自动驾驶框架。ORION实现了视觉问答（VQA）和规划任务的端到端优化，并在Bench2Drive数据集上取得了显著的闭环性能，显著优于其他先进方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>现有端到端自动驾驶方法在闭环评估中存在决策困难，因为缺乏因果推理能力。</li>
<li>研究者正尝试利用视觉语言模型（VLMs）解决这一问题。</li>
<li>语义推理空间和动作输出之间的鸿沟限制了现有VLMs在闭环评估中的表现。</li>
<li>ORION框架结合了QT-Former、大型语言模型（LLM）和生成式规划器，以解决这个问题。</li>
<li>ORION实现了视觉问答（VQA）和规划任务的端到端优化。</li>
<li>ORION在Bench2Drive数据集上取得了显著的闭环性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5a2866d6591e42f508a7e4e60329124f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d16e53ff0e849fd6a21ac12bf522c1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b7738e3f7c73461656202c92717b8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a46ab5675ebcffdc095dba5dbe12632.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d17867080e144ed161adc2e3c4319c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Benchmarking-Spatial-Reasoning-in-Vision-Language-Models"><a href="#Mind-the-Gap-Benchmarking-Spatial-Reasoning-in-Vision-Language-Models" class="headerlink" title="Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models"></a>Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models</h2><p><strong>Authors:Ilias Stogiannidis, Steven McDonagh, Sotirios A. Tsaftaris</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged as powerful tools, excelling in tasks that integrate visual and textual comprehension, such as image captioning, visual question answering, and image-text retrieval. However, existing benchmarks for VLMs include spatial components, which often fail to isolate spatial reasoning from related tasks such as object detection or semantic comprehension. In this paper, we address these deficiencies with a multi-faceted approach towards understanding spatial reasoning. Informed by the diverse and multi-dimensional nature of human spatial reasoning abilities, we present a detailed analysis that first delineates the core elements of spatial reasoning: spatial relations, orientation and navigation, mental rotation, and spatial visualization, and then assesses the performance of these models in both synthetic and real-world images, bridging controlled and naturalistic contexts. We analyze 13 state-of-the-art Vision-Language Models, uncovering pivotal insights into their spatial reasoning performance. Our results reveal profound shortcomings in current VLMs, with average accuracy across the 13 models approximating random chance, highlighting spatial reasoning as a persistent obstacle. This work not only exposes the pressing need to advance spatial reasoning within VLMs but also establishes a solid platform for future exploration. Code available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/stogiannidis/srbench">https://github.com/stogiannidis/srbench</a>) and dataset available on HuggingFace (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stogiannidis/srbench">https://huggingface.co/datasets/stogiannidis/srbench</a>). </p>
<blockquote>
<p>视觉语言模型（VLMs）最近作为强大的工具出现，在集成视觉和文本理解的任务中表现出色，例如图像描述、视觉问答和图像文本检索。然而，现有的VLM基准测试通常包含空间成分，这些成分往往无法从相关任务（如目标检测或语义理解）中隔离出空间推理。在本文中，我们通过多方面的方法来解决这些不足，以理解空间推理。受到人类空间推理能力多样性和多维性的启发，我们进行了详细的分析，首先阐述了空间推理的核心要素：空间关系、方向导航、心理旋转和空间可视化，然后评估了这些模型在合成图像和真实世界图像中的性能，在受控和自然语境之间建立了桥梁。我们分析了1b种最先进的视觉语言模型，对它们在空间推理方面的表现获得了关键见解。我们的结果表明，当前VLMs存在深刻缺陷，平均准确率接近随机概率水平，突显了空间推理是一个持久障碍。这项工作不仅揭示了推动VLMs中空间推理的迫切需求，而且为未来探索建立了可靠的平台。代码可在GitHub（<a target="_blank" rel="noopener" href="https://github.com/stogiannidis/srbench%EF%BC%89%E4%B8%8A%E8%8E%B7%E5%BE%97%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E5%9C%A8HuggingFace%EF%BC%88https://huggingface.co/datasets/stogiannidis/srbench%EF%BC%89%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/stogiannidis/srbench）上获得，数据集可在HuggingFace（https://huggingface.co/datasets/stogiannidis/srbench）上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19707v1">PDF</a> 8 main pages, 4 pages Appendix, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了视觉语言模型（VLMs）在空间推理方面的表现。现有VLMs在集成视觉和文本理解的任务中表现出色，但在空间推理方面存在缺陷。本文提出了一种多面方法，分析了人类空间推理能力的多样性和多维性质，并评估了这些模型在合成和真实世界图像中的性能。分析结果显示，当前VLMs存在显著短板，平均准确率接近随机水平，凸显出空间推理的障碍。本文不仅揭示了提升VLMs中空间推理能力的紧迫需求，还为未来的研究提供了坚实平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在集成视觉和文本理解的任务中表现出色，如图像描述、视觉问答和图像文本检索。</li>
<li>现有VLMs的基准测试包含空间成分，但往往无法将空间推理与相关任务（如目标检测或语义理解）区分开来。</li>
<li>本文通过多面方法分析人类空间推理能力的多样性和多维性质，并评估了VLMs在合成和真实世界图像中的性能。</li>
<li>分析涵盖了空间推理的核心元素，包括空间关系、方向感知、导航、心理旋转和空间可视化。</li>
<li>研究结果显示，当前VLMs在空间推理方面存在显著短板，平均准确率接近随机水平。</li>
<li>论文强调了提升VLMs中空间推理能力的紧迫需求，并指出了未来的研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01a53ee6599de99f528bbeeba6b1a40f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dab53535bb85433acbfb08ddcb1025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f550b514f9f70ead608088190282ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2d4c8a79a78b1dcacdd5fe9b74622b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models"><a href="#RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models" class="headerlink" title="RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models"></a>RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models</h2><p><strong>Authors:Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen</strong></p>
<p>We introduce RGB-Th-Bench, the first benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs. While VLMs have demonstrated remarkable progress in visual reasoning and multimodal understanding, their evaluation has been predominantly limited to RGB-based benchmarks, leaving a critical gap in assessing their capabilities in infrared vision tasks. Existing visible-infrared datasets are either task-specific or lack high-quality annotations necessary for rigorous model evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive evaluation framework covering 14 distinct skill dimensions, with a total of 1,600+ expert-annotated Yes&#x2F;No questions. The benchmark employs two accuracy metrics: a standard question-level accuracy and a stricter skill-level accuracy, which evaluates model robustness across multiple questions within each skill dimension. This design ensures a thorough assessment of model performance, including resilience to adversarial and hallucinated responses. We conduct extensive evaluations on 19 state-of-the-art VLMs, revealing significant performance gaps in RGB-Thermal understanding. Our results show that even the strongest models struggle with thermal image comprehension, with performance heavily constrained by their RGB-based capabilities. Additionally, the lack of large-scale application-specific and expert-annotated thermal-caption-pair datasets in pre-training is an important reason of the observed performance gap. RGB-Th-Bench highlights the urgent need for further advancements in multimodal learning to bridge the gap between visible and thermal image understanding. The dataset is available through this link, and the evaluation code will also be made publicly available. </p>
<blockquote>
<p>我们介绍了RGB-Th-Bench，这是第一个旨在评估视觉语言模型（VLM）理解RGB-热成像图像对的能力的基准测试。尽管VLM在视觉推理和多模态理解方面取得了显著的进步，但其评估主要局限于基于RGB的基准测试，在评估红外视觉任务的能力方面存在关键差距。现有的可见光-红外数据集是任务特定的，或者缺乏进行严格模型评估所需的高质量注释。为了解决这些局限性，RGB-Th-Bench提供了一个全面的评估框架，涵盖14个不同的技能维度，总共有1600多个专家注释的是非问题。该基准测试采用两种准确率指标：标准问题级准确率和严格的技能级准确率，后者评估模型在各个技能维度内多个问题上的稳健性。这种设计确保了全面评估模型性能，包括对抗性和虚构响应的韧性。我们对19个最先进的VLM进行了广泛评估，发现在RGB-热成像理解方面存在显著的性能差距。我们的结果表明，即使是最强大的模型在热成像理解方面也遇到困难，其性能严重受到基于RGB的能力的限制。此外，缺乏大规模应用特定的和专家注释的热成像字幕对数据集进行预训练是观察到的性能差距的重要原因。RGB-Th-Bench强调了进一步改进多模态学习以弥合可见光与热成像理解之间差距的紧迫需求。数据集可通过此链接获取，评估代码也将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19654v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>介绍了一个名为RGB-Th-Bench的基准测试平台，它是首个评估视觉语言模型（VLMs）对RGB-Thermal图像对理解能力的基准测试。尽管VLMs在视觉推理和多模态理解方面取得了显著进展，但其评估主要局限于RGB基准测试，在红外视觉任务评估能力方面存在关键差距。RGB-Th-Bench提供了一个全面的评估框架，涵盖14个不同的技能维度，总共有1600多个专家注释的是非问题。该基准测试采用两种准确率度量标准：标准问题级准确率和更严格的技能级准确率，以评估模型在不同技能维度内多个问题上的稳健性。对19个最先进的VLMs进行了广泛评估，结果显示在RGB-Thermal理解方面存在显著的性能差距。最强大的模型在热图像理解方面仍有困难，性能受到其基于RGB的能力的严重制约。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>RGB-Th-Bench是首个评估视觉语言模型（VLMs）理解RGB-Thermal图像对能力的基准测试。</li>
<li>现有基准测试主要局限于RGB图像，使得在红外视觉任务评估上存在差距。</li>
<li>RGB-Th-Bench涵盖14个技能维度，包含1600+专家标注的是非问题。</li>
<li>该基准测试采用两种准确率度量标准，以全面评估模型的性能。</li>
<li>对19个最先进的VLMs的广泛评估显示，在RGB-Thermal理解方面存在显著性能差距。</li>
<li>最强的模型在热图像理解方面仍有困难，制约因素之一是缺乏大规模应用特定和专家注释的热图像描述对数据库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e0a5b0cc890f0d5bb24812c3f4968f79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a493ba8ec1a57de70b64417cda26f36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05634fc4f20012f0c4d9a8bf2a175f98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa83dd7a4503f9123391c25d286d5824.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bdf9fe9b56260d24c25c628883879f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7733da6c7be120aad291b22a16afd62b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training"><a href="#1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training" class="headerlink" title="1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training"></a>1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training</h2><p><strong>Authors:Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, Xiangang Li</strong></p>
<p>The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces for general reasoning tasks, composed of high-quality and challenging reasoning problems. These problems are collected from a multitude of open-source datasets, subjected to semantic deduplication and meticulous cleaning to eliminate test set contamination. All responses within the dataset are distilled from reasoning models (predominantly DeepSeek-R1) and have undergone rigorous verification procedures. Mathematical problems are validated by checking against reference answers, code problems are verified using test cases, and other tasks are evaluated with the aid of a reward model. The AM-Distill-Qwen-32B model, which was trained through only simple Supervised Fine-Tuning (SFT) using this batch of data, outperformed the DeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model surpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We are releasing these 1.4 million problems and their corresponding responses to the research community with the objective of fostering the development of powerful reasoning-oriented Large Language Models (LLMs). The dataset was published in \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D%7Bhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}</a>. </p>
<blockquote>
<p>AM-DeepSeek-R1-Distilled是一个大规模带有思考痕迹的一般推理任务数据集，包含高质量且富有挑战性的推理问题。这些问题来自多个开源数据集，经过语义去重和细致清理，消除了测试集污染。数据集中的所有答案都经过推理模型（主要是DeepSeek-R1）的提炼，并经过严格的验证程序。数学问题的答案通过对照参考答案进行验证，代码问题的答案通过使用测试用例进行验证，其他任务则借助奖励模型进行评估。仅通过简单监督微调（SFT）使用这批数据训练的AM-Distill-Qwen-32B模型在四个基准测试上超越了DeepSeek-R1-Distill-Qwen-32B模型：AIME2024、MATH-500、GPQA-Diamond和LiveCodeBench。此外，AM-Distill-Qwen-72B模型在所有基准测试上都超过了DeepSeek-R1-Distill-Llama-70B模型。我们发布这140万问题及相应答案的目的是为了推动研究社区开发强大的以推理为导向的大型语言模型（LLM）。数据集已在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-%E3%8C%AE%E9%A1%B5%E9%A2%98%E5%AE%B6/AM-DeepSeek-R1-Distilled-%E4%BA%8C-%E5%A3%B3-%E9%97%AE%E7%AD%BE-%E5%BC%8F-%E6%B3%A8-%E6%8E%A5-%E7%BA%BF-%E6%AF%8F-%E6%B2%BB-%E6%AC%A6-%F46%E5%8D%A1">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M发布。</a>!</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19633v1">PDF</a> </p>
<p><strong>Summary</strong><br>     大型数据集AM-DeepSeek-R1-Distilled专注于通用推理任务，包含高质量和具有挑战性的推理问题。该数据集通过语义去重和严格清理来消除测试集污染，其响应由推理模型（主要是DeepSeek-R1）蒸馏得出，并经过严格的验证程序。通过使用该数据集进行简单的监督微调（SFT），AM-Distill-Qwen-32B模型在多个基准测试上优于DeepSeek-R1-Distill-Qwen-32B模型。同时，我们公开了这些包含约一千万问题的数据集及其响应，旨在促进强大的推理导向的大型语言模型（LLM）的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AM-DeepSeek-R1-Distilled是一个大型数据集，包含针对通用推理任务的高质量、挑战性推理问题。</li>
<li>数据集经过语义去重和清理，以消除测试集污染。</li>
<li>数据集中的响应由推理模型（主要是DeepSeek-R1）蒸馏得出，并经过严格的验证程序确保准确性。</li>
<li>通过简单的监督微调（SFT），AM-Distill-Qwen-32B模型在多个基准测试上表现出优于DeepSeek-R1-Distill模型的性能。</li>
<li>AM-Distill-Qwen-72B模型在所有基准测试上超过了DeepSeek-R1-Distill-Llama-70B模型。</li>
<li>数据集旨在促进对通用推理任务的大型语言模型（LLM）的发展。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f6df3c97952c719378666d41a3ce4ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61c4635aa84efbe4f96ecf75988557d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77132bd24b561f2554256e0f6e60dfb5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language"><a href="#HoarePrompt-Structural-Reasoning-About-Program-Correctness-in-Natural-Language" class="headerlink" title="HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language"></a>HoarePrompt: Structural Reasoning About Program Correctness in Natural   Language</h2><p><strong>Authors:Dimitrios Stamatios Bouras, Yihan Dai, Tairan Wang, Yingfei Xiong, Sergey Mechtaev</strong></p>
<p>While software requirements are often expressed in natural language, verifying the correctness of a program against natural language requirements is a hard and underexplored problem. Large language models (LLMs) are promising candidates for addressing this challenge, however our experience shows that they are ineffective in this task, often failing to detect even straightforward bugs. To address this gap, we introduce HoarePrompt, a novel approach that adapts fundamental ideas from program analysis and verification to natural language artifacts. Drawing inspiration from the strongest postcondition calculus, HoarePrompt employs a systematic, step-by-step process in which an LLM generates natural language descriptions of reachable program states at various points in the code. To manage loops, we propose few-shot-driven k-induction, an adaptation of the k-induction method widely used in model checking. Once program states are described, HoarePrompt leverages the LLM to assess whether the program, annotated with these state descriptions, conforms to the natural language requirements. For evaluating the quality of classifiers of program correctness with respect to natural language requirements, we constructed CoCoClaNeL, a challenging dataset of solutions to programming competition problems. Our experiments show that HoarePrompt improves the MCC by 62% compared to directly using Zero-shot-CoT prompts for correctness classification. Furthermore, HoarePrompt outperforms a classifier that assesses correctness via LLM-based test generation by increasing the MCC by 93%. The inductive reasoning mechanism contributes a 28% boost to MCC, underscoring its effectiveness in managing loops. </p>
<blockquote>
<p>虽然软件需求通常使用自然语言来表达，但根据自然语言要求验证程序的正确性是一个困难且未被充分探索的问题。大型语言模型（LLMs）是应对这一挑战的有希望的候选者，但我们的经验表明，它们在此任务中效果不佳，甚至常常无法检测到简单的错误。为了解决这一差距，我们引入了HoarePrompt，这是一种将程序分析和验证的基本思想适应到自然语言工件的新方法。HoarePrompt借鉴了最强的后条件演算的理念，采用系统、分步的方法，其中LLM会在代码的各个点生成可访问程序状态的自然语言描述。为了管理循环，我们提出了基于少量样本的k归纳法，这是对广泛应用于模型检查的k归纳方法的适应。一旦描述了程序状态，HoarePrompt就会利用LLM来评估带有这些状态描述的程序是否符合自然语言要求。为了评估分类器对于自然语言要求的程序正确性的质量，我们构建了CoCoClaNeL数据集，这是一系列来自编程竞赛问题的解决方案的挑战集。我们的实验表明，与直接使用零样本CoT提示进行正确性分类相比，HoarePrompt的MCC提高了62%。此外，与通过LLM生成的测试评估正确性的分类器相比，HoarePrompt的MCC提高了93%。归纳推理机制对MCC的贡献提升了28%，这突显了其在管理循环中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19599v1">PDF</a> </p>
<p><strong>Summary</strong>：针对自然语言描述的软件需求验证问题，引入了一种新方法HoarePrompt。该方法借鉴程序分析和验证的基本思想，应用于自然语言工件。通过系统、分步骤的过程，LLM生成代码各点的可达程序状态的自然语言描述。针对循环管理，提出基于少样本驱动的k归纳法。使用LLM评估标注有状态描述的程序的自然语言要求符合程度。实验表明，HoarePrompt相比直接使用方法分类器性能提升显著。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自然语言描述的软件需求验证是一个挑战且未得到充分探索的问题。</li>
<li>LLMs在直接处理自然语言软件需求验证任务时效果不佳。</li>
<li>HoarePrompt是一种新的方法，借鉴程序分析和验证来处理自然语言工件。</li>
<li>HoarePrompt通过系统步骤让LLM生成代码各点的程序状态的自然语言描述。</li>
<li>针对循环管理，采用基于少样本驱动的k归纳法。</li>
<li>使用LLM评估标注程序状态的程序的符合自然语言要求的程度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9aac91c900d2b9664b95be1f1b5e1c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-483e45bef9eaa1c647c8a784d07e3ba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51be63da55cae30c60ebb0207831fe94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acb9ff9354c9a6291e2ec1bc9530388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30db18e69155615e68bc738105d9f4a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4210fc0b739d8df99f0cfa9f64c1cd18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e88ae79be101f21e2b916b0c38d2145.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models"><a href="#ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models" class="headerlink" title="ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models"></a>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models</h2><p><strong>Authors:Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim</strong></p>
<p>Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: <a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>. </p>
<blockquote>
<p>时空推理在理解各个领域的真实世界环境，例如自动驾驶和运动分析中，都是至关重要的。最近的进步通过引入大规模数据提高了视觉语言模型（VLMs）的空间推理能力，但这些模型仍然难以分析运动物体的行程距离和速度等运动学元素。为了弥补这一差距，我们构建了一个涉及运动学指令调整的时空推理数据集和基准测试，称为STKit和STKit-Bench。它们由包含3D注释的真实世界视频组成，详细描述了物体的运动动态：行程距离、速度、运动方向、物体间距离比较和相对运动方向。为了将此类数据构建进一步扩展到没有3D标签的视频，我们提出了一种使用真实世界规模的4D重建生成伪标签的自动管道。使用我们的时空推理运动学指令调整数据，我们推出了ST-VLM，这是一个增强时空推理的VLM，在STKit-Bench上表现出卓越的性能。此外，我们展示了ST-VLM在不同领域和任务中的稳健泛化能力，在其他时空基准测试（例如ActivityNet、TVQA+）上的表现优于基线。最后，通过整合学到的时空推理与现有能力，ST-VLM能够实现复杂的多步骤推理。项目页面：<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM%E3%80%82">https://ikodoh.github.io/ST-VLM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19355v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了时空推理在现实环境理解中的重要性，特别是在自动驾驶和体育分析等领域。针对视觉语言模型（VLMs）在动态元素分析上的不足，提出了一个时空推理数据集和基准测试，即STKit和STKit-Bench。该数据集包含带有3D注释的真实世界视频，详细描述了物体运动的动力学，如行驶距离、速度、运动方向等。为扩大此类数据构建范围至无3D标签的视频，提出了利用真实世界规模的4D重建生成伪标签的自动管道。通过时空推理的动力学指令调整数据，推出了增强时空推理能力的ST-VLM模型，在STKit-Bench上表现出卓越性能。此外，ST-VLM在不同领域和任务中表现出强大的泛化能力，在其他时空基准测试（如ActivityNet、TVQA+）上优于基线。最后，通过将学到的时空推理与现有能力相结合，ST-VLM能够实现复杂的多步推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时空推理在现实环境理解中扮演重要角色，尤其在自动驾驶和体育分析领域。</li>
<li>当前视觉语言模型（VLMs）在动态元素分析上存在不足。</li>
<li>提出了STKit和STKit-Bench数据集，包含真实世界视频及其3D注释，用于物体运动动力学分析。</li>
<li>为扩大数据构建范围至无3D标签的视频，提出了利用4D重建生成伪标签的自动管道。</li>
<li>推出了ST-VLM模型，增强时空推理能力，并在STKit-Bench上表现优越。</li>
<li>ST-VLM在不同领域和任务中表现出强大的泛化能力，优于其他基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d05948da4e5493c49a016f2f5cf526f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e0fc995603c14e9dbb59bec545fd16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c084e9998c5bd21ee62e3b8ab498e4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83fee94008c4eb63eb5913ba6b6fdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf26d67f83df6c8c93a5c2dc429dcce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32a68729a67a41a12a06f038493170e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2baa39b34006ef12edd50ab6a02d4a0d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ImageGen-CoT-Enhancing-Text-to-Image-In-context-Learning-with-Chain-of-Thought-Reasoning"><a href="#ImageGen-CoT-Enhancing-Text-to-Image-In-context-Learning-with-Chain-of-Thought-Reasoning" class="headerlink" title="ImageGen-CoT: Enhancing Text-to-Image In-context Learning with   Chain-of-Thought Reasoning"></a>ImageGen-CoT: Enhancing Text-to-Image In-context Learning with   Chain-of-Thought Reasoning</h2><p><strong>Authors:Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin, Yu Cheng, Lijuan Wang</strong></p>
<p>In this work, we study the problem of Text-to-Image In-Context Learning (T2I-ICL). While Unified Multimodal LLMs (MLLMs) have advanced rapidly in recent years, they struggle with contextual reasoning in T2I-ICL scenarios. To address this limitation, we propose a novel framework that incorporates a thought process called ImageGen-CoT prior to image generation. To avoid generating unstructured ineffective reasoning steps, we develop an automatic pipeline to curate a high-quality ImageGen-CoT dataset. We then fine-tune MLLMs using this dataset to enhance their contextual reasoning capabilities. To further enhance performance, we explore test-time scale-up strategies and propose a novel hybrid scaling approach. This approach first generates multiple ImageGen-CoT chains and then produces multiple images for each chain via sampling. Extensive experiments demonstrate the effectiveness of our proposed method. Notably, fine-tuning with the ImageGen-CoT dataset leads to a substantial 80% performance gain for SEED-X on T2I-ICL tasks. See our project page at <a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/">https://ImageGen-CoT.github.io/</a>. Code and model weights will be open-sourced. </p>
<blockquote>
<p>在这项工作中，我们研究了文本到图像上下文学习（T2I-ICL）的问题。尽管统一多模态大型语言模型（MLLMs）近年来发展迅速，但在T2I-ICL场景中它们在进行上下文推理时仍面临困难。为了解决这一局限性，我们提出了一种新的框架，该框架在图像生成之前融入了一个名为ImageGen-CoT的思维过程。为了避免生成无序且无效的推理步骤，我们开发了一个自动流程来创建高质量的ImageGen-CoT数据集。然后，我们使用此数据集对MLLMs进行微调，以增强其上下文推理能力。为了进一步提高性能，我们探索了测试时扩展策略，并提出了一种新型混合扩展方法。该方法首先生成多个ImageGen-CoT链，然后通过采样为每个链生成多个图像。大量实验证明了我们提出的方法的有效性。值得注意的是，使用ImageGen-CoT数据集进行微调使得SEED-X在T2I-ICL任务上的性能提高了80%。更多详情，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/%E3%80%82%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%B0%86%E5%BC%80%E6%BA%90%E3%80%82">https://ImageGen-CoT.github.io/。代码和模型权重将开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19312v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/">https://ImageGen-CoT.github.io/</a></p>
<p><strong>Summary</strong><br>     本文研究了文本到图像上下文学习（T2I-ICL）的问题。针对统一跨模态大型语言模型（MLLMs）在T2I-ICL场景中的上下文推理困难，提出了融入ImageGen-CoT思维过程的新框架，用于图像生成前的推理。为避免生成无结构的不合理推理步骤，开发了自动管道来创建高质量的ImageGen-CoT数据集。使用此数据集对MLLMs进行微调，以增强其上下文推理能力。还探索了测试时的扩展策略，并提出了新的混合扩展方法。该方法首先生成多个ImageGen-CoT链，然后通过采样为每个链生成多个图像。实验表明，使用ImageGen-CoT数据集微调后，SEED-X在T2I-ICL任务上的性能提高了80%。有关详细信息，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://imagegen-cot.github.io/%E3%80%82%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%B0%86%E5%BC%80%E6%BA%90%E3%80%82">https://ImageGen-CoT.github.io/。代码和模型权重将开源。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了文本到图像上下文学习（T2I-ICL）的挑战，特别是在统一跨模态大型语言模型（MLLMs）中的应用。</li>
<li>针对MLLMs在T2I-ICL场景中的上下文推理困难，提出了融入ImageGen-CoT思维过程的新框架。</li>
<li>为提高模型性能，开发了一个自动管道来创建高质量的ImageGen-CoT数据集，用于微调MLLMs。</li>
<li>提出了测试时的扩展策略，包括生成多个ImageGen-CoT链并为每个链生成多个图像的新方法。</li>
<li>实验结果表明，使用ImageGen-CoT数据集微调后，模型在特定任务上的性能有显著提高。</li>
<li>项目页面提供了更多详细信息，包括开放源代码和模型权重。</li>
<li>该研究为文本到图像生成领域开辟了新的方向，特别是在上下文推理方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4b37b3fd31065017dd407161a65e359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e913e4c3092a3a6e54b91f4353b46dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8be7e8a78605cf0766342258184c195a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e6c055eedfe6b0217a1846e9b31545a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c25a7ab4dc71edd0231694b15d97d8f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DWIM-Towards-Tool-aware-Visual-Reasoning-via-Discrepancy-aware-Workflow-Generation-Instruct-Masking-Tuning"><a href="#DWIM-Towards-Tool-aware-Visual-Reasoning-via-Discrepancy-aware-Workflow-Generation-Instruct-Masking-Tuning" class="headerlink" title="DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow   Generation &amp; Instruct-Masking Tuning"></a>DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow   Generation &amp; Instruct-Masking Tuning</h2><p><strong>Authors:Fucai Ke, Vijay Kumar B G, Xingjian Leng, Zhixi Cai, Zaid Khan, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, Manmohan Chandraker</strong></p>
<p>Visual reasoning (VR), which is crucial in many fields for enabling human-like visual understanding, remains highly challenging. Recently, compositional visual reasoning approaches, which leverage the reasoning abilities of large language models (LLMs) with integrated tools to solve problems, have shown promise as more effective strategies than end-to-end VR methods. However, these approaches face limitations, as frozen LLMs lack tool awareness in VR, leading to performance bottlenecks. While leveraging LLMs for reasoning is widely used in other domains, they are not directly applicable to VR due to limited training data, imperfect tools that introduce errors and reduce data collection efficiency in VR, and challenging in fine-tuning on noisy workflows. To address these challenges, we propose DWIM: i) Discrepancy-aware training Workflow generation, which assesses tool usage and extracts more viable workflows for training; and ii) Instruct-Masking fine-tuning, which guides the model to only clone effective actions, enabling the generation of more practical solutions. Our experiments demonstrate that DWIM achieves state-of-the-art performance across various VR tasks, exhibiting strong generalization on multiple widely-used datasets. </p>
<blockquote>
<p>视觉推理（VR）在许多领域中对于实现人类般的视觉理解至关重要，仍然是一项巨大的挑战。最近，利用大型语言模型（LLM）的推理能力与集成工具解决问题的组合视觉推理方法，已经显示出比端到端VR方法更有效的策略前景。然而，这些方法面临局限性，因为冻结的LLM缺乏VR中的工具意识，导致性能瓶颈。虽然利用LLM进行推理在其他领域广泛应用，但由于训练数据有限、工具不完美而引入错误并降低VR中的数据收集效率，以及在嘈杂工作流程中微调具有挑战性，它们并不能直接应用于VR。为了解决这些挑战，我们提出DWIM：一）差异感知训练工作流程生成，它评估工具使用并提取更多可行的工作流程用于训练；二)指令屏蔽微调，它引导模型只复制有效操作，从而生成更实用的解决方案。我们的实验表明，DWIM在各种VR任务上实现了最先进的性能表现，并在多个广泛使用的数据集上表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19263v1">PDF</a> </p>
<p><strong>Summary</strong><br>视觉推理（VR）在许多领域都至关重要，但仍面临挑战。最近，利用大型语言模型（LLMs）的推理能力与工具集成解决问题的组合视觉推理方法显示出巨大潜力。然而，这些方法面临局限性，因为冻结的LLMs缺乏工具意识，导致性能瓶颈。为解决这些挑战，我们提出DWIM方法，包括差异感知训练工作流程生成和指令屏蔽微调。实验证明DWIM在各种视觉推理任务上达到最新技术水平，并在多个常用数据集上表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉推理（VR）在许多领域的重要性及其当前的挑战。</li>
<li>组合视觉推理方法利用大型语言模型（LLMs）的推理能力。</li>
<li>现有方法面临因为LLMs缺乏工具意识而产生的性能瓶颈问题。</li>
<li>DWIM方法包括差异感知训练工作流程生成，评估工具使用并提取更多可行的工作流程用于训练。</li>
<li>DWIM的另一种技术是Instruct-masking fine-tuning，引导模型只复制有效操作，生成更实际的解决方案。</li>
<li>实验证明DWIM在多种视觉推理任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-82b3459712ba55a2f8a84150e99f9dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65760e3a58c8609b32c0cf29a9de68ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad159a64d35b3fa75c9120b933a29920.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58db9a095c1f5feb429247135651e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6ce6f2b15ea066180057216e7eb66c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Browsing-Lost-Unformed-Recollections-A-Benchmark-for-Tip-of-the-Tongue-Search-and-Reasoning"><a href="#Browsing-Lost-Unformed-Recollections-A-Benchmark-for-Tip-of-the-Tongue-Search-and-Reasoning" class="headerlink" title="Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue   Search and Reasoning"></a>Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue   Search and Reasoning</h2><p><strong>Authors:Sky CH-Wang, Darshan Deshpande, Smaranda Muresan, Anand Kannappan, Rebecca Qian</strong></p>
<p>We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multi-modal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set. </p>
<blockquote>
<p>我们介绍了“浏览丢失的未成形回忆”（Browsing Lost Unformed Recollections，简称BLUR）这一针对通用人工智能助理的舌尖已知项目搜索与推理基准测试。BLUR引入了一组经过现实世界验证的573个问题，这些问题需要跨多模态和多语言输入进行搜索和推理，并熟练掌握工具使用，以取得卓越表现。人类很容易解决这些问题（平均得分为98%），而表现最佳的系统的得分约为56%。为了促进针对通用人工智能助理这一具有挑战性和期望的用例的进步，我们通过公开排行榜发布350个问题，保留其中250个问题的答案，其余作为私人测试集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>浏览丢失未成形回忆（BLUR）是一个针对通用人工智能助理的尖端已知项目搜索和推理基准测试。BLUR包含573个真实世界验证问题，要求人工智能助理在跨模态和多语言输入的情况下进行搜索和推理，以及熟练使用工具的能力。人类可以轻松解决这些问题（平均得分98%），而最佳系统得分约为56%。为了推动通用人工智能助理解决这一具有挑战性和前瞻性的用例，我们发布了350个问题通过公共排行榜，保留对其中250个问题的答案，其余作为私有测试集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BLUR是一个针对通用AI助理的搜索和推理基准测试。</li>
<li>它包含573个真实世界验证问题，需要跨模态和多语言输入。</li>
<li>这些问题要求AI助理熟练使用工具进行搜索和推理。</li>
<li>人类可以轻松解决这些问题，而当前最佳AI系统的表现较低。</li>
<li>350个问题将通过公共排行榜发布，其中250个问题的答案被保留。</li>
<li>该基准测试旨在推动通用AI助理在解决挑战性用例方面的进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19193">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8b779ff1edf87eda9327a2fa29417d84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf866dac599779d5a5699a282e1ba8bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e44c41f4be8fd3403809a07dc051735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4a61466c1aa80b0763084dff98e1a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f17a5de1aff763d97177599d2a201dec.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models"><a href="#SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models" class="headerlink" title="SoK: How Robust is Audio Watermarking in Generative AI models?"></a>SoK: How Robust is Audio Watermarking in Generative AI models?</h2><p><strong>Authors:Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan</strong></p>
<p>Audio watermarking is increasingly used to verify the provenance of AI-generated content, enabling applications such as detecting AI-generated speech, protecting music IP, and defending against voice cloning. To be effective, audio watermarks must resist removal attacks that distort signals to evade detection. While many schemes claim robustness, these claims are typically tested in isolation and against a limited set of attacks. A systematic evaluation against diverse removal attacks is lacking, hindering practical deployment. In this paper, we investigate whether recent watermarking schemes that claim robustness can withstand a broad range of removal attacks. First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we summarize their underlying technologies and potential vulnerabilities. We then present a large-scale empirical study to assess their robustness. To support this, we build an evaluation framework encompassing 22 types of removal attacks (109 configurations) including signal-level, physical-level, and AI-induced distortions. We reproduce 9 watermarking schemes using open-source code, identify 8 new highly effective attacks, and highlight 11 key findings that expose the fundamental limitations of these methods across 3 public datasets. Our results reveal that none of the surveyed schemes can withstand all tested distortions. This evaluation offers a comprehensive view of how current watermarking methods perform under real-world threats. Our demo and code are available at <a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/">https://sokaudiowm.github.io/</a>. </p>
<blockquote>
<p>音频水印技术越来越多地用于验证人工智能生成内容的出处，支持检测人工智能生成的语音、保护音乐知识产权和防范声音克隆等应用。要有效发挥作用，音频水印必须抵抗能够扭曲信号以躲避检测的去除攻击。虽然许多方案都声称具有稳健性，但这些声称通常是单独测试并且在有限的攻击集上得到验证。缺乏对多种去除攻击的系统的评估，阻碍了实际部署。在本文中，我们调查了最近的声称具有稳健性的水印方案是否能够承受广泛的去除攻击。首先，我们介绍了一个涵盖22种音频水印方案的分类。接下来，我们总结了它们的基础技术和潜在漏洞。然后，我们进行了一项大规模实证研究来评估它们的稳健性。为此，我们建立了一个评估框架，包括22种类型的去除攻击（109种配置），包括信号级、物理级和人工智能引起的失真。我们使用开源代码重新制作了9种水印方案，识别了8种新的高度有效的攻击，并强调了11个关键发现，揭示了这些方法在3个公共数据集上的根本局限性。我们的结果表明，没有任何调查过的方案能够承受所有测试过的失真。这一评估提供了对当前水印方法在现实世界威胁下的表现的综合观点。我们的演示和代码可在[<a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://sokaudiowm.github.io/]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨了音频水印技术在验证AI生成内容方面的应用，并指出音频水印技术需要抵抗移除攻击，以避免信号失真以逃避检测。尽管有许多方案声称具有稳健性，但它们通常仅在有限的攻击环境下进行测试，缺乏对各种移除攻击的全方位评估，阻碍了实际应用部署。本文介绍了最新的水印方案是否经得起大范围攻击的考验，同时揭示这些方法在应对现实威胁时的表现如何。总体而言，现有方案均无法抵御所有测试中的失真攻击。这项评估提供了对当前水印方法在实际威胁下表现的全面视角。 </p>
<p><strong>Key Takeaways</strong> </p>
<ol>
<li>音频水印技术广泛应用于验证AI生成内容的来源。</li>
<li>音频水印需要抵抗移除攻击，以防信号失真逃避检测。 </li>
<li>目前缺乏对各种移除攻击的全方位评估方法，阻碍了音频水印的实际应用部署。 </li>
<li>对多种水印方案进行了大规模实证研究，评估其稳健性。 </li>
<li>研究中引入了新的攻击方式并揭示了现有方案的局限性。 </li>
<li>没有一种方案能够抵御所有测试中的失真攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d306ac484b621e9574fecdc8675bab0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51d5f17e700fedafdd6907f1b290749b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68abb1c89524d8dcac34d99f76c530b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3736acb676bae171e1f460cf29ed7044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ff7648c6efba1ec4a8ab3c97ef303d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MIRAGE-Multimodal-Immersive-Reasoning-and-Guided-Exploration-for-Red-Team-Jailbreak-Attacks"><a href="#MIRAGE-Multimodal-Immersive-Reasoning-and-Guided-Exploration-for-Red-Team-Jailbreak-Attacks" class="headerlink" title="MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for   Red-Team Jailbreak Attacks"></a>MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for   Red-Team Jailbreak Attacks</h2><p><strong>Authors:Wenhao You, Bryan Hooi, Yiwei Wang, Youke Wang, Zong Ke, Ming-Hsuan Yang, Zi Huang, Yujun Cai</strong></p>
<p>While safety mechanisms have significantly progressed in filtering harmful text inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit their cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal jailbreak framework that exploits narrative-driven context and role immersion to circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By systematically decomposing the toxic query into environment, role, and action triplets, MIRAGE constructs a multi-turn visual storytelling sequence of images and text using Stable Diffusion, guiding the target model through an engaging detective narrative. This process progressively lowers the model’s defences and subtly guides its reasoning through structured contextual cues, ultimately eliciting harmful responses. In extensive experiments on the selected datasets with six mainstream MLLMs, MIRAGE achieves state-of-the-art performance, improving attack success rates by up to 17.5% over the best baselines. Moreover, we demonstrate that role immersion and structured semantic reconstruction can activate inherent model biases, facilitating the model’s spontaneous violation of ethical safeguards. These results highlight critical weaknesses in current multimodal safety mechanisms and underscore the urgent need for more robust defences against cross-modal threats. </p>
<blockquote>
<p>虽然安全机制在过滤有害文本输入方面已经取得了重大进展，但多模态大型语言模型（MLLMs）仍然容易受到利用跨模态推理能力的多模态越狱攻击。我们提出了MIRAGE，这是一个新的多模态越狱框架，它利用叙事驱动的上下文和角色沉浸来绕过多模态大型语言模型（MLLMs）中的安全机制。MIRAGE通过系统地将有毒查询分解成环境、角色和动作三元组，使用Stable Diffusion构建由图像和文本组成的多轮视觉故事叙述序列，通过引人入胜的侦探叙事引导目标模型。这个过程逐步降低了模型的防御能力，并通过结构化的上下文线索微妙地引导其推理，最终引发有害的反应。在对所选数据集进行的广泛实验中，与六款主流MLLM相比，MIRAGE达到了最先进的性能水平，在最佳基线的基础上提高了高达17.5%的攻击成功率。此外，我们证明了角色沉浸和结构化语义重建可以激活模型本身的偏见，促使模型自发违反道德保障。这些结果凸显了当前多模态安全机制的关键弱点，并强调了对抗跨模态威胁的更稳健防御的迫切需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19134v1">PDF</a> </p>
<p><strong>Summary</strong><br>多模态大型语言模型虽然具有先进的过滤有害文本输入的安全机制，但仍容易受到多模态突破攻击的影响。为此，研究人员提出了MIRAGE框架，通过构建以故事叙述引导模型的多模态攻击方式，绕过安全机制。MIRAGE通过将有毒查询分解为环境、角色和行为三元组，并利用Stable Diffusion构建图像和文字的多轮故事叙述序列来指导目标模型。通过结构化的上下文线索，MIRAGE逐渐降低模型的防御能力并引导其推理，最终引发有害反应。在大量数据集上的实验表明，MIRAGE在攻击成功率方面取得了最先进的性能，相较于最佳基线提高了高达17.5%。此外，研究还表明角色沉浸和结构化语义重建可以激活模型固有偏见，促使模型自发违反安全保护措施。这些结果揭示了当前多模态安全机制的重大弱点，并强调了对抗跨模态威胁的更强大防御手段的紧迫需求。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMs（多模态大型语言模型）尽管在安全机制方面取得了显著进展，但仍面临跨模态攻击的风险。</li>
<li>MIRAGE是一种新型的多模态攻击框架，通过构造故事叙述的方式绕过MLLMs的安全机制。</li>
<li>MIRAGE将有毒查询分解为环境、角色和行为三元组，并使用Stable Diffusion技术构建图像和文字的多轮叙述序列来指导模型推理。</li>
<li>MIRAGE能有效降低模型的防御能力，并通过结构化的上下文线索引导模型产生有害反应。</li>
<li>实验表明，相较于现有最佳基线方法，MIRAGE在攻击成功率上提高了高达17.5%。</li>
<li>研究指出角色沉浸和结构化语义重建能够激活模型的固有偏见，可能使模型自发违反安全保护措施。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19134">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fee1975508503d35bec32009cd076956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04cc0814f44c21c6a530f52a9ea1c11.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Understanding-and-Improving-Information-Preservation-in-Prompt-Compression-for-LLMs"><a href="#Understanding-and-Improving-Information-Preservation-in-Prompt-Compression-for-LLMs" class="headerlink" title="Understanding and Improving Information Preservation in Prompt   Compression for LLMs"></a>Understanding and Improving Information Preservation in Prompt   Compression for LLMs</h2><p><strong>Authors:Weronika Łajewska, Momchil Hardalov, Laura Aina, Neha Anna John, Hang Su, Lluís Màrquez</strong></p>
<p>Recent advancements in large language models (LLMs) have enabled their successful application to a broad range of tasks. However, in information-intensive tasks, the prompt length can grow fast, leading to increased computational requirements, performance degradation, and induced biases from irrelevant or redundant information. Recently, various prompt compression techniques have been introduced to optimize the trade-off between reducing input length and retaining performance. We propose a holistic evaluation framework that allows for in-depth analysis of prompt compression methods. We focus on three key aspects, besides compression ratio: (i) downstream task performance, (ii) grounding in the input context, and (iii) information preservation. Through this framework, we investigate state-of-the-art soft and hard compression methods, showing that they struggle to preserve key details from the original prompt, limiting their performance on complex tasks. We demonstrate that modifying soft prompting methods to control better the granularity of the compressed information can significantly improve their effectiveness – up to +23% in downstream task performance, more than +8 BERTScore points in grounding, and 2.7x more entities preserved in compression. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步使其能够成功应用于广泛的任务。然而，在信息密集型任务中，提示长度可能会快速增长，导致计算需求增加、性能下降以及由无关或冗余信息引起的偏见。最近，已经推出了各种提示压缩技术，以优化减少输入长度和保留性能之间的权衡。我们提出了一个全面的评估框架，允许对提示压缩方法进行深入分析。除了压缩率外，我们关注三个关键方面：（i）下游任务性能，（ii）对输入上下文的依赖，（iii）信息保留。通过此框架，我们研究了最新的软压缩和硬压缩方法，发现它们在保留原始提示的关键细节方面存在困难，在复杂任务上的性能有限。我们证明，修改软提示方法以更好地控制压缩信息的粒度可以显著提高它们的有效性——下游任务性能提高+23%，依赖BERTScore提高超过+8分，压缩中保留的实体数量提高至原来的两倍以上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19114v1">PDF</a> 21 pages, 6 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在众多任务中展现出强大能力，但在信息密集型任务中，由于提示信息的增长带来的计算需求增加、性能下降和由无关或冗余信息引起的偏见等问题日益突出。为优化输入长度与性能之间的平衡，出现了多种提示压缩技术。本文提出一个全面的评估框架，除压缩率外，还关注下游任务性能、输入上下文中的基础性和信息保留三个方面。调查表明，现有软硬压缩方法难以保留原始提示的关键细节，在复杂任务上的性能受限。通过改进软提示方法以更好地控制压缩信息的粒度，可显著提高有效性，下游任务性能提高达+23%，基础性能提高超过+8 BERTScore点，压缩时保留实体数量增加2.7倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在信息密集型任务中面临提示信息增长带来的问题，包括计算需求增加、性能下降和由无关或冗余信息引起的偏见。</li>
<li>为解决这些问题，出现了多种提示压缩技术，旨在优化输入长度与性能之间的平衡。</li>
<li>本文提出一个全面的评估框架，评估提示压缩方法的三个方面：下游任务性能、输入上下文中的基础性和信息保留。</li>
<li>现有软硬压缩方法在复杂任务上难以保留原始提示的关键细节，性能受限。</li>
<li>通过改进软提示方法以更好地控制压缩信息的粒度，可以显著提高有效性。</li>
<li>改进后的方法在下游任务性能、基础性能和压缩时保留实体数量方面均有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15530d8efa363526efad247326390643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c4345b74233f347984960c5a898e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce25502c1fe64cc10ef6bb620482dc58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-023f2d5fde8f8b3b795938dbb245f8b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93c2ad54eb3d5b3f925127d3af57a7b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3878ccb002210a135d7fbc7bf4b10f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Option-Discovery-Using-LLM-guided-Semantic-Hierarchical-Reinforcement-Learning"><a href="#Option-Discovery-Using-LLM-guided-Semantic-Hierarchical-Reinforcement-Learning" class="headerlink" title="Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement   Learning"></a>Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement   Learning</h2><p><strong>Authors:Chak Lam Shek, Pratap Tokekar</strong></p>
<p>Large Language Models (LLMs) have shown remarkable promise in reasoning and decision-making, yet their integration with Reinforcement Learning (RL) for complex robotic tasks remains underexplored. In this paper, we propose an LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven subgoal selection and option reuse to enhance sample efficiency, generalization, and multi-task adaptability. Traditional RL methods often suffer from inefficient exploration and high computational cost. Hierarchical RL helps with these challenges, but existing methods often fail to reuse options effectively when faced with new tasks. To address these limitations, we introduce a three-stage framework that uses LLMs for subgoal generation given natural language description of the task, a reusable option learning and selection method, and an action-level policy, enabling more effective decision-making across diverse tasks. By incorporating LLMs for subgoal prediction and policy guidance, our approach improves exploration efficiency and enhances learning performance. On average, LDSC outperforms the baseline by 55.9% in average reward, demonstrating its effectiveness in complex RL settings. More details and experiment videos could be found in \href{<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC/%7D%7Bthis">https://raaslab.org/projects/LDSC/}{this</a> link\footnote{<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC%7D%7D">https://raaslab.org/projects/LDSC}}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在推理和决策制定方面显示出巨大的潜力，然而，它们与强化学习（RL）在复杂机器人任务中的融合仍然未被充分探索。在本文中，我们提出了一种名为LDSC的LLM引导分层RL框架，该框架利用LLM驱动的子目标选择和选项重用，以提高样本效率、通用性和多任务适应性。传统RL方法常常面临效率低下和计算成本高昂的问题。分层RL有助于应对这些挑战，但现有方法在面对新任务时往往无法有效地重用选项。为了解决这些局限性，我们引入了一个三阶段框架，该框架使用LLM根据任务的自然语言描述生成子目标，提供一种可重用的选项学习和选择方法，以及动作级策略，从而在各种任务中更有效地进行决策制定。通过结合LLM进行子目标预测和政策指导，我们的方法提高了探索效率并增强了学习效果。平均而言，LDSC在平均奖励方面比基线高出55.9%，证明了其在复杂RL环境中的有效性。更多细节和实验视频可在<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC/">此链接</a>找到[^<a target="_blank" rel="noopener" href="https://raaslab.org/projects/LDSC]%E3%80%82">https://raaslab.org/projects/LDSC]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19007v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLMs与强化学习（RL）的结合在复杂的机器人任务中显示出巨大的潜力。本研究提出了一种基于LLM指导的分层RL框架，称为LDSC。它利用LLM驱动的子目标选择和选项重用，提高了样本效率、泛化能力和多任务适应性。该框架解决了传统RL方法所面临的挑战，通过LLM进行子目标生成和任务描述的自然语言处理，提高探索效率和学习性能。LDSC在平均奖励方面比基线高出55.9%，在复杂的RL环境中表现出优异的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在机器人复杂任务中的潜力巨大，特别是与强化学习（RL）结合时。</li>
<li>LDSC是一种基于LLM指导的分层RL框架，旨在解决传统RL方法的不足。</li>
<li>LDSC利用LLM驱动的子目标选择来提高样本效率、泛化能力和多任务适应性。</li>
<li>LDSC通过引入选项重用方法，更有效地应对新任务。</li>
<li>LDSC利用LLM进行子目标生成和任务描述的自然语言处理，提高探索效率和学习性能。</li>
<li>LDSC框架在平均奖励方面显著提高，相比基线方法提升了55.9%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f085797ab54bbbe586fda646a8f214ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-539c34fe905bc0aa95da2d0f23e68817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac0554db7c8cbf58176c2731618542f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28c8a75392296635052256193825c343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42032ea8bdb6d2f011ac926b54055074.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e229d2fcd72469613f92df5e3c3d1175.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="I-Have-Covered-All-the-Bases-Here-Interpreting-Reasoning-Features-in-Large-Language-Models-via-Sparse-Autoencoders"><a href="#I-Have-Covered-All-the-Bases-Here-Interpreting-Reasoning-Features-in-Large-Language-Models-via-Sparse-Autoencoders" class="headerlink" title="I Have Covered All the Bases Here: Interpreting Reasoning Features in   Large Language Models via Sparse Autoencoders"></a>I Have Covered All the Bases Here: Interpreting Reasoning Features in   Large Language Models via Sparse Autoencoders</h2><p><strong>Authors:Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ‘’reasoning features’’ from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model’s reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at <a target="_blank" rel="noopener" href="https://github.com/AIRI-Institute/SAE-Reasoning">https://github.com/AIRI-Institute/SAE-Reasoning</a> </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理方面取得了显著的成功。最近的进展导致了一类新型推理LLM的发展；例如，开源的DeepSeek-R1通过深度思考和复杂推理的集成，实现了最先进的性能。尽管这些能力令人印象深刻，但这些模型的内部推理机制仍然未被探索。在这项工作中，我们采用了稀疏自动编码器（SAE），这是一种学习神经网络潜在表示的稀疏分解并将其转化为可解释特征的方法，以识别驱动DeepSeek-R1系列模型中推理的特征。首先，我们提出了一种从SAE表示中提取候选“推理特征”的方法。我们通过实证分析和可解释性方法对这些特征进行验证，证明它们与模型的推理能力存在直接关联。最重要的是，我们证明了系统地控制这些特征可以增强推理性能，这为LLM中的推理提供了第一个机械解释。代码可通过<a target="_blank" rel="noopener" href="https://github.com/AIRI-Institute/SAE-Reasoning%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AIRI-Institute/SAE-Reasoning获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18878v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在自然语言处理领域取得了显著的成功，最新的进展中出现了一种新的推理型LLM。本研究使用稀疏自编码器（SAE）来探索DeepSeek-R1系列模型的内部推理机制。通过提取候选的“推理特征”，我们验证了这些特征与模型推理能力的直接关联，并证明系统地引导这些特征可以提高模型的推理性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自然语言处理领域表现出卓越性能。</li>
<li>新型推理型LLM的出现进一步提升了LLM的性能。</li>
<li>稀疏自编码器（SAE）被用于探索DeepSeek-R1系列模型的内部推理机制。</li>
<li>通过提取和验证候选的“推理特征”，揭示了这些特征与模型推理能力的关联。</li>
<li>引导这些“推理特征”可系统地提高模型的推理性能。</li>
<li>本研究为大型语言模型中的推理机制提供了初步的机制性解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1241f8da312aecec45189939405f446d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f36a31b68957d35c2d351bfd11173c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b894919d79675a3a35a01883dda0417.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3592478ec334c8687593cc7f2a323183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddbdb35698ee997c058868060139a037.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AlphaSpace-Enabling-Robotic-Actions-through-Semantic-Tokenization-and-Symbolic-Reasoning"><a href="#AlphaSpace-Enabling-Robotic-Actions-through-Semantic-Tokenization-and-Symbolic-Reasoning" class="headerlink" title="AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and   Symbolic Reasoning"></a>AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and   Symbolic Reasoning</h2><p><strong>Authors:Alan Dao, Dinh Bach Vu, Bui Quang Huy</strong></p>
<p>This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. </p>
<blockquote>
<p>本文介绍了AlphaSpace，这是一种旨在增强大型语言模型（LLM）在三维笛卡尔空间导航中的空间推理能力的新型方法。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记对高度信息进行编码，并主要整合符号合成推理数据。这种方法使得LLM能够准确地将物体定位在特定的[x，y，z]坐标上。实验结果表明，在操纵子任务方面，AlphaSpace显著优于现有模型，总准确率达到了66.67%，而GPT-4o的准确率为37.5%，Claude 3.5 Sonnet的准确率为29.17%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了AlphaSpace这一新型方法，旨在提高大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的令牌化策略，通过专用语义令牌编码高度信息，并主要融入符号合成推理数据。该方法使LLMs能够准确地对物体进行定位操作，实验结果显示AlphaSpace在操控子任务上的表现显著优于现有模型，总准确率达到66.67%，而GPT-4o和Claude 3.5 Sonnet分别为37.5%和29.17%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaSpace是一种旨在增强大型语言模型空间推理能力的新方法。</li>
<li>AlphaSpace采用基于语义的令牌化策略来处理高度信息。</li>
<li>AlphaSpace主要通过融入符号合成推理数据来提升LLMs的空间操控能力。</li>
<li>实验结果显示AlphaSpace在操控任务上的表现优于其他模型。</li>
<li>AlphaSpace的总准确率达到66.67%，高于GPT-4o和Claude 3.5 Sonnet的表现。</li>
<li>AlphaSpace的应用有助于LLMs更准确地执行物体定位操作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1dcb48e7331995109113939f9675d8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd03bd88758205fa4395abca4a4bbe44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78b622ba5cab70a498ccbd1bde8c17a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-wise-Multi-dimensional-and-Generalist-Reward-Model-with-Benchmark"><a href="#Boosting-Virtual-Agent-Learning-and-Reasoning-A-Step-wise-Multi-dimensional-and-Generalist-Reward-Model-with-Benchmark" class="headerlink" title="Boosting Virtual Agent Learning and Reasoning: A Step-wise,   Multi-dimensional, and Generalist Reward Model with Benchmark"></a>Boosting Virtual Agent Learning and Reasoning: A Step-wise,   Multi-dimensional, and Generalist Reward Model with Benchmark</h2><p><strong>Authors:Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li</strong></p>
<p>The development of Generalist Virtual Agents (GVAs) powered by Multimodal Large Language Models (MLLMs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-wise Multi-dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Galery23/Similar-v1">https://github.com/Galery23/Similar-v1</a>. </p>
<blockquote>
<p>由多模态大型语言模型（MLLMs）驱动的全能虚拟代理（GVAs）的发展在自主任务执行方面显示出巨大的潜力。然而，当前的训练模式面临关键局限性，包括依赖于结果监督和劳动密集型人工注释。为了解决这些挑战，我们提出了“相似”，一种逐步多维全能奖励模型，它为代理训练提供精细的信号，并为推理时间尺度选择更好的操作。具体来说，我们首先系统地定义了五个维度来评估代理操作。在此基础上，我们设计了一种MCTS-P算法，该算法可以自动收集和注释逐步的、五维的代理执行数据。使用这些数据，我们用三重M策略训练“相似”。此外，我们引入了虚拟代理领域中第一个用于逐步多维奖励模型训练和评估的基准测试，名为SRM。该基准测试由两部分组成：用作“相似”训练集的SRMTrain和用于评估奖励模型的SRMEval手动选定测试集。实验结果表明，“相似”通过其逐步多维评估和协同增益，为GVAs在训练和推理时间尺度上提供了有效的中间信号。代码可在<a target="_blank" rel="noopener" href="https://github.com/Galery23/Similar-v1%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Galery23/Similar-v1中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18665v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于多模态大型语言模型的通用虚拟代理（GVAs）在自主任务执行方面展现出巨大潜力，但当前训练模式存在依赖结果监督和劳动密集型人工标注等局限性。为此，本文提出一种名为Similar的逐步多维通用奖励模型，为代理训练提供精细信号，并在推理时间缩放时选择更好的动作。文章定义五个维度评估代理动作，设计MCTS-P算法自动收集和标注逐步的五维代理执行数据。此外，引入虚拟代理领域的首个逐步多维奖励模型训练和评估基准，名为SRM，包括用于训练Similar的SRMTrain和用于评估奖励模型的SRMEval。实验结果表明，Similar通过逐步多维评估和协同增益，为GVAs在训练和推理时间缩放期间提供有效的中间信号。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Generalist Virtual Agents (GVAs) powered by Multimodal Large Language Models (MLLMs) 在自主任务执行方面展现出巨大潜力。</li>
<li>当前GVAs训练面临依赖结果监督和劳动密集型人工标注等局限性。</li>
<li>Similar是一种逐步多维通用奖励模型，提供精细信号用于代理训练，并在推理时帮助选择更好的动作。</li>
<li>定义了五个维度评估代理动作。</li>
<li>使用MCTS-P算法自动收集和标注代理执行数据。</li>
<li>引入虚拟代理领域的首个基准SRM，包括SRMTrain和SRMEval，用于训练和评估奖励模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a931af52e2894d497c517a1a841f3cb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f739315c21c403e2ca7d1a01e417fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce07ac3e319f12b147d3084fdf28bcb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ab38ff767563a24153349f862b6f2f9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Training-Free-Personalization-via-Retrieval-and-Reasoning-on-Fingerprints"><a href="#Training-Free-Personalization-via-Retrieval-and-Reasoning-on-Fingerprints" class="headerlink" title="Training-Free Personalization via Retrieval and Reasoning on   Fingerprints"></a>Training-Free Personalization via Retrieval and Reasoning on   Fingerprints</h2><p><strong>Authors:Deepayan Das, Davide Talon, Yiming Wang, Massimiliano Mancini, Elisa Ricci</strong></p>
<p>Vision Language Models (VLMs) have lead to major improvements in multimodal reasoning, yet they still struggle to understand user-specific concepts. Existing personalization methods address this limitation but heavily rely on training procedures, that can be either costly or unpleasant to individual users. We depart from existing work, and for the first time explore the training-free setting in the context of personalization. We propose a novel method, Retrieval and Reasoning for Personalization (R2P), leveraging internal knowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint, i.e., key attributes uniquely defining the concept within its semantic class. When a query arrives, the most similar fingerprints are retrieved and scored via chain-of-thought-reasoning. To reduce the risk of hallucinations, the scores are validated through cross-modal verification at the attribute level: in case of a discrepancy between the scores, R2P refines the concept association via pairwise multimodal matching, where the retrieved fingerprints and their images are directly compared with the query. We validate R2P on two publicly available benchmarks and a newly introduced dataset, Personal Concepts with Visual Ambiguity (PerVA), for concept identification highlighting challenges in visual ambiguity. R2P consistently outperforms state-of-the-art approaches on various downstream tasks across all benchmarks. Code will be available upon acceptance. </p>
<blockquote>
<p>视觉语言模型（VLMs）在多模态推理方面取得了重大改进，但它们仍然难以理解用户特定的概念。现有的个性化方法解决了这一局限性，但严重依赖于训练程序，这可能既昂贵又令个别用户感到不适。我们与现有工作有所不同，首次在个性化的背景下探索无训练设置。我们提出了一种新方法，名为个性化检索与推理（R2P），它利用VLMs的内部知识。首先，我们利用VLMs提取概念指纹，即在其语义类别中唯一定义概念的关键属性。当查询到来时，通过思维链推理检索最相似的指纹并进行评分。为了减少幻觉的风险，通过跨模态验证在属性级别验证评分：在评分不一致的情况下，R2P通过一对一的多模态匹配细化概念关联，其中检索到的指纹及其图像直接与查询进行比较。我们在两个公开可用的基准测试集和一个新引入的数据集Personal Concepts with Visual Ambiguity（PerVA）上验证了R2P在概念识别方面的表现，突出了视觉模糊性的挑战。R2P在各种下游任务上的表现始终优于所有基准测试集的最新方法。代码将在接受后提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为R2P的新方法，利用VLMs的内部知识，无需训练即可实现个性化。该方法通过提取概念指纹和进行链式思维推理，检索最相似的指纹并进行评分。为减少幻觉风险，通过跨模态验证属性级别的评分。在存在评分差异的情况下，R2P通过成对的多模态匹配来完善概念关联。该方法在公开基准测试、新引入的PerVA数据集上的概念识别任务中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在多模态推理方面取得了重大改进，但在理解用户特定概念方面仍存在挑战。</li>
<li>现有个性化方法虽能解决此问题，但依赖繁琐且耗时的训练过程。</li>
<li>本文首次探索了无训练设置的个性化方法，提出了R2P方法。</li>
<li>R2P利用VLMs提取概念指纹，并通过链式思维推理检索并评分最相似的指纹。</li>
<li>为减少幻觉风险，R2P采用跨模态验证属性级别的评分。</li>
<li>当存在评分差异时，R2P通过成对的多模态匹配完善概念关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18623">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b55a66b6117f3ee305754eae40af2d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47832bf33976f39f74787b175ff4fd61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cca1a1b40c6f5157ec851645557ad4c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MetaSpatial-Reinforcing-3D-Spatial-Reasoning-in-VLMs-for-the-Metaverse"><a href="#MetaSpatial-Reinforcing-3D-Spatial-Reasoning-in-VLMs-for-the-Metaverse" class="headerlink" title="MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse"></a>MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse</h2><p><strong>Authors:Zhenyu Pan, Han Liu</strong></p>
<p>We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR&#x2F;VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at <a target="_blank" rel="noopener" href="https://github.com/PzySeere/MetaSpatial">https://github.com/PzySeere/MetaSpatial</a>. </p>
<blockquote>
<p>我们推出了MetaSpatial，这是第一个基于强化学习（RL）的框架，旨在增强视觉语言模型（VLMs）中的3D空间推理能力，实现实时3D场景生成而无需硬编码优化。MetaSpatial解决了两个核心挑战：（i）VLMs内部缺乏3D空间推理能力，限制了其生成真实布局的能力；（ii）对于布局生成任务，传统的监督微调（SFT）效率低下，因为完美的真实标注不可用。我们的主要创新之处在于采用了一种多回合的基于RL的优化机制，该机制融合了物理感知约束和渲染图像评估，确保生成的3D布局连贯、物理上可行且审美上一致。方法上，MetaSpatial引入了一种自适应的迭代推理过程，VLM通过分析渲染输出来完善空间布局，逐步改进场景的一致性。经验评估表明，MetaSpatial显著提高了各种规模模型的空间一致性和格式稳定性。训练后，对象放置更加真实、对齐和功能连贯，验证了强化学习在元宇宙、AR&#x2F;VR、数字孪生和游戏开发应用程序中的3D空间推理的有效性。我们的代码、数据和训练管道可在<a target="_blank" rel="noopener" href="https://github.com/PzySeere/MetaSpatial">https://github.com/PzySeere/MetaSpatial</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18470v1">PDF</a> Working Paper</p>
<p><strong>Summary</strong></p>
<p>MetaSpatial是首个基于强化学习（RL）的框架，旨在增强视觉语言模型（VLM）的3D空间推理能力，实现实时3D场景生成，无需硬编码优化。该框架解决了VLM在生成真实布局方面的两大挑战：缺乏内在的3D空间推理能力和传统监督微调（SFT）在布局生成任务中的低效性。MetaSpatial通过多回合的RL优化机制，结合物理感知约束和渲染图像评估，确保生成的3D布局在逻辑上连贯、物理上可行和美学上一致。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaSpatial是首个强化学习（RL）框架，用于增强视觉语言模型（VLM）的3D空间推理能力。</li>
<li>MetaSpatial解决了VLM在生成真实布局方面的两大挑战。</li>
<li>缺乏内在的3D空间推理能力是VLM生成真实布局的一个限制。</li>
<li>传统监督微调（SFT）在布局生成任务中的低效性是一个问题。</li>
<li>MetaSpatial通过多回合的RL优化机制来确保生成的3D布局的质量。</li>
<li>MetaSpatial结合了物理感知约束和渲染图像评估来确保生成的布局在逻辑上连贯、物理上可行和美学上一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9a3097db8d905794c82d950a1274ae88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106860fd65527a76611b25de4bdc9519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f965290fe05553f063fc920b3ff4cd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PALATE-Peculiar-Application-of-the-Law-of-Total-Expectation-to-Enhance-the-Evaluation-of-Deep-Generative-Models"><a href="#PALATE-Peculiar-Application-of-the-Law-of-Total-Expectation-to-Enhance-the-Evaluation-of-Deep-Generative-Models" class="headerlink" title="PALATE: Peculiar Application of the Law of Total Expectation to Enhance   the Evaluation of Deep Generative Models"></a>PALATE: Peculiar Application of the Law of Total Expectation to Enhance   the Evaluation of Deep Generative Models</h2><p><strong>Authors:Tadeusz Dziarmaga, Marcin Kądziołka, Artur Kasymov, Marcin Mazur</strong></p>
<p>Deep generative models (DGMs) have caused a paradigm shift in the field of machine learning, yielding noteworthy advancements in domains such as image synthesis, natural language processing, and other related areas. However, a comprehensive evaluation of these models that accounts for the trichotomy between fidelity, diversity, and novelty in generated samples remains a formidable challenge. A recently introduced solution that has emerged as a promising approach in this regard is the Feature Likelihood Divergence (FLD), a method that offers a theoretically motivated practical tool, yet also exhibits some computational challenges. In this paper, we propose PALATE, a novel enhancement to the evaluation of DGMs that addresses limitations of existing metrics. Our approach is based on a peculiar application of the law of total expectation to random variables representing accessible real data. When combined with the MMD baseline metric and DINOv2 feature extractor, PALATE offers a holistic evaluation framework that matches or surpasses state-of-the-art solutions while providing superior computational efficiency and scalability to large-scale datasets. Through a series of experiments, we demonstrate the effectiveness of the PALATE enhancement, contributing a computationally efficient, holistic evaluation approach that advances the field of DGMs assessment, especially in detecting sample memorization and evaluating generalization capabilities. </p>
<blockquote>
<p>深度生成模型（DGMs）在机器学习领域引起了范式转变，在图像合成、自然语言处理等领域取得了值得注意的进展。然而，对生成的样本中保真度、多样性和新颖性三者之间的三分关系进行全面评估仍然是一个巨大的挑战。最近出现的一种具有前景的解决方案是特征似然散度（FLD），它是一种提供理论驱动的实用工具的方法，但也表现出一些计算挑战。在本文中，我们提出了针对DGMs评估的新型增强方法PALATE，解决了现有指标的局限性。我们的方法基于对可访问实际数据代表的随机变量应用全期望定律的一种特殊应用。当与MMD基线指标和DINOv2特征提取器结合时，PALATE提供了一个全面的评估框架，该框架在匹配或超越最新解决方案的同时，为大规模数据集提供了优越的计算效率和可扩展性。通过一系列实验，我们证明了PALATE增强的有效性，为深度生成模型评估领域提出了一种计算高效、全面的评估方法，特别是在检测样本记忆和评估泛化能力方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>深生成模型（DGMs）在机器学习领域引起了范式转变，并在图像合成、自然语言处理等领域取得了显著进展。然而，对模型的全面评估仍然是一个挑战，需要考虑到生成样本的逼真度、多样性和新颖性之间的三元关系。最近出现了一种名为特征可能性发散（FLD）的评估方法，尽管该方法提供了一个实用的工具，但也存在一些计算挑战。本文提出PALATE，一种针对DGMs评估的新型增强方法，解决了现有指标的限制。PALATE基于可访问真实数据代表的随机变量的总期望定律的特殊应用。与MMD基线指标和DINOv2特征提取器结合，PALATE提供了一个全面的评估框架，在匹配或超越最新解决方案的同时，为大规模数据集提供了更高的计算效率和可扩展性。实验证明，PALATE增强方法有效，为DGMs评估领域提供了一种计算高效、全面的评估方法，特别是在检测样本记忆和评估泛化能力方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深生成模型（DGMs）在机器学习领域有重大突破，尤其在图像合成和自然语言处理等领域。</li>
<li>对DGMs的评估需要综合考虑生成样本的逼真度、多样性和新颖性。</li>
<li>特征可能性发散（FLD）是一种新兴的DGMs评估方法，但存在计算挑战。</li>
<li>PALATE是一种新型的DGMs评估增强方法，基于总期望定律，旨在解决现有指标的限制。</li>
<li>PALATE与MMD基线指标和DINOv2特征提取器结合，提供了全面的评估框架，具有高效计算和可扩展性。</li>
<li>实验证明PALATE增强方法有效，特别是在检测样本记忆和评估模型泛化能力方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fc1e2df360a3a3e0f3a0e4dfbae962d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef95342bd7801b296a9beb92668e6a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b022379f3aa8285b24e663417803a175.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding"><a href="#On-the-Perception-Bottleneck-of-VLMs-for-Chart-Understanding" class="headerlink" title="On the Perception Bottleneck of VLMs for Chart Understanding"></a>On the Perception Bottleneck of VLMs for Chart Understanding</h2><p><strong>Authors:Junteng Liu, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He</strong></p>
<p>Chart understanding requires models to effectively analyze and reason about numerical data, textual elements, and complex visual components. Our observations reveal that the perception capabilities of existing large vision-language models (LVLMs) constitute a critical bottleneck in this process. In this study, we delve into this perception bottleneck by decomposing it into two components: the vision encoder bottleneck, where the visual representation may fail to encapsulate the correct information, and the extraction bottleneck, where the language model struggles to extract the necessary information from the provided visual representations. Through comprehensive experiments, we find that (1) the information embedded within visual representations is substantially richer than what is typically captured by linear extractors, such as the widely used retrieval accuracy metric; (2) While instruction tuning effectively enhances the extraction capability of LVLMs, the vision encoder remains a critical bottleneck, demanding focused attention and improvement. Therefore, we further enhance the visual encoder to mitigate the vision encoder bottleneck under a contrastive learning framework. Empirical results demonstrate that our approach significantly mitigates the perception bottleneck and improves the ability of LVLMs to comprehend charts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart">https://github.com/hkust-nlp/Vision4Chart</a>. </p>
<blockquote>
<p>图表理解需要模型有效地分析和推理数值数据、文本元素和复杂的视觉成分。我们的观察表明，现有大型视觉语言模型的感知能力构成了这一过程中的关键瓶颈。在这项研究中，我们通过将其分解为两个组成部分来深入研究这一感知瓶颈：视觉编码器瓶颈，其中视觉表示可能无法包含正确的信息；和提取瓶颈，其中语言模型难以从提供的视觉表示中提取必要的信息。通过全面的实验，我们发现（1）视觉表示中所嵌入的信息远比线性提取器（如广泛使用的检索准确率指标）所捕获的要丰富得多；（2）虽然指令调整有效地提高了LVLMs的提取能力，但视觉编码器仍然是一个关键的瓶颈，需要重点关注和改进。因此，我们在对比学习框架下进一步增强了视觉编码器，以缓解视觉编码器瓶颈。实证结果表明，我们的方法显著缓解了感知瓶颈，提高了LVLMs理解图表的能力。代码已公开在<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Vision4Chart%E4%B8%8A%E3%80%82">https://github.com/hkust-nlp/Vision4Chart上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18435v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型视觉语言模型在处理图表理解时的瓶颈问题，将其分解为视觉编码器瓶颈和信息提取瓶颈两个组成部分。研究指出，视觉表示的信息丰富程度超过线性提取器的捕捉能力，而指令微调虽能提高语言模型的提取能力，但视觉编码器仍是关键瓶颈。为缓解视觉编码器瓶颈，研究在对比学习框架下增强了视觉编码器。实验结果证明，该方法显著减轻了感知瓶颈，提高了大型视觉语言模型对图表的理解能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图表理解需要模型分析数值数据、文本元素和复杂视觉成分。</li>
<li>现有大型视觉语言模型在感知过程中存在关键瓶颈。</li>
<li>视觉表示的信息丰富程度超过线性提取器的捕捉能力。</li>
<li>指令微调提高了语言模型的提取能力，但视觉编码器仍是关键瓶颈。</li>
<li>对比学习框架下增强了视觉编码器以缓解视觉编码器瓶颈。</li>
<li>方法显著减轻了感知瓶颈，提高了大型视觉语言模型对图表的理解能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44af51b11fa63a1eb20ce5ea5246515c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63a7596c12fd7308cb9c1c5e95bc870e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc82eda912ecb1ca48d328289b07f47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c4d962c16e96c7d3cab5fe875f9970c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-709fcb5d7cf493b5393161df9d10f8e3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1fd0df86edf04907f3a27458febd4eb5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-27  CoLLM A Large Language Model for Composed Image Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8b5ba8281591458d5956e0ce43c3bee.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-24  ReLearn Unlearning via Learning for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
