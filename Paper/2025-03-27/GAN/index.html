<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2025-03-27  AvatarArtist Open-Domain 4D Avatarization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c44b9fac00940094057255e461b9d913.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-27-更新"><a href="#2025-03-27-更新" class="headerlink" title="2025-03-27 更新"></a>2025-03-27 更新</h1><h2 id="AvatarArtist-Open-Domain-4D-Avatarization"><a href="#AvatarArtist-Open-Domain-4D-Avatarization" class="headerlink" title="AvatarArtist: Open-Domain 4D Avatarization"></a>AvatarArtist: Open-Domain 4D Avatarization</h2><p><strong>Authors:Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, Qifeng Chen</strong></p>
<p>This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies.. </p>
<blockquote>
<p>本文专注于开放域4D avatar化，旨在从任意风格的肖像图像创建4D avatar。我们选择参数化triplanes作为中间4D表示，并提出了一种实用的训练范式，该范式结合了生成对抗网络（GANs）和扩散模型。我们的设计源于对以下现象的观察：4D GANs在无监督的情况下擅长于在图像和triplanes之间建立桥梁，但在处理多样数据分布时通常面临挑战。一个稳健的二维扩散先验作为解决方案出现，它有助于GAN在不同领域之间转移其专业知识。这些专家之间的协同作用使得可以构建多域图像-triplane数据集，从而推动通用4D avatar创作者的发展。大量实验表明，我们的模型AvatarArtist能够生成高质量的4D avatars，对各种源图像域具有很强的稳健性。代码、数据和模型将公开提供，以便于未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19906v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文关注开放域4D个性化角色创建，旨在从任意风格的肖像图片中创建4D个性化角色。研究采用参数化triplanes作为中间4D表示形式，并提出一种结合生成对抗网络（GANs）和扩散模型的实际训练范式。设计思路源于观察，即4D GANs在无需监督的情况下擅长于图像和triplanes之间的桥梁搭建，但在处理多样化数据分布时面临挑战。因此，引入稳健的2D扩散先验，帮助GAN在不同领域之间转移知识。专家之间的协同作用使得构建多域图像-triplane数据集成为可能，进而推动通用4D个性化角色创建器的发展。实验表明，所开发的AvatarArtist模型能够生成高质量的4D个性化角色，对不同源图像域的鲁棒性强。代码、数据和模型将公开提供，以推动未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文关注开放域4D个性化角色创建，旨在通过GANs和扩散模型创建4D个性化角色。</li>
<li>采用参数化triplanes作为中间4D表示形式。</li>
<li>4D GANs在无需监督的情况下擅长桥梁搭建，但在处理多样化数据分布时面临挑战。</li>
<li>引入稳健的2D扩散先验，协助GAN在不同领域间转移知识。</li>
<li>通过协同作用构建多域图像-triplane数据集。</li>
<li>开发了一个名为AvatarArtist的模型，能够生成高质量的4D个性化角色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c44b9fac00940094057255e461b9d913.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea16d6a700d3d7ab1b01cea39a78f1f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f846e42e4d2905e2771588ee66b27a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e29711d534a981fd7ac40ac11f5578c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3014ffdb5e751ad6bd3b845ec67cf6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush"><a href="#In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush" class="headerlink" title="In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush"></a>In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush</h2><p><strong>Authors:Vitaly Gnatyuk, Valeriia Koriukina Ilya Levoshevich, Pavel Nurminskiy, Guenter Wallner</strong></p>
<p>With video games steadily increasing in complexity, automated generation of game content has found widespread interest. However, the task of 3D gaming map art creation remains underexplored to date due to its unique complexity and domain-specific challenges. While recent works have addressed related topics such as retro-style level generation and procedural terrain creation, these works primarily focus on simpler data distributions. To the best of our knowledge, we are the first to demonstrate the application of modern AI techniques for high-resolution texture manipulation in complex, highly detailed AAA 3D game environments. We introduce a novel Smart Brush for map editing, designed to assist artists in seamlessly modifying selected areas of a game map with minimal effort. By leveraging generative adversarial networks and diffusion models we propose two variants of the brush that enable efficient and context-aware generation. Our hybrid workflow aims to enhance both artistic flexibility and production efficiency, enabling the refinement of environments without manually reworking every detail, thus helping to bridge the gap between automation and creative control in game development. A comparative evaluation of our two methods with adapted versions of several state-of-the art models shows that our GAN-based brush produces the sharpest and most detailed outputs while preserving image context while the evaluated state-of-the-art models tend towards blurrier results and exhibit difficulties in maintaining contextual consistency. </p>
<blockquote>
<p>随着视频游戏的复杂度不断提升，游戏内容的自动生成已引起广泛关注。然而，由于3D游戏地图艺术创作的独特复杂性和特定领域的挑战，至今该任务仍被较少探索。虽然近期有一些关于复古风格关卡生成和程序化地形生成的研究，但这些研究主要集中在更简单的数据分布上。据我们所知，我们是首次展示现代AI技术在复杂、高度详细的AAA 3D游戏环境中进行高分辨率纹理操作的应用。我们引入了一种新型的智能地图编辑笔刷，旨在帮助艺术家轻松修改游戏地图的选定区域。通过利用生成对抗网络和扩散模型，我们提出了两种笔刷变体，以实现高效和上下文感知的生成。我们的混合工作流程旨在提高艺术灵活性和生产效率，能够在不手动重新处理每个细节的情况下优化环境，从而有助于弥合游戏开发中自动化和创意控制之间的差距。我们对两种方法与几种最新模型的改编版本进行比较评估，结果表明，我们的GAN笔刷产生的输出最清晰、最详细，同时保留了图像上下文，而被评估的最新模型往往结果模糊，并且在保持上下文一致性方面存在困难。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19793v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着游戏的复杂度不断提升，游戏内容的自动生成受到了广泛关注。然而，由于3D游戏地图艺术创作的独特复杂性和特定领域挑战，该任务至今仍未得到充分探索。尽管有相关工作涉及复古风格层级生成和程序化地形创建等话题，但它们主要关注更简单的数据分布。我们是首次展示将现代AI技术应用于复杂、高度详细的AAA级3D游戏环境中高分辨率纹理操作的实践。我们推出了一款新型智能刷子工具，旨在帮助艺术家轻松修改游戏地图的选定区域。借助对抗生成网络和扩散模型，我们提出了两款刷子的变体，实现了高效且语境感知的生成。我们的混合工作流程旨在提高艺术灵活性和生产效率，可以在不重新手动处理每个细节的情况下优化环境，有助于弥合游戏开发中自动化与创意控制之间的鸿沟。评估显示，我们的GAN基础刷子生成的结果最清晰、最详细，同时保持了图像上下文，而其他评估的先进模型则倾向于结果模糊，并且在保持上下文一致性方面存在困难。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D游戏地图艺术创作因其独特性、复杂性及领域特定挑战而仍被较少探索。</li>
<li>现有工作主要关注简单的数据分布和较低复杂度的游戏内容生成。</li>
<li>研究首次展示了现代AI技术在复杂、高详细度的AAA级3D游戏环境高分辨率纹理操作中的应用。</li>
<li>引入了一种新型智能刷子工具，可帮助艺术家轻松修改游戏地图的选定区域。</li>
<li>利用对抗生成网络和扩散模型的刷子变体实现了高效且语境感知的生成。</li>
<li>混合工作流程旨在提高艺术灵活性和生产效率，可以在不重新处理每个细节的情况下优化环境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bbabe2f8a853b755eea495cc0dfd9e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e15d179d1580f2c0dd020d404c96858e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96909c2845a90845202c28a6c501a4e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23b3ef9229cf940119438fd7eeca86d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d8d75da2de1068e7f1c407f3e6ea88a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Semantic-Feature-Discrimination-for-Perceptual-Image-Super-Resolution-and-Opinion-Unaware-No-Reference-Image-Quality-Assessment"><a href="#Exploring-Semantic-Feature-Discrimination-for-Perceptual-Image-Super-Resolution-and-Opinion-Unaware-No-Reference-Image-Quality-Assessment" class="headerlink" title="Exploring Semantic Feature Discrimination for Perceptual Image   Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment"></a>Exploring Semantic Feature Discrimination for Perceptual Image   Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment</h2><p><strong>Authors:Guanglu Dong, Xiangyu Liao, Mingyang Li, Guihuan Guo, Chao Ren</strong></p>
<p>Generative Adversarial Networks (GANs) have been widely applied to image super-resolution (SR) to enhance the perceptual quality. However, most existing GAN-based SR methods typically perform coarse-grained discrimination directly on images and ignore the semantic information of images, making it challenging for the super resolution networks (SRN) to learn fine-grained and semantic-related texture details. To alleviate this issue, we propose a semantic feature discrimination method, SFD, for perceptual SR. Specifically, we first design a feature discriminator (Feat-D), to discriminate the pixel-wise middle semantic features from CLIP, aligning the feature distributions of SR images with that of high-quality images. Additionally, we propose a text-guided discrimination method (TG-D) by introducing learnable prompt pairs (LPP) in an adversarial manner to perform discrimination on the more abstract output feature of CLIP, further enhancing the discriminative ability of our method. With both Feat-D and TG-D, our SFD can effectively distinguish between the semantic feature distributions of low-quality and high-quality images, encouraging SRN to generate more realistic and semantic-relevant textures. Furthermore, based on the trained Feat-D and LPP, we propose a novel opinion-unaware no-reference image quality assessment (OU NR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any additional targeted training. Extensive experiments on classical SISR, real-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our proposed methods. </p>
<blockquote>
<p>生成对抗网络（GANs）已被广泛应用于图像超分辨率（SR）以提高感知质量。然而，大多数现有的基于GAN的SR方法通常直接在图像上进行粗略的鉴别，忽略了图像的语义信息，这使得超分辨率网络（SRN）在学习细致和语义相关的纹理细节方面面临挑战。为了缓解这个问题，我们提出了一种用于感知SR的语义特征鉴别方法SFD。具体来说，我们首先设计了一个特征鉴别器（Feat-D），以鉴别CLIP的中间语义特征，使SR图像的特征分布与高质图像的特征分布对齐。此外，我们通过引入可学习的提示对（LPP）以对抗性的方式，提出了一种文本引导鉴别方法（TG-D），对CLIP的更抽象输出特征进行鉴别，进一步增强我们方法的判别能力。通过Feat-D和TG-D的结合，我们的SFD可以有效地区分低质量和高质量图像的语义特征分布，鼓励SRN生成更真实和语义相关的纹理。此外，基于已训练的Feat-D和LPP，我们提出了一种新型的无参考意见图像质量评估（OU NR-IQA）方法SFD-IQA，在不需要任何额外针对性训练的情况下，大大提高了OU NR-IQA的性能。在经典SISR、现实SISR和OU NR-IQA任务上的大量实验证明了我们提出方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19295v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对生成对抗网络（GANs）在图像超分辨率（SR）应用中的挑战，提出了一种语义特征判别方法（SFD）。通过设计特征判别器（Feat-D）和文本引导判别方法（TG-D），SFD能够区分低质量和高质量图像的语义特征分布，鼓励超分辨率网络（SRN）生成更真实、语义相关的纹理。此外，基于训练的Feat-D和可学习提示对（LPP），还提出了一种新型的无参考意见图像质量评估方法SFD-IQA，大大提高了无参考意见图像质量评估的性能，且无需任何额外的针对性训练。实验表明，该方法在经典SISR、现实世界SISR和无参考意见图像质量评估任务中均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANs在图像超分辨率中面临挑战，缺乏语义信息的考虑使得学习精细纹理变得困难。</li>
<li>提出一种语义特征判别方法（SFD），通过设计特征判别器（Feat-D）来识别图像的中间语义特征。</li>
<li>结合文本引导判别方法（TG-D）和可学习提示对（LPP），提高方法的判别能力。</li>
<li>SFD能有效区分低质量和高质量图像的语义特征分布，鼓励超分辨率网络生成更真实、语义相关的纹理。</li>
<li>基于训练的Feat-D和LPP，提出了一种新型的无参考意见图像质量评估方法SFD-IQA。</li>
<li>SFD-IQA在无参考意见图像质量评估任务中表现出良好的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74ee0bea2125ed6d1ec8ada6e9bd52fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d03334023f870a70a09bdacc1fadea4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a484e0f169b0e65cedf79fb2e2cb8107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c5e7c00ea54cdbbd9e31dbdacfe9a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a72c2938ced68d1c2d393af89ecbcd68.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HingeRLC-GAN-Combating-Mode-Collapse-with-Hinge-Loss-and-RLC-Regularization"><a href="#HingeRLC-GAN-Combating-Mode-Collapse-with-Hinge-Loss-and-RLC-Regularization" class="headerlink" title="HingeRLC-GAN: Combating Mode Collapse with Hinge Loss and RLC   Regularization"></a>HingeRLC-GAN: Combating Mode Collapse with Hinge Loss and RLC   Regularization</h2><p><strong>Authors:Osman Goni, Himadri Saha Arka, Mithun Halder, Mir Moynuddin Ahmed Shibly, Swakkhar Shatabda</strong></p>
<p>Recent advances in Generative Adversarial Networks (GANs) have demonstrated their capability for producing high-quality images. However, a significant challenge remains mode collapse, which occurs when the generator produces a limited number of data patterns that do not reflect the diversity of the training dataset. This study addresses this issue by proposing a number of architectural changes aimed at increasing the diversity and stability of GAN models. We start by improving the loss function with Wasserstein loss and Gradient Penalty to better capture the full range of data variations. We also investigate various network architectures and conclude that ResNet significantly contributes to increased diversity. Building on these findings, we introduce HingeRLC-GAN, a novel approach that combines RLC Regularization and the Hinge loss function. With a FID Score of 18 and a KID Score of 0.001, our approach outperforms existing methods by effectively balancing training stability and increased diversity. </p>
<blockquote>
<p>生成对抗网络（GANs）的最新进展已经证明了其生成高质量图像的能力。然而，仍然存在一个重大挑战，即模式崩溃。当生成器生成的数据模式有限，无法反映训练数据集的多样性时，就会发生这种情况。本研究通过提出一系列旨在提高GAN模型多样性和稳定性的架构改进来解决这个问题。我们通过改进损失函数，使用Wasserstein损失和梯度惩罚来更好地捕捉数据变化的全范围。我们还研究了各种网络架构，并得出结论认为ResNet对增加多样性做出了重大贡献。基于这些发现，我们引入了HingeRLC-GAN，这是一种将RLC正则化和Hinge损失函数相结合的新方法。我们的方法FID得分为18，KID得分为0.001，通过有效平衡训练稳定性和增加多样性，优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19074v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究通过改进生成对抗网络（GANs）的架构来解决模式崩溃问题，旨在提高GAN模型的多样性和稳定性。研究通过改进损失函数、引入ResNet网络架构以及结合RLC正则化和Hinge损失函数等方法，实现了高质量图像的生成，并获得了较高的FID和KID评分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究针对生成对抗网络（GANs）的模式崩溃问题进行了深入的探讨。</li>
<li>通过改进损失函数，采用Wasserstein损失和梯度惩罚，以更好地捕捉数据的全范围变化。</li>
<li>研究发现ResNet网络架构对提高GAN模型的多样性有显著贡献。</li>
<li>引入了HingeRLC-GAN这一新方法，结合了RLC正则化和Hinge损失函数。</li>
<li>该方法实现了高质量图像的生成，并获得了较高的FID评分和KID评分。</li>
<li>该研究为解决GANs的模式崩溃问题提供了新的思路和方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ccffac182d73dd00f5ac18990850d6cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab72ff8a35bfd703b7c5d99dc1d74273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22538c5ef3e487c90839edbc71dbb4b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8130de28b5eb4f35f88f338f9f37fa5a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Uncertainty-guided-Perturbation-for-Image-Super-Resolution-Diffusion-Model"><a href="#Uncertainty-guided-Perturbation-for-Image-Super-Resolution-Diffusion-Model" class="headerlink" title="Uncertainty-guided Perturbation for Image Super-Resolution Diffusion   Model"></a>Uncertainty-guided Perturbation for Image Super-Resolution Diffusion   Model</h2><p><strong>Authors:Leheng Zhang, Weiyi You, Kexuan Shi, Shuhang Gu</strong></p>
<p>Diffusion-based image super-resolution methods have demonstrated significant advantages over GAN-based approaches, particularly in terms of perceptual quality. Building upon a lengthy Markov chain, diffusion-based methods possess remarkable modeling capacity, enabling them to achieve outstanding performance in real-world scenarios. Unlike previous methods that focus on modifying the noise schedule or sampling process to enhance performance, our approach emphasizes the improved utilization of LR information. We find that different regions of the LR image can be viewed as corresponding to different timesteps in a diffusion process, where flat areas are closer to the target HR distribution but edge and texture regions are farther away. In these flat areas, applying a slight noise is more advantageous for the reconstruction. We associate this characteristic with uncertainty and propose to apply uncertainty estimate to guide region-specific noise level control, a technique we refer to as Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e., flat regions) receive reduced noise to preserve more LR information, therefore improving performance. Furthermore, we modify the network architecture of previous methods to develop our Uncertainty-guided Perturbation Super-Resolution (UPSR) model. Extensive experimental results demonstrate that, despite reduced model size and training overhead, the proposed UWSR method outperforms current state-of-the-art methods across various datasets, both quantitatively and qualitatively. </p>
<blockquote>
<p>基于扩散的图像超分辨率方法相对于基于GAN的方法表现出了显著的优势，特别是在感知质量方面。基于长时间的马尔可夫链，扩散方法具有出色的建模能力，能够在真实场景中实现卓越的性能。与之前的方法不同，这些方法侧重于通过修改噪声时间表或采样过程来提高性能，我们的方法则侧重于改进对LR信息的利用。我们发现LR图像的不同区域可以看作是对应于扩散过程中的不同时间点，其中平坦区域更接近目标HR分布，而边缘和纹理区域则相距较远。在平坦区域应用轻微噪声更有利于重建。我们将这一特性与不确定性相关联，并提出应用不确定性估计来指导特定区域的噪声水平控制，这是一种我们称为“不确定性引导噪声加权”的技术。不确定性较低的像素（即平坦区域）减少噪声，以保留更多LR信息，从而提高性能。此外，我们修改了之前方法的网络架构，以开发我们的不确定性引导扰动超分辨率（UPSR）模型。大量的实验结果表明，尽管模型规模减小且训练开销降低，但所提出的UWSR方法在多个数据集上在定量和定性方面都优于当前最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18512v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>：基于扩散的图像超分辨率方法相较于GAN方法具有显著优势，尤其在感知质量上表现更优秀。该方法利用马尔可夫链进行建模，并在利用低分辨率信息方面进行了改进。通过关联不确定性和噪声权重控制，提出了不确定性引导噪声权重方法。修改网络架构后，提出不确定性引导扰动超分辨率模型，实验结果显示，该方法在多个数据集上均优于当前先进方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散方法相较于GAN在图像超分辨率上具有优势，尤其在感知质量上。</li>
<li>扩散方法利用马尔可夫链进行建模，具有强大的建模能力。</li>
<li>提出利用不确定性引导噪声权重控制，以提高重建效果。</li>
<li>在低不确定性区域（如平坦区域），应用较少的噪声以保留更多低分辨率信息。</li>
<li>修改了先前的网络架构，提出了不确定性引导扰动超分辨率模型（UPSR）。</li>
<li>实验证明，UPSR模型在多个数据集上的表现均优于当前先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fe76d60f9a8eb0bfc5297c0c373aa058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088c49fd108318d6bf2ccd5b7fb2ea34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4d580022e7147bd9019587febf218db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd97799756549a7f5555354f91d00d27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-252b18e865a04f5bc7bf7bac93dcb41c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbbe82f5b44b9d718ec3e5074f363644.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FundusGAN-A-Hierarchical-Feature-Aware-Generative-Framework-for-High-Fidelity-Fundus-Image-Generation"><a href="#FundusGAN-A-Hierarchical-Feature-Aware-Generative-Framework-for-High-Fidelity-Fundus-Image-Generation" class="headerlink" title="FundusGAN: A Hierarchical Feature-Aware Generative Framework for   High-Fidelity Fundus Image Generation"></a>FundusGAN: A Hierarchical Feature-Aware Generative Framework for   High-Fidelity Fundus Image Generation</h2><p><strong>Authors:Qingshan Hou, Meng Wang, Peng Cao, Zou Ke, Xiaoli Liu, Huazhu Fu, Osmar R. Zaiane</strong></p>
<p>Recent advancements in ophthalmology foundation models such as RetFound have demonstrated remarkable diagnostic capabilities but require massive datasets for effective pre-training, creating significant barriers for development and deployment. To address this critical challenge, we propose FundusGAN, a novel hierarchical feature-aware generative framework specifically designed for high-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid Network within its encoder to comprehensively extract multi-scale information, capturing both large anatomical structures and subtle pathological features. The framework incorporates a modified StyleGAN-based generator with dilated convolutions and strategic upsampling adjustments to preserve critical retinal structures while enhancing pathological detail representation. Comprehensive evaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN consistently outperforms state-of-the-art methods across multiple metrics (SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease classification experiments reveal that augmenting training data with FundusGAN-generated images significantly improves diagnostic accuracy across multiple CNN architectures (up to 6.49% improvement with ResNet50). These results establish FundusGAN as a valuable foundation model component that effectively addresses data scarcity challenges in ophthalmological AI research, enabling more robust and generalizable diagnostic systems while reducing dependency on large-scale clinical data collection. </p>
<blockquote>
<p>最近眼科基础模型（如RetFound）的最新进展已经表现出了令人瞩目的诊断能力，但需要大规模数据集进行有效的预训练，这为发展和部署带来了重大障碍。为了解决这一关键挑战，我们提出了FundusGAN，这是一种专门用于高保真眼底图像合成的新型分层特征感知生成框架。我们的方法在其编码器中使用特征金字塔网络，以全面提取多尺度信息，捕捉大型解剖结构和微妙的病理特征。该框架采用基于StyleGAN的修改过的生成器，结合了膨胀卷积和策略性上采样调整，以保留关键的视网膜结构，同时增强病理细节表示。在DDR、DRIVE和IDRiD数据集上的综合评估表明，FundusGAN在多个指标上始终优于最先进的方法（DDR上的SSIM：0.8863，FID：54.2，KID：0.0436）。此外，疾病分类实验表明，使用FundusGAN生成的图像增强训练数据可以显著提高多种CNN架构的诊断准确性（使用ResNet50可提高6.49%）。这些结果确立了FundusGAN作为一个有价值的基础模型组件，有效地解决了眼科人工智能研究中的数据稀缺挑战，能够构建更稳健和通用的诊断系统，同时减少对大规模临床数据收集的依赖。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17831v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>FundusGAN是一种新型的分级特征感知生成框架，专为高保真眼底图像合成设计。它通过采用特征金字塔网络和修改后的StyleGAN生成器，能够全面提取多尺度信息，并捕捉眼底的大型解剖结构和微妙的病理特征。该框架解决了眼科基础模型需要大量数据进行预训练的问题，提高了诊断准确性并降低了对大规模临床数据收集的依赖。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FundusGAN是一个针对眼底图像合成的生成框架，旨在解决眼科基础模型需要大量数据进行预训练的问题。</li>
<li>FundusGAN采用特征金字塔网络，能够全面提取眼底图像的多尺度信息。</li>
<li>该框架结合修改后的StyleGAN生成器，通过采用扩张卷积和战略上采样调整，保留了关键的视网膜结构，同时增强了病理细节表示。</li>
<li>FundusGAN在多个数据集上的综合评估表现优异，包括DDR、DRIVE和IDRiD数据集。</li>
<li>与现有方法相比，FundusGAN在多个指标上表现更出色，如结构相似性指数（SSIM）、弗雷歇距离（FID）和关键病变识别距离（KID）。</li>
<li>使用FundusGAN生成的图像进行训练数据增强，可以显著提高疾病分类实验的诊断准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-980b081b6cc21711ba88a04dd73c024c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fca3e9aff3e74a56aed38855930c7d7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Vision-Transformer-Based-Semantic-Communications-for-Next-Generation-Wireless-Networks"><a href="#Vision-Transformer-Based-Semantic-Communications-for-Next-Generation-Wireless-Networks" class="headerlink" title="Vision Transformer Based Semantic Communications for Next Generation   Wireless Networks"></a>Vision Transformer Based Semantic Communications for Next Generation   Wireless Networks</h2><p><strong>Authors:Muhammad Ahmed Mohsin, Muhammad Jazib, Zeeshan Alam, Muhmmad Farhan Khan, Muhammad Saad, Muhammad Ali Jamshed</strong></p>
<p>In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications. </p>
<blockquote>
<p>在6G网络不断演进的背景下，语义通信通过优先传输语义意义而不是原始数据准确性，正逐步革新数据传输方式。本文提出了一种基于视觉转换器（ViT）的语义通信框架，该框架经过精心设计，旨在实现图像传输过程中的高语义相似性，同时最小化对带宽的需求。通过采用ViT作为编码器-解码器框架，所提出的架构能够高效地将图像编码为具有高语义内容的信息，并在接收端考虑到实际世界的衰落和噪声因素，精确重建图像。基于ViT固有的注意力机制，我们的模型表现优于用于生成此类图像的卷积神经网络（CNNs）和生成对抗网络（GANs）。基于所提出的ViT网络的架构实现了峰值信噪比（PSNR）为38分贝，这在不同的通信环境中保持语义相似性方面高于其他深度学习（DL）方法。这些发现证明了我们基于ViT的方法在语义通信领域具有重大突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17275v1">PDF</a> Accepted @ ICC 2025</p>
<p><strong>Summary</strong><br>     在6G网络不断演变的背景下，语义通信正以其独特的优势革新数据传输方式，以语义意义传输为主轴超越原始数据的精确传输。本文提出了基于视觉转换器（ViT）的语义通信框架，该框架旨在确保图像传输时的高语义相似性，同时最大限度地减少对带宽的需求。使用ViT作为编码解码器框架，此架构能够高效地将图像编码为富含语义的内容，并在考虑现实世界的衰落和噪声因素的同时，精确地在接收端重建图像。借助ViT的内在注意力机制，我们的模型表现优于卷积神经网络（CNN）和生成对抗网络（GAN），这些网络专门用于生成此类图像。基于所提出的ViT网络的架构实现了峰值信噪比（PSNR）的38分贝，这在维持不同通信环境中的语义相似性方面高于其他深度学习（DL）方法，确立了基于ViT的方法在语义通信中的重大突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义通信在6G网络中占据重要地位，以传输语义意义为主轴，超越原始数据的精确传输。</li>
<li>提出基于视觉转换器（ViT）的语义通信框架，旨在确保图像传输的高语义相似性并减少带宽需求。</li>
<li>ViT作为编码解码器框架，能够高效编码图像并精确重建。</li>
<li>该模型借助ViT的内在注意力机制表现优异，优于CNN和GAN等网络。</li>
<li>基于ViT网络的架构实现高PSNR值，表明其在维持不同通信环境中的语义相似性方面效果卓越。</li>
<li>此研究在语义通信领域具有重大突破。</li>
<li>该框架在实际应用中的性能表现有待进一步研究和验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0df534ae5b470e64576af7600b755f3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbbb7e60332633c34c364b801488e4c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21209048b7e9606b9b6a28830f1830e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d270606b6b6e3e9d05c2c1607efd1556.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe64521aff8c50ce552d0e908684eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e567987f1d7b2f1df1e554f4da7309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b7f2666c41efb12f7db35f8ea712403.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b93f024f9013a40f1e6a437c1faf0579.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment"><a href="#DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment" class="headerlink" title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment"></a>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment</h2><p><strong>Authors:Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</strong></p>
<p>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style&#x2F;color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance. </p>
<blockquote>
<p>视频驱动神经面部再现旨在合成逼真的面部图像，成功保留源面部的身份和外观，同时转移目标头部姿势和面部表情。现有的基于GAN的方法存在扭曲和视觉伪影或重建质量差的问题，即背景和某些重要的外观细节（如发型&#x2F;颜色、眼镜和配饰）不能忠实重建。最近扩散概率模型（DPMs）的进展使得能够生成高质量的逼真图像。为此，本文提出了DiffusionAct，一种利用扩散模型的光栅化图像生成进行神经面部再现的新方法。具体来说，我们提出控制扩散自动编码器（DiffAE）的语义空间，以编辑输入图像的面部姿态，定义为头部姿态方向和面部表情。我们的方法允许一次、自我和跨主体再现，无需针对特定主体进行微调。我们与最新的GAN、StyleGAN2和基于扩散的方法进行了比较，显示出更好或相当的再现性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.17217v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stelabou.github.io/diffusionact/">https://stelabou.github.io/diffusionact/</a></p>
<p><strong>摘要</strong><br>基于视频驱动的神经网络面部复现旨在合成逼真的面部图像，成功保留源脸的身份和外观，同时转移目标头部姿势和面部表情。现有的基于GAN的方法存在失真和视觉伪影或重建质量差的问题，即背景和一些重要的外观细节，如发型&#x2F;颜色、眼镜和配饰，不能忠实重建。本文利用扩散概率模型（DPMs）的最新进展，生成高质量逼真图像。为此，我们提出了DiffusionAct方法，该方法利用扩散模型的逼真图像生成进行神经网络面部复现。具体来说，我们提出控制扩散自编码器（DiffAE）的语义空间，以编辑输入图像的面部姿势，定义为头部姿势方向和面部表情。我们的方法允许一次性、自我和跨主体复现，无需针对特定主体进行微调。我们与最先进GAN、StyleGAN2和基于扩散的方法进行比较，显示复现性能更好或相当。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频驱动神经网络面部复现旨在合成逼真面部图像，保留源脸身份和外观，同时转移目标头部姿势和表情。</li>
<li>基于GAN的方法存在失真和视觉伪影问题，或在背景及重要外观细节上的重建质量不佳。</li>
<li>DiffusionAct利用扩散模型的逼真图像生成进行面部复现。</li>
<li>通过控制扩散自编码器（DiffAE）的语义空间来编辑面部姿势。</li>
<li>DiffusionAct允许一次性、自我和跨主体复现，无需特定主体微调。</li>
<li>DiffusionAct与现有方法比较，复现性能更优或与最先进方法相当。</li>
<li>DiffusionAct为面部图像生成和编辑提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.17217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-239484d517c17b27046f61cf6519ea2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5860e5598e68cc87a546e6c31dee055e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d14a0d7f6781e1438a3a78de39f33397.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-03-27  AvatarArtist Open-Domain 4D Avatarization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a15b168ebe31fb6b22bc4004ac396872.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-03-27  NullSwap Proactive Identity Cloaking Against Deepfake Face Swapping
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
