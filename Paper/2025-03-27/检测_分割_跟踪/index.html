<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  LPOSS Label Propagation Over Patches and Pixels for Open-vocabulary   Semantic Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fe8e2ec1f311e54b959baac9c0821624.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="LPOSS-Label-Propagation-Over-Patches-and-Pixels-for-Open-vocabulary-Semantic-Segmentation"><a href="#LPOSS-Label-Propagation-Over-Patches-and-Pixels-for-Open-vocabulary-Semantic-Segmentation" class="headerlink" title="LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary   Semantic Segmentation"></a>LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary   Semantic Segmentation</h2><p><strong>Authors:Vladan StojniÄ‡, Yannis Kalantidis, JiÅ™Ã­ Matas, Giorgos Tolias</strong></p>
<p>We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: <a target="_blank" rel="noopener" href="https://github.com/vladan-stojnic/LPOSS">https://github.com/vladan-stojnic/LPOSS</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ— è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ ‡ç­¾ä¼ æ’­å¢å¼ºVLMsçš„åˆå§‹è¡¥ä¸é¢„æµ‹ï¼Œé€šè¿‡ç»“åˆè¡¥ä¸åˆ°è¡¥ä¸çš„å…³ç³»æ¥è”åˆä¼˜åŒ–é¢„æµ‹ã€‚ç”±äºVLMä¸»è¦ä¼˜åŒ–è·¨æ¨¡æ€å¯¹é½è€Œä¸æ˜¯æ¨¡æ€å†…ç›¸ä¼¼æ€§ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨è§‚å¯Ÿåˆ°èƒ½æ›´å¥½åœ°æ•æ‰è¿™äº›å…³ç³»çš„è§†è§‰æ¨¡å‹ï¼ˆVMï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åƒç´ çº§çš„æ ‡ç­¾ä¼ æ’­ä½œä¸ºç»†åŒ–æ­¥éª¤æ¥è§£å†³åŸºäºè¡¥ä¸çš„ç¼–ç å™¨æ‰€å›ºæœ‰çš„åˆ†è¾¨ç‡é™åˆ¶ï¼Œä»è€Œæ˜¾è‘—æé«˜ç±»è¾¹ç•Œé™„è¿‘çš„åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºLPOSS+ï¼Œå¯åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ¨æ–­ï¼Œé¿å…äº†åŸºäºçª—å£çš„å¤„ç†ï¼Œä»è€Œæ•è·äº†å…¨å›¾åƒçš„ä¸Šä¸‹æ–‡äº¤äº’ã€‚LPOSS+åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè·¨è¶Šå¤šä¸ªæ•°æ®é›†ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/vladan-stojnic/LPOSS">https://github.com/vladan-stojnic/LPOSS</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— è®­ç»ƒæ–¹æ³•çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ ‡ç­¾ä¼ æ’­ï¼Œé€šè¿‡ä¼˜åŒ–patchä¹‹é—´çš„é¢„æµ‹æ¥æé«˜è¯­ä¹‰åˆ†å‰²ç²¾åº¦ã€‚åˆ©ç”¨ä¸“é—¨çš„è§†è§‰æ¨¡å‹ï¼ˆVMï¼‰æ•æ‰patché—´çš„å…³ç³»ï¼Œå¹¶åœ¨åƒç´ çº§åˆ«åº”ç”¨æ ‡ç­¾ä¼ æ’­ä½œä¸ºä¼˜åŒ–æ­¥éª¤ï¼Œè§£å†³äº†åŸºäºpatchç¼–ç å™¨çš„å›ºæœ‰åˆ†è¾¨ç‡é™åˆ¶é—®é¢˜ã€‚è¯¥æ–¹æ³•ç§°ä¸ºLPOSS+ï¼Œå¯åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ¨ç†ï¼Œé¿å…äº†åŸºäºçª—å£çš„å¤„ç†ï¼Œä»è€Œæ•è·äº†å…¨å›¾çš„ä¸Šä¸‹æ–‡äº¤äº’ã€‚LPOSS+åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è®­ç»ƒå¤–çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ ‡ç­¾ä¼ æ’­ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–patché—´çš„é¢„æµ‹æ¥æé«˜è¯­ä¹‰åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨ä¸“é—¨çš„è§†è§‰æ¨¡å‹æ•æ‰patché—´çš„å…³ç³»ã€‚</li>
<li>åœ¨åƒç´ çº§åˆ«åº”ç”¨æ ‡ç­¾ä¼ æ’­ä»¥è§£å†³åŸºäºpatchç¼–ç å™¨çš„åˆ†è¾¨ç‡é™åˆ¶é—®é¢˜ã€‚</li>
<li>LPOSS+æ–¹æ³•åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ¨ç†ï¼Œé¿å…äº†åŸºäºçª—å£çš„å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b37247f6bba10874457c87c237fd6e58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70969a0dbc4083da1a360f0698b3d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aab5547ab5661f0e0050450024fce4e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BIMII-Net-Brain-Inspired-Multi-Iterative-Interactive-Network-for-RGB-T-Road-Scene-Semantic-Segmentation"><a href="#BIMII-Net-Brain-Inspired-Multi-Iterative-Interactive-Network-for-RGB-T-Road-Scene-Semantic-Segmentation" class="headerlink" title="BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T   Road Scene Semantic Segmentation"></a>BIMII-Net: Brain-Inspired Multi-Iterative Interactive Network for RGB-T   Road Scene Semantic Segmentation</h2><p><strong>Authors:Hanshuo Qiu, Jie Jiang, Ruoli Yang, Lixin Zhan, Jizhao Liu</strong></p>
<p>RGB-T road scene semantic segmentation enhances visual scene understanding in complex environments characterized by inadequate illumination or occlusion by fusing information from RGB and thermal images. Nevertheless, existing RGB-T semantic segmentation models typically depend on simple addition or concatenation strategies or ignore the differences between information at different levels. To address these issues, we proposed a novel RGB-T road scene semantic segmentation network called Brain-Inspired Multi-Iteration Interaction Network (BIMII-Net). First, to meet the requirements of accurate texture and local information extraction in road scenarios like autonomous driving, we proposed a deep continuous-coupled neural network (DCCNN) architecture based on a brain-inspired model. Second, to enhance the interaction and expression capabilities among multi-modal information, we designed a cross explicit attention-enhanced fusion module (CEAEF-Module) in the feature fusion stage of BIMII-Net to effectively integrate features at different levels. Finally, we constructed a complementary interactive multi-layer decoder structure, incorporating the shallow-level feature iteration module (SFI-Module), the deep-level feature iteration module (DFI-Module), and the multi-feature enhancement module (MFE-Module) to collaboratively extract texture details and global skeleton information, with multi-module joint supervision further optimizing the segmentation results. Experimental results demonstrate that BIMII-Net achieves state-of-the-art (SOTA) performance in the brain-inspired computing domain and outperforms most existing RGB-T semantic segmentation methods. It also exhibits strong generalization capabilities on multiple RGB-T datasets, proving the effectiveness of brain-inspired computer models in multi-modal image segmentation tasks. </p>
<blockquote>
<p>RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²é€šè¿‡èåˆRGBå’Œçº¢å¤–å›¾åƒçš„ä¿¡æ¯ï¼Œæé«˜äº†å¤æ‚ç¯å¢ƒä¸‹çš„è§†è§‰åœºæ™¯ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰çº¿ä¸è¶³æˆ–é®æŒ¡çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸ä¾èµ–äºç®€å•çš„åŠ æ³•æˆ–æ‹¼æ¥ç­–ç•¥ï¼Œæˆ–è€…å¿½ç•¥äº†ä¸åŒå±‚çº§ä¿¡æ¯ä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼Œç§°ä¸ºâ€œåŸºäºè„‘å¯å‘çš„å¤šè¿­ä»£äº¤äº’ç½‘ç»œâ€ï¼ˆBIMII-Netï¼‰ã€‚é¦–å…ˆï¼Œä¸ºäº†æ»¡è¶³è‡ªåŠ¨é©¾é©¶ç­‰é“è·¯åœºæ™¯ä¸‹å¯¹ç²¾ç¡®çº¹ç†å’Œå±€éƒ¨ä¿¡æ¯æå–çš„è¦æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè„‘å¯å‘æ¨¡å‹çš„æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å¢å¼ºå¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„äº¤äº’å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨BIMII-Netçš„ç‰¹å¾èåˆé˜¶æ®µè®¾è®¡äº†è·¨æ˜¾æ³¨æ„åŠ›å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEFæ¨¡å—ï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆä¸åŒå±‚çº§çš„ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªäº’è¡¥çš„å¤šå±‚è§£ç å™¨ç»“æ„ï¼ŒåŒ…å«æµ…å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ï¼ˆSFIæ¨¡å—ï¼‰ã€æ·±å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ï¼ˆDFIæ¨¡å—ï¼‰å’Œå¤šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆMFEæ¨¡å—ï¼‰ï¼Œä»¥ååŒæå–çº¹ç†ç»†èŠ‚å’Œå…¨å±€éª¨æ¶ä¿¡æ¯ï¼Œå¤šæ¨¡å—è”åˆç›‘ç£è¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å‰²ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºå¤§å¤šæ•°ç°æœ‰çš„RGB-Tè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚å®ƒåœ¨å¤šä¸ªRGB-Tæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†è„‘å¯å‘è®¡ç®—æœºæ¨¡å‹åœ¨å¤šæ¨¡æ€å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²çš„é‡è¦æ€§åŠå…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„åº”ç”¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²ç½‘ç»œBIMII-Netã€‚è¯¥ç½‘ç»œåŒ…å«æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„ã€è·¨æ˜¾å¼æ³¨æ„åŠ›å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEF-Moduleï¼‰ä»¥åŠå¤šå±‚è§£ç å™¨ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªRGB-Tæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGB-Té“è·¯åœºæ™¯è¯­ä¹‰åˆ†å‰²å¯¹äºæé«˜è§†è§‰åœºæ™¯ç†è§£åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰RGB-Tè¯­ä¹‰åˆ†å‰²æ¨¡å‹å­˜åœ¨ä¾èµ–ç®€å•èåˆç­–ç•¥æˆ–å¿½ç•¥ä¸åŒå±‚çº§ä¿¡æ¯å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>BIMII-Netç½‘ç»œåŒ…å«æ·±åº¦è¿ç»­è€¦åˆç¥ç»ç½‘ç»œï¼ˆDCCNNï¼‰æ¶æ„ï¼Œæ»¡è¶³å‡†ç¡®çº¹ç†å’Œå±€éƒ¨ä¿¡æ¯æå–çš„è¦æ±‚ã€‚</li>
<li>è·¨æ˜¾å¼æ³¨æ„åŠ›å¢å¼ºèåˆæ¨¡å—ï¼ˆCEAEF-Moduleï¼‰å¢å¼ºäº†å¤šæ¨¡æ€ä¿¡æ¯ä¹‹é—´çš„äº¤äº’å’Œè¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>å¤šå±‚è§£ç å™¨ç»“æ„åŒ…æ‹¬æµ…å±‚ç‰¹å¾è¿­ä»£æ¨¡å—ã€æ·±å±‚ç‰¹å¾è¿­ä»£æ¨¡å—å’Œå¤šç‰¹å¾å¢å¼ºæ¨¡å—ï¼Œæœ‰åŠ©äºæå–çº¹ç†ç»†èŠ‚å’Œå…¨å±€éª¨æ¶ä¿¡æ¯ã€‚</li>
<li>BIMII-Netåœ¨è„‘å¯å‘è®¡ç®—é¢†åŸŸå–å¾—é¢†å…ˆæ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f28645406850ca71a7d44b1410618348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-605c59d5c9d75d8e8c5f1ca084902ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ff7b1c7411577ec4c994887776c869.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Context-Aware-Semantic-Segmentation-Enhancing-Pixel-Level-Understanding-with-Large-Language-Models-for-Advanced-Vision-Applications"><a href="#Context-Aware-Semantic-Segmentation-Enhancing-Pixel-Level-Understanding-with-Large-Language-Models-for-Advanced-Vision-Applications" class="headerlink" title="Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding   with Large Language Models for Advanced Vision Applications"></a>Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding   with Large Language Models for Advanced Vision Applications</h2><p><strong>Authors:Ben Rahman</strong></p>
<p>Semantic segmentation has made significant strides in pixel-level image understanding, yet it remains limited in capturing contextual and semantic relationships between objects. Current models, such as CNN and Transformer-based architectures, excel at identifying pixel-level features but fail to distinguish semantically similar objects (e.g., â€œdoctorâ€ vs. â€œnurseâ€ in a hospital scene) or understand complex contextual scenarios (e.g., differentiating a running child from a regular pedestrian in autonomous driving). To address these limitations, we proposed a novel Context-Aware Semantic Segmentation framework that integrates Large Language Models (LLMs) with state-of-the-art vision backbones. Our hybrid model leverages the Swin Transformer for robust visual feature extraction and GPT-4 for enriching semantic understanding through text embeddings. A Cross-Attention Mechanism is introduced to align vision and language features, enabling the model to reason about context more effectively. Additionally, Graph Neural Networks (GNNs) are employed to model object relationships within the scene, capturing dependencies that are overlooked by traditional models. Experimental results on benchmark datasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the existing methods in both pixel-level accuracy (mIoU) and contextual understanding (mAP). This work bridges the gap between vision and language, paving the path for more intelligent and context-aware vision systems in applications including autonomous driving, medical imaging, and robotics. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²åœ¨åƒç´ çº§å›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨æ•æ‰å¯¹è±¡ä¹‹é—´ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å…³ç³»æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚å½“å‰æ¨¡å‹ï¼Œå¦‚CNNå’ŒåŸºäºTransformerçš„æ¶æ„ï¼Œåœ¨è¯†åˆ«åƒç´ çº§ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼å¯¹è±¡ï¼ˆä¾‹å¦‚åœ¨åŒ»é™¢åœºæ™¯ä¸­â€œåŒ»ç”Ÿâ€ä¸â€œæŠ¤å£«â€ï¼‰æˆ–ç†è§£å¤æ‚ä¸Šä¸‹æ–‡åœºæ™¯ï¼ˆä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶ä¸­åŒºåˆ†è·‘æ­¥çš„å­©å­ä¸æ™®é€šè¡Œäººï¼‰æ—¶å´è¡¨ç°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Context-Awareè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æœ€å…ˆè¿›çš„è§†è§‰ä¸»å¹²ç½‘ç»œç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ··åˆæ¨¡å‹åˆ©ç”¨Swin Transformerè¿›è¡Œç¨³å¥çš„è§†è§‰ç‰¹å¾æå–ï¼Œå¹¶åˆ©ç”¨GPT-4é€šè¿‡æ–‡æœ¬åµŒå…¥ä¸°å¯Œè¯­ä¹‰ç†è§£ã€‚å¼•å…¥äº†Cross Attentionæœºåˆ¶ï¼Œç”¨äºå¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œä¸Šä¸‹æ–‡æ¨ç†ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¯¹åœºæ™¯ä¸­çš„å¯¹è±¡å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œæ•æ‰ä¼ ç»Ÿæ¨¡å‹å¿½ç•¥çš„ä¾èµ–å…³ç³»ã€‚åœ¨COCOã€Cityscapesç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åƒç´ çº§ç²¾åº¦ï¼ˆmIoUï¼‰å’Œä¸Šä¸‹æ–‡ç†è§£ï¼ˆmAPï¼‰æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæ¶èµ·äº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—æˆåƒå’Œæœºå™¨äººæŠ€æœ¯åœ¨å†…çš„åº”ç”¨ä¸­çš„æ›´æ™ºèƒ½ã€æ›´å…³æ³¨ä¸Šä¸‹æ–‡çš„è§†è§‰ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19276v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯­ä¹‰åˆ†å‰²åœ¨åƒç´ çº§å›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ•æ‰å¯¹è±¡é—´ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å…³ç³»çš„å±€é™æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹Context-Awareè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æœ€æ–°è§†è§‰æŠ€æœ¯ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Swin Transformerè¿›è¡Œç¨³å¥çš„è§†è§‰ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡GPT-4ä¸°å¯Œè¯­ä¹‰ç†è§£ã€‚å¼•å…¥çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°ç†è§£ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ¥æ¨¡æ‹Ÿåœºæ™¯ä¸­çš„å¯¹è±¡å…³ç³»ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†åƒç´ çº§ç²¾åº¦å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒå’Œæœºå™¨äººç­‰åº”ç”¨é¢†åŸŸæä¾›æ›´ä¸ºæ™ºèƒ½çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨åƒç´ çº§å›¾åƒç†è§£ä¸Šæœ‰æ‰€çªç ´ï¼Œä½†å­˜åœ¨æ•æ‰ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰å…³ç³»çš„å±€é™æ€§ã€‚</li>
<li>å½“å‰æ¨¡å‹å¦‚CNNå’ŒTransformeråœ¨åŒºåˆ†è¯­ä¹‰ç›¸ä¼¼å¯¹è±¡å’Œç†è§£å¤æ‚ä¸Šä¸‹æ–‡åœºæ™¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†Context-Awareè¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæœ€æ–°è§†è§‰æŠ€æœ¯æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨Swin Transformerè¿›è¡Œè§†è§‰ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡GPT-4ä¸°å¯Œè¯­ä¹‰ç†è§£ã€‚</li>
<li>å¼•å…¥è·¨æ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ¨¡æ‹Ÿåœºæ™¯ä¸­çš„å¯¹è±¡å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca67ec62d87dbb60fba23f869c9f80e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c9b1fc289e47bb4e70d03488e8ee57.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Building-Blocks-for-Robust-and-Effective-Semi-Supervised-Real-World-Object-Detection"><a href="#Building-Blocks-for-Robust-and-Effective-Semi-Supervised-Real-World-Object-Detection" class="headerlink" title="Building Blocks for Robust and Effective Semi-Supervised Real-World   Object Detection"></a>Building Blocks for Robust and Effective Semi-Supervised Real-World   Object Detection</h2><p><strong>Authors:Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, Sahin Albayrak</strong></p>
<p>Semi-supervised object detection (SSOD) based on pseudo-labeling significantly reduces dependence on large labeled datasets by effectively leveraging both labeled and unlabeled data. However, real-world applications of SSOD often face critical challenges, including class imbalance, label noise, and labeling errors. We present an in-depth analysis of SSOD under real-world conditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs between label quality and quantity. Based on our findings, we propose four building blocks that can be seamlessly integrated into an SSOD framework. Rare Class Collage (RCC): a data augmentation method that enhances the representation of rare classes by creating collages of rare objects. Rare Class Focus (RCF): a stratified batch sampling strategy that ensures a more balanced representation of all classes during training. Ground Truth Label Correction (GLC): a label refinement method that identifies and corrects false, missing, and noisy ground truth labels by leveraging the consistency of teacher model predictions. Pseudo-Label Selection (PLS): a selection method for removing low-quality pseudo-labeled images, guided by a novel metric estimating the missing detection rate while accounting for class rarity. We validate our methods through comprehensive experiments on autonomous driving datasets, resulting in up to 6% increase in SSOD performance. Overall, our investigation and novel, data-centric, and broadly applicable building blocks enable robust and effective SSOD in complex, real-world scenarios. Code is available at <a target="_blank" rel="noopener" href="https://mos-ks.github.io/publications">https://mos-ks.github.io/publications</a>. </p>
<blockquote>
<p>åŸºäºä¼ªæ ‡ç­¾çš„åŠç›‘ç£ç›®æ ‡æ£€æµ‹ï¼ˆSSODï¼‰é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹å¤§æ ‡ç­¾æ•°æ®é›†çš„ä¾èµ–ã€‚ç„¶è€Œï¼ŒSSODåœ¨ç°å®ä¸–ç•Œåº”ç”¨æ–¹é¢å¸¸å¸¸é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç±»åˆ«ä¸å¹³è¡¡ã€æ ‡ç­¾å™ªå£°å’Œæ ‡æ³¨é”™è¯¯ã€‚æˆ‘ä»¬å¯¹SSODåœ¨ç°å®ä¸–ç•Œæ¡ä»¶ä¸‹çš„è¡¨ç°è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†æ¬¡ä¼˜ä¼ªæ ‡ç­¾çš„åŸå› ä»¥åŠæ ‡ç­¾è´¨é‡ä¸æ•°é‡ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†å››ä¸ªå¯ä»¥æ— ç¼é›†æˆåˆ°SSODæ¡†æ¶ä¸­çš„æ„å»ºå—ã€‚ç½•è§ç±»åˆ«æ‹¼è´´ï¼ˆRCCï¼‰ï¼šä¸€ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡åˆ›å»ºç½•è§å¯¹è±¡çš„æ‹¼è´´å›¾å¢å¼ºç½•è§ç±»åˆ«çš„è¡¨ç¤ºã€‚ç½•è§ç±»åˆ«ç„¦ç‚¹ï¼ˆRCFï¼‰ï¼šåˆ†å±‚æ‰¹é‡é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ‰€æœ‰ç±»åˆ«åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¹³è¡¡è¡¨ç¤ºã€‚çœŸå®æ ‡ç­¾æ ¡æ­£ï¼ˆGLCï¼‰ï¼šä¸€ç§æ ‡ç­¾ç»†åŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹é¢„æµ‹çš„ä¸€è‡´æ€§æ¥è¯†åˆ«å¹¶çº æ­£é”™è¯¯ã€ç¼ºå¤±å’Œå˜ˆæ‚çš„çœŸå®æ ‡ç­¾ã€‚ä¼ªæ ‡ç­¾é€‰æ‹©ï¼ˆPLSï¼‰ï¼šä¸€ç§å»é™¤ä½è´¨é‡ä¼ªæ ‡ç­¾å›¾åƒçš„é€‰æ‹©æ–¹æ³•ï¼Œç”±ä¸€ç§æ–°å‹æŒ‡æ ‡å¼•å¯¼ï¼Œè¯¥æŒ‡æ ‡åœ¨è€ƒè™‘åˆ°ç±»åˆ«ç¨€ç¼ºæ€§çš„åŒæ—¶ä¼°è®¡ç¼ºå¤±æ£€æµ‹ç‡ã€‚æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœä½¿SSODæ€§èƒ½æé«˜äº†é«˜è¾¾6%ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥ä»¥åŠæ–°é¢–ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒä¸”å¹¿æ³›é€‚ç”¨çš„æ„å»ºå—ï¼Œä½¿SSODåœ¨å¤æ‚çš„ç°å®åœºæ™¯ä¸­å®ç°äº†ç¨³å¥å’Œæœ‰æ•ˆçš„è¡¨ç°ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://mos-ks.github.io/publications%E3%80%82">https://mos-ks.github.io/publicationsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18903v1">PDF</a> Accepted to Transactions on Machine Learning Research (TMLR).   OpenReview: <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=vRYt8QLKqK">https://openreview.net/forum?id=vRYt8QLKqK</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¼ªæ ‡ç­¾çš„åŠç›‘ç£ç›®æ ‡æ£€æµ‹ï¼ˆSSODï¼‰èƒ½å¤Ÿåˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®ï¼Œå‡å°‘å¯¹å¤§é‡æœ‰æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼ŒSSODé¢ä¸´ç±»ä¸å¹³è¡¡ã€æ ‡ç­¾å™ªå£°å’Œæ ‡ç­¾é”™è¯¯ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æ·±å…¥åˆ†æäº†SSODåœ¨ç°å®æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œæ¢è®¨äº†ä¼ªæ ‡ç­¾è´¨é‡ä¸ä½³çš„åŸå› ä»¥åŠæ ‡ç­¾è´¨é‡ä¸æ•°é‡ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å››é¡¹æŠ€æœ¯ç»„åˆå—ä»¥å¢å¼ºSSODçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç”¨äºæ•°æ®å¢å¼ºçš„ç½•è§ç±»ç»„åˆï¼ˆRCCï¼‰ã€åˆ†å±‚æ‰¹å¤„ç†é‡‡æ ·ç­–ç•¥çš„ç½•è§ç±»å…³æ³¨ï¼ˆRCFï¼‰ã€é€šè¿‡åˆ©ç”¨æ•™å¸ˆæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§è¿›è¡Œæ ‡ç­¾ä¿®æ­£çš„åœ°é¢çœŸå®æ ‡ç­¾ä¿®æ­£ï¼ˆGLCï¼‰ä»¥åŠå»é™¤ä½è´¨é‡ä¼ªæ ‡ç­¾å›¾åƒçš„ä¼ªæ ‡ç­¾é€‰æ‹©ï¼ˆPLSï¼‰ã€‚é€šè¿‡è‡ªä¸»é©¾é©¶æ•°æ®é›†çš„ç»¼åˆå®éªŒéªŒè¯ï¼Œè¿™äº›æŠ€æœ¯æå‡SSODæ€§èƒ½è‡³é«˜è¾¾6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠç›‘ç£ç›®æ ‡æ£€æµ‹ï¼ˆSSODï¼‰åˆ©ç”¨ä¼ªæ ‡ç­¾å‡å°‘äº†å¯¹å¤§é‡æœ‰æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>SSODåœ¨ç°å®åº”ç”¨ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ç±»ä¸å¹³è¡¡ã€æ ‡ç­¾å™ªå£°å’Œé”™è¯¯ã€‚</li>
<li>åˆ†æäº†ä¼ªæ ‡ç­¾è´¨é‡ä¸ä½³çš„åŸå› åŠæ ‡ç­¾è´¨é‡ä¸æ•°é‡é—´çš„æƒè¡¡ã€‚</li>
<li>æå‡ºäº†å››é¡¹æŠ€æœ¯ç»„åˆå—ï¼šç½•è§ç±»ç»„åˆï¼ˆRCCï¼‰ã€ç½•è§ç±»å…³æ³¨ï¼ˆRCFï¼‰ã€åœ°é¢çœŸå®æ ‡ç­¾ä¿®æ­£ï¼ˆGLCï¼‰å’Œä¼ªæ ‡ç­¾é€‰æ‹©ï¼ˆPLSï¼‰ã€‚</li>
<li>è¿™äº›æŠ€æœ¯åœ¨è‡ªä¸»é©¾é©¶æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒéªŒè¯ï¼Œæé«˜äº†SSODæ€§èƒ½è‡³é«˜è¾¾6%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f7ed0ee9ab34402fb6f6c4e4ef9586c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1353dbfe94293676bf3322812d4e024d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Integration-of-Key-Value-Attention-Into-Pure-and-Hybrid-Transformers-for-Semantic-Segmentation"><a href="#Exploring-the-Integration-of-Key-Value-Attention-Into-Pure-and-Hybrid-Transformers-for-Semantic-Segmentation" class="headerlink" title="Exploring the Integration of Key-Value Attention Into Pure and Hybrid   Transformers for Semantic Segmentation"></a>Exploring the Integration of Key-Value Attention Into Pure and Hybrid   Transformers for Semantic Segmentation</h2><p><strong>Authors:DeShin Hwa, Tobias Holmes, Klaus Drechsler</strong></p>
<p>While CNNs were long considered state of the art for image processing, the introduction of Transformer architectures has challenged this position. While achieving excellent results in image classification and segmentation, Transformers remain inherently reliant on large training datasets and remain computationally expensive. A newly introduced Transformer derivative named KV Transformer shows promising results in synthetic, NLP, and image classification tasks, while reducing complexity and memory usage. This is especially conducive to use cases where local inference is required, such as medical screening applications. We endeavoured to further evaluate the merit of KV Transformers on semantic segmentation tasks, specifically in the domain of medical imaging. By directly comparing traditional and KV variants of the same base architectures, we provide further insight into the practical tradeoffs of reduced model complexity. We observe a notable reduction in parameter count and multiply accumulate operations, while achieving similar performance from most of the KV variant models when directly compared to their QKV implementation. </p>
<blockquote>
<p>è™½ç„¶CNNé•¿æœŸä»¥æ¥è¢«è®¤ä¸ºæ˜¯å›¾åƒå¤„ç†çš„æœ€æ–°æŠ€æœ¯ï¼Œä½†Transformeræ¶æ„çš„å¼•å…¥å¯¹å…¶åœ°ä½æå‡ºäº†æŒ‘æˆ˜ã€‚Transformerè™½ç„¶åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œä½†å…¶æœ¬è´¨ä¸Šä»ä¾èµ–äºå¤§é‡è®­ç»ƒæ•°æ®é›†ï¼Œä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ–°å¼•å…¥çš„ä¸€ç§åä¸ºKV Transformerçš„Transformerè¡ç”Ÿç‰©åœ¨åˆæˆã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœï¼ŒåŒæ—¶é™ä½äº†å¤æ‚æ€§å’Œå†…å­˜ä½¿ç”¨ã€‚è¿™å¯¹äºéœ€è¦æœ¬åœ°æ¨ç†çš„ä½¿ç”¨æƒ…å†µå°¤å…¶æœ‰åˆ©ï¼Œä¾‹å¦‚åŒ»ç–—ç­›æŸ¥åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬åŠªåŠ›è¿›ä¸€æ­¥è¯„ä¼°KV Transformeråœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„ä¼˜ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚é€šè¿‡ç›´æ¥æ¯”è¾ƒåŒä¸€åŸºç¡€æ¶æ„çš„ä¼ ç»Ÿå’ŒKVå˜ä½“ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥äº†è§£äº†é™ä½æ¨¡å‹å¤æ‚æ€§çš„å®é™…æƒè¡¡ã€‚æˆ‘ä»¬æ³¨æ„åˆ°å‚æ•°æ•°é‡å’Œä¹˜ç§¯ç´¯æ“ä½œæ˜æ˜¾å‡å°‘ï¼Œè€Œåœ¨ä¸QKVå®ç°ç›´æ¥æ¯”è¾ƒæ—¶ï¼Œå¤§å¤šæ•°KVå˜ä½“æ¨¡å‹çš„æ€§èƒ½ç›¸ä¼¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18862v1">PDF</a> 6 pages, 3 figures, Preprint. Final version published in:   Bildverarbeitung f&quot;ur die Medizin 2025, Springer. DOI:   <a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-658-47422-5_71">https://doi.org/10.1007/978-3-658-47422-5_71</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å›¾åƒå¤„ç†é¢†åŸŸï¼ŒCNNé•¿æœŸè¢«è®¤ä¸ºæ˜¯å°–ç«¯æŠ€æœ¯ï¼Œä½†Transformeræ¶æ„çš„å¼•å…¥å¯¹å…¶åœ°ä½æå‡ºäº†æŒ‘æˆ˜ã€‚è™½ç„¶Transformeråœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œä½†å®ƒä»¬æœ¬è´¨ä¸Šä¾èµ–äºå¤§å‹è®­ç»ƒæ•°æ®é›†ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ–°æ¨å‡ºçš„KV Transformeråœ¨åˆæˆã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼ŒåŒæ—¶é™ä½äº†å¤æ‚æ€§å’Œå†…å­˜ä½¿ç”¨ã€‚å…¶åœ¨åŒ»ç–—ç­›æŸ¥åº”ç”¨ç­‰éœ€è¦æœ¬åœ°æ¨ç†çš„åœºæ™¯ä¸­å°¤ä¸ºé€‚ç”¨ã€‚æˆ‘ä»¬å¯¹KV Transformeråœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿è¿›è¡Œäº†è¿›ä¸€æ­¥è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒé¢†åŸŸã€‚é€šè¿‡ç›´æ¥æ¯”è¾ƒåŸºäºç›¸åŒæ¶æ„çš„ä¼ ç»Ÿå’ŒKVç‰ˆæœ¬ï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†é™ä½æ¨¡å‹å¤æ‚æ€§çš„å®é™…æƒè¡¡ã€‚è§‚å¯Ÿåˆ°å‚æ•°è®¡æ•°å’Œä¹˜æ³•ç´¯åŠ æ“ä½œæ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶å¤§å¤šæ•°KVå˜ä½“æ¨¡å‹åœ¨ä¸QKVå®ç°ç›´æ¥æ¯”è¾ƒæ—¶è¡¨ç°å‡ºç›¸ä¼¼çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNNsé•¿æœŸè¢«è®¤ä¸ºæ˜¯å›¾åƒå¤„ç†é¢†åŸŸçš„å°–ç«¯æŠ€æœ¯ï¼Œä½†Transformeræ¶æ„çš„å¼•å…¥å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>Transformersåœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¾èµ–å¤§å‹è®­ç»ƒæ•°æ®é›†ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>KV Transformeræ˜¯Transformerçš„è¡ç”Ÿï¼Œåœ¨åˆæˆã€NLPå’Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶é™ä½äº†å¤æ‚æ€§å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>KV Transformerç‰¹åˆ«é€‚ç”¨äºåŒ»ç–—ç­›æŸ¥åº”ç”¨ç­‰éœ€è¦æœ¬åœ°æ¨ç†çš„åœºæ™¯ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿå’ŒKV Transformeråœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°KV Transformeråœ¨é™ä½å‚æ•°è®¡æ•°å’Œä¹˜æ³•ç´¯åŠ æ“ä½œçš„åŒæ—¶ï¼Œæ€§èƒ½ç›¸ä¼¼ã€‚</li>
<li>KV Transformeråœ¨åŒ»ç–—æˆåƒç­‰é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c941cc1daea9f52daadea090bd5a644a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2af8e95a68c259591969cf872e12f5f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb11b17734ed1eaddc077b51664c3dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0424aa87a513860e074b172b0d861628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4408803c1a0f8069e2726cdb0b17e577.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LGI-DETR-Local-Global-Interaction-for-UAV-Object-Detection"><a href="#LGI-DETR-Local-Global-Interaction-for-UAV-Object-Detection" class="headerlink" title="LGI-DETR: Local-Global Interaction for UAV Object Detection"></a>LGI-DETR: Local-Global Interaction for UAV Object Detection</h2><p><strong>Authors:Zifa Chen</strong></p>
<p>UAV has been widely used in various fields. However, most of the existing object detectors used in drones are not end-to-end and require the design of various complex components and careful fine-tuning. Most of the existing end-to-end object detectors are designed for natural scenes. It is not ideal to apply them directly to UAV images. In order to solve the above challenges, we design an local-global information interaction DETR for UAVs, namely LGI-DETR. Cross-layer bidirectional low-level and high-level feature information enhancement, this fusion method is effective especially in the field of small objection detection. At the initial stage of encoder, we propose a local spatial enhancement module (LSE), which enhances the low-level rich local spatial information into the high-level feature, and reduces the loss of local information in the transmission process of high-level information. At the final stage of the encoder, we propose a novel global information injection module (GII) designed to integrate rich high-level global semantic representations with low-level feature maps. This hierarchical fusion mechanism effectively addresses the inherent limitations of local receptive fields by propagating contextual information across the feature hierarchy. Experimental results on two challenging UAV image object detection benchmarks, VisDrone2019 and UAVDT, show that our proposed model outperforms the SOTA model. Compared to the baseline model, AP and AP50 improved by 1.9% and 2.4%, respectively. </p>
<blockquote>
<p>æ— äººæœºå·²å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç”¨äºæ— äººæœºçš„ç›®æ ‡æ£€æµ‹å™¨å¹¶éç«¯åˆ°ç«¯ï¼Œéœ€è¦è®¾è®¡å„ç§å¤æ‚ç»„ä»¶å¹¶è°¨æ…å¾®è°ƒã€‚å¤§å¤šæ•°ç°æœ‰çš„ç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹å™¨æ˜¯é’ˆå¯¹è‡ªç„¶åœºæ™¯è®¾è®¡çš„ï¼Œç›´æ¥åº”ç”¨äºæ— äººæœºå›¾åƒå¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºæ— äººæœºçš„æœ¬åœ°å…¨å±€ä¿¡æ¯äº¤äº’DETRï¼Œå³LGI-DETRã€‚æˆ‘ä»¬é‡‡ç”¨è·¨å±‚åŒå‘çš„åº•å±‚å’Œé«˜å±‚ç‰¹å¾ä¿¡æ¯å¢å¼ºæ–¹æ³•ï¼Œè¿™ç§èåˆæ–¹æ³•åœ¨å°ç›®æ ‡æ£€æµ‹é¢†åŸŸå°¤ä¸ºæœ‰æ•ˆã€‚åœ¨ç¼–ç å™¨åˆå§‹é˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†å±€éƒ¨ç©ºé—´å¢å¼ºæ¨¡å—ï¼ˆLSEï¼‰ï¼Œè¯¥æ¨¡å—å°†ä¸°å¯Œçš„å±€éƒ¨ç©ºé—´ä¿¡æ¯ä»åº•å±‚å¢å¼ºåˆ°é«˜å±‚ç‰¹å¾ï¼Œå¹¶å‡å°‘é«˜å±‚ä¿¡æ¯ä¼ è¾“è¿‡ç¨‹ä¸­å±€éƒ¨ä¿¡æ¯çš„æŸå¤±ã€‚åœ¨ç¼–ç å™¨æœ€åé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å…¨å±€ä¿¡æ¯æ³¨å…¥æ¨¡å—ï¼ˆGIIï¼‰ï¼Œæ—¨åœ¨å°†ä¸°å¯Œçš„é«˜å±‚å…¨å±€è¯­ä¹‰è¡¨ç¤ºä¸åº•å±‚ç‰¹å¾å›¾ç›¸ç»“åˆã€‚è¿™ç§åˆ†å±‚èåˆæœºåˆ¶é€šè¿‡åœ¨æ•´ä¸ªç‰¹å¾å±‚æ¬¡ç»“æ„ä¸­ä¼ æ’­ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å±€éƒ¨æ„Ÿå—é‡çš„å†…åœ¨å±€é™æ€§ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•VisDrone2019å’ŒUAVDTä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒAPå’ŒAP50åˆ†åˆ«æé«˜äº†1.9%å’Œ2.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18785v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong><br>æ— äººæœºåœ¨å„ä¸ªé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†ç°æœ‰æ— äººæœºç›®æ ‡æ£€æµ‹å™¨å­˜åœ¨ä¸è¿è´¯çš„é—®é¢˜ï¼Œéœ€è¦è®¾è®¡å¤šç§å¤æ‚ç»„ä»¶å¹¶ä»”ç»†å¾®è°ƒã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹å™¨å¤§å¤šé’ˆå¯¹è‡ªç„¶åœºæ™¯è®¾è®¡ï¼Œç›´æ¥åº”ç”¨äºæ— äººæœºå›¾åƒå¹¶ä¸ç†æƒ³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç”¨äºæ— äººæœºçš„æœ¬åœ°å…¨å±€ä¿¡æ¯äº¤äº’DETRï¼Œå³LGI-DETRã€‚è¯¥æ¨¡å‹é‡‡ç”¨è·¨å±‚åŒå‘çš„åº•å±‚å’Œé«˜å±‚ç‰¹å¾ä¿¡æ¯å¢å¼ºèåˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå°ç›®æ ‡æ£€æµ‹é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†å±€éƒ¨ç©ºé—´å¢å¼ºæ¨¡å—ï¼ˆLSEï¼‰å’Œå…¨å±€ä¿¡æ¯æ³¨å…¥æ¨¡å—ï¼ˆGIIï¼‰ï¼Œåˆ†åˆ«å¢å¼ºå±€éƒ¨ç©ºé—´ä¿¡æ¯å’Œå…¨å±€è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å±‚æ¬¡èåˆæœºåˆ¶è§£å†³å±€éƒ¨æ„Ÿå—é‡çš„å›ºæœ‰å±€é™æ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚ç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼Œå…¶åœ¨APå’ŒAP50æ–¹é¢çš„æå‡åˆ†åˆ«ä¸º1.9%å’Œ2.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ— äººæœºåœ¨å¤šä¸ªé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†ç›®æ ‡æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ— äººæœºç›®æ ‡æ£€æµ‹å™¨å­˜åœ¨å¤æ‚æ€§åŠä¸è¿è´¯çš„é—®é¢˜ã€‚</li>
<li>LGI-DETRæ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€‚ç”¨äºæ— äººæœºå›¾åƒçš„ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>LGI-DETRé‡‡ç”¨è·¨å±‚åŒå‘ç‰¹å¾èåˆæ–¹æ³•ï¼Œå¢å¼ºå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬å±€éƒ¨ç©ºé—´å¢å¼ºæ¨¡å—ï¼ˆLSEï¼‰å’Œå…¨å±€ä¿¡æ¯æ³¨å…¥æ¨¡å—ï¼ˆGIIï¼‰ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºLGI-DETRåœ¨æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12c994801b8ad9bda7f436f9b8f33363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49f4b00efc7fa2eb33df6061f8f1aad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f91c07263f991af094b37b74eb7806b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aae1671cc59add804473f55a7a97cf71.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection"><a href="#CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection" class="headerlink" title="CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection"></a>CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection</h2><p><strong>Authors:Zhichao Sun, Huazhang Hu, Yidong Ma, Gang Liu, Nemo Chen, Xu Tang, Yao Hu, Yongchao Xu</strong></p>
<p>With the exponential growth of data, traditional object detection methods are increasingly struggling to handle vast vocabulary object detection tasks effectively. We analyze two key limitations of classification-based detectors: positive gradient dilution, where rare positive categories receive insufficient learning signals, and hard negative gradient dilution, where discriminative gradients are overwhelmed by numerous easy negatives. To address these challenges, we propose CQ-DINO, a category query-based object detection framework that reformulates classification as a contrastive task between object queries and learnable category queries. Our method introduces image-guided query selection, which reduces the negative space by adaptively retrieving top-K relevant categories per image via cross-attention, thereby rebalancing gradient distributions and facilitating implicit hard example mining. Furthermore, CQ-DINO flexibly integrates explicit hierarchical category relationships in structured datasets (e.g., V3Det) or learns implicit category correlations via self-attention in generic datasets (e.g., COCO). Experiments demonstrate that CQ-DINO achieves superior performance on the challenging V3Det benchmark (surpassing previous methods by 2.1% AP) while maintaining competitiveness in COCO. Our work provides a scalable solution for real-world detection systems requiring wide category coverage. The dataset and code will be publicly at <a target="_blank" rel="noopener" href="https://github.com/RedAIGC/CQ-DINO">https://github.com/RedAIGC/CQ-DINO</a>. </p>
<blockquote>
<p>éšç€æ•°æ®çš„æŒ‡æ•°çº§å¢é•¿ï¼Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ–¹æ³•è¶Šæ¥è¶Šéš¾ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤§è§„æ¨¡è¯æ±‡è¡¨çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šæ­£å‘æ¢¯åº¦ç¨€é‡Šï¼Œå…¶ä¸­ç½•è§çš„æ­£ç±»åˆ«æ¥æ”¶åˆ°çš„å­¦ä¹ ä¿¡å·ä¸è¶³ï¼›ä»¥åŠéš¾ä»¥åº”å¯¹çš„è´Ÿæ¢¯åº¦ç¨€é‡Šï¼Œå…¶ä¸­åŒºåˆ†æ€§çš„æ¢¯åº¦è¢«å¤§é‡æ˜“äºåŒºåˆ†çš„è´Ÿæ ·æœ¬æ‰€æ·¹æ²¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç±»åˆ«æŸ¥è¯¢çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶CQ-DINOï¼Œå®ƒå°†åˆ†ç±»é‡æ–°è¡¨è¿°ä¸ºå¯¹è±¡æŸ¥è¯¢å’Œå¯å­¦ä¹ ç±»åˆ«æŸ¥è¯¢ä¹‹é—´çš„å¯¹æ¯”ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è‡ªé€‚åº”åœ°æ£€ç´¢æ¯å¼ å›¾åƒçš„å‰Kä¸ªç›¸å…³ç±»åˆ«ï¼Œä»è€Œå‡å°‘è´Ÿç©ºé—´ï¼Œä»è€Œé‡æ–°å¹³è¡¡æ¢¯åº¦åˆ†å¸ƒå¹¶ä¿ƒè¿›éšå¼ç¡¬ç¤ºä¾‹æŒ–æ˜ã€‚æ­¤å¤–ï¼ŒCQ-DINOèƒ½å¤Ÿçµæ´»åœ°é›†æˆç»“æ„åŒ–æ•°æ®é›†ï¼ˆä¾‹å¦‚V3Detï¼‰ä¸­çš„æ˜¾å¼å±‚æ¬¡ç±»åˆ«å…³ç³»ï¼Œæˆ–åœ¨é€šç”¨æ•°æ®é›†ï¼ˆä¾‹å¦‚COCOï¼‰ä¸­é€šè¿‡è‡ªæ³¨æ„åŠ›å­¦ä¹ éšå¼ç±»åˆ«å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCQ-DINOåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„V3DetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ˆè¶…å‡ºä¹‹å‰çš„æ–¹æ³•2.1%çš„APï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨COCOä¸­ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºéœ€è¦å¹¿æ³›ç±»åˆ«è¦†ç›–çš„çœŸå®ä¸–ç•Œæ£€æµ‹ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/RedAIGC/CQ-DINO%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/RedAIGC/CQ-DINOå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18430v2">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€æ•°æ®æŒ‡æ•°çº§å¢é•¿ï¼Œä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨å¤§é‡è¯æ±‡ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­è¶Šæ¥è¶Šéš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚æœ¬æ–‡åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼Œå¹¶æå‡ºäº†CQ-DINOæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚CQ-DINOå°†åˆ†ç±»é‡æ–°å®šä¹‰ä¸ºå¯¹è±¡æŸ¥è¯¢ä¸å¯å­¦ä¹ ç±»åˆ«æŸ¥è¯¢ä¹‹é—´çš„å¯¹æ¯”ä»»åŠ¡ï¼Œé€šè¿‡å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©å‡å°‘è´Ÿç©ºé—´ï¼Œå¼•å…¥è·¨æ³¨æ„åŠ›æœºåˆ¶å¹³è¡¡æ¢¯åº¦åˆ†å¸ƒï¼Œå¹¶çµæ´»é›†æˆç»“æ„åŒ–æ•°æ®é›†ä¸­çš„æ˜¾å¼å±‚æ¬¡ç±»åˆ«å…³ç³»æˆ–å­¦ä¹ é€šç”¨æ•°æ®é›†ä¸­çš„éšå¼ç±»åˆ«å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCQ-DINOåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„V3Detæ•°æ®é›†ä¸Šä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ˆé«˜å‡º2.1%çš„APï¼‰ï¼ŒåŒæ—¶åœ¨COCOä¸­ä¿æŒç«äº‰åŠ›ã€‚æœ¬ç ”ç©¶ä¸ºéœ€è¦å¹¿æ³›ç±»åˆ«è¦†ç›–çš„çœŸå®ä¸–ç•Œæ£€æµ‹ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°æ®é‡çš„å¢é•¿ä½¿å¾—ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªå±€é™æ€§ï¼šæ­£å‘æ¢¯åº¦ç¨€é‡Šå’Œç¡¬è´Ÿæ¢¯åº¦ç¨€é‡Šã€‚</li>
<li>æå‡ºäº†CQ-DINOæ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å¯¹æ¯”ä»»åŠ¡é‡æ–°å®šä¹‰åˆ†ç±»ã€‚</li>
<li>CQ-DINOé€šè¿‡å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©å‡å°‘è´Ÿç©ºé—´ï¼Œå¼•å…¥è·¨æ³¨æ„åŠ›æœºåˆ¶å¹³è¡¡æ¢¯åº¦åˆ†å¸ƒã€‚</li>
<li>CQ-DINOçµæ´»é›†æˆæ˜¾å¼å±‚æ¬¡ç±»åˆ«å…³ç³»æˆ–å­¦ä¹ éšå¼ç±»åˆ«å…³è”ã€‚</li>
<li>åœ¨V3Detæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ•ˆæœï¼Œå¹¶åœ¨COCOä¸­ä¿æŒç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-628028d6851a80f1d7a2845108d4572f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2936b0016561c6eefcd0802f9564ddc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae3a29632fed747575180e9c1bce8b15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e349aea7dfd023c92809781cb9db1b5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PDDM-Pseudo-Depth-Diffusion-Model-for-RGB-PD-Semantic-Segmentation-Based-in-Complex-Indoor-Scenes"><a href="#PDDM-Pseudo-Depth-Diffusion-Model-for-RGB-PD-Semantic-Segmentation-Based-in-Complex-Indoor-Scenes" class="headerlink" title="PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation   Based in Complex Indoor Scenes"></a>PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation   Based in Complex Indoor Scenes</h2><p><strong>Authors:Xinhua Xu, Hong Liu, Jianbing Wu, Jinfu Liu</strong></p>
<p>The integration of RGB and depth modalities significantly enhances the accuracy of segmenting complex indoor scenes, with depth data from RGB-D cameras playing a crucial role in this improvement. However, collecting an RGB-D dataset is more expensive than an RGB dataset due to the need for specialized depth sensors. Aligning depth and RGB images also poses challenges due to sensor positioning and issues like missing data and noise. In contrast, Pseudo Depth (PD) from high-precision depth estimation algorithms can eliminate the dependence on RGB-D sensors and alignment processes, as well as provide effective depth information and show significant potential in semantic segmentation. Therefore, to explore the practicality of utilizing pseudo depth instead of real depth for semantic segmentation, we design an RGB-PD segmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo Depth Aggregation Module (PDAM) for fully exploiting the informative clues provided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo depth maps into a single modality, making it easily adaptable to other RGB-D segmentation methods. In addition, the pre-trained diffusion model serves as a strong feature extractor for RGB segmentation tasks, but multi-modal diffusion-based segmentation methods remain unexplored. Therefore, we present a Pseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image diffusion model as a feature extractor and a simple yet effective fusion strategy to integrate pseudo depth. To verify the applicability of pseudo depth and our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D datasets. The experimental results demonstrate that pseudo depth can effectively enhance segmentation performance, and our PDDM achieves state-of-the-art performance, outperforming other methods by +6.98 mIoU on NYUv2 and +2.11 mIoU on SUNRGB-D. </p>
<blockquote>
<p>å°†RGBå’Œæ·±åº¦æ¨¡æ€çš„èåˆæ˜¾è‘—æé«˜äº†å¯¹å¤æ‚å®¤å†…åœºæ™¯çš„åˆ†å‰²ç²¾åº¦ï¼Œå…¶ä¸­RGB-Dç›¸æœºçš„æ·±åº¦æ•°æ®åœ¨æ­¤æ”¹è¿›ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ä¸“é—¨çš„æ·±åº¦ä¼ æ„Ÿå™¨ï¼Œæ”¶é›†RGB-Dæ•°æ®é›†æ¯”æ”¶é›†RGBæ•°æ®é›†æ›´ä¸ºæ˜‚è´µã€‚æ·±åº¦å›¾åƒä¸RGBå›¾åƒçš„åŒ¹é…ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œæ¯”å¦‚ä¼ æ„Ÿå™¨å®šä½ã€ç¼ºå¤±æ•°æ®å’Œå™ªå£°ç­‰é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¥è‡ªé«˜ç²¾åº¦æ·±åº¦ä¼°è®¡ç®—æ³•çš„ä¼ªæ·±åº¦ï¼ˆPDï¼‰å¯ä»¥æ¶ˆé™¤å¯¹RGB-Dä¼ æ„Ÿå™¨å’ŒåŒ¹é…è¿‡ç¨‹çš„ä¾èµ–ï¼Œå¹¶æä¾›æœ‰æ•ˆçš„æ·±åº¦ä¿¡æ¯ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œä¸ºäº†æ¢ç´¢ä½¿ç”¨ä¼ªæ·±åº¦ä»£æ›¿çœŸå®æ·±åº¦è¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªRGB-PDåˆ†å‰²ç®¡é“ï¼Œä»¥æ•´åˆRGBå’Œä¼ªæ·±åº¦ï¼Œå¹¶æå‡ºä¸€ä¸ªä¼ªæ·±åº¦èšåˆæ¨¡å—ï¼ˆPDAMï¼‰ï¼Œä»¥å……åˆ†åˆ©ç”¨ç”±ä¸åŒä¼ªæ·±åº¦å›¾æä¾›çš„ä¿¡æ¯çº¿ç´¢ã€‚PDAMå°†å¤šä¸ªä¼ªæ·±åº¦å›¾èšåˆä¸ºå•ä¸€æ¨¡æ€ï¼Œä½¿å…¶æ˜“äºé€‚åº”å…¶ä»–RGB-Dåˆ†å‰²æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ˜¯RGBåˆ†å‰²ä»»åŠ¡çš„å¼ºå¤§ç‰¹å¾æå–å™¨ï¼Œä½†åŸºäºå¤šæ¨¡æ€æ‰©æ•£çš„åˆ†å‰²æ–¹æ³•ä»æœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼ªæ·±åº¦æ‰©æ•£æ¨¡å‹ï¼ˆPDDMï¼‰ï¼Œå®ƒé‡‡ç”¨å¤§è§„æ¨¡æ–‡æœ¬å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œä»¥åŠä¸€ç§ç®€å•æœ‰æ•ˆçš„èåˆç­–ç•¥æ¥æ•´åˆä¼ªæ·±åº¦ã€‚ä¸ºäº†éªŒè¯ä¼ªæ·±åº¦å’Œæˆ‘ä»¬çš„PDDMçš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨NYUv2å’ŒSUNRGB-Dæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ªæ·±åº¦å¯ä»¥æœ‰æ•ˆåœ°æé«˜åˆ†å‰²æ€§èƒ½ï¼Œæˆ‘ä»¬çš„PDDMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨NYUv2ä¸Šæ¯”å…¶ä»–æ–¹æ³•é«˜å‡º+6.98 mIoUï¼Œåœ¨SUNRGB-Dä¸Šé«˜å‡º+2.11 mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18393v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RGBä¸æ·±åº¦æ¨¡æ€çš„èåˆæ˜¾è‘—æé«˜äº†å¤æ‚å®¤å†…åœºæ™¯çš„åˆ†å‰²ç²¾åº¦ï¼Œæ·±åº¦æ•°æ®åœ¨æ”¹è¿›ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç›¸è¾ƒäºRGBæ•°æ®é›†ï¼ŒRGB-Dæ•°æ®é›†çš„é‡‡é›†æˆæœ¬è¾ƒé«˜ï¼Œä¸”æ·±åº¦ä¸RGBå›¾åƒçš„é…å‡†ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¼ªæ·±åº¦ï¼ˆPDï¼‰æŠ€æœ¯å¯æœ‰æ•ˆæ¶ˆé™¤å¯¹RGB-Dä¼ æ„Ÿå™¨å’Œé…å‡†è¿‡ç¨‹çš„ä¾èµ–ï¼Œå¹¶æä¾›æœ‰æ•ˆçš„æ·±åº¦ä¿¡æ¯ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºæ¢ç´¢ä½¿ç”¨ä¼ªæ·±åº¦æ›¿ä»£çœŸå®æ·±åº¦è¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„å®ç”¨æ€§ï¼Œè®¾è®¡äº†ä¸€ç§RGB-PDåˆ†å‰²ç®¡é“ï¼Œå¹¶æå‡ºä¼ªæ·±åº¦èšåˆæ¨¡å—ï¼ˆPDAMï¼‰ï¼Œä»¥å……åˆ†åˆ©ç”¨ä¼ªæ·±åº¦å›¾æä¾›çš„ä¿¡æ¯çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”¨äºRGBåˆ†å‰²ä»»åŠ¡ï¼Œå¹¶æå‡ºä¼ªæ·±åº¦æ‰©æ•£æ¨¡å‹ï¼ˆPDDMï¼‰ï¼Œé‡‡ç”¨å¤§è§„æ¨¡æ–‡æœ¬-å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œä»¥åŠç®€å•æœ‰æ•ˆçš„èåˆç­–ç•¥æ¥æ•´åˆä¼ªæ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ªæ·±åº¦å¯æœ‰æ•ˆæå‡åˆ†å‰²æ€§èƒ½ï¼ŒPDDMæ–¹æ³•å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œåœ¨NYUv2å’ŒSUNRGB-Dæ•°æ®é›†ä¸Šåˆ†åˆ«æ¯”å…¶ä»–æ–¹æ³•é«˜å‡º+6.98 mIoUå’Œ+2.11 mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGBä¸æ·±åº¦æ¨¡æ€èåˆèƒ½æé«˜å®¤å†…åœºæ™¯åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>RGB-Dæ•°æ®é›†çš„é‡‡é›†æˆæœ¬é«˜äºRGBæ•°æ®é›†ã€‚</li>
<li>ä¼ªæ·±åº¦æŠ€æœ¯å¯æ¶ˆé™¤å¯¹RGB-Dä¼ æ„Ÿå™¨å’Œé…å‡†è¿‡ç¨‹çš„ä¾èµ–ã€‚</li>
<li>ä¼ªæ·±åº¦åœ¨è¯­ä¹‰åˆ†å‰²é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºRGB-PDåˆ†å‰²ç®¡é“å’Œä¼ªæ·±åº¦èšåˆæ¨¡å—ï¼ˆPDAMï¼‰ä»¥åˆ©ç”¨ä¼ªæ·±åº¦ä¿¡æ¯ã€‚</li>
<li>é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨RGBåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-157069e0098d894b50e66d2b79ce7e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ff75c4a34d1c8025bb56ba62d517daa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acac75378d95f18d8a89f1c404e9b0cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-139e72d320ebc4945f30c0e6b888b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b831c275e965775790bcdfc140ef3e21.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MaSS13K-A-Matting-level-Semantic-Segmentation-Benchmark"><a href="#MaSS13K-A-Matting-level-Semantic-Segmentation-Benchmark" class="headerlink" title="MaSS13K: A Matting-level Semantic Segmentation Benchmark"></a>MaSS13K: A Matting-level Semantic Segmentation Benchmark</h2><p><strong>Authors:Chenxi Xie, Minghan Li, Hui Zeng, Jun Luo, Lei Zhang</strong></p>
<p>High-resolution semantic segmentation is essential for applications such as image editing, bokeh imaging, AR&#x2F;VR, etc. Unfortunately, existing datasets often have limited resolution and lack precise mask details and boundaries. In this work, we build a large-scale, matting-level semantic segmentation dataset, named MaSS13K, which consists of 13,348 real-world images, all at 4K resolution. MaSS13K provides high-quality mask annotations of a number of objects, which are categorized into seven categories: human, vegetation, ground, sky, water, building, and others. MaSS13K features precise masks, with an average mask complexity 20-50 times higher than existing semantic segmentation datasets. We consequently present a method specifically designed for high-resolution semantic segmentation, namely MaSSFormer, which employs an efficient pixel decoder that aggregates high-level semantic features and low-level texture features across three stages, aiming to produce high-resolution masks with minimal computational cost. Finally, we propose a new learning paradigm, which integrates the high-quality masks of the seven given categories with pseudo labels from new classes, enabling MaSSFormer to transfer its accurate segmentation capability to other classes of objects. Our proposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark together with 14 representative segmentation models. We expect that our meticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate the research of high-resolution and high-quality semantic segmentation. Datasets and codes can be found at <a target="_blank" rel="noopener" href="https://github.com/xiechenxi99/MaSS13K">https://github.com/xiechenxi99/MaSS13K</a>. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²å¯¹äºå›¾åƒç¼–è¾‘ã€æ•£æ™¯æˆåƒã€AR&#x2F;VRç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†å¾€å¾€åˆ†è¾¨ç‡æœ‰é™ï¼Œç¼ºä¹ç²¾ç¡®çš„é®ç½©ç»†èŠ‚å’Œè¾¹ç•Œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€ç£¨çš®çº§åˆ«çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼Œåä¸ºMaSS13Kï¼Œç”±13,348å¼ çœŸå®ä¸–ç•Œçš„4Kåˆ†è¾¨ç‡å›¾åƒç»„æˆã€‚MaSS13Kæä¾›äº†é«˜è´¨é‡çš„å¯¹è±¡é®ç½©æ³¨é‡Šï¼Œè¿™äº›å¯¹è±¡è¢«åˆ†ä¸ºä¸ƒå¤§ç±»ï¼šäººç±»ã€æ¤è¢«ã€åœ°é¢ã€å¤©ç©ºã€æ°´ã€å»ºç­‘å’Œå…¶ä»–ã€‚MaSS13Kçš„é®ç½©éå¸¸ç²¾ç¡®ï¼Œå…¶å¹³å‡é®ç½©å¤æ‚åº¦æ¯”ç°æœ‰è¯­ä¹‰åˆ†å‰²æ•°æ®é›†é«˜20-50å€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºé«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•ï¼Œå³MaSSFormerã€‚å®ƒé‡‡ç”¨é«˜æ•ˆçš„åƒç´ è§£ç å™¨ï¼Œä¸‰ä¸ªé˜¶æ®µåˆ†åˆ«èšåˆé«˜çº§è¯­ä¹‰ç‰¹å¾å’Œä½çº§çº¹ç†ç‰¹å¾ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„è®¡ç®—æˆæœ¬ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„é®ç½©ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œå®ƒå°†ç»™å®šçš„ä¸ƒç±»é«˜è´¨é‡é®ç½©ä¸æ¥è‡ªæ–°ç±»åˆ«çš„ä¼ªæ ‡ç­¾ç›¸ç»“åˆï¼Œä½¿MaSSFormerèƒ½å¤Ÿå°†å‡†ç¡®çš„åˆ†å‰²èƒ½åŠ›è½¬ç§»åˆ°å…¶ä»–ç±»çš„å¯¹è±¡ä¸Šã€‚MaSSFormeråœ¨MaSS13KåŸºå‡†æµ‹è¯•ä¸Šä¸14ç§ä»£è¡¨æ€§çš„åˆ†å‰²æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç²¾å¿ƒæ ‡æ³¨çš„MaSS13Kæ•°æ®é›†å’ŒMaSSFormeræ¨¡å‹èƒ½å¤Ÿä¿ƒè¿›é«˜åˆ†è¾¨ç‡å’Œé«˜è´¨é‡è¯­ä¹‰åˆ†å‰²çš„ç ”ç©¶ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiechenxi99/MaSS13K%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiechenxi99/MaSS13Kæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18364v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€æˆåƒçº§åˆ«è¯­ä¹‰åˆ†å‰²æ•°æ®é›†MaSS13Kï¼ŒåŒ…å«13,348å¼ çœŸå®ä¸–ç•Œçš„4Kå›¾åƒå’Œé«˜è´¨é‡çš„ä¸ƒç±»å¯¹è±¡æ©è†œæ³¨é‡Šã€‚æå‡ºäº†é’ˆå¯¹é«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•MaSSFormerï¼Œèƒ½æœ‰æ•ˆèåˆé«˜çº§è¯­ä¹‰ç‰¹å¾å’Œä½çº§çº¹ç†ç‰¹å¾ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡æ©è†œã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ–°å­¦ä¹ æ¨¡å¼ï¼Œç»“åˆé«˜è´¨é‡æ©è†œå’Œä¼ªæ ‡ç­¾ï¼Œå®ç°è·¨ç±»åˆ«å¯¹è±¡çš„å‡†ç¡®åˆ†å‰²èƒ½åŠ›ã€‚åœ¨MaSS13Kæ•°æ®é›†ä¸Šï¼Œä¸ä»£è¡¨æ¨¡å‹çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºäº†å…¶æ½œåŠ›ã€‚æ­¤ç ”ç©¶å°†æ¨åŠ¨é«˜è´¨é‡è¯­ä¹‰åˆ†å‰²çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²æ•°æ®é›†MaSS13Kï¼ŒåŒ…å«çœŸå®ä¸–ç•Œå›¾åƒå¹¶å…·æœ‰ç²¾ç»†çš„æ©è†œæ³¨é‡Šã€‚</li>
<li>MaSS13Kæä¾›ä¸ƒç±»å¯¹è±¡çš„æ©è†œæ³¨é‡Šï¼ŒåŒ…æ‹¬äººç±»ã€æ¤è¢«ã€åœ°é¢ã€å¤©ç©ºã€æ°´åŸŸã€å»ºç­‘ç‰©å’Œå…¶ä»–ã€‚</li>
<li>MaSSFormræ˜¯ä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•ï¼Œå…¶è®¾è®¡çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„åƒç´ è§£ç å™¨ï¼Œå¯åœ¨ä¸‰ä¸ªé˜¶æ®µèåˆé«˜çº§å’Œä½çº§ç‰¹å¾ã€‚</li>
<li>MaSSFormrä½¿ç”¨äº†ä¸€ç§æ–°å­¦ä¹ æ¨¡å¼ï¼Œç»“åˆäº†é«˜è´¨é‡æ©è†œä¸ä¼ªæ ‡ç­¾ï¼Œæå‡äº†è·¨ç±»åˆ«å¯¹è±¡çš„åˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>MaSSFormråœ¨MaSS13Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ç»è¿‡ç»¼åˆè¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</li>
<li>MaSS13Kæ•°æ®é›†å’ŒMaSSFormræ¨¡å‹æœ‰åŠ©äºæ¨åŠ¨é«˜è´¨é‡è¯­ä¹‰åˆ†å‰²çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6a92c5752523af1d4d3bf860b7717e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c262b9525d3e0fc982a381cef083bd6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b50db114e97b3c73e63c072bb7425c2c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="An-Image-like-Diffusion-Method-for-Human-Object-Interaction-Detection"><a href="#An-Image-like-Diffusion-Method-for-Human-Object-Interaction-Detection" class="headerlink" title="An Image-like Diffusion Method for Human-Object Interaction Detection"></a>An Image-like Diffusion Method for Human-Object Interaction Detection</h2><p><strong>Authors:Xiaofei Hui, Haoxuan Qu, Hossein Rahmani, Jun Liu</strong></p>
<p>Human-object interaction (HOI) detection often faces high levels of ambiguity and indeterminacy, as the same interaction can appear vastly different across different human-object pairs. Additionally, the indeterminacy can be further exacerbated by issues such as occlusions and cluttered backgrounds. To handle such a challenging task, in this work, we begin with a key observation: the output of HOI detection for each human-object pair can be recast as an image. Thus, inspired by the strong image generation capabilities of image diffusion models, we propose a new framework, HOI-IDiff. In HOI-IDiff, we tackle HOI detection from a novel perspective, using an Image-like Diffusion process to generate HOI detection outputs as images. Furthermore, recognizing that our recast images differ in certain properties from natural images, we enhance our framework with a customized HOI diffusion process and a slice patchification model architecture, which are specifically tailored to generate our recast &#96;&#96;HOI imagesâ€™â€™. Extensive experiments demonstrate the efficacy of our framework. </p>
<blockquote>
<p>äººç±»ä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹ç»å¸¸é¢ä¸´é«˜å±‚æ¬¡çš„æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§ï¼Œå› ä¸ºåŒä¸€äº¤äº’åœ¨ä¸åŒçš„äººä¸ç‰©ä½“ç»„åˆä¹‹é—´å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œç”±äºé®æŒ¡å’ŒèƒŒæ™¯æ‚ä¹±ç­‰é—®é¢˜ï¼Œä¸ç¡®å®šæ€§å¯èƒ½ä¼šè¿›ä¸€æ­¥åŠ å‰§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹å…³é”®è§‚å¯Ÿï¼šå¯¹äºæ¯ä¸ªäººä¸ç‰©ä½“çš„ç»„åˆï¼ŒHOIæ£€æµ‹çš„è¾“å‡ºå¯ä»¥è¢«é‡å¡‘ä¸ºä¸€å¼ å›¾åƒã€‚å› æ­¤ï¼Œå—åˆ°æ‰©æ•£æ¨¡å‹å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶HOI-IDiffã€‚åœ¨HOI-IDiffä¸­ï¼Œæˆ‘ä»¬ä»å…¨æ–°çš„è§’åº¦è§£å†³HOIæ£€æµ‹é—®é¢˜ï¼Œä½¿ç”¨ç±»ä¼¼å›¾åƒçš„æ‰©æ•£è¿‡ç¨‹æ¥ç”ŸæˆHOIæ£€æµ‹è¾“å‡ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°æˆ‘ä»¬çš„é‡å¡‘å›¾åƒåœ¨æŸäº›å±æ€§ä¸Šä¸è‡ªç„¶å›¾åƒæœ‰æ‰€ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨å®šåˆ¶çš„HOIæ‰©æ•£è¿‡ç¨‹å’Œåˆ‡ç‰‡æ¨¡å‹æ¶æ„æ¥å¢å¼ºæˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¿™äº›ç‰¹å®šè®¾è®¡éƒ½æ˜¯ä¸ºäº†ç”Ÿæˆæˆ‘ä»¬çš„é‡å¡‘â€œHOIå›¾åƒâ€ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18134v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>ï¼šäººç±»ä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹é¢ä¸´é«˜æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§é—®é¢˜ï¼Œç›¸åŒäº¤äº’åœ¨ä¸åŒçš„äººä¸ç‰©ä½“é—´è¡¨ç°å·®å¼‚å·¨å¤§ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºå…³é”®è§‚å¯Ÿï¼šHOIæ£€æµ‹ç»“æœå¯è½¬åŒ–ä¸ºå›¾åƒã€‚å› æ­¤ï¼Œå—å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºæ–°çš„æ¡†æ¶HOI-IDiffï¼Œä»¥å›¾åƒæ‰©æ•£è¿‡ç¨‹è¿›è¡ŒHOIæ£€æµ‹è¾“å‡ºã€‚é’ˆå¯¹ç”Ÿæˆçš„å›¾åƒç‰¹æ€§å·®å¼‚ï¼Œæ¡†æ¶åŠ å…¥äº†å®šåˆ¶åŒ–çš„HOIæ‰©æ•£è¿‡ç¨‹å’Œåˆ‡ç‰‡è´´ç‰‡æ¨¡å‹æ¶æ„ï¼Œå¹¶è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹å…·æœ‰é«˜çš„æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>ä¸åŒçš„äººä¸ç‰©ä½“ä¹‹é—´åŒä¸€äº¤äº’å¯èƒ½è¡¨ç°å‡ºæå¤§å·®å¼‚ã€‚</li>
<li>æå‡ºæ–°çš„è§†è§’åº”å¯¹HOIæ£€æµ‹æŒ‘æˆ˜ï¼šå°†HOIæ£€æµ‹ç»“æœè½¬åŒ–ä¸ºå›¾åƒå½¢å¼è¿›è¡Œå¤„ç†ã€‚</li>
<li>åˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ„å»ºHOI-IDiffæ¡†æ¶è¿›è¡ŒHOIæ£€æµ‹ã€‚</li>
<li>é’ˆå¯¹ç”Ÿæˆçš„å›¾åƒç‰¹æ€§å·®å¼‚ï¼Œå®šåˆ¶åŒ–çš„HOIæ‰©æ•£è¿‡ç¨‹å’Œåˆ‡ç‰‡è´´ç‰‡æ¨¡å‹æ¶æ„è¢«å¼•å…¥ä»¥å¢å¼ºæ¡†æ¶æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6938c3a42d4008c2e4563b58ee6b7f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f581a6ddd5b1e48319dfcee8be8474ca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Semantic-Segmentation-with-Multi-Constraint-Consistency-Learning"><a href="#Semi-supervised-Semantic-Segmentation-with-Multi-Constraint-Consistency-Learning" class="headerlink" title="Semi-supervised Semantic Segmentation with Multi-Constraint Consistency   Learning"></a>Semi-supervised Semantic Segmentation with Multi-Constraint Consistency   Learning</h2><p><strong>Authors:Jianjian Yin, Tao Chen, Gensheng Pei, Yazhou Yao, Liqiang Nie, Xiansheng Hua</strong></p>
<p>Consistency regularization has prevailed in semi-supervised semantic segmentation and achieved promising performance. However, existing methods typically concentrate on enhancing the Image-augmentation based Prediction consistency and optimizing the segmentation network as a whole, resulting in insufficient utilization of potential supervisory information. In this paper, we propose a Multi-Constraint Consistency Learning (MCCL) approach to facilitate the staged enhancement of the encoder and decoder. Specifically, we first design a feature knowledge alignment (FKA) strategy to promote the feature consistency learning of the encoder from image-augmentation. Our FKA encourages the encoder to derive consistent features for strongly and weakly augmented views from the perspectives of point-to-point alignment and prototype-based intra-class compactness. Moreover, we propose a self-adaptive intervention (SAI) module to increase the discrepancy of aligned intermediate feature representations, promoting Feature-perturbation based Prediction consistency learning. Self-adaptive feature masking and noise injection are designed in an instance-specific manner to perturb the features for robust learning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes datasets demonstrate that our proposed MCCL achieves new state-of-the-art performance. The source code and models are made available at <a target="_blank" rel="noopener" href="https://github.com/NUST-Machine-Intelligence-Laboratory/MCCL">https://github.com/NUST-Machine-Intelligence-Laboratory/MCCL</a>. </p>
<blockquote>
<p>ä¸€è‡´æ€§æ­£åˆ™åŒ–åœ¨åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸­éå¸¸æµè¡Œï¼Œå¹¶å®ç°äº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå¢å¼ºåŸºäºå›¾åƒå¢å¼ºçš„é¢„æµ‹ä¸€è‡´æ€§å¹¶ä¼˜åŒ–æ•´ä¸ªåˆ†å‰²ç½‘ç»œï¼Œå¯¼è‡´æ½œåœ¨ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šçº¦æŸä¸€è‡´æ€§å­¦ä¹ ï¼ˆMCCLï¼‰æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›ç¼–ç å™¨å’Œè§£ç å™¨çš„åˆ†é˜¶æ®µå¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§ç‰¹å¾çŸ¥è¯†å¯¹é½ï¼ˆFKAï¼‰ç­–ç•¥ï¼Œä»¥ä¿ƒè¿›ä»å›¾åƒå¢å¼ºä¸­å­¦ä¹ ç¼–ç å™¨çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„FKAé¼“åŠ±ç¼–ç å™¨ä»ç‚¹å¯¹ç‚¹å¯¹é½å’ŒåŸºäºåŸå‹çš„ç±»å†…ç´§å‡‘æ€§çš„è§’åº¦ï¼Œä¸ºå¼ºå¢å¼ºå’Œå¼±å¢å¼ºè§†å›¾æ¨å¯¼å‡ºä¸€è‡´çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¹²é¢„ï¼ˆSAIï¼‰æ¨¡å—ï¼Œä»¥å¢åŠ å¯¹é½çš„ä¸­é—´ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚æ€§ï¼Œä¿ƒè¿›åŸºäºç‰¹å¾æ‰°åŠ¨çš„é¢„æµ‹ä¸€è‡´æ€§å­¦ä¹ ã€‚é’ˆå¯¹ç‰¹å®šå®ä¾‹è®¾è®¡äº†è‡ªé€‚åº”ç‰¹å¾æ©ç å’Œå™ªå£°æ³¨å…¥ï¼Œä»¥æ‰°åŠ¨ç‰¹å¾ï¼Œå®ç°è§£ç å™¨çš„ç¨³å¥å­¦ä¹ ã€‚åœ¨Pascal VOC2012å’ŒCityscapesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MCCLè¾¾åˆ°äº†æœ€æ–°çŠ¶æ€ã€‚æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUST-Machine-Intelligence-Laboratory/MCCL%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NUST-Machine-Intelligence-Laboratory/MCCLä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17914v1">PDF</a> accepted by IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šçº¦æŸä¸€è‡´æ€§å­¦ä¹ ï¼ˆMCCLï¼‰æ–¹æ³•ï¼Œç”¨äºä¿ƒè¿›ç¼–ç å™¨å’Œè§£ç å™¨çš„åˆ†é˜¶æ®µå¢å¼ºã€‚é€šè¿‡è®¾è®¡ç‰¹å¾çŸ¥è¯†å¯¹é½ï¼ˆFKAï¼‰ç­–ç•¥ï¼Œä¿ƒè¿›ç¼–ç å™¨ä»å›¾åƒå¢å¼ºä¸­å­¦ä¹ ç‰¹å¾ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œæå‡ºäº†è‡ªé€‚åº”å¹²é¢„ï¼ˆSAIï¼‰æ¨¡å—ï¼Œå¢åŠ å¯¹é½çš„ä¸­é—´ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚æ€§ï¼Œä¿ƒè¿›åŸºäºç‰¹å¾æ‰°åŠ¨çš„é¢„æµ‹ä¸€è‡´æ€§å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Pascal VOC2012å’ŒCityscapesæ•°æ®é›†ä¸Šï¼ŒMCCLå–å¾—äº†æœ€æ–° state-of-the-art æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MCCLæ–¹æ³•ä¿ƒè¿›ç¼–ç å™¨å’Œè§£ç å™¨çš„åˆ†é˜¶æ®µå¢å¼ºã€‚</li>
<li>FKAç­–ç•¥ä¿ƒè¿›ç¼–ç å™¨ä»å›¾åƒå¢å¼ºä¸­å­¦ä¹ ç‰¹å¾ä¸€è‡´æ€§ã€‚</li>
<li>FKAç­–ç•¥é¼“åŠ±å¯¹å¼ºå¢å¼ºå’Œå¼±å¢å¼ºè§†å›¾è¿›è¡Œç‰¹å¾ä¸€è‡´æ€§å­¦ä¹ ã€‚</li>
<li>SAIæ¨¡å—å¢åŠ å¯¹é½çš„ä¸­é—´ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚æ€§ã€‚</li>
<li>ç‰¹å¾æ‰°åŠ¨ä¿ƒè¿›é¢„æµ‹ä¸€è‡´æ€§å­¦ä¹ ã€‚</li>
<li>è‡ªé€‚åº”ç‰¹å¾æ©ç å’Œå™ªå£°æ³¨å…¥ä»¥å®ä¾‹ç‰¹å®šæ–¹å¼è¿›è¡Œè®¾è®¡ï¼Œä»¥æ‰°åŠ¨ç‰¹å¾ï¼Œå®ç°è§£ç å™¨çš„ç¨³å¥å­¦ä¹ ã€‚</li>
<li>åœ¨Pascal VOC2012å’ŒCityscapesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜MCCLå–å¾—äº†æœ€æ–° state-of-the-art æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc316c5c77c5cc6a930b4cca2e7f2595.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2327cb35c715efe9403781a057958b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb481bafc168b43690b62ba24e513c4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddae523701eb76ad3f673c6dfb74fc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cff704465c4b976f78c015df72a47d34.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MUST-The-First-Dataset-and-Unified-Framework-for-Multispectral-UAV-Single-Object-Tracking"><a href="#MUST-The-First-Dataset-and-Unified-Framework-for-Multispectral-UAV-Single-Object-Tracking" class="headerlink" title="MUST: The First Dataset and Unified Framework for Multispectral UAV   Single Object Tracking"></a>MUST: The First Dataset and Unified Framework for Multispectral UAV   Single Object Tracking</h2><p><strong>Authors:Haolin Qin, Tingfa Xu, Tianhao Li, Zhenxiang Chen, Tao Feng, Jianan Li</strong></p>
<p>UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on <a target="_blank" rel="noopener" href="https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking">https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking</a>. </p>
<blockquote>
<p>æ— äººæœºè·Ÿè¸ªåœ¨çœŸå®åœºæ™¯ä¸Šé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œä¾‹å¦‚ç›®æ ‡å°ºå¯¸è¾ƒå°å’Œé®æŒ¡é—®é¢˜ï¼Œè¿™äº›æŒ‘æˆ˜é™åˆ¶äº†åŸºäºRGBçš„è·Ÿè¸ªå™¨çš„æ€§èƒ½ã€‚å¤šå…‰è°±å›¾åƒï¼ˆMSIï¼‰èƒ½å¤Ÿæ•æ‰é¢å¤–çš„å…‰è°±ä¿¡æ¯ï¼Œä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜æä¾›äº†å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç›¸å…³æ•°æ®é›†ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°äº†é˜»ç¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šå…‰è°±æ— äººæœºå•ç›®æ ‡è·Ÿè¸ªæ•°æ®é›†ï¼ˆMUSTï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«250ä¸ªè·¨è¶Šä¸åŒç¯å¢ƒå’ŒæŒ‘æˆ˜çš„è§†é¢‘åºåˆ—ï¼Œä¸ºæ— äººæœºå¤šå…‰è°±è·Ÿè¸ªæä¾›äº†å…¨é¢çš„æ•°æ®åŸºç¡€ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹è·Ÿè¸ªæ¡†æ¶UNTrackï¼Œè¯¥æ¡†æ¶ä»å…‰è°±æç¤ºã€åˆå§‹æ¨¡æ¿å’Œé¡ºåºæœç´¢ä¸­ç¼–ç ç»Ÿä¸€çš„å…‰è°±ã€ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚UNTracké‡‡ç”¨å…·æœ‰å…‰è°±èƒŒæ™¯æ¶ˆé™¤æœºåˆ¶çš„ä¸å¯¹ç§°å˜å‹å™¨è¿›è¡Œæœ€ä¼˜å…³ç³»å»ºæ¨¡ï¼Œå¹¶ä¸”å…¶ç¼–ç å™¨èƒ½å¤ŸæŒç»­æ›´æ–°å…‰è°±æç¤ºä»¥æ”¹è¿›è·Ÿè¸ªï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„UNTrackä¼˜äºæœ€æ–°çš„æ— äººæœºè·Ÿè¸ªå™¨ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¡†æ¶å°†æ¨åŠ¨è¯¥é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Trackingä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17699v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— äººæœºè¿½è¸ªåœ¨å®é™…åœºæ™¯ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å°ç›®æ ‡å’Œé®æŒ¡é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†å¤šå…‰è°±å›¾åƒæŠ€æœ¯ï¼Œå¹¶åˆ›å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šå…‰è°±æ— äººæœºå•ç›®æ ‡è¿½è¸ªæ•°æ®é›†MUSTã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¿½è¸ªæ¡†æ¶UNTrackï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å…‰è°±ã€ç©ºé—´å’Œæ—¶é—´çš„ç‰¹å¾ï¼Œé‡‡ç”¨ä¸å¯¹ç§°çš„è½¬æ¢å™¨è¿›è¡Œå…³ç³»å»ºæ¨¡ï¼Œå¹¶ä¸æ–­æ›´æ–°å…‰è°±æç¤ºä»¥æé«˜è¿½è¸ªçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒUNTrackåœ¨æ— äººæœºè¿½è¸ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººæœºè¿½è¸ªé¢ä¸´å°ç›®æ ‡å’Œé®æŒ¡ç­‰å®é™…åœºæ™¯æŒ‘æˆ˜ã€‚</li>
<li>å¤šå…‰è°±å›¾åƒæŠ€æœ¯ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡çš„å¤šå…‰è°±æ— äººæœºå•ç›®æ ‡è¿½è¸ªæ•°æ®é›†MUSTã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¿½è¸ªæ¡†æ¶UNTrackï¼Œç»“åˆäº†å…‰è°±ã€ç©ºé—´å’Œæ—¶é—´çš„ç‰¹å¾ã€‚</li>
<li>UNTracké‡‡ç”¨ä¸å¯¹ç§°è½¬æ¢å™¨è¿›è¡Œå…³ç³»å»ºæ¨¡ï¼Œå¹¶ä¸æ–­æ›´æ–°å…‰è°±æç¤ºã€‚</li>
<li>UNTrackåœ¨æ— äººæœºè¿½è¸ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5115d318086a59a148258378f1c8da14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-437c2addd12480d4e940f6cb5e7ab00a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7b52aea77b0a44fc7ef42654765aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6ba3d33e56f90ef64704ff5604de0d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Temporal-Modeling-Framework-for-Video-Pre-Training-on-Video-Instance-Segmentation"><a href="#A-Temporal-Modeling-Framework-for-Video-Pre-Training-on-Video-Instance-Segmentation" class="headerlink" title="A Temporal Modeling Framework for Video Pre-Training on Video Instance   Segmentation"></a>A Temporal Modeling Framework for Video Pre-Training on Video Instance   Segmentation</h2><p><strong>Authors:Qing Zhong, Peng-Tao Jiang, Wen Wang, Guodong Ding, Lin Wu, Kaiqi Huang</strong></p>
<p>Contemporary Video Instance Segmentation (VIS) methods typically adhere to a pre-train then fine-tune regime, where a segmentation model trained on images is fine-tuned on videos. However, the lack of temporal knowledge in the pre-trained model introduces a domain gap which may adversely affect the VIS performance. To effectively bridge this gap, we present a novel video pre-training approach to enhance VIS models, especially for videos with intricate instance relationships. Our crucial innovation focuses on reducing disparities between the pre-training and fine-tuning stages. Specifically, we first introduce consistent pseudo-video augmentations to create diverse pseudo-video samples for pre-training while maintaining the instance consistency across frames. Then, we incorporate a multi-scale temporal module to enhance the modelâ€™s ability to model temporal relations through self- and cross-attention at short- and long-term temporal spans. Our approach does not set constraints on model architecture and can integrate seamlessly with various VIS methods. Experiment results on commonly adopted VIS benchmarks show that our method consistently outperforms state-of-the-art methods. Our approach achieves a notable 4.0% increase in average precision on the challenging OVIS dataset. </p>
<blockquote>
<p>ç°ä»£è§†é¢‘å®ä¾‹åˆ†å‰²ï¼ˆVISï¼‰æ–¹æ³•é€šå¸¸éµå¾ªé¢„è®­ç»ƒç„¶åç²¾ç»†è°ƒæ•´çš„åˆ¶åº¦ï¼Œå…¶ä¸­åœ¨å›¾åƒä¸Šè®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ä¼šåœ¨è§†é¢‘ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒæ¨¡å‹ä¸­ç¼ºä¹æ—¶é—´çŸ¥è¯†ï¼Œè¿™ä¼šäº§ç”ŸåŸŸå·®è·ï¼Œå¯èƒ½ä¼šä¸åˆ©äºVISæ€§èƒ½ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥æé«˜VISæ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¤æ‚å®ä¾‹å…³ç³»çš„è§†é¢‘ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå‡å°‘é¢„è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´é˜¶æ®µä¹‹é—´çš„å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸€è‡´çš„ä¼ªè§†é¢‘å¢å¼ºæŠ€æœ¯ï¼Œä»¥åˆ›å»ºå¤šæ ·çš„ä¼ªè§†é¢‘æ ·æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒè·¨å¸§çš„å®ä¾‹ä¸€è‡´æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬èå…¥å¤šå°ºåº¦æ—¶é—´æ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹é€šè¿‡è‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›åœ¨çŸ­æœŸå’Œé•¿æœŸæ—¶é—´è·¨åº¦ä¸Šå»ºç«‹æ—¶é—´å…³ç³»çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å¯¹æ¨¡å‹æ¶æ„è®¾ç½®çº¦æŸï¼Œå¹¶èƒ½ä¸å„ç§VISæ–¹æ³•æ— ç¼é›†æˆã€‚åœ¨å¸¸ç”¨çš„VISåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OVISæ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦4.0%çš„æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17672v1">PDF</a> 7 pages, 5figures, 6 tables, Accepted to ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é¢„è®­ç»ƒæ–¹æ³•æ¥æå‡è§†é¢‘å®ä¾‹åˆ†å‰²ï¼ˆVISï¼‰æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰å¤æ‚å®ä¾‹å…³ç³»çš„è§†é¢‘æ—¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€è‡´çš„ä¼ªè§†é¢‘å¢å¼ºå’Œå¤šå°ºåº¦æ—¶é—´æ¨¡å—ï¼Œç¼©å°äº†é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†æ¨¡å‹åœ¨è·¨å¸§å®ä¾‹ä¸€è‡´æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸¸ç”¨çš„VISåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OVISæ•°æ®é›†ä¸Šå¹³å‡ç²¾åº¦æé«˜äº†4.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é¢„è®­ç»ƒç­–ç•¥ï¼Œé’ˆå¯¹è§†é¢‘å®ä¾‹åˆ†å‰²ï¼ˆVISï¼‰æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸€è‡´çš„ä¼ªè§†é¢‘å¢å¼ºï¼Œç»´æŒè·¨å¸§å®ä¾‹çš„ä¸€è‡´æ€§ï¼Œåˆ›å»ºå¤šæ ·çš„ä¼ªè§†é¢‘æ ·æœ¬ç”¨äºé¢„è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨äº†å¤šå°ºåº¦æ—¶é—´æ¨¡å—ï¼Œå¢å¼ºæ¨¡å‹åœ¨çŸ­æœŸå’Œé•¿æœŸæ—¶é—´è·¨åº¦ä¸Šè‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›çš„æ—¶é—´å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸é™åˆ¶æ¨¡å‹æ¶æ„ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°å„ç§VISæ–¹æ³•ä¸­ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸¸ç”¨çš„VISåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OVISæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„å¹³å‡ç²¾åº¦æé«˜äº†4.0%ï¼Œè¡¨ç°æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d0ec7ec730db0e285292ea693512595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-514806c8965744383e61d75cff31609f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d1a57376bf91faf9eef592b783b28fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed8d278873cee06d7f1f33986fcebd4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90f88228ac5059271a2959f105a43d42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f75e548ad414c67a7d39c0c7aa4907c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff316747876db5241e76b15666f499bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2810ed5737cf435433c268dc8d6345b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes"><a href="#Exploring-Few-Shot-Object-Detection-on-Blood-Smear-Images-A-Case-Study-of-Leukocytes-and-Schistocytes" class="headerlink" title="Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes"></a>Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study   of Leukocytes and Schistocytes</h2><p><strong>Authors:Davide Antonio Mura, Michela Pinna, Lorenzo Putzu, Andrea Loddo, Alessandra Perniciano, Olga Mulas, Cecilia Di Ruberto</strong></p>
<p>The detection of blood disorders often hinges upon the quantification of specific blood cell types. Variations in cell counts may indicate the presence of pathological conditions. Thus, the significance of developing precise automatic systems for blood cell enumeration is underscored. The investigation focuses on a novel approach termed DE-ViT. This methodology is employed in a Few-Shot paradigm, wherein training relies on a limited number of images. Two distinct datasets are utilised for experimental purposes: the Raabin-WBC dataset for Leukocyte detection and a local dataset for Schistocyte identification. In addition to the DE-ViT model, two baseline models, Faster R-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being compared against those of the proposed model. While DE-ViT has demonstrated state-of-the-art performance on the COCO and LVIS datasets, both baseline models surpassed its performance on the Raabin-WBC dataset. Moreover, only Faster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed disparities in performance may possibly be attributed to domain shift phenomena. </p>
<blockquote>
<p>è¡€æ¶²ç–¾ç—…çš„æ£€æµ‹é€šå¸¸ä¾èµ–äºç‰¹å®šè¡€ç»†èƒçš„é‡åŒ–ã€‚è¡€ç»†èƒè®¡æ•°çš„å˜åŒ–å¯èƒ½è¡¨æ˜å­˜åœ¨ç—…ç†çŠ¶å†µã€‚å› æ­¤ï¼Œå¼€å‘ç²¾ç¡®è‡ªåŠ¨è¡€ç»†èƒè®¡æ•°ç³»ç»Ÿçš„æ„ä¹‰å°¤ä¸ºé‡è¦ã€‚ç ”ç©¶é‡ç‚¹æ˜¯ä¸€ç§ç§°ä¸ºDE-ViTçš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨å°æ ·æœ¬ï¼ˆFew-Shotï¼‰èŒƒå¼ï¼Œè®­ç»ƒä¾èµ–äºå°‘é‡å›¾åƒã€‚ä¸ºäº†å®éªŒç›®çš„ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†ï¼šç”¨äºç™½ç»†èƒæ£€æµ‹çš„Raabin-WBCæ•°æ®é›†å’Œç”¨äºè£‚çº¢ç»†èƒè¯†åˆ«çš„æœ¬åœ°æ•°æ®é›†ã€‚é™¤äº†DE-ViTæ¨¡å‹å¤–ï¼Œè¿˜ä½¿ç”¨äº†Faster R-CNN 50å’ŒFaster R-CNN X 101è¿™ä¸¤ä¸ªåŸºçº¿æ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬çš„ç»“æœä¸æ‰€æå‡ºæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è™½ç„¶DE-ViTåœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åŸºçº¿æ¨¡å‹åœ¨Raabin-WBCæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡äº†å®ƒã€‚æ­¤å¤–ï¼Œåªæœ‰Faster R-CNN X 101åœ¨SC-IDBä¸Šäº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚è§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®å¼‚å¯èƒ½å½’å› äºé¢†åŸŸåç§»ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17107v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè¡€ç»†èƒè®¡æ•°çš„è‡ªåŠ¨ç³»ç»Ÿæ£€æµ‹è¡€æ¶²ç–¾ç—…çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºDE-ViTã€‚è¯¥æ–¹æ³•é‡‡ç”¨å°æ ·æœ¬è®­ç»ƒæ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†è¿›è¡Œå®éªŒè¯„ä¼°ã€‚å°½ç®¡DE-ViTåœ¨æŸäº›æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é’ˆå¯¹ç‰¹å®šè¡€ç»†èƒç±»å‹æ£€æµ‹çš„Raabin-WBCæ•°æ®é›†ä¸Šï¼Œå…¶æ€§èƒ½è¢«å…¶ä»–æ¨¡å‹è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€æ¶²ç–¾ç—…çš„æ£€æµ‹é€šå¸¸ä¾èµ–äºç‰¹å®šè¡€ç»†èƒçš„é‡åŒ–ã€‚</li>
<li>DE-ViTæ˜¯ä¸€ç§ç”¨äºè¡€ç»†èƒè®¡æ•°çš„æ–°çš„è‡ªåŠ¨æ£€æµ‹æ–¹æ³•ï¼Œé‡‡ç”¨å°æ ·æœ¬è®­ç»ƒæ¨¡å¼ã€‚</li>
<li>å®éªŒä½¿ç”¨äº†Raabin-WBCæ•°æ®é›†è¿›è¡Œç™½ç»†èƒæ£€æµ‹ï¼Œä»¥åŠå¦ä¸€ä¸ªæœ¬åœ°æ•°æ®é›†è¿›è¡Œè£‚çº¢ç»†èƒè¯†åˆ«ã€‚</li>
<li>åœ¨Raabin-WBCæ•°æ®é›†ä¸Šï¼ŒDE-ViTçš„æ€§èƒ½è¢«å…¶ä»–æ¨¡å‹ï¼ˆå¦‚Faster R-CNN 50å’ŒFaster R-CNN X 101ï¼‰è¶…è¶Šã€‚</li>
<li>åªæœ‰Faster R-CNN X 101åœ¨SC-IDBæ•°æ®é›†ä¸Šäº§ç”Ÿäº†ä»¤äººæ»¡æ„çš„ç»“æœã€‚</li>
<li>æ¨¡å‹æ€§èƒ½çš„å·®å¼‚å¯èƒ½æºäºé¢†åŸŸè¿ç§»ç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e20ca7f60177aa8ec79c9f40acf0c782.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe8e2ec1f311e54b959baac9c0821624.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8172ea314037f0aa69f8bfc90f4aca7a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Center-guided-Classifier-for-Semantic-Segmentation-of-Remote-Sensing-Images"><a href="#Center-guided-Classifier-for-Semantic-Segmentation-of-Remote-Sensing-Images" class="headerlink" title="Center-guided Classifier for Semantic Segmentation of Remote Sensing   Images"></a>Center-guided Classifier for Semantic Segmentation of Remote Sensing   Images</h2><p><strong>Authors:Wei Zhang, Mengting Ma, Yizhen Jiang, Rongrong Lian, Zhenkai Wu, Kangning Cui, Xiaowen Ma</strong></p>
<p>Compared with natural images, remote sensing images (RSIs) have the unique characteristic. i.e., larger intraclass variance, which makes semantic segmentation for remote sensing images more challenging. Moreover, existing semantic segmentation models for remote sensing images usually employ a vanilla softmax classifier, which has three drawbacks: (1) non-direct supervision for the pixel representations during training; (2) inadequate modeling ability of parametric softmax classifiers under large intraclass variance; and (3) opaque process of classification decision. In this paper, we propose a novel classifier (called CenterSeg) customized for RSI semantic segmentation, which solves the abovementioned problems with multiple prototypes, direct supervision under Grassmann manifold, and interpretability strategy. Specifically, for each class, our CenterSeg obtains local class centers by aggregating corresponding pixel features based on ground-truth masks, and generates multiple prototypes through hard attention assignment and momentum updating. In addition, we introduce the Grassmann manifold and constrain the joint embedding space of pixel features and prototypes based on two additional regularization terms. Especially, during the inference, CenterSeg can further provide interpretability to the model by restricting the prototype as a sample of the training set. Experimental results on three remote sensing segmentation datasets validate the effectiveness of the model. Besides the superior performance, CenterSeg has the advantages of simplicity, lightweight, compatibility, and interpretability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a>. </p>
<blockquote>
<p>ä¸è‡ªç„¶å›¾åƒç›¸æ¯”ï¼Œé¥æ„Ÿå›¾åƒï¼ˆRSIsï¼‰å…·æœ‰æ›´å¤§çš„ç±»å†…å·®å¼‚è¿™ä¸€ç‹¬ç‰¹ç‰¹å¾ï¼Œè¿™ä½¿å¾—é¥æ„Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ›´å…·æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹é€šå¸¸é‡‡ç”¨æ™®é€šçš„softmaxåˆ†ç±»å™¨ï¼Œå®ƒå­˜åœ¨ä»¥ä¸‹ä¸‰ä¸ªç¼ºç‚¹ï¼šï¼ˆ1ï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­åƒç´ è¡¨ç¤ºçš„é—´æ¥ç›‘ç£ï¼›ï¼ˆ2ï¼‰åœ¨è¾ƒå¤§çš„ç±»å†…å·®å¼‚ä¸‹ï¼Œå‚æ•°åŒ–softmaxåˆ†ç±»å™¨çš„å»ºæ¨¡èƒ½åŠ›ä¸è¶³ï¼›ï¼ˆ3ï¼‰åˆ†ç±»å†³ç­–è¿‡ç¨‹ä¸é€æ˜ã€‚é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹RSIè¯­ä¹‰åˆ†å‰²çš„æ–°å‹åˆ†ç±»å™¨ï¼ˆç§°ä¸ºCenterSegï¼‰ï¼Œé€šè¿‡å¤šä¸ªåŸå‹ã€Grassmannæµå½¢ä¸‹çš„ç›´æ¥ç›‘ç£ä»¥åŠè§£é‡Šç­–ç•¥æ¥è§£å†³ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬çš„CenterSegé€šè¿‡åŸºäºçœŸå®æ©ç çš„åƒç´ ç‰¹å¾èšåˆè·å¾—å±€éƒ¨ç±»ä¸­å¿ƒï¼Œå¹¶é€šè¿‡ç¡¬æ³¨æ„åŠ›åˆ†é…å’ŒåŠ¨é‡æ›´æ–°ç”Ÿæˆå¤šä¸ªåŸå‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Grassmannæµå½¢ï¼Œå¹¶åŸºäºä¸¤ä¸ªé¢å¤–çš„æ­£åˆ™åŒ–é¡¹çº¦æŸåƒç´ ç‰¹å¾å’ŒåŸå‹çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒCenterSegå¯ä»¥é€šè¿‡å°†åŸå‹é™åˆ¶ä¸ºè®­ç»ƒé›†çš„æ ·æœ¬ä¸ºæ¨¡å‹æä¾›è¿›ä¸€æ­¥çš„è§£é‡Šæ€§ã€‚åœ¨ä¸‰ä¸ªé¥æ„Ÿåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é™¤äº†å“è¶Šçš„æ€§èƒ½å¤–ï¼ŒCenterSegè¿˜å…·æœ‰ç®€å•ã€è½»ä¾¿ã€å…¼å®¹å’Œå¯è§£é‡Šæ€§çš„ä¼˜ç‚¹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16963v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿå›¾åƒç›¸è¾ƒäºè‡ªç„¶å›¾åƒå…·æœ‰è¾ƒå¤§çš„ç±»å†…å·®å¼‚ï¼Œä¸ºè¯­ä¹‰åˆ†å‰²å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸é‡‡ç”¨æ ‡å‡†softmaxåˆ†ç±»å™¨ï¼Œå­˜åœ¨éç›´æ¥ç›‘ç£åƒç´ è¡¨ç¤ºã€å¯¹ç±»å†…å·®å¼‚å»ºæ¨¡èƒ½åŠ›ä¸è¶³åŠå†³ç­–è¿‡ç¨‹ä¸é€æ˜ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„æ–°å‹åˆ†ç±»å™¨CenterSegï¼Œé€šè¿‡å¤šåŸå‹ã€Grassmannæµå½¢ä¸‹çš„ç›´æ¥ç›‘ç£åŠè§£é‡Šç­–ç•¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚CenterSegé€šè¿‡åŸºäºçœŸå®æ ‡ç­¾æ©è†œèšåˆåƒç´ ç‰¹å¾è·å¾—å±€éƒ¨ç±»ä¸­å¿ƒï¼Œç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œå¹¶åœ¨Grassmannæµå½¢ä¸Šçº¦æŸåƒç´ ç‰¹å¾ä¸åŸå‹çš„è”åˆåµŒå…¥ç©ºé—´ã€‚å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€ä¼˜åŠ¿åœ¨äºç®€å•ã€è½»ä¾¿ã€å…¼å®¹åŠå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒç›¸è¾ƒäºè‡ªç„¶å›¾åƒå…·æœ‰æ›´å¤§çš„ç±»å†…å·®å¼‚ï¼Œä½¿å¾—è¯­ä¹‰åˆ†å‰²æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°å­˜æ¨¡å‹é‡‡ç”¨çš„æ ‡å‡†softmaxåˆ†ç±»å™¨å­˜åœ¨å¯¹éç›´æ¥ç›‘ç£åƒç´ è¡¨ç¤ºã€å¯¹ç±»å†…å·®å¼‚å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„CenterSegåˆ†ç±»å™¨é€šè¿‡å¤šåŸå‹å’ŒGrassmannæµå½¢ä¸‹çš„ç›´æ¥ç›‘ç£è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>CenterSegç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œé€šè¿‡ç¡¬æ³¨æ„åŠ›åˆ†é…å’ŒåŠ¨é‡æ›´æ–°å®ç°ã€‚</li>
<li>CenterSegåœ¨é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­å¼•å…¥Grassmannæµå½¢ï¼Œçº¦æŸåƒç´ ç‰¹å¾ä¸åŸå‹çš„è”åˆåµŒå…¥ç©ºé—´ã€‚</li>
<li>CenterSegåœ¨æ¨ç†é˜¶æ®µå¯ä»¥æä¾›æ¨¡å‹è§£é‡Šæ€§ï¼Œé€šè¿‡é™åˆ¶åŸå‹ä¸ºè®­ç»ƒé›†çš„æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1f514b0201bd36b96419aa876e740ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27c512dc999f0e6b1bad81bff8bc141d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70f5a35225ee13ce3303f5c3b0606fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8198267db45b9566bf6a6590ae83a06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72924ce67c34b817b375ad8588b94df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d9fcbf15f22f7d2f2e2831b3a03233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-600a0a663d78bf47a48761aff7f0641e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Salient-Object-Detection-in-Traffic-Scene-through-the-TSOD10K-Dataset"><a href="#Salient-Object-Detection-in-Traffic-Scene-through-the-TSOD10K-Dataset" class="headerlink" title="Salient Object Detection in Traffic Scene through the TSOD10K Dataset"></a>Salient Object Detection in Traffic Scene through the TSOD10K Dataset</h2><p><strong>Authors:Yu Qiu, Yuhang Sun, Jie Mei, Lin Xiao, Jing Xu</strong></p>
<p>Traffic Salient Object Detection (TSOD) aims to segment the objects critical to driving safety by combining semantic (e.g., collision risks) and visual saliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes visually distinctive regions, TSOD emphasizes the objects that demand immediate driver attention due to their semantic impact, even with low visual contrast. This dual criterion, i.e., bridging perception and contextual risk, re-defines saliency for autonomous and assisted driving systems. To address the lack of task-specific benchmarks, we collect the first large-scale TSOD dataset with pixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse object categories in various real-world traffic scenes under various challenging weather&#x2F;illumination variations (e.g., fog, snowstorms, low-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD model, termed Tramba. Considering the challenge of distinguishing inconspicuous visual information from complex traffic backgrounds, Tramba introduces a novel Dual-Frequency Visual State Space module equipped with shifted window partitioning and dilated scanning to enhance the perception of fine details and global structure by hierarchically decomposing high&#x2F;low-frequency components. To emphasize critical regions in traffic scenes, we propose a traffic-oriented Helix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention priors while effectively capturing global multi-direction spatial dependencies. We establish a comprehensive benchmark by evaluating Tramba and 22 existing NSI-SOD models on TSOD10K, demonstrating Trambaâ€™s superiority. Our research establishes the first foundation for safety-aware saliency analysis in intelligent transportation systems. </p>
<blockquote>
<p>äº¤é€šæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆTSODï¼‰æ—¨åœ¨é€šè¿‡ç»“åˆè¯­ä¹‰ï¼ˆä¾‹å¦‚ç¢°æ’é£é™©ï¼‰å’Œè§†è§‰æ˜¾è‘—æ€§ï¼Œåˆ†å‰²å¯¹é©¾é©¶å®‰å…¨è‡³å…³é‡è¦çš„ç›®æ ‡ã€‚ä¸åŒäºè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„SODï¼ˆNSI-SODï¼‰ï¼Œåè€…ä¼˜å…ˆå…³æ³¨è§†è§‰ä¸Šæœ‰æ˜¾è‘—ç‰¹å¾çš„åŒºåŸŸï¼ŒTSODå¼ºè°ƒé‚£äº›ç”±äºè¯­ä¹‰å½±å“è€Œéœ€è¦é©¾é©¶å‘˜ç«‹å³å…³æ³¨çš„ç›®æ ‡ï¼Œå³ä½¿å®ƒä»¬çš„è§†è§‰å¯¹æ¯”åº¦è¾ƒä½ã€‚è¿™ä¸€åŒé‡æ ‡å‡†ï¼Œå³æ„ŸçŸ¥ä¸ä¸Šä¸‹æ–‡é£é™©çš„æ¡¥æ¢ä½œç”¨ï¼Œä¸ºè‡ªä¸»å’Œè¾…åŠ©é©¾é©¶ç³»ç»Ÿé‡æ–°å®šä¹‰äº†æ˜¾è‘—æ€§ã€‚ä¸ºäº†è§£å†³ç¼ºä¹ç‰¹å®šä»»åŠ¡åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ”¶é›†äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„TSODæ•°æ®é›†ï¼Œåä¸ºTSOD10Kï¼Œè¯¥æ•°æ®é›†å…·æœ‰åƒç´ çº§çš„æ˜¾è‘—æ€§æ³¨é‡Šã€‚TSOD10Kæ¶µç›–äº†å„ç§ç°å®ä¸–ç•Œäº¤é€šåœºæ™¯ä¸­å¤šæ ·åŒ–çš„ç›®æ ‡ç±»åˆ«ï¼Œä»¥åŠå„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©æ°”&#x2F;ç…§æ˜å˜åŒ–ï¼ˆä¾‹å¦‚é›¾ã€æš´é£é›ªã€ä½å¯¹æ¯”åº¦å’Œä½å…‰ç…§ï¼‰ã€‚æ–¹æ³•è®ºä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMambaçš„TSODæ¨¡å‹ï¼Œåä¸ºTrambaã€‚è€ƒè™‘åˆ°ä»å¤æ‚çš„äº¤é€šèƒŒæ™¯ä¸­åŒºåˆ†ä¸æ˜æ˜¾è§†è§‰ä¿¡æ¯çš„æŒ‘æˆ˜ï¼ŒTrambaå¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„åŒé¢‘è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œè¯¥æ¨¡å—é…å¤‡äº†ç§»ä½çª—å£åˆ’åˆ†å’Œè†¨èƒ€æ‰«æï¼Œé€šè¿‡åˆ†å±‚åˆ†è§£é«˜é¢‘&#x2F;ä½é¢‘ç»„ä»¶ï¼Œå¢å¼ºå¯¹ç»†èŠ‚å’Œå…¨å±€ç»“æ„çš„æ„ŸçŸ¥ã€‚ä¸ºäº†å¼ºè°ƒäº¤é€šåœºæ™¯ä¸­çš„é‡è¦åŒºåŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘äº¤é€šçš„Helix 2D-Selective-Scanï¼ˆHelix-SS2Dï¼‰æœºåˆ¶ï¼Œå®ƒæ³¨å…¥é©¾é©¶æ³¨æ„åŠ›ä¼˜å…ˆçº§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°æ•æ‰å…¨å±€å¤šæ–¹å‘ç©ºé—´ä¾èµ–æ€§ã€‚æˆ‘ä»¬åœ¨TSOD10Kä¸Šå¯¹Trambaå’Œ22ä¸ªç°æœ‰çš„NSI-SODæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†Trambaçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å®‰å…¨æ„ŸçŸ¥æ˜¾è‘—æ€§åˆ†æå¥ å®šäº†ç¬¬ä¸€å—åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16910v1">PDF</a> 12 pages, 12 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äº¤é€šæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆTSODï¼‰æ—¨åœ¨ç»“åˆè¯­ä¹‰ï¼ˆå¦‚ç¢°æ’é£é™©ï¼‰å’Œè§†è§‰æ˜¾è‘—æ€§ï¼Œåˆ†å‰²é©¾é©¶å®‰å…¨ä¸­å…³é”®çš„å¯¹è±¡ã€‚ä¸åŒäºè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„SODï¼ˆNSI-SODï¼‰ï¼ŒTSODæ›´ä¾§é‡äºç”±äºè¯­ä¹‰å½±å“è€Œéœ€è¦é©¾é©¶å‘˜ç«‹å³æ³¨æ„çš„ç›®æ ‡ï¼Œå³ä½¿è§†è§‰å¯¹æ¯”åº¦è¾ƒä½ã€‚è¿™ç§åŒé‡æ ‡å‡†ï¼Œå³æ„ŸçŸ¥å’Œä¸Šä¸‹æ–‡é£é™©çš„æ¡¥æ¢ï¼Œä¸ºè‡ªä¸»å’Œè¾…åŠ©é©¾é©¶ç³»ç»Ÿé‡æ–°å®šä¹‰äº†æ˜¾è‘—æ€§ã€‚ä¸ºäº†è§£å†³ç‰¹å®šä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ”¶é›†äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„TSODæ•°æ®é›†ï¼Œåä¸ºTSOD10Kï¼Œå…·æœ‰åƒç´ çº§çš„æ˜¾è‘—æ€§æ³¨é‡Šã€‚TSOD10Kæ¶µç›–äº†å„ç§ç°å®ä¸–ç•Œäº¤é€šåœºæ™¯ä¸­å¤šæ ·åŒ–çš„å¯¹è±¡ç±»åˆ«ï¼Œä»¥åŠå„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©æ°”&#x2F;ç…§æ˜å˜åŒ–ï¼ˆä¾‹å¦‚é›¾ã€æš´é£é›ªã€ä½å¯¹æ¯”åº¦å’Œä½å…‰ï¼‰ã€‚æ–¹æ³•è®ºä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMambaçš„TSODæ¨¡å‹ï¼Œåä¸ºTrambaã€‚è€ƒè™‘åˆ°ä»å¤æ‚çš„äº¤é€šèƒŒæ™¯ä¸­åŒºåˆ†ä¸æ˜æ˜¾è§†è§‰ä¿¡æ¯çš„æŒ‘æˆ˜ï¼ŒTrambaå¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„åŒé¢‘è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œé…å¤‡äº†ç§»ä½çª—å£åˆ†åŒºå’Œè†¨èƒ€æ‰«æï¼Œé€šè¿‡åˆ†å±‚åˆ†è§£é«˜ä½é¢‘ç»„ä»¶æ¥å¢å¼ºå¯¹ç»†å¾®ç»†èŠ‚å’Œå…¨å±€ç»“æ„çš„æ„ŸçŸ¥ã€‚ä¸ºäº†å¼ºè°ƒäº¤é€šåœºæ™¯ä¸­çš„å…³é”®åŒºåŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘äº¤é€šçš„Helix 2D-Selective-Scanï¼ˆHelix-SS2Dï¼‰æœºåˆ¶ï¼Œå®ƒæ³¨å…¥é©¾é©¶æ³¨æ„åŠ›ä¼˜å…ˆçº§ï¼ŒåŒæ—¶æœ‰æ•ˆæ•æ‰å…¨å±€å¤šæ–¹å‘ç©ºé—´ä¾èµ–æ€§ã€‚æˆ‘ä»¬åœ¨TSOD10Kä¸Šè¯„ä¼°äº†Trambaå’Œ22ç§ç°æœ‰çš„NSI-SODæ¨¡å‹ï¼Œå»ºç«‹äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†Trambaçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å®‰å…¨æ„ŸçŸ¥æ˜¾è‘—æ€§åˆ†æå¥ å®šäº†é¦–è¦åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TSODæ—¨åœ¨ç»“åˆè¯­ä¹‰å’Œè§†è§‰æ˜¾è‘—æ€§æ¥æ£€æµ‹é©¾é©¶ä¸­çš„å…³é”®å¯¹è±¡ï¼Œä¸åŒäºè‡ªç„¶åœºæ™¯å›¾åƒä¸­çš„SODã€‚</li>
<li>æå‡ºäº†å¤§è§„æ¨¡TSODæ•°æ®é›†TSOD10Kï¼ŒåŒ…å«åƒç´ çº§æ˜¾è‘—æ€§æ³¨é‡Šï¼Œè¦†ç›–å„ç§äº¤é€šåœºæ™¯å’Œå¤šç§å¤©æ°”&#x2F;ç…§æ˜æ¡ä»¶ã€‚</li>
<li>ä»‹ç»äº†åŸºäºMambaçš„TSODæ¨¡å‹Trambaï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ç»†å¾®çš„è§†è§‰ä¿¡æ¯å’Œå¤æ‚çš„äº¤é€šèƒŒæ™¯ã€‚</li>
<li>Trambaå…·æœ‰åŒé¢‘è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—å’Œé¢å‘äº¤é€šçš„Helix-SS2Dæœºåˆ¶ï¼Œæé«˜äº†å¯¹ç»†å¾®ç»†èŠ‚å’Œå…¨å±€ç»“æ„çš„æ„ŸçŸ¥ï¼Œå¹¶å¼ºè°ƒäº†äº¤é€šåœºæ™¯ä¸­çš„å…³é”®åŒºåŸŸã€‚</li>
<li>åœ¨TSOD10Kæ•°æ®é›†ä¸Šå»ºç«‹çš„åŸºå‡†æµ‹è¯•è¯æ˜äº†Trambaç›¸å¯¹äºå…¶ä»–NSI-SODæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ç ”ç©¶ä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å®‰å…¨æ„ŸçŸ¥æ˜¾è‘—æ€§åˆ†æå¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-821dc0978757a5ee151ae4ce67e2f7b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-828d49ed64d2017266fab8d7df449426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db08102c27c1d4f6fb8c2c04c09f2f39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e768a356ecc3868e486bd4a3bca1e59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4dccdd1a82a7caafee9808b4388c3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bc636ab090d780f367ec7286066db9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SUM-Parts-Benchmarking-Part-Level-Semantic-Segmentation-of-Urban-Meshes"><a href="#SUM-Parts-Benchmarking-Part-Level-Semantic-Segmentation-of-Urban-Meshes" class="headerlink" title="SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes"></a>SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes</h2><p><strong>Authors:Weixiao Gao, Liangliang Nan, Hugo Ledoux</strong></p>
<p>Semantic segmentation in urban scene analysis has mainly focused on images or point clouds, while textured meshes - offering richer spatial representation - remain underexplored. This paper introduces SUM Parts, the first large-scale dataset for urban textured meshes with part-level semantic labels, covering about 2.5 km2 with 21 classes. The dataset was created using our own annotation tool, which supports both face- and texture-based annotations with efficient interactive selection. We also provide a comprehensive evaluation of 3D semantic segmentation and interactive annotation methods on this dataset. Our project page is available at <a target="_blank" rel="noopener" href="https://tudelft3d.github.io/SUMParts/">https://tudelft3d.github.io/SUMParts/</a>. </p>
<blockquote>
<p>åŸå¸‚åœºæ™¯åˆ†æä¸­çš„è¯­ä¹‰åˆ†å‰²ä¸»è¦å…³æ³¨å›¾åƒæˆ–ç‚¹äº‘ï¼Œè€Œæä¾›ä¸°å¯Œç©ºé—´è¡¨ç¤ºçš„çº¹ç†ç½‘æ ¼ä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚æœ¬æ–‡ä»‹ç»äº†SUM Partsï¼Œè¿™æ˜¯é’ˆå¯¹åŸå¸‚çº¹ç†ç½‘æ ¼çš„é¦–ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«éƒ¨åˆ†çº§åˆ«çš„è¯­ä¹‰æ ‡ç­¾ï¼Œè¦†ç›–çº¦2.5å¹³æ–¹å…¬é‡Œçš„21ä¸ªç±»åˆ«ã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„æ³¨é‡Šå·¥å…·åˆ›å»ºçš„ï¼Œè¯¥å·¥å…·æ”¯æŒåŸºäºé¢éƒ¨å’Œçº¹ç†çš„æ³¨é‡Šï¼Œå¹¶å…·æœ‰é«˜æ•ˆçš„äº¤äº’å¼é€‰æ‹©åŠŸèƒ½ã€‚æˆ‘ä»¬è¿˜åœ¨æ­¤æ•°æ®é›†ä¸Šå…¨é¢è¯„ä¼°äº†3Dè¯­ä¹‰åˆ†å‰²å’Œäº¤äº’å¼æ³¨é‡Šæ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://tudelft3d.github.io/SUMParts/">https://tudelft3d.github.io/SUMParts/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15300v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŸå¸‚çº¹ç†ç½‘æ ¼çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†SUM Partsã€‚è¯¥æ•°æ®é›†é‡‡ç”¨è‡ªä¸»ç ”å‘çš„æ”¯æŒé¢çº¹ç»“åˆçš„æ ‡æ³¨å·¥å…·è¿›è¡Œå¤§è§„æ¨¡æ ‡æ³¨ï¼ŒåŒ…å«çº¦2.5å¹³æ–¹å…¬é‡Œçš„21ç±»è¯­ä¹‰æ ‡ç­¾ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜å¯¹è¯¥æ•°æ®é›†ä¸Šçš„ä¸‰ç»´è¯­ä¹‰åˆ†å‰²å’Œäº¤äº’å¼æ ‡æ³¨æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡é’ˆå¯¹åŸå¸‚çº¹ç†ç½‘æ ¼è¯­ä¹‰åˆ†å‰²ç ”ç©¶è¿›è¡Œäº†æ¢ç´¢ï¼Œçªç ´äº†ä¹‹å‰å›¾åƒæˆ–ç‚¹äº‘ç ”ç©¶çš„å±€é™ã€‚</li>
<li>SUM Partsæ•°æ®é›†ä¸ºåŸå¸‚çº¹ç†ç½‘æ ¼æä¾›é¦–ä¸ªå¤§è§„æ¨¡éƒ¨åˆ†çº§åˆ«çš„è¯­ä¹‰æ ‡ç­¾æ•°æ®é›†ã€‚</li>
<li>SUM Partsæ•°æ®é›†è¦†ç›–çº¦2.5å¹³æ–¹å…¬é‡Œçš„åŒºåŸŸï¼ŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ç±»åˆ«ï¼Œå…±æœ‰21ç±»ã€‚</li>
<li>è¯¥è®ºæ–‡ä½¿ç”¨äº†è‡ªä¸»ç ”å‘çš„æ ‡æ³¨å·¥å…·è¿›è¡Œé«˜æ•ˆäº’åŠ¨é€‰æ‹©çš„é¢çº¹ç»“åˆæ ‡æ³¨ã€‚</li>
<li>æ–‡ç« æä¾›äº†å¯¹è¯¥æ•°æ®é›†ä¸Šçš„ä¸‰ç»´è¯­ä¹‰åˆ†å‰²çš„å…¨é¢è¯„ä¼°ã€‚</li>
<li>SUM Partsæ•°æ®é›†çš„é¡¹ç›®é¡µé¢å¯ä¾›åœ¨çº¿è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-21d6de505d839343942137ed6c924432.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca3d4e2aea391cd27221118263037aca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0225a13ce757bbccc624b858369e7a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ad00e1ae58b84c839231a452d406759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45cd357652c2cbea2c98aa9eff88b70f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MemorySAM-Memorize-Modalities-and-Semantics-with-Segment-Anything-Model-2-for-Multi-modal-Semantic-Segmentation"><a href="#MemorySAM-Memorize-Modalities-and-Semantics-with-Segment-Anything-Model-2-for-Multi-modal-Semantic-Segmentation" class="headerlink" title="MemorySAM: Memorize Modalities and Semantics with Segment Anything Model   2 for Multi-modal Semantic Segmentation"></a>MemorySAM: Memorize Modalities and Semantics with Segment Anything Model   2 for Multi-modal Semantic Segmentation</h2><p><strong>Authors:Chenfei Liao, Xu Zheng, Yuanhuiyi Lyu, Haiwei Xue, Yihong Cao, Jiawen Wang, Kailun Yang, Xuming Hu</strong></p>
<p>Research has focused on Multi-Modal Semantic Segmentation (MMSS), where pixel-wise predictions are derived from multiple visual modalities captured by diverse sensors. Recently, the large vision model, Segment Anything Model 2 (SAM2), has shown strong zero-shot segmentation performance on both images and videos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be adapted to multi-modal data? 2. How can SAM2 better understand semantics? Inspired by cross-frame correlation in videos, we propose to treat multi-modal data as a sequence of frames representing the same scene. Our key idea is to â€˜â€™memorizeâ€™â€™ the modality-agnostic information and â€˜memorizeâ€™ the semantics related to the targeted scene. To achieve this, we apply SAM2â€™s memory mechanisms across multi-modal data to capture modality-agnostic features. Meanwhile, to memorize the semantic knowledge, we propose a training-only Semantic Prototype Memory Module (SPMM) to store category-level prototypes across training for facilitating SAM2â€™s transition from instance to semantic segmentation. A prototypical adaptation loss is imposed between global and local prototypes iteratively to align and refine SAM2â€™s semantic understanding. Extensive experimental results demonstrate that our proposed MemorySAM outperforms SoTA methods by large margins on both synthetic and real-world benchmarks (65.38% on DELIVER, 52.88% on MCubeS). Source code will be made publicly available. </p>
<blockquote>
<p>ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ï¼ˆMMSSï¼‰ä¸Šï¼Œå…¶ä¸­åƒç´ çº§çš„é¢„æµ‹ç»“æœæ¥æºäºç”±å¤šç§ä¼ æ„Ÿå™¨æ•è·çš„å¤šä¸ªè§†è§‰æ¨¡æ€ã€‚æœ€è¿‘ï¼Œå¤§å‹è§†è§‰æ¨¡å‹Segment Anything Model 2ï¼ˆSAM2ï¼‰åœ¨å›¾åƒå’Œè§†é¢‘ä¸Šéƒ½è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚å½“å°†SAM2æ‰©å±•åˆ°MMSSæ—¶ï¼Œå‡ºç°äº†ä¸¤ä¸ªé—®é¢˜ï¼š1. å¦‚ä½•ä½¿SAM2é€‚åº”å¤šæ¨¡æ€æ•°æ®ï¼Ÿ2. å¦‚ä½•ä½¿SAM2æ›´å¥½åœ°ç†è§£è¯­ä¹‰ï¼Ÿå—è§†é¢‘ä¸­è·¨å¸§å…³è”çš„å¯å‘ï¼Œæˆ‘ä»¬æè®®å°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºè¡¨ç¤ºåŒä¸€åœºæ™¯çš„å¸§åºåˆ—ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œè®°å¿†â€ä¸æ¨¡æ€æ— å…³çš„ä¿¡æ¯å’Œä¸ç›®æ ‡åœºæ™¯ç›¸å…³çš„è¯­ä¹‰ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åº”ç”¨äº†SAM2çš„è®°å¿†æœºåˆ¶æ¥æ•è·å¤šæ¨¡æ€æ•°æ®çš„ä¸æ¨¡æ€æ— å…³çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œä¸ºäº†è®°å¿†è¯­ä¹‰çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä»…ç”¨äºè®­ç»ƒçš„è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜å‚¨ç±»åˆ«çº§åˆ«çš„åŸå‹ï¼Œä»è€Œæœ‰åŠ©äºSAM2ä»å®ä¾‹åˆ°è¯­ä¹‰åˆ†å‰²çš„è¿‡æ¸¡ã€‚é€šè¿‡å…¨å±€å’Œå±€éƒ¨åŸå‹ä¹‹é—´è¿­ä»£æ–½åŠ åŸå‹é€‚åº”æŸå¤±ï¼Œä»¥å¯¹é½å’Œç»†åŒ–SAM2çš„è¯­ä¹‰ç†è§£ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºMemorySAMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆåœ¨DELIVERä¸Šè¾¾åˆ°65.38%ï¼Œåœ¨MCubeSä¸Šè¾¾åˆ°52.88%ï¼‰ä¸Šçš„æ€§èƒ½å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06700v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ‘˜è¦ä¸»è¦è®¨è®ºäº†åŸºäºå¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²çš„ç ”ç©¶ã€‚é€šè¿‡å°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºåœºæ™¯çš„ä¸€ç³»åˆ—å¸§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨SAM2æ¨¡å‹è·¨å¤šæ¨¡æ€æ•°æ®æ•è·æ¨¡æ€æ— å…³ç‰¹å¾çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰ä»¥å¢å¼ºæ¨¡å‹ä»å®ä¾‹åˆ†å‰²åˆ°è¯­ä¹‰åˆ†å‰²çš„è½¬æ¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é›†ä¸­äºå¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ï¼ˆMMSSï¼‰ï¼Œå¤„ç†æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨æ•æ‰çš„å¤šç§è§†è§‰æ¨¡æ€çš„åƒç´ çº§é¢„æµ‹ã€‚</li>
<li>Segment Anything Model 2 (SAM2) åœ¨å›¾åƒå’Œè§†é¢‘ä¸Šå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å°†SAM2æ‰©å±•åˆ°MMSSé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¦‚ä½•é€‚åº”å¤šæ¨¡æ€æ•°æ®å’Œå¦‚ä½•æ›´å¥½åœ°ç†è§£è¯­ä¹‰ã€‚</li>
<li>æå‡ºå°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºåœºæ™¯çš„ä¸€ç³»åˆ—å¸§ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æ•è·æ¨¡æ€æ— å…³ç‰¹å¾ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰ï¼Œç”¨äºå­˜å‚¨ç±»åˆ«çº§åˆ«çš„åŸå‹ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›SAM2ä»å®ä¾‹åˆ†å‰²åˆ°è¯­ä¹‰åˆ†å‰²çš„è½¬å˜ã€‚</li>
<li>é€šè¿‡åŸå‹é€‚åº”æŸå¤±æ¥è¿­ä»£åœ°å¯¹é½å’Œç²¾ç‚¼SAM2çš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­å‡å¤§å¹…è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3922c717cc24581087f65b886fef08dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a970d2bc1b88dc0e20af6b0599dfd5f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25ba7b3c2479ede9099cc4b4126c3b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb57a241bd65b551df2a319affaa418.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77565cdca5e9f37133fd9cbc95b846d5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Omnidirectional-Multi-Object-Tracking"><a href="#Omnidirectional-Multi-Object-Tracking" class="headerlink" title="Omnidirectional Multi-Object Tracking"></a>Omnidirectional Multi-Object Tracking</h2><p><strong>Authors:Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang</strong></p>
<p>Panoramic imagery, with its 360{\deg} field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in panoramic field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack datasetâ€“a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The established dataset and source code are available at <a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>. </p>
<blockquote>
<p>å…¨æ™¯å½±åƒå…·æœ‰360Â°çš„è§†é‡ï¼Œèƒ½å¤Ÿæä¾›å…¨é¢çš„ä¿¡æ¯ï¼Œä»¥æ”¯æŒå¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰æ•æ‰å‘¨å›´ç‰©ä½“çš„æ—¶ç©ºå…³ç³»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MOTç®—æ³•éƒ½æ˜¯é’ˆå¯¹è§†é‡æœ‰é™çš„é’ˆå­”å›¾åƒè€Œå®šåˆ¶çš„ï¼Œå®ƒä»¬åœ¨å…¨æ™¯è®¾ç½®ä¸­çš„æ•ˆæœå—åˆ°é™åˆ¶ã€‚æ­¤å¤–ï¼Œå…¨æ™¯å›¾åƒå¤±çœŸï¼Œå¦‚åˆ†è¾¨ç‡æŸå¤±ã€å‡ ä½•å˜å½¢å’Œå…‰çº¿ä¸å‡åŒ€ï¼Œé˜»ç¢äº†ç°æœ‰MOTæ–¹æ³•çš„ç›´æ¥é€‚åº”ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniTrackï¼Œä¸€ä¸ªå…¨å‘MOTæ¡†æ¶ï¼Œå®ƒç»“åˆäº†è½¨è¿¹ç®¡ç†ä»¥å¼•å…¥æ—¶é—´çº¿ç´¢ï¼ŒFlexiTrackå®ä¾‹è¿›è¡Œç›®æ ‡å®šä½å’Œå…³è”ï¼Œä»¥åŠCircularStatEæ¨¡å—ä»¥ç¼“è§£å›¾åƒå’Œå‡ ä½•å¤±çœŸã€‚è¿™ç§æ•´åˆä½¿å¾—å³ä½¿åœ¨å¿«é€Ÿä¼ æ„Ÿå™¨è¿åŠ¨ä¸‹ï¼Œä¹Ÿèƒ½åœ¨å…¨æ™¯è§†é‡åœºæ™¯ä¸­è¿›è¡Œè·Ÿè¸ªã€‚ä¸ºäº†ç¼“è§£å…¨æ™¯MOTæ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†QuadTrackæ•°æ®é›†â€”â€”ä¸€ä¸ªç”±å››è¶³æœºå™¨äººæ”¶é›†çš„å…¨æ–¹ä½å…¨æ™¯æ•°æ®é›†ï¼Œå…·æœ‰å„ç§æŒ‘æˆ˜ï¼Œå¦‚å…¨æ™¯è§†é‡ã€å‰§çƒˆè¿åŠ¨å’Œå¤æ‚ç¯å¢ƒã€‚åœ¨å…¬å…±JRDBæ•°æ®é›†å’Œæ–°å¼•å…¥çš„QuadTrackåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚OmniTrackåœ¨JRDBä¸Šçš„HOTAå¾—åˆ†ä¸º26.92%ï¼Œæé«˜äº†3.43%ï¼Œå¹¶åœ¨QuadTrackä¸Šå®ç°äº†23.45%ï¼Œè¶…å‡ºåŸºçº¿6.81%ã€‚å¯ç”¨æ•°æ®é›†å’Œæºä»£ç è®¿é—®åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack%E3%80%82">https://github.com/xifen523/OmniTrackã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04565v2">PDF</a> Accepted to CVPR 2025. The established dataset and source code are   available at <a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a></p>
<p><strong>Summary</strong>ï¼šå…¨æ™¯æˆåƒå…·æœ‰360Â°è§†é‡ï¼Œå¯ä¸ºå¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰æä¾›å…¨é¢çš„ä¿¡æ¯æ”¯æŒï¼Œæ•æ‰å‘¨å›´ç‰©ä½“çš„ç©ºé—´å’Œæ—¶é—´å…³ç³»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MOTç®—æ³•é’ˆå¯¹è§†é‡æœ‰é™çš„é’ˆå­”å›¾åƒè¿›è¡Œè®¾è®¡ï¼Œåœ¨å…¨æ™¯è®¾ç½®ä¸­çš„æ•ˆæœæœ‰é™ã€‚æ­¤å¤–ï¼Œå…¨æ™¯å›¾åƒå¤±çœŸï¼Œå¦‚åˆ†è¾¨ç‡æŸå¤±ã€å‡ ä½•å˜å½¢å’Œå…‰ç…§ä¸å‡ï¼Œé˜»ç¢äº†ç°æœ‰MOTæ–¹æ³•çš„ç›´æ¥åº”ç”¨ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†OmniTrackï¼Œä¸€ä¸ªå…¨æ–¹ä½çš„å¤šç›®æ ‡è·Ÿè¸ªæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆè½¨è¿¹ç®¡ç†æ¥å¼•å…¥æ—¶é—´çº¿ç´¢ã€FlexiTrackå®ä¾‹è¿›è¡Œç›®æ ‡å®šä½å’Œå…³è”ï¼Œä»¥åŠCircularStatEæ¨¡å—æ¥ç¼“è§£å›¾åƒå’Œå‡ ä½•å¤±çœŸã€‚é›†æˆä½¿å¾—åœ¨å…¨æ™¯è§†é‡åœºæ™¯ä¸­å®ç°è·Ÿè¸ªæˆä¸ºå¯èƒ½ï¼Œå³ä½¿åœ¨ä¼ æ„Ÿå™¨å¿«é€Ÿè¿åŠ¨çš„æƒ…å†µä¸‹äº¦æ˜¯å¦‚æ­¤ã€‚ä¸ºç¼“è§£å…¨æ™¯MOTæ•°æ®é›†çš„ç¼ºä¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†QuadTrackæ•°æ®é›†â€”â€”ä¸€ä¸ªç”±å››è¶³æœºå™¨äººæ”¶é›†çš„å…¨æ–¹ä½æ•°æ®é›†ï¼Œå…·æœ‰å…¨æ™¯è§†é‡ã€å¼ºçƒˆè¿åŠ¨å’Œå¤æ‚ç¯å¢ƒç­‰å¤šæ ·åŒ–æŒ‘æˆ˜ã€‚åœ¨å…¬å…±JRDBæ•°æ®é›†å’Œæ–°å¼•å…¥çš„QuadTrackåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚OmniTrackåœ¨JRDBä¸Šçš„HOTAå¾—åˆ†ç‡ä¸º26.92%ï¼Œæ¯”åŸºçº¿æé«˜äº†3.43%ï¼Œå¹¶åœ¨QuadTrackä¸Šå®ç°äº†23.45%çš„å¾—åˆ†ç‡ï¼Œè¶…è¿‡äº†åŸºçº¿6.81%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Panoramic imagery offers comprehensive information for Multi-Object Tracking (MOT) due to its 360Â° field of view.</li>
<li>Most MOT algorithms are tailored for pinhole images, limiting their effectiveness in panoramic settings.</li>
<li>Panoramic image distortions, such as resolution loss and uneven lighting, hinder the direct application of existing MOT methods.</li>
<li>OmniTrack is an omnidirectional MOT framework that integrates tracklet management, FlexiTrack Instances, and the CircularStatE Module to address challenges in panoramic tracking.</li>
<li>OmniTrack achieves state-of-the-art performance on both the public JRDB dataset and the newly introduced QuadTrack benchmark.</li>
<li>The QuadTrack dataset is a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic views, intense motion, and complex environments.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e42121e130fc6b8e6886c69d57fabae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ac3547f8b332a1c53fcb7581cf2c551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8dc012b97fd1a253f06f08c191be1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-722872c4dc59c4c90dbcf6df51f4037d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12f8ac091275077b655ea23f2111612a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RaCFormer-Towards-High-Quality-3D-Object-Detection-via-Query-based-Radar-Camera-Fusion"><a href="#RaCFormer-Towards-High-Quality-3D-Object-Detection-via-Query-based-Radar-Camera-Fusion" class="headerlink" title="RaCFormer: Towards High-Quality 3D Object Detection via Query-based   Radar-Camera Fusion"></a>RaCFormer: Towards High-Quality 3D Object Detection via Query-based   Radar-Camera Fusion</h2><p><strong>Authors:Xiaomeng Chu, Jiajun Deng, Guoliang You, Yifan Duan, Houqiang Li, Yanyong Zhang</strong></p>
<p>We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy of 3D object detection by the following insight. The Radar-Camera fusion in outdoor 3D scene perception is capped by the image-to-BEV transformationâ€“if the depth of pixels is not accurately estimated, the naive combination of BEV features actually integrates unaligned visual content. To avoid this problem, we propose a query-based framework that enables adaptive sampling of instance-relevant features from both the birdâ€™s-eye view (BEV) and the original image view. Furthermore, we enhance system performance by two key designs: optimizing query initialization and strengthening the representational capacity of BEV. For the former, we introduce an adaptive circular distribution in polar coordinates to refine the initialization of object queries, allowing for a distance-based adjustment of query density. For the latter, we initially incorporate a radar-guided depth head to refine the transformation from image view to BEV. Subsequently, we focus on leveraging the Doppler effect of radar and introduce an implicit dynamic catcher to capture the temporal elements within the BEV. Extensive experiments on nuScenes and View-of-Delft (VoD) datasets validate the merits of our design. Remarkably, our method achieves superior results of 64.9% mAP and 70.2% NDS on nuScenes. RaCFormer also secures the state-of-the-art performance on the VoD dataset. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cxmomo/RaCFormer">https://github.com/cxmomo/RaCFormer</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†é›·è¾¾æ‘„åƒå¤´èåˆå˜å‹å™¨ï¼ˆRaCFormerï¼‰æ¥æé«˜æˆ·å¤–ä¸‰ç»´åœºæ™¯æ„ŸçŸ¥ä¸­çš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹ç²¾åº¦ã€‚é€šè¿‡å¯¹é›·è¾¾æ‘„åƒå¤´èåˆçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒåˆ°é¸Ÿç°å›¾è½¬æ¢æ˜¯é™åˆ¶é›·è¾¾æ‘„åƒå¤´èåˆçš„å…³é”®å› ç´ â€”â€”å¦‚æœåƒç´ æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®ï¼Œç®€å•çš„é¸Ÿç°å›¾ç‰¹å¾ç»„åˆå®é™…ä¸Šä¼šé›†æˆæœªå¯¹é½çš„è§†è§‰å†…å®¹ã€‚ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŸ¥è¯¢çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»é¸Ÿç°å›¾å’ŒåŸå§‹å›¾åƒè§†å›¾ä¸­è‡ªé€‚åº”é‡‡æ ·ä¸å®ä¾‹ç›¸å…³çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªå…³é”®è®¾è®¡æé«˜äº†ç³»ç»Ÿæ€§èƒ½ï¼šä¼˜åŒ–æŸ¥è¯¢åˆå§‹åŒ–å’Œå¢å¼ºé¸Ÿç°å›¾çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å¯¹äºå‰è€…ï¼Œæˆ‘ä»¬åœ¨æåæ ‡ç³»ä¸­å¼•å…¥è‡ªé€‚åº”åœ†å½¢åˆ†å¸ƒæ¥ä¼˜åŒ–å¯¹è±¡æŸ¥è¯¢çš„åˆå§‹åŒ–ï¼Œå…è®¸åŸºäºè·ç¦»çš„æŸ¥è¯¢å¯†åº¦è°ƒæ•´ã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬æœ€åˆå¼•å…¥é›·è¾¾å¼•å¯¼çš„æ·±åº¦å¤´æ¥æ”¹è¿›ä»å›¾åƒè§†å›¾åˆ°é¸Ÿç°å›¾çš„è½¬æ¢ã€‚éšåï¼Œæˆ‘ä»¬ä¸“æ³¨äºåˆ©ç”¨é›·è¾¾çš„å¤šæ™®å‹’æ•ˆåº”ï¼Œå¹¶å¼•å…¥éšå¼åŠ¨æ€æ•è·å™¨æ¥æ•è·é¸Ÿç°å›¾å†…çš„æ—¶é—´å…ƒç´ ã€‚åœ¨nuSceneså’ŒView-of-Delftï¼ˆVoDï¼‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬è®¾è®¡çš„ä¼˜ç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†64.9%çš„mAPå’Œ70.2%çš„NDSï¼Œè¾¾åˆ°å“è¶Šç»“æœã€‚RaCFormeråœ¨VoDæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cxmomo/RaCFormer">https://github.com/cxmomo/RaCFormer</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12725v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>é›·è¾¾ç›¸æœºèåˆTransformerï¼ˆRaCFormerï¼‰é€šè¿‡è‡ªé€‚åº”é‡‡æ ·é¸Ÿç°å›¾å’ŒåŸå§‹å›¾åƒä¸­çš„å®ä¾‹ç›¸å…³ç‰¹å¾ï¼Œæé«˜äº†é›·è¾¾ç›¸æœºèåˆåœ¨æˆ·å¤–ä¸‰ç»´åœºæ™¯æ„ŸçŸ¥ä¸­çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡ä¼˜åŒ–æŸ¥è¯¢åˆå§‹åŒ–å’Œå¢å¼ºé¸Ÿç°å›¾çš„è¡¨å¾å®¹é‡ä¸¤ä¸ªå…³é”®è®¾è®¡æ¥å¢å¼ºç³»ç»Ÿæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨nuSceneså’ŒVoDæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RaCFormeråˆ©ç”¨é›·è¾¾ç›¸æœºèåˆæé«˜ä¸‰ç»´ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºæŸ¥è¯¢çš„æ¡†æ¶æ¥é¿å…æ·±åº¦åƒç´ ä¼°è®¡ä¸å‡†ç¡®å¯¼è‡´çš„é—®é¢˜ï¼Œé€šè¿‡è‡ªé€‚åº”é‡‡æ ·é¸Ÿç°å›¾å’ŒåŸå§‹å›¾åƒä¸­çš„å®ä¾‹ç›¸å…³ç‰¹å¾ã€‚</li>
<li>ä¼˜åŒ–æŸ¥è¯¢åˆå§‹åŒ–ï¼Œé€šè¿‡å¼•å…¥è‡ªé€‚åº”åœ†å½¢åˆ†å¸ƒæ¥ç»†åŒ–å¯¹è±¡æŸ¥è¯¢çš„åˆå§‹åŒ–ã€‚</li>
<li>å¢å¼ºé¸Ÿç°å›¾çš„è¡¨å¾å®¹é‡ï¼Œé€šè¿‡å¼•å…¥é›·è¾¾å¼•å¯¼çš„æ·±åº¦å¤´å’Œåˆ©ç”¨é›·è¾¾çš„Doppleræ•ˆåº”ã€‚</li>
<li>åœ¨nuSceneså’ŒVoDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>RaCFormeråœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†64.9%çš„mAPå’Œ70.2%çš„NDSï¼Œè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaed78dda205d4a66f27f2e098481c78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1031492bf274fbca539d57599dc610d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a6044ff8f7bdb2c12cf818924e110d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd06d3903f9464e75f3fcd1c1af8c0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d4bed42cfabcabfc543b312539ccca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e35f7addefd3c109fcc75c57fdd3520d.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Face Spoofing Detection using Deep Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1b9aeb002c5dca7ac7509bd575c42d14.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Scaling Vision Pre-Training to 4K Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
