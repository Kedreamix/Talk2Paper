<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  PartRM Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-952d00c55fe995e0feab67eb01a3ecc7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="PartRM-Modeling-Part-Level-Dynamics-with-Large-Cross-State-Reconstruction-Model"><a href="#PartRM-Modeling-Part-Level-Dynamics-with-Large-Cross-State-Reconstruction-Model" class="headerlink" title="PartRM: Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model"></a>PartRM: Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model</h2><p><strong>Authors:Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, Hao Zhao</strong></p>
<p>As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the modelâ€™s understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research. </p>
<blockquote>
<p>éšç€ä¸–ç•Œæ¨¡å‹ï¼ˆå³ä»å½“å‰è§‚æµ‹å’Œè¡ŒåŠ¨ä¸­é¢„æµ‹æœªæ¥çŠ¶æ€ï¼‰çš„å…´è¶£å¢é•¿ï¼Œå¯¹éƒ¨åˆ†çº§åˆ«çš„åŠ¨æ€è¿›è¡Œå‡†ç¡®å»ºæ¨¡å¯¹äºå„ç§åº”ç”¨æ¥è¯´å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚Puppet-Masterï¼‰ä¾èµ–äºå¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¾®è°ƒï¼Œä½†ç”±äº2Dè§†é¢‘è¡¨ç¤ºçš„å±€é™æ€§å’Œç¼“æ…¢çš„å¤„ç†æ—¶é—´ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PartRMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„4Dé‡å»ºæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶å¯¹é™æ€å¯¹è±¡çš„å¤šè§†å›¾å›¾åƒè¿›è¡Œå¤–è§‚ã€å‡ ä½•å’Œé›¶ä»¶çº§åˆ«çš„è¿åŠ¨å»ºæ¨¡ã€‚PartRMå»ºç«‹åœ¨å¤§å‹3Dé«˜æ–¯é‡å»ºæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å…¶å¯¹é™æ€å¯¹è±¡çš„å¤–è§‚å’Œå‡ ä½•çš„ä¸°å¯ŒçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³4Dä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PartDrag-4Dæ•°æ®é›†ï¼Œæä¾›äº†è¶…è¿‡2ä¸‡ä¸ªçŠ¶æ€çš„é›¶ä»¶çº§åˆ«è¿åŠ¨çš„å¤šä¸ªè§†å›¾è§‚å¯Ÿã€‚æˆ‘ä»¬é€šè¿‡å¤šå°ºåº¦é˜»åŠ›åµŒå…¥æ¨¡å—å¢å¼ºäº†æ¨¡å‹å¯¹äº¤äº’æ¡ä»¶çš„ç†è§£ï¼Œè¯¥æ¨¡å—ä»¥ä¸åŒçš„ç²’åº¦æ•è·åŠ¨æ€ã€‚ä¸ºäº†é˜²æ­¢å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Œæˆ‘ä»¬å®æ–½äº†åˆ†é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æŒ‰é¡ºåºä¸“æ³¨äºè¿åŠ¨å’Œå¤–è§‚å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartRMåœ¨é›¶ä»¶çº§åˆ«è¿åŠ¨å­¦ä¹ æ–¹é¢æ ‘ç«‹äº†æœ€æ–°æŠ€æœ¯æ ‡æ†ï¼Œå¹¶å¯åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€æä¾›ï¼Œä»¥æ–¹ä¾¿æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19913v1">PDF</a> Accepted to CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://partrm.c7w.tech/">https://partrm.c7w.tech/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹4Dé‡å»ºæ¡†æ¶PartRMï¼Œç”¨äºä»å¤šè§†è§’å›¾åƒå¯¹é™æ€ç‰©ä½“çš„å¤–è§‚ã€å‡ ä½•å½¢çŠ¶å’Œéƒ¨åˆ†çº§åˆ«è¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºå…‹æœ4Dæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†PartDrag-4Dæ•°æ®é›†ï¼Œæä¾›è¶…è¿‡20,000ç§çŠ¶æ€ä¸‹çš„éƒ¨åˆ†çº§åˆ«è¿åŠ¨çš„å¤šè§†è§’è§‚å¯Ÿã€‚é€šè¿‡å¤šå°ºåº¦é˜»åŠ›åµŒå…¥æ¨¡å—å¢å¼ºæ¨¡å‹å¯¹äº¤äº’æ¡ä»¶çš„äº†è§£ï¼Œæ•è·ä¸åŒç²’åº¦çš„åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartRMåœ¨éƒ¨åˆ†çº§åˆ«è¿åŠ¨å­¦ä¹ ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¯åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartRMæ˜¯ä¸€ä¸ªæ–°å‹çš„4Dé‡å»ºæ¡†æ¶ï¼Œç”¨äºå¯¹é™æ€ç‰©ä½“çš„éƒ¨åˆ†çº§åˆ«è¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>å®ƒå»ºç«‹åœ¨å¤§å‹3Dé«˜æ–¯é‡å»ºæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å…¶åœ¨é™æ€ç‰©ä½“çš„å¤–è§‚å’Œå‡ ä½•çŸ¥è¯†ã€‚</li>
<li>PartRMå¼•å…¥äº†PartDrag-4Dæ•°æ®é›†ï¼Œç”¨äºè§£å†³4Dæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>å¤šå°ºåº¦é˜»åŠ›åµŒå…¥æ¨¡å—å¢å¼ºäº†æ¨¡å‹å¯¹äº¤äº’æ¡ä»¶çš„äº†è§£ï¼Œèƒ½å¤Ÿæ•è·ä¸åŒç²’åº¦çš„åŠ¨æ€ã€‚</li>
<li>ç ”ç©¶äººå‘˜é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œè¯¥è¿‡ç¨‹é¡ºåºå…³æ³¨è¿åŠ¨å’Œå¤–è§‚å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPartRMåœ¨éƒ¨åˆ†çº§åˆ«è¿åŠ¨å­¦ä¹ ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¯åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9706c6510ed019d9cad45c87e2574685.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1509a3a728a2593bd03227c6f13fb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a63c4a8c14d965019fcee0f13d83257.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8d1cdab8d95e01d117e10b4737dc855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2681fd3f36a17d890f2af1e70f8a1dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32665bbd78e4d445e82050bf460cbc3f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GaussianUDF-Inferring-Unsigned-Distance-Functions-through-3D-Gaussian-Splatting"><a href="#GaussianUDF-Inferring-Unsigned-Distance-Functions-through-3D-Gaussian-Splatting" class="headerlink" title="GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian   Splatting"></a>GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian   Splatting</h2><p><strong>Authors:Shujuan Li, Yu-Shen Liu, Zhizhong Han</strong></p>
<p>Reconstructing open surfaces from multi-view images is vital in digitalizing complex objects in daily life. A widely used strategy is to learn unsigned distance functions (UDFs) by checking if their appearance conforms to the image observations through neural rendering. However, it is still hard to learn continuous and implicit UDF representations through 3D Gaussians splatting (3DGS) due to the discrete and explicit scene representation, i.e., 3D Gaussians. To resolve this issue, we propose a novel approach to bridge the gap between 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D Gaussian planes on surfaces, and then, leverage the self-supervision and gradient-based inference to supervise unsigned distances in both near and far area to surfaces. To this end, we introduce novel constraints and strategies to constrain the learning of 2D Gaussians to pursue more stable optimization and more reliable self-supervision, addressing the challenges brought by complicated gradient field on or near the zero level set of UDFs. We report numerical and visual comparisons with the state-of-the-art on widely used benchmarks and real data to show our advantages in terms of accuracy, efficiency, completeness, and sharpness of reconstructed open surfaces with boundaries. Project page: <a target="_blank" rel="noopener" href="https://lisj575.github.io/GaussianUDF/">https://lisj575.github.io/GaussianUDF/</a> </p>
<blockquote>
<p>ä»å¤šè§†è§’å›¾åƒé‡å»ºå¼€æ”¾è¡¨é¢å¯¹äºæ•°å­—åŒ–æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¤æ‚çš„ç‰©ä½“éå¸¸é‡è¦ã€‚ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç­–ç•¥æ˜¯é€šè¿‡ç¥ç»æ¸²æŸ“å­¦ä¹ æ— ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆUDFsï¼‰ï¼Œæ£€æŸ¥å…¶å¤–è§‚æ˜¯å¦ç¬¦åˆå›¾åƒè§‚å¯Ÿã€‚ç„¶è€Œï¼Œç”±äºç¦»æ•£å’Œæ˜ç¡®çš„åœºæ™¯è¡¨ç¤ºï¼Œå³3Dé«˜æ–¯ï¼Œé€šè¿‡3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰å­¦ä¹ è¿ç»­å’Œéšå¼çš„UDFè¡¨ç¤ºä»ç„¶å¾ˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥å¼¥åˆ3Dé«˜æ–¯å’ŒUDFsä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è¡¨é¢ä¸Šæ‹Ÿåˆè–„è€Œå¹³çš„2Dé«˜æ–¯å¹³é¢ï¼Œç„¶ååˆ©ç”¨è‡ªç›‘ç£å’ŒåŸºäºæ¢¯åº¦çš„æ¨ç†æ¥ç›‘ç£è¡¨é¢è¿œè¿‘åŒºåŸŸçš„æ— ç¬¦å·è·ç¦»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„çº¦æŸå’Œç­–ç•¥ï¼Œä»¥çº¦æŸ2Dé«˜æ–¯çš„å­¦ä¹ ï¼Œä»¥è¿½æ±‚æ›´ç¨³å®šçš„ä¼˜åŒ–å’Œæ›´å¯é çš„è‡ªæˆ‘ç›‘ç£ï¼Œè§£å†³ç”±UDFçš„é›¶æ°´å¹³é›†ä¸Šæˆ–å…¶é™„è¿‘çš„å¤æ‚æ¢¯åº¦åœºå¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•å’ŒçœŸå®æ•°æ®ä¸Šä¸æœ€å…ˆè¿›æŠ€æœ¯è¿›è¡Œäº†æ•°å€¼å’Œè§†è§‰æ¯”è¾ƒï¼Œä»¥å±•ç¤ºæˆ‘ä»¬åœ¨é‡å»ºå¸¦æœ‰è¾¹ç•Œçš„å¼€æ”¾è¡¨é¢çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lisj575.github.io/GaussianUDF/">https://lisj575.github.io/GaussianUDF/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19458v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯å­¦ä¹ æ— ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆUDFsï¼‰æ¥é‡å»ºå¤šè§†è§’å›¾åƒä¸­çš„å¼€æ”¾è¡¨é¢çš„é‡è¦æ€§ã€‚é’ˆå¯¹é€šè¿‡ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰å­¦ä¹ è¿ç»­éšå¼UDFè¡¨ç¤ºæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥å¼¥åˆä¸‰ç»´é«˜æ–¯ä¸UDFsä¹‹é—´çš„å·®è·ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‡åº¦æ‹Ÿåˆè¡¨é¢ä¸Šçš„è–„å¹³é¢äºŒç»´é«˜æ–¯ï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£ä¸åŸºäºæ¢¯åº¦çš„æ¨æ–­æ¥ç›‘ç£è¿‘è¿œåŒºåŸŸçš„æ— ç¬¦å·è·ç¦»ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æ–°çš„çº¦æŸå’Œç­–ç•¥æ¥çº¦æŸäºŒç»´é«˜æ–¯çš„å­¦ä¹ ï¼Œä»¥è¿½æ±‚æ›´ç¨³å®šçš„ä¼˜åŒ–å’Œæ›´å¯é çš„è‡ªæˆ‘ç›‘ç£ï¼Œè§£å†³UDFé›¶æ°´å¹³é›†ä¸Šæˆ–é™„è¿‘å¤æ‚æ¢¯åº¦åœºå¸¦æ¥çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡å»ºå¤šè§†è§’å›¾åƒä¸­çš„å¼€æ”¾è¡¨é¢æ˜¯æ•°å­—åŒ–æ—¥å¸¸ç”Ÿæ´»ä¸­å¤æ‚å¯¹è±¡çš„å…³é”®ã€‚</li>
<li>æ— ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆUDFsï¼‰æ˜¯é€šè¿‡ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯å­¦ä¹ çš„å…³é”®ã€‚</li>
<li>ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰å­¦ä¹ è¿ç»­éšå¼UDFè¡¨ç¤ºé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥å¼¥åˆä¸‰ç»´é«˜æ–¯ä¸UDFsä¹‹é—´çš„å·®è·ã€‚</li>
<li>æ–¹æ³•é€šè¿‡è¿‡åº¦æ‹Ÿåˆè¡¨é¢ä¸Šçš„è–„å¹³é¢äºŒç»´é«˜æ–¯ï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£ä¸åŸºäºæ¢¯åº¦çš„æ¨æ–­æ¥ç›‘ç£æ— ç¬¦å·è·ç¦»ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„çº¦æŸå’Œç­–ç•¥ä»¥è¿½æ±‚æ›´ç¨³å®šçš„ä¼˜åŒ–å’Œæ›´å¯é çš„è‡ªæˆ‘ç›‘ç£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-855e5b5686b7b439f950671bd46280aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e68c8d17d199ad21719d20409170f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bef2f80fd641a2f48c3d5ba353db88d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3e1fac9d4d0b6591bde7afea00c6a3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e60a7bf2f71e2830401e08444c71eec.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SparseGS-W-Sparse-View-3D-Gaussian-Splatting-in-the-Wild-with-Generative-Priors"><a href="#SparseGS-W-Sparse-View-3D-Gaussian-Splatting-in-the-Wild-with-Generative-Priors" class="headerlink" title="SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with   Generative Priors"></a>SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with   Generative Priors</h2><p><strong>Authors:Yiqing Li, Xuan Wang, Jiawei Wu, Yikun Ma, Zhi Jin</strong></p>
<p>Synthesizing novel views of large-scale scenes from unconstrained in-the-wild images is an important but challenging task in computer vision. Existing methods, which optimize per-image appearance and transient occlusion through implicit neural networks from dense training views (approximately 1000 images), struggle to perform effectively under sparse input conditions, resulting in noticeable artifacts. To this end, we propose SparseGS-W, a novel framework based on 3D Gaussian Splatting that enables the reconstruction of complex outdoor scenes and handles occlusions and appearance changes with as few as five training images. We leverage geometric priors and constrained diffusion priors to compensate for the lack of multi-view information from extremely sparse input. Specifically, we propose a plug-and-play Constrained Novel-View Enhancement module to iteratively improve the quality of rendered novel views during the Gaussian optimization process. Furthermore, we propose an Occlusion Handling module, which flexibly removes occlusions utilizing the inherent high-quality inpainting capability of constrained diffusion priors. Both modules are capable of extracting appearance features from any user-provided reference image, enabling flexible modeling of illumination-consistent scenes. Extensive experiments on the PhotoTourism and Tanks and Temples datasets demonstrate that SparseGS-W achieves state-of-the-art performance not only in full-reference metrics, but also in commonly used non-reference metrics such as FID, ClipIQA, and MUSIQ. </p>
<blockquote>
<p>ä»ä¸å—çº¦æŸçš„é‡å¤–å›¾åƒåˆæˆå¤§è§„æ¨¡åœºæ™¯çš„æ–°è§†è§’æ˜¯ä¸€é¡¹è®¡ç®—æœºè§†è§‰ä¸­é‡è¦ä½†æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡éšå¼ç¥ç»ç½‘ç»œä»å¯†é›†çš„è®­ç»ƒè§†è§’ï¼ˆçº¦1000å¼ å›¾åƒï¼‰ä¼˜åŒ–æ¯å¼ å›¾åƒçš„å¤–è§‚å’Œç¬æ—¶é®æŒ¡ï¼Œä½†åœ¨ç¨€ç–è¾“å…¥æ¡ä»¶ä¸‹éš¾ä»¥æœ‰æ•ˆæ‰§è¡Œï¼Œå¯¼è‡´æ˜æ˜¾çš„ä¼ªå½±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SparseGS-Wï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº3Dé«˜æ–¯æ‹¼è´´çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿé‡å»ºå¤æ‚çš„å®¤å¤–åœºæ™¯ï¼Œå¹¶ç”¨æœ€å°‘çš„äº”å¼ è®­ç»ƒå›¾åƒå¤„ç†é®æŒ¡å’Œå¤–è§‚å˜åŒ–ã€‚æˆ‘ä»¬åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œçº¦æŸæ‰©æ•£å…ˆéªŒæ¥å¼¥è¡¥æç«¯ç¨€ç–è¾“å…¥çš„å¤šè§†è§’ä¿¡æ¯ä¸è¶³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå³æ’å³ç”¨çš„çº¦æŸæ–°è§†è§’å¢å¼ºæ¨¡å—ï¼Œä»¥åœ¨é«˜æ–¯ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿­ä»£æé«˜æ¸²æŸ“æ–°è§†è§’çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé®æŒ¡å¤„ç†æ¨¡å—ï¼Œåˆ©ç”¨çº¦æŸæ‰©æ•£å…ˆéªŒçš„å›ºæœ‰é«˜è´¨é‡ä¿®å¤èƒ½åŠ›çµæ´»åœ°å»é™¤é®æŒ¡ã€‚è¿™ä¸¤ä¸ªæ¨¡å—éƒ½èƒ½å¤Ÿä»ç”¨æˆ·æä¾›çš„ä»»ä½•å‚è€ƒå›¾åƒä¸­æå–å¤–è§‚ç‰¹å¾ï¼Œå®ç°å…‰ç…§ä¸€è‡´åœºæ™¯çš„çµæ´»å»ºæ¨¡ã€‚åœ¨PhotoTourismå’ŒTanks and Templesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSparseGS-Wä¸ä»…åœ¨å…¨å‚è€ƒæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè€Œä¸”åœ¨å¸¸ç”¨çš„æ— å‚è€ƒæŒ‡æ ‡ï¼ˆå¦‚FIDã€ClipIQAå’ŒMUSIQï¼‰ä¸Šä¹Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼è´´çš„æ–°æ¡†æ¶SparseGS-Wï¼Œç”¨äºä»ç¨€ç–è¾“å…¥çš„é‡å¤–å›¾åƒé‡å»ºå¤æ‚å®¤å¤–åœºæ™¯ï¼Œå¹¶å¤„ç†é®æŒ¡å’Œå¤–è§‚å˜åŒ–ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œçº¦æŸæ‰©æ•£å…ˆéªŒæ¥å¼¥è¡¥å¤šè§†è§’ä¿¡æ¯çš„ç¼ºå¤±ï¼Œæå‡ºä¸€ç§å³æ’å³ç”¨çš„çº¦æŸæ–°è§†è§’å¢å¼ºæ¨¡å—å’Œé®æŒ¡å¤„ç†æ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæ”¹è¿›æ¸²æŸ“çš„æ–°è§†è§’è´¨é‡å’Œçµæ´»å»é™¤é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒSparseGS-Wåœ¨PhotoTourismå’ŒTanks and Templesæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ä»…åœ¨å…¨å‚è€ƒæŒ‡æ ‡ä¸Šï¼Œè€Œä¸”åœ¨éå‚è€ƒæŒ‡æ ‡å¦‚FIDã€ClipIQAå’ŒMUSIQä¸Šä¹Ÿè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SparseGS-Wæ˜¯ä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼è´´çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿä»ç¨€ç–è¾“å…¥çš„é‡å¤–å›¾åƒé‡å»ºå¤æ‚å®¤å¤–åœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œçº¦æŸæ‰©æ•£å…ˆéªŒæ¥å¼¥è¡¥ç¼ºä¹å¤šè§†è§’ä¿¡æ¯ã€‚</li>
<li>SparseGS-Wé€šè¿‡æå‡ºçº¦æŸæ–°è§†è§’å¢å¼ºæ¨¡å—ï¼Œæ”¹è¿›æ¸²æŸ“çš„æ–°è§†è§’è´¨é‡ã€‚</li>
<li>é®æŒ¡å¤„ç†æ¨¡å—èƒ½å¤Ÿçµæ´»å»é™¤é®æŒ¡ï¼Œåˆ©ç”¨çº¦æŸæ‰©æ•£å…ˆéªŒçš„å†…åœ¨é«˜è´¨é‡è¡¥å…¨èƒ½åŠ›ã€‚</li>
<li>SparseGS-Wèƒ½å¤Ÿä»ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒä¸­æå–å¤–è§‚ç‰¹å¾ï¼Œå®ç°å…‰ç…§ä¸€è‡´çš„åœºæ™¯å»ºæ¨¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSparseGS-Wåœ¨å…¨å‚è€ƒæŒ‡æ ‡å’Œéå‚è€ƒæŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83f5a461f8da6e3558fce6dcaa09305d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-904278f1c50a4e21a54c64b85a87a273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4478c4029a00b55df4b51656dba1717.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52f730b5f198add1e72826bbd814a912.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="COB-GS-Clear-Object-Boundaries-in-3DGS-Segmentation-Based-on-Boundary-Adaptive-Gaussian-Splitting"><a href="#COB-GS-Clear-Object-Boundaries-in-3DGS-Segmentation-Based-on-Boundary-Adaptive-Gaussian-Splitting" class="headerlink" title="COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on   Boundary-Adaptive Gaussian Splitting"></a>COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on   Boundary-Adaptive Gaussian Splitting</h2><p><strong>Authors:Jiaxin Zhang, Junjun Jiang, Youyu Chen, Kui Jiang, Xianming Liu</strong></p>
<p>Accurate object segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D segmentation based on 3D Gaussian Splatting (3DGS) struggles with accurately delineating object boundaries, as Gaussian primitives often span across object edges due to their inherent volume and the lack of semantic guidance during training. In order to tackle these challenges, we introduce Clear Object Boundaries for 3DGS Segmentation (COB-GS), which aims to improve segmentation accuracy by clearly delineating blurry boundaries of interwoven Gaussian primitives within the scene. Unlike existing approaches that remove ambiguous Gaussians and sacrifice visual quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and visual information, allowing the two different levels to cooperate with each other effectively. Specifically, for the semantic guidance, we introduce a boundary-adaptive Gaussian splitting technique that leverages semantic gradient statistics to identify and split ambiguous Gaussians, aligning them closely with object boundaries. For the visual optimization, we rectify the degraded suboptimal texture of the 3DGS scene, particularly along the refined boundary structures. Experimental results show that COB-GS substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained model, yielding clear boundaries while preserving high visual quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZestfulJX/COB-GS">https://github.com/ZestfulJX/COB-GS</a>. </p>
<blockquote>
<p>åœ¨3Dè§†è§‰é¢†åŸŸï¼Œç²¾ç¡®çš„ç›®æ ‡åˆ†å‰²å¯¹äºé«˜è´¨é‡çš„åœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒåŸºäº3Dé«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰çš„3Dåˆ†å‰²åœ¨å‡†ç¡®æç»˜å¯¹è±¡è¾¹ç•Œæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç”±äºé«˜æ–¯åŸºå…ƒå›ºæœ‰çš„ä½“ç§¯ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹è¯­ä¹‰æŒ‡å¯¼ï¼Œé«˜æ–¯åŸºå…ƒé€šå¸¸è·¨è¶Šå¯¹è±¡è¾¹ç¼˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äº3DGSåˆ†å‰²çš„æ¸…æ™°å¯¹è±¡è¾¹ç•Œï¼ˆCOB-GSï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æ¸…æ™°åœ°æç»˜åœºæ™¯ä¸­äº¤ç»‡é«˜æ–¯åŸºå…ƒçš„æ¨¡ç³Šè¾¹ç•Œæ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¼šç§»é™¤æ¨¡ç³Šçš„é«˜æ–¯åŸºå…ƒå¹¶ç‰ºç‰²è§†è§‰è´¨é‡ï¼Œè€ŒCOB-GSä½œä¸ºä¸€ç§3DGSç»†åŒ–æ–¹æ³•ï¼Œå¯ä»¥è”åˆä¼˜åŒ–è¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯ï¼Œä½¿è¿™ä¸¤ä¸ªä¸åŒå±‚æ¬¡èƒ½å¤Ÿç›¸äº’æœ‰æ•ˆé…åˆã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºè¯­ä¹‰æŒ‡å¯¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç•Œè‡ªé€‚åº”é«˜æ–¯åˆ†è£‚æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨è¯­ä¹‰æ¢¯åº¦ç»Ÿè®¡æ¥è¯†åˆ«å’Œåˆ†è£‚æ¨¡ç³Šçš„é«˜æ–¯åŸºå…ƒï¼Œä½¿å…¶ç´§å¯†å¯¹é½å¯¹è±¡è¾¹ç•Œã€‚å¯¹äºè§†è§‰ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä¿®æ­£äº†3DGSåœºæ™¯çš„é€€åŒ–æ¬¡ä¼˜çº¹ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†çš„è¾¹ç•Œç»“æ„ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOB-GSå¤§å¤§æé«˜äº†åˆ†å‰²ç²¾åº¦å’Œå¯¹é¢„è®­ç»ƒæ¨¡å‹ä¸å‡†ç¡®æ©ç çš„é²æ£’æ€§ï¼Œåœ¨ä¿æŒé«˜è§†è§‰è´¨é‡çš„åŒæ—¶äº§ç”Ÿäº†æ¸…æ™°çš„è¾¹ç•Œã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZestfulJX/COB-GS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZestfulJX/COB-GSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19443v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäº3DGSçš„ç‰©ä½“åˆ†å‰²æ–¹æ³•å¯¹äºå‡†ç¡®çš„ä¸‰ç»´åœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œä½†å…¶å¯¹è±¡è¾¹ç•Œå¾€å¾€ä¸æ¸…æ™°ï¼Œä¸»è¦æ˜¯ç”±äºå…¶å›ºæœ‰ä½“ç§¯ç‰¹æ€§å’Œè®­ç»ƒæœŸé—´ç¼ºä¹è¯­ä¹‰æŒ‡å¯¼ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºCOB-GSçš„æ”¹è¿›æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ˜ç¡®æ¨¡ç³Šè¾¹ç•Œæ¥æ”¹è¿›åˆ†å‰²ç²¾åº¦ï¼ŒåŒºåˆ†åœºæ™¯ä¸­äº¤ç»‡çš„é«˜æ–¯åŸºæœ¬å…ƒç´ ã€‚ä¸ç°æœ‰çš„å»é™¤æ¨¡ç³Šé«˜æ–¯å½±å“å¹¶ç‰ºç‰²è§†è§‰è´¨é‡çš„æ–¹æ³•ä¸åŒï¼ŒCOB-GSä½œä¸º3DGSçš„æ”¹è¿›æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–è¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯ï¼Œä½¿ä¸¤ä¸ªä¸åŒå±‚é¢èƒ½å¤Ÿç›¸äº’æœ‰æ•ˆåä½œã€‚é‡‡ç”¨è¾¹ç•Œè‡ªé€‚åº”é«˜æ–¯åˆ†è£‚æŠ€æœ¯ä¸ºè¯­ä¹‰å¼•å¯¼è¯†åˆ«æ¨¡ç³Šçš„é«˜æ–¯éƒ¨åˆ†ï¼Œå¹¶ä¸å¯¹è±¡è¾¹ç•Œç´§å¯†å¯¹é½ã€‚é’ˆå¯¹è§†è§‰ä¼˜åŒ–é—®é¢˜ï¼Œä¿®å¤äº†è¢«è¯†åˆ«çš„å­æ¨¡å‹ç”Ÿæˆçš„è¾ƒå·®çº¹ç†å’Œä¿®æ”¹åçš„è¾¹ç•Œç»“æ„éƒ¨åˆ†ç­‰æ˜æ˜¾åŒºåŸŸçš„å›¾åƒè´¨é‡ä½ä¸‹é—®é¢˜ã€‚å®éªŒç»“æœè¯å®ï¼Œç›¸å¯¹äºé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ¨¡ç³Šé®ç½©ç»“æœï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—å¢å¼ºåˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œå¹¶åœ¨ä¿æŒé«˜è´¨é‡è§†è§‰çš„åŒæ—¶æ¸…æ™°åœ°å±•ç¤ºè¾¹ç•Œã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å‡†ç¡®çš„å¯¹è±¡åˆ†å‰²å¯¹äºä¸‰ç»´è§†è§‰é¢†åŸŸçš„åœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰çš„ä¸‰ç»´åˆ†å‰²é¢ä¸´éš¾ä»¥å‡†ç¡®æç»˜å¯¹è±¡è¾¹ç•Œçš„é—®é¢˜ã€‚</li>
<li>COB-GSæ–¹æ³•æ—¨åœ¨é€šè¿‡æ˜ç¡®æ¨¡ç³Šè¾¹ç•Œæ”¹è¿›åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>COB-GSåŒºåˆ†åœºæ™¯ä¸­äº¤ç»‡çš„é«˜æ–¯åŸºæœ¬å…ƒç´ ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒCOB-GSè”åˆä¼˜åŒ–è¯­ä¹‰å’Œè§†è§‰ä¿¡æ¯ã€‚</li>
<li>COB-GSé‡‡ç”¨è¾¹ç•Œè‡ªé€‚åº”é«˜æ–¯åˆ†è£‚æŠ€æœ¯è¯†åˆ«æ¨¡ç³Šé«˜æ–¯å¹¶è¿›è¡Œè¯­ä¹‰å¼•å¯¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6c7971bb35771c641a2b4defacddb0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f878c0275e73b41c73f67fc1bf4ced72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-167167f9481b06536ff90e661c39aed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46e73f559acb9e9eed659a66b4664fea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ab6a055996ec89dcd65a7595b66d550.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Sparse-to-Dense-Camera-Relocalization-with-Scene-Specific-Detector-from-Feature-Gaussian-Splatting"><a href="#From-Sparse-to-Dense-Camera-Relocalization-with-Scene-Specific-Detector-from-Feature-Gaussian-Splatting" class="headerlink" title="From Sparse to Dense: Camera Relocalization with Scene-Specific Detector   from Feature Gaussian Splatting"></a>From Sparse to Dense: Camera Relocalization with Scene-Specific Detector   from Feature Gaussian Splatting</h2><p><strong>Authors:Zhiwei Huang, Hailin Yu, Yichun Shentu, Jin Yuan, Guofeng Zhang</strong></p>
<p>This paper presents a novel camera relocalization method, STDLoc, which leverages Feature Gaussian as scene representation. STDLoc is a full relocalization pipeline that can achieve accurate relocalization without relying on any pose prior. Unlike previous coarse-to-fine localization methods that require image retrieval first and then feature matching, we propose a novel sparse-to-dense localization paradigm. Based on this scene representation, we introduce a novel matching-oriented Gaussian sampling strategy and a scene-specific detector to achieve efficient and robust initial pose estimation. Furthermore, based on the initial localization results, we align the query feature map to the Gaussian feature field by dense feature matching to enable accurate localization. The experiments on indoor and outdoor datasets show that STDLoc outperforms current state-of-the-art localization methods in terms of localization accuracy and recall. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç›¸æœºé‡å®šä½æ–¹æ³•STDLocï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç‰¹å¾é«˜æ–¯ä½œä¸ºåœºæ™¯è¡¨ç¤ºã€‚STDLocæ˜¯ä¸€ä¸ªå®Œæ•´çš„é‡å®šä½æµç¨‹ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å§¿æ€å…ˆéªŒå³å¯å®ç°ç²¾ç¡®çš„é‡å®šä½ã€‚ä¸åŒäºä¹‹å‰éœ€è¦ä»ç²—åˆ°ç»†çš„å®šä½æ–¹æ³•ï¼ˆå…ˆå›¾åƒæ£€ç´¢å†è¿›è¡Œç‰¹å¾åŒ¹é…ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»ç¨€ç–åˆ°å¯†é›†çš„å®šä½èŒƒå¼ã€‚åŸºäºè¿™ç§åœºæ™¯è¡¨ç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢å‘åŒ¹é…çš„é«˜æ–¯é‡‡æ ·ç­–ç•¥å’Œä¸€ç§é’ˆå¯¹åœºæ™¯çš„æ¢æµ‹å™¨ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç¨³å¥çš„åˆå§‹å§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒåŸºäºåˆå§‹å®šä½ç»“æœï¼Œæˆ‘ä»¬é€šè¿‡å¯†é›†ç‰¹å¾åŒ¹é…å°†æŸ¥è¯¢ç‰¹å¾å›¾ä¸é«˜æ–¯ç‰¹å¾åœºå¯¹é½ï¼Œä»¥å®ç°ç²¾ç¡®å®šä½ã€‚åœ¨å®¤å†…å¤–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTDLocåœ¨å®šä½ç²¾åº¦å’Œå¬å›ç‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å®šä½æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19358v1">PDF</a> 15 pages, 12 figures, CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç›¸æœºé‡å®šä½æ–¹æ³•STDLocï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç‰¹å¾é«˜æ–¯ä½œä¸ºåœºæ™¯è¡¨ç¤ºã€‚STDLocæ˜¯ä¸€ä¸ªå®Œæ•´çš„é‡å®šä½æµç¨‹ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å§¿æ€å…ˆéªŒå³å¯å®ç°ç²¾ç¡®é‡å®šä½ã€‚ä¸ä¼ ç»Ÿä»ç²—åˆ°ç²¾ç»†çš„å®šä½æ–¹æ³•ä¸åŒï¼Œå®ƒé¦–å…ˆè¿›è¡Œå›¾åƒæ£€ç´¢ï¼Œç„¶åè¿›è¡Œç‰¹å¾åŒ¹é…ï¼Œæå‡ºäº†ç¨€ç–åˆ°å¯†é›†çš„å®šä½èŒƒå¼ã€‚åŸºäºåœºæ™¯è¡¨ç¤ºï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹é¢å‘åŒ¹é…çš„é«˜æ–¯é‡‡æ ·ç­–ç•¥å’Œåœºæ™¯ç‰¹å®šæ£€æµ‹å™¨ï¼Œå®ç°é«˜æ•ˆè€Œç¨³å¥çš„åˆå§‹å§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒåŸºäºåˆå§‹å®šä½ç»“æœï¼Œé€šè¿‡å¯†é›†ç‰¹å¾åŒ¹é…å°†æŸ¥è¯¢ç‰¹å¾å›¾ä¸é«˜æ–¯ç‰¹å¾åœºå¯¹é½ï¼Œä»è€Œå®ç°ç²¾ç¡®å®šä½ã€‚åœ¨å®¤å†…å¤–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTDLocåœ¨å®šä½ç²¾åº¦å’Œå¬å›ç‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å®šä½æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STDLocæ˜¯ä¸€ç§æ–°å‹ç›¸æœºé‡å®šä½æ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾é«˜æ–¯è¿›è¡Œåœºæ™¯è¡¨ç¤ºã€‚</li>
<li>STDLocå®ç°äº†æ— éœ€å§¿æ€å…ˆéªŒçš„ç²¾ç¡®é‡å®šä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨€ç–åˆ°å¯†é›†çš„å®šä½èŒƒå¼ï¼Œä¸åŒäºä¼ ç»Ÿçš„ä»ç²—åˆ°ç²¾ç»†çš„å®šä½æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é¢å‘åŒ¹é…çš„é«˜æ–¯é‡‡æ ·ç­–ç•¥å’Œåœºæ™¯ç‰¹å®šæ£€æµ‹å™¨ï¼Œç”¨äºé«˜æ•ˆä¸”ç¨³å¥çš„åˆå§‹å§¿æ€ä¼°è®¡ã€‚</li>
<li>é€šè¿‡å¯†é›†ç‰¹å¾åŒ¹é…å°†æŸ¥è¯¢ç‰¹å¾å›¾ä¸é«˜æ–¯ç‰¹å¾åœºå¯¹é½ï¼Œå®ç°ç²¾ç¡®å®šä½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTDLocåœ¨å®šä½ç²¾åº¦å’Œå¬å›ç‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å®šä½æ–¹æ³•ã€‚</li>
<li>STDLocå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºæœºå™¨äººå¯¼èˆªã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34f004218dc2cc03d1a3713aab7c570f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bd0b4abe93c393cdf50e849b7436608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb52fb68bb6503fa2ffc380942c67d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8343a420ef2c777c128caaa1d90627c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d9e83467b940fc127e2ab7fc17e7d9d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Divide-and-Conquer-Dual-Hierarchical-Optimization-for-Semantic-4D-Gaussian-Spatting"><a href="#Divide-and-Conquer-Dual-Hierarchical-Optimization-for-Semantic-4D-Gaussian-Spatting" class="headerlink" title="Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D   Gaussian Spatting"></a>Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D   Gaussian Spatting</h2><p><strong>Authors:Zhiying Yan, Yiyuan Liang, Shilv Cai, Tao Zhang, Sheng Zhong, Luxin Yan, Xu Zou</strong></p>
<p>Semantic 4D Gaussians can be used for reconstructing and understanding dynamic scenes, with temporal variations than static scenes. Directly applying static methods to understand dynamic scenes will fail to capture the temporal features. Few works focus on dynamic scene understanding based on Gaussian Splatting, since once the same update strategy is employed for both dynamic and static parts, regardless of the distinction and interaction between Gaussians, significant artifacts and noise appear. We propose Dual-Hierarchical Optimization (DHO), which consists of Hierarchical Gaussian Flow and Hierarchical Gaussian Guidance in a divide-and-conquer manner. The former implements effective division of static and dynamic rendering and features. The latter helps to mitigate the issue of dynamic foreground rendering distortion in textured complex scenes. Extensive experiments show that our method consistently outperforms the baselines on both synthetic and real-world datasets, and supports various downstream tasks. Project Page: <a target="_blank" rel="noopener" href="https://sweety-yan.github.io/DHO">https://sweety-yan.github.io/DHO</a>. </p>
<blockquote>
<p>è¯­ä¹‰å››ç»´é«˜æ–¯æ¨¡å‹å¯ä»¥ç”¨äºé‡å»ºå’Œç†è§£åŠ¨æ€åœºæ™¯ï¼Œç›¸æ¯”é™æ€åœºæ™¯åŒ…å«æ—¶é—´å˜åŒ–å› ç´ ã€‚ç›´æ¥åº”ç”¨é™æ€æ–¹æ³•å»ç†è§£åŠ¨æ€åœºæ™¯å°†æ— æ³•æ•æ‰æ—¶é—´ç‰¹å¾ã€‚å¾ˆå°‘æœ‰å·¥ä½œå…³æ³¨åŸºäºé«˜æ–¯ç‚¹äº‘æŠ€æœ¯çš„åŠ¨æ€åœºæ™¯ç†è§£ï¼Œå› ä¸ºå½“å¯¹åŠ¨æ€å’Œé™æ€éƒ¨åˆ†é‡‡ç”¨ç›¸åŒçš„æ›´æ–°ç­–ç•¥æ—¶ï¼Œæ— è®ºé«˜æ–¯ä¹‹é—´çš„åŒºåˆ«å’Œäº¤äº’å¦‚ä½•ï¼Œä¼šå‡ºç°é‡å¤§ä¼ªå½±å’Œå™ªå£°ã€‚æˆ‘ä»¬æå‡ºäº†åˆ†è€Œæ²»ä¹‹çš„åŒå±‚æ¬¡ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬åˆ†å±‚é«˜æ–¯æµå’Œåˆ†å±‚é«˜æ–¯å¼•å¯¼ã€‚å‰è€…å®ç°äº†é™æ€å’ŒåŠ¨æ€æ¸²æŸ“åŠç‰¹å¾çš„æœ‰æ•ˆåˆ’åˆ†ã€‚åè€…æœ‰åŠ©äºç¼“è§£çº¹ç†å¤æ‚åœºæ™¯ä¸­åŠ¨æ€å‰æ™¯æ¸²æŸ“å¤±çœŸé—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’Œç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶æ”¯æŒå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sweety-yan.github.io/DHO%E3%80%82">https://sweety-yan.github.io/DHOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19332v1">PDF</a> ICME 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­ä¹‰å››ç»´é«˜æ–¯æ¨¡å‹å¯ç”¨äºé‡å»ºå’Œç†è§£åŠ¨æ€åœºæ™¯ï¼Œç›¸æ¯”é™æ€åœºæ™¯æ›´èƒ½æ•æ‰æ—¶é—´å˜åŒ–ç‰¹å¾ã€‚ç›´æ¥åº”ç”¨é™æ€æ–¹æ³•ç†è§£åŠ¨æ€åœºæ™¯æ— æ³•æ•æ‰æ—¶é—´ç‰¹å¾ã€‚é’ˆå¯¹é«˜æ–¯æ˜ å°„åœ¨åŠ¨æ€åœºæ™¯ç†è§£ä¸Šçš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†åŒé‡å±‚æ¬¡ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å±‚æ¬¡é«˜æ–¯æµå’Œå±‚æ¬¡é«˜æ–¯å¼•å¯¼ï¼Œä»¥åˆ†æ²»æ–¹å¼å¤„ç†é™æ€å’ŒåŠ¨æ€æ¸²æŸ“ä¸ç‰¹å¾ã€‚å‰è€…å®ç°äº†é™æ€å’ŒåŠ¨æ€æ¸²æŸ“å’Œç‰¹å¾çš„æœ‰æ•ˆåˆ’åˆ†ï¼Œåè€…æœ‰åŠ©äºå‡è½»å¤æ‚çº¹ç†åœºæ™¯ä¸­åŠ¨æ€å‰æ™¯æ¸²æŸ“å¤±çœŸé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶æ”¯æŒå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰å››ç»´é«˜æ–¯æ¨¡å‹å¯ç”¨äºé‡å»ºå’Œç†è§£åŠ¨æ€åœºæ™¯ã€‚</li>
<li>ç›´æ¥åº”ç”¨é™æ€æ–¹æ³•ç†è§£åŠ¨æ€åœºæ™¯æ— æ³•æ•æ‰æ—¶é—´ç‰¹å¾ã€‚</li>
<li>ç›®å‰å…³äºé«˜æ–¯æ˜ å°„åœ¨åŠ¨æ€åœºæ™¯ç†è§£ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>åŒé‡å±‚æ¬¡ä¼˜åŒ–ï¼ˆDHOï¼‰æ–¹æ³•é€šè¿‡å±‚æ¬¡é«˜æ–¯æµå’Œå±‚æ¬¡é«˜æ–¯å¼•å¯¼å¤„ç†é™æ€å’ŒåŠ¨æ€æ¸²æŸ“ä¸ç‰¹å¾ã€‚</li>
<li>å±‚æ¬¡é«˜æ–¯æµå®ç°äº†é™æ€å’ŒåŠ¨æ€æ¸²æŸ“å’Œç‰¹å¾çš„æœ‰æ•ˆåˆ’åˆ†ã€‚</li>
<li>å±‚æ¬¡é«˜æ–¯å¼•å¯¼æœ‰åŠ©äºå‡è½»å¤æ‚çº¹ç†åœºæ™¯ä¸­åŠ¨æ€å‰æ™¯æ¸²æŸ“å¤±çœŸé—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8320830ff6d464916528f74e086d768b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c31f111dc598c685c6223b90af15f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60074d730e6445802b72e1cb604a93cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d06c6c0c7500fb405a7a54ae97c88b96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-597f4edae71097a89be1c24a8fc2b175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca7bb169a494a76b2d7e55f6b14379c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1383ae08c73abd7fa1022e64dde398c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07c6cf0efbd99b6c3c28cd65a99e4a39.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HoGS-Unified-Near-and-Far-Object-Reconstruction-via-Homogeneous-Gaussian-Splatting"><a href="#HoGS-Unified-Near-and-Far-Object-Reconstruction-via-Homogeneous-Gaussian-Splatting" class="headerlink" title="HoGS: Unified Near and Far Object Reconstruction via Homogeneous   Gaussian Splatting"></a>HoGS: Unified Near and Far Object Reconstruction via Homogeneous   Gaussian Splatting</h2><p><strong>Authors:Xinpeng Liu, Zeyi Huang, Fumio Okura, Yasuyuki Matsushita</strong></p>
<p>Novel view synthesis has demonstrated impressive progress recently, with 3D Gaussian splatting (3DGS) offering efficient training time and photorealistic real-time rendering. However, reliance on Cartesian coordinates limits 3DGSâ€™s performance on distant objects, which is important for reconstructing unbounded outdoor environments. We found that, despite its ultimate simplicity, using homogeneous coordinates, a concept on the projective geometry, for the 3DGS pipeline remarkably improves the rendering accuracies of distant objects. We therefore propose Homogeneous Gaussian Splatting (HoGS) incorporating homogeneous coordinates into the 3DGS framework, providing a unified representation for enhancing near and distant objects. HoGS effectively manages both expansive spatial positions and scales particularly in outdoor unbounded environments by adopting projective geometry principles. Experiments show that HoGS significantly enhances accuracy in reconstructing distant objects while maintaining high-quality rendering of nearby objects, along with fast training speed and real-time rendering capability. Our implementations are available on our project page <a target="_blank" rel="noopener" href="https://kh129.github.io/hogs/">https://kh129.github.io/hogs/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ–°å‹è§†å›¾åˆæˆå·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œå…¶ä¸­ï¼Œä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰æä¾›äº†é«˜æ•ˆçš„è®­ç»ƒæ—¶é—´å’Œé€¼çœŸçš„å®æ—¶æ¸²æŸ“ã€‚ç„¶è€Œï¼Œå¯¹ç¬›å¡å°”åæ ‡çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨è¿œè·ç¦»ç‰©ä½“ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œè¿™å¯¹äºé‡å»ºæ— è¾¹ç•Œçš„å®¤å¤–ç¯å¢ƒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡å…¶æå…¶ç®€å•ï¼Œä½†åœ¨ä½¿ç”¨æŠ•å½±å‡ ä½•çš„æ¦‚å¿µä¸­çš„é½æ¬¡åæ ‡ç”¨äºä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹ç®¡çº¿æ—¶ï¼Œä¼šæ˜¾è‘—æ”¹å–„è¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“ç²¾åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†èåˆé½æ¬¡åæ ‡åˆ°ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹æ¡†æ¶ä¸­çš„é½æ¬¡é«˜æ–¯æ¶‚æŠ¹ï¼ˆHoGSï¼‰ï¼Œä¸ºå¢å¼ºè¿‘å¤„å’Œè¿œå¤„ç‰©ä½“æä¾›äº†ä¸€ä¸ªç»Ÿä¸€è¡¨ç¤ºã€‚HoGSé€šè¿‡é‡‡ç”¨æŠ•å½±å‡ ä½•åŸç†ï¼Œæœ‰æ•ˆåœ°ç®¡ç†äº†æˆ·å¤–æ— è¾¹ç•Œç¯å¢ƒä¸­çš„å¹¿é˜”ç©ºé—´ä½ç½®å’Œå°ºåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒHoGSåœ¨é‡å»ºè¿œè·ç¦»ç‰©ä½“æ—¶æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¿‘è·ç¦»ç‰©ä½“çš„é«˜è´¨é‡æ¸²æŸ“ï¼Œå¹¶ä¿æŒäº†å¿«é€Ÿçš„è®­ç»ƒé€Ÿåº¦å’Œå®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨é¡¹ç›®é¡µé¢ <a target="_blank" rel="noopener" href="https://kh129.github.io/hogs/">https://kh129.github.io/hogs/</a> ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19232v1">PDF</a> Proc. IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition   (CVPRâ€™25)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œä¸‰ç»´é«˜æ–¯ç‚¹æ¸²æŸ“ï¼ˆ3DGSï¼‰æŠ€æœ¯å±•ç°å‡ºå¼ºå¤§çš„å‘å±•æ½œåŠ›ï¼Œä½†å—é™äºç¬›å¡å°”åæ ‡åœ¨æ¸²æŸ“è¿œè·ç¦»ç‰©ä½“æ—¶å­˜åœ¨ç“¶é¢ˆã€‚ç ”ç©¶å‘ç°å°†é½æ¬¡åæ ‡è¿™ä¸€æŠ•å½±å‡ ä½•æ¦‚å¿µå¼•å…¥3DGSæµç¨‹ä¸­ï¼Œå¯æ˜¾è‘—æå‡è¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“ç²¾åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºèåˆé½æ¬¡åæ ‡çš„Homogeneous Gaussian Splattingï¼ˆHoGSï¼‰æŠ€æœ¯ï¼Œåœ¨3DGSæ¡†æ¶ä¸‹æä¾›ç»Ÿä¸€è¡¨ç¤ºæ³•ä»¥æå‡è¿‘ä¸è¿œè·ç¦»ç‰©ä½“çš„æ€§èƒ½ã€‚åœ¨æˆ·å¤–æ— è¾¹ç•Œç¯å¢ƒä¸­ï¼ŒHoGSå€ŸåŠ©æŠ•å½±å‡ ä½•åŸåˆ™å¯æœ‰æ•ˆå¤„ç†å®½é˜”çš„ç©ºé—´ä½ç½®å’Œå°ºåº¦ã€‚å®éªŒè¯æ˜ï¼ŒHoGSèƒ½æ˜¾è‘—å¢å¼ºè¿œè·ç¦»ç‰©ä½“çš„é‡å»ºç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒå¯¹è¿‘è·ç¦»ç‰©ä½“çš„é«˜è´¨é‡æ¸²æŸ“ã€å¿«é€Ÿçš„è®­ç»ƒé€Ÿåº¦å’Œå®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚ç›¸å…³å®ç°ä»£ç å¯è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://kh129.github.io/hogs/">https://kh129.github.io/hogs/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSæŠ€æœ¯å±•ç°å‡ºé«˜æ•ˆè®­ç»ƒæ—¶é—´å’Œé€¼çœŸå®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>ç¬›å¡å°”åæ ‡é™åˆ¶äº†å…¶åœ¨è¿œè·ç¦»ç‰©ä½“æ€§èƒ½ä¸Šçš„è¡¨ç°ã€‚</li>
<li>é½æ¬¡åæ ‡çš„å¼•å…¥å¯ä»¥æ˜¾è‘—æå‡è¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“ç²¾åº¦ã€‚</li>
<li>æå‡ºHomogeneous Gaussian Splattingï¼ˆHoGSï¼‰æŠ€æœ¯ï¼Œèåˆé½æ¬¡åæ ‡åˆ°3DGSæ¡†æ¶ä¸­ã€‚</li>
<li>HoGSæä¾›äº†å¯¹è¿‘ä¸è¿œè·ç¦»ç‰©ä½“çš„ç»Ÿä¸€è¡¨ç¤ºæ³•ã€‚</li>
<li>åœ¨æˆ·å¤–æ— è¾¹ç•Œç¯å¢ƒä¸­ï¼ŒHoGSæœ‰æ•ˆå¤„ç†å®½é˜”çš„ç©ºé—´ä½ç½®å’Œå°ºåº¦é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7693b864b4cc7fcf481d478d88947c0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c570c02beff6a8c777243efed350bd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68c5a71c14dadec57d488c6e6c5312eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15138fe4066fc7eee64849a53a9b53b5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting"><a href="#NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting" class="headerlink" title="NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting"></a>NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting</h2><p><strong>Authors:Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du</strong></p>
<p>Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably advanced photo-realistic novel view synthesis using images from densely spaced camera viewpoints. However, these methods struggle in few-shot scenarios due to limited supervision. In this paper, we present NexusGS, a 3DGS-based approach that enhances novel view synthesis from sparse-view images by directly embedding depth information into point clouds, without relying on complex manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our method introduces a novel point cloud densification strategy that initializes 3DGS with a dense point cloud, reducing randomness in point placement while preventing over-smoothing and overfitting. Specifically, NexusGS comprises three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses to compute accurate depth maps, while mitigating the inaccuracies often associated with optical flow. By incorporating epipolar depth priors, NexusGS ensures reliable dense point cloud coverage and supports stable 3DGS training under sparse-view conditions. Experiments demonstrate that NexusGS significantly enhances depth accuracy and rendering quality, surpassing state-of-the-art methods by a considerable margin. Furthermore, we validate the superiority of our generated point clouds by substantially boosting the performance of competing methods. Project page: <a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/">https://usmizuki.github.io/NexusGS/</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åˆ©ç”¨å¯†é›†é—´éš”çš„ç›¸æœºè§†è§’çš„å›¾åƒæ˜¾è‘—åœ°æ¨è¿›äº†é€¼çœŸçš„æ–°è§†è§’åˆæˆæŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ ·æœ¬é‡å°çš„æƒ…å†µä¸‹ç”±äºç¼ºä¹ç›‘ç£è€Œé¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NexusGSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç›´æ¥å°†æ·±åº¦ä¿¡æ¯åµŒå…¥ç‚¹äº‘ä¸­ï¼Œå¢å¼ºäº†ç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆæ•ˆæœï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„æ‰‹åŠ¨æ­£åˆ™åŒ–ã€‚åˆ©ç”¨ä¸‰ç»´é«˜æ–¯å–·æº…çš„å›ºæœ‰æçº¿å‡ ä½•ç‰¹æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç‚¹äº‘åŠ å¯†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¯†é›†çš„ç‚¹äº‘åˆå§‹åŒ–ä¸‰ç»´é«˜æ–¯å–·æº…ï¼Œå‡å°‘äº†ç‚¹æ”¾ç½®çš„éšæœºæ€§ï¼ŒåŒæ—¶é¿å…äº†è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚å…·ä½“æ¥è¯´ï¼ŒNexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šæçº¿æ·±åº¦çº½ã€æµå¼¹æ€§æ·±åº¦æ··åˆå’Œæµè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚è¿™äº›æ­¥éª¤åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€æ¥è®¡ç®—å‡†ç¡®çš„æ·±åº¦å›¾ï¼ŒåŒæ—¶å‡è½»ä¸å…‰å­¦æµç›¸å…³çš„å¸¸è§è¯¯å·®ã€‚é€šè¿‡å¼•å…¥æçº¿æ·±åº¦å…ˆéªŒçŸ¥è¯†ï¼ŒNexusGSç¡®ä¿äº†å¯é çš„å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹æ”¯æŒç¨³å®šçš„3DGSè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNexusGSæ˜¾è‘—æé«˜æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œæ˜æ˜¾ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤§å¹…å¢å¼ºç«äº‰å¯¹æ‰‹æ–¹æ³•çš„æ€§èƒ½éªŒè¯äº†æ‰€ç”Ÿæˆç‚¹äº‘çš„ä¼˜è¶Šæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/%E3%80%82">https://usmizuki.github.io/NexusGS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18794v1">PDF</a> This paper is accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†NexusGSï¼Œä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆã€‚å®ƒé€šè¿‡ç›´æ¥åµŒå…¥æ·±åº¦ä¿¡æ¯åˆ°ç‚¹äº‘ä¸­ï¼Œåˆ©ç”¨å†…è•´çš„æå‡ ä½•ç‰¹æ€§ï¼Œå®ç°äº†åœ¨ç¨€ç–è§†è§’ä¸‹çš„å¯é å¯†é›†ç‚¹äº‘è¦†ç›–å’Œç¨³å®šçš„3DGSè®­ç»ƒã€‚æ­¤æ–¹æ³•è¿˜åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æŠ—æ¼‚ç§»æ·±åº¦æ··åˆå’Œè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚å®éªŒè¯æ˜ï¼ŒNexusGSèƒ½æ˜¾è‘—æé«˜æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NexusGSæ˜¯ä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œç”¨äºç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆå¢å¼ºã€‚</li>
<li>å®ƒé€šè¿‡ç›´æ¥åµŒå…¥æ·±åº¦ä¿¡æ¯åˆ°ç‚¹äº‘ä¸­ï¼Œæé«˜äº†æ–°è§†è§’åˆæˆçš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å†…è•´çš„æå‡ ä½•ç‰¹æ€§ï¼Œå®ç°äº†å¯é çš„å¯†é›†ç‚¹äº‘è¦†ç›–å’Œç¨³å®šçš„3DGSè®­ç»ƒã€‚</li>
<li>NexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æŠ—æ¼‚ç§»æ·±åº¦æ··åˆå’Œè¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚</li>
<li>æ­¤æ–¹æ³•é€šè¿‡åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€ï¼Œè®¡ç®—å‡†ç¡®æ·±åº¦å›¾ï¼Œå‡è½»äº†å…‰å­¦æµå¸¸æœ‰çš„ä¸å‡†ç¡®é—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒNexusGSåœ¨æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ä¸Šæ˜¾è‘—æé«˜ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d91262626222a26061a4d223c84fbca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93ed83d2b2f76ab2cb97bd4a5f9f65b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a47a15036cbec97db110b1320e4f846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf847bb4e3c6ffe7de25a2effa479071.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GS-Marker-Generalizable-and-Robust-Watermarking-for-3D-Gaussian-Splatting"><a href="#GS-Marker-Generalizable-and-Robust-Watermarking-for-3D-Gaussian-Splatting" class="headerlink" title="GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian   Splatting"></a>GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian   Splatting</h2><p><strong>Authors:Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu</strong></p>
<p>In the Generative AI era, safeguarding 3D models has become increasingly urgent. While invisible watermarking is well-established for 2D images with encoder-decoder frameworks, generalizable and robust solutions for 3D remain elusive. The main difficulty arises from the renderer between the 3D encoder and 2D decoder, which disrupts direct gradient flow and complicates training. Existing 3D methods typically rely on per-scene iterative optimization, resulting in time inefficiency and limited generalization. In this work, we propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a well-known yet underexplored representation for watermarking. We identify two major challenges: (1) ensuring effective training generalized across diverse 3D models, and (2) reliably extracting watermarks from free-view renderings, even under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to embed messages, distortion layers to enhance resilience against various distortions, and a 2D decoder to extract watermarks from renderings. A key innovation is the Adaptive Marker Control mechanism that adaptively perturbs the initially optimized 3DGS, escaping local minima and improving both training stability and convergence. Extensive experiments show that GS-Marker outperforms per-scene training approaches in terms of decoding accuracy and model fidelity, while also significantly reducing computation time. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆå¼AIæ—¶ä»£ï¼Œä¿æŠ¤3Dæ¨¡å‹çš„å®‰å…¨å˜å¾—è¶Šæ¥è¶Šç´§è¿«ã€‚è™½ç„¶åŸºäºç¼–ç å™¨-è§£ç å™¨æ¡†æ¶çš„äºŒç»´å›¾åƒéšå½¢æ°´å°æŠ€æœ¯å·²ç»æˆç†Ÿï¼Œä½†é€‚ç”¨äºä¸‰ç»´æ¨¡å‹çš„é€šç”¨å’Œç¨³å¥è§£å†³æ–¹æ¡ˆä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚ä¸»è¦å›°éš¾æ¥è‡ªäºä¸‰ç»´ç¼–ç å™¨ä¸äºŒç»´è§£ç å™¨ä¹‹é—´çš„æ¸²æŸ“å™¨ï¼Œå®ƒç ´åäº†ç›´æ¥çš„æ¢¯åº¦æµå¹¶ä½¿å¾—è®­ç»ƒå¤æ‚åŒ–ã€‚ç°æœ‰çš„ä¸‰ç»´æ–¹æ³•é€šå¸¸ä¾èµ–äºåœºæ™¯è¿­ä»£ä¼˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´æ•ˆç‡ä½ä¸‹ä¸”é€šç”¨æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰çš„å•æ¬¡é€šè¿‡æ°´å°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä¼—æ‰€å‘¨çŸ¥ä½†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„æ°´å°è¡¨ç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¡®ä¿åœ¨å„ç§ä¸‰ç»´æ¨¡å‹ä¸Šè¿›è¡Œæœ‰æ•ˆçš„é€šç”¨åŒ–è®­ç»ƒï¼›ï¼ˆ2ï¼‰å³ä½¿åœ¨å¤±çœŸæƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½ä»è‡ªç”±è§†å›¾æ¸²æŸ“ä¸­å¯é åœ°æå–æ°´å°ã€‚æˆ‘ä»¬çš„æ¡†æ¶GS-Markerç»“åˆäº†ä¸‰ç»´ç¼–ç å™¨ä»¥åµŒå…¥æ¶ˆæ¯ã€å¤±çœŸå±‚ä»¥å¢å¼ºå¯¹å„ç§å¤±çœŸçš„é€‚åº”æ€§ï¼Œä»¥åŠäºŒç»´è§£ç å™¨ä»¥ä»æ¸²æŸ“ä¸­æå–æ°´å°ã€‚ä¸€ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹æ˜¯è‡ªé€‚åº”æ ‡è®°æ§åˆ¶æœºåˆ¶ï¼Œå®ƒè‡ªé€‚åº”åœ°æ‰°åŠ¨æœ€åˆä¼˜åŒ–çš„3DGSï¼Œé€ƒç¦»å±€éƒ¨æœ€å°å€¼å¹¶æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGS-Markeråœ¨è§£ç ç²¾åº¦å’Œæ¨¡å‹ä¿çœŸåº¦æ–¹é¢ä¼˜äºåœºæ™¯è®­ç»ƒæ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18718v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç”Ÿæˆå¼AIæ—¶ä»£ï¼Œä¿æŠ¤ä¸‰ç»´æ¨¡å‹å˜å¾—æ›´åŠ ç´§è¿«ã€‚å½“å‰å¯¹äºä¸‰ç»´æ¨¡å‹æ°´å°çš„æŠ€æœ¯ä¾æ—§ä¸æˆç†Ÿï¼Œç”±äºå­˜åœ¨è¯¸å¤šéš¾ç‚¹ï¼Œå¦‚æ°´å°åœ¨ä¸‰ç»´ç¼–ç å™¨å’ŒäºŒç»´è§£ç å™¨ä¹‹é—´çš„æ¸²æŸ“é—®é¢˜å¯¼è‡´ç›´æ¥æ¢¯åº¦æµè¢«ä¸­æ–­ç­‰ã€‚æœ¬æ–‡ä¸»è¦æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ç”Ÿæˆç®—æ³•ï¼ˆGSï¼‰çš„å•é€šé“æ°´å°åµŒå…¥æ–¹æ³•ï¼Œå¹¶è§£å†³äº†ä¸¤å¤§æŒ‘æˆ˜ï¼šç¡®ä¿è·¨ä¸åŒä¸‰ç»´æ¨¡å‹çš„é€šç”¨è®­ç»ƒæ•ˆæœä»¥åŠä»è‡ªç”±è§†è§’æ¸²æŸ“ä¸­æå–æ°´å°çš„å¯é æ€§ã€‚é€šè¿‡åˆ›æ–°çš„è‡ªé€‚åº”æ ‡è®°æ§åˆ¶æœºåˆ¶æé«˜äº†è§£ç å‡†ç¡®æ€§å’Œæ¨¡å‹ä¿çœŸåº¦ã€‚ç ”ç©¶è¯å®GS-Markeræ–¹æ³•çš„ä¼˜è¶Šæ€§è¿œè¶…é’ˆå¯¹åœºæ™¯çš„å¸¸è§„è®­ç»ƒæ³•ã€‚åŒæ—¶å®ƒçš„è¿ç®—æ—¶é—´ä¹Ÿæ›´çŸ­ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼AIæ—¶ä»£å¯¹ä¸‰ç»´æ¨¡å‹æ°´å°æŠ€æœ¯çš„éœ€æ±‚è¿«åˆ‡ã€‚</li>
<li>ä¸‰ç»´æ¨¡å‹æ°´å°æŠ€æœ¯é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æ¸²æŸ“è¿‡ç¨‹ä¸­çš„æ¢¯åº¦æµä¸­æ–­å’Œè®­ç»ƒéš¾åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ç”Ÿæˆç®—æ³•ï¼ˆGSï¼‰çš„å•é€šé“æ°´å°åµŒå…¥æ–¹æ³•ï¼ˆGS-Markerï¼‰ã€‚</li>
<li>GS-Markerè§£å†³äº†è·¨ä¸åŒä¸‰ç»´æ¨¡å‹çš„é€šç”¨è®­ç»ƒé—®é¢˜å’Œä»è‡ªç”±è§†è§’æ¸²æŸ“ä¸­æå–æ°´å°çš„å¯é æ€§é—®é¢˜ã€‚</li>
<li>åˆ›æ–°æ€§çš„è‡ªé€‚åº”æ ‡è®°æ§åˆ¶æœºåˆ¶æé«˜äº†è§£ç å‡†ç¡®æ€§å’Œæ¨¡å‹ä¿çœŸåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac0580d3c210eb27841248a88df21f68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19bf454900a435dd0830dc9600cb0f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c33feca77c4eadaa69c39b49a51aaf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99ff0144abb56fbdea05cf0805589677.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Hardware-Rasterized-Ray-Based-Gaussian-Splatting"><a href="#Hardware-Rasterized-Ray-Based-Gaussian-Splatting" class="headerlink" title="Hardware-Rasterized Ray-Based Gaussian Splatting"></a>Hardware-Rasterized Ray-Based Gaussian Splatting</h2><p><strong>Authors:Samuel Rota BulÃ², Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder</strong></p>
<p>We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing MIP-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶å…‰çº¿æŠ•å°„æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºåŸºäºå°„çº¿çš„ä¸‰ç»´é«˜æ–¯ç»˜åˆ¶ï¼ˆRayGSï¼‰ï¼Œä¸ºæ–°é¢–è§†å›¾åˆæˆè·å¾—å¿«é€Ÿä¸”é«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨æ•°å­¦ä¸Šä¸¥è°¨ã€å‡ ä½•ä¸Šç›´è§‚åœ°æ¨å¯¼å‡ºå¦‚ä½•æœ‰æ•ˆåœ°ä¼°è®¡æ‰€æœ‰ä¸æ¸²æŸ“RayGSæ¨¡å‹ç›¸å…³çš„æ•°é‡ï¼Œå¹¶ä»¥æ ‡å‡†çš„ç¡¬ä»¶å…‰æ …åŒ–ç€è‰²å™¨è¿›è¡Œç»“æ„åŒ–ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿä»¥è¶³å¤Ÿé«˜çš„å¸§ç‡æ¸²æŸ“RayGSæ¨¡å‹ï¼Œä»¥æ”¯æŒåƒè™šæ‹Ÿå’Œæ··åˆç°å®è¿™æ ·çš„å¯¹è´¨é‡æ•æ„Ÿçš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯é€šè¿‡è§£å†³åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´å‘ˆç°å‘æ•£å°ºåº¦æ—¶å‡ºç°çš„MIPç›¸å…³é—®é¢˜ï¼Œå®ç°äº†RayGSçš„æ— åˆ«åæ¸²æŸ“ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†åœºæ™¯ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†RayGSçš„æœ€æ–°å¤–è§‚è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18682v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶å…‰æ …åŒ–æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºåŸºäºå°„çº¿çš„ä¸‰ç»´é«˜æ–¯æ¨¡ç³Šï¼ˆRayGSï¼‰ï¼Œå®ç°äº†å¿«é€Ÿä¸”é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚æœ¬æ–‡åœ¨æ•°å­¦ä¸Šä¸¥æ ¼ä¸”å‡ ä½•ç›´è§‚ä¸Šæ¨å¯¼äº†å¦‚ä½•æœ‰æ•ˆåœ°ä¼°è®¡RayGSæ¨¡å‹çš„æ‰€æœ‰ç›¸å…³é‡ï¼Œå¹¶ä»¥æ ‡å‡†çš„ç¡¬ä»¶å…‰æ …åŒ–ç€è‰²å™¨è¿›è¡Œç»“æ„åŒ–è®¾è®¡ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆé¦–æ¬¡æ”¯æŒä»¥è¶³å¤Ÿé«˜çš„å¸§ç‡æ¸²æŸ“RayGSæ¨¡å‹ï¼Œé€‚ç”¨äºè™šæ‹Ÿç°å®å’Œæ··åˆç°å®ç­‰è´¨é‡æ•æ„Ÿå‹åº”ç”¨ã€‚ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯é€šè¿‡è§£å†³MIPç›¸å…³çš„æ¸²æŸ“å‘æ•£å°ºåº¦é—®é¢˜ï¼Œå®ç°äº†RayGSçš„æ— æ··å æ¸²æŸ“ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†RayGSçš„å…ˆè¿›å¤–è§‚è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºç¡¬ä»¶å…‰æ …åŒ–çš„æ–°å‹æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºå¿«é€Ÿä¸”é«˜è´¨é‡åœ°å‘ˆç°åŸºäºå°„çº¿çš„ä¸‰ç»´é«˜æ–¯æ¨¡ç³Šï¼ˆRayGSï¼‰ã€‚</li>
<li>æä¾›äº†æ•°å­¦ä¸Šçš„ä¸¥æ ¼æ¨å¯¼å’Œå‡ ä½•ç›´è§‚çš„è§£é‡Šï¼Œä»¥æœ‰æ•ˆåœ°ä¼°è®¡RayGSæ¨¡å‹çš„ç›¸å…³å‚æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡ä»¥é«˜å¸§ç‡æ”¯æŒRayGSæ¨¡å‹çš„æ¸²æŸ“ï¼Œé€‚ç”¨äºè™šæ‹Ÿç°å®å’Œæ··åˆç°å®ç­‰åº”ç”¨ã€‚</li>
<li>é€šè¿‡è§£å†³åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´å‡ºç°çš„MIPç›¸å…³é—®é¢˜ï¼Œå®ç°äº†æ— æ··å çš„RayGSæ¸²æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿æŒRayGSå…ˆè¿›å¤–è§‚è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ä¸åŒåŸºå‡†åœºæ™¯çš„æ€§èƒ½ã€‚</li>
<li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ¸²æŸ“æ•ˆç‡å’Œè´¨é‡ä¸Šçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e26084064e6b73a6c984dfbde70b8065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7995e058ec4e9ca0b64b62ddda8a825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce52d244ff81aa5dc43e0238caca814c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408c0b0fc4e13e0acc36841564792c0e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StableGS-A-Floater-Free-Framework-for-3D-Gaussian-Splatting"><a href="#StableGS-A-Floater-Free-Framework-for-3D-Gaussian-Splatting" class="headerlink" title="StableGS: A Floater-Free Framework for 3D Gaussian Splatting"></a>StableGS: A Floater-Free Framework for 3D Gaussian Splatting</h2><p><strong>Authors:Luchao Wang, Qian Ren, Kaimin Liao, Hua Wang, Zhi Chen, Yaohua Tang</strong></p>
<p>Recent years have witnessed remarkable success of 3D Gaussian Splatting (3DGS) in novel view synthesis, surpassing prior differentiable rendering methods in both quality and efficiency. However, its training process suffers from coupled opacity-color optimization that frequently converges to local minima, producing floater artifacts that degrade visual fidelity. We present StableGS, a framework that eliminates floaters through cross-view depth consistency constraints while introducing a dual-opacity GS model to decouple geometry and material properties of translucent objects. To further enhance reconstruction quality in weakly-textured regions, we integrate DUSt3R depth estimation, significantly improving geometric stability. Our method fundamentally addresses 3DGS training instabilities, outperforming existing state-of-the-art methods across open-source datasets. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œ3Dé«˜æ–¯æ˜ å°„ï¼ˆ3DGSï¼‰åœ¨æ–°è§†è§’åˆæˆä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œåœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„å¯å¾®åˆ†æ¸²æŸ“æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶è®­ç»ƒè¿‡ç¨‹å—åˆ°è€¦åˆçš„ä¸é€æ˜è‰²ä¼˜åŒ–çš„å½±å“ï¼Œç»å¸¸é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œäº§ç”Ÿæµ®åŠ¨ä¼ªå½±ï¼Œé™ä½äº†è§†è§‰ä¿çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºäº†StableGSæ¡†æ¶ï¼Œé€šè¿‡è·¨è§†å›¾æ·±åº¦ä¸€è‡´æ€§çº¦æŸæ¥æ¶ˆé™¤æµ®åŠ¨ä¼ªå½±ï¼ŒåŒæ—¶å¼•å…¥åŒä¸é€æ˜GSæ¨¡å‹æ¥è§£è€¦åŠé€æ˜ç‰©ä½“çš„å‡ ä½•å’Œææ–™å±æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¼±çº¹ç†åŒºåŸŸçš„é‡å»ºè´¨é‡ï¼Œæˆ‘ä»¬é›†æˆäº†DUSt3Ræ·±åº¦ä¼°è®¡ï¼Œæ˜¾è‘—æé«˜äº†å‡ ä½•ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»æ ¹æœ¬ä¸Šè§£å†³äº†3DGSè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œåœ¨å¼€æºæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18458v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘å¹´3Dé«˜æ–¯æç”»ï¼ˆ3DGSï¼‰åœ¨æ–°è§†è§’åˆæˆä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œåœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„å¯å¾®åˆ†æ¸²æŸ“æ–¹æ³•ã€‚ä½†å…¶è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨è€¦åˆçš„é®ç½©-é¢œè‰²ä¼˜åŒ–é—®é¢˜ï¼Œæ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œäº§ç”Ÿæ¼‚æµ®ç‰©å½±å“è§†è§‰é€¼çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºStableGSæ¡†æ¶ï¼Œé€šè¿‡è·¨è§†å›¾æ·±åº¦ä¸€è‡´æ€§çº¦æŸæ¶ˆé™¤æ¼‚æµ®ç‰©ï¼Œå¹¶å¼•å…¥åŒé®ç½©GSæ¨¡å‹ä»¥è§£è€¦åŠé€æ˜ç‰©ä½“çš„å‡ ä½•å’Œææ–™å±æ€§ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å¼±çº¹ç†åŒºåŸŸçš„é‡å»ºè´¨é‡ï¼Œæˆ‘ä»¬é›†æˆäº†DUSt3Ræ·±åº¦ä¼°è®¡ï¼Œæ˜¾è‘—æé«˜äº†å‡ ä½•ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•ä»æ ¹æœ¬ä¸Šè§£å†³äº†3DGSè®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œåœ¨å¼€æºæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSåœ¨æ–°è§†è§’åˆæˆä¸­å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è€¦åˆé®ç½©-é¢œè‰²ä¼˜åŒ–é—®é¢˜ï¼Œæ˜“äº§ç”Ÿæ¼‚æµ®ç‰©å½±å“è§†è§‰è´¨é‡ã€‚</li>
<li>StableGSæ¡†æ¶é€šè¿‡è·¨è§†å›¾æ·±åº¦ä¸€è‡´æ€§çº¦æŸæ¶ˆé™¤äº†æ¼‚æµ®ç‰©ã€‚</li>
<li>StableGSå¼•å…¥åŒé®ç½©GSæ¨¡å‹ä»¥è§£è€¦åŠé€æ˜ç‰©ä½“çš„å‡ ä½•å’Œææ–™å±æ€§ã€‚</li>
<li>é›†æˆDUSt3Ræ·±åº¦ä¼°è®¡æé«˜äº†å¼±çº¹ç†åŒºåŸŸçš„é‡å»ºè´¨é‡å’Œå‡ ä½•ç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†3DGSè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ed23b122b2f14273473e580b4c302a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c493bf418cb40831404b756c3169fb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec7c7092dbf5984051021607bb000d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de732ebdc8e31a1ac17cd99b23c11851.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting"><a href="#TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting" class="headerlink" title="TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting"></a>TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting</h2><p><strong>Authors:Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv</strong></p>
<p>Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we â€œbakeâ€ the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro. </p>
<blockquote>
<p>ç°å®ä¸»ä¹‰çš„3Då…¨èº«äº¤è°ˆé˜¿å‡¡è¾¾ï¼ˆavatarsï¼‰åœ¨å¢å¼ºç°å®ï¼ˆARï¼‰é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå…¶åº”ç”¨èŒƒå›´ä»ç”µå­å•†åŠ¡ç›´æ’­åˆ°å…¨æ¯é€šä¿¡ã€‚å°½ç®¡3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨åˆ›å»ºé€¼çœŸé˜¿å‡¡è¾¾æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…¨èº«äº¤è°ˆä»»åŠ¡çš„é¢éƒ¨è¡¨æƒ…å’Œèº¯ä½“åŠ¨ä½œçš„ç²¾ç»†æ§åˆ¶æ–¹é¢ä»ç„¶é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚ï¼Œæ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œã€‚æˆ‘ä»¬å±•ç¤ºäº†TaoAvatarï¼Œä¸€ä¸ªåŸºäºå¤šç§ä¿¡å·é©±åŠ¨çš„é«˜ä¿çœŸã€è½»é‡çº§çš„3DGSå…¨èº«äº¤è°ˆé˜¿å‡¡è¾¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ›å»ºä¸€ä¸ªä¸ªæ€§åŒ–çš„ç©¿è¡£äººç±»å‚æ•°æ¨¡æ¿ï¼Œå°†é«˜æ–¯ç»‘å®šä»¥è¡¨ç¤ºå¤–è§‚ã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºStyleUnetç½‘ç»œè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å¤„ç†å¤æ‚çš„å§¿åŠ¿ç›¸å…³çš„éåˆšæ€§å˜å½¢ï¼Œè¿™å¯ä»¥æ•æ‰é«˜é¢‘å¤–è§‚ç»†èŠ‚ï¼Œä½†å¯¹äºç§»åŠ¨è®¾å¤‡è€Œè¨€ï¼Œèµ„æºæ¶ˆè€—è¿‡å¤§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€åˆ°ä¸€ä¸ªåŸºäºè½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„ç½‘ç»œä¸­ï¼Œå¹¶å¼€å‘æ··åˆå½¢çŠ¶ä»¥å¼¥è¡¥ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTaoAvatarè¾¾åˆ°äº†å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶å¯åœ¨å„ç§è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œï¼Œåœ¨é«˜åˆ†è¾¨ç‡ç«‹ä½“å£°è®¾å¤‡ä¸Šï¼ˆå¦‚Apple Vision Proï¼‰ä¿æŒ90å¸§&#x2F;ç§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17032v1">PDF</a> Accepted by CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://pixelai-team.github.io/TaoAvatar">https://PixelAI-Team.github.io/TaoAvatar</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°å®çš„ä¸‰ç»´å…¨èº«å¯¹è¯è™šæ‹Ÿäººå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ç”µå­å•†åŠ¡ç›´æ’­å’Œå…¨æ¯é€šä¿¡ç­‰é¢†åŸŸã€‚å°½ç®¡åœ¨åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ˜ å°„æŠ€æœ¯ï¼ˆ3DGSï¼‰åˆ›å»ºé€¼çœŸè™šæ‹Ÿäººæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ç²¾ç»†æ§åˆ¶é¢éƒ¨è¡¨æƒ…å’Œå…¨èº«åŠ¨ä½œæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚ï¼Œæ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œã€‚æˆ‘ä»¬æå‡ºäº†TaoAvatarç³»ç»Ÿï¼Œä¸€ä¸ªåŸºäºä¸‰ç»´é«˜æ–¯æ˜ å°„æŠ€æœ¯çš„é«˜ä¿çœŸã€è½»é‡çº§çš„å…¨èº«å¯¹è¯è™šæ‹Ÿäººé©±åŠ¨æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ›å»ºä¸ªæ€§åŒ–çš„ç€è£…äººç±»å‚æ•°æ¨¡æ¿æ¥ä»£è¡¨å¤–è§‚ï¼Œå¹¶é¢„è®­ç»ƒStyleUnetç½‘ç»œå¤„ç†å¤æ‚çš„å§¿æ€ç›¸å…³éåˆšæ€§å˜å½¢ã€‚ä¸ºäº†å…‹æœåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„èµ„æºå¯†é›†å‹è®¡ç®—é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€åˆ°åŸºäºMLPçš„è½»é‡çº§ç½‘ç»œä¸­ï¼Œå¹¶å¼€å‘æ··åˆå½¢çŠ¶æ¥è¡¥å¿ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒTaoAvataråœ¨ä¿æŒå®æ—¶è¿è¡Œçš„åŒæ—¶å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨å„ç§è®¾å¤‡ä¸Šéƒ½èƒ½ä¿æŒ90å¸§çš„æµç•…åº¦ï¼ŒåŒ…æ‹¬è‹¹æœVision Proç­‰é«˜åˆ†è¾¨ç‡ç«‹ä½“è®¾å¤‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°å®çš„ä¸‰ç»´å…¨èº«å¯¹è¯è™šæ‹Ÿäººåœ¨ARä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œæ¶µç›–å¤šä¸ªåº”ç”¨é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç²¾ç»†æ§åˆ¶é¢éƒ¨è¡¨æƒ…å’Œå…¨èº«åŠ¨ä½œæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚å’Œå®æ—¶è¿è¡Œèƒ½åŠ›ã€‚</li>
<li>TaoAvatarç³»ç»Ÿé€šè¿‡åˆ›å»ºä¸ªæ€§åŒ–çš„ç€è£…äººç±»å‚æ•°æ¨¡æ¿å’Œé«˜ä¿çœŸçš„æŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨StyleUnetç½‘ç»œå¤„ç†å¤æ‚çš„å§¿æ€ç›¸å…³éåˆšæ€§å˜å½¢ï¼Œå®ç°é«˜è´¨é‡çš„æ¸²æŸ“ã€‚</li>
<li>é€šè¿‡è’¸é¦æŠ€æœ¯å’Œè½»é‡çº§ç½‘ç»œä¼˜åŒ–ï¼ŒTaoAvatarèƒ½åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å®æ—¶è¿è¡Œå’Œé«˜è´¨é‡æ¸²æŸ“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d251751954528217baa3a66e8a790fbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2df1e3379d1a25574bbd5482acac7dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4577a665cc5fa624c1edd48d3f87970.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seeing-A-3D-World-in-A-Grain-of-Sand"><a href="#Seeing-A-3D-World-in-A-Grain-of-Sand" class="headerlink" title="Seeing A 3D World in A Grain of Sand"></a>Seeing A 3D World in A Grain of Sand</h2><p><strong>Authors:Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye</strong></p>
<p>We present a snapshot imaging technique for recovering 3D surrounding views of miniature scenes. Due to their intricacy, miniature scenes with objects sized in millimeters are difficult to reconstruct, yet miniatures are common in life and their 3D digitalization is desirable. We design a catadioptric imaging system with a single camera and eight pairs of planar mirrors for snapshot 3D reconstruction from a dollhouse perspective. We place paired mirrors on nested pyramid surfaces for capturing surrounding multi-view images in a single shot. Our mirror design is customizable based on the size of the scene for optimized view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene reconstruction and novel view synthesis. We overcome the challenge posed by our sparse view input by integrating visual hull-derived depth constraint. Our method demonstrates state-of-the-art performance on a variety of synthetic and real miniature scenes. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«ç…§æˆåƒæŠ€æœ¯ï¼Œç”¨äºæ¢å¤å¾®å‹åœºæ™¯çš„3Då‘¨å›´ç¯å¢ƒè§†å›¾ã€‚ç”±äºå¾®å‹åœºæ™¯çš„å¤æ‚æ€§ï¼Œæ¯«ç±³çº§ç‰©ä½“çš„é‡å»ºéå¸¸å›°éš¾ï¼Œä½†å¾®å‹åœºæ™¯åœ¨ç”Ÿæ´»ä¸­å¾ˆå¸¸è§ï¼Œå…¶3Dæ•°å­—åŒ–æ˜¯å¯è¡Œçš„ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå•ç›®é±¼çœ¼ç›¸æœºå’Œå…«å¯¹å¹³é¢é•œçš„ç»„åˆæˆåƒç³»ç»Ÿï¼Œç”¨äºä»ç©å¶å±‹çš„è§†è§’è¿›è¡Œå¿«ç…§3Dé‡å»ºã€‚æˆ‘ä»¬å°†æˆå¯¹çš„é•œå­æ”¾ç½®åœ¨åµŒå¥—çš„é‡‘å­—å¡”è¡¨é¢ä¸Šï¼Œä»¥åœ¨ä¸€æ¬¡æ‹æ‘„ä¸­æ•è·å‘¨å›´çš„å¤šä¸ªè§†å›¾ã€‚æˆ‘ä»¬çš„é•œå­è®¾è®¡å¯ä»¥æ ¹æ®åœºæ™¯å¤§å°è¿›è¡Œå®šåˆ¶ï¼Œä»¥ä¼˜åŒ–è§†å›¾è¦†ç›–èŒƒå›´ã€‚æˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰è¡¨ç¤ºæ³•è¿›è¡Œåœºæ™¯é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚æˆ‘ä»¬é€šè¿‡é›†æˆç”±è§†è§‰å¤–å£³æ´¾ç”Ÿçš„æ·±åº¦çº¦æŸï¼Œå…‹æœäº†ç¨€ç–è§†å›¾è¾“å…¥æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§åˆæˆå’ŒçœŸå®å¾®å‹åœºæ™¯ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00260v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ¢å¤å¾®å‹åœºæ™¯ä¸‰ç»´ç¯ç»•è§†å›¾çš„å¿«ç…§æˆåƒæŠ€æœ¯ã€‚é’ˆå¯¹æ¯«ç±³çº§å¾®å‹åœºæ™¯çš„å¤æ‚æ€§å’Œéš¾ä»¥é‡å»ºçš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§å•ç›¸æœºå’Œå…«å¯¹å¹³é¢é•œç»„æˆçš„æŠ˜åå°„æˆåƒç³»ç»Ÿï¼Œç”¨äºä»ç©å¶å±‹è§†è§’è¿›è¡Œå¿«ç…§ä¸‰ç»´é‡å»ºã€‚åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºæ³•è¿›è¡Œåœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆï¼Œé€šè¿‡ä¼˜åŒ–è§†å›¾è¦†ç›–çš„é•œå­è®¾è®¡å¯å®šåˆ¶ç³»ç»Ÿã€‚è¯¥æ–¹æ³•è§£å†³äº†ç¨€ç–è§†å›¾è¾“å…¥å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé€šè¿‡é›†æˆè§†è§‰æ·±åº¦çº¦æŸæ¥å®ç°å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§åˆæˆå’ŒçœŸå®å¾®å‹åœºæ™¯ä¸­å¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§å¿«ç…§æˆåƒæŠ€æœ¯ï¼Œç”¨äºæ¢å¤å¾®å‹åœºæ™¯çš„ä¸‰ç»´ç¯ç»•è§†å›¾ã€‚</li>
<li>åˆ©ç”¨æŠ˜åå°„æˆåƒç³»ç»Ÿï¼Œé€šè¿‡å•ç›¸æœºå’Œå…«å¯¹å¹³é¢é•œè¿›è¡Œå¿«ç…§ä¸‰ç»´é‡å»ºã€‚</li>
<li>é•œå­è®¾è®¡å¯æ ¹æ®åœºæ™¯å¤§å°è¿›è¡Œå®šåˆ¶ï¼Œä»¥ä¼˜åŒ–è§†å›¾è¦†ç›–ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¡¨ç¤ºæ³•è¿›è¡Œåœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆã€‚</li>
<li>é€šè¿‡é›†æˆè§†è§‰æ·±åº¦çº¦æŸè§£å†³ç¨€ç–è§†å›¾è¾“å…¥å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®å¾®å‹åœºæ™¯ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9067d9df98d4e177676f6cdc91ff76bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcee3f7ef9111c6c3dcfe580faa83bd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a2e4be6062901783f5a09a39c89dd09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9e69616cdc83e182850df9860e6a05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2c04505e5219861cc7f6d75b6947d88.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AuraFusion360-Augmented-Unseen-Region-Alignment-for-Reference-based-360Â°-Unbounded-Scene-Inpainting"><a href="#AuraFusion360-Augmented-Unseen-Region-Alignment-for-Reference-based-360Â°-Unbounded-Scene-Inpainting" class="headerlink" title="AuraFusion360: Augmented Unseen Region Alignment for Reference-based   360Â° Unbounded Scene Inpainting"></a>AuraFusion360: Augmented Unseen Region Alignment for Reference-based   360Â° Unbounded Scene Inpainting</h2><p><strong>Authors:Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</strong></p>
<p>Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. </p>
<blockquote>
<p>ä¸‰ç»´åœºæ™¯è¡¥å…¨å¯¹äºä»è™šæ‹Ÿç°å®åˆ°å»ºç­‘å¯è§†åŒ–ç­‰åº”ç”¨è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•åœ¨360Â°æ— ç•Œåœºæ™¯ä¸­çš„è§†è§’ä¸€è‡´æ€§å’Œå‡ ä½•ç²¾åº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†AuraFusion360ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå‚è€ƒçš„æ–°å‹æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»¥é«˜æ–¯å–·æº…è¡¨ç¤ºçš„ä¸‰ç»´åœºæ™¯ä¸­è¿›è¡Œé«˜è´¨é‡çš„å¯¹è±¡ç§»é™¤å’Œç©ºæ´å¡«å……ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ï¼ˆ1ï¼‰æ·±åº¦æ„ŸçŸ¥æœªè§æ©æ¨¡ç”Ÿæˆï¼Œç”¨äºå‡†ç¡®è¯†åˆ«é®æŒ¡ï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”å¼•å¯¼æ·±åº¦æ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒå³å¯å‡†ç¡®æ”¾ç½®åˆå§‹ç‚¹çš„é›¶æ ·æœ¬æ–¹æ³•ï¼›ï¼ˆ3ï¼‰åŸºäºSDEditçš„ç»†èŠ‚å¢å¼ºï¼Œä»¥å®ç°å¤šè§†è§’ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†360-USIDï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äº360Â°æ— ç•Œåœºæ™¯è¡¥å…¨çš„ç»¼åˆæ•°æ®é›†ï¼Œå¸¦æœ‰çœŸå®åœ°é¢æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAuraFusion360åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è§†ç‚¹å¤§å¹…å˜åŒ–çš„æƒ…å†µä¸‹ä»èƒ½ç»´æŒå‡ ä½•ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05176v2">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://kkennethwu.github.io/aurafusion360/">https://kkennethwu.github.io/aurafusion360/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä¸‰ç»´åœºæ™¯è¡¥å…¨å¯¹äºä»è™šæ‹Ÿç°å®åˆ°å»ºç­‘å¯è§†åŒ–ç­‰å¤šç§åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨360Â°æ— ç•Œåœºæ™¯çš„è§†å›¾å’Œå‡ ä½•å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºAuraFusion360ï¼Œä¸€ç§åŸºäºå‚è€ƒçš„æ–°æ–¹æ³•ï¼Œå¯åœ¨é«˜æ–¯æ‹¼è´´è¡¨ç¤ºçš„ä¸‰ç»´åœºæ™¯ä¸­è¿›è¡Œé«˜è´¨é‡çš„å¯¹è±¡ç§»é™¤å’Œç©ºæ´å¡«å……ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ï¼ˆ1ï¼‰æ·±åº¦æ„ŸçŸ¥æœªè§æ©æ¨¡ç”Ÿæˆï¼Œç”¨äºå‡†ç¡®è¯†åˆ«é®æŒ¡ï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”å¼•å¯¼æ·±åº¦æ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„é›¶å°„å‡»æ–¹æ³•ï¼Œå¯å®ç°ç²¾ç¡®åˆå§‹ç‚¹æ”¾ç½®ï¼›ï¼ˆ3ï¼‰åŸºäºSDEditçš„ç»†èŠ‚å¢å¼ºï¼Œä»¥å®ç°å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†é¦–ä¸ªä¸º360Â°æ— ç•Œåœºæ™¯è¡¥å…¨è®¾è®¡çš„ç»¼åˆæ•°æ®é›†360-USIDï¼Œå…¶ä¸­åŒ…å«çœŸå®åœºæ™¯ä¸‹çš„åœ°é¢çœŸå®æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAuraFusion360æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨è§†ç‚¹å˜åŒ–å‰§çƒˆçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒå‡ ä½•å‡†ç¡®æ€§ï¼Œå¹¶å®ç°å“è¶Šçš„ä¸»è§‚æ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä¸‰ç»´åœºæ™¯è¡¥å…¨åœ¨è™šæ‹Ÿç°å®å’Œå»ºç­‘å¯è§†åŒ–ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨360Â°æ— ç•Œåœºæ™¯çš„è§†å›¾å’Œå‡ ä½•å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>AuraFusion360æ˜¯ä¸€ç§æ–°çš„åŸºäºå‚è€ƒçš„æ–¹æ³•ï¼Œç”¨äºé«˜è´¨é‡çš„å¯¹è±¡ç§»é™¤å’Œç©ºæ´å¡«å……åœ¨ä¸‰ç»´åœºæ™¯ä¸­ã€‚</li>
<li>AuraFusion360å¼•å…¥äº†æ·±åº¦æ„ŸçŸ¥æœªè§æ©æ¨¡ç”Ÿæˆã€è‡ªé€‚åº”å¼•å¯¼æ·±åº¦æ‰©æ•£å’ŒSDEditç»†èŠ‚å¢å¼ºç­‰æŠ€æœ¯ã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªä¸º360Â°æ— ç•Œåœºæ™¯è¡¥å…¨è®¾è®¡çš„ç»¼åˆæ•°æ®é›†360-USIDã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAuraFusion360åœ¨è§†ç‚¹å˜åŒ–å‰§çƒˆçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒå‡ ä½•å‡†ç¡®æ€§ï¼Œå®ç°å“è¶Šçš„ä¸»è§‚æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>AuraFusion360æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63f1297390c09a6603d14b4b780fec44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75201d279e878ef6871431ce60a2294f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc67871e8574ddcaf04a30559721e16d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6954822733a26b950ff878703bc48008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c9b4fae92ab66deb55d1fdcff1e7ecd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9cf5ef9c4a6e6feaa9f0d4548d367e4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Instruct-4DGS-Efficient-Dynamic-Scene-Editing-via-4D-Gaussian-based-Static-Dynamic-Separation"><a href="#Instruct-4DGS-Efficient-Dynamic-Scene-Editing-via-4D-Gaussian-based-Static-Dynamic-Separation" class="headerlink" title="Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based   Static-Dynamic Separation"></a>Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based   Static-Dynamic Separation</h2><p><strong>Authors:Joohyun Kwon, Hanbyel Cho, Junmo Kim</strong></p>
<p>Recent 4D dynamic scene editing methods require editing thousands of 2D images used for dynamic scene synthesis and updating the entire scene with additional training loops, resulting in several hours of processing to edit a single dynamic scene. Therefore, these methods are not scalable with respect to the temporal dimension of the dynamic scene (i.e., the number of timesteps). In this work, we propose Instruct-4DGS, an efficient dynamic scene editing method that is more scalable in terms of temporal dimension. To achieve computational efficiency, we leverage a 4D Gaussian representation that models a 4D dynamic scene by combining static 3D Gaussians with a Hexplane-based deformation field, which captures dynamic information. We then perform editing solely on the static 3D Gaussians, which is the minimal but sufficient component required for visual editing. To resolve the misalignment between the edited 3D Gaussians and the deformation field, which may arise from the editing process, we introduce a refinement stage using a score distillation mechanism. Extensive editing results demonstrate that Instruct-4DGS is efficient, reducing editing time by more than half compared to existing methods while achieving high-quality edits that better follow user instructions. </p>
<blockquote>
<p>æœ€æ–°çš„4DåŠ¨æ€åœºæ™¯ç¼–è¾‘æ–¹æ³•éœ€è¦å¯¹ç”¨äºåŠ¨æ€åœºæ™¯åˆæˆçš„æ•°åƒå¼ 2Då›¾åƒè¿›è¡Œç¼–è¾‘ï¼Œå¹¶é€šè¿‡é¢å¤–çš„è®­ç»ƒå¾ªç¯æ›´æ–°æ•´ä¸ªåœºæ™¯ï¼Œè¿™å¯¼è‡´ç¼–è¾‘å•ä¸ªåŠ¨æ€åœºæ™¯éœ€è¦æ•°å°æ—¶çš„å¤„ç†æ—¶é—´ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯çš„æ—¶é—´ç»´åº¦ï¼ˆå³æ—¶æ­¥æ•°ï¼‰ä¸Šå¹¶ä¸å¯æ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Instruct-4DGSï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŠ¨æ€åœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼Œåœ¨æ—¶é—´ç»´åº¦ä¸Šæ›´å…·å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å®ç°è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§4Dé«˜æ–¯è¡¨ç¤ºæ³•ï¼Œé€šè¿‡ç»“åˆé™æ€3Dé«˜æ–¯å’ŒåŸºäºå…­å¹³é¢çš„å˜å½¢åœºæ¥æ¨¡æ‹Ÿ4DåŠ¨æ€åœºæ™¯ï¼Œæ•æ‰åŠ¨æ€ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»…åœ¨é™æ€3Dé«˜æ–¯ä¸Šè¿›è¡Œç¼–è¾‘ï¼Œè¿™æ˜¯è§†è§‰ç¼–è¾‘æ‰€éœ€çš„æœ€å°ä½†è¶³å¤Ÿçš„ç»„ä»¶ã€‚ä¸ºäº†è§£å†³ç¼–è¾‘åçš„3Dé«˜æ–¯å’Œå˜å½¢åœºä¹‹é—´å¯èƒ½å‡ºç°çš„å¯¹é½é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨è¯„åˆ†è’¸é¦æœºåˆ¶çš„ç»†åŒ–é˜¶æ®µã€‚å¤§é‡çš„ç¼–è¾‘ç»“æœè¯æ˜ï¼ŒInstruct-4DGSéå¸¸é«˜æ•ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œç¼–è¾‘æ—¶é—´å‡å°‘äº†ä¸€åŠä»¥ä¸Šï¼ŒåŒæ—¶å®ç°äº†é«˜è´¨é‡çš„ç¼–è¾‘ï¼Œæ›´å¥½åœ°éµå¾ªäº†ç”¨æˆ·çš„æŒ‡ä»¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02091v2">PDF</a> Accepted to CVPR 2025. The first two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„åŠ¨æ€åœºæ™¯ç¼–è¾‘æ–¹æ³•â€”â€”Instruct-4DGSï¼Œé€šè¿‡åˆ©ç”¨4Dé«˜æ–¯è¡¨ç¤ºæ¥æ¨¡æ‹ŸåŠ¨æ€åœºæ™¯ï¼Œåªç¼–è¾‘é™æ€çš„3Dé«˜æ–¯ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶è§£å†³äº†ç¼–è¾‘è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä¸å˜å½¢åœºçš„ä¸å¯¹é½é—®é¢˜ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒInstruct-4DGSåœ¨ç¼–è¾‘æ•ˆç‡ä¸Šæœ‰æ‰€æå‡ï¼Œç¼©çŸ­äº†è¶…è¿‡ä¸€åŠçš„ç¼–è¾‘æ—¶é—´ï¼ŒåŒæ—¶ä¿è¯äº†é«˜è´¨é‡çš„ç¼–è¾‘æ•ˆæœï¼Œæ›´å¥½åœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Instruct-4DGSæ˜¯ä¸€ç§é’ˆå¯¹åŠ¨æ€åœºæ™¯çš„é«˜æ•ˆç¼–è¾‘æ–¹æ³•ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨4Dé«˜æ–¯è¡¨ç¤ºæ¨¡æ‹ŸåŠ¨æ€åœºæ™¯ï¼Œç»“åˆäº†é™æ€3Dé«˜æ–¯å’ŒåŸºäºHexplaneçš„å˜å½¢åœºã€‚</li>
<li>ç¼–è¾‘è¿‡ç¨‹ä¸»è¦é’ˆå¯¹é™æ€çš„3Dé«˜æ–¯è¿›è¡Œï¼Œè¿™æ˜¯è§†è§‰ç¼–è¾‘æ‰€éœ€çš„æœ€å°ä½†è¶³å¤Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>ä¸ºäº†è§£å†³ç¼–è¾‘è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä¸å˜å½¢åœºçš„ä¸å¯¹é½é—®é¢˜ï¼Œå¼•å…¥äº†åŸºäºåˆ†æ•°è’¸é¦æœºåˆ¶çš„ç»†åŒ–é˜¶æ®µã€‚</li>
<li>Instruct-4DGSåœ¨ç¼–è¾‘æ•ˆç‡ä¸Šæœ‰æ‰€æå‡ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ç¼©çŸ­äº†è¶…è¿‡ä¸€åŠçš„ç¼–è¾‘æ—¶é—´ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯é«˜è´¨é‡ç¼–è¾‘æ•ˆæœçš„åŒæ—¶ï¼Œæ›´å¥½åœ°éµå¾ªäº†ç”¨æˆ·æŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23f16710ebab7ad5821312d637022dbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d06d97a6a06d81f3261bfb33b2ed4644.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d373dba2b2e0eaced75c9e267c6117fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d0a47a1eecb653cfabe528e0b375132.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="3DGUT-Enabling-Distorted-Cameras-and-Secondary-Rays-in-Gaussian-Splatting"><a href="#3DGUT-Enabling-Distorted-Cameras-and-Secondary-Rays-in-Gaussian-Splatting" class="headerlink" title="3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian   Splatting"></a>3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian   Splatting</h2><p><strong>Authors:Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, Zan Gojcic</strong></p>
<p>3D Gaussian Splatting (3DGS) enables efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing the particles instead, but, this comes at the cost of significantly slower rendering. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/nv-tlabs/3dgrut">https://github.com/nv-tlabs/3dgrut</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰èƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°å¤æ‚åœºæ™¯çš„é«˜æ•ˆé‡å»ºå’Œé«˜ä¿çœŸå®æ—¶æ¸²æŸ“ã€‚ç„¶è€Œï¼Œç”±äºå®ƒåŸºäºå…‰çº¿è¿½è¸ªçš„æŠ€æœ¯å®ç°ï¼Œ3DGSä»…é™äºç†æƒ³çš„é’ˆå­”ç›¸æœºï¼Œä¸æ”¯æŒæ¬¡å…‰æºæ•ˆæœã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡è·Ÿè¸ªç²’å­æ¥è§£å†³è¿™äº›é™åˆ¶ï¼Œä½†è¿™ä¼šæ˜¾è‘—å‡æ…¢æ¸²æŸ“é€Ÿåº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡º3Dé«˜æ–¯æ— è¿¹å˜æ¢ï¼ˆ3DGUTï¼‰ï¼Œç”¨æ— è¿¹å˜æ¢ä»£æ›¿EWAè´´å›¾å…¬å¼ï¼Œé€šè¿‡sigmaç‚¹è¿‘ä¼¼ç²’å­ï¼Œå¯ä»¥ç²¾ç¡®æŠ•å½±åœ¨ä»»ä½•éçº¿æ€§æŠ•å½±å‡½æ•°ä¸‹ã€‚è¿™ä¸€æ”¹è¿›ä½¿å¾—è½»æ¾æ”¯æŒå…·æœ‰æ—¶é—´ä¾èµ–æ•ˆæœçš„å¤±çœŸç›¸æœºæˆä¸ºå¯èƒ½ï¼Œå¦‚æ»šåŠ¨å¿«é—¨ï¼ŒåŒæ—¶ä¿ç•™äº†å…‰çº¿è¿½è¸ªçš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¸²æŸ“å…¬å¼ä¸åŸºäºå…‰çº¿è¿½è¸ªçš„æ–¹æ³•å¯¹é½ï¼Œæ”¯æŒæ¬¡çº§å…‰çº¿è¿½è¸ªï¼Œä»¥åœ¨ç›¸åŒçš„3Dè¡¨ç¤ºä¸­è¡¨ç¤ºåå°„å’ŒæŠ˜å°„ç­‰ç°è±¡ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nv-tlabs/3dgrut%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nv-tlabs/3dgrutæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12507v2">PDF</a> Our paper has been accepted by CVPR 2025. For more details and   updates, please visit our project website:   <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/toronto-ai/3DGUT">https://research.nvidia.com/labs/toronto-ai/3DGUT</a></p>
<p><strong>Summary</strong></p>
<p>3D Gaussian Unscented Transformï¼ˆ3DGUTï¼‰æ–¹æ³•æ”¹è¿›äº†3D Gaussian Splattingï¼ˆ3DGSï¼‰ï¼Œä½¿å…¶æ”¯æŒå¤æ‚ç›¸æœºæ¨¡å‹çš„æ¸²æŸ“ï¼ŒåŒ…æ‹¬å¤±çœŸç›¸æœºå’Œæ—¶é—´ä¾èµ–æ•ˆæœï¼Œå¦‚æ»šåŠ¨å¿«é—¨ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†å…‰æ …åŒ–çš„æ•ˆç‡ï¼Œå¹¶å®ç°äº†ä¸è¿½è¸ªæ–¹æ³•ç›¸ç¬¦çš„æ¸²æŸ“å…¬å¼ï¼Œæ”¯æŒäºŒæ¬¡å…‰çº¿è¿½è¸ªï¼Œä»¥å‘ˆç°åå°„å’ŒæŠ˜å°„ç­‰ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGUTä½¿ç”¨Unscented Transformæ›¿ä»£EWA splattingå…¬å¼ï¼Œä»¥é€šè¿‡sigmaç‚¹è¿‘ä¼¼ç²’å­ã€‚</li>
<li>è¿™ç§æ–¹æ³•æ”¯æŒä»»ä½•éçº¿æ€§æŠ•å½±å‡½æ•°ä¸‹çš„ç²’å­æŠ•å½±ï¼ŒåŒ…æ‹¬æ‰­æ›²ç›¸æœºå’Œæ—¶é—´ä¾èµ–æ•ˆæœï¼Œå¦‚æ»šåŠ¨å¿«é—¨ã€‚</li>
<li>3DGUTä¿ç•™äº†å…‰æ …åŒ–çš„æ•ˆç‡ã€‚</li>
<li>å®ç°äº†ä¸è¿½è¸ªæ–¹æ³•ç›¸ç¬¦çš„æ¸²æŸ“å…¬å¼ï¼Œæ”¯æŒäºŒæ¬¡å…‰çº¿è¿½è¸ªã€‚</li>
<li>3DGUTèƒ½å¤Ÿåœ¨åŒä¸€3Dè¡¨ç¤ºä¸­å‘ˆç°åå°„å’ŒæŠ˜å°„ç­‰ç°è±¡ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2565432180f09820e904a34ed119d071.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42c2c460d10052ff6462b3b1a50cd2b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea35ab74b60d6ef74d1d6d7cb938c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f26b424bbc049ffd4119d9164d10078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a76439ae26f0f97e4ea58eb53e9e3c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives"><a href="#Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives" class="headerlink" title="Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives"></a>Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives</h2><p><strong>Authors:Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein</strong></p>
<p>3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS to substantially improve rendering speed. These improvements also yield the ancillary benefits of reduced model size and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\mathit{6.71\times}$ across scenes from the Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets. </p>
<blockquote>
<p>3Dé«˜æ–¯å±•å¼€ï¼ˆ3D-GSï¼‰æ˜¯ä¸€ç§æœ€æ–°çš„3Dåœºæ™¯é‡å»ºæŠ€æœ¯ï¼Œå®ƒé€šè¿‡æŠŠåœºæ™¯å»ºæ¨¡ä¸ºå¯å¾®åˆ†çš„3Dé«˜æ–¯å‚æ•°ç‚¹äº‘æ¥å®ç°å®æ—¶æ¸²æŸ“æ–°çš„è§†è§’ã€‚ç„¶è€Œï¼Œå…¶æ¸²æŸ“é€Ÿåº¦å’Œæ¨¡å‹å¤§å°ä»å­˜åœ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«å’Œè§£å†³äº†3D-GSä¸­çš„ä¸¤ä¸ªå…³é”®ä½æ•ˆç‡é—®é¢˜ï¼Œä»¥æ˜¾è‘—æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ”¹è¿›ä¹Ÿå¸¦æ¥äº†å‡å°æ¨¡å‹å¤§å°å’Œç¼©çŸ­è®­ç»ƒæ—¶é—´çš„é¢å¤–å¥½å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æ¸²æŸ“æµç¨‹ï¼Œç²¾ç¡®åœ°åœ¨åœºæ™¯ä¸­å®šä½é«˜æ–¯åˆ†å¸ƒï¼Œæé«˜æ¸²æŸ“é€Ÿåº¦è€Œä¸å½±å“è§†è§‰ä¿çœŸåº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¿®å‰ªæŠ€æœ¯å¹¶å°†å…¶é›†æˆåˆ°è®­ç»ƒæµç¨‹ä¸­ï¼Œåœ¨è¿›ä¸€æ­¥åŠ å¿«æ¸²æŸ“é€Ÿåº¦çš„åŒæ—¶ï¼Œå¤§å¤§å‡å°‘äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„Speedy-Splatæ–¹æ³•ç»“åˆäº†è¿™äº›æŠ€æœ¯ï¼Œåœ¨Mip-NeRF 360ã€Tanksï¼†Templeså’ŒDeep Blendingæ•°æ®é›†çš„åœºæ™¯ä¸­å¹³å‡æ¸²æŸ“é€Ÿåº¦æé«˜äº†æƒŠäººçš„6.71å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00578v2">PDF</a> CVPR 2025, Project Page: <a target="_blank" rel="noopener" href="https://speedysplat.github.io/">https://speedysplat.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºé«˜æ–¯æ¨¡å‹çš„ä¸‰ç»´åœºæ™¯é‡å»ºæŠ€æœ¯ï¼ˆ3D Gaussian Splattingï¼Œç®€ç§°3D-GSï¼‰ã€‚è¯¥æŠ€æœ¯èƒ½å®æ—¶æ¸²æŸ“æ–°çš„è§†è§’ï¼Œé€šè¿‡å‚æ•°åŒ–å°†åœºæ™¯è¡¨ç¤ºä¸ºå¯åˆ†åŒ–çš„ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ã€‚ä¸ºæ”¹å–„æ¸²æŸ“é€Ÿåº¦å’Œæ¨¡å‹å¤§å°ç“¶é¢ˆï¼Œæ–‡ç« ä¼˜åŒ–äº†æ¸²æŸ“ç®¡é“å’Œé«˜æ–¯åœºæ™¯ä¸­çš„ç²¾ç¡®å®šä½æ¥æå‡æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ–°å‹çš„å‰ªææŠ€æœ¯å¹¶é›†æˆåˆ°è®­ç»ƒç®¡é“ä¸­ï¼Œæ˜¾è‘—å‡å°äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚æå‡ºçš„Speedy-Splatæ–¹æ³•èƒ½å¤§å¹…æå‡åœ¨Mip-NeRF 360ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡æ¸²æŸ“é€Ÿåº¦è¾¾$\times 6.71$å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D Gaussian Splattingæ˜¯ä¸€ç§å®æ—¶æ¸²æŸ“æŠ€æœ¯ï¼ŒåŸºäºå‚æ•°åŒ–çš„ä¸‰ç»´é«˜æ–¯ç‚¹äº‘å»ºæ¨¡åœºæ™¯ã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡ä¼˜åŒ–æ¸²æŸ“ç®¡é“å’Œé«˜æ–¯å®šä½æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚</li>
<li>æ–°å¼•å…¥çš„å‰ªææŠ€æœ¯æœ‰åŠ©äºå‡å°‘æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>Speedy-Splatæ–¹æ³•ç»“åˆäº†ä¸Šè¿°æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0312e3d05a4d9cfbf73e61f5aff10dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4aa5af078a50a81b2fa7a82fb96f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0779e7661672a9a13f06c4514c1211c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48bca12f73986c1407d0764da3620e60.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UrbanCAD-Towards-Highly-Controllable-and-Photorealistic-3D-Vehicles-for-Urban-Scene-Simulation"><a href="#UrbanCAD-Towards-Highly-Controllable-and-Photorealistic-3D-Vehicles-for-Urban-Scene-Simulation" class="headerlink" title="UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for   Urban Scene Simulation"></a>UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for   Urban Scene Simulation</h2><p><strong>Authors:Yichong Lu, Yichi Cai, Shangzhan Zhang, Hongyu Zhou, Haoji Hu, Huimin Yu, Andreas Geiger, Yiyi Liao</strong></p>
<p>Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that generates highly controllable and photorealistic 3D vehicle digital twins from a single urban image, leveraging a large collection of free 3D CAD models and handcrafted materials. To achieve this, we propose a novel pipeline that follows a retrieval-optimization manner, adapting to observational data while preserving fine-grained expert-designed priors for both geometry and material. This enables vehiclesâ€™ realistic 360-degree rendering, background insertion, material transfer, relighting, and component manipulation. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications. </p>
<blockquote>
<p>é«˜ç²¾åº¦çš„å¯æ“æ§ä¸‰ç»´è½¦è¾†æ¨¡å‹å¯¹äºè‡ªåŠ¨é©¾é©¶ä»¿çœŸå’Œæ•°æ®å¢å¼ºè‡³å…³é‡è¦ã€‚è™½ç„¶æ‰‹å·¥åˆ¶ä½œçš„CADæ¨¡å‹æä¾›äº†çµæ´»çš„å¯æ“æ§æ€§ï¼Œä½†å…è´¹çš„CADåº“é€šå¸¸ç¼ºä¹å®ç°é€¼çœŸçš„æ¸²æŸ“æ‰€éœ€çš„é«˜è´¨é‡æè´¨ã€‚ç›¸åï¼Œé‡å»ºçš„ä¸‰ç»´æ¨¡å‹è™½ç„¶æä¾›äº†é«˜ä¿çœŸæ¸²æŸ“ï¼Œä½†ç¼ºä¹å¯æ“æ§æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UrbanCADæ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»å•ä¸ªåŸå¸‚å›¾åƒç”Ÿæˆé«˜åº¦å¯æ§ä¸”é€¼çœŸçš„ä¸‰ç»´è½¦è¾†æ•°å­—åŒèƒèƒï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº†å¤§é‡çš„å…è´¹ä¸‰ç»´CADæ¨¡å‹å’Œæ‰‹å·¥åˆ¶ä½œçš„æè´¨ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æµç¨‹ç®¡é“ï¼Œéµå¾ªæ£€ç´¢ä¼˜åŒ–æ–¹å¼ï¼Œæ—¢é€‚åº”è§‚æµ‹æ•°æ®åˆä¿ç•™å‡ ä½•å’Œæè´¨çš„ç²¾ç»†ä¸“å®¶è®¾è®¡å…ˆéªŒçŸ¥è¯†ã€‚è¿™èƒ½å¤Ÿå®ç°è½¦è¾†çš„360åº¦çœŸå®æ¸²æŸ“ã€èƒŒæ™¯æ’å…¥ã€æè´¨è½¬æ¢ã€é‡æ–°ç…§æ˜å’Œç»„ä»¶æ“ä½œã€‚æ­¤å¤–ï¼Œé€šè¿‡é±¼çœ¼å›¾åƒè¿‘ä¼¼ç¯å¢ƒç…§æ˜å¹¶é€šè¿‡ä¸‰ç»´å‡ ä½•æ‰«æé‡å»ºèƒŒæ™¯ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰å¤šè§†è§’èƒŒæ™¯é€è§†å’Œé±¼çœ¼å›¾åƒçš„æƒ…å†µä¸‹ï¼Œå°†ä¼˜åŒ–åçš„CADæ¨¡å‹é€¼çœŸåœ°æ’å…¥åˆ°æ¸²æŸ“çš„æ–°è§†è§’èƒŒæ™¯ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€¼çœŸåº¦æ–¹é¢ï¼ŒUrbanCADä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œåœ¨UrbanCADçš„ç¬¦åˆåˆ†å¸ƒé…ç½®ä¸­è¯„ä¼°æ—¶ï¼Œå„ç§æ„ŸçŸ¥æ¨¡å‹çš„å‡†ç¡®æ€§å¾—ä»¥ä¿æŒï¼Œä½†åº”ç”¨äºæˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„é€¼çœŸåº¦è¾ƒé«˜çš„éåˆ†å¸ƒæ•°æ®æ—¶ï¼Œå…¶æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚è¿™è¡¨æ˜UrbanCADåœ¨åˆ›å»ºé€¼çœŸçš„ã€å¯¹å®‰å…¨è‡³å…³é‡è¦çš„é©¾é©¶åœºæ™¯ä»¥è¿›è¡Œä¸‹æ¸¸åº”ç”¨æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19292v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://xdimlab.github.io/UrbanCAD/">https://xdimlab.github.io/UrbanCAD/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è‡ªä¸»é©¾é©¶æ¨¡æ‹Ÿå’Œæ•°æ®å¢å¼ºä¸­éœ€è¦å…·æœ‰é«˜åº¦å¯æ§æ€§å’Œé€¼çœŸåº¦çš„ä¸‰ç»´è½¦è¾†æ¨¡å‹ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºUrbanCADçš„æ–°æ¡†æ¶ï¼Œå¯ä»å•ä¸€åŸå¸‚å›¾åƒç”Ÿæˆé«˜åº¦å¯æ§ä¸”é€¼çœŸçš„ä¸‰ç»´è½¦è¾†æ•°å­—åŒèƒèƒã€‚è¯¥æ¡†æ¶ç»“åˆå¤§é‡å…è´¹çš„3D CADæ¨¡å‹å’Œæ‰‹å·¥ææ–™ï¼Œé€šè¿‡æ£€ç´¢ä¼˜åŒ–æµç¨‹ï¼Œåœ¨ä¿ç•™ç²¾ç»†å‡ ä½•å’Œææ–™å…ˆéªŒçŸ¥è¯†çš„åŒæ—¶é€‚åº”è§‚æµ‹æ•°æ®ã€‚UrbanCADèƒ½å¤Ÿå®ç°è½¦è¾†çš„360åº¦æ¸²æŸ“ã€èƒŒæ™¯æ’å…¥ã€ææ–™è½¬ç§»ã€é‡æ–°ç…§æ˜å’Œç»„ä»¶æ“ä½œç­‰åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©å¤šè§†è§’èƒŒæ™¯é€è§†å’Œé±¼çœ¼å›¾åƒï¼ŒUrbanCADåˆ©ç”¨é±¼çœ¼å›¾åƒè¿‘ä¼¼ç¯å¢ƒç…§æ˜å¹¶é‡å»ºèƒŒæ™¯ï¼Œå®ç°äº†ä¼˜åŒ–CADæ¨¡å‹åœ¨æ–°å‹è§†å›¾èƒŒæ™¯ä¸­çš„é€¼çœŸæ’å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUrbanCADåœ¨é€¼çœŸåº¦æ–¹é¢è¶…è¶Šäº†åŸºå‡†æ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æé«˜ä¸‹æ¸¸åº”ç”¨ä¸­çš„å®‰å…¨å…³é”®é©¾é©¶åœºæ™¯çš„é€¼çœŸåº¦å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åº¦å¯æ§å’Œé€¼çœŸçš„3Dè½¦è¾†æ¨¡å‹å¯¹è‡ªä¸»é©¾é©¶æ¨¡æ‹Ÿå’Œæ•°æ®å¢å¼ºè‡³å…³é‡è¦ã€‚</li>
<li>UrbanCADæ¡†æ¶èƒ½ä»å•ä¸€åŸå¸‚å›¾åƒç”Ÿæˆé«˜åº¦å¯æ§ä¸”é€¼çœŸçš„3Dè½¦è¾†æ•°å­—åŒèƒèƒã€‚</li>
<li>UrbanCADç»“åˆäº†å…è´¹CADæ¨¡å‹å’Œæ‰‹å·¥ææ–™ï¼Œé€šè¿‡æ£€ç´¢ä¼˜åŒ–æµç¨‹å®ç°é€¼çœŸæ¸²æŸ“ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å®ç°è½¦è¾†çš„360åº¦æ¸²æŸ“ã€èƒŒæ™¯æ’å…¥ã€ææ–™è½¬ç§»ã€é‡æ–°ç…§æ˜å’Œç»„ä»¶æ“ä½œã€‚</li>
<li>UrbanCADåˆ©ç”¨å¤šè§†è§’èƒŒæ™¯é€è§†å’Œé±¼çœ¼å›¾åƒé‡å»ºèƒŒæ™¯ï¼Œå®ç°ä¼˜åŒ–CADæ¨¡å‹åœ¨æ–°å‹è§†å›¾ä¸­çš„æ’å…¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUrbanCADåœ¨é€¼çœŸåº¦æ–¹é¢è¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f09c9041fdc1020adaa87ce5e3738027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04101a745c47a4e8b12825335f69aeb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a016785bc5fbd582b944b6eaa68125a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07433c3fe6aa0c0c9f23104e3d26dec4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SelfSplat-Pose-Free-and-3D-Prior-Free-Generalizable-3D-Gaussian-Splatting"><a href="#SelfSplat-Pose-Free-and-3D-Prior-Free-Generalizable-3D-Gaussian-Splatting" class="headerlink" title="SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian   Splatting"></a>SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian   Splatting</h2><p><strong>Authors:Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Sangpil Kim, Hyeonsoo Im, Sangheon Shin, Eunbyung Park</strong></p>
<p>We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/">https://gynjn.github.io/selfsplat/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SelfSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´é«˜æ–¯æ‘Šé“ºæ¨¡å‹ï¼Œæ—¨åœ¨ä»æœªæ‘†æ”¾çš„å¤šè§†è§’å›¾åƒä¸­æ‰§è¡Œæ— éœ€å§¿åŠ¿å’Œä¸‰ç»´å…ˆéªŒçš„å¯æ³›åŒ–çš„ä¸‰ç»´é‡å»ºã€‚è¿™äº›è®¾ç½®å› ç¼ºä¹çœŸå®æ•°æ®ã€å­¦ä¹ åˆ°çš„å‡ ä½•ä¿¡æ¯å’Œæ— éœ€å¾®è°ƒå³å¯å®ç°ç²¾ç¡®ä¸‰ç»´é‡å»ºçš„éœ€æ±‚è€Œæœ¬è´¨ä¸Šæ˜¯ç—…æ€çš„ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å®ç°é«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æœ‰æ•ˆåœ°å°†æ˜¾å¼ä¸‰ç»´è¡¨ç¤ºä¸è‡ªç›‘ç£çš„æ·±åº¦å’Œå§¿æ€ä¼°è®¡æŠ€æœ¯ç›¸ç»“åˆï¼Œè§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œä»è€Œåœ¨å§¿æ€å‡†ç¡®æ€§å’Œä¸‰ç»´é‡å»ºè´¨é‡æ–¹é¢éƒ½å®ç°äº†ç›¸äº’æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…·æœ‰åŒ¹é…æ„è¯†çš„å§¿æ€ä¼°è®¡ç½‘ç»œå’Œæ·±åº¦ç»†åŒ–æ¨¡å—ï¼Œä»¥æé«˜è·¨è§†å›¾çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œç¡®ä¿æ›´å‡†ç¡®ã€æ›´ç¨³å®šçš„ä¸‰ç»´é‡å»ºã€‚ä¸ºäº†å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„è¡¨ç°ï¼Œæˆ‘ä»¬åœ¨å¤§è§„æ¨¡ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬RealEstate10Kã€ACIDå’ŒDL3DVã€‚SelfSplatåœ¨å¤–è§‚å’Œå‡ ä½•è´¨é‡æ–¹é¢å‡è¾¾åˆ°äº†ä¼˜äºä»¥å‰æœ€å…ˆè¿›æ–¹æ³•çš„ç»“æœï¼Œå¹¶è¡¨ç°å‡ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œåˆ†æä¹ŸéªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://gynjn.github.io/selfsplat/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17190v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gynjn.github.io/selfsplat/">https://gynjn.github.io/selfsplat/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SelfSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´é«˜æ–¯æ¨¡ç³Šæ¨¡å‹ï¼Œç”¨äºå®ç°æ— éœ€å§¿åŠ¿å’Œä¸‰ç»´å…ˆéªŒçŸ¥è¯†çš„é€šç”¨ä¸‰ç»´é‡å»ºï¼Œå¯ä»æœªå®šä½çš„å¤šè§†è§’å›¾åƒä¸­è¿›è¡Œã€‚è¯¥æ¨¡å‹é€šè¿‡æœ‰æ•ˆç»“åˆæ˜¾å¼ä¸‰ç»´è¡¨ç¤ºå’Œè‡ªç›‘ç£çš„æ·±åº¦ä¸å§¿æ€ä¼°è®¡æŠ€æœ¯ï¼Œè§£å†³äº†ç¼ºä¹çœŸå®æ•°æ®ã€å‡ ä½•ä¿¡æ¯éš¾ä»¥å‡†ç¡®è¿›è¡Œä¸‰ç»´é‡å»ºçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†åŒ¹é…æ„ŸçŸ¥å§¿æ€ä¼°è®¡ç½‘ç»œå’Œæ·±åº¦ä¼˜åŒ–æ¨¡å—ï¼Œç¡®ä¿è·¨è§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œå®ç°æ›´å‡†ç¡®ç¨³å®šçš„ä¸‰ç»´é‡å»ºã€‚åœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSelfSplatåœ¨å¤–è§‚å’Œå‡ ä½•è´¨é‡æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SelfSplatæ˜¯ä¸€ç§æ–°å‹3D Gaussian Splattingæ¨¡å‹ï¼Œç”¨äºä»å¤šè§†è§’å›¾åƒè¿›è¡Œ3Dé‡å»ºã€‚</li>
<li>æ¨¡å‹èƒ½åœ¨æ— å§¿åŠ¿å’Œ3Då…ˆéªŒçŸ¥è¯†çš„æ¡ä»¶ä¸‹è¿›è¡Œé€šç”¨ä¸‰ç»´é‡å»ºã€‚</li>
<li>è¯¥æ¨¡å‹è§£å†³äº†ç”±äºç¼ºä¹çœŸå®æ•°æ®å’Œå‡ ä½•ä¿¡æ¯è€Œå¯¼è‡´çš„å›ºæœ‰ä¸é€‚å®šé—®é¢˜ã€‚</li>
<li>SelfSplaté€šè¿‡ç»“åˆæ˜¾å¼3Dè¡¨ç¤ºå’Œè‡ªç›‘ç£æŠ€æœ¯ï¼Œæé«˜äº†å§¿æ€å‡†ç¡®æ€§å’Œ3Dé‡å»ºè´¨é‡ã€‚</li>
<li>å§¿æ€ä¼°è®¡ç½‘ç»œå’Œæ·±åº¦ä¼˜åŒ–æ¨¡å—çš„èåˆç¡®ä¿äº†è·¨è§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-297366d449a7425e1aed30d9b2176401.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-010093d9b72f6230f5992418e15cd1c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60abd01a82e63e279de79c28c1e12a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c288f90d43e8f383e302879a6aef74ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00980a56c3cad7fef54ca3a989e662a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5a301387b3715b0169537bd7aec78fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa8785ec118c6a612c676cc71fc564a2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SpectroMotion-Dynamic-3D-Reconstruction-of-Specular-Scenes"><a href="#SpectroMotion-Dynamic-3D-Reconstruction-of-Specular-Scenes" class="headerlink" title="SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes"></a>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</h2><p><strong>Authors:Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, Yu-Lun Liu</strong></p>
<p>We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to represent specular surfaces accurately. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy significantly enhancing scene geometry and specular color prediction. It is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SpectroMotionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå®ƒå°†3Dé«˜æ–¯æ‘Šé“ºï¼ˆ3DGSï¼‰ä¸åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰å’Œå˜å½¢åœºç›¸ç»“åˆï¼Œä»¥é‡å»ºåŠ¨æ€é•œé¢åœºæ™¯ã€‚ä¹‹å‰å°†3DGSæ‰©å±•åˆ°åŠ¨æ€åœºæ™¯å»ºæ¨¡çš„æ–¹æ³•åœ¨å‡†ç¡®è¡¨ç¤ºé•œé¢è¡¨é¢æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥æ®‹å·®æ ¡æ­£æŠ€æœ¯æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œä»¥åœ¨å˜å½¢è¿‡ç¨‹ä¸­è¿›è¡Œå‡†ç¡®çš„è¡¨é¢æ³•çº¿è®¡ç®—ï¼Œè¾…ä»¥å¯é€‚åº”æ—¶é—´å˜åŒ–ç…§æ˜æ¡ä»¶çš„å¯å˜å½¢ç¯å¢ƒè´´å›¾ã€‚æˆ‘ä»¬å®ç°äº†ä»ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯å‡ ä½•å’Œé•œé¢è‰²å½©é¢„æµ‹èƒ½åŠ›ã€‚å®ƒæ˜¯ç›®å‰å”¯ä¸€èƒ½å¤Ÿå°†3DGSæ–¹æ³•ç”¨äºåˆæˆé€¼çœŸç°å®ä¸–ç•ŒåŠ¨æ€é•œé¢åœºæ™¯çš„ç°æœ‰æŠ€æœ¯ï¼Œåœ¨å‘ˆç°å¤æ‚ã€åŠ¨æ€å’Œé•œé¢åœºæ™¯æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17249v3">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://cdfan0627.github.io/spectromotion/">https://cdfan0627.github.io/spectromotion/</a></p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºSpectroMotionçš„æ–°æ–¹æ³•ï¼Œç»“åˆ3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ã€åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰å’Œå˜å½¢åœºï¼Œä»¥é‡å»ºåŠ¨æ€é•œé¢åœºæ™¯ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»¥å¾€ä½¿ç”¨3DGSå»ºæ¨¡åŠ¨æ€åœºæ™¯æ—¶éš¾ä»¥å‡†ç¡®è¡¨ç¤ºé•œé¢è¡¨é¢çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ®‹å·®ä¿®æ­£æŠ€æœ¯å’Œå¯å˜å½¢ç¯å¢ƒæ˜ å°„ï¼Œè¯¥æ–¹æ³•åœ¨å˜å½¢è¿‡ç¨‹ä¸­èƒ½å‡†ç¡®è®¡ç®—è¡¨é¢æ³•çº¿ï¼Œå¹¶é€‚åº”éšæ—¶é—´å˜åŒ–çš„å…‰ç…§æ¡ä»¶ã€‚é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜åœºæ™¯å‡ ä½•å’Œé•œé¢é¢œè‰²çš„é¢„æµ‹æ•ˆæœã€‚è¯¥æ–¹æ³•æ˜¯ç›®å‰å”¯ä¸€èƒ½å¤Ÿåˆæˆé€¼çœŸåŠ¨æ€é•œé¢åœºæ™¯çš„3DGSæ–¹æ³•ï¼Œåœ¨æ¸²æŸ“å¤æ‚ã€åŠ¨æ€å’Œé•œé¢åœºæ™¯æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SpectroMotionç»“åˆ3DGSã€PBRå’Œå˜å½¢åœºæŠ€æœ¯ï¼Œç”¨äºé‡å»ºåŠ¨æ€é•œé¢åœºæ™¯ã€‚</li>
<li>å¼•å…¥æ®‹å·®ä¿®æ­£æŠ€æœ¯ï¼Œå‡†ç¡®è®¡ç®—è¡¨é¢æ³•çº¿ï¼Œè§£å†³ä»¥å¾€æ–¹æ³•çš„å±€é™ã€‚</li>
<li>å¯å˜å½¢ç¯å¢ƒæ˜ å°„é€‚åº”éšæ—¶é—´å˜åŒ–çš„å…‰ç…§æ¡ä»¶ã€‚</li>
<li>ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥æé«˜äº†åœºæ™¯å‡ ä½•å’Œé•œé¢é¢œè‰²é¢„æµ‹çš„ç²¾åº¦ã€‚</li>
<li>ç›®å‰å”¯ä¸€èƒ½å¤Ÿåˆæˆé€¼çœŸåŠ¨æ€é•œé¢åœºæ™¯çš„3DGSæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ¸²æŸ“å¤æ‚ã€åŠ¨æ€å’Œé•œé¢åœºæ™¯æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-952d00c55fe995e0feab67eb01a3ecc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557e28720e142d8d00a13639c82d500e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f2b0ea9be9f6642b1e3bd8b8db8d180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cbc11c3c38b15acbe208d32b5401f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1726cb83292b27615935ee3c4b0b9a55.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2ddd0e44dbb57c1d995323af6c0e8f90.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  MultimodalStudio A Heterogeneous Sensor Dataset and Framework for   Neural Rendering across Multiple Imaging Modalities
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d14a0d7f6781e1438a3a78de39f33397.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  AvatarArtist Open-Domain 4D Avatarization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
