<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  PartRM Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3ba9c1323f6e44da60a300846902f26f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="PartRM-Modeling-Part-Level-Dynamics-with-Large-Cross-State-Reconstruction-Model"><a href="#PartRM-Modeling-Part-Level-Dynamics-with-Large-Cross-State-Reconstruction-Model" class="headerlink" title="PartRM: Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model"></a>PartRM: Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model</h2><p><strong>Authors:Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, Hao Zhao</strong></p>
<p>As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the modelâ€™s understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research. </p>
<blockquote>
<p>éšç€ä¸–ç•Œæ¨¡å‹ä»å½“å‰è§‚æµ‹å’Œè¡ŒåŠ¨ä¸­é¢„æµ‹æœªæ¥çŠ¶æ€çš„å…´è¶£å¢é•¿ï¼Œå¯¹éƒ¨åˆ†çº§åˆ«çš„åŠ¨æ€è¿›è¡Œå‡†ç¡®å»ºæ¨¡å¯¹äºå„ç§åº”ç”¨è€Œè¨€å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚Puppet-Masterï¼‰ä¾èµ–äºå¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¾®è°ƒï¼Œä½†ç”±äºäºŒç»´è§†é¢‘è¡¨ç¤ºçš„å±€é™æ€§å’Œç¼“æ…¢çš„å¤„ç†æ—¶é—´ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ä½¿ç”¨ä¸­æ˜¯ä¸å®é™…çš„ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PartRMï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å››ç»´é‡å»ºæ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä»é™æ€å¯¹è±¡çš„å¤šè§†è§’å›¾åƒå¯¹å¤–è§‚ã€å‡ ä½•å’Œéƒ¨åˆ†çº§åˆ«çš„è¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚PartRMå»ºç«‹åœ¨å¤§å‹ä¸‰ç»´é«˜æ–¯é‡å»ºæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å…¶å¯¹é™æ€å¯¹è±¡çš„å¤–è§‚å’Œå‡ ä½•çš„å¹¿æ³›çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³å››ç»´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PartDrag-4Dæ•°æ®é›†ï¼Œæä¾›äº†è¶…è¿‡2ä¸‡ä¸ªçŠ¶æ€çš„å±€éƒ¨è¿åŠ¨çš„å¤šè§†è§’è§‚å¯Ÿã€‚æˆ‘ä»¬é€šè¿‡å¤šå°ºåº¦é˜»åŠ›åµŒå…¥æ¨¡å—å¢å¼ºæ¨¡å‹å¯¹äº¤äº’æ¡ä»¶çš„äº†è§£ï¼Œè¯¥æ¨¡å—å¯ä»¥åœ¨ä¸åŒç²’åº¦ä¸Šæ•è·åŠ¨æ€ã€‚ä¸ºäº†é˜²æ­¢å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Œæˆ‘ä»¬å®æ–½äº†åˆ†é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æŒ‰é¡ºåºä¸“æ³¨äºè¿åŠ¨å’Œå­¦ä¹ çš„å¤–è§‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartRMåœ¨éƒ¨åˆ†çº§åˆ«çš„è¿åŠ¨å­¦ä¹ ä¸­å»ºç«‹äº†æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œå¹¶å¯åº”ç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ–¹ä¾¿æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19913v1">PDF</a> Accepted to CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://partrm.c7w.tech/">https://partrm.c7w.tech/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPartRMçš„4Dé‡å»ºæ¡†æ¶ï¼Œç”¨äºä»å¤šè§†è§’å›¾åƒæ¨¡æ‹Ÿé™æ€ç‰©ä½“çš„éƒ¨åˆ†çº§åˆ«åŠ¨æ€ã€‚PartRMåŸºäºå¤§å‹3Dé«˜æ–¯é‡å»ºæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥PartDrag-4Dæ•°æ®é›†å’Œå¤šå°ºåº¦æ‹–æ‹½åµŒå…¥æ¨¡å—ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºå’Œäº¤äº’æ¡ä»¶ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å®ç°ç²¾ç»†çš„è¿åŠ¨å­¦ä¹ å’Œæ“æ§ä»»åŠ¡åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartRMæ¡†æ¶æ˜¯é¦–ä¸ªèƒ½åŒæ—¶å»ºæ¨¡é™æ€ç‰©ä½“çš„å¤–è§‚ã€å‡ ä½•å’Œéƒ¨åˆ†çº§åˆ«è¿åŠ¨çš„4Dé‡å»ºæ¡†æ¶ã€‚</li>
<li>PartRMåˆ©ç”¨å¤§å‹3Dé«˜æ–¯é‡å»ºæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œä¸ºé™æ€ç‰©ä½“çš„å»ºæ¨¡æä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚</li>
<li>å¼•å…¥PartDrag-4Dæ•°æ®é›†ï¼Œè§£å†³äº†4Dæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæä¾›äº†å¤šè§†è§’çš„éƒ¨åˆ†çº§åˆ«åŠ¨æ€è§‚å¯Ÿã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦æ‹–æ‹½åµŒå…¥æ¨¡å—ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹äº¤äº’æ¡ä»¶çš„ç†è§£ã€‚</li>
<li>PartRMé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé˜²æ­¢åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Œä¾æ¬¡ä¸“æ³¨äºè¿åŠ¨å­¦ä¹ å’Œå¤–è§‚å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPartRMåœ¨éƒ¨åˆ†çº§åˆ«è¿åŠ¨å­¦ä¹ æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶èƒ½åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9706c6510ed019d9cad45c87e2574685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1509a3a728a2593bd03227c6f13fb6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a63c4a8c14d965019fcee0f13d83257.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8d1cdab8d95e01d117e10b4737dc855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2681fd3f36a17d890f2af1e70f8a1dd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32665bbd78e4d445e82050bf460cbc3f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AvatarArtist-Open-Domain-4D-Avatarization"><a href="#AvatarArtist-Open-Domain-4D-Avatarization" class="headerlink" title="AvatarArtist: Open-Domain 4D Avatarization"></a>AvatarArtist: Open-Domain 4D Avatarization</h2><p><strong>Authors:Hongyu Liu, Xuan Wang, Ziyu Wan, Yue Ma, Jingye Chen, Yanbo Fan, Yujun Shen, Yibing Song, Qifeng Chen</strong></p>
<p>This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies.. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹å…³æ³¨å¼€æ”¾åŸŸ4Dè™šæ‹ŸåŒ–èº«åˆ›å»ºï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒä¸­åˆ›å»º4Dè™šæ‹ŸåŒ–èº«ã€‚æˆ‘ä»¬é€‰æ‹©å‚æ•°åŒ–ä¸‰å¹³é¢ä½œä¸ºä¸­é—´4Dè¡¨ç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ç§å®ç”¨çš„è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼ç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„è®¾è®¡çµæ„Ÿæ¥è‡ªäºä¸€é¡¹è§‚å¯Ÿï¼Œå³4DGANåœ¨æ— éœ€ç›‘ç£çš„æƒ…å†µä¸‹æ“…é•¿äºå¤„ç†å›¾åƒå’Œä¸‰å¹³é¢ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œä½†åœ¨å¤„ç†å¤šæ ·æ•°æ®åˆ†å¸ƒæ—¶é€šå¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸€ä¸ªç¨³å¥çš„2Dæ‰©æ•£å…ˆéªŒçŸ¥è¯†ä½œä¸ºè§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸä¹‹é—´è½¬ç§»å…¶ä¸“ä¸šçŸ¥è¯†ã€‚è¿™äº›ä¸“å®¶ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—èƒ½å¤Ÿæ„å»ºå¤šåŸŸå›¾åƒ-ä¸‰å¹³é¢æ•°æ®é›†ï¼Œè¿™æ¨åŠ¨äº†é€šç”¨4Dè™šæ‹ŸåŒ–èº«åˆ›å»ºå™¨çš„å‘å±•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹AvatarArtistèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„4Dè™šæ‹ŸåŒ–èº«ï¼Œå¯¹å„ç§æºå›¾åƒåŸŸå…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å…¬å¼€ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19906v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¼€æ”¾åŸŸ4Dä¸ªæ€§åŒ–è§’è‰²åˆ›å»ºï¼Œæ—¨åœ¨ä»ä»»æ„é£æ ¼çš„è‚–åƒå›¾åƒä¸­åˆ›å»º4Dä¸ªæ€§åŒ–è§’è‰²ã€‚ç ”ç©¶é‡‡ç”¨å‚æ•°åŒ–triplanesä½œä¸ºä¸­é—´4Dè¡¨ç¤ºå½¢å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å®é™…è®­ç»ƒèŒƒå¼ã€‚è®¾è®¡æ€è·¯æºäºè§‚å¯Ÿåˆ°4D GANsåœ¨æ¡¥æ¥å›¾åƒå’Œtriplanesæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¤„ç†å¤šæ ·æ•°æ®åˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚2Dæ‰©æ•£å…ˆéªŒçŸ¥è¯†çš„å‡ºç°ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸè½¬ç§»çŸ¥è¯†ã€‚è¿™äº›ä¸“å®¶ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—æ„å»ºå¤šåŸŸå›¾åƒ-triplaneæ•°æ®é›†æˆä¸ºå¯èƒ½ï¼Œè¿›è€Œæ¨åŠ¨äº†é€šç”¨4Dä¸ªæ€§åŒ–è§’è‰²åˆ›å»ºå™¨çš„å‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒAvatarArtistæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€é«˜é²æ£’æ€§çš„4Dä¸ªæ€§åŒ–è§’è‰²ï¼Œé€‚åº”å„ç§æºå›¾åƒåŸŸã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€æä¾›ï¼Œä»¥æ¨åŠ¨æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨å¼€æ”¾åŸŸ4Dä¸ªæ€§åŒ–è§’è‰²åˆ›å»ºï¼Œæ—¨åœ¨ä»è‚–åƒå›¾åƒåˆ›å»º4Dä¸ªæ€§åŒ–è§’è‰²ã€‚</li>
<li>é‡‡ç”¨å‚æ•°åŒ–triplanesä½œä¸ºä¸­é—´4Dè¡¨ç¤ºå½¢å¼ã€‚</li>
<li>ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå®é™…è®­ç»ƒã€‚</li>
<li>è§‚å¯Ÿåˆ°4D GANsåœ¨æ¡¥æ¥å›¾åƒå’Œtriplanesæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¤„ç†å¤šæ ·æ•°æ®åˆ†å¸ƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥2Dæ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œå¸®åŠ©GANåœ¨ä¸åŒé¢†åŸŸè½¬ç§»çŸ¥è¯†ã€‚</li>
<li>å¤šåŸŸå›¾åƒ-triplaneæ•°æ®é›†çš„æ„å»ºæ¨åŠ¨äº†é€šç”¨4Dä¸ªæ€§åŒ–è§’è‰²åˆ›å»ºå™¨çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c44b9fac00940094057255e461b9d913.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea16d6a700d3d7ab1b01cea39a78f1f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f846e42e4d2905e2771588ee66b27a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e29711d534a981fd7ac40ac11f5578c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3014ffdb5e751ad6bd3b845ec67cf6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scaling-Down-Text-Encoders-of-Text-to-Image-Diffusion-Models"><a href="#Scaling-Down-Text-Encoders-of-Text-to-Image-Diffusion-Models" class="headerlink" title="Scaling Down Text Encoders of Text-to-Image Diffusion Models"></a>Scaling Down Text Encoders of Text-to-Image Diffusion Models</h2><p><strong>Authors:Lifu Wang, Daqing Liu, Xinchen Liu, Xiaodong He</strong></p>
<p>Text encoders in diffusion models have rapidly evolved, transitioning from CLIP to T5-XXL. Although this evolution has significantly enhanced the modelsâ€™ ability to understand complex prompts and generate text, it also leads to a substantial increase in the number of parameters. Despite T5 series encoders being trained on the C4 natural language corpus, which includes a significant amount of non-visual data, diffusion models with T5 encoder do not respond to those non-visual prompts, indicating redundancy in representational power. Therefore, it raises an important question: â€œDo we really need such a large text encoder?â€ In pursuit of an answer, we employ vision-based knowledge distillation to train a series of T5 encoder models. To fully inherit its capabilities, we constructed our dataset based on three criteria: image quality, semantic understanding, and text-rendering. Our results demonstrate the scaling down pattern that the distilled T5-base model can generate images of comparable quality to those produced by T5-XXL, while being 50 times smaller in size. This reduction in model size significantly lowers the GPU requirements for running state-of-the-art models such as FLUX and SD3, making high-quality text-to-image generation more accessible. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨å·²ç»è¿…é€Ÿè¿›åŒ–ï¼Œä»CLIPè¿‡æ¸¡åˆ°T5-XXLã€‚è™½ç„¶è¿™ç§è¿›åŒ–æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹ç†è§£å¤æ‚æç¤ºå’Œç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¯¼è‡´äº†å‚æ•°æ•°é‡çš„æ˜¾è‘—å¢åŠ ã€‚å°½ç®¡T5ç³»åˆ—ç¼–ç å™¨æ˜¯åœ¨åŒ…å«å¤§é‡éè§†è§‰æ•°æ®çš„C4è‡ªç„¶è¯­è¨€è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†ä½¿ç”¨T5ç¼–ç å™¨çš„æ‰©æ•£æ¨¡å‹å¹¶ä¸å“åº”è¿™äº›éè§†è§‰æç¤ºï¼Œè¿™è¡¨æ˜å­˜åœ¨è¡¨ç¤ºèƒ½åŠ›çš„å†—ä½™ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šâ€œæˆ‘ä»¬çœŸçš„éœ€è¦è¿™ä¹ˆå¤§çš„æ–‡æœ¬ç¼–ç å™¨å—ï¼Ÿâ€ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè§†è§‰çš„çŸ¥è¯†è’¸é¦æ¥è®­ç»ƒä¸€ç³»åˆ—T5ç¼–ç å™¨æ¨¡å‹ã€‚ä¸ºäº†å®Œå…¨ç»§æ‰¿å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬åŸºäºå›¾åƒè´¨é‡ã€è¯­ä¹‰ç†è§£å’Œæ–‡æœ¬æ¸²æŸ“ä¸‰ä¸ªæ ‡å‡†æ„å»ºäº†æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»è¿‡è’¸é¦çš„T5åŸºç¡€æ¨¡å‹å‘ˆç°å‡ºç¼©å°æ¨¡å¼ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸T5-XXLç›¸å½“è´¨é‡çš„å›¾åƒï¼Œè€Œå¤§å°åªæœ‰å…¶1&#x2F;50ã€‚æ¨¡å‹å°ºå¯¸çš„ç¼©å°æ˜¾è‘—é™ä½äº†è¿è¡Œæœ€æ–°æ¨¡å‹ï¼ˆå¦‚FLUXå’ŒSD3ï¼‰æ‰€éœ€çš„GPUè¦æ±‚ï¼Œä½¿é«˜è´¨é‡çš„æ–‡å­—è½¬å›¾åƒç”Ÿæˆæ›´åŠ æ˜“äºå®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19897v1">PDF</a> accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬ç¼–ç å™¨åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç»å†äº†ä»CLIPåˆ°T5-XXLçš„è¿…é€Ÿæ¼”å˜ã€‚è™½ç„¶è¿™æé«˜äº†æ¨¡å‹ç†è§£å¤æ‚æç¤ºå’Œç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¯¼è‡´äº†å‚æ•°æ•°é‡çš„å¤§å¹…å¢åŠ ã€‚æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨åŸºäºè§†è§‰çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯è®­ç»ƒä¸€ç³»åˆ—T5ç¼–ç å™¨æ¨¡å‹ï¼Œå®ç°äº†åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ç¼©å°æ¨¡å‹è§„æ¨¡çš„ç›®æ ‡ã€‚æ‰€æ„å»ºçš„åŸºäºå›¾åƒè´¨é‡ã€è¯­ä¹‰ç†è§£å’Œæ–‡æœ¬æ¸²æŸ“ä¸‰ä¸ªæ ‡å‡†çš„æ•°æ®é›†ï¼Œè®©è’¸é¦åçš„T5åŸºç¡€æ¨¡å‹èƒ½ç”Ÿæˆä¸T5-XXLç›¸å½“è´¨é‡çš„å›¾åƒï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°ç¼©å°äº†50å€ã€‚è¿™æ˜¾è‘—é™ä½äº†è¿è¡Œå…ˆè¿›æ¨¡å‹å¦‚FLUXå’ŒSD3æ‰€éœ€çš„GPUè¦æ±‚ï¼Œä½¿é«˜è´¨é‡çš„æ–‡å­—åˆ°å›¾åƒç”Ÿæˆæ›´åŠ è§¦æ‰‹å¯åŠã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ç¼–ç å™¨åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç»å†äº†ä»CLIPåˆ°T5-XXLçš„æ¼”å˜ï¼Œå¢å¼ºäº†ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†å‚æ•°å¤§å¹…å¢åŠ ã€‚</li>
<li>éè§†è§‰æ•°æ®åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„å¤„ç†å­˜åœ¨å†—ä½™ï¼Œæš—ç¤ºå¤§å‹æ–‡æœ¬ç¼–ç å™¨çš„å¿…è¦æ€§æœ‰å¾…å•†æ¦·ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨åŸºäºè§†è§‰çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯æ¥è®­ç»ƒT5ç¼–ç å™¨æ¨¡å‹ï¼Œä»¥ç¼©å°æ¨¡å‹è§„æ¨¡å¹¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>æ„å»ºçš„æ•°æ®é›†åŸºäºå›¾åƒè´¨é‡ã€è¯­ä¹‰ç†è§£å’Œæ–‡æœ¬æ¸²æŸ“ä¸‰ä¸ªæ ‡å‡†ã€‚</li>
<li>è’¸é¦åçš„T5åŸºç¡€æ¨¡å‹èƒ½ç”Ÿæˆä¸T5-XXLç›¸å½“è´¨é‡çš„å›¾åƒï¼Œä½†æ¨¡å‹å¤§å°ç¼©å°äº†50å€ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡çš„ç¼©å°æ˜¾è‘—é™ä½äº†è¿è¡Œå…ˆè¿›æ¨¡å‹æ‰€éœ€çš„GPUè¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e974da7102fa9c463060ae05dfbb4e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-524b33ce66b470cda4df3567bdd3bdc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e390a3fbe6230db2cf468a9948abfa74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3d65e4752cd5299505dadb83475cde8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad41a22c85438f88054ad15033f5b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb6e11d4736d6d7c7c57b1f4c5d68c36.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FireEdit-Fine-grained-Instruction-based-Image-Editing-via-Region-aware-Vision-Language-Model"><a href="#FireEdit-Fine-grained-Instruction-based-Image-Editing-via-Region-aware-Vision-Language-Model" class="headerlink" title="FireEdit: Fine-grained Instruction-based Image Editing via Region-aware   Vision Language Model"></a>FireEdit: Fine-grained Instruction-based Image Editing via Region-aware   Vision Language Model</h2><p><strong>Authors:Jun Zhou, Jiahao Li, Zunnan Xu, Hanhui Li, Yiji Cheng, Fa-Ting Hong, Qin Lin, Qinglin Lu, Xiaodan Liang</strong></p>
<p>Currently, instruction-based image editing methods have made significant progress by leveraging the powerful cross-modal understanding capabilities of vision language models (VLMs). However, they still face challenges in three key areas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained editing. To address these issues, we propose FireEdit, an innovative Fine-grained Instruction-based image editing framework that exploits a REgion-aware VLM. FireEdit is designed to accurately comprehend user instructions and ensure effective control over the editing process. Specifically, we enhance the fine-grained visual perception capabilities of the VLM by introducing additional region tokens. Relying solely on the output of the LLM to guide the diffusion model may lead to suboptimal editing results. Therefore, we propose a Time-Aware Target Injection module and a Hybrid Visual Cross Attention module. The former dynamically adjusts the guidance strength at various denoising stages by integrating timestep embeddings with the text embeddings. The latter enhances visual details for image editing, thereby preserving semantic consistency between the edited result and the source image. By combining the VLM enhanced with fine-grained region tokens and the time-dependent diffusion model, FireEdit demonstrates significant advantages in comprehending editing instructions and maintaining high semantic consistency. Extensive experiments indicate that our approach surpasses the state-of-the-art instruction-based image editing methods. Our project is available at <a target="_blank" rel="noopener" href="https://zjgans.github.io/fireedit.github.io">https://zjgans.github.io/fireedit.github.io</a>. </p>
<blockquote>
<p>ç›®å‰ï¼ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ–¹æ³•å·²ç»å€ŸåŠ©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¼ºå¤§è·¨æ¨¡æ€ç†è§£èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»é¢ä¸´ä¸‰ä¸ªå…³é”®é¢†åŸŸçš„æŒ‘æˆ˜ï¼š1ï¼‰å¤æ‚åœºæ™¯ï¼›2ï¼‰è¯­ä¹‰ä¸€è‡´æ€§ï¼›ä»¥åŠ3ï¼‰ç²¾ç»†ç²’åº¦ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FireEditï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„åŸºäºç²¾ç»†ç²’åº¦æŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åŒºåŸŸæ„ŸçŸ¥VLMã€‚FireEditæ—¨åœ¨å‡†ç¡®ç†è§£ç”¨æˆ·æŒ‡ä»¤ï¼Œå¹¶ç¡®ä¿å¯¹ç¼–è¾‘è¿‡ç¨‹çš„æœ‰æ•ˆæ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥é¢å¤–çš„åŒºåŸŸä»¤ç‰Œæ¥å¢å¼ºVLMçš„ç²¾ç»†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚ä»…ä¾é å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹å¯èƒ½ä¼šå¯¼è‡´æ¬¡ä¼˜çš„ç¼–è¾‘ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ„ŸçŸ¥ç›®æ ‡æ³¨å…¥æ¨¡å—å’Œæ··åˆè§†è§‰äº¤å‰æ³¨æ„æ¨¡å—ã€‚å‰è€…é€šè¿‡ç»“åˆæ—¶é—´æ­¥åµŒå…¥ä¸æ–‡æœ¬åµŒå…¥ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒå»å™ªé˜¶æ®µçš„æŒ‡å¯¼å¼ºåº¦ã€‚åè€…å¢å¼ºäº†å›¾åƒç¼–è¾‘çš„è§†è§‰ç»†èŠ‚ï¼Œä»è€Œä¿æŒäº†ç¼–è¾‘ç»“æœä¸æºå›¾åƒä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚é€šè¿‡ç»“åˆä½¿ç”¨å¢å¼ºæœ‰ç²¾ç»†ç²’åº¦åŒºåŸŸä»¤ç‰Œçš„VLMå’Œæ—¶é—´ä¾èµ–çš„æ‰©æ•£æ¨¡å‹ï¼ŒFireEditåœ¨ç†è§£ç¼–è¾‘æŒ‡ä»¤å’Œä¿æŒé«˜è¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€æ–°çš„åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://zjgans.github.io/fireedit.github.io">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19839v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤æ‚åœºæ™¯ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç²¾ç»†ç¼–è¾‘ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FireEditæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åœ°åŒºæ„ŸçŸ¥çš„VLMå’Œé™„åŠ åŒºåŸŸä»¤ç‰Œæ¥å¢å¼ºç²¾ç»†ç²’åº¦çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ—¶é—´æ„ŸçŸ¥ç›®æ ‡æ³¨å…¥æ¨¡å—å’Œæ··åˆè§†è§‰äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥ç¡®ä¿åœ¨å„ç§å»å™ªé˜¶æ®µåŠ¨æ€è°ƒæ•´æŒ‡å¯¼å¼ºåº¦å¹¶å¢å¼ºè§†è§‰ç»†èŠ‚ã€‚å®éªŒè¯æ˜ï¼ŒFireEditåœ¨ç†è§£ç¼–è¾‘æŒ‡ä»¤å’Œä¿æŒè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è·¨æ¨¡æ€ç†è§£åŠ›å–å¾—äº†é‡è¦è¿›å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•ä»é¢ä¸´å¤æ‚åœºæ™¯ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œç²¾ç»†ç¼–è¾‘çš„æŒ‘æˆ˜ã€‚</li>
<li>FireEditæ¡†æ¶é€šè¿‡å¼•å…¥åœ°åŒºæ„ŸçŸ¥çš„VLMå’Œé™„åŠ åŒºåŸŸä»¤ç‰Œï¼Œå¢å¼ºäº†ç²¾ç»†ç²’åº¦çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>Time-Aware Target Injectionæ¨¡å—å’ŒHybrid Visual Cross Attentionæ¨¡å—ç¡®ä¿åŠ¨æ€è°ƒæ•´æŒ‡å¯¼å¼ºåº¦å¹¶å¢å¼ºè§†è§‰ç»†èŠ‚ã€‚</li>
<li>FireEditåœ¨ç†è§£ç¼–è¾‘æŒ‡ä»¤å’Œä¿æŒè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒFireEditåœ¨æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3200885f5375f9aa51f24f2feedce896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba9c1323f6e44da60a300846902f26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd5c0b50086a93dd1efd44b531dc531.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e376f85e3a0cfa9f0cf6c1048b970bf6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models"><a href="#Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models" class="headerlink" title="Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models"></a>Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models</h2><p><strong>Authors:Ruixi You, Hecheng Jia, Feng Xu</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and high-resolution imaging capabilities but its unique imaging mechanism makes interpretation heavily reliant on expert knowledge, limiting interpretability, especially in complex target tasks. Translating SAR images into optical images is a promising solution to enhance interpretation and support downstream tasks. Most existing research focuses on scene-level translation, with limited work on object-level translation due to the scarcity of paired data and the challenge of accurately preserving contour and texture details. To address these issues, this study proposes a keypoint-guided diffusion model (KeypointDiff) for SAR-to-optical image translation of unpaired aircraft targets. This framework introduces supervision on target class and azimuth angle via keypoints, along with a training strategy for unpaired data. Based on the classifier-free guidance diffusion architecture, a class-angle guidance module (CAGM) is designed to integrate class and angle information into the diffusion generation process. Furthermore, adversarial loss and consistency loss are employed to improve image fidelity and detail quality, tailored for aircraft targets. During sampling, aided by a pre-trained keypoint detector, the model eliminates the requirement for manually labeled class and azimuth information, enabling automated SAR-to-optical translation. Experimental results demonstrate that the proposed method outperforms existing approaches across multiple metrics, providing an efficient and effective solution for object-level SAR-to-optical translation and downstream tasks. Moreover, the method exhibits strong zero-shot generalization to untrained aircraft types with the assistance of the keypoint detector. </p>
<blockquote>
<p>åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å½±åƒæä¾›äº†å…¨å¤©å€™ã€å…¨å¤©æ—¶å’Œé«˜åˆ†è¾¨ç‡çš„æˆåƒèƒ½åŠ›ï¼Œä½†å…¶ç‹¬ç‰¹çš„æˆåƒæœºåˆ¶ä½¿å¾—è§£è¯»ä¸¥é‡ä¾èµ–äºä¸“ä¸šçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç›®æ ‡ä»»åŠ¡ä¸­ã€‚å°†SARå›¾åƒè½¬æ¢ä¸ºå…‰å­¦å›¾åƒæ˜¯ä¸€ç§æé«˜è§£è¯»èƒ½åŠ›å¹¶æ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶é›†ä¸­åœ¨åœºæ™¯çº§åˆ«çš„ç¿»è¯‘ä¸Šï¼Œç”±äºé…å¯¹æ•°æ®çš„ç¨€ç¼ºå’Œå‡†ç¡®ä¿ç•™è½®å»“å’Œçº¹ç†ç»†èŠ‚çš„æŒ‘æˆ˜ï¼Œå¯¹è±¡çº§åˆ«çš„ç¿»è¯‘å·¥ä½œæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºSAR-å…‰å­¦å›¾åƒç¿»è¯‘çš„æœªé…å¯¹é£æœºç›®æ ‡çš„å…³é”®ç‚¹å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å…³é”®ç‚¹å¼•å…¥äº†å¯¹ç›®æ ‡ç±»åˆ«å’Œæ–¹ä½è§’çš„ç›‘ç£ï¼Œä»¥åŠä¸€ç§é’ˆå¯¹æœªé…å¯¹æ•°æ®çš„è®­ç»ƒç­–ç•¥ã€‚åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£æ¶æ„ï¼Œè®¾è®¡äº†ä¸€ä¸ªç±»è§’å¼•å¯¼æ¨¡å—ï¼ˆCAGMï¼‰ï¼Œå°†ç±»åˆ«å’Œè§’åº¦ä¿¡æ¯é›†æˆåˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†å¯¹æŠ—æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±æ¥æé«˜å›¾åƒä¿çœŸåº¦å’Œç»†èŠ‚è´¨é‡ï¼Œé€‚ç”¨äºé£æœºç›®æ ‡ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œè¯¥æ¨¡å‹æ— éœ€æ‰‹åŠ¨æ ‡æ³¨ç±»åˆ«å’Œæ–¹ä½ä¿¡æ¯ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„SAR-å…‰å­¦è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¯¹è±¡çº§åˆ«çš„SAR-å…‰å­¦è½¬æ¢å’Œä¸‹æ¸¸ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è€Œä¸”ï¼Œåœ¨å…³é”®ç‚¹çš„æ£€æµ‹å™¨å¸®åŠ©ä¸‹ï¼Œè¯¥æ–¹æ³•å¯¹æœªè®­ç»ƒçš„é£æœºç±»å‹å…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SARå›¾åƒå…·å¤‡å…¨å¤©å€™ã€å…¨å¤©æ—¶å’Œé«˜åˆ†è¾¨ç‡çš„æˆåƒèƒ½åŠ›ï¼Œä½†å…¶ç‹¬ç‰¹çš„æˆåƒæœºåˆ¶å¯¼è‡´è§£è¯»ä¾èµ–ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§å…³é”®ç‚¹å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰ï¼Œç”¨äºæ— é…å¯¹é£æœºç›®æ ‡çš„SAR-å…‰å­¦å›¾åƒè½¬æ¢ã€‚è¯¥ç ”ç©¶é€šè¿‡å…³é”®ç‚¹å¼•å…¥ç›®æ ‡å’Œæ–¹ä½è§’çš„ç›‘ç£ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ— é…å¯¹æ•°æ®çš„è®­ç»ƒç­–ç•¥ã€‚åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£æ¶æ„ï¼Œè®¾è®¡äº†ä¸€ä¸ªç±»è§’å¼•å¯¼æ¨¡å—ï¼ˆCAGMï¼‰ä»¥å°†ç±»åˆ«å’Œè§’åº¦ä¿¡æ¯èå…¥æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¯å®è¯¥æ–¹æ³•åœ¨å¤šæŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œä¸ºSAR-å…‰å­¦å›¾åƒè½¬æ¢å’Œä¸‹æ¸¸ä»»åŠ¡æä¾›é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå›¾åƒå…·æœ‰å…¨å¤©å€™ã€å…¨å¤©æ—¶å’Œé«˜åˆ†è¾¨ç‡çš„æˆåƒä¼˜åŠ¿ï¼Œä½†è§£è¯»éœ€ä¾èµ–ä¸“å®¶çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šèšç„¦äºåœºæ™¯çº§ç¿»è¯‘ï¼Œé’ˆå¯¹é£æœºç›®æ ‡çš„å¯¹è±¡çº§ç¿»è¯‘ç ”ç©¶æœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å…³é”®ç‚¹å¼•å…¥ç›®æ ‡å’Œæ–¹ä½è§’çš„ç›‘ç£ä¿¡æ¯ï¼Œè§£å†³æ•°æ®é…å¯¹ç¨€å°‘å’Œè½®å»“çº¹ç†ç»†èŠ‚ä¿ç•™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†å…³é”®ç‚¹å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰ï¼Œé€‚ç”¨äºæ— é…å¯¹é£æœºç›®æ ‡çš„SAR-å…‰å­¦å›¾åƒè½¬æ¢ã€‚</li>
<li>è®¾è®¡äº†ç±»è§’å¼•å¯¼æ¨¡å—ï¼ˆCAGMï¼‰ä»¥é›†æˆç±»åˆ«å’Œè§’åº¦ä¿¡æ¯åˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚</li>
<li>é‡‡ç”¨å¯¹æŠ—æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±æé«˜å›¾åƒä¿çœŸåº¦å’Œç»†èŠ‚è´¨é‡ï¼Œç‰¹åˆ«é’ˆå¯¹é£æœºç›®æ ‡è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ad34ff18565dbad742fc0c13da1bd9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca778238e964609473c2b1e38f00e6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9bc955b0864fb2ea0800c9f0986e80c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d8567d568dec08834d5dcfcb89b40fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7776725ee57eaf5ae1fd8a22f4cab904.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de9e32d70831f58caa3361855117ea1e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush"><a href="#In-the-Blink-of-an-Eye-Instant-Game-Map-Editing-using-a-Generative-AI-Smart-Brush" class="headerlink" title="In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush"></a>In the Blink of an Eye: Instant Game Map Editing using a Generative-AI   Smart Brush</h2><p><strong>Authors:Vitaly Gnatyuk, Valeriia Koriukina Ilya Levoshevich, Pavel Nurminskiy, Guenter Wallner</strong></p>
<p>With video games steadily increasing in complexity, automated generation of game content has found widespread interest. However, the task of 3D gaming map art creation remains underexplored to date due to its unique complexity and domain-specific challenges. While recent works have addressed related topics such as retro-style level generation and procedural terrain creation, these works primarily focus on simpler data distributions. To the best of our knowledge, we are the first to demonstrate the application of modern AI techniques for high-resolution texture manipulation in complex, highly detailed AAA 3D game environments. We introduce a novel Smart Brush for map editing, designed to assist artists in seamlessly modifying selected areas of a game map with minimal effort. By leveraging generative adversarial networks and diffusion models we propose two variants of the brush that enable efficient and context-aware generation. Our hybrid workflow aims to enhance both artistic flexibility and production efficiency, enabling the refinement of environments without manually reworking every detail, thus helping to bridge the gap between automation and creative control in game development. A comparative evaluation of our two methods with adapted versions of several state-of-the art models shows that our GAN-based brush produces the sharpest and most detailed outputs while preserving image context while the evaluated state-of-the-art models tend towards blurrier results and exhibit difficulties in maintaining contextual consistency. </p>
<blockquote>
<p>éšç€è§†é¢‘æ¸¸æˆçš„å¤æ‚æ€§ä¸æ–­æé«˜ï¼Œæ¸¸æˆå†…å®¹çš„è‡ªåŠ¨ç”Ÿæˆå·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äº3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œçš„ç‹¬ç‰¹å¤æ‚æ€§å’Œç‰¹å®šé¢†åŸŸæŒ‘æˆ˜ï¼Œè‡³ä»Šè¯¥ä»»åŠ¡ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è™½ç„¶è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¶‰åŠäº†ç›¸å…³ä¸»é¢˜ï¼Œå¦‚å¤å¤é£æ ¼çº§åˆ«ç”Ÿæˆå’Œç¨‹åºåŒ–åœ°å½¢ç”Ÿæˆï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ›´ç®€å•çš„æ•°æ®åˆ†å¸ƒä¸Šã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å±•ç¤ºç°ä»£äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å¤æ‚ã€é«˜åº¦è¯¦ç»†çš„AAA 3Dæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œé«˜åˆ†è¾¨ç‡çº¹ç†æ“ä½œçš„åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”¨äºåœ°å›¾ç¼–è¾‘çš„æ–°å‹æ™ºèƒ½ç”»ç¬”ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹æ¸¸æˆåœ°å›¾çš„é€‰å®šåŒºåŸŸã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”»ç¬”å˜ä½“ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ··åˆå·¥ä½œæµç¨‹æ—¨åœ¨æé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨ä¸æ‰‹åŠ¨é‡æ–°å¤„ç†æ¯ä¸ªç»†èŠ‚çš„æƒ…å†µä¸‹ä¼˜åŒ–ç¯å¢ƒï¼Œä»è€Œæœ‰åŠ©äºå¼¥åˆæ¸¸æˆå¼€å‘ä¸­è‡ªåŠ¨åŒ–å’Œåˆ›æ„æ§åˆ¶ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¯¹ä¸¤ç§æ–¹æ³•ä¸å‡ ç§æœ€æ–°æ¨¡å‹çš„æ”¹ç¼–ç‰ˆæœ¬è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºGANçš„ç”»ç¬”äº§ç”Ÿçš„è¾“å‡ºæœ€æ¸…æ™°ã€æœ€è¯¦ç»†ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒä¸Šä¸‹æ–‡ï¼Œè€Œè¢«è¯„ä¼°çš„å…ˆè¿›æ¨¡å‹å¾€å¾€ç»“æœæ›´æ¨¡ç³Šï¼Œå¹¶ä¸”åœ¨ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19793v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€è§†é¢‘æ¸¸æˆçš„å¤æ‚åº¦ä¸æ–­æå‡ï¼Œæ¸¸æˆå†…å®¹è‡ªåŠ¨ç”ŸæˆæŠ€æœ¯å¤‡å—å…³æ³¨ã€‚ä½†3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œä»»åŠ¡å› ç‹¬ç‰¹å¤æ‚æ€§å’Œé¢†åŸŸç‰¹å®šæŒ‘æˆ˜è€Œä»è¢«å¿½è§†ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ›´ç®€å•çš„æ•°æ®åˆ†å¸ƒï¼Œå¦‚å¤å¤é£æ ¼å…³å¡ç”Ÿæˆå’Œç¨‹åºåŒ–åœ°å½¢åˆ›å»ºã€‚æœ¬æ–‡é¦–æ¬¡å±•ç¤ºç°ä»£AIæŠ€æœ¯åœ¨å¤æ‚ã€é«˜åº¦è¯¦ç»†çš„AAAçº§3Dæ¸¸æˆç¯å¢ƒä¸­çš„é«˜åˆ†è¾¨ç‡çº¹ç†æ“æ§åº”ç”¨ã€‚å¼•å…¥äº†ä¸€ç§æ–°å‹æ™ºèƒ½åœ°å›¾ç¼–è¾‘ç¬”åˆ·ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹æ¸¸æˆåœ°å›¾çš„é€‰å®šåŒºåŸŸã€‚åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§ç¬”åˆ·å˜ä½“ï¼Œå¯å®ç°é«˜æ•ˆä¸”è¯­å¢ƒæ„ŸçŸ¥çš„ç”Ÿæˆã€‚è¯¥æ··åˆå·¥ä½œæµç¨‹æ—¨åœ¨æé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ï¼Œæ— éœ€æ‰‹åŠ¨ä¿®æ”¹æ¯ä¸ªç»†èŠ‚å³å¯ä¼˜åŒ–ç¯å¢ƒï¼Œä»è€Œç¼©å°è‡ªåŠ¨åŒ–å’Œåˆ›æ„æ§åˆ¶åœ¨æ¸¸æˆå¼€å‘ä¹‹é—´çš„å·®è·ã€‚å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ï¼Œè¯æ˜åŸºäºGANçš„ç¬”åˆ·èƒ½å¤Ÿäº§ç”Ÿæœ€æ¸…æ™°ã€æœ€è¯¦ç»†çš„è¾“å‡ºå¹¶ä¿ç•™å›¾åƒä¸Šä¸‹æ–‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç”Ÿæˆæ¸¸æˆå†…å®¹é€æ¸æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œä½†3Dæ¸¸æˆåœ°å›¾è‰ºæœ¯åˆ›ä½œä»ç„¶é¢ä¸´ç‹¬ç‰¹å¤æ‚æ€§å’Œç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç›¸å…³ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€å•æ•°æ®åˆ†å¸ƒä¸Šï¼Œä¾‹å¦‚å¤å¤é£æ ¼å…³å¡ç”Ÿæˆå’Œç¨‹åºåŒ–åœ°å½¢åˆ›å»ºã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å°†ç°ä»£AIæŠ€æœ¯åº”ç”¨äºAAAçº§3Dæ¸¸æˆç¯å¢ƒçš„é«˜åˆ†è¾¨ç‡çº¹ç†æ“æ§ï¼Œå±•ç°äº†å…¶æ½œåŠ›å’Œä»·å€¼ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æ™ºèƒ½åœ°å›¾ç¼–è¾‘ç¬”åˆ·ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶è½»æ¾ä¿®æ”¹æ¸¸æˆåœ°å›¾ï¼Œæé«˜è‰ºæœ¯çµæ´»æ€§å’Œç”Ÿäº§æ•ˆç‡ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹æå‡ºäº†ä¸¤ç§ç¬”åˆ·å˜ä½“ï¼Œå®ç°äº†é«˜æ•ˆä¸”è¯­å¢ƒæ„ŸçŸ¥çš„ç”Ÿæˆã€‚</li>
<li>å¯¹æ¯”è¯„ä¼°æ˜¾ç¤ºåŸºäºGANçš„ç¬”åˆ·èƒ½å¤Ÿäº§ç”Ÿæ¸…æ™°ã€è¯¦ç»†çš„è¾“å‡ºå¹¶ä¿ç•™å›¾åƒä¸Šä¸‹æ–‡ï¼Œä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbabe2f8a853b755eea495cc0dfd9e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e15d179d1580f2c0dd020d404c96858e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96909c2845a90845202c28a6c501a4e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23b3ef9229cf940119438fd7eeca86d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d8d75da2de1068e7f1c407f3e6ea88a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SITA-Structurally-Imperceptible-and-Transferable-Adversarial-Attacks-for-Stylized-Image-Generation"><a href="#SITA-Structurally-Imperceptible-and-Transferable-Adversarial-Attacks-for-Stylized-Image-Generation" class="headerlink" title="SITA: Structurally Imperceptible and Transferable Adversarial Attacks   for Stylized Image Generation"></a>SITA: Structurally Imperceptible and Transferable Adversarial Attacks   for Stylized Image Generation</h2><p><strong>Authors:Jingdan Kang, Haoxin Yang, Yan Cai, Huaidong Zhang, Xuemiao Xu, Yong Du, Shengfeng He</strong></p>
<p>Image generation technology has brought significant advancements across various fields but has also raised concerns about data misuse and potential rights infringements, particularly with respect to creating visual artworks. Current methods aimed at safeguarding artworks often employ adversarial attacks. However, these methods face challenges such as poor transferability, high computational costs, and the introduction of noticeable noise, which compromises the aesthetic quality of the original artwork. To address these limitations, we propose a Structurally Imperceptible and Transferable Adversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss, which decouples and disrupts the robust style representation of the image. This disruption hinders style extraction during stylized image generation, thereby impairing the overall stylization process. Importantly, SITA eliminates the need for a surrogate diffusion model, leading to significantly reduced computational overhead. The methodâ€™s robust style feature disruption ensures high transferability across diverse models. Moreover, SITA introduces perturbations by embedding noise within the imperceptible structural details of the image. This approach effectively protects against style extraction without compromising the visual quality of the artwork. Extensive experiments demonstrate that SITA offers superior protection for artworks against unauthorized use in stylized generation. It significantly outperforms existing methods in terms of transferability, computational efficiency, and noise imperceptibility. Code is available at <a target="_blank" rel="noopener" href="https://github.com/A-raniy-day/SITA">https://github.com/A-raniy-day/SITA</a>. </p>
<blockquote>
<p>å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºæ•°æ®æ»¥ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›é€ è§†è§‰è‰ºæœ¯ä½œå“æ–¹é¢ã€‚ç›®å‰æ—¨åœ¨ä¿æŠ¤è‰ºæœ¯ä½œå“çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å¯¹æŠ—æ€§æ”»å‡»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€å¯è½¬ç§»æ€§å·®ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠå¼•å…¥æ˜æ˜¾å™ªå£°ç­‰æŒ‘æˆ˜ï¼Œè¿™æŸå®³äº†åŸå§‹è‰ºæœ¯ä½œå“çš„å®¡ç¾è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„ä¸å¯å¯Ÿè§‰å’Œå¯è½¬ç§»çš„å¯¹æŠ—æ€§ï¼ˆSITAï¼‰æ”»å‡»ã€‚SITAåˆ©ç”¨åŸºäºCLIPçš„å»é£æ ¼åŒ–æŸå¤±ï¼Œè§£å¼€å¹¶ç ´åå›¾åƒçš„ç¨³å¥é£æ ¼è¡¨ç¤ºã€‚è¿™ç§ç ´åé˜»ç¢äº†é£æ ¼åŒ–å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„é£æ ¼æå–ï¼Œä»è€Œå½±å“äº†æ•´ä½“çš„é£æ ¼åŒ–è¿‡ç¨‹ã€‚é‡è¦çš„æ˜¯ï¼ŒSITAä¸éœ€è¦æ›¿ä»£æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•çš„ç¨³å¥é£æ ¼ç‰¹å¾ç ´åç¡®ä¿äº†åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´çš„è‰¯å¥½å¯è½¬ç§»æ€§ã€‚æ­¤å¤–ï¼ŒSITAé€šè¿‡åœ¨å›¾åƒçš„ä¸æ˜“å¯Ÿè§‰çš„ç»“æ„ç»†èŠ‚ä¸­åµŒå…¥å™ªå£°æ¥å¼•å…¥æ‰°åŠ¨ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é˜²æ­¢é£æ ¼æå–ï¼ŒåŒæ—¶ä¸æŸå®³è‰ºæœ¯ä½œå“çš„å¯è§†è´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSITAåœ¨é£æ ¼åŒ–ç”Ÿæˆä¸­å¯¹æŠ—æœªç»æˆæƒçš„ä½¿ç”¨æ–¹é¢æä¾›äº†å‡ºè‰²çš„ä¿æŠ¤ã€‚åœ¨å¯è½¬ç§»æ€§ã€è®¡ç®—æ•ˆç‡å’Œå™ªå£°ä¸å¯å¯Ÿè§‰æ€§æ–¹é¢ï¼Œå®ƒæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/A-raniy-day/SITA">https://github.com/A-raniy-day/SITA</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19791v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨å„é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•çš„åŒæ—¶ï¼Œä¹Ÿå¼•å‘äº†æ•°æ®æ»¥ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›ä½œè§†è§‰è‰ºæœ¯ä½œå“æ–¹é¢ã€‚ä¸ºä¿æŠ¤è‰ºæœ¯ä½œå“ï¼Œç°æœ‰æ–¹æ³•å¸¸é‡‡ç”¨å¯¹æŠ—æ€§æ”»å‡»ï¼Œä½†é¢ä¸´ä¼ è¾“æ€§å·®ã€è®¡ç®—æˆæœ¬é«˜å’Œå¼•å…¥æ˜æ˜¾å™ªå£°ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç»“æ„éšå½¢ä¸”å¯è½¬ç§»çš„å¯¹æŠ—æ€§ï¼ˆSITAï¼‰æ”»å‡»ã€‚SITAåˆ©ç”¨åŸºäºCLIPçš„å»é£æ ¼åŒ–æŸå¤±ï¼Œè§£è€¦å¹¶ç ´åå›¾åƒçš„ç¨³å¥é£æ ¼è¡¨ç¤ºï¼Œé˜»ç¢é£æ ¼æå–ï¼Œå½±å“æ•´ä½“é£æ ¼åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚SITAæ— éœ€æ›¿ä»£æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚å…¶ç¨³å¥çš„é£æ ¼ç‰¹å¾ç ´åç¡®ä¿äº†åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´çš„é«˜å¯è½¬ç§»æ€§ã€‚æ­¤å¤–ï¼ŒSITAé€šè¿‡åµŒå…¥å›¾åƒä¸å¯è§ç»“æ„ç»†èŠ‚ä¸­çš„å™ªå£°è¿›è¡Œæ‰°åŠ¨ï¼Œæœ‰æ•ˆé˜²æ­¢é£æ ¼æå–ï¼ŒåŒæ—¶ä¸æŸå®³è‰ºæœ¯ä½œå“çš„å¯è§†è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSITAåœ¨é£æ ¼åŒ–ç”Ÿæˆä¸­ä¿æŠ¤è‰ºæœ¯ä½œå“å…å—æœªç»æˆæƒçš„ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨å¯è½¬ç§»æ€§ã€è®¡ç®—æ•ˆç‡å’Œå™ªå£°éšè”½æ€§æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”ŸæˆæŠ€æœ¯å¼•å‘æ•°æ®æ»¥ç”¨å’Œæƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›ä½œè§†è§‰è‰ºæœ¯ä½œå“æ–¹é¢ã€‚</li>
<li>ç°æœ‰ä¿æŠ¤è‰ºæœ¯ä½œå“çš„æ–¹æ³•é‡‡ç”¨å¯¹æŠ—æ€§æ”»å‡»ï¼Œä½†é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¼ è¾“æ€§å·®ã€è®¡ç®—æˆæœ¬é«˜å’Œå¼•å…¥å™ªå£°ã€‚</li>
<li>SITAæ”»å‡»é€šè¿‡åˆ©ç”¨CLIPå»é£æ ¼åŒ–æŸå¤±æ¥ç ´åå›¾åƒçš„ç¨³å¥é£æ ¼è¡¨ç¤ºï¼Œå½±å“é£æ ¼åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>SITAä¸éœ€è¦æ›¿ä»£æ‰©æ•£æ¨¡å‹ï¼Œé™ä½è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ç¡®ä¿é«˜å¯è½¬ç§»æ€§ã€‚</li>
<li>SITAé€šè¿‡åµŒå…¥å›¾åƒä¸å¯è§ç»“æ„ç»†èŠ‚ä¸­çš„å™ªå£°è¿›è¡Œæ‰°åŠ¨ï¼Œé˜²æ­¢é£æ ¼æå–ï¼ŒåŒæ—¶ä¿æŒè‰ºæœ¯ä½œå“çš„å¯è§†è´¨é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜SITAåœ¨ä¿æŠ¤è‰ºæœ¯ä½œå“æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯è½¬ç§»æ€§ã€è®¡ç®—æ•ˆç‡å’Œå™ªå£°éšè”½æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-715eeb2a8b23878cf3e8e672f030b60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b5cbc9590ebe481363125979a0aeb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f628f53d60801d9a07ef95e6db60360.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Erasure-in-Text-to-Image-Diffusion-based-Foundation-Models"><a href="#Fine-Grained-Erasure-in-Text-to-Image-Diffusion-based-Foundation-Models" class="headerlink" title="Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models"></a>Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models</h2><p><strong>Authors:Kartik Thakral, Tamar Glaser, Tal Hassner, Mayank Vatsa, Richa Singh</strong></p>
<p>Existing unlearning algorithms in text-to-image generative models often fail to preserve the knowledge of semantically related concepts when removing specific target concepts: a challenge known as adjacency. To address this, we propose FADE (Fine grained Attenuation for Diffusion Erasure), introducing adjacency aware unlearning in diffusion models. FADE comprises two components: (1) the Concept Neighborhood, which identifies an adjacency set of related concepts, and (2) Mesh Modules, employing a structured combination of Expungement, Adjacency, and Guidance loss components. These enable precise erasure of target concepts while preserving fidelity across related and unrelated concepts. Evaluated on datasets like Stanford Dogs, Oxford Flowers, CUB, I2P, Imagenette, and ImageNet1k, FADE effectively removes target concepts with minimal impact on correlated concepts, achieving atleast a 12% improvement in retention performance over state-of-the-art methods. </p>
<blockquote>
<p>ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„é—å¿˜ç®—æ³•åœ¨ç§»é™¤ç‰¹å®šç›®æ ‡æ¦‚å¿µæ—¶ï¼Œå¾€å¾€æ— æ³•ä¿ç•™è¯­ä¹‰ç›¸å…³æ¦‚å¿µçš„çŸ¥è¯†ï¼Œè¿™è¢«ç§°ä¸ºé‚»è¿‘æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FADEï¼ˆæ‰©æ•£æ¶ˆé™¤çš„ç²¾ç»†ç²’åº¦è¡°å‡ï¼‰ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥é‚»è¿‘æ„ŸçŸ¥é—å¿˜ã€‚FADEåŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰æ¦‚å¿µé‚»åŸŸï¼Œç”¨äºè¯†åˆ«ç›¸å…³æ¦‚å¿µé›†çš„é‚»è¿‘é›†ï¼›ï¼ˆ2ï¼‰ç½‘æ ¼æ¨¡å—ï¼Œé‡‡ç”¨æ’æ–¥ã€é‚»è¿‘å’Œå¯¼å‘æŸå¤±ç»„ä»¶çš„ç»“æ„ç»„åˆã€‚è¿™äº›åŠŸèƒ½èƒ½å¤Ÿåœ¨ç²¾ç¡®åˆ é™¤ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ï¼Œä¿ç•™ç›¸å…³å’Œä¸ç›¸å…³æ¦‚å¿µçš„ä¿çœŸåº¦ã€‚åœ¨Stanford Dogsã€Oxford Flowersã€CUBã€I2Pã€Imagenetteå’ŒImageNet1kç­‰æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒFADEæœ‰æ•ˆåœ°ç§»é™¤äº†ç›®æ ‡æ¦‚å¿µï¼Œå¯¹å…³è”æ¦‚å¿µçš„å½±å“æœ€å°ï¼Œåœ¨ä¿ç•™æ€§èƒ½ä¸Šè¾ƒç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•æé«˜äº†è‡³å°‘12%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19783v1">PDF</a> Published in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†FADEï¼ˆç”¨äºæ‰©æ•£æ“¦é™¤çš„ç²¾ç»†ç²’åº¦è¡°å‡ï¼‰æ–¹æ³•ï¼Œä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ç°æœ‰æ“¦é™¤ç®—æ³•åœ¨ç§»é™¤ç‰¹å®šç›®æ ‡æ¦‚å¿µæ—¶æ— æ³•ä¿ç•™è¯­ä¹‰ç›¸å…³æ¦‚å¿µçŸ¥è¯†çš„é—®é¢˜ï¼Œå³æ‰€è°“çš„é‚»æ¥é—®é¢˜ã€‚FADEåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šä¸€æ˜¯æ¦‚å¿µé‚»åŸŸï¼Œç”¨äºè¯†åˆ«ç›¸å…³æ¦‚å¿µçš„é‚»æ¥é›†ï¼›äºŒæ˜¯ç½‘æ ¼æ¨¡å—ï¼Œé‡‡ç”¨æ’é™¤ã€é‚»æ¥å’Œå¯¼å‘æŸå¤±ç»„ä»¶çš„ç»“æ„ç»„åˆã€‚è¿™ä½¿å¾—åœ¨ç²¾ç¡®æ“¦é™¤ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™ç›¸å…³å’Œä¸ç›¸å…³æ¦‚å¿µçš„ä¿çœŸåº¦ã€‚åœ¨Stanford Dogsã€Oxford Flowersã€CUBã€I2Pã€Imagenetteå’ŒImageNet1kç­‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFADEåœ¨ä¿ç•™æ€§èƒ½ä¸Šè¾ƒç°æœ‰æŠ€æœ¯è‡³å°‘æé«˜äº†12%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FADEæ–¹æ³•è¢«æå‡ºä»¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ç°æœ‰æ“¦é™¤ç®—æ³•çš„é‚»æ¥é—®é¢˜ã€‚</li>
<li>FADEåŒ…å«æ¦‚å¿µé‚»åŸŸå’Œç½‘æ ¼æ¨¡å—ä¸¤ä¸ªä¸»è¦ç»„ä»¶ã€‚</li>
<li>æ¦‚å¿µé‚»åŸŸç”¨äºè¯†åˆ«ç›¸å…³æ¦‚å¿µçš„é‚»æ¥é›†ã€‚</li>
<li>ç½‘æ ¼æ¨¡å—é‡‡ç”¨æ’é™¤ã€é‚»æ¥å’Œå¯¼å‘æŸå¤±ç»„ä»¶çš„ç»“æ„ç»„åˆã€‚</li>
<li>FADEèƒ½å¤Ÿç²¾ç¡®æ“¦é™¤ç›®æ ‡æ¦‚å¿µï¼ŒåŒæ—¶ä¿ç•™ç›¸å…³å’Œä¸ç›¸å…³æ¦‚å¿µçš„ä¿çœŸåº¦ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFADEåœ¨ä¿ç•™æ€§èƒ½ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-546bdc64a657775e2c6ccae49a99179e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eded41b64117fbf516a12bdd9ab7c3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c462d0f988050735e8d162eb19f4442.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PCM-Picard-Consistency-Model-for-Fast-Parallel-Sampling-of-Diffusion-Models"><a href="#PCM-Picard-Consistency-Model-for-Fast-Parallel-Sampling-of-Diffusion-Models" class="headerlink" title="PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion   Models"></a>PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion   Models</h2><p><strong>Authors:Junhyuk So, Jiwoong Shin, Chaeyeon Jang, Eunhyeok Park</strong></p>
<p>Recently, diffusion models have achieved significant advances in vision, text, and robotics. However, they still face slow generation speeds due to sequential denoising processes. To address this, a parallel sampling method based on Picard iteration was introduced, effectively reducing sequential steps while ensuring exact convergence to the original output. Nonetheless, Picard iteration does not guarantee faster convergence, which can still result in slow generation in practice. In this work, we propose a new parallelization scheme, the Picard Consistency Model (PCM), which significantly reduces the number of generation steps in Picard iteration. Inspired by the consistency model, PCM is directly trained to predict the fixed-point solution, or the final output, at any stage of the convergence trajectory. Additionally, we introduce a new concept called model switching, which addresses PCMâ€™s limitations and ensures exact convergence. Extensive experiments demonstrate that PCM achieves up to a 2.71x speedup over sequential sampling and a 1.77x speedup over Picard iteration across various tasks, including image generation and robotic control. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ã€æ–‡æœ¬å’Œæœºå™¨äººé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºè¿ç»­çš„é™å™ªè¿‡ç¨‹ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºçš®å¡è¿­ä»£ï¼ˆPicard iterationï¼‰çš„å¹¶è¡Œé‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘é¡ºåºæ­¥éª¤çš„åŒæ—¶ç¡®ä¿äº†å‘åŸå§‹è¾“å‡ºçš„ç²¾ç¡®æ”¶æ•›ã€‚ç„¶è€Œï¼Œçš®å¡è¿­ä»£å¹¶ä¸èƒ½ä¿è¯æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå®é™…ä¸­ä»å¯èƒ½å¯¼è‡´ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆï¼Œå³çš®å¡ä¸€è‡´æ€§æ¨¡å‹ï¼ˆPCMï¼‰ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—å‡å°‘äº†çš®å¡è¿­ä»£ä¸­çš„ç”Ÿæˆæ­¥éª¤ã€‚å—ä¸€è‡´æ€§æ¨¡å‹çš„å¯å‘ï¼ŒPCMç›´æ¥è®­ç»ƒä»¥é¢„æµ‹æ”¶æ•›è½¨è¿¹ä»»ä½•é˜¶æ®µçš„å®šç‚¹è§£æˆ–æœ€ç»ˆè¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºæ¨¡å‹åˆ‡æ¢çš„æ–°æ¦‚å¿µï¼Œè§£å†³äº†PCMçš„é™åˆ¶å¹¶ç¡®ä¿ç²¾ç¡®æ”¶æ•›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPCMç›¸å¯¹äºé¡ºåºé‡‡æ ·å®ç°äº†é«˜è¾¾2.71å€çš„åŠ é€Ÿï¼Œç›¸å¯¹äºçš®å¡è¿­ä»£åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†1.77å€çš„åŠ é€Ÿï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œæœºå™¨äººæ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19731v1">PDF</a> Accepted to the CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ã€æ–‡æœ¬å’Œæœºå™¨äººé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨ç”Ÿæˆé€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åŸºäºPicardè¿­ä»£çš„å¹¶è¡Œé‡‡æ ·æ–¹æ³•ï¼Œå‡å°‘äº†åºåˆ—æ­¥éª¤å¹¶ç¡®ä¿æ”¶æ•›åˆ°åŸå§‹è¾“å‡ºã€‚ç„¶è€Œï¼ŒPicardè¿­ä»£å¹¶ä¸ä¿è¯å¿«é€Ÿæ”¶æ•›ã€‚æœ¬æ–‡æå‡ºæ–°çš„å¹¶è¡Œæ–¹æ¡ˆâ€”â€”Picardä¸€è‡´æ€§æ¨¡å‹ï¼ˆPCMï¼‰ï¼Œæ˜¾è‘—å‡å°‘Picardè¿­ä»£çš„ç”Ÿæˆæ­¥éª¤ã€‚PCMç›´æ¥é¢„æµ‹æ”¶æ•›è½¨è¿¹ä»»ä½•é˜¶æ®µçš„å®šç‚¹è§£æˆ–æœ€ç»ˆè¾“å‡ºã€‚æ­¤å¤–ï¼Œå¼•å…¥æ¨¡å‹åˆ‡æ¢æ¦‚å¿µï¼Œè§£å†³PCMçš„å±€é™æ€§å¹¶ç¡®ä¿ç²¾ç¡®æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼ŒPCMç›¸å¯¹äºåºåˆ—é‡‡æ ·å’ŒPicardè¿­ä»£ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜è¾¾2.71å€å’Œ1.77å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—è¿›å±•ï¼Œä½†ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>å¹¶è¡Œé‡‡æ ·æ–¹æ³•åŸºäºPicardè¿­ä»£ï¼Œå‡å°‘åºåˆ—æ­¥éª¤å¹¶ç¡®ä¿æ”¶æ•›ã€‚</li>
<li>Picardè¿­ä»£å¹¶ä¸æ€»æ˜¯ä¿è¯å¿«é€Ÿæ”¶æ•›ã€‚</li>
<li>æå‡ºæ–°çš„å¹¶è¡Œæ–¹æ¡ˆâ€”â€”Picardä¸€è‡´æ€§æ¨¡å‹ï¼ˆPCMï¼‰ï¼Œæ˜¾è‘—å‡å°‘ç”Ÿæˆæ­¥éª¤ã€‚</li>
<li>PCMç›´æ¥é¢„æµ‹æ”¶æ•›è½¨è¿¹çš„å®šç‚¹è§£æˆ–æœ€ç»ˆè¾“å‡ºã€‚</li>
<li>å¼•å…¥æ¨¡å‹åˆ‡æ¢æ¦‚å¿µï¼Œè§£å†³PCMçš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-de9206b65e32b86285ecbb942c433738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7815460173cc3656aeb5370e4db7ebe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da903321f8e6a00668f8412a9d1998d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1582901ae6d69b9f16e1fd08ebcd61a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efffa7de85c6f0819892a6524d5b447a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40bb40e3b2197b8595c381bce7fbd234.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World"><a href="#OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World" class="headerlink" title="OpenSDI: Spotting Diffusion-Generated Images in the Open World"></a>OpenSDI: Spotting Diffusion-Generated Images in the Open World</h2><p><strong>Authors:Yabin Wang, Zhiwu Huang, Xiaopeng Hong</strong></p>
<p>This paper identifies OpenSDI, a challenge for spotting diffusion-generated images in open-world settings. In response to this challenge, we define a new benchmark, the OpenSDI dataset (OpenSDID), which stands out from existing datasets due to its diverse use of large vision-language models that simulate open-world diffusion-based manipulations. Another outstanding feature of OpenSDID is its inclusion of both detection and localization tasks for images manipulated globally and locally by diffusion models. To address the OpenSDI challenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up a mixture of foundation models. This approach exploits a collaboration mechanism with multiple pretrained foundation models to enhance generalization in the OpenSDI context, moving beyond traditional training by synergizing multiple pretrained models through prompting and attending strategies. Building on this scheme, we introduce MaskCLIP, an SPM-based model that aligns Contrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE). Extensive evaluations on OpenSDID show that MaskCLIP significantly outperforms current state-of-the-art methods for the OpenSDI challenge, achieving remarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in accuracy (2.38% in F1) compared to the second-best model in localization and detection tasks, respectively. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI">https://github.com/iamwangyabin/OpenSDI</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†OpenSDIæŒ‘æˆ˜ï¼Œå³åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¯†åˆ«æ‰©æ•£ç”Ÿæˆå›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå³OpenSDIæ•°æ®é›†ï¼ˆOpenSDIDï¼‰ã€‚ç”±äºå®ƒä½¿ç”¨äº†æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œæ‰©æ•£æ“ä½œçš„å¤šæ ·åŒ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤ä¸å…¶ä»–ç°æœ‰æ•°æ®é›†æœ‰æ‰€ä¸åŒã€‚OpenSDIDçš„å¦ä¸€ä¸ªçªå‡ºç‰¹ç‚¹æ˜¯ï¼Œå®ƒåŒ…å«äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹å…¨å±€å’Œå±€éƒ¨æ“ä½œå›¾åƒçš„æ£€æµ‹å’Œå®šä½ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³OpenSDIæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆï¼Œå»ºç«‹äº†ä¸€ç³»åˆ—åŸºç¡€æ¨¡å‹çš„æ··åˆã€‚è¯¥æ–¹æ³•é€šè¿‡åä½œæœºåˆ¶åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå¢å¼ºäº†OpenSDIä¸Šä¸‹æ–‡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æç¤ºå’Œæ³¨æ„ç­–ç•¥ååŒå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè®­ç»ƒã€‚åŸºäºè¯¥æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†MaskCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºSPMçš„æ¨¡å‹ï¼Œå®ƒå°†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ç›¸ç»“åˆã€‚åœ¨OpenSDIDä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„OpenSDIæŒ‘æˆ˜æ–¹æ³•ç›¸æ¯”ï¼ŒMaskCLIPåœ¨IoUï¼ˆæé«˜14.23%ï¼ŒF1æé«˜14.11%ï¼‰ã€å‡†ç¡®ç‡ï¼ˆæé«˜2.05%ï¼ŒF1æé«˜2.38%ï¼‰æ–¹é¢å®ç°äº†æ˜¾è‘—ç›¸å¯¹æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/iamwangyabin/OpenSDIè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19653v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†OpenSDIæŒ‘æˆ˜ï¼Œå³è¯†åˆ«å¼€æ”¾ä¸–ç•Œä¸­æ‰©æ•£ç”Ÿæˆçš„å›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•é›†OpenSDIæ•°æ®é›†ï¼ˆOpenSDIDï¼‰ï¼Œå®ƒä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œçš„æ‰©æ•£æ“ä½œï¼Œå¹¶åŒ…å«æ£€æµ‹å’Œå®šä½ä»»åŠ¡ã€‚æå‡ºååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆï¼Œé€šè¿‡ååŒå¤šç§é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œæé«˜åœ¨OpenSDIèƒŒæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºè¯¥æ–¹æ¡ˆï¼Œå¼•å…¥äº†MaskCLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å¯¹æ¯”æ€§è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ç›¸ç»“åˆã€‚åœ¨OpenSDIDä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMaskCLIPåœ¨OpenSDIæŒ‘æˆ˜ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å®šä½å’Œæ£€æµ‹ä»»åŠ¡ä¸Šçš„IoUå’ŒF1åˆ†æ•°å‡æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenSDIæŒ‘æˆ˜ï¼šè¯†åˆ«å¼€æ”¾ä¸–ç•Œä¸­æ‰©æ•£ç”Ÿæˆçš„å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>OpenSDIDæ•°æ®é›†ï¼šåŒ…å«æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œæ‰©æ•£æ“ä½œçš„å›¾åƒï¼Œæ¶µç›–æ£€æµ‹å’Œå®šä½ä»»åŠ¡ã€‚</li>
<li>ååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆï¼šé€šè¿‡ååŒå¤šç§é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MaskCLIPæ¨¡å‹ï¼šç»“åˆCLIPå’ŒMAEæŠ€æœ¯ï¼Œé’ˆå¯¹OpenSDIæŒ‘æˆ˜è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MaskCLIPåœ¨OpenSDIDä¸Šçš„è¯„ä¼°ï¼šç›¸è¾ƒäºç¬¬äºŒä½³æ¨¡å‹ï¼ŒIoUå’ŒF1åˆ†æ•°æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>å¯ç”¨èµ„æºï¼šæ•°æ®é›†å’Œä»£ç å¯åœ¨æŒ‡å®šGitHubä»“åº“æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2177884040417073ed3351bf6e65c32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7fabf16842a315f69ace544e7d8ef57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b614775e99b1cb1eeb929018d1f37bbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12cf42c909018a00a44b0d92d138a5a2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Time-of-Flight-Depth-Denoising-with-Confidence-Aware-Diffusion-Model"><a href="#Towards-Robust-Time-of-Flight-Depth-Denoising-with-Confidence-Aware-Diffusion-Model" class="headerlink" title="Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware   Diffusion Model"></a>Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware   Diffusion Model</h2><p><strong>Authors:Changyong He, Jin Zeng, Jiawei Zhang, Jiajie Guo</strong></p>
<p>Time-of-Flight (ToF) sensors efficiently capture scene depth, but the nonlinear depth construction procedure often results in extremely large noise variance or even invalid areas. Recent methods based on deep neural networks (DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when presented with severe noise corruption due to limited prior knowledge of ToF data distribution. In this paper, we propose DepthCAD, a novel ToF denoising approach that ensures global structural smoothness by leveraging the rich prior knowledge in Stable Diffusion and maintains local metric accuracy by steering the diffusion process with confidence guidance. To adopt the pretrained image diffusion model to ToF depth denoising, we apply the diffusion on raw ToF correlation measurements with dynamic range normalization before converting to depth maps. Experimental results validate the state-of-the-art performance of the proposed scheme, and the evaluation on real data further verifies its robustness against real-world ToF noise. </p>
<blockquote>
<p>é£è¡Œæ—¶é—´ï¼ˆToFï¼‰ä¼ æ„Ÿå™¨å¯ä»¥æœ‰æ•ˆåœ°æ•æ‰åœºæ™¯æ·±åº¦ï¼Œä½†éçº¿æ€§æ·±åº¦æ„å»ºè¿‡ç¨‹å¾€å¾€ä¼šå¯¼è‡´æå¤§çš„å™ªå£°æ–¹å·®ç”šè‡³æ— æ•ˆåŒºåŸŸã€‚æœ€è¿‘åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æ–¹æ³•æé«˜äº†ToFå»å™ªç²¾åº¦ï¼Œä½†ç”±äºå¯¹ToFæ•°æ®åˆ†å¸ƒçš„å…ˆéªŒçŸ¥è¯†æœ‰é™ï¼Œå½“é¢ä¸´ä¸¥é‡å™ªå£°è…èš€æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DepthCADï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ToFå»å™ªæ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨Stable Diffusionä¸­çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†æ¥ä¿è¯å…¨å±€ç»“æ„å¹³æ»‘æ€§ï¼Œå¹¶é€šè¿‡ä¿¡å¿ƒå¼•å¯¼æ§åˆ¶æ‰©æ•£è¿‡ç¨‹æ¥ç»´æŒå±€éƒ¨åº¦é‡ç²¾åº¦ã€‚ä¸ºäº†å°†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºToFæ·±åº¦å»å™ªï¼Œæˆ‘ä»¬åœ¨å°†åŸå§‹ToFç›¸å…³æµ‹é‡å€¼è½¬æ¢ä¸ºæ·±åº¦å›¾ä¹‹å‰ï¼Œå¯¹å…¶è¿›è¡Œäº†åŠ¨æ€èŒƒå›´å½’ä¸€åŒ–çš„æ‰©æ•£å¤„ç†ã€‚å®éªŒç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ¡ˆçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¯¹çœŸå®æ•°æ®çš„è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†å…¶å¯¹ç°å®ä¸–ç•ŒToFå™ªå£°çš„é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ToFä¼ æ„Ÿå™¨æ•æ‰åœºæ™¯æ·±åº¦æ—¶æ•ˆç‡é«˜ï¼Œä½†å…¶éçº¿æ€§æ·±åº¦æ„å»ºè¿‡ç¨‹ä¼šå¯¼è‡´æå¤§çš„å™ªå£°æ–¹å·®ç”šè‡³æ— æ•ˆåŒºåŸŸã€‚æœ¬æ–‡æå‡ºåŸºäºStable Diffusionçš„æ·±åº¦å»å™ªæŠ€æœ¯çš„æ–°æ–¹æ³•DepthCADï¼Œå®ƒé€šè¿‡ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å®ç°å…¨å±€ç»“æ„å¹³æ»‘ï¼ŒåŒæ—¶å€ŸåŠ©æ‰©æ•£è¿‡ç¨‹é€šè¿‡ç½®ä¿¡å¼•å¯¼ç¡®ä¿å±€éƒ¨åº¦é‡çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å°†å›¾åƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºToFæ·±åº¦å»å™ªå’ŒåŠ¨æ€èŒƒå›´å½’ä¸€åŒ–æ¥æ”¹è¿›åŸå§‹çš„ToFç›¸å…³æ€§æµ‹é‡å€¼ï¼Œåœ¨è½¬ä¸ºæ·±åº¦å›¾å‰é‡‡ç”¨è¯¥æ–¹æ¡ˆéªŒè¯äº†æå‡ºçš„ç®—æ³•ä¸ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®æ•°æ®ä¸Šçš„è¯„ä¼°éªŒè¯äº†å…¶å¯¹çœŸå®ä¸–ç•ŒToFå™ªå£°çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ToFä¼ æ„Ÿå™¨æ•æ‰åœºæ™¯æ·±åº¦æ•ˆç‡é«˜ï¼Œä½†å­˜åœ¨éçº¿æ€§æ·±åº¦æ„å»ºé—®é¢˜å¯¼è‡´å™ªå£°é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•åœ¨ä¸¥é‡å™ªå£°å¹²æ‰°ä¸‹è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ToFæ•°æ®åˆ†å¸ƒçš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>DepthCADæ–¹æ³•åˆ©ç”¨Stable Diffusionçš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ç¡®ä¿å…¨å±€ç»“æ„å¹³æ»‘ã€‚</li>
<li>DepthCADé€šè¿‡ç½®ä¿¡å¼•å¯¼ç»´æŒå±€éƒ¨åº¦é‡å‡†ç¡®æ€§ã€‚</li>
<li>DepthCADå°†å›¾åƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºToFæ·±åº¦å»å™ªã€‚</li>
<li>åŠ¨æ€èŒƒå›´å½’ä¸€åŒ–æ”¹è¿›äº†åŸå§‹çš„ToFç›¸å…³æ€§æµ‹é‡å€¼ï¼Œåœ¨è½¬ä¸ºæ·±åº¦å›¾å‰é‡‡ç”¨è¯¥æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99c78c43e04f8f517287a42d2a99e097.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af4a07ffb0402cbf8a1fc5696b61de4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbe6364e527be94c22848fe44628b49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef194dd426eeb7e2527caeec791ba04d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ea64c7a811993d3962810f0bb9b2196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2ef365fed8754d5c2ce0528dc86fb1c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing"><a href="#Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing" class="headerlink" title="Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing"></a>Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing</h2><p><strong>Authors:Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung</strong></p>
<p>We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion modelsâ€“offering faster generation and high-quality outputs in state-of-the-art image and video generative modelsâ€“efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹ï¼ˆflow modelsï¼‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚æœ€è¿‘ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰ä¸­å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå®ƒé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ä½¿è¾“å‡ºä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œç”±äºä¸­é—´å»å™ªæ­¥éª¤çš„éšæœºæ€§ï¼Œç²’å­é‡‡æ ·ï¼ˆparticle samplingï¼‰å®ç°äº†æ›´æœ‰æ•ˆçš„ç¼©æ”¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£å“è€Œå¹¿å—æ¬¢è¿ï¼Œå®ƒä»¬æä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦å’Œé«˜è´¨é‡è¾“å‡ºï¼Œåœ¨å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶ç¡®å®šæ€§ç”Ÿæˆè¿‡ç¨‹ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹çš„æ•ˆç‡æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨ã€‚ä¸ºäº†å®ç°æµæ¨¡å‹çš„æ¨ç†æ—¶é—´æœ‰æ•ˆç¼©æ”¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼š1ï¼‰åŸºäºSDEçš„ç”Ÿæˆï¼Œä½¿æµæ¨¡å‹èƒ½å¤Ÿå®ç°ç²’å­é‡‡æ ·ï¼›2ï¼‰æ’å€¼è½¬æ¢ï¼ˆInterpolant conversionï¼‰ï¼Œæ‰©å¤§æœç´¢ç©ºé—´å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ï¼›3ï¼‰æ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRollover Budget Forcingï¼ŒRBFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”åˆ†é…æ—¶é—´æ­¥é•¿ä¸Šçš„è®¡ç®—èµ„æºï¼Œä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºSDEçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºæ–¹å·®ä¿æŒï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆï¼Œæé«˜äº†æµæ¨¡å‹ä¸­æ¨ç†æ—¶é—´ç¼©æ”¾çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ç»“åˆVP-SDEçš„RBFå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19385v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://flow-inference-time-scaling.github.io/">https://flow-inference-time-scaling.github.io/</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨ç†æ—¶é—´ç¼©æ”¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªå…³é”®æ€è·¯ï¼šåŸºäºSDEçš„ç”Ÿæˆï¼Œå®ç°æµæ¨¡å‹ä¸­çš„ç²’å­é‡‡æ ·ï¼›æ’å€¼è½¬æ¢ï¼Œæ‰©å¤§æœç´¢ç©ºé—´å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ï¼›ä»¥åŠæ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ï¼Œåœ¨æ—¶åºä¸Šè‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºSDEçš„ç”Ÿæˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ–¹å·®ä¿ç•™ï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆæ–¹æ³•ï¼Œæ”¹è¿›äº†æµæ¨¡å‹ä¸­æ¨ç†æ—¶é—´ç¼©æ”¾çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç»“åˆVP-SDEçš„RBFæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ‰€æœ‰æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨åŸºäºSDEçš„ç”Ÿæˆæ–¹æ³•å®ç°æµæ¨¡å‹ä¸­çš„ç²’å­é‡‡æ ·ã€‚</li>
<li>æ’å€¼è½¬æ¢æ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œå¢å¼ºäº†æ ·æœ¬å¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥æ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ä»¥è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ–¹å·®ä¿ç•™ï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆæ–¹æ³•æ”¹è¿›äº†ç²’å­é‡‡æ ·æ€§èƒ½ã€‚</li>
<li>ç»“åˆVP-SDEçš„RBFæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œä¼˜äºå…¶ä»–æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5f7be64cfd205526939bda78755800a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae294f9952228030d7ceab9a0a80ec7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96e2e8dc1d7645016f61b405a55d3289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49c4f121fe0b6c9024e6989610830aef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MVPortrait-Text-Guided-Motion-and-Emotion-Control-for-Multi-view-Vivid-Portrait-Animation"><a href="#MVPortrait-Text-Guided-Motion-and-Emotion-Control-for-Multi-view-Vivid-Portrait-Animation" class="headerlink" title="MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid   Portrait Animation"></a>MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid   Portrait Animation</h2><p><strong>Authors:Yukang Lin, Hokit Fung, Jianjin Xu, Zeping Ren, Adela S. M. Lau, Guosheng Yin, Xiu Li</strong></p>
<p>Recent portrait animation methods have made significant strides in generating realistic lip synchronization. However, they often lack explicit control over head movements and facial expressions, and cannot produce videos from multiple viewpoints, resulting in less controllable and expressive animations. Moreover, text-guided portrait animation remains underexplored, despite its user-friendly nature. We present a novel two-stage text-guided framework, MVPortrait (Multi-view Vivid Portrait), to generate expressive multi-view portrait animations that faithfully capture the described motion and emotion. MVPortrait is the first to introduce FLAME as an intermediate representation, effectively embedding facial movements, expressions, and view transformations within its parameter space. In the first stage, we separately train the FLAME motion and emotion diffusion models based on text input. In the second stage, we train a multi-view video generation model conditioned on a reference portrait image and multi-view FLAME rendering sequences from the first stage. Experimental results exhibit that MVPortrait outperforms existing methods in terms of motion and emotion control, as well as view consistency. Furthermore, by leveraging FLAME as a bridge, MVPortrait becomes the first controllable portrait animation framework that is compatible with text, speech, and video as driving signals. </p>
<blockquote>
<p>æœ€è¿‘çš„è‚–åƒåŠ¨ç”»æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„å”‡åŒæ­¥æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹å¯¹å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„æ˜ç¡®æ§åˆ¶ï¼Œå¹¶ä¸”ä¸èƒ½ä»å¤šä¸ªè§†è§’ç”Ÿæˆè§†é¢‘ï¼Œå¯¼è‡´åŠ¨ç”»çš„å¯æ§æ€§å’Œè¡¨ç°åŠ›è¾ƒå·®ã€‚å°½ç®¡æ–‡æœ¬å¼•å¯¼çš„è‚–åƒåŠ¨ç”»åœ¨ç”¨æˆ·å‹å¥½æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼çš„ä¸¤é˜¶æ®µæ¡†æ¶MVPortraitï¼ˆå¤šè§†è§’ç”ŸåŠ¨è‚–åƒï¼‰ï¼Œç”¨äºç”Ÿæˆè¡¨ç°åŠ›å¼ºçš„å¤šè§†è§’è‚–åƒåŠ¨ç”»ï¼Œå¿ å®å‘ˆç°æè¿°çš„è¿åŠ¨å’Œæƒ…æ„Ÿã€‚MVPortraité¦–æ¬¡å¼•å…¥äº†FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œæœ‰æ•ˆåœ°åœ¨å…¶å‚æ•°ç©ºé—´ä¸­åµŒå…¥é¢éƒ¨è¿åŠ¨ã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬è¾“å…¥åˆ†åˆ«è®­ç»ƒFLAMEè¿åŠ¨å’Œæƒ…æ„Ÿæ‰©æ•£æ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªåŸºäºå‚è€ƒè‚–åƒå›¾åƒå’Œç¬¬ä¸€é˜¶æ®µçš„å¤šè§†è§’FLAMEæ¸²æŸ“åºåˆ—çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMVPortraitåœ¨åŠ¨ä½œã€æƒ…æ„Ÿæ§åˆ¶å’Œè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨FLAMEä½œä¸ºæ¡¥æ¢ï¼ŒMVPortraitæˆä¸ºç¬¬ä¸€ä¸ªå…¼å®¹æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘é©±åŠ¨ä¿¡å·çš„å¯æ§åˆ¶è‚–åƒåŠ¨ç”»æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19383v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬é©±åŠ¨çš„ä¸¤é˜¶æ®µåŠ¨ç”»æ¡†æ¶MVPortraitï¼Œå®ƒèƒ½ç”Ÿæˆè¡¨è¾¾æ€§å¼ºçš„å¤šè§†è§’è‚–åƒåŠ¨ç”»ï¼Œå¿ å®å†ç°æè¿°çš„åŠ¨ä½œå’Œæƒ…æ„Ÿã€‚é€šè¿‡ä½¿ç”¨FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæœ‰æ•ˆåµŒå…¥é¢éƒ¨åŠ¨ä½œã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMVPortraitåœ¨åŠ¨ä½œå’Œæƒ…ç»ªæ§åˆ¶ä»¥åŠè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªå…¼å®¹æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘é©±åŠ¨ä¿¡å·çš„å¯æ§åˆ¶è‚–åƒåŠ¨ç”»æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVPortraitæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬é©±åŠ¨çš„ä¸¤é˜¶æ®µåŠ¨ç”»æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šè§†è§’è‚–åƒåŠ¨ç”»ã€‚</li>
<li>MVPortraitä½¿ç”¨FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæœ‰æ•ˆåµŒå…¥é¢éƒ¨åŠ¨ä½œã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚</li>
<li>MVPortraitèƒ½å¤Ÿå¿ å®å†ç°æè¿°çš„åŠ¨ä½œå’Œæƒ…æ„Ÿï¼Œå®ç°é«˜çº§çš„è¡¨è¾¾æ€§åŠ¨ç”»ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒMVPortraitåœ¨åŠ¨ä½œå’Œæƒ…ç»ªæ§åˆ¶ä»¥åŠè§†è§’ä¸€è‡´æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>MVPortraitæ¡†æ¶æ˜¯é¦–ä¸ªå…¼å®¹æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘ä½œä¸ºé©±åŠ¨ä¿¡å·çš„å¯æ§åˆ¶è‚–åƒåŠ¨ç”»ç³»ç»Ÿã€‚</li>
<li>MVPortraité€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒæ¨¡å‹ï¼Œå…ˆè®­ç»ƒFLAMEåŠ¨ä½œå’Œæƒ…ç»ªæ‰©æ•£æ¨¡å‹ï¼Œå†è®­ç»ƒå¤šè§†è§’è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd660a5d22e882d2115fcdd4b112ab8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e888d4aa15441baf8bd9c46652036b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca20a3d3fb5284e9d4538186569ed9ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-749a8d01bf6be1d450c42159bff7c75f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Interpretable-Generative-Models-through-Post-hoc-Concept-Bottlenecks"><a href="#Interpretable-Generative-Models-through-Post-hoc-Concept-Bottlenecks" class="headerlink" title="Interpretable Generative Models through Post-hoc Concept Bottlenecks"></a>Interpretable Generative Models through Post-hoc Concept Bottlenecks</h2><p><strong>Authors:Akshay Kulkarni, Ge Yan, Chung-En Sun, Tuomas Oikarinen, Tsui-Wei Weng</strong></p>
<p>Concept bottleneck models (CBM) aim to produce inherently interpretable models that rely on human-understandable concepts for their predictions. However, existing approaches to design interpretable generative models based on CBMs are not yet efficient and scalable, as they require expensive generative model training from scratch as well as real images with labor-intensive concept supervision. To address these challenges, we present two novel and low-cost methods to build interpretable generative models through post-hoc techniques and we name our approaches: concept-bottleneck autoencoder (CB-AE) and concept controller (CC). Our proposed approaches enable efficient and scalable training without the need of real data and require only minimal to no concept supervision. Additionally, our methods generalize across modern generative model families including generative adversarial networks and diffusion models. We demonstrate the superior interpretability and steerability of our methods on numerous standard datasets like CelebA, CelebA-HQ, and CUB with large improvements (average ~25%) over the prior work, while being 4-15x faster to train. Finally, a large-scale user study is performed to validate the interpretability and steerability of our methods. </p>
<blockquote>
<p>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰æ—¨åœ¨äº§ç”Ÿæœ¬è´¨ä¸Šå¯è§£é‡Šæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºäººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥è¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼ŒåŸºäºCBMè®¾è®¡å¯è§£é‡Šçš„ç”Ÿæˆæ¨¡å‹ç°æœ‰æ–¹æ³•å°šç¼ºä¹æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä»é›¶å¼€å§‹è¿›è¡Œæ˜‚è´µçš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒï¼Œå¹¶ä¸”è¿˜éœ€è¦åŠ³åŠ¨å¯†é›†å‹çš„æ¦‚å¿µç›‘ç£ä»¥è·å–çœŸå®å›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°å‹ä¸”ä½æˆæœ¬çš„æ–¹æ³•ï¼Œé€šè¿‡äº‹åæŠ€æœ¯æ„å»ºå¯è§£é‡Šçš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºï¼šæ¦‚å¿µç“¶é¢ˆè‡ªç¼–ç å™¨ï¼ˆCB-AEï¼‰å’Œæ¦‚å¿µæ§åˆ¶å™¨ï¼ˆCCï¼‰ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ— éœ€çœŸå®æ•°æ®ï¼Œä»…éœ€è¦æœ€å°‘æˆ–æ— éœ€æ¦‚å¿µç›‘ç£ï¼Œå³å¯å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹åœ¨å†…çš„ç°ä»£ç”Ÿæˆæ¨¡å‹å®¶æ—ã€‚æˆ‘ä»¬åœ¨CelebAã€CelebA-HQå’ŒCUBç­‰å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æ–¹æ³•çš„å“è¶Šå¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ï¼Œå¹³å‡æ”¹è¿›å¹…åº¦è¾¾çº¦25%ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ¯”å…ˆå‰å·¥ä½œå¿«4-15å€ã€‚æœ€åï¼Œè¿›è¡Œå¤§è§„æ¨¡ç”¨æˆ·ç ”ç©¶ä»¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19377v1">PDF</a> CVPR 2025. Project Page:   <a target="_blank" rel="noopener" href="https://lilywenglab.github.io/posthoc-generative-cbm/">https://lilywenglab.github.io/posthoc-generative-cbm/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰çš„ç”Ÿæˆæ¨¡å‹æ—¨åœ¨ç”Ÿæˆå¯è§£é‡Šçš„é¢„æµ‹æ¨¡å‹ï¼Œä½†å…¶è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è§„æ¨¡åŒ–æ¨å¹¿ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°å‹çš„ã€ä½æˆæœ¬çš„æ–¹æ³•æ„å»ºå¯è§£é‡Šçš„ç”Ÿæˆæ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯äº‹åæŠ€æœ¯ä¸­çš„æ¦‚å¿µç“¶é¢ˆè‡ªåŠ¨ç¼–ç å™¨ï¼ˆCB-AEï¼‰å’Œæ¦‚å¿µæ§åˆ¶å™¨ï¼ˆCCï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•æ— éœ€çœŸå®æ•°æ®ï¼Œä»…éœ€æœ€å°‘çš„æ¦‚å¿µç›‘ç£ï¼Œå°±èƒ½å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„è®­ç»ƒï¼Œå¹¶é€‚ç”¨äºç°ä»£ç”Ÿæˆæ¨¡å‹å®¶æ—ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ã€‚åœ¨CelebAã€CelebA-HQå’ŒCUBç­‰æ ‡å‡†æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ï¼Œå¹³å‡æ”¹è¿›äº†çº¦25%ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æé«˜äº†4-15å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡çš„ç”¨æˆ·ç ”ç©¶æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰æ—¨åœ¨ç”Ÿäº§å¯è§£é‡Šçš„é¢„æµ‹æ¨¡å‹ï¼Œä½†å…¶ç°æœ‰æ–¹æ³•é¢ä¸´è®­ç»ƒæˆæœ¬é«˜å’Œä¸å¯æ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„ä¸¤ç§æ–¹æ³•ï¼ˆCB-AEå’ŒCCï¼‰é€šè¿‡äº‹åæŠ€æœ¯æ„å»ºå¯è§£é‡Šçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ— éœ€çœŸå®æ•°æ®å’Œå¤§é‡çš„æ¦‚å¿µç›‘ç£ã€‚</li>
<li>ä¸¤ç§æ–¹æ³•å®ç°äº†é«˜æ•ˆå’Œå¯æ‰©å±•çš„è®­ç»ƒï¼Œé€‚ç”¨äºå¤šç§ç°ä»£ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šï¼Œæ–°æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ï¼Œå¹³å‡æ”¹è¿›çº¦25%ã€‚</li>
<li>æ–°æ–¹æ³•çš„è®­ç»ƒé€Ÿåº¦æ˜¯å…ˆå‰å·¥ä½œçš„4-15å€ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶éªŒè¯äº†æ–°æ–¹æ³•çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5e09ce606c935da625caa19758ef0fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c17434a301a647f680f5a236f8bb9502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac856d68e9ecfa2060747b1491c5340e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df8e5606e30c74999e550c05db59803b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc0be75bbe9d6df75d2029cc31d6f241.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1f968295c2ab192a35cc61062726ad5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DeClotH-Decomposable-3D-Cloth-and-Human-Body-Reconstruction-from-a-Single-Image"><a href="#DeClotH-Decomposable-3D-Cloth-and-Human-Body-Reconstruction-from-a-Single-Image" class="headerlink" title="DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a   Single Image"></a>DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a   Single Image</h2><p><strong>Authors:Hyeongjin Nam, Donghwan Kim, Jeongtaek Oh, Kyoung Mu Lee</strong></p>
<p>Most existing methods of 3D clothed human reconstruction from a single image treat the clothed human as a single object without distinguishing between cloth and human body. In this regard, we present DeClotH, which separately reconstructs 3D cloth and human body from a single image. This task remains largely unexplored due to the extreme occlusion between cloth and the human body, making it challenging to infer accurate geometries and textures. Moreover, while recent 3D human reconstruction methods have achieved impressive results using text-to-image diffusion models, directly applying such an approach to this problem often leads to incorrect guidance, particularly in reconstructing 3D cloth. To address these challenges, we propose two core designs in our framework. First, to alleviate the occlusion issue, we leverage 3D template models of cloth and human body as regularizations, which provide strong geometric priors to prevent erroneous reconstruction by the occlusion. Second, we introduce a cloth diffusion model specifically designed to provide contextual information about cloth appearance, thereby enhancing the reconstruction of 3D cloth. Qualitative and quantitative experiments demonstrate that our proposed approach is highly effective in reconstructing both 3D cloth and the human body. More qualitative results are provided at <a target="_blank" rel="noopener" href="https://hygenie1228.github.io/DeClotH/">https://hygenie1228.github.io/DeClotH/</a>. </p>
<blockquote>
<p>ç°æœ‰å¤§å¤šæ•°åŸºäºå•å¼ å›¾åƒçš„ä¸‰ç»´æœè£…äººä½“é‡å»ºæ–¹æ³•å°†ç©¿ç€è¡£æœçš„äººä½“è§†ä¸ºä¸€ä¸ªæ•´ä½“å¯¹è±¡ï¼Œå¹¶æœªåŒºåˆ†è¡£ç‰©å’Œäººä½“ã€‚é’ˆå¯¹è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†DeClotHæ–¹æ³•ï¼Œå®ƒå¯ä»¥ä»å•å¼ å›¾åƒä¸­åˆ†åˆ«é‡å»ºä¸‰ç»´è¡£ç‰©å’Œäººä½“ã€‚ç”±äºè¡£ç‰©ä¸äººä½“ä¹‹é—´çš„æåº¦é®æŒ¡ï¼Œè¿™ä¸€ä»»åŠ¡åœ¨æ¨æ–­å‡†ç¡®å‡ ä½•ç»“æ„å’Œçº¹ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„3Däººä½“é‡å»ºæ–¹æ³•ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†ç›´æ¥å°†è¿™ç§æ–¹æ³•åº”ç”¨äºè¿™ä¸ªé—®é¢˜é€šå¸¸ä¼šå¯¼è‡´é”™è¯¯çš„æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡å»º3Dè¡£ç‰©æ–¹é¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¼“è§£é®æŒ¡é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨è¡£ç‰©å’Œäººä½“çš„3Dæ¨¡æ¿æ¨¡å‹ä½œä¸ºæ­£åˆ™åŒ–ï¼Œæä¾›å¼ºå¤§çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œä»¥é˜²æ­¢é®æŒ¡å¯¼è‡´çš„é”™è¯¯é‡å»ºã€‚å…¶æ¬¡ å¼•å…¥ä¸“é—¨çš„è¡£ç‰©æ‰©æ•£æ¨¡å‹ï¼Œæä¾›è¡£ç‰©å¤–è§‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜3Dè¡£ç‰©çš„é‡å»ºæ•ˆæœã€‚å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨é‡å»º3Dè¡£ç‰©å’Œäººä½“æ–¹é¢éƒ½éå¸¸æœ‰æ•ˆã€‚æ›´å¤šçš„å®šæ€§ç»“æœè¯·å‚è§ï¼š[<a target="_blank" rel="noopener" href="https://hygenie1228.github.io/DeClotH/]">https://hygenie1228.github.io/DeClotH/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19373v1">PDF</a> Published at CVPR 2025, 17 pages including the supplementary material</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDeClotHçš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾ç‰‡ä¸­åˆ†åˆ«é‡å»º3Dè¡£ç‰©å’Œäººä½“ã€‚è¯¥æ–¹æ³•è§£å†³äº†è¡£ç‰©ä¸äººä½“ä¹‹é—´çš„é®æŒ¡é—®é¢˜ï¼Œåˆ©ç”¨3Dæ¨¡æ¿æ¨¡å‹ä½œä¸ºæ­£åˆ™åŒ–ï¼Œå¹¶å¼•å…¥è¡£ç‰©æ‰©æ•£æ¨¡å‹æä¾›è¡£ç‰©å¤–è§‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†3Dè¡£ç‰©å’Œäººä½“çš„é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeClotHæ–¹æ³•èƒ½å¤Ÿä»å•å¼ å›¾ç‰‡ä¸­åˆ†åˆ«é‡å»º3Dè¡£ç‰©å’Œäººä½“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸å°†è¡£ç‰©å’Œäººä½“è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œè€ŒDeClotHåˆ™èƒ½åŒºåˆ†ä¸¤è€…ã€‚</li>
<li>è¡£ç‰©ä¸äººä½“ä¹‹é—´çš„é®æŒ¡æ˜¯é‡å»ºè¿‡ç¨‹ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>DeClotHåˆ©ç”¨3Dæ¨¡æ¿æ¨¡å‹ä½œä¸ºæ­£åˆ™åŒ–ï¼Œè§£å†³é®æŒ¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥è¡£ç‰©æ‰©æ•£æ¨¡å‹ï¼Œæä¾›è¡£ç‰©å¤–è§‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>DeClotHåœ¨é‡å»º3Dè¡£ç‰©å’Œäººä½“æ–¹é¢å–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0c7ae0a30320ec8cc4ff91aa4666773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87ce113474438260174a52a8baa311f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dae74e9a1eb0ab519f32294f223a244.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e415d6b18eb231c99dfe2abe25d9f34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-972bc350b4e5281c443e82712860db2c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AIM2PC-Aerial-Image-to-3D-Building-Point-Cloud-Reconstruction"><a href="#AIM2PC-Aerial-Image-to-3D-Building-Point-Cloud-Reconstruction" class="headerlink" title="AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction"></a>AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction</h2><p><strong>Authors:Soulaimene Turki, Daniel Panangian, Houda Chaabouni-Chouayakh, Ksenia Bittner</strong></p>
<p>Three-dimensional urban reconstruction of buildings from single-view images has attracted significant attention over the past two decades. However, recent methods primarily focus on rooftops from aerial images, often overlooking essential geometrical details. Additionally, there is a notable lack of datasets containing complete 3D point clouds for entire buildings, along with challenges in obtaining reliable camera pose information for aerial images. This paper addresses these challenges by presenting a novel methodology, AIM2PC , which utilizes our generated dataset that includes complete 3D point clouds and determined camera poses. Our approach takes features from a single aerial image as input and concatenates them with essential additional conditions, such as binary masks and Sobel edge maps, to enable more edge-aware reconstruction. By incorporating a point cloud diffusion model based on Centered denoising Diffusion Probabilistic Models (CDPM), we project these concatenated features onto the partially denoised point cloud using our camera poses at each diffusion step. The proposed method is able to reconstruct the complete 3D building point cloud, including wall information and demonstrates superior performance compared to existing baseline techniques. To allow further comparisons with our methodology the dataset has been made available at <a target="_blank" rel="noopener" href="https://github.com/Soulaimene/AIM2PCDataset">https://github.com/Soulaimene/AIM2PCDataset</a> </p>
<blockquote>
<p>åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œä»å•è§†è§’å›¾åƒé‡å»ºå»ºç­‘ç‰©çš„ä¸‰ç»´åŸå¸‚æ¨¡å‹å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»èˆªç©ºå›¾åƒä¸­é‡å»ºå±‹é¡¶ï¼Œç»å¸¸å¿½ç•¥é‡è¦çš„å‡ ä½•ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œç¼ºä¹åŒ…å«æ•´ä¸ªå»ºç­‘ç‰©çš„å®Œæ•´ä¸‰ç»´ç‚¹äº‘çš„æ•°æ®é›†ï¼Œä»¥åŠè·å–å¯é çš„èˆªç©ºå›¾åƒç›¸æœºå§¿æ€ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•AIM2PCæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å®Œæ•´çš„ä¸‰ç»´ç‚¹äº‘å’Œç¡®å®šçš„ç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å•å¼ èˆªç©ºå›¾åƒçš„ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå°†å®ƒä»¬ä¸äºŒè¿›åˆ¶æ©ç å’ŒSobelè¾¹ç¼˜å›¾ç­‰å…³é”®é™„åŠ æ¡ä»¶ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´è¾¹ç¼˜æ„ŸçŸ¥çš„é‡å»ºã€‚é€šè¿‡ç»“åˆåŸºäºä¸­å¿ƒé™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆCDPMï¼‰çš„ç‚¹äº‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡æ‰©æ•£æ­¥éª¤ä¸­éƒ½ä½¿ç”¨ç›¸æœºå§¿æ€å°†è¿™äº›ç»„åˆç‰¹å¾æŠ•å½±åˆ°éƒ¨åˆ†å»å™ªçš„ç‚¹äº‘ä¸Šã€‚æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿé‡å»ºåŒ…æ‹¬å¢™ä½“ä¿¡æ¯åœ¨å†…çš„å®Œæ•´ä¸‰ç»´å»ºç­‘ç‰©ç‚¹äº‘ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šå±•ç°å‡ºç›¸è¾ƒäºç°æœ‰åŸºçº¿æŠ€æœ¯çš„ä¼˜è¶Šæ€§ã€‚ä¸ºäº†ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Soulaimene/AIM2PCDataset%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Soulaimene/AIM2PCDatasetä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18527v2">PDF</a> Accepted to ISPRS Geospatial Week 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰ç»´å»ºç­‘é‡å»ºæ–¹æ³•AIM2PCï¼Œèƒ½å¤Ÿä»å•è§†è§’çš„èˆªç©ºå›¾åƒä¸­é‡å»ºå‡ºå®Œæ•´çš„ä¸‰ç»´å»ºç­‘ç‚¹äº‘ï¼ŒåŒ…æ‹¬å¢™ä½“ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆçš„æ•°æ®é›†å’Œç¡®å®šçš„ç›¸æœºå§¿æ€ï¼Œç»“åˆç‚¹äº‘æ‰©æ•£æ¨¡å‹ï¼Œå°†ç‰¹å¾æŠ•å½±åˆ°éƒ¨åˆ†å»å™ªçš„ç‚¹äº‘ä¸Šï¼Œå®ç°äº†ä¼˜äºç°æœ‰åŸºçº¿æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´åŸå¸‚é‡å»ºä»å•è§†è§’å›¾åƒä¸­é‡å»ºå»ºç­‘å¸å¼•äº†å¤§é‡å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å±‹é¡¶ï¼Œå¿½ç•¥äº†é‡è¦å‡ ä½•ç»†èŠ‚ã€‚</li>
<li>ç¼ºä¹åŒ…å«å®Œæ•´ä¸‰ç»´ç‚¹äº‘çš„æ•°æ®é›†ä»¥åŠè·å–å¯é ç›¸æœºå§¿æ€ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•AIM2PCï¼Œåˆ©ç”¨ç”Ÿæˆçš„æ•°æ®é›†å’Œç¡®å®šçš„ç›¸æœºå§¿æ€ï¼Œèƒ½å¤Ÿé‡å»ºå®Œæ•´çš„ä¸‰ç»´å»ºç­‘ç‚¹äº‘ï¼ŒåŒ…æ‹¬å¢™ä½“ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å•è§†è§’èˆªç©ºå›¾åƒçš„ç‰¹å¾ï¼Œä½¿ç”¨äºŒè¿›åˆ¶æ©ç å’ŒSobelè¾¹ç¼˜æ˜ å°„ç­‰é™„åŠ æ¡ä»¶ï¼Œå®ç°äº†æ›´è¾¹ç¼˜æ„ŸçŸ¥çš„é‡å»ºã€‚</li>
<li>åˆ©ç”¨åŸºäºä¸­å¿ƒå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆCDPMï¼‰çš„ç‚¹äº‘æ‰©æ•£æ¨¡å‹ï¼Œå°†ç‰¹å¾æŠ•å½±åˆ°éƒ¨åˆ†å»å™ªçš„ç‚¹äº‘ä¸Šã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºäº†ä¼˜äºç°æœ‰åŸºçº¿æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0bb6bfd9aa68c24b1d33476523865cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6467e9d8bee841ad19088eb2e6ed483c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef456618b4db133a1c1a907cb43cc17d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c77ffece70db967f1a72f69303cb334.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation"><a href="#Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation" class="headerlink" title="Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation"></a>Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation</h2><p><strong>Authors:Byung Hyun Lee, Sungjin Lim, Se Young Chun</strong></p>
<p>Fine-tuning based concept erasing has demonstrated promising results in preventing generation of harmful contents from text-to-image diffusion models by removing target concepts while preserving remaining concepts. To maintain the generation capability of diffusion models after concept erasure, it is necessary to remove only the image region containing the target concept when it locally appears in an image, leaving other regions intact. However, prior arts often compromise fidelity of the other image regions in order to erase the localized target concept appearing in a specific area, thereby reducing the overall performance of image generation. To address these limitations, we first introduce a framework called localized concept erasure, which allows for the deletion of only the specific area containing the target concept in the image while preserving the other regions. As a solution for the localized concept erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation for Concept Erasure (GLoCE), that injects a lightweight module into the diffusion model. GLoCE consists of low-rank matrices and a simple gate, determined only by several generation steps for concepts without training. By directly applying GLoCE to image embeddings and designing the gate to activate only for target concepts, GLoCE can selectively remove only the region of the target concepts, even when target and remaining concepts coexist within an image. Extensive experiments demonstrated GLoCE not only improves the image fidelity to text prompts after erasing the localized target concepts, but also outperforms prior arts in efficacy, specificity, and robustness by large margin and can be extended to mass concept erasure. </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µæ“¦é™¤çš„å¾®è°ƒå·²åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æ˜¾ç¤ºå‡ºä»¤äººç©ç›®çš„ç»“æœï¼Œå®ƒé€šè¿‡æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µåŒæ—¶ä¿ç•™å‰©ä½™æ¦‚å¿µï¼Œé˜²æ­¢ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ä¸ºäº†ä¿æŒæ‰©æ•£æ¨¡å‹åœ¨æ¦‚å¿µæ“¦é™¤åçš„ç”Ÿæˆèƒ½åŠ›ï¼Œæœ‰å¿…è¦ä»…åœ¨å›¾åƒä¸­å‡ºç°ç›®æ ‡æ¦‚å¿µçš„åŒºåŸŸä¸­è¿›è¡Œæ“¦é™¤ï¼Œè€Œå…¶ä»–åŒºåŸŸä¿æŒä¸å˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æŠ€æœ¯ç»å¸¸åœ¨ä¸ºäº†æ“¦é™¤ç‰¹å®šåŒºåŸŸä¸­å‡ºç°çš„å±€éƒ¨ç›®æ ‡æ¦‚å¿µæ—¶ï¼Œç‰ºç‰²äº†å…¶ä»–å›¾åƒåŒºåŸŸçš„ä¿çœŸåº¦ï¼Œä»è€Œé™ä½äº†å›¾åƒç”Ÿæˆçš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªåä¸ºå±€éƒ¨æ¦‚å¿µæ“¦é™¤çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ä»…åˆ é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–åŒºåŸŸã€‚ä½œä¸ºå±€éƒ¨æ¦‚å¿µæ“¦é™¤çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºç”¨äºæ¦‚å¿µæ“¦é™¤çš„å¸¦é—¨æ§çš„ä½ç§©è‡ªé€‚åº”æ–¹æ³•ï¼ˆGLoCEï¼‰ï¼Œå®ƒå°†ä¸€ä¸ªè½»é‡çº§æ¨¡å—æ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚GLoCEç”±ä½ç§©çŸ©é˜µå’Œä»…ç”±å‡ ä¸ªç”Ÿæˆæ­¥éª¤ç¡®å®šçš„ç®€å•é—¨ç»„æˆï¼Œç”¨äºæ¦‚å¿µå¤„ç†è€Œæ— éœ€è®­ç»ƒã€‚é€šè¿‡å°†GLoCEç›´æ¥åº”ç”¨äºå›¾åƒåµŒå…¥å¹¶è®¾è®¡ä»…åœ¨ç›®æ ‡æ¦‚å¿µä¸Šæ¿€æ´»çš„é—¨ï¼ŒGLoCEå¯ä»¥é€‰æ‹©æ€§åœ°ä»…åˆ é™¤ç›®æ ‡æ¦‚å¿µçš„åŒºåŸŸï¼Œå³ä½¿ç›®æ ‡æ¦‚å¿µå’Œå‰©ä½™æ¦‚å¿µå…±å­˜äºå›¾åƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGLoCEä¸ä»…åœ¨æ“¦é™¤å±€éƒ¨ç›®æ ‡æ¦‚å¿µåå¯¹æ–‡æœ¬æç¤ºçš„å›¾åƒä¿çœŸåº¦æœ‰æ‰€æé«˜ï¼Œè€Œä¸”åœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¹Ÿå¤§å¤§è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡çš„æ¦‚å¿µæ“¦é™¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12356v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç²¾ç»†è°ƒæ•´çš„æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯å·²åœ¨é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ä¸Šæ˜¾ç¤ºå‡ºå‰æ™¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µåŒæ—¶ä¿ç•™å‰©ä½™æ¦‚å¿µæ¥ç»´æŠ¤æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†åœ¨å›¾åƒä¸­å±€éƒ¨å»é™¤ç›®æ ‡æ¦‚å¿µçš„åŒæ—¶ä¿æŒå…¶ä»–åŒºåŸŸçš„å®Œæ•´æ€§ï¼Œåªéœ€ç§»é™¤åŒ…å«ç›®æ ‡æ¦‚å¿µçš„å›¾åƒåŒºåŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯å¸¸å¸¸ä¸ºäº†æ¶ˆé™¤ç‰¹å®šåŒºåŸŸçš„ç›®æ ‡æ¦‚å¿µè€ŒæŸå®³å…¶ä»–å›¾åƒåŒºåŸŸçš„ä¿çœŸåº¦ï¼Œä»è€Œé™ä½å›¾åƒç”Ÿæˆçš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ä¸ªåä¸ºå±€éƒ¨æ¦‚å¿µæ¶ˆé™¤çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ä»…åˆ é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¶ä»–åŒºåŸŸã€‚ä½œä¸ºå±€éƒ¨æ¦‚å¿µæ¶ˆé™¤çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºç”¨äºæ¦‚å¿µæ¶ˆé™¤çš„å—æ§ä½é˜¶é€‚åº”ï¼ˆGLoCEï¼‰ï¼Œå®ƒå°†ä¸€ä¸ªè½»é‡çº§æ¨¡å—æ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚GLoCEç”±ä½é˜¶çŸ©é˜µå’Œä»…ç”±å‡ ä¸ªç”Ÿæˆæ­¥éª¤ç¡®å®šçš„ç®€å•é—¨ç»„æˆï¼Œç”¨äºæ¦‚å¿µå¤„ç†ã€‚é€šè¿‡å°†GLoCEç›´æ¥åº”ç”¨äºå›¾åƒåµŒå…¥å¹¶è®¾è®¡ä»…é’ˆå¯¹ç›®æ ‡æ¦‚å¿µæ¿€æ´»çš„é—¨ï¼ŒGLoCEå¯ä»¥é€‰æ‹©æ€§åœ°å»é™¤äº†ç›®æ ‡æ¦‚å¿µçš„åŒºåŸŸï¼Œå³ä½¿ç›®æ ‡æ¦‚å¿µå’Œå‰©ä½™æ¦‚å¿µå…±å­˜äºå›¾åƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGLoCEåœ¨æé«˜åˆ é™¤å±€éƒ¨ç›®æ ‡æ¦‚å¿µåçš„å›¾åƒå¯¹æ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦æ–¹é¢æœ‰æ‰€æ”¹å–„ï¼Œå¹¶ä¸”åœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¯æ‰©å±•åˆ°å¤§è§„æ¨¡çš„æ¦‚å¿µæ¶ˆé™¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç»†è°ƒæ¦‚å¿µæ¶ˆé™¤å¯é˜²æ­¢æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œé€šè¿‡ç§»é™¤ç›®æ ‡æ¦‚å¿µåŒæ—¶ä¿ç•™å…¶ä»–æ¦‚å¿µæ¥å®ç°ã€‚</li>
<li>å±€éƒ¨æ¦‚å¿µæ¶ˆé™¤æ¡†æ¶å…è®¸ä»…åˆ é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„ç‰¹å®šåŒºåŸŸï¼Œä¿æŒå…¶ä»–åŒºåŸŸä¸å˜ã€‚</li>
<li>GLoCEæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡æ³¨å…¥è½»é‡çº§æ¨¡å—åˆ°æ‰©æ•£æ¨¡å‹ä¸­å®ç°å±€éƒ¨æ¦‚å¿µæ¶ˆé™¤ã€‚</li>
<li>GLoCEç”±ä½é˜¶çŸ©é˜µå’Œç®€å•é—¨æ„æˆï¼Œå¯é€‰æ‹©æ€§å»é™¤ç›®æ ‡æ¦‚å¿µåŒºåŸŸï¼Œå³ä½¿ç›®æ ‡æ¦‚å¿µå’Œå‰©ä½™æ¦‚å¿µå…±å­˜äºå›¾åƒä¸­ã€‚</li>
<li>GLoCEåœ¨åˆ é™¤å±€éƒ¨ç›®æ ‡æ¦‚å¿µåæé«˜äº†å›¾åƒå¯¹æ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦ã€‚</li>
<li>GLoCEåœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>GLoCEå¯æ‰©å±•åˆ°å¤§è§„æ¨¡çš„æ¦‚å¿µæ¶ˆé™¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-896e78cd1f3007a95978e0ec0d4b44b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d196095f55585ddcb9d61d6de16975fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141712c7a0ae182942897243edffcd0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2eb732eaccbcf8fb7462bad3d1c651c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f13502c0508bf83613907869935ad38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89013911ec3360982e50460ab4a59391.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RayFlow-Instance-Aware-Diffusion-Acceleration-via-Adaptive-Flow-Trajectories"><a href="#RayFlow-Instance-Aware-Diffusion-Acceleration-via-Adaptive-Flow-Trajectories" class="headerlink" title="RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow   Trajectories"></a>RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow   Trajectories</h2><p><strong>Authors:Huiyang Shao, Xin Xia, Yuhong Yang, Yuxi Ren, Xing Wang, Xuefeng Xiao</strong></p>
<p>Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlowâ€™s superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå…¶ç¼“æ…¢çš„ç”Ÿæˆé€Ÿåº¦ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•è™½ç„¶æ—¨åœ¨å‡å°‘æ­¥éª¤ï¼Œä½†å¾€å¾€ä¼šç‰ºç‰²æ ·æœ¬è´¨é‡ã€å¯æ§æ€§æˆ–å¢åŠ è®­ç»ƒå¤æ‚æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RayFlowï¼Œä¸€ç§è§£å†³è¿™äº›é™åˆ¶çš„æ–°å‹æ‰©æ•£æ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ³•ï¼ŒRayFlowå¼•å¯¼æ¯ä¸ªæ ·æœ¬æ²¿ç€ä¸€æ¡ç‹¬ç‰¹çš„è·¯å¾„å‘ç‰¹å®šå®ä¾‹çš„åˆ†å¸ƒå‰è¿›ã€‚è¿™ç§æ–¹æ³•åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ï¼Œä¿æŒäº†ç”Ÿæˆçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´é‡‡æ ·å™¨ï¼ˆTime Samplerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡è¦æ€§é‡‡æ ·æŠ€æœ¯ï¼Œé€šè¿‡å…³æ³¨å…³é”®çš„æ—¶é—´æ­¥é•¿æ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„åŠ é€ŸæŠ€æœ¯ç›¸æ¯”ï¼ŒRayFlowåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ï¼ŒåŒæ—¶æé«˜äº†é€Ÿåº¦ã€å¯æ§æ€§å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07699v2">PDF</a> 23 pages, 5 figures, CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å„é¢†åŸŸå–å¾—çš„æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶ç¼“æ…¢çš„ç”Ÿæˆé€Ÿåº¦ä»æ˜¯å…³é”®é—®é¢˜ã€‚ç°æœ‰åŠ é€Ÿæ–¹æ³•è™½æ—¨åœ¨å‡å°‘æ­¥éª¤ï¼Œä½†å¾€å¾€ç‰ºç‰²äº†æ ·æœ¬è´¨é‡ã€å¯æ§æ€§æˆ–å¼•å…¥äº†è®­ç»ƒå¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RayFlowè¿™ä¸€æ–°å‹æ‰©æ•£æ¡†æ¶ï¼Œè§£å†³äº†è¿™äº›å±€é™ã€‚RayFlowä¸åŒäºç°æœ‰æ–¹æ³•ï¼Œå®ƒå¼•å¯¼æ¯ä¸ªæ ·æœ¬æ²¿ç€ç‹¬ç‰¹çš„è·¯å¾„å‘ç‰¹å®šå®ä¾‹åˆ†å¸ƒã€‚æ­¤æ–¹æ³•åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ï¼Œä¿æŒäº†ç”Ÿæˆçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†æ—¶é—´é‡‡æ ·å™¨ï¼ˆTime Samplerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡è¦æ€§é‡‡æ ·æŠ€æœ¯ï¼Œé€šè¿‡ä¸“æ³¨äºå…³é”®çš„æ—¶é—´æ­¥éª¤æ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒRayFlowåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ç°æœ‰åŠ é€ŸæŠ€æœ¯ç›¸æ¯”ï¼Œå…¶é€Ÿåº¦ã€æ§åˆ¶å’Œè®­ç»ƒæ•ˆç‡éƒ½æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åœ¨å„é¢†åŸŸå–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†ç”Ÿæˆé€Ÿåº¦æ…¢ä»æ˜¯å…³é”®é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŠ é€Ÿæ–¹æ³•å¾€å¾€ç‰ºç‰²äº†æ ·æœ¬è´¨é‡ã€å¯æ§æ€§æˆ–å¼•å…¥è®­ç»ƒå¤æ‚æ€§ã€‚</li>
<li>RayFlowæ˜¯ä¸€ç§æ–°å‹æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¼•å¯¼æ¯ä¸ªæ ·æœ¬æ²¿ç€ç‹¬ç‰¹è·¯å¾„å‘ç‰¹å®šå®ä¾‹åˆ†å¸ƒï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>RayFlowå¼•å…¥äº†æ—¶é—´é‡‡æ ·å™¨ï¼ˆTime Samplerï¼‰ï¼Œä¸€ç§é‡è¦æ€§é‡‡æ ·æŠ€æœ¯ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>RayFlowåœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰åŠ é€ŸæŠ€æœ¯ç›¸æ¯”ï¼ŒRayFlowåœ¨é€Ÿåº¦ã€æ§åˆ¶å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢éƒ½æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d63f38595ae5a81d99693c5d055bf31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4348cf2e99c28e953f93fdbb140af5e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0307334bbb89c694c16e6bd5a08a711.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e838055721dd46038c82610d74a9bbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfd435235fd213c54dde0a4f6d65b4f8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GCC-Generative-Color-Constancy-via-Diffusing-a-Color-Checker"><a href="#GCC-Generative-Color-Constancy-via-Diffusing-a-Color-Checker" class="headerlink" title="GCC: Generative Color Constancy via Diffusing a Color Checker"></a>GCC: Generative Color Constancy via Diffusing a Color Checker</h2><p><strong>Authors:Chen-Wei Chang, Cheng-De Fan, Chia-Che Chang, Yi-Chen Lo, Yu-Chee Tseng, Jiun-Long Huang, Yu-Lun Liu</strong></p>
<p>Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. By harnessing rich priors from pre-trained diffusion models, GCC demonstrates strong robustness in challenging cross-camera scenarios. These results highlight our methodâ€™s effective generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile and practical solution for real-world applications. </p>
<blockquote>
<p>é¢œè‰²æ’å¸¸æ€§æ–¹æ³•ç”±äºä¸åŒçš„å…‰è°±æ•æ„Ÿæ€§ï¼Œé€šå¸¸éš¾ä»¥åœ¨ä¸åŒç›¸æœºä¼ æ„Ÿå™¨ä¸Šè¿›è¡Œæ¨å¹¿ã€‚æˆ‘ä»¬æå‡ºäº†GCCï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†é¢œè‰²æ¡å¡«å……åˆ°å›¾åƒä¸­è¿›è¡Œç…§æ˜ä¼°è®¡ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å•æ­¥ç¡®å®šæ€§æ¨ç†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¡«å……åæ˜ åœºæ™¯ç…§æ˜çš„é¢œè‰²æ¡ï¼›ï¼ˆ2ï¼‰æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥ä¿ç•™é¢œè‰²æ¡çš„ç»“æ„ï¼ŒåŒæ—¶å…è®¸ä¸ç…§æ˜ç›¸å…³çš„é¢œè‰²é€‚åº”ï¼›ï¼ˆ3ï¼‰åŸºäºæ©ç çš„å¢å¼ºç­–ç•¥ï¼Œç”¨äºå¤„ç†ä¸ç²¾ç¡®çš„é¢œè‰²æ¡æ³¨é‡Šã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ï¼ŒGCCåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨ç›¸æœºåœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒç›¸æœºç‰¹æ€§ä¹‹é—´çš„æœ‰æ•ˆæ¨å¹¿èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ä¼ æ„Ÿå™¨è¿›è¡Œç‰¹å®šè®­ç»ƒï¼Œä½¿å…¶æˆä¸ºçœŸå®ä¸–ç•Œåº”ç”¨çš„é€šç”¨å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17435v2">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://chenwei891213.github.io/GCC/">https://chenwei891213.github.io/GCC/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒä¸­çš„è‰²å½©æ ¡æ­£å™¨è¿›è¡Œä¿®å¤çš„æ–¹æ³•ï¼Œä»¥æé«˜é¢œè‰²ä¸€è‡´æ€§ã€‚é€šè¿‡é‡‡ç”¨å•æ­¥ç¡®å®šæ€§æ¨æ–­ã€æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯å’ŒåŸºäºæ©è†œçš„å¢å¹¿ç­–ç•¥ï¼Œè¯¥æ–¹æ³•åœ¨è·¨ä¸åŒç›¸æœºä¼ æ„Ÿå™¨æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚å…¶ä¼˜åŠ¿åœ¨äºæ— éœ€é’ˆå¯¹ç‰¹å®šä¼ æ„Ÿå™¨è¿›è¡Œè®­ç»ƒï¼Œå³å¯æœ‰æ•ˆé€‚åº”ä¸åŒç›¸æœºç‰¹æ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†é€šç”¨ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¿®å¤å›¾åƒä¸­çš„è‰²å½©æ ¡æ­£å™¨ä»¥æé«˜é¢œè‰²ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨å•æ­¥ç¡®å®šæ€§æ¨æ–­æ–¹æ³•ä¿®å¤è‰²å½©æ ¡æ­£å™¨ï¼Œåæ˜ åœºæ™¯ç…§æ˜ã€‚</li>
<li>ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†è§£æŠ€æœ¯ï¼Œåœ¨ä¿æŒæ ¡æ­£å™¨ç»“æ„çš„åŒæ—¶å®ç°ä¸ç…§æ˜ç›¸å…³çš„é¢œè‰²è‡ªé€‚åº”ã€‚</li>
<li>æå‡ºåŸºäºæ©è†œçš„æ•°æ®å¢å¹¿ç­–ç•¥ï¼Œä»¥å¤„ç†ä¸ç²¾ç¡®çš„è‰²å½©æ ¡æ­£å™¨æ³¨é‡Šã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒä¿¡æ¯ï¼Œåœ¨è·¨ç›¸æœºåœºæ™¯ä¸­å®ç°å¼ºå¤§çš„é²æ£’æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§ç›¸æœºç‰¹æ€§é—´è¡¨ç°å‡ºæœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¼ æ„Ÿå™¨è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90b243ba9ad81c9c85d19b42f3380005.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d94663367a903f591518e53bb06ea874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebe1e07b7dab828ecf8041281256cbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f157a21f2b5247b7a34c45bb83e0b7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGenâ€™s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>. </p>
<blockquote>
<p>è¿‘æœŸé‡å»ºå’Œç”Ÿæˆæ¨¡å‹æ–¹é¢çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºä¸€ä¸ªå°åŒºåŸŸï¼Œä½¿å¾—å¤§èŒƒå›´ä¸€è‡´çš„åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼è¿›è¡Œå¤§èŒƒå›´åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”Ÿæˆéƒ½æ˜¯ä»¥ç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘åœ¨æ—¶é—´ä¸Šé‡å çš„å›¾åƒä¸ºæ¡ä»¶ï¼Œæé«˜äº†å¤§èŒƒå›´åœºæ™¯ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿åŠ¿æ§åˆ¶ã€‚æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œå¯ä»¥ä¿ƒè¿›åŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆç­‰åœ¨å†…çš„å„ç§ä»»åŠ¡ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿åŠ¿å‡†ç¡®æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen%E3%80%82">https://zju3dv.github.io/StarGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨ç†éƒ½å±€é™äºå°èŒƒå›´ï¼Œä½¿å¾—é•¿è·ç¦»ä¸€è‡´åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenæ¡†æ¶ï¼Œå®ƒé‡‡ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼è¿›è¡Œé•¿è·ç¦»åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”Ÿæˆéƒ½åŸºäºç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­æ—¶é—´ä¸Šé‡å çš„å›¾åƒï¼Œæé«˜äº†é•¿è·ç¦»åœºæ™¯ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿æ€æ§åˆ¶ã€‚è¿™ç§æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œå¯ä¿ƒè¿›åŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆåœ¨å†…çš„å„ç§ä»»åŠ¡ã€‚è¯„ä¼°å’Œå®éªŒè¯æ˜ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„å‘å±•ã€‚</li>
<li>ç”±äºè®¡ç®—é™åˆ¶ï¼Œå¤§å‹æ¨¡å‹åœ¨åœºæ™¯ç”Ÿæˆæ–¹é¢å­˜åœ¨é•¿è·ç¦»ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>StarGenæ¡†æ¶é€šè¿‡é‡‡ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>StarGenåˆ©ç”¨ç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­çš„æ—¶é—´é‡å å›¾åƒè¿›è¡Œç”Ÿæˆï¼Œæé«˜æ—¶ç©ºä¸€è‡´æ€§å’Œå§¿æ€ç²¾åº¦ã€‚</li>
<li>StarGenæ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆã€‚</li>
<li>StarGenç›¸æ¯”ç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-519f1e8b0a719f84a181f791be21913e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb63b2c7304a947938d974ae2b0ba9da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-327807363bdbe61b89e53f7d763be010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a06c79e0bda0eaafa3decfa32243b901.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0e4d62530a026b201a3a8adfeccfe9b0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  The cross-correlation between soft X-rays and galaxies A new benchmark   for galaxy evolution models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2ddd0e44dbb57c1d995323af6c0e8f90.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  MultimodalStudio A Heterogeneous Sensor Dataset and Framework for   Neural Rendering across Multiple Imaging Modalities
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
