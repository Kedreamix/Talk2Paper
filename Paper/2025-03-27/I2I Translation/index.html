<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models"><a href="#Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models" class="headerlink" title="Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models"></a>Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models</h2><p><strong>Authors:Ruixi You, Hecheng Jia, Feng Xu</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and high-resolution imaging capabilities but its unique imaging mechanism makes interpretation heavily reliant on expert knowledge, limiting interpretability, especially in complex target tasks. Translating SAR images into optical images is a promising solution to enhance interpretation and support downstream tasks. Most existing research focuses on scene-level translation, with limited work on object-level translation due to the scarcity of paired data and the challenge of accurately preserving contour and texture details. To address these issues, this study proposes a keypoint-guided diffusion model (KeypointDiff) for SAR-to-optical image translation of unpaired aircraft targets. This framework introduces supervision on target class and azimuth angle via keypoints, along with a training strategy for unpaired data. Based on the classifier-free guidance diffusion architecture, a class-angle guidance module (CAGM) is designed to integrate class and angle information into the diffusion generation process. Furthermore, adversarial loss and consistency loss are employed to improve image fidelity and detail quality, tailored for aircraft targets. During sampling, aided by a pre-trained keypoint detector, the model eliminates the requirement for manually labeled class and azimuth information, enabling automated SAR-to-optical translation. Experimental results demonstrate that the proposed method outperforms existing approaches across multiple metrics, providing an efficient and effective solution for object-level SAR-to-optical translation and downstream tasks. Moreover, the method exhibits strong zero-shot generalization to untrained aircraft types with the assistance of the keypoint detector. </p>
<blockquote>
<p>é›·è¾¾åˆæˆå­”å¾„ï¼ˆSARï¼‰å›¾åƒæä¾›äº†å…¨å¤©å€™ã€å…¨å¤©æ—¶å’Œé«˜åˆ†è¾¨ç‡çš„æˆåƒèƒ½åŠ›ï¼Œä½†å…¶ç‹¬ç‰¹çš„æˆåƒæœºåˆ¶ä½¿å¾—è§£é‡Šå·¥ä½œä¸¥é‡ä¾èµ–äºä¸“ä¸šçŸ¥è¯†ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç›®æ ‡ä»»åŠ¡ä¸­ã€‚å°†SARå›¾åƒè½¬æ¢ä¸ºå…‰å­¦å›¾åƒæ˜¯ä¸€ä¸ªæé«˜è§£é‡Šèƒ½åŠ›å¹¶æ”¯æŒä¸‹æ¸¸ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰çš„ç ”ç©¶å¤§å¤šé›†ä¸­åœ¨åœºæ™¯çº§çš„ç¿»è¯‘ä¸Šï¼Œè€Œé’ˆå¯¹ç›®æ ‡çº§çš„ç¿»è¯‘å·¥ä½œåˆ™å› é…å¯¹æ•°æ®çš„ç¨€ç¼ºä»¥åŠå‡†ç¡®ä¿ç•™è½®å»“å’Œçº¹ç†ç»†èŠ‚çš„æŒ‘æˆ˜è€Œå—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºéé…å¯¹é£æœºç›®æ ‡SARåˆ°å…‰å­¦å›¾åƒè½¬æ¢çš„å…³é”®ç‚¹å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å…³é”®ç‚¹å¼•å…¥äº†å¯¹ç›®æ ‡ç±»åˆ«å’Œæ–¹ä½è§’çš„ç›‘ç£ï¼Œå¹¶é‡‡ç”¨äº†é’ˆå¯¹éé…å¯¹æ•°æ®çš„è®­ç»ƒç­–ç•¥ã€‚åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£æ¶æ„ï¼Œè®¾è®¡äº†ä¸€ä¸ªç±»è§’å¼•å¯¼æ¨¡å—ï¼ˆCAGMï¼‰ï¼Œå°†ç±»åˆ«å’Œè§’åº¦ä¿¡æ¯é›†æˆåˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†å¯¹æŠ—æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±æ¥æé«˜å›¾åƒä¿çœŸåº¦å’Œç»†èŠ‚è´¨é‡ï¼Œä»¥é€‚åº”é£æœºç›®æ ‡ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œè¯¥æ¨¡å‹æ— éœ€æ‰‹åŠ¨æ ‡æ³¨ç±»åˆ«å’Œæ–¹ä½ä¿¡æ¯ï¼Œå³å¯å®ç°è‡ªåŠ¨åŒ–çš„SARåˆ°å…‰å­¦è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºé¢å‘å¯¹è±¡çš„SARåˆ°å…‰å­¦è½¬æ¢åŠä¸‹æ¸¸ä»»åŠ¡æä¾›äº†é«˜æ•ˆæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå€ŸåŠ©å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œè¯¥æ–¹æ³•å¯¹æœªè®­ç»ƒçš„é£æœºç±»å‹å…·æœ‰è¾ƒå¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå…³é”®ç‚¹å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰ï¼Œç”¨äºSARåˆ°å…‰å­¦å›¾åƒçš„é…å¯¹é£æœºç›®æ ‡ç¿»è¯‘ã€‚è¯¥ç ”ç©¶è§£å†³äº†ç”±äºæ•°æ®é…å¯¹ç¨€ç¼ºå’Œè½®å»“å’Œçº¹ç†ç»†èŠ‚å‡†ç¡®ä¿ç•™çš„æŒ‘æˆ˜æ‰€å¸¦æ¥çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç›®æ ‡ç±»åˆ«å’Œæ–¹ä½è§’çš„å…³é”®ç‚¹ç›‘ç£ï¼Œç»“åˆæœªé…å¯¹æ•°æ®çš„è®­ç»ƒç­–ç•¥ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ— éœ€åˆ†ç±»å™¨å¼•å¯¼çš„åˆ†ç±»å™¨å¼•å¯¼æ¨¡å—ï¼ˆCAGMï¼‰ï¼Œå°†ç±»åˆ«å’Œè§’åº¦ä¿¡æ¯é›†æˆåˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºSARåˆ°å…‰å­¦å›¾åƒçš„ç‰©ä½“çº§åˆ«ç¿»è¯‘å’Œä¸‹æ¸¸ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œè¯¥æ–¹æ³•å¯¹æœªè®­ç»ƒçš„é£æœºç±»å‹å…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SARæˆåƒå…·æœ‰å…¨å¤©å€™ã€å…¨å¤©æ—¶ã€é«˜åˆ†è¾¨ç‡çš„æˆåƒèƒ½åŠ›ï¼Œä½†è§£è¯»ä¾èµ–äºä¸“ä¸šçŸ¥è¯†ï¼Œå°¤å…¶åœ¨å¤æ‚ç›®æ ‡ä»»åŠ¡ä¸­ã€‚</li>
<li>å°†SARå›¾åƒè½¬åŒ–ä¸ºå…‰å­¦å›¾åƒæ˜¯æé«˜è§£è¯»èƒ½åŠ›çš„ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åœºæ™¯çº§åˆ«çš„ç¿»è¯‘ï¼Œå¯¹äºç‰©ä½“çº§åˆ«çš„ç¿»è¯‘ç”±äºé…å¯¹æ•°æ®çš„ç¨€ç¼ºæ€§å’Œå‡†ç¡®ä¿ç•™è½®å»“å’Œçº¹ç†ç»†èŠ‚çš„æŒ‘æˆ˜è€Œç ”ç©¶æœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå…³é”®ç‚¹å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆKeypointDiffï¼‰æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå°¤å…¶é’ˆå¯¹é£æœºç›®æ ‡çš„SARåˆ°å…‰å­¦å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ç›®æ ‡ç±»åˆ«å’Œæ–¹ä½è§’çš„å…³é”®ç‚¹ç›‘ç£ï¼Œç»“åˆæœªé…å¯¹æ•°æ®çš„è®­ç»ƒç­–ç•¥ï¼Œæé«˜äº†å›¾åƒç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ad34ff18565dbad742fc0c13da1bd9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca778238e964609473c2b1e38f00e6cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9bc955b0864fb2ea0800c9f0986e80c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8567d568dec08834d5dcfcb89b40fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7776725ee57eaf5ae1fd8a22f4cab904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9e32d70831f58caa3361855117ea1e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiffV2IR-Visible-to-Infrared-Diffusion-Model-via-Vision-Language-Understanding"><a href="#DiffV2IR-Visible-to-Infrared-Diffusion-Model-via-Vision-Language-Understanding" class="headerlink" title="DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language   Understanding"></a>DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language   Understanding</h2><p><strong>Authors:Lingyan Ran, Lidong Wang, Guangcong Wang, Peng Wang, Yanning Zhang</strong></p>
<p>The task of translating visible-to-infrared images (V2IR) is inherently challenging due to three main obstacles: 1) achieving semantic-aware translation, 2) managing the diverse wavelength spectrum in infrared imagery, and 3) the scarcity of comprehensive infrared datasets. Current leading methods tend to treat V2IR as a conventional image-to-image synthesis challenge, often overlooking these specific issues. To address this, we introduce DiffV2IR, a novel framework for image translation comprising two key elements: a Progressive Learning Module (PLM) and a Vision-Language Understanding Module (VLUM). PLM features an adaptive diffusion model architecture that leverages multi-stage knowledge learning to infrared transition from full-range to target wavelength. To improve V2IR translation, VLUM incorporates unified Vision-Language Understanding. We also collected a large infrared dataset, IR-500K, which includes 500,000 infrared images compiled by various scenes and objects under various environmental conditions. Through the combination of PLM, VLUM, and the extensive IR-500K dataset, DiffV2IR markedly improves the performance of V2IR. Experiments validate DiffV2IRâ€™s excellence in producing high-quality translations, establishing its efficacy and broad applicability. The code, dataset, and DiffV2IR model will be available at <a target="_blank" rel="noopener" href="https://github.com/LidongWang-26/DiffV2IR">https://github.com/LidongWang-26/DiffV2IR</a>. </p>
<blockquote>
<p>å°†å¯è§å…‰åˆ°çº¢å¤–å›¾åƒï¼ˆV2IRï¼‰çš„ç¿»è¯‘ä»»åŠ¡å…·æœ‰å†…åœ¨çš„æŒ‘æˆ˜æ€§ï¼Œä¸»è¦å› ä¸ºä»¥ä¸‹ä¸‰ä¸ªéšœç¢ï¼š1ï¼‰å®ç°è¯­ä¹‰æ„ŸçŸ¥ç¿»è¯‘ï¼›2ï¼‰ç®¡ç†çº¢å¤–å›¾åƒä¸­çš„å¤šæ ·åŒ–æ³¢é•¿å…‰è°±ï¼›3ï¼‰å…¨é¢çš„çº¢å¤–æ•°æ®é›†ç¨€ç¼ºã€‚å½“å‰çš„ä¸»æµæ–¹æ³•å¾€å¾€å°†V2IRè§†ä¸ºä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒåˆæˆæŒ‘æˆ˜ï¼Œç»å¸¸å¿½ç•¥è¿™äº›ç‰¹å®šé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffV2IRï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªå…³é”®å…ƒç´ ç»„æˆçš„æ–°å‹å›¾åƒç¿»è¯‘æ¡†æ¶ï¼šæ¸è¿›å­¦ä¹ æ¨¡å—ï¼ˆPLMï¼‰å’Œè§†è§‰è¯­è¨€ç†è§£æ¨¡å—ï¼ˆVLUMï¼‰ã€‚PLMé‡‡ç”¨è‡ªé€‚åº”æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨å¤šé˜¶æ®µçŸ¥è¯†å­¦ä¹ ä»å…¨æ³¢é•¿èŒƒå›´å‘ç›®æ ‡æ³¢é•¿è¿›è¡Œçº¢å¤–è½¬æ¢ã€‚ä¸ºäº†æé«˜V2IRçš„ç¿»è¯‘è´¨é‡ï¼ŒVLUMç»“åˆäº†ç»Ÿä¸€çš„è§†è§‰è¯­è¨€ç†è§£ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€ä¸ªå¤§å‹çº¢å¤–æ•°æ®é›†IR-500Kï¼Œå…¶ä¸­åŒ…æ‹¬50ä¸‡å¼ åœ¨å„ç§ç¯å¢ƒæ¡ä»¶ä¸‹ç”±ä¸åŒåœºæ™¯å’Œå¯¹è±¡ç»„æˆçš„çº¢å¤–å›¾åƒã€‚é€šè¿‡PLMã€VLUMå’Œå¹¿æ³›çš„IR-500Kæ•°æ®é›†çš„ç»“åˆï¼ŒDiffV2IRæ˜¾è‘—æé«˜äº†V2IRçš„æ€§èƒ½ã€‚å®éªŒéªŒè¯äº†DiffV2IRåœ¨äº§ç”Ÿé«˜è´¨é‡ç¿»è¯‘æ–¹é¢çš„å“è¶Šè¡¨ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚ä»£ç ã€æ•°æ®é›†å’ŒDiffV2IRæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/LidongWang-26/DiffV2IR">https://github.com/LidongWang-26/DiffV2IR</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19012v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://diffv2ir.github.io/">https://diffv2ir.github.io/</a></p>
<p><strong>Summary</strong><br>å¯è§å…‰åˆ°çº¢å¤–å›¾åƒç¿»è¯‘ä»»åŠ¡é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šå®ç°è¯­ä¹‰æ„ŸçŸ¥ç¿»è¯‘ã€ç®¡ç†çº¢å¤–æˆåƒçš„å¤šæ ·æ³¢é•¿è°±ä»¥åŠç¼ºä¹å…¨é¢çš„çº¢å¤–æ•°æ®é›†ã€‚å½“å‰ä¸»æµæ–¹æ³•å¸¸å¿½ç•¥è¿™äº›ç‰¹å®šé—®é¢˜ï¼Œå°†å…¶è§†ä¸ºå¸¸è§„å›¾åƒåˆ°å›¾åƒåˆæˆæŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºDiffV2IRæ¡†æ¶ï¼ŒåŒ…å«ä¸¤å¤§å…³é”®å…ƒç´ ï¼šæ¸è¿›å­¦ä¹ æ¨¡å—å’Œè§†è§‰è¯­è¨€ç†è§£æ¨¡å—ã€‚æ¸è¿›å­¦ä¹ æ¨¡å—é‡‡ç”¨è‡ªé€‚åº”æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨å¤šé˜¶æ®µçŸ¥è¯†å­¦ä¹ å®ç°çº¢å¤–è½¬æ¢ã€‚è§†è§‰è¯­è¨€ç†è§£æ¨¡å—åˆ™é€šè¿‡ç»Ÿä¸€è§†è§‰è¯­è¨€ç†è§£æå‡V2IRç¿»è¯‘ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†å¤§å‹çº¢å¤–æ•°æ®é›†IR-500Kï¼ŒåŒ…å«å„ç§åœºæ™¯å’Œå¯¹è±¡åœ¨ä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„50ä¸‡å¼ çº¢å¤–å›¾åƒã€‚ç»“åˆæ¸è¿›å­¦ä¹ æ¨¡å—ã€è§†è§‰è¯­è¨€ç†è§£æ¨¡å—å’Œä¸°å¯Œçš„IR-500Kæ•°æ®é›†ï¼ŒDiffV2IRæ˜¾è‘—æé«˜V2IRæ€§èƒ½ã€‚å®éªŒéªŒè¯äº†DiffV2IRäº§ç”Ÿé«˜è´¨é‡ç¿»è¯‘çš„æ•ˆæœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯è§å…‰åˆ°çº¢å¤–å›¾åƒç¿»è¯‘é¢ä¸´è¯­ä¹‰æ„ŸçŸ¥ã€æ³¢é•¿è°±ç®¡ç†å’Œæ•°æ®é›†ç¨€ç¼ºä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¸¸å¿½ç•¥è¿™äº›ç‰¹å®šæŒ‘æˆ˜ï¼Œå°†å…¶è§†ä¸ºå¸¸è§„å›¾åƒè½¬æ¢ä»»åŠ¡ã€‚</li>
<li>DiffV2IRæ¡†æ¶åŒ…å«æ¸è¿›å­¦ä¹ æ¨¡å—å’Œè§†è§‰è¯­è¨€ç†è§£æ¨¡å—ï¼Œåˆ†åˆ«é€šè¿‡è‡ªé€‚åº”æ‰©æ•£æ¨¡å‹å’Œå¤šé˜¶æ®µçŸ¥è¯†å­¦ä¹ ã€ç»Ÿä¸€è§†è§‰è¯­è¨€ç†è§£æ¥æå‡V2IRç¿»è¯‘æ•ˆæœã€‚</li>
<li>æ¨å‡ºå¤§å‹çº¢å¤–æ•°æ®é›†IR-500Kï¼Œä¸ºV2IRç ”ç©¶æä¾›ä¸°å¯Œèµ„æºã€‚</li>
<li>ç»“åˆæ¨¡å—å’Œæ•°æ®é›†ï¼ŒDiffV2IRæ˜¾è‘—æé«˜V2IRæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜DiffV2IRäº§ç”Ÿé«˜è´¨é‡ç¿»è¯‘çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0619ff56310f772665ef9fdfd2a47f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-108938161e78560c79974e95a33a36f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f78541ad40f5fbb9e732f857ea4324b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Equivariant-Image-Modeling"><a href="#Equivariant-Image-Modeling" class="headerlink" title="Equivariant Image Modeling"></a>Equivariant Image Modeling</h2><p><strong>Authors:Ruixiao Dong, Mengde Xu, Zigang Geng, Li Li, Han Hu, Shuyang Gu</strong></p>
<p>Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/drx-code/EquivariantModeling">https://github.com/drx-code/EquivariantModeling</a>. </p>
<blockquote>
<p>å½“å‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚è‡ªå›å½’å’Œæ‰©æ•£æ–¹æ³•ï¼Œå°†é«˜ç»´æ•°æ®åˆ†å¸ƒå­¦ä¹ åˆ†è§£æˆä¸€ç³»åˆ—æ›´ç®€å•çš„å­ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›å­ä»»åŠ¡çš„è”åˆä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šå‡ºç°å›ºæœ‰çš„å†²çªï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨è§£å†³è¿™äº›å†²çªæ—¶è¦ä¹ˆç‰ºç‰²æ•ˆç‡ï¼Œè¦ä¹ˆç‰ºç‰²å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç­‰å˜å›¾åƒå»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è‡ªç„¶è§†è§‰ä¿¡å·çš„å¹³ç§»ä¸å˜æ€§ï¼Œå†…åœ¨åœ°å¯¹é½å­ä»»åŠ¡ä¸­çš„ä¼˜åŒ–ç›®æ ‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ï¼ˆ1ï¼‰åˆ—å¼ä»¤ç‰ŒåŒ–ï¼Œå¢å¼ºäº†æ°´å¹³è½´ä¸Šçš„å¹³ç§»å¯¹ç§°æ€§ï¼›ï¼ˆ2ï¼‰çª—å£å› æœæ³¨æ„åŠ›ï¼Œå¼ºåˆ¶è·¨ä½ç½®çš„ä¸Šä¸‹æ–‡å…³ç³»ä¸€è‡´ã€‚åœ¨256x256åˆ†è¾¨ç‡çš„ç±»æ¡ä»¶ImageNetç”Ÿæˆä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„ARæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„è®¡ç®—èµ„æºå–å¾—äº†ç›¸å½“çš„æ€§èƒ½ã€‚ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼Œå¢å¼ºçš„ç­‰å˜æ€§å‡å°‘äº†ä»»åŠ¡é—´çš„å†²çªï¼Œæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å®ç°äº†è¶…é•¿å›¾åƒåˆæˆã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ç”Ÿæˆæ¨¡å‹ä¸­ä»»åŠ¡å¯¹é½åˆ†è§£çš„ç¬¬ä¸€æ¡†æ¶ï¼Œä¸ºæœ‰æ•ˆçš„å‚æ•°å…±äº«å’Œæ— å†²çªä¼˜åŒ–æä¾›äº†è§è§£ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/drx-code/EquivariantModeling%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/drx-code/EquivariantModelingå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18948v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­‰ä»·å›¾åƒå»ºæ¨¡æ¡†æ¶ï¼Œç”¨äºåœ¨ç”Ÿæˆæ¨¡å‹ä¸­å®ç°å­ä»»åŠ¡çš„ç›®æ ‡å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶è§†è§‰ä¿¡å·çš„å¹³ç§»ä¸å˜æ€§ï¼Œè¯¥æ¡†æ¶æé«˜äº†å­ä»»åŠ¡ä¹‹é—´çš„ä¼˜åŒ–ä¸€è‡´æ€§ï¼Œå¢å¼ºäº†å›¾åƒåˆæˆçš„æ•ˆæœå¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚è‡ªå›å½’å’Œæ‰©æ•£æ–¹æ³•ï¼‰é¢ä¸´å­ä»»åŠ¡è”åˆä¼˜åŒ–ä¸­çš„å†…åœ¨å†²çªé—®é¢˜ã€‚</li>
<li>æå‡ºçš„ç­‰ä»·å›¾åƒå»ºæ¨¡æ¡†æ¶åˆ©ç”¨è‡ªç„¶è§†è§‰ä¿¡å·çš„å¹³ç§»ä¸å˜æ€§ï¼Œå®ç°å­ä»»åŠ¡ä¼˜åŒ–ç›®æ ‡çš„å¯¹é½ã€‚</li>
<li>é€šè¿‡åˆ—å¼ç¬¦å·åŒ–å’Œçª—å£å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æ¡†æ¶æé«˜äº†ç¿»è¯‘å¯¹ç§°æ€§å’Œä¸Šä¸‹æ–‡å…³ç³»çš„ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨é«˜åˆ†è¾¨ç‡ImageNetå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸æœ€å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹ç›¸å½“ï¼Œä½†è®¡ç®—èµ„æºæ¶ˆè€—æ›´å°‘ã€‚</li>
<li>å¢å¼ºç­‰ä»·æ€§å‡å°‘äº†å­ä»»åŠ¡é—´çš„å†²çªï¼Œæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å®ç°äº†è¶…é•¿å›¾åƒåˆæˆã€‚</li>
<li>è¯¥å·¥ä½œä¸ºç”Ÿæˆæ¨¡å‹ä¸­çš„ä»»åŠ¡å¯¹é½åˆ†è§£å»ºç«‹äº†é¦–ä¸ªæ¡†æ¶ï¼Œæä¾›äº†å…³äºæœ‰æ•ˆå‚æ•°å…±äº«å’Œæ— å†²çªä¼˜åŒ–çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-116dbf247880a9340d38bccf56fd7d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fddb8be8e4d58fc4ae542a5e3acd230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547bc81d4e49ff068abb71daee8300eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8fb1ef56d69040d93508a02fd409461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25e09890fdf1d5ef0904245a2310066f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Blind-structured-illumination-microscopy-via-generalized-Richardson-Lucy-method"><a href="#Blind-structured-illumination-microscopy-via-generalized-Richardson-Lucy-method" class="headerlink" title="Blind structured illumination microscopy via generalized Richardson-Lucy   method"></a>Blind structured illumination microscopy via generalized Richardson-Lucy   method</h2><p><strong>Authors:Valentina Capalbo, Damiana Battaglini, Marialaura Petroni, Giancarlo Ruocco, Marco Leonetti</strong></p>
<p>Structured illumination microscopy (SIM) can achieve a $2\times$ resolution enhancement beyond the classical diffraction limit by employing illumination translations with respect to the object. This method has also been successfully implemented in a <code>blind&#39;&#39; configuration, i.e., with unknown illumination patterns, allowing for more relaxed constraints on the control of illumination delivery. Here, we present a similar blind-SIM approach using a novel super-resolution algorithm that employs a generalized version of the popular Richardson-Lucy algorithm, alongside an optimized and customized optical setup. Both numerical and experimental validations demonstrate that our technique exhibits high noise resilience. Moreover, by implementing random translations instead of </code>orderedâ€™â€™ ones, noise-related artifacts are reduced. These advancements enable wide-field super-resolved imaging with significantly reduced optical complexity. </p>
<blockquote>
<p>ç»“æ„å…‰ç…§æ˜¾å¾®é•œï¼ˆSIMï¼‰é€šè¿‡ç›¸å¯¹äºç‰©ä½“è¿›è¡Œå…‰ç…§ç¿»è¯‘ï¼Œå¯ä»¥å®ç°è¶…è¿‡ç»å…¸è¡å°„æé™çš„$2\times$åˆ†è¾¨ç‡å¢å¼ºã€‚è¯¥æ–¹æ³•è¿˜æˆåŠŸåœ°åœ¨â€œç›²â€é…ç½®ä¸­å®ç°ï¼Œå³å…·æœ‰æœªçŸ¥ç…§æ˜æ¨¡å¼ï¼Œå¯¹ç…§æ˜äº¤ä»˜çš„æ§åˆ¶è¦æ±‚æ›´åŠ å®½æ¾ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼çš„ç›²SIMæ–¹æ³•ï¼Œä½¿ç”¨ä¸€ç§æ–°å‹è¶…åˆ†è¾¨ç‡ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨æµè¡Œçš„Richardson-Lucyç®—æ³•çš„é€šç”¨ç‰ˆæœ¬ï¼Œä»¥åŠä¼˜åŒ–å’Œå®šåˆ¶çš„å…‰å­¦è£…ç½®ã€‚æ•°å€¼å’Œå®éªŒéªŒè¯å‡è¡¨æ˜æˆ‘ä»¬çš„æŠ€æœ¯å…·æœ‰å¾ˆé«˜çš„æŠ—å™ªå£°æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®æ–½éšæœºç¿»è¯‘è€Œä¸æ˜¯â€œæœ‰åºâ€ç¿»è¯‘ï¼Œå‡å°‘äº†ä¸å™ªå£°ç›¸å…³çš„ä¼ªå½±ã€‚è¿™äº›è¿›æ­¥å®ç°äº†å…·æœ‰æ˜¾è‘—é™ä½å…‰å­¦å¤æ‚æ€§çš„å®½åœºè¶…åˆ†è¾¨ç‡æˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18786v1">PDF</a> 12 pages, 6 figures, including Appendix A and B</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»“æ„ç…§æ˜æ˜¾å¾®é•œï¼ˆSIMï¼‰æŠ€æœ¯ï¼Œé€šè¿‡å¯¹è±¡ç…§æ˜çš„å¹³ç§»å®ç°è¶…è¿‡ç»å…¸è¡å°„æé™çš„2å€åˆ†è¾¨ç‡æå‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä½¿ç”¨æµè¡ŒRichardson-Lucyç®—æ³•çš„é€šç”¨ç‰ˆæœ¬å’Œç»è¿‡ä¼˜åŒ–å’Œå®šåˆ¶çš„å…‰å­¦è®¾ç½®çš„æ–°å‹ç›²SIMæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å±•ç°äº†è‰¯å¥½çš„å™ªå£°é²æ£’æ€§ï¼Œå¹¶é€šè¿‡å®ç°éšæœºå¹³ç§»è€Œéæœ‰åºå¹³ç§»ï¼Œå‡å°‘äº†å™ªå£°ç›¸å…³ä¼ªå½±ã€‚æ­¤è¿›æ­¥ä½¿å®½åœºè¶…åˆ†è¾¨ç‡æˆåƒçš„å…‰å­¦å¤æ‚æ€§æ˜¾è‘—é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“æ„ç…§æ˜æ˜¾å¾®é•œï¼ˆSIMï¼‰å¯é€šè¿‡å¯¹è±¡ç…§æ˜çš„å¹³ç§»å®ç°è¶…è¿‡è¡å°„æé™çš„åˆ†è¾¨ç‡å¢å¼ºã€‚</li>
<li>æå‡ºäº†åŸºäºé€šç”¨Richardson-Lucyç®—æ³•çš„ç›²SIMæ–¹æ³•ã€‚</li>
<li>è¯¥æŠ€æœ¯å…·æœ‰è‰¯å¥½çš„å™ªå£°é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡éšæœºå¹³ç§»å‡å°‘å™ªå£°ç›¸å…³ä¼ªå½±ã€‚</li>
<li>å…‰å­¦å¤æ‚æ€§æ˜¾è‘—é™ä½çš„è¶…åˆ†è¾¨ç‡æˆåƒã€‚</li>
<li>æ–¹æ³•å¯ç”¨äºå®½åœºæˆåƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5794f2f96fd0e9c8d8dd3f3e301b09d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96d2a67500cf9498b86a976c35add1c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e111bf9dd87fdb26185e337d85ac21fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-045d5c387e082ef71212052f3a6158fa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation"><a href="#Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation" class="headerlink" title="Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation"></a>Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation</h2><p><strong>Authors:Qin Wang, Benjamin Bruns, Hanno Scharr, Kai Krajsek</strong></p>
<p>The equivariant behaviour of features is essential in many computer vision tasks, yet popular self-supervised learning (SSL) methods tend to constrain equivariance by design. We propose a self-supervised learning approach where the system learns transformations independently by reconstructing images that have undergone previously unseen transformations. Specifically, the model is tasked to reconstruct intermediate transformed images, e.g. translated or rotated images, without prior knowledge of these transformations. This auxiliary task encourages the model to develop equivariance-coherent features without relying on predefined transformation rules. To this end, we apply transformations to the input image, generating an image pair, and then split the extracted features into two sets per image. One set is used with a usual SSL loss encouraging invariance, the other with our loss based on the auxiliary task to reconstruct the intermediate transformed images. Our loss and the SSL loss are linearly combined with weighted terms. Evaluating on synthetic tasks with natural images, our proposed method strongly outperforms all competitors, regardless of whether they are designed to learn equivariance. Furthermore, when trained alongside augmentation-based methods as the invariance tasks, such as iBOT or DINOv2, we successfully learn a balanced combination of invariant and equivariant features. Our approach performs strong on a rich set of realistic computer vision downstream tasks, almost always improving over all baselines. </p>
<blockquote>
<p>ç‰¹å¾çš„ç­‰å˜è¡Œä¸ºåœ¨è®¡ç®—æœºè§†è§‰çš„è®¸å¤šä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œä½†æµè¡Œçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆSSLï¼‰å€¾å‘äºé€šè¿‡è®¾è®¡æ¥é™åˆ¶ç­‰å˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸ç³»ç»Ÿé€šè¿‡é‡å»ºç»è¿‡å…ˆå‰æœªè§å˜æ¢çš„å›¾åƒæ¥ç‹¬ç«‹åœ°å­¦ä¹ å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯é‡å»ºä¸­é—´å˜æ¢åçš„å›¾åƒï¼Œä¾‹å¦‚å¹³ç§»æˆ–æ—‹è½¬å›¾åƒï¼Œè€Œæ— éœ€äº‹å…ˆäº†è§£è¿™äº›å˜æ¢ã€‚è¿™ä¸€è¾…åŠ©ä»»åŠ¡é¼“åŠ±æ¨¡å‹å‘å±•ç­‰å˜ä¸€è‡´æ€§ç‰¹å¾ï¼Œè€Œä¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„å˜æ¢è§„åˆ™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥å›¾åƒåº”ç”¨å˜æ¢ï¼Œç”Ÿæˆä¸€å¯¹å›¾åƒï¼Œç„¶åå°†æå–çš„ç‰¹å¾æŒ‰å›¾åƒåˆ†æˆä¸¤ç»„ã€‚ä¸€ç»„ç”¨äºä½¿ç”¨å¸¸è§„SSLæŸå¤±æ¥é¼“åŠ±ä¸å˜æ€§ï¼Œå¦ä¸€ç»„ç”¨äºåŸºäºé‡å»ºä¸­é—´å˜æ¢å›¾åƒçš„è¾…åŠ©ä»»åŠ¡çš„æŸå¤±ã€‚æˆ‘ä»¬çš„æŸå¤±å’ŒSSLæŸå¤±é€šè¿‡åŠ æƒé¡¹è¿›è¡Œçº¿æ€§ç»„åˆã€‚åœ¨å¯¹è‡ªç„¶å›¾åƒè¿›è¡Œåˆæˆä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œæ— è®ºæ˜¯å¦è®¾è®¡ä¸ºå­¦ä¹ ç­‰å˜æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éƒ½å¼ºçƒˆä¼˜äºæ‰€æœ‰ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œå½“ä¸åŸºäºå¢å¼ºçš„æ–¹æ³•ï¼ˆå¦‚iBOTæˆ–DINOv2ï¼‰ä½œä¸ºä¸å˜æ€§ä»»åŠ¡ä¸€èµ·è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æˆåŠŸå­¦ä¼šäº†ä¸å˜ç‰¹å¾å’Œç­‰å˜ç‰¹å¾çš„å¹³è¡¡ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸°å¯Œçš„ç°å®è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå‡ ä¹æ€»æ˜¯ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18753v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç«‹å­¦ä¹ å˜æ¢æ¥é‡å»ºç»è¿‡æœªè§å˜æ¢çš„å›¾åƒï¼Œä¿ƒè¿›æ¨¡å‹å‘å±•ç­‰å˜ç›¸å¹²ç‰¹å¾ã€‚æ¨¡å‹æ— éœ€é¢„å…ˆè®¾å®šçš„å˜æ¢è§„åˆ™ï¼Œä¾¿èƒ½é‡å»ºä¸­é—´å˜æ¢å›¾åƒï¼Œå¦‚å¹³ç§»æˆ–æ—‹è½¬å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒåˆæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”æœ‰æ˜¾è‘—æé«˜ã€‚åŒæ—¶ï¼Œå½“ä¸å…¶ä»–å¢å¼ºæ–¹æ³•ç»“åˆä½œä¸ºä¸å˜æ€§ä»»åŠ¡æ—¶ï¼Œè¯¥æ–¹æ³•èƒ½æˆåŠŸå­¦ä¹ å¹³è¡¡çš„ä¸å˜å’Œç­‰å˜ç‰¹å¾ï¼Œå¹¶åœ¨å¤šç§çœŸå®è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é‡å»ºç»è¿‡æœªè§å˜æ¢çš„å›¾åƒæ¥ä¿ƒè¿›æ¨¡å‹å‘å±•ç­‰å˜ç›¸å¹²ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹æ— éœ€é¢„å…ˆè®¾å®šçš„å˜æ¢è§„åˆ™ï¼Œå³å¯é‡å»ºä¸­é—´å˜æ¢å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¼ºçƒˆä¼˜äºæ‰€æœ‰ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>å½“ä¸å…¶ä»–å¢å¼ºæ–¹æ³•ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½æˆåŠŸå­¦ä¹ å¹³è¡¡çš„ä¸å˜å’Œç­‰å˜ç‰¹å¾ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§çœŸå®è®¡ç®—æœºè§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œæ€»æ˜¯ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚</li>
<li>é€šè¿‡åº”ç”¨ä¸åŒçš„å˜æ¢äºè¾“å…¥å›¾åƒï¼Œç”Ÿæˆå›¾åƒå¯¹ï¼Œå¹¶æå–ç‰¹å¾è¿›è¡Œä¸åŒå¤„ç†æ¥è¾¾æˆå­¦ä¹ ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4624e380e737448d093a3f8980418dc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a84161630958b82b2d090cd96b8cce4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-719fb08ef408a26365be538a9207d86d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91a3e6831f344e4761ea4bd13483ba6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-feff8439c69758e551d9d94b3261bb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b667ed4a87e288129c7ae8edbcc9859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b041a509a2a5f5b42a308a78d2f200dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deda0d46342074f6b8bc70a3a9a09b41.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MotionDiff-Training-free-Zero-shot-Interactive-Motion-Editing-via-Flow-assisted-Multi-view-Diffusion"><a href="#MotionDiff-Training-free-Zero-shot-Interactive-Motion-Editing-via-Flow-assisted-Multi-view-Diffusion" class="headerlink" title="MotionDiff: Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion"></a>MotionDiff: Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion</h2><p><strong>Authors:Yikun Ma, Yiqing Li, Jiawei Wu, Zhi Jin</strong></p>
<p>Generative models have made remarkable advancements and are capable of producing high-quality content. However, performing controllable editing with generative models remains challenging, due to their inherent uncertainty in outputs. This challenge is praticularly pronounced in motion editing, which involves the processing of spatial information. While some physics-based generative methods have attempted to implement motion editing, they typically operate on single-view images with simple motions, such as translation and dragging. These methods struggle to handle complex rotation and stretching motions and ensure multi-view consistency, often necessitating resource-intensive retraining. To address these challenges, we propose MotionDiff, a training-free zero-shot diffusion method that leverages optical flow for complex multi-view motion editing. Specifically, given a static scene, users can interactively select objects of interest to add motion priors. The proposed Point Kinematic Model (PKM) then estimates corresponding multi-view optical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently, these optical flows are utilized to generate multi-view motion results through decoupled motion representation in the Multi-view Motion Diffusion Stage (MMDS). Extensive experiments demonstrate that MotionDiff outperforms other physics-based generative motion editing methods in achieving high-quality multi-view consistent motion results. Notably, MotionDiff does not require retraining, enabling users to conveniently adapt it for various down-stream tasks. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¹¶èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„å†…å®¹ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆæ¨¡å‹åœ¨è¾“å‡ºæ–¹é¢å­˜åœ¨å›ºæœ‰çš„ä¸ç¡®å®šæ€§ï¼Œå› æ­¤å¯¹å…¶è¿›è¡Œå¯æ§ç¼–è¾‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨è¿åŠ¨ç¼–è¾‘ä¸­å°¤å…¶çªå‡ºï¼Œè¿åŠ¨ç¼–è¾‘æ¶‰åŠç©ºé—´ä¿¡æ¯çš„å¤„ç†ã€‚è™½ç„¶ä¸€äº›åŸºäºç‰©ç†çš„ç”Ÿæˆæ–¹æ³•å·²ç»å°è¯•å®ç°è¿åŠ¨ç¼–è¾‘ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…åœ¨å…·æœ‰ç®€å•è¿åŠ¨çš„å•è§†å›¾å›¾åƒä¸Šè¿è¡Œï¼Œä¾‹å¦‚å¹³ç§»å’Œæ‹–åŠ¨ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„æ—‹è½¬å’Œä¼¸å±•è¿åŠ¨ä»¥åŠç¡®ä¿å¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œé€šå¸¸éœ€è¦èµ„æºå¯†é›†å‹çš„é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionDiffï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯ä½¿ç”¨çš„é›¶æ ·æœ¬æ‰©æ•£æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å…‰æµè¿›è¡Œå¤æ‚çš„å¤šè§†å›¾è¿åŠ¨ç¼–è¾‘ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªé™æ€åœºæ™¯ï¼Œç”¨æˆ·å¯ä»¥äº¤äº’å¼åœ°é€‰æ‹©æ„Ÿå…´è¶£çš„å¯¹è±¡æ¥æ·»åŠ è¿åŠ¨å…ˆéªŒã€‚ç„¶åï¼Œæ‰€æå‡ºçš„å…³é”®ç‚¹è¿åŠ¨å­¦æ¨¡å‹ï¼ˆPKMï¼‰åœ¨å¤šç‚¹æµä¼°è®¡é˜¶æ®µï¼ˆMFESï¼‰ä¼°è®¡ç›¸åº”çš„å¤šè§†å›¾å…‰æµã€‚éšåï¼Œè¿™äº›å…‰æµåœ¨å¤šè§†å›¾è¿åŠ¨æ‰©æ•£é˜¶æ®µï¼ˆMMDSï¼‰é€šè¿‡è§£è€¦çš„è¿åŠ¨è¡¨ç¤ºæ¥ç”Ÿæˆå¤šè§†å›¾è¿åŠ¨ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMotionDiffåœ¨å…¶ä»–åŸºäºç‰©ç†çš„ç”Ÿæˆè¿åŠ¨ç¼–è¾‘æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„å¤šè§†å›¾ä¸€è‡´è¿åŠ¨ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMotionDiffä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°å°†å…¶é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17695v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆæ¨¡å‹åœ¨è¿åŠ¨ç¼–è¾‘æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†MotionDiffæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚MotionDiffæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°å¤æ‚å¤šè§†è§’è¿åŠ¨ç¼–è¾‘çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å…‰å­¦æµè¿›è¡Œè¿åŠ¨é¢„æµ‹å’Œå¤šè§†è§’ä¸€è‡´æ€§ä¿æŒã€‚ç”¨æˆ·å¯ä»¥ä¸ºé™æ€åœºæ™¯ä¸­çš„å¯¹è±¡æ·»åŠ è¿åŠ¨å…ˆéªŒï¼Œå¹¶é€šè¿‡PKMæ¨¡å‹ä¼°è®¡å¤šè§†è§’å…‰å­¦æµï¼Œç„¶ååœ¨å¤šè§†è§’è¿åŠ¨æ‰©æ•£é˜¶æ®µç”Ÿæˆå¤šè§†è§’è¿åŠ¨ç»“æœã€‚è¯¥æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨äº§ç”Ÿé«˜è´¨é‡å†…å®¹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¯æ§ç¼–è¾‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç©ºé—´ä¿¡æ¯çš„è¿åŠ¨ç¼–è¾‘æ–¹é¢ã€‚</li>
<li>ç°æœ‰ç‰©ç†åŸºç¡€ç”Ÿæˆæ–¹æ³•ä¸»è¦å¤„ç†ç®€å•è¿åŠ¨ï¼Œå¦‚å¹³ç§»å’Œæ‹–åŠ¨ï¼Œéš¾ä»¥å¤„ç†å¤æ‚æ—‹è½¬å’Œæ‹‰ä¼¸è¿åŠ¨ï¼Œä»¥åŠç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>MotionDiffæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„é›¶å°„å‡»æ‰©æ•£æ–¹æ³•ï¼Œå¯è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>MotionDiffåˆ©ç”¨å…‰å­¦æµè¿›è¡Œå¤æ‚å¤šè§†è§’è¿åŠ¨ç¼–è¾‘ï¼Œç”¨æˆ·å¯ä»¥ä¸ºæ„Ÿå…´è¶£çš„å¯¹è±¡æ·»åŠ è¿åŠ¨å…ˆéªŒã€‚</li>
<li>MotionDiffé€šè¿‡PKMæ¨¡å‹ä¼°è®¡å¤šè§†è§’å…‰å­¦æµï¼Œå¹¶åœ¨å¤šè§†è§’è¿åŠ¨æ‰©æ•£é˜¶æ®µç”Ÿæˆå¤šè§†è§’è¿åŠ¨ç»“æœã€‚</li>
<li>æ— éœ€é‡æ–°è®­ç»ƒï¼ŒMotionDiffå¯æ–¹ä¾¿åœ°åº”ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d482e62f56cfa4216428062113d0e232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-787c81b4274434a525a0f85a06bc375e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e57beb3a284ef7705c26ef0afa87e8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4715f604a6903c6000dc2b806cecbde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b5926f11bdc29d91e3367aa441afe83.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image"><a href="#Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image" class="headerlink" title="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image"></a>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image</h2><p><strong>Authors:Jerred Chen, Ronald Clark</strong></p>
<p>In many robotics and VR&#x2F;AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯å’Œè™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®åº”ç”¨çš„è®¸å¤šåœºæ™¯ä¸­ï¼Œå¿«é€Ÿçš„ç›¸æœºè¿åŠ¨ä¼šäº§ç”Ÿè¾ƒé«˜æ°´å¹³çš„è¿åŠ¨æ¨¡ç³Šï¼Œå¯¼è‡´ç°æœ‰çš„ç›¸æœºå§¿æ€ä¼°è®¡æ–¹æ³•å¤±æ•ˆã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºè¿åŠ¨ä¼°è®¡çš„ä¸°å¯Œçº¿ç´¢ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºä¸éœ€è¦çš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¯¹å•ä¸ªè¿åŠ¨æ¨¡ç³Šå›¾åƒç›´æ¥é¢„æµ‹å¯†é›†è¿åŠ¨æµåœºå’Œå•ç›®æ·±åº¦å›¾æ¥å·¥ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å°è¿åŠ¨å‡è®¾ä¸‹é€šè¿‡è§£å†³çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜æ¥æ¢å¤ç›¸æœºçš„ç¬æ—¶é€Ÿåº¦ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ä¸€ç§ç±»ä¼¼äºIMUçš„æµ‹é‡å€¼ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ•è·å¿«é€Ÿä¸”æ¿€çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ScanNet++v2æ„å»ºäº†å…·æœ‰ç°å®åˆæˆè¿åŠ¨æ¨¡ç³Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é€šè¿‡åœ¨æˆ‘ä»¬çš„å®Œå…¨å¯å¾®åˆ†ç®¡é“ä¸Šå¯¹çœŸå®æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒæ¥è¿›ä¸€æ­¥å®Œå–„æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§’é€Ÿåº¦å’Œçº¿é€Ÿåº¦ä¼°è®¡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¶…è¶Šäº†MASt3Rå’ŒCOLMAPç­‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17358v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jerredchen.github.io/image-as-imu/">https://jerredchen.github.io/image-as-imu/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹æœºå™¨äººæŠ€æœ¯å’Œè™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®é¢†åŸŸä¸­çš„å¿«é€Ÿç›¸æœºè¿åŠ¨å¯¼è‡´çš„é«˜è¿åŠ¨æ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨ä¼°è®¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºä¸°å¯Œçš„è¿åŠ¨ä¼°è®¡çº¿ç´¢ï¼Œè€Œéå°†å…¶è§†ä¸ºä¸éœ€è¦çš„ä¼ªå½±ã€‚é€šè¿‡é¢„æµ‹å•å¼ è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­çš„å¯†é›†è¿åŠ¨æµåœºå’Œå•çœ¼æ·±åº¦å›¾ï¼Œå¹¶ç»“åˆå°è¿åŠ¨å‡è®¾ä¸‹çš„çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜æ±‚è§£ç¬æ—¶ç›¸æœºé€Ÿåº¦ã€‚æœ¬è´¨ä¸Šï¼Œè¯¥æ–¹æ³•ç”Ÿæˆäº†ä¸€ç§IMUå¼çš„æµ‹é‡å€¼ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ•è·å¿«é€Ÿä¸”å‰§çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚é€šè¿‡å¤§è§„æ¨¡åˆæˆè¿åŠ¨æ¨¡ç³Šæ•°æ®é›†ScanNet++v2å’ŒçœŸå®æ•°æ®çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…ˆè¿›çš„è§’é€Ÿåº¦å’Œçº¿é€Ÿåº¦ä¼°è®¡æ•ˆæœï¼Œä¼˜äºMASt3Rå’ŒCOLMAPç­‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹æœºå™¨äººå’ŒVR&#x2F;ARåº”ç”¨ä¸­å¿«é€Ÿç›¸æœºè¿åŠ¨å¯¼è‡´çš„é«˜è¿åŠ¨æ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨ä¼°è®¡æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è¿åŠ¨æ¨¡ç³Šä½œä¸ºè¿åŠ¨ä¼°è®¡çš„çº¿ç´¢ï¼Œä¸åŒäºä»¥å¾€å°†å…¶è§†ä¸ºä¸éœ€è¦çš„ä¼ªå½±ã€‚</li>
<li>é€šè¿‡é¢„æµ‹å¯†é›†è¿åŠ¨æµåœºå’Œå•çœ¼æ·±åº¦å›¾æ¥ä¼°è®¡ç¬æ—¶ç›¸æœºé€Ÿåº¦ã€‚</li>
<li>ç¬æ—¶ç›¸æœºé€Ÿåº¦é€šè¿‡è§£å†³å°è¿åŠ¨å‡è®¾ä¸‹çš„çº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜å¾—åˆ°ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆäº†ä¸€ç§IMUå¼çš„æµ‹é‡å€¼ï¼Œç¨³å¥åœ°æ•è·å¿«é€Ÿä¸”å‰§çƒˆçš„ç›¸æœºè¿åŠ¨ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ¨¡å‹å¾—åˆ°äº†è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78369df8376d9f15ee1cbfd9befd8bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640e7682b4f7e849cdb40071911e0898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33cfb97cb85d5aff11886df2a728eaf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46a3deb70455431abc4a32e2ac770182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e90c3afa34dfaa3572ab8d7ebfb5337.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion embeds an input reference image into arbitrary scenes as described by the text prompts, while exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion featuresâ€™ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing harmonious fusion of the reference structural information and the textual semantic information. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning, all while substantially outperforming related methods in image quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. </p>
<blockquote>
<p>éšè—å›¾ç‰‡çš„è§†è§‰é”™è§‰æ˜¯ä¸€ç§æœ‰è¶£çš„è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå…¶ä¸­ä¸€å¼ å›¾ç‰‡è¢«å·§å¦™åœ°èå…¥å¦ä¸€å¼ å›¾ç‰‡ä¸­ï¼Œè§‚ä¼—æ— æ³•ç«‹å³å¯Ÿè§‰ã€‚æˆ‘ä»¬åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡å­—å¼•å¯¼å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶ï¼Œåä¸ºâ€œé˜¶æ®µè½¬ç§»æ‰©æ•£æ¨¡å‹â€ï¼ˆPTDiffusionï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯ã€‚PTDiffusionå°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥åˆ°æ–‡æœ¬æç¤ºæ‰€æè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå‚è€ƒå›¾åƒçš„éšè—è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ¨æ€ä¸”é€æ­¥åœ°ç§»æ¤æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œä»å»å™ªè¿‡ç¨‹ä¸­é‡å»ºå‚è€ƒå›¾åƒï¼Œå°†å…¶é‡‡æ ·ä¸ºç”Ÿæˆçš„é”™è§‰å›¾åƒï¼Œå®ç°å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å’Œè°èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹å¯è¯†åˆ«åº¦çš„ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¿å…äº†ä»»ä½•æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼ŒåŒæ—¶åœ¨å›¾åƒè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰å¯è¯†åˆ«åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶æ€§æ–¹é¢å¤§å¤§ä¼˜äºç›¸å…³æ–¹æ³•åœ¨é”™è§‰å›¾ç‰‡åˆæˆæ–¹é¢çš„è¡¨ç°ï¼Œè¿™å·²é€šè¿‡å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒå¾—åˆ°è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v3">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æ¡†æ¶â€”â€”PTDiffusionã€‚é€šè¿‡æ­¤æ¡†æ¶ï¼Œå¯ä»¥å·§å¦™åœ°å°†å‚è€ƒå›¾åƒåµŒå…¥åˆ°ä»»æ„åœºæ™¯ä¸­ï¼Œå¹¶æŒ‰ç…§æ–‡æœ¬æç¤ºç”Ÿæˆéšè—çš„è§†è§‰çº¿ç´¢ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªå³æ—¶å¯ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°é€æ­¥è½¬ç§»æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œä»è€Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­å°†å‚è€ƒå›¾åƒèå…¥ç”Ÿæˆçš„å¹»è§‰å›¾åƒä¸­ï¼Œå®ç°å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å’Œè°èåˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†çµæ´»çš„éšè”½å†…å®¹å¯è¾¨è¯†åº¦æ§åˆ¶åŠŸèƒ½ã€‚æ— éœ€è¿›è¡Œä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡ã€æ–‡æœ¬ä¿çœŸåº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦ç­‰æ–¹é¢å¤§å¤§è¶…è¶Šäº†ç›¸å…³æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PTDiffusionæ˜¯ä¸€ä¸ªåŸºäºç°æœ‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå¤–I2Iç¿»è¯‘æ¡†æ¶ã€‚</li>
<li>å®ƒèƒ½å°†å‚è€ƒå›¾åƒå·§å¦™åœ°åµŒå…¥åˆ°ä»»æ„åœºæ™¯ä¸­ï¼Œå¹¶ç”Ÿæˆéšè—çš„è§†è§‰çº¿ç´¢ã€‚</li>
<li>ç›¸ä½è½¬ç§»æœºåˆ¶æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒï¼Œèƒ½å¤ŸåŠ¨æ€åœ°é€æ­¥è½¬ç§»æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å’Œè°èåˆã€‚</li>
<li>PTDiffusionæä¾›äº†çµæ´»çš„éšè”½å†…å®¹å¯è¾¨è¯†åº¦æ§åˆ¶åŠŸèƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4e5777d13795243fd75cec3d975d28a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b77ea1cae8b7ea5e706e5bbd9b40650b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c357f2ab32379793821f600f5c7f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97f9cc33a146cafad80fb4946166eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-372d816e0098104ced66081bc2c673c5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Automatic-Evaluation-for-Image-Transcreation"><a href="#Towards-Automatic-Evaluation-for-Image-Transcreation" class="headerlink" title="Towards Automatic Evaluation for Image Transcreation"></a>Towards Automatic Evaluation for Image Transcreation</h2><p><strong>Authors:Simran Khanuja, Vivek Iyer, Claire He, Graham Neubig</strong></p>
<p>Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we identify three critical dimensions of image transcreation: cultural relevance, semantic equivalence and visual similarity, and design our metrics to evaluate systems along these axes. Our results show that proprietary VLMs best identify cultural relevance and semantic equivalence, while vision-encoder representations are adept at measuring visual similarity. Meta-evaluation across 7 countries shows our metrics agree strongly with human ratings, with average segment-level correlations ranging from 0.55-0.87. Finally, through a discussion of the merits and demerits of each metric, we offer a robust framework for automated image transcreation evaluation, grounded in both theoretical foundations and practical application. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-img-transcreation">https://github.com/simran-khanuja/automatic-eval-img-transcreation</a>. </p>
<blockquote>
<p>åœ¨ä¼ ç»Ÿç¿»è¯‘è¯­éŸ³å’Œæ–‡æœ¬çš„æ¨¡å¼ä¹‹å¤–ï¼Œæœ€è¿‘äººä»¬å¯¹è‡ªåŠ¨åˆ›å»ºå›¾åƒä»¥åœ¨ä¸åŒæ–‡åŒ–ä¸­å®ç°è§†è§‰å†…å®¹æœ¬åœ°åŒ–çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚å°†è¿™ä¸€é¢†åŸŸå®šä¹‰ä¸ºæ­£å¼çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰é—®é¢˜çš„å°è¯•å—åˆ°äº†ç¼ºä¹è‡ªåŠ¨è¯„ä¼°æœºåˆ¶çš„é˜»ç¢ï¼Œæ—©æœŸçš„å·¥ä½œå®Œå…¨ä¾èµ–äºäººå·¥è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡æå‡ºä¸€ç³»åˆ—å—æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æŒ‡æ ‡å¯å‘çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ¥ç¼©å°è¿™ä¸€å·®è·ï¼Œè¿™äº›æŒ‡æ ‡å¯åˆ†ä¸ºä¸‰ç±»ï¼šaï¼‰åŸºäºå¯¹è±¡çš„æŒ‡æ ‡ã€bï¼‰åŸºäºåµŒå…¥çš„æŒ‡æ ‡å’Œcï¼‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å€Ÿé‰´ç¿»è¯‘ç ”ç©¶ç†è®ºå’Œç°å®ä¸–ç•Œçš„å›¾åƒåˆ›ä½œå®è·µï¼Œç¡®å®šäº†å›¾åƒåˆ›ä½œçš„ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰å€¼å’Œè§†è§‰ç›¸ä¼¼æ€§ï¼Œå¹¶è®¾è®¡äº†æˆ‘ä»¬çš„æŒ‡æ ‡æ¥è¯„ä¼°è¿™äº›ç»´åº¦ä¸Šçš„ç³»ç»Ÿæ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œä¸“å±çš„VLMåœ¨è¯†åˆ«æ–‡åŒ–ç›¸å…³æ€§å’Œè¯­ä¹‰ç­‰å€¼æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œè§†è§‰ç¼–ç å™¨è¡¨ç¤ºåˆ™æ“…é•¿æµ‹é‡è§†è§‰ç›¸ä¼¼æ€§ã€‚åœ¨7ä¸ªå›½å®¶çš„å…ƒè¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æŒ‡æ ‡ä¸äººç±»è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡æ®µçº§ç›¸å…³ç³»æ•°åœ¨0.55è‡³0.87ä¹‹é—´ã€‚æœ€åï¼Œé€šè¿‡è®¨è®ºæ¯ä¸ªæŒ‡æ ‡çš„ä¼˜ç¼ºç‚¹ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäºç†è®ºæ¡†æ¶å’Œå®è·µåº”ç”¨çš„ç¨³å¥çš„è‡ªåŠ¨åŒ–å›¾åƒåˆ›ä½œè¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-img-transcreation%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/simran-khanuja/automatic-eval-img-transcreationä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13717v3">PDF</a> To be presented at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³å›¾åƒè·¨æ–‡åŒ–è‡ªåŠ¨ç¿»è¯‘è¯„ä¼°æœºåˆ¶çš„ç¼ºå¤±é—®é¢˜ï¼Œå€Ÿé‰´æœºå™¨ç¿»è¯‘ç†è®ºå’ŒæŠ€æœ¯ï¼Œæå‡ºä¸€å¥—é’ˆå¯¹å›¾åƒè½¬è¯‘æ•ˆæœçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬åŸºäºå¯¹è±¡ã€åµŒå…¥å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä»·æ–¹æ³•ï¼Œæ¶µç›–æ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰ä»·æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„ä»·é«˜åº¦ä¸€è‡´ï¼Œä¸ºå›¾åƒè½¬è¯‘çš„è‡ªåŠ¨åŒ–è¯„ä¼°æä¾›äº†åšå®çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè½¬è¯‘è¯„ä¼°é—®é¢˜å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå­˜åœ¨éœ€æ±‚å¼€å‘è‡ªåŠ¨è¯„ä¼°æœºåˆ¶ä»¥æ¨è¿›å…¶æŠ€æœ¯å‘å±•ã€‚</li>
<li>è¯¥ç ”ç©¶å€Ÿé‰´æœºå™¨ç¿»è¯‘ç†è®ºå’ŒæŠ€æœ¯ï¼Œæå‡ºä¸€å¥—åŸºäºå¯¹è±¡ã€åµŒå…¥å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å›¾åƒè½¬è¯‘è¯„ä¼°çš„å…³é”®ç»´åº¦åŒ…æ‹¬æ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰ä»·æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°è¯†åˆ«æ–‡åŒ–ç›¸å…³æ€§å’Œè¯­ä¹‰ç­‰ä»·æ€§ã€‚</li>
<li>åŸºäºè§†è§‰ç¼–ç å™¨è¡¨ç¤ºçš„å›¾åƒç›¸ä¼¼æ€§æµ‹é‡æ–¹æ³•å¾—åˆ°äº†æœ‰æ•ˆçš„è¯„ä¼°ç»“æœã€‚</li>
<li>ç ”ç©¶è¿›è¡Œçš„è·¨ä¸ƒå›½çš„è¯„ä¼°è¡¨æ˜è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„åˆ†å­˜åœ¨å¼ºçƒˆä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21be0dd3c32790e2ba10c043d1c20c7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ae5c92e42b3b7e87825512d129a77e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba0132b4f1076ffac25ff78434e5967.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b141b1eee7f3cf4539884115ee35437f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8753beb91d227130bf9e7af0242dcc3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Omni6D-Large-Vocabulary-3D-Object-Dataset-for-Category-Level-6D-Object-Pose-Estimation"><a href="#Omni6D-Large-Vocabulary-3D-Object-Dataset-for-Category-Level-6D-Object-Pose-Estimation" class="headerlink" title="Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object   Pose Estimation"></a>Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object   Pose Estimation</h2><p><strong>Authors:Mengchen Zhang, Tong Wu, Tai Wang, Tengfei Wang, Ziwei Liu, Dahua Lin</strong></p>
<p>6D object pose estimation aims at determining an objectâ€™s translation, rotation, and scale, typically from a single RGBD image. Recent advancements have expanded this estimation from instance-level to category-level, allowing models to generalize across unseen instances within the same category. However, this generalization is limited by the narrow range of categories covered by existing datasets, such as NOCS, which also tend to overlook common real-world challenges like occlusion. To tackle these challenges, we introduce Omni6D, a comprehensive RGBD dataset featuring a wide range of categories and varied backgrounds, elevating the task to a more realistic context. 1) The dataset comprises an extensive spectrum of 166 categories, 4688 instances adjusted to the canonical pose, and over 0.8 million captures, significantly broadening the scope for evaluation. 2) We introduce a symmetry-aware metric and conduct systematic benchmarks of existing algorithms on Omni6D, offering a thorough exploration of new challenges and insights. 3) Additionally, we propose an effective fine-tuning approach that adapts models from previous datasets to our extensive vocabulary setting. We believe this initiative will pave the way for new insights and substantial progress in both the industrial and academic fields, pushing forward the boundaries of general 6D pose estimation. </p>
<blockquote>
<p>6Dç‰©ä½“å§¿æ€ä¼°è®¡æ—¨åœ¨ç¡®å®šç‰©ä½“çš„å¹³ç§»ã€æ—‹è½¬å’Œå°ºåº¦ï¼Œé€šå¸¸æ˜¯ä»å•ä¸€çš„RGBDå›¾åƒå‡ºå‘ã€‚è¿‘æœŸçš„å‘å±•å·²ç»å°†æ­¤ä¼°è®¡ä»å®ä¾‹å±‚é¢æ‰©å±•åˆ°ç±»åˆ«å±‚é¢ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨åŒä¸€ç±»åˆ«ä¸­æ³›åŒ–åˆ°æœªè§è¿‡çš„å®ä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ³›åŒ–å—é™äºç°æœ‰æ•°æ®é›†è¦†ç›–çš„ç±»åˆ«èŒƒå›´è¾ƒçª„ï¼Œå¦‚NOCSæ•°æ®é›†ï¼Œå®ƒä»¬è¿˜å¾€å¾€å¿½è§†é®æŒ¡ç­‰å¸¸è§çš„ç°å®ä¸–ç•ŒæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Omni6Dï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆRGBDæ•°æ®é›†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„ç±»åˆ«å’Œå¤šæ ·åŒ–çš„èƒŒæ™¯ï¼Œå°†ä»»åŠ¡æå‡åˆ°ä¸€ä¸ªæ›´ç°å®çš„è¯­å¢ƒä¸­ã€‚1ï¼‰è¯¥æ•°æ®é›†åŒ…å«äº†166ä¸ªç±»åˆ«çš„å¹¿æ³›å…‰è°±ã€è°ƒæ•´ä¸ºæ ‡å‡†å§¿æ€çš„4688ä¸ªå®ä¾‹ä»¥åŠè¶…è¿‡80ä¸‡ä¸ªæ•è·å¯¹è±¡ï¼Œæ˜¾è‘—æ‰©å¤§äº†è¯„ä¼°èŒƒå›´ã€‚2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹ç§°æ„ŸçŸ¥åº¦é‡æ ‡å‡†ï¼Œå¹¶åœ¨Omni6Dä¸Šå¯¹ç°æœ‰ç®—æ³•è¿›è¡Œäº†ç³»ç»ŸåŸºå‡†æµ‹è¯•ï¼Œæä¾›äº†å¯¹æ–°æŒ‘æˆ˜å’Œè§è§£çš„æ·±å…¥æ¢è®¨ã€‚3ï¼‰æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æˆ‘ä»¬å¹¿æ³›çš„è¯æ±‡è®¾ç½®ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸€ä¸¾æªå°†ä¸ºå·¥ä¸šå’Œå­¦æœ¯ç•Œå¸¦æ¥æ–°çš„è§è§£å’Œå®è´¨æ€§è¿›å±•ï¼Œæ¨åŠ¨é€šç”¨6Då§¿æ€ä¼°è®¡çš„è¾¹ç•Œä¸æ–­å‘å‰å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18261v3">PDF</a> ECCV 2024 (poster). Github page: <a target="_blank" rel="noopener" href="https://github.com/3DTopia/Omni6D">https://github.com/3DTopia/Omni6D</a></p>
<p><strong>Summary</strong></p>
<p>Omni6Dæ•°æ®é›†è§£å†³äº†ç°æœ‰RGBDæ•°æ®é›†å¦‚NOCSå­˜åœ¨çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬ç±»åˆ«èŒƒå›´ç‹­çª„å’Œå¿½ç•¥çœŸå®ä¸–ç•ŒæŒ‘æˆ˜å¦‚é®æŒ¡é—®é¢˜ã€‚Omni6DåŒ…å«å¹¿æ³›çš„166ä¸ªç±»åˆ«ï¼Œå¤§é‡å®ä¾‹å’Œè°ƒæ•´è‡³è§„èŒƒå§¿æ€çš„æ•æ‰ï¼Œæå¤§æ‰©å±•äº†è¯„ä¼°èŒƒå›´ã€‚å¼•å…¥å¯¹ç§°æ„ŸçŸ¥åº¦é‡å¹¶å¯¹ç°æœ‰ç®—æ³•è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œä¸ºæ–°çš„æŒ‘æˆ˜å’Œè§è§£æä¾›å…¨é¢æ¢ç´¢ã€‚æ­¤å¤–ï¼Œæå‡ºæœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä½¿æ¨¡å‹é€‚åº”å¹¿æ³›çš„è¯æ±‡è®¾ç½®ã€‚æ­¤å€¡è®®å°†æ¨åŠ¨å·¥ä¸šå’Œå­¦æœ¯ç•Œå¯¹é€šç”¨6Då§¿æ€ä¼°è®¡çš„æ·±å…¥ç ”ç©¶å’Œè¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni6Dæ˜¯ä¸€ä¸ªå…¨é¢çš„RGBDæ•°æ®é›†ï¼ŒåŒ…å«å¹¿æ³›çš„ç±»åˆ«å’Œå¤šæ ·åŒ–çš„èƒŒæ™¯ï¼Œä½¿6Då¯¹è±¡å§¿æ€ä¼°è®¡ä»»åŠ¡æ›´åŠ è´´è¿‘çœŸå®åœºæ™¯ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤§é‡å®ä¾‹å’Œè°ƒæ•´è‡³è§„èŒƒå§¿æ€çš„æ•æ‰ï¼Œæ˜¾è‘—æ‰©å¤§äº†è¯„ä¼°èŒƒå›´ã€‚</li>
<li>å¼•å…¥å¯¹ç§°æ„ŸçŸ¥åº¦é‡ï¼Œå¯¹ç°æœ‰ç®—æ³•è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œä¸ºæ–°çš„æŒ‘æˆ˜å’Œè§è§£æä¾›å…¨é¢æ¢ç´¢ã€‚</li>
<li>Omni6Dè§£å†³äº†ç°æœ‰æ•°æ®é›†å¿½ç•¥çœŸå®ä¸–ç•ŒæŒ‘æˆ˜ï¼ˆå¦‚é®æŒ¡ï¼‰çš„é—®é¢˜ã€‚</li>
<li>æå‡ºæœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”å¹¿æ³›çš„è¯æ±‡è®¾ç½®ã€‚</li>
<li>Omni6Dçš„å¼•å…¥å°†æ¨åŠ¨å·¥ä¸šå’Œå­¦æœ¯ç•Œåœ¨6Då§¿æ€ä¼°è®¡æ–¹é¢çš„æ·±å…¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52a82c318ddbaa8e12ceebb9d15c92d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42b2537cb0a4b34c5a9fad40119b3006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements"><a href="#Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements" class="headerlink" title="Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements"></a>Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements</h2><p><strong>Authors:Jipeng Yan, Qingyuan Tan, Shusei Kawara, Jingwen Zhu, Bingxue Wang, Matthieu Toulemonde, Honghai Liu, Ying Tan, Meng-Xing Tang</strong></p>
<p>Super-Resolution Ultrasound (SRUS) imaging through localising and tracking microbubbles, also known as Ultrasound Localisation Microscopy (ULM), has demonstrated significant potential for reconstructing microvasculature and flows with sub-diffraction resolution in clinical diagnostics. However, imaging organs with large tissue movements, such as those caused by respiration, presents substantial challenges. Existing methods often require breath holding to maintain accumulation accuracy, which limits data acquisition time and ULM image saturation. To improve image quality in the presence of large tissue movements, this study introduces an approach integrating high-frame-rate ultrasound with online precise robotic probe control. Tested on a microvasculature phantom with translation motions up to 20 mm, twice the aperture size of the matrix array used, our method achieved real-time tracking of the moving phantom and imaging volume rate at 85 Hz, keeping majority of the target volume in the imaging field of view. ULM images of the moving cross channels in the phantom were successfully reconstructed in post-processing, demonstrating the feasibility of super-resolution imaging under large tissue motions. This represents a significant step towards ULM imaging of organs with large motion. </p>
<blockquote>
<p>è¶…å£°å®šä½æˆåƒæŠ€æœ¯ï¼ˆSRUSï¼‰é€šè¿‡å®šä½å’Œè¿½è¸ªå¾®æ³¡ï¼Œä¹Ÿè¢«ç§°ä¸ºè¶…å£°å®šä½æ˜¾å¾®é•œï¼ˆULMï¼‰ï¼Œåœ¨ä¸´åºŠè¯Šæ–­ä¸­æ˜¾ç¤ºå‡ºé‡å»ºå…·æœ‰äºšè¡å°„åˆ†è¾¨ç‡çš„å¾®è¡€ç®¡å’Œè¡€æµçš„æ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºå­˜åœ¨ç”±å‘¼å¸ç­‰å¼•èµ·çš„å¤§ç»„ç»‡è¿åŠ¨çš„å™¨å®˜æˆåƒï¼Œç°æœ‰çš„æ–¹æ³•é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å±ä½å‘¼å¸ä»¥ä¿æŒç§¯ç´¯å‡†ç¡®æ€§ï¼Œè¿™é™åˆ¶äº†æ•°æ®é‡‡é›†æ—¶é—´å’ŒULMå›¾åƒçš„é¥±å’Œåº¦ã€‚ä¸ºäº†æé«˜å¤§ç»„ç»‡è¿åŠ¨ä¸‹çš„å›¾åƒè´¨é‡ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å°†é«˜å¸§ç‡è¶…å£°ä¸åœ¨çº¿ç²¾å¯†æœºæ¢°æ¢é’ˆæ§åˆ¶ç›¸ç»“åˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å…·æœ‰é«˜è¾¾20æ¯«ç±³å¹³ç§»è¿åŠ¨çš„å¾®è¡€ç®¡æˆåƒå¹»å½±ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¿™ä¸€æ•°å€¼ä¸ºæ‰€ä½¿ç”¨çš„çŸ©é˜µé˜µåˆ—å­”å¾„å°ºå¯¸çš„ä¸¤å€ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¯¹ç§»åŠ¨å¹»å½±çš„å®æ—¶è·Ÿè¸ªå’Œæˆåƒä½“ç§¯ç‡è¾¾åˆ°äº†85èµ«å…¹ï¼Œä½¿å¤§éƒ¨åˆ†ç›®æ ‡ä½“ç§¯ä¿æŒåœ¨æˆåƒè§†é‡å†…ã€‚åœ¨åæœŸåˆ¶ä½œä¸­æˆåŠŸé‡å»ºäº†ç§»åŠ¨äº¤å‰é€šé“å¹»å½±çš„ULMå›¾åƒï¼Œè¯æ˜äº†åœ¨å¤§ç»„ç»‡è¿åŠ¨ä¸‹è¿›è¡Œè¶…åˆ†è¾¨ç‡æˆåƒçš„å¯è¡Œæ€§ã€‚è¿™æ ‡å¿—ç€å¯¹å…·æœ‰å¤§è¿åŠ¨çš„å™¨å®˜è¿›è¡ŒULMæˆåƒè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11391v3">PDF</a> </p>
<p><strong>Summary</strong><br>SRUSæˆåƒé€šè¿‡å®šä½è¿½è¸ªå¾®æ³¡æŠ€æœ¯ï¼Œå³è¶…å£°å®šä½æ˜¾å¾®é•œï¼ˆULMï¼‰ï¼Œåœ¨ä¸´åºŠè¯Šæ–­ä¸­é‡å»ºå…·æœ‰äºšè¡å°„åˆ†è¾¨ç‡çš„å¾®è¡€ç®¡å’Œç»„ç»‡æµæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å­˜åœ¨å¤§ç»„ç»‡è¿åŠ¨çš„æƒ…å†µä¸‹æˆåƒï¼Œå¦‚å‘¼å¸å¼•èµ·çš„è¿åŠ¨ï¼Œå­˜åœ¨å¾ˆå¤§æŒ‘æˆ˜ã€‚ä¸ºæ”¹å–„å›¾åƒè´¨é‡ï¼Œæœ¬ç ”ç©¶å°†é«˜å¸§ç‡è¶…å£°ä¸åœ¨çº¿ç²¾å¯†æœºæ¢°æ¢é’ˆæ§åˆ¶ç›¸ç»“åˆã€‚åœ¨å…·æœ‰è¾¾çŸ©é˜µé˜µåˆ—å­”å¾„å°ºå¯¸ä¸¤å€çš„ä½ç§»è¿åŠ¨çš„å¾®è¡€ç®¡æ¨¡å‹æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¯¹ç§»åŠ¨æ¨¡å‹çš„å®æ—¶è·Ÿè¸ªï¼Œæˆåƒä½“ç§¯ç‡é«˜è¾¾æ¯ç§’æ›´æ–°ç”»é¢å…«åäº”æ¬¡ä»¥ä¸Šï¼Œä¸”å°†å¤šæ•°ç›®æ ‡ä½“ç§¯ä¿æŒåœ¨ä¸€ä¸ªå¯è§‚å¯ŸèŒƒå›´ä¸­ã€‚æˆåŠŸé‡å»ºäº†ç§»åŠ¨äº¤å‰é€šé“æ¨¡å‹ä¸­çš„ULMå›¾åƒï¼Œè¯æ˜äº†åœ¨å¤§ç»„ç»‡è¿åŠ¨ä¸‹è¿›è¡Œè¶…åˆ†è¾¨ç‡æˆåƒçš„å¯è¡Œæ€§ã€‚è¿™æ ‡å¿—ç€å¯¹è¿åŠ¨å™¨å®˜è¿›è¡ŒULMæˆåƒçš„é‡è¦è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…çº§åˆ†è¾¨ç‡è¶…å£°æˆåƒé€šè¿‡å®šä½è¿½è¸ªå¾®æ³¡æŠ€æœ¯ï¼ˆULMï¼‰åœ¨ä¸´åºŠè¯Šæ–­ä¸­å…·æœ‰é‡å»ºå¾®è¡€ç®¡çš„æ½œåŠ›ã€‚</li>
<li>å­˜åœ¨å¤§ç»„ç»‡è¿åŠ¨æ—¶ï¼ˆå¦‚å‘¼å¸å¼•èµ·çš„è¿åŠ¨ï¼‰ï¼ŒSRUSæˆåƒé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†é«˜å¸§ç‡è¶…å£°å’Œåœ¨çº¿ç²¾å¯†æœºæ¢°æ¢é’ˆæ§åˆ¶æ¥æ”¹è¿›å›¾åƒè´¨é‡ã€‚</li>
<li>æ–¹æ³•å®ç°äº†å¯¹ç§»åŠ¨æ¨¡å‹çš„å®æ—¶è·Ÿè¸ªå’Œè¾ƒé«˜çš„æˆåƒä½“ç§¯ç‡ã€‚</li>
<li>æˆåŠŸé‡å»ºäº†ç§»åŠ¨äº¤å‰é€šé“æ¨¡å‹ä¸­çš„ULMå›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ä»£è¡¨äº†åœ¨å¤§ç»„ç»‡è¿åŠ¨ä¸‹è¿›è¡ŒSRUSæˆåƒçš„é‡è¦è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9a6cac96c931c7569cfe7376c7e9ecf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7e286a2006f4efcd1c94be7525bde2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4afe89cecaa57e9b26794d5aba299201.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs"><a href="#WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs" class="headerlink" title="WAIT: Feature Warping for Animation to Illustration video Translation   using GANs"></a>WAIT: Feature Warping for Animation to Illustration video Translation   using GANs</h2><p><strong>Authors:Samet Hicsonmez, Nermin Samet, Fidan Samet, Oguz Bakir, Emre Akbas, Pinar Duygulu</strong></p>
<p>In this paper, we explore a new domain for video-to-video translation. Motivated by the availability of animation movies that are adopted from illustrated books for children, we aim to stylize these videos with the style of the original illustrations. Current state-of-the-art video-to-video translation models rely on having a video sequence or a single style image to stylize an input video. We introduce a new problem for video stylizing where an unordered set of images are used. This is a challenging task for two reasons: i) we do not have the advantage of temporal consistency as in video sequences; ii) it is more difficult to obtain consistent styles for video frames from a set of unordered images compared to using a single image. Most of the video-to-video translation methods are built on an image-to-image translation model, and integrate additional networks such as optical flow, or temporal predictors to capture temporal relations. These additional networks make the model training and inference complicated and slow down the process. To ensure temporal coherency in video-to-video style transfer, we propose a new generator network with feature warping layers which overcomes the limitations of the previous methods. We show the effectiveness of our method on three datasets both qualitatively and quantitatively. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/wait">https://github.com/giddyyupp/wait</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘çš„æ–°é¢†åŸŸã€‚å—å„¿ç«¥æ’ç”»ä¹¦æ”¹ç¼–çš„åŠ¨ç”»ç”µå½±çš„å¯å‘ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™äº›è§†é¢‘é£æ ¼åŒ–ä¸ºåŸå§‹æ’ç”»çš„é£æ ¼ã€‚ç›®å‰æœ€å…ˆè¿›çš„è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘æ¨¡å‹ä¾èµ–äºè§†é¢‘åºåˆ—æˆ–å•ä¸ªæ ·å¼å›¾åƒæ¥å¯¹è¾“å…¥è§†é¢‘è¿›è¡Œé£æ ¼åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†é¢‘é£æ ¼åŒ–çš„æ–°é—®é¢˜ï¼Œå…¶ä¸­ä½¿ç”¨æ— åºå›¾åƒé›†ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒåŸå› æœ‰ä¸¤ç‚¹ï¼šiï¼‰æˆ‘ä»¬æ²¡æœ‰è§†é¢‘åºåˆ—ä¸­çš„æ—¶é—´ä¸€è‡´æ€§ä¼˜åŠ¿ï¼›iiï¼‰ä¸å•å¼ å›¾åƒç›¸æ¯”ï¼Œä»æ— åºå›¾åƒé›†ä¸­ä¸ºè§†é¢‘å¸§è·å–ä¸€è‡´çš„é£æ ¼æ›´åŠ å›°éš¾ã€‚å¤§å¤šæ•°è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘æ–¹æ³•éƒ½æ˜¯å»ºç«‹åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¹¶é›†æˆäº†é¢å¤–çš„ç½‘ç»œï¼Œå¦‚å…‰æµæˆ–æ—¶é—´é¢„æµ‹å™¨æ¥æ•è·æ—¶é—´å…³ç³»ã€‚è¿™äº›é¢å¤–çš„ç½‘ç»œä½¿æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å˜å¾—å¤æ‚ï¼Œå¹¶å‡æ…¢äº†æ•´ä¸ªè¿‡ç¨‹ã€‚ä¸ºäº†ç¡®ä¿è§†é¢‘åˆ°è§†é¢‘é£æ ¼è½¬æ¢çš„æ—¶é—´è¿è´¯æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œå…·æœ‰ç‰¹å¾æ‰­æ›²å±‚ï¼Œå…‹æœäº†ä»¥å‰æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®šæ€§å’Œå®šé‡åœ°å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/giddyyupp/wait%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/giddyyupp/waitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04901v2">PDF</a> Accepted to Neurocomputing</p>
<p><strong>Summary</strong><br>è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘é¢†åŸŸçš„æ–°æ¢ç´¢ï¼Œé’ˆå¯¹åŠ¨ç”»ç”µå½±çš„åŸå§‹æ’å›¾é£æ ¼è¿›è¡Œè§†é¢‘é£æ ¼åŒ–ã€‚è¯¥ç ”ç©¶è§£å†³çš„æ–°é—®é¢˜æ˜¯ä½¿ç”¨æ— åºå›¾åƒé›†è¿›è¡Œè§†é¢‘é£æ ¼åŒ–ï¼Œæå‡ºæ–°çš„ç”Ÿæˆå™¨ç½‘ç»œå…‹æœä¹‹å‰æ–¹æ³•çš„å±€é™ï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°æœ‰æ•ˆæ€§å’Œå®šé‡è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘çš„æ–°é¢†åŸŸï¼Œä¸“æ³¨äºå°†åŠ¨ç”»ç”µå½±çš„æ’å›¾é£æ ¼åº”ç”¨äºè§†é¢‘ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯ä¸»è¦ä¾èµ–äºè§†é¢‘åºåˆ—æˆ–å•å¼ å›¾ç‰‡è¿›è¡Œè§†é¢‘é£æ ¼åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œå³ä½¿ç”¨æ— åºå›¾åƒé›†è¿›è¡Œè§†é¢‘é£æ ¼åŒ–ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>ç¼ºä¹æ—¶é—´è¿è´¯æ€§æ˜¯ä½¿ç”¨æ— åºå›¾åƒé›†è¿›è¡Œè§†é¢‘é£æ ¼åŒ–çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ä¸ä½¿ç”¨å•ä¸€å›¾åƒç›¸æ¯”ï¼Œä»æ— åºå›¾åƒé›†ä¸­ä¸ºè§†é¢‘å¸§è·å–ä¸€è‡´çš„é£æ ¼æ›´åŠ å›°éš¾ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå™¨ç½‘ç»œï¼Œå…·æœ‰ç‰¹å¾æ‰­æ›²å±‚ï¼Œä»¥ç¡®ä¿è§†é¢‘åˆ°è§†é¢‘é£æ ¼è½¬æ¢çš„æ—¶é—´è¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b88c393c605f66ac054071119a81fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-172a6078a1a09470c7350377314a6d04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38e024b754b42a4616acaa6e6a15c697.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Exploring Hallucination of Large Multimodal Models in Video   Understanding Benchmark, Analysis and Mitigation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b9a423886f9f0e59e16c6b1f850b2916.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Show or Tell? Effectively prompting Vision-Language Models for semantic   segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
