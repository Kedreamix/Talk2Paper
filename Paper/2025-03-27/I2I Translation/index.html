<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-27  Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-27-更新"><a href="#2025-03-27-更新" class="headerlink" title="2025-03-27 更新"></a>2025-03-27 更新</h1><h2 id="Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models"><a href="#Unpaired-Object-Level-SAR-to-Optical-Image-Translation-for-Aircraft-with-Keypoints-Guided-Diffusion-Models" class="headerlink" title="Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models"></a>Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models</h2><p><strong>Authors:Ruixi You, Hecheng Jia, Feng Xu</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and high-resolution imaging capabilities but its unique imaging mechanism makes interpretation heavily reliant on expert knowledge, limiting interpretability, especially in complex target tasks. Translating SAR images into optical images is a promising solution to enhance interpretation and support downstream tasks. Most existing research focuses on scene-level translation, with limited work on object-level translation due to the scarcity of paired data and the challenge of accurately preserving contour and texture details. To address these issues, this study proposes a keypoint-guided diffusion model (KeypointDiff) for SAR-to-optical image translation of unpaired aircraft targets. This framework introduces supervision on target class and azimuth angle via keypoints, along with a training strategy for unpaired data. Based on the classifier-free guidance diffusion architecture, a class-angle guidance module (CAGM) is designed to integrate class and angle information into the diffusion generation process. Furthermore, adversarial loss and consistency loss are employed to improve image fidelity and detail quality, tailored for aircraft targets. During sampling, aided by a pre-trained keypoint detector, the model eliminates the requirement for manually labeled class and azimuth information, enabling automated SAR-to-optical translation. Experimental results demonstrate that the proposed method outperforms existing approaches across multiple metrics, providing an efficient and effective solution for object-level SAR-to-optical translation and downstream tasks. Moreover, the method exhibits strong zero-shot generalization to untrained aircraft types with the assistance of the keypoint detector. </p>
<blockquote>
<p>雷达合成孔径（SAR）图像提供了全天候、全天时和高分辨率的成像能力，但其独特的成像机制使得解释工作严重依赖于专业知识，限制了可解释性，特别是在复杂的目标任务中。将SAR图像转换为光学图像是一个提高解释能力并支持下游任务的解决方案。现有的研究大多集中在场景级的翻译上，而针对目标级的翻译工作则因配对数据的稀缺以及准确保留轮廓和纹理细节的挑战而受到限制。为了解决这些问题，本研究提出了一种用于非配对飞机目标SAR到光学图像转换的关键点引导扩散模型（KeypointDiff）。该框架通过关键点引入了对目标类别和方位角的监督，并采用了针对非配对数据的训练策略。基于无分类器引导扩散架构，设计了一个类角引导模块（CAGM），将类别和角度信息集成到扩散生成过程中。此外，还采用了对抗损失和一致性损失来提高图像保真度和细节质量，以适应飞机目标。在采样过程中，借助预训练的关键点检测器，该模型无需手动标注类别和方位信息，即可实现自动化的SAR到光学转换。实验结果表明，所提方法在多指标上优于现有方法，为面向对象的SAR到光学转换及下游任务提供了高效有效的解决方案。此外，借助关键点检测器，该方法对未训练的飞机类型具有较强的零样本泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个基于关键点引导的扩散模型（KeypointDiff），用于SAR到光学图像的配对飞机目标翻译。该研究解决了由于数据配对稀缺和轮廓和纹理细节准确保留的挑战所带来的问题。通过引入目标类别和方位角的关键点监督，结合未配对数据的训练策略，设计了一个无需分类器引导的分类器引导模块（CAGM），将类别和角度信息集成到扩散生成过程中。实验结果表明，该方法在多个指标上优于现有方法，为SAR到光学图像的物体级别翻译和下游任务提供了高效且有效的解决方案。此外，借助预训练的关键点检测器，该方法对未训练的飞机类型具有很强的零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAR成像具有全天候、全天时、高分辨率的成像能力，但解读依赖于专业知识，尤其在复杂目标任务中。</li>
<li>将SAR图像转化为光学图像是提高解读能力的一种有前景的解决方案。</li>
<li>现有研究主要集中在场景级别的翻译，对于物体级别的翻译由于配对数据的稀缺性和准确保留轮廓和纹理细节的挑战而研究有限。</li>
<li>本研究提出了一个基于关键点引导的扩散模型（KeypointDiff）来解决上述问题，尤其针对飞机目标的SAR到光学图像的翻译。</li>
<li>该模型通过引入目标类别和方位角的关键点监督，结合未配对数据的训练策略，提高了图像翻译的准确性和效果。</li>
<li>实验结果表明，该方法在多个评估指标上优于现有技术，并展示了强大的零样本泛化能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6ad34ff18565dbad742fc0c13da1bd9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca778238e964609473c2b1e38f00e6cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9bc955b0864fb2ea0800c9f0986e80c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8567d568dec08834d5dcfcb89b40fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7776725ee57eaf5ae1fd8a22f4cab904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9e32d70831f58caa3361855117ea1e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiffV2IR-Visible-to-Infrared-Diffusion-Model-via-Vision-Language-Understanding"><a href="#DiffV2IR-Visible-to-Infrared-Diffusion-Model-via-Vision-Language-Understanding" class="headerlink" title="DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language   Understanding"></a>DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language   Understanding</h2><p><strong>Authors:Lingyan Ran, Lidong Wang, Guangcong Wang, Peng Wang, Yanning Zhang</strong></p>
<p>The task of translating visible-to-infrared images (V2IR) is inherently challenging due to three main obstacles: 1) achieving semantic-aware translation, 2) managing the diverse wavelength spectrum in infrared imagery, and 3) the scarcity of comprehensive infrared datasets. Current leading methods tend to treat V2IR as a conventional image-to-image synthesis challenge, often overlooking these specific issues. To address this, we introduce DiffV2IR, a novel framework for image translation comprising two key elements: a Progressive Learning Module (PLM) and a Vision-Language Understanding Module (VLUM). PLM features an adaptive diffusion model architecture that leverages multi-stage knowledge learning to infrared transition from full-range to target wavelength. To improve V2IR translation, VLUM incorporates unified Vision-Language Understanding. We also collected a large infrared dataset, IR-500K, which includes 500,000 infrared images compiled by various scenes and objects under various environmental conditions. Through the combination of PLM, VLUM, and the extensive IR-500K dataset, DiffV2IR markedly improves the performance of V2IR. Experiments validate DiffV2IR’s excellence in producing high-quality translations, establishing its efficacy and broad applicability. The code, dataset, and DiffV2IR model will be available at <a target="_blank" rel="noopener" href="https://github.com/LidongWang-26/DiffV2IR">https://github.com/LidongWang-26/DiffV2IR</a>. </p>
<blockquote>
<p>将可见光到红外图像（V2IR）的翻译任务具有内在的挑战性，主要因为以下三个障碍：1）实现语义感知翻译；2）管理红外图像中的多样化波长光谱；3）全面的红外数据集稀缺。当前的主流方法往往将V2IR视为传统的图像到图像合成挑战，经常忽略这些特定问题。为了解决这一问题，我们引入了DiffV2IR，这是一个由两个关键元素组成的新型图像翻译框架：渐进学习模块（PLM）和视觉语言理解模块（VLUM）。PLM采用自适应扩散模型架构，利用多阶段知识学习从全波长范围向目标波长进行红外转换。为了提高V2IR的翻译质量，VLUM结合了统一的视觉语言理解。我们还收集了一个大型红外数据集IR-500K，其中包括50万张在各种环境条件下由不同场景和对象组成的红外图像。通过PLM、VLUM和广泛的IR-500K数据集的结合，DiffV2IR显著提高了V2IR的性能。实验验证了DiffV2IR在产生高质量翻译方面的卓越表现，证明了其有效性和广泛的适用性。代码、数据集和DiffV2IR模型将在<a target="_blank" rel="noopener" href="https://github.com/LidongWang-26/DiffV2IR">https://github.com/LidongWang-26/DiffV2IR</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19012v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://diffv2ir.github.io/">https://diffv2ir.github.io/</a></p>
<p><strong>Summary</strong><br>可见光到红外图像翻译任务面临三大挑战：实现语义感知翻译、管理红外成像的多样波长谱以及缺乏全面的红外数据集。当前主流方法常忽略这些特定问题，将其视为常规图像到图像合成挑战。为解决此问题，我们推出DiffV2IR框架，包含两大关键元素：渐进学习模块和视觉语言理解模块。渐进学习模块采用自适应扩散模型架构，利用多阶段知识学习实现红外转换。视觉语言理解模块则通过统一视觉语言理解提升V2IR翻译。我们还收集了大型红外数据集IR-500K，包含各种场景和对象在不同环境条件下的50万张红外图像。结合渐进学习模块、视觉语言理解模块和丰富的IR-500K数据集，DiffV2IR显著提高V2IR性能。实验验证了DiffV2IR产生高质量翻译的效果，证明了其有效性和广泛适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>可见光到红外图像翻译面临语义感知、波长谱管理和数据集稀缺三大挑战。</li>
<li>当前方法常忽略这些特定挑战，将其视为常规图像转换任务。</li>
<li>DiffV2IR框架包含渐进学习模块和视觉语言理解模块，分别通过自适应扩散模型和多阶段知识学习、统一视觉语言理解来提升V2IR翻译效果。</li>
<li>推出大型红外数据集IR-500K，为V2IR研究提供丰富资源。</li>
<li>结合模块和数据集，DiffV2IR显著提高V2IR性能。</li>
<li>实验证明DiffV2IR产生高质量翻译的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19012">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e0619ff56310f772665ef9fdfd2a47f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-108938161e78560c79974e95a33a36f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f78541ad40f5fbb9e732f857ea4324b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Equivariant-Image-Modeling"><a href="#Equivariant-Image-Modeling" class="headerlink" title="Equivariant Image Modeling"></a>Equivariant Image Modeling</h2><p><strong>Authors:Ruixiao Dong, Mengde Xu, Zigang Geng, Li Li, Han Hu, Shuyang Gu</strong></p>
<p>Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/drx-code/EquivariantModeling">https://github.com/drx-code/EquivariantModeling</a>. </p>
<blockquote>
<p>当前的生成模型，例如自回归和扩散方法，将高维数据分布学习分解成一系列更简单的子任务。然而，在这些子任务的联合优化过程中会出现固有的冲突，现有解决方案在解决这些冲突时要么牺牲效率，要么牺牲可扩展性。我们提出了一种新型的等变图像建模框架，通过利用自然视觉信号的平移不变性，内在地对齐子任务中的优化目标。我们的方法引入了（1）列式令牌化，增强了水平轴上的平移对称性；（2）窗口因果注意力，强制跨位置的上下文关系一致。在256x256分辨率的类条件ImageNet生成上进行评估，我们的方法与最先进的AR模型相比，使用更少的计算资源取得了相当的性能。系统分析表明，增强的等变性减少了任务间的冲突，显著提高了零样本泛化能力，并实现了超长图像合成。这项工作建立了生成模型中任务对齐分解的第一框架，为有效的参数共享和无冲突优化提供了见解。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/drx-code/EquivariantModeling%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/drx-code/EquivariantModeling公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18948v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的等价图像建模框架，用于在生成模型中实现子任务的目标对齐。通过利用自然视觉信号的平移不变性，该框架提高了子任务之间的优化一致性，增强了图像合成的效果并降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前生成模型（如自回归和扩散方法）面临子任务联合优化中的内在冲突问题。</li>
<li>提出的等价图像建模框架利用自然视觉信号的平移不变性，实现子任务优化目标的对齐。</li>
<li>通过列式符号化和窗口因果注意力机制，该框架提高了翻译对称性和上下文关系的一致性。</li>
<li>在高分辨率ImageNet图像生成任务上，该方法性能与最先进的自回归模型相当，但计算资源消耗更少。</li>
<li>增强等价性减少了子任务间的冲突，显著提高了零样本泛化能力，并实现了超长图像合成。</li>
<li>该工作为生成模型中的任务对齐分解建立了首个框架，提供了关于有效参数共享和无冲突优化的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-116dbf247880a9340d38bccf56fd7d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fddb8be8e4d58fc4ae542a5e3acd230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547bc81d4e49ff068abb71daee8300eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8fb1ef56d69040d93508a02fd409461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25e09890fdf1d5ef0904245a2310066f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Blind-structured-illumination-microscopy-via-generalized-Richardson-Lucy-method"><a href="#Blind-structured-illumination-microscopy-via-generalized-Richardson-Lucy-method" class="headerlink" title="Blind structured illumination microscopy via generalized Richardson-Lucy   method"></a>Blind structured illumination microscopy via generalized Richardson-Lucy   method</h2><p><strong>Authors:Valentina Capalbo, Damiana Battaglini, Marialaura Petroni, Giancarlo Ruocco, Marco Leonetti</strong></p>
<p>Structured illumination microscopy (SIM) can achieve a $2\times$ resolution enhancement beyond the classical diffraction limit by employing illumination translations with respect to the object. This method has also been successfully implemented in a <code>blind&#39;&#39; configuration, i.e., with unknown illumination patterns, allowing for more relaxed constraints on the control of illumination delivery. Here, we present a similar blind-SIM approach using a novel super-resolution algorithm that employs a generalized version of the popular Richardson-Lucy algorithm, alongside an optimized and customized optical setup. Both numerical and experimental validations demonstrate that our technique exhibits high noise resilience. Moreover, by implementing random translations instead of </code>ordered’’ ones, noise-related artifacts are reduced. These advancements enable wide-field super-resolved imaging with significantly reduced optical complexity. </p>
<blockquote>
<p>结构光照显微镜（SIM）通过相对于物体进行光照翻译，可以实现超过经典衍射极限的$2\times$分辨率增强。该方法还成功地在“盲”配置中实现，即具有未知照明模式，对照明交付的控制要求更加宽松。在这里，我们提出了一种类似的盲SIM方法，使用一种新型超分辨率算法，该算法使用流行的Richardson-Lucy算法的通用版本，以及优化和定制的光学装置。数值和实验验证均表明我们的技术具有很高的抗噪声性。此外，通过实施随机翻译而不是“有序”翻译，减少了与噪声相关的伪影。这些进步实现了具有显著降低光学复杂性的宽场超分辨率成像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18786v1">PDF</a> 12 pages, 6 figures, including Appendix A and B</p>
<p><strong>Summary</strong></p>
<p>基于结构照明显微镜（SIM）技术，通过对象照明的平移实现超过经典衍射极限的2倍分辨率提升。本文提出一种使用流行Richardson-Lucy算法的通用版本和经过优化和定制的光学设置的新型盲SIM方法。这种方法展现了良好的噪声鲁棒性，并通过实现随机平移而非有序平移，减少了噪声相关伪影。此进步使宽场超分辨率成像的光学复杂性显著降低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>结构照明显微镜（SIM）可通过对象照明的平移实现超过衍射极限的分辨率增强。</li>
<li>提出了基于通用Richardson-Lucy算法的盲SIM方法。</li>
<li>该技术具有良好的噪声鲁棒性。</li>
<li>通过随机平移减少噪声相关伪影。</li>
<li>光学复杂性显著降低的超分辨率成像。</li>
<li>方法可用于宽场成像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18786">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5794f2f96fd0e9c8d8dd3f3e301b09d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96d2a67500cf9498b86a976c35add1c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e111bf9dd87fdb26185e337d85ac21fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-045d5c387e082ef71212052f3a6158fa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation"><a href="#Self-Supervised-Learning-based-on-Transformed-Image-Reconstruction-for-Equivariance-Coherent-Feature-Representation" class="headerlink" title="Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation"></a>Self-Supervised Learning based on Transformed Image Reconstruction for   Equivariance-Coherent Feature Representation</h2><p><strong>Authors:Qin Wang, Benjamin Bruns, Hanno Scharr, Kai Krajsek</strong></p>
<p>The equivariant behaviour of features is essential in many computer vision tasks, yet popular self-supervised learning (SSL) methods tend to constrain equivariance by design. We propose a self-supervised learning approach where the system learns transformations independently by reconstructing images that have undergone previously unseen transformations. Specifically, the model is tasked to reconstruct intermediate transformed images, e.g. translated or rotated images, without prior knowledge of these transformations. This auxiliary task encourages the model to develop equivariance-coherent features without relying on predefined transformation rules. To this end, we apply transformations to the input image, generating an image pair, and then split the extracted features into two sets per image. One set is used with a usual SSL loss encouraging invariance, the other with our loss based on the auxiliary task to reconstruct the intermediate transformed images. Our loss and the SSL loss are linearly combined with weighted terms. Evaluating on synthetic tasks with natural images, our proposed method strongly outperforms all competitors, regardless of whether they are designed to learn equivariance. Furthermore, when trained alongside augmentation-based methods as the invariance tasks, such as iBOT or DINOv2, we successfully learn a balanced combination of invariant and equivariant features. Our approach performs strong on a rich set of realistic computer vision downstream tasks, almost always improving over all baselines. </p>
<blockquote>
<p>特征的等变行为在计算机视觉的许多任务中至关重要，但流行的自监督学习方法（SSL）倾向于通过设计来限制等变性。我们提出了一种自监督学习方法，该方法允许系统通过重建经过先前未见变换的图像来独立地学习变换。具体来说，模型的任务是重建中间变换后的图像，例如平移或旋转图像，而无需事先了解这些变换。这一辅助任务鼓励模型发展等变一致性特征，而不依赖于预先定义的变换规则。为此，我们对输入图像应用变换，生成一对图像，然后将提取的特征按图像分成两组。一组用于使用常规SSL损失来鼓励不变性，另一组用于基于重建中间变换图像的辅助任务的损失。我们的损失和SSL损失通过加权项进行线性组合。在对自然图像进行合成任务的评估中，无论是否设计为学习等变性，我们的方法都强烈优于所有竞争对手。此外，当与基于增强的方法（如iBOT或DINOv2）作为不变性任务一起训练时，我们成功学会了不变特征和等变特征的平衡组合。我们的方法在丰富的现实计算机视觉下游任务中表现强劲，几乎总是优于所有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18753v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究提出了一种新的自监督学习方法，通过独立学习变换来重建经过未见变换的图像，促进模型发展等变相干特征。模型无需预先设定的变换规则，便能重建中间变换图像，如平移或旋转图像。该方法在自然图像合成任务上表现优异，与其他方法相比有显著提高。同时，当与其他增强方法结合作为不变性任务时，该方法能成功学习平衡的不变和等变特征，并在多种真实计算机视觉下游任务上表现强劲。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一种新的自监督学习方法，通过重建经过未见变换的图像来促进模型发展等变相干特征。</li>
<li>模型无需预先设定的变换规则，即可重建中间变换图像。</li>
<li>该方法在合成任务上表现优异，强烈优于所有竞争对手。</li>
<li>当与其他增强方法结合时，该方法能成功学习平衡的不变和等变特征。</li>
<li>方法在多种真实计算机视觉下游任务上表现强劲，总是优于所有基线。</li>
<li>通过应用不同的变换于输入图像，生成图像对，并提取特征进行不同处理来达成学习任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4624e380e737448d093a3f8980418dc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a84161630958b82b2d090cd96b8cce4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-719fb08ef408a26365be538a9207d86d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91a3e6831f344e4761ea4bd13483ba6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-feff8439c69758e551d9d94b3261bb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b667ed4a87e288129c7ae8edbcc9859.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b041a509a2a5f5b42a308a78d2f200dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deda0d46342074f6b8bc70a3a9a09b41.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MotionDiff-Training-free-Zero-shot-Interactive-Motion-Editing-via-Flow-assisted-Multi-view-Diffusion"><a href="#MotionDiff-Training-free-Zero-shot-Interactive-Motion-Editing-via-Flow-assisted-Multi-view-Diffusion" class="headerlink" title="MotionDiff: Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion"></a>MotionDiff: Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion</h2><p><strong>Authors:Yikun Ma, Yiqing Li, Jiawei Wu, Zhi Jin</strong></p>
<p>Generative models have made remarkable advancements and are capable of producing high-quality content. However, performing controllable editing with generative models remains challenging, due to their inherent uncertainty in outputs. This challenge is praticularly pronounced in motion editing, which involves the processing of spatial information. While some physics-based generative methods have attempted to implement motion editing, they typically operate on single-view images with simple motions, such as translation and dragging. These methods struggle to handle complex rotation and stretching motions and ensure multi-view consistency, often necessitating resource-intensive retraining. To address these challenges, we propose MotionDiff, a training-free zero-shot diffusion method that leverages optical flow for complex multi-view motion editing. Specifically, given a static scene, users can interactively select objects of interest to add motion priors. The proposed Point Kinematic Model (PKM) then estimates corresponding multi-view optical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently, these optical flows are utilized to generate multi-view motion results through decoupled motion representation in the Multi-view Motion Diffusion Stage (MMDS). Extensive experiments demonstrate that MotionDiff outperforms other physics-based generative motion editing methods in achieving high-quality multi-view consistent motion results. Notably, MotionDiff does not require retraining, enabling users to conveniently adapt it for various down-stream tasks. </p>
<blockquote>
<p>生成模型已经取得了显著的进步，并能够产生高质量的内容。然而，由于生成模型在输出方面存在固有的不确定性，因此对其进行可控编辑仍然是一个挑战。这一挑战在运动编辑中尤其突出，运动编辑涉及空间信息的处理。虽然一些基于物理的生成方法已经尝试实现运动编辑，但它们通常仅在具有简单运动的单视图图像上运行，例如平移和拖动。这些方法在处理复杂的旋转和伸展运动以及确保多视图一致性方面遇到困难，通常需要资源密集型的重新训练。为了解决这些挑战，我们提出了MotionDiff，这是一种无需训练即可使用的零样本扩散方法，它利用光流进行复杂的多视图运动编辑。具体来说，给定一个静态场景，用户可以交互式地选择感兴趣的对象来添加运动先验。然后，所提出的关键点运动学模型（PKM）在多点流估计阶段（MFES）估计相应的多视图光流。随后，这些光流在多视图运动扩散阶段（MMDS）通过解耦的运动表示来生成多视图运动结果。大量实验表明，MotionDiff在其他基于物理的生成运动编辑方法中表现出色，能够实现高质量的多视图一致运动结果。值得注意的是，MotionDiff不需要重新训练，使用户能够轻松地将其适应各种下游任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17695v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了生成模型在运动编辑方面的挑战，并提出了MotionDiff方法来解决这些问题。MotionDiff是一种无需训练即可实现复杂多视角运动编辑的方法，它通过利用光学流进行运动预测和多视角一致性保持。用户可以为静态场景中的对象添加运动先验，并通过PKM模型估计多视角光学流，然后在多视角运动扩散阶段生成多视角运动结果。该方法无需重新训练，可广泛应用于各种下游任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型在产生高质量内容方面取得了显著进展，但在可控编辑方面仍面临挑战，特别是在处理空间信息的运动编辑方面。</li>
<li>现有物理基础生成方法主要处理简单运动，如平移和拖动，难以处理复杂旋转和拉伸运动，以及确保多视角一致性。</li>
<li>MotionDiff是一种无需训练的零射击扩散方法，可解决这些挑战。</li>
<li>MotionDiff利用光学流进行复杂多视角运动编辑，用户可以为感兴趣的对象添加运动先验。</li>
<li>MotionDiff通过PKM模型估计多视角光学流，并在多视角运动扩散阶段生成多视角运动结果。</li>
<li>无需重新训练，MotionDiff可方便地应用于各种下游任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17695">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d482e62f56cfa4216428062113d0e232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-787c81b4274434a525a0f85a06bc375e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e57beb3a284ef7705c26ef0afa87e8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4715f604a6903c6000dc2b806cecbde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b5926f11bdc29d91e3367aa441afe83.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image"><a href="#Image-as-an-IMU-Estimating-Camera-Motion-from-a-Single-Motion-Blurred-Image" class="headerlink" title="Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image"></a>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred   Image</h2><p><strong>Authors:Jerred Chen, Ronald Clark</strong></p>
<p>In many robotics and VR&#x2F;AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP. </p>
<blockquote>
<p>在机器人技术和虚拟现实&#x2F;增强现实应用的许多场景中，快速的相机运动会产生较高水平的运动模糊，导致现有的相机姿态估计方法失效。在此工作中，我们提出了一种新的框架，它利用运动模糊作为运动估计的丰富线索，而不是将其视为不需要的伪影。我们的方法通过对单个运动模糊图像直接预测密集运动流场和单目深度图来工作。然后，我们在小运动假设下通过解决线性最小二乘问题来恢复相机的瞬时速度。从本质上讲，我们的方法产生了一种类似于IMU的测量值，能够稳健地捕获快速且激烈的相机运动。为了训练我们的模型，我们使用ScanNet++v2构建了具有现实合成运动模糊的大规模数据集，并通过在我们的完全可微分管道上对真实数据进行端到端训练来进一步完善我们的模型。在真实世界基准测试上的广泛评估表明，我们的方法在角速度和线速度估计方面达到了最新水平，超越了MASt3R和COLMAP等方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17358v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jerredchen.github.io/image-as-imu/">https://jerredchen.github.io/image-as-imu/</a></p>
<p><strong>Summary</strong></p>
<p>本摘要针对机器人技术和虚拟现实&#x2F;增强现实领域中的快速相机运动导致的高运动模糊问题，提出了一种新的运动估计框架。该框架利用运动模糊作为丰富的运动估计线索，而非将其视为不需要的伪影。通过预测单张运动模糊图像中的密集运动流场和单眼深度图，并结合小运动假设下的线性最小二乘问题求解瞬时相机速度。本质上，该方法生成了一种IMU式的测量值，能够稳健地捕获快速且剧烈的相机运动。通过大规模合成运动模糊数据集ScanNet++v2和真实数据的端到端训练，该方法在真实世界基准测试中达到了先进的角速度和线速度估计效果，优于MASt3R和COLMAP等方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>针对机器人和VR&#x2F;AR应用中快速相机运动导致的高运动模糊问题，提出了一种新的运动估计框架。</li>
<li>该框架利用运动模糊作为运动估计的线索，不同于以往将其视为不需要的伪影。</li>
<li>通过预测密集运动流场和单眼深度图来估计瞬时相机速度。</li>
<li>瞬时相机速度通过解决小运动假设下的线性最小二乘问题得到。</li>
<li>该方法生成了一种IMU式的测量值，稳健地捕获快速且剧烈的相机运动。</li>
<li>通过大规模合成数据集和真实数据的端到端训练，模型得到了进一步优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78369df8376d9f15ee1cbfd9befd8bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640e7682b4f7e849cdb40071911e0898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33cfb97cb85d5aff11886df2a728eaf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46a3deb70455431abc4a32e2ac770182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e90c3afa34dfaa3572ab8d7ebfb5337.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion embeds an input reference image into arbitrary scenes as described by the text prompts, while exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion features’ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing harmonious fusion of the reference structural information and the textual semantic information. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning, all while substantially outperforming related methods in image quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. </p>
<blockquote>
<p>隐藏图片的视觉错觉是一种有趣的视觉感知现象，其中一张图片被巧妙地融入另一张图片中，观众无法立即察觉。我们基于现成的文本到图像（T2I）扩散模型，提出了一个无需训练的文字引导图像到图像（I2I）转换框架，名为“阶段转移扩散模型”（PTDiffusion），用于合成隐藏艺术。PTDiffusion将输入参考图像嵌入到文本提示所描述的任意场景中，同时显示出参考图像的隐藏视觉线索。我们方法的核心是一个即插即用的相位转移机制，该机制动态且逐步地移植扩散特征的相位谱，从去噪过程中重建参考图像，将其采样为生成的错觉图像，实现参考结构信息和文本语义信息的和谐融合。此外，我们提出了异步相位转移，以实现灵活控制隐藏内容可识别度的程度。我们的方法避免了任何模型的训练和微调，同时在图像质量、文本忠实度、视觉可识别度和上下文自然性方面大大优于相关方法在错觉图片合成方面的表现，这已通过广泛的定性和定量实验得到证明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v3">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于文本引导的图像到图像（I2I）翻译框架——PTDiffusion。通过此框架，可以巧妙地将参考图像嵌入到任意场景中，并按照文本提示生成隐藏的视觉线索。该方法的核心在于一个即时可用的相位转移机制，能够动态地逐步转移扩散特征的相位谱，从而在重建过程中将参考图像融入生成的幻觉图像中，实现参考结构信息和文本语义信息的和谐融合。此外，它还提供了灵活的隐蔽内容可辨识度控制功能。无需进行任何模型训练和微调，该方法在图像质量、文本保真度、视觉辨识度和上下文自然度等方面大大超越了相关方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PTDiffusion是一个基于现有文本到图像（T2I）扩散模型的训练外I2I翻译框架。</li>
<li>它能将参考图像巧妙地嵌入到任意场景中，并生成隐藏的视觉线索。</li>
<li>相位转移机制是该方法的核心，能够动态地逐步转移扩散特征的相位谱。</li>
<li>该方法实现了参考结构信息和文本语义信息的和谐融合。</li>
<li>PTDiffusion提供了灵活的隐蔽内容可辨识度控制功能。</li>
<li>该方法无需进行模型训练和微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b4e5777d13795243fd75cec3d975d28a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b77ea1cae8b7ea5e706e5bbd9b40650b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c357f2ab32379793821f600f5c7f73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97f9cc33a146cafad80fb4946166eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-372d816e0098104ced66081bc2c673c5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Automatic-Evaluation-for-Image-Transcreation"><a href="#Towards-Automatic-Evaluation-for-Image-Transcreation" class="headerlink" title="Towards Automatic Evaluation for Image Transcreation"></a>Towards Automatic Evaluation for Image Transcreation</h2><p><strong>Authors:Simran Khanuja, Vivek Iyer, Claire He, Graham Neubig</strong></p>
<p>Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we identify three critical dimensions of image transcreation: cultural relevance, semantic equivalence and visual similarity, and design our metrics to evaluate systems along these axes. Our results show that proprietary VLMs best identify cultural relevance and semantic equivalence, while vision-encoder representations are adept at measuring visual similarity. Meta-evaluation across 7 countries shows our metrics agree strongly with human ratings, with average segment-level correlations ranging from 0.55-0.87. Finally, through a discussion of the merits and demerits of each metric, we offer a robust framework for automated image transcreation evaluation, grounded in both theoretical foundations and practical application. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-img-transcreation">https://github.com/simran-khanuja/automatic-eval-img-transcreation</a>. </p>
<blockquote>
<p>在传统翻译语音和文本的模式之外，最近人们对自动创建图像以在不同文化中实现视觉内容本地化的兴趣日益浓厚。将这一领域定义为正式的机器学习（ML）问题的尝试受到了缺乏自动评估机制的阻碍，早期的工作完全依赖于人工评估。在本文中，我们试图通过提出一系列受机器翻译（MT）指标启发的自动评估指标来缩小这一差距，这些指标可分为三类：a）基于对象的指标、b）基于嵌入的指标和c）基于视觉语言模型（VLM）的指标。我们借鉴翻译研究理论和现实世界的图像创作实践，确定了图像创作的三个关键维度：文化相关性、语义等值和视觉相似性，并设计了我们的指标来评估这些维度上的系统性能。结果表明，专属的VLM在识别文化相关性和语义等值方面表现最佳，而视觉编码器表示则擅长测量视觉相似性。在7个国家的元评估显示，我们的指标与人类评分高度一致，平均段级相关系数在0.55至0.87之间。最后，通过讨论每个指标的优缺点，我们提供了一个基于理论框架和实践应用的稳健的自动化图像创作评估框架。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-img-transcreation%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/simran-khanuja/automatic-eval-img-transcreation中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13717v3">PDF</a> To be presented at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>本文旨在解决图像跨文化自动翻译评估机制的缺失问题，借鉴机器翻译理论和技术，提出一套针对图像转译效果的自动评估指标。这些指标包括基于对象、嵌入和视觉语言模型的评价方法，涵盖文化相关性、语义等价性和视觉相似性三个关键维度。实验结果显示，所提出的评估指标与人类评价高度一致，为图像转译的自动化评估提供了坚实的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像转译评估问题受到广泛关注，存在需求开发自动评估机制以推进其技术发展。</li>
<li>该研究借鉴机器翻译理论和技术，提出一套基于对象、嵌入和视觉语言模型的自动评估指标。</li>
<li>图像转译评估的关键维度包括文化相关性、语义等价性和视觉相似性。</li>
<li>视觉语言模型能够较好地识别文化相关性和语义等价性。</li>
<li>基于视觉编码器表示的图像相似性测量方法得到了有效的评估结果。</li>
<li>研究进行的跨七国的评估表明自动评估指标与人类评分存在强烈一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-21be0dd3c32790e2ba10c043d1c20c7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ae5c92e42b3b7e87825512d129a77e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba0132b4f1076ffac25ff78434e5967.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b141b1eee7f3cf4539884115ee35437f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8753beb91d227130bf9e7af0242dcc3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Omni6D-Large-Vocabulary-3D-Object-Dataset-for-Category-Level-6D-Object-Pose-Estimation"><a href="#Omni6D-Large-Vocabulary-3D-Object-Dataset-for-Category-Level-6D-Object-Pose-Estimation" class="headerlink" title="Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object   Pose Estimation"></a>Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object   Pose Estimation</h2><p><strong>Authors:Mengchen Zhang, Tong Wu, Tai Wang, Tengfei Wang, Ziwei Liu, Dahua Lin</strong></p>
<p>6D object pose estimation aims at determining an object’s translation, rotation, and scale, typically from a single RGBD image. Recent advancements have expanded this estimation from instance-level to category-level, allowing models to generalize across unseen instances within the same category. However, this generalization is limited by the narrow range of categories covered by existing datasets, such as NOCS, which also tend to overlook common real-world challenges like occlusion. To tackle these challenges, we introduce Omni6D, a comprehensive RGBD dataset featuring a wide range of categories and varied backgrounds, elevating the task to a more realistic context. 1) The dataset comprises an extensive spectrum of 166 categories, 4688 instances adjusted to the canonical pose, and over 0.8 million captures, significantly broadening the scope for evaluation. 2) We introduce a symmetry-aware metric and conduct systematic benchmarks of existing algorithms on Omni6D, offering a thorough exploration of new challenges and insights. 3) Additionally, we propose an effective fine-tuning approach that adapts models from previous datasets to our extensive vocabulary setting. We believe this initiative will pave the way for new insights and substantial progress in both the industrial and academic fields, pushing forward the boundaries of general 6D pose estimation. </p>
<blockquote>
<p>6D物体姿态估计旨在确定物体的平移、旋转和尺度，通常是从单一的RGBD图像出发。近期的发展已经将此估计从实例层面扩展到类别层面，使得模型能够在同一类别中泛化到未见过的实例。然而，这种泛化受限于现有数据集覆盖的类别范围较窄，如NOCS数据集，它们还往往忽视遮挡等常见的现实世界挑战。为了应对这些挑战，我们引入了Omni6D，这是一个综合RGBD数据集，涵盖了广泛的类别和多样化的背景，将任务提升到一个更现实的语境中。1）该数据集包含了166个类别的广泛光谱、调整为标准姿态的4688个实例以及超过80万个捕获对象，显著扩大了评估范围。2）我们引入了一种对称感知度量标准，并在Omni6D上对现有算法进行了系统基准测试，提供了对新挑战和见解的深入探讨。3）此外，我们还提出了一种有效的微调方法，该方法使模型能够适应我们广泛的词汇设置。我们相信这一举措将为工业和学术界带来新的见解和实质性进展，推动通用6D姿态估计的边界不断向前发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18261v3">PDF</a> ECCV 2024 (poster). Github page: <a target="_blank" rel="noopener" href="https://github.com/3DTopia/Omni6D">https://github.com/3DTopia/Omni6D</a></p>
<p><strong>Summary</strong></p>
<p>Omni6D数据集解决了现有RGBD数据集如NOCS存在的局限性，包括类别范围狭窄和忽略真实世界挑战如遮挡问题。Omni6D包含广泛的166个类别，大量实例和调整至规范姿态的捕捉，极大扩展了评估范围。引入对称感知度量并对现有算法进行系统评估，为新的挑战和见解提供全面探索。此外，提出有效的微调方法，使模型适应广泛的词汇设置。此倡议将推动工业和学术界对通用6D姿态估计的深入研究和进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni6D是一个全面的RGBD数据集，包含广泛的类别和多样化的背景，使6D对象姿态估计任务更加贴近真实场景。</li>
<li>数据集涵盖大量实例和调整至规范姿态的捕捉，显著扩大了评估范围。</li>
<li>引入对称感知度量，对现有算法进行系统评估，为新的挑战和见解提供全面探索。</li>
<li>Omni6D解决了现有数据集忽略真实世界挑战（如遮挡）的问题。</li>
<li>提出有效的微调方法，使模型能够适应广泛的词汇设置。</li>
<li>Omni6D的引入将推动工业和学术界在6D姿态估计方面的深入研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18261">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-52a82c318ddbaa8e12ceebb9d15c92d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42b2537cb0a4b34c5a9fad40119b3006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements"><a href="#Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements" class="headerlink" title="Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements"></a>Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements</h2><p><strong>Authors:Jipeng Yan, Qingyuan Tan, Shusei Kawara, Jingwen Zhu, Bingxue Wang, Matthieu Toulemonde, Honghai Liu, Ying Tan, Meng-Xing Tang</strong></p>
<p>Super-Resolution Ultrasound (SRUS) imaging through localising and tracking microbubbles, also known as Ultrasound Localisation Microscopy (ULM), has demonstrated significant potential for reconstructing microvasculature and flows with sub-diffraction resolution in clinical diagnostics. However, imaging organs with large tissue movements, such as those caused by respiration, presents substantial challenges. Existing methods often require breath holding to maintain accumulation accuracy, which limits data acquisition time and ULM image saturation. To improve image quality in the presence of large tissue movements, this study introduces an approach integrating high-frame-rate ultrasound with online precise robotic probe control. Tested on a microvasculature phantom with translation motions up to 20 mm, twice the aperture size of the matrix array used, our method achieved real-time tracking of the moving phantom and imaging volume rate at 85 Hz, keeping majority of the target volume in the imaging field of view. ULM images of the moving cross channels in the phantom were successfully reconstructed in post-processing, demonstrating the feasibility of super-resolution imaging under large tissue motions. This represents a significant step towards ULM imaging of organs with large motion. </p>
<blockquote>
<p>超声定位成像技术（SRUS）通过定位和追踪微泡，也被称为超声定位显微镜（ULM），在临床诊断中显示出重建具有亚衍射分辨率的微血管和血流的显著潜力。然而，对于存在由呼吸等引起的大组织运动的器官成像，现有的方法面临巨大的挑战。现有方法通常需要屏住呼吸以保持积累准确性，这限制了数据采集时间和ULM图像的饱和度。为了提高大组织运动下的图像质量，本研究引入了一种将高帧率超声与在线精密机械探针控制相结合的方法。我们在具有高达20毫米平移运动的微血管成像幻影上进行了测试，这一数值为所使用的矩阵阵列孔径尺寸的两倍。我们的方法实现了对移动幻影的实时跟踪和成像体积率达到了85赫兹，使大部分目标体积保持在成像视野内。在后期制作中成功重建了移动交叉通道幻影的ULM图像，证明了在大组织运动下进行超分辨率成像的可行性。这标志着对具有大运动的器官进行ULM成像迈出了重要的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11391v3">PDF</a> </p>
<p><strong>Summary</strong><br>SRUS成像通过定位追踪微泡技术，即超声定位显微镜（ULM），在临床诊断中重建具有亚衍射分辨率的微血管和组织流方面显示出巨大潜力。然而，在存在大组织运动的情况下成像，如呼吸引起的运动，存在很大挑战。为改善图像质量，本研究将高帧率超声与在线精密机械探针控制相结合。在具有达矩阵阵列孔径尺寸两倍的位移运动的微血管模型测试中，我们的方法实现了对移动模型的实时跟踪，成像体积率高达每秒更新画面八十五次以上，且将多数目标体积保持在一个可观察范围中。成功重建了移动交叉通道模型中的ULM图像，证明了在大组织运动下进行超分辨率成像的可行性。这标志着对运动器官进行ULM成像的重要进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超级分辨率超声成像通过定位追踪微泡技术（ULM）在临床诊断中具有重建微血管的潜力。</li>
<li>存在大组织运动时（如呼吸引起的运动），SRUS成像面临挑战。</li>
<li>研究结合了高帧率超声和在线精密机械探针控制来改进图像质量。</li>
<li>方法实现了对移动模型的实时跟踪和较高的成像体积率。</li>
<li>成功重建了移动交叉通道模型中的ULM图像。</li>
<li>该方法代表了在大组织运动下进行SRUS成像的重要进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9a6cac96c931c7569cfe7376c7e9ecf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7e286a2006f4efcd1c94be7525bde2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4afe89cecaa57e9b26794d5aba299201.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs"><a href="#WAIT-Feature-Warping-for-Animation-to-Illustration-video-Translation-using-GANs" class="headerlink" title="WAIT: Feature Warping for Animation to Illustration video Translation   using GANs"></a>WAIT: Feature Warping for Animation to Illustration video Translation   using GANs</h2><p><strong>Authors:Samet Hicsonmez, Nermin Samet, Fidan Samet, Oguz Bakir, Emre Akbas, Pinar Duygulu</strong></p>
<p>In this paper, we explore a new domain for video-to-video translation. Motivated by the availability of animation movies that are adopted from illustrated books for children, we aim to stylize these videos with the style of the original illustrations. Current state-of-the-art video-to-video translation models rely on having a video sequence or a single style image to stylize an input video. We introduce a new problem for video stylizing where an unordered set of images are used. This is a challenging task for two reasons: i) we do not have the advantage of temporal consistency as in video sequences; ii) it is more difficult to obtain consistent styles for video frames from a set of unordered images compared to using a single image. Most of the video-to-video translation methods are built on an image-to-image translation model, and integrate additional networks such as optical flow, or temporal predictors to capture temporal relations. These additional networks make the model training and inference complicated and slow down the process. To ensure temporal coherency in video-to-video style transfer, we propose a new generator network with feature warping layers which overcomes the limitations of the previous methods. We show the effectiveness of our method on three datasets both qualitatively and quantitatively. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/wait">https://github.com/giddyyupp/wait</a>. </p>
<blockquote>
<p>本文中，我们探索了视频到视频翻译的新领域。受儿童插画书改编的动画电影的启发，我们的目标是将这些视频风格化为原始插画的风格。目前最先进的视频到视频翻译模型依赖于视频序列或单个样式图像来对输入视频进行风格化。我们引入了视频风格化的新问题，其中使用无序图像集。这是一个具有挑战性的任务，原因有两点：i）我们没有视频序列中的时间一致性优势；ii）与单张图像相比，从无序图像集中为视频帧获取一致的风格更加困难。大多数视频到视频翻译方法都是建立在图像到图像翻译模型的基础上，并集成了额外的网络，如光流或时间预测器来捕获时间关系。这些额外的网络使模型训练和推理变得复杂，并减慢了整个过程。为了确保视频到视频风格转换的时间连贯性，我们提出了一种新的生成器网络，该网络具有特征扭曲层，克服了以前方法的局限性。我们在三个数据集上定性和定量地展示了我们的方法的有效性。代码和预训练模型可在<a target="_blank" rel="noopener" href="https://github.com/giddyyupp/wait%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/giddyyupp/wait上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04901v2">PDF</a> Accepted to Neurocomputing</p>
<p><strong>Summary</strong><br>视频到视频翻译领域的新探索，针对动画电影的原始插图风格进行视频风格化。该研究解决的新问题是使用无序图像集进行视频风格化，提出新的生成器网络克服之前方法的局限，在三个数据集上实现有效性和定量评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究探索了视频到视频翻译的新领域，专注于将动画电影的插图风格应用于视频。</li>
<li>现有技术主要依赖于视频序列或单张图片进行视频风格化。</li>
<li>提出了一种新的任务，即使用无序图像集进行视频风格化，这是一个具有挑战性的任务。</li>
<li>缺乏时间连贯性是使用无序图像集进行视频风格化的主要挑战之一。</li>
<li>与使用单一图像相比，从无序图像集中为视频帧获取一致的风格更加困难。</li>
<li>研究提出了一种新的生成器网络，具有特征扭曲层，以确保视频到视频风格转换的时间连贯性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04901">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b88c393c605f66ac054071119a81fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-172a6078a1a09470c7350377314a6d04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38e024b754b42a4616acaa6e6a15c697.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-27  Exploring Hallucination of Large Multimodal Models in Video   Understanding Benchmark, Analysis and Mitigation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b9a423886f9f0e59e16c6b1f850b2916.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-27  Show or Tell? Effectively prompting Vision-Language Models for semantic   segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
