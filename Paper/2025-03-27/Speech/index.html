<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  AudCast Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a874755b58c9b5a52e0a2843afe763b1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="AudCast-Audio-Driven-Human-Video-Generation-by-Cascaded-Diffusion-Transformers"><a href="#AudCast-Audio-Driven-Human-Video-Generation-by-Cascaded-Diffusion-Transformers" class="headerlink" title="AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers"></a>AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers</h2><p><strong>Authors:Jiazhi Guan, Kaisiyuan Wang, Zhiliang Xu, Quanwei Yang, Yasheng Sun, Shengyi He, Borong Liang, Yukang Cao, Yingying Li, Haocheng Feng, Errui Ding, Jingdong Wang, Youjian Zhao, Hang Zhou, Ziwei Liu</strong></p>
<p>Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details. Resources can be found at <a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast">https://guanjz20.github.io/projects/AudCast</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆå–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é©±åŠ¨é¢éƒ¨è¿åŠ¨ä¸Šï¼Œå¯¼è‡´å¤´éƒ¨å’Œèº«ä½“çš„åŠ¨æ€éè¿è´¯ã€‚å±•æœ›æœªæ¥ï¼Œå¯¹äºæ ¹æ®ç»™å®šéŸ³é¢‘ç”Ÿæˆå…¼å…·ç²¾ç¡®å”‡åŒæ­¥å’Œç²¾ç»†éšè®²è¯æ‰‹åŠ¿çš„å®Œæ•´äººç±»è§†é¢‘ï¼Œæ—¢æ˜¯å¯å–çš„ä¹Ÿæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudCastï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨çº§è”æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTsï¼‰èŒƒå¼çš„ä¸€èˆ¬åŒ–éŸ³é¢‘é©±åŠ¨äººç±»è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåŸºäºå‚è€ƒå›¾åƒå’Œç»™å®šéŸ³é¢‘åˆæˆå®Œæ•´çš„äººç±»è§†é¢‘ã€‚1ï¼‰é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§éŸ³é¢‘è°ƒèŠ‚çš„å…¨èº«äººç±»DiTæ¶æ„ï¼Œå¯ç›´æ¥é©±åŠ¨ä»»ä½•äººä½“è¿åŠ¨ï¼Œå…·æœ‰ç”ŸåŠ¨çš„æ‰‹åŠ¿åŠ¨æ€ã€‚2ï¼‰ç„¶åï¼Œä¸ºäº†æé«˜æ‰‹å’Œè„¸çš„ç»†èŠ‚ï¼ˆè¿™æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„éš¾ä»¥å¤„ç†çš„éƒ¨åˆ†ï¼‰ï¼ŒåŒºåŸŸç»†åŒ–DiTåˆ©ç”¨åŒºåŸŸ3Dæ‹Ÿåˆä½œä¸ºæ¡¥æ¢æ¥é‡å¡‘ä¿¡å·ï¼Œäº§ç”Ÿæœ€ç»ˆç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ—¶é—´è¿è´¯æ€§å’Œç²¾ç»†é¢éƒ¨åŠæ‰‹éƒ¨ç»†èŠ‚çš„å¿ å®éŸ³é¢‘é©±åŠ¨å®Œæ•´äººç±»è§†é¢‘ã€‚èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast%E6%89%BE%E5%88%B0%E3%80%82">https://guanjz20.github.io/projects/AudCastæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19824v1">PDF</a> Accepted to IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR), 2025. Project page:   <a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast">https://guanjz20.github.io/projects/AudCast</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸€ç§åä¸ºAudCastçš„éŸ³é¢‘é©±åŠ¨äººä½“è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨çº§è”æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰èŒƒå¼ï¼ŒåŸºäºå‚è€ƒå›¾åƒå’Œç»™å®šéŸ³é¢‘åˆæˆå…¨èº«äººä½“è§†é¢‘ã€‚è¯¥æ¡†æ¶èƒ½é©±åŠ¨ä»»ä½•äººä½“åŠ¨ä½œï¼Œå…·æœ‰ç”ŸåŠ¨çš„æ‰‹åŠ¿åŠ¨æ€ï¼Œå¹¶åœ¨æ‰‹å’Œé¢éƒ¨ç»†èŠ‚ä¸Šè¿›è¡Œäº†ä¼˜åŒ–å¤„ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudCastæ˜¯ä¸€ä¸ªéŸ³é¢‘é©±åŠ¨çš„äººä½“è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥åˆæˆå…¨èº«äººä½“è§†é¢‘ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨äº†çº§è”æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰èŒƒå¼ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªéŸ³é¢‘æ¡ä»¶ä¸‹çš„å…¨èº«äººä½“DiTæ¶æ„ï¼Œèƒ½ç›´æ¥é©±åŠ¨ä»»ä½•äººä½“åŠ¨ä½œï¼ŒåŒ…æ‹¬æ‰‹åŠ¿åŠ¨æ€ã€‚</li>
<li>ä¸ºäº†å¢å¼ºæ‰‹å’Œé¢éƒ¨çš„ç»†èŠ‚å¤„ç†ï¼Œé‡‡ç”¨äº†åŒºåŸŸç»†åŒ–DiTï¼Œåˆ©ç”¨åŒºåŸŸ3Dæ‹Ÿåˆä½œä¸ºæ¡¥æ¢æ¥ä¼˜åŒ–ä¿¡å·ã€‚</li>
<li>è¯¥æ¡†æ¶ç”Ÿæˆçš„è§†é¢‘å…·æœ‰é«˜ä¿çœŸåº¦ï¼Œå¹¶ä¸”å…·æœ‰æ—¶é—´è¿è´¯æ€§å’Œç²¾ç»†çš„é¢éƒ¨å’Œæ‰‹éƒ¨ç»†èŠ‚ã€‚</li>
<li>è¯¥æ¡†æ¶çš„èµ„æºå’Œå®éªŒç»“æœå¯ä»¥åœ¨æŒ‡å®šçš„ç½‘é¡µä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7baaede395011d688eb195c30caa0a6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e5328a993ab8d2d505bf0057de099e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1135c624827926e0d1490721f0ca8c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-700425759607ea60e7537372b8806fde.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Boosting-the-Transferability-of-Audio-Adversarial-Examples-with-Acoustic-Representation-Optimization"><a href="#Boosting-the-Transferability-of-Audio-Adversarial-Examples-with-Acoustic-Representation-Optimization" class="headerlink" title="Boosting the Transferability of Audio Adversarial Examples with Acoustic   Representation Optimization"></a>Boosting the Transferability of Audio Adversarial Examples with Acoustic   Representation Optimization</h2><p><strong>Authors:Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, Jie Hao</strong></p>
<p>With the widespread application of automatic speech recognition (ASR) systems, their vulnerability to adversarial attacks has been extensively studied. However, most existing adversarial examples are generated on specific individual models, resulting in a lack of transferability. In real-world scenarios, attackers often cannot access detailed information about the target model, making query-based attacks unfeasible. To address this challenge, we propose a technique called Acoustic Representation Optimization that aligns adversarial perturbations with low-level acoustic characteristics derived from speech representation models. Rather than relying on model-specific, higher-layer abstractions, our approach leverages fundamental acoustic representations that remain consistent across diverse ASR architectures. By enforcing an acoustic representation loss to guide perturbations toward these robust, lower-level representations, we enhance the cross-model transferability of adversarial examples without degrading audio quality. Our method is plug-and-play and can be integrated with any existing attack methods. We evaluate our approach on three modern ASR models, and the experimental results demonstrate that our method significantly improves the transferability of adversarial examples generated by previous methods while preserving the audio quality. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬æ˜“å—å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“å·²å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å¯¹æŠ—æ€§ç¤ºä¾‹ä»…åœ¨ç‰¹å®šå•ä¸ªæ¨¡å‹ä¸Šç”Ÿæˆï¼Œå¯¼è‡´ç¼ºä¹å¯è½¬ç§»æ€§ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæ”»å‡»è€…å¾€å¾€æ— æ³•è·å¾—å…³äºç›®æ ‡æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¿™ä½¿å¾—åŸºäºæŸ¥è¯¢çš„æ”»å‡»å˜å¾—ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå£°å­¦è¡¨ç¤ºä¼˜åŒ–çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†å¯¹æˆ˜æ‰°åŠ¨ä¸ä»è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ä¸­å¾—å‡ºçš„ä½çº§å£°å­¦ç‰¹å¾å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹¶ä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹çš„è¾ƒé«˜å±‚æ¬¡æŠ½è±¡ï¼Œè€Œæ˜¯åˆ©ç”¨åŸºæœ¬å£°å­¦è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå½¢å¼åœ¨å¤šç§ASRæ¶æ„ä¸­ä¿æŒä¸€è‡´ã€‚é€šè¿‡å¼ºåˆ¶å®æ–½å£°å­¦è¡¨ç¤ºæŸå¤±æ¥å¼•å¯¼æ‰°åŠ¨æœå‘è¿™äº›ç¨³å¥çš„è¾ƒä½çº§åˆ«è¡¨ç¤ºï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸é™ä½éŸ³é¢‘è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºå¯¹æŠ—æ€§ç¤ºä¾‹çš„è·¨æ¨¡å‹å¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å³æ’å³ç”¨ï¼Œå¯ä»¥ä¸ä»»ä½•ç°æœ‰çš„æ”»å‡»æ–¹æ³•ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç°ä»£ASRæ¨¡å‹ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒéŸ³é¢‘è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ä¹‹å‰æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹çš„å¯è½¬ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19591v1">PDF</a> Accepted to ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„è„†å¼±æ€§ï¼ŒæŒ‡å‡ºå…¶æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„é—®é¢˜ã€‚ç°æœ‰çš„å¯¹æŠ—æ€§ç¤ºä¾‹å¤§å¤šé’ˆå¯¹ç‰¹å®šæ¨¡å‹ç”Ÿæˆï¼Œç¼ºä¹è¿ç§»æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºå£°å­¦è¡¨ç¤ºä¼˜åŒ–çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†å¯¹æŠ—æ€§æ‰°åŠ¨ä¸åŸºäºè¯­éŸ³è¡¨ç¤ºæ¨¡å‹çš„ä½çº§å£°å­¦ç‰¹æ€§ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨è·¨ä¸åŒASRæ¶æ„ä¿æŒä¸€è‡´çš„åº•å±‚å£°å­¦è¡¨ç¤ºï¼Œé€šè¿‡å¼ºåˆ¶å®æ–½å£°å­¦è¡¨ç¤ºæŸå¤±æ¥å¼•å¯¼æ‰°åŠ¨æœå‘è¿™äº›ç¨³å¥çš„åº•å±‚è¡¨ç¤ºï¼Œä»è€Œæé«˜å¯¹æŠ—æ€§ç¤ºä¾‹çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ï¼Œä¸”ä¸å½±å“éŸ³é¢‘è´¨é‡ã€‚æ­¤æ–¹æ³•å¯ä¸ç°æœ‰çš„æ”»å‡»æ–¹æ³•ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å…ˆå‰æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹çš„è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ˜“å—å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li>
<li>ç°æœ‰å¯¹æŠ—æ€§ç¤ºä¾‹å¤§å¤šé’ˆå¯¹ç‰¹å®šæ¨¡å‹ç”Ÿæˆï¼Œç¼ºä¹è¿ç§»æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºå£°å­¦è¡¨ç¤ºä¼˜åŒ–çš„æŠ€æœ¯ï¼Œç»“åˆå¯¹æŠ—æ€§æ‰°åŠ¨ä¸ä½çº§å£°å­¦ç‰¹æ€§ã€‚</li>
<li>åˆ©ç”¨è·¨ä¸åŒASRæ¶æ„ä¿æŒä¸€è‡´çš„åº•å±‚å£°å­¦è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å¼ºåˆ¶å®æ–½å£°å­¦è¡¨ç¤ºæŸå¤±ï¼Œæé«˜å¯¹æŠ—æ€§ç¤ºä¾‹çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯é›†æˆåˆ°ä»»ä½•ç°æœ‰çš„æ”»å‡»æ–¹æ³•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64986358c83685e44d7523086c0f1b92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c785e490e3ec66540ca7de29f2b89178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5d2f59bcdb477bdd45b387d516a24ac.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MVPortrait-Text-Guided-Motion-and-Emotion-Control-for-Multi-view-Vivid-Portrait-Animation"><a href="#MVPortrait-Text-Guided-Motion-and-Emotion-Control-for-Multi-view-Vivid-Portrait-Animation" class="headerlink" title="MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid   Portrait Animation"></a>MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid   Portrait Animation</h2><p><strong>Authors:Yukang Lin, Hokit Fung, Jianjin Xu, Zeping Ren, Adela S. M. Lau, Guosheng Yin, Xiu Li</strong></p>
<p>Recent portrait animation methods have made significant strides in generating realistic lip synchronization. However, they often lack explicit control over head movements and facial expressions, and cannot produce videos from multiple viewpoints, resulting in less controllable and expressive animations. Moreover, text-guided portrait animation remains underexplored, despite its user-friendly nature. We present a novel two-stage text-guided framework, MVPortrait (Multi-view Vivid Portrait), to generate expressive multi-view portrait animations that faithfully capture the described motion and emotion. MVPortrait is the first to introduce FLAME as an intermediate representation, effectively embedding facial movements, expressions, and view transformations within its parameter space. In the first stage, we separately train the FLAME motion and emotion diffusion models based on text input. In the second stage, we train a multi-view video generation model conditioned on a reference portrait image and multi-view FLAME rendering sequences from the first stage. Experimental results exhibit that MVPortrait outperforms existing methods in terms of motion and emotion control, as well as view consistency. Furthermore, by leveraging FLAME as a bridge, MVPortrait becomes the first controllable portrait animation framework that is compatible with text, speech, and video as driving signals. </p>
<blockquote>
<p>æœ€è¿‘çš„è‚–åƒåŠ¨ç”»æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„å”‡åŒæ­¥æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹å¯¹å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„æ˜ç¡®æ§åˆ¶ï¼Œå¹¶ä¸”ä¸èƒ½ä»å¤šä¸ªè§†è§’ç”Ÿæˆè§†é¢‘ï¼Œå¯¼è‡´åŠ¨ç”»çš„å¯æ§æ€§å’Œè¡¨ç°åŠ›è¾ƒå·®ã€‚å°½ç®¡æ–‡æœ¬å¼•å¯¼çš„è‚–åƒåŠ¨ç”»å› å…¶ç”¨æˆ·å‹å¥½æ€§è€Œå—åˆ°æ¬¢è¿ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ä¸ªè¢«å¿½ç•¥çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼ä¸¤é˜¶æ®µæ¡†æ¶â€”â€”MVPortraitï¼ˆå¤šè§†è§’ç”ŸåŠ¨è‚–åƒï¼‰ï¼Œä»¥ç”Ÿæˆè¡¨ç°ä¸°å¯Œçš„å¤šè§†è§’è‚–åƒåŠ¨ç”»ï¼Œå¿ å®å‘ˆç°æè¿°çš„åŠ¨ä½œå’Œæƒ…æ„Ÿã€‚MVPortraité¦–æ¬¡å¼•å…¥FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°åœ¨å…¶å‚æ•°ç©ºé—´ä¸­åµŒå…¥é¢éƒ¨åŠ¨ä½œã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬è¾“å…¥åˆ†åˆ«è®­ç»ƒFLAMEåŠ¨ä½œå’Œæƒ…æ„Ÿæ‰©æ•£æ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªä»¥å‚è€ƒè‚–åƒå›¾åƒå’Œç¬¬ä¸€é˜¶æ®µçš„å¤šè§†è§’FLAMEæ¸²æŸ“åºåˆ—ä¸ºæ¡ä»¶çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMVPortraitåœ¨åŠ¨ä½œã€æƒ…æ„Ÿæ§åˆ¶å’Œè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨FLAMEä½œä¸ºæ¡¥æ¢ï¼ŒMVPortraitæˆä¸ºç¬¬ä¸€ä¸ªå¯æ§çš„è‚–åƒåŠ¨ç”»æ¡†æ¶ï¼Œå¯ä¸æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘ä½œä¸ºé©±åŠ¨ä¿¡å·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19383v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°çš„äººåƒåŠ¨ç”»æ–¹æ³•åœ¨ç”Ÿæˆé€¼çœŸçš„å”‡åŒæ­¥æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸å¯¹å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„æ§åˆ¶ä¸æ˜ç¡®ï¼Œæ— æ³•ä»å¤šä¸ªè§†è§’ç”Ÿæˆè§†é¢‘ï¼Œå¯¼è‡´åŠ¨ç”»çš„å¯æ§æ€§å’Œè¡¨ç°åŠ›è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼çš„ä¸¤é˜¶æ®µæ¡†æ¶MVPortraitï¼ˆå¤šè§†è§’ç”ŸåŠ¨äººåƒï¼‰ï¼Œç”¨äºç”Ÿæˆè¡¨è¾¾åŠ›å¼ºçš„å¤šè§†è§’äººåƒåŠ¨ç”»ï¼Œå¿ å®æ•æ‰æè¿°çš„åŠ¨ä½œå’Œæƒ…æ„Ÿã€‚MVPortraité¦–æ¬¡å¼•å…¥FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°åœ¨å…¶å‚æ•°ç©ºé—´ä¸­åµŒå…¥é¢éƒ¨åŠ¨ä½œã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬è¾“å…¥åˆ†åˆ«è®­ç»ƒFLAMEè¿åŠ¨å’Œæƒ…æ„Ÿæ‰©æ•£æ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤šè§†è§’è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥å‚è€ƒäººåƒå›¾åƒå’Œç¬¬ä¸€é˜¶æ®µçš„å¤šè§†è§’FLAMEæ¸²æŸ“åºåˆ—ä¸ºæ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMVPortraitåœ¨åŠ¨ä½œå’Œæƒ…æ„Ÿæ§åˆ¶ä»¥åŠè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©FLAMEä½œä¸ºæ¡¥æ¢ï¼ŒMVPortraitæˆä¸ºé¦–ä¸ªå…¼å®¹æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘çš„é©±åŠ¨ä¿¡å·çš„å¯æ§åˆ¶äººåƒåŠ¨ç”»æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°äººåƒåŠ¨ç”»æ–¹æ³•åœ¨å”‡åŒæ­¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹å¤´éƒ¨çš„è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„æ˜ç¡®æ§åˆ¶ã€‚</li>
<li>MVPortraitæ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šè§†è§’äººåƒåŠ¨ç”»ï¼Œå¹¶å¿ å®æ•æ‰åŠ¨ä½œå’Œæƒ…æ„Ÿã€‚</li>
<li>MVPortraité¦–æ¬¡å¼•å…¥FLAMEä½œä¸ºä¸­é—´è¡¨ç¤ºï¼ŒåµŒå…¥é¢éƒ¨åŠ¨ä½œã€è¡¨æƒ…å’Œè§†è§’è½¬æ¢ã€‚</li>
<li>MVPortraité€šè¿‡ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«è®­ç»ƒè¿åŠ¨å’Œæƒ…æ„Ÿæ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºæ–‡æœ¬è¾“å…¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>MVPortraitèƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’è§†é¢‘ï¼Œä»å¤šä¸ªè§’åº¦å±•ç¤ºäººåƒåŠ¨ç”»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMVPortraitåœ¨åŠ¨ä½œå’Œæƒ…æ„Ÿæ§åˆ¶ä»¥åŠè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd660a5d22e882d2115fcdd4b112ab8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e888d4aa15441baf8bd9c46652036b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca20a3d3fb5284e9d4538186569ed9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-749a8d01bf6be1d450c42159bff7c75f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unifying-EEG-and-Speech-for-Emotion-Recognition-A-Two-Step-Joint-Learning-Framework-for-Handling-Missing-EEG-Data-During-Inference"><a href="#Unifying-EEG-and-Speech-for-Emotion-Recognition-A-Two-Step-Joint-Learning-Framework-for-Handling-Missing-EEG-Data-During-Inference" class="headerlink" title="Unifying EEG and Speech for Emotion Recognition: A Two-Step Joint   Learning Framework for Handling Missing EEG Data During Inference"></a>Unifying EEG and Speech for Emotion Recognition: A Two-Step Joint   Learning Framework for Handling Missing EEG Data During Inference</h2><p><strong>Authors:Upasana Tiwari, Rupayan Chakraborty, Sunil Kumar Kopparapu</strong></p>
<p>Computer interfaces are advancing towards using multi-modalities to enable better human-computer interactions. The use of automatic emotion recognition (AER) can make the interactions natural and meaningful thereby enhancing the user experience. Though speech is the most direct and intuitive modality for AER, it is not reliable because it can be intentionally faked by humans. On the other hand, physiological modalities like EEG, are more reliable and impossible to fake. However, use of EEG is infeasible for realistic scenarios usage because of the need for specialized recording setup. In this paper, one of our primary aims is to ride on the reliability of the EEG modality to facilitate robust AER on the speech modality. Our approach uses both the modalities during training to reliably identify emotion at the time of inference, even in the absence of the more reliable EEG modality. We propose, a two-step joint multi-modal learning approach (JMML) that exploits both the intra- and inter- modal characteristics to construct emotion embeddings that enrich the performance of AER. In the first step, using JEC-SSL, intra-modal learning is done independently on the individual modalities. This is followed by an inter-modal learning using the proposed extended variant of deep canonically correlated cross-modal autoencoder (E-DCC-CAE). The approach learns the joint properties of both the modalities by mapping them into a common representation space, such that the modalities are maximally correlated. These emotion embeddings, hold properties of both the modalities there by enhancing the performance of ML classifier used for AER. Experimental results show the efficacy of the proposed approach. To best of our knowledge, this is the first attempt to combine speech and EEG with joint multi-modal learning approach for reliable AER. </p>
<blockquote>
<p>éšç€è®¡ç®—æœºæ¥å£å‘å¤šæ¨¡æ€äº¤äº’æ–¹å¼å‘å±•ï¼Œä¸ºäº†æ›´å¥½åœ°å®ç°äººæœºäº¤äº’ï¼Œè‡ªåŠ¨æƒ…ç»ªè¯†åˆ«ï¼ˆAERï¼‰çš„ä½¿ç”¨å˜å¾—è‡³å…³é‡è¦ã€‚è™½ç„¶è¯­éŸ³æ˜¯AERä¸­æœ€ç›´æ¥å’Œç›´è§‚çš„æ–¹å¼ï¼Œä½†å®ƒå¹¶ä¸å¯é ï¼Œå› ä¸ºäººç±»å¯èƒ½ä¼šæ•…æ„ä¼ªè£…è‡ªå·±çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚å¦ä¸€æ–¹é¢ï¼Œåƒè„‘ç”µå›¾ï¼ˆEEGï¼‰è¿™æ ·çš„ç”Ÿç†æŒ‡æ ‡æ›´ä¸ºå¯é ä¸”æ— æ³•ä¼ªè£…ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­åº”ç”¨EEGå´ä¸å¯è¡Œï¼Œå› ä¸ºéœ€è¦ä¸“ä¸šçš„è®°å½•è®¾å¤‡ã€‚æœ¬æ–‡çš„ä¸»è¦ç›®æ ‡ä¹‹ä¸€æ˜¯ä¾é EEGæ¨¡æ€çš„å¯é æ€§æ¥ä¿ƒè¿›è¯­éŸ³æ¨¡æ€ä¸Šçš„ç¨³å¥AERã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶ä½¿ç”¨è¿™ä¸¤ç§æ¨¡æ€ï¼Œå³ä½¿åœ¨ç¼ºä¹æ›´å¯é çš„EEGæ¨¡æ€çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½åœ¨æ¨ç†æ—¶å¯é åœ°è¯†åˆ«æƒ…æ„Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ä¸¤æ­¥è¿›è¡Œçš„è”åˆå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼ˆJMMLï¼‰ï¼Œå®ƒåˆ©ç”¨è·¨å†…å’Œè·¨æ¨¡æ€ç‰¹æ€§æ¥æ„å»ºæƒ…æ„ŸåµŒå…¥ï¼Œä»¥ä¸°å¯ŒAERçš„æ€§èƒ½ã€‚ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨JEC-SSLè¿›è¡Œç‹¬ç«‹æ¨¡æ€å†…çš„å­¦ä¹ ã€‚æ¥ä¸‹æ¥æ˜¯åˆ©ç”¨æå‡ºçš„æ·±åº¦å…¸å‹ç›¸å…³è·¨æ¨¡æ€è‡ªç¼–ç å™¨çš„æ‰©å±•å˜ä½“è¿›è¡Œè·¨æ¨¡æ€å­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¸¤ç§æ¨¡æ€æ˜ å°„åˆ°ä¸€ä¸ªå…±åŒçš„è¡¨ç¤ºç©ºé—´æ¥å­¦ä¹ å®ƒä»¬çš„è”åˆå±æ€§ï¼Œä½¿å¾—è¿™ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§æœ€å¤§åŒ–ã€‚è¿™äº›æƒ…æ„ŸåµŒå…¥åŒ…å«äº†ä¸¤ç§æ¨¡æ€çš„å±æ€§ï¼Œä»è€Œæé«˜äº†ç”¨äºAERçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ç»“åˆè¯­éŸ³å’Œè„‘ç”µå›¾æ•°æ®ï¼Œé‡‡ç”¨è”åˆå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•è¿›è¡Œå¯é çš„è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18964v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æ¢è®¨åˆ©ç”¨å¤šæ¨¡æ€è®¡ç®—æœºç•Œé¢æå‡äººç±»ä¸è®¡ç®—æœºçš„äº¤äº’ä½“éªŒã€‚ç ”ç©¶ä¸­é€šè¿‡é‡‡ç”¨è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«æŠ€æœ¯å¢å¼ºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ„ä¹‰æ€§ã€‚è™½ç„¶è¯­éŸ³æ˜¯æœ€ç›´æ¥ä¸”ç›´è§‰æ€§çš„æƒ…ç»ªè¯†åˆ«æ¨¡æ€ï¼Œä½†å…¶å¯é æ€§å› äººä¸ºå¹²æ‰°è€Œå—é™ã€‚ç›¸åï¼Œè„‘ç”µå›¾ç­‰ç”Ÿç†æ¨¡æ€æ›´ä¸ºå¯é ä¸”éš¾ä»¥ä¼ªé€ ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­åº”ç”¨å´ä¸å¯è¡Œã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨è„‘ç”µå›¾çš„å¯é æ€§ï¼Œæ¨åŠ¨åŸºäºè¯­éŸ³æ¨¡æ€çš„ç¨³å¥è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«ã€‚é‡‡ç”¨åŒæ¨¡æ€è®­ç»ƒæ–¹æ³•ï¼Œåœ¨æ¨æ–­æ—¶å³ä½¿ç¼ºä¹æ›´å¯é çš„è„‘ç”µå›¾æ¨¡æ€ï¼Œä¹Ÿèƒ½å¯é åœ°è¯†åˆ«æƒ…ç»ªã€‚ç ”ç©¶æå‡ºä¸€ç§åˆ†ä¸¤æ­¥è¿›è¡Œçš„è”åˆå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨è·¨æ¨¡æ€ç‰¹æ€§æ„å»ºæƒ…æ„ŸåµŒå…¥ï¼Œæé«˜è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«çš„æ€§èƒ½ã€‚é¦–å…ˆè¿›è¡Œç‹¬ç«‹æ¨¡æ€å†…å­¦ä¹ ï¼Œç„¶åä½¿ç”¨æ‰©å±•çš„æ·±åº¦å…¸å‹ç›¸å…³è·¨æ¨¡æ€è‡ªç¼–ç å™¨è¿›è¡Œè·¨æ¨¡æ€å­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜ å°„åˆ°å…±åŒè¡¨å¾ç©ºé—´ï¼Œä½¿ä¸¤ç§æ¨¡æ€çš„æœ€å¤§ç›¸å…³æ€§å¾—ä»¥å®ç°ï¼Œæ‰€ç”Ÿæˆçš„æƒ…æ„ŸåµŒå…¥èåˆäº†ä¸¤ç§æ¨¡æ€çš„ç‰¹æ€§ï¼Œè¿›è€Œæå‡ç”¨äºæƒ…ç»ªè¯†åˆ«çš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ç»“åˆè¯­éŸ³å’Œè„‘ç”µå›¾ï¼Œé‡‡ç”¨è”åˆå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•è¿›è¡Œå¯é çš„è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—æœºç•Œé¢æ­£æœç€å¤šæ¨¡æ€å‘å±•ï¼Œä»¥æ”¹è¿›äººæœºäº¤äº’ä½“éªŒã€‚</li>
<li>è‡ªåŠ¨æƒ…ç»ªè¯†åˆ«æŠ€æœ¯å¯å¢å¼ºäº¤äº’çš„è‡ªç„¶æ€§å’Œæ„ä¹‰æ€§ã€‚</li>
<li>è¯­éŸ³ä½œä¸ºæƒ…ç»ªè¯†åˆ«çš„ä¸»è¦æ¨¡æ€è™½ç›´è§‚ä½†å¯é æ€§å—é™ï¼Œè€Œç”Ÿç†æ¨¡æ€å¦‚è„‘ç”µå›¾æ›´å¯é ä½†å®é™…åº”ç”¨å›°éš¾ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨ç»“åˆè¯­éŸ³å’Œè„‘ç”µå›¾æ¨¡æ€ï¼Œåˆ©ç”¨åè€…çš„å¯é æ€§æå‡å‰è€…çš„æƒ…ç»ªè¯†åˆ«æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ç§åˆ†ä¸¤æ­¥çš„è”åˆå¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€å­¦ä¹ å¢å¼ºè‡ªåŠ¨æƒ…ç»ªè¯†åˆ«çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ˜ å°„åˆ°å…±åŒè¡¨å¾ç©ºé—´èåˆä¸¤ç§æ¨¡æ€çš„ç‰¹æ€§ï¼Œæå‡æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10b05190fb130878872a10f5996afaec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60416161ac779340e71603873ba99956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-884a2fb02f1cf7a3d19f7bf04c40f2a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60c0f84cfbb7945236b62cdd88f8315.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Seeing-Speech-and-Sound-Distinguishing-and-Locating-Audios-in-Visual-Scenes"><a href="#Seeing-Speech-and-Sound-Distinguishing-and-Locating-Audios-in-Visual-Scenes" class="headerlink" title="Seeing Speech and Sound: Distinguishing and Locating Audios in Visual   Scenes"></a>Seeing Speech and Sound: Distinguishing and Locating Audios in Visual   Scenes</h2><p><strong>Authors:Hyeonggon Ryu, Seongyu Kim, Joon Son Chung, Arda Senocak</strong></p>
<p>We present a unified model capable of simultaneously grounding both spoken language and non-speech sounds within a visual scene, addressing key limitations in current audio-visual grounding models. Existing approaches are typically limited to handling either speech or non-speech sounds independently, or at best, together but sequentially without mixing. This limitation prevents them from capturing the complexity of real-world audio sources that are often mixed. Our approach introduces a â€˜mix-and-separateâ€™ framework with audio-visual alignment objectives that jointly learn correspondence and disentanglement using mixed audio. Through these objectives, our model learns to produce distinct embeddings for each audio type, enabling effective disentanglement and grounding across mixed audio sources. Additionally, we created a new dataset to evaluate simultaneous grounding of mixed audio sources, demonstrating that our model outperforms prior methods. Our approach also achieves comparable or better performance in standard segmentation and cross-modal retrieval tasks, highlighting the benefits of our mix-and-separate approach. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶åœ¨è§†è§‰åœºæ™¯ä¸­å¯¹å£è¯­å’Œéè¯­éŸ³å£°éŸ³è¿›è¡Œå®šä½ï¼Œè§£å†³äº†å½“å‰è§†å¬å®šä½æ¨¡å‹çš„å…³é”®å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…é™äºç‹¬ç«‹å¤„ç†è¯­éŸ³æˆ–éè¯­éŸ³å£°éŸ³ï¼Œæˆ–è€…æœ€å¤šä¸€èµ·å¤„ç†ä½†æ— æ³•æ··åˆã€‚è¿™ç§å±€é™æ€§ä½¿ä»–ä»¬æ— æ³•æ•æ‰ç»å¸¸æ··åˆçš„å¤æ‚ç°å®ä¸–ç•ŒéŸ³é¢‘æºçš„ç²¾é«“ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªâ€œæ··åˆä¸åˆ†ç¦»â€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰è§†å¬å¯¹é½ç›®æ ‡ï¼Œä½¿ç”¨æ··åˆéŸ³é¢‘è”åˆå­¦ä¹ å¯¹åº”å…³ç³»å’Œåˆ†ç¦»ã€‚é€šè¿‡è¿™äº›ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šä¸ºæ¯ç§éŸ³é¢‘ç±»å‹ç”Ÿæˆç‹¬ç‰¹çš„åµŒå…¥ï¼Œä»è€Œå®ç°è·¨æ··åˆéŸ³é¢‘æºçš„æœ‰æ•ˆåˆ†ç¦»å’Œå®šä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†æ¥è¯„ä¼°æ··åˆéŸ³é¢‘æºçš„åŒæ—¶å®šä½æ€§èƒ½ï¼Œè¯æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†åˆ†å‰²å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ä¹Ÿå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„â€œæ··åˆä¸åˆ†ç¦»â€æ–¹æ³•çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18880v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å°†åœ¨è§†è§‰åœºæ™¯ä¸­çš„å£è¯­å’Œéè¯­éŸ³å£°éŸ³è¿›è¡Œå®šä½ï¼Œè§£å†³äº†å½“å‰éŸ³é¢‘è§†è§‰å®šä½æ¨¡å‹çš„å…³é”®é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½ç‹¬ç«‹å¤„ç†è¯­éŸ³æˆ–éè¯­éŸ³å£°éŸ³ï¼Œæˆ–è€…æœ€å¤šåŒæ—¶å¤„ç†ä½†ä¸èƒ½æ··åˆã€‚è¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰çœŸå®ä¸–ç•ŒéŸ³é¢‘æºæ··åˆçš„å¤æ‚æ€§ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ä¸ªå¸¦æœ‰è§†å¬å¯¹é½ç›®æ ‡çš„â€œæ··åˆåˆ†ç¦»â€æ¡†æ¶ï¼Œè”åˆå­¦ä¹ å¯¹åº”å…³ç³»å’Œåˆ†ç¦»ä»»åŠ¡ï¼Œä½¿ç”¨æ··åˆéŸ³é¢‘ã€‚é€šè¿‡è¿™äº›ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šä¸ºæ¯ç§éŸ³é¢‘ç±»å‹ç”Ÿæˆç‹¬ç‰¹çš„åµŒå…¥ï¼Œå®ç°æœ‰æ•ˆçš„åˆ†ç¦»å’Œæ··åˆéŸ³é¢‘æºçš„å®šä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†æ¥è¯„ä¼°æ··åˆéŸ³é¢‘æºçš„åŒæ—¶å®šä½æ•ˆæœï¼Œæ˜¾ç¤ºæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å£è¯­å’Œéè¯­éŸ³å£°éŸ³çš„å®šä½é—®é¢˜ã€‚</li>
<li>è§£å†³äº†ç°æœ‰éŸ³é¢‘è§†è§‰å®šä½æ¨¡å‹åœ¨å¤„ç†æ··åˆéŸ³é¢‘æºæ—¶çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªâ€œæ··åˆåˆ†ç¦»â€æ¡†æ¶å’Œè§†å¬å¯¹é½ç›®æ ‡è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>æ¨¡å‹èƒ½ç”Ÿæˆç‹¬ç‰¹åµŒå…¥ä»¥åŒºåˆ†ä¸åŒçš„éŸ³é¢‘ç±»å‹ã€‚</li>
<li>é€šè¿‡åˆ›å»ºæ–°çš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨æ··åˆéŸ³é¢‘æºå®šä½æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f996c51f2943e49c7e4658d6bcb24187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77371909acebcb535580fbb3c827546.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e8ba4ce9b3516b98589443bae6db5d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Wireless-Hearables-With-Programmable-Speech-AI-Accelerators"><a href="#Wireless-Hearables-With-Programmable-Speech-AI-Accelerators" class="headerlink" title="Wireless Hearables With Programmable Speech AI Accelerators"></a>Wireless Hearables With Programmable Speech AI Accelerators</h2><p><strong>Authors:Malek Itani, Tuochao Chen, Arun Raghavan, Gavriel Kohlberg, Shyamnath Gollakota</strong></p>
<p>The conventional wisdom has been that designing ultra-compact, battery-constrained wireless hearables with on-device speech AI models is challenging due to the high computational demands of streaming deep learning models. Speech AI models require continuous, real-time audio processing, imposing strict computational and I&#x2F;O constraints. We present NeuralAids, a fully on-device speech AI system for wireless hearables, enabling real-time speech enhancement and denoising on compact, battery-constrained devices. Our system bridges the gap between state-of-the-art deep learning for speech enhancement and low-power AI hardware by making three key technical contributions: 1) a wireless hearable platform integrating a speech AI accelerator for efficient on-device streaming inference, 2) an optimized dual-path neural network designed for low-latency, high-quality speech enhancement, and 3) a hardware-software co-design that uses mixed-precision quantization and quantization-aware training to achieve real-time performance under strict power constraints. Our system processes 6 ms audio chunks in real-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In real-world evaluations, including a user study with 28 participants, our system outperforms prior on-device models in speech quality and noise suppression, paving the way for next-generation intelligent wireless hearables that can enhance hearing entirely on-device. </p>
<blockquote>
<p>ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œè®¾è®¡è¶…ç´§å‡‘ã€å—ç”µæ± é™åˆ¶çš„æ— çº¿å¯ç©¿æˆ´å¬åŠ›è®¾å¤‡ï¼Œå¹¶æ­è½½è®¾å¤‡ç«¯çš„è¯­éŸ³äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºæµå¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é«˜è®¡ç®—éœ€æ±‚ã€‚è¯­éŸ³äººå·¥æ™ºèƒ½æ¨¡å‹éœ€è¦è¿ç»­ã€å®æ—¶çš„éŸ³é¢‘å¤„ç†ï¼Œè¿™å°±å¸¦æ¥äº†ä¸¥æ ¼çš„è®¡ç®—å’Œè¾“å…¥&#x2F;è¾“å‡ºçº¦æŸã€‚æˆ‘ä»¬æå‡ºäº†NeuralAidsï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ— çº¿å¯ç©¿æˆ´è®¾å¤‡çš„å®Œå…¨è®¾å¤‡ç«¯è¯­éŸ³äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ç´§å‡‘ã€ç”µæ± å—é™çš„è®¾å¤‡ä¸Šå®ç°å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼Œå¡«è¡¥äº†å…ˆè¿›è¯­éŸ³å¢å¼ºæ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ä½åŠŸè€—äººå·¥æ™ºèƒ½ç¡¬ä»¶ä¹‹é—´çš„ç©ºç™½ï¼š1ï¼‰ä¸€ä¸ªé›†æˆäº†è¯­éŸ³äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨çš„æ— çº¿å¯ç©¿æˆ´å¹³å°ï¼Œç”¨äºé«˜æ•ˆçš„è®¾å¤‡ç«¯æµå¼æ¨ç†ï¼›2ï¼‰ä¸€ä¸ªé’ˆå¯¹ä½å»¶è¿Ÿã€é«˜è´¨é‡è¯­éŸ³å¢å¼ºçš„ä¼˜åŒ–åŒè·¯å¾„ç¥ç»ç½‘ç»œï¼›3ï¼‰ä¸€ç§è½¯ç¡¬ä»¶ååŒè®¾è®¡ï¼Œé‡‡ç”¨æ··åˆç²¾åº¦é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œåœ¨ä¸¥æ ¼çš„åŠŸç‡çº¦æŸä¸‹å®ç°å®æ—¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå®æ—¶å¤„ç†6æ¯«ç§’çš„éŸ³é¢‘å—ï¼Œæ¨ç†æ—¶é—´ä¸º5.54æ¯«ç§’ï¼ŒåŠŸè€—ä¸º71.6æ¯«ç“¦ã€‚åœ¨åŒ…æ‹¬28åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶åœ¨å†…çš„çœŸå®ä¸–ç•Œè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨è¯­éŸ³è´¨é‡å’Œå™ªå£°æŠ‘åˆ¶æ–¹é¢è¶…è¶Šäº†å…ˆå‰çš„è®¾å¤‡ç«¯æ¨¡å‹ï¼Œä¸ºä¸‹ä¸€ä»£èƒ½å¤Ÿåœ¨è®¾å¤‡ä¸Šå®Œå…¨å¢å¼ºå¬åŠ›çš„æ™ºèƒ½æ— çº¿å¯ç©¿æˆ´è®¾å¤‡é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18698v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç”µæ± å—é™çš„æ— çº¿å¬åŠ›è®¾å¤‡ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¨è®¾å¤‡è¯­éŸ³AIç³»ç»ŸNeuralAidsï¼Œå®ç°äº†å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯çªç ´å®ç°äº†é«˜æ€§èƒ½ï¼šé›†æˆäº†è¯­éŸ³AIåŠ é€Ÿå™¨çš„æ— çº¿å¬åŠ›å¹³å°ã€ä¸ºä½å»¶è¿Ÿé«˜éŸ³è´¨è¯­éŸ³å¢å¼ºè®¾è®¡çš„ä¼˜åŒ–åŒè·¯å¾„ç¥ç»ç½‘ç»œä»¥åŠå®ç°å®æ—¶æ€§èƒ½çš„ä½åŠŸè€—è½¯ç¡¬ä»¶ååŒè®¾è®¡ã€‚å…¶åœ¨ä¸¥æ ¼çš„ç”µæºé™åˆ¶ä¸‹å–å¾—äº†å®é™…è¿è¡Œæ•ˆæœå’Œç”¨æˆ·è¯„ä»·ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeuralAidsæ˜¯ä¸€ç§å…¨è®¾å¤‡è¯­éŸ³AIç³»ç»Ÿï¼Œä¸“ä¸ºç”µæ± å—é™çš„æ— çº¿å¬åŠ›è®¾å¤‡è®¾è®¡ï¼Œå¯å®ç°å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é›†æˆè¯­éŸ³AIåŠ é€Ÿå™¨å®ç°äº†é«˜æ•ˆçš„è®¾å¤‡æµæ¨æ–­ã€‚</li>
<li>é‡‡ç”¨ä¼˜åŒ–åŒè·¯å¾„ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å®ç°ä½å»¶è¿Ÿå’Œé«˜éŸ³è´¨è¯­éŸ³å¢å¼ºã€‚</li>
<li>è½¯ç¡¬ä»¶ååŒè®¾è®¡é€šè¿‡æ··åˆç²¾åº¦é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå®ç°å®æ—¶æ€§èƒ½ï¼Œæ»¡è¶³ä¸¥æ ¼çš„ç”µæºé™åˆ¶ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å®æ—¶éŸ³é¢‘å—ï¼Œå…·æœ‰å¿«é€Ÿæ¨æ–­æ—¶é—´ï¼ˆ5.54æ¯«ç§’ï¼‰å’Œä½åŠŸè€—æ¶ˆè€—ï¼ˆ71.6æ¯«ç“¦ï¼‰ã€‚</li>
<li>åœ¨åŒ…æ‹¬ç”¨æˆ·ç ”ç©¶åœ¨å†…çš„å®é™…è¯„ä¼°ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨è¯­éŸ³è´¨é‡å’Œå™ªå£°æŠ‘åˆ¶æ–¹é¢ä¼˜äºå…ˆå‰çš„è®¾å¤‡æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9def523b9031a7f475951aed334d0ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308b6e760e0b276ffefeec3898967440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-648eaca0feb2724a6fc191b44b346ebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16c9820b4fa7e388d33ee87798a14ed2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language"><a href="#Whispering-in-Amharic-Fine-tuning-Whisper-for-Low-resource-Language" class="headerlink" title="Whispering in Amharic: Fine-tuning Whisper for Low-resource Language"></a>Whispering in Amharic: Fine-tuning Whisper for Low-resource Language</h2><p><strong>Authors:Dawit Ketema Gete, Bedru Yimam Ahamed, Tadesse Destaw Belay, Yohannes Ayana Ejigu, Sukairaj Hafiz Imam, Alemu Belay Tessema, Mohammed Oumer Adem, Tadesse Amare Belay, Robert Geislinger, Umma Aliyu Musa, Martin Semmann, Shamsuddeen Hassan Muhammad, Henning Schreiber, Seid Muhie Yimam</strong></p>
<p>This work explores fine-tuning OpenAIâ€™s Whisper automatic speech recognition (ASR) model for Amharic, a low-resource language, to improve transcription accuracy. While the foundational Whisper model struggles with Amharic due to limited representation in its training data, we fine-tune it using datasets like Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The best-performing model, Whispersmall-am, significantly improves when finetuned on a mix of existing FLEURS data and new, unseen Amharic datasets. Training solely on new data leads to poor performance, but combining it with FLEURS data reinforces the model, enabling better specialization in Amharic. We also demonstrate that normalizing Amharic homophones significantly enhances Word Error Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study underscores the importance of fine-tuning strategies and dataset composition for improving ASR in low-resource languages, providing insights for future Amharic speech recognition research. </p>
<blockquote>
<p>è¿™é¡¹å·¥ä½œæ¢è®¨äº†é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­ï¼ˆä¸€ç§ä½èµ„æºè¯­è¨€ï¼‰å¯¹OpenAIçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚è™½ç„¶åŸºç¡€whisperæ¨¡å‹ç”±äºè®­ç»ƒæ•°æ®ä¸­é˜¿å§†å“ˆæ‹‰è¯­çš„ä»£è¡¨æ€§æœ‰é™è€Œéš¾ä»¥åº”å¯¹é˜¿å§†å“ˆæ‹‰è¯­ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨Mozilla Common Voiceã€FLEURSå’ŒBDU-speechç­‰æ•°æ®é›†å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹Whispersmall-amï¼Œåœ¨ç°æœ‰çš„FLEURSæ•°æ®å’Œæ–°çš„æœªè§è¿‡çš„é˜¿å§†å“ˆæ‹‰è¯­æ•°æ®é›†æ··åˆè¿›è¡Œå¾®è°ƒæ—¶ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä»…åœ¨æ–°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œä½†å°†å…¶ä¸FLEURSæ•°æ®ç›¸ç»“åˆå¯ä»¥åŠ å¼ºæ¨¡å‹ï¼Œä½¿é˜¿å§†å“ˆæ‹‰è¯­ä¸“ä¸šåŒ–æ›´å¥½ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œå¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’ŒåŒè¯­è¯„ä¼°ä¸‹ç ”ç©¶ï¼ˆBLEUï¼‰çš„åˆ†æ•°ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ç»„æˆåœ¨æé«˜ä½èµ„æºè¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥é˜¿å§†å“ˆæ‹‰è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶æä¾›äº†å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18485v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­ï¼ˆä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼‰å¯¹OpenAIçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚åœ¨åŸºç¡€Whisperæ¨¡å‹ç”±äºå…¶è®­ç»ƒæ•°æ®ä¸­é˜¿å§†å“ˆæ‹‰è¯­ä»£è¡¨æ€§æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨Mozilla Common Voiceã€FLEURSå’ŒBDU-speechç­‰æ•°æ®é›†å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹Whispersmall-amï¼Œåœ¨ç°æœ‰FLEURSæ•°æ®å’Œæ–°çš„æœªè§è¿‡çš„é˜¿å§†å“ˆæ‹‰è¯­æ•°æ®é›†çš„æ··åˆä¸Šå¾®è°ƒæ—¶è¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚ä»…åœ¨æ–°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œä½†å°†å…¶ä¸FLEURSæ•°æ®ç›¸ç»“åˆå¯ä»¥åŠ å¼ºæ¨¡å‹çš„è®­ç»ƒï¼Œä½¿å…¶åœ¨é˜¿å§†å“ˆæ‹‰è¯­ä¸­æ›´å¥½åœ°ä¸“ä¸šåŒ–ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œå¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¯ä»¥æ˜¾è‘—æé«˜å•è¯é”™è¯¯ç‡å’ŒåŒè¯­è¯„ä¼°ä¸‹çš„å¾—åˆ†ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ç»„æˆå¯¹äºæé«˜ä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥é˜¿å§†å“ˆæ‹‰è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶æä¾›äº†è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é€šè¿‡å¾®è°ƒOpenAIçš„Whisperæ¨¡å‹ï¼Œé’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€æé«˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è½¬å½•å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨å¤šç§æ•°æ®é›†å¦‚Mozilla Common Voiceã€FLEURSå’ŒBDU-speechå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>è¡¨ç°æœ€ä½³çš„æ¨¡å‹Whispersmall-amåœ¨æ··åˆç°æœ‰FLEURSæ•°æ®å’Œæ–°çš„é˜¿å§†å“ˆæ‹‰è¯­æ•°æ®é›†ä¸Šå¾®è°ƒæ—¶è¡¨ç°æœ€ä½³ã€‚</li>
<li>ä»…ä¾èµ–æ–°æ•°æ®è®­ç»ƒæ¨¡å‹ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦ä¸FLEURSæ•°æ®ç»“åˆä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚</li>
<li>å¯¹é˜¿å§†å“ˆæ‹‰è¯­çš„åŒéŸ³å­—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†èƒ½æ˜¾è‘—æé«˜å•è¯é”™è¯¯ç‡å’ŒåŒè¯­è¯„ä¼°å¾—åˆ†ã€‚</li>
<li>å¾®è°ƒç­–ç•¥å’Œæ•°æ®é›†ç»„æˆå¯¹äºæé«˜ä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5c009d0fb8e693593c4090089ac71ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3849c2b4fcd9ee28e1b6c1206ac7c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b690b59afd52e0322e37dfc7b2eea545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b06f5e26ca6b4048b8446abac6c61c1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DiffusionTalker-Efficient-and-Compact-Speech-Driven-3D-Talking-Head-via-Personalizer-Guided-Distillation"><a href="#DiffusionTalker-Efficient-and-Compact-Speech-Driven-3D-Talking-Head-via-Personalizer-Guided-Distillation" class="headerlink" title="DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via   Personalizer-Guided Distillation"></a>DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via   Personalizer-Guided Distillation</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Real-time speech-driven 3D facial animation has been attractive in academia and industry. Traditional methods mainly focus on learning a deterministic mapping from speech to animation. Recent approaches start to consider the nondeterministic fact of speech-driven 3D face animation and employ the diffusion model for the task. Existing diffusion-based methods can improve the diversity of facial animation. However, personalized speaking styles conveying accurate lip language is still lacking, besides, efficiency and compactness still need to be improved. In this work, we propose DiffusionTalker to address the above limitations via personalizer-guided distillation. In terms of personalization, we introduce a contrastive personalizer that learns identity and emotion embeddings to capture speaking styles from audio. We further propose a personalizer enhancer during distillation to enhance the influence of embeddings on facial animation. For efficiency, we use iterative distillation to reduce the steps required for animation generation and achieve more than 8x speedup in inference. To achieve compactness, we distill the large teacher model into a smaller student model, reducing our modelâ€™s storage by 86.4% while minimizing performance loss. After distillation, users can derive their identity and emotion embeddings from audio to quickly create personalized animations that reflect specific speaking styles. Extensive experiments are conducted to demonstrate that our method outperforms state-of-the-art methods. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/DiffusionTalker">https://github.com/ChenVoid/DiffusionTalker</a>. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å…·æœ‰å¸å¼•åŠ›ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨ä»è¯­éŸ³åˆ°åŠ¨ç”»çš„ç¡®å®šæ€§æ˜ å°„å­¦ä¹ ã€‚æœ€è¿‘çš„æ–¹æ³•å¼€å§‹è€ƒè™‘è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»çš„éç¡®å®šæ€§å› ç´ ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥å®Œæˆæ­¤ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å¯ä»¥æé«˜é¢éƒ¨åŠ¨ç”»çš„å¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œé™¤äº†ç¼ºä¹ä¼ è¾¾å‡†ç¡®å”‡è¯­çš„ä¸ªæ€§åŒ–è¯´è¯é£æ ¼å¤–ï¼Œè¿˜éœ€è¦æé«˜æ•ˆç‡å’Œç´§å‡‘æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨DiffusionTalkeré€šè¿‡ä¸ªæ€§åŒ–å¼•å¯¼è’¸é¦æ¥è§£å†³ä¸Šè¿°å±€é™æ€§ã€‚åœ¨ä¸ªæ€§åŒ–æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨æ¥å­¦ä¹ èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œä»¥ä»éŸ³é¢‘ä¸­æ•è·è¯´è¯é£æ ¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åœ¨è’¸é¦è¿‡ç¨‹ä¸­çš„ä¸ªæ€§åŒ–å¢å¼ºå™¨ï¼Œä»¥å¢å¼ºåµŒå…¥å¯¹é¢éƒ¨åŠ¨ç”»çš„å½±å“ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿­ä»£è’¸é¦æ¥å‡å°‘åŠ¨ç”»ç”Ÿæˆæ‰€éœ€çš„æ­¥éª¤ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†è¶…è¿‡8å€çš„é€Ÿåº¦æå‡ã€‚ä¸ºå®ç°ç´§å‡‘æ€§ï¼Œæˆ‘ä»¬å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå°†æ¨¡å‹çš„å­˜å‚¨å‡å°‘äº†86.4%ï¼ŒåŒæ—¶æœ€å°åŒ–æ€§èƒ½æŸå¤±ã€‚ç»è¿‡è’¸é¦åï¼Œç”¨æˆ·å¯ä»¥ä»éŸ³é¢‘ä¸­å¯¼å‡ºä»–ä»¬çš„èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œå¿«é€Ÿåˆ›å»ºåæ˜ ç‰¹å®šè¯´è¯é£æ ¼çš„ä¸ªæ€§åŒ–åŠ¨ç”»ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚ä»£ç å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/DiffusionTalker%E3%80%82">https://github.com/ChenVoid/DiffusionTalkerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18159v1">PDF</a> Accepted by ICME2025</p>
<p><strong>æ‘˜è¦</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å®æ—¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸ªæ€§åŒ–ã€æ•ˆç‡å’Œç´§å‡‘æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨å’Œå­¦ä¹ èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œæé«˜åŠ¨ç”»çš„ä¸ªæ€§åŒ–ç¨‹åº¦ï¼›é‡‡ç”¨è¿­ä»£è’¸é¦æŠ€æœ¯ï¼Œå‡å°‘åŠ¨ç”»ç”Ÿæˆæ­¥éª¤ï¼Œæé«˜æ¨ç†æ•ˆç‡ï¼›åŒæ—¶å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œå‡å°‘æ¨¡å‹å­˜å‚¨å¤§å°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨å’Œå­¦ä¹ èº«ä»½ã€æƒ…æ„ŸåµŒå…¥ï¼Œæé«˜è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»çš„ä¸ªæ€§åŒ–ç¨‹åº¦ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£è’¸é¦æŠ€æœ¯ï¼Œæé«˜åŠ¨ç”»ç”Ÿæˆæ•ˆç‡ï¼Œå®ç°è¶…è¿‡8å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚</li>
<li>é€šè¿‡å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œå‡å°‘æ¨¡å‹å­˜å‚¨å¤§å°ï¼Œé™ä½æ€§èƒ½æŸå¤±ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨åŠ¨ç”»æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰æ›´å¤šçš„è¯­éŸ³é£æ ¼ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡éŸ³é¢‘è·å–èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œå¿«é€Ÿåˆ›å»ºåæ˜ ç‰¹å®šè¯­éŸ³é£æ ¼çš„ä¸ªæ€§åŒ–åŠ¨ç”»ã€‚</li>
<li>æ‰©æ•£Talkeræ–¹æ³•åœ¨å®éªŒä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-df90fdd19aee17b7de3612891077e609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2bd15c0b513d06e0a31e2bd260f4cdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbf3f00752a37d60c9878e9838e22c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e0444b8af0163f2a8e6ab876d9d28a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ecdd222028b48c3ac0a03ce1f3b3b4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c624f647de655de8c2201e1d4046e4f8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Elevating-Robust-Multi-Talker-ASR-by-Decoupling-Speaker-Separation-and-Speech-Recognition"><a href="#Elevating-Robust-Multi-Talker-ASR-by-Decoupling-Speaker-Separation-and-Speech-Recognition" class="headerlink" title="Elevating Robust Multi-Talker ASR by Decoupling Speaker Separation and   Speech Recognition"></a>Elevating Robust Multi-Talker ASR by Decoupling Speaker Separation and   Speech Recognition</h2><p><strong>Authors:Yufeng Yang, Hassan Taherian, Vahid Ahmadi Kalkhorani, DeLiang Wang</strong></p>
<p>Despite the tremendous success of automatic speech recognition (ASR) with the introduction of deep learning, its performance is still unsatisfactory in many real-world multi-talker scenarios. Speaker separation excels in separating individual talkers but, as a frontend, it introduces processing artifacts that degrade the ASR backend trained on clean speech. As a result, mainstream robust ASR systems train the backend on noisy speech to avoid processing artifacts. In this work, we propose to decouple the training of the speaker separation frontend and the ASR backend, with the latter trained on clean speech only. Our decoupled system achieves 5.1% word error rates (WER) on the Libri2Mix dev&#x2F;test sets, significantly outperforming other multi-talker ASR baselines. Its effectiveness is also demonstrated with the state-of-the-art 7.60%&#x2F;5.74% WERs on 1-ch and 6-ch SMS-WSJ. Furthermore, on recorded LibriCSS, we achieve the speaker-attributed WER of 2.92%. These state-of-the-art results suggest that decoupling speaker separation and recognition is an effective approach to elevate robust multi-talker ASR. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢çš„åº”ç”¨å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œå…¶è¡¨ç°ä»ç„¶ä¸å°½äººæ„ã€‚è¯´è¯äººåˆ†ç¦»æŠ€æœ¯æ“…é•¿åˆ†ç¦»å•ä¸ªè¯´è¯äººï¼Œä½†ä½œä¸ºå‰ç«¯æŠ€æœ¯ï¼Œå®ƒå¼•å…¥äº†å¤„ç†ä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±ä¼šæ¶åŒ–åœ¨å¹²å‡€è¯­éŸ³ä¸Šè®­ç»ƒçš„ASRåç«¯æ€§èƒ½ã€‚å› æ­¤ï¼Œä¸»æµçš„é²æ£’ASRç³»ç»Ÿä¼šå¯¹åç«¯è¿›è¡Œå™ªå£°è¯­éŸ³è®­ç»ƒï¼Œä»¥é¿å…å¤„ç†ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°†è¯´è¯äººåˆ†ç¦»å‰ç«¯å’ŒASRåç«¯çš„è®­ç»ƒè§£è€¦ï¼Œåç«¯ä»…åœ¨å¹²å‡€è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„è§£è€¦ç³»ç»Ÿå®ç°äº†Libri2Mixå¼€å‘&#x2F;æµ‹è¯•é›†ä¸Šçš„5.1%è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¤šè¯´è¯äººASRåŸºå‡†ã€‚å…¶æœ‰æ•ˆæ€§è¿˜ä½“ç°åœ¨æœ€å…ˆè¿›çš„1é€šé“å’Œ6é€šé“çš„SMS-WSJçš„7.60%&#x2F;5.74% WERã€‚æ­¤å¤–ï¼Œåœ¨å½•åˆ¶çš„LibriCSSä¸Šï¼Œæˆ‘ä»¬è¾¾åˆ°äº†2.92%çš„è¯´è¯äººå±æ€§WERã€‚è¿™äº›æœ€å…ˆè¿›çš„ç»“æœè¡¨æ˜ï¼Œå°†è¯´è¯äººåˆ†ç¦»å’Œè¯†åˆ«è§£è€¦æ˜¯ä¸€ç§æœ‰æ•ˆçš„æé«˜é²æ£’å¤šè¯´è¯äººASRçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17886v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸Šå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šè¯´è¯äººç°å®åœºæ™¯ä¸­ï¼Œå…¶è¡¨ç°ä»ä¸å°½å¦‚äººæ„ã€‚å°½ç®¡è¯´è¯äººåˆ†ç¦»æŠ€æœ¯åœ¨åˆ†ç¦»å•ä¸ªè¯´è¯äººæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä½œä¸ºå‰ç«¯æŠ€æœ¯ï¼Œå®ƒå¼•å…¥äº†å¤„ç†ä¼ªè¿¹ï¼Œè¿™äº›ä¼ªè¿¹ä¼šç ´åå¯¹å¹²å‡€è¯­éŸ³è¿›è¡Œè®­ç»ƒçš„ASRåç«¯çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œä¸»æµçš„é²æ£’ASRç³»ç»Ÿéƒ½å¯¹å«å™ªè¯­éŸ³è¿›è¡Œåç«¯è®­ç»ƒï¼Œä»¥é¿å…å¤„ç†ä¼ªè¿¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°†è¯´è¯äººåˆ†ç¦»å‰ç«¯å’ŒASRåç«¯è®­ç»ƒè§£è€¦çš„æ–¹æ³•ï¼Œåç«¯ä»…åœ¨å¹²å‡€è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„è§£è€¦ç³»ç»Ÿåœ¨Libri2Mixå¼€å‘&#x2F;æµ‹è¯•é›†ä¸Šå®ç°äº†5.1%çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¤šè¯´è¯äººASRåŸºçº¿ã€‚å…¶åœ¨SMS-WSJçš„1é€šé“å’Œ6é€šé“ä¸Šçš„æœ€æ–°æ€§èƒ½åˆ†åˆ«ä¸º7.60%&#x2F;5.74%çš„WERã€‚æ­¤å¤–ï¼Œåœ¨å½•åˆ¶çš„LibriCSSä¸Šï¼Œæˆ‘ä»¬å®ç°äº†è¯´è¯äººå½’å› çš„WERä¸º2.92%ã€‚è¿™äº›æœ€æ–°ç»“æœè¡¨æ˜ï¼Œè§£è€¦è¯´è¯äººåˆ†ç¦»å’Œè¯†åˆ«æ˜¯ä¸€ç§æœ‰æ•ˆçš„æé«˜é²æ£’å¤šè¯´è¯äººASRçš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨çœŸå®ä¸–ç•Œçš„å¤šè¯´è¯äººåœºæ™¯ä¸­ä»å­˜åœ¨æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>è¯´è¯äººåˆ†ç¦»æŠ€æœ¯è™½ç„¶æ“…é•¿åˆ†ç¦»å•ä¸ªè¯´è¯äººï¼Œä½†ä½œä¸ºå‰ç«¯æŠ€æœ¯ï¼Œå®¹æ˜“å¼•å…¥å¤„ç†ä¼ªè¿¹ã€‚</li>
<li>å¤„ç†ä¼ªè¿¹å¯èƒ½ä¼šå½±å“å¯¹å¹²å‡€è¯­éŸ³è¿›è¡Œè®­ç»ƒçš„ASRåç«¯çš„æ€§èƒ½ã€‚</li>
<li>å½“å‰ä¸»æµåšæ³•æ˜¯å¯¹å«å™ªè¯­éŸ³è¿›è¡Œåç«¯è®­ç»ƒä»¥é¿å…å¤„ç†ä¼ªè¿¹ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†å°†è¯´è¯äººåˆ†ç¦»å‰ç«¯å’ŒASRåç«¯è®­ç»ƒè§£è€¦çš„æ–¹æ³•ã€‚</li>
<li>è§£è€¦ç³»ç»Ÿåœ¨Libri2Mixå¼€å‘&#x2F;æµ‹è¯•é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1966c594d58fd626ab406528d7c0ddb9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Measuring-the-Robustness-of-Audio-Deepfake-Detectors"><a href="#Measuring-the-Robustness-of-Audio-Deepfake-Detectors" class="headerlink" title="Measuring the Robustness of Audio Deepfake Detectors"></a>Measuring the Robustness of Audio Deepfake Detectors</h2><p><strong>Authors:Xiang Li, Pin-Yu Chen, Wenqi Wei</strong></p>
<p>Deepfakes have become a universal and rapidly intensifying concern of generative AI across various media types such as images, audio, and videos. Among these, audio deepfakes have been of particular concern due to the ease of high-quality voice synthesis and distribution via platforms such as social media and robocalls. Consequently, detecting audio deepfakes plays a critical role in combating the growing misuse of AI-synthesized speech. However, real-world scenarios often introduce various audio corruptions, such as noise, modification, and compression, that may significantly impact detection performance. This work systematically evaluates the robustness of 10 audio deepfake detection models against 16 common corruptions, categorized into noise perturbation, audio modification, and compression. Using both traditional deep learning models and state-of-the-art foundation models, we make four unique observations. First, our findings show that while most models demonstrate strong robustness to noise, they are notably more vulnerable to modifications and compression, especially when neural codecs are applied. Second, speech foundation models generally outperform traditional models across most scenarios, likely due to their self-supervised learning paradigm and large-scale pre-training. Third, our results show that increasing model size improves robustness, albeit with diminishing returns. Fourth, we demonstrate how targeted data augmentation during training can enhance model resilience to unseen perturbations. A case study on political speech deepfakes highlights the effectiveness of foundation models in achieving high accuracy under real-world conditions. These findings emphasize the importance of developing more robust detection frameworks to ensure reliability in practical deployment settings. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ å·²ç»æˆä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å…¨åª’ä½“æ—¶ä»£æ‰€é¢ä¸´çš„æ™®éä¸”æ—¥ç›ŠåŠ å‰§çš„æ‹…å¿§ï¼Œæ¶‰åŠçš„åª’ä½“ç±»å‹åŒ…æ‹¬å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰ã€‚å…¶ä¸­ï¼ŒéŸ³é¢‘æ·±åº¦ä¼ªé€ å°¤ä¸ºå¼•äººå…³æ³¨ï¼Œå› ä¸ºé€šè¿‡ç¤¾äº¤åª’ä½“å’Œæœºå™¨äººç”µè¯ç­‰å¹³å°ï¼Œé«˜è´¨é‡çš„å£°éŸ³åˆæˆå’Œä¼ æ’­å˜å¾—è½»è€Œæ˜“ä¸¾ã€‚å› æ­¤ï¼Œæ£€æµ‹éŸ³é¢‘æ·±åº¦ä¼ªé€ åœ¨æ‰“å‡»æ—¥ç›Šå¢é•¿çš„AIåˆæˆè¯­éŸ³æ»¥ç”¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç°å®åœºæ™¯ä¸­çš„éŸ³é¢‘å¾€å¾€å­˜åœ¨å„ç§å¤±çœŸï¼Œå¦‚å™ªå£°ã€ä¿®æ”¹å’Œå‹ç¼©ç­‰ï¼Œè¿™äº›å¤±çœŸå¯èƒ½ä¼šå¯¹æ£€æµ‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚è¿™é¡¹å·¥ä½œç³»ç»Ÿåœ°è¯„ä¼°äº†10ç§éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å¯¹16ç§å¸¸è§å¤±çœŸçš„é²æ£’æ€§ï¼Œè¿™äº›å¤±çœŸè¢«åˆ†ç±»ä¸ºå™ªå£°æ‰°åŠ¨ã€éŸ³é¢‘ä¿®æ”¹å’Œå‹ç¼©ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œæœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶åšå‡ºäº†å››ä¸ªç‹¬ç‰¹çš„è§‚å¯Ÿã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å¤šæ•°æ¨¡å‹å¯¹å™ªå£°å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ï¼Œä½†å®ƒä»¬å¯¹ä¿®æ”¹å’Œå‹ç¼©çš„æ•æ„Ÿæ€§æ˜¾è‘—æ›´é«˜ï¼Œç‰¹åˆ«æ˜¯åº”ç”¨ç¥ç»ç½‘ç»œç¼–ç æ—¶ã€‚å…¶æ¬¡ï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­çš„è¡¨ç°é€šå¸¸ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå…¶è‡ªæˆ‘ç›‘ç£çš„å­¦ä¹ æ¨¡å¼å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰€è‡´ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¢åŠ æ¨¡å‹å¤§å°è™½ç„¶å¯ä»¥æé«˜å…¶é²æ£’æ€§ï¼Œä½†æ”¶ç›Šä¼šé€’å‡ã€‚ç¬¬å››ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºå¦‚ä½•å¢å¼ºæ¨¡å‹å¯¹æœªè§æ‰°åŠ¨çš„æŠ—æ€§ã€‚å…³äºæ”¿æ²»æ¼”è®²æ·±åº¦ä¼ªé€ æ¡ˆä¾‹ç ”ç©¶çªå‡ºäº†åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œæ¡ä»¶ä¸‹å®ç°é«˜å‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒå¼€å‘æ›´ç¨³å¥çš„æ£€æµ‹æ¡†æ¶çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17577v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦ä¼ªé€ å·²é€æ¸æˆä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§åª’ä½“ç±»å‹ä¸­çš„æ™®éä¸”æ—¥ç›ŠåŠ å‰§çš„å…³åˆ‡ç‚¹ã€‚ç‰¹åˆ«æ˜¯éŸ³é¢‘æ·±åº¦ä¼ªé€ å¼•å‘äº†äººä»¬çš„ç‰¹åˆ«å…³æ³¨ï¼Œå› ä¸ºé«˜è´¨é‡çš„å£°éŸ³åˆæˆéå¸¸å®¹æ˜“å®ç°ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç¤¾äº¤åª’ä½“å’Œè¯­éŸ³æœºå™¨äººç”µè¯ç­‰å¹³å°å¹¿æ³›ä¼ æ’­ã€‚å› æ­¤ï¼Œæ£€æµ‹éŸ³é¢‘æ·±åº¦ä¼ªé€ åœ¨æ‰“å‡»äººå·¥æ™ºèƒ½åˆæˆè¯­éŸ³çš„æ»¥ç”¨æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„åœºæ™¯ç»å¸¸ä¼šå¼•å…¥å„ç§éŸ³é¢‘å¤±çœŸï¼Œå¦‚å™ªå£°ã€ä¿®æ”¹å’Œå‹ç¼©ç­‰ï¼Œè¿™äº›å¤±çœŸå¯èƒ½ä¼šå¯¹æ£€æµ‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚è¿™é¡¹å·¥ä½œç³»ç»Ÿåœ°è¯„ä¼°äº†10ç§éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å¯¹16ç§å¸¸è§å¤±çœŸçš„é²æ£’æ€§ï¼Œè¿™äº›å¤±çœŸè¢«åˆ†ç±»ä¸ºå™ªå£°æ‰°åŠ¨ã€éŸ³é¢‘ä¿®æ”¹å’Œå‹ç¼©ã€‚é€šè¿‡ä½¿ç”¨ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œæœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬å¾—å‡ºäº†å››ä¸ªç‹¬ç‰¹çš„è§‚å¯Ÿç»“æœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å¤šæ•°æ¨¡å‹å¯¹å™ªå£°å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ï¼Œä½†å®ƒä»¬å¯¹ä¿®æ”¹å’Œå‹ç¼©çš„æ•æ„Ÿæ€§æ›´é«˜ï¼Œç‰¹åˆ«æ˜¯åº”ç”¨ç¥ç»ç½‘ç»œç¼–ç æ—¶ã€‚å…¶æ¬¡ï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸€èˆ¬è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå®ƒä»¬çš„å¤§å‹é¢„è®­ç»ƒå’Œè‡ªç›‘ç£å­¦ä¹ æ¨¡å¼ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ”¶ç›Šé€’å‡ï¼Œä½†å¢åŠ æ¨¡å‹å¤§å°å¯ä»¥æé«˜å…¶ç¨³å¥æ€§ã€‚ç¬¬å››ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºå¦‚ä½•å¢å¼ºæ¨¡å‹å¯¹æœªè§è¿‡çš„æ‰°åŠ¨çš„æŠµæŠ—åŠ›ã€‚æ”¿æ²»æ¼”è®²æ·±åº¦ä¼ªé€ æ¡ˆä¾‹ç ”ç©¶çªå‡ºäº†åŸºç¡€æ¨¡å‹åœ¨å®é™…æ¡ä»¶ä¸‹å®ç°é«˜å‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼€å‘æ›´ç¨³å¥çš„æ£€æµ‹æ¡†æ¶çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘æ·±åº¦ä¼ªé€ æ˜¯ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å…³åˆ‡ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾äº¤åª’ä½“å’Œè¯­éŸ³æœºå™¨äººç”µè¯ç­‰å¹³å°çš„å¹¿æ³›ä¼ æ’­ä¸‹ã€‚</li>
<li>ç°å®ä¸–ç•Œçš„éŸ³é¢‘å¤±çœŸï¼Œå¦‚ä¿®æ”¹å’Œå‹ç¼©ï¼Œå¯¹éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚</li>
<li>è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¾—ç›Šäºå…¶å¤§å‹é¢„è®­ç»ƒå’Œè‡ªç›‘ç£å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>æ¨¡å‹å¤§å°å¢åŠ å¯ä»¥æé«˜å…¶ç¨³å¥æ€§ï¼Œä½†æ”¶ç›Šä¼šé€’å‡ã€‚</li>
<li>é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºå¯ä»¥æé«˜æ¨¡å‹å¯¹æœªè§è¿‡çš„æ‰°åŠ¨çš„æŠµæŠ—åŠ›ã€‚</li>
<li>åœ¨æ”¿æ²»æ¼”è®²æ·±åº¦ä¼ªé€ çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒåŸºç¡€æ¨¡å‹åœ¨é«˜å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-76894a96dec020465a06b728e70f92ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b683f0d9642e6265bdc092f093b7c20.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DIDiffGes-Decoupled-Semi-Implicit-Diffusion-Models-for-Real-time-Gesture-Generation-from-Speech"><a href="#DIDiffGes-Decoupled-Semi-Implicit-Diffusion-Models-for-Real-time-Gesture-Generation-from-Speech" class="headerlink" title="DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time   Gesture Generation from Speech"></a>DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time   Gesture Generation from Speech</h2><p><strong>Authors:Yongkang Cheng, Shaoli Huang, Xuelin Chen, Jifeng Ning, Mingming Gong</strong></p>
<p>Diffusion models have demonstrated remarkable synthesis quality and diversity in generating co-speech gestures. However, the computationally intensive sampling steps associated with diffusion models hinder their practicality in real-world applications. Hence, we present DIDiffGes, for a Decoupled Semi-Implicit Diffusion model-based framework, that can synthesize high-quality, expressive gestures from speech using only a few sampling steps. Our approach leverages Generative Adversarial Networks (GANs) to enable large-step sampling for diffusion model. We decouple gesture data into body and hands distributions and further decompose them into marginal and conditional distributions. GANs model the marginal distribution implicitly, while L2 reconstruction loss learns the conditional distributions exciplictly. This strategy enhances GAN training stability and ensures expressiveness of generated full-body gestures. Our framework also learns to denoise root noise conditioned on local body representation, guaranteeing stability and realism. DIDiffGes can generate gestures from speech with just 10 sampling steps, without compromising quality and expressiveness, reducing the number of sampling steps by a factor of 100 compared to existing methods. Our user study reveals that our method outperforms state-of-the-art approaches in human likeness, appropriateness, and style correctness. Project is <a target="_blank" rel="noopener" href="https://cyk990422.github.io/DIDiffGes">https://cyk990422.github.io/DIDiffGes</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¼´éšè¯­éŸ³çš„æ‰‹åŠ¿æ—¶è¡¨ç°å‡ºäº†å“è¶Šçš„åˆæˆè´¨é‡å’Œå¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œä¸æ‰©æ•£æ¨¡å‹ç›¸å…³çš„è®¡ç®—å¯†é›†å‹çš„é‡‡æ ·æ­¥éª¤é˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DIDiffGesï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§£è€¦åŠéšå¼æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨å°‘æ•°é‡‡æ ·æ­¥éª¤å°±èƒ½ä»è¯­éŸ³ä¸­åˆæˆé«˜è´¨é‡ã€è¡¨è¾¾ä¸°å¯Œçš„æ‰‹åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ¥å®ç°æ‰©æ•£æ¨¡å‹çš„å¤§æ­¥é‡‡æ ·ã€‚æˆ‘ä»¬å°†æ‰‹åŠ¿æ•°æ®è§£è€¦ä¸ºèº«ä½“å’Œæ‰‹éƒ¨åˆ†å¸ƒï¼Œå¹¶è¿›ä¸€æ­¥å°†å®ƒä»¬åˆ†è§£ä¸ºè¾¹ç¼˜åˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒã€‚GANséšå¼åœ°å»ºæ¨¡è¾¹ç¼˜åˆ†å¸ƒï¼Œè€ŒL2é‡å»ºæŸå¤±æ˜¾å¼åœ°å­¦ä¹ æ¡ä»¶åˆ†å¸ƒã€‚è¿™ç§ç­–ç•¥æé«˜äº†GANçš„è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶ç¡®ä¿äº†ç”Ÿæˆçš„å…¨èº«æ‰‹åŠ¿çš„è¡¨è¾¾æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å­¦ä¹ åœ¨å±€éƒ¨èº«ä½“è¡¨å¾çš„æ¡ä»¶ä¸‹æ¶ˆé™¤æ ¹å™ªå£°ï¼Œä¿è¯ç¨³å®šæ€§å’ŒçœŸå®æ€§ã€‚DIDiffGesåªéœ€10ä¸ªé‡‡æ ·æ­¥éª¤å°±èƒ½ä»è¯­éŸ³ä¸­ç”Ÿæˆæ‰‹åŠ¿ï¼Œä¸”ä¸å¦¥åäºè´¨é‡å’Œè¡¨è¾¾æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œé‡‡æ ·æ­¥éª¤çš„æ•°é‡å‡å°‘äº†100å€ã€‚æˆ‘ä»¬çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äººçš„ç›¸ä¼¼æ€§ã€é€‚å½“æ€§å’Œé£æ ¼æ­£ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚é¡¹ç›®æ˜¯<a target="_blank" rel="noopener" href="https://cyk990422.github.io/DIDiffGes%E3%80%82">https://cyk990422.github.io/DIDiffGesã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17059v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DIDiffGesï¼Œä¸€ç§åŸºäºè§£è€¦åŠéšå¼æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤å†…ä»è¯­éŸ³åˆæˆé«˜è´¨é‡ã€è¡¨è¾¾ä¸°å¯Œçš„æ‰‹åŠ¿ã€‚è¯¥ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å®ç°å¤§æ­¥é‡‡æ ·æ‰©æ•£æ¨¡å‹ï¼Œå°†æ‰‹åŠ¿æ•°æ®è§£è€¦ä¸ºèº«ä½“å’Œæ‰‹éƒ¨åˆ†å¸ƒå¹¶è¿›ä¸€æ­¥åˆ†è§£ä¸ºè¾¹é™…å’Œæ¡ä»¶åˆ†å¸ƒã€‚è¯¥æ–¹æ³•å¢å¼ºäº†GANè®­ç»ƒç¨³å®šæ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„å…¨èº«æ‰‹åŠ¿çš„è¡¨è¾¾æ€§ã€‚DIDiffGesèƒ½å¤Ÿä»…åœ¨10ä¸ªé‡‡æ ·æ­¥éª¤å†…ä»è¯­éŸ³ç”Ÿæˆæ‰‹åŠ¿ï¼Œä¸”ä¸å½±å“è´¨é‡å’Œè¡¨ç°åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å‡å°‘äº†100å€é‡‡æ ·æ­¥éª¤ã€‚ç”¨æˆ·ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººæ€§åŒ–ã€é€‚å½“æ€§å’Œé£æ ¼æ­£ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIDiffGesæ˜¯ä¸€ä¸ªåŸºäºè§£è€¦åŠéšå¼æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºä»è¯­éŸ³ç”Ÿæˆé«˜è´¨é‡ã€è¡¨è¾¾ä¸°å¯Œçš„æ‰‹åŠ¿ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å®ç°å¤§æ­¥é‡‡æ ·æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DIDiffGeså°†æ‰‹åŠ¿æ•°æ®è§£è€¦ä¸ºèº«ä½“å’Œæ‰‹éƒ¨åˆ†å¸ƒï¼Œå¹¶è¿›ä¸€æ­¥åˆ†è§£ä¸ºè¾¹é™…å’Œæ¡ä»¶åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡éšå¼å»ºæ¨¡è¾¹é™…åˆ†å¸ƒå’Œæ˜¾å¼å­¦ä¹ æ¡ä»¶åˆ†å¸ƒçš„ç­–ç•¥ï¼Œå¢å¼ºäº†GANè®­ç»ƒç¨³å®šæ€§å¹¶ä¿è¯äº†æ‰‹åŠ¿çš„è¡¨è¾¾æ€§ã€‚</li>
<li>DIDiffGesèƒ½å¤Ÿåœ¨ä»…10ä¸ªé‡‡æ ·æ­¥éª¤å†…ç”Ÿæˆæ‰‹åŠ¿ï¼Œè¾ƒç°æœ‰æ–¹æ³•å‡å°‘äº†é‡‡æ ·æ­¥éª¤æ•°é‡ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒDIDiffGesåœ¨äººæ€§åŒ–ã€é€‚å½“æ€§å’Œé£æ ¼æ­£ç¡®æ€§æ–¹é¢ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bab0de8a6cc1b2a82baaa0ed3319976.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e9ce51345231c8950dc39d5df0d9d42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3044bae71a936f3ebd196e50603a7ffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c110d750a0db758bba22f8774acaa721.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="From-Faces-to-Voices-Learning-Hierarchical-Representations-for-High-quality-Video-to-Speech"><a href="#From-Faces-to-Voices-Learning-Hierarchical-Representations-for-High-quality-Video-to-Speech" class="headerlink" title="From Faces to Voices: Learning Hierarchical Representations for   High-quality Video-to-Speech"></a>From Faces to Voices: Learning Hierarchical Representations for   High-quality Video-to-Speech</h2><p><strong>Authors:Ji-Hoon Kim, Jeongsoo Choi, Jaehun Kim, Chaeyoung Jung, Joon Son Chung</strong></p>
<p>The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis. A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech. This is achieved by learning of hierarchical representations from video to speech. Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages â€“ content, timbre, and prosody modeling. In each stage, we align visual factors â€“ lip movements, face identity, and facial expressions â€“ with corresponding acoustic counterparts to ensure the seamless transformation. Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution. Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯ä»æ— å£°çš„å¯¹è¯è§†é¢‘ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ï¼Œè¿™ä¸€ä»»åŠ¡ä¹Ÿç§°ä¸ºè§†é¢‘åˆ°è¯­éŸ³çš„åˆæˆã€‚è§†é¢‘åˆ°è¯­éŸ³åˆæˆä¸­çš„ä¸€å¤§æŒ‘æˆ˜åœ¨äºæ— å£°è§†é¢‘å’Œå¤šç»´åº¦çš„è¯­éŸ³ä¹‹é—´å­˜åœ¨çš„å·¨å¤§æ¨¡æ€å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘åˆ°è¯­éŸ³ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæœ‰æ•ˆåœ°å¼¥è¡¥äº†è¿™ä¸€æ¨¡æ€å·®è·ï¼Œå¤§å¤§æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚è¿™æ˜¯é€šè¿‡ä»è§†é¢‘åˆ°è¯­éŸ³å­¦ä¹ å±‚æ¬¡åŒ–è¡¨ç¤ºæ¥å®ç°çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªè¿ç»­é˜¶æ®µâ€”â€”å†…å®¹ã€éŸ³è´¨å’Œè¯­è°ƒå»ºæ¨¡ï¼Œå°†æ— å£°è§†é¢‘é€æ¸è½¬åŒ–ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†è§†è§‰å› ç´ ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œã€é¢éƒ¨èº«ä»½å’Œé¢éƒ¨è¡¨æƒ…ï¼‰ä¸ç›¸åº”çš„å£°éŸ³å¯¹åº”å› ç´ è¿›è¡Œå¯¹é½ï¼Œä»¥ç¡®ä¿æ— ç¼è½¬æ¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»è§†è§‰è¡¨ç¤ºç”Ÿæˆç°å®ä¸”è¿è´¯çš„è¯­éŸ³ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æµåŒ¹é…æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»ç®€å•çš„å…ˆéªŒåˆ†å¸ƒä¼°è®¡å‡ºåˆ°ç›®æ ‡è¯­éŸ³åˆ†å¸ƒçš„ç›´æ¥è½¨è¿¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸çœŸå®è¯­éŸ³ç›¸æ¯”çš„å‡ºè‰²ç”Ÿæˆè´¨é‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16956v1">PDF</a> CVPR 2025, demo page: <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/faces2voices/">https://mm.kaist.ac.kr/projects/faces2voices/</a></p>
<p><strong>Summary</strong><br>æœ¬ç ”ç©¶æ—¨åœ¨ä»æ— å£°çš„è„¸éƒ¨è§†é¢‘ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ï¼Œä¹Ÿè¢«ç§°ä¸ºè§†é¢‘åˆ°è¯­éŸ³çš„åˆæˆã€‚è¯¥æŒ‘æˆ˜åœ¨äºæ— å£°è§†é¢‘å’Œå¤šå…ƒåŒ–çš„è¯­éŸ³ä¹‹é—´æœ‰ç€æ˜¾è‘—çš„æ¨¡å¼å·®è·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘åˆ°è¯­éŸ³ç³»ç»Ÿï¼Œé€šè¿‡ä»è§†é¢‘å­¦ä¹ å±‚çº§è¡¨è¾¾æ¥æœ‰æ•ˆåœ°å¼¥åˆè¿™ä¸€æ¨¡å¼å·®è·ï¼Œä»è€Œæå¤§åœ°æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬é€šè¿‡å†…å®¹ã€éŸ³è´¨å’Œè¯­è°ƒå»ºæ¨¡ä¸‰ä¸ªé˜¶æ®µï¼Œé€æ­¥å°†æ— å£°è§†é¢‘è½¬åŒ–ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ã€‚åœ¨æ¯ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬éƒ½èƒ½å°†è§†è§‰å› ç´ ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œã€é¢éƒ¨èº«ä»½å’Œé¢éƒ¨è¡¨æƒ…ï¼‰ä¸ç›¸åº”çš„å£°éŸ³å› ç´ è¿›è¡ŒåŒ¹é…ï¼Œç¡®ä¿æµç•…çš„è½¬åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»è§†è§‰è¡¨è¾¾ç”ŸæˆçœŸå®è¿è´¯çš„è¯­éŸ³ï¼Œæˆ‘ä»¬é‡‡ç”¨æµé‡åŒ¹é…æ¨¡å‹ä¼°ç®—ä»ç®€å•å…ˆéªŒåˆ†å¸ƒåˆ°ç›®æ ‡è¯­éŸ³åˆ†å¸ƒçš„è½¨è¿¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸çœŸå®å‘éŸ³ç›¸æ¯”çš„å‡ºè‰²ç”Ÿæˆè´¨é‡ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ—¨åœ¨å®ç°è§†é¢‘åˆ°è¯­éŸ³çš„åˆæˆæŠ€æœ¯ï¼Œå³ä»æ— å£°çš„è„¸éƒ¨è§†é¢‘ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ã€‚</li>
<li>è§†é¢‘ä¸è¯­éŸ³é—´å­˜åœ¨æ¨¡å¼å·®è·ï¼Œä¸ºæ­¤æå‡ºä¸€ç§æ–°é¢–çš„è§†é¢‘åˆ°è¯­éŸ³ç³»ç»Ÿæ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚</li>
<li>é€šè¿‡å†…å®¹ã€éŸ³è´¨å’Œè¯­è°ƒå»ºæ¨¡ä¸‰ä¸ªé˜¶æ®µï¼Œå°†æ— å£°è§†é¢‘è½¬åŒ–ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ã€‚</li>
<li>åœ¨æ¯ä¸ªé˜¶æ®µä¸­åŒ¹é…è§†è§‰å’Œå£°éŸ³å› ç´ ï¼Œç¡®ä¿æµç•…çš„è½¬åŒ–è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨æµé‡åŒ¹é…æ¨¡å‹ç”ŸæˆçœŸå®è¿è´¯çš„è¯­éŸ³ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cde97e2ffe93194b9c3831c7794a6585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b4e74c7247e36b83834c10b69bfc1c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-243c863070db1aed085e4ec0c3daf6ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dab6905813082bb76130317c6986d512.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Modelling-Emotions-in-Face-to-Face-Setting-The-Interplay-of-Eye-Tracking-Personality-and-Temporal-Dynamics"><a href="#Modelling-Emotions-in-Face-to-Face-Setting-The-Interplay-of-Eye-Tracking-Personality-and-Temporal-Dynamics" class="headerlink" title="Modelling Emotions in Face-to-Face Setting: The Interplay of   Eye-Tracking, Personality, and Temporal Dynamics"></a>Modelling Emotions in Face-to-Face Setting: The Interplay of   Eye-Tracking, Personality, and Temporal Dynamics</h2><p><strong>Authors:Meisam Jamshidi Seikavandi, Jostein Fimland, Maria Barrett, Paolo Burelli</strong></p>
<p>Accurate emotion recognition is pivotal for nuanced and engaging human-computer interactions, yet remains difficult to achieve, especially in dynamic, conversation-like settings. In this study, we showcase how integrating eye-tracking data, temporal dynamics, and personality traits can substantially enhance the detection of both perceived and felt emotions. Seventy-three participants viewed short, speech-containing videos from the CREMA-D dataset, while being recorded for eye-tracking signals (pupil size, fixation patterns), Big Five personality assessments, and self-reported emotional states. Our neural network models combined these diverse inputs including stimulus emotion labels for contextual cues and yielded marked performance gains compared to the state-of-the-art. Specifically, perceived valence predictions reached a macro F1-score of 0.76, and models incorporating personality traits and stimulus information demonstrated significant improvements in felt emotion accuracy. These results highlight the benefit of unifying physiological, individual and contextual factors to address the subjectivity and complexity of emotional expression. Beyond validating the role of user-specific data in capturing subtle internal states, our findings inform the design of future affective computing and human-agent systems, paving the way for more adaptive and cross-individual emotional intelligence in real-world interactions. </p>
<blockquote>
<p>ç²¾ç¡®çš„æƒ…ç»ªè¯†åˆ«å¯¹äºå¾®å¦™ä¸”å¼•äººå…¥èƒœçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œç„¶è€Œä»éš¾ä»¥å®ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€ç±»ä¼¼å¯¹è¯çš„ç¯å¢ƒä¸­ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•æ•´åˆçœ¼çƒè¿½è¸ªæ•°æ®ã€æ—¶é—´åŠ¨æ€å’Œäººæ ¼ç‰¹è´¨ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ„ŸçŸ¥æƒ…ç»ªå’Œæ„Ÿå—æƒ…ç»ªçš„è¯†åˆ«ã€‚73åå‚ä¸è€…è§‚çœ‹äº†æ¥è‡ªCREMA-Dæ•°æ®é›†çš„åŒ…å«è¯­éŸ³çš„çŸ­ç‰‡ï¼ŒåŒæ—¶è®°å½•çœ¼çƒè¿½è¸ªä¿¡å·ï¼ˆç³å­”å¤§å°ã€æ³¨è§†æ¨¡å¼ï¼‰ã€äº”å¤§äººæ ¼è¯„ä¼°å’Œè‡ªæŠ¥çš„æƒ…ç»ªçŠ¶æ€ã€‚æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¨¡å‹ç»“åˆäº†è¿™äº›å¤šæ ·åŒ–çš„è¾“å…¥ï¼ŒåŒ…æ‹¬åˆºæ¿€æƒ…ç»ªæ ‡ç­¾ä½œä¸ºä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œæ„ŸçŸ¥ä»·å€¼é¢„æµ‹çš„å®è§‚F1åˆ†æ•°è¾¾åˆ°äº†0.76ï¼Œç»“åˆäººæ ¼ç‰¹è´¨å’Œåˆºæ¿€ä¿¡æ¯çš„æ¨¡å‹åœ¨æ„Ÿå—æƒ…ç»ªå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ç»Ÿä¸€ç”Ÿç†ã€ä¸ªä½“å’Œä¸Šä¸‹æ–‡å› ç´ çš„å¥½å¤„ï¼Œä»¥è§£å†³æƒ…ç»ªè¡¨è¾¾çš„ä¸»è§‚æ€§å’Œå¤æ‚æ€§ã€‚é™¤äº†éªŒè¯ç”¨æˆ·ç‰¹å®šæ•°æ®åœ¨æ•æ‰å¾®å¦™å†…éƒ¨çŠ¶æ€ä¸­çš„ä½œç”¨ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜ä¸ºæœªæ¥æƒ…æ„Ÿè®¡ç®—å’Œäººæœºä»£ç†ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†ä¿¡æ¯ï¼Œä¸ºç°å®ä¸–ç•Œä¸­æ›´è‡ªé€‚åº”å’Œè·¨ä¸ªä½“çš„æƒ…ç»ªæ™ºèƒ½é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16532v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†å°†çœ¼åŠ¨æ•°æ®ã€æ—¶é—´åŠ¨æ€ä»¥åŠäººæ ¼ç‰¹è´¨èåˆï¼Œå¯¹æ„ŸçŸ¥å’Œå†…åœ¨æƒ…ç»ªè¿›è¡Œè¯†åˆ«çš„ç ”ç©¶è¿›å±•ã€‚ç ”ç©¶é‡‡ç”¨äº†73åå‚ä¸è€…çš„å®éªŒæ•°æ®ï¼Œåˆ†æç³å­”å¤§å°ã€æ³¨è§†æ¨¡å¼ç­‰çœ¼åŠ¨ä¿¡å·ä»¥åŠåŸºäºâ€œäº”å¤§äººæ ¼æ¨¡å‹â€çš„ä¸ªæ€§è¯„ä¼°ä¸è‡ªæˆ‘æŠ¥å‘Šæƒ…æ„ŸçŠ¶æ€çš„ç»“æœã€‚åˆ©ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç»“åˆæƒ…ç»ªæ ‡ç­¾ä½œä¸ºä¸Šä¸‹æ–‡çº¿ç´¢å¤„ç†å„ç§æ•°æ®è¾“å…¥ï¼Œç›¸æ¯”äºä¼ ç»Ÿæ¨¡å‹å±•ç°å‡ºäº†æ˜¾è‘—çš„è¯†åˆ«æå‡æ•ˆæœã€‚è¯¥ç ”ç©¶å¼ºåŒ–äº†ä¸ªæ€§åŒ–å¿ƒç†çŠ¶å†µçš„ç»Ÿä¸€æ•´åˆç†å¿µçš„é‡è¦æ€§ã€‚å…¶å®éªŒè¯æ˜äº†é’ˆå¯¹ä¸ªæ€§åŒ–å’Œæƒ…å¢ƒçš„æ¡†æ¶å…·æœ‰å¯è¡Œæ€§åŠæœ‰æ•ˆæ”¹è¿›å†…å¿ƒæƒ…æ„ŸçŠ¶æ€çš„æ£€æµ‹ã€‚è¿™å¯¹äºå‘å±•æ›´ä¸ºæ™ºèƒ½åŒ–çš„äººç±»æƒ…æ„Ÿåˆ†ææŠ€æœ¯å’Œä¿ƒè¿›ç°å®åœºæ™¯ä¸­æ›´ä¸ºäººæ€§åŒ–çš„äººå·¥æ™ºèƒ½åº”ç”¨æ„ä¹‰é‡å¤§ã€‚å› æ­¤æ€»ç»“èµ·æ¥è¯¥æ–‡ç« çš„æ ¸å¿ƒå†…å®¹åœ¨äºï¼Œå¤šæ¨¡æ€ä¿¡æ¯èåˆï¼Œå°¤å…¶æ˜¯å°†äººæ ¼ç‰¹è´¨èå…¥æƒ…ç»ªè¯†åˆ«é¢†åŸŸèƒ½å¤Ÿæœ‰æ•ˆæé«˜å‡†ç¡®æ€§ï¼Œæ¨åŠ¨æƒ…æ„Ÿè®¡ç®—çš„è¿›æ­¥å’Œäººå·¥æ™ºèƒ½äº¤äº’ä½“éªŒçš„å‘å±•ã€‚è¯¥ç ”ç©¶æˆæœå¯¹äººæœºäº¤äº’ç³»ç»Ÿæœªæ¥çš„è®¾è®¡äº§ç”Ÿäº†é‡è¦çš„å¯ç¤ºä½œç”¨ã€‚åŒæ—¶å¯¹äºç†è§£äººç±»æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§å’Œä¸»è§‚æ€§ï¼Œæœ‰åŠ©äºä¸ºå»ºç«‹é€‚åº”æ€§æ›´å¹¿çš„ä¸ªäººåŒ–æ™ºèƒ½æä¾›åŸºç¡€æ”¯æŒã€‚æœªæ¥çš„æƒ…ç»ªè¯†åˆ«æŠ€æœ¯åœ¨é¢„æµ‹æ¨¡å‹ä¸­å¯ä»¥å€Ÿé‰´å¹¶çº³å…¥å¤šç§å½¢å¼çš„è®¤çŸ¥èåˆä½œä¸ºåˆ¤æ–­ä¾æ®æ¥æå‡ç»“æœå‡†ç¡®ç‡å’Œç²¾ç»†åº¦ï¼Œå°†çœŸå®ç¯å¢ƒå¯¹äº’åŠ¨æ‰€é€ æˆçš„å½±å“ç»†åŒ–å…¥å¿ƒç†è¾…åŠ©ä»¥åŠæ²Ÿé€šç•Œé¢æ”¹å–„ç­–ç•¥ä¹‹ä¸­ã€‚æ­¤é¡¹ç ”ç©¶éªŒè¯äº†åˆ©ç”¨ç”Ÿç†ä¿¡æ¯è¾…åŠ©ç†è§£æƒ…æ„Ÿæ„ŸçŸ¥çš„æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ã€‚è¯¥å‘ç°ä¹Ÿä¸ºè®¾è®¡æ›´äººæ€§åŒ–çš„äº¤äº’ç³»ç»Ÿå’Œæƒ…ç»ªå“åº”ç•Œé¢æŒ‡æ˜äº†æ–¹å‘ã€‚å°†æœ‰åˆ©äºæƒ…ç»ªåˆ†ææŠ€æœ¯å’Œæœªæ¥è·¨ä¸ªä½“å·®å¼‚æƒ…ç»ªç†è§£çš„æ¨è¿›å’Œæœªæ¥å‘å±•ä¼˜åŒ–ç°æœ‰æ¨¡å‹çš„æ€§èƒ½å’Œå±€é™æ€§å¯¹äºæ‰©å¤§ç°å®åº”ç”¨çš„æƒ…å¢ƒä½¿ç”¨èŒƒç•´å°†äº§ç”Ÿå·¨å¤§å½±å“ä»¥åŠåº”ç”¨ä»·å€¼æ–¹é¢çš„é•¿è¿œæŒ–æ˜å’Œåˆ›æ–°å»¶ä¼¸éƒ½æœ‰æå…¶é‡è¦æ„ä¹‰ä½œç”¨æœªæ¥å…·å¤‡æ›´æ·±å…¥äººæ€§çš„æƒ…å•†æ°´å¹³äººå·¥æ™ºèƒ½ä½“ç³»åŒ–çš„å…¨é¢å‘å±•è¿›ç¨‹ä¹‹ä¼˜åŒ–èåˆå¤šä¸ªç»´åº¦çš„ä¿¡æ¯æ¥å…±åŒæå‡æƒ…ç»ªæ„ŸçŸ¥çš„ç²¾å‡†åº¦ã€‚æœªæ¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼çš„å·®å¼‚ä»¥åŠå¦‚ä½•åˆ©ç”¨è¿™äº›å·®å¼‚æå‡æƒ…ç»ªè¯†åˆ«çš„ç²¾åº¦ã€‚å¯¹æ•´ä½“è¯­è¨€ä¹ æƒ¯å’Œè§†è§‰ç¯å¢ƒçš„å…±åŒå½±å“å’Œæ·±åŒ–å¤šç»´èåˆå¯¹æœªæ¥å®Œå–„è¯­å¢ƒç»†åŒ–å› ç´ ä¹Ÿå°†æ¨è¿›æ­¤é¡¹ç ”ç©¶æœ‰å¹¿æ³›åº”ç”¨ç©ºé—´é€‚ç”¨äºæ”¯æŒå¤šä»»åŠ¡æ¨ç†å’Œæœªæ¥è·¨ç•Œè®¡ç®—çš„ä¸æ–­å°è¯•å’Œæé«˜äº§å“æˆç†Ÿèƒ½åŠ›äººå·¥æ™ºèƒ½å·²æ¶‰è¶³å½“ä¸‹ç¬æ¯ä¸‡å˜çš„ä¸ªæ€§åŒ–ç»¼åˆä¿¡æ¯å¤„ç†é¢†åŸŸåœ¨é¢ä¸´æ›´å¤æ‚çš„æƒ…æ„Ÿè¡¨è¾¾æŒ‘æˆ˜æ—¶æä¾›äº†æ›´çµæ´»æœ‰æ•ˆçš„åº”å¯¹æ–¹æ¡ˆåœ¨ä¸ªæ€§åŒ–æœåŠ¡é¢†åŸŸæœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚è¯¥ç ”ç©¶å°†å¯¹æé«˜äººæœºäº¤äº’çš„å‡†ç¡®æ€§å’Œç²¾ç»†åº¦äº§ç”Ÿæ·±è¿œå½±å“æ˜¯æ¨è¿›æƒ…æ„Ÿæ™ºèƒ½å‘å±•çš„é‡è¦ä¸€æ­¥åŒæ—¶æä¾›äº†å°†å¿ƒç†æƒ…æ„Ÿè®¤çŸ¥åº”ç”¨äºç°å®äº’åŠ¨åœºæ™¯çš„ç†è®ºä¾æ®å’Œå¯èƒ½è·¯å¾„å¹¶æ¨åŠ¨äº†æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’Œå¯ç¤ºä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦çš„æ€è·¯å’Œæ–¹å‘ã€‚å¯¹äºæœªæ¥çš„æ™ºèƒ½äº¤äº’ç³»ç»Ÿæ¥è¯´å…·æœ‰æå…¶é‡è¦çš„æ„ä¹‰ã€‚é€šè¿‡èåˆå¤šæºä¿¡æ¯æ„å»ºæ›´åŠ ç²¾å‡†çš„æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿæ¥æé«˜ç”¨æˆ·æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ä¸ºè§£å†³æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’é¢†åŸŸçš„å¤æ‚é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•åŒæ—¶ä¸ºæ”¹å–„äººæœºäº¤äº’çš„æµç•…æ€§å’Œæå‡ç”¨æˆ·ä½“éªŒæä¾›äº†æœ‰åŠ›çš„æ”¯æŒå¹¶æ¨åŠ¨äº†äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•è¿›æ­¥å’Œé©æ–°çªç ´å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„ç¤¾ä¼šä»·å€¼ã€‚è¯¥ç ”ç©¶åœ¨æƒ…ç»ªè¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå¯ç¤ºåŒæ—¶ä¹Ÿé¢ä¸´ç€ä¸€äº›æŒ‘æˆ˜å¦‚ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æƒ…æ„Ÿè¡¨è¾¾å·®å¼‚ä»¥åŠæ–°æŠ€æœ¯ä¸æ–­æ¶Œç°ç»™è¿™ä¸€é¢†åŸŸå¸¦æ¥çš„æ–°çš„æŒ‘æˆ˜éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ä¸ºäººç±»çš„å¿ƒç†å¥åº·å’Œå¿ƒç†æœåŠ¡æä¾›æ›´åŠ å…¨é¢æœ‰æ•ˆçš„æ”¯æŒä¸ºæ„å»ºæ›´åŠ å’Œè°çš„äººæœºäº¤äº’ç¯å¢ƒæä¾›æœ‰åŠ›æ”¯æŒã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®æƒ…æ„Ÿè¯†åˆ«å¯¹äºè‡ªç„¶ä¸”å¯Œæœ‰å†…æ¶µçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œå°¤å…¶å¯¹äºåŠ¨æ€å¯¹è¯åœºæ™¯è€Œè¨€å°¤ä¸ºé‡è¦ã€‚</li>
<li>é€šè¿‡ç»“åˆçœ¼åŠ¨æ•°æ®ï¼ˆå¦‚ç³å­”å¤§å°å’Œæ³¨è§†æ¨¡å¼ï¼‰ã€æ—¶é—´åŠ¨æ€ä»¥åŠäººæ ¼ç‰¹è´¨ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æƒ…æ„Ÿæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†å¤šç§è¾“å…¥æ•°æ®ï¼ˆåŒ…æ‹¬åˆºæ¿€æƒ…ç»ªæ ‡ç­¾ï¼‰ï¼Œå¯è·å–æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥ä»·å€¼é¢„æµ‹æ–¹é¢è¾¾åˆ°è¾ƒé«˜çš„F1å¾—åˆ†ã€‚</li>
<li>å½“æ¨¡å‹èå…¥äººæ ¼ç‰¹è´¨å’Œåˆºæ¿€ä¿¡æ¯æ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„å†…åœ¨æƒ…æ„Ÿçš„å‡†ç¡®åº¦è¯†åˆ«ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ•´åˆç”Ÿç†ã€ä¸ªä½“åŠä¸Šä¸‹æ–‡å› ç´ çš„é‡è¦æ€§ï¼Œä»¥åº”å¯¹æƒ…æ„Ÿè¡¨è¾¾ä¸­çš„ä¸»è§‚æ€§å’Œå¤æ‚æ€§ã€‚</li>
<li>é™¤äº†éªŒè¯ç”¨æˆ·ç‰¹å®šæ•°æ®åœ¨æ•æ‰å¾®å¦™å†…åœ¨çŠ¶æ€ä¸­çš„ä½œç”¨å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä¸ºæƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç³»ç»Ÿçš„æœªæ¥è®¾è®¡æä¾›äº†å®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e0f5ff70734fca53693b8e2dc5cdb32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75b8c9e333203edc8670e6c83f3152b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63e4bbbc27c8811467306417c051110f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a874755b58c9b5a52e0a2843afe763b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159ab7c4c54ccce0a290e33591095e39.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition"><a href="#Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition" class="headerlink" title="Bimodal Connection Attention Fusion for Speech Emotion Recognition"></a>Bimodal Connection Attention Fusion for Speech Emotion Recognition</h2><p><strong>Authors:Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss</strong></p>
<p>Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intra- and inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åœ¨æå–æ•æ‰å¾®å¦™æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥æ˜¯æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ¨¡æ€è¿æ¥æ³¨æ„åŠ›èåˆï¼ˆBCAFï¼‰æ–¹æ³•ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚äº¤äº’è¿æ¥ç½‘ç»œé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¯¹éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰è¡¥å……ï¼Œå¹¶æŒ–æ˜äº†æ¨¡æ€å†…éƒ¨å’Œæ¨¡æ€ä¹‹é—´çš„äº¤äº’ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œé™ä½äº†è·¨æ¨¡æ€å™ªå£°ï¼Œæ•æ‰äº†éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05858v3">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æŒ‘æˆ˜åœ¨äºæå–æ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾çš„éš¾åº¦ã€‚ä¸ºäº†å»ºç«‹æœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼Œç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥æ˜¯å…³é”®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŒæ¨¡æ€è¿æ¥æ³¨æ„åŠ›èåˆï¼ˆBCAFï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚äº¤äº’è¿æ¥ç½‘ç»œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å¯¹éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰è¡¥å……æ€§å¹¶æ¢è®¨äº†æ¨¡æ€å†…éƒ¨å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œå‡å°‘äº†è·¨æ¨¡æ€å™ªå£°å¹¶æ•æ‰äº†éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæå–æ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾ã€‚</li>
<li>å»ºç«‹æœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿéœ€è¦ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥ã€‚</li>
<li>BCAFæ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚</li>
<li>äº¤äº’è¿æ¥ç½‘ç»œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚</li>
<li>åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰è¡¥å……æ€§ï¼Œæ¢è®¨äº†æ¨¡æ€å†…éƒ¨å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ã€‚</li>
<li>ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œå‡å°‘äº†è·¨æ¨¡æ€å™ªå£°ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´ç›¸å…³æ€§çš„æ•æ‰èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a90c72a47049316459ea506606004dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b152150f4127bfeeeb61fa35767cd406.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f1d85dab88f0d70a01c8bc9f5b187a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37a353dc3cedfb7a239b847111d82cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d29b3f719366cf26dd167bad0e298b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f228ba52ba5d222be261a00ef566ca81.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis"></a>Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{S-DiT}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to S-DiT to reduce the difficulty of alignment learning without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that S-DiT achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²ç»æ˜¾è‘—æé«˜äº†è¯­éŸ³è´¨é‡å’Œè¡¨ç°åŠ›ï¼Œä½†ä¸»æµç³»ç»Ÿä»ç„¶é¢ä¸´ä¸è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡ç›¸å…³çš„é—®é¢˜ï¼š1ï¼‰æ²¡æœ‰æ˜ç¡®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨ä¸­çš„éš¾å¥ï¼›2ï¼‰åŸºäºé¢„å®šä¹‰å¯¹é½çš„æ¨¡å‹å—åˆ°å¼ºåˆ¶å¯¹é½çš„è‡ªç„¶æ€§çº¦æŸã€‚æœ¬æ–‡ä»‹ç»äº†S-DiTï¼Œä¸€ç§å…·æœ‰åˆ›æ–°ç¨€ç–å¯¹é½ç®—æ³•çš„TTSç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸæŒ‡å¯¼æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºS-DiTæä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œä»¥å‡å°‘å¯¹é½å­¦ä¹ çš„éš¾åº¦ï¼ŒåŒæ—¶ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥è¿›è¡Œå£éŸ³å¼ºåº¦è°ƒæ•´ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯æ¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒS-DiTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ï¼Œå¹¶å¯¹å£éŸ³å¼ºåº¦å®ç°äº†é«˜åº¦çµæ´»çš„æ§åˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥åœ¨ä»…8ä¸ªé‡‡æ ·æ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸€åˆ†é’Ÿè¯­éŸ³ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºS-DiTçš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå…¶é‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•æŒ‡å¯¼æ½œåœ¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ã€‚é€šè¿‡æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼ŒS-DiTé™ä½äº†å¯¹é½å­¦ä¹ çš„éš¾åº¦ï¼ŒåŒæ—¶ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œå®ç°äº†é«˜åº¦è‡ªç„¶æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»å¼•å¯¼ç­–ç•¥è¿›è¡Œå£éŸ³å¼ºåº¦è°ƒæ•´ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒS-DiTåœ¨é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶æ”¯æŒé«˜åº¦çµæ´»çš„å£éŸ³å¼ºåº¦æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>S-DiTæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œé‡‡ç”¨ç¨€ç–å¯¹é½ç®—æ³•æŒ‡å¯¼æ½œåœ¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ã€‚</li>
<li>ç¨€ç–å¯¹é½ç®—æ³•æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œé™ä½å¯¹é½å­¦ä¹ éš¾åº¦ï¼ŒåŒæ—¶ä¿æŒæœç´¢ç©ºé—´çš„é«˜åº¦çµæ´»ã€‚</li>
<li>S-DiTç³»ç»Ÿå®ç°äº†é«˜åº¦è‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»å¼•å¯¼ç­–ç•¥è¿›è¡Œå£éŸ³å¼ºåº¦è°ƒæ•´ã€‚</li>
<li>åˆ†æ®µæ•´æµæµæŠ€æœ¯è¢«ç”¨äºåŠ é€Ÿè¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒS-DiTåœ¨é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-617827defbfc37fe71a8094f7a096acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c61848e90138a47623544e6508ddede3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7235bde963e172c8da3b664a261253c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d61b51cb69a1990f12eb9062880aadfb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multimodal-Large-Language-Models-for-Image-Text-and-Speech-Data-Augmentation-A-Survey"><a href="#Multimodal-Large-Language-Models-for-Image-Text-and-Speech-Data-Augmentation-A-Survey" class="headerlink" title="Multimodal Large Language Models for Image, Text, and Speech Data   Augmentation: A Survey"></a>Multimodal Large Language Models for Image, Text, and Speech Data   Augmentation: A Survey</h2><p><strong>Authors:Ranjan Sapkota, Shaina Raza, Maged Shoman, Achyut Paudel, Manoj Karkee</strong></p>
<p>In the past five years, research has shifted from traditional Machine Learning (ML) and Deep Learning (DL) approaches to leveraging Large Language Models (LLMs) , including multimodality, for data augmentation to enhance generalization, and combat overfitting in training deep convolutional neural networks. However, while existing surveys predominantly focus on ML and DL techniques or limited modalities (text or images), a gap remains in addressing the latest advancements and multi-modal applications of LLM-based methods. This survey fills that gap by exploring recent literature utilizing multimodal LLMs to augment image, text, and audio data, offering a comprehensive understanding of these processes. We outlined various methods employed in the LLM-based image, text and speech augmentation, and discussed the limitations identified in current approaches. Additionally, we identified potential solutions to these limitations from the literature to enhance the efficacy of data augmentation practices using multimodal LLMs. This survey serves as a foundation for future research, aiming to refine and expand the use of multimodal LLMs in enhancing dataset quality and diversity for deep learning applications. (Surveyed Paper GitHub Repo: <a target="_blank" rel="noopener" href="https://github.com/WSUAgRobotics/data-aug-multi-modal-llm">https://github.com/WSUAgRobotics/data-aug-multi-modal-llm</a>. Keywords: LLM data augmentation, Grok text data augmentation, DeepSeek image data augmentation, Grok speech data augmentation, GPT audio augmentation, voice augmentation, DeepSeek for data augmentation, DeepSeek R1 text data augmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM, Text Augmentation using LLM, LLM data augmentation for deep learning applications) </p>
<blockquote>
<p>è¿‡å»äº”å¹´ï¼Œç ”ç©¶å·²ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•è½¬å‘åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ•°æ®å¢å¼ºæ¥æå‡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åº”å¯¹æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç„¶è€Œï¼Œå°½ç®¡ç°æœ‰è°ƒæŸ¥ä¸»è¦é›†ä¸­åœ¨MLå’ŒDLæŠ€æœ¯æˆ–æœ‰é™æ¨¡å¼ï¼ˆæ–‡æœ¬æˆ–å›¾åƒï¼‰ä¸Šï¼Œä½†åœ¨è§£å†³åŸºäºLLMçš„æ–¹æ³•çš„æœ€æ–°è¿›å±•å’Œå¤šæ¨¡æ€åº”ç”¨æ–¹é¢ä»å­˜åœ¨å·®è·ã€‚æœ¬è°ƒæŸ¥é€šè¿‡æ¢ç´¢åˆ©ç”¨å¤šæ¨¡æ€LLMå¢å¼ºå›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘æ•°æ®çš„æœ€æ–°æ–‡çŒ®æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå…¨é¢ç†è§£è¿™äº›è¿‡ç¨‹ã€‚æˆ‘ä»¬æ¦‚è¿°äº†åŸºäºLLMçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³å¢å¼ºæ‰€é‡‡ç”¨çš„å„ç§æ–¹æ³•ï¼Œå¹¶è®¨è®ºäº†å½“å‰æ–¹æ³•ä¸­æ‰€è¯†åˆ«çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»æ–‡çŒ®ä¸­ç¡®å®šäº†è§£å†³è¿™äº›å±€é™æ€§çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜ä½¿ç”¨å¤šæ¨¡æ€LLMçš„æ•°æ®å¢å¼ºå®è·µçš„æœ‰æ•ˆæ€§ã€‚æœ¬è°ƒæŸ¥ä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œæ—¨åœ¨å®Œå–„å¹¶æ‰©å¤§å¤šæ¨¡æ€LLMåœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­çš„ä½¿ç”¨ï¼Œä»¥æé«˜æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ï¼ˆå·²è°ƒæŸ¥è®ºæ–‡GitHubä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/WSUAgRobotics/data-aug-multi-modal-llm%E3%80%82%E5%85%B3%E9%94%AE%E8%AF%8D%EF%BC%9ALLM%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81Grok%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81DeepSeek%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81Grok%E8%AF%AD%E9%9F%B3%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81GPT%E9%9F%B3%E9%A2%91%E5%A2%9E%E5%BC%BA%E3%80%81%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA%E3%80%81DeepSeek%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81DeepSeek">https://github.com/WSUAgRobotics/data-aug-multi-modal-llmã€‚å…³é”®è¯ï¼šLLMæ•°æ®å¢å¼ºã€Grokæ–‡æœ¬æ•°æ®å¢å¼ºã€DeepSeekå›¾åƒæ•°æ®å¢å¼ºã€Grokè¯­éŸ³æ•°æ®å¢å¼ºã€GPTéŸ³é¢‘å¢å¼ºã€è¯­éŸ³å¢å¼ºã€DeepSeekç”¨äºæ•°æ®å¢å¼ºã€DeepSeek</a> R1æ–‡æœ¬æ•°æ®å¢å¼ºã€DeepSeek R1å›¾åƒå¢å¼ºã€åˆ©ç”¨LLMè¿›è¡Œå›¾åƒå¢å¼ºã€åˆ©ç”¨LLMè¿›è¡Œæ–‡æœ¬å¢å¼ºã€LLMæ•°æ®å¢å¼ºåœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18648v2">PDF</a> 52 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿™ç¯‡è°ƒç ”è®ºæ–‡å¡«è¡¥äº†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å¢å¼ºé¢†åŸŸçš„ç©ºç™½ï¼Œè¯¦ç»†æ¢è®¨äº†åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®çš„æœ€æ–°æ–‡çŒ®ã€‚æ–‡ç« æ¦‚è¿°äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³å¢å¼ºçš„å„ç§æ–¹æ³•ï¼Œå¹¶è®¨è®ºäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ä»¥åŠä»æ–‡çŒ®ä¸­è¯†åˆ«å‡ºçš„è§£å†³è¿™äº›å±€é™æ€§çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œæ—¨åœ¨æ”¹è¿›å’Œæ‹“å±•å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå‡æ·±åº¦å­¦ä¹ åº”ç”¨æ•°æ®é›†è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢çš„ä½¿ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è°ƒç ”è®ºæ–‡å¡«è¡¥äº†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®å¢å¼ºé¢†åŸŸçš„ç©ºç™½ã€‚</li>
<li>è®ºæ–‡è¯¦ç»†æ¢è®¨äº†LLMsåœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®å¢å¼ºä¸­çš„æœ€æ–°åº”ç”¨ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†åŸºäºLLMsçš„å„ç§æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬çš„å±€é™æ€§ã€‚</li>
<li>è°ƒç ”è®ºæ–‡ä»æ–‡çŒ®ä¸­è¯†åˆ«å‡ºäº†è§£å†³LLMsæ•°æ®å¢å¼ºæ–¹æ³•å±€é™æ€§çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥è®ºæ–‡æ—¨åœ¨ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¹è¿›å’Œæ‹“å±•å¤šæ¨¡æ€LLMsåœ¨æé«˜æ·±åº¦å­¦ä¹ æ•°æ®é›†è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†å¤šæ¨¡æ€LLMsåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å¢å¼ºå’Œå¯¹æŠ—è¿‡æ‹Ÿåˆæ–¹é¢çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-490e5966f5d560a225dc90ecb732f7be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95d534ffdc69bc96971111fddd519445.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb37708e4ce94b0baf260c789a28432d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="k2SSL-A-Faster-and-Better-Framework-for-Self-Supervised-Speech-Representation-Learning"><a href="#k2SSL-A-Faster-and-Better-Framework-for-Self-Supervised-Speech-Representation-Learning" class="headerlink" title="k2SSL: A Faster and Better Framework for Self-Supervised Speech   Representation Learning"></a>k2SSL: A Faster and Better Framework for Self-Supervised Speech   Representation Learning</h2><p><strong>Authors:Yifan Yang, Jianheng Zhuo, Zengrui Jin, Ziyang Ma, Xiaoyu Yang, Zengwei Yao, Liyong Guo, Wei Kang, Fangjun Kuang, Long Lin, Daniel Povey, Xie Chen</strong></p>
<p>Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Largeâ€™s performance while requiring only 5&#x2F;8 pre-training steps. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³ç›¸å…³ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è™½ç„¶Transformerå’ŒConformeræ¶æ„åœ¨SSLä¸»å¹²ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†åƒZipformerè¿™æ ·çš„ç¼–ç å™¨åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨SSLä¸­ä»æœªè¢«æ¢ç´¢ã€‚åŒæ—¶ï¼Œç°æœ‰SSLè®­ç»ƒæ¡†æ¶ï¼ˆå¦‚fairseqï¼‰åœ¨æ•°æ®å¤„ç†æ–¹é¢çš„ä½æ•ˆæ€§ï¼Œä¸ºç®¡ç†ä¸æ–­å¢é•¿çš„è®­ç»ƒæ•°æ®å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†k2SSLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæä¾›æ›´å¿«ã€æ›´çœå†…å­˜ã€æ€§èƒ½æ›´å¥½çš„è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ï¼Œä¸“æ³¨äºä¸‹æ¸¸ASRä»»åŠ¡ã€‚ä¼˜åŒ–çš„HuBERTå’Œæå‡ºçš„åŸºäºZipformerçš„SSLç³»ç»Ÿï¼Œåœ¨SSLè®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜ä½¿ç”¨çš„æ˜¾è‘—å‡å°‘ã€‚åœ¨LibriSpeechä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZipformer Baseåœ¨å¾®è°ƒåç›¸å¯¹äºHuBERTå’ŒWavLMæœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜è¾¾34.8%çš„ç›¸å¯¹WERï¼ˆè¯é”™è¯¯ç‡ï¼‰é™ä½ã€‚æ­¤å¤–ï¼Œä¸HuBERT Baseç›¸æ¯”ï¼ŒZipformer Largeåœ¨æ‰©å±•åˆ°60kå°æ—¶çš„LibriLightæ•°æ®æ—¶ï¼Œå±•ç°äº†å“è¶Šçš„æ•ˆç‡ï¼ŒåŒ¹é…äº†HuBERT Largeçš„æ€§èƒ½ï¼Œä½†ä»…éœ€è¦5&#x2F;8çš„é¢„è®­ç»ƒæ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17100v2">PDF</a> Accepted in ICME 2025</p>
<p><strong>Summary</strong></p>
<p>SSLåœ¨è¯­éŸ³ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ç°æœ‰çš„æ¡†æ¶å¦‚fairseqåœ¨å¤„ç†å¤§é‡è®­ç»ƒæ•°æ®æ—¶å­˜åœ¨æ•ˆç‡é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†k2SSLæ¡†æ¶ï¼Œä¸“æ³¨äºä¸‹æ¸¸ASRä»»åŠ¡çš„è‡ªæˆ‘ç›‘ç£è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ï¼Œå…·æœ‰æ›´å¿«ã€æ›´çœå†…å­˜ã€æ€§èƒ½æ›´å¥½çš„ç‰¹ç‚¹ã€‚åŸºäºZipformerçš„SSLç³»ç»Ÿä¼˜åŒ–åï¼Œåœ¨LibriSpeechä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒZipformer Baseåœ¨å¾®è°ƒåç›¸å¯¹äºHuBERTå’ŒWavLMæœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹å­—é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†34.8%ã€‚åŒæ—¶ï¼ŒGPUå°æ—¶é¢„è®­ç»ƒé€Ÿåº¦æé«˜äº†3.5å€ã€‚å½“æ‰©å±•åˆ°LibriLightçš„6ä¸‡å°æ—¶æ•°æ®æ—¶ï¼ŒZipformer Largeå±•ç°äº†å“è¶Šçš„æ•ˆç‡ï¼ŒåŒ¹é…äº†HuBERT Largeçš„æ€§èƒ½ï¼Œä½†é¢„è®­ç»ƒæ­¥éª¤ä»…éœ€è¦å…¶äº”åˆ†ä¹‹å…«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SSLåœ¨è¯­éŸ³ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨è¢«å¹¿æ³›æŠ¥é“ï¼Œä½†ä»å­˜åœ¨æ•°æ®å¤„ç†æ•ˆç‡é—®é¢˜ã€‚</li>
<li>k2SSLæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæä¾›æ›´é«˜æ•ˆã€æ›´å¿«é€Ÿçš„è‡ªæˆ‘ç›‘ç£è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>Zipformerä½œä¸ºä¸€ç§æ–°çš„ç¼–ç å™¨æ¶æ„ï¼Œåœ¨SSLä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ASRä»»åŠ¡ä¸­ã€‚</li>
<li>åœ¨LibriSpeechæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼ŒZipformer Baseç›¸å¯¹äºå…¶ä»–æ¨¡å‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>Zipformer Largeåœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶å±•ç°å‡ºå“è¶Šçš„æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-58e5fe2af6be24c1f7758a92369efa7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a8172062ff5340d82c7376ef869241.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eee5da56fdd7025a7c91a7f1f0121ad8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7e65641e4185272961668f00c2068f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb6a6b6d5c63f5dab527fb632bd225e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70252eb1ad710009b25ef47e1db5dfa0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Sentiment-Analysis-for-Missing-Modality-through-Self-Distillation-and-Unified-Modality-Cross-Attention"><a href="#Enhancing-Multimodal-Sentiment-Analysis-for-Missing-Modality-through-Self-Distillation-and-Unified-Modality-Cross-Attention" class="headerlink" title="Enhancing Multimodal Sentiment Analysis for Missing Modality through   Self-Distillation and Unified Modality Cross-Attention"></a>Enhancing Multimodal Sentiment Analysis for Missing Modality through   Self-Distillation and Unified Modality Cross-Attention</h2><p><strong>Authors:Yuzhe Weng, Haotian Wang, Tian Gao, Kewei Li, Shutong Niu, Jun Du</strong></p>
<p>In multimodal sentiment analysis, collecting text data is often more challenging than video or audio due to higher annotation costs and inconsistent automatic speech recognition (ASR) quality. To address this challenge, our study has developed a robust model that effectively integrates multimodal sentiment information, even in the absence of text modality. Specifically, we have developed a Double-Flow Self-Distillation Framework, including Unified Modality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA), which excels at processing both scenarios with complete modalities and those with missing text modality. In detail, when the text modality is missing, our framework uses the LLM-based model to simulate the text representation from the audio modality, while the MIA module supplements information from the other two modalities to make the simulated text representation similar to the real text representation. To further align the simulated and real representations, and to enable the model to capture the continuous nature of sample orders in sentiment valence regression tasks, we have also introduced the Rank-N Contrast (RNC) loss function. When testing on the CMU-MOSEI, our model achieved outstanding performance on MAE and significantly outperformed other models when text modality is missing. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/WarmCongee/SDUMC">https://github.com/WarmCongee/SDUMC</a> </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­ï¼Œç”±äºæ ‡æ³¨æˆæœ¬æ›´é«˜å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è´¨é‡çš„ä¸ä¸€è‡´æ€§ï¼Œæ”¶é›†æ–‡æœ¬æ•°æ®é€šå¸¸æ¯”è§†é¢‘æˆ–éŸ³é¢‘æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç¨³å¥çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€æƒ…æ„Ÿä¿¡æ¯ï¼Œå³ä½¿åœ¨ç¼ºå°‘æ–‡æœ¬æ¨¡å¼çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†åŒæµè‡ªè’¸é¦æ¡†æ¶ï¼ŒåŒ…æ‹¬ç»Ÿä¸€æ¨¡æ€äº¤å‰æ³¨æ„ï¼ˆUMCAï¼‰å’Œæ¨¡æ€æƒ³è±¡è‡ªç¼–ç å™¨ï¼ˆMIAï¼‰ï¼Œå®ƒæ“…é•¿å¤„ç†å…·æœ‰å®Œæ•´æ¨¡æ€çš„åœºæ™¯å’Œç¼ºå¤±æ–‡æœ¬æ¨¡æ€çš„åœºæ™¯ã€‚åœ¨ç»†èŠ‚ä¸Šï¼Œå½“ç¼ºå°‘æ–‡æœ¬æ¨¡æ€æ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨åŸºäºLLMçš„æ¨¡å‹æ¥æ¨¡æ‹Ÿä»éŸ³é¢‘æ¨¡æ€çš„æ–‡æœ¬è¡¨ç¤ºï¼Œè€ŒMIAæ¨¡å—è¡¥å……äº†æ¥è‡ªå…¶ä»–ä¸¤ä¸ªæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»¥ä½¿æ¨¡æ‹Ÿçš„æ–‡æœ¬è¡¨ç¤ºä¸çœŸå®çš„æ–‡æœ¬è¡¨ç¤ºç›¸ä¼¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¯¹é½æ¨¡æ‹Ÿå’ŒçœŸå®è¡¨ç¤ºï¼Œå¹¶ä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·æƒ…æ„Ÿä»·å›å½’ä»»åŠ¡ä¸­æ ·æœ¬é¡ºåºçš„è¿ç»­æ€§ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Rank-N Contrastï¼ˆRNCï¼‰æŸå¤±å‡½æ•°ã€‚åœ¨CMU-MOSEIä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨MAEä¸Šè¡¨ç°å“è¶Šï¼Œå¹¶ä¸”åœ¨ç¼ºå°‘æ–‡æœ¬æ¨¡æ€çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/WarmCongee/SDUMC">https://github.com/WarmCongee/SDUMC</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15029v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„æ–‡æœ¬æ•°æ®æ”¶é›†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŒæµè‡ªè’¸é¦æ¡†æ¶ï¼ŒåŒ…æ‹¬ç»Ÿä¸€æ¨¡æ€è·¨æ³¨æ„åŠ›å’Œæ¨¡æ€æƒ³è±¡è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç¼ºå¤±æ–‡æœ¬æ¨¡æ€çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€æƒ…æ„Ÿä¿¡æ¯ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»éŸ³é¢‘æ¨¡æ€æ¨¡æ‹Ÿæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨MIAæ¨¡å—è¡¥å……å…¶ä»–ä¸¤ä¸ªæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„æ–‡æœ¬è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†Rank-N ContrastæŸå¤±å‡½æ•°ï¼Œä»¥è¿›ä¸€æ­¥å¯¹é½æ¨¡æ‹Ÿå’ŒçœŸå®è¡¨ç¤ºï¼Œå¹¶æ•è·æƒ…æ„Ÿä»·å€¼å›å½’ä»»åŠ¡ä¸­æ ·æœ¬é¡ºåºçš„è¿ç»­æ€§ã€‚åœ¨CMU-MOSEIæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨MAEä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨ç¼ºå¤±æ–‡æœ¬æ¨¡æ€çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­ï¼Œæ–‡æœ¬æ•°æ®æ”¶é›†é¢ä¸´é«˜æ ‡æ³¨æˆæœ¬å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«è´¨é‡ä¸ä¸€è‡´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„åŒæµè‡ªè’¸é¦æ¡†æ¶èƒ½å¤Ÿæ•´åˆå¤šæ¨¡æ€æƒ…æ„Ÿä¿¡æ¯ï¼Œå°¤å…¶åœ¨ç¼ºå¤±æ–‡æœ¬æ¨¡æ€æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»éŸ³é¢‘æ¨¡æ€æ¨¡æ‹Ÿæ–‡æœ¬è¡¨ç¤ºï¼Œä»¥å¡«è¡¥æ–‡æœ¬ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥MIAæ¨¡å—æ¥è¡¥å……å…¶ä»–æ¨¡æ€çš„ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡æ‹Ÿæ–‡æœ¬è¡¨ç¤ºçš„çœŸå®æ€§ã€‚</li>
<li>é‡‡ç”¨Rank-N ContrastæŸå¤±å‡½æ•°ï¼Œä»¥å¯¹é½æ¨¡æ‹Ÿå’ŒçœŸå®è¡¨ç¤ºï¼Œå¹¶æ•è·æƒ…æ„Ÿä»·å€¼å›å½’ä»»åŠ¡çš„æ ·æœ¬è¿ç»­æ€§ã€‚</li>
<li>åœ¨CMU-MOSEIæ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨MAEä¸Šè¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bd3961cf43418199dc1232f86aad3d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559e1a3fc48de71a7cda1026ab7634d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4fd60eac2571e19d3beb01367c63109.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4d5791b0ef546ac24a5fa3e2f0772b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac6e7bcde12e542eb70c3630cc1212e1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Speech-Emotion-Recognition-with-ASR-Transcripts-A-Comprehensive-Study-on-Word-Error-Rate-and-Fusion-Techniques"><a href="#Speech-Emotion-Recognition-with-ASR-Transcripts-A-Comprehensive-Study-on-Word-Error-Rate-and-Fusion-Techniques" class="headerlink" title="Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study   on Word Error Rate and Fusion Techniques"></a>Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study   on Word Error Rate and Fusion Techniques</h2><p><strong>Authors:Yuanchao Li, Peter Bell, Catherine Lai</strong></p>
<p>Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications. </p>
<blockquote>
<p>æ–‡æœ¬æ•°æ®é€šå¸¸ä½œä¸ºä¸»è¦çš„è¾“å…¥ï¼Œç”¨æ¥æé«˜è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„æ€§èƒ½å’Œå¯é æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶å¯¹äººç±»è½¬å½•æ–‡æœ¬çš„ä¾èµ–é˜»ç¢äº†å®ç”¨SERç³»ç»Ÿçš„å‘å±•ï¼Œä»è€Œåœ¨å®éªŒå®¤ç ”ç©¶å’Œç°å®ä¸–ç•Œçš„åœºæ™¯ä¹‹é—´äº§ç”Ÿäº†ä¸€ä¸ªå·®è·ï¼Œåœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä½œä¸ºæ–‡æœ¬æ¥æºã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶ä»¥ASRè½¬å½•ç¨¿çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ºåŸºå‡†ï¼Œè¯„ä¼°äº†åä¸€ä¸ªæ¨¡å‹åœ¨ä¸‰ä¸ªçŸ¥åè¯­æ–™åº“ï¼ˆIEMOCAPã€CMU-MOSIå’ŒMSP-Podcastï¼‰ä¸Šçš„SERæ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬ä»…ä½¿ç”¨æ–‡æœ¬å’ŒåŒå‘SERä¸¤ç§æ–¹æ³•ï¼Œä½¿ç”¨å…­ç§èåˆæŠ€æœ¯ï¼Œæ—¨åœ¨è¿›è¡Œå…¨é¢çš„åˆ†æï¼Œæ­ç¤ºå½“å‰SERç ”ç©¶é¢ä¸´çš„æ–°å‘ç°å’ŒæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ASRé”™è¯¯é²æ£’æ¡†æ¶ï¼Œèåˆäº†ASRé”™è¯¯æ ¡æ­£å’Œæ¨¡æ€é—¨èåˆæŠ€æœ¯ï¼Œå®ç°äº†è¾ƒä½çš„WERå’Œè¾ƒé«˜çš„SERç»“æœï¼Œè¶…è¿‡äº†è¡¨ç°æœ€ä½³çš„ASRè½¬å½•ã€‚è¿™äº›å‘ç°å¯¹ASRè¾…åŠ©çš„SERæœ‰æ·±å…¥è§è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08353v3">PDF</a> Accepted to IEEE SLT 2024. Fixed table and figure reference mistakes   in the IEEE Xplore version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­åˆ©ç”¨æ–‡æœ¬æ•°æ®ä½œä¸ºä¸»è¾“å…¥çš„é—®é¢˜ã€‚ç ”ç©¶ä¸­å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å†™çš„æ–‡æœ¬åœ¨å„ç§æƒ…å†µä¸‹çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ASRè¯¯å·®é²æ£’æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜SERåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ•°æ®æ˜¯å¢å¼ºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ€§èƒ½å’Œå¯é æ€§çš„ä¸»è¦è¾“å…¥ã€‚</li>
<li>å¤§å¤šæ•°ç ”ç©¶è¿‡äºä¾èµ–äººå·¥è½¬å†™çš„æ–‡æœ¬ï¼Œé˜»ç¢äº†å®é™…SERç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>ç ”ç©¶è€…å¯¹ASRè½¬å†™çš„æ–‡æœ¬åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œé€‰æ‹©äº†ä¸‰ä¸ªè‘—åè¯­æ–™åº“å’Œåä¸€æ¬¾æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>é‡‡ç”¨å¤šç§èåˆæŠ€æœ¯çš„å…¨é¢è¯„ä¼°åŒ…æ‹¬æ–‡æœ¬æ¨¡å¼å’ŒåŒæ¨¡æ€SERï¼Œæ—¨åœ¨å‘ç°æ–°çš„ç ”ç©¶é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5dcc3e9f12265b36afb1b1b38ad27a48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caeed278d23b1e7baf5f66ff0ce1b305.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-301c7c58e0ff13db64535348c683830b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04ac3c30a0b074e548c54c51cd0fd4a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d05e4124bf888d0ad15a05ef02cd373.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e222266868a426988cf8491e6eafb29.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a15b168ebe31fb6b22bc4004ac396872.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  NullSwap Proactive Identity Cloaking Against Deepfake Face Swapping
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1404d48fffab21cc97835e72a71b4285.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  CAFe Unifying Representation and Generation with   Contrastive-Autoregressive Finetuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
