<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  MultimodalStudio A Heterogeneous Sensor Dataset and Framework for   Neural Rendering across Multiple Imaging Modalities">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2ddd0e44dbb57c1d995323af6c0e8f90.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="MultimodalStudio-A-Heterogeneous-Sensor-Dataset-and-Framework-for-Neural-Rendering-across-Multiple-Imaging-Modalities"><a href="#MultimodalStudio-A-Heterogeneous-Sensor-Dataset-and-Framework-for-Neural-Rendering-across-Multiple-Imaging-Modalities" class="headerlink" title="MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for   Neural Rendering across Multiple Imaging Modalities"></a>MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for   Neural Rendering across Multiple Imaging Modalities</h2><p><strong>Authors:Federico Lincetto, Gianluca Agresti, Mattia Rossi, Pietro Zanuttigh</strong></p>
<p>Neural Radiance Fields (NeRF) have shown impressive performances in the rendering of 3D scenes from arbitrary viewpoints. While RGB images are widely preferred for training volume rendering models, the interest in other radiance modalities is also growing. However, the capability of the underlying implicit neural models to learn and transfer information across heterogeneous imaging modalities has seldom been explored, mostly due to the limited training data availability. For this purpose, we present MultimodalStudio (MMS): it encompasses MMS-DATA and MMS-FW. MMS-DATA is a multimodal multi-view dataset containing 32 scenes acquired with 5 different imaging modalities: RGB, monochrome, near-infrared, polarization and multispectral. MMS-FW is a novel modular multimodal NeRF framework designed to handle multimodal raw data and able to support an arbitrary number of multi-channel devices. Through extensive experiments, we demonstrate that MMS-FW trained on MMS-DATA can transfer information between different imaging modalities and produce higher quality renderings than using single modalities alone. We publicly release the dataset and the framework, to promote the research on multimodal volume rendering and beyond. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ä»ä»»æ„è§†è§’æ¸²æŸ“3Dåœºæ™¯æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚è™½ç„¶RGBå›¾åƒå¹¿æ³›ç”¨äºè®­ç»ƒä½“ç§¯æ¸²æŸ“æ¨¡å‹ï¼Œä½†å¯¹å…¶ä»–è¾å°„æ¨¡æ€çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œåº•å±‚éšå¼ç¥ç»æ¨¡å‹åœ¨è·¨ä¸åŒæˆåƒæ¨¡æ€å­¦ä¹ å’Œè½¬ç§»ä¿¡æ¯çš„èƒ½åŠ›æ–¹é¢å¾ˆå°‘è¢«æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MultimodalStudioï¼ˆMMSï¼‰ï¼šå®ƒæ¶µç›–äº†MMS-DATAå’ŒMMS-FWã€‚MMS-DATAæ˜¯ä¸€ä¸ªåŒ…å«é€šè¿‡äº”ç§ä¸åŒæˆåƒæ¨¡æ€ï¼ˆRGBã€å•è‰²ã€è¿‘çº¢å¤–ã€åæŒ¯å’Œå¤šå…‰è°±ï¼‰è·å–çš„32ä¸ªåœºæ™¯çš„å¤šæ¨¡æ€å¤šè§†è§’æ•°æ®é›†ã€‚MMS-FWæ˜¯ä¸€ä¸ªæ–°å‹æ¨¡å—åŒ–å¤šæ¨¡æ€NeRFæ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†å¤šæ¨¡æ€åŸå§‹æ•°æ®ï¼Œå¹¶æ”¯æŒä»»æ„æ•°é‡çš„å¤šé€šé“è®¾å¤‡ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜åœ¨MMS-DATAä¸Šè®­ç»ƒçš„MMS-FWå¯ä»¥åœ¨ä¸åŒçš„æˆåƒæ¨¡æ€ä¹‹é—´è½¬ç§»ä¿¡æ¯ï¼Œå¹¶ä¸”äº§ç”Ÿçš„æ¸²æŸ“è´¨é‡é«˜äºä»…ä½¿ç”¨å•ä¸€æ¨¡æ€ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œæ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€ä½“ç§¯æ¸²æŸ“åŠç›¸å…³é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19673v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>NeRFæŠ€æœ¯å·²ç»å¹¿æ³›ç”¨äºä»ä»»æ„è§†è§’æ¸²æŸ“ä¸‰ç»´åœºæ™¯ã€‚è™½ç„¶RGBå›¾åƒå¸¸ç”¨äºè®­ç»ƒä½“ç§¯æ¸²æŸ“æ¨¡å‹ï¼Œä½†å…¶ä»–è¾å°„æ¨¡æ€çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®æœ‰é™ï¼ŒåŸºç¡€éšå«ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è·¨ä¸åŒæˆåƒæ¨¡æ€å­¦ä¹ å’Œè½¬ç§»ä¿¡æ¯çš„èƒ½åŠ›å¾ˆå°‘è¢«æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MultimodalStudioï¼ˆMMSï¼‰ï¼Œå®ƒåŒ…æ‹¬MMS-DATAå’ŒMMS-FWã€‚MMS-DATAæ˜¯ä¸€ä¸ªåŒ…å«é€šè¿‡äº”ç§ä¸åŒæˆåƒæ¨¡æ€è·å–çš„32ä¸ªåœºæ™¯çš„å¤šæ¨¡æ€å¤šè§†è§’æ•°æ®é›†ï¼šRGBã€å•è‰²ã€è¿‘çº¢å¤–ã€æåŒ–å’Œå¤šå…‰è°±ã€‚MMS-FWæ˜¯ä¸€ä¸ªæ–°å‹æ¨¡å—åŒ–å¤šæ¨¡æ€NeRFæ¡†æ¶ï¼Œè®¾è®¡ç”¨äºå¤„ç†å¤šæ¨¡æ€åŸå§‹æ•°æ®ï¼Œå¹¶æ”¯æŒä»»æ„æ•°é‡çš„å¤šé€šé“è®¾å¤‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜MMS-FWåœ¨MMS-DATAä¸Šè®­ç»ƒå¯ä»¥åœ¨ä¸åŒæˆåƒæ¨¡æ€ä¹‹é—´è½¬ç§»ä¿¡æ¯ï¼Œä¸”æ¸²æŸ“è´¨é‡é«˜äºä½¿ç”¨å•ä¸€æ¨¡æ€ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œæ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€ä½“ç§¯æ¸²æŸ“åŠç›¸å…³é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯åœ¨ä»»æ„è§†è§’æ¸²æŸ“ä¸‰ç»´åœºæ™¯è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è™½ç„¶RGBå›¾åƒæ˜¯ä¸»è¦çš„è®­ç»ƒä½“ç§¯æ¸²æŸ“æ¨¡å‹çš„å›¾åƒæ¥æºï¼Œä½†å…¶ä»–è¾å°„æ¨¡æ€çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚</li>
<li>éšå«ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è·¨ä¸åŒæˆåƒæ¨¡æ€å­¦ä¹ å’Œè½¬ç§»ä¿¡æ¯çš„èƒ½åŠ›å—é™äºè®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§ã€‚</li>
<li>MultimodalStudioï¼ˆMMSï¼‰åŒ…æ‹¬ä¸€ä¸ªåŒ…å«å¤šç§æˆåƒæ¨¡æ€çš„æ•°æ®é›†å’Œä¸€ä¸ªæ¨¡å—åŒ–å¤šæ¨¡æ€NeRFæ¡†æ¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº”ç§ä¸åŒæˆåƒæ¨¡æ€é‡‡é›†çš„å¤šä¸ªåœºæ™¯ã€‚</li>
<li>NeRFæ¡†æ¶è®¾è®¡ç”¨äºå¤„ç†å¤šæ¨¡æ€åŸå§‹æ•°æ®ï¼Œå¹¶æ”¯æŒä»»æ„æ•°é‡çš„å¤šé€šé“è®¾å¤‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a356b2df94ca244c0acac7dc0faebbb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a574b1a72e26f21c2cdd05cd490cadca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ddd0e44dbb57c1d995323af6c0e8f90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c88f9563b3b0b9d0bdb83f9551b05e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7efdb83f85ad27fcaa46abadf6a92a8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SINR-Sparsity-Driven-Compressed-Implicit-Neural-Representations"><a href="#SINR-Sparsity-Driven-Compressed-Implicit-Neural-Representations" class="headerlink" title="SINR: Sparsity Driven Compressed Implicit Neural Representations"></a>SINR: Sparsity Driven Compressed Implicit Neural Representations</h2><p><strong>Authors:Dhananjaya Jayasundara, Sudarshan Rajagopalan, Yasiru Ranasinghe, Trac D. Tran, Vishal M. Patel</strong></p>
<p>Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on the quantization and entropy coding schemes employed. In this paper, we introduce SINR, an innovative compression algorithm that leverages the patterns in the vector spaces formed by weights of INRs. We compress these vector spaces using a high-dimensional sparse code within a dictionary. Further analysis reveals that the atoms of the dictionary used to generate the sparse code do not need to be learned or transmitted to successfully recover the INR weights. We demonstrate that the proposed approach can be integrated with any existing INR-based signal compression technique. Our results indicate that SINR achieves substantial reductions in storage requirements for INRs across various configurations, outperforming conventional INR-based compression baselines. Furthermore, SINR maintains high-quality decoding across diverse data modalities, including images, occupancy fields, and Neural Radiance Fields. </p>
<blockquote>
<p>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«è®¤å¯ä¸ºä¸€ç§ç”¨äºè¡¨ç¤ºç¦»æ•£ä¿¡å·çš„é€šç”¨æ•°æ®å½¢å¼ï¼Œå®ƒæä¾›äº†æ— é™æŸ¥è¯¢åˆ†è¾¨ç‡å’Œé™ä½å­˜å‚¨éœ€æ±‚ç­‰å¥½å¤„ã€‚ç°æœ‰çš„ç”¨äºINRçš„ä¿¡å·å‹ç¼©æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤ç§ç­–ç•¥ä¹‹ä¸€ï¼š1.ç›´æ¥é‡åŒ–è®­ç»ƒåçš„INRå¹¶è¿›è¡Œç†µç¼–ç ï¼›2.é€šè¿‡åœ¨INRä¹‹ä¸Šè¿›è¡Œå¯å­¦ä¹ è½¬æ¢æ¥æ´¾ç”Ÿå‡ºæ½œåœ¨ä»£ç ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‰€é‡‡ç”¨çš„é‡åŒ–å’Œç†µç¼–ç æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SINRï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„å‹ç¼©ç®—æ³•ï¼Œå®ƒåˆ©ç”¨ç”±INRæƒé‡å½¢æˆçš„å‘é‡ç©ºé—´ä¸­çš„æ¨¡å¼ã€‚æˆ‘ä»¬ä½¿ç”¨å­—å…¸ä¸­çš„é«˜ç»´ç¨€ç–ä»£ç æ¥å‹ç¼©è¿™äº›å‘é‡ç©ºé—´ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œç”¨äºç”Ÿæˆç¨€ç–ä»£ç çš„å­—å…¸çš„åŸå­ä¸éœ€è¦å­¦ä¹ æˆ–ä¼ è¾“å³å¯æˆåŠŸæ¢å¤INRæƒé‡ã€‚æˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥ä¸ä»»ä½•ç°æœ‰çš„åŸºäºINRçš„ä¿¡å·å‹ç¼©æŠ€æœ¯ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§é…ç½®ä¸‹ï¼ŒSINRåœ¨é™ä½INRå­˜å‚¨éœ€æ±‚æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œè¶…è¿‡äº†ä¼ ç»Ÿçš„åŸºäºINRçš„å‹ç¼©åŸºçº¿ã€‚æ­¤å¤–ï¼ŒSINRåœ¨å›¾åƒã€å ç”¨å­—æ®µå’Œç¥ç»è¾å°„åœºç­‰å¤šæ ·åŒ–æ•°æ®æ¨¡æ€ä¸Šéƒ½èƒ½ä¿æŒé«˜è´¨é‡çš„è§£ç æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SINRè¿™ä¸€æ–°å‹çš„INRå‹ç¼©ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨INRæƒé‡å½¢æˆçš„å‘é‡ç©ºé—´ä¸­çš„æ¨¡å¼ï¼Œé‡‡ç”¨é«˜ç»´ç¨€ç–ç è¿›è¡Œå‹ç¼©ï¼Œå¹¶å­˜å‚¨åœ¨å­—å…¸ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œæ— éœ€å­¦ä¹ æˆ–ä¼ è¾“å­—å…¸ä¸­çš„åŸå­å³å¯æˆåŠŸæ¢å¤INRæƒé‡ã€‚SINRå¯ä¸å…¶ä»–ç°æœ‰çš„INRä¿¡å·å‹ç¼©æŠ€æœ¯ç›¸ç»“åˆï¼Œåœ¨å¤šç§é…ç½®ä¸‹å¤§å¹…å‡å°‘INRçš„å­˜å‚¨éœ€æ±‚ï¼Œä¸”åœ¨ä¸åŒæ•°æ®æ¨¡æ€çš„è§£ç ä¸­éƒ½èƒ½ä¿æŒé«˜è´¨é‡ï¼ŒåŒ…æ‹¬å›¾åƒã€å ç”¨åœºå’Œç¥ç»è¾å°„åœºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SINRæ˜¯ä¸€ç§é’ˆå¯¹éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„å‹ç¼©ç®—æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨INRæƒé‡çš„å‘é‡ç©ºé—´æ¨¡å¼è¿›è¡Œé«˜æ•ˆå‹ç¼©ã€‚</li>
<li>è¯¥ç®—æ³•é‡‡ç”¨é«˜ç»´ç¨€ç–ç å’Œå­—å…¸ç»“åˆçš„æ–¹å¼ï¼Œæœ‰æ•ˆé™ä½äº†å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>SINRç®—æ³•æ— éœ€å­¦ä¹ å’Œä¼ è¾“å­—å…¸ä¸­çš„åŸå­ï¼Œå³å¯æˆåŠŸæ¢å¤INRæƒé‡ã€‚</li>
<li>SINRå¯ä¸å…¶ä»–ç°æœ‰çš„INRä¿¡å·å‹ç¼©æŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSINRåœ¨å¤šç§é…ç½®ä¸‹æ˜¾è‘—å‡å°‘äº†INRçš„å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>SINRåœ¨è§£ç è¿‡ç¨‹ä¸­èƒ½ä¿æŒé«˜è´¨é‡ï¼Œé€‚ç”¨äºå¤šç§æ•°æ®æ¨¡æ€ï¼ŒåŒ…æ‹¬å›¾åƒã€å ç”¨åœºå’Œç¥ç»è¾å°„åœºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-316622003b537dd45e86b521a2763416.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfbccc976593968f5048a55e03903111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2b5586ac54a9d8ca85328edbfc35a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-561e6de1c139e90552f6ca6eb882b30e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting"><a href="#NexusGS-Sparse-View-Synthesis-with-Epipolar-Depth-Priors-in-3D-Gaussian-Splatting" class="headerlink" title="NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting"></a>NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian   Splatting</h2><p><strong>Authors:Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du</strong></p>
<p>Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably advanced photo-realistic novel view synthesis using images from densely spaced camera viewpoints. However, these methods struggle in few-shot scenarios due to limited supervision. In this paper, we present NexusGS, a 3DGS-based approach that enhances novel view synthesis from sparse-view images by directly embedding depth information into point clouds, without relying on complex manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our method introduces a novel point cloud densification strategy that initializes 3DGS with a dense point cloud, reducing randomness in point placement while preventing over-smoothing and overfitting. Specifically, NexusGS comprises three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses to compute accurate depth maps, while mitigating the inaccuracies often associated with optical flow. By incorporating epipolar depth priors, NexusGS ensures reliable dense point cloud coverage and supports stable 3DGS training under sparse-view conditions. Experiments demonstrate that NexusGS significantly enhances depth accuracy and rendering quality, surpassing state-of-the-art methods by a considerable margin. Furthermore, we validate the superiority of our generated point clouds by substantially boosting the performance of competing methods. Project page: <a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/">https://usmizuki.github.io/NexusGS/</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æç”»ï¼ˆ3DGSï¼‰åˆ©ç”¨å¯†é›†é—´éš”ç›¸æœºè§†è§’çš„å›¾åƒæ˜¾è‘—æ¨è¿›äº†é€¼çœŸçš„æ–°å‹è§†å›¾åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­ç”±äºç›‘ç£æœ‰é™è€Œè¡¨ç°æŒ£æ‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NexusGSï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å°†æ·±åº¦ä¿¡æ¯åµŒå…¥ç‚¹äº‘ä¸­ï¼Œæ”¹è¿›äº†ä»ç¨€ç–è§†å›¾å›¾åƒè¿›è¡Œçš„æ–°å‹è§†å›¾åˆæˆï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„æ‰‹åŠ¨æ­£åˆ™åŒ–ã€‚åˆ©ç”¨3DGSçš„å›ºæœ‰æçº¿å‡ ä½•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„ç‚¹äº‘å¢å¯†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨å¯†é›†ç‚¹äº‘åˆå§‹åŒ–3DGSï¼Œå‡å°‘ç‚¹æ”¾ç½®çš„éšæœºæ€§ï¼ŒåŒæ—¶é˜²æ­¢è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚å…·ä½“æ¥è¯´ï¼ŒNexusGSåŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šæçº¿æ·±åº¦Nexusã€æµé‡å¼¹æ€§æ·±åº¦æ··åˆå’Œæµé‡è¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚è¿™äº›æ­¥éª¤åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€è®¡ç®—å‡†ç¡®æ·±åº¦å›¾ï¼ŒåŒæ—¶å‡è½»ä¸å…‰å­¦æµå¸¸ä¼´éšçš„ä¸å‡†ç¡®æ€§ã€‚é€šè¿‡èå…¥æçº¿æ·±åº¦å…ˆéªŒçŸ¥è¯†ï¼ŒNexusGSç¡®ä¿å¯é çš„å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹æ”¯æŒç¨³å®šçš„3DGSè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒNexusGSæ˜¾è‘—æé«˜äº†æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œå¤§å¤§è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤§å¹…æå‡ç«äº‰æ–¹æ³•çš„æ€§èƒ½æ¥éªŒè¯äº†æ‰€ç”Ÿæˆç‚¹äº‘çš„ä¼˜è¶Šæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://usmizuki.github.io/NexusGS/%E3%80%82">https://usmizuki.github.io/NexusGS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18794v1">PDF</a> This paper is accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>NeRFå’Œ3DGSåœ¨å¯†é›†ç›¸æœºè§†è§’çš„å›¾åƒä¸Šå®ç°äº†é€¼çœŸçš„æ–°è§†è§’åˆæˆã€‚ä½†åœ¨å°‘é‡å›¾åƒçš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºNexusGSï¼Œä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å°†æ·±åº¦ä¿¡æ¯åµŒå…¥ç‚¹äº‘ä¸­ï¼Œå¢å¼ºç¨€ç–è§†è§’å›¾åƒçš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨3DGSçš„å›ºæœ‰æå‡ ä½•ç»“æ„ï¼Œå¼•å…¥æ–°å‹ç‚¹äº‘å¯†é›†åŒ–ç­–ç•¥ï¼Œä»¥å¯†é›†ç‚¹äº‘åˆå§‹åŒ–3DGSï¼Œå‡å°‘ç‚¹æ”¾ç½®çš„éšæœºæ€§ï¼Œé˜²æ­¢è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚NexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æµé‡æ¢å¤æ·±åº¦æ··åˆå’Œæµé‡è¿‡æ»¤æ·±åº¦ä¿®å‰ªã€‚è¿™äº›æ­¥éª¤åˆ©ç”¨å…‰å­¦æµå’Œç›¸æœºå§¿æ€è®¡ç®—å‡†ç¡®æ·±åº¦å›¾ï¼Œå‡è½»å…‰å­¦æµçš„ä¸å‡†ç¡®æ€§ã€‚é€šè¿‡èå…¥ææ·±åº¦å…ˆéªŒï¼ŒNexusGSç¡®ä¿å¯é å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–æ¡ä»¶ä¸‹æ”¯æŒç¨³å®šçš„3DGSè®­ç»ƒã€‚å®éªŒè¯æ˜NexusGSæ˜¾è‘—æé«˜æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ï¼Œå¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFå’Œ3DGSåœ¨å¯†é›†ç›¸æœºè§†è§’çš„å›¾åƒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å°‘é‡å›¾åƒçš„æƒ…å†µä¸‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>NexusGSæ˜¯ä¸€ç§åŸºäº3DGSçš„æ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥æ·±åº¦ä¿¡æ¯åˆ°ç‚¹äº‘ä¸­å¢å¼ºæ–°è§†è§’åˆæˆæ•ˆæœã€‚</li>
<li>NexusGSåˆ©ç”¨3DGSçš„å›ºæœ‰æå‡ ä½•ç»“æ„ï¼Œå‡å°‘ç‚¹æ”¾ç½®çš„éšæœºæ€§å¹¶é˜²æ­¢è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>NexusGSåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šææ·±åº¦Nexusã€æµé‡æ¢å¤æ·±åº¦æ··åˆå’Œæµé‡è¿‡æ»¤æ·±åº¦ä¿®å‰ªï¼Œè¿™äº›æ­¥éª¤æœ‰åŠ©äºæé«˜æ·±åº¦å›¾è®¡ç®—çš„å‡†ç¡®æ€§ã€‚</li>
<li>NexusGSèå…¥ææ·±åº¦å…ˆéªŒï¼Œç¡®ä¿å¯é å¯†é›†ç‚¹äº‘è¦†ç›–ï¼Œå¹¶åœ¨ç¨€ç–æ¡ä»¶ä¸‹æ”¯æŒç¨³å®šçš„3DGSè®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜NexusGSåœ¨æ·±åº¦å‡†ç¡®æ€§å’Œæ¸²æŸ“è´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d91262626222a26061a4d223c84fbca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93ed83d2b2f76ab2cb97bd4a5f9f65b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a47a15036cbec97db110b1320e4f846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf847bb4e3c6ffe7de25a2effa479071.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LookCloser-Frequency-aware-Radiance-Field-for-Tiny-Detail-Scene"><a href="#LookCloser-Frequency-aware-Radiance-Field-for-Tiny-Detail-Scene" class="headerlink" title="LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene"></a>LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene</h2><p><strong>Authors:Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao</strong></p>
<p>Humans perceive and comprehend their surroundings through information spanning multiple frequencies. In immersive scenes, people naturally scan their environment to grasp its overall structure while examining fine details of objects that capture their attention. However, current NeRF frameworks primarily focus on modeling either high-frequency local views or the broad structure of scenes with low-frequency information, which is limited to balancing both. We introduce FA-NeRF, a novel frequency-aware framework for view synthesis that simultaneously captures the overall scene structure and high-definition details within a single NeRF model. To achieve this, we propose a 3D frequency quantification method that analyzes the sceneâ€™s frequency distribution, enabling frequency-aware rendering. Our framework incorporates a frequency grid for fast convergence and querying, a frequency-aware feature re-weighting strategy to balance features across different frequency contents. Extensive experiments show that our method significantly outperforms existing approaches in modeling entire scenes while preserving fine details. Project page: <a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser/">https://coscatter.github.io/LookCloser/</a> </p>
<blockquote>
<p>äººç±»é€šè¿‡è·¨è¶Šå¤šä¸ªé¢‘ç‡çš„ä¿¡æ¯æ¥æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒã€‚åœ¨æ²‰æµ¸å¼åœºæ™¯ä¸­ï¼Œäººä»¬è‡ªç„¶åœ°ä¼šæ‰«æç¯å¢ƒï¼Œä»¥æŠŠæ¡å…¶æ•´ä½“ç»“æ„ï¼ŒåŒæ—¶å…³æ³¨å¸å¼•ä»–ä»¬çš„ç‰©ä½“çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œå½“å‰çš„NeRFæ¡†æ¶ä¸»è¦ä¾§é‡äºå¯¹é«˜é¢‘å±€éƒ¨è§†å›¾æˆ–åœºæ™¯å®è§‚ç»“æ„çš„å»ºæ¨¡ï¼Œæ¶‰åŠä½é¢‘ä¿¡æ¯ï¼Œè¿™åœ¨å¹³è¡¡ä¸¤è€…æ—¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†FA-NeRFï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†å›¾åˆæˆçš„æ–°å‹é¢‘ç‡æ„ŸçŸ¥æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªNeRFæ¨¡å‹ä¸­åŒæ—¶æ•æ‰åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰ç»´é¢‘ç‡é‡åŒ–æ–¹æ³•ï¼Œåˆ†æåœºæ™¯çš„é¢‘ç‡åˆ†å¸ƒï¼Œä»è€Œå®ç°é¢‘ç‡æ„ŸçŸ¥æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†é¢‘ç‡ç½‘æ ¼ä»¥å®ç°å¿«é€Ÿæ”¶æ•›å’ŒæŸ¥è¯¢ï¼Œä»¥åŠé¢‘ç‡æ„ŸçŸ¥ç‰¹å¾é‡åŠ æƒç­–ç•¥ï¼Œä»¥å¹³è¡¡ä¸åŒé¢‘ç‡å†…å®¹ä¸‹çš„ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºæ¨¡æ•´ä¸ªåœºæ™¯æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†ç»†èŠ‚ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser/">https://coscatter.github.io/LookCloser/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18513v2">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser">https://coscatter.github.io/LookCloser</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FA-NeRFï¼Œä¸€ç§æ–°å‹é¢‘ç‡æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºåœºæ™¯è§†å›¾åˆæˆã€‚è¯¥æ¡†æ¶èƒ½åŒæ—¶æ•æ‰åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ï¼Œé€šè¿‡3Dé¢‘ç‡é‡åŒ–æ–¹æ³•åˆ†æåœºæ™¯é¢‘ç‡åˆ†å¸ƒï¼Œå®ç°é¢‘ç‡æ„ŸçŸ¥æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é‡‡ç”¨é¢‘ç‡ç½‘æ ¼å’Œç‰¹å¾é‡åŠ æƒç­–ç•¥æ¥å¹³è¡¡ä¸åŒé¢‘ç‡å†…å®¹çš„ç‰¹ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»ºæ¨¡æ•´ä¸ªåœºæ™¯æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FA-NeRFæ˜¯ä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºè§†å›¾åˆæˆã€‚</li>
<li>å®ƒç»“åˆäº†åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ã€‚</li>
<li>ä½¿ç”¨3Dé¢‘ç‡é‡åŒ–æ–¹æ³•æ¥åˆ†æåœºæ™¯çš„é¢‘ç‡åˆ†å¸ƒã€‚</li>
<li>å®ç°é¢‘ç‡æ„ŸçŸ¥æ¸²æŸ“ï¼Œé‡‡ç”¨é¢‘ç‡ç½‘æ ¼å’Œç‰¹å¾é‡åŠ æƒç­–ç•¥ã€‚</li>
<li>æ¡†æ¶å…·æœ‰å¿«é€Ÿæ”¶æ•›å’ŒæŸ¥è¯¢çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»ºæ¨¡æ•´ä¸ªåœºæ™¯æ—¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e97db8155e44991a4530cdf5512c75cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d6af326c03133e7876a4526aa26da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92560f81720ceacb5474f277e835d72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de36ecc4767a264f0af183c8b34f5f4a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction"><a href="#NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction" class="headerlink" title="NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction"></a>NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction</h2><p><strong>Authors:Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu</strong></p>
<p>Recently, it has shown that priors are vital for neural implicit functions to reconstruct high-quality surfaces from multi-view RGB images. However, current priors require large-scale pre-training, and merely provide geometric clues without considering the importance of color. In this paper, we present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for surface reconstruction. Our NeRF prior can provide both geometric and color clues, and also get trained fast under the same scene without additional data. Based on the NeRF prior, we are enabled to learn a signed distance function (SDF) by explicitly imposing a multi-view consistency constraint on each ray intersection for surface inference. Specifically, at each ray intersection, we use the density in the prior as a coarse geometry estimation, while using the color near the surface as a clue to check its visibility from another view angle. For the textureless areas where the multi-view consistency constraint does not work well, we further introduce a depth consistency loss with confidence weights to infer the SDF. Our experimental results outperform the state-of-the-art methods under the widely used benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç ”ç©¶è¡¨æ˜å…ˆéªŒçŸ¥è¯†å¯¹äºç¥ç»éšå¼å‡½æ•°ä»å¤šè§†è§’RGBå›¾åƒé‡å»ºé«˜è´¨é‡è¡¨é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆéªŒçŸ¥è¯†éœ€è¦å¤§é‡é¢„è®­ç»ƒï¼Œå¹¶ä¸”ä»…æä¾›å‡ ä½•çº¿ç´¢ï¼Œè€Œæ²¡æœ‰è€ƒè™‘é¢œè‰²çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NeRFPriorï¼Œå®ƒé‡‡ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œä½¿ç”¨ä½“ç§¯æ¸²æŸ“æ¥å­¦ä¹ æœ‰å‘è·ç¦»åœºè¿›è¡Œè¡¨é¢é‡å»ºã€‚æˆ‘ä»¬çš„NeRFå…ˆéªŒå¯ä»¥æä¾›å‡ ä½•å’Œé¢œè‰²çº¿ç´¢ï¼Œå¹¶ä¸”åœ¨åŒä¸€åœºæ™¯ä¸‹æ— éœ€é¢å¤–æ•°æ®å³å¯å¿«é€Ÿè¿›è¡Œè®­ç»ƒã€‚åŸºäºNeRFå…ˆéªŒï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åœ¨æ¯æ¡å°„çº¿äº¤ç‚¹ä¸Šæ˜ç¡®æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ¥å­¦ä¹ æœ‰å‘è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰ï¼Œä»¥å®ç°è¡¨é¢æ¨æ–­ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯æ¡å°„çº¿äº¤ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆéªŒä¸­çš„å¯†åº¦ä½œä¸ºç²—ç•¥çš„å‡ ä½•ä¼°è®¡ï¼ŒåŒæ—¶ä½¿ç”¨æ¥è¿‘è¡¨é¢çš„é¢œè‰²ä½œä¸ºä»å¦ä¸€ä¸ªè§†è§’æ£€æŸ¥å…¶å¯è§æ€§çš„çº¿ç´¢ã€‚å¯¹äºçº¹ç†ç¼ºå¤±çš„åŒºåŸŸï¼Œå¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ— æ³•å¾ˆå¥½åœ°å·¥ä½œï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFã€‚æˆ‘ä»¬çš„å®éªŒç»“æœåœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18361v1">PDF</a> Accepted by CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://wen-yuan-zhang.github.io/NeRFPrior/">https://wen-yuan-zhang.github.io/NeRFPrior/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†NeRFPrioræ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºä½œä¸ºå…ˆéªŒæ¥å­¦ä¹ ä½¿ç”¨ä½“ç§¯æ¸²æŸ“çš„è¡¨é¢é‡å»ºç¬¦å·è·ç¦»åœºã€‚NeRFPriorä¸ä»…å¯ä»¥æä¾›å‡ ä½•çº¿ç´¢ï¼Œè¿˜å¯ä»¥æä¾›é¢œè‰²çº¿ç´¢ï¼Œå¹¶ä¸”åœ¨åŒä¸€åœºæ™¯ä¸‹æ— éœ€é¢å¤–æ•°æ®å³å¯å¿«é€Ÿè®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®åœ°å¯¹æ¯æ¡å°„çº¿äº¤ç‚¹æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°äº†å¯¹è¡¨é¢æ¨æ–­çš„ç­¾è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFPrioråˆ©ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºä½œä¸ºå…ˆéªŒï¼Œç”¨äºä»å¤šè§†è§’RGBå›¾åƒé‡å»ºé«˜è´¨é‡è¡¨é¢ã€‚</li>
<li>å½“å‰å…ˆéªŒæ–¹æ³•éœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè€ŒNeRFPrioråˆ™èƒ½å¤Ÿåœ¨åŒä¸€åœºæ™¯ä¸‹è¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼Œæ— éœ€é¢å¤–æ•°æ®ã€‚</li>
<li>NeRFPriorç»“åˆå‡ ä½•å’Œé¢œè‰²çº¿ç´¢ï¼Œæé«˜äº†è¡¨é¢é‡å»ºçš„ç²¾åº¦ã€‚</li>
<li>é€šè¿‡æ˜ç¡®åœ°å¯¹æ¯æ¡å°„çº¿äº¤ç‚¹æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼ŒNeRFPriorå®ç°äº†ç­¾è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰çš„å­¦ä¹ ã€‚</li>
<li>å¯¹äºçº¹ç†ç¼ºå¤±çš„åŒºåŸŸï¼ŒNeRFPriorå¼•å…¥äº†å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFã€‚</li>
<li>NeRFPriorçš„å®éªŒç»“æœåœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>NeRFPrioræ–¹æ³•ç»“åˆä½“ç§¯æ¸²æŸ“å’Œç¥ç»ç½‘ç»œè¾å°„åœºï¼Œå®ç°äº†æ›´ä¸ºç²¾å‡†å’Œé«˜æ•ˆçš„è¡¨é¢é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1eb76b3427a4c2317604f767beabc421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-329c32e673c33ffb395103bf6fad45f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529d9fe25e668a950d63460ca52e07fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a736b9c84b1f2095dd212bde8fde2f75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2894203a097369e7df754db7fa9dca19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22dcbf7ed51c78afe9d29df43db16aba.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LGPS-A-Lightweight-GAN-Based-Approach-for-Polyp-Segmentation-in-Colonoscopy-Images"><a href="#LGPS-A-Lightweight-GAN-Based-Approach-for-Polyp-Segmentation-in-Colonoscopy-Images" class="headerlink" title="LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in   Colonoscopy Images"></a>LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in   Colonoscopy Images</h2><p><strong>Authors:Fiseha B. Tesema, Alejandro Guerra Manzanares, Tianxiang Cui, Qian Zhang, Moses Solomon, Sean He</strong></p>
<p>Colorectal cancer (CRC) is a major global cause of cancer-related deaths, with early polyp detection and removal during colonoscopy being crucial for prevention. While deep learning methods have shown promise in polyp segmentation, challenges such as high computational costs, difficulty in segmenting small or low-contrast polyps, and limited generalizability across datasets persist. To address these issues, we propose LGPS, a lightweight GAN-based framework for polyp segmentation. LGPS incorporates three key innovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks and Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2) Convolutional Conditional Random Fields (ConvCRF) for precise boundary refinement; and (3) a hybrid loss function combining Binary Cross-Entropy, Weighted IoU Loss, and Dice Loss to address class imbalance and enhance segmentation accuracy. LGPS is validated on five benchmark datasets and compared with state-of-the-art(SOTA) methods. On the largest and challenging PolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867, outperformed all SOTA works and demonstrating robust generalization. With only 1.07 million parameters, LGPS is 17 times smaller than the smallest existing model, making it highly suitable for real-time clinical applications. Its lightweight design and strong performance underscore its potential for improving early CRC diagnosis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Falmi/LGPS/">https://github.com/Falmi/LGPS/</a>. </p>
<blockquote>
<p>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒå¯¼è‡´ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œç»“è‚ é•œæ£€æŸ¥ä¸­æ—©æœŸæ¯è‚‰çš„æ£€æµ‹å’Œæ‘˜é™¤å¯¹äºé¢„é˜²è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ¯è‚‰åˆ†å‰²æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€åˆ†å‰²å°æ¯è‚‰æˆ–ä½å¯¹æ¯”åº¦æ¯è‚‰å›°éš¾ä»¥åŠæ•°æ®é›†ä¹‹é—´æ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGPSï¼Œä¸€ç§åŸºäºè½»é‡çº§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¯è‚‰åˆ†å‰²æ¡†æ¶ã€‚LGPSç»“åˆäº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ä¿®æ”¹åçš„æ®‹å·®å—å’ŒæŒ¤å‹æ¿€å‘ï¼ˆResEï¼‰æ¨¡å—çš„MobileNetV2ä¸»å¹²ï¼Œè¿›è¡Œé«˜æ•ˆçš„ç‰¹å¾æå–ï¼›ï¼ˆ2ï¼‰å·ç§¯æ¡ä»¶éšæœºåœºï¼ˆConvCRFï¼‰ç”¨äºç²¾ç¡®çš„è¾¹ç•Œç»†åŒ–ï¼›ï¼ˆ3ï¼‰æ··åˆæŸå¤±å‡½æ•°ç»“åˆäº†äºŒå…ƒäº¤å‰ç†µã€åŠ æƒIoUæŸå¤±å’ŒDiceæŸå¤±ï¼Œä»¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶æé«˜åˆ†å‰²ç²¾åº¦ã€‚LGPSåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨æœ€å¤§ä¸”æœ€å…·æŒ‘æˆ˜æ€§çš„PolypGenæµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒLGPSçš„Diceç³»æ•°ä¸º0.7299ï¼ŒIoUä¸º0.7867ï¼Œè¶…è¶Šäº†æ‰€æœ‰æœ€æ–°ä½œå“ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚LGPSä»…æœ‰1070ä¸‡å‚æ•°ï¼Œæ¯”ç°æœ‰æœ€å°æ¨¡å‹å°17å€ï¼Œéå¸¸é€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚å…¶è½»å·§çš„è®¾è®¡å’Œå“è¶Šæ€§èƒ½çªæ˜¾äº†å…¶åœ¨æ”¹è¿›æ—©æœŸCRCè¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Falmi/LGPS/%E3%80%82">https://github.com/Falmi/LGPS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18294v1">PDF</a> 10 pages, 6 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºç»“è‚ æ¯è‚‰åˆ†å‰²çš„è½»é‡çº§GANæ¡†æ¶LGPSï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°è§£å†³äº†æ·±åº¦å­¦ä¹ åœ¨æ¯è‚‰åˆ†å‰²æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚LGPSåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„PolypGenæµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ï¼ŒåŒæ—¶æ¨¡å‹ä½“ç§¯å°å·§ï¼Œé€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ï¼Œæœ‰æœ›æé«˜æ—©æœŸç»“è‚ ç™Œè¯Šæ–­æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LGPSæ˜¯ä¸€ä¸ªåŸºäºGANçš„è½»é‡çº§æ¡†æ¶ï¼Œç”¨äºç»“è‚ æ¯è‚‰åˆ†å‰²ã€‚</li>
<li>LGPSç»“åˆäº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä½¿ç”¨ä¿®æ”¹åçš„MobileNetV2éª¨æ¶å’ŒSqueeze-and-Excitationï¼ˆResEï¼‰æ¨¡å—è¿›è¡Œç‰¹å¾æå–ï¼Œä½¿ç”¨å·ç§¯æ¡ä»¶éšæœºåœºï¼ˆConvCRFï¼‰è¿›è¡Œç²¾ç¡®è¾¹ç•Œç»†åŒ–ï¼Œä»¥åŠæ··åˆæŸå¤±å‡½æ•°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å¹¶å¢å¼ºåˆ†å‰²ç²¾åº¦ã€‚</li>
<li>LGPSåœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶åœ¨PolypGenæµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LGPSæ¨¡å‹ä½“ç§¯å°å·§ï¼Œä»…1.07ç™¾ä¸‡å‚æ•°ï¼Œé€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</li>
<li>LGPSæœ‰åŠ©äºæé«˜æ—©æœŸç»“è‚ ç™Œè¯Šæ–­æ°´å¹³ã€‚</li>
<li>LGPSä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb9ad45475f1cbe14a9d43f187ee2a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a3344e45a5c77690a18bd7d42e7f605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b39ab3e8ff08dad20cb84a1b0ce41f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1b75140fde32c25e0b31e1d6f4be810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ee9fe8e83b467b6466bed6e0a9865db.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="End-to-End-Implicit-Neural-Representations-for-Classification"><a href="#End-to-End-Implicit-Neural-Representations-for-Classification" class="headerlink" title="End-to-End Implicit Neural Representations for Classification"></a>End-to-End Implicit Neural Representations for Classification</h2><p><strong>Authors:Alexander Gielisse, Jan van Gemert</strong></p>
<p>Implicit neural representations (INRs) such as NeRF and SIREN encode a signal in neural network parameters and show excellent results for signal reconstruction. Using INRs for downstream tasks, such as classification, is however not straightforward. Inherent symmetries in the parameters pose challenges and current works primarily focus on designing architectures that are equivariant to these symmetries. However, INR-based classification still significantly under-performs compared to pixel-based methods like CNNs. This work presents an end-to-end strategy for initializing SIRENs together with a learned learning-rate scheme, to yield representations that improve classification accuracy. We show that a simple, straightforward, Transformer model applied to a meta-learned SIREN, without incorporating explicit symmetry equivariances, outperforms the current state-of-the-art. On the CIFAR-10 SIREN classification task, we improve the state-of-the-art without augmentations from 38.8% to 59.6%, and from 63.4% to 64.7% with augmentations. We demonstrate scalability on the high-resolution Imagenette dataset achieving reasonable reconstruction quality with a classification accuracy of 60.8% and are the first to do INR classification on the full ImageNet-1K dataset where we achieve a SIREN classification performance of 23.6%. To the best of our knowledge, no other SIREN classification approach has managed to set a classification baseline for any high-resolution image dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SanderGielisse/MWT">https://github.com/SanderGielisse/MWT</a> </p>
<blockquote>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆå¦‚NeRFå’ŒSIRENï¼‰é€šè¿‡å°†ä¿¡å·ç¼–ç åˆ°ç¥ç»ç½‘ç»œå‚æ•°ä¸­ï¼Œå¹¶åœ¨ä¿¡å·é‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ç»“æœã€‚ç„¶è€Œï¼Œå°†INRç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰å¹¶ä¸ç›´æ¥ã€‚å‚æ•°ä¸­çš„å›ºæœ‰å¯¹ç§°æ€§å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå½“å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è®¾è®¡å¯¹è¿™äº›å¯¹ç§°æ€§å…·æœ‰ç­‰ä»·æ€§çš„æ¶æ„ä¸Šã€‚ç„¶è€Œï¼ŒåŸºäºINRçš„åˆ†ç±»ä¸åŸºäºåƒç´ çš„æ–¹æ³•ï¼ˆå¦‚CNNï¼‰ç›¸æ¯”ï¼Œæ€§èƒ½ä»ç„¶å¤§å¤§ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18123v1">PDF</a> Accepted to CVPR 2025. 8 pages, supplementary material included</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰å¦‚NeRFå’ŒSIRENè¿›è¡Œä¿¡å·é‡å»ºçš„ä¼˜ç§€è¡¨ç°ï¼Œä½†å°†å…¶åº”ç”¨äºåˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºSIRENçš„ç«¯åˆ°ç«¯åˆå§‹åŒ–ç­–ç•¥ï¼Œç»“åˆå­¦ä¹ é€Ÿç‡æ–¹æ¡ˆï¼Œä»¥ç”Ÿæˆæé«˜åˆ†ç±»å‡†ç¡®ç‡çš„è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨CIFAR-10å’ŒImagenetteæ•°æ®é›†ä¸Šï¼Œç®€å•ç›´æ¥çš„Transformeræ¨¡å‹åº”ç”¨äºå…ƒå­¦ä¹ çš„SIRENï¼Œåœ¨ä¸ä½¿ç”¨å¢å¼ºæŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„åˆ†ç±»æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¯¥ç­–ç•¥åœ¨Imagenet-1Kæ•°æ®é›†ä¸Šä¹Ÿå±•ç°äº†åˆæ­¥çš„åˆ†ç±»æ€§èƒ½ã€‚ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>INRå¦‚NeRFå’ŒSIRENåœ¨ä¿¡å·é‡å»ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ä¸­çš„åº”ç”¨å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„æŒ‘æˆ˜åœ¨äºè§£å†³å‚æ•°ä¸­çš„å›ºæœ‰å¯¹ç§°æ€§ï¼Œè€Œç›¸å…³å·¥ä½œä¸»è¦é›†ä¸­åœ¨è®¾è®¡å¯¹æ­¤ç±»å¯¹ç§°æ€§ç­‰ä»·çš„ç»“æ„ä¸Šã€‚</li>
<li>é‡‡ç”¨åŸºäºSIRENçš„ç«¯åˆ°ç«¯åˆå§‹åŒ–ç­–ç•¥å¹¶ç»“åˆå­¦ä¹ é€Ÿç‡æ–¹æ¡ˆèƒ½å¤Ÿæé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>ç®€å•ç›´æ¥çš„Transformeræ¨¡å‹åº”ç”¨äºå…ƒå­¦ä¹ çš„SIRENï¼Œåœ¨ä¸ä½¿ç”¨å¢å¼ºæŠ€æœ¯çš„æƒ…å†µä¸‹è¶…è¶Šå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-559e2a4775cedd9608686a459f62dc2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cc4643f366510c310491429bc18a16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c353dcc5adcc6f65e5b0a202bd999b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93128348d909aac9d44a76019f438d84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40a7dfd2c8005c878c2d0baec2b92867.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Splat-LOAM-Gaussian-Splatting-LiDAR-Odometry-and-Mapping"><a href="#Splat-LOAM-Gaussian-Splatting-LiDAR-Odometry-and-Mapping" class="headerlink" title="Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping"></a>Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping</h2><p><strong>Authors:Emanuele Giacomini, Luca Di Giammarino, Lorenzo De Rebotti, Giorgio Grisetti, Martin R. Oswald</strong></p>
<p>LiDARs provide accurate geometric measurements, making them valuable for ego-motion estimation and reconstruction tasks. Although its success, managing an accurate and lightweight representation of the environment still poses challenges. Both classic and NeRF-based solutions have to trade off accuracy over memory and processing times. In this work, we build on recent advancements in Gaussian Splatting methods to develop a novel LiDAR odometry and mapping pipeline that exclusively relies on Gaussian primitives for its scene representation. Leveraging spherical projection, we drive the refinement of the primitives uniquely from LiDAR measurements. Experiments show that our approach matches the current registration performance, while achieving SOTA results for mapping tasks with minimal GPU requirements. This efficiency makes it a strong candidate for further exploration and potential adoption in real-time robotics estimation tasks. </p>
<blockquote>
<p>æ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰æä¾›å‡†ç¡®çš„å‡ ä½•æµ‹é‡ï¼Œä½¿å…¶æˆä¸ºè‡ªæˆ‘è¿åŠ¨ä¼°è®¡å’Œé‡å»ºä»»åŠ¡ä¸­çš„å®è´µå·¥å…·ã€‚å°½ç®¡å…¶å–å¾—äº†æˆåŠŸï¼Œä½†ç®¡ç†å’Œè¡¨è¾¾ä¸€ä¸ªå‡†ç¡®ä¸”è½»é‡çº§çš„ç¯å¢ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ— è®ºæ˜¯ä¼ ç»Ÿçš„è§£å†³æ–¹æ¡ˆè¿˜æ˜¯åŸºäºNeRFçš„è§£å†³æ–¹æ¡ˆï¼Œéƒ½å¿…é¡»åœ¨å†…å­˜å’Œå¤„ç†æ—¶é—´ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºé«˜æ–¯ç»˜ç”»æ–¹æ³•ï¼ˆGaussian Splatting methodsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹çš„æ¿€å…‰é›·è¾¾æµ‹è·å’Œæ˜ å°„ç®¡é“ï¼Œè¯¥ç®¡é“ä»…ä¾èµ–äºé«˜æ–¯åŸå§‹æ•°æ®æ¥è¿›è¡Œåœºæ™¯è¡¨ç¤ºã€‚å€ŸåŠ©çƒé¢æŠ•å½±æŠ€æœ¯ï¼Œæˆ‘ä»¬ä»æ¿€å…‰é›·è¾¾æµ‹é‡ä¸­å”¯ä¸€åœ°æ¨åŠ¨åŸå§‹æ•°æ®çš„ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å½“å‰çš„æ³¨å†Œæ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†å…·æœ‰æœ€å°GPUè¦æ±‚çš„æ˜ å°„ä»»åŠ¡çš„æœ€æ–°ç»“æœã€‚è¿™ç§æ•ˆç‡ä½¿å…¶æˆä¸ºè¿›ä¸€æ­¥æ¢ç´¢å’Œæ½œåœ¨åº”ç”¨äºå®æ—¶æœºå™¨äººä¼°è®¡ä»»åŠ¡çš„å¼ºå¤§å€™é€‰è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17491v1">PDF</a> submitted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>LiDARæŠ€æœ¯æä¾›å‡†ç¡®çš„å‡ ä½•æµ‹é‡ï¼Œå¯¹äºè‡ªæˆ‘è¿åŠ¨ä¼°è®¡å’Œé‡å»ºä»»åŠ¡å…·æœ‰é‡è¦ä»·å€¼ã€‚ç®¡ç†ç¯å¢ƒçš„å‡†ç¡®ä¸”è½»é‡åŒ–è¡¨ç¤ºä»ç„¶å…·æœ‰æŒ‘æˆ˜ã€‚æœ¬æ–‡å€ŸåŠ©é«˜æ–¯æ¶‚æŠ¹æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹LiDARé‡Œç¨‹è®¡å’Œæ˜ å°„æµç¨‹ï¼Œè¯¥æµç¨‹ä»…ä¾èµ–é«˜æ–¯åŸå§‹æ•°æ®è¿›è¡Œåœºæ™¯è¡¨ç¤ºã€‚åˆ©ç”¨çƒé¢æŠ•å½±ï¼Œæˆ‘ä»¬ä»LiDARæµ‹é‡ä¸­é©±åŠ¨åŸå§‹æ•°æ®çš„ç²¾ç‚¼ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ¹é…å½“å‰çš„æ³¨å†Œæ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ˜ å°„ä»»åŠ¡ä¸Šå®ç°äº†SOTAç»“æœï¼Œä¸”å…·æœ‰æœ€å°çš„GPUè¦æ±‚ã€‚å…¶é«˜æ•ˆæ€§ä½¿å…¶æˆä¸ºå®æ—¶æœºå™¨äººä¼°è®¡ä»»åŠ¡çš„è¿›ä¸€æ­¥æ¢ç´¢å’Œæ½œåœ¨é‡‡ç”¨çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LiDARæŠ€æœ¯æä¾›å‡†ç¡®çš„å‡ ä½•æµ‹é‡ï¼Œå¯¹è‡ªæˆ‘è¿åŠ¨ä¼°è®¡å’Œé‡å»ºä»»åŠ¡å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>ç®¡ç†ç¯å¢ƒçš„å‡†ç¡®ä¸”è½»é‡åŒ–è¡¨ç¤ºä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨é«˜æ–¯æ¶‚æŠ¹æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œå¼€å‘å‡ºæ–°å‹LiDARé‡Œç¨‹è®¡å’Œæ˜ å°„æµç¨‹ã€‚</li>
<li>è¯¥æµç¨‹ä»…ä¾èµ–é«˜æ–¯åŸå§‹æ•°æ®è¿›è¡Œåœºæ™¯è¡¨ç¤ºï¼Œåˆ©ç”¨çƒé¢æŠ•å½±ä»LiDARæµ‹é‡ä¸­é©±åŠ¨åŸå§‹æ•°æ®çš„ç²¾ç‚¼ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åŒ¹é…å½“å‰æ³¨å†Œæ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ˜ å°„ä»»åŠ¡ä¸Šå®ç°SOTAç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æœ€å°çš„GPUè¦æ±‚ï¼Œè¡¨ç°å‡ºé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d9be9d1f5377c68a42c25fb0783e3483.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-997b218a1e3dc2e9b5a355c1d7ac228e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ae882f17efb5f3562ba446abfc3df85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666130839d212ad36e9cb022ace8d1cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-667c8cdd7b4e0a225d99436b67e29c68.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FFaceNeRF-Few-shot-Face-Editing-in-Neural-Radiance-Fields"><a href="#FFaceNeRF-Few-shot-Face-Editing-in-Neural-Radiance-Fields" class="headerlink" title="FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields"></a>FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields</h2><p><strong>Authors:Kwan Yun, Chaelin Kim, Hangyeul Shin, Junyong Noh</strong></p>
<p>Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {\href{<a target="_blank" rel="noopener" href="https://kwanyun.github.io/FFaceNeRF_page/%7D%7Bproject-page%7D%7D">https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}</a>. </p>
<blockquote>
<p>é‡‡ç”¨æ©è†œæŠ€æœ¯çš„æœ€æ–°ä¸‰ç»´äººè„¸ç¼–è¾‘æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”Ÿæˆäº†é«˜è´¨é‡ç¼–è¾‘å›¾åƒã€‚å°½ç®¡è¿™äº›æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºä½¿ç”¨äº†é¢„è®­ç»ƒåˆ†å‰²æ©è†œï¼Œå®ƒä»¬é€šå¸¸æä¾›çš„ç”¨æˆ·æ§åˆ¶æœ‰é™ã€‚ä¸ºäº†ä½¿ç”¨å…·æœ‰æ‰€éœ€å¸ƒå±€çš„é¢ç½©ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™å¾ˆéš¾æ”¶é›†ã€‚æˆ‘ä»¬æå‡ºäº†FFaceNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºNeRFçš„äººè„¸ç¼–è¾‘æŠ€æœ¯ï¼Œå…‹æœäº†å› ä½¿ç”¨å›ºå®šæ©è†œå¸ƒå±€è€Œå¯¼è‡´çš„ç”¨æˆ·æ§åˆ¶æœ‰é™çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å‡ ä½•é€‚é…å™¨ä¸ç‰¹å¾æ³¨å…¥ï¼Œå…è®¸æœ‰æ•ˆåœ°æ“ä½œå‡ ä½•å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ½œåœ¨æ··åˆæŠ€æœ¯è¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œè¿™èƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚è¿™ä¿ƒè¿›äº†æ¨¡å‹å¿«é€Ÿé€‚åº”æ‰€éœ€çš„æ©è†œå¸ƒå±€ï¼Œå¯¹äºä¸ªæ€§åŒ–åŒ»å­¦å½±åƒæˆ–åˆ›æ„äººè„¸ç¼–è¾‘ç­‰é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼ŒFFaceNeRFåœ¨çµæ´»æ€§ã€æ§åˆ¶å’Œç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„åŸºäºæ©è†œçš„äººè„¸ç¼–è¾‘æ–¹æ³•ï¼Œä¸ºå®šåˆ¶å’Œé«˜ä¿çœŸä¸‰ç»´äººè„¸ç¼–è¾‘çš„æœªæ¥è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ä»£ç å·²åœ¨é¡¹ç›®é¡µé¢ï¼ˆ<a target="_blank" rel="noopener" href="https://kwanyun.github.io/FFaceNeRF_page/%EF%BC%89%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://kwanyun.github.io/FFaceNeRF_page/ï¼‰ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17095v1">PDF</a> CVPR2025, 11 pages, 14 figures</p>
<p><strong>Summary</strong><br>ä½¿ç”¨NeRFæŠ€æœ¯çš„3Dé¢éƒ¨ç¼–è¾‘æ–¹æ³•é€šè¿‡é¢å…·è¿›è¡Œé«˜è´¨å›¾åƒç¼–è¾‘ã€‚ç°æœ‰æ–¹æ³•å› ä½¿ç”¨é¢„è®­ç»ƒåˆ†å‰²é¢å…·è€Œç”¨æˆ·æ§åˆ¶å—é™ã€‚æˆ‘ä»¬æå‡ºFFaceNeRFæŠ€æœ¯ï¼Œé€šè¿‡å‡ ä½•é€‚é…å™¨å’Œç‰¹å¾æ³¨å…¥è®¾è®¡ï¼Œå…‹æœäº†ä½¿ç”¨å›ºå®šé¢å…·å¸ƒå±€å¯¼è‡´çš„ç”¨æˆ·æ§åˆ¶æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ½œåœ¨æ··åˆæŠ€æœ¯è¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹å¿«é€Ÿé€‚åº”æ‰€éœ€çš„é¢å…·å¸ƒå±€ã€‚è¿™æŠ€æœ¯ä¸ºä¸ªæ€§åŒ–åŒ»ç–—æˆåƒå’Œåˆ›æ„é¢éƒ¨ç¼–è¾‘ç­‰åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FFaceNeRFæ˜¯åŸºäºNeRFçš„é¢éƒ¨ç¼–è¾‘æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å› ä½¿ç”¨é¢„è®­ç»ƒåˆ†å‰²é¢å…·å¯¼è‡´çš„ç”¨æˆ·æ§åˆ¶å—é™çš„é—®é¢˜ã€‚</li>
<li>FFaceNeRFé€šè¿‡å‡ ä½•é€‚é…å™¨å’Œç‰¹å¾æ³¨å…¥è®¾è®¡ï¼Œå®ç°äº†æœ‰æ•ˆçš„å‡ ä½•å±æ€§æ“ä½œã€‚</li>
<li>é‡‡ç”¨æ½œåœ¨æ··åˆæŠ€æœ¯è¿›è¡Œä¸‰å¹³é¢å¢å¼ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¿«é€Ÿé€‚åº”æ‰€éœ€çš„é¢å…·å¸ƒå±€ã€‚</li>
<li>FFaceNeRFåœ¨çµæ´»æ€§ã€æ§åˆ¶å’Œç”Ÿæˆçš„å›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„åŸºäºé¢å…·çš„é¢éƒ¨ç¼–è¾‘æ–¹æ³•ã€‚</li>
<li>FFaceNeRFä¸ºä¸ªæ€§åŒ–åŒ»ç–—æˆåƒå’Œåˆ›æ„é¢éƒ¨ç¼–è¾‘ç­‰åº”ç”¨æä¾›äº†å¯èƒ½ã€‚</li>
<li>FFaceNeRFçš„æŠ€æœ¯åœ¨ç”Ÿæˆé«˜è´¨é‡ç¼–è¾‘å›¾åƒæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc159b82dbe17b48b3ef37e8157479d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce7a46159b8e2ff1bc339e0fcaaf7987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5819230720ea1883f74fbd5f797383f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae26aac97fbeb6bfea7a1bc46523922b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97501c3dda47cc54e1c1efa600ac13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4ad5718adc27195c33254b7b834450.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DroneSplat-3D-Gaussian-Splatting-for-Robust-3D-Reconstruction-from-In-the-Wild-Drone-Imagery"><a href="#DroneSplat-3D-Gaussian-Splatting-for-Robust-3D-Reconstruction-from-In-the-Wild-Drone-Imagery" class="headerlink" title="DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from   In-the-Wild Drone Imagery"></a>DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from   In-the-Wild Drone Imagery</h2><p><strong>Authors:Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang</strong></p>
<p>Drones have become essential tools for reconstructing wild scenes due to their outstanding maneuverability. Recent advances in radiance field methods have achieved remarkable rendering quality, providing a new avenue for 3D reconstruction from drone imagery. However, dynamic distractors in wild environments challenge the static scene assumption in radiance fields, while limited view constraints hinder the accurate capture of underlying scene geometry. To address these challenges, we introduce DroneSplat, a novel framework designed for robust 3D reconstruction from in-the-wild drone imagery. Our method adaptively adjusts masking thresholds by integrating local-global segmentation heuristics with statistical approaches, enabling precise identification and elimination of dynamic distractors in static scenes. We enhance 3D Gaussian Splatting with multi-view stereo predictions and a voxel-guided optimization strategy, supporting high-quality rendering under limited view constraints. For comprehensive evaluation, we provide a drone-captured 3D reconstruction dataset encompassing both dynamic and static scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS and NeRF baselines in handling in-the-wild drone imagery. </p>
<blockquote>
<p>ç”±äºæ— äººæœºçš„å‡ºè‰²æœºåŠ¨æ€§ï¼Œå®ƒä»¬å·²æˆä¸ºé‡å»ºé‡å¤–åœºæ™¯çš„é‡è¦å·¥å…·ã€‚æœ€è¿‘ï¼Œè¾å°„åœºæ–¹æ³•çš„è¿›æ­¥å®ç°äº†æ˜¾è‘—çš„æ¸²æŸ“è´¨é‡ï¼Œä¸ºä»æ— äººæœºå›¾åƒè¿›è¡Œ3Dé‡å»ºæä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œé‡å¤–ç¯å¢ƒä¸­çš„åŠ¨æ€å¹²æ‰°å› ç´ æŒ‘æˆ˜äº†è¾å°„åœºçš„é™æ€åœºæ™¯å‡è®¾ï¼Œè€Œæœ‰é™çš„è§†é‡çº¦æŸé˜»ç¢äº†åº•å±‚åœºæ™¯å‡ ä½•çš„å‡†ç¡®æ•æ‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DroneSplatï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä»é‡å¤–æ— äººæœºå›¾åƒè¿›è¡Œç¨³å¥3Dé‡å»ºè®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆå±€éƒ¨-å…¨å±€åˆ†å‰²å¯å‘ä¸ç»Ÿè®¡æ–¹æ³•è‡ªé€‚åº”åœ°è°ƒæ•´æ©æ¨¡é˜ˆå€¼ï¼Œä»è€Œå®ç°åœ¨é™æ€åœºæ™¯ä¸­ç²¾ç¡®è¯†åˆ«å’Œæ¶ˆé™¤åŠ¨æ€å¹²æ‰°å› ç´ ã€‚æˆ‘ä»¬å¢å¼ºäº†3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼Œå€ŸåŠ©å¤šè§†è§’ç«‹ä½“é¢„æµ‹å’Œä½“ç´ å¼•å¯¼ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨æœ‰é™çš„è§†é‡çº¦æŸä¸‹å®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŒ…å«åŠ¨æ€å’Œé™æ€åœºæ™¯çš„æ— äººæœºæ‹æ‘„3Dé‡å»ºæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤„ç†é‡å¤–æ— äººæœºå›¾åƒæ—¶ï¼ŒDroneSplatçš„è¡¨ç°ä¼˜äº3DGSå’ŒNeRFåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16964v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ— äººæœºç”±äºå…¶å“è¶Šçš„æœºåŠ¨æ€§ï¼Œå·²æˆä¸ºé‡å»ºé‡å¤–åœºæ™¯çš„é‡è¦å·¥å…·ã€‚è¿‘æœŸï¼Œå…‰çº¿åœºæ–¹æ³•çš„è¿›å±•ä¸ºé‡å»ºæ•ˆæœå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æ¸²æŸ“è´¨é‡ï¼Œä¸ºä»æ— äººæœºå½±åƒä¸­è¿›è¡Œä¸‰ç»´é‡å»ºå¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œé‡å¤–ç¯å¢ƒä¸­çš„åŠ¨æ€å¹²æ‰°å› ç´ æŒ‘æˆ˜äº†å…‰çº¿åœºçš„é™æ€åœºæ™¯å‡è®¾ï¼Œè€Œè§†é‡é™åˆ¶åˆ™é˜»ç¢äº†å‡†ç¡®æ•æ‰åœºæ™¯çš„åº•å±‚å‡ ä½•ç»“æ„ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DroneSplatæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å®ç°ä»é‡å¤–æ— äººæœºå½±åƒä¸­è¿›è¡Œç¨³å¥çš„ä¸‰ç»´é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆå±€éƒ¨å…¨å±€åˆ†å‰²å¯å‘ä¸ç»Ÿè®¡æ–¹æ³•è‡ªé€‚åº”åœ°è°ƒæ•´æ©æ¨¡é˜ˆå€¼ï¼Œä»è€Œå®ç°äº†ç²¾ç¡®è¯†åˆ«å’Œæ¶ˆé™¤é™æ€åœºæ™¯ä¸­çš„åŠ¨æ€å¹²æ‰°å› ç´ ã€‚æˆ‘ä»¬é‡‡ç”¨å¢å¼ºå‹ä¸‰ç»´é«˜æ–¯å¹³é“ºæŠ€æœ¯ï¼Œç»“åˆå¤šè§†è§’ç«‹ä½“é¢„æµ‹å’Œä½“ç´ å¼•å¯¼ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨è§†é‡å—é™çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŒ…å«åŠ¨æ€å’Œé™æ€åœºæ™¯åœ¨å†…çš„æ— äººæœºæ‹æ‘„ä¸‰ç»´é‡å»ºæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤„ç†é‡å¤–æ— äººæœºå½±åƒæ—¶ï¼ŒDroneSplatçš„è¡¨ç°ä¼˜äºä¸‰ç»´é«˜æ–¯å¹³é“ºæŠ€æœ¯å’ŒNeRFåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>æ— äººæœºæˆä¸ºé‡å»ºé‡å¤–åœºæ™¯çš„å…³é”®å·¥å…·ï¼Œå…¶å“è¶Šçš„æœºåŠ¨æ€§å¯¹äºé‡å»ºè‡³å…³é‡è¦ã€‚</li>
<li>å…‰çº¿åœºæ–¹æ³•å®ç°äº†é«˜è´¨é‡æ¸²æŸ“æ•ˆæœï¼Œä¸ºä»æ— äººæœºå½±åƒè¿›è¡Œä¸‰ç»´é‡å»ºæä¾›äº†æ–°çš„è·¯å¾„ã€‚</li>
<li>åŠ¨æ€å¹²æ‰°å› ç´ å’Œè§†é‡é™åˆ¶æ˜¯ç°æœ‰é‡å»ºæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>DroneSplatæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ©æ¨¡é˜ˆå€¼è¯†åˆ«å¹¶æ¶ˆé™¤åŠ¨æ€å¹²æ‰°å› ç´ ã€‚</li>
<li>DroneSplaté‡‡ç”¨å¢å¼ºå‹ä¸‰ç»´é«˜æ–¯å¹³é“ºæŠ€æœ¯ï¼Œç»“åˆå¤šè§†è§’ç«‹ä½“é¢„æµ‹å’Œä½“ç´ å¼•å¯¼ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ— äººæœºæ‹æ‘„ä¸‰ç»´é‡å»ºæ•°æ®é›†ç”¨äºè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-922c7720b371bf3dcb2a93b014237096.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-904c1fcb8233427073534db893229517.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0009ab6e6c42a25e3cea22bac9263324.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fd22c8e18ba11e9755aad8ff775eac0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Simulation-Based-Design-Enhancement-of-Multilayer-GaN-and-InN-GaN-AlN-MSM-Photodetectors-for-Ultraviolet-Sensing"><a href="#Simulation-Based-Design-Enhancement-of-Multilayer-GaN-and-InN-GaN-AlN-MSM-Photodetectors-for-Ultraviolet-Sensing" class="headerlink" title="Simulation Based Design Enhancement of Multilayer GaN and InN&#x2F;GaN&#x2F;AlN   MSM Photodetectors for Ultraviolet Sensing"></a>Simulation Based Design Enhancement of Multilayer GaN and InN&#x2F;GaN&#x2F;AlN   MSM Photodetectors for Ultraviolet Sensing</h2><p><strong>Authors:M. Kilin, O. Tanriverdi, B. Karahan, F. Yasar</strong></p>
<p>A GaN-based photodetector structure was optimized and modeled in the mesa region, incorporating a nickel-gold contact layer on a sapphire substrate to enhance ultraviolet (UV) detection. To further improve performance, an alternative design integrating thin InN (top) and AlN (buffer) layers was proposed. These additional layers were introduced to enhance carrier transport and optical absorption within the device. Following mesa thickness simulations, a silicon carbide (SiC) substrate was tested to assess its impact on detector performance. Each simulation phase aimed to optimize current-voltage (I-V) characteristics, photoabsorption rate, and spectral responsivity, ensuring the design approached realistic operational conditions. A new Sapphire&#x2F;GaN(p-type)&#x2F;GaN&#x2F;GaN(n-type)&#x2F;Ni&#x2F;Au detector was designed based on the optimized buffer layer thicknesses in the mesa region, using parameters from the Sapphire&#x2F;AlN&#x2F;GaN&#x2F;InN&#x2F;Ni&#x2F;Au structure. This new detector was further optimized as a function of doping concentration. Additionally, the contact electrode finger thickness and inter-finger spacing were geometrically refined to maximize performance. This comprehensive simulation study demonstrates a significant enhancement in photodetector response through structural and doping optimizations. Normalized I-V characteristics revealed a photocurrent increase of 1.16-fold to 1.38-fold at each optimization stage. Similarly, mesa region thickness optimizations improved the photoabsorption rate from 1.97e24 1&#x2F;s . cm^3 to 2.18e24 1&#x2F;s . cm^3. Furthermore, the spectral responsivity in the UV region increased from 0.28e-7 A to 0.47e-7 A at 367 nm.These results show significant improvements in the structural and electrical performance of GaN-based photodetectors, providing a promising way to develop high-efficiency UV detection devices. </p>
<blockquote>
<p>åŸºäºGaNçš„å…‰ç”µæ¢æµ‹å™¨ç»“æ„åœ¨å°é¢åŒºåŸŸè¿›è¡Œäº†ä¼˜åŒ–å’Œå»ºæ¨¡ï¼Œå…¶ä¸­åœ¨è“å®çŸ³åŸºæ¿ä¸ŠåŠ å…¥äº†é•é‡‘æ¥è§¦å±‚ï¼Œä»¥æé«˜ç´«å¤–ï¼ˆUVï¼‰æ¢æµ‹æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæå‡ºäº†ä¸€ç§é›†æˆè–„InNï¼ˆé¡¶éƒ¨ï¼‰å’ŒAlNï¼ˆç¼“å†²ï¼‰å±‚çš„è®¾è®¡æ–¹æ¡ˆã€‚å¼•å…¥è¿™äº›é™„åŠ å±‚æ˜¯ä¸ºäº†å¢å¼ºå™¨ä»¶å†…çš„è½½æµå­ä¼ è¾“å’Œå…‰å­¦å¸æ”¶ã€‚åœ¨å¯¹å°é¢åšåº¦è¿›è¡Œæ¨¡æ‹Ÿåï¼Œå¯¹ç¢³åŒ–ç¡…ï¼ˆSiCï¼‰åŸºæ¿è¿›è¡Œäº†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å…¶å¯¹æ¢æµ‹å™¨æ€§èƒ½çš„å½±å“ã€‚æ¯ä¸ªæ¨¡æ‹Ÿé˜¶æ®µçš„ç›®æ ‡éƒ½æ˜¯ä¼˜åŒ–ç”µæµ-ç”µå‹ï¼ˆI-Vï¼‰ç‰¹æ€§ã€å…‰å¸æ”¶ç‡å’Œå…‰è°±å“åº”åº¦ï¼Œç¡®ä¿è®¾è®¡æ¥è¿‘ç°å®æ“ä½œæ¡ä»¶ã€‚åŸºäºå°é¢åŒºåŸŸä¼˜åŒ–çš„ç¼“å†²å±‚åšåº¦ï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„Sapphire&#x2F;GaN(på‹)&#x2F;GaN&#x2F;GaN(nå‹)&#x2F;Ni&#x2F;Auæ¢æµ‹å™¨ï¼Œå¹¶ä½¿ç”¨Sapphire&#x2F;AlN&#x2F;GaN&#x2F;InN&#x2F;Ni&#x2F;Auç»“æ„çš„å‚æ•°ã€‚è¿›ä¸€æ­¥å¯¹æ–°çš„æ¢æµ‹å™¨è¿›è¡Œäº†æºæ‚æµ“åº¦ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæ¥è§¦ç”µææŒ‡åšå’ŒæŒ‡é—´é—´è·ä¹Ÿè¿›è¡Œäº†å‡ ä½•ç²¾åŒ–ï¼Œä»¥æœ€å¤§åŒ–æ€§èƒ½ã€‚è¿™é¡¹å…¨é¢çš„æ¨¡æ‹Ÿç ”ç©¶é€šè¿‡ç»“æ„å’Œæºæ‚ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å…‰ç”µæ¢æµ‹å™¨çš„å“åº”èƒ½åŠ›ã€‚å½’ä¸€åŒ–çš„I-Vç‰¹æ€§æ˜¾ç¤ºï¼Œåœ¨æ¯ä¸ªä¼˜åŒ–é˜¶æ®µï¼Œå…‰ç”µæµå¢åŠ äº†1.16å€è‡³1.38å€ã€‚åŒæ ·ï¼Œå°é¢åŒºåŸŸåšåº¦ä¼˜åŒ–å°†å…‰å¸æ”¶ç‡ä»1.97e24 1&#x2F;sÂ·cm^3æé«˜åˆ°2.18e24 1&#x2F;sÂ·cm^3ã€‚æ­¤å¤–ï¼Œåœ¨ç´«å¤–åŒºåŸŸçš„å…‰è°±å“åº”åº¦åœ¨367çº³ç±³å¤„ä»0.28e-7 Aå¢åŠ åˆ°0.47e-7 Aã€‚è¿™äº›ç»“æœæ˜¾è‘—æé«˜äº†GaNåŸºå…‰ç”µæ¢æµ‹å™¨çš„ç»“æ„å’Œç”µæ°”æ€§èƒ½ï¼Œä¸ºå¼€å‘é«˜æ•ˆç´«å¤–æ£€æµ‹å™¨ä»¶æä¾›äº†æœ‰å‰é€”çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14670v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æŠ¥é“äº†å¯¹åŸºäºGaNçš„å…‰æ¢æµ‹å™¨ç»“æ„çš„ä¼˜åŒ–å’Œå»ºæ¨¡è¿‡ç¨‹ã€‚ç ”ç©¶ä¸­ï¼Œé€šè¿‡åœ¨ mesa åŒºåŸŸå¼•å…¥é•é‡‘æ¥è§¦å±‚ã€è–„InNå’ŒAlNå±‚æ¥å¢å¼ºç´«å¤–æ£€æµ‹æ€§èƒ½ã€‚æ¨¡æ‹Ÿè¿‡ç¨‹æ—¨åœ¨ä¼˜åŒ–ç”µæµ-ç”µå‹ç‰¹æ€§ã€å…‰å¸æ”¶ç‡å’Œå…‰è°±å“åº”åº¦ï¼Œç¡®ä¿è®¾è®¡æ¥è¿‘å®é™…è¿è¡Œæ¡ä»¶ã€‚æ–°çš„Sapphire&#x2F;GaN(p-type)&#x2F;GaN&#x2F;GaN(n-type)&#x2F;Ni&#x2F;Auæ¢æµ‹å™¨åŸºäºä¼˜åŒ–åçš„ç¼“å†²å±‚åšåº¦è®¾è®¡ï¼Œå¹¶ä½¿ç”¨Sapphire&#x2F;AlN&#x2F;GaN&#x2F;InN&#x2F;Ni&#x2F;Auç»“æ„çš„å‚æ•°ã€‚é€šè¿‡æºæ‚æµ“åº¦ã€æ¥è§¦ç”µææŒ‡åšå’ŒæŒ‡é—´é—´è·çš„å‡ ä½•ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¢æµ‹å™¨çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ç»“æ„å’Œæºæ‚ä¼˜åŒ–æ˜¾è‘—æé«˜å…‰æ¢æµ‹å™¨å“åº”çš„ç»¼åˆæ¨¡æ‹Ÿç ”ç©¶ç»“æœã€‚å½’ä¸€åŒ–ç”µæµ-ç”µå‹ç‰¹æ€§æ˜¾ç¤ºï¼Œåœ¨æ¯ä¸ªä¼˜åŒ–é˜¶æ®µï¼Œå…‰ç”µæµå¢åŠ äº†1.16è‡³1.38å€ã€‚æ­¤å¤–ï¼ŒmesaåŒºåŸŸåšåº¦ä¼˜åŒ–å°†å…‰å¸æ”¶ç‡ä»1.97e24 1&#x2F;sÂ·cmÂ³æé«˜åˆ°2.18e24 1&#x2F;sÂ·cmÂ³ã€‚åœ¨UVåŒºåŸŸçš„å…‰è°±å“åº”åº¦ä»0.28e-7 Aæé«˜åˆ°0.47e-7 Aã€‚è¿™äº›ç»“æœæ˜¾è‘—æé«˜äº†GaNåŸºå…‰æ¢æµ‹å™¨çš„ç»“æ„å’Œç”µæ°”æ€§èƒ½ï¼Œä¸ºå¼€å‘é«˜æ•ˆç´«å¤–æ£€æµ‹å™¨ä»¶æä¾›äº†æœ‰å‰é€”çš„é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GaNåŸºå…‰ç”µæ¢æµ‹å™¨ç»“æ„åœ¨mesaåŒºåŸŸè¿›è¡Œäº†ä¼˜åŒ–å’Œå»ºæ¨¡ï¼Œå¼•å…¥äº†é•é‡‘æ¥è§¦å±‚ä»¥æé«˜ç´«å¤–æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥è–„InNå’ŒAlNå±‚ï¼Œå¢å¼ºäº†è½½æµå­ä¼ è¾“å’Œå™¨ä»¶å†…çš„å…‰å¸æ”¶ã€‚</li>
<li>æ¨¡æ‹Ÿè¿‡ç¨‹æ—¨åœ¨ä¼˜åŒ–ç”µæµ-ç”µå‹ç‰¹æ€§ã€å…‰å¸æ”¶ç‡å’Œå…‰è°±å“åº”åº¦ï¼Œç¡®ä¿è®¾è®¡æ¥è¿‘å®é™…è¿è¡Œæ¡ä»¶ã€‚</li>
<li>æ–°çš„æ¢æµ‹å™¨è®¾è®¡åŸºäºä¼˜åŒ–åçš„ç¼“å†²å±‚åšåº¦ï¼Œå¹¶é€šè¿‡æºæ‚æµ“åº¦ã€æ¥è§¦ç”µææŒ‡åšå’ŒæŒ‡é—´é—´è·çš„å‡ ä½•ä¼˜åŒ–è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå…‰æ¢æµ‹å™¨æ€§èƒ½é€šè¿‡ç»“æ„å’Œæºæ‚ä¼˜åŒ–å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</li>
<li>mesaåŒºåŸŸåšåº¦ä¼˜åŒ–æ˜¾è‘—æé«˜äº†å…‰å¸æ”¶ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-312983ab7aa032b9e74ca74c0a37ac41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-711b7c9275f5abdb371425b303b6ee25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7278b744910bcb248bd33144656e0fdf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14b773120a87966faf187b66f238156c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e7bfccd91e3feabaa77772af7abb09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9613c67c9332e47d6800bbbc2c262dd7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="These-Magic-Moments-Differentiable-Uncertainty-Quantification-of-Radiance-Field-Models"><a href="#These-Magic-Moments-Differentiable-Uncertainty-Quantification-of-Radiance-Field-Models" class="headerlink" title="These Magic Moments: Differentiable Uncertainty Quantification of   Radiance Field Models"></a>These Magic Moments: Differentiable Uncertainty Quantification of   Radiance Field Models</h2><p><strong>Authors:Parker Ewen, Hao Chen, Seth Isaacson, Joey Wilson, Katherine A. Skinner, Ram Vasudevan</strong></p>
<p>This paper introduces a novel approach to uncertainty quantification for radiance fields by leveraging higher-order moments of the rendering equation. Uncertainty quantification is crucial for downstream tasks including view planning and scene understanding, where safety and robustness are paramount. However, the high dimensionality and complexity of radiance fields pose significant challenges for uncertainty quantification, limiting the use of these uncertainty quantification methods in high-speed decision-making. We demonstrate that the probabilistic nature of the rendering process enables efficient and differentiable computation of higher-order moments for radiance field outputs, including color, depth, and semantic predictions. Our method outperforms existing radiance field uncertainty estimation techniques while offering a more direct, computationally efficient, and differentiable formulation without the need for post-processing. Beyond uncertainty quantification, we also illustrate the utility of our approach in downstream applications such as next-best-view (NBV) selection and active ray sampling for neural radiance field training. Extensive experiments on synthetic and real-world scenes confirm the efficacy of our approach, which achieves state-of-the-art performance while maintaining simplicity. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ¸²æŸ“æ–¹ç¨‹çš„é«˜é˜¶çŸ©æ¥é‡åŒ–è¾å°„åœºä¸ç¡®å®šæ€§çš„æ–°æ–¹æ³•ã€‚ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è§†å›¾è§„åˆ’å’Œåœºæ™¯ç†è§£ï¼Œè¿™äº›ä»»åŠ¡ä¸­å®‰å…¨å’Œç¨³å¥æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¾å°„åœºçš„é«˜ç»´æ€§å’Œå¤æ‚æ€§ç»™ä¸ç¡®å®šæ€§é‡åŒ–å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†è¿™äº›ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•åœ¨é«˜é€Ÿå†³ç­–åˆ¶å®šä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ¸²æŸ“è¿‡ç¨‹çš„æ¦‚ç‡æ€§è´¨èƒ½å¤Ÿå®ç°è¾å°„åœºè¾“å‡ºï¼ˆåŒ…æ‹¬é¢œè‰²ã€æ·±åº¦å’Œè¯­ä¹‰é¢„æµ‹ï¼‰çš„é«˜é˜¶çŸ©çš„é«˜æ•ˆå’Œå¯å¾®è®¡ç®—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è¾å°„åœºä¸ç¡®å®šæ€§ä¼°è®¡æŠ€æœ¯ï¼ŒåŒæ—¶æä¾›äº†æ›´ç›´æ¥ã€è®¡ç®—æ›´é«˜æ•ˆã€å¯å¾®çš„å…¬å¼ï¼Œæ— éœ€åå¤„ç†ã€‚é™¤äº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæˆ‘ä»¬è¿˜è¯´æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œå¦‚æœ€ä½³ä¸‹è§†ï¼ˆNBVï¼‰é€‰æ‹©å’Œç¥ç»è¾å°„åœºè®­ç»ƒçš„ä¸»åŠ¨å°„çº¿é‡‡æ ·ã€‚åœ¨åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯çš„å¤§é‡å®éªŒè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç®€æ´æ€§çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14665v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ¸²æŸ“æ–¹ç¨‹çš„é«˜é˜¶çŸ©è¿›è¡Œè¾å°„åœºä¸ç¡®å®šæ€§é‡åŒ–çš„æ–°æ–¹æ³•ã€‚ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºä¸‹æ¸¸ä»»åŠ¡å¦‚è§†å›¾è§„åˆ’å’Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œå…¶ä¸­å®‰å…¨æ€§å’Œç¨³å¥æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡é€šè¿‡å±•ç¤ºæ¦‚ç‡æ¸²æŸ“è¿‡ç¨‹çš„æ€§è´¨ï¼Œå®ç°äº†è¾å°„åœºè¾“å‡ºï¼ˆåŒ…æ‹¬é¢œè‰²ã€æ·±åº¦å’Œè¯­ä¹‰é¢„æµ‹ï¼‰çš„é«˜é˜¶çŸ©çš„é«˜æ•ˆå¯å¾®è®¡ç®—ã€‚è¯¥æ–¹æ³•åœ¨ä¸éœ€è¦åå¤„ç†çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰è¾å°„åœºä¸ç¡®å®šæ€§ä¼°è®¡æŠ€æœ¯çš„æ€§èƒ½ï¼Œå…·æœ‰æ›´ç›´æ¥ã€è®¡ç®—æ•ˆç‡é«˜å’Œå¯å¾®åˆ†çš„ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æœ€ä½³è§†å›¾é€‰æ‹©å’Œç¥ç»è¾å°„åœºè®­ç»ƒæ´»åŠ¨å°„çº¿é‡‡æ ·ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç®€å•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¸²æŸ“æ–¹ç¨‹çš„é«˜é˜¶çŸ©è¿›è¡Œè¾å°„åœºçš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
<li>å±•ç¤ºäº†æ¦‚ç‡æ¸²æŸ“è¿‡ç¨‹çš„æ€§è´¨ï¼Œèƒ½é«˜æ•ˆè®¡ç®—è¾å°„åœºè¾“å‡ºçš„é«˜é˜¶çŸ©ã€‚</li>
<li>æ–¹æ³•æ— éœ€åå¤„ç†ï¼Œè®¡ç®—æ•ˆç‡é«˜ä¸”å…·å¤‡å¯å¾®åˆ†æ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºè§†å›¾è§„åˆ’å’Œåœºæ™¯ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æœ€ä½³è§†å›¾é€‰æ‹©ã€ç¥ç»è¾å°„åœºè®­ç»ƒç­‰æ´»åŠ¨ä¸­æœ‰å®ç”¨ä»·å€¼ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-55fae528950f92ea6d7b91ab87f2024a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69fbe8ddf942f044f9dea2a9d603869d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b665b4251366a8be36fa3189ccaecec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdda7f63928f94517f96ecc4eb0c72db.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Uni-Gaussians-Unifying-Camera-and-Lidar-Simulation-with-Gaussians-for-Dynamic-Driving-Scenarios"><a href="#Uni-Gaussians-Unifying-Camera-and-Lidar-Simulation-with-Gaussians-for-Dynamic-Driving-Scenarios" class="headerlink" title="Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for   Dynamic Driving Scenarios"></a>Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for   Dynamic Driving Scenarios</h2><p><strong>Authors:Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang</strong></p>
<p>Ensuring the safety of autonomous vehicles necessitates comprehensive simulation of multi-sensor data, encompassing inputs from both cameras and LiDAR sensors, across various dynamic driving scenarios. Neural rendering techniques, which utilize collected raw sensor data to simulate these dynamic environments, have emerged as a leading methodology. While NeRF-based approaches can uniformly represent scenes for rendering data from both camera and LiDAR, they are hindered by slow rendering speeds due to dense sampling. Conversely, Gaussian Splatting-based methods employ Gaussian primitives for scene representation and achieve rapid rendering through rasterization. However, these rasterization-based techniques struggle to accurately model non-linear optical sensors. This limitation restricts their applicability to sensors beyond pinhole cameras. To address these challenges and enable unified representation of dynamic driving scenarios using Gaussian primitives, this study proposes a novel hybrid approach. Our method utilizes rasterization for rendering image data while employing Gaussian ray-tracing for LiDAR data rendering. Experimental results on public datasets demonstrate that our approach outperforms current state-of-the-art methods. This work presents a unified and efficient solution for realistic simulation of camera and LiDAR data in autonomous driving scenarios using Gaussian primitives, offering significant advancements in both rendering quality and computational efficiency. </p>
<blockquote>
<p>ç¡®ä¿è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®‰å…¨éœ€è¦å…¨é¢æ¨¡æ‹Ÿå¤šä¼ æ„Ÿå™¨æ•°æ®ï¼ŒåŒ…æ‹¬æ¥è‡ªç›¸æœºå’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„è¾“å…¥ï¼Œä»¥åŠåœ¨ä¸åŒåŠ¨æ€é©¾é©¶åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚åˆ©ç”¨æ”¶é›†çš„åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®æ¨¡æ‹Ÿè¿™äº›åŠ¨æ€ç¯å¢ƒçš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å·²æˆä¸ºä¸€ç§ä¸»æµæ–¹æ³•ã€‚è™½ç„¶åŸºäºNeRFçš„æ–¹æ³•å¯ä»¥ç»Ÿä¸€è¡¨ç¤ºåœºæ™¯ï¼Œä»è€Œä»ç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ¸²æŸ“æ•°æ®ï¼Œä½†å®ƒä»¬å—åˆ°å¯†é›†é‡‡æ ·å¯¼è‡´æ¸²æŸ“é€Ÿåº¦æ…¢çš„é˜»ç¢ã€‚ç›¸åï¼ŒåŸºäºé«˜æ–¯å¹³é“ºçš„æ–¹æ³•ä½¿ç”¨é«˜æ–¯åŸå§‹æ•°æ®è¿›è¡Œåœºæ™¯è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å…‰æ …åŒ–å®ç°å¿«é€Ÿæ¸²æŸ“ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºå…‰æ …åŒ–çš„æŠ€æœ¯åœ¨æ¨¡æ‹Ÿéçº¿æ€§å…‰å­¦ä¼ æ„Ÿå™¨æ—¶é‡åˆ°äº†å›°éš¾ã€‚è¿™ä¸€å±€é™æ€§é™åˆ¶äº†å®ƒä»¬å¯¹é’ˆå­”ç›¸æœºä»¥å¤–ä¼ æ„Ÿå™¨çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ç°å¯¹åŠ¨æ€é©¾é©¶åœºæ™¯çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å…‰æ …åŒ–è¿›è¡Œå›¾åƒæ•°æ®æ¸²æŸ“ï¼ŒåŒæ—¶é‡‡ç”¨é«˜æ–¯å…‰çº¿è¿½è¸ªè¿›è¡Œæ¿€å…‰é›·è¾¾æ•°æ®æ¸²æŸ“ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨é«˜æ–¯åŸå§‹æ•°æ®åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­æ¨¡æ‹Ÿç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ•°æ®ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08317v3">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯å·²æˆä¸ºæ¨¡æ‹Ÿè‡ªä¸»é©¾é©¶è½¦è¾†åŠ¨æ€ç¯å¢ƒçš„ä¸»è¦æ–¹æ³•ï¼Œåˆ©ç”¨æ”¶é›†çš„åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œåœºæ™¯æ¸²æŸ“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºNeRFå’ŒGaussian Splattingçš„æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨rasterizationè¿›è¡Œå›¾åƒæ•°æ®æ¸²æŸ“ï¼Œé‡‡ç”¨Gaussianå°„çº¿è¿½è¸ªè¿›è¡Œæ¿€å…‰é›·è¾¾æ•°æ®æ¸²æŸ“ï¼Œå®ç°äº†åŠ¨æ€é©¾é©¶åœºæ™¯çš„ç»Ÿä¸€è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»è½¦è¾†çš„å®‰å…¨ä¿éšœéœ€è¦å…¨é¢æ¨¡æ‹Ÿå¤šä¼ æ„Ÿå™¨æ•°æ®ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„æ•°æ®ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯å·²æˆä¸ºæ¨¡æ‹Ÿè‡ªä¸»é©¾é©¶è½¦è¾†åŠ¨æ€ç¯å¢ƒçš„ä¸»è¦æ–¹æ³•ã€‚</li>
<li>NeRFå’ŒGaussian Splattingåœ¨æ¨¡æ‹Ÿä¸­æœ‰ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œæ€§èƒ½è¡¨ç°ã€‚</li>
<li>NeRF-basedæ–¹æ³•èƒ½å¤Ÿç»Ÿä¸€è¡¨ç¤ºç›¸æœºå’Œæ¿€å…‰é›·è¾¾çš„æ•°æ®ï¼Œä½†æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>Gaussian Splattingæ–¹æ³•å¯ä»¥è¿…é€Ÿæ¸²æŸ“ï¼Œä½†éš¾ä»¥å‡†ç¡®æ¨¡æ‹Ÿéçº¿æ€§å…‰å­¦ä¼ æ„Ÿå™¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºNeRFå’ŒGaussian Splattingçš„æ··åˆæ–¹æ³•ï¼Œå®ç°äº†é«˜è´¨é‡ä¸”é«˜æ•ˆçš„æ¸²æŸ“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a2b5623ea676f109d0166d8b76e5390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f09ea8ea4addd961ea8f0d8d90f713c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767bc213fb655e5f758fa1762e395d85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8f057714ec127ff4cb8a32d9b3dc26d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f87efb9c3b92cc8c55631d6ed26feb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b520610c17d90889fb1a1ed8b22392.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Identity-preserving-Distillation-Sampling-by-Fixed-Point-Iterator"><a href="#Identity-preserving-Distillation-Sampling-by-Fixed-Point-Iterator" class="headerlink" title="Identity-preserving Distillation Sampling by Fixed-Point Iterator"></a>Identity-preserving Distillation Sampling by Fixed-Point Iterator</h2><p><strong>Authors:SeonHwa Kim, Jiwon Kim, Soobin Park, Donghoon Ahn, Jiwon Kang, Seungryong Kim, Kyong Hwan Jin, Eunju Cha</strong></p>
<p>Score distillation sampling (SDS) demonstrates a powerful capability for text-conditioned 2D image and 3D object generation by distilling the knowledge from learned score functions. However, SDS often suffers from blurriness caused by noisy gradients. When SDS meets the image editing, such degradations can be reduced by adjusting bias shifts using reference pairs, but the de-biasing techniques are still corrupted by erroneous gradients. To this end, we introduce Identity-preserving Distillation Sampling (IDS), which compensates for the gradient leading to undesired changes in the results. Based on the analysis that these errors come from the text-conditioned scores, a new regularization technique, called fixed-point iterative regularization (FPR), is proposed to modify the score itself, driving the preservation of the identity even including poses and structures. Thanks to a self-correction by FPR, the proposed method provides clear and unambiguous representations corresponding to the given prompts in image-to-image editing and editable neural radiance field (NeRF). The structural consistency between the source and the edited data is obviously maintained compared to other state-of-the-art methods. </p>
<blockquote>
<p>åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰é€šè¿‡ä»å­¦ä¹ åˆ°çš„åˆ†æ•°å‡½æ•°ä¸­è’¸é¦çŸ¥è¯†ï¼Œå±•ç¤ºäº†ç”¨äºæ–‡æœ¬æ§åˆ¶çš„2Då›¾åƒå’Œ3Då¯¹è±¡ç”Ÿæˆçš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSDSç»å¸¸å› å™ªå£°æ¢¯åº¦è€Œå¯¼è‡´æ¨¡ç³Šã€‚å½“SDSé‡åˆ°å›¾åƒç¼–è¾‘æ—¶ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚è€ƒå¯¹çš„åå·®åç§»æ¥å‡å°‘è¿™ç§é€€åŒ–ï¼Œä½†å»åæŠ€æœ¯ä»ç„¶ä¼šå—åˆ°é”™è¯¯æ¢¯åº¦çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ’ç­‰ä¿æŒè’¸é¦é‡‡æ ·ï¼ˆIDSï¼‰ï¼Œå®ƒå¼¥è¡¥äº†å¯¼è‡´ç»“æœä¸­å‡ºç°ä¸éœ€è¦æ›´æ”¹çš„æ¢¯åº¦ã€‚åŸºäºè¿™äº›é”™è¯¯æ¥è‡ªæ–‡æœ¬æ§åˆ¶çš„åˆ†æ•°çš„åˆ†æï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç§°ä¸ºå®šç‚¹è¿­ä»£æ­£åˆ™åŒ–ï¼ˆFPRï¼‰ï¼Œç”¨äºä¿®æ”¹åˆ†æ•°æœ¬èº«ï¼Œä¿ƒä½¿èº«ä»½å¾—åˆ°ä¿ç•™ï¼Œç”šè‡³åŒ…æ‹¬å§¿åŠ¿å’Œç»“æ„ã€‚ç”±äºFPRçš„è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒåˆ°å›¾åƒçš„ç¼–è¾‘å’Œå¯ç¼–è¾‘çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¸­æä¾›äº†æ¸…æ™°ä¸”æ— æ­§ä¹‰çš„è¡¨ç°ï¼Œå¯¹åº”äºç»™å®šçš„æç¤ºã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæºæ•°æ®å’Œç¼–è¾‘æ•°æ®ä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§å¾—åˆ°äº†æ˜æ˜¾çš„ä¿æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19930v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†SDSï¼ˆScore Distillation Samplingï¼‰åœ¨å¤„ç†æ–‡æœ¬æ¡ä»¶ä¸‹çš„äºŒç»´å›¾åƒå’Œä¸‰ç»´å¯¹è±¡ç”Ÿæˆæ—¶çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†å…¶å­˜åœ¨çš„æ¨¡ç³Šé—®é¢˜ç”±å™ªå£°æ¢¯åº¦å¼•èµ·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†IDSï¼ˆIdentity-preserving Distillation Samplingï¼‰ï¼Œé€šè¿‡è¡¥å¿æ¢¯åº¦æ¥é¿å…ç»“æœä¸­çš„ä¸å½“å˜åŒ–ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŠ€æœ¯FPRï¼ˆFixed-point Iterative Regularizationï¼‰ï¼Œä¿®æ”¹è¯„åˆ†æœ¬èº«ï¼Œä»¥åœ¨å›¾åƒåˆ°å›¾åƒçš„ç¼–è¾‘å’Œå¯ç¼–è¾‘çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¸­ä¿æŒä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡FPRçš„è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½ï¼Œæä¾›äº†æ¸…æ™°ä¸”æ— æ­§ä¹‰çš„è¡¨ç°ï¼Œä¿æŒäº†æºæ•°æ®å’Œç¼–è¾‘æ•°æ®ä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SDSåœ¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„äºŒç»´å›¾åƒå’Œä¸‰ç»´å¯¹è±¡ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ¨¡ç³Šé—®é¢˜ï¼Œä¸»è¦ç”±å™ªå£°æ¢¯åº¦å¼•èµ·ã€‚</li>
<li>IDSé€šè¿‡è¡¥å¿æ¢¯åº¦æ¥è§£å†³SDSä¸­çš„æ¨¡ç³Šé—®é¢˜ï¼Œé¿å…ç»“æœä¸­çš„ä¸å½“å˜åŒ–ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„æ­£åˆ™åŒ–æŠ€æœ¯FPRï¼Œç”¨äºä¿®æ”¹è¯„åˆ†æœ¬èº«ï¼Œä»¥åœ¨å›¾åƒç¼–è¾‘ä¸­ä¿æŒä¸€è‡´æ€§ã€‚</li>
<li>FPRçš„è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½ä½¿å¾—æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´æ¸…æ™°ã€æ— æ­§ä¹‰çš„è¡¨ç°ã€‚</li>
<li>IDSå’ŒFPRçš„ç»“åˆæœ‰åŠ©äºåœ¨å›¾åƒåˆ°å›¾åƒçš„ç¼–è¾‘å’Œå¯ç¼–è¾‘çš„NeRFä¸­ä¿æŒæºæ•°æ®å’Œç¼–è¾‘æ•°æ®ä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜æ–‡æœ¬æ¡ä»¶ä¸‹çš„å›¾åƒå’Œ3Då¯¹è±¡ç”Ÿæˆçš„è´¨é‡å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8d591944411138ba1d2d49f7241ee60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c859505b5dccf9972c41192ecb99148f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98b2cb0c01883a687ad6d42c42938f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e6c35411b2b9a656a9f07fb0268d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f465eadad2c991b54850ec880c00d220.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c12b001576b7304b3139d989dcf647.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-439bf48a6d540177e13ce5d3e43d0e2b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RelationField-Relate-Anything-in-Radiance-Fields"><a href="#RelationField-Relate-Anything-in-Radiance-Fields" class="headerlink" title="RelationField: Relate Anything in Radiance Fields"></a>RelationField: Relate Anything in Radiance Fields</h2><p><strong>Authors:Sebastian Koch, Johanna Wald, Mirco Colosi, Narunas Vaskevicius, Pedro Hermosilla, Federico Tombari, Timo Ropinski</strong></p>
<p>Neural radiance fields are an emerging 3D scene representation and recently even been extended to learn features for scene understanding by distilling open-vocabulary features from vision-language models. However, current method primarily focus on object-centric representations, supporting object segmentation or detection, while understanding semantic relationships between objects remains largely unexplored. To address this gap, we propose RelationField, the first method to extract inter-object relationships directly from neural radiance fields. RelationField represents relationships between objects as pairs of rays within a neural radiance field, effectively extending its formulation to include implicit relationship queries. To teach RelationField complex, open-vocabulary relationships, relationship knowledge is distilled from multi-modal LLMs. To evaluate RelationField, we solve open-vocabulary 3D scene graph generation tasks and relationship-guided instance segmentation, achieving state-of-the-art performance in both tasks. See the project website at <a target="_blank" rel="noopener" href="https://relationfield.github.io/">https://relationfield.github.io</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºæ˜¯ä¸€ç§æ–°å…´çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œæœ€è¿‘ç”šè‡³è¢«æ‰©å±•ä¸ºé€šè¿‡ä»è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è’¸é¦å¼€æ”¾è¯æ±‡ç‰¹å¾æ¥å­¦ä¹ åœºæ™¯ç†è§£çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é¢å‘å¯¹è±¡çš„è¡¨ç¤ºä¸Šï¼Œæ”¯æŒå¯¹è±¡åˆ†å‰²æˆ–æ£€æµ‹ï¼Œè€Œå¯¹å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»çš„ç†è§£åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†RelationFieldï¼Œè¿™æ˜¯ç¬¬ä¸€ç§ç›´æ¥ä»ç¥ç»è¾å°„åœºä¸­æå–å¯¹è±¡é—´å…³ç³»çš„æ–¹æ³•ã€‚RelationFieldå°†å¯¹è±¡ä¹‹é—´çš„å…³ç³»è¡¨ç¤ºä¸ºç¥ç»è¾å°„åœºå†…çš„ä¸€å¯¹å°„çº¿ï¼Œæœ‰æ•ˆåœ°å°†å…¶å…¬å¼æ‰©å±•ä¸ºåŒ…æ‹¬éšå¼å…³ç³»æŸ¥è¯¢ã€‚ä¸ºäº†å‘RelationFieldä¼ æˆå¤æ‚ã€å¼€æ”¾è¯æ±‡çš„å…³ç³»ï¼Œå…³ç³»çŸ¥è¯†æ˜¯ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è’¸é¦å¾—åˆ°çš„ã€‚ä¸ºäº†è¯„ä¼°RelationFieldï¼Œæˆ‘ä»¬è§£å†³äº†å¼€æ”¾å¼è¯æ±‡3Dåœºæ™¯å›¾ç”Ÿæˆä»»åŠ¡å’Œå…³ç³»å¼•å¯¼å®ä¾‹åˆ†å‰²é—®é¢˜ï¼Œåœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://relationfield.github.io./">https://relationfield.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13652v2">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://relationfield.github.io/">https://relationfield.github.io</a></p>
<p><strong>Summary</strong></p>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºé¢†åŸŸå±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼Œå¯¹äºå¯¹è±¡é—´çš„è¯­ä¹‰å…³ç³»ç†è§£æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºRelationFieldï¼Œé¦–æ¬¡ç›´æ¥ä»ç¥ç»è¾å°„åœºä¸­æå–å¯¹è±¡é—´å…³ç³»ã€‚RelationFieldé€šè¿‡ç¥ç»è¾å°„åœºå†…çš„å°„çº¿å¯¹è¡¨ç¤ºå¯¹è±¡é—´å…³ç³»ï¼Œå¹¶æ‰©å±•äº†å…¶å…¬å¼ä»¥æ¶µç›–éšå¼å…³ç³»æŸ¥è¯¢ã€‚ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æç‚¼å…³ç³»çŸ¥è¯†æ¥è®­ç»ƒRelationFieldå¤æ‚çš„å¼€æ”¾è¯æ±‡å…³ç³»ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒRelationFieldåœ¨å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆä»»åŠ¡å’Œå…³ç³»å¼•å¯¼å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰NeRFä¸»è¦å…³æ³¨å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼Œå¯¹å¯¹è±¡é—´è¯­ä¹‰å…³ç³»çš„ç†è§£æœ‰é™ã€‚</li>
<li>RelationFieldé¦–æ¬¡ç›´æ¥ä»ç¥ç»è¾å°„åœºä¸­æå–å¯¹è±¡é—´å…³ç³»ã€‚</li>
<li>RelationFieldé€šè¿‡ç¥ç»è¾å°„åœºå†…çš„å°„çº¿å¯¹æ¥è¡¨ç¤ºå¯¹è±¡é—´çš„å…³ç³»ã€‚</li>
<li>RelationFieldæ‰©å±•äº†å…¶å…¬å¼ä»¥æ¶µç›–éšå¼å…³ç³»æŸ¥è¯¢ã€‚</li>
<li>RelationFieldåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…³ç³»çŸ¥è¯†æ¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>RelationFieldåœ¨å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯å›¾ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b9c70171db4c6b64b6112f173875b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-004f258fc33778006a7b93a091912df1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-592e163f0571f53f117fcb76fcbd9de9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Gaussian-Splatting-for-Efficient-Satellite-Image-Photogrammetry"><a href="#Gaussian-Splatting-for-Efficient-Satellite-Image-Photogrammetry" class="headerlink" title="Gaussian Splatting for Efficient Satellite Image Photogrammetry"></a>Gaussian Splatting for Efficient Satellite Image Photogrammetry</h2><p><strong>Authors:Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret</strong></p>
<p>Recently, Gaussian splatting has emerged as a strong alternative to NeRF, demonstrating impressive 3D modeling capabilities while requiring only a fraction of the training and rendering time. In this paper, we show how the standard Gaussian splatting framework can be adapted for remote sensing, retaining its high efficiency. This enables us to achieve state-of-the-art performance in just a few minutes, compared to the day-long optimization required by the best-performing NeRF-based Earth observation methods. The proposed framework incorporates remote-sensing improvements from EO-NeRF, such as radiometric correction and shadow modeling, while introducing novel components, including sparsity, view consistency, and opacity regularizations. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé«˜æ–¯æ¶‚æŠ¹ä½œä¸ºä¸€ç§å¼ºå¤§çš„NeRFæ›¿ä»£æ–¹æ³•å´­éœ²å¤´è§’ï¼Œå®ƒå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„3Då»ºæ¨¡èƒ½åŠ›ï¼ŒåŒæ—¶åªéœ€ä¸€å°éƒ¨åˆ†è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿æ ‡å‡†çš„é«˜æ–¯æ¶‚æŠ¹æ¡†æ¶é€‚åº”é¥æ„Ÿé¢†åŸŸï¼ŒåŒæ—¶ä¿æŒå…¶é«˜æ•ˆç‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œè¡¨ç°æœ€ä½³çš„åŸºäºNeRFçš„åœ°çƒè§‚æµ‹æ–¹æ³•éœ€è¦ä¸€æ•´å¤©çš„ä¼˜åŒ–ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç»“åˆäº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›ï¼Œå¦‚è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°é¢–ç»„ä»¶ï¼ŒåŒ…æ‹¬ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œä¸é€æ˜åº¦æ­£åˆ™åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13047v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜æ–¯å–·æº…ä½œä¸ºä¸€ç§å¼ºå¤§çš„NeRFæ›¿ä»£æ–¹æ³•ï¼Œè¿‘æœŸåœ¨3Då»ºæ¨¡é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå…¶è®­ç»ƒä¸æ¸²æŸ“æ—¶é—´ä»…éœ€ä¸€å°éƒ¨åˆ†ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•å°†æ ‡å‡†é«˜æ–¯å–·æº…æ¡†æ¶é€‚åº”äºé¥æ„Ÿé¢†åŸŸï¼ŒåŒæ—¶ä¿æŒå…¶é«˜æ•ˆç‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å®ç°å“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œè¡¨ç°æœ€ä½³çš„NeRFåœ°çƒè§‚æµ‹æ–¹æ³•éœ€è¦ä¸€æ•´å¤©çš„ä¼˜åŒ–ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç»“åˆäº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›ï¼Œå¦‚è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°é¢–æˆåˆ†ï¼ŒåŒ…æ‹¬ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œä¸é€æ˜åº¦æ­£åˆ™åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯å–·æº…ä½œä¸ºä¸€ç§NeRFçš„æ›¿ä»£æ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ï¼Œä¸”è®­ç»ƒä¸æ¸²æŸ“æ—¶é—´çŸ­ã€‚</li>
<li>æ ‡å‡†é«˜æ–¯å–·æº…æ¡†æ¶å¯é€‚åº”é¥æ„Ÿé¢†åŸŸï¼Œä¿æŒé«˜æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡å®ç°çš„æ€§èƒ½è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼ŒçŸ­æ—¶é—´å†…å³å¯å®Œæˆã€‚</li>
<li>å¯¹æ¯”å…¶ä»–NeRFåœ°çƒè§‚æµ‹æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•æ— éœ€é•¿æ—¶é—´çš„ä¼˜åŒ–ã€‚</li>
<li>æ‰€æå‡ºçš„æ¡†æ¶ç»“åˆäº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›ï¼ŒåŒ…æ‹¬è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ã€‚</li>
<li>æ¡†æ¶å¼•å…¥äº†æ–°é¢–æˆåˆ†ï¼ŒåŒ…æ‹¬ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œä¸é€æ˜åº¦æ­£åˆ™åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e75a043b48bbcaba41cf104b2ee102b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d73c26cf60730c23c1f3c2fe83bcd27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6423b347f0d22f1248c49fb300a8060c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca6b3c6de3b6eb1d412024d093f8f0a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives"><a href="#Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives" class="headerlink" title="Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives"></a>Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives</h2><p><strong>Authors:Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein</strong></p>
<p>3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS to substantially improve rendering speed. These improvements also yield the ancillary benefits of reduced model size and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\mathit{6.71\times}$ across scenes from the Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets. </p>
<blockquote>
<p>3Dé«˜æ–¯ç‚¹äº‘æ³•ï¼ˆ3D-GSï¼‰æ˜¯ä¸€ç§æœ€æ–°çš„ä¸‰ç»´åœºæ™¯é‡å»ºæŠ€æœ¯ï¼Œå®ƒé€šè¿‡å‚æ•°åŒ–ç‚¹äº‘æ¨¡å‹å°†åœºæ™¯å»ºæ¨¡ä¸ºå¯å¾®åˆ†çš„ä¸‰ç»´é«˜æ–¯æ¨¡å‹ï¼Œå®ç°äº†æ–°è§†è§’çš„å®æ—¶æ¸²æŸ“ã€‚ç„¶è€Œï¼Œå…¶æ¸²æŸ“é€Ÿåº¦å’Œæ¨¡å‹å¤§å°ä»å­˜åœ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡è¯†åˆ«å¹¶è§£å†³äº†3D-GSä¸­çš„ä¸¤ä¸ªå…³é”®ä½æ•ˆé—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚è¿™äº›æ”¹è¿›è¿˜å¸¦æ¥äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´å‡å°‘çš„é¢å¤–å¥½å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æ¸²æŸ“æµç¨‹ï¼Œç²¾ç¡®åœ°å°†é«˜æ–¯å®šä½åœ¨åœºæ™¯ä¸­ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦ï¼Œè€Œè§†è§‰ä¿çœŸåº¦ä¿æŒä¸å˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å‰ªææŠ€æœ¯å¹¶å°†å…¶é›†æˆåˆ°è®­ç»ƒæµç¨‹ä¸­ï¼Œè¿™ä¸ä»…å¤§å¤§å‡å°‘äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ï¼Œè€Œä¸”è¿›ä¸€æ­¥æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚æˆ‘ä»¬çš„Speedy-Splatæ–¹æ³•ç»“åˆäº†è¿™äº›æŠ€æœ¯ï¼Œåœ¨Mip-NeRF 360ã€Tanksï¼†Templeså’ŒDeep Blendingæ•°æ®é›†çš„åœºæ™¯ä¸­ï¼Œå¹³å‡æ¸²æŸ“é€Ÿåº¦æé«˜äº†6.7 ä½¿ç”¨äº†è¿™é¡¹æŠ€æœ¯ä¹‹åä¸ä»…æå‡äº†æ¸²æŸ“çš„é€Ÿåº¦è€Œä¸”ä¹Ÿèƒ½é€šè¿‡å‹ç¼©æ¨¡å‹çš„å¤§å°å’Œç¼©çŸ­è®­ç»ƒæ—¶é—´å®ç°èŠ‚çº¦å¼€æ”¯çš„æ•ˆæœå€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00578v2">PDF</a> CVPR 2025, Project Page: <a target="_blank" rel="noopener" href="https://speedysplat.github.io/">https://speedysplat.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºé«˜æ–¯ç‚¹äº‘æ¨¡å‹çš„å®æ—¶æ¸²æŸ“æŠ€æœ¯æ”¹è¿›ã€‚é€šè¿‡ä¼˜åŒ–æ¸²æŸ“ç®¡é“å’Œå¼•å…¥æ–°å‹ä¿®å‰ªæŠ€æœ¯ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦ï¼ŒåŒæ—¶å‡å°äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ã€‚æ–°çš„æ–¹æ³•Speedy-Splatèƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dé«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼ˆGaussian Splattingï¼‰æ˜¯å®ç°å®æ—¶æ¸²æŸ“çš„ä¸€ç§æ–°æŠ€æœ¯ã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿåœºæ™¯ä½œä¸ºå¯å¾®åˆ†çš„ä¸‰ç»´é«˜æ–¯ç‚¹äº‘æ¥æ¸²æŸ“æ–°çš„è§†è§’ã€‚</li>
<li>å­˜åœ¨ä¸¤ä¸ªä¸»è¦çš„é—®é¢˜éœ€è¦è§£å†³ä»¥æé«˜æ•ˆç‡ï¼šæ¸²æŸ“é€Ÿåº¦ï¼Œæ¨¡å‹å¤§å°ä»¥åŠè®­ç»ƒæ—¶é—´ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æŠ€æœ¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æ¸²æŸ“ç®¡é“å¹¶ç²¾ç¡®å®šä½åœºæ™¯ä¸­çš„é«˜æ–¯åˆ†å¸ƒï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦è€Œä¸å½±å“è§†è§‰è´¨é‡ã€‚è¿™æ˜¯Speedy-Splatæ–¹æ³•çš„ä¸€éƒ¨åˆ†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä¿®å‰ªæŠ€æœ¯å¹¶å°†å…¶é›†æˆåˆ°è®­ç»ƒç®¡é“ä¸­ï¼Œè¿™è¿›ä¸€æ­¥å‡å°‘äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ï¼Œå¹¶æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚ä¿®å‰ªè¿‡ç¨‹æ˜¯å…³é”®çš„ä¸€æ­¥æ¥å®ç°æ›´é«˜çš„æ¸²æŸ“æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0312e3d05a4d9cfbf73e61f5aff10dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4aa5af078a50a81b2fa7a82fb96f0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0779e7661672a9a13f06c4514c1211c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48bca12f73986c1407d0764da3620e60.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors"><a href="#FrugalNeRF-Fast-Convergence-for-Few-shot-Novel-View-Synthesis-without-Learned-Priors" class="headerlink" title="FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors"></a>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without   Learned Priors</h2><p><strong>Authors:Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, Yu-Lun Liu</strong></p>
<p>Neural Radiance Fields (NeRF) face significant challenges in extreme few-shot scenarios, primarily due to overfitting and long training times. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æç«¯å°æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè¿‡æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´é•¿ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚FreeNeRFå’ŒSparseNeRFï¼Œä½¿ç”¨é¢‘ç‡æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒå…ˆéªŒï¼Œä½†é¢ä¸´å¤æ‚çš„è°ƒåº¦å’Œåå·®é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†FrugalNeRFï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°‘æ ·æœ¬NeRFæ¡†æ¶ï¼Œå®ƒé€šè¿‡è·¨å¤šä¸ªå°ºåº¦å…±äº«æƒé‡ä½“ç§¯æ¥æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ä¸€ç§è·¨å°ºåº¦å‡ ä½•é€‚åº”æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ ¹æ®è·¨å°ºåº¦çš„é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªçœŸå®æ·±åº¦ã€‚è¿™å¯ä»¥åœ¨ä¸ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒçš„æƒ…å†µä¸‹æŒ‡å¯¼è®­ç»ƒï¼Œå®ç°è®­ç»ƒæ•°æ®çš„å……åˆ†åˆ©ç”¨ã€‚å®ƒè¿˜å¯ä»¥é›†æˆé¢„è®­ç»ƒçš„å…ˆéªŒï¼Œæé«˜è´¨é‡è€Œä¸ä¼šå‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚åœ¨LLFFã€DTUå’ŒRealEstate-10Kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFrugalNeRFåœ¨å…¶ä»–å°‘æ ·æœ¬NeRFæ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œæˆä¸ºé«˜æ•ˆä¸”å‡†ç¡®çš„3Dåœºæ™¯é‡å»ºçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16271v2">PDF</a> Paper accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://linjohnss.github.io/frugalnerf/">https://linjohnss.github.io/frugalnerf/</a></p>
<p><strong>Summary</strong></p>
<p>NeRFåœ¨æç«¯å°‘æ ·æœ¬åœºæ™¯ä¸­å­˜åœ¨è¿‡æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´é•¿çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚FreeNeRFå’ŒSparseNeRFä½¿ç”¨é¢‘ç‡æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒå…ˆéªŒï¼Œä½†é¢ä¸´å¤æ‚è°ƒåº¦å’Œåå·®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºFrugalNeRFï¼Œä¸€ç§æ–°å‹å°‘æ ·æœ¬NeRFæ¡†æ¶ï¼Œé€šè¿‡è·¨å°ºåº¦æƒé‡å…±äº«ä½“ç´ é«˜æ•ˆè¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚å…¶å…³é”®è´¡çŒ®æ˜¯è·¨å°ºåº¦å‡ ä½•è‡ªé€‚åº”æ–¹æ¡ˆï¼Œæ ¹æ®è·¨å°ºåº¦çš„é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªåœ°é¢çœŸå®æ·±åº¦ï¼ŒæŒ‡å¯¼è®­ç»ƒï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒï¼Œå……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚åŒæ—¶ï¼Œå®ƒèƒ½æ•´åˆé¢„è®­ç»ƒå…ˆéªŒï¼Œæé«˜è´¨é‡è€Œä¸å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒFrugalNeRFåœ¨LLFFã€DTUå’ŒRealEstate-10Kæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–å°‘æ ·æœ¬NeRFæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ—¶é—´å¤§å¤§å‡å°‘ï¼Œä¸ºé«˜æ•ˆã€å‡†ç¡®çš„3Dåœºæ™¯é‡å»ºæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­å­˜åœ¨è¿‡æ‹Ÿåˆå’Œè®­ç»ƒæ—¶é—´é•¿çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚FreeNeRFå’ŒSparseNeRFè™½ä½¿ç”¨é¢‘ç‡æ­£åˆ™åŒ–æˆ–é¢„è®­ç»ƒå…ˆéªŒï¼Œä½†é¢ä¸´å¤æ‚è°ƒåº¦å’Œåå·®æŒ‘æˆ˜ã€‚</li>
<li>FrugalNeRFæ˜¯ä¸€ç§æ–°å‹å°‘æ ·æœ¬NeRFæ¡†æ¶ï¼Œé€šè¿‡è·¨å°ºåº¦æƒé‡å…±äº«ä½“ç´ é«˜æ•ˆè¡¨ç¤ºåœºæ™¯ç»†èŠ‚ã€‚</li>
<li>FrugalNeRFçš„è·¨å°ºåº¦å‡ ä½•è‡ªé€‚åº”æ–¹æ¡ˆæ ¹æ®é‡æŠ•å½±è¯¯å·®é€‰æ‹©ä¼ªåœ°é¢çœŸå®æ·±åº¦ï¼ŒæŒ‡å¯¼è®­ç»ƒã€‚</li>
<li>FrugalNeRFæ— éœ€ä¾èµ–å¤–éƒ¨å­¦ä¹ å…ˆéªŒï¼Œèƒ½å……åˆ†åˆ©ç”¨è®­ç»ƒæ•°æ®ã€‚</li>
<li>FrugalNeRFèƒ½æ•´åˆé¢„è®­ç»ƒå…ˆéªŒï¼Œæé«˜è´¨é‡åŒæ—¶ä¸å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-50699341b52a4a7b2529ace90b706f17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43245106be4869fc1597a67e5e0b5280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fe536df3ec9788b35f091f9817c2d15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616ab28c0df68c6bcf579767ac650c1b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EF-3DGS-Event-Aided-Free-Trajectory-3D-Gaussian-Splatting"><a href="#EF-3DGS-Event-Aided-Free-Trajectory-3D-Gaussian-Splatting" class="headerlink" title="EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting"></a>EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting</h2><p><strong>Authors:Bohao Liao, Wei Zhai, Zengyu Wan, Zhixin Cheng, Wenfei Yang, Tianzhu Zhang, Yang Cao, Zheng-Jun Zha</strong></p>
<p>Scene reconstruction from casually captured videos has wide applications in real-world scenarios. With recent advancements in differentiable rendering techniques, several methods have attempted to simultaneously optimize scene representations (NeRF or 3DGS) and camera poses. Despite recent progress, existing methods relying on traditional camera input tend to fail in high-speed (or equivalently low-frame-rate) scenarios. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event camera to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, supervising the rendered views observed by the event stream. Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise manner to extract motion information by maximizing the contrast of the Image of Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on the Linear Event Generation Model (LEGM), the brightness information encoded in the IWE is also utilized to constrain the 3DGS in the gradient domain. Third, to mitigate the absence of color information of events, we introduce photometric bundle adjustment (PBA) to ensure view consistency across events and frames. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our project page is <a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/">https://lbh666.github.io/ef-3dgs/</a>. </p>
<blockquote>
<p>åœºæ™¯é‡å»ºä»éšæ„æ•æ‰çš„è§†é¢‘åœ¨çœŸå®åœºæ™¯åº”ç”¨ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚éšç€å¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯çš„æœ€æ–°å‘å±•ï¼Œä¸€äº›æ–¹æ³•å·²ç»å°è¯•åŒæ—¶ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼ˆNeRFæˆ–3DGSï¼‰å’Œç›¸æœºå§¿æ€ã€‚å°½ç®¡æœ‰æœ€è¿‘çš„è¿›å±•ï¼Œä½†ä¾èµ–äºä¼ ç»Ÿç›¸æœºè¾“å…¥çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨é«˜é€Ÿåœºæ™¯ï¼ˆæˆ–ç­‰æ•ˆçš„ä½å¸§ç‡åœºæ™¯ï¼‰ä¸­å¤±è´¥ã€‚äº‹ä»¶ç›¸æœºï¼Œå—ç”Ÿç‰©è§†è§‰çš„å¯å‘ï¼Œä»¥é«˜æ—¶é—´åˆ†è¾¨ç‡å¼‚æ­¥è®°å½•åƒç´ çº§çš„å¼ºåº¦å˜åŒ–ï¼Œä¸ºç›²å¸§é—´éš”æä¾›äº†å®è´µçš„åœºæ™¯å’Œè¿åŠ¨ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº‹ä»¶ç›¸æœºï¼Œä»¥è¾…åŠ©ä»éšæ„æ•è·çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºï¼Œå¹¶æå‡ºäº‹ä»¶è¾…åŠ©è‡ªç”±è½¨è¿¹3DGSï¼Œç§°ä¸ºEF-3DGSï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶æ— ç¼æ•´åˆäº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿åˆ°3DGSä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰èåˆäº‹ä»¶å’Œå¸§ï¼Œç›‘ç£äº‹ä»¶æµè§‚å¯Ÿåˆ°çš„æ¸²æŸ“è§†å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»¥åˆ†æ®µçš„æ–¹å¼é‡‡ç”¨å¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–å˜å½¢äº‹ä»¶çš„å›¾åƒå¯¹æ¯”åº¦æ¥æå–è¿åŠ¨ä¿¡æ¯ï¼Œä»è€Œæ ¡å‡†ä¼°è®¡çš„å§¿æ€ã€‚æ­¤å¤–ï¼ŒåŸºäºçº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰ï¼ŒIWEä¸­ç¼–ç çš„äº®åº¦ä¿¡æ¯è¿˜ç”¨äºåœ¨æ¢¯åº¦åŸŸçº¦æŸ3DGSã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†ç¼“è§£äº‹ä»¶ç¼ºå°‘é¢œè‰²ä¿¡æ¯çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ä»¥ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±çš„å¦å…‹ä¸å¯ºåº™åŸºå‡†æµ‹è¯•ä»¥åŠæ–°æ”¶é›†çš„ç°å®ä¸–ç•Œæ•°æ®é›†RealEv-DAVISä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/%E3%80%82">https://lbh666.github.io/ef-3dgs/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15392v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/">https://lbh666.github.io/ef-3dgs/</a></p>
<p><strong>Summary</strong><br>    æœ¬æ–‡é¦–æ¬¡å¼•å…¥äº‹ä»¶ç›¸æœºè¾…åŠ©ä»éšæ„æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºã€‚æå‡ºEvent-Aided Free-Trajectory 3DGSï¼ˆEF-3DGSï¼‰ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶æ— ç¼é›†æˆäº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿åˆ°3DGSä¸­ã€‚åŒ…æ‹¬åˆ©ç”¨äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰èåˆäº‹ä»¶å’Œå¸§ä¿¡æ¯ï¼Œé‡‡ç”¨å¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰æ¡†æ¶æå–è¿åŠ¨ä¿¡æ¯ï¼Œä»¥åŠåˆ©ç”¨çº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰ä¸­çš„äº®åº¦ä¿¡æ¯çº¦æŸ3DGSã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ä»¥ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº‹ä»¶ç›¸æœºè¾…åŠ©ä»éšæ„æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºï¼Œæ‹“å®½äº†åº”ç”¨åœºæ™¯ã€‚</li>
<li>æå‡ºäº†Event-Aided Free-Trajectory 3DGSï¼ˆEF-3DGSï¼‰æ–¹æ³•ï¼Œé›†æˆäº†äº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿ã€‚</li>
<li>åˆ©ç”¨äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰èåˆäº‹ä»¶å’Œå¸§ä¿¡æ¯ï¼Œç›‘ç£äº‹ä»¶æµè§‚å¯Ÿåˆ°çš„æ¸²æŸ“è§†å›¾ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰æ¡†æ¶æå–è¿åŠ¨ä¿¡æ¯ï¼Œæ ¡å‡†ä¼°è®¡çš„å§¿æ€ã€‚</li>
<li>åˆ©ç”¨çº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰ä¸­çš„äº®åº¦ä¿¡æ¯åœ¨æ¢¯åº¦åŸŸçº¦æŸ3DGSã€‚</li>
<li>å¼•å…¥å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å…¬å…±çš„Tanks and TemplesåŸºå‡†æµ‹è¯•å’Œæ–°æ”¶é›†çš„ç°å®ä¸–ç•Œæ•°æ®é›†RealEv-DAVISä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2af6556972edb16c1143eabcff897fde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e92deb9a945c01210958b353712b66fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-353b34630c263b5197b29d8e44184510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8741e953d668eb666b6f8a9bf622117.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="IncEventGS-Pose-Free-Gaussian-Splatting-from-a-Single-Event-Camera"><a href="#IncEventGS-Pose-Free-Gaussian-Splatting-from-a-Single-Event-Camera" class="headerlink" title="IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera"></a>IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera</h2><p><strong>Authors:Jian Huang, Chengrui Dong, Xuanhua Chen, Peidong Liu</strong></p>
<p>Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for novel view synthesis have achieved remarkable progress with frame-based camera (e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel type of bio-inspired visual sensor, i.e. event camera, has demonstrated advantages in high temporal resolution, high dynamic range, low power consumption and low latency. Due to its unique asynchronous and irregular data capturing process, limited work has been proposed to apply neural representation or 3D Gaussian splatting for an event camera. In this work, we present IncEventGS, an incremental 3D Gaussian Splatting reconstruction algorithm with a single event camera. To recover the 3D scene representation incrementally, we exploit the tracking and mapping paradigm of conventional SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker firstly estimates an initial camera motion based on prior reconstructed 3D-GS scene representation. The mapper then jointly refines both the 3D scene representation and camera motion based on the previously estimated motion trajectory from the tracker. The experimental results demonstrate that IncEventGS delivers superior performance compared to prior NeRF-based methods and other related baselines, even we do not have the ground-truth camera poses. Furthermore, our method can also deliver better performance compared to state-of-the-art event visual odometry methods in terms of camera motion estimation. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/wu-cvgl/IncEventGS">https://github.com/wu-cvgl/IncEventGS</a>. </p>
<blockquote>
<p>å…³äºéšå¼ç¥ç»è¡¨ç¤ºå’Œæ˜¾å¼ä¸‰ç»´é«˜æ–¯é‡‡æ ·ï¼ˆ3D-GSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢çš„åº”ç”¨ï¼Œè¿‘æœŸåŸºäºå¸§çš„ç›¸æœºï¼ˆä¾‹å¦‚RGBå’ŒRGB-Dç›¸æœºï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç›¸å¯¹äºåŸºäºå¸§çš„ç›¸æœºï¼Œä¸€ç§æ–°å‹çš„ç”Ÿç‰©å¯å‘å‹è§†è§‰ä¼ æ„Ÿå™¨â€”â€”äº‹ä»¶ç›¸æœºï¼Œåœ¨æé«˜æ—¶é—´åˆ†è¾¨ç‡ã€é«˜åŠ¨æ€èŒƒå›´ã€ä½åŠŸè€—å’Œä½å»¶è¿Ÿæ–¹é¢å±•ç°å‡ºäº†ä¼˜åŠ¿ã€‚ç”±äºå…¶ç‹¬ç‰¹çš„å¼‚æ­¥å’Œä¸è§„åˆ™æ•°æ®æ•è·è¿‡ç¨‹ï¼Œç›®å‰å¾ˆå°‘æœ‰ç ”ç©¶æå‡ºå°†ç¥ç»è¡¨ç¤ºæˆ–ä¸‰ç»´é«˜æ–¯é‡‡æ ·åº”ç”¨äºäº‹ä»¶ç›¸æœºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†IncEventGSï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å•ä¸€äº‹ä»¶ç›¸æœºè¿›è¡Œå¢é‡ä¸‰ç»´é«˜æ–¯é‡‡æ ·é‡å»ºç®—æ³•ã€‚ä¸ºäº†å¢é‡æ¢å¤ä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼Œæˆ‘ä»¬ä¸ºIncEventGSå¼€å‘äº†ä¼ ç»Ÿçš„SLAMç®¡é“è·Ÿè¸ªå’Œæ˜ å°„èŒƒå¼ã€‚ç»™å®šä¼ å…¥çš„äº‹ä»¶æµï¼Œè·Ÿè¸ªå™¨é¦–å…ˆåŸºäºå…ˆå‰é‡å»ºçš„ä¸‰ç»´é«˜æ–¯é‡‡æ ·åœºæ™¯è¡¨ç¤ºæ¥ä¼°è®¡åˆå§‹ç›¸æœºè¿åŠ¨ã€‚ç„¶åæ˜ å°„å™¨æ ¹æ®è·Ÿè¸ªå™¨å…ˆå‰ä¼°è®¡çš„è¿åŠ¨è½¨è¿¹è”åˆä¼˜åŒ–ä¸‰ç»´åœºæ™¯è¡¨ç¤ºå’Œç›¸æœºè¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•çœŸå®ç›¸æœºå§¿æ€çš„æƒ…å†µä¸‹ï¼ŒIncEventGSç›¸è¾ƒäºå…ˆå‰çš„NeRFæ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŸºçº¿ä¹Ÿèƒ½å®ç°å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸æœºè¿åŠ¨ä¼°è®¡æ–¹é¢ä¹Ÿä¼˜äºæœ€å…ˆè¿›çš„äº‹ä»¶è§†è§‰é‡Œç¨‹è®¡æ–¹æ³•ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/wu-cvgl/IncEventGS%E3%80%82">https://github.com/wu-cvgl/IncEventGSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08107v4">PDF</a> Code Page: <a target="_blank" rel="noopener" href="https://github.com/wu-cvgl/IncEventGS">https://github.com/wu-cvgl/IncEventGS</a></p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶ç›¸æœºæ˜¯ä¸€ç§ç”Ÿç‰©å¯å‘å‹è§†è§‰ä¼ æ„Ÿå™¨ï¼Œå…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ã€é«˜åŠ¨æ€èŒƒå›´ã€ä½åŠŸè€—å’Œä½å»¶è¿Ÿç­‰ä¼˜ç‚¹ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¸§çš„ç›¸æœºç›¸æ¯”ï¼Œè¿‘æœŸç ”ç©¶è€…å¼€å§‹åœ¨æ–°å‹äº‹ä»¶ç›¸æœºä¸Šåº”ç”¨ç¥ç»è¡¨å¾å’Œä¸‰ç»´é«˜æ–¯é‡‡æ ·ï¼ˆNeRFï¼‰ã€‚æœ¬æ–‡å°†å¢é‡ä¸‰ç»´é«˜æ–¯é‡‡æ ·æŠ€æœ¯åº”ç”¨äºäº‹ä»¶ç›¸æœºï¼Œæå‡ºäº†ä¸€ç§åä¸ºIncEventGSçš„æ–°ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨è·Ÿè¸ªå’Œæ˜ å°„æœºåˆ¶æ¥æ¢å¤ä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ä¼˜åŒ–å…ˆå‰ä¼°è®¡çš„è¿åŠ¨è½¨è¿¹æ¥å®ç°ç²¾ç¡®çš„ç›¸æœºè¿åŠ¨ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ²¡æœ‰çœŸå®ç›¸æœºå§¿æ€ï¼ŒIncEventGSç›¸æ¯”ä¹‹å‰çš„NeRFæ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŸºçº¿æ€§èƒ½è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚ä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»æ–‡ä¸­æå–çš„å…³é”®è¦ç‚¹ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f6601abbee96ac3743c0f19c493a2dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0db054e979f3135464867d0d59fb5a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e195a9f091a9aca1646efffa22fa730.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3ba9c1323f6e44da60a300846902f26f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  PartRM Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-952d00c55fe995e0feab67eb01a3ecc7.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  PartRM Modeling Part-Level Dynamics with Large Cross-State   Reconstruction Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25370.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
