<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  CoLLM A Large Language Model for Composed Image Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1fd0df86edf04907f3a27458febd4eb5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="CoLLM-A-Large-Language-Model-for-Composed-Image-Retrieval"><a href="#CoLLM-A-Large-Language-Model-for-Composed-Image-Retrieval" class="headerlink" title="CoLLM: A Large Language Model for Composed Image Retrieval"></a>CoLLM: A Large Language Model for Composed Image Retrieval</h2><p><strong>Authors:Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul Chilimbi, Abhinav Shrivastava</strong></p>
<p>Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field. </p>
<blockquote>
<p>å›¾åƒç»„åˆæ£€ç´¢ï¼ˆCIRï¼‰æ˜¯ä¸€é¡¹æ—¨åœ¨åŸºäºå¤šæ¨¡æ€æŸ¥è¯¢æ£€ç´¢å›¾åƒçš„å¤æ‚ä»»åŠ¡ã€‚å…¸å‹è®­ç»ƒæ•°æ®åŒ…å«ç”±å‚è€ƒå›¾åƒã€æ‰€éœ€ä¿®æ”¹çš„æ–‡æœ¬æè¿°å’Œç›®æ ‡å›¾åƒç»„æˆçš„ä¸‰å…ƒç»„ï¼Œè¿™äº›ä¸‰å…ƒç»„çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—è´¹æ—¶é—´ã€‚CIRæ•°æ®é›†çš„ç¨€ç¼ºå¯¼è‡´é‡‡ç”¨åˆæˆä¸‰å…ƒç»„æˆ–åˆ©ç”¨ç½‘ç»œçˆ¬å–çš„å›¾åƒ-å­—å¹•å¯¹æ„å»ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é›¶æ ·æœ¬æ–¹æ³•å‡ºç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šåˆæˆä¸‰å…ƒç»„å—é™äºè§„æ¨¡ã€ç¼ºä¹å¤šæ ·æ€§ä»¥åŠä¸è‡ªç„¶çš„ä¿®æ”¹æ–‡æœ¬ï¼Œè€Œå›¾åƒå­—å¹•å¯¹åˆ™å› ç¼ºä¹ä¸‰å…ƒç»„æ•°æ®è€Œé˜»ç¢å¤šæ¨¡æ€æŸ¥è¯¢çš„è”åˆåµŒå…¥å­¦ä¹ ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä¸”å¾®å¦™çš„ä¿®æ”¹æ–‡æœ¬æ—¶é¢ä¸´å›°éš¾ï¼Œè¿™äº›æ–‡æœ¬éœ€è¦é«˜åº¦èåˆå’Œç†è§£è§†è§‰å’Œè¯­è¨€æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†CoLLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€ç«™å¼æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å³æ—¶ä»å›¾åƒå­—å¹•å¯¹ä¸­ç”Ÿæˆä¸‰å…ƒç»„ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šå³å¯è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„è”åˆåµŒå…¥ï¼Œä¿ƒè¿›æ›´æ·±å±‚çš„å¤šæ¨¡æ€èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«340ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†Multi-Text CIRï¼ˆMTCIRï¼‰ï¼Œå¹¶å¯¹ç°æœ‰çš„CIRåŸºå‡†æµ‹è¯•é›†ï¼ˆCIRRå’ŒFashion-IQï¼‰è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜è¯„ä¼°å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLLMåœ¨å¤šä¸ªCIRåŸºå‡†æµ‹è¯•é›†å’Œè®¾ç½®ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚MTCIRè¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾15%ã€‚æˆ‘ä»¬æ”¹è¿›çš„åŸºå‡†æµ‹è¯•é›†ä¸ºCIRæ¨¡å‹æä¾›äº†æ›´å¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºè¿™ä¸€é‡è¦é¢†åŸŸçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19910v1">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://collm-cvpr25.github.io/">https://collm-cvpr25.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼ŒCoLLMæ¡†æ¶é€šè¿‡å³æ—¶ç”Ÿæˆå›¾åƒ-æè¿°æ–‡æœ¬å¯¹ä¸­çš„ä¸‰å…ƒç»„ï¼Œè§£å†³äº†å¤æ‚å›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„å…±åŒåµŒå…¥ï¼Œä¿ƒè¿›äº†æ›´æ·±å±‚æ¬¡çš„å¤šåª’ä½“èåˆã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®é›†Multi-Text CIR (MTCIR)ï¼ŒåŒ…å«340ä¸‡æ ·æœ¬ï¼Œæé«˜äº†ç°æœ‰CIRåŸºå‡†æµ‹è¯•çš„è¯„ä¼°å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLLMåœ¨å¤šä¸ªCIRåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€ŒMTCIRçš„ç«äº‰åŠ›ä¹Ÿå¾—åˆ°äº†æå‡ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾15%ã€‚æˆ‘ä»¬æ”¹è¿›çš„åŸºå‡†æµ‹è¯•ä¸ºCIRæ¨¡å‹æä¾›äº†æ›´å¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºè¿™ä¸€é‡è¦é¢†åŸŸçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CoLLMæ¡†æ¶è§£å†³äº†CIRä»»åŠ¡ä¸­è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé€šè¿‡å³æ—¶ç”Ÿæˆå›¾åƒ-æè¿°æ–‡æœ¬å¯¹ä¸­çš„ä¸‰å…ƒç»„ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>CoLLMåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„å…±åŒåµŒå…¥ï¼Œä¿ƒè¿›äº†å¤šåª’ä½“èåˆçš„æ·±åº¦è¿›è¡Œã€‚</li>
<li>å¼•å…¥äº†æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†MTCIRï¼ŒåŒ…å«340ä¸‡æ ·æœ¬ï¼Œç”¨äºæé«˜CIRä»»åŠ¡çš„è®­ç»ƒæ•ˆæœã€‚</li>
<li>CoLLMåœ¨å¤šä¸ªCIRåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>MTCIRæ•°æ®é›†çš„åº”ç”¨æé«˜äº†CIRæ¨¡å‹çš„æ€§èƒ½ï¼Œæœ€å¤šå¯æé«˜15%ã€‚</li>
<li>æ”¹è¿›çš„åŸºå‡†æµ‹è¯•ä¸ºCIRæ¨¡å‹æä¾›äº†æ›´å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>CoLLMå’ŒMTCIRçš„è´¡çŒ®æ¨åŠ¨äº†CIRé¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48cd0cc26d6fc4445bff83dc1c8de49e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-936d2c8c463427f28e279c64923f8278.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a1ae94052e0512bdff46a4c9caab13b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae5704600e7713e7d8623dac8af46a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937dec835e9b6757355b1908f9df26df.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Multi-Agent-Framework-Integrating-Large-Language-Models-and-Generative-AI-for-Accelerated-Metamaterial-Design"><a href="#A-Multi-Agent-Framework-Integrating-Large-Language-Models-and-Generative-AI-for-Accelerated-Metamaterial-Design" class="headerlink" title="A Multi-Agent Framework Integrating Large Language Models and Generative   AI for Accelerated Metamaterial Design"></a>A Multi-Agent Framework Integrating Large Language Models and Generative   AI for Accelerated Metamaterial Design</h2><p><strong>Authors:Jie Tian, Martin Taylor Sobczak, Dhanush Patil, Jixin Hou, Lin Pang, Arunachalam Ramanathan, Libin Yang, Xianyan Chen, Yuval Golan, Hongyue Sun, Kenan Song, Xianqiao Wang</strong></p>
<p>Metamaterials, renowned for their exceptional mechanical, electromagnetic, and thermal properties, hold transformative potential across diverse applications, yet their design remains constrained by labor-intensive trial-and-error methods and limited data interoperability. Here, we introduce CrossMatAgentâ€“a novel multi-agent framework that synergistically integrates large language models with state-of-the-art generative AI to revolutionize metamaterial design. By orchestrating a hierarchical team of agentsâ€“each specializing in tasks such as pattern analysis, architectural synthesis, prompt engineering, and supervisory feedbackâ€“our system leverages the multimodal reasoning of GPT-4o alongside the generative precision of DALL-E 3 and a fine-tuned Stable Diffusion XL model. This integrated approach automates data augmentation, enhances design fidelity, and produces simulation- and 3D printing-ready metamaterial patterns. Comprehensive evaluations, including CLIP-based alignment, SHAP interpretability analyses, and mechanical simulations under varied load conditions, demonstrate the frameworkâ€™s ability to generate diverse, reproducible, and application-ready designs. CrossMatAgent thus establishes a scalable, AI-driven paradigm that bridges the gap between conceptual innovation and practical realization, paving the way for accelerated metamaterial development. </p>
<blockquote>
<p>è¶…ææ–™ä»¥å…¶å‡ºè‰²çš„æœºæ¢°ã€ç”µç£å’Œçƒ­æ€§èƒ½è€Œé—»åï¼Œåœ¨ä¸åŒåº”ç”¨ä¸­å…·æœ‰å˜é©æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡ä»å—åˆ°åŠ³åŠ¨å¯†é›†å‹çš„è¯•é”™æ–¹æ³•å’Œæœ‰é™æ•°æ®äº’æ“ä½œæ€§çš„é™åˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†CrossMatAgentâ€”â€”ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æœ€æ–°ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œä»¥é©å‘½æ€§æ–¹å¼æ¨åŠ¨è¶…ææ–™è®¾è®¡ã€‚é€šè¿‡åè°ƒåˆ†å±‚æ™ºèƒ½ä½“å›¢é˜Ÿâ€”â€”æ¯ä¸ªæ™ºèƒ½ä½“ä¸“é—¨è´Ÿè´£æ¨¡å¼åˆ†æã€æ¶æ„ç»¼åˆã€æç¤ºå·¥ç¨‹å’Œç›‘ç£åé¦ˆç­‰ä»»åŠ¡â€”â€”æˆ‘ä»¬çš„ç³»ç»Ÿåˆ©ç”¨GPT-4oçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œç»“åˆDALL-E 3çš„ç”Ÿæˆç²¾åº¦å’Œå¾®è°ƒåçš„Stable Diffusion XLæ¨¡å‹ã€‚è¿™ç§é›†æˆæ–¹æ³•è‡ªåŠ¨æ‰§è¡Œæ•°æ®å¢å¼ºï¼Œæé«˜è®¾è®¡ä¿çœŸåº¦ï¼Œå¹¶ç”Ÿæˆå¯ç”¨äºæ¨¡æ‹Ÿå’Œ3Dæ‰“å°çš„è¶…ææ–™å›¾æ¡ˆã€‚å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŸºäºCLIPçš„å¯¹é½ã€SHAPè§£é‡Šæ€§åˆ†æå’Œå„ç§è´Ÿè½½æ¡ä»¶ä¸‹çš„æœºæ¢°æ¨¡æ‹Ÿï¼Œè¯æ˜äº†è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€å¯é‡å¤å’Œé€‚ç”¨äºåº”ç”¨çš„è®¾è®¡ã€‚å› æ­¤ï¼ŒCrossMatAgentå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ¨¡å¼ï¼Œå¼¥åˆäº†æ¦‚å¿µåˆ›æ–°ä¸å®è·µå®ç°ä¹‹é—´çš„å·®è·ï¼Œä¸ºåŠ é€Ÿè¶…ææ–™å¼€å‘é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19889v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºCrossMatAgentçš„ä»‹ç»ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æœ€æ–°ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é›†æˆçš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºé©å‘½æ€§è®¾è®¡å˜ææ–™ã€‚é€šè¿‡ååŒä¸åŒæ™ºèƒ½ä½“ï¼Œåˆ©ç”¨GPT-4oçš„å¤šæ¨¡å¼æ¨ç†ã€DALL-E 3çš„ç”Ÿæˆç²¾åº¦å’Œå¾®è°ƒåçš„Stable Diffusion XLæ¨¡å‹ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ•°æ®å¢å¼ºï¼Œæé«˜è®¾è®¡ä¿çœŸåº¦ï¼Œå¹¶äº§ç”Ÿå¯ç”¨äºæ¨¡æ‹Ÿå’Œ3Dæ‰“å°çš„å˜ææ–™å›¾æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrossMatAgentæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè®¾è®¡å˜ææ–™ã€‚</li>
<li>å®ƒé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæœ€æ–°çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚</li>
<li>é€šè¿‡ååŒä¸åŒæ™ºèƒ½ä½“ï¼ŒCrossMatAgentèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ•°æ®å¢å¼ºï¼Œæé«˜è®¾è®¡è´¨é‡ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡åˆ©ç”¨GPT-4oã€DALL-E 3å’ŒStable Diffusion XLæ¨¡å‹ï¼Œäº§ç”Ÿé«˜è´¨é‡çš„è®¾è®¡ã€‚</li>
<li>CrossMatAgentçš„è®¾è®¡è¿‡ç¨‹åŒ…æ‹¬æ¨¡å¼åˆ†æã€æ¶æ„åˆæˆã€æç¤ºå·¥ç¨‹å’Œåé¦ˆç›‘ç£ã€‚</li>
<li>ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒCrossMatAgentèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€å¯é‡å¤å’Œé€‚ç”¨äºå®é™…åº”ç”¨çš„è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3cc487016c76d545640673386d01f1c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CausalRAG-Integrating-Causal-Graphs-into-Retrieval-Augmented-Generation"><a href="#CausalRAG-Integrating-Causal-Graphs-into-Retrieval-Augmented-Generation" class="headerlink" title="CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation"></a>CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation</h2><p><strong>Authors:Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¢å¼ºå‹æ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†æå‡äº†LLMçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RAGç³»ç»Ÿé¢ä¸´é‡å¤§å±€é™ï¼ŒåŒ…æ‹¬å› æ–‡æœ¬åˆ†å—è€Œå¯¼è‡´ä¸Šä¸‹æ–‡å®Œæ•´æ€§è¢«ç ´åï¼Œä»¥åŠè¿‡äºä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CausalRAGè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å› æœå›¾èå…¥åˆ°æ£€ç´¢è¿‡ç¨‹ä¸­ã€‚é€šè¿‡æ„å»ºå’Œè¿½è¸ªå› æœå…³ç³»ï¼ŒCausalRAGä¿ç•™äº†ä¸Šä¸‹æ–‡çš„è¿ç»­æ€§å¹¶æé«˜äº†æ£€ç´¢ç²¾åº¦ï¼Œä»è€Œç”Ÿæˆäº†æ›´å‡†ç¡®ã€æ›´å¯è§£é‡Šçš„å›ç­”ã€‚æˆ‘ä»¬å°†CausalRAGä¸å¸¸è§„RAGå’Œå›¾åŸºRAGæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°å¯¹æ¯”ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå±•ç°äº†å…¶ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¥å› æœæ¨ç†ä¸ºåŸºç¡€çš„çŸ¥è¯†æ£€ç´¢æ–¹æ³•ä¸ºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æä¾›äº†å¯Œæœ‰å‰æ™¯çš„è§£å†³é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡èåˆå¤–éƒ¨çŸ¥è¯†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é©æ–°äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRAGç³»ç»Ÿå­˜åœ¨å…³é”®å±€é™ï¼Œå¦‚å› æ–‡æœ¬åˆ†å—é€ æˆçš„è¯­å¢ƒå®Œæ•´æ€§ç ´åï¼Œä»¥åŠè¿‡äºä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CausalRAGæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å› æœå›¾èå…¥æ£€ç´¢è¿‡ç¨‹ã€‚é€šè¿‡æ„å»ºå’Œè¿½è¸ªå› æœå…³ç³»ï¼ŒCausalRAGä¿æŒäº†è¯­å¢ƒè¿è´¯æ€§ï¼Œæé«˜äº†æ£€ç´¢ç²¾åº¦ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šæ€§å¼ºçš„å›åº”ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸è¾ƒäºå¸¸è§„RAGå’ŒåŸºäºå›¾çš„RAGæ–¹æ³•ï¼ŒCausalRAGåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°å“è¶Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†æ£€ç´¢æ ¹æ¤äºå› æœæ¨ç†ä¸ºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æä¾›äº†é¢‡å…·å‰æ™¯çš„è§£å†³é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡èåˆå¤–éƒ¨çŸ¥è¯†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æå‡äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»ŸRAGç³»ç»Ÿå­˜åœ¨è¯­å¢ƒå®Œæ•´æ€§ç ´åå’Œè¿‡åº¦ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢çš„é—®é¢˜ã€‚</li>
<li>CausalRAGæ¡†æ¶é€šè¿‡èå…¥å› æœå›¾æŠ€æœ¯æ¥è§£å†³ä¼ ç»ŸRAGç³»ç»Ÿçš„å±€é™ã€‚</li>
<li>CausalRAGèƒ½å¤Ÿä¿æŒè¯­å¢ƒè¿è´¯æ€§ï¼Œæé«˜æ£€ç´¢ç²¾åº¦ã€‚</li>
<li>CausalRAGç”Ÿæˆçš„å›åº”æ›´å‡†ç¡®ä¸”å…·å¤‡æ›´å¼ºçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç›¸è¾ƒäºå¸¸è§„RAGå’ŒåŸºäºå›¾çš„RAGæ–¹æ³•ï¼ŒCausalRAGåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af5a891f3e37544c7992c2787cf3ddc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcd5d3a671fd784b61411836f58c529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a25e1ed47b39ae10b1a390dea42c96.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SLA-Awareness-for-AI-assisted-coding"><a href="#SLA-Awareness-for-AI-assisted-coding" class="headerlink" title="SLA-Awareness for AI-assisted coding"></a>SLA-Awareness for AI-assisted coding</h2><p><strong>Authors:Kishanthan Thangarajah, Arthur Leung, Boyuan Chen, Ahmed E. Hassan</strong></p>
<p>The integration of AI-assisted coding tools within development environments drastically reduces development time, and allows developers to focus more on creative and critical aspects of software engineering through the use of Code Large Language Models (CodeLLMs). These coding assistants automate repetitive and time-consuming coding tasks such as code generation, code completion, code summarization, and code translation. Responsiveness is a crucial requirement of these coding assistants to maintain real-time interactivity, such that their use does not impede the developersâ€™ workflows. Different coding tasks have unique characteristics and latency requirements: Time-To-First-Token (TTFT) latency is essential for code completion tasks, while End-To-End (E2E) latency is crucial for code translation tasks. Managing these varying requirements simultaneously while optimizing resource usage poses significant challenges. Existing work adopts the Model-as-a-Service paradigm for serving individual CodeLLMs, but cannot effectively manage latency requirements of concurrent coding tasks and sequences of CodeLLM inference calls, due to a lack of end-to-end latency awareness. Another challenge is keeping resource utilization high, when the serving system is deployed on a shared cluster environment. To address these challenges, we propose Coding Assistant Task Orchestrator (CATO), a runtime system designed to serve a diverse assortment of coding tasks while meeting latency requirements and maximizing resource utilization. Our experiments demonstrate that when all types of coding tasks were served simultaneously, for TTFT-critical tasks, CATO improves overall Goodput rate and resource utilization by up to 10% and 41.1%, respectively. P95 E2E latency was also reduced by 18% for code summarization tasks, and P95 TTFT for code generation tasks were reduced by 14% compared against state-of-the-art systems. </p>
<blockquote>
<p>å°†AIè¾…åŠ©ç¼–ç å·¥å…·é›†æˆåˆ°å¼€å‘ç¯å¢ƒä¸­ï¼Œèƒ½å¤§å¹…åº¦ç¼©çŸ­å¼€å‘æ—¶é—´ï¼Œå¹¶å…è®¸å¼€å‘è€…é€šè¿‡åˆ©ç”¨ä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMï¼‰æ¥æ›´å¤šåœ°å…³æ³¨è½¯ä»¶å·¥ç¨‹çš„åˆ›æ–°å’Œå…³é”®æ–¹é¢ã€‚è¿™äº›ç¼–ç åŠ©æ‰‹èƒ½è‡ªåŠ¨åŒ–é‡å¤ä¸”è€—æ—¶çš„ç¼–ç ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆã€ä»£ç è¡¥å…¨ã€ä»£ç æ‘˜è¦å’Œä»£ç ç¿»è¯‘ã€‚å“åº”æ€§æ˜¯è¿™äº›ç¼–ç åŠ©æ‰‹ç»´æŒå®æ—¶äº¤äº’çš„å…³é”®è¦æ±‚ï¼Œå®ƒä»¬çš„ä½¿ç”¨ä¸ä¼šé˜»ç¢å¼€å‘è€…çš„å·¥ä½œæµç¨‹ã€‚ä¸åŒçš„ç¼–ç ä»»åŠ¡å…·æœ‰ä¸åŒçš„ç‰¹æ€§å’Œå»¶è¿Ÿè¦æ±‚ï¼šå¯¹äºä»£ç è¡¥å…¨ä»»åŠ¡ï¼Œé¦–æ¬¡å“åº”æ—¶é—´ï¼ˆTTFTï¼‰å»¶è¿Ÿè‡³å…³é‡è¦ï¼Œè€Œå¯¹äºä»£ç ç¿»è¯‘ä»»åŠ¡ï¼Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰å»¶è¿Ÿååˆ†å…³é”®ã€‚åœ¨åŒæ—¶ç®¡ç†è¿™äº›ä¸åŒçš„è¦æ±‚å¹¶ä¼˜åŒ–èµ„æºä½¿ç”¨æ—¶ä¼šé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰å·¥ä½œé‡‡ç”¨æ¨¡å‹å³æœåŠ¡ï¼ˆModel-as-a-Serviceï¼‰èŒƒå¼æ¥æœåŠ¡å•ä¸ªCodeLLMï¼Œä½†ç”±äºç¼ºä¹ç«¯åˆ°ç«¯å»¶è¿Ÿæ„è¯†ï¼Œæ— æ³•æœ‰æ•ˆåœ°ç®¡ç†å¹¶å‘ç¼–ç ä»»åŠ¡çš„å»¶è¿Ÿè¦æ±‚å’ŒCodeLLMæ¨ç†è°ƒç”¨çš„åºåˆ—ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯åœ¨å…±äº«é›†ç¾¤ç¯å¢ƒä¸­éƒ¨ç½²æœåŠ¡ç³»ç»Ÿæ—¶å¦‚ä½•ä¿æŒè¾ƒé«˜çš„èµ„æºåˆ©ç”¨ç‡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¼–ç åŠ©æ‰‹ä»»åŠ¡ç¼–æ’å™¨ï¼ˆCATOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿è¡Œæ—¶ç³»ç»Ÿï¼Œæ—¨åœ¨æœåŠ¡å„ç§ç¼–ç ä»»åŠ¡ï¼ŒåŒæ—¶æ»¡è¶³å»¶è¿Ÿè¦æ±‚å¹¶æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“åŒæ—¶æœåŠ¡æ‰€æœ‰ç±»å‹çš„ç¼–ç ä»»åŠ¡æ—¶ï¼Œå¯¹äºTTFTå…³é”®ä»»åŠ¡ï¼ŒCATOçš„æ€»ä½“ååé‡æå‡ç‡å’Œèµ„æºåˆ©ç”¨ç‡åˆ†åˆ«æé«˜äº†é«˜è¾¾10%å’Œ41.1%ã€‚æ­¤å¤–ï¼Œä»£ç æ‘˜è¦ä»»åŠ¡çš„P95 E2Eå»¶è¿Ÿå‡å°‘äº†18%ï¼Œä»£ç ç”Ÿæˆä»»åŠ¡çš„P95 TTFTå‡å°‘äº†14%ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›ç³»ç»Ÿæœ‰æ‰€ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19876v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AIè¾…åŠ©ç¼–ç å·¥å…·é›†æˆå¼€å‘ç¯å¢ƒï¼Œå¤§å¹…å‡å°‘å¼€å‘æ—¶é—´ï¼Œè®©å¼€å‘è€…ä¸“æ³¨äºè½¯ä»¶å·¥ç¨‹ä¸­çš„åˆ›é€ æ€§ä¸å…³é”®æ–¹é¢ã€‚ç¼–ç åŠ©æ‰‹è‡ªåŠ¨åŒ–é‡å¤æ€§é«˜è€—æ—¶çš„ç¼–ç ä»»åŠ¡ï¼Œå¦‚ä»£ç ç”Ÿæˆã€ä»£ç è¡¥å…¨ç­‰ã€‚å¯¹äºä¸åŒçš„ç¼–ç ä»»åŠ¡æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹æ€§ä¸å»¶è¿Ÿè¦æ±‚ï¼Œéœ€ç»´æŒå®æ—¶äº’åŠ¨ä»¥æ”¹å–„å·¥ä½œæµç¨‹ã€‚ç°æœ‰çš„Model-as-a-Serviceæ¨¡å¼æ— æ³•æœ‰æ•ˆç®¡ç†å¹¶å‘ä»»åŠ¡çš„å»¶è¿Ÿè¦æ±‚ä¸èµ„æºä¼˜åŒ–é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºCoding Assistant Task Orchestrator (CATO)ï¼Œå¯åœ¨æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„åŒæ—¶æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡å¹¶æœåŠ¡äºå„ç§ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºç°æœ‰çš„é¡¶å°–ç³»ç»Ÿï¼ŒCATOå¯¹æ‰€æœ‰ç±»å‹çš„ç¼–ç ä»»åŠ¡æ•´ä½“è¡¨ç°æ›´ä½³ã€‚é’ˆå¯¹å»¶è¿Ÿéœ€æ±‚çš„ä¼˜åŒ–åŠæ”¹å–„èµ„æºçš„åˆ©ç”¨æ•ˆç‡ç‰¹åˆ«æ˜¾è‘—ã€‚ç®€å•æ¥è¯´ï¼ŒCATOæé«˜äº†å¼€å‘æ•ˆç‡å¹¶ä¼˜åŒ–äº†èµ„æºåˆ†é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIè¾…åŠ©ç¼–ç å·¥å…·ç¼©çŸ­äº†å¼€å‘æ—¶é—´ï¼Œä½¿å¼€å‘è€…ä¸“æ³¨äºåˆ›é€ æ€§ä¸å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç¼–ç åŠ©æ‰‹è‡ªåŠ¨åŒ–é‡å¤æ€§é«˜è€—æ—¶çš„ç¼–ç ä»»åŠ¡ã€‚</li>
<li>ä¸åŒçš„ç¼–ç ä»»åŠ¡æœ‰ç‰¹å®šçš„å»¶è¿Ÿè¦æ±‚ã€‚ä¸ºæ»¡è¶³å®æ—¶äº’åŠ¨å’Œå·¥ä½œæµç¨‹çš„éœ€æ±‚ï¼Œåº”æœ‰æ•ˆåœ°ç®¡ç†è¿™äº›è¦æ±‚ã€‚</li>
<li>Model-as-a-Serviceæ¨¡å¼æ— æ³•æœ‰æ•ˆç®¡ç†å¹¶å‘ä»»åŠ¡çš„å»¶è¿Ÿå’Œèµ„æºä¼˜åŒ–æŒ‘æˆ˜ã€‚</li>
<li>Coding Assistant Task Orchestrator (CATO)æ—¨åœ¨æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„åŒæ—¶æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ï¼Œå¹¶å¤„ç†å¤šç§ç¼–ç ä»»åŠ¡ã€‚</li>
<li>CATOç›¸è¾ƒäºç°æœ‰ç³»ç»Ÿè¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨æ”¹å–„å»¶è¿Ÿéœ€æ±‚å’Œèµ„æºåˆ©ç”¨æ•ˆç‡æ–¹é¢ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒCATOæé«˜äº†å¼€å‘æ•ˆç‡å¹¶ä¼˜åŒ–äº†èµ„æºåˆ†é…ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9897f9bd0b80ed7cf2da723c7c75bee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75646a1bf5a60afcb1ff267d72ccbe2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc20763bada9d9396a2a2eefc312560d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-524d65635c78f481bbd20ed2db654520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9a6f39c9f43e39177a1a4a94b410e1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b97dff1aca12ff7eeb73506419f756.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Online-Multi-Modal-Social-Interaction-Understanding"><a href="#Towards-Online-Multi-Modal-Social-Interaction-Understanding" class="headerlink" title="Towards Online Multi-Modal Social Interaction Understanding"></a>Towards Online Multi-Modal Social Interaction Understanding</h2><p><strong>Authors:Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, Yapeng Tian</strong></p>
<p>Multimodal social interaction understanding (MMSI) is critical in human-robot interaction systems. In real-world scenarios, AI agents are required to provide real-time feedback. However, existing models often depend on both past and future contexts, which hinders them from applying to real-world problems. To bridge this gap, we propose an online MMSI setting, where the model must resolve MMSI tasks using only historical information, such as recorded dialogues and video streams. To address the challenges of missing the useful future context, we develop a novel framework, named Online-MMSI-VLM, that leverages two complementary strategies: multi-party conversation forecasting and social-aware visual prompting with multi-modal large language models. First, to enrich linguistic context, the multi-party conversation forecasting simulates potential future utterances in a coarse-to-fine manner, anticipating upcoming speaker turns and then generating fine-grained conversational details. Second, to effectively incorporate visual social cues like gaze and gesture, social-aware visual prompting highlights the social dynamics in video with bounding boxes and body keypoints for each person and frame. Extensive experiments on three tasks and two datasets demonstrate that our method achieves state-of-the-art performance and significantly outperforms baseline models, indicating its effectiveness on Online-MMSI. The code and pre-trained models will be publicly released at: <a target="_blank" rel="noopener" href="https://github.com/Sampson-Lee/OnlineMMSI">https://github.com/Sampson-Lee/OnlineMMSI</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç¤¾ä¼šäº¤äº’ç†è§£ï¼ˆMMSIï¼‰åœ¨äººæœºäº’åŠ¨ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œäººå·¥æ™ºèƒ½ä»£ç†éœ€è¦æä¾›å®æ—¶åé¦ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–äºè¿‡å»å’Œæœªæ¥çš„ä¸Šä¸‹æ–‡ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œé—®é¢˜ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿MMSIè®¾ç½®ï¼Œæ¨¡å‹å¿…é¡»ä»…ä½¿ç”¨å†å²ä¿¡æ¯è§£å†³MMSIä»»åŠ¡ï¼Œä¾‹å¦‚å½•åˆ¶çš„å¯¹è¯å’Œè§†é¢‘æµã€‚ä¸ºäº†è§£å†³ç¼ºå°‘æœ‰ç”¨æœªæ¥ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºOnline-MMSI-VLMçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šå¤šæ–¹å¯¹è¯é¢„æµ‹å’Œç¤¾ä¼šæ„ŸçŸ¥è§†è§‰æç¤ºä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†ä¸°å¯Œè¯­è¨€ç¯å¢ƒï¼Œå¤šæ–¹å¯¹è¯é¢„æµ‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„æ–¹å¼æ¨¡æ‹Ÿæ½œåœ¨çš„æœªæ¥è¯è¯­ï¼Œé¢„æµ‹å³å°†åˆ°æ¥çš„å‘è¨€è€…é¡ºåºï¼Œç„¶åç”Ÿæˆç»†è‡´çš„å¯¹è¯ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æœ‰æ•ˆåœ°ç»“åˆè¯¸å¦‚ç›®å…‰å’Œæ‰‹åŠ¿ä¹‹ç±»çš„è§†è§‰ç¤¾ä¼šçº¿ç´¢ï¼Œç¤¾ä¼šæ„ŸçŸ¥è§†è§‰æç¤ºé€šè¿‡è¾¹ç•Œæ¡†å’Œæ¯ä¸ªäººçš„èº«ä½“å…³é”®ç‚¹çªå‡ºè§†é¢‘ä¸­ç¤¾äº¤åŠ¨æ€ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨åœ¨çº¿MMSIä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sampson-Lee/OnlineMMSI">https://github.com/Sampson-Lee/OnlineMMSI</a>å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19851v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“ç¤¾äº¤äº¤äº’ç†è§£ï¼ˆMMSIï¼‰å¯¹äºäººæœºäº¤äº’ç³»ç»Ÿè‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å®æ—¶åé¦ˆæ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨çº¿MMSIè®¾ç½®ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åä¸ºOnline-MMSI-VLMçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ½œåœ¨æœªæ¥è¨€è®ºå’Œæ•´åˆè§†è§‰ç¤¾äº¤çº¿ç´¢ï¼Œä»¥æé«˜æ¨¡å‹çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿MMSIè®¾ç½®åœ¨å¤„ç†å®æ—¶åé¦ˆçš„äººæœºäº¤äº’ä¸­éå¸¸é‡è¦ã€‚ç°æœ‰æ¨¡å‹ä¾èµ–è¿‡å»å’Œæœªæ¥ä¸Šä¸‹æ–‡ï¼Œä¸é€‚åˆè§£å†³å®é™…é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿MMSIæ¡†æ¶ï¼ˆOnline-MMSI-VLMï¼‰ã€‚</li>
<li>Online-MMSI-VLMåŒ…å«ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šå¤šå‚ä¸æ–¹å¯¹è¯é¢„æµ‹å’Œç¤¾äº¤æ„ŸçŸ¥è§†è§‰æç¤ºã€‚å¤šå‚ä¸æ–¹å¯¹è¯é¢„æµ‹ä¸°å¯Œäº†è¯­è¨€ä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹Ÿæ½œåœ¨æœªæ¥è¨€è®ºã€‚ç¤¾äº¤æ„ŸçŸ¥è§†è§‰æç¤ºåˆ™æ•´åˆäº†è§†è§‰ç¤¾äº¤çº¿ç´¢ï¼Œå¦‚ç›®å…‰å’Œæ‰‹åŠ¿ã€‚</li>
<li>åœ¨çº¿MMSIæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿæ½œåœ¨æœªæ¥è¨€è®ºå’Œæ•´åˆè§†è§‰ç¤¾äº¤çº¿ç´¢æ¥æé«˜æ¨¡å‹çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚è¿™æœ‰åŠ©äºå®ç°æ›´åŠ è‡ªç„¶å’Œé«˜æ•ˆçš„äººæœºäº¤äº’ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOnline-MMSI-VLMåœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶åœ¨å¤„ç†å¤šåª’ä½“ç¤¾äº¤äº¤äº’æ–¹é¢å…·æœ‰æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c552d185b3d3b97bf3982a5275304bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66a4d33b06b21622f437ab362be3aeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16bdadd689334a926b165ae9015c8ff4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf45bce7aba9b899e4f8a352e74252e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FALCONEye-Finding-Answers-and-Localizing-Content-in-ONE-hour-long-videos-with-multi-modal-LLMs"><a href="#FALCONEye-Finding-Answers-and-Localizing-Content-in-ONE-hour-long-videos-with-multi-modal-LLMs" class="headerlink" title="FALCONEye: Finding Answers and Localizing Content in ONE-hour-long   videos with multi-modal LLMs"></a>FALCONEye: Finding Answers and Localizing Content in ONE-hour-long   videos with multi-modal LLMs</h2><p><strong>Authors:Carlos Plou, Cesar Borja, Ruben Martinez-Cantin, Ana C. Murillo</strong></p>
<p>Information retrieval in hour-long videos presents a significant challenge, even for state-of-the-art Vision-Language Models (VLMs), particularly when the desired information is localized within a small subset of frames. Long video data presents challenges for VLMs due to context window limitations and the difficulty of pinpointing frames containing the answer. Our novel video agent, FALCONEye, combines a VLM and a Large Language Model (LLM) to search relevant information along the video, and locate the frames with the answer. FALCONEye novelty relies on 1) the proposed meta-architecture, which is better suited to tackle hour-long videos compared to short video approaches in the state-of-the-art; 2) a new efficient exploration algorithm to locate the information using short clips, captions and answer confidence; and 3) our state-of-the-art VLMs calibration analysis for the answer confidence. Our agent is built over a small-size VLM and a medium-size LLM being accessible to run on standard computational resources. We also release FALCON-Bench, a benchmark to evaluate long (average &gt; 1 hour) Video Answer Search challenges, highlighting the need for open-ended question evaluation. Our experiments show FALCONEyeâ€™s superior performance than the state-of-the-art in FALCON-Bench, and similar or better performance in related benchmarks. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢åœ¨é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ä¸­æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œç‰¹åˆ«æ˜¯å½“æ‰€éœ€ä¿¡æ¯ä½äºä¸€å°éƒ¨åˆ†å¸§å†…æ—¶ã€‚é•¿è§†é¢‘æ•°æ®ä¸ºVLMå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶å’Œå®šä½å«æœ‰ç­”æ¡ˆçš„å¸§çš„éš¾åº¦ã€‚æˆ‘ä»¬çš„æ–°å‹è§†é¢‘ä»£ç†FALCONEyeç»“åˆäº†VLMså’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥åœ¨è§†é¢‘ä¸­è¿›è¡Œç›¸å…³ä¿¡æ¯çš„æœç´¢å¹¶å®šä½å«æœ‰ç­”æ¡ˆçš„å¸§ã€‚FALCONEyeçš„æ–°é¢–æ€§åœ¨äºå…¶ï¼š1ï¼‰æ‰€æå‡ºçš„å…ƒæ¶æ„ï¼Œä¸ä¼ ç»Ÿçš„çŸ­è§†é¢‘å¤„ç†æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ›´é€‚åˆå¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼›2ï¼‰ä¸€ç§ç”¨äºå®šä½ä¿¡æ¯çš„æ–°é¢–é«˜æ•ˆæœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨çŸ­ç‰‡ã€å­—å¹•å’Œç­”æ¡ˆç½®ä¿¡åº¦ï¼›ä»¥åŠ3ï¼‰æˆ‘ä»¬å¯¹ç­”æ¡ˆç½®ä¿¡åº¦çš„æœ€å‰æ²¿VLMæ ¡å‡†åˆ†æã€‚æˆ‘ä»¬çš„ä»£ç†æ˜¯å»ºç«‹åœ¨å°å‹VLMå’Œä¸­å‹LLMä¹‹ä¸Šçš„ï¼Œå¯åœ¨æ ‡å‡†è®¡ç®—èµ„æºä¸Šè¿è¡Œã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†FALCON-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°é•¿æ—¶é—´ï¼ˆå¹³å‡å¤§äº1å°æ—¶ï¼‰çš„è§†é¢‘ç­”æ¡ˆæœç´¢æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†å¼€æ”¾å¼é—®é¢˜è¯„ä¼°çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒFALCONEyeåœ¨FALCON-Benchä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç›¸åŒæˆ–æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19850v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ä¸­æ£€ç´¢ä¿¡æ¯æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå°¤å…¶æ˜¯å½“æ‰€éœ€ä¿¡æ¯å±€é™äºä¸€å°éƒ¨åˆ†å¸§å†…æ—¶ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹è§†é¢‘ä»£ç†FALCONEyeï¼Œå®ƒç»“åˆäº†VLMå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿåœ¨è§†é¢‘ä¸­æœç´¢ç›¸å…³ä¿¡æ¯å¹¶å®šä½åŒ…å«ç­”æ¡ˆçš„å¸§ã€‚FALCONEyeçš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼š1ï¼‰æå‡ºçš„å…ƒæ¶æ„æ›´é€‚åˆå¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œä¸ç°æœ‰çŸ­è§†é¢‘æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼›2ï¼‰é‡‡ç”¨æ–°çš„é«˜æ•ˆæœç´¢ç®—æ³•ï¼Œåˆ©ç”¨çŸ­è§†é¢‘ç‰‡æ®µã€å­—å¹•å’Œç­”æ¡ˆç½®ä¿¡åº¦æ¥å®šä½ä¿¡æ¯ï¼›3ï¼‰æˆ‘ä»¬å¯¹ç­”æ¡ˆç½®ä¿¡åº¦çš„å…ˆè¿›VLMsæ ¡å‡†åˆ†æã€‚è¯¥ä»£ç†æ„å»ºäºå°å‹VLMå’Œä¸­ç­‰è§„æ¨¡LLMä¹‹ä¸Šï¼Œå¯åœ¨æ ‡å‡†è®¡ç®—èµ„æºä¸Šè¿è¡Œã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†FALCON-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°é•¿æ—¶é—´ï¼ˆå¹³å‡è¶…è¿‡1å°æ—¶ï¼‰çš„è§†é¢‘ç­”æ¡ˆæœç´¢æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å¼€æ”¾å¼é—®é¢˜è¯„ä¼°çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒFALCONEyeåœ¨FALCON-Benchä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹è§†é¢‘ä»£ç†FALCONEyeç»“åˆäº†VLMå’ŒLLMæ¥å¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ä¿¡æ¯æ£€ç´¢é—®é¢˜ã€‚</li>
<li>FALCONEyeå…·å¤‡é«˜æ•ˆçš„æœç´¢ç®—æ³•å’Œå…ˆè¿›çš„VLMsæ ¡å‡†åˆ†æï¼Œèƒ½å¤Ÿå®šä½åŒ…å«ç­”æ¡ˆçš„å¸§ã€‚</li>
<li>FALCONEyeé€šè¿‡ç»“åˆå°å‹VLMå’Œä¸­ç­‰è§„æ¨¡LLMåœ¨æ ‡å‡†è®¡ç®—èµ„æºä¸Šè¿è¡Œã€‚</li>
<li>å‘å¸ƒæ–°çš„åŸºå‡†æµ‹è¯•FALCON-Benchï¼Œç”¨äºè¯„ä¼°é•¿æ—¶é—´è§†é¢‘ç­”æ¡ˆæœç´¢çš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¼ºè°ƒäº†å¯¹å¼€æ”¾å¼é—®é¢˜è¯„ä¼°çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFALCONEyeåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨ç›¸å…³æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4497c60761509a77920ae92752c80528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7220fdfc741d771f646e88e87cf95dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8250c2392c74e5da10c829d922493a74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f108b2e717501dd19b1d2fc31884275e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae50d1b81f1ba90369578f30f1bd384e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PAVE-Patching-and-Adapting-Video-Large-Language-Models"><a href="#PAVE-Patching-and-Adapting-Video-Large-Language-Models" class="headerlink" title="PAVE: Patching and Adapting Video Large Language Models"></a>PAVE: Patching and Adapting Video Large Language Models</h2><p><strong>Authors:Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li</strong></p>
<p>Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as â€œpatches,â€ which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/dragonlzm/PAVE">https://github.com/dragonlzm/PAVE</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œå°†è¿™äº›æ¨¡å‹é€‚åº”æ¶‰åŠå…¶ä»–æ¨¡æ€æˆ–æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚éŸ³é¢‘æˆ–3Dä¿¡æ¯ï¼‰çš„æ–°ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PAVEï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„Video LLMsé€‚åº”äºå…·æœ‰ä¾§é€šé“ä¿¡å·çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¾‹å¦‚éŸ³é¢‘ã€3Dçº¿ç´¢æˆ–å¤šè§†è§’è§†é¢‘ã€‚PAVEå¼•å…¥äº†è½»é‡çº§é€‚é…å™¨ï¼Œç§°ä¸ºâ€œè¡¥ä¸â€ï¼Œè¿™äº›é€‚é…å™¨åªéœ€æ·»åŠ å°‘é‡å‚æ•°å’Œæ“ä½œåˆ°åŸºç¡€æ¨¡å‹ä¸Šï¼Œæ— éœ€ä¿®æ”¹å…¶æ¶æ„æˆ–é¢„è®­ç»ƒæƒé‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPAVEå¯ä»¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†å¬é—®ç­”ã€3Dæ¨ç†ã€å¤šè§†è§’è§†é¢‘è¯†åˆ«å’Œé«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼ŒPAVEæ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œå¹¶ä¸”åªå¢åŠ äº†çº¦0.1%çš„é¢å¤–FLOPså’Œå‚æ•°å¼€é”€ã€‚æ­¤å¤–ï¼ŒPAVEæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶åœ¨ä¸åŒçš„Video LLMsä¹‹é—´å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dragonlzm/PAVE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dragonlzm/PAVEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19794v1">PDF</a> CVPR2025 Camera Ready</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰å…·å¤‡å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å°†è¿™äº›æ¨¡å‹é€‚åº”æ¶‰åŠå…¶ä»–æ¨¡æ€æˆ–æ•°æ®ç±»å‹çš„æ–°ä»»åŠ¡ï¼ˆå¦‚éŸ³é¢‘æˆ–3Dä¿¡æ¯ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†PAVEæ¡†æ¶ï¼Œå¯ä»¥çµæ´»åœ°å°†é¢„è®­ç»ƒçš„è§†é¢‘LLMsé€‚åº”äºå…·æœ‰ä¾§é€šé“ä¿¡å·çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚éŸ³é¢‘ã€3Dçº¿ç´¢æˆ–å¤šè§†è§’è§†é¢‘ã€‚PAVEå¼•å…¥äº†è½»é‡çº§é€‚é…å™¨ï¼ˆâ€œè¡¥ä¸â€ï¼‰ï¼Œåªéœ€æ·»åŠ å°‘é‡å‚æ•°å’Œæ“ä½œåˆ°åŸºç¡€æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹å…¶æ¶æ„æˆ–é¢„è®­ç»ƒæƒé‡ã€‚å› æ­¤ï¼ŒPAVEå¯ä»¥æœ‰æ•ˆåœ°ä½¿é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹é€‚åº”æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬éŸ³é¢‘è§†è§‰é—®ç­”ã€3Dæ¨ç†ã€å¤šè§†è§’è§†é¢‘è¯†åˆ«å’Œé«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚åœ¨å„ç§ä»»åŠ¡ä¸­ï¼ŒPAVEæ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œå¹¶ä¸”åªå¢åŠ äº†çº¦0.1%çš„FLOPså’Œå‚æ•°å¼€é”€ã€‚æ­¤å¤–ï¼ŒPAVEæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶åœ¨ä¸åŒçš„è§†é¢‘LLMsä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†é€‚åº”æ–°ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>PAVEæ¡†æ¶æ—¨åœ¨çµæ´»é€‚åº”é¢„è®­ç»ƒçš„è§†é¢‘LLMsåˆ°å¸¦æœ‰ä¾§é€šé“ä¿¡å·çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>PAVEé€šè¿‡å¼•å…¥è½»é‡çº§é€‚é…å™¨ï¼ˆè¡¥ä¸ï¼‰æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€æ¨¡å‹çš„æ¶æ„æˆ–é¢„è®­ç»ƒæƒé‡ã€‚</li>
<li>PAVEæ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬éŸ³é¢‘è§†è§‰é—®ç­”ã€3Dæ¨ç†ã€å¤šè§†è§’è§†é¢‘è¯†åˆ«å’Œé«˜å¸§ç‡è§†é¢‘ç†è§£ã€‚</li>
<li>PAVEåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œä¸”å¢åŠ çš„è®¡ç®—å’Œå‚æ•°å¼€é”€è¾ƒå°ï¼ˆçº¦0.1%ï¼‰ã€‚</li>
<li>PAVEæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è·¨ä¸åŒè§†é¢‘LLMsçš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9dce8e36f3eb9929feb0f0478c36558c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c5fb082c2492a1cdc779496392b09aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-516fb31c5e65095ab902862e98541958.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fa8fece6135bffc3445e575edd735a5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dita-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Policy"><a href="#Dita-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Policy" class="headerlink" title="Dita: Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy"></a>Dita: Scaling Diffusion Transformer for Generalist   Vision-Language-Action Policy</h2><p><strong>Authors:Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, Yuntao Chen</strong></p>
<p>While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning â€“ enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformerâ€™s scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: <a target="_blank" rel="noopener" href="https://robodita.github.io/">https://robodita.github.io</a>. </p>
<blockquote>
<p>æœ€è¿‘ä½¿ç”¨å¤šç§æœºå™¨äººæ•°æ®é›†è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œåœ¨æœ‰é™é¢†åŸŸå†…å±•ç°å‡ºä»¤äººé¼“èˆçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºç´§å‡‘çš„åŠ¨ä½œå¤´æ¥é¢„æµ‹ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¼‚æ„åŠ¨ä½œç©ºé—´ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Ditaï¼Œä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨Transformeræ¶æ„é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹ç›´æ¥å¯¹è¿ç»­çš„åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•é€šè¿‡æµ…å±‚ç½‘ç»œèåˆåµŒå…¥æ¥è¿›è¡Œå»å™ªæ¡ä»¶è®¾ç½®ï¼Œè€ŒDitaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶â€”â€”å®ç°åœ¨å»å™ªåŠ¨ä½œå’Œæ¥è‡ªå†å²è§‚å¯Ÿçš„åŸè§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾å¼åœ°æ¨¡æ‹ŸåŠ¨ä½œå·®å¼‚å’Œç¯å¢ƒç»†å¾®å·®åˆ«ã€‚é€šè¿‡æ‰©æ•£åŠ¨ä½œå»å™ªå™¨ä¸Transformerçš„å¯æ‰©å±•æ€§ç›¸ç»“åˆï¼ŒDitaå¯ä»¥æœ‰æ•ˆåœ°æ•´åˆè·¨ä¸åŒç›¸æœºè§’åº¦ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´çš„è·¨ä½“æ€æ•°æ®é›†ã€‚è¿™ç§ååŒä½œç”¨æé«˜äº†å¯¹å„ç§å·®å¼‚çš„é²æ£’æ€§ï¼Œå¹¶ä¿ƒè¿›äº†é•¿æœŸä»»åŠ¡çš„æˆåŠŸæ‰§è¡Œã€‚åœ¨å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨ä»¿çœŸç¯å¢ƒä¸­çš„æœ€æ–°æˆ–ç›¸å½“æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDitaä»…é€šè¿‡ç¬¬ä¸‰äººç§°ç›¸æœºè¾“å…¥å®ç°äº†å¯¹ç¯å¢ƒå’Œå¤æ‚é•¿æœŸä»»åŠ¡çš„ç¨³å¥é€‚åº”ï¼Œé€šè¿‡10æ¬¡å¾®è°ƒå°„å‡»å³å¯å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ¶æ„ä¸ºé€šç”¨æœºå™¨äººç­–ç•¥å­¦ä¹ å»ºç«‹äº†é€šç”¨ã€è½»ä¾¿å’Œå¼€æºçš„åŸºçº¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://robodita.github.io./">https://robodita.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19757v1">PDF</a> Preprint; <a target="_blank" rel="noopener" href="https://robodita.github.io/">https://robodita.github.io</a>;</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåŸºäºå¤šæ ·åŒ–æœºå™¨äººæ•°æ®é›†è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœ‰é™é¢†åŸŸå†…æ•°æ®ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬é¢„æµ‹ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œæ—¶ä¾èµ–äºç´§å‡‘çš„åŠ¨ä½œå¤´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼‚æ„åŠ¨ä½œç©ºé—´ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Ditaæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨Transformeræ¶æ„é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡å¼æ‰©æ•£è¿‡ç¨‹ç›´æ¥å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªã€‚ä¸ä»¥å¾€ä¾èµ–èåˆåµŒå…¥çš„æµ…å±‚ç½‘ç»œè¿›è¡Œå»å™ªçš„æ–¹æ³•ä¸åŒï¼ŒDitaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œè¿™æœ‰åˆ©äºå¯¹åŠ¨ä½œå·®å¼‚å’Œç¯å¢ƒç»†å¾®å·®åˆ«çš„æ˜¾å¼å»ºæ¨¡ã€‚éšç€æ‰©æ•£åŠ¨ä½œå»å™ªå™¨ä¸Transformerçš„å¯æ‰©å±•æ€§ç›¸ç»“åˆï¼ŒDitaèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆè·¨ä½“æ€æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„ç›¸æœºè§’åº¦ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´ã€‚è¿™ç§ååŒä½œç”¨æé«˜äº†å¯¹å„ç§å˜é‡çš„ç¨³å¥æ€§ï¼Œå¹¶ä¿ƒè¿›äº†é•¿æœŸä»»åŠ¡çš„æˆåŠŸæ‰§è¡Œã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¾¾åˆ°äº†æœ€æ–°æˆ–ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDitaä»…åœ¨10æ¬¡å¾®è°ƒçš„æƒ…å†µä¸‹å°±èƒ½å®ç°é€‚åº”ç¯å¢ƒå˜åŒ–å’Œå¤æ‚é•¿æœŸä»»åŠ¡çš„ç¨³å¥ç°å®ä¸–ç•Œè¡¨ç°ã€‚è¯¥é¡¹ç›®å»ºç«‹ä¸€ä¸ªé€šç”¨ã€è½»ä¾¿ä¸”å¼€æºçš„æœºå™¨äººæ”¿ç­–å­¦ä¹ åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¤šæ ·åŒ–æœºå™¨äººæ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œæ—¶ä¾èµ–äºç´§å‡‘çš„åŠ¨ä½œå¤´ï¼Œé™åˆ¶äº†å…¶åœ¨å¼‚æ„åŠ¨ä½œç©ºé—´çš„é€‚åº”æ€§ã€‚</li>
<li>Ditaæ¡†æ¶åˆ©ç”¨Transformeræ¶æ„ç›´æ¥å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»å™ªã€‚</li>
<li>Ditaé‡‡ç”¨ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå®ç°å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°çš„å¯¹é½ã€‚</li>
<li>Ditaèƒ½å¤Ÿæ•´åˆè·¨ä½“æ€æ•°æ®é›†ï¼Œé€‚åº”ä¸åŒçš„ç›¸æœºè§’åº¦ã€è§‚å¯Ÿåœºæ™¯ã€ä»»åŠ¡å’ŒåŠ¨ä½œç©ºé—´ã€‚</li>
<li>Ditaé€šè¿‡ååŒä½œç”¨æé«˜äº†å¯¹å„ç§å˜é‡çš„ç¨³å¥æ€§ï¼Œå¹¶ä¿ƒè¿›äº†é•¿æœŸä»»åŠ¡çš„æ‰§è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df73b74b59a6700aba5f13e661ee6b6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edfe98c3cfd7008c4425a93f2f2be2e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07dcff6dc6bd34353d8ab50e7afcb74f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-128a78cc7993e8f45d1f998322936074.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5344f4917d937d1816f0e0c3ab27a67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33da7aac5cf8239cfeb97104c64992c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5864ce46ae71ca411e86c8bf04a7fde3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation"><a href="#ORION-A-Holistic-End-to-End-Autonomous-Driving-Framework-by-Vision-Language-Instructed-Action-Generation" class="headerlink" title="ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation"></a>ORION: A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation</h2><p><strong>Authors:Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai</strong></p>
<p>End-to-end (E2E) autonomous driving methods still struggle to make correct decisions in interactive closed-loop evaluation due to limited causal reasoning capability. Current methods attempt to leverage the powerful understanding and reasoning abilities of Vision-Language Models (VLMs) to resolve this dilemma. However, the problem is still open that few VLMs for E2E methods perform well in the closed-loop evaluation due to the gap between the semantic reasoning space and the purely numerical trajectory output in the action space. To tackle this issue, we propose ORION, a holistic E2E autonomous driving framework by vision-language instructed action generation. ORION uniquely combines a QT-Former to aggregate long-term history context, a Large Language Model (LLM) for driving scenario reasoning, and a generative planner for precision trajectory prediction. ORION further aligns the reasoning space and the action space to implement a unified E2E optimization for both visual question-answering (VQA) and planning tasks. Our method achieves an impressive closed-loop performance of 77.74 Driving Score (DS) and 54.62% Success Rate (SR) on the challenge Bench2Drive datasets, which outperforms state-of-the-art (SOTA) methods by a large margin of 14.28 DS and 19.61% SR. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•ç”±äºåœ¨å› æœæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œåœ¨äº¤äº’å¼é—­ç¯è¯„ä¼°ä¸­ä»ç„¶éš¾ä»¥åšå‡ºæ­£ç¡®å†³ç­–ã€‚å½“å‰çš„æ–¹æ³•è¯•å›¾åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼ºå¤§ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™ä¸€å›°å¢ƒã€‚ç„¶è€Œï¼Œé—®é¢˜ä¾ç„¶ä¸¥å³»ï¼šå¾ˆå°‘æœ‰é’ˆå¯¹E2Eæ–¹æ³•çš„VLMåœ¨é—­ç¯è¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¿™æ˜¯å› ä¸ºè¯­ä¹‰æ¨ç†ç©ºé—´ä¸è¡ŒåŠ¨ç©ºé—´ä¸­çº¯æ•°å€¼è½¨è¿¹è¾“å‡ºä¹‹é—´å­˜åœ¨å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ORIONï¼Œä¸€ä¸ªå…¨é¢çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æŒ‡å¯¼è¡ŒåŠ¨ç”Ÿæˆã€‚ORIONç‹¬ç‰¹åœ°ç»“åˆäº†QT-Formerä»¥èšé›†é•¿æœŸå†å²è¯­å¢ƒï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºé©¾é©¶åœºæ™¯æ¨ç†ï¼Œä»¥åŠç”Ÿæˆå¼è§„åˆ’å™¨ç”¨äºç²¾ç¡®è½¨è¿¹é¢„æµ‹ã€‚ORIONè¿›ä¸€æ­¥å¯¹é½æ¨ç†ç©ºé—´å’Œè¡ŒåŠ¨ç©ºé—´ï¼Œä»¥å®ç°è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç»Ÿä¸€ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Bench2Driveæ•°æ®é›†ä¸Šå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é—­ç¯æ€§èƒ½ï¼Œé©¾é©¶å¾—åˆ†ï¼ˆDSï¼‰è¾¾77.74åˆ†ï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰è¾¾54.62%ï¼Œè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œé©¾é©¶å¾—åˆ†é«˜å‡º14.28åˆ†ï¼ŒæˆåŠŸç‡é«˜å‡º19.61%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19755v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ç«¯å¯¹ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•å—é™äºå› æœæ¨ç†èƒ½åŠ›ï¼Œåœ¨äº¤äº’å¼é—­ç¯è¯„ä¼°ä¸­éš¾ä»¥åšå‡ºæ­£ç¡®å†³ç­–ã€‚å½“å‰æ–¹æ³•è¯•å›¾åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼ºå¤§ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™ä¸€éš¾é¢˜ã€‚ç„¶è€Œï¼Œç”±äºè¯­ä¹‰æ¨ç†ç©ºé—´ä¸åŠ¨ä½œç©ºé—´ä¸­çº¯æ•°å€¼è½¨è¿¹è¾“å‡ºä¹‹é—´çš„é¸¿æ²Ÿï¼Œå¾ˆå°‘æœ‰é¢å‘E2Eæ–¹æ³•çš„VLMåœ¨é—­ç¯è¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ORIONï¼Œä¸€ä¸ªå…¨é¢çš„E2Eè‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚ORIONç‹¬ç‰¹åœ°ç»“åˆäº†QT-Formerä»¥èšé›†é•¿æœŸå†å²ä¸Šä¸‹æ–‡ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºé©¾é©¶åœºæ™¯æ¨ç†ï¼Œä»¥åŠç”Ÿæˆå¼è§„åˆ’å™¨ä»¥å®ç°ç²¾ç¡®è½¨è¿¹é¢„æµ‹ã€‚ORIONè¿›ä¸€æ­¥å¯¹é½æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œä»¥æ‰§è¡Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç»Ÿä¸€E2Eä¼˜åŒ–ã€‚åœ¨Bench2Driveæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„é—­ç¯æ€§èƒ½ï¼Œé©¾é©¶å¾—åˆ†ï¼ˆDSï¼‰ä¸º77.74ï¼ŒæˆåŠŸç‡ï¼ˆSRï¼‰ä¸º54.62%ï¼Œè¾ƒä¹‹å‰çš„æœ€ä¼˜æ–¹æ³•å¤§å¹…æé«˜äº†14.28 DSå’Œ19.61% SRã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç«¯å¯¹ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨é©¾é©¶æ–¹æ³•åœ¨äº¤äº’å¼é—­ç¯è¯„ä¼°ä¸­å­˜åœ¨å†³ç­–å›°éš¾ï¼Œä¸»è¦å› ä¸ºæœ‰é™çš„å› æœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å°è¯•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è§£å†³æ­¤é—®é¢˜ï¼Œä½†è¯­ä¹‰æ¨ç†ä¸åŠ¨ä½œè¾“å‡ºé—´çš„é¸¿æ²Ÿä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ORIONæ¡†æ¶ç»“åˆQT-Formerã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼è§„åˆ’å™¨ï¼Œä»¥æ”¹å–„E2Eè‡ªåŠ¨é©¾é©¶çš„é—­ç¯æ€§èƒ½ã€‚</li>
<li>ORIONé€šè¿‡å¯¹é½æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œå®ç°è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§„åˆ’ä»»åŠ¡çš„ç»Ÿä¸€ä¼˜åŒ–ã€‚</li>
<li>åœ¨Bench2Driveæ•°æ®é›†ä¸Šï¼ŒORIONè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œé©¾é©¶å¾—åˆ†å’ŒæˆåŠŸç‡å‡æ˜¾è‘—è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
<li>ORIONæ¡†æ¶æœ‰åŠ©äºç¼©å°è¯­ä¹‰ç†è§£å’Œå®é™…é©¾é©¶åŠ¨ä½œä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a2866d6591e42f508a7e4e60329124f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d16e53ff0e849fd6a21ac12bf522c1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b7738e3f7c73461656202c92717b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a46ab5675ebcffdc095dba5dbe12632.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d17867080e144ed161adc2e3c4319c2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Inducing-Personality-in-LLM-Based-Honeypot-Agents-Measuring-the-Effect-on-Human-Like-Agenda-Generation"><a href="#Inducing-Personality-in-LLM-Based-Honeypot-Agents-Measuring-the-Effect-on-Human-Like-Agenda-Generation" class="headerlink" title="Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect   on Human-Like Agenda Generation"></a>Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect   on Human-Like Agenda Generation</h2><p><strong>Authors:Lewis Newsham, Ryan Hyland, Daniel Prince</strong></p>
<p>This paper presents SANDMAN, an architecture for cyber deception that leverages Language Agents to emulate convincing human simulacra. Our â€˜Deceptive Agentsâ€™ serve as advanced cyber decoys, designed for high-fidelity engagement with attackers by extending the observation period of attack behaviours. Through experimentation, measurement, and analysis, we demonstrate how a prompt schema based on the five-factor model of personality systematically induces distinct â€˜personalitiesâ€™ in Large Language Models. Our results highlight the feasibility of persona-driven Language Agents for generating diverse, realistic behaviours, ultimately improving cyber deception strategies. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SANDMANï¼Œè¿™æ˜¯ä¸€ç§ç½‘ç»œæ¬ºéª—æ¶æ„ï¼Œå®ƒåˆ©ç”¨è¯­è¨€ä»£ç†æ¥æ¨¡æ‹Ÿä»¤äººä¿¡æœçš„äººç±»æ¨¡æ‹Ÿç‰©ã€‚æˆ‘ä»¬çš„â€œæ¬ºéª—æ€§ä»£ç†â€å……å½“å…ˆè¿›çš„ç½‘ç»œè¯±é¥µï¼Œé€šè¿‡å»¶é•¿æ”»å‡»è¡Œä¸ºçš„è§‚å¯ŸæœŸï¼Œæ—¨åœ¨ä¸æ”»å‡»è€…è¿›è¡Œé«˜ä¿çœŸäº’åŠ¨ã€‚é€šè¿‡å®éªŒã€æµ‹é‡å’Œåˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºäº”å› ç´ äººæ ¼æ¨¡å‹çš„æç¤ºæ¨¡å¼å¦‚ä½•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç³»ç»Ÿåœ°è¯±å¯¼å‡ºä¸åŒçš„â€œäººæ ¼â€ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ä¸ªæ€§é©±åŠ¨çš„è¯­è¨€ä»£ç†åœ¨ç”Ÿæˆå¤šæ ·åŒ–ã€ç°å®è¡Œä¸ºæ–¹é¢çš„å¯è¡Œæ€§ï¼Œæœ€ç»ˆæé«˜äº†ç½‘ç»œæ¬ºéª—ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19752v1">PDF</a> 11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024</p>
<p><strong>Summary</strong><br>æ²™æ¼å‹ç½‘ç»œæ¶æ„é€šè¿‡åˆ©ç”¨è¯­è¨€ä»£ç†äººæ¥æ¨¡æ‹Ÿä»¤äººä¿¡æœçš„äººç±»å½¢è±¡ï¼Œæå‡ºä¸€ç§ç½‘ç»œæ¬ºéª—ç³»ç»Ÿâ€œSANDMANâ€ã€‚è¿™ç§ç³»ç»Ÿè¿ç”¨äº”å› ç´ äººæ ¼æ¨¡å‹çš„æç¤ºæœºåˆ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°ä¸åŒçš„â€œäººæ ¼â€ï¼Œè®¾è®¡å‡ºæ¬ºéª—æ€§ä»£ç†äººä½œä¸ºé«˜çº§ç½‘ç»œè¯±é¥µã€‚é€šè¿‡å®éªŒæ“ä½œå’Œåˆ†æï¼Œè¯å®è¯¥æ¶æ„å¯ç”Ÿæˆå¤šæ ·çš„çœŸå®è¡Œä¸ºï¼Œä»è€Œæ”¹è¿›ç½‘ç»œæ¬ºéª—ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SANDMANæ˜¯ä¸€ç§åŸºäºè¯­è¨€ä»£ç†çš„ç½‘ç»œæ¬ºéª—æ¶æ„ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿé€¼çœŸçš„äººç±»å½¢è±¡ã€‚</li>
<li>è¯¥æ¶æ„åˆ©ç”¨äº”å› ç´ äººæ ¼æ¨¡å‹ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡æç¤ºæœºåˆ¶ï¼Œä½¿å…¶å±•ç°ä¸åŒâ€œäººæ ¼â€ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œè¿™ç§æ¶æ„èƒ½æ˜¾è‘—æé«˜ç½‘ç»œè¯±é¥µçš„é€¼çœŸåº¦å’Œå®æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡å»¶é•¿æ”»å‡»è¡Œä¸ºçš„è§‚å¯ŸæœŸï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹ç½‘ç»œæ”»å‡»è€…ã€‚</li>
<li>è¯¥æ¶æ„ç”Ÿæˆçš„å¤šæ ·åŒ–å’ŒçœŸå®çš„è¡Œä¸ºæ¨¡å¼æœ‰åŠ©äºæå‡ç½‘ç»œæ¬ºéª—ç­–ç•¥çš„æ•ˆæœã€‚</li>
<li>è¿™ç§æ¶æ„å¯ä¸ºç½‘ç»œå®‰å…¨é¢†åŸŸæä¾›ä¸€ç§æœ‰æ•ˆçš„é˜²å¾¡æ‰‹æ®µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9df469b3378ecc6b58645386da66434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2cc7650f6a671578509111ffe9c84f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b820845909bfabac55b6442ae95863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fcc4a81229ea58dbeced767d93ba5ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e63e255451c76e409ab48eae0354dba.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-multitask-transformer-to-sign-language-translation-using-motion-gesture-primitives"><a href="#A-multitask-transformer-to-sign-language-translation-using-motion-gesture-primitives" class="headerlink" title="A multitask transformer to sign language translation using motion   gesture primitives"></a>A multitask transformer to sign language translation using motion   gesture primitives</h2><p><strong>Authors:Fredy Alejandro Mendoza LÃ³pez, Jefferson Rodriguez, Fabio MartÃ­nez</strong></p>
<p>The absence of effective communication the deaf population represents the main social gap in this community. Furthermore, the sign language, main deaf communication tool, is unlettered, i.e., there is no formal written representation. In consequence, main challenge today is the automatic translation among spatiotemporal sign representation and natural text language. Recent approaches are based on encoder-decoder architectures, where the most relevant strategies integrate attention modules to enhance non-linear correspondences, besides, many of these approximations require complex training and architectural schemes to achieve reasonable predictions, because of the absence of intermediate text projections. However, they are still limited by the redundant background information of the video sequences. This work introduces a multitask transformer architecture that includes a gloss learning representation to achieve a more suitable translation. The proposed approach also includes a dense motion representation that enhances gestures and includes kinematic information, a key component in sign language. From this representation it is possible to avoid background information and exploit the geometry of the signs, in addition, it includes spatiotemporal representations that facilitate the alignment between gestures and glosses as an intermediate textual representation. The proposed approach outperforms the state-of-the-art evaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and a BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the RWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%. </p>
<blockquote>
<p>å¯¹äºè‹å“‘äººç¾¤è€Œè¨€ï¼Œç¼ºä¹æœ‰æ•ˆæ²Ÿé€šä»£è¡¨ç€è¿™ä¸ªç¤¾åŒºä¸­çš„ä¸»è¦ç¤¾ä¼šé¸¿æ²Ÿã€‚æ­¤å¤–ï¼Œæ‰‹è¯­æ˜¯ä¸»è¦æ²Ÿé€šå·¥å…·ï¼Œä½†æ²¡æœ‰æ–‡å­—è®°å½•å½¢å¼ï¼Œå³æ²¡æœ‰æ­£å¼çš„ä¹¦é¢è¡¨ç°å½¢å¼ã€‚å› æ­¤ï¼Œå½“ä»Šçš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ—¶ç©ºæ‰‹åŠ¿è¡¨ç¤ºä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„è‡ªåŠ¨ç¿»è¯‘ã€‚è¿‘æœŸçš„æ–¹æ³•åŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå…¶ä¸­æœ€é‡è¦çš„ç­–ç•¥ç»“åˆäº†æ³¨æ„åŠ›æ¨¡å—ä»¥å¢å¼ºéçº¿æ€§å¯¹åº”å…³ç³»ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œç”±äºç¼ºå°‘ä¸­é—´æ–‡æœ¬æŠ•å½±ï¼Œè®¸å¤šè¿™äº›è¿‘ä¼¼æ–¹æ³•éœ€è¦å¤æ‚çš„è®­ç»ƒå’Œæ¶æ„æ–¹æ¡ˆæ‰èƒ½è¾¾åˆ°åˆç†çš„é¢„æµ‹æ•ˆæœã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å—åˆ°è§†é¢‘åºåˆ—çš„å†—ä½™èƒŒæ™¯ä¿¡æ¯çš„é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19668v1">PDF</a> 32 pages, 10 tables, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨è‹å“‘äººç¾¤æ²Ÿé€šçš„ä¸»è¦éšœç¢ï¼Œå³ç¼ºä¹æœ‰æ•ˆæ²Ÿé€šçš„é—®é¢˜ã€‚æ‰‹è¯­ä½œä¸ºè‹å“‘äººçš„ä¸»è¦æ²Ÿé€šå·¥å…·ï¼Œç”±äºå…¶æ²¡æœ‰æ­£å¼çš„ä¹¦é¢è¡¨ç°å½¢å¼ï¼Œç»™è‡ªåŠ¨ç¿»è¯‘å¸¦æ¥å›°éš¾ã€‚ç›®å‰ä¸»è¦çš„æŒ‘æˆ˜æ˜¯å®ç°æ—¶ç©ºç¬¦å·è¡¨ç¤ºä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„è‡ªåŠ¨ç¿»è¯‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å˜æ¢å™¨æ¶æ„ï¼ŒåŒ…æ‹¬è¯æ±‡å­¦ä¹ è¡¨ç¤ºï¼Œä»¥å®ç°æ›´åˆé€‚çš„ç¿»è¯‘ã€‚è¯¥æ–¹æ³•è¿˜åŒ…æ‹¬å¯†é›†çš„è¿åŠ¨è¡¨ç¤ºï¼Œä»¥å¢å¼ºæ‰‹åŠ¿å¹¶åŒ…æ‹¬è¿åŠ¨å­¦ä¿¡æ¯ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿé¿å…èƒŒæ™¯ä¿¡æ¯å¹¶æ¢ç´¢ç¬¦å·çš„å‡ ä½•å½¢çŠ¶ï¼Œæ­¤å¤–è¿˜åŒ…æ‹¬æ—¶ç©ºè¡¨ç¤ºï¼Œä»¥ä¾¿äºåœ¨ä½œä¸ºä¸­é—´æ–‡æœ¬è¡¨ç¤ºæ—¶å¯¹å§¿æ€å’Œç¬¦å·è¿›è¡Œå¯¹é½ã€‚è¯¥æ–¹æ³•åœ¨CoL-SLTDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä½³æ°´å¹³ï¼ŒSplit 1çš„BLEU-4å¾—åˆ†è¾¾åˆ°72.64%ï¼ŒSplit 2çš„BLEU-4å¾—åˆ†ä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨RWTH-PHOENIX-Weather 2014 Tæ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹æœ‰æ•ˆæ²Ÿé€šæ˜¯è‹å“‘äººç¾¤é¢ä¸´çš„ä¸»è¦ç¤¾ä¼šéšœç¢ã€‚</li>
<li>æ‰‹è¯­ä½œä¸ºè‹å“‘äººçš„ä¸»è¦æ²Ÿé€šå·¥å…·æ²¡æœ‰æ­£å¼çš„ä¹¦é¢è¡¨ç°å½¢å¼ï¼Œå¯¼è‡´è‡ªåŠ¨ç¿»è¯‘çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æŒ‘æˆ˜æ˜¯å®ç°æ—¶ç©ºç¬¦å·è¡¨ç¤ºä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„è‡ªåŠ¨ç¿»è¯‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å˜æ¢å™¨æ¶æ„ï¼ŒåŒ…æ‹¬è¯æ±‡å­¦ä¹ è¡¨ç¤ºå’Œå¯†é›†è¿åŠ¨è¡¨ç¤ºä»¥å¢å¼ºæ‰‹åŠ¿è¯†åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé¿å…èƒŒæ™¯ä¿¡æ¯å¹¶æ¢ç´¢ç¬¦å·çš„å‡ ä½•å½¢çŠ¶ï¼ŒåŒ…æ‹¬æ—¶ç©ºè¡¨ç¤ºä»¥è¿›è¡Œå§¿æ€å’Œç¬¦å·å¯¹é½ã€‚</li>
<li>æ–¹æ³•åœ¨CoL-SLTDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä½³æ°´å¹³ï¼ŒSplit 1çš„BLEU-4å¾—åˆ†è¾¾åˆ°72.64%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d687655e26d2c4f238a8f4b1523507bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfdc139e414c131780228f3f076ce0c0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BiblioPage-A-Dataset-of-Scanned-Title-Pages-for-Bibliographic-Metadata-Extraction"><a href="#BiblioPage-A-Dataset-of-Scanned-Title-Pages-for-Bibliographic-Metadata-Extraction" class="headerlink" title="BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata   Extraction"></a>BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata   Extraction</h2><p><strong>Authors:Jan KohÃºt, Martin DoÄekal, Michal HradiÅ¡, Marek VaÅ¡ko</strong></p>
<p>Manual digitization of bibliographic metadata is time consuming and labor intensive, especially for historical and real-world archives with highly variable formatting across documents. Despite advances in machine learning, the absence of dedicated datasets for metadata extraction hinders automation. To address this gap, we introduce BiblioPage, a dataset of scanned title pages annotated with structured bibliographic metadata. The dataset consists of approximately 2,000 monograph title pages collected from 14 Czech libraries, spanning a wide range of publication periods, typographic styles, and layout structures. Each title page is annotated with 16 bibliographic attributes, including title, contributors, and publication metadata, along with precise positional information in the form of bounding boxes. To extract structured information from this dataset, we valuated object detection models such as YOLO and DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and an F1 score of 59. Additionally, we assess the performance of various visual large language models, including LlamA 3.2-Vision and GPT-4o, with the best model reaching an F1 score of 67. BiblioPage serves as a real-world benchmark for bibliographic metadata extraction, contributing to document understanding, document question answering, and document information extraction. Dataset and evaluation scripts are availible at: <a target="_blank" rel="noopener" href="https://github.com/DCGM/biblio-dataset">https://github.com/DCGM/biblio-dataset</a> </p>
<blockquote>
<p>æ–‡çŒ®å…ƒæ•°æ®çš„æ‰‹åŠ¨æ•°å­—åŒ–æ—¢è€—æ—¶åˆè€—åŠ³åŠ›ï¼Œå°¤å…¶æ˜¯å¯¹äºæ ¼å¼å¤šå˜çš„çš„å†å²å’Œç°å®æ¡£æ¡ˆã€‚å°½ç®¡æœºå™¨å­¦ä¹ æœ‰æ‰€è¿›å±•ï¼Œä½†ç¼ºä¹ä¸“é—¨çš„å…ƒæ•°æ®æå–æ•°æ®é›†é˜»ç¢äº†è‡ªåŠ¨åŒ–è¿›ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BiblioPageæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä»æ·å…‹å…±å’Œå›½çš„14ä¸ªå›¾ä¹¦é¦†æ”¶é›†çš„çº¦2000ä¸ªå•æœ¬æ ‡é¢˜é¡µï¼Œæ¶µç›–äº†å¹¿æ³›çš„å‡ºç‰ˆå‘¨æœŸã€æ’ç‰ˆé£æ ¼å’Œå¸ƒå±€ç»“æ„ã€‚æ¯ä¸ªæ ‡é¢˜é¡µéƒ½æ ‡æ³¨äº†åŒ…æ‹¬æ ‡é¢˜ã€è´¡çŒ®è€…å’Œå‡ºç‰ˆå…ƒæ•°æ®åœ¨å†…çš„16ä¸ªæ–‡çŒ®å±æ€§ï¼Œä»¥åŠè¾¹ç•Œæ¡†å½¢å¼çš„ç²¾ç¡®ä½ç½®ä¿¡æ¯ã€‚ä¸ºäº†ä»è¿™ä¸ªæ•°æ®é›†ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œæˆ‘ä»¬è¯„ä¼°äº†YOLOå’ŒDETRç­‰ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œå¹¶ç»“åˆåŸºäºå˜å‹å™¨çš„OCRæŠ€æœ¯ï¼Œå–å¾—äº†æœ€å¤§mAPä¸º52å’ŒF1åˆ†æ•°ä¸º59çš„æˆç»©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹å„ç§è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬LlamA 3.2-Visionå’ŒGPT-4oï¼‰çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œæœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°è¾¾åˆ°67ã€‚BiblioPageä½œä¸ºæ–‡çŒ®å…ƒæ•°æ®æå–çš„ç°å®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ï¼Œä¸ºæ–‡çŒ®ç†è§£ã€æ–‡çŒ®é—®ç­”å’Œæ–‡çŒ®ä¿¡æ¯æå–åšå‡ºäº†è´¡çŒ®ã€‚æ•°æ®é›†å’Œè¯„ä¼°è„šæœ¬å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/DCGM/biblio-dataset">https://github.com/DCGM/biblio-dataset</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19658v1">PDF</a> Submitted to ICDAR2025 conference</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†æ‰‹åŠ¨æ•°å­—åŒ–æ–‡çŒ®å…ƒæ•°æ®çš„ä¸è¶³ä»¥åŠæœºå™¨å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œä¸ºæ­¤æå‡ºäº†BiblioPageæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä»æ·å…‹å›¾ä¹¦é¦†æ”¶é›†çš„çº¦2000ç§å•æœ¬ä¹¦åé¡µï¼Œæ¶µç›–å¹¿æ³›çš„å‡ºç‰ˆå‘¨æœŸã€æ’ç‰ˆé£æ ¼å’Œå¸ƒå±€ç»“æ„ã€‚æ•°æ®é›†ä¸ºæ¯ä¸ªä¹¦åé¡µæ ‡æ³¨äº†ç»“æ„åŒ–æ–‡çŒ®å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä½œè€…ç­‰ã€‚åŒæ—¶è¯„ä¼°äº†å¯¹è±¡æ£€æµ‹æ¨¡å‹ä¸è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚BiblioPageä¸ºæ–‡çŒ®å…ƒæ•°æ®æå–æä¾›äº†ç°å®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ–‡çŒ®ç†è§£ã€é—®ç­”å’Œä¿¡æ¯æå–ç­‰é¢†åŸŸæœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹åŠ¨æ•°å­—åŒ–æ–‡çŒ®å…ƒæ•°æ®è€—æ—¶è€—åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå†å²å’Œç°å®ä¸–ç•Œçš„æ¡£æ¡ˆæ–‡ä»¶ã€‚</li>
<li>ç›®å‰æœºå™¨å­¦ä¹ åœ¨æ–‡çŒ®å…ƒæ•°æ®æå–è‡ªåŠ¨åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹ä¸“ç”¨æ•°æ®é›†ã€‚</li>
<li>BiblioPageæ•°æ®é›†åŒ…å«çº¦2000ç§å•æœ¬ä¹¦åé¡µï¼Œæ¶µç›–å¤šç§å‡ºç‰ˆå‘¨æœŸã€æ’ç‰ˆé£æ ¼å’Œå¸ƒå±€ç»“æ„ã€‚</li>
<li>æ•°æ®é›†ä¸ºæ¯ä¸ªä¹¦åé¡µæ ‡æ³¨äº†ç»“æ„åŒ–æ–‡çŒ®å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä½œè€…ç­‰ï¼Œä»¥åŠç²¾ç¡®çš„ä½ç½®ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨å¯¹è±¡æ£€æµ‹æ¨¡å‹å¦‚YOLOå’ŒDETRç»“åˆåŸºäºTransformerçš„OCRæŠ€æœ¯æå–ç»“æ„åŒ–ä¿¡æ¯è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡çŒ®å…ƒæ•°æ®æå–æ–¹é¢ä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ï¼Œæœ€ä½³æ¨¡å‹è¾¾åˆ°F1åˆ†æ•°ä¸º67ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4cf0e2a537af90516efc59ca2b5f1dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bb173ccc040cb900e26bcb852261e44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0442e01daf1daac997d1da2349bc57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d290d8670cd63cab87c460bf487cd078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3bc30043a6301d14abb78fcfe53c96.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training"><a href="#1-4-Million-Open-Source-Distilled-Reasoning-Dataset-to-Empower-Large-Language-Model-Training" class="headerlink" title="1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training"></a>1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large   Language Model Training</h2><p><strong>Authors:Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, Xiangang Li</strong></p>
<p>The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces for general reasoning tasks, composed of high-quality and challenging reasoning problems. These problems are collected from a multitude of open-source datasets, subjected to semantic deduplication and meticulous cleaning to eliminate test set contamination. All responses within the dataset are distilled from reasoning models (predominantly DeepSeek-R1) and have undergone rigorous verification procedures. Mathematical problems are validated by checking against reference answers, code problems are verified using test cases, and other tasks are evaluated with the aid of a reward model. The AM-Distill-Qwen-32B model, which was trained through only simple Supervised Fine-Tuning (SFT) using this batch of data, outperformed the DeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500, GPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model surpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We are releasing these 1.4 million problems and their corresponding responses to the research community with the objective of fostering the development of powerful reasoning-oriented Large Language Models (LLMs). The dataset was published in \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D%7Bhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M%7D">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}</a>. </p>
<blockquote>
<p>AM-DeepSeek-R1-Distilledæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¸¦æœ‰æ€è€ƒç—•è¿¹çš„é€šç”¨æ¨ç†ä»»åŠ¡æ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„æ¨ç†é—®é¢˜å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ¥è‡ªå¤šä¸ªå¼€æºæ•°æ®é›†ï¼Œç»è¿‡è¯­ä¹‰å»é‡å’Œä»”ç»†æ¸…ç†ï¼Œæ¶ˆé™¤äº†æµ‹è¯•é›†æ±¡æŸ“ã€‚æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç­”æ¡ˆéƒ½ç»è¿‡æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦æ˜¯DeepSeek-R1ï¼‰çš„æç‚¼ï¼Œå¹¶ç»è¿‡äº†ä¸¥æ ¼çš„éªŒè¯ç¨‹åºã€‚æ•°å­¦é—®é¢˜çš„ç­”æ¡ˆé€šè¿‡å¯¹ç…§å‚è€ƒç­”æ¡ˆè¿›è¡ŒéªŒè¯ï¼Œä»£ç é—®é¢˜çš„ç­”æ¡ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯ï¼Œå…¶ä»–ä»»åŠ¡åˆ™å€ŸåŠ©å¥–åŠ±æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ä»…é€šè¿‡ç®€å•ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„AM-Distill-Qwen-32Bæ¨¡å‹ï¼Œåœ¨è¿™ä¸€æ‰¹æ•°æ®ä¸Šä¼˜äºDeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹ï¼Œåœ¨AIME2024ã€MATH-501ã€GPQA-Diamondå’ŒLiveCodeBenchå››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼ŒAM-Distill-Qwen-72Bæ¨¡å‹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿè¶…è¿‡äº†DeepSeek-R1-Distill-Llama-70Bæ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒè¿™140ä¸‡ä¸ªé—®é¢˜å’Œç›¸åº”çš„ç­”æ¡ˆï¼Œæ—¨åœ¨ä¿ƒè¿›å¼ºå¤§çš„é¢å‘æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚æ•°æ®é›†å·²å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M]%E3%80%82">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19633v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>AM-DeepSeek-R1-Distilledå¤§è§„æ¨¡æ•°æ®é›†åŒ…å«ç”¨äºé€šç”¨æ¨ç†ä»»åŠ¡çš„æ€è€ƒè½¨è¿¹ï¼Œç”±é«˜è´¨é‡ä¸”å…·æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ç»„æˆã€‚è¿™äº›é—®é¢˜æ¥è‡ªå¤šä¸ªå¼€æºæ•°æ®é›†ï¼Œç»è¿‡è¯­ä¹‰å»é‡å’Œç²¾å¿ƒæ¸…ç†ï¼Œä»¥æ¶ˆé™¤æµ‹è¯•é›†æ±¡æŸ“ã€‚è¯¥æ•°æ®é›†çš„æ‰€æœ‰ç­”æ¡ˆå‡ç”±æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦æ˜¯DeepSeek-R1ï¼‰æç‚¼ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼éªŒè¯ç¨‹åºã€‚é€šè¿‡å‚è€ƒç­”æ¡ˆå¯¹æ•°å­¦é—®é¢˜è¿›è¡ŒéªŒè¯ï¼Œé€šè¿‡æµ‹è¯•ç”¨ä¾‹å¯¹ä»£ç é—®é¢˜è¿›è¡ŒéªŒè¯ï¼Œå…¶ä»–ä»»åŠ¡åˆ™å€ŸåŠ©å¥–åŠ±æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†ä»…é€šè¿‡ç®€å•çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„AM-Distill-Qwen-32Bæ¨¡å‹ï¼Œåœ¨AIME2024ã€MATH-500ã€GPQA-Diamondå’ŒLiveCodeBenchå››é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¼˜äºDeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒAM-Distill-Qwen-72Bæ¨¡å‹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡è¶…è¿‡äº†DeepSeek-R1-Distill-Llama-70Bæ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒè¿™140ä¸‡é—®é¢˜åŠç›¸åº”ç­”æ¡ˆï¼Œæ—¨åœ¨æ¨åŠ¨å¼ºå¤§çš„æ¨ç†å¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚è¯¥æ•°æ®é›†å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AM-DeepSeek-R1-Distilledæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«ç”¨äºé€šç”¨æ¨ç†ä»»åŠ¡çš„é«˜è´¨é‡ã€å…·æŒ‘æˆ˜æ€§é—®é¢˜çš„æ€è€ƒè½¨è¿¹ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„é—®é¢˜ä»å¤šä¸ªå¼€æºæ•°æ®é›†ä¸­æ”¶é›†ï¼Œç»è¿‡è¯­ä¹‰å»é‡å’Œæ¸…ç†ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„ç­”æ¡ˆé€šè¿‡æ¨ç†æ¨¡å‹æç‚¼ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯ç¨‹åºï¼ŒåŒ…æ‹¬æ•°å­¦ã€ä»£ç å’Œå…¶ä»–ä»»åŠ¡çš„éªŒè¯ã€‚</li>
<li>ä½¿ç”¨AM-Distill-Qwenæ¨¡å‹é€šè¿‡ç®€å•æœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>AM-Distill-Qwen-32Bæ¨¡å‹åœ¨AIME2024ã€MATH-500ã€GPQA-Diamondå’ŒLiveCodeBenchæµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºDeepSeek-R1æ¨¡å‹ã€‚</li>
<li>AM-Distill-Qwen-72Bæ¨¡å‹åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡è¶…è¿‡äº†DeepSeek-R1-Distill-Llama-70Bæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f6df3c97952c719378666d41a3ce4ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61c4635aa84efbe4f96ecf75988557d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77132bd24b561f2554256e0f6e60dfb5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VecTrans-LLM-Transformation-Framework-for-Better-Auto-vectorization-on-High-performance-CPU"><a href="#VecTrans-LLM-Transformation-Framework-for-Better-Auto-vectorization-on-High-performance-CPU" class="headerlink" title="VecTrans: LLM Transformation Framework for Better Auto-vectorization on   High-performance CPU"></a>VecTrans: LLM Transformation Framework for Better Auto-vectorization on   High-performance CPU</h2><p><strong>Authors:Zhongchun Zheng, Long Cheng, Lu Li, Rodrigo C. O. Rocha, Tianyi Liu, Wei Wei, Xianwei Zhang, Yaoqing Gao</strong></p>
<p>Large language models (LLMs) have demonstrated great capabilities in code generation, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. Vectorization, a crucial optimization for enhancing code performance, often fails because of the compilerâ€™s inability to recognize complex code patterns, which commonly require extensive empirical expertise. LLMs, with their ability to capture intricate patterns, thus providing a promising solution to this challenge. This paper presents VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilerâ€™s auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. Experimental results show that among all 50 TSVC functions unvectorizable by Clang, GCC, and BiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves an average speedup of 2.02x, greatly surpassing state-of-the-art performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œåœ¨ç¼–è¯‘å™¨ä¼˜åŒ–æ–¹é¢çš„æœ‰æ•ˆåº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå­˜åœ¨çš„é—®é¢˜åŒ…æ‹¬å¹»è±¡å’Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¨ç†ã€‚å‘é‡åŒ–æ˜¯æé«˜ä»£ç æ€§èƒ½çš„å…³é”®ä¼˜åŒ–æ‰‹æ®µï¼Œä½†ç”±äºç¼–è¯‘å™¨æ— æ³•è¯†åˆ«å¤æ‚çš„ä»£ç æ¨¡å¼ï¼Œé€šå¸¸éœ€è¦ä¸°å¯Œçš„å®è¯ç»éªŒï¼Œå› æ­¤å¸¸å¸¸å¤±æ•ˆã€‚LLMå…·æœ‰æ•æ‰å¤æ‚æ¨¡å¼çš„èƒ½åŠ›ï¼Œå› æ­¤ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p>æœ¬æ–‡æå‡ºäº†VecTransï¼Œä¸€ä¸ªåˆ©ç”¨LLMå¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç å‘é‡åŒ–çš„æ–°å‹æ¡†æ¶ã€‚VecTransé¦–å…ˆä½¿ç”¨ç¼–è¯‘å™¨åˆ†ææ¥è¯†åˆ«å¯èƒ½å¯è¿›è¡Œå‘é‡åŒ–çš„ä»£ç åŒºåŸŸã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨LLMå°†è¿™äº›åŒºåŸŸé‡æ„ä¸ºæ›´æ˜“äºç¼–è¯‘å™¨è‡ªåŠ¨å‘é‡åŒ–çš„æ¨¡å¼ã€‚ä¸ºç¡®ä¿è¯­ä¹‰æ­£ç¡®æ€§ï¼ŒVecTransè¿›ä¸€æ­¥åœ¨ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰çº§åˆ«æ•´åˆäº†æ··åˆéªŒè¯æœºåˆ¶ã€‚é€šè¿‡ä»¥ä¸ŠåŠªåŠ›ï¼ŒVecTransç»“åˆäº†LLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨å‘é‡çš„ç²¾ç¡®æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°å¼€å¯äº†å‘é‡åŒ–æœºä¼šã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19449v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>LLMåœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­çš„åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºVecTransæ¡†æ¶ï¼Œåˆ©ç”¨LLMå¢å¼ºåŸºäºç¼–è¯‘å™¨çš„ä»£ç å‘é‡åŒ–ã€‚VecTransé¦–å…ˆé€šè¿‡ç¼–è¯‘å™¨åˆ†æè¯†åˆ«å¯å‘é‡åŒ–çš„ä»£ç åŒºåŸŸï¼Œå¹¶åˆ©ç”¨LLMå¯¹è¿™äº›åŒºåŸŸè¿›è¡Œé‡æ„ï¼Œä»¥ä¾¿ç¼–è¯‘å™¨è‡ªåŠ¨çŸ¢é‡åŒ–ã€‚å®éªŒç»“æœè¯æ˜äº†VecTransçš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ— æ³•è¢«Clangã€GCCå’ŒBiShengCompilerå‘é‡åŒ–çš„50ä¸ªTSVCå‡½æ•°ä¸­ï¼ŒæˆåŠŸå‘é‡åŒ–äº†23ä¸ªæ¡ˆä¾‹ï¼ˆ46%ï¼‰ï¼Œå¹³å‡æé€Ÿ2.02å€ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­çš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯†åˆ«å¤æ‚ä»£ç æ¨¡å¼æ–¹é¢çš„é™åˆ¶ã€‚</li>
<li>VecTransæ¡†æ¶åˆ©ç”¨LLMæ¥å¢å¼ºç¼–è¯‘å™¨åŸºäºä»£ç çš„çŸ¢é‡åŒ–èƒ½åŠ›ã€‚</li>
<li>VecTransé€šè¿‡ç¼–è¯‘å™¨åˆ†æè¯†åˆ«å¯å‘é‡åŒ–çš„ä»£ç åŒºåŸŸã€‚</li>
<li>LLMè¢«ç”¨æ¥é‡æ„è¿™äº›åŒºåŸŸï¼Œä½¿å…¶æ›´å®¹æ˜“è¢«ç¼–è¯‘å™¨è‡ªåŠ¨çŸ¢é‡åŒ–ã€‚</li>
<li>VecTransç»“åˆLLMçš„é€‚åº”æ€§å’Œç¼–è¯‘å™¨çŸ¢é‡åŒ–çš„ç²¾ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒVecTransæˆåŠŸå‘é‡åŒ–äº†46%çš„TSVCå‡½æ•°ï¼Œè¿œè¶…Clangã€GCCå’ŒBiShengCompilerçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77cb4d46a90d06657cd737ea578b863e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81910f7f9b34e4b88f69129a66fe6746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1e041aefd0db96d9606d48b06d39046.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b88efdf3da95d4ba3ff3387eb15f4b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d84188e391552285747d1e0ebeebbce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e37044871c0c47d47260d8b8a287c67d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LangBridge-Interpreting-Image-as-a-Combination-of-Language-Embeddings"><a href="#LangBridge-Interpreting-Image-as-a-Combination-of-Language-Embeddings" class="headerlink" title="LangBridge: Interpreting Image as a Combination of Language Embeddings"></a>LangBridge: Interpreting Image as a Combination of Language Embeddings</h2><p><strong>Authors:Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, Jifeng Dai, Yu Cheng</strong></p>
<p>Recent years have witnessed remarkable advances in Large Vision-Language Models (LVLMs), which have achieved human-level performance across various complex vision-language tasks. Following LLaVAâ€™s paradigm, mainstream LVLMs typically employ a shallow MLP for visual-language alignment through a two-stage training process: pretraining for cross-modal alignment followed by instruction tuning. While this approach has proven effective, the underlying mechanisms of how MLPs bridge the modality gap remain poorly understood. Although some research has explored how LLMs process transformed visual tokens, few studies have investigated the fundamental alignment mechanism. Furthermore, the MLP adapter requires retraining whenever switching LLM backbones. To address these limitations, we first investigate the working principles of MLP adapters and discover that they learn to project visual embeddings into subspaces spanned by corresponding text embeddings progressively. Based on this insight, we propose LangBridge, a novel adapter that explicitly maps visual tokens to linear combinations of LLM vocabulary embeddings. This innovative design enables pretraining-free adapter transfer across different LLMs while maintaining performance. Our experimental results demonstrate that a LangBridge adapter pre-trained on Qwen2-0.5B can be directly applied to larger models such as LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall, LangBridge enables interpretable vision-language alignment by grounding visual representations in LLM vocab embedding, while its plug-and-play design ensures efficient reuse across multiple LLMs with nearly no performance degradation. See our project page at <a target="_blank" rel="noopener" href="https://langbridge.github.io/">https://LangBridge.github.io/</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨å„ç§å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚éµå¾ªLLaVAçš„æ¨¡å¼ï¼Œä¸»æµçš„LVLMé€šå¸¸é‡‡ç”¨æµ…å±‚çš„MLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆæ˜¯è·¨æ¨¡æ€å¯¹é½çš„é¢„è®­ç»ƒï¼Œç„¶åæ˜¯æŒ‡ä»¤å¾®è°ƒã€‚å°½ç®¡è¿™ç§æ–¹æ³•å·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†MLPå¦‚ä½•å¼¥åˆæ¨¡æ€å·®è·çš„åŸºç¡€æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚è™½ç„¶æœ‰ä¸€äº›ç ”ç©¶æ¢è®¨äº†LLMå¦‚ä½•å¤„ç†è½¬æ¢åçš„è§†è§‰ä»¤ç‰Œï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶æ¢è®¨åŸºæœ¬çš„å¯¹é½æœºåˆ¶ã€‚æ­¤å¤–ï¼ŒMLPé€‚é…å™¨åœ¨åˆ‡æ¢LLMä¸»å¹²æ—¶éœ€è¦é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆè°ƒæŸ¥äº†MLPé€‚é…å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶å‘ç°å®ƒä»¬å­¦ä¹ å°†è§†è§‰åµŒå…¥é€æ­¥æŠ•å½±åˆ°ç”±ç›¸åº”æ–‡æœ¬åµŒå…¥æ‰€å¼ æˆçš„å­ç©ºé—´ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†LangBridgeï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é€‚é…å™¨ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å°†è§†è§‰ä»¤ç‰Œæ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥çš„çº¿æ€§ç»„åˆã€‚è¿™ä¸€åˆ›æ–°è®¾è®¡å®ç°äº†è·¨ä¸åŒLLMçš„æ— é¢„è®­ç»ƒé€‚é…å™¨è½¬ç§»ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Qwen2-0.5Bä¸Šé¢„è®­ç»ƒçš„LangBridgeé€‚é…å™¨å¯ä»¥ç›´æ¥åº”ç”¨äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œå¦‚LLaMA3-8Bæˆ–Qwen2.5-14Bï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒLangBridgeé€šè¿‡ä»¥LLMè¯æ±‡åµŒå…¥ä¸ºåŸºç¡€å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œä½¿å…¶å…·æœ‰å¯è§£é‡Šæ€§ï¼Œè€Œå…¶å³æ’å³ç”¨è®¾è®¡ç¡®ä¿äº†åœ¨å¤šä¸ªLLMä¹‹é—´é«˜æ•ˆé‡ç”¨ï¼Œå‡ ä¹æ²¡æœ‰ä»»ä½•æ€§èƒ½ä¸‹é™ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://langbridge.github.io/">LangBridge.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19404v1">PDF</a> The code and weights will be open-sourced. Project page:   <a target="_blank" rel="noopener" href="https://langbridge.github.io/">https://LangBridge.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’Œè¯­è¨€å¯¹é½æ–¹é¢çš„çªç ´ã€‚è™½ç„¶ä¸»æµçš„LVLMsé‡‡ç”¨æµ…å±‚MLPè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LangBridgeé€‚é…å™¨ï¼Œå®ƒèƒ½å¤Ÿæ˜¾å¼åœ°å°†è§†è§‰ä»¤ç‰Œæ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥çš„çº¿æ€§ç»„åˆä¸­ï¼Œä»è€Œå®ç°æ— éœ€é¢„è®­ç»ƒçš„è·¨ä¸åŒLLMæ¨¡å‹çš„é€‚é…å™¨è½¬ç§»ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLangBridgeé€‚é…å™¨å¯ä»¥åœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ç›´æ¥åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚æ€»ä¹‹ï¼ŒLangBridgeæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„è§†è§‰è¯­è¨€å¯¹é½æ–¹æ³•ï¼ŒåŒæ—¶é€šè¿‡å…¶å³æ’å³ç”¨è®¾è®¡å®ç°äº†åœ¨å¤šä¸ªLLMæ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆå¤ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰å’Œè¯­è¨€å¯¹é½æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œè¾¾åˆ°äººç±»æ°´å¹³æ€§èƒ½ã€‚</li>
<li>ç›®å‰ä¸»æµçš„LVLMsä½¿ç”¨æµ…å±‚MLPè¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œä½†å¯¹å…¶æœºåˆ¶çš„ç†è§£ä»ç„¶æœ‰é™ã€‚</li>
<li>LangBridgeé€‚é…å™¨èƒ½å¤Ÿæ˜¾å¼åœ°å°†è§†è§‰ä»¤ç‰Œæ˜ å°„åˆ°LLMè¯æ±‡åµŒå…¥çš„çº¿æ€§ç»„åˆä¸­ï¼Œä»è€Œå®ç°è·¨ä¸åŒLLMæ¨¡å‹çš„é€‚é…å™¨è½¬ç§»ã€‚</li>
<li>LangBridgeé€‚é…å™¨æ— éœ€é¢„è®­ç»ƒï¼Œå¯ç›´æ¥åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œå¹¶ä¿æŒç«äº‰åŠ›ã€‚</li>
<li>LangBridgeæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„è§†è§‰è¯­è¨€å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡å…¶è®¾è®¡å®ç°äº†åœ¨å¤šä¸ªLLMæ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆå¤ç”¨ã€‚</li>
<li>LangBridgeé€‚é…å™¨èƒ½å¤Ÿå®ç°å³æ’å³ç”¨ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b78c4045fe28fda8a273c874f6a396a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6acd40e5c8d2c484b3250b5de22871b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd0df86edf04907f3a27458febd4eb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e73028dce61d7fcaa7b6d6462807a9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models"><a href="#ST-VLM-Kinematic-Instruction-Tuning-for-Spatio-Temporal-Reasoning-in-Vision-Language-Models" class="headerlink" title="ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models"></a>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in   Vision-Language Models</h2><p><strong>Authors:Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim</strong></p>
<p>Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: <a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>. </p>
<blockquote>
<p>æ—¶ç©ºæ¨ç†åœ¨ç†è§£å„ä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œè¿åŠ¨åˆ†æï¼‰ä¸­è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„è¿›æ­¥é€šè¿‡å¼•å…¥å¤§è§„æ¨¡æ•°æ®æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶éš¾ä»¥åˆ†æè¿åŠ¨ç‰©ä½“çš„è¿åŠ¨å­¦å…ƒç´ ï¼Œå¦‚è¡Œè¿›è·ç¦»å’Œé€Ÿåº¦ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†æ¶‰åŠè¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºSTKitå’ŒSTKit-Benchã€‚å®ƒä»¬åŒ…å«å¸¦æœ‰ä¸‰ç»´æ³¨é‡Šçš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œè¯¦ç»†æè¿°äº†ç‰©ä½“çš„è¿åŠ¨åŠ¨æ€ï¼šè¡Œè¿›è·ç¦»ã€é€Ÿåº¦ã€è¿åŠ¨æ–¹å‘ã€ç‰©ä½“ä¹‹é—´çš„è·ç¦»æ¯”è¾ƒå’Œç›¸å¯¹è¿åŠ¨æ–¹å‘ã€‚ä¸ºäº†å°†æ­¤ç±»æ•°æ®æ„å»ºè¿›ä¸€æ­¥æ‰©å±•åˆ°æ²¡æœ‰ä¸‰ç»´æ ‡ç­¾çš„è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨çœŸå®ä¸–ç•Œè§„æ¨¡çš„å››ç»´é‡å»ºæ¥ç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„æ—¶ç©ºæ¨ç†è¿åŠ¨å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ST-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºæ—¶ç©ºæ¨ç†çš„VLMï¼Œåœ¨STKit-Benchä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ST-VLMåœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–æ—¶ç©ºåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚ActivityNetã€TVQA+ï¼‰ä¸Šä¼˜äºåŸºçº¿ã€‚æœ€åï¼Œé€šè¿‡æ•´åˆä¹ å¾—çš„æ—¶ç©ºæ¨ç†ä¸ç°æœ‰èƒ½åŠ›ï¼ŒST-VLMèƒ½å¤Ÿå®ç°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM%E3%80%82">https://ikodoh.github.io/ST-VLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19355v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†æ—¶ç©ºæ¨ç†åœ¨ç†è§£çœŸå®ä¸–ç•Œç¯å¢ƒçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚ä¸ºäº†æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®ï¼Œä½†ä»å­˜åœ¨å¯¹è¿åŠ¨ç‰©ä½“çš„è·ç¦»å’Œé€Ÿåº¦ç­‰åŠ¨åŠ›å­¦å…ƒç´ çš„åˆ†æå›°éš¾ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†åŒ…å«è¿åŠ¨ç‰©ä½“åŠ¨åŠ›å­¦è¯¦ç»†ä¿¡æ¯çš„æ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°â€”â€”STKitå’ŒSTKit-Benchã€‚åŒæ—¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç®¡é“æŠ€æœ¯ï¼Œå¯ä¸ºæ— çœŸå®ä¸‰ç»´æ ‡ç­¾çš„è§†é¢‘ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚å¦å¤–æå‡ºä¸€ä¸ªæ—¶ç©ºå¢å¼ºè¯­è¨€æ¨¡å‹ST-VLMï¼Œå…¶åœ¨STKit-Benchä¸Šçš„è¡¨ç°ä¼˜ç§€å¹¶è¯æ˜äº†å…¶è·¨é¢†åŸŸè·¨ä»»åŠ¡çš„é²æ£’æ€§ï¼Œè€Œä¸”èƒ½å¤Ÿå®ç°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†åŠŸèƒ½ã€‚å…³äºå…¶ç›¸å…³ä¿¡æ¯å¯é€šè¿‡å®˜ç½‘æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://ikodoh.github.io/ST-VLM">https://ikodoh.github.io/ST-VLM</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>æ—¶ç©ºæ¨ç†å¯¹äºç†è§£çœŸå®ä¸–ç•Œç¯å¢ƒçš„é‡è¦æ€§å¾—åˆ°äº†å¼ºè°ƒï¼Œå°¤å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œä½“è‚²åˆ†æç­‰é¢†åŸŸã€‚</li>
<li>å°½ç®¡å¼•å…¥äº†å¤§è§„æ¨¡æ•°æ®æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹è¿åŠ¨ç‰©ä½“çš„åŠ¨åŠ›å­¦åˆ†æä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªæ—¶ç©ºæ¨ç†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°â€”â€”STKitå’ŒSTKit-Benchï¼Œå…¶ä¸­åŒ…å«ç°å®ä¸–ç•Œçš„è§†é¢‘ä¸ä¸‰ç»´æ³¨é‡Šä¿¡æ¯ï¼Œæè¿°äº†ç‰©ä½“çš„è¿åŠ¨åŠ¨åŠ›å­¦ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹æ— çœŸå®ä¸‰ç»´æ ‡ç­¾çš„è§†é¢‘ç”Ÿæˆä¼ªæ ‡ç­¾çš„è‡ªåŠ¨ç®¡é“æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯çš„åº”ç”¨ç®€åŒ–äº†æ—¶ç©ºæ¨ç†æ•°æ®çš„å»ºè®¾æµç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d05948da4e5493c49a016f2f5cf526f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8e0fc995603c14e9dbb59bec545fd16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c084e9998c5bd21ee62e3b8ab498e4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a83fee94008c4eb63eb5913ba6b6fdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf26d67f83df6c8c93a5c2dc429dcce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32a68729a67a41a12a06f038493170e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2baa39b34006ef12edd50ab6a02d4a0d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Structuring-Scientific-Innovation-A-Framework-for-Modeling-and-Discovering-Impactful-Knowledge-Combinations"><a href="#Structuring-Scientific-Innovation-A-Framework-for-Modeling-and-Discovering-Impactful-Knowledge-Combinations" class="headerlink" title="Structuring Scientific Innovation: A Framework for Modeling and   Discovering Impactful Knowledge Combinations"></a>Structuring Scientific Innovation: A Framework for Modeling and   Discovering Impactful Knowledge Combinations</h2><p><strong>Authors:Junlan Chen, Kexin Zhang, Daifeng Li, Yangyang Feng, Yuxuan Zhang, Bowen Deng</strong></p>
<p>The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unitâ€“especially those tied to methodological designâ€“can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ä¸ºç§‘å­¦çŸ¥è¯†çš„ç»“æ„åŒ–æ¢ç´¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬å¹¶ä¸å°†ç§‘å­¦å‘ç°è§†ä¸ºå­¤ç«‹çš„æ€æƒ³æˆ–å†…å®¹ï¼Œè€Œæ˜¯æå‡ºä¸€ç§ç»“æ„åŒ–æ–¹æ³•ï¼Œå¼ºè°ƒæ–¹æ³•ç»„åˆåœ¨å½¢æˆçªç ´æ€§è§è§£ä¸­çš„ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•å¯¹çŸ¥è¯†å•å…ƒâ€”â€”ç‰¹åˆ«æ˜¯é‚£äº›ä¸æ–¹æ³•è®ºè®¾è®¡ç›¸å…³çš„çŸ¥è¯†å•å…ƒâ€”â€”è¿›è¡Œå»ºæ¨¡å’Œé‡ç»„ï¼Œä»¥äº§ç”Ÿç ”ç©¶çªç ´ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æœºåˆ¶ï¼Œä»¥è¯†åˆ«å‡ºåœ¨é—®é¢˜é©±åŠ¨èƒŒæ™¯ä¸‹å†å²ä¸Šå…·æœ‰ç ´åæ€§æ–¹æ³•ç»„åˆçš„åŒºåˆ«ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†çš„è’™ç‰¹å¡æ´›æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€ç»´é“¾èƒ½åŠ›ï¼Œä¸ºæ–°çš„å‘½é¢˜é—®é¢˜ç¡®å®šæœ‰å‰é€”çš„çŸ¥è¯†é‡ç»„ã€‚åœ¨å¤šé¢†åŸŸçš„å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ¨¡æ‹Ÿåˆ›æ–°çš„ç»“æ„åŠ¨æ€ï¼Œå¹¶æˆåŠŸçªå‡ºå…·æœ‰ç ´åæ€§æ½œåŠ›çš„ç»„åˆã€‚è¿™é¡¹ç ”ç©¶ä¸ºåŸºäºç»“æ„åŒ–æ¨ç†å’Œå†å²æ•°æ®å»ºæ¨¡çš„è®¡ç®—å¼•å¯¼çš„ç§‘å­¦æ„æ€æä¾›äº†ä¸€æ¡æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18865v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ä¸ºç§‘å­¦çŸ¥è¯†çš„ç»“æ„åŒ–æ¢ç´¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ–¹æ³•ï¼Œå¼ºè°ƒæ–¹æ³•è®ºç»„åˆåœ¨å½¢æˆçªç ´æ€§è§è§£ä¸­çš„ä½œç”¨ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å»ºæ¨¡å’Œé‡ç»„çŸ¥è¯†å•å…ƒï¼ˆç‰¹åˆ«æ˜¯ä¸æ–¹æ³•è®ºè®¾è®¡ç›¸å…³çš„çŸ¥è¯†å•å…ƒï¼‰ä»¥å®ç°ç ”ç©¶çªç ´ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯å¼•å…¥å¯¹æ¯”å­¦ä¹ æœºåˆ¶æ¥è¯†åˆ«é—®é¢˜é©±åŠ¨èƒŒæ™¯ä¸‹å…·æœ‰ç ´åæ€§çš„æ–¹æ³•è®ºç»„åˆçš„åŒºåˆ†ç‰¹å¾ï¼›äºŒæ˜¯æå‡ºä¸€ç§åŸºäºæ¨ç†å¼•å¯¼Monte Carloæœç´¢ç®—æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ¥è¯†åˆ«æ–°é—®é¢˜çš„çŸ¥è¯†é‡ç»„å¯èƒ½æ€§ã€‚å®è¯ç ”ç©¶è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å»ºæ¨¡åˆ›æ–°ç»“æ„åŠ¨æ€ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶ä¸ºè®¡ç®—å¼•å¯¼çš„ç§‘å­¦æ„æ€æä¾›äº†ä¸€æ¡æ–°è·¯ï¼ŒåŸºäºç»“æ„åŒ–æ¨ç†å’Œå†å²æ•°æ®å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºç§‘å­¦çŸ¥è¯†çš„ç»“æ„åŒ–æ¢ç´¢æä¾›äº†æ–°æœºä¼šã€‚</li>
<li>å¼ºè°ƒæ–¹æ³•è®ºç»„åˆåœ¨å½¢æˆçªç ´æ€§è§è§£ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡å»ºæ¨¡å’Œé‡ç»„çŸ¥è¯†å•å…ƒï¼ˆç‰¹åˆ«æ˜¯æ–¹æ³•è®ºè®¾è®¡ç›¸å…³çš„ï¼‰å®ç°ç ”ç©¶çªç ´ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”å­¦ä¹ æœºåˆ¶æ¥è¯†åˆ«ç ´åæ€§æ–¹æ³•è®ºç»„åˆçš„åŒºåˆ†ç‰¹å¾ã€‚</li>
<li>æå‡ºåŸºäºæ¨ç†å¼•å¯¼çš„Monte Carloæœç´¢ç®—æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹è¯†åˆ«çŸ¥è¯†é‡ç»„çš„å¯èƒ½æ€§ã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜äº†æ¡†æ¶åœ¨å»ºæ¨¡åˆ›æ–°ç»“æ„åŠ¨æ€ä¸Šçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ecccb33af12c83d5f62d2c7e7aca421.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model"><a href="#MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model" class="headerlink" title="MC-LLaVA: Multi-Concept Personalized Vision-Language Model"></a>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</h2><p><strong>Authors:Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang</strong></p>
<p>Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies investigate VLM personalization to understand user-provided concepts. However, they mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes the first multi-concept personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the costs related to joint training, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location confidence maps for enhanced recognition and grounding capabilities. To advance multi-concept personalization research, we further contribute a high-quality instruction tuning dataset. We carefully collect images with multiple characters and objects from movies and manually generate question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive qualitative and quantitative experiments demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA%7D">https://github.com/arctanxarc/MC-LLaVA}</a>. </p>
<blockquote>
<p>å½“å‰çš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰é—®ç­”ã€‚ä¸ºäº†æå‡ç”¨æˆ·ä½“éªŒï¼Œè¿‘æœŸçš„ç ”ç©¶å¼€å§‹æ¢ç´¢VLMä¸ªæ€§åŒ–ï¼Œä»¥ç†è§£ç”¨æˆ·æä¾›çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¦‚å¿µçš„ä¸ªæ€§åŒ–ä¸Šï¼Œå¿½ç•¥äº†å¤šä¸ªæ¦‚å¿µçš„å­˜åœ¨å’Œç›¸äº’ä½œç”¨ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤šæ¦‚å¿µä¸ªæ€§åŒ–èŒƒå¼MC-LLaVAã€‚å…·ä½“æ¥è¯´ï¼ŒMC-LLaVAé‡‡ç”¨å¤šæ¦‚å¿µæŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼Œåœ¨å•ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æœ‰æ•ˆåœ°é›†æˆå¤šä¸ªæ¦‚å¿µã€‚ä¸ºäº†é™ä½è”åˆè®­ç»ƒçš„ç›¸å…³æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–çš„æ–‡æœ¬æç¤ºï¼Œä½¿ç”¨è§†è§‰ä»¤ç‰Œä¿¡æ¯æ¥åˆå§‹åŒ–æ¦‚å¿µä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸ªæ€§åŒ–çš„è§†è§‰æç¤ºï¼Œèšåˆä½ç½®ç½®ä¿¡å›¾ä»¥å¢å¼ºè¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚ä¸ºäº†æ¨åŠ¨å¤šæ¦‚å¿µä¸ªæ€§åŒ–ç ”ç©¶çš„å‘å±•ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚æˆ‘ä»¬ä»ç”µå½±ä¸­ç²¾å¿ƒæ”¶é›†äº†å«æœ‰å¤šä¸ªè§’è‰²å’Œå¯¹è±¡çš„å›¾åƒï¼Œå¹¶æ‰‹åŠ¨ç”Ÿæˆé’ˆå¯¹å¤šæ¦‚å¿µåœºæ™¯çš„é—®é¢˜ç­”æ¡ˆæ ·æœ¬ï¼Œå…·æœ‰å‡ºè‰²çš„å¤šæ ·æ€§ã€‚ç»¼åˆçš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒMC-LLaVAå¯ä»¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–å“åº”ï¼Œä¸ºVLMsæˆä¸ºæ›´å¥½çš„ç”¨æˆ·ç‰¹å®šåŠ©æ‰‹é“ºå¹³äº†é“è·¯ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/arctanxarc/MC-LLaVAä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18854v2">PDF</a> I sincerely apologize for any inconvenience caused. We actually   uploaded this paper to arXiv in November 2024, as arXiv:2411.11706. During   this update, we did not consider the replacement operation of arXiv, which   led to duplicate submissions. We have made modifications at the original   address arXiv:2411.11706</p>
<p><strong>Summary</strong><br>å¤šæ¦‚å¿µä¸ªæ€§åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç”¨æˆ·ä¸ªæ€§åŒ–æ¦‚å¿µæ—¶çš„å±€é™æ€§ï¼Œæå‡ºé¦–ä¸ªå¤šæ¦‚å¿µä¸ªæ€§åŒ–èŒƒå¼MC-LLaVAã€‚é‡‡ç”¨å¤šæ¦‚å¿µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œåœ¨å•ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æœ‰æ•ˆæ•´åˆå¤šä¸ªæ¦‚å¿µã€‚åˆ©ç”¨è§†è§‰ä»¤ç‰Œä¿¡æ¯åˆå§‹åŒ–æ¦‚å¿µä»¤ç‰Œçš„ä¸ªäººåŒ–æ–‡æœ¬æç¤ºï¼Œä»¥åŠåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸ªæ€§åŒ–è§†è§‰æç¤ºï¼Œæé«˜è¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚ä¸ºæ¨è¿›å¤šæ¦‚å¿µä¸ªæ€§åŒ–ç ”ç©¶ï¼Œè´¡çŒ®é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒåŒ…å«ç”µå½±ä¸­çš„å¤šè§’è‰²å’Œç‰©ä½“å›¾åƒï¼Œä¸ºå¤šå…ƒåœºæ™¯æä¾›é—®ç­”æ ·æœ¬ã€‚å®éªŒè¯æ˜MC-LLaVAå¯å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–å“åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰é—®ç­”ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•ä¸€æ¦‚å¿µä¸ªæ€§åŒ–ï¼Œå¿½ç•¥äº†å¤šæ¦‚å¿µçš„å­˜åœ¨å’Œç›¸äº’ä½œç”¨ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºé¦–ä¸ªå¤šæ¦‚å¿µä¸ªæ€§åŒ–èŒƒå¼MC-LLaVAï¼Œèƒ½å¤Ÿæ•´åˆå¤šä¸ªæ¦‚å¿µã€‚</li>
<li>MC-LLaVAé‡‡ç”¨å¤šæ¦‚å¿µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œåœ¨å•ä¸€è®­ç»ƒæ­¥éª¤ä¸­æ•´åˆå¤šä¸ªæ¦‚å¿µã€‚</li>
<li>åˆ©ç”¨è§†è§‰ä»¤ç‰Œä¿¡æ¯æ¥åˆå§‹åŒ–æ¦‚å¿µä»¤ç‰Œçš„ä¸ªäººåŒ–æ–‡æœ¬æç¤ºï¼Œé™ä½è”åˆè®­ç»ƒçš„æˆæœ¬ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸ªæ€§åŒ–è§†è§‰æç¤ºï¼Œæé«˜æ¨¡å‹çš„è¯†åˆ«å’Œå®šä½èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2d6a3aff734c48c5cc1c8d280d667ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2075661619ba41019437cf247b33ef23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a0842e0a2d456dbf4d45bf98d6195d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3030174a11a73b336e1ac75622104458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a138bc677a822b68424c8e7ee15f6052.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a8a62d63fe32f510729dfeebedf4e24.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SKDU-at-De-Factify-4-0-Vision-Transformer-with-Data-Augmentation-for-AI-Generated-Image-Detection"><a href="#SKDU-at-De-Factify-4-0-Vision-Transformer-with-Data-Augmentation-for-AI-Generated-Image-Detection" class="headerlink" title="SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for   AI-Generated Image Detection"></a>SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for   AI-Generated Image Detection</h2><p><strong>Authors:Shrikant Malviya, Neelanjan Bhowmik, Stamos Katsigiannis</strong></p>
<p>The aim of this work is to explore the potential of pre-trained vision-language models, e.g. Vision Transformers (ViT), enhanced with advanced data augmentation strategies for the detection of AI-generated images. Our approach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset, which includes images generated by state-of-the-art models such as Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and MidJourney. We employ perturbation techniques like flipping, rotation, Gaussian noise injection, and JPEG compression during training to improve model robustness and generalisation. The experimental results demonstrate that our ViT-based pipeline achieves state-of-the-art performance, significantly outperforming competing methods on both validation and test datasets. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨æ¢ç´¢é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Vision Transformersï¼ˆViTï¼‰ï¼‰çš„æ½œåŠ›ï¼Œé€šè¿‡é‡‡ç”¨å…ˆè¿›çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åœ¨Defactify-4.0æ•°æ®é›†ä¸Šå¾®è°ƒè¿‡çš„ViTæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±æœ€æ–°æ¨¡å‹ï¼ˆå¦‚Stable Diffusion 2.1ã€Stable Diffusion XLã€Stable Diffusion 3ã€DALL-E 3å’ŒMidJourneyï¼‰ç”Ÿæˆçš„å›¾åƒã€‚æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨æ‰°åŠ¨æŠ€æœ¯ï¼Œå¦‚ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºViTçš„ç®¡é“å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šéƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18812v1">PDF</a> De-Factify 4.0 workshop at the 39th Annual AAAI Conference on   Artificial Intelligence (AAAI 2025)</p>
<p><strong>Summary</strong>:</p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚è§†è§‰è½¬æ¢å™¨ï¼ŒViTï¼‰ä¸å…ˆè¿›çš„æ•°æ®å¢å¼ºç­–ç•¥ç›¸ç»“åˆï¼Œç”¨äºæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒã€‚ç ”ç©¶é‡‡ç”¨å¾®è°ƒè¿‡çš„ViTæ¨¡å‹ï¼Œåœ¨Defactify-4.0æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±æœ€æ–°æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚é€šè¿‡æ‰°åŠ¨æŠ€æœ¯å¦‚ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ç­‰æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºViTçš„ç®¡é“è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨æ¢ç´¢é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ViTï¼‰åœ¨æ£€æµ‹AIç”Ÿæˆå›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨Defactify-4.0æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«å¤šç§æœ€æ–°æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¦‚ç¿»è½¬ã€æ—‹è½¬ã€é«˜æ–¯å™ªå£°æ³¨å…¥å’ŒJPEGå‹ç¼©ç­‰ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºViTçš„ç®¡é“è®¾è®¡å®ç°äº†å¯¹AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ­¤é¡¹ç ”ç©¶å±•ç¤ºäº†ç»“åˆé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å’Œå…ˆè¿›æ•°æ®å¢å¼ºç­–ç•¥åœ¨å›¾åƒæ£€æµ‹é¢†åŸŸçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b7da089d458f7765eedc93d9a9134a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0ba8b816af1da84b786f18bd158d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e532019f587a00554c84bb49e23123e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e996881c98831bff97fcc740073fbbcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9556c486bff7e77f49cb364683070cb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformer-based-Ranking-Approaches-for-Keyword-Queries-over-Relational-Databases"><a href="#Transformer-based-Ranking-Approaches-for-Keyword-Queries-over-Relational-Databases" class="headerlink" title="Transformer-based Ranking Approaches for Keyword Queries over Relational   Databases"></a>Transformer-based Ranking Approaches for Keyword Queries over Relational   Databases</h2><p><strong>Authors:Paulo Martins, Altigran da Silva, Johny Moreira, Edleno de Moura</strong></p>
<p>Relational Keyword Search (R-KwS) systems enable naive&#x2F;informal users to explore and retrieve information from relational databases without requiring schema knowledge or query-language proficiency. Although numerous R-KwS methods have been proposed, most still focus on queries referring only to attribute values or primarily address performance enhancements, providing limited support for queries referencing schema elements. We previously introduced Lathe, a system that accommodates schema-based keyword queries and employs an eager CJN evaluation strategy to filter out spurious Candidate Joining Networks (CJNs). However, Lathe still faces challenges in accurately ranking CJNs when queries are ambiguous. In this work, we propose a new transformer-based ranking approach that provides a more context-aware evaluation of Query Matches (QMs) and CJNs. Our solution introduces a linearization process to convert relational structures into textual sequences suitable for transformer models. It also includes a data augmentation strategy aimed at handling diverse and ambiguous queries more effectively. Experimental results, comparing our transformer-based ranking to Latheâ€™s original Bayesian-based method, show significant improvements in recall and R@k, demonstrating the effectiveness of our neural approach in delivering the most relevant query results. </p>
<blockquote>
<p>å…³ç³»å…³é”®è¯æœç´¢ï¼ˆR-KwSï¼‰ç³»ç»Ÿä½¿å¤©çœŸ&#x2F;éæ­£å¼ç”¨æˆ·èƒ½å¤Ÿæ¢ç´¢å¹¶ä»ä¸ç†Ÿæ‚‰çš„å…³ç³»æ•°æ®åº“ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œæ— éœ€å…·å¤‡æ¨¡å¼çŸ¥è¯†æˆ–æŸ¥è¯¢è¯­è¨€ç†Ÿç»ƒåº¦ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šR-KwSæ–¹æ³•ï¼Œä½†å¤§å¤šæ•°ä»ç„¶é›†ä¸­åœ¨æŸ¥è¯¢ä»…æ¶‰åŠå±æ€§å€¼çš„æŸ¥è¯¢æˆ–ä¸»è¦å…³æ³¨æ€§èƒ½æå‡æ–¹é¢ï¼Œå¯¹äºå¼•ç”¨æ¨¡å¼å…ƒç´ çš„æŸ¥è¯¢æ”¯æŒæœ‰é™ã€‚æˆ‘ä»¬ä¹‹å‰å¼•å…¥äº†Latheç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯å®¹çº³åŸºäºæ¨¡å¼çš„å…³é”®è¯æŸ¥è¯¢ï¼Œå¹¶é‡‡ç”¨ç§¯æçš„CJNè¯„ä¼°ç­–ç•¥æ¥è¿‡æ»¤æ‰è™šå‡çš„å€™é€‰è¿æ¥ç½‘ç»œï¼ˆCJNsï¼‰ã€‚ç„¶è€Œï¼Œå½“æŸ¥è¯¢ä¸æ˜ç¡®æ—¶ï¼ŒLatheåœ¨å‡†ç¡®æ’åCJNsæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè½¬æ¢å™¨çš„æ’åæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æä¾›äº†å¯¹æŸ¥è¯¢åŒ¹é…ï¼ˆQMsï¼‰å’ŒCJNsçš„æ›´ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯„ä¼°ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸€ä¸ªçº¿æ€§åŒ–è¿‡ç¨‹ï¼Œå°†å…³ç³»ç»“æ„è½¬æ¢ä¸ºé€‚åˆè½¬æ¢å™¨æ¨¡å‹çš„æ–‡æœ¬åºåˆ—ã€‚è¿˜åŒ…æ‹¬ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°å¤„ç†å¤šæ ·åŒ–å’Œæ¨¡ç³Šçš„æŸ¥è¯¢ã€‚å°†æˆ‘ä»¬çš„åŸºäºè½¬æ¢å™¨çš„æ’åæ–¹æ³•ä¸Latheçš„åŸå§‹åŸºäºè´å¶æ–¯çš„æ’åæ–¹æ³•è¿›è¡Œå¯¹æ¯”çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¬å›ç‡å’ŒR@kæ–¹é¢æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨æä¾›æœ€ç›¸å…³çš„æŸ¥è¯¢ç»“æœæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18768v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…³ç³»å…³é”®è¯æœç´¢ï¼ˆR-KwSï¼‰ç³»ç»Ÿä½¿éä¸“ä¸šç”¨æˆ·èƒ½å¤Ÿæ¢ç´¢å¹¶æ£€ç´¢å…³ç³»æ•°æ®åº“ä¸­çš„ä¿¡æ¯ï¼Œè€Œæ— éœ€äº†è§£æ¨¡å¼çŸ¥è¯†æˆ–æŸ¥è¯¢è¯­è¨€ç†Ÿç»ƒåº¦ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šR-KwSæ–¹æ³•ï¼Œä½†å®ƒä»¬å¤§å¤šæ•°ä»ç„¶åªå…³æ³¨å±æ€§å€¼çš„æŸ¥è¯¢æˆ–ä¸»è¦å…³æ³¨æ€§èƒ½æå‡ï¼Œå¯¹äºæŸ¥è¯¢å¼•ç”¨æ¨¡å¼å…ƒç´ çš„æ”¯æŒæœ‰é™ã€‚å…ˆå‰æˆ‘ä»¬å¼•å…¥äº†Latheç³»ç»Ÿï¼Œå®ƒå®¹çº³åŸºäºæ¨¡å¼çš„å…³é”®è¯æŸ¥è¯¢ï¼Œå¹¶é‡‡ç”¨ç§¯æçš„CJNè¯„ä¼°ç­–ç•¥è¿‡æ»¤æ‰è™šå‡çš„Candidate Joining Networksï¼ˆCJNsï¼‰ã€‚ç„¶è€Œï¼ŒLatheåœ¨æŸ¥è¯¢æ¨¡ç³Šæ—¶å‡†ç¡®æ’åCJNæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„æ’åæ–¹æ³•ï¼Œå®ƒä¸ºQuery Matchesï¼ˆQMï¼‰å’ŒCJNsæä¾›æ›´ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸€ä¸ªçº¿æ€§åŒ–è¿‡ç¨‹ï¼Œå°†å…³ç³»ç»“æ„è½¬æ¢ä¸ºé€‚åˆtransformeræ¨¡å‹çš„æ–‡æœ¬åºåˆ—ã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°å¤„ç†å¤šæ ·åŒ–å’Œæ¨¡ç³ŠæŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Latheçš„åŸå§‹è´å¶æ–¯æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŸºäºtransformerçš„æ’åæ–¹æ³•åœ¨å¬å›ç‡å’ŒR@kæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨æä¾›æœ€ç›¸å…³æŸ¥è¯¢ç»“æœæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…³ç³»å…³é”®è¯æœç´¢ï¼ˆR-KwSï¼‰ç³»ç»Ÿå…è®¸éä¸“ä¸šç”¨æˆ·æŸ¥è¯¢å…³ç³»æ•°æ®åº“ï¼Œæ— éœ€äº†è§£æ¨¡å¼å’ŒæŸ¥è¯¢è¯­è¨€æŠ€èƒ½ã€‚</li>
<li>å½“å‰R-KwSæ–¹æ³•ä¸»è¦å…³æ³¨å±æ€§å€¼æŸ¥è¯¢å’Œæ€§èƒ½æå‡ï¼Œå¯¹æ¨¡å¼å…ƒç´ æŸ¥è¯¢çš„æ”¯æŒæœ‰é™ã€‚</li>
<li>Latheç³»ç»Ÿèƒ½å¤Ÿå¤„ç†åŸºäºæ¨¡å¼çš„å…³é”®è¯æŸ¥è¯¢ï¼Œä½†åœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢æ—¶CJNsçš„å‡†ç¡®æ’åå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºtransformerçš„æ’åæ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§åŒ–è¿‡ç¨‹å°†å…³ç³»ç»“æ„è½¬åŒ–ä¸ºæ–‡æœ¬åºåˆ—ï¼Œä¸ºQuery Matchesï¼ˆQMï¼‰å’ŒCandidate Joining Networksï¼ˆCJNsï¼‰æä¾›æ›´ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯„ä¼°ã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆè¿˜åŒ…æ‹¬ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†å¤šæ ·åŒ–å’Œæ¨¡ç³Šçš„æŸ¥è¯¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Latheçš„è´å¶æ–¯æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºtransformerçš„æ’åæ–¹æ³•åœ¨å¬å›ç‡å’ŒR@kæ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨æä¾›æœ€ç›¸å…³æŸ¥è¯¢ç»“æœæ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c44955db5a871e9a19c59121da26fdb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb2d3837ceb875cc8b1fa18ed3ab8601.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  A Multi-Agent Framework Integrating Large Language Models and Generative   AI for Accelerated Metamaterial Design
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-067245d4a7665c70304e9a0c7c15c8da.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  ORION A Holistic End-to-End Autonomous Driving Framework by   Vision-Language Instructed Action Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16573k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
