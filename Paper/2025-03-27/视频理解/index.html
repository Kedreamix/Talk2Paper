<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Exploring Hallucination of Large Multimodal Models in Video   Understanding Benchmark, Analysis and Mitigation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="Exploring-Hallucination-of-Large-Multimodal-Models-in-Video-Understanding-Benchmark-Analysis-and-Mitigation"><a href="#Exploring-Hallucination-of-Large-Multimodal-Models-in-Video-Understanding-Benchmark-Analysis-and-Mitigation" class="headerlink" title="Exploring Hallucination of Large Multimodal Models in Video   Understanding: Benchmark, Analysis and Mitigation"></a>Exploring Hallucination of Large Multimodal Models in Video   Understanding: Benchmark, Analysis and Mitigation</h2><p><strong>Authors:Hongcheng Gao, Jiashu Qu, Jingyi Tang, Baolong Bi, Yue Liu, Hongyu Chen, Li Liang, Li Su, Qingming Huang</strong></p>
<p>The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)â€“ where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at <a target="_blank" rel="noopener" href="https://github.com/Hongcheng-Gao/HAVEN">https://github.com/Hongcheng-Gao/HAVEN</a>. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å¹»è§‰é—®é¢˜ï¼Œå³æä¾›çœ‹ä¼¼æ­£ç¡®ä½†å®é™…é”™è¯¯çš„å›åº”ï¼Œé™åˆ¶äº†å…¶å¯é æ€§å’Œé€‚ç”¨æ€§ã€‚æœ¬æ–‡é’ˆå¯¹è§†é¢‘æ¨¡æ€çš„LMMå¹»è§‰é—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œä¸å›¾åƒå’Œæ–‡æœ¬ç­‰é™æ€æ¨¡æ€ç›¸æ¯”ï¼Œè§†é¢‘æ¨¡æ€æ›´åŠ åŠ¨æ€ä¸”æ›´å…·æŒ‘æˆ˜æ€§ã€‚åŸºäºè¿™ä¸€åŠ¨æœºï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªåä¸ºHAVENçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç†è§£ä»»åŠ¡ä¸­LMMçš„å¹»è§‰ã€‚å®ƒå»ºç«‹åœ¨å¹»è§‰åŸå› ã€å¹»è§‰æ–¹é¢å’Œé—®é¢˜æ ¼å¼ä¸‰ä¸ªç»´åº¦ä¸Šï¼Œå½¢æˆäº†6000ä¸ªé—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šå¯¹16ä¸ªLMMè¿›è¡Œå®éªŒï¼Œå®šé‡ç ”ç©¶äº†7ä¸ªå½±å“å¹»è§‰çš„å› ç´ ï¼Œä¾‹å¦‚è§†é¢‘çš„æŒç»­æ—¶é—´ã€æ¨¡å‹å¤§å°å’Œæ¨¡å‹æ¨ç†ç­‰ã€‚æ­¤å¤–ï¼Œå—åˆ°æœ€è¿‘æ€è€ƒæ¨¡å‹ï¼ˆå¦‚OpenAI o1ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†é¢‘æ€è€ƒæ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£æ¨ç†å¾®è°ƒï¼ˆSRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆTDPOï¼‰æ¥ç¼“è§£LMMçš„å¹»è§‰é—®é¢˜â€”â€”å…¶ä¸­SRFTå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œè€ŒTDPOå‡å°‘æ€è€ƒè¿‡ç¨‹ä¸­çš„å¹»è§‰ã€‚å¤§é‡å®éªŒå’Œåˆ†æè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å¹»è§‰è¯„ä¼°æ–¹é¢çš„å‡†ç¡®ç‡æé«˜äº†7.65%ï¼Œåè§åˆ†æ•°é™ä½äº†4.5%ã€‚ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hongcheng-Gao/HAVEN%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/Hongcheng-Gao/HAVENå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19622v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜ã€‚ä¸ºè¯„ä¼°LMMsåœ¨è§†é¢‘æ¨¡æ€çš„å¹»è§‰ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•HAVENã€‚é€šè¿‡å®éªŒç ”ç©¶ï¼Œå®šé‡åˆ†æäº†7ä¸ªå½±å“å¹»è§‰çš„å› ç´ ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§†é¢‘æ€è€ƒæ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£æ¨ç†å¾®è°ƒï¼ˆSRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆTDPOï¼‰æ¥ç¼“è§£LMMsçš„å¹»è§‰é—®é¢˜ã€‚è¿™æé«˜äº†åŸºå‡†æµ‹è¯•è¯„ä¼°å¹»è§‰çš„å‡†ç¡®æ€§ï¼Œå¹¶é™ä½äº†åè§åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œå½±å“å…¶å¯é æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
<li>å»ºç«‹äº†HAVENåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LMMsåœ¨è§†é¢‘æ¨¡æ€çš„å¹»è§‰è¡¨ç°ã€‚</li>
<li>HAVENåŸºå‡†æµ‹è¯•åŒ…å«ä¸‰ä¸ªç»´åº¦ï¼šå¹»è§‰åŸå› ã€å¹»è§‰æ–¹é¢å’Œé—®é¢˜æ ¼å¼ã€‚</li>
<li>é€šè¿‡å®éªŒå®šé‡ç ”ç©¶äº†7ä¸ªå½±å“å¹»è§‰çš„å› ç´ ï¼ŒåŒ…æ‹¬è§†é¢‘æŒç»­æ—¶é—´ã€æ¨¡å‹å¤§å°å’Œæ¨¡å‹æ¨ç†ç­‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è§†é¢‘æ€è€ƒæ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£æ¨ç†å¾®è°ƒï¼ˆSRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆTDPOï¼‰æ¥ç¼“è§£LMMsçš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>è§†é¢‘æ€è€ƒæ¨¡å‹åœ¨å¹»è§‰è¯„ä¼°æ–¹é¢æé«˜äº†åŸºçº¿å‡†ç¡®æ€§7.65%ï¼Œå¹¶é™ä½äº†åè§åˆ†æ•°4.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7c6306c0289fd3c5eff88a9c0c9a9c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e2695ec10a7b1885dc6bf03a531956c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e5f3a5d43ec18fc5a47b12f05399c7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf993f2ba3d600a5a1a453d16b7beb70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97bc500ace9e7b654d00d3178394f530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f857b2fd2070f7f87f251170ea7595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76059a9d9996ed8e49d827f8a9092c09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c46ef5a411418fad12b451497a99dd12.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding"><a href="#SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding" class="headerlink" title="SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding"></a>SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding</h2><p><strong>Authors:Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. This model family employs the two-stream SlowFast mechanism, enabling efficient modeling of long-range temporal context to meet the demand for lightweight, mobile-friendly Video LLMs. We provide models ranging from 1B to 7B parameters, optimized through a streamlined training pipeline and a high-quality data mixture composed of publicly available datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves competitive performance on a wide range of video and image benchmarks, with robust results across all model sizes. Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales (1B and 3B) across various video benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SlowFast-LLaVA-1.5ï¼ˆç®€ç§°SF-LLaVA-1.5ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„é«˜æ•ˆæ ‡è®°è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»åˆ—ã€‚è¯¥æ¨¡å‹ç³»åˆ—é‡‡ç”¨åŒæµSlowFastæœºåˆ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆå»ºæ¨¡é•¿æ—¶åºä¸Šä¸‹æ–‡ï¼Œæ»¡è¶³è½»é‡çº§ã€ç§»åŠ¨å‹å¥½çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹çš„éœ€æ±‚ã€‚æˆ‘ä»¬æä¾›äº†ä»1Båˆ°7Bå‚æ•°çš„æ¨¡å‹ï¼Œé€šè¿‡ç®€åŒ–çš„è®­ç»ƒç®¡é“å’Œç”±å…¬å¼€æ•°æ®é›†ç»„æˆçš„é«˜è´¨é‡æ•°æ®æ··åˆè¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-LLaVA-1.5åœ¨å¹¿æ³›çš„è§†é¢‘å’Œå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸”å„ç§æ¨¡å‹å¤§å°çš„ç»“æœéƒ½å¾ˆç¨³å¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSF-LLaVA-1.5åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆä¾‹å¦‚åœ¨LongVideoBenchå’ŒMLVUä¸Šï¼‰ï¼Œå¹¶åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­åœ¨å°è§„æ¨¡ï¼ˆ1Bå’Œ3Bï¼‰ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18943v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>SF-LLaVA-1.5ç³»åˆ—è§†é¢‘å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨ä¸¤æµå¿«æ…¢æœºåˆ¶ï¼Œå®ç°é«˜æ•ˆå»ºæ¨¡ï¼Œæ»¡è¶³è½»é‡çº§ç§»åŠ¨å‹å¥½å‹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„éœ€æ±‚ã€‚è¯¥æ¨¡å‹å®¶æ—å…·æœ‰å¤šç§å‚æ•°è§„æ¨¡ï¼Œä»1Båˆ°7Bä¸ç­‰ï¼Œä¼˜åŒ–è®­ç»ƒæµç¨‹å’Œé«˜è´¨é‡æ•°æ®æ··åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-LLaVA-1.5åœ¨è§†é¢‘å’Œå›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-LLaVA-1.5æ˜¯ä¸€ç§é’ˆå¯¹é•¿å½¢å¼è§†é¢‘ç†è§£çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹å®¶æ—ã€‚</li>
<li>é‡‡ç”¨ä¸¤æµå¿«æ…¢æœºåˆ¶ï¼Œå®ç°é«˜æ•ˆå»ºæ¨¡é•¿è·ç¦»æ—¶é—´ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ¨¡å‹ç³»åˆ—æä¾›ä»1Båˆ°7Bå‚æ•°çš„å¤šç§é€‰æ‹©ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–è®­ç»ƒæµç¨‹å’Œä½¿ç”¨é«˜è´¨é‡æ•°æ®æ··åˆæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¼ºåŠ²ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
<li>åœ¨ä¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸‹å‡è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a9b60ea403842ae4d2b3d44f4ab3755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85da2a7be139f5e8ab48b701a4b7ccad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8628e0afa40c6539ed32e41c52e319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5050ab2a7b818501e85769024de54621.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Video-XL-Pro-Reconstructive-Token-Compression-for-Extremely-Long-Video-Understanding"><a href="#Video-XL-Pro-Reconstructive-Token-Compression-for-Extremely-Long-Video-Understanding" class="headerlink" title="Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video   Understanding"></a>Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video   Understanding</h2><p><strong>Authors:Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao</strong></p>
<p>Despite advanced token compression techniques, existing multimodal large language models (MLLMs) still struggle with hour-long video understanding. In this work, we propose Video-XL-Pro, an efficient method for extremely long video understanding, built upon Reconstructive Compression of Tokens (ReCoT), a learnable module that leverages self-supervised learning to generate comprehensive and compact video tokens. ReCoT introduces two key components: (i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from static image tokens by learning intra-token relationships, which are then used in masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively masks redundant visual tokens to facilitate more effective reconstructive learning. To improve training efficiency in MLLMs fine-tuning, we introduce a video-specific dataset pruning strategy and design a simple yet Query-aware Selector that enables the model to precisely locate query-relevant video tokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models trained on larger datasets across multiple long video understanding benchmarks. Moreover, it can process over 8K frames on a single A100 GPU while maintaining high-quality performance. </p>
<blockquote>
<p>å°½ç®¡æœ‰å…ˆè¿›çš„ä»¤ç‰Œå‹ç¼©æŠ€æœ¯ï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Video-XL-Proï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä»¤ç‰Œé‡å»ºå‹ç¼©ï¼ˆReCoTï¼‰çš„æç«¯é•¿è§†é¢‘ç†è§£çš„æœ‰æ•ˆæ–¹æ³•ã€‚ReCoTæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æ¨¡å—ï¼Œå®ƒåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆå…¨é¢ä¸”ç´§å‡‘çš„è§†é¢‘ä»¤ç‰Œã€‚ReCoTå¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆiï¼‰åŠ¨æ€ä»¤ç‰Œåˆæˆå™¨ï¼ˆDTSï¼‰ï¼šDTSé€šè¿‡å­¦ä¹ ä»¤ç‰Œå†…éƒ¨å…³ç³»ä»é™æ€å›¾åƒä»¤ç‰Œç”Ÿæˆä¼ªè§†é¢‘ä»¤ç‰Œï¼Œç„¶åç”¨äºé®ç½©è§†é¢‘å»ºæ¨¡ã€‚ï¼ˆiiï¼‰è¯­ä¹‰å¼•å¯¼é®ç½©ï¼ˆSGMï¼‰ï¼šSGMè‡ªé€‚åº”åœ°é®ç½©å†—ä½™çš„è§†è§‰ä»¤ç‰Œï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„é‡å»ºå­¦ä¹ ã€‚ä¸ºäº†æé«˜MLLMå¾®è°ƒä¸­çš„è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§é’ˆå¯¹è§†é¢‘ç‰¹å®šçš„æ•°æ®é›†ä¿®å‰ªç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•ä½†æŸ¥è¯¢æ„ŸçŸ¥çš„é€‰æ‹©å™¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®å®šä½ä¸æŸ¥è¯¢ç›¸å…³çš„è§†é¢‘ä»¤ç‰Œã€‚ä»…ä½¿ç”¨3Bå‚æ•°ï¼ŒVideo-XL-Proåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¿‡å¤§å¤šæ•°åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„7Bæ¨¡å‹çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥åœ¨å•ä¸ªA100 GPUä¸Šå¤„ç†è¶…è¿‡8Kå¸§çš„åŒæ—¶ä¿æŒé«˜è´¨é‡çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–è§£é‡Š</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Video-XL-Proæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨é‡å»ºä»¤ç‰Œå‹ç¼©æŠ€æœ¯ï¼ˆReCoTï¼‰ï¼Œèƒ½å¤Ÿå®ç°å¯¹è¶…é•¿è§†é¢‘çš„é«˜æ•ˆç†è§£ã€‚ReCoTåŒ…å«åŠ¨æ€ä»¤ç‰Œåˆæˆå™¨å’Œè¯­ä¹‰å¼•å¯¼é®è”½æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆä¼ªè§†é¢‘ä»¤ç‰Œå¹¶è‡ªé€‚åº”åœ°é®è”½å†—ä½™è§†è§‰ä»¤ç‰Œä»¥ä¿ƒè¿›é‡å»ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•ˆç‡ï¼Œè¿˜å¼•å…¥äº†è§†é¢‘ç‰¹å®šæ•°æ®é›†ä¿®å‰ªç­–ç•¥å’ŒæŸ¥è¯¢æ„ŸçŸ¥é€‰æ‹©å™¨ã€‚Video-XL-Proåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå¤„ç†å•ä¸ªA100 GPUä¸Šçš„è¶…è¿‡8Kå¸§ï¼Œå¹¶ä¿æŒé«˜è´¨é‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Video-XL-Proåˆ©ç”¨é‡å»ºä»¤ç‰Œå‹ç¼©æŠ€æœ¯ï¼ˆReCoTï¼‰å®ç°äº†å¯¹è¶…é•¿è§†é¢‘çš„é«˜æ•ˆç†è§£ã€‚</li>
<li>ReCoTåŒ…å«åŠ¨æ€ä»¤ç‰Œåˆæˆå™¨ï¼ˆDTSï¼‰å’Œè¯­ä¹‰å¼•å¯¼é®è”½æŠ€æœ¯ï¼ˆSGMï¼‰ï¼Œç”¨äºç”Ÿæˆä¼ªè§†é¢‘ä»¤ç‰Œå’Œè‡ªé€‚åº”é®è”½å†—ä½™è§†è§‰ä»¤ç‰Œã€‚</li>
<li>Video-XL-Proåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨æ›´å¤§æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰é«˜æ•ˆçš„è®­ç»ƒæ•ˆç‡ï¼Œèƒ½å¤Ÿå¤„ç†å•ä¸ªA100 GPUä¸Šçš„å¤§é‡è§†é¢‘å¸§ã€‚</li>
<li>Video-XL-Proé‡‡ç”¨äº†è§†é¢‘ç‰¹å®šæ•°æ®é›†ä¿®å‰ªç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16a6288c7b16557c0cb6e908f401e2ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1db384f30d7077a5012ff41b227618ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-455e34295eeb274261b7cf682fa29a7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d98a51eaa31550fe6746537bceb51788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ff81a2052ae1def2ec0d519b619e4b9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="V2P-Bench-Evaluating-Video-Language-Understanding-with-Visual-Prompts-for-Better-Human-Model-Interaction"><a href="#V2P-Bench-Evaluating-Video-Language-Understanding-with-Visual-Prompts-for-Better-Human-Model-Interaction" class="headerlink" title="V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts   for Better Human-Model Interaction"></a>V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts   for Better Human-Model Interaction</h2><p><strong>Authors:Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Lin Chen, Zehui Chen, Xikun Bao, Jie Zhao, Feng Zhao</strong></p>
<p>Large Vision-Language Models (LVLMs) have made significant progress in the field of video understanding recently. However, current benchmarks uniformly lean on text prompts for evaluation, which often necessitate complex referential language and fail to provide precise spatial and temporal references. This limitation diminishes the experience and efficiency of human-model interaction. To address this limitation, we propose the Video Visual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically designed to evaluate LVLMsâ€™ video understanding capabilities in multimodal human-model interaction scenarios. V2P-Bench includes 980 unique videos and 1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating instance-level fine-grained understanding aligned with human cognition. Benchmarking results reveal that even the most powerful models perform poorly on V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly lower than the human expertsâ€™ 88.3%, highlighting the current shortcomings of LVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a foundation for advancing multimodal human-model interaction and video understanding evaluation. Project page: <a target="_blank" rel="noopener" href="https://github.com/gaotiexinqu/V2P-Bench">https://github.com/gaotiexinqu/V2P-Bench</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•æ™®éå€¾å‘äºä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œè¯„ä¼°ï¼Œè¿™é€šå¸¸éœ€è¦å¤æ‚çš„æŒ‡ä»£è¯­è¨€ï¼Œå¹¶ä¸”æ— æ³•æä¾›ç²¾ç¡®çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒã€‚è¿™ä¸€å±€é™æ€§é™ä½äº†äººä¸æ¨¡å‹äº’åŠ¨çš„ä½“éªŒå’Œæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è§†é¢‘è§†è§‰æç¤ºåŸºå‡†æµ‹è¯•ï¼ˆV2P-Benchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LVLMsåœ¨å¤šåª’ä½“äººæœºäº’åŠ¨åœºæ™¯ä¸­çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚V2P-BenchåŒ…æ‹¬980ä¸ªç‹¬ç‰¹è§†é¢‘å’Œ1172ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–5ä¸ªä¸»è¦ä»»åŠ¡å’Œ12ä¸ªç»´åº¦ï¼Œä¿ƒè¿›ä¸äººç±»è®¤çŸ¥ç›¸ä¸€è‡´çš„å®ä¾‹çº§ç²¾ç»†ç²’åº¦ç†è§£ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨V2P-Benchä¸Šï¼Œæœ€å¼ºå¤§çš„æ¨¡å‹è¡¨ç°ä¹Ÿä¸ä½³ï¼ˆGPT-4oä¸º65.4%ï¼ŒGemini-1.5-Proä¸º67.9%ï¼‰ï¼Œè¿œä½äºäººç±»ä¸“å®¶çš„88.3%ï¼Œè¿™å‡¸æ˜¾äº†LVLMsåœ¨ç†è§£è§†é¢‘è§†è§‰æç¤ºæ–¹é¢çš„å½“å‰ä¸è¶³ã€‚æˆ‘ä»¬å¸Œæœ›V2P-Benchèƒ½æˆä¸ºæ¨è¿›å¤šåª’ä½“äººæœºäº’åŠ¨å’Œè§†é¢‘ç†è§£è¯„ä¼°çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://github.com/gaotiexinqu/V2P-Bench%E3%80%82]%EF%BC%88%E8%AF%B7%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%89">https://github.com/gaotiexinqu/V2P-Benchã€‚]ï¼ˆè¯·ç‚¹å‡»é“¾æ¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17736v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†å€¾å‘äºä½¿ç”¨æ–‡æœ¬æç¤ºï¼Œè¿™å¸¸å¸¸éœ€è¦å¤æ‚çš„æŒ‡ä»£è¯­è¨€ï¼Œæ— æ³•æä¾›ç²¾ç¡®çš„ç©ºé—´å’Œæ—¶é—´å‚è€ƒï¼Œé™åˆ¶äº†äººä¸æ¨¡å‹çš„äº’åŠ¨ä½“éªŒä¸æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè§†é¢‘è§†è§‰æç¤ºåŸºå‡†æµ‹è¯•ï¼ˆV2P-Benchï¼‰ï¼Œä¸“é—¨è¯„ä¼°LVLMsåœ¨å¤šæ¨¡æ€äººæœºäº’åŠ¨åœºæ™¯ä¸­çš„è§†é¢‘ç†è§£èƒ½åŠ›ã€‚V2P-BenchåŒ…å«980ä¸ªç‹¬ç‰¹è§†é¢‘å’Œ1172ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–5ä¸ªä¸»è¦ä»»åŠ¡å’Œ12ä¸ªç»´åº¦ï¼Œä¿ƒè¿›ä¸äººç±»è®¤çŸ¥ç›¸ä¸€è‡´çš„å®ä¾‹çº§ç²¾ç»†ç†è§£ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€å¼ºå¤§çš„æ¨¡å‹åœ¨V2P-Benchä¸Šçš„è¡¨ç°ä»ç„¶ä¸ä½³ï¼ˆGPT-4oä¸º65.4%ï¼ŒGemini-1.5-Proä¸º67.9%ï¼‰ï¼Œè¿œä½äºäººç±»ä¸“å®¶çš„88.3%ï¼Œçªæ˜¾å‡ºLVLMsåœ¨ç†è§£è§†é¢‘è§†è§‰æç¤ºæ–¹é¢çš„å½“å‰ä¸è¶³ã€‚æˆ‘ä»¬æœŸæœ›V2P-Benchèƒ½ä¸ºæ¨è¿›å¤šæ¨¡æ€äººæœºäº’åŠ¨å’Œè§†é¢‘ç†è§£è¯„ä¼°å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸæœ‰é‡è¦è¿›å±•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†å€¾å‘äºä½¿ç”¨æ–‡æœ¬æç¤ºï¼Œå­˜åœ¨å¤æ‚æŒ‡ä»£è¯­è¨€å’Œç¼ºä¹ç²¾ç¡®ç©ºé—´æ—¶é—´å‚è€ƒçš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºè§†é¢‘è§†è§‰æç¤ºåŸºå‡†æµ‹è¯•ï¼ˆV2P-Benchï¼‰ã€‚</li>
<li>V2P-BenchåŒ…å«980ä¸ªè§†é¢‘å’Œ1172ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–5ä¸ªä»»åŠ¡å’Œ12ä¸ªç»´åº¦ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰å¼ºå¤§æ¨¡å‹åœ¨V2P-Benchä¸Šçš„è¡¨ç°ä»ä½äºäººç±»ä¸“å®¶ï¼Œçªæ˜¾LVLMsåœ¨ç†è§£è§†é¢‘è§†è§‰æç¤ºæ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>V2P-Benchä¸ºæ¨è¿›å¤šæ¨¡æ€äººæœºäº’åŠ¨å’Œè§†é¢‘ç†è§£è¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09e77d5635bf3cef6e6004919259eb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31141ae2bca5d9c6f587e13779d1c9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f0cf8c977489b140f726598b2609bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec5bb0665dedd75b2de4464d3590b24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f92e5144145f785896d7882a19fb8e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="STOP-Integrated-Spatial-Temporal-Dynamic-Prompting-for-Video-Understanding"><a href="#STOP-Integrated-Spatial-Temporal-Dynamic-Prompting-for-Video-Understanding" class="headerlink" title="STOP: Integrated Spatial-Temporal Dynamic Prompting for Video   Understanding"></a>STOP: Integrated Spatial-Temporal Dynamic Prompting for Video   Understanding</h2><p><strong>Authors:Zichen Liu, Kunlun Xu, Bing Su, Xu Zou, Yuxin Peng, Jiahuan Zhou</strong></p>
<p>Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the modelâ€™s ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/CVPR2025-STOP">https://github.com/zhoujiahuan1991/CVPR2025-STOP</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒäºæµ·é‡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨ä¼—å¤šçš„å›¾åƒä»»åŠ¡ä¸­å±•ç°å‡ºäº†æœ‰å‰æ™¯çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ ‡æ³¨çš„è§†é¢‘æ•°æ®æœ‰é™ä¸”è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚æœ€è¿‘çš„è§†é¢‘æç¤ºæ–¹æ³•è¯•å›¾é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æç¤ºæ¥é€‚åº”CLIPè¿›è¡Œè§†é¢‘ä»»åŠ¡ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨å•ä¸€é™æ€æç¤ºæ¥å¤„ç†æ‰€æœ‰è§†é¢‘åºåˆ—ï¼Œå¿½è§†äº†è·¨å¸§å­˜åœ¨çš„å„ç§æ—¶é—´åŠ¨æ€å’Œç©ºé—´å˜åŒ–ã€‚è¿™ä¸€å±€é™æ€§æ˜¾è‘—é˜»ç¢äº†æ¨¡å‹æ•æ‰æœ‰æ•ˆè§†é¢‘ç†è§£æ‰€éœ€çš„å…³é”®æ—¶é—´ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›†æˆæ—¶ç©ºåŠ¨æ€æç¤ºï¼ˆSTOPï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±ä¸¤ä¸ªäº’è¡¥æ¨¡å—ç»„æˆï¼šå¸§å†…ç©ºé—´æç¤ºå’Œå¸§é—´æ—¶é—´æç¤ºã€‚æˆ‘ä»¬çš„å¸§å†…ç©ºé—´æç¤ºæ—¨åœ¨åˆ©ç”¨å¸§å†…æ³¨æ„åŠ›å’Œæ—¶é—´å˜åŒ–æ¥åŠ¨æ€çªå‡ºæ˜¾ç¤ºæ¯ä¸ªå¸§ä¸­çš„åˆ¤åˆ«åŒºåŸŸï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨å…·æœ‰é‡å¤§æ—¶é—´åŠ¨æ€çš„åŒºåŸŸå¹¶æ•æ‰ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†çªå‡ºä¸åŒå¸§å¯¹è§†é¢‘ç†è§£çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†å¸§é—´æ—¶é—´æç¤ºï¼Œæ ¹æ®å¸§ç›¸ä¼¼æ€§æµ‹é‡åœ¨å…·æœ‰é«˜æ—¶é—´æ–¹å·®çš„å¸§ä¹‹é—´åŠ¨æ€æ’å…¥æç¤ºã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆå¤„ç†å…³é”®å¸§å¹¶å¢å¼ºå…¶ç†è§£åºåˆ—ä¸­æ—¶é—´ä¾èµ–æ€§çš„èƒ½åŠ›ã€‚åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSTOPå§‹ç»ˆå®ç°äº†å¯¹æœ€å…ˆè¿›æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/CVPR2025-STOP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhoujiahuan1991/CVPR2025-STOPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15973v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è§†é¢‘ä»»åŠ¡ï¼ŒCLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å› ç¼ºå°‘æ ‡æ³¨è§†é¢‘æ•°æ®å’Œè®­ç»ƒæˆæœ¬é«˜æ˜‚è€Œå—åˆ°æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å¼•å…¥å¯å­¦ä¹ æç¤ºçš„è§†é¢‘æç¤ºæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•å¿½è§†äº†è§†é¢‘å¸§é—´çš„å¤šæ ·æ€§å’Œæ—¶ç©ºåŠ¨æ€å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é›†æˆæ—¶ç©ºåŠ¨æ€æç¤ºï¼ˆSTOPï¼‰æ¨¡å‹ï¼ŒåŒ…æ‹¬å¸§å†…ç©ºé—´æç¤ºå’Œå¸§é—´æ—¶é—´æç¤ºä¸¤ä¸ªäº’è¡¥æ¨¡å—ã€‚STOPæ¨¡å‹èƒ½è‡ªé€‚åº”åœ°çªå‡ºå…³é”®å¸§å’Œç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œå¹¶åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨è§†é¢‘ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è§†é¢‘æç¤ºæ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€é™æ€æç¤ºï¼Œå¿½ç•¥äº†è§†é¢‘çš„æ—¶ç©ºåŠ¨æ€å˜åŒ–ã€‚</li>
<li>STOPæ¨¡å‹é€šè¿‡å¸§å†…ç©ºé—´æç¤ºå’Œå¸§é—´æ—¶é—´æç¤ºä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼Œè‡ªé€‚åº”åœ°çªå‡ºå…³é”®å¸§å’Œç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>STOPæ¨¡å‹åˆ©ç”¨å¸§å†…æ³¨æ„åŠ›å’Œæ—¶é—´å˜åŒ–è®¾è®¡å¸§å†…ç©ºé—´æç¤ºã€‚</li>
<li>STOPæ¨¡å‹é€šè¿‡æµ‹é‡å¸§ç›¸ä¼¼æ€§ï¼ŒåŠ¨æ€åœ°åœ¨å¸§é—´æ’å…¥æç¤ºï¼Œä»¥çªå‡ºä¸åŒå¸§çš„é‡è¦æ€§ã€‚</li>
<li>STOPæ¨¡å‹åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5499694aa46b500aff75ed1dc00b6a7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606daea0298781be07469dc01eb868cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c6474405d90cd1bd0bb4110be85e38.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks"><a href="#Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks" class="headerlink" title="Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks"></a>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks</h2><p><strong>Authors:Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</strong></p>
<p>We present Omni-RGPT, a multimodal large language model designed to facilitate region-level comprehension for both images and videos. To achieve consistent region representation across spatio-temporal dimensions, we introduce Token Mark, a set of tokens highlighting the target regions within the visual feature space. These tokens are directly embedded into spatial regions using region prompts (e.g., boxes or masks) and simultaneously incorporated into the text prompt to specify the target, establishing a direct connection between visual and text tokens. To further support robust video understanding without requiring tracklets, we introduce an auxiliary task that guides Token Mark by leveraging the consistency of the tokens, enabling stable region interpretation across the video. Additionally, we introduce a large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT achieves state-of-the-art results on image and video-based commonsense reasoning benchmarks while showing strong performance in captioning and referring expression comprehension tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Omni-RGPTï¼Œè¿™æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ƒè¿›å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§ç†è§£ã€‚ä¸ºäº†å®ç°æ—¶ç©ºç»´åº¦ä¸Šçš„ä¸€è‡´åŒºåŸŸè¡¨ç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†Token Markï¼Œè¿™æ˜¯ä¸€ç»„çªå‡ºæ˜¾ç¤ºç›®æ ‡åŒºåŸŸçš„ä»¤ç‰Œï¼Œç›´æ¥åµŒå…¥åˆ°ç©ºé—´åŒºåŸŸä¸­ä½¿ç”¨åŒºåŸŸæç¤ºï¼ˆä¾‹å¦‚ï¼Œç›’å­æˆ–è’™ç‰ˆï¼‰ï¼Œå¹¶åŒæ—¶çº³å…¥æ–‡æœ¬æç¤ºä»¥æŒ‡å®šç›®æ ‡ï¼Œä»è€Œåœ¨è§†è§‰ä»¤ç‰Œå’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å»ºç«‹ç›´æ¥è”ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¯æŒä¸éœ€è¦è½¨è¿¹çš„ç¨³å¥è§†é¢‘ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨ä»¤ç‰Œçš„ä¸€è‡´æ€§æ¥æŒ‡å¯¼Token Markï¼Œå®ç°åœ¨è§†é¢‘ä¸­çš„ç¨³å®šåŒºåŸŸè§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒºåŸŸçº§è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ï¼ˆRegVID-300kï¼‰ã€‚Omni-RGPTåœ¨å›¾åƒå’ŒåŸºäºè§†é¢‘çš„å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å‡†ï¼ŒåŒæ—¶åœ¨æè¿°å’ŒæŒ‡ä»£è¡¨è¾¾ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08326v2">PDF</a> CVPR 2025, Project page: <a target="_blank" rel="noopener" href="https://miranheo.github.io/omni-rgpt/">https://miranheo.github.io/omni-rgpt/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æå‡ºäº†Omni-RGPTï¼Œè¿™æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§åˆ«ç†è§£ã€‚é€šè¿‡å¼•å…¥Token Markï¼Œè¯¥æ¨¡å‹èƒ½åœ¨æ—¶ç©ºç»´åº¦ä¸Šå®ç°ä¸€è‡´çš„åŒºåŸŸè¡¨ç¤ºã€‚Token Markæ˜¯ä¸€ç»„æ ‡è®°ç›®æ ‡åŒºåŸŸçš„ä»¤ç‰Œï¼Œå®ƒä»¬é€šè¿‡åŒºåŸŸæç¤ºï¼ˆå¦‚æ¡†æˆ–æ©è†œï¼‰ç›´æ¥åµŒå…¥åˆ°ç©ºé—´åŒºåŸŸä¸­ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºæŒ‡å®šç›®æ ‡ï¼Œä»è€Œå»ºç«‹è§†è§‰å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚ä¸ºäº†æ”¯æŒä¸éœ€è¦è½¨è¿¹çš„ç¨³å¥è§†é¢‘ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨ä»¤ç‰Œçš„ä¸€è‡´æ€§æ¥å¼•å¯¼Token Markï¼Œå®ç°åœ¨è§†é¢‘ä¸­çš„ç¨³å®šåŒºåŸŸè§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒºåŸŸçº§åˆ«è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ï¼ˆRegVID-300kï¼‰ã€‚Omni-RGPTåœ¨å›¾åƒå’ŒåŸºäºè§†é¢‘çš„å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒæ—¶åœ¨æè¿°å’Œå¼•ç”¨è¡¨è¾¾ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Omni-RGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ƒè¿›å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§åˆ«ç†è§£ã€‚</li>
<li>Token Markæ˜¯Omni-RGPTçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒé€šè¿‡åŒºåŸŸæç¤ºå’Œæ–‡æœ¬æç¤ºå»ºç«‹è§†è§‰å’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡æ¥æ”¯æŒç¨³å®šçš„åŒºåŸŸè§£é‡Šï¼Œå¯ä»¥åœ¨è§†é¢‘ä¸­è¿›è¡Œä¸€è‡´çš„ç†è§£ã€‚</li>
<li>Token Marké€šè¿‡åœ¨æ—¶ç©ºç»´åº¦ä¸Šå®ç°ä¸€è‡´çš„åŒºåŸŸè¡¨ç¤ºï¼Œå®ç°å›¾åƒå’Œè§†é¢‘ç†è§£çš„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥çš„å¤§è§„æ¨¡åŒºåŸŸçº§åˆ«è§†é¢‘æŒ‡ä»¤æ•°æ®é›†RegVID-300kå¯¹Omni-RGPTçš„æ€§èƒ½æå‡èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
<li>Omni-RGPTåœ¨å›¾åƒå’ŒåŸºäºè§†é¢‘çš„å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fec3e2306a706e8c2c5630719fbef014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f07f38c015ae46bf24d393c527160e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ab497dc5cb11ce5f96c9d14fe3f1f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad065e7e1e95a10fdf75b28a4cb781f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b55d3a8e389ae8313d7467a917f8097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaa82cddb667954cb3478be45a9928f5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding"><a href="#ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding" class="headerlink" title="ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding"></a>ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding</h2><p><strong>Authors:Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie</strong></p>
<p>Video Large Language Models (VideoLLMs) have made significant strides in video understanding but struggle with long videos due to the limitations of their backbone LLMs. Existing solutions rely on length extrapolation, which is memory-constrained, or visual token compression, which primarily leverages low-level temporal redundancy while overlooking the more effective high-level knowledge redundancy. To address this, we propose $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect and PivotKV, to jointly reduce both temporal visual redundancy and knowledge redundancy for video compression. To align with the way of human temporal perception, DPSelect identifies keyframes based on inter-frame distance peaks. To leverage LLMsâ€™ learned prior knowledge, PivotKV marks the keyframes as pivots and compress non-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to process 8 times longer frames (up to 2048), outperforming similar-sized models by 3-5% and even rivaling much larger ones on VideoMME, MLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression operations with prefilling, ReTaKe introduces only ~10% prefilling latency overhead while reducing decoding latency by ~20%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe">https://github.com/SCZwangxiao/video-ReTaKe</a>. </p>
<blockquote>
<p>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºå…¶èƒŒåçš„å¤§è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œåœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´å›°éš¾ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆä¾èµ–äºå†…å­˜å—é™çš„é•¿åº¦æ‰©å±•æˆ–è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼Œåè€…ä¸»è¦åˆ©ç”¨ä½çº§çš„æ—¶åºå†—ä½™ï¼Œè€Œå¿½ç•¥äº†æ›´é«˜æ•ˆçš„é«˜çº§çŸ¥è¯†å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„ReTaKeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ–°é¢–æ¨¡å—DPSelectå’ŒPivotKVï¼Œå¯è”åˆå‡å°‘æ—¶åºè§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ï¼Œç”¨äºè§†é¢‘å‹ç¼©ã€‚ä¸ºäº†ä¸äººç±»çš„æ—¶é—´æ„ŸçŸ¥æ–¹å¼ä¿æŒä¸€è‡´ï¼ŒDPSelectåŸºäºå¸§é—´è·ç¦»å³°å€¼è¯†åˆ«å…³é”®å¸§ã€‚ä¸ºäº†åˆ©ç”¨LLMsçš„å…ˆéªŒçŸ¥è¯†ï¼ŒPivotKVå°†å…³é”®å¸§æ ‡è®°ä¸ºæ¢è½´ï¼Œå¹¶é€šè¿‡åˆ é™¤å…¶KVç¼“å­˜ä¸­çš„ä½å…³æ³¨ä»¤ç‰Œæ¥å‹ç¼©éæ¢è½´å¸§ã€‚ReTaKeä½¿VideoLLMsèƒ½å¤Ÿå¤„ç†8å€æ›´é•¿çš„å¸§ï¼ˆæœ€å¤šå¯è¾¾2048å¸§ï¼‰ï¼Œåœ¨VideoMMEã€MLVUã€LongVideoBenchå’ŒLVBenchä¸Šçš„è¡¨ç°ä¼˜äºç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹3-5%ï¼Œç”šè‡³ä¸æ›´å¤§çš„æ¨¡å‹ç›¸æŠ—è¡¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‹ç¼©æ“ä½œä¸é¢„å¡«å……çš„é‡å ï¼ŒReTaKeä»…å¼•å…¥çº¦10%çš„é¢„å¡«å……å»¶è¿Ÿå¼€é”€ï¼ŒåŒæ—¶å‡å°‘çº¦20%çš„è§£ç å»¶è¿Ÿã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe%E3%80%82">https://github.com/SCZwangxiao/video-ReTaKeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20504v5">PDF</a> Rewrite the methods section. Add more ablation studies and results in   LongVideoBench. Update metadata</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºå…¶éª¨å¹²LLMsçš„å±€é™æ€§ï¼Œåœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸é‡‡ç”¨å†…å­˜å—é™çš„é•¿åº¦æ‰©å±•æˆ–ä¸»è¦åˆ©ç”¨ä½çº§åˆ«æ—¶é—´å†—ä½™çš„è§†è§‰ä»¤ç‰Œå‹ç¼©ï¼Œå¿½è§†äº†æ›´é«˜æ•ˆçš„é«˜çº§çŸ¥è¯†å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„ReTaKeæ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ–°é¢–æ¨¡å—DPSelectå’ŒPivotKVï¼Œå¯è”åˆå‡å°‘æ—¶é—´è§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ï¼Œå®ç°è§†é¢‘å‹ç¼©ã€‚DPSelectåŸºäºå¸§é—´è·ç¦»å³°å€¼è¯†åˆ«å…³é”®å¸§ï¼Œä¸äººçš„æ—¶é—´æ„ŸçŸ¥æ–¹å¼ç›¸ç¬¦ã€‚PivotKVå°†å…³é”®å¸§æ ‡è®°ä¸ºæ”¯ç‚¹ï¼Œå¹¶åˆ©ç”¨LLMsçš„å…ˆéªŒçŸ¥è¯†å‹ç¼©éæ”¯ç‚¹å¸§ï¼Œé€šè¿‡åˆ é™¤ä½å…³æ³¨ä»¤ç‰Œæ¥å‡å°‘å…¶KVç¼“å­˜ã€‚ReTaKeä½¿VideoLLMsèƒ½å¤Ÿå¤„ç†é•¿è¾¾8å€çš„è§†é¢‘å¸§ï¼ˆæœ€å¤šè¾¾2048å¸§ï¼‰ï¼Œåœ¨VideoMMEã€MLVUã€LongVideoBenchå’ŒLVBenchä¸Šçš„æ€§èƒ½ä¼˜äºåŒç±»æ¨¡å‹3-5%ï¼Œç”šè‡³å¯ä¸æ›´å¤§çš„æ¨¡å‹ç›¸æŠ—è¡¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‹ç¼©æ“ä½œä¸é¢„å¡«å……çš„é‡å ï¼ŒReTaKeä»…å¼•å…¥çº¦10%çš„é¢„å¡«å……å»¶è¿Ÿå¼€é”€ï¼ŒåŒæ—¶é™ä½çº¦20%çš„è§£ç å»¶è¿Ÿã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SCZwangxiao/video-ReTaKeè·å–ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>VideoLLMsåœ¨è§†é¢‘ç†è§£ä¸Šè¡¨ç°å‡ºæ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†é•¿è§†é¢‘æ—¶å› éª¨å¹²LLMsçš„å±€é™æ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆä¸»è¦ä¾èµ–å†…å­˜å—é™çš„é•¿åº¦æ‰©å±•æˆ–ä»…åˆ©ç”¨ä½çº§åˆ«æ—¶é—´å†—ä½™çš„è§†è§‰ä»¤ç‰Œå‹ç¼©ã€‚</li>
<li>ReTaKeæ–¹æ³•é€šè¿‡ä¸¤ä¸ªæ–°é¢–æ¨¡å—DPSelectå’ŒPivotKVè”åˆå‡å°‘æ—¶é—´è§†è§‰å†—ä½™å’ŒçŸ¥è¯†å†—ä½™ï¼Œå®ç°è§†é¢‘å‹ç¼©ã€‚</li>
<li>DPSelectæ ¹æ®å¸§é—´è·ç¦»å³°å€¼è¯†åˆ«å…³é”®å¸§ï¼Œä¸äººçš„æ—¶é—´æ„ŸçŸ¥ç›¸ç¬¦ã€‚</li>
<li>PivotKVåˆ©ç”¨LLMsçš„å…ˆéªŒçŸ¥è¯†å‹ç¼©éæ”¯ç‚¹å¸§ã€‚</li>
<li>ReTaKeæå‡äº†VideoLLMså¤„ç†é•¿è§†é¢‘çš„èƒ½åŠ›ï¼Œæœ€å¤šå¯å¤„ç†é•¿è¾¾ä¸¤å€çš„è§†é¢‘é•¿åº¦ï¼ˆå³æœ€å¤šè¾¾2048å¸§ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-446386478b9bbb0a0986cdcd78de0e22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ec2c81658834abde1c97eb727bea5bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a00103e34a75a19679aa504eb809259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9026225b9be41f510d7630003d48ea92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d990fba2e2cecf45f073fa1e6d99b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc8bd4295efd2a33e7d6b8d44fa980.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Universal-Soccer-Video-Understanding"><a href="#Towards-Universal-Soccer-Video-Understanding" class="headerlink" title="Towards Universal Soccer Video Understanding"></a>Towards Universal Soccer Video Understanding</h2><p><strong>Authors:Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present an advanced soccer-specific visual encoder, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research. </p>
<blockquote>
<p>ä½œä¸ºä¸€é¡¹å…¨çƒç››è¡Œçš„è¿åŠ¨ï¼Œè¶³çƒå¸å¼•äº†ä¸–ç•Œå„åœ°çƒè¿·çš„å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªç”¨äºè¶³çƒè§†é¢‘ç†è§£çš„å…¨é¢å¤šæ¨¡æ€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åšå‡ºäº†ä»¥ä¸‹è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬ä»‹ç»äº†SoccerReplay-1988ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šï¼Œé‡‡ç”¨è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„è¶³çƒç‰¹å®šè§†è§‰ç¼–ç å™¨MatchVisionï¼Œå®ƒåˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬åœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«ç­‰æ–¹é¢è¿›è¡Œäº†å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶ã€‚MatchVisionåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æå‡ºçš„æ•°æ®å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™é¡¹å·¥ä½œå°†ä¸ºä½“è‚²ç†è§£ç ”ç©¶æä¾›æ ‡å‡†èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01820v3">PDF</a> CVPR 2025; Project Page: <a target="_blank" rel="noopener" href="https://jyrao.github.io/UniSoccer/">https://jyrao.github.io/UniSoccer/</a></p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªç”¨äºè¶³çƒè§†é¢‘ç†è§£çš„å…¨é¢å¤šæ¨¡æ€æ¡†æ¶ã€‚å…¶è´¡çŒ®åŒ…æ‹¬ï¼šå¼•å…¥è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†SoccerReplay-1988ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šï¼›æå‡ºå…ˆè¿›çš„è¶³çƒç‰¹å®šè§†è§‰ç¼–ç å™¨MatchVisionï¼Œèƒ½åˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼›è¿›è¡Œäº†å…³äºäº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«çš„å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒMatchVisionåœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸Šéƒ½å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†SoccerReplay-1988ï¼ŒåŒ…å«è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…ˆè¿›çš„è¶³çƒç‰¹å®šè§†è§‰ç¼–ç å™¨MatchVisionï¼Œåˆ©ç”¨æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†MatchVisionåœ¨å„ç§ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>MatchVisionåœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥å·¥ä½œæä¾›äº†å¯¹ä½“è‚²ç†è§£ç ”ç©¶çš„æ–°è§†è§’å’Œæ–¹æ³•è®ºåŸºç¡€ã€‚</li>
<li>è¯¥è®ºæ–‡æå‡ºçš„å¤šæ¨¡æ€æ¡†æ¶å¯ä¸ºæœªæ¥çš„ä½“è‚²è§†é¢‘åˆ†ææä¾›æœ‰åŠ›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5565d47ae668dd2524e82e72dd57b9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a1efc28f9bdfd89d9966e166cd33ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34144c312c6b6c1381beba3588c25a97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-831209e09f291d8bd7d649a2f6f7062f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding"><a href="#Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding" class="headerlink" title="Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding"></a>Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding</h2><p><strong>Authors:Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, Yining Sun</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have opened new avenues for video understanding. However, achieving high fidelity in zero-shot video tasks remains challenging. Traditional video processing methods rely heavily on fine-tuning to capture nuanced spatial-temporal details, which incurs significant data and computation costs. In contrast, training-free approaches, though efficient, often lack robustness in preserving context-rich features across complex video content. To this end, we propose DYTO, a novel dynamic token merging framework for zero-shot video understanding that adaptively optimizes token efficiency while preserving crucial scene details. DYTO integrates a hierarchical frame selection and a bipartite token merging strategy to dynamically cluster key frames and selectively compress token sequences, striking a balance between computational efficiency with semantic richness. Extensive experiments across multiple benchmarks demonstrate the effectiveness of DYTO, achieving superior performance compared to both fine-tuned and training-free methods and setting a new state-of-the-art for zero-shot video understanding. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºè§†é¢‘ç†è§£å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œåœ¨é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡ä¸­å®ç°é«˜ä¿çœŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¾®è°ƒæ¥æ•æ‰ç»†å¾®çš„ç©ºé—´æ—¶é—´ç»†èŠ‚ï¼Œè¿™äº§ç”Ÿäº†å¤§é‡çš„æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ— è®­ç»ƒçš„æ–¹æ³•è™½ç„¶æ•ˆç‡é«˜ï¼Œä½†åœ¨ä¿ç•™å¤æ‚è§†é¢‘å†…å®¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢å¾€å¾€ç¼ºä¹ç¨³å¥æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DYTOï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé›¶æ ·æœ¬è§†é¢‘ç†è§£çš„æ–°å‹åŠ¨æ€ä»¤ç‰Œåˆå¹¶æ¡†æ¶ï¼Œå®ƒè‡ªé€‚åº”åœ°ä¼˜åŒ–ä»¤ç‰Œæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™å…³é”®çš„åœºæ™¯ç»†èŠ‚ã€‚DYTOç»“åˆäº†åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œä»¥åŠ¨æ€åœ°èšç±»å…³é”®å¸§å¹¶é€‰æ‹©æ€§å‹ç¼©ä»¤ç‰Œåºåˆ—ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰ä¸°å¯Œæ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†DYTOçš„æœ‰æ•ˆæ€§ï¼Œä¸ç»è¿‡ç²¾ç»†è°ƒæ•´å’Œæ— è®­ç»ƒçš„æ–¹æ³•ç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸ºé›¶æ ·æœ¬è§†é¢‘ç†è§£åˆ›é€ äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14401v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/Jam1ezhang/DYTO">https://github.com/Jam1ezhang/DYTO</a></p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè§†é¢‘ç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œä½†ä»å­˜åœ¨é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡ä¸­ä¿çœŸåº¦ä¸é«˜çš„é—®é¢˜ã€‚ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•ä¾èµ–äºå¾®è°ƒä»¥æ•æ‰ç»†å¾®çš„æ—¶ç©ºç»†èŠ‚ï¼Œè¿™å¸¦æ¥äº†å¤§é‡çš„æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠ¨æ€ä»¤ç‰Œåˆå¹¶æ¡†æ¶DYTOï¼Œå®ƒèƒ½åœ¨ä¿æŒå…³é”®åœºæ™¯ç»†èŠ‚çš„åŒæ—¶è‡ªé€‚åº”åœ°ä¼˜åŒ–ä»¤ç‰Œæ•ˆç‡ï¼Œä»¥å®ç°é›¶æ ·æœ¬è§†é¢‘ç†è§£ã€‚DYTOç»“åˆäº†åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œä»¥åŠ¨æ€èšç±»å…³é”®å¸§å’Œé€‰æ‹©æ€§åœ°å‹ç¼©ä»¤ç‰Œåºåˆ—ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰ä¸°å¯Œæ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼ŒDYTOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå¾®è°ƒæ–¹æ³•å’Œæ— è®­ç»ƒæ–¹æ³•éƒ½æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºé›¶æ ·æœ¬è§†é¢‘ç†è§£é¢†åŸŸæ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ¨åŠ¨äº†è§†é¢‘ç†è§£çš„æ–°æ–¹å‘ã€‚</li>
<li>é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡ä¸­çš„ä¿çœŸåº¦é—®é¢˜ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•ä¾èµ–å¾®è°ƒï¼Œå¯¼è‡´é«˜æ•°æ®è®¡ç®—æˆæœ¬ã€‚</li>
<li>DYTOæ¡†æ¶å®ç°äº†é›¶æ ·æœ¬è§†é¢‘ç†è§£ä¸­çš„åŠ¨æ€ä»¤ç‰Œç®¡ç†ã€‚</li>
<li>DYTOç»“åˆäº†åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ã€‚</li>
<li>DYTOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f3f6c05254472fb45c4cafd3e2d9e6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ed6b9e4538f70daac5f890b0522ad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0be21a9e54d037de124f34595de7c115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffabd7b1e998dbaef2c1a14349be8229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd3f0193c69158ac680cbb4fc3ccaac.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Principles-of-Visual-Tokens-for-Efficient-Video-Understanding"><a href="#Principles-of-Visual-Tokens-for-Efficient-Video-Understanding" class="headerlink" title="Principles of Visual Tokens for Efficient Video Understanding"></a>Principles of Visual Tokens for Efficient Video Understanding</h2><p><strong>Authors:Xinyue Hao, Gen Li, Shreyank N Gowda, Robert B Fisher, Jonathan Huang, Anurag Arnab, Laura Sevilla-Lara</strong></p>
<p>Video understanding has made huge strides in recent years, relying largely on the power of transformers. As this architecture is notoriously expensive and video data is highly redundant, research into improving efficiency has become particularly relevant. Some creative solutions include token selection and merging. While most methods succeed in reducing the cost of the model and maintaining accuracy, an interesting pattern arises: most methods do not outperform the baseline of randomly discarding tokens. In this paper we take a closer look at this phenomenon and observe 5 principles of the nature of visual tokens. For example, we observe that the value of tokens follows a clear Pareto-distribution where most tokens have remarkably low value, and just a few carry most of the perceptual information. We build on these and further insights to propose a lightweight video model, LITE, that can select a small number of tokens effectively, outperforming state-of-the-art and existing baselines across datasets (Kinetics-400 and Something-Something-V2) in the challenging trade-off of computation (GFLOPs) vs accuracy. Experiments also show that LITE generalizes across datasets and even other tasks without the need for retraining. </p>
<blockquote>
<p>è§†é¢‘ç†è§£è¿‘å¹´æ¥å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œè¿™ä¸»è¦å¾—ç›Šäºå˜å‹å™¨çš„å¼ºå¤§åŠŸèƒ½ã€‚ç”±äºè¿™ç§æ¶æ„éå¸¸æ˜‚è´µä¸”è§†é¢‘æ•°æ®é«˜åº¦å†—ä½™ï¼Œå› æ­¤ç ”ç©¶æé«˜æ•ˆç‡å˜å¾—å°¤ä¸ºé‡è¦ã€‚ä¸€äº›åˆ›æ„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä»¤ç‰Œé€‰æ‹©å’Œåˆå¹¶ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•åœ¨é™ä½æ¨¡å‹æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ï¼Œä½†å‡ºç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå¤§å¤šæ•°æ–¹æ³•å¹¶ä¸ä¼˜äºéšæœºä¸¢å¼ƒä»¤ç‰Œçš„åŸºæœ¬çº¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»”ç»†è§‚å¯Ÿäº†è¿™ä¸€ç°è±¡ï¼Œå¹¶è§‚å¯Ÿäº†è§†è§‰ä»¤ç‰Œæ€§è´¨çš„5æ¡åŸåˆ™ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å‘ç°ä»¤ç‰Œçš„å€¼éµå¾ªæ¸…æ™°çš„å¸•ç´¯æ‰˜åˆ†å¸ƒï¼Œå…¶ä¸­å¤§å¤šæ•°ä»¤ç‰Œçš„ä»·å€¼éå¸¸ä½ï¼Œåªæœ‰å°‘æ•°ä»¤ç‰Œæºå¸¦å¤§éƒ¨åˆ†æ„ŸçŸ¥ä¿¡æ¯ã€‚æˆ‘ä»¬åŸºäºè¿™äº›è§è§£å’Œå…¶ä»–è¿›ä¸€æ­¥è§è§£ï¼Œæå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„è§†é¢‘æ¨¡å‹LITEï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©å°‘é‡ä»¤ç‰Œï¼Œåœ¨æ•°æ®é›†ï¼ˆKinetics-400å’ŒSomething-Something-V2ï¼‰çš„è®¡ç®—ï¼ˆGFLOPsï¼‰ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯å’Œç°æœ‰åŸºçº¿ã€‚å®éªŒè¿˜è¡¨æ˜ï¼ŒLITEå¯ä»¥åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è·¨æ•°æ®é›†ç”šè‡³å…¶ä»–ä»»åŠ¡è¿›è¡Œæ¨å¹¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13626v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†é¢‘ç†è§£é¢†åŸŸè¿‘å¹´æ¥å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä¸»è¦ä¾èµ–äºå˜å‹å™¨æ¶æ„çš„å¨åŠ›ã€‚ç”±äºè¯¥æ¶æ„æˆæœ¬é«˜æ˜‚ä¸”è§†é¢‘æ•°æ®é«˜åº¦å†—ä½™ï¼Œæé«˜æ•ˆç‡çš„ç ”ç©¶å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†è§†é¢‘ç†è§£ä¸­è§†è§‰ä»¤ç‰Œçš„æœ¬è´¨ç‰¹å¾ï¼Œæå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„è§†é¢‘æ¨¡å‹LITEï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©å°‘é‡çš„ä»¤ç‰Œå¹¶ä¼˜äºç°æœ‰çš„æœ€ä½³æ¨¡å‹å’Œå…¶ä»–åŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—å’Œå‡†ç¡®åº¦ä¹‹é—´å®ç°æ›´å¥½çš„æƒè¡¡ã€‚è¯¥æ¨¡å‹è¿˜èƒ½å¤Ÿè·¨æ•°æ®é›†ç”šè‡³è·¨ä»»åŠ¡è¿›è¡Œæ³›åŒ–è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£é¢†åŸŸä¾èµ–å˜å‹å™¨æ¶æ„å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¯¥æ¶æ„æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>è§†é¢‘æ•°æ®å­˜åœ¨é«˜åº¦å†—ä½™æ€§ã€‚</li>
<li>ä»¤ç‰Œä»·å€¼éµå¾ªå¸•ç´¯æ‰˜åˆ†å¸ƒï¼Œå¤§éƒ¨åˆ†ä»¤ç‰Œä»·å€¼è¾ƒä½ï¼Œåªæœ‰å°‘æ•°ä»¤ç‰Œæºå¸¦å¤§éƒ¨åˆ†æ„ŸçŸ¥ä¿¡æ¯ã€‚</li>
<li>LITEæ¨¡å‹æ˜¯ä¸€ä¸ªè½»é‡çº§è§†é¢‘æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆé€‰æ‹©é‡è¦ä»¤ç‰Œã€‚</li>
<li>LITEæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æœ€ä½³æ¨¡å‹å’Œå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>LITEæ¨¡å‹åœ¨è®¡ç®—å’Œå‡†ç¡®åº¦ä¹‹é—´å®ç°äº†è‰¯å¥½çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2ec2bd4dc3282fe7c16b79b71ae7963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6977cf0c68a52653b8f2fadf69ecd90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c13fa9985a31362b0890c68216ae2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff08a61dd584e4ccc2725e91fc080755.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding"><a href="#DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding" class="headerlink" title="DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding"></a>DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding</h2><p><strong>Authors:Yudong Han, Qingpei Guo, Liyuan Pan, Liu Liu, Yu Guan, Ming Yang</strong></p>
<p>The challenge in LLM-based video understanding lies in preserving visual and semantic information in long videos while maintaining a memory-affordable token count. However, redundancy and correspondence in videos have hindered the performance potential of existing methods. Through statistical learning on current datasets, we observe that redundancy occurs in both repeated and answer-irrelevant frames, and the corresponding frames vary with different questions. This suggests the possibility of adopting dynamic encoding to balance detailed video information preservation with token budget reduction. To this end, we propose a dynamic cooperative network, DynFocus, for memory-efficient video encoding in this paper. Specifically, i) a Dynamic Event Prototype Estimation (DPE) module to dynamically select meaningful frames for question answering; (ii) a Compact Cooperative Encoding (CCE) module that encodes meaningful frames with detailed visual appearance and the remaining frames with sketchy perception separately. We evaluate our method on five publicly available benchmarks, and experimental results consistently demonstrate that our method achieves competitive performance. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£æŒ‘æˆ˜åœ¨äºåœ¨é•¿è§†é¢‘ä¸­ä¿ç•™è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œä¿æŒå¯æ‰¿å—çš„ä»¤ç‰Œè®¡æ•°ã€‚ç„¶è€Œï¼Œè§†é¢‘ä¸­çš„å†—ä½™å’Œå¯¹åº”å…³ç³»é˜»ç¢äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æ½œåŠ›ã€‚é€šè¿‡å¯¹å½“å‰æ•°æ®é›†è¿›è¡Œç»Ÿè®¡å­¦ä¹ ï¼Œæˆ‘ä»¬å‘ç°å†—ä½™ç°è±¡æ—¢å‡ºç°åœ¨é‡å¤å¸§ä¸­ä¹Ÿå‡ºç°åœ¨ä¸ç­”æ¡ˆæ— å…³çš„å¸§ä¸­ï¼Œå¹¶ä¸”å¯¹åº”å¸§ä¼šå› ä¸åŒé—®é¢˜è€Œå˜åŒ–ã€‚è¿™æç¤ºæˆ‘ä»¬æœ‰å¯èƒ½é‡‡ç”¨åŠ¨æ€ç¼–ç æ¥å¹³è¡¡ä¿ç•™è¯¦ç»†çš„è§†é¢‘ä¿¡æ¯ä¸å‡å°‘ä»¤ç‰Œé¢„ç®—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå†…å­˜é«˜æ•ˆè§†é¢‘ç¼–ç çš„åŠ¨æ€ååŒç½‘ç»œDynFocusã€‚å…·ä½“æ¥è¯´ï¼Œä¸€æ˜¯å¯¹åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡ï¼ˆDPEï¼‰æ¨¡å—è¿›è¡ŒåŠ¨æ€é€‰æ‹©æœ‰æ„ä¹‰çš„å¸§ç”¨äºé—®ç­”ï¼›äºŒæ˜¯ç´§å‡‘ååŒç¼–ç ï¼ˆCCEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹æœ‰æ„ä¹‰çš„å¸§è¿›è¡Œè¯¦ç»†è§†è§‰å¤–è§‚ç¼–ç ï¼Œå¯¹å‰©ä½™å¸§è¿›è¡Œè‰å›¾æ„ŸçŸ¥ç¼–ç ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒç»“æœä¸€è‡´è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12355v2">PDF</a> Accepted by CVPR 25</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºLLMçš„è§†é¢‘ç†è§£æŒ‘æˆ˜åœ¨äºåœ¨é•¿è§†é¢‘ä¸­ä¿ç•™è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œä¿æŒå¯è®°å¿†çš„æ ‡è®°è®¡æ•°åˆç†ã€‚å†—ä½™å’Œè§†é¢‘çš„å¯¹åº”å…³ç³»é˜»ç¢äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æ½œåŠ›ã€‚é€šè¿‡å¯¹å½“å‰æ•°æ®é›†çš„ç»Ÿè®¡å­¦ä¹ ï¼Œæˆ‘ä»¬å‘ç°å†—ä½™å‡ºç°åœ¨é‡å¤å’Œä¸ç­”æ¡ˆæ— å…³çš„å¸§ä¸­ï¼Œå¹¶ä¸”å¯¹åº”çš„å¸§å› é—®é¢˜çš„ä¸åŒè€Œå˜åŒ–ã€‚è¿™æç¤ºæˆ‘ä»¬å¯èƒ½é‡‡ç”¨åŠ¨æ€ç¼–ç æ¥å¹³è¡¡ä¿ç•™è¯¦ç»†çš„è§†é¢‘ä¿¡æ¯å’Œå‡å°‘æ ‡è®°é¢„ç®—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå†…å­˜é«˜æ•ˆè§†é¢‘ç¼–ç çš„åŠ¨æ€ååŒç½‘ç»œDynFocusã€‚å…·ä½“æ¥è¯´ï¼Œä¸€ã€åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡ï¼ˆDPEï¼‰æ¨¡å—ï¼Œç”¨äºåŠ¨æ€é€‰æ‹©å¯¹é—®ç­”æœ‰æ„ä¹‰çš„å¸§ï¼›äºŒã€ç´§å‡‘ååŒç¼–ç ï¼ˆCCEï¼‰æ¨¡å—ï¼Œå¯¹æœ‰æ„ä¹‰çš„å¸§è¿›è¡Œè¯¦ç»†è§†è§‰å¤–è§‚ç¼–ç ï¼Œå¯¹å‰©ä½™å¸§è¿›è¡Œè‰å›¾æ„ŸçŸ¥ç¼–ç ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…¬å¼€çš„åŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>LLMåœ¨è§†é¢‘ç†è§£ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨ä¿ç•™è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶æ§åˆ¶æ ‡è®°è®¡æ•°ã€‚</li>
<li>è§†é¢‘å†—ä½™ç°è±¡å½±å“ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é‡å¤å’Œä¸ç­”æ¡ˆæ— å…³çš„å¸§ã€‚</li>
<li>ä¸åŒé—®é¢˜å¯¹åº”çš„å¸§ä¸åŒï¼Œè¡¨æ˜éœ€è¦åŠ¨æ€é€‰æ‹©é‡è¦å¸§è¿›è¡Œå¤„ç†ã€‚</li>
<li>æå‡ºDynFocusåŠ¨æ€ååŒç½‘ç»œè¿›è¡Œå†…å­˜é«˜æ•ˆè§†é¢‘ç¼–ç ã€‚</li>
<li>DynFocusåŒ…å«åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡ï¼ˆDPEï¼‰æ¨¡å—ï¼Œç”¨äºé€‰æ‹©é‡è¦å¸§ã€‚</li>
<li>DynFocusè¿˜åŒ…å«ç´§å‡‘ååŒç¼–ç ï¼ˆCCEï¼‰æ¨¡å—ï¼Œå¯ä»¥å¯¹æœ‰æ„ä¹‰çš„å¸§è¿›è¡Œè¯¦ç»†ç¼–ç ï¼Œè€Œå¯¹å…¶ä»–å¸§è¿›è¡Œç®€åŒ–ç¼–ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee5278ccc68c898be8b678a16af0f1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e99817d6f34abcc73fc533343322d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fb054fc88c2e12919112ee65cb1ca22.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GUI-World-A-Video-Benchmark-and-Dataset-for-Multimodal-GUI-oriented-Understanding"><a href="#GUI-World-A-Video-Benchmark-and-Dataset-for-Multimodal-GUI-oriented-Understanding" class="headerlink" title="GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented   Understanding"></a>GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented   Understanding</h2><p><strong>Authors:Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding commands. However, current agents primarily demonstrate strong understanding capabilities in static environments and are mainly applied to relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including Image LLMs and Video LLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that current models struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, Video LLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Therefore, we take the initial step of leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using video LLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. All the dataset and code are publicly available at: <a target="_blank" rel="noopener" href="https://gui-world.github.io/">https://gui-world.github.io</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¢«ç”¨ä½œé€šè¿‡ç›´æ¥æ„ŸçŸ¥å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å¹¶ç”Ÿæˆç›¸åº”å‘½ä»¤æ¥æ§åˆ¶é”®ç›˜å’Œé¼ æ ‡è¾“å…¥çš„ä»£ç†ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»£ç†ä¸»è¦è¡¨ç°å‡ºåœ¨é™æ€ç¯å¢ƒä¸­çš„å¼ºå¤§ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸»è¦åº”ç”¨äºç›¸å¯¹ç®€å•çš„é¢†åŸŸï¼Œå¦‚ç½‘é¡µæˆ–ç§»åŠ¨ç•Œé¢ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸€ä¸ªç¨³å¥çš„GUIä»£ç†åº”è¯¥èƒ½å¤Ÿæ„ŸçŸ¥GUIä¸Šçš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŠ¨æ€Webå†…å®¹å’Œå¤šæ­¥éª¤ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åº”å…¨é¢ç†è§£å„ç§GUIåœºæ™¯ï¼ŒåŒ…æ‹¬æ¡Œé¢è½¯ä»¶å’Œå¤šçª—å£äº¤äº’ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œåä¸ºGUI-Worldï¼Œå®ƒå…·æœ‰ç²¾å¿ƒåˆ¶ä½œçš„äººç±»-MLLMæ³¨é‡Šï¼Œå¹¿æ³›æ¶µç›–äº†å…­ç§GUIåœºæ™¯å’Œä¸‰ç§æ ¼å¼ä¸­çš„å…«ç§é¢å‘GUIçš„é—®é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å½“å‰æœ€å…ˆè¿›çš„MLLMsï¼ŒåŒ…æ‹¬å›¾åƒLLMså’Œè§†é¢‘LLMsï¼Œåœ¨ç†è§£å„ç§ç±»å‹çš„GUIå†…å®¹ï¼Œå°¤å…¶æ˜¯åŠ¨æ€å’Œé¡ºåºå†…å®¹æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€GUIå†…å®¹æ—¶ï¼Œå¦‚æœæ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„å…³é”®å¸§æˆ–æ“ä½œå†å²ï¼Œä¼šé‡åˆ°å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œç”±äºGUIè§†é¢‘æ•°æ®é›†è¾ƒä¸ºç¨€ç–ï¼Œè§†é¢‘LLMsåœ¨æ‰€æœ‰é¢å‘GUIçš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°ä¸è¶³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡å–äº†åˆ©ç”¨ç²¾ç»†è°ƒæ•´çš„è§†é¢‘LLMâ€”â€”GUI-Vidä½œä¸ºé¢å‘GUIçš„åŠ©æ‰‹çš„åˆæ­¥æªæ–½ï¼Œå±•ç¤ºäº†å¯¹å„ç§GUIä»»åŠ¡çš„æ”¹è¿›ç†è§£ã€‚ç„¶è€Œï¼Œç”±äºåŸºç¡€LLMsçš„æ€§èƒ½é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨è§†é¢‘LLMsä½œä¸ºGUIä»£ç†ä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥çš„åŠ¨æ€GUIå†…å®¹ç†è§£ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æ‰€æœ‰æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://gui-world.github.ioä¸Šå…¬å¼€æ‰¾åˆ°./">https://gui-world.github.ioä¸Šå…¬å¼€æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10819v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬çš„è®¨è®ºï¼Œæœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„åº”ç”¨æƒ…å†µã€‚å½“å‰æ¨¡å‹ä¸»è¦ç†è§£é™æ€ç¯å¢ƒï¼Œå¹¶åº”ç”¨äºç®€å•çš„é¢†åŸŸå¦‚ç½‘é¡µæˆ–ç§»åŠ¨ç•Œé¢ã€‚ä½œè€…ä¸»å¼ ä¸€ä¸ªå¼ºå¤§çš„GUIä»£ç†åº”è¯¥èƒ½å¤Ÿæ„ŸçŸ¥GUIä¸Šçš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŠ¨æ€ç½‘é¡µå†…å®¹å’Œå¤šæ­¥éª¤ä»»åŠ¡ï¼Œå¹¶å…¨é¢ç†è§£å„ç§GUIåœºæ™¯ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æ–°çš„æ•°æ®é›†GUI-Worldï¼Œç”¨äºè¯„ä¼°å½“å‰å…ˆè¿›çš„MLLMsï¼ˆåŒ…æ‹¬å›¾åƒLLMså’Œè§†é¢‘LLMsï¼‰ç†è§£GUIå†…å®¹çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€GUIå†…å®¹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œè§†é¢‘LLMsåœ¨æ‰€æœ‰GUIå¯¼å‘çš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°ä¸ä½³ã€‚ä½œè€…å°è¯•ä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„è§†é¢‘LLMï¼ˆGUI-Vidï¼‰ä½œä¸ºGUIå¯¼å‘çš„åŠ©ç†ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦ç”¨ä½œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œä¸»è¦ç†è§£é™æ€ç¯å¢ƒå¹¶åº”ç”¨äºç®€å•é¢†åŸŸã€‚</li>
<li>ä¸€ä¸ªå¼ºå¤§çš„GUIä»£ç†åº”èƒ½æ„ŸçŸ¥GUIä¸Šçš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŠ¨æ€å†…å®¹å’Œå¤šæ­¥éª¤ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥çš„æ–°æ•°æ®é›†GUI-Worldæ—¨åœ¨è¯„ä¼°LLMsåœ¨ç†è§£å„ç§GUIå†…å®¹æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€å’Œé¡ºåºå†…å®¹ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€GUIå†…å®¹æ—¶é‡åˆ°å›°éš¾ï¼Œéœ€è¦æ‰‹åŠ¨æ³¨é‡Šçš„å…³é”®å¸§æˆ–æ“ä½œå†å²ã€‚</li>
<li>è§†é¢‘LLMsåœ¨æ‰€æœ‰GUIå¯¼å‘çš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°ä¸ä½³ï¼Œå°½ç®¡å°è¯•ä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„è§†é¢‘LLMï¼ˆGUI-Vidï¼‰ä½œä¸ºåŠ©ç†ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨è§†é¢‘LLMsä½œä¸ºGUIä»£ç†ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¾ç¤ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-752c24ead6a7ca72fa4848a4b095164b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c27294b06614b90a4eaf77412a49460b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f87219f962a12f1b67e115adf6ed3e4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0f546d383dd19620af5a1e1e9f0ea2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adf1204d425af1478f9b8c56cf59c84e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb55092923a8e24749ae212eea48642f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b509f2629f44055b6c312ddc2f25ed75.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1b9aeb002c5dca7ac7509bd575c42d14.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Scaling Vision Pre-Training to 4K Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
