<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-27  Exploring Hallucination of Large Multimodal Models in Video   Understanding Benchmark, Analysis and Mitigation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-27-更新"><a href="#2025-03-27-更新" class="headerlink" title="2025-03-27 更新"></a>2025-03-27 更新</h1><h2 id="Exploring-Hallucination-of-Large-Multimodal-Models-in-Video-Understanding-Benchmark-Analysis-and-Mitigation"><a href="#Exploring-Hallucination-of-Large-Multimodal-Models-in-Video-Understanding-Benchmark-Analysis-and-Mitigation" class="headerlink" title="Exploring Hallucination of Large Multimodal Models in Video   Understanding: Benchmark, Analysis and Mitigation"></a>Exploring Hallucination of Large Multimodal Models in Video   Understanding: Benchmark, Analysis and Mitigation</h2><p><strong>Authors:Hongcheng Gao, Jiashu Qu, Jingyi Tang, Baolong Bi, Yue Liu, Hongyu Chen, Li Liang, Li Su, Qingming Huang</strong></p>
<p>The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)– where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at <a target="_blank" rel="noopener" href="https://github.com/Hongcheng-Gao/HAVEN">https://github.com/Hongcheng-Gao/HAVEN</a>. </p>
<blockquote>
<p>大型多模态模型（LMM）的幻觉问题，即提供看似正确但实际错误的回应，限制了其可靠性和适用性。本文针对视频模态的LMM幻觉问题进行研究，与图像和文本等静态模态相比，视频模态更加动态且更具挑战性。基于这一动机，我们首先提出了一个名为HAVEN的综合基准测试，用于评估视频理解任务中LMM的幻觉。它建立在幻觉原因、幻觉方面和问题格式三个维度上，形成了6000个问题。然后，我们通过在该基准测试上对16个LMM进行实验，定量研究了7个影响幻觉的因素，例如视频的持续时间、模型大小和模型推理等。此外，受到最近思考模型（如OpenAI o1）的启发，我们提出了一种视频思考模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来缓解LMM的幻觉问题——其中SRFT增强推理能力，而TDPO减少思考过程中的幻觉。大量实验和分析证明了其有效性。值得注意的是，它在幻觉评估方面的准确率提高了7.65%，偏见分数降低了4.5%。代码和数据在<a target="_blank" rel="noopener" href="https://github.com/Hongcheng-Gao/HAVEN%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/Hongcheng-Gao/HAVEN公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19622v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了大型多模态模型（LMMs）在视频理解任务中的幻觉问题。为评估LMMs在视频模态的幻觉，建立了一个综合基准测试HAVEN。通过实验研究，定量分析了7个影响幻觉的因素，并提出了一种视频思考模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来缓解LMMs的幻觉问题。这提高了基准测试评估幻觉的准确性，并降低了偏见分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）在视频理解任务中存在幻觉问题，影响其可靠性和适用性。</li>
<li>建立了HAVEN基准测试，用于评估LMMs在视频模态的幻觉表现。</li>
<li>HAVEN基准测试包含三个维度：幻觉原因、幻觉方面和问题格式。</li>
<li>通过实验定量研究了7个影响幻觉的因素，包括视频持续时间、模型大小和模型推理等。</li>
<li>提出了一种视频思考模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来缓解LMMs的幻觉问题。</li>
<li>视频思考模型在幻觉评估方面提高了基线准确性7.65%，并降低了偏见分数4.5%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7c6306c0289fd3c5eff88a9c0c9a9c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e2695ec10a7b1885dc6bf03a531956c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e5f3a5d43ec18fc5a47b12f05399c7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf993f2ba3d600a5a1a453d16b7beb70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97bc500ace9e7b654d00d3178394f530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f857b2fd2070f7f87f251170ea7595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76059a9d9996ed8e49d827f8a9092c09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c46ef5a411418fad12b451497a99dd12.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding"><a href="#SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding" class="headerlink" title="SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding"></a>SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding</h2><p><strong>Authors:Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. This model family employs the two-stream SlowFast mechanism, enabling efficient modeling of long-range temporal context to meet the demand for lightweight, mobile-friendly Video LLMs. We provide models ranging from 1B to 7B parameters, optimized through a streamlined training pipeline and a high-quality data mixture composed of publicly available datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves competitive performance on a wide range of video and image benchmarks, with robust results across all model sizes. Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales (1B and 3B) across various video benchmarks. </p>
<blockquote>
<p>我们推出了SlowFast-LLaVA-1.5（简称SF-LLaVA-1.5），这是一款针对长视频理解的高效标记视频大型语言模型（LLM）系列。该模型系列采用双流SlowFast机制，能够高效建模长时序上下文，满足轻量级、移动友好的视频大型语言模型的需求。我们提供了从1B到7B参数的模型，通过简化的训练管道和由公开数据集组成的高质量数据混合进行优化。实验结果表明，SF-LLaVA-1.5在广泛的视频和图像基准测试中表现出竞争力，且各种模型大小的结果都很稳健。值得注意的是，SF-LLaVA-1.5在长视频理解方面达到了最新水平（例如在LongVideoBench和MLVU上），并在各种视频基准测试中在小规模（1B和3B）上表现出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18943v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>SF-LLaVA-1.5系列视频大语言模型采用两流快慢机制，实现高效建模，满足轻量级移动友好型视频大语言模型的需求。该模型家族具有多种参数规模，从1B到7B不等，优化训练流程和高质量数据混合。实验结果表明，SF-LLaVA-1.5在视频和图像基准测试中表现优异，特别是长视频理解方面达到业界领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-LLaVA-1.5是一种针对长形式视频理解的视频大语言模型家族。</li>
<li>采用两流快慢机制，实现高效建模长距离时间上下文。</li>
<li>模型系列提供从1B到7B参数的多种选择。</li>
<li>通过优化训练流程和使用高质量数据混合来提高模型性能。</li>
<li>在各种视频基准测试中表现强劲，特别是长视频理解方面表现突出。</li>
<li>在不同的模型规模下均表现出稳健的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a9b60ea403842ae4d2b3d44f4ab3755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85da2a7be139f5e8ab48b701a4b7ccad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8628e0afa40c6539ed32e41c52e319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5050ab2a7b818501e85769024de54621.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Video-XL-Pro-Reconstructive-Token-Compression-for-Extremely-Long-Video-Understanding"><a href="#Video-XL-Pro-Reconstructive-Token-Compression-for-Extremely-Long-Video-Understanding" class="headerlink" title="Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video   Understanding"></a>Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video   Understanding</h2><p><strong>Authors:Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao</strong></p>
<p>Despite advanced token compression techniques, existing multimodal large language models (MLLMs) still struggle with hour-long video understanding. In this work, we propose Video-XL-Pro, an efficient method for extremely long video understanding, built upon Reconstructive Compression of Tokens (ReCoT), a learnable module that leverages self-supervised learning to generate comprehensive and compact video tokens. ReCoT introduces two key components: (i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from static image tokens by learning intra-token relationships, which are then used in masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively masks redundant visual tokens to facilitate more effective reconstructive learning. To improve training efficiency in MLLMs fine-tuning, we introduce a video-specific dataset pruning strategy and design a simple yet Query-aware Selector that enables the model to precisely locate query-relevant video tokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models trained on larger datasets across multiple long video understanding benchmarks. Moreover, it can process over 8K frames on a single A100 GPU while maintaining high-quality performance. </p>
<blockquote>
<p>尽管有先进的令牌压缩技术，现有的多模态大型语言模型（MLLMs）在理解长达数小时的视频时仍然面临挑战。在这项工作中，我们提出了Video-XL-Pro，这是一种基于令牌重建压缩（ReCoT）的极端长视频理解的有效方法。ReCoT是一个可学习的模块，它利用自监督学习生成全面且紧凑的视频令牌。ReCoT引入了两个关键组件：（i）动态令牌合成器（DTS）：DTS通过学习令牌内部关系从静态图像令牌生成伪视频令牌，然后用于遮罩视频建模。（ii）语义引导遮罩（SGM）：SGM自适应地遮罩冗余的视觉令牌，以促进更有效的重建学习。为了提高MLLM微调中的训练效率，我们采用了一种针对视频特定的数据集修剪策略，并设计了一个简单但查询感知的选择器，使模型能够精确定位与查询相关的视频令牌。仅使用3B参数，Video-XL-Pro在多个长视频理解基准测试中表现出超过大多数在更大数据集上训练的7B模型的效果。此外，它可以在单个A100 GPU上处理超过8K帧的同时保持高质量的性能。</p>
</blockquote>
<p><strong>简化解释</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Video-XL-Pro模型，该模型采用重建令牌压缩技术（ReCoT），能够实现对超长视频的高效理解。ReCoT包含动态令牌合成器和语义引导遮蔽技术，能够生成伪视频令牌并自适应地遮蔽冗余视觉令牌以促进重建学习。此外，为了提高多模态大型语言模型的微调效率，还引入了视频特定数据集修剪策略和查询感知选择器。Video-XL-Pro在多个长视频理解基准测试中表现优异，能够处理单个A100 GPU上的超过8K帧，并保持高质量性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Video-XL-Pro利用重建令牌压缩技术（ReCoT）实现了对超长视频的高效理解。</li>
<li>ReCoT包含动态令牌合成器（DTS）和语义引导遮蔽技术（SGM），用于生成伪视频令牌和自适应遮蔽冗余视觉令牌。</li>
<li>Video-XL-Pro在多个长视频理解基准测试中表现优异，甚至超越了使用更大数据集训练的模型。</li>
<li>该模型具有高效的训练效率，能够处理单个A100 GPU上的大量视频帧。</li>
<li>Video-XL-Pro采用了视频特定数据集修剪策略，以提高训练效率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-16a6288c7b16557c0cb6e908f401e2ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1db384f30d7077a5012ff41b227618ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-455e34295eeb274261b7cf682fa29a7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d98a51eaa31550fe6746537bceb51788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ff81a2052ae1def2ec0d519b619e4b9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="V2P-Bench-Evaluating-Video-Language-Understanding-with-Visual-Prompts-for-Better-Human-Model-Interaction"><a href="#V2P-Bench-Evaluating-Video-Language-Understanding-with-Visual-Prompts-for-Better-Human-Model-Interaction" class="headerlink" title="V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts   for Better Human-Model Interaction"></a>V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts   for Better Human-Model Interaction</h2><p><strong>Authors:Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Lin Chen, Zehui Chen, Xikun Bao, Jie Zhao, Feng Zhao</strong></p>
<p>Large Vision-Language Models (LVLMs) have made significant progress in the field of video understanding recently. However, current benchmarks uniformly lean on text prompts for evaluation, which often necessitate complex referential language and fail to provide precise spatial and temporal references. This limitation diminishes the experience and efficiency of human-model interaction. To address this limitation, we propose the Video Visual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically designed to evaluate LVLMs’ video understanding capabilities in multimodal human-model interaction scenarios. V2P-Bench includes 980 unique videos and 1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating instance-level fine-grained understanding aligned with human cognition. Benchmarking results reveal that even the most powerful models perform poorly on V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly lower than the human experts’ 88.3%, highlighting the current shortcomings of LVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a foundation for advancing multimodal human-model interaction and video understanding evaluation. Project page: <a target="_blank" rel="noopener" href="https://github.com/gaotiexinqu/V2P-Bench">https://github.com/gaotiexinqu/V2P-Bench</a>. </p>
<blockquote>
<p>大型视觉语言模型（LVLMs）在视频理解领域取得了显著进展。然而，当前的基准测试普遍倾向于使用文本提示进行评估，这通常需要复杂的指代语言，并且无法提供精确的空间和时间参考。这一局限性降低了人与模型互动的体验和效率。为了解决这一局限性，我们提出了视频视觉提示基准测试（V2P-Bench），这是一个专门设计的全面基准测试，用于评估LVLMs在多媒体人机互动场景中的视频理解能力。V2P-Bench包括980个独特视频和1172个问答对，涵盖5个主要任务和12个维度，促进与人类认知相一致的实例级精细粒度理解。基准测试结果表明，即使在V2P-Bench上，最强大的模型表现也不佳（GPT-4o为65.4%，Gemini-1.5-Pro为67.9%），远低于人类专家的88.3%，这凸显了LVLMs在理解视频视觉提示方面的当前不足。我们希望V2P-Bench能成为推进多媒体人机互动和视频理解评估的基础。项目页面：[<a target="_blank" rel="noopener" href="https://github.com/gaotiexinqu/V2P-Bench%E3%80%82]%EF%BC%88%E8%AF%B7%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%89">https://github.com/gaotiexinqu/V2P-Bench。]（请点击链接查看详细信息）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17736v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>大型视觉语言模型（LVLMs）在视频理解领域取得显著进展，但现有评估基准倾向于使用文本提示，这常常需要复杂的指代语言，无法提供精确的空间和时间参考，限制了人与模型的互动体验与效率。为此，我们提出视频视觉提示基准测试（V2P-Bench），专门评估LVLMs在多模态人机互动场景中的视频理解能力。V2P-Bench包含980个独特视频和1172个问答对，涵盖5个主要任务和12个维度，促进与人类认知相一致的实例级精细理解。评估结果显示，最强大的模型在V2P-Bench上的表现仍然不佳（GPT-4o为65.4%，Gemini-1.5-Pro为67.9%），远低于人类专家的88.3%，突显出LVLMs在理解视频视觉提示方面的当前不足。我们期望V2P-Bench能为推进多模态人机互动和视频理解评估奠定基础。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型视觉语言模型（LVLMs）在视频理解领域有重要进展。</li>
<li>现有评估基准倾向于使用文本提示，存在复杂指代语言和缺乏精确空间时间参考的问题。</li>
<li>为解决此问题，提出视频视觉提示基准测试（V2P-Bench）。</li>
<li>V2P-Bench包含980个视频和1172个问答对，涵盖5个任务和12个维度。</li>
<li>评估显示，现有强大模型在V2P-Bench上的表现仍低于人类专家，突显LVLMs在理解视频视觉提示方面的不足。</li>
<li>V2P-Bench为推进多模态人机互动和视频理解评估奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-09e77d5635bf3cef6e6004919259eb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31141ae2bca5d9c6f587e13779d1c9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f0cf8c977489b140f726598b2609bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ec5bb0665dedd75b2de4464d3590b24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f92e5144145f785896d7882a19fb8e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="STOP-Integrated-Spatial-Temporal-Dynamic-Prompting-for-Video-Understanding"><a href="#STOP-Integrated-Spatial-Temporal-Dynamic-Prompting-for-Video-Understanding" class="headerlink" title="STOP: Integrated Spatial-Temporal Dynamic Prompting for Video   Understanding"></a>STOP: Integrated Spatial-Temporal Dynamic Prompting for Video   Understanding</h2><p><strong>Authors:Zichen Liu, Kunlun Xu, Bing Su, Xu Zou, Yuxin Peng, Jiahuan Zhou</strong></p>
<p>Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model’s ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/CVPR2025-STOP">https://github.com/zhoujiahuan1991/CVPR2025-STOP</a>. </p>
<blockquote>
<p>预训练于海量的图像文本对上，视觉语言模型如CLIP在众多的图像任务中展现出了有前景的零样本泛化能力。然而，将这些能力扩展到视频任务仍然具有挑战性，因为标注的视频数据有限且训练成本高昂。最近的视频提示方法试图通过引入可学习的提示来适应CLIP进行视频任务，但它们通常使用单一静态提示来处理所有视频序列，忽视了跨帧存在的各种时间动态和空间变化。这一局限性显著阻碍了模型捕捉有效视频理解所需的关键时间信息的能力。为了解决这一问题，我们提出了一种集成时空动态提示（STOP）模型，该模型由两个互补模块组成：帧内空间提示和帧间时间提示。我们的帧内空间提示旨在利用帧内注意力和时间变化来动态突出显示每个帧中的判别区域，使模型能够关注具有重大时间动态的区域并捕捉精细的空间细节。此外，为了突出不同帧对视频理解的重要性，我们进一步引入了帧间时间提示，根据帧相似性测量在具有高时间方差的帧之间动态插入提示。这使得模型能够优先处理关键帧并增强其理解序列中时间依赖性的能力。在各种视频基准测试上的大量实验表明，STOP始终实现了对最先进方法的优越性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/CVPR2025-STOP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhoujiahuan1991/CVPR2025-STOP上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15973v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对视频任务，CLIP等视觉语言模型的零样本泛化能力因缺少标注视频数据和训练成本高昂而受到挑战。为应对此问题，研究人员提出了引入可学习提示的视频提示方法，但现有方法忽视了视频帧间的多样性和时空动态变化。为此，我们提出了集成时空动态提示（STOP）模型，包括帧内空间提示和帧间时间提示两个互补模块。STOP模型能自适应地突出关键帧和精细的空间细节，并在各种视频基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP等视觉语言模型在图像任务上表现出良好的零样本泛化能力，但在视频任务上仍面临挑战。</li>
<li>当前视频提示方法主要依赖单一静态提示，忽略了视频的时空动态变化。</li>
<li>STOP模型通过帧内空间提示和帧间时间提示两个互补模块，自适应地突出关键帧和精细的空间细节。</li>
<li>STOP模型利用帧内注意力和时间变化设计帧内空间提示。</li>
<li>STOP模型通过测量帧相似性，动态地在帧间插入提示，以突出不同帧的重要性。</li>
<li>STOP模型在各种视频基准测试中的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5499694aa46b500aff75ed1dc00b6a7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606daea0298781be07469dc01eb868cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46fa5afcddafcd8ad7e5e151d756c12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c6474405d90cd1bd0bb4110be85e38.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks"><a href="#Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks" class="headerlink" title="Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks"></a>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks</h2><p><strong>Authors:Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</strong></p>
<p>We present Omni-RGPT, a multimodal large language model designed to facilitate region-level comprehension for both images and videos. To achieve consistent region representation across spatio-temporal dimensions, we introduce Token Mark, a set of tokens highlighting the target regions within the visual feature space. These tokens are directly embedded into spatial regions using region prompts (e.g., boxes or masks) and simultaneously incorporated into the text prompt to specify the target, establishing a direct connection between visual and text tokens. To further support robust video understanding without requiring tracklets, we introduce an auxiliary task that guides Token Mark by leveraging the consistency of the tokens, enabling stable region interpretation across the video. Additionally, we introduce a large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT achieves state-of-the-art results on image and video-based commonsense reasoning benchmarks while showing strong performance in captioning and referring expression comprehension tasks. </p>
<blockquote>
<p>我们推出了Omni-RGPT，这是一款多模态大型语言模型，旨在促进图像和视频的区域级理解。为了实现时空维度上的一致区域表示，我们引入了Token Mark，这是一组突出显示目标区域的令牌，直接嵌入到空间区域中使用区域提示（例如，盒子或蒙版），并同时纳入文本提示以指定目标，从而在视觉令牌和文本令牌之间建立直接联系。为了进一步支持不需要轨迹的稳健视频理解，我们引入了一个辅助任务，通过利用令牌的一致性来指导Token Mark，实现在视频中的稳定区域解释。此外，我们还引入了一个大规模的区域级视频指令数据集（RegVID-300k）。Omni-RGPT在图像和基于视频的常识推理基准测试上达到了最新技术水准，同时在描述和指代表达理解任务中表现出强劲性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08326v2">PDF</a> CVPR 2025, Project page: <a target="_blank" rel="noopener" href="https://miranheo.github.io/omni-rgpt/">https://miranheo.github.io/omni-rgpt/</a></p>
<p><strong>Summary</strong>：</p>
<p>我们提出了Omni-RGPT，这是一款多模态大型语言模型，旨在实现图像和视频的区域级别理解。通过引入Token Mark，该模型能在时空维度上实现一致的区域表示。Token Mark是一组标记目标区域的令牌，它们通过区域提示（如框或掩膜）直接嵌入到空间区域中，并通过文本提示指定目标，从而建立视觉和文本令牌之间的直接联系。为了支持不需要轨迹的稳健视频理解，我们引入了一个辅助任务，通过利用令牌的一致性来引导Token Mark，实现在视频中的稳定区域解释。此外，我们还引入了一个大规模的区域级别视频指令数据集（RegVID-300k）。Omni-RGPT在图像和基于视频的常识推理基准测试上达到了最先进的成果，同时在描述和引用表达理解任务中表现出强大的性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Omni-RGPT是一个多模态大型语言模型，旨在促进图像和视频的区域级别理解。</li>
<li>Token Mark是Omni-RGPT的核心组件，它通过区域提示和文本提示建立视觉和文本令牌之间的直接联系。</li>
<li>引入了一个辅助任务来支持稳定的区域解释，可以在视频中进行一致的理解。</li>
<li>Token Mark通过在时空维度上实现一致的区域表示，实现图像和视频理解的一致性。</li>
<li>引入的大规模区域级别视频指令数据集RegVID-300k对Omni-RGPT的性能提升起到了重要作用。</li>
<li>Omni-RGPT在图像和基于视频的常识推理基准测试中取得了最先进的成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fec3e2306a706e8c2c5630719fbef014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f07f38c015ae46bf24d393c527160e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ab497dc5cb11ce5f96c9d14fe3f1f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad065e7e1e95a10fdf75b28a4cb781f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b55d3a8e389ae8313d7467a917f8097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaa82cddb667954cb3478be45a9928f5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding"><a href="#ReTaKe-Reducing-Temporal-and-Knowledge-Redundancy-for-Long-Video-Understanding" class="headerlink" title="ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding"></a>ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding</h2><p><strong>Authors:Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie</strong></p>
<p>Video Large Language Models (VideoLLMs) have made significant strides in video understanding but struggle with long videos due to the limitations of their backbone LLMs. Existing solutions rely on length extrapolation, which is memory-constrained, or visual token compression, which primarily leverages low-level temporal redundancy while overlooking the more effective high-level knowledge redundancy. To address this, we propose $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect and PivotKV, to jointly reduce both temporal visual redundancy and knowledge redundancy for video compression. To align with the way of human temporal perception, DPSelect identifies keyframes based on inter-frame distance peaks. To leverage LLMs’ learned prior knowledge, PivotKV marks the keyframes as pivots and compress non-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to process 8 times longer frames (up to 2048), outperforming similar-sized models by 3-5% and even rivaling much larger ones on VideoMME, MLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression operations with prefilling, ReTaKe introduces only ~10% prefilling latency overhead while reducing decoding latency by ~20%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe">https://github.com/SCZwangxiao/video-ReTaKe</a>. </p>
<blockquote>
<p>视频大语言模型（VideoLLMs）在视频理解方面取得了显著进展，但由于其背后的大语言模型的局限性，在处理长视频时面临困难。现有解决方案依赖于内存受限的长度扩展或视觉令牌压缩，后者主要利用低级的时序冗余，而忽略了更高效的高级知识冗余。为了解决这个问题，我们提出了无需训练的ReTaKe方法，该方法包含两个新颖模块DPSelect和PivotKV，可联合减少时序视觉冗余和知识冗余，用于视频压缩。为了与人类的时间感知方式保持一致，DPSelect基于帧间距离峰值识别关键帧。为了利用LLMs的先验知识，PivotKV将关键帧标记为枢轴，并通过删除其KV缓存中的低关注令牌来压缩非枢轴帧。ReTaKe使VideoLLMs能够处理8倍更长的帧（最多可达2048帧），在VideoMME、MLVU、LongVideoBench和LVBench上的表现优于类似规模的模型3-5%，甚至与更大的模型相抗衡。此外，通过压缩操作与预填充的重叠，ReTaKe仅引入约10%的预填充延迟开销，同时减少约20%的解码延迟。我们的代码可用在<a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe%E3%80%82">https://github.com/SCZwangxiao/video-ReTaKe。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20504v5">PDF</a> Rewrite the methods section. Add more ablation studies and results in   LongVideoBench. Update metadata</p>
<p><strong>摘要</strong></p>
<p>视频大型语言模型（VideoLLMs）在视频理解方面取得了显著进展，但由于其骨干LLMs的局限性，在处理长视频时面临挑战。现有解决方案通常采用内存受限的长度扩展或主要利用低级别时间冗余的视觉令牌压缩，忽视了更高效的高级知识冗余。为解决这一问题，我们提出了无需训练的ReTaKe方法，包含两个新颖模块DPSelect和PivotKV，可联合减少时间视觉冗余和知识冗余，实现视频压缩。DPSelect基于帧间距离峰值识别关键帧，与人的时间感知方式相符。PivotKV将关键帧标记为支点，并利用LLMs的先验知识压缩非支点帧，通过删除低关注令牌来减少其KV缓存。ReTaKe使VideoLLMs能够处理长达8倍的视频帧（最多达2048帧），在VideoMME、MLVU、LongVideoBench和LVBench上的性能优于同类模型3-5%，甚至可与更大的模型相抗衡。此外，通过压缩操作与预填充的重叠，ReTaKe仅引入约10%的预填充延迟开销，同时降低约20%的解码延迟。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/SCZwangxiao/video-ReTaKe%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SCZwangxiao/video-ReTaKe获取。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>VideoLLMs在视频理解上表现出显著进步，但在处理长视频时因骨干LLMs的局限性而面临挑战。</li>
<li>现有解决方案主要依赖内存受限的长度扩展或仅利用低级别时间冗余的视觉令牌压缩。</li>
<li>ReTaKe方法通过两个新颖模块DPSelect和PivotKV联合减少时间视觉冗余和知识冗余，实现视频压缩。</li>
<li>DPSelect根据帧间距离峰值识别关键帧，与人的时间感知相符。</li>
<li>PivotKV利用LLMs的先验知识压缩非支点帧。</li>
<li>ReTaKe提升了VideoLLMs处理长视频的能力，最多可处理长达两倍的视频长度（即最多达2048帧）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-446386478b9bbb0a0986cdcd78de0e22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ec2c81658834abde1c97eb727bea5bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a00103e34a75a19679aa504eb809259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9026225b9be41f510d7630003d48ea92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d990fba2e2cecf45f073fa1e6d99b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc8bd4295efd2a33e7d6b8d44fa980.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Universal-Soccer-Video-Understanding"><a href="#Towards-Universal-Soccer-Video-Understanding" class="headerlink" title="Towards Universal Soccer Video Understanding"></a>Towards Universal Soccer Video Understanding</h2><p><strong>Authors:Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present an advanced soccer-specific visual encoder, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research. </p>
<blockquote>
<p>作为一项全球盛行的运动，足球吸引了世界各地球迷的广泛关注。本文旨在开发一个用于足球视频理解的全面多模态框架。具体来说，本文做出了以下贡献：（i）我们介绍了SoccerReplay-1988，这是迄今为止最大的多模态足球数据集，包含1988场完整比赛的视频和详细注释，采用自动化注释管道；（ii）我们提出了一种先进的足球特定视觉编码器MatchVision，它利用足球视频中的时空信息，在各种下游任务中表现出色；（iii）我们在事件分类、评论生成和多视角犯规识别等方面进行了大量实验和消融研究。MatchVision在所有任务上均表现出卓越的性能，显著优于现有模型，这凸显了我们提出的数据和模型的优越性。我们相信，这项工作将为体育理解研究提供标准范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01820v3">PDF</a> CVPR 2025; Project Page: <a target="_blank" rel="noopener" href="https://jyrao.github.io/UniSoccer/">https://jyrao.github.io/UniSoccer/</a></p>
<p><strong>Summary</strong><br>该论文旨在开发一个用于足球视频理解的全面多模态框架。其贡献包括：引入迄今为止最大的多模态足球数据集SoccerReplay-1988，包含1988场完整比赛的视频和详细注释；提出先进的足球特定视觉编码器MatchVision，能利用足球视频中的时空信息，并在各种下游任务中表现出卓越性能；进行了关于事件分类、评论生成和多视角犯规识别的大量实验和消融研究，MatchVision在所有这些任务上都展示了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一个大规模的多模态足球数据集SoccerReplay-1988，包含自动化注释管道的视频和详细注释。</li>
<li>提出了一种先进的足球特定视觉编码器MatchVision，利用时空信息，并在多种下游任务中表现优秀。</li>
<li>通过广泛实验和消融研究验证了MatchVision在各种任务上的优越性。</li>
<li>MatchVision在事件分类、评论生成和多视角犯规识别方面表现出卓越性能。</li>
<li>该工作提供了对体育理解研究的新视角和方法论基础。</li>
<li>该论文提出的多模态框架可为未来的体育视频分析提供有力支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01820">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b5565d47ae668dd2524e82e72dd57b9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a1efc28f9bdfd89d9966e166cd33ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34144c312c6b6c1381beba3588c25a97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-831209e09f291d8bd7d649a2f6f7062f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding"><a href="#Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding" class="headerlink" title="Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding"></a>Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding</h2><p><strong>Authors:Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, Yining Sun</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have opened new avenues for video understanding. However, achieving high fidelity in zero-shot video tasks remains challenging. Traditional video processing methods rely heavily on fine-tuning to capture nuanced spatial-temporal details, which incurs significant data and computation costs. In contrast, training-free approaches, though efficient, often lack robustness in preserving context-rich features across complex video content. To this end, we propose DYTO, a novel dynamic token merging framework for zero-shot video understanding that adaptively optimizes token efficiency while preserving crucial scene details. DYTO integrates a hierarchical frame selection and a bipartite token merging strategy to dynamically cluster key frames and selectively compress token sequences, striking a balance between computational efficiency with semantic richness. Extensive experiments across multiple benchmarks demonstrate the effectiveness of DYTO, achieving superior performance compared to both fine-tuned and training-free methods and setting a new state-of-the-art for zero-shot video understanding. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLMs）的进步为视频理解开辟了新的途径。然而，在零样本视频任务中实现高保真仍然具有挑战性。传统视频处理方法严重依赖于微调来捕捉细微的空间时间细节，这产生了大量的数据和计算成本。相比之下，无训练的方法虽然效率高，但在保留复杂视频内容的丰富上下文特征方面往往缺乏稳健性。为此，我们提出了DYTO，这是一种用于零样本视频理解的新型动态令牌合并框架，它自适应地优化令牌效率，同时保留关键的场景细节。DYTO结合了分层帧选择和二分令牌合并策略，以动态地聚类关键帧并选择性压缩令牌序列，在计算效率和语义丰富性之间取得平衡。在多个基准测试上的广泛实验证明了DYTO的有效性，与经过精细调整和无训练的方法相比取得了优越的性能，为零样本视频理解创造了新的技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14401v2">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/Jam1ezhang/DYTO">https://github.com/Jam1ezhang/DYTO</a></p>
<p><strong>Summary</strong><br>多媒体模态大型语言模型（MLLMs）的最新进展为视频理解开辟了新途径，但仍存在零样本视频任务中保真度不高的问题。传统视频处理方法依赖于微调以捕捉细微的时空细节，这带来了大量的数据和计算成本。本文提出了一种新型的动态令牌合并框架DYTO，它能在保持关键场景细节的同时自适应地优化令牌效率，以实现零样本视频理解。DYTO结合了分层帧选择和二分令牌合并策略，以动态聚类关键帧和选择性地压缩令牌序列，在计算效率和语义丰富性之间取得了平衡。实验证明，DYTO在多个基准测试上表现出卓越的性能，相较于微调方法和无训练方法都有显著优势，为零样本视频理解领域树立了新的里程碑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的进展推动了视频理解的新方向。</li>
<li>零样本视频任务中的保真度问题仍是挑战。</li>
<li>传统视频处理方法依赖微调，导致高数据计算成本。</li>
<li>DYTO框架实现了零样本视频理解中的动态令牌管理。</li>
<li>DYTO结合了分层帧选择和二分令牌合并策略。</li>
<li>DYTO在多个基准测试上表现出卓越性能，优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f3f6c05254472fb45c4cafd3e2d9e6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ed6b9e4538f70daac5f890b0522ad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0be21a9e54d037de124f34595de7c115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffabd7b1e998dbaef2c1a14349be8229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd3f0193c69158ac680cbb4fc3ccaac.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Principles-of-Visual-Tokens-for-Efficient-Video-Understanding"><a href="#Principles-of-Visual-Tokens-for-Efficient-Video-Understanding" class="headerlink" title="Principles of Visual Tokens for Efficient Video Understanding"></a>Principles of Visual Tokens for Efficient Video Understanding</h2><p><strong>Authors:Xinyue Hao, Gen Li, Shreyank N Gowda, Robert B Fisher, Jonathan Huang, Anurag Arnab, Laura Sevilla-Lara</strong></p>
<p>Video understanding has made huge strides in recent years, relying largely on the power of transformers. As this architecture is notoriously expensive and video data is highly redundant, research into improving efficiency has become particularly relevant. Some creative solutions include token selection and merging. While most methods succeed in reducing the cost of the model and maintaining accuracy, an interesting pattern arises: most methods do not outperform the baseline of randomly discarding tokens. In this paper we take a closer look at this phenomenon and observe 5 principles of the nature of visual tokens. For example, we observe that the value of tokens follows a clear Pareto-distribution where most tokens have remarkably low value, and just a few carry most of the perceptual information. We build on these and further insights to propose a lightweight video model, LITE, that can select a small number of tokens effectively, outperforming state-of-the-art and existing baselines across datasets (Kinetics-400 and Something-Something-V2) in the challenging trade-off of computation (GFLOPs) vs accuracy. Experiments also show that LITE generalizes across datasets and even other tasks without the need for retraining. </p>
<blockquote>
<p>视频理解近年来取得了巨大进展，这主要得益于变压器的强大功能。由于这种架构非常昂贵且视频数据高度冗余，因此研究提高效率变得尤为重要。一些创意解决方案包括令牌选择和合并。虽然大多数方法在降低模型成本的同时保持了准确性，但出现了一个有趣的现象：大多数方法并不优于随机丢弃令牌的基本线。在本文中，我们仔细观察了这一现象，并观察了视觉令牌性质的5条原则。例如，我们发现令牌的值遵循清晰的帕累托分布，其中大多数令牌的价值非常低，只有少数令牌携带大部分感知信息。我们基于这些见解和其他进一步见解，提出了一个轻量级的视频模型LITE，它能够有效地选择少量令牌，在数据集（Kinetics-400和Something-Something-V2）的计算（GFLOPs）与准确性之间的权衡上优于最新技术和现有基线。实验还表明，LITE可以在不需要重新训练的情况下跨数据集甚至其他任务进行推广。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13626v2">PDF</a> </p>
<p><strong>Summary</strong><br>     视频理解领域近年来取得了巨大进展，主要依赖于变压器架构的威力。由于该架构成本高昂且视频数据高度冗余，提高效率的研究变得尤为重要。本文深入探讨了视频理解中视觉令牌的本质特征，提出了一个轻量级的视频模型LITE，能够有效地选择少量的令牌并优于现有的最佳模型和其他基线模型，能够在计算和准确度之间实现更好的权衡。该模型还能够跨数据集甚至跨任务进行泛化而无需重新训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频理解领域依赖变压器架构取得显著进展，但该架构成本高昂。</li>
<li>视频数据存在高度冗余性。</li>
<li>令牌价值遵循帕累托分布，大部分令牌价值较低，只有少数令牌携带大部分感知信息。</li>
<li>LITE模型是一个轻量级视频模型，能有效选择重要令牌。</li>
<li>LITE模型在多个数据集上表现优于现有最佳模型和其他基线模型。</li>
<li>LITE模型在计算和准确度之间实现了良好的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a2ec2bd4dc3282fe7c16b79b71ae7963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6977cf0c68a52653b8f2fadf69ecd90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c13fa9985a31362b0890c68216ae2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff08a61dd584e4ccc2725e91fc080755.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding"><a href="#DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding" class="headerlink" title="DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding"></a>DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding</h2><p><strong>Authors:Yudong Han, Qingpei Guo, Liyuan Pan, Liu Liu, Yu Guan, Ming Yang</strong></p>
<p>The challenge in LLM-based video understanding lies in preserving visual and semantic information in long videos while maintaining a memory-affordable token count. However, redundancy and correspondence in videos have hindered the performance potential of existing methods. Through statistical learning on current datasets, we observe that redundancy occurs in both repeated and answer-irrelevant frames, and the corresponding frames vary with different questions. This suggests the possibility of adopting dynamic encoding to balance detailed video information preservation with token budget reduction. To this end, we propose a dynamic cooperative network, DynFocus, for memory-efficient video encoding in this paper. Specifically, i) a Dynamic Event Prototype Estimation (DPE) module to dynamically select meaningful frames for question answering; (ii) a Compact Cooperative Encoding (CCE) module that encodes meaningful frames with detailed visual appearance and the remaining frames with sketchy perception separately. We evaluate our method on five publicly available benchmarks, and experimental results consistently demonstrate that our method achieves competitive performance. </p>
<blockquote>
<p>基于大型语言模型（LLM）的视频理解挑战在于在长视频中保留视觉和语义信息的同时，保持可承受的令牌计数。然而，视频中的冗余和对应关系阻碍了现有方法的性能潜力。通过对当前数据集进行统计学习，我们发现冗余现象既出现在重复帧中也出现在与答案无关的帧中，并且对应帧会因不同问题而变化。这提示我们有可能采用动态编码来平衡保留详细的视频信息与减少令牌预算。为此，本文提出了一种用于内存高效视频编码的动态协同网络DynFocus。具体来说，一是对动态事件原型估计（DPE）模块进行动态选择有意义的帧用于问答；二是紧凑协同编码（CCE）模块，该模块对有意义的帧进行详细视觉外观编码，对剩余帧进行草图感知编码。我们在五个公开可用的基准测试集上评估了我们的方法，实验结果一致表明，我们的方法具有竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12355v2">PDF</a> Accepted by CVPR 25</p>
<p><strong>摘要</strong></p>
<p>基于LLM的视频理解挑战在于在长视频中保留视觉和语义信息的同时，保持可记忆的标记计数合理。冗余和视频的对应关系阻碍了现有方法的性能潜力。通过对当前数据集的统计学习，我们发现冗余出现在重复和与答案无关的帧中，并且对应的帧因问题的不同而变化。这提示我们可能采用动态编码来平衡保留详细的视频信息和减少标记预算。为此，本文提出了一种用于内存高效视频编码的动态协同网络DynFocus。具体来说，一、动态事件原型估计（DPE）模块，用于动态选择对问答有意义的帧；二、紧凑协同编码（CCE）模块，对有意义的帧进行详细视觉外观编码，对剩余帧进行草图感知编码。我们在五个公开的基准测试集上评估了我们的方法，实验结果表明，我们的方法具有竞争力。</p>
<p><strong>要点</strong></p>
<ol>
<li>LLM在视频理解中面临挑战，需要在保留视觉和语义信息的同时控制标记计数。</li>
<li>视频冗余现象影响现有方法的性能，包括重复和与答案无关的帧。</li>
<li>不同问题对应的帧不同，表明需要动态选择重要帧进行处理。</li>
<li>提出DynFocus动态协同网络进行内存高效视频编码。</li>
<li>DynFocus包含动态事件原型估计（DPE）模块，用于选择重要帧。</li>
<li>DynFocus还包含紧凑协同编码（CCE）模块，可以对有意义的帧进行详细编码，而对其他帧进行简化编码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee5278ccc68c898be8b678a16af0f1d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e99817d6f34abcc73fc533343322d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fb054fc88c2e12919112ee65cb1ca22.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GUI-World-A-Video-Benchmark-and-Dataset-for-Multimodal-GUI-oriented-Understanding"><a href="#GUI-World-A-Video-Benchmark-and-Dataset-for-Multimodal-GUI-oriented-Understanding" class="headerlink" title="GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented   Understanding"></a>GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented   Understanding</h2><p><strong>Authors:Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding commands. However, current agents primarily demonstrate strong understanding capabilities in static environments and are mainly applied to relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including Image LLMs and Video LLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that current models struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, Video LLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Therefore, we take the initial step of leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using video LLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. All the dataset and code are publicly available at: <a target="_blank" rel="noopener" href="https://gui-world.github.io/">https://gui-world.github.io</a>. </p>
<blockquote>
<p>近期，多模态大型语言模型（MLLMs）被用作通过直接感知图形用户界面（GUI）并生成相应命令来控制键盘和鼠标输入的代理。然而，当前的代理主要表现出在静态环境中的强大理解能力，并主要应用于相对简单的领域，如网页或移动界面。我们认为，一个稳健的GUI代理应该能够感知GUI上的时间信息，包括动态Web内容和多步骤任务。此外，它还应全面理解各种GUI场景，包括桌面软件和多窗口交互。为此，本文引入了一个新数据集，名为GUI-World，它具有精心制作的人类-MLLM注释，广泛涵盖了六种GUI场景和三种格式中的八种面向GUI的问题。我们评估了当前最先进的MLLMs，包括图像LLMs和视频LLMs，在理解各种类型的GUI内容，尤其是动态和顺序内容方面的能力。我们的研究发现，当前模型在处理动态GUI内容时，如果没有手动注释的关键帧或操作历史，会遇到困难。另一方面，由于GUI视频数据集较为稀疏，视频LLMs在所有面向GUI的任务中都表现不足。因此，我们采取了利用精细调整的视频LLM——GUI-Vid作为面向GUI的助手的初步措施，展示了对各种GUI任务的改进理解。然而，由于基础LLMs的性能限制，我们使用视频LLMs作为GUI代理仍然存在重大挑战。我们相信我们的工作为未来的动态GUI内容理解研究提供了有价值的见解。所有数据集和代码可在<a target="_blank" rel="noopener" href="https://gui-world.github.io上公开找到./">https://gui-world.github.io上公开找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10819v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>基于文本的讨论，本文主要介绍了多模态大型语言模型（MLLMs）作为图形用户界面（GUI）代理的应用情况。当前模型主要理解静态环境，并应用于简单的领域如网页或移动界面。作者主张一个强大的GUI代理应该能够感知GUI上的时间信息，包括动态网页内容和多步骤任务，并全面理解各种GUI场景。为此，引入了新的数据集GUI-World，用于评估当前先进的MLLMs（包括图像LLMs和视频LLMs）理解GUI内容的能力。研究发现，当前模型在处理动态GUI内容时面临挑战，而视频LLMs在所有GUI导向的任务中都表现不佳。作者尝试使用精细调整的视频LLM（GUI-Vid）作为GUI导向的助理，但仍然存在挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的多模态大型语言模型（MLLMs）主要用作图形用户界面（GUI）代理，主要理解静态环境并应用于简单领域。</li>
<li>一个强大的GUI代理应能感知GUI上的时间信息，包括动态内容和多步骤任务。</li>
<li>引入的新数据集GUI-World旨在评估LLMs在理解各种GUI内容方面的能力，特别是动态和顺序内容。</li>
<li>当前模型在处理动态GUI内容时遇到困难，需要手动注释的关键帧或操作历史。</li>
<li>视频LLMs在所有GUI导向的任务中都表现不佳，尽管尝试使用精细调整的视频LLM（GUI-Vid）作为助理，但仍存在挑战。</li>
<li>使用视频LLMs作为GUI代理仍然是一个重大挑战，这显示了该领域未来研究的价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-752c24ead6a7ca72fa4848a4b095164b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c27294b06614b90a4eaf77412a49460b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f87219f962a12f1b67e115adf6ed3e4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0f546d383dd19620af5a1e1e9f0ea2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adf1204d425af1478f9b8c56cf59c84e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb55092923a8e24749ae212eea48642f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b509f2629f44055b6c312ddc2f25ed75.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1b9aeb002c5dca7ac7509bd575c42d14.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-27  Scaling Vision Pre-Training to 4K Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c1328615cf0f316f1aa61d012ac538c0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-27  Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with   Keypoints-Guided Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
