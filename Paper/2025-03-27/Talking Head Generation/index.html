<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  AudCast Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c13422ea76101cc85ff51869dd379f9b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-27-æ›´æ–°"><a href="#2025-03-27-æ›´æ–°" class="headerlink" title="2025-03-27 æ›´æ–°"></a>2025-03-27 æ›´æ–°</h1><h2 id="AudCast-Audio-Driven-Human-Video-Generation-by-Cascaded-Diffusion-Transformers"><a href="#AudCast-Audio-Driven-Human-Video-Generation-by-Cascaded-Diffusion-Transformers" class="headerlink" title="AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers"></a>AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion   Transformers</h2><p><strong>Authors:Jiazhi Guan, Kaisiyuan Wang, Zhiliang Xu, Quanwei Yang, Yasheng Sun, Shengyi He, Borong Liang, Yukang Cao, Yingying Li, Haocheng Feng, Errui Ding, Jingdong Wang, Youjian Zhao, Hang Zhou, Ziwei Liu</strong></p>
<p>Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details. Resources can be found at <a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast">https://guanjz20.github.io/projects/AudCast</a>. </p>
<blockquote>
<p>å°½ç®¡è¿‘æœŸéŸ³é¢‘é©±åŠ¨è§†é¢‘ç”ŸæˆæŠ€æœ¯æœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¸“æ³¨äºé©±åŠ¨é¢éƒ¨è¿åŠ¨ï¼Œå¯¼è‡´å¤´éƒ¨å’Œèº«ä½“çš„åŠ¨æ€ä¸ä¸€è‡´ã€‚æœªæ¥ï¼Œå¯¹äºç»™å®šéŸ³é¢‘ï¼Œç”Ÿæˆå…¼å…·ç²¾ç¡®å”‡åŒæ­¥å’Œç»†è…»è¯­éŸ³æ‰‹åŠ¿çš„å…¨äººç±»è§†é¢‘æ˜¯ä»¤äººå‘å¾€ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudCastï¼Œä¸€ä¸ªé‡‡ç”¨çº§è”æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰æ¨¡å¼çš„é€šç”¨éŸ³é¢‘é©±åŠ¨äººç±»è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºå‚è€ƒå›¾åƒå’Œç»™å®šéŸ³é¢‘åˆæˆå…¨äººç±»è§†é¢‘ã€‚1ï¼‰é¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§éŸ³é¢‘è°ƒèŠ‚çš„å…¨äººç±»DiTæ¶æ„ï¼Œå¯ç›´æ¥é©±åŠ¨ä»»ä½•äººç±»èº«ä½“çš„è¿åŠ¨ï¼Œå…·æœ‰ç”ŸåŠ¨çš„æ‰‹åŠ¿åŠ¨æ€ã€‚2ï¼‰ç„¶åï¼Œä¸ºäº†å¢å¼ºæ‰‹å’Œè„¸éƒ¨çš„ç»†èŠ‚ï¼ˆè¿™äº›ç»†èŠ‚æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„éš¾ä»¥å¤„ç†çš„ï¼‰ï¼ŒåŒºåŸŸç»†åŒ–DiTåˆ©ç”¨åŒºåŸŸ3Dæ‹Ÿåˆä½œä¸ºæ¡¥æ¢æ¥é‡å»ºä¿¡å·ï¼Œäº§ç”Ÿæœ€ç»ˆç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„éŸ³é¢‘é©±åŠ¨å…¨äººç±»è§†é¢‘ï¼Œå…·æœ‰æ—¶é—´è¿è´¯æ€§å’Œç²¾ç»†çš„é¢éƒ¨å’Œæ‰‹éƒ¨ç»†èŠ‚ã€‚èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast%E6%89%BE%E5%88%B0%E3%80%82">https://guanjz20.github.io/projects/AudCastæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19824v1">PDF</a> Accepted to IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR), 2025. Project page:   <a target="_blank" rel="noopener" href="https://guanjz20.github.io/projects/AudCast">https://guanjz20.github.io/projects/AudCast</a></p>
<p><strong>æ‘˜è¦</strong><br>éšç€éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿‘æœŸè¿›å±•ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢éƒ¨è¿åŠ¨çš„é©±åŠ¨ï¼Œå¯¼è‡´å¤´éƒ¨å’Œèº«ä½“çš„åŠ¨æ€ä¸ä¸€è‡´ã€‚å› æ­¤ï¼Œç”Ÿæˆä¸ç»™å®šéŸ³é¢‘ç›¸å¯¹åº”çš„å…·æœ‰å‡†ç¡®å”‡åŒæ­¥å’Œç²¾ç»†è¯­éŸ³æ‰‹åŠ¿çš„å®Œæ•´äººç±»è§†é¢‘æ—¢å…·æœ‰æŒ‘æˆ˜æ€§åˆä»¤äººå‘å¾€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudCastï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨çº§è”æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰èŒƒå¼çš„é€šç”¨éŸ³é¢‘é©±åŠ¨äººç±»è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåŸºäºå‚è€ƒå›¾åƒå’Œç»™å®šéŸ³é¢‘åˆæˆå®Œæ•´çš„äººç±»è§†é¢‘ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—éŸ³é¢‘æ§åˆ¶çš„æ•´ä½“äººç±»DiTæ¶æ„ï¼Œå¯ä»¥ç›´æ¥é©±åŠ¨ä»»ä½•äººçš„èº«ä½“è¿åŠ¨ï¼Œå‘ˆç°ç”ŸåŠ¨çš„æ‰‹åŠ¿åŠ¨æ€ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æé«˜æ‰‹å’Œè„¸éƒ¨çš„ç»†èŠ‚å¤„ç†ï¼ˆè¿™æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„éš¾ç‚¹ï¼‰ï¼ŒåŒºåŸŸç»†åŒ–DiTåˆ©ç”¨åŒºåŸŸ3Dæ‹Ÿåˆä½œä¸ºæ¡¥æ¢æ¥é‡å¡‘ä¿¡å·ï¼Œäº§ç”Ÿæœ€ç»ˆç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„éŸ³é¢‘é©±åŠ¨å®Œæ•´äººç±»è§†é¢‘ï¼Œå…·æœ‰æ—¶é—´è¿è´¯æ€§å’Œç²¾ç»†çš„é¢éƒ¨å’Œæ‰‹éƒ¨ç»†èŠ‚ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç°æœ‰éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ä¸»è¦å…³æ³¨é¢éƒ¨è¿åŠ¨ï¼Œå¯¼è‡´å¤´éƒ¨å’Œèº«ä½“çš„åŠ¨æ€ä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºAudCastæ¡†æ¶ï¼Œé‡‡ç”¨çº§è”æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰èŒƒå¼ï¼ŒåŸºäºå‚è€ƒå›¾åƒå’ŒéŸ³é¢‘åˆæˆå®Œæ•´äººç±»è§†é¢‘ã€‚</li>
<li>å¼•å…¥å—éŸ³é¢‘æ§åˆ¶çš„æ•´ä½“äººç±»DiTæ¶æ„ï¼Œç›´æ¥é©±åŠ¨ä»»ä½•äººçš„èº«ä½“è¿åŠ¨ï¼ŒåŒ…æ‹¬ç”ŸåŠ¨çš„æ‰‹åŠ¿åŠ¨æ€ã€‚</li>
<li>é‡‡ç”¨åŒºåŸŸç»†åŒ–DiTï¼Œç»“åˆåŒºåŸŸ3Dæ‹Ÿåˆï¼Œæé«˜æ‰‹å’Œè„¸éƒ¨çš„ç»†èŠ‚å¤„ç†ã€‚</li>
<li>AudCastæ¡†æ¶ç”Ÿæˆçš„é«˜ä¿çœŸéŸ³é¢‘é©±åŠ¨è§†é¢‘å…·æœ‰æ—¶é—´è¿è´¯æ€§å’Œç²¾ç»†çš„é¢éƒ¨åŠæ‰‹éƒ¨ç»†èŠ‚ã€‚</li>
<li>æ¡†æ¶é€‚ç”¨äºå¹¿æ³›çš„éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œä¸ºç”µå½±ã€åŠ¨ç”»ã€è™šæ‹Ÿç°å®ç­‰é¢†åŸŸæä¾›æœ‰åŠ›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7baaede395011d688eb195c30caa0a6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5328a993ab8d2d505bf0057de099e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1135c624827926e0d1490721f0ca8c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-700425759607ea60e7537372b8806fde.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DisentTalk-Cross-lingual-Talking-Face-Generation-via-Semantic-Disentangled-Diffusion-Model"><a href="#DisentTalk-Cross-lingual-Talking-Face-Generation-via-Semantic-Disentangled-Diffusion-Model" class="headerlink" title="DisentTalk: Cross-lingual Talking Face Generation via Semantic   Disentangled Diffusion Model"></a>DisentTalk: Cross-lingual Talking Face Generation via Semantic   Disentangled Diffusion Model</h2><p><strong>Authors:Kangwei Liu, Junwu Liu, Yun Cao, Jinlin Guo, Xiaowei Yi</strong></p>
<p>Recent advances in talking face generation have significantly improved facial animation synthesis. However, existing approaches face fundamental limitations: 3DMM-based methods maintain temporal consistency but lack fine-grained regional control, while Stable Diffusion-based methods enable spatial manipulation but suffer from temporal inconsistencies. The integration of these approaches is hindered by incompatible control mechanisms and semantic entanglement of facial representations. This paper presents DisentTalk, introducing a data-driven semantic disentanglement framework that decomposes 3DMM expression parameters into meaningful subspaces for fine-grained facial control. Building upon this disentangled representation, we develop a hierarchical latent diffusion architecture that operates in 3DMM parameter space, integrating region-aware attention mechanisms to ensure both spatial precision and temporal coherence. To address the scarcity of high-quality Chinese training data, we introduce CHDTF, a Chinese high-definition talking face dataset. Extensive experiments show superior performance over existing methods across multiple metrics, including lip synchronization, expression quality, and temporal consistency. Project Page: <a target="_blank" rel="noopener" href="https://kangweiiliu.github.io/DisentTalk">https://kangweiiliu.github.io/DisentTalk</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨è¯´è¯äººè„¸ç”Ÿæˆæ–¹é¢çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†é¢éƒ¨åŠ¨ç”»åˆæˆçš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨åŸºæœ¬å±€é™ï¼šåŸºäº3DMMçš„æ–¹æ³•ä¿æŒäº†æ—¶é—´ä¸€è‡´æ€§ï¼Œä½†ç¼ºä¹ç²¾ç»†çš„åŒºåŸŸæ§åˆ¶ï¼Œè€ŒåŸºäºStable Diffusionçš„æ–¹æ³•å®ç°äº†ç©ºé—´æ“ä½œï¼Œä½†å­˜åœ¨æ—¶é—´ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™äº›æ–¹æ³•çš„èåˆå—åˆ°æ§åˆ¶æœºåˆ¶ä¸å…¼å®¹å’Œé¢éƒ¨è¡¨ç¤ºè¯­ä¹‰çº ç¼ çš„é˜»ç¢ã€‚æœ¬æ–‡æå‡ºDisentTalkï¼Œå¼•å…¥æ•°æ®é©±åŠ¨è¯­ä¹‰è§£è€¦æ¡†æ¶ï¼Œå°†3DMMè¡¨è¾¾å¼å‚æ•°åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­ç©ºé—´ï¼Œä»¥å®ç°ç²¾ç»†çš„é¢éƒ¨æ§åˆ¶ã€‚åŸºäºè¿™ç§è§£è€¦çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåˆ†å±‚æ½œåœ¨æ‰©æ•£æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨3DMMå‚æ•°ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå¹¶ç»“åˆåŒºåŸŸæ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç¡®ä¿ç©ºé—´ç²¾åº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚ä¸ºäº†è§£å†³é«˜è´¨é‡ä¸­æ–‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CHDTFï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­æ–‡é«˜æ¸…è¯´è¯äººè„¸æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å˜´å”‡åŒæ­¥ã€è¡¨æƒ…è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://kangweiiliu.github.io/DisentTalk%E3%80%82">https://kangweiiliu.github.io/DisentTalkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DisentTalkæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†3DMMæ–¹æ³•å’ŒStable Diffusionæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†é¢éƒ¨åŠ¨ç”»åˆæˆä¸­çš„å…³é”®é—®é¢˜ã€‚é€šè¿‡æ•°æ®é©±åŠ¨çš„è¯­ä¹‰è§£è€¦æŠ€æœ¯ï¼Œå®ç°å¯¹è¡¨æƒ…å‚æ•°çš„ç²¾ç»†æ§åˆ¶ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æ½œåœ¨æ‰©æ•£æ¶æ„ï¼Œä¿è¯äº†ç©ºé—´ç²¾åº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³é«˜è´¨é‡ä¸­æ–‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†ä¸­æ–‡é«˜æ¸…å¯¹è¯äººè„¸æ•°æ®é›†CHDTFã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DisentTalkç»“åˆäº†3DMMå’ŒStable Diffusionæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨è§£å†³é¢éƒ¨åŠ¨ç”»åˆæˆä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ•°æ®é©±åŠ¨çš„è¯­ä¹‰è§£è€¦æŠ€æœ¯ï¼Œå®ç°å¯¹è¡¨æƒ…å‚æ•°çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æ½œåœ¨æ‰©æ•£æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨3DMMå‚æ•°ç©ºé—´å†…æ“ä½œï¼Œç¡®ä¿äº†ç©ºé—´ç²¾åº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä¸­æ–‡é«˜æ¸…å¯¹è¯äººè„¸æ•°æ®é›†CHDTFï¼Œç”¨äºè§£å†³é«˜è´¨é‡ä¸­æ–‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒDisentTalkåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>DisentTalkæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å¦‚3DMMå’ŒStable Diffusionæ‰€é¢ä¸´çš„æ—¶ç©ºè¿è´¯æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-307062fbc3cfa667259f0504966da038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8683756ed3f44bc9703188144d249ada.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c86cccddc9db0e27208ef1e735abdc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f20f864ed4adc8f30de50ec0e9db6fa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dceae76da7e6610604d2ef10b0a66f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dc0d5479e8a05f6ad3e8d2ddc65eed0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91d6033d11e25eb0fb36a52e6fb4651d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d8c200964a2088f92ff09b7ff3f4e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Teller-Real-Time-Streaming-Audio-Driven-Portrait-Animation-with-Autoregressive-Motion-Generation"><a href="#Teller-Real-Time-Streaming-Audio-Driven-Portrait-Animation-with-Autoregressive-Motion-Generation" class="headerlink" title="Teller: Real-Time Streaming Audio-Driven Portrait Animation with   Autoregressive Motion Generation"></a>Teller: Real-Time Streaming Audio-Driven Portrait Animation with   Autoregressive Motion Generation</h2><p><strong>Authors:Dingcheng Zhen, Shunshun Yin, Shiyang Qin, Hou Yi, Ziwei Zhang, Siyuan Liu, Gan Qi, Ming Tao</strong></p>
<p>In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion. Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements. Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism. </p>
<blockquote>
<p>åœ¨æœ¬æ¬¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§å®æ—¶éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»ï¼ˆä¹Ÿç§°ä¸ºè°ˆè¯å¤´ï¼‰çš„è‡ªå›å½’æ¡†æ¶ã€‚é™¤äº†åŠ¨ç”»æ—¶é—´é•¿çš„æŒ‘æˆ˜å¤–ï¼ŒçœŸå®è°ˆè¯å¤´ç”Ÿæˆçš„å…³é”®æŒ‘æˆ˜åœ¨äºä¿æŒä¸åŒèº«ä½“éƒ¨ä½çš„è‡ªç„¶è¿åŠ¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Tellerï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰è‡ªå›å½’è¿åŠ¨ç”ŸæˆåŠŸèƒ½çš„æµå¼éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒTelleré¦–å…ˆå°†é¢éƒ¨å’Œèº«ä½“çš„ç»†èŠ‚åŠ¨ç”»åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šåŸºäºè‡ªå›å½’è½¬æ¢å™¨çš„é¢éƒ¨è¿åŠ¨æ½œåœ¨ç”Ÿæˆï¼ˆFMLGï¼‰å’Œä½¿ç”¨é«˜æ•ˆæ—¶é—´æ¨¡å—ï¼ˆETMï¼‰è¿›è¡Œè¿åŠ¨çœŸå®æ€§ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼ŒFMLGé‡‡ç”¨æ®‹å·®VQæ¨¡å‹å°†é¢éƒ¨è¿åŠ¨æ½œåœ¨ä»åŸºäºéšå…³é”®ç‚¹çš„æ¨¡å‹æ˜ å°„åˆ°ç¦»æ•£è¿åŠ¨ä»¤ç‰Œï¼Œç„¶åä¸éŸ³é¢‘åµŒå…¥è¿›è¡Œæ—¶é—´åˆ‡ç‰‡ã€‚è¿™ä½¿ARè½¬æ¢å™¨èƒ½å¤Ÿå®æ—¶å­¦ä¹ éŸ³é¢‘åˆ°è¿åŠ¨çš„æ˜ å°„ã€‚æ­¤å¤–ï¼ŒTellerç»“åˆäº†ETMæ¥æ•æ‰æ›´ç²¾ç»†çš„è¿åŠ¨ç»†èŠ‚ã€‚è¯¥æ¨¡å—ç¡®ä¿èº«ä½“éƒ¨ä½å’Œé…é¥°ï¼ˆå¦‚é¢ˆéƒ¨è‚Œè‚‰å’Œè€³ç¯ï¼‰çš„ç‰©ç†ä¸€è‡´æ€§ï¼Œæé«˜äº†è¿™äº›è¿åŠ¨çš„çœŸå®æ€§ã€‚Tellerè®¾è®¡é«˜æ•ˆï¼Œè¶…è¶Šäº†åŸºäºæ‰©æ•£çš„æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼ˆHalloä¸ºç”Ÿæˆä¸€ç§’è§†é¢‘éœ€è¦20.93ç§’ï¼Œè€ŒTelleråªéœ€0.92ç§’ï¼‰ï¼Œå¹¶å®ç°äº†é«˜è¾¾25 FPSçš„å®æ—¶æµå¤„ç†èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘é©±åŠ¨çš„è‚–åƒåŠ¨ç”»æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†å¾®è¿åŠ¨ä¸­ï¼Œå¦‚äººç±»è¯„ä¼°æ‰€ç¤ºï¼Œåœ¨è´¨é‡å’ŒçœŸå®æ€§æ–¹é¢æœ‰å¾ˆå¤§ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18429v1">PDF</a> Accept in CVPR 2025 Conference Submission</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„è‡ªå›å½’å®æ—¶è‚–åƒåŠ¨ç”»æ¡†æ¶Tellerï¼Œå®ç°äº†çœŸå®è¯´è¯çš„å¤´éƒ¨ç”Ÿæˆã€‚å®ƒé€šè¿‡åˆ†è§£é¢éƒ¨å’ŒåŠ¨ä½œåŠ¨ç”»ä¸ºä¸¤éƒ¨åˆ†æ¥è§£å†³æŒ‘æˆ˜ï¼šåŸºäºè‡ªå›å½’è½¬æ¢å™¨çš„é¢éƒ¨è¿åŠ¨æ½œåœ¨ç”Ÿæˆï¼ˆFMLGï¼‰å’Œè¿åŠ¨çœŸå®æ€§çš„ç²¾ç»†æ—¶é—´æ¨¡å—ï¼ˆETMï¼‰ç²¾ç‚¼ã€‚Tellerä½¿ç”¨é«˜æ•ˆçš„æ¶æ„ï¼Œè¾¾åˆ°å®æ—¶çš„æµå¼å¤„ç†æ€§èƒ½ï¼ŒåŒæ—¶æé«˜è¿åŠ¨è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åŠ¨ä½œæ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ã€‚é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œå…¶åœ¨éŸ³é¢‘é©±åŠ¨çš„è‚–åƒåŠ¨ç”»æ¨¡å‹ä¸­è¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Telleræ˜¯ä¸€ä¸ªå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è‚–åƒåŠ¨ç”»æ¡†æ¶ï¼Œç”¨äºç”ŸæˆçœŸå®è¯´è¯çš„å¤´éƒ¨ã€‚</li>
<li>å®ƒé€šè¿‡åˆ†è§£é¢éƒ¨å’ŒåŠ¨ä½œåŠ¨ç”»ä¸ºä¸¤éƒ¨åˆ†æ¥è§£å†³æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢éƒ¨è¿åŠ¨æ½œåœ¨ç”Ÿæˆå’Œè¿åŠ¨çœŸå®æ€§çš„ç²¾ç‚¼ã€‚</li>
<li>Tellerå¼•å…¥äº†ä¸€ç§æ–°çš„é¢éƒ¨è¿åŠ¨æ½œåœ¨ç”Ÿæˆæ–¹æ³•ï¼ˆFMLGï¼‰ï¼ŒåŸºäºè‡ªå›å½’è½¬æ¢å™¨ï¼Œå®ç°éŸ³é¢‘ä¸è¿åŠ¨çš„å®æ—¶æ˜ å°„ã€‚</li>
<li>Telleré‡‡ç”¨ç²¾ç»†æ—¶é—´æ¨¡å—ï¼ˆETMï¼‰æ¥æ•æ‰æ›´ç²¾ç»†çš„è¿åŠ¨ç»†èŠ‚ï¼Œç¡®ä¿èº«ä½“éƒ¨ä½çš„ç‰©ç†ä¸€è‡´æ€§ï¼Œæé«˜è¿åŠ¨çœŸå®æ„Ÿã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15d2c726e66ee1a57e803768639a7fce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57428397468b6474c609d38796b547a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b85aa1dc1c5bd194660ffa2ec2b9d43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2981a1bbc6c861e7cf307e10e05ac8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03be4bc2b290ec429a2e8030c195d120.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffusionTalker-Efficient-and-Compact-Speech-Driven-3D-Talking-Head-via-Personalizer-Guided-Distillation"><a href="#DiffusionTalker-Efficient-and-Compact-Speech-Driven-3D-Talking-Head-via-Personalizer-Guided-Distillation" class="headerlink" title="DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via   Personalizer-Guided Distillation"></a>DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via   Personalizer-Guided Distillation</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Real-time speech-driven 3D facial animation has been attractive in academia and industry. Traditional methods mainly focus on learning a deterministic mapping from speech to animation. Recent approaches start to consider the nondeterministic fact of speech-driven 3D face animation and employ the diffusion model for the task. Existing diffusion-based methods can improve the diversity of facial animation. However, personalized speaking styles conveying accurate lip language is still lacking, besides, efficiency and compactness still need to be improved. In this work, we propose DiffusionTalker to address the above limitations via personalizer-guided distillation. In terms of personalization, we introduce a contrastive personalizer that learns identity and emotion embeddings to capture speaking styles from audio. We further propose a personalizer enhancer during distillation to enhance the influence of embeddings on facial animation. For efficiency, we use iterative distillation to reduce the steps required for animation generation and achieve more than 8x speedup in inference. To achieve compactness, we distill the large teacher model into a smaller student model, reducing our modelâ€™s storage by 86.4% while minimizing performance loss. After distillation, users can derive their identity and emotion embeddings from audio to quickly create personalized animations that reflect specific speaking styles. Extensive experiments are conducted to demonstrate that our method outperforms state-of-the-art methods. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/DiffusionTalker">https://github.com/ChenVoid/DiffusionTalker</a>. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å…·æœ‰å¸å¼•åŠ›ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å­¦ä¹ ä»è¯­éŸ³åˆ°åŠ¨ç”»çš„ç¡®å®šæ€§æ˜ å°„ã€‚æœ€è¿‘çš„æ–¹æ³•å¼€å§‹è€ƒè™‘è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»çš„éç¡®å®šæ€§å› ç´ ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹å®Œæˆä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å¯ä»¥æé«˜é¢éƒ¨åŠ¨ç”»çš„å¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œé™¤äº†ç¼ºä¹ä¼ è¾¾å‡†ç¡®å”‡è¯­çš„ä¸ªæ€§åŒ–è¯´è¯é£æ ¼å¤–ï¼Œè¿˜éœ€è¦æé«˜æ•ˆç‡å’Œç´§å‡‘æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºDiffusionTalkerï¼Œé€šè¿‡ä¸ªæ€§åŒ–å¼•å¯¼è’¸é¦è§£å†³ä¸Šè¿°å±€é™æ€§ã€‚åœ¨ä¸ªæ€§åŒ–æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨æ¥å­¦ä¹ èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œä»éŸ³é¢‘ä¸­æ•æ‰è¯´è¯é£æ ¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºåœ¨è’¸é¦è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸ªæ€§åŒ–å¢å¼ºå™¨ï¼Œä»¥å¢å¼ºåµŒå…¥å¯¹é¢éƒ¨åŠ¨ç”»çš„å½±å“ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿­ä»£è’¸é¦å‡å°‘åŠ¨ç”»ç”Ÿæˆæ‰€éœ€çš„æ­¥éª¤ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†è¶…è¿‡8å€çš„åŠ é€Ÿã€‚ä¸ºäº†å®ç°ç´§å‡‘æ€§ï¼Œæˆ‘ä»¬å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œåœ¨å°½é‡å‡å°‘æ€§èƒ½æŸå¤±çš„åŒæ—¶ï¼Œå°†æˆ‘ä»¬çš„æ¨¡å‹å­˜å‚¨å‡å°‘äº†8.4%ã€‚è’¸é¦åï¼Œç”¨æˆ·å¯ä»¥ä»éŸ³é¢‘ä¸­å¾—å‡ºè‡ªå·±çš„èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œå¿«é€Ÿåˆ›å»ºåæ˜ ç‰¹å®šè¯´è¯é£æ ¼çš„ä¸ªæ€§åŒ–åŠ¨ç”»ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/DiffusionTalker">https://github.com/ChenVoid/DiffusionTalker</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18159v1">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å®æ—¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»æ–¹æ³•ï¼Œç§°ä¸ºDiffusionTalkerã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ªæ€§åŒ–å¼•å¯¼è’¸é¦è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æé«˜åŠ¨ç”»çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æé«˜æ•ˆç‡å’Œç´§å‡‘æ€§ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨å­¦ä¹ èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œæ•æ‰éŸ³é¢‘ä¸­çš„è¯´è¯é£æ ¼ã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­æå‡ºä¸ªæ€§åŒ–å¢å¼ºå™¨ï¼Œå¢å¼ºåµŒå…¥å¯¹é¢éƒ¨åŠ¨ç”»çš„å½±å“ã€‚ä½¿ç”¨è¿­ä»£è’¸é¦å‡å°‘åŠ¨ç”»ç”Ÿæˆæ‰€éœ€çš„æ­¥éª¤ï¼Œå®ç°æ¨ç†æ—¶è¶…è¿‡8å€çš„åŠ é€Ÿã€‚é€šè¿‡å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œå‡å°‘æ¨¡å‹å­˜å‚¨é‡è¾¾86.4%ï¼ŒåŒæ—¶å°½é‡å‡å°‘æ€§èƒ½æŸå¤±ã€‚ç”¨æˆ·å¯ä»¥ä»éŸ³é¢‘ä¸­æå–èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œå¿«é€Ÿåˆ›å»ºåæ˜ ç‰¹å®šè¯´è¯é£æ ¼çš„ä¸ªæ€§åŒ–åŠ¨ç”»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffusionTalkeræ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å®æ—¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¸ªæ€§åŒ–å¼•å¯¼è’¸é¦æé«˜åŠ¨ç”»çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”ä¸ªæ€§åŒ–å™¨å­¦ä¹ èº«ä»½å’Œæƒ…æ„ŸåµŒå…¥ï¼Œä»¥æ•æ‰éŸ³é¢‘ä¸­çš„è¯´è¯é£æ ¼ã€‚</li>
<li>æå‡ºä¸ªæ€§åŒ–å¢å¼ºå™¨åœ¨è’¸é¦è¿‡ç¨‹ä¸­å¢å¼ºåµŒå…¥å¯¹åŠ¨ç”»çš„å½±å“ã€‚</li>
<li>ä½¿ç”¨è¿­ä»£è’¸é¦åŠ é€ŸåŠ¨ç”»ç”Ÿæˆï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å°†å¤§å‹æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°æ¨¡å‹çš„ç´§å‡‘æ€§ï¼Œå‡å°‘å­˜å‚¨éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-df90fdd19aee17b7de3612891077e609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2bd15c0b513d06e0a31e2bd260f4cdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbf3f00752a37d60c9878e9838e22c97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e0444b8af0163f2a8e6ab876d9d28a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ecdd222028b48c3ac0a03ce1f3b3b4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c624f647de655de8c2201e1d4046e4f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting"><a href="#TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting" class="headerlink" title="TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting"></a>TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting</h2><p><strong>Authors:Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv</strong></p>
<p>Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we â€œbakeâ€ the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro. </p>
<blockquote>
<p>åœ¨å¢å¼ºç°å®ï¼ˆARï¼‰é¢†åŸŸï¼Œé€¼çœŸçš„3Då…¨èº«å¯¹è¯åŠèº«åƒå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå…¶åº”ç”¨åœºæ™¯ä»ç”µå­å•†åŠ¡ç›´æ’­åˆ°å…¨æ¯é€šä¿¡ç­‰ä¸ä¸€è€Œè¶³ã€‚å°½ç®¡åœ¨ç”¨äºåˆ›å»ºé€¼çœŸåŠèº«åƒçš„3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…¨èº«å¯¹è¯ä»»åŠ¡ä¸­çš„é¢éƒ¨è¡¨æƒ…å’Œèº¯ä½“åŠ¨ä½œçš„ç²¾ç»†æ§åˆ¶æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚ï¼Œæ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œã€‚æˆ‘ä»¬æ¨å‡ºäº†TaoAvatarï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜ä¿çœŸã€è½»é‡çº§çš„3DGSå…¨èº«å¯¹è¯åŠèº«åƒï¼Œç”±å„ç§ä¿¡å·é©±åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆæ˜¯é€šè¿‡ç»‘å®šé«˜æ–¯æ¥åˆ›å»ºä¸ªæ€§åŒ–çš„ç©¿è¡£äººä½“å‚æ•°æ¨¡æ¿æ¥ä»£è¡¨å¤–è§‚ã€‚ç„¶åï¼Œæˆ‘ä»¬é¢„å…ˆè®­ç»ƒä¸€ä¸ªåŸºäºStyleUnetçš„ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„å§¿åŠ¿ç›¸å…³éåˆšæ€§å˜å½¢ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿæ•è·é«˜é¢‘å¤–è§‚ç»†èŠ‚ï¼Œä½†å¯¹äºç§»åŠ¨è®¾å¤‡æ¥è¯´èµ„æºè¿‡äºå¯†é›†ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€åˆ°ä¸€ä¸ªåŸºäºMLPçš„è½»é‡çº§ç½‘ç»œä¸­ï¼Œå¹¶å¼€å‘æ··åˆå½¢çŠ¶ä»¥å¼¥è¡¥ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTaoAvataråœ¨è¿è¡Œæ—¶å®ç°äº†å“è¶Šçš„è´¨é‡æ¸²æŸ“æ•ˆæœï¼ŒåŒæ—¶è·¨è¶Šå„ç§è®¾å¤‡è¿›è¡Œå®æ—¶æ¸²æŸ“ï¼Œåœ¨é«˜åˆ†è¾¨ç‡ç«‹ä½“å£°è®¾å¤‡ä¸Šå¦‚è‹¹æœè§†è§‰ä¸“ä¸šç‰ˆä¸Šä¿æŒ90å¸§æ¯ç§’çš„è¿è¡Œé€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17032v1">PDF</a> Accepted by CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://pixelai-team.github.io/TaoAvatar">https://PixelAI-Team.github.io/TaoAvatar</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå®æ—¶ä¿¡å·é©±åŠ¨çš„é«˜ä¿çœŸè½»é‡åŒ–3Då…¨èº«è¯´è¯Avatarçš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨3Dé«˜æ–¯è´´å›¾æŠ€æœ¯åˆ›å»ºä¸ªæ€§åŒ–æ¨¡æ¿ï¼Œå¹¶å¼•å…¥StyleUnetç½‘ç»œå¤„ç†å¤æ‚å§¿æ€ç›¸å…³çš„éåˆšæ€§å˜å½¢ã€‚ä¸ºè§£å†³ç§»åŠ¨è®¾å¤‡èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€è‡³è½»é‡çº§MLPç½‘ç»œä¸­ï¼Œå¹¶é€šè¿‡blend shapesè¡¥å¿ç»†èŠ‚ã€‚TaoAvataråœ¨ä¿æŒé«˜æ¸²æŸ“è´¨é‡çš„åŒæ—¶å®ç°å®æ—¶è¿è¡Œï¼Œé€‚ç”¨äºå„ç§è®¾å¤‡ï¼Œå¹¶åœ¨é«˜æ¸…ç«‹ä½“å£°è®¾å¤‡ä¸Šä¿æŒ90 FPSçš„å¸§ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Då…¨èº«è¯´è¯Avatarsåœ¨ARä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œä»ç”µå•†ç›´æ’­åˆ°å…¨æ¯é€šä¿¡ã€‚</li>
<li>ç°æœ‰3Dé«˜æ–¯è´´å›¾æŠ€æœ¯åœ¨ç²¾ç»†é¢éƒ¨è¡¨è¾¾å’Œå…¨èº«åŠ¨ä½œæ§åˆ¶ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>TaoAvataråˆ©ç”¨ä¸ªæ€§åŒ–æ¨¡æ¿å’ŒStyleUnetç½‘ç»œå¤„ç†å¤æ‚å§¿æ€çš„éåˆšæ€§å˜å½¢ã€‚</li>
<li>ä¸ºè§£å†³ç§»åŠ¨è®¾å¤‡èµ„æºæ¶ˆè€—é—®é¢˜ï¼Œé‡‡ç”¨è’¸é¦æŠ€æœ¯ä¼˜åŒ–ç½‘ç»œã€‚</li>
<li>é€šè¿‡blend shapesè¡¥å¿ç»†èŠ‚ï¼Œå®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</li>
<li>TaoAvatarè¾¾æˆå®æ—¶è¿è¡Œä¸é«˜è´¨é‡æ¸²æŸ“çš„å¹³è¡¡ï¼Œé€‚ç”¨äºå„ç§è®¾å¤‡ã€‚</li>
<li>TaoAvataråœ¨é«˜æ¸…ç«‹ä½“å£°è®¾å¤‡ä¸Šä¿æŒ90 FPSçš„å¸§ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d251751954528217baa3a66e8a790fbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2df1e3379d1a25574bbd5482acac7dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4577a665cc5fa624c1edd48d3f87970.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-Faces-to-Voices-Learning-Hierarchical-Representations-for-High-quality-Video-to-Speech"><a href="#From-Faces-to-Voices-Learning-Hierarchical-Representations-for-High-quality-Video-to-Speech" class="headerlink" title="From Faces to Voices: Learning Hierarchical Representations for   High-quality Video-to-Speech"></a>From Faces to Voices: Learning Hierarchical Representations for   High-quality Video-to-Speech</h2><p><strong>Authors:Ji-Hoon Kim, Jeongsoo Choi, Jaehun Kim, Chaeyoung Jung, Joon Son Chung</strong></p>
<p>The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis. A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech. This is achieved by learning of hierarchical representations from video to speech. Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages â€“ content, timbre, and prosody modeling. In each stage, we align visual factors â€“ lip movements, face identity, and facial expressions â€“ with corresponding acoustic counterparts to ensure the seamless transformation. Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution. Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç ”ç©¶ç›®æ ‡æ˜¯ä»æ— å£°çš„å¯¹è¯è§†é¢‘ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ï¼Œè¿™ä¸€ä»»åŠ¡ä¹Ÿè¢«ç§°ä¸ºè§†é¢‘åˆ°è¯­éŸ³çš„åˆæˆã€‚è§†é¢‘åˆ°è¯­éŸ³åˆæˆä¸­çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜åœ¨äºæ— å£°è§†é¢‘å’Œå¤šå…ƒè¯­éŸ³ä¹‹é—´å­˜åœ¨çš„å·¨å¤§æ¨¡æ€å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘åˆ°è¯­éŸ³ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæœ‰æ•ˆåœ°æ¡¥æ¥äº†è¿™ä¸€æ¨¡æ€å·®è·ï¼Œå¤§å¤§æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚è¿™æ˜¯é€šè¿‡å­¦ä¹ ä»è§†é¢‘åˆ°è¯­éŸ³çš„åˆ†å±‚è¡¨ç¤ºæ¥å®ç°çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªè¿ç»­é˜¶æ®µâ€”â€”å†…å®¹ã€éŸ³è°ƒå’ŒéŸµå¾‹å»ºæ¨¡ï¼Œé€æ¸å°†æ— å£°è§†é¢‘è½¬æ¢ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†è§†è§‰å› ç´ ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œã€é¢éƒ¨èº«ä»½å’Œé¢éƒ¨è¡¨æƒ…ï¼‰ä¸ç›¸åº”çš„å£°éŸ³å¯¹åº”å› ç´ è¿›è¡Œå¯¹é½ï¼Œä»¥ç¡®ä¿æ— ç¼è½¬æ¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»è§†è§‰è¡¨ç¤ºç”ŸæˆçœŸå®ä¸”è¿è´¯çš„è¯­éŸ³ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æµé‡åŒ¹é…æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»ç®€å•çš„å…ˆéªŒåˆ†å¸ƒä¼°è®¡å‡ºç›®æ ‡è¯­éŸ³åˆ†å¸ƒçš„ç›´æ¥è½¨è¿¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸çœŸå®è¯­éŸ³ç›¸æ¯”çš„ä¼˜ç§€ç”Ÿæˆè´¨é‡ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16956v1">PDF</a> CVPR 2025, demo page: <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/faces2voices/">https://mm.kaist.ac.kr/projects/faces2voices/</a></p>
<p><strong>Summary</strong><br>æä¾›ä¸€ç§æ–°çš„è§†é¢‘è½¬è¯­éŸ³ç³»ç»Ÿç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ã€‚é€šè¿‡è§†é¢‘å­¦ä¹ å±‚æ¬¡è¡¨ç¤ºä»¥ç¼©å°è§†é¢‘å’Œå¤æ‚è¯­éŸ³ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚é€šè¿‡å†…å®¹ã€éŸ³è´¨å’Œè¯­è°ƒå»ºæ¨¡å°†é™éŸ³è§†é¢‘è½¬æ¢ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ã€‚åˆ©ç”¨æµé‡åŒ¹é…æ¨¡å‹ä»è§†è§‰è¡¨ç¤ºç”Ÿæˆé€¼çœŸã€è¿è´¯çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘è½¬è¯­éŸ³åˆæˆä¸­çš„æŒ‘æˆ˜åœ¨äºé™é»˜è§†é¢‘ä¸å¤æ‚è¯­éŸ³ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘è½¬è¯­éŸ³ç³»ç»Ÿï¼Œé€šè¿‡å­¦ä¹ å±‚æ¬¡è¡¨ç¤ºæ¥æœ‰æ•ˆåœ°ç¼©çŸ­æ¨¡æ€å·®è·ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å°†é™é»˜è§†é¢‘é€æ­¥è½¬æ¢ä¸ºå£°éŸ³ç‰¹å¾ç©ºé—´ï¼ŒåŒ…æ‹¬å†…å®¹ã€éŸ³è´¨å’Œè¯­è°ƒå»ºæ¨¡ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>åœ¨æ¯ä¸ªé˜¶æ®µï¼Œè§†è§‰å› ç´ ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œã€é¢éƒ¨èº«ä»½å’Œé¢éƒ¨è¡¨æƒ…ï¼‰ä¸ç›¸åº”çš„å£°éŸ³å¯¹åº”ç‰©å¯¹é½ï¼Œä»¥ç¡®ä¿æ— ç¼è½¬æ¢ã€‚</li>
<li>åˆ©ç”¨æµé‡åŒ¹é…æ¨¡å‹ä»è§†è§‰è¡¨ç¤ºç”Ÿæˆé€¼çœŸçš„è¯­éŸ³ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸çœŸå®è¯­éŸ³ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cde97e2ffe93194b9c3831c7794a6585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b4e74c7247e36b83834c10b69bfc1c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-243c863070db1aed085e4ec0c3daf6ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dab6905813082bb76130317c6986d512.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synergizing-Motion-and-Appearance-Multi-Scale-Compensatory-Codebooks-for-Talking-Head-Video-Generation"><a href="#Synergizing-Motion-and-Appearance-Multi-Scale-Compensatory-Codebooks-for-Talking-Head-Video-Generation" class="headerlink" title="Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks   for Talking Head Video Generation"></a>Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks   for Talking Head Video Generation</h2><p><strong>Authors:Shuling Zhao, Fa-Ting Hong, Xiaoshui Huang, Dan Xu</strong></p>
<p>Talking head video generation aims to generate a realistic talking head video that preserves the personâ€™s identity from a source image and the motion from a driving video. Despite the promising progress made in the field, it remains a challenging and critical problem to generate videos with accurate poses and fine-grained facial details simultaneously. Essentially, facial motion is often highly complex to model precisely, and the one-shot source face image cannot provide sufficient appearance guidance during generation due to dynamic pose changes. To tackle the problem, we propose to jointly learn motion and appearance codebooks and perform multi-scale codebook compensation to effectively refine both the facial motion conditions and appearance features for talking face image decoding. Specifically, the designed multi-scale motion and appearance codebooks are learned simultaneously in a unified framework to store representative global facial motion flow and appearance patterns. Then, we present a novel multi-scale motion and appearance compensation module, which utilizes a transformer-based codebook retrieval strategy to query complementary information from the two codebooks for joint motion and appearance compensation. The entire process produces motion flows of greater flexibility and appearance features with fewer distortions across different scales, resulting in a high-quality talking head video generation framework. Extensive experiments on various benchmarks validate the effectiveness of our approach and demonstrate superior generation results from both qualitative and quantitative perspectives when compared to state-of-the-art competitors. </p>
<blockquote>
<p>å¤´éƒ¨è¯´è¯è§†é¢‘ç”Ÿæˆæ—¨åœ¨ä»æºå›¾åƒä¸­ä¿ç•™äººçš„èº«ä»½å’Œé©±åŠ¨è§†é¢‘ä¸­çš„åŠ¨ä½œï¼Œç”Ÿæˆé€¼çœŸçš„å¤´éƒ¨è¯´è¯è§†é¢‘ã€‚å°½ç®¡è¯¥é¢†åŸŸå–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ï¼Œä½†åŒæ—¶ç”Ÿæˆå…·æœ‰å‡†ç¡®å§¿åŠ¿å’Œç²¾ç»†é¢éƒ¨ç»†èŠ‚çš„è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œå…³é”®æ€§çš„é—®é¢˜ã€‚æœ¬è´¨ä¸Šï¼Œé¢éƒ¨åŠ¨ä½œé€šå¸¸éå¸¸å¤æ‚ï¼Œéš¾ä»¥ç²¾ç¡®å»ºæ¨¡ï¼Œè€Œä¸”ç”±äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€å§¿åŠ¿çš„æ”¹å˜ï¼Œå•æ¬¡æºé¢éƒ¨å›¾åƒæ— æ³•æä¾›è¶³å¤Ÿçš„å¤–è§‚æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè”åˆå­¦ä¹ è¿åŠ¨ä¸å¤–è§‚ç æœ¬ï¼Œå¹¶æ‰§è¡Œå¤šå°ºåº¦ç æœ¬è¡¥å¿ï¼Œä»¥æœ‰æ•ˆåœ°è°ƒæ•´é¢éƒ¨è¿åŠ¨æ¡ä»¶å’Œå¤–è§‚ç‰¹å¾ï¼Œç”¨äºè§£ç è¯´è¯é¢éƒ¨å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡çš„å¤šå°ºåº¦è¿åŠ¨ä¸å¤–è§‚ç æœ¬æ˜¯åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­åŒæ—¶å­¦ä¹ çš„ï¼Œä»¥å­˜å‚¨ä»£è¡¨æ€§çš„å…¨å±€é¢éƒ¨è¿åŠ¨æµå’Œå¤–è§‚æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šå°ºåº¦è¿åŠ¨å’Œå¤–è§‚è¡¥å¿æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨åŸºäºå˜å‹å™¨çš„ç æœ¬æ£€ç´¢ç­–ç•¥ï¼Œä»ä¸¤ä¸ªç æœ¬ä¸­æ£€ç´¢äº’è¡¥ä¿¡æ¯è¿›è¡Œè”åˆè¿åŠ¨å’Œå¤–è§‚è¡¥å¿ã€‚æ•´ä¸ªè¿‡ç¨‹äº§ç”Ÿäº†æ›´å¤§çµæ´»æ€§çš„è¿åŠ¨æµå’Œåœ¨ä¸åŒå°ºåº¦ä¸Šè¾ƒå°‘å˜å½¢çš„å¤–è§‚ç‰¹å¾ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤´éƒ¨è¯´è¯è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€å…ˆè¿›çš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œä»å®šæ€§å’Œå®šé‡ä¸¤ä¸ªè§’åº¦éƒ½å±•ç¤ºäº†æ›´ä¼˜è¶Šçš„ç”Ÿæˆç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00719v2">PDF</a> Accepted by CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://shaelynz.github.io/synergize-motion-appearance/">https://shaelynz.github.io/synergize-motion-appearance/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ç”ŸæˆæŠ€æœ¯åœ¨åˆ›å»ºçœŸå®æ„Ÿå¼ºçš„è¯´è¯äººè§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”ŸæˆåŒæ—¶åŒ…å«å‡†ç¡®å§¿åŠ¿å’Œç²¾ç»†é¢éƒ¨ç»†èŠ‚çš„è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆå­¦ä¹ è¿åŠ¨ä¸å¤–è§‚ç æœ¬å¹¶è¿›è¡Œå¤šå°ºåº¦ç æœ¬è¡¥å¿çš„æ–¹æ³•ï¼Œä»¥æé«˜é¢éƒ¨è¿åŠ¨æ¡ä»¶å’Œç‰¹å¾è¡¨ç°çš„ç²¾ç»†åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘ç”Ÿæˆæ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯å¤´è§†é¢‘ç”Ÿæˆæ—¨åœ¨ä»æºå›¾åƒä¸­ä¿ç•™ä¸ªäººèº«ä»½å¹¶ä»é©±åŠ¨è§†é¢‘ä¸­è·å–åŠ¨ä½œã€‚</li>
<li>ç²¾ç¡®å»ºæ¨¡é¢éƒ¨è¿åŠ¨å¹¶ä¿æŒåŠ¨æ€å§¿åŠ¿å˜åŒ–ä¸­çš„èº«ä»½ä¸€è‡´æ€§æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è”åˆå­¦ä¹ è¿åŠ¨ä¸å¤–è§‚ç æœ¬çš„æ–¹æ³•ï¼Œä»¥å­˜å‚¨ä»£è¡¨æ€§çš„é¢éƒ¨è¿åŠ¨æµå’Œå¤–è§‚æ¨¡å¼ã€‚</li>
<li>å¼•å…¥äº†å¤šå°ºåº¦è¿åŠ¨ä¸å¤–è§‚è¡¥å¿æ¨¡å—ï¼Œåˆ©ç”¨åŸºäºå˜å‹å™¨çš„ç æœ¬æ£€ç´¢ç­–ç•¥ï¼Œä»ä¸¤ä¸ªç æœ¬ä¸­æŸ¥è¯¢äº’è¡¥ä¿¡æ¯è¿›è¡Œè”åˆè¿åŠ¨å’Œå¤–è§‚è¡¥å¿ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†è¿åŠ¨æµçš„çµæ´»æ€§å’Œä¸åŒå°ºåº¦ä¸‹å¤–è§‚ç‰¹å¾çš„çœŸå®æ€§ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œä»å®šæ€§å’Œå®šé‡ä¸¤ä¸ªè§’åº¦éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-292d2b8fc152ec828601c57589910e35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362bc4b9ce3db373f8533bc24cccf30b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c13422ea76101cc85ff51869dd379f9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17cb045b007653c821a30674aca35ae.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LaDTalk-Latent-Denoising-for-Synthesizing-Talking-Head-Videos-with-High-Frequency-Details"><a href="#LaDTalk-Latent-Denoising-for-Synthesizing-Talking-Head-Videos-with-High-Frequency-Details" class="headerlink" title="LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High   Frequency Details"></a>LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High   Frequency Details</h2><p><strong>Authors:Jian Yang, Xukun Wang, Wentao Wang, Guoming Li, Qihang Fang, Ruihong Yuan, Tianyang Wang, Xiaomei Zhang, Yeying Jin, Zhaoxin Fan</strong></p>
<p>Audio-driven talking head generation is a pivotal area within film-making and Virtual Reality. Although existing methods have made significant strides following the end-to-end paradigm, they still encounter challenges in producing videos with high-frequency details due to their limited expressivity in this domain. This limitation has prompted us to explore an effective post-processing approach to synthesize photo-realistic talking head videos. Specifically, we employ a pretrained Wav2Lip model as our foundation model, leveraging its robust audio-lip alignment capabilities. Drawing on the theory of Lipschitz Continuity, we have theoretically established the noise robustness of Vector Quantised Auto Encoders (VQAEs). Our experiments further demonstrate that the high-frequency texture deficiency of the foundation model can be temporally consistently recovered by the Space-Optimised Vector Quantised Auto Encoder (SOVQAE) we introduced, thereby facilitating the creation of realistic talking head videos. We conduct experiments on both the conventional dataset and the High-Frequency TalKing head (HFTK) dataset that we curated. The results indicate that our method, LaDTalk, achieves new state-of-the-art video quality and out-of-domain lip synchronization performance. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è°ˆè¯å¤´ç”Ÿæˆæ˜¯ç”µå½±åˆ¶ä½œå’Œè™šæ‹Ÿç°å®ä¸­çš„ä¸€ä¸ªå…³é”®é¢†åŸŸã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å·²ç»éµå¾ªç«¯åˆ°ç«¯èŒƒå¼å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºå…¶åœ¨è¯¥é¢†åŸŸçš„è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆå…·æœ‰é«˜é¢‘ç»†èŠ‚çš„è§†é¢‘æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸€å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢ä¸€ç§æœ‰æ•ˆçš„åå¤„ç†æ–¹æ³•æ¥åˆæˆé€¼çœŸçš„è°ˆè¯å¤´è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„Wav2Lipæ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶ç¨³å¥çš„éŸ³é¢‘-å˜´å”‡å¯¹é½åŠŸèƒ½ã€‚å€ŸåŠ©Lipschitzè¿ç»­æ€§çš„ç†è®ºï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šå»ºç«‹äº†å‘é‡é‡åŒ–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQAEï¼‰çš„å™ªå£°é²æ£’æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œé€šè¿‡æˆ‘ä»¬å¼•å…¥çš„ç©ºé—´ä¼˜åŒ–å‘é‡é‡åŒ–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSOVQAEï¼‰ï¼Œå¯ä»¥æŒç»­æ¢å¤åŸºç¡€æ¨¡å‹çš„é«˜é¢‘çº¹ç†ç¼ºé™·ï¼Œä»è€Œæœ‰åŠ©äºåˆ›å»ºé€¼çœŸçš„è°ˆè¯å¤´è§†é¢‘ã€‚æˆ‘ä»¬åœ¨è‡ªå·±æ•´ç†çš„å¸¸è§„æ•°æ®é›†å’Œé«˜é¢‘è°ˆè¯å¤´ï¼ˆHFTKï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LaDTalkæ–¹æ³•å®ç°äº†æœ€æ–°çš„è§†é¢‘è´¨é‡å’ŒåŸŸå¤–å˜´å”‡åŒæ­¥æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00990v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆåœ¨å½±è§†åˆ¶ä½œå’Œè™šæ‹Ÿç°å®é¢†åŸŸçš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶éµå¾ªç«¯åˆ°ç«¯èŒƒå¼å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå…·æœ‰é«˜é¢‘ç»†èŠ‚çš„è§†é¢‘æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æœ‰æ•ˆçš„åå¤„ç†æ–¹æ³•æ¥åˆæˆé€¼çœŸçš„è¯´è¯äººå¤´è§†é¢‘ã€‚åŸºäºLipschitzè¿ç»­æ€§çš„ç†è®ºï¼Œä»ç†è®ºä¸Šè¯æ˜äº†å‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆVQAEsï¼‰çš„å™ªå£°é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå¼•å…¥çš„ç©ºé—´ä¼˜åŒ–å‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆSOVQAEï¼‰å¯ä»¥å¼¥è¡¥åŸºç¡€æ¨¡å‹çš„é«˜é¢‘çº¹ç†ç¼ºé™·ï¼Œä»è€Œç”Ÿæˆæ›´é€¼çœŸçš„è¯´è¯äººå¤´è§†é¢‘ã€‚åœ¨å¸¸è§„æ•°æ®é›†å’Œæˆ‘ä»¬æ•´ç†çš„é«˜é¢‘è¯´è¯äººå¤´ï¼ˆHFTKï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLaDTalkæ–¹æ³•å®ç°äº†æ–°çš„è§†é¢‘è´¨é‡é‡Œç¨‹ç¢‘å’Œå‡ºè‰²çš„è·¨åŸŸå”‡åŒæ­¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´ç”Ÿæˆåœ¨å½±è§†åˆ¶ä½œå’Œè™šæ‹Ÿç°å®é¢†åŸŸå…·æœ‰é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½ç„¶æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ä¸°å¯Œçš„è§†é¢‘æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åå¤„ç†æ–¹æ³•æ¥åˆæˆé€¼çœŸçš„è¯´è¯äººå¤´è§†é¢‘ã€‚</li>
<li>åˆ©ç”¨å‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆVQAEsï¼‰çš„å™ªå£°é²æ£’æ€§ç†è®ºæ¥å¢å¼ºè§†é¢‘è´¨é‡ã€‚</li>
<li>å¼•å…¥ç©ºé—´ä¼˜åŒ–å‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆSOVQAEï¼‰æ¥å¼¥è¡¥åŸºç¡€æ¨¡å‹çš„é«˜é¢‘çº¹ç†ç¼ºé™·ã€‚</li>
<li>LaDTalkæ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„è§†é¢‘åˆæˆå’Œå‡ºè‰²çš„è·¨åŸŸå”‡åŒæ­¥æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaa5cdcf1f0924a181a59dac310f26ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0b4d7713954709784f579d5d53f390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c910a4d4748a608ea901a9ab3abe048d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-061f41b8a40c046947f99300a6937603.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DEEPTalk-Dynamic-Emotion-Embedding-for-Probabilistic-Speech-Driven-3D-Face-Animation"><a href="#DEEPTalk-Dynamic-Emotion-Embedding-for-Probabilistic-Speech-Driven-3D-Face-Animation" class="headerlink" title="DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D   Face Animation"></a>DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D   Face Animation</h2><p><strong>Authors:Jisoo Kim, Jungbin Cho, Joonho Park, Soonmin Hwang, Da Eun Kim, Geon Kim, Youngjae Yu</strong></p>
<p>Speech-driven 3D facial animation has garnered lots of attention thanks to its broad range of applications. Despite recent advancements in achieving realistic lip motion, current methods fail to capture the nuanced emotional undertones conveyed through speech and produce monotonous facial motion. These limitations result in blunt and repetitive facial animations, reducing user engagement and hindering their applicability. To address these challenges, we introduce DEEPTalk, a novel approach that generates diverse and emotionally rich 3D facial expressions directly from speech inputs. To achieve this, we first train DEE (Dynamic Emotion Embedding), which employs probabilistic contrastive learning to forge a joint emotion embedding space for both speech and facial motion. This probabilistic framework captures the uncertainty in interpreting emotions from speech and facial motion, enabling the derivation of emotion vectors from its multifaceted space. Moreover, to generate dynamic facial motion, we design TH-VQVAE (Temporally Hierarchical VQ-VAE) as an expressive and robust motion prior overcoming limitations of VAEs and VQ-VAEs. Utilizing these strong priors, we develop DEEPTalk, a talking head generator that non-autoregressively predicts codebook indices to create dynamic facial motion, incorporating a novel emotion consistency loss. Extensive experiments on various datasets demonstrate the effectiveness of our approach in creating diverse, emotionally expressive talking faces that maintain accurate lip-sync. Our project page is available at <a target="_blank" rel="noopener" href="https://whwjdqls.github.io/deeptalk/_website/">https://whwjdqls.github.io/deeptalk\_website/</a> </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»å› å…¶å¹¿æ³›çš„åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡æœ€è¿‘å®ç°äº†ç°å®ä¸»ä¹‰çš„å”‡éƒ¨è¿åŠ¨ï¼Œä½†å½“å‰çš„æ–¹æ³•æ— æ³•æ•æ‰é€šè¿‡è¯­éŸ³ä¼ è¾¾çš„å¾®å¦™æƒ…ç»ªç»†å¾®å·®åˆ«ï¼Œå¹¶äº§ç”Ÿå•è°ƒçš„é¢éƒ¨è¿åŠ¨ã€‚è¿™äº›å±€é™æ€§å¯¼è‡´é¢éƒ¨åŠ¨ç”»ç”Ÿç¡¬ä¸”é‡å¤ï¼Œé™ä½äº†ç”¨æˆ·å‚ä¸åº¦å¹¶é˜»ç¢äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DEEPTalkï¼Œè¿™æ˜¯ä¸€ç§ä»è¯­éŸ³è¾“å…¥ç›´æ¥ç”Ÿæˆå¤šæ ·ä¸”æƒ…æ„Ÿä¸°å¯Œçš„3Dé¢éƒ¨è¡¨æƒ…çš„æ–°æ–¹æ³•ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒDEEï¼ˆåŠ¨æ€æƒ…ç»ªåµŒå…¥ï¼‰ï¼Œå®ƒä½¿ç”¨æ¦‚ç‡å¯¹æ¯”å­¦ä¹ æ¥æ„å»ºåŒ…å«è¯­éŸ³å’Œé¢éƒ¨è¿åŠ¨çš„è”åˆæƒ…ç»ªåµŒå…¥ç©ºé—´ã€‚æ­¤æ¦‚ç‡æ¡†æ¶æ•æ‰äº†ä»è¯­éŸ³å’Œé¢éƒ¨è¿åŠ¨ä¸­è§£é‡Šæƒ…ç»ªçš„çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œèƒ½å¤Ÿä»å¤šå…ƒæƒ…ç»ªç©ºé—´ä¸­å¾—å‡ºæƒ…ç»ªå‘é‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç”ŸæˆåŠ¨æ€é¢éƒ¨è¿åŠ¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†TH-VQVAEï¼ˆæ—¶åºåˆ†å±‚VQ-VAEï¼‰ä½œä¸ºè¡¨ç°åŠ›å¼ºä¸”ç¨³å¥çš„è¿åŠ¨å…ˆéªŒï¼Œå…‹æœäº†VAEå’ŒVQ-VAEçš„å±€é™æ€§ã€‚åˆ©ç”¨è¿™äº›å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬å¼€å‘äº†DEEPTalké¢éƒ¨è¯´è¯è€…ç”Ÿæˆå™¨ï¼Œå®ƒä»¥éè‡ªå›å½’æ–¹å¼é¢„æµ‹ç¼–ç æœ¬ç´¢å¼•æ¥åˆ›å»ºåŠ¨æ€é¢éƒ¨è¿åŠ¨ï¼Œå¹¶ç»“åˆäº†æ–°é¢–çš„æƒ…ç»ªä¸€è‡´æ€§æŸå¤±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ›å»ºå¤šæ ·ä¸”æƒ…æ„Ÿä¸°å¯Œçš„è¯´è¯é¢éƒ¨æ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®çš„å”‡åŒæ­¥ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://whwjdqls.github.io/deeptalk%5C_website/%E6%89%BE%E5%88%B0%E3%80%82">https://whwjdqls.github.io/deeptalk_website&#x2F;æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06010v3">PDF</a> First two authors contributed equally. This is a revised version of   the original submission, which has been accepted for publication at AAAI 2025</p>
<p><strong>Summary</strong><br>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»å› å¹¿æ³›çš„åº”ç”¨é¢†åŸŸè€Œå—åˆ°å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•æ— æ³•å®ç°ä»è¯­éŸ³ä¸­æ•æ‰å¾®å¦™çš„æƒ…ç»ªå˜åŒ–ï¼Œå¯¼è‡´åŠ¨ç”»è¡¨æƒ…å•è°ƒã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDEEPTalkæ–¹æ³•ï¼Œç›´æ¥ä»è¯­éŸ³è¾“å…¥ç”Ÿæˆä¸°å¯Œæƒ…æ„Ÿçš„3Dé¢éƒ¨è¡¨æƒ…ã€‚é€šè¿‡è®­ç»ƒåŠ¨æ€æƒ…æ„ŸåµŒå…¥ï¼ˆDEEï¼‰å’Œä½¿ç”¨TH-VQVAEä½œä¸ºåŠ¨æ€é¢éƒ¨è¿åŠ¨ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºå…·æœ‰å¤šæ ·æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾çš„è¯´è¯äººè„¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»å­˜åœ¨æ— æ³•æ•æ‰æƒ…ç»ªç»†å¾®å˜åŒ–çš„é—®é¢˜ï¼Œå¯¼è‡´åŠ¨ç”»è¡¨æƒ…å•è°ƒã€‚</li>
<li>DEEPTalkæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç›´æ¥ä»è¯­éŸ³è¾“å…¥ç”Ÿæˆä¸°å¯Œæƒ…æ„Ÿçš„3Dé¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>DEEPTalké€šè¿‡è®­ç»ƒåŠ¨æ€æƒ…æ„ŸåµŒå…¥ï¼ˆDEEï¼‰å’Œä½¿ç”¨TH-VQVAEæŠ€æœ¯æ¥è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>åŠ¨æ€æƒ…æ„ŸåµŒå…¥ï¼ˆDEEï¼‰é‡‡ç”¨æ¦‚ç‡å¯¹æ¯”å­¦ä¹ ï¼Œä¸ºè¯­éŸ³å’Œé¢éƒ¨è¿åŠ¨åˆ›å»ºè”åˆæƒ…æ„ŸåµŒå…¥ç©ºé—´ã€‚</li>
<li>TH-VQVAEä½œä¸ºä¸€ç§è¡¨è¾¾æ€§å¼ºä¸”ç¨³å¥çš„è¿åŠ¨å…ˆéªŒï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸVAEå’ŒVQ-VAEçš„å±€é™æ€§ã€‚</li>
<li>DEEPTalkåˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†éè‡ªå›å½’åœ°é¢„æµ‹ä»£ç æœ¬ç´¢å¼•ï¼Œä»¥åˆ›å»ºåŠ¨æ€é¢éƒ¨è¿åŠ¨ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEEPTalkæ–¹æ³•èƒ½åˆ›å»ºå¤šæ ·ä¸”æƒ…æ„Ÿä¸°å¯Œçš„è¯´è¯äººè„¸ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®çš„å”‡åŒæ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc662c1d4ed73ae28e5b18155d997e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488e3980a19bcbaa53d7db7d00425d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b780d64de5508ab8e0e84fd9cd391b87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a83b1c6f89114ff4d79de3cca28e6a68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58dee970715d51609b399098cb619b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-27/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ded493e8d8cf71fa8db80d83aaf16444.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Dance Like a Chicken Low-Rank Stylization for Human Motion Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-27/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0d1d23a0ee3093c5e5ba8bdc856c1f9e.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-27  Information geometry of chemical reaction networks Cramer-Rao bound and   absolute sensitivity revisited
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
