<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-05-29  ArVoice A Multi-Speaker Dataset for Arabic Speech Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8de2ae4d28c9195f7096460090da067c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-29-更新"><a href="#2025-05-29-更新" class="headerlink" title="2025-05-29 更新"></a>2025-05-29 更新</h1><h2 id="ArVoice-A-Multi-Speaker-Dataset-for-Arabic-Speech-Synthesis"><a href="#ArVoice-A-Multi-Speaker-Dataset-for-Arabic-Speech-Synthesis" class="headerlink" title="ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis"></a>ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis</h2><p><strong>Authors:Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki</strong></p>
<p>We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech corpus with diacritized transcriptions, intended for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a new professionally recorded set from six voice talents with diverse demographics, (2) a modified subset of the Arabic Speech Corpus; and (3) high-quality synthetic speech from two commercial systems. The complete corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. We train three open-source TTS and two voice conversion systems to illustrate the use cases of the dataset. The corpus is available for research use. </p>
<blockquote>
<p>我们介绍了ArVoice，这是一个多说话人现代标准阿拉伯语（MSA）语音语料库，带有加音符号的转录，旨在用于多说话人语音合成，并且对其他任务如基于语音的音标恢复、声音转换和深度伪造检测等也有用。ArVoice包括：（1）由具有不同人口统计特征的六位语音人才录制的新专业集，（2）阿拉伯语音库的修改子集；（3）两个商业系统的高品质合成语音。完整的语料库包含总计83.52小时的语音，跨越11种声音；其中约10小时是人类声音，来自7位说话者。我们训练了三个开源的TTS和两种语音转换系统，以说明该数据集的使用情况。该语料库可用于研究用途。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20506v1">PDF</a> Accepted at INTERSPEECH 2025 The dataset is available at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MBZUAI/ArVoice">https://huggingface.co/datasets/MBZUAI/ArVoice</a></p>
<p><strong>Summary</strong></p>
<p>ArVoice是一个多说话人现代标准阿拉伯语（MSA）语音语料库，包含带变音符的转录，主要用于多说话人语音合成，也可用于其他任务，如变音符恢复、语音转换和深度伪造检测。它包含专业录制的数据集、修改的阿拉伯语音语料库子集以及两个商业系统的高质量合成语音。该语料库共有83.52小时的语音数据，涵盖11个声音，其中约10小时是人类声音。为展示数据集的应用场景，训练了三个开源文本转语音系统和两个语音转换系统。该语料库可用于研究用途。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ArVoice是一个多说话人的现代标准阿拉伯语语音语料库，包含带变音符的转录。</li>
<li>该语料库可用于多说话人语音合成及其他任务，如变音符恢复、语音转换和深度伪造检测。</li>
<li>ArVoice包含专业录制的数据集、修改的阿拉伯语音语料库子集以及从两个商业系统生成的高质量合成语音。</li>
<li>完整语料库包含83.52小时的语音数据，涵盖11个声音，其中约10小时是人类声音。</li>
<li>为展示数据集的应用场景，该论文训练了三个开源文本转语音系统和两个语音转换系统。</li>
<li>ArVoice语料库可用于研究用途。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f508c0c483d618d7f908187ab936b4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11b575fd77bd35091d87f19508d7b79a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde2ea52cb9f2c6846d87bab7ed1a453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43aae3111c5affe9771afe481b18d071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e433b42d4f9eac3742545d79da9730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8653c45e54f3c54dcf5aa31f799f25d8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Guided-by-Gut-Efficient-Test-Time-Scaling-with-Reinforced-Intrinsic-Confidence"><a href="#Guided-by-Gut-Efficient-Test-Time-Scaling-with-Reinforced-Intrinsic-Confidence" class="headerlink" title="Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic   Confidence"></a>Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic   Confidence</h2><p><strong>Authors:Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu</strong></p>
<p>Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques. </p>
<blockquote>
<p>测试时缩放（TTS）方法通常用于增强大型语言模型（LLM）的推理能力，但会产生巨大的计算成本，这主要是因为严重依赖外部过程奖励模型（PRM）或如最佳N（BoN）这样的采样方法。本文介绍了“听从内心”（GG）这一高效的自我引导TTS框架，它无需昂贵的外部验证模型即可实现PRM级别的性能。我们的方法采用轻量级的树搜索，仅由内在LLM信号、令牌级信心和步骤新颖性来指导。一个关键的创新点是通过有针对性的强化学习微调阶段来提高内部信心估计的可靠性。在具有挑战性的数学推理基准测试上的经验评估表明，GG使较小的模型（例如，1.5B参数）能够达到或超过显著较大的模型（例如，32B-70B参数）的准确度，同时将GPU内存使用率降低高达10倍。与基于PRM的方法相比，GG以8倍的推理速度实现了相当的精度，并降低了4-5倍的内存使用率。此外，与BoN策略相比，GG减少了大约50%的KV缓存内存使用，使TTS技术的部署更加高效实用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20325v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了名为Guided by Gut的TTS框架，它能够在不使用昂贵的外部验证模型的情况下实现PRM级别的性能。该框架通过采用轻量级树搜索、基于LLM的固有信号、令牌级别的置信度和步骤新颖性来工作。重要创新之一是通过有针对性的强化学习微调阶段提高了内部置信度估计的可靠性。它在具有挑战性的数学推理基准测试上的实证评估表明，GG使较小的模型能够实现与较大的模型相匹配或更高的准确性，同时减少了GPU内存使用量。与PRM方法相比，GG实现了相当的准确性，同时推理速度提高了8倍，内存使用率降低了4-5倍。此外，与BoN策略相比，GG减少了KV缓存内存的使用量约50%，使得TTS技术的部署更加高效实用。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本的主要见解：</p>
<ul>
<li>Guided by Gut是一个高效的自我引导TTS框架，无需依赖昂贵的外部验证模型即可实现PRM级别的性能。</li>
<li>该框架通过轻量级树搜索、固有LLM信号进行工作，并结合令牌级别的置信度和步骤新颖性来实现高效推理。</li>
<li>通过强化学习微调阶段提高了内部置信度估计的可靠性。</li>
<li>GG能在较小的模型上实现与较大的模型相匹配或更高的准确性，并在数学推理基准测试中表现出优越性能。</li>
<li>与PRM方法相比，GG提供了更快的推理速度和更低的内存使用效率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20325">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d8dff4a4feda3b392affa35b98fa022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a12c12765a5cfdb3707d540665c56432.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-815c2765b5ca6af423d8602dc80e110a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PMOA-TTS-Introducing-the-PubMed-Open-Access-Textual-Times-Series-Corpus"><a href="#PMOA-TTS-Introducing-the-PubMed-Open-Access-Textual-Times-Series-Corpus" class="headerlink" title="PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus"></a>PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</h2><p><strong>Authors:Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss</strong></p>
<p>Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open Access (PMOA) case reports, each converted into structured (event, time) timelines via a scalable LLM-based pipeline. Our approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1, resulting in over 5.6 million timestamped clinical events. To assess timeline quality, we evaluate against a clinician-curated reference set using three metrics: (i) event-level matching (80% match at a cosine similarity threshold of 0.1), (ii) temporal concordance (c-index &gt; 0.90), and (iii) Area Under the Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide diagnostic and demographic coverage. In a downstream survival prediction task, embeddings from extracted timelines achieve time-dependent concordance indices up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally structured narratives. PMOA-TTS provides a scalable foundation for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP. The dataset is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/snoroozi/pmoa-tts">https://huggingface.co/datasets/snoroozi/pmoa-tts</a> . </p>
<blockquote>
<p>理解临床叙述中的时间动态对于建立患者轨迹模型至关重要，然而大规模的时间注释资源仍然有限。我们推出了PMOA-TTS，这是第一个公开可用的数据集，包含124,699篇PubMed Open Access（PMOA）病例报告，每个报告都通过可扩展的LLM管道转换为结构化（事件，时间）时间表。我们的方法结合了启发式过滤和Llama 3.3来识别单个患者的病例报告，然后使用Llama 3.3和DeepSeek R1进行提示驱动提取，产生了超过560万的时间戳临床事件。为了评估时间表的质量，我们使用临床医生编制的参考集，通过三个指标进行评估：（i）事件级别匹配（在余弦相似性阈值为0.1的情况下达到80%的匹配度），（ii）时间一致性（c指数&gt; 0.9），以及（iii）时间戳对齐的Log-Time CDF下的面积（AULTC）。语料库级别的分析显示出广泛的诊断和人口覆盖面。在下游的生存预测任务中，从提取的时间表中获得的嵌入达到了高达0.82±0.01的时间依赖性一致性指数，证明了时序结构化叙述的预测价值。PMOA-TTS为时间序列提取、时间推理和纵向建模提供了可扩展的基础，在生物医学NLP领域具有广泛应用。数据集可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/snoroozi/pmoa-tts%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/datasets/snoroozi/pmoa-tts找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了PMOA-TTS数据集，这是首个公开的、包含124,699篇PubMed Open Access案例报告的数据集。通过可扩展的LLM管道将这些报告转换为结构化（事件，时间）时间线。数据集用于评估时间线质量，并在下游生存预测任务中表现出良好的预测性能。PMOA-TTS为生物医学NLP中的时间线提取、时间推理和纵向建模提供了可扩展的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PMOA-TTS是首个公开的、大规模的临床叙事数据集，包含124,699篇PubMed Open Access案例报告。</li>
<li>利用LLM管道实现了结构化的（事件，时间）时间线转换。</li>
<li>结合启发式过滤和Llama 3.3技术，以及基于prompt的提取和DeepSeek R1技术，实现了超过560万的时间戳临床事件提取。</li>
<li>通过三个指标评估时间线质量：事件级别匹配、时间一致性和时间戳对齐的累积分布函数下的面积（AULTC）。</li>
<li>数据集涵盖广泛的诊断和人口统计数据，具有良好的覆盖面。</li>
<li>在下游生存预测任务中，提取的时间线嵌入达到了较高的预测性能，显示了结构化叙事的时间预测价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20323">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b4eb108c98939a7c7531fc4f37f997c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a25ea353f18ae4d849ab2147236fb4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9753622025e9386ab3a4959b2c2d71f9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling"><a href="#Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling" class="headerlink" title="Faster and Better LLMs via Latency-Aware Test-Time Scaling"></a>Faster and Better LLMs via Latency-Aware Test-Time Scaling</h2><p><strong>Authors:Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu</strong></p>
<p>Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demonstrate that a compute-optimal TTS does not always result in the lowest latency in scenarios where latency is critical. To address this gap and achieve latency-optimal TTS, we propose two key approaches by optimizing the concurrency configurations: (1) branch-wise parallelism, which leverages multiple concurrent inference branches, and (2) sequence-wise parallelism, enabled by speculative decoding. By integrating these two approaches and allocating computational resources properly to each, our latency-optimal TTS enables a 32B model to reach 82.3% accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4% within 10 seconds. Our work emphasizes the importance of latency-aware TTS and demonstrates its ability to deliver both speed and accuracy in latency-sensitive scenarios. </p>
<blockquote>
<p>测试时缩放（TTS）已证明在推理过程中可以提高大型语言模型（LLM）的性能。然而，现有研究从延迟敏感的角度忽视了TTS的效率。通过对代表性TTS方法进行延迟感知评估，我们证明计算最优的TTS并不总是导致延迟最低，这在延迟至关重要的场景中尤为关键。为了解决这一差距并实现延迟最优的TTS，我们提出了两种通过优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）通过猜测解码实现序列并行性。通过整合这两种方法并为每种方法适当分配计算资源，我们的延迟最优TTS使32B模型在MATH-500上1分钟内达到82.3%的准确率，较小的3B模型在10秒内达到72.4%的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在延迟敏感场景中实现速度和准确性的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19634v2">PDF</a> </p>
<p><strong>Summary</strong><br>     测试时间尺度（TTS）可改善大型语言模型（LLM）的推理性能。但现有研究未从延迟敏感的角度充分评估TTS的效率。通过延迟感知的评估方法，我们发现计算最优的TTS并不总是导致最低延迟，这在延迟至关重要的情况下尤为明显。为解决这一差距并实现延迟优化的TTS，我们提出两种优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）序列并行性，通过投机解码实现。通过整合这两种方法并为每个分配适当的计算资源，我们的延迟优化TTS使32B模型在MATH-500上在1分钟内达到82.3％的准确率，较小的3B模型在10秒内达到72.4％的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在延迟敏感场景中实现速度和准确性的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS在提高大型语言模型推理性能方面具有有效性。</li>
<li>现有研究未充分从延迟敏感的角度评估TTS的效率。</li>
<li>计算最优的TTS并不总是导致最低延迟。</li>
<li>提出分支并行性和序列并行性两种优化并发配置的方法。</li>
<li>通过整合这两种方法，延迟优化TTS能在短时间内实现较高的准确率。</li>
<li>延迟优化TTS在延迟敏感场景中能同时提高速度和准确性。</li>
<li>重视延迟感知的TTS是很重要的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e20890718528c2f1bb2901688234450.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-235616815750083229d9ac47ab79bf1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc11049b2a067213003b77acf7e59eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fa7f64c34abc1a76feb38474c46ef11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb2ab025abe0288742ee07fedafd092.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c9a08d1815a39687e6ed039c7f59d70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training"><a href="#CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training" class="headerlink" title="CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training"></a>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training</h2><p><strong>Authors:Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye</strong></p>
<p>In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3">https://funaudiollm.github.io/cosyvoice3</a>. </p>
<blockquote>
<p>在我们之前的工作中，我们推出了一款可扩展的流式语音合成模型CosyVoice 2，它集成了一个大型语言模型（LLM）和一块分块感知流匹配（FM）模型，实现了低延迟双流式语音合成和与人类相当的质量。尽管有了这些进步，CosyVoice 2在语言覆盖、领域多样性、数据量、文本格式和后续训练技术方面仍存在局限性。在本文中，我们介绍了CosyVoice 3，这是一款改进后的模型，旨在实现野外零镜头多语言语音合成，在内容一致性、演讲者相似性和语调自然性方面超越了其前身。CosyVoice 3的主要特点包括：1）一种新型语音标记器，通过监督多任务训练开发，旨在提高语调的自然性，包括自动语音识别、语音情感识别、语言识别、音频事件检测和说话人分析。2）一种新的可微奖励模型，适用于训练后的训练，不仅适用于CosyVoice 3，而且适用于其他基于LLM的语音合成模型。3）数据集大小缩放：训练数据从一万小时扩展到一百万小时，涵盖9种语言和18种中文方言，跨越各种领域和文本格式。4）模型大小缩放：模型参数从0.5亿增加到15亿，由于模型容量更大，我们的多语种基准测试性能得到提升。这些进步对野外语音合成的进展做出了重大贡献。我们鼓励读者在<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3%E4%B8%8A%E5%90%AC%E5%8F%96%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice3上听取演示。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17589v2">PDF</a> Preprint, work in progress</p>
<p><strong>Summary</strong><br>     新一代语音合成模型CosyVoice 3问世，集成了大型语言模型与一系列新技术，支持零起点多语种语音合成。相比上一代，它在内容一致性、说话人相似性和语调自然度方面取得显著进步。采用了全新的语音分词器，能应对各种语音任务。推出新型可微调节奖励模型，扩大训练数据集至一千万小时，涵盖九种语言和十八种中文方言。模型参数增至十五亿，显著提升多语种基准测试性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CosyVoice 3引入了新型零起点多语种语音合成技术，提高了内容一致性、说话人相似性和语调自然度。</li>
<li>采用了全新的语音分词器，通过多任务训练提升语调自然度。</li>
<li>推出新型可微调节奖励模型，适用于多种语言模型。</li>
<li>训练数据集大幅扩展至一千万小时，涵盖多种语言和方言。</li>
<li>模型参数增至十五亿，增强了多语种性能表现。</li>
<li>新模型在多种领域和文本格式上表现出强大的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a7352d250abd85e3ba1e29ce146bf593.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a151dcf188a216bc252c52f1eeb9500f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-becc3688ac33167ec55c47a327468b56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c3b320b2d2bf7d701dc185ecaa1938c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning"><a href="#SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning" class="headerlink" title="SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning"></a>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</h2><p><strong>Authors:Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</strong></p>
<p>Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model’s parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT">https://github.com/xuyige/SoftCoT</a>. </p>
<blockquote>
<p>测试时缩放（TTS）是指通过在推理过程中分配额外的计算资源来提高推理性能的方法，而不会改变模型的参数。虽然现有的TTS方法在离散标记空间中进行操作，通过生成更多的中间步骤来工作，但最近的Coconut和SoftCoT研究表明，在连续潜在空间中进行思考可以进一步提高推理性能。这种潜在的想法可以编码信息丰富的思考过程，而没有自回归标记生成所带来的信息损失，这引发了人们对连续空间推理的浓厚兴趣。与离散解码不同，离散解码通过重复采样可以探索多样的推理路径，而连续空间中的潜在表示对于给定输入是固定的，这限制了多样的探索，因为所有解码路径都源于相同的潜在想法。为了克服这一局限性，我们引入SoftCoT++，将SoftCoT扩展到测试时缩放范式，通过特殊的初始标记扰动思考路径，并应用对比学习来促进软思考表示之间的多样性。在五个推理基准测试和两种不同的大型语言模型架构上的实验表明，SoftCoT++显著提升了SoftCoT的性能，并且优于SoftCoT的自一致性缩放。此外，它显示出与常规缩放技术（如自一致性）的强大兼容性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xuyige/SoftCoT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11484v2">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>文本描述了Test-Time Scaling（TTS）的概念及其在推理性能提升方面的作用。传统TTS方法在离散标记空间操作，而Coconut和SoftCoT研究显示连续潜在空间思考能提高推理性能。为克服连续空间中潜在表示带来的探索多样性限制，提出SoftCoT++方法，通过多个专用初始标记扰动潜在思考，并采用对比学习促进软思考表示之间的多样性。实验表明，SoftCoT++在五个推理基准测试和两个不同的大型语言模型架构上显著提升了SoftCoT的性能，并且与传统扩展技术如自我一致性扩展兼容。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-Time Scaling (TTS) 是指在推理过程中分配额外的计算资源以提高性能，而不改变模型的参数。</li>
<li>传统TTS方法在离散标记空间操作，但连续潜在空间思考能提高推理性能。</li>
<li>SoftCoT++方法通过多个专用初始标记扰动潜在思考，促进推理过程中的多样性探索。</li>
<li>SoftCoT++显著提升了SoftCoT的性能，并在多个推理基准测试上表现优异。</li>
<li>SoftCoT++与传统扩展技术如自我一致性扩展兼容。</li>
<li>连续空间思考减少了信息损失，不同于离散解码的重复采样能够探索不同的推理路径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11484">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d2acca183235a222b1ca6a2aa8c76020.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2382ea27746d2e90e5e24c55497ccfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec8f495f6f4e58fe912ea8227a6d7907.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-MUSHRA-Addressing-Modern-Challenges-in-Text-to-Speech-Evaluation"><a href="#Rethinking-MUSHRA-Addressing-Modern-Challenges-in-Text-to-Speech-Evaluation" class="headerlink" title="Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech   Evaluation"></a>Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech   Evaluation</h2><p><strong>Authors:Praveen Srinivasa Varadhan, Amogh Gulati, Ashwin Sankar, Srija Anand, Anirudh Gupta, Anirudh Mukherjee, Shiva Kumar Marepally, Ankur Bhatia, Saloni Jaju, Suvrat Bhooshan, Mitesh M. Khapra</strong></p>
<p>Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS’s pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems. </p>
<blockquote>
<p>尽管TTS模型发展迅速，但缺乏一个稳定且强大的人类评估框架。例如，MOS测试无法区分相似的模型，CMOS的配对比较又很耗时。MUSHRA测试是评估多个TTS系统的一个很有前景的替代方法，但在这项工作中我们发现，它对匹配人类参考语音的依赖，过分地惩罚了现代TTS系统的得分，这些系统的语音质量甚至超过了人类。更具体地说，我们对MUSHRA测试进行了全面的评估，重点研究其对评分者差异性、听众疲劳和参考偏见等因素的敏感性。基于涉及印度语和泰米尔语共492名听众的广泛评估，我们发现了两个主要问题：（i）参考匹配偏见，评分者受到人为参考的过度影响；（ii）由于缺少明确的精细指导而产生的判断模糊性。为了解决这些问题，我们提出了两种改进的MUSHRA测试变种。第一种变体允许对超越人类参考质量的合成样本进行更公平的评分。第二种变体通过减少评分者之间的相对较低的方差来降低模糊性。通过结合这两种方法，我们实现了更可靠、更精细的评估。我们还发布了MANGO数据集，这是一个人类评分的巨大数据集，包含24万6千条评分，是印度语言领域首创的此类数据集，有助于分析人类偏好并开发用于评估TTS系统的自动度量指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12719v3">PDF</a> Accepted in TMLR</p>
<p><strong>摘要</strong></p>
<p>本文探讨了TTS模型评估中存在的问题。尽管TTS技术发展迅速，但仍缺乏一致且稳健的人机评估框架。现有的MOS测试无法区分相似模型，CMOS的成对比较则耗时过长。MUSHRA测试是一种有望同时评估多个TTS系统的测试方法，但它依赖匹配人类参考语音，对现代超过人类语音质量的TTS系统有所不公。本文对MUSHRA测试进行了全面的评估，重点关注评委差异性、听者疲劳和参考偏见等因素的影响。通过涉及印地语和泰米尔语的492名人类听者的广泛评估，发现了两大问题：一是参考匹配偏见，评委受到人类参考的不当影响；二是判断模糊，由于缺乏明确的细分指南。为解决这些问题，本文提出了两种改进的MUSHRA测试方法，一种为合成样本超过人类参考质量提供更公平的评分，另一种减少模糊性，降低评委之间的方差。此外，还发布了MANGO数据集，包含24.6万条人类评分，成为印度语言领域的首创集合，有助于分析人类偏好并为TTS系统的发展提供自动度量标准。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TTS领域缺乏一致且稳健的人机评估框架，现有测试方法存在缺陷。</li>
<li>MUSHRA测试是一种有前途的评估方法，但存在参考匹配偏见和对现代TTS系统的评分不公问题。</li>
<li>通过广泛评估发现评委差异性、听者疲劳和参考偏见等因素影响MUSHRA测试的准确性。</li>
<li>提出两种改进的MUSHRA测试方法，旨在更公平地评估超过人类参考质量的TTS系统并减少判断模糊性。</li>
<li>发布了MANGO数据集，成为印度语言领域的首创大规模人类评分集合，有助于分析人类偏好和发展TTS系统的自动度量标准。</li>
<li>数据集包含丰富的语音样本和评分数据，为深入研究提供了宝贵的资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12719">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8de2ae4d28c9195f7096460090da067c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91ee62cc7411685dfeddd28fa1220a37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d62a5eae999578a945d67fd015fb7bbd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Speech-Synthesis-without-Vector-Quantization"><a href="#Autoregressive-Speech-Synthesis-without-Vector-Quantization" class="headerlink" title="Autoregressive Speech Synthesis without Vector Quantization"></a>Autoregressive Speech Synthesis without Vector Quantization</h2><p><strong>Authors:Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei</strong></p>
<p>We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at <a target="_blank" rel="noopener" href="https://aka.ms/melle">https://aka.ms/melle</a>. </p>
<blockquote>
<p>我们提出了MELLE，这是一种基于连续值标记的新的文本转语音合成（TTS）语言建模方法。MELLE通过文本条件直接自回归生成连续的梅尔频谱帧，避免了向量量化的需要。向量量化通常是为音频压缩设计的，相较于连续表示形式牺牲了保真度。具体来说，（i）我们没有使用交叉熵损失，而是应用了回归损失和提出的频谱流损失函数来模拟连续值标记的概率分布；（ii）我们将变分推断融入MELLE，以优化采样机制，从而提高了输出多样性和模型稳健性。实验证明，相较于两阶段编码语言模型VALL-E及其变体，单阶段的MELLE通过避免采样向量量化码的固有缺陷，解决了稳健性问题，在多个指标上实现了卓越的性能，并且最重要的是，提供了更为简洁的模式。我们的工作演示地址为<a target="_blank" rel="noopener" href="https://aka.ms/melle%E3%80%82">https://aka.ms/melle。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08551v2">PDF</a> Accepted to ACL 2025 Main</p>
<p><strong>Summary</strong><br>MELLE是一种基于连续值令牌的语言建模方法，用于文本到语音合成（TTS）。它直接生成连续的梅尔频谱帧，绕过向量量化的需求，从而提高了音频质量。该方法使用回归损失和提出的频谱流损失函数建模连续值令牌的概率分布，并融入变分推断提高采样机制和模型稳健性。实验证明，相比两阶段编解码器语言模型VALL-E及其变体，单阶段的MELLE避免了采样向量量化代码的固有缺陷，实现了跨多个指标的优越性能，并提供了更简洁的范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MELLE是一种新型的连续值令牌语言建模方法，用于文本到语音合成（TTS）。</li>
<li>MELLE直接生成连续的梅尔频谱帧，避免了向量量化的需求，从而提高音频质量。</li>
<li>使用回归损失和频谱流损失函数建模连续值令牌的概率分布。</li>
<li>变分推断被融入MELLE以提高采样机制和模型稳健性。</li>
<li>MELLE相比两阶段编解码器语言模型VALL-E及其变体，在多个指标上实现优越性能。</li>
<li>MELLE避免了采样向量量化代码的固有缺陷。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08551">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea5639a13fcda3600c5da98d3fc1dd31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a2810e80e26ecc6acffc9a29bd8f08a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-819d5efdc144be34311fc6a378eef33a.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-29  Def-DTS Deductive Reasoning for Open-domain Dialogue Topic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-29  Lunguage A Benchmark for Structured and Sequential Chest X-ray   Interpretation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
