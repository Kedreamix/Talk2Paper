<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Be Decisive Noise-Induced Layouts for Multi-Subject Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ea46b9c6e3bba5c3d8ca0e6052498fa2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="Be-Decisive-Noise-Induced-Layouts-for-Multi-Subject-Generation"><a href="#Be-Decisive-Noise-Induced-Layouts-for-Multi-Subject-Generation" class="headerlink" title="Be Decisive: Noise-Induced Layouts for Multi-Subject Generation"></a>Be Decisive: Noise-Induced Layouts for Multi-Subject Generation</h2><p><strong>Authors:Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or</strong></p>
<p>Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subjectâ€™s spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the modelâ€™s prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the modelâ€™s prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the modelâ€™s original distribution. </p>
<blockquote>
<p>ç”Ÿæˆå¤šä¸ªä¸åŒçš„ä¸»é¢˜ä»ç„¶æ˜¯ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ã€‚å¤æ‚çš„æç¤ºå¾€å¾€ä¼šå¯¼è‡´ä¸»é¢˜æ³„æ¼ï¼Œä»è€Œå¯¼è‡´æ•°é‡ã€å±æ€§å’Œè§†è§‰ç‰¹å¾çš„ä¸å‡†ç¡®ã€‚é˜²æ­¢ä¸»é¢˜ä¹‹é—´çš„æ³„æ¼éœ€è¦äº†è§£æ¯ä¸ªä¸»é¢˜çš„ç©ºé—´ä½ç½®ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å¤–éƒ¨å¸ƒå±€æ§åˆ¶æä¾›è¿™äº›ç©ºé—´ä½ç½®ã€‚ç„¶è€Œï¼Œå¼ºåˆ¶å®æ–½è§„å®šçš„å¸ƒå±€å¾€å¾€ä¸ç”±é‡‡æ ·åˆå§‹å™ªå£°å†³å®šçš„å›ºæœ‰å¸ƒå±€ç›¸å†²çªï¼Œå¯¼è‡´ä¸æ¨¡å‹å…ˆéªŒä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥é¢„æµ‹ä¸æç¤ºå¯¹é½çš„ç©ºé—´å¸ƒå±€ï¼Œè¯¥å¸ƒå±€ç”±åˆå§‹å™ªå£°å¯¼å‡ºï¼Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡ä¾èµ–è¿™ç§å™ªå£°å¼•èµ·çš„å¸ƒå±€ï¼Œæˆ‘ä»¬é¿å…äº†ä¸å¤–éƒ¨å¼ºåˆ¶å®æ–½çš„å¸ƒå±€å†²çªï¼Œå¹¶æ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå°ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å’Œç²¾ç‚¼åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ä¸æ–­å‘å±•çš„å™ªå£°å¼•èµ·çš„å¸ƒå±€ï¼Œä»è€Œåœ¨ä¿æŒä¸»é¢˜ä¸€è‡´æ€§çš„åŒæ—¶ç¡®ä¿ä¸»é¢˜ä¹‹é—´çš„æ¸…æ™°è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¸ƒå±€å¼•å¯¼æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™ç§ä¸å™ªå£°å¯¹é½çš„ç­–ç•¥å®ç°äº†æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½å’Œæ›´ç¨³å®šçš„å¤šä¸»é¢˜ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹åŸå§‹åˆ†å¸ƒçš„ä¸°å¯Œå¤šæ ·æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21488v1">PDF</a> SIGGRAPH 2025. Project page: <a target="_blank" rel="noopener" href="https://omer11a.github.io/be-decisive/">https://omer11a.github.io/be-decisive/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤šä¸ªä¸åŒä¸»é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å¤æ‚æç¤ºä¼šå¯¼è‡´ä¸»é¢˜æ³„éœ²ï¼Œå½±å“æ•°é‡ã€å±æ€§å’Œè§†è§‰ç‰¹å¾å‡†ç¡®æ€§ã€‚é˜²æ­¢ä¸»é¢˜é—´æ³„éœ²éœ€è¦äº†è§£æ¯ä¸ªä¸»é¢˜çš„ç©ºé—´ä½ç½®ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å¤–éƒ¨å¸ƒå±€æ§åˆ¶æä¾›è¿™äº›ç©ºé—´ä½ç½®ï¼Œä½†å¼ºåˆ¶æ‰§è¡Œè§„å®šçš„å¸ƒå±€å¾€å¾€ä¸ç”±é‡‡æ ·åˆå§‹å™ªå£°å†³å®šçš„å›ºæœ‰å¸ƒå±€å†²çªï¼Œå¯¼è‡´ä¸æ¨¡å‹å…ˆéªŒçš„ä¸å¯¹é½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®æç¤ºé¢„æµ‹ä¸æç¤ºå¯¹é½çš„ç©ºé—´å¸ƒå±€ï¼Œå¹¶ä»åˆå§‹å™ªå£°ä¸­æ´¾ç”Ÿå‡ºæ¥ï¼Œåœ¨æ¶ˆå™ªè¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡ä¾èµ–è¿™ç§å™ªå£°å¼•èµ·çš„å¸ƒå±€ï¼Œé¿å…äº†ä¸å¤–éƒ¨æ–½åŠ çš„å¸ƒå±€å†²çªï¼Œå¹¶æ›´å¥½åœ°ä¿æŒäº†æ¨¡å‹çš„å…ˆéªŒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å°å‹ç¥ç»ç½‘ç»œæ¥é¢„æµ‹å’Œç²¾ç‚¼æ¯ä¸ªæ¶ˆå™ªæ­¥éª¤ä¸­çš„å™ªå£°å¼•èµ·çš„å¸ƒå±€çš„æ¼”å˜ï¼Œç¡®ä¿ä¸»é¢˜ä¹‹é—´çš„æ¸…æ™°ç•Œé™ï¼ŒåŒæ—¶ä¿æŒä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¸ƒå±€æŒ‡å¯¼æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™ç§ä¸å™ªå£°å¯¹é½çš„ç­–ç•¥å®ç°äº†æ›´å¥½çš„æ–‡æœ¬å›¾åƒå¯¹é½å’Œæ›´ç¨³å®šçš„å¤šä¸»é¢˜ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹åˆ†å¸ƒçš„ä¸°å¯Œå¤šæ ·æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤šä¸ªä¸»é¢˜æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³ä¸»é¢˜æ³„éœ²é—®é¢˜ã€‚</li>
<li>ä¸»é¢˜æ³„éœ²é—®é¢˜éœ€è¦é€šè¿‡äº†è§£æ¯ä¸ªä¸»é¢˜çš„ç©ºé—´ä½ç½®æ¥é¢„é˜²ã€‚</li>
<li>æœ€è¿‘çš„è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨å¤–éƒ¨å¸ƒå±€æ§åˆ¶æä¾›ç©ºé—´ä½ç½®ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¸æ¨¡å‹å…ˆéªŒå†²çªçš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ ¹æ®åˆå§‹å™ªå£°é¢„æµ‹å¹¶ä¸æç¤ºå¯¹é½çš„ç©ºé—´å¸ƒå±€ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¾èµ–å™ªå£°å¼•èµ·çš„å¸ƒå±€ï¼Œé¿å…äº†ä¸å¤–éƒ¨å¸ƒå±€çš„å†²çªï¼Œå¹¶ä¿æŒäº†æ¨¡å‹çš„å…ˆéªŒã€‚</li>
<li>ä½¿ç”¨å°å‹ç¥ç»ç½‘ç»œåœ¨æ¶ˆå™ªè¿‡ç¨‹ä¸­é¢„æµ‹å’Œç²¾ç‚¼å™ªå£°å¼•èµ·çš„å¸ƒå±€çš„æ¼”å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19668573944bb7f68f648051cb7298ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88b649e4f10dcaee1d8d751bb7992839.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e530cc4287aec4bbb3733dbb16edf21b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b217e1acefed29242950186ab25aa5cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a4ec58a2733922e2c19a0d96163a81c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2604cd5eb21e1c176e567732ee114c19.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MagicTryOn-Harnessing-Diffusion-Transformer-for-Garment-Preserving-Video-Virtual-Try-on"><a href="#MagicTryOn-Harnessing-Diffusion-Transformer-for-Garment-Preserving-Video-Virtual-Try-on" class="headerlink" title="MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving   Video Virtual Try-on"></a>MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving   Video Virtual Try-on</h2><p><strong>Authors:Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang</strong></p>
<p>Video Virtual Try-On (VVT) aims to simulate the natural appearance of garments across consecutive video frames, capturing their dynamic variations and interactions with human body motion. However, current VVT methods still face challenges in terms of spatiotemporal consistency and garment content preservation. First, they use diffusion models based on the U-Net, which are limited in their expressive capability and struggle to reconstruct complex details. Second, they adopt a separative modeling approach for spatial and temporal attention, which hinders the effective capture of structural relationships and dynamic consistency across frames. Third, their expression of garment details remains insufficient, affecting the realism and stability of the overall synthesized results, especially during human motion. To address the above challenges, we propose MagicTryOn, a video virtual try-on framework built upon the large-scale video diffusion Transformer.We replace the U-Net architecture with a diffusion Transformer and combine full self-attention to jointly model the spatiotemporal consistency of videos. We design a coarse-to-fine garment preservation strategy. The coarse strategy integrates garment tokens during the embedding stage, while the fine strategy incorporates multiple garment-based conditions, such as semantics, textures, and contour lines during the denoising stage. Moreover, we introduce a mask-aware loss to further optimize garment region fidelity. Extensive experiments on both image and video try-on datasets demonstrate that our method outperforms existing SOTA methods in comprehensive evaluations and generalizes to in-the-wild scenarios. </p>
<blockquote>
<p>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æ—¨åœ¨æ¨¡æ‹Ÿè¡£ç‰©åœ¨è¿ç»­è§†é¢‘å¸§ä¸­çš„è‡ªç„¶å¤–è§‚ï¼Œæ•æ‰å…¶åŠ¨æ€å˜åŒ–å’Œä¸äººç±»åŠ¨ä½œä¹‹é—´çš„äº¤äº’ã€‚ç„¶è€Œï¼Œå½“å‰çš„VVTæ–¹æ³•ä»é¢ä¸´æ—¶ç©ºä¸€è‡´æ€§å’Œæœè£…å†…å®¹ä¿ç•™æ–¹é¢çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬ä½¿ç”¨åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é‡å»ºå¤æ‚çš„ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬é‡‡ç”¨ç©ºé—´å’Œæ—¶é—´çš„åˆ†ç¦»å»ºæ¨¡æ–¹æ³•ï¼Œè¿™é˜»ç¢äº†è·¨å¸§çš„ç»“æ„å…³ç³»å’ŒåŠ¨æ€ä¸€è‡´æ€§çš„æœ‰æ•ˆæ•æ‰ã€‚ç¬¬ä¸‰ï¼Œå®ƒä»¬å¯¹æœè£…ç»†èŠ‚çš„è¡¨è¾¾ä»ç„¶ä¸è¶³ï¼Œå½±å“äº†åˆæˆç»“æœçš„é€¼çœŸåº¦å’Œç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨äººä½“è¿åŠ¨æœŸé—´ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MagicTryOnï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerçš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨æ‰©æ•£Transformeræ›¿ä»£U-Netæ¶æ„ï¼Œå¹¶ç»“åˆå…¨è‡ªæ³¨æ„åŠ›æ¥è”åˆå»ºæ¨¡è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”±ç²—åˆ°ç»†çš„æœè£…ä¿ç•™ç­–ç•¥ã€‚ç²—ç­–ç•¥åœ¨åµŒå…¥é˜¶æ®µé›†æˆæœè£…ä»¤ç‰Œï¼Œè€Œç»†ç­–ç•¥åœ¨é™å™ªé˜¶æ®µèå…¥å¤šç§åŸºäºæœè£…çš„æ¡ä»¶ï¼Œå¦‚è¯­ä¹‰ã€çº¹ç†å’Œè½®å»“çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ©è†œæŸå¤±æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æœè£…åŒºåŸŸçš„ä¿çœŸåº¦ã€‚åœ¨å›¾åƒå’Œè§†é¢‘è¯•ç©¿æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»¼åˆè¯„ä¼°ä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å¯ä»¥æ¨å¹¿åˆ°é‡å¤–åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21325v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹ç°æœ‰VVTæ–¹æ³•åœ¨æ—¶ç©ºä¸€è‡´æ€§åŠæœè£…å†…å®¹ä¿å­˜æ–¹é¢çš„é—®é¢˜ï¼Œæå‡ºäº†MagicTryOnæ¡†æ¶ï¼ŒåŸºäºå¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerã€‚é‡‡ç”¨å…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è”åˆå»ºæ¨¡è§†é¢‘æ—¶ç©ºä¸€è‡´æ€§ï¼Œè®¾è®¡ç”±ç²—åˆ°ç»†çš„æœè£…ä¿å­˜ç­–ç•¥ï¼Œå¹¶å¼•å…¥æ©è†œæ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æœè£…åŒºåŸŸä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æ—¨åœ¨æ¨¡æ‹Ÿæœè£…åœ¨è¿ç»­è§†é¢‘å¸§ä¸­çš„è‡ªç„¶å¤–è§‚ï¼Œæ•æ‰å…¶åŠ¨æ€å˜åŒ–å’Œä¸äººä½“è¿åŠ¨çš„äº¤äº’ã€‚</li>
<li>å½“å‰VVTæ–¹æ³•åœ¨æ—¶ç©ºä¸€è‡´æ€§å’Œæœè£…å†…å®¹ä¿å­˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MagicTryOnæ¡†æ¶ä½¿ç”¨åŸºäºå¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerçš„æ¨¡å‹ï¼Œæ›¿ä»£äº†U-Netæ¶æ„ã€‚</li>
<li>å…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ç”¨äºè”åˆå»ºæ¨¡è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>æœè£…ä¿å­˜ç­–ç•¥é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„è®¾è®¡ï¼Œæ•´åˆæœè£…æ ‡è®°å¹¶è€ƒè™‘å¤šç§æœè£…æ¡ä»¶ã€‚</li>
<li>å¼•å…¥æ©è†œæ„ŸçŸ¥æŸå¤±ä»¥ä¼˜åŒ–æœè£…åŒºåŸŸçš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e82632171104e45b10bbe41a3d06568.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fde2db8af9c328a1754379fb7af96df0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc565d87e14ce3df5aad38a9baadf61.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sci-Fi-Symmetric-Constraint-for-Frame-Inbetweening"><a href="#Sci-Fi-Symmetric-Constraint-for-Frame-Inbetweening" class="headerlink" title="Sci-Fi: Symmetric Constraint for Frame Inbetweening"></a>Sci-Fi: Symmetric Constraint for Frame Inbetweening</h2><p><strong>Authors:Liuhan Chen, Xiaodong Cun, Xiaoyu Li, Xianyi He, Shenghai Yuan, Jie Chen, Ying Shan, Li Yuan</strong></p>
<p>Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably canâ€™t make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines. </p>
<blockquote>
<p>å¸§é—´æ’å€¼æ—¨åœ¨æ ¹æ®ç»™å®šçš„èµ·å§‹å¸§å’Œç»“æŸå¸§åˆæˆä¸­é—´è§†é¢‘åºåˆ—ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡èå…¥ç«¯å¸§çº¦æŸæ¥æ‰©å±•å¤§è§„æ¨¡é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMsï¼‰ï¼Œè¿™åŒ…æ‹¬ç›´æ¥å¾®è°ƒæˆ–çœç•¥è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°äº†å®ƒä»¬è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®å±€é™ï¼šå®ƒä»¬æ³¨å…¥ç«¯å¸§çº¦æŸé€šå¸¸ä½¿ç”¨ä¸åŸæ¥æ–½åŠ èµ·å§‹å¸§ï¼ˆå•å¼ å›¾åƒï¼‰çº¦æŸç›¸åŒçš„æœºåˆ¶ã€‚ç„¶è€Œï¼Œç”±äºåŸå§‹çš„I2V-DMså·²ç»æå‰å¯¹èµ·å§‹å¸§æ¡ä»¶è¿›è¡Œäº†å……åˆ†çš„è®­ç»ƒï¼Œå› æ­¤ï¼Œé€šè¿‡å‡ ä¹æ²¡æœ‰ä»»ä½•ï¼ˆç”šè‡³é›¶ï¼‰ä¸“é—¨è®­ç»ƒçš„ç›¸åŒæœºåˆ¶ç®€å•åœ°å¼•å…¥ç«¯å¸§çº¦æŸå¯èƒ½æ— æ³•ä½¿ç«¯å¸§å¯¹ä¸­é—´å†…å®¹äº§ç”Ÿä¸èµ·å§‹å¸§ä¸€æ ·å¼ºçƒˆçš„å½±å“ã€‚è¿™ç§å¯¹ä¸­é—´å†…å®¹æ§åˆ¶åŠ›çš„ä¸å¯¹ç§°æ€§å¯èƒ½å¯¼è‡´ç”Ÿæˆå¸§çš„è¿åŠ¨ä¸ä¸€è‡´æˆ–å¤–è§‚å´©æºƒã€‚ä¸ºäº†æœ‰æ•ˆåœ°å®ç°èµ·å§‹å¸§å’Œç»“æŸå¸§çš„å¯¹ç§°çº¦æŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç§°ä¸ºSci-Fiï¼Œå®ƒé€šè¿‡è¾ƒå°çš„è®­ç»ƒè§„æ¨¡åº”ç”¨æ›´å¼ºçš„çº¦æŸæ³¨å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåƒè¿‡å»ä¸€æ ·å¤„ç†èµ·å§‹å¸§çº¦æŸï¼ŒåŒæ—¶ç”¨ä¸€ä¸ªæ”¹è¿›çš„æœºåˆ¶å¼•å…¥ç«¯å¸§çº¦æŸã€‚æ–°æœºåˆ¶åŸºäºä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è½»é‡çº§æ¨¡å—ï¼Œåä¸ºEF-Netï¼Œå®ƒåªç¼–ç ç«¯å¸§ï¼Œå¹¶å°†å…¶æ‰©å±•ä¸ºæ³¨å…¥åˆ°I2V-DMä¸­çš„æ—¶é—´è‡ªé€‚åº”å¸§çº§ç‰¹å¾ã€‚è¿™ä½¿å¾—ç«¯å¸§çº¦æŸä¸èµ·å§‹å¸§çº¦æŸä¸€æ ·å¼ºçƒˆï¼Œä»è€Œè®©æˆ‘ä»¬çš„Sci-Fièƒ½å¤Ÿåœ¨å„ç§åœºæ™¯ä¸­äº§ç”Ÿæ›´å’Œè°çš„è¿‡æ¸¡ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„Sci-Fiç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21205v1">PDF</a> 22 pages, 9 figures</p>
<p><strong>Summary</strong><br>     å½“å‰ä¸»æµçš„è§†é¢‘æ’å¸§æ–¹æ³•ä¸»è¦åŸºäºé¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMsï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¼•å…¥ç»“æŸå¸§çº¦æŸæ—¶å­˜åœ¨è®¾è®¡ä¸Šçš„å±€é™æ€§ï¼Œå³å®ƒä»¬é€šå¸¸ä½¿ç”¨ä¸å¼€å§‹å¸§ç›¸åŒçš„æœºåˆ¶æ¥å¼•å…¥çº¦æŸï¼Œä½†è¿™æ ·çš„æ–¹å¼å¯¼è‡´äº†å¯¹ä¸­é—´å†…å®¹çš„æ§åˆ¶åŠ›ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶Sci-Fiï¼Œé€šè¿‡è®¾è®¡ä¸€ç§æ›´å¼ºå¤§çš„æ³¨å…¥æœºåˆ¶æ¥å®ç°å¯¹å¼€å§‹å’Œç»“æŸå¸§çš„å¯¹ç§°çº¦æŸã€‚è¯¥æœºåˆ¶åŒ…å«ä¸€ä¸ªè½»é‡çº§çš„EF-Netæ¨¡å—ï¼Œä»…å¯¹ç»“æŸå¸§è¿›è¡Œç¼–ç å¹¶æ‰©å±•ä¸ºæ—¶é—´è‡ªé€‚åº”çš„å¸§çº§ç‰¹å¾ï¼Œæ³¨å…¥åˆ°I2V-DMsä¸­ã€‚è¿™ä½¿å¾—ç»“æŸå¸§çš„çº¦æŸä¸å¼€å§‹å¸§ç›¸åŒï¼Œä»è€Œæé«˜äº†è¿‡æ¸¡çš„è‡ªç„¶æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSci-Fiç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å…ˆè¿›çš„æ–¹æ³•ä¸»è¦åŸºäºå¤§å‹é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘æ’å¸§ã€‚</li>
<li>è¿™äº›æ–¹æ³•å­˜åœ¨è®¾è®¡ä¸Šçš„å±€é™æ€§ï¼Œå³åœ¨å¼•å…¥ç»“æŸå¸§çº¦æŸæ—¶æœªèƒ½å……åˆ†åˆ©ç”¨å…¶å½±å“åŠ›ã€‚</li>
<li>å¯¹å¼€å§‹å’Œç»“æŸå¸§çš„çº¦æŸä¸å¯¹ç§°å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„ä¸­é—´å†…å®¹ä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºçš„Sci-Fiæ¡†æ¶æ—¨åœ¨é€šè¿‡åŠ å¼ºç»“æŸå¸§çš„çº¦æŸæ¥æ”¹è¿›è¿™ä¸€é—®é¢˜ã€‚</li>
<li>EF-Netæ¨¡å—ç”¨äºå¯¹ç»“æŸå¸§è¿›è¡Œç¼–ç å¹¶æ‰©å±•ä¸ºæ—¶é—´è‡ªé€‚åº”çš„å¸§çº§ç‰¹å¾ã€‚</li>
<li>EF-Netæ³¨å…¥æœºåˆ¶ä½¿å¾—ç»“æŸå¸§çš„çº¦æŸä¸å¼€å§‹å¸§ç›¸åŒï¼Œå¢å¼ºäº†ä¸­é—´è¿‡æ¸¡çš„è‡ªç„¶æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd42fdcce37c7e79723e6efca79445f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa19939d6dc29ccd0e7a773a4a932a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3353739535001feb2bb817b3b376a4da.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IKMo-Image-Keyframed-Motion-Generation-with-Trajectory-Pose-Conditioned-Motion-Diffusion-Model"><a href="#IKMo-Image-Keyframed-Motion-Generation-with-Trajectory-Pose-Conditioned-Motion-Diffusion-Model" class="headerlink" title="IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned   Motion Diffusion Model"></a>IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned   Motion Diffusion Model</h2><p><strong>Authors:Yang Zhao, Yan Zhang, Xubo Yang</strong></p>
<p>Existing human motion generation methods with trajectory and pose inputs operate global processing on both modalities, leading to suboptimal outputs. In this paper, we propose IKMo, an image-keyframed motion generation method based on the diffusion model with trajectory and pose being decoupled. The trajectory and pose inputs go through a two-stage conditioning framework. In the first stage, the dedicated optimization module is applied to refine inputs. In the second stage, trajectory and pose are encoded via a Trajectory Encoder and a Pose Encoder in parallel. Then, motion with high spatial and semantic fidelity is guided by a motion ControlNet, which processes the fused trajectory and pose data. Experiment results based on HumanML3D and KIT-ML datasets demonstrate that the proposed method outperforms state-of-the-art on all metrics under trajectory-keyframe constraints. In addition, MLLM-based agents are implemented to pre-process model inputs. Given texts and keyframe images from users, the agents extract motion descriptions, keyframe poses, and trajectories as the optimized inputs into the motion generation model. We conducts a user study with 10 participants. The experiment results prove that the MLLM-based agents pre-processing makes generated motion more in line with usersâ€™ expectation. We believe that the proposed method improves both the fidelity and controllability of motion generation by the diffusion model. </p>
<blockquote>
<p>ç°æœ‰çš„äººè¿åŠ¨ç”Ÿæˆæ–¹æ³•ä½¿ç”¨è½¨è¿¹å’Œå§¿æ€è¾“å…¥ï¼Œå¯¹ä¸¤ç§æ¨¡å¼è¿›è¡Œå…¨å±€å¤„ç†ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç†æƒ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒå…³é”®å¸§è¿åŠ¨ç”Ÿæˆæ–¹æ³•IKMoï¼Œè¯¥æ–¹æ³•å°†è½¨è¿¹å’Œå§¿æ€è§£è€¦ã€‚è½¨è¿¹å’Œå§¿æ€è¾“å…¥ç»è¿‡ä¸¤é˜¶æ®µæ¡ä»¶æ¡†æ¶å¤„ç†ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œåº”ç”¨ä¸“ç”¨ä¼˜åŒ–æ¨¡å—å¯¹è¾“å…¥è¿›è¡Œç»†åŒ–ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œè½¨è¿¹å’Œå§¿æ€é€šè¿‡è½¨è¿¹ç¼–ç å™¨å’Œå§¿æ€ç¼–ç å™¨å¹¶è¡Œç¼–ç ã€‚ç„¶åï¼Œç”±è¿åŠ¨æ§åˆ¶ç½‘ç»œå¼•å¯¼äº§ç”Ÿå…·æœ‰é«˜ç©ºé—´å’Œè¯­ä¹‰ä¿çœŸåº¦çš„è¿åŠ¨ï¼Œè¯¥ç½‘ç»œå¤„ç†èåˆçš„è½¨è¿¹å’Œå§¿æ€æ•°æ®ã€‚åŸºäºHumanML3Då’ŒKIT-MLæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è½¨è¿¹å…³é”®å¸§çº¦æŸä¸‹ï¼Œæ‰€ææ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†åŸºäºMLLMçš„ä»£ç†æ¥é¢„å¤„ç†æ¨¡å‹è¾“å…¥ã€‚ç»™å®šç”¨æˆ·æä¾›çš„æ–‡æœ¬å’Œå…³é”®å¸§å›¾åƒï¼Œä»£ç†æå–è¿åŠ¨æè¿°ã€å…³é”®å¸§å§¿æ€å’Œè½¨è¿¹ï¼Œä½œä¸ºä¼˜åŒ–åçš„è¾“å…¥æ”¾å…¥è¿åŠ¨ç”Ÿæˆæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬å¯¹10åå‚ä¸è€…è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒåŸºäºMLLMçš„ä»£ç†é¢„å¤„ç†ä½¿å¾—ç”Ÿæˆçš„è¿åŠ¨æ›´ç¬¦åˆç”¨æˆ·çš„é¢„æœŸã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢çš„ä¿çœŸåº¦å’Œå¯æ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21146v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒå…³é”®å¸§è¿åŠ¨ç”Ÿæˆæ–¹æ³•IKMoï¼Œè¯¥æ–¹æ³•å°†è½¨è¿¹å’Œå§¿æ€è¾“å…¥è§£è€¦ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæ¡ä»¶æ¡†æ¶å¯¹è¾“å…¥è¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚åœ¨æ§åˆ¶ç½‘ç»œæŒ‡å¯¼ä¸‹ç”Ÿæˆå…·æœ‰é«˜åº¦ç©ºé—´å’Œæ—¶é—´å¿ å®åº¦çš„è¿åŠ¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è½¨è¿¹å…³é”®å¸§çº¦æŸä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†é¢„å¤„ç†çš„MLLMä»£ç†èƒ½ä½¿ç”Ÿæˆçš„è¿åŠ¨æ›´è´´è¿‘ç”¨æˆ·æœŸæœ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨ç”Ÿæˆæ–¹æ³•IKMoï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ï¼Œå°†è½¨è¿¹å’Œå§¿æ€è¾“å…¥è§£è€¦å¤„ç†ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µæ¡ä»¶æ¡†æ¶ï¼Œé¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>é€šè¿‡è¿åŠ¨æ§åˆ¶ç½‘ç»œç”Ÿæˆå…·æœ‰é«˜åº¦ç©ºé—´å’Œæ—¶é—´å¿ å®åº¦çš„è¿åŠ¨ã€‚</li>
<li>åœ¨HumanML3Då’ŒKIT-MLæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è½¨è¿¹å…³é”®å¸§çº¦æŸä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>å¼•å…¥äº†MLLM-basedä»£ç†é¢„å¤„ç†æ¨¡å‹è¾“å…¥ï¼ŒåŒ…æ‹¬ä»ç”¨æˆ·æå–è¿åŠ¨æè¿°ã€å…³é”®å¸§å§¿æ€å’Œè½¨è¿¹ç­‰ä¼˜åŒ–è¾“å…¥ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯æ˜äº†é¢„å¤„ç†ä½¿ç”Ÿæˆçš„è¿åŠ¨æ›´ç¬¦åˆç”¨æˆ·æœŸæœ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b80a07fa554fcf41ec68f284ca6fa759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57f79b944a10b3407617a7476701eab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-829313f810d6c35522cd0d1d4a82193e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a5242f7713c16da9c3c8c91c86d3d26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c36cd143637991eca81a134c7205c74.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Single-Index-Models-with-Diffusion-Priors"><a href="#Learning-Single-Index-Models-with-Diffusion-Priors" class="headerlink" title="Learning Single Index Models with Diffusion Priors"></a>Learning Single Index Models with Diffusion Priors</h2><p><strong>Authors:Anqi Tang, Youming Chen, Shuchen Xue, Zhaoqiang Liu</strong></p>
<p>Diffusion models (DMs) have demonstrated remarkable ability to generate diverse and high-quality images by efficiently modeling complex data distributions. They have also been explored as powerful generative priors for signal recovery, resulting in a substantial improvement in the quality of reconstructed signals. However, existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with discontinuous or unknown link functions. In this work, we focus on using DMs to achieve accurate recovery from semi-parametric single index models, which encompass a variety of popular nonlinear models that may have {\em discontinuous} and {\em unknown} link functions. We propose an efficient reconstruction method that only requires one round of unconditional sampling and (partial) inversion of DMs. Theoretical analysis on the effectiveness of the proposed methods has been established under appropriate conditions. We perform numerical experiments on image datasets for different nonlinear measurement models. We observe that compared to competing methods, our approach can yield more accurate reconstructions while utilizing significantly fewer neural function evaluations. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡å›¾åƒçš„å¼ºå¤§èƒ½åŠ›ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å»ºæ¨¡å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚å®ƒä»¬è¿˜è¢«æ¢ç´¢ä¸ºä¿¡å·æ¢å¤çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒï¼Œä»è€Œå¤§å¤§æé«˜äº†é‡å»ºä¿¡å·çš„è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…³äºä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œä¿¡å·æ¢å¤çš„ç ”ç©¶è¦ä¹ˆé›†ä¸­åœ¨ç‰¹å®šçš„é‡å»ºé—®é¢˜ä¸Šï¼Œè¦ä¹ˆæ— æ³•å¤„ç†å…·æœ‰ä¸è¿ç»­æˆ–æœªçŸ¥é“¾æ¥å‡½æ•°çš„éçº¿æ€§æµ‹é‡æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä½¿ç”¨æ‰©æ•£æ¨¡å‹å®ç°åŠå‚æ•°å•ç´¢å¼•æ¨¡å‹çš„ç²¾ç¡®æ¢å¤ï¼Œè¯¥æ¨¡å‹æ¶µç›–äº†å„ç§æµè¡Œçš„éçº¿æ€§æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½å…·æœ‰ä¸è¿ç»­å’ŒæœªçŸ¥çš„é“¾æ¥å‡½æ•°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é‡å»ºæ–¹æ³•ï¼Œåªéœ€è¿›è¡Œä¸€è½®æ— æ¡ä»¶é‡‡æ ·å’Œæ‰©æ•£æ¨¡å‹çš„ï¼ˆéƒ¨åˆ†ï¼‰åæ¼”ã€‚åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼Œå¯¹æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ç†è®ºåˆ†æã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„éçº¿æ€§æµ‹é‡æ¨¡å‹çš„å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ•°å€¼å®éªŒã€‚æˆ‘ä»¬çš„è§‚å¯Ÿç»“æœæ˜¯ï¼Œä¸ç«äº‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿›è¡Œç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°æ—¶ï¼Œå¯ä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„é‡å»ºç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21135v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œé€šè¿‡æœ‰æ•ˆå»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜è¢«æ¢ç´¢ä¸ºä¿¡å·æ¢å¤çš„å¼ºå¤§å…ˆéªŒï¼Œå¤§å¤§æé«˜äº†é‡å»ºä¿¡å·çš„è´¨é‡ã€‚æœ¬ç ”ç©¶å…³æ³¨ä½¿ç”¨æ‰©æ•£æ¨¡å‹å®ç°åŠå‚æ•°å•ç´¢å¼•æ¨¡å‹çš„ç²¾ç¡®æ¢å¤ï¼Œæ¶µç›–å„ç§å¯èƒ½å…·æœ‰é—´æ–­å’ŒæœªçŸ¥é“¾æ¥å‡½æ•°çš„éçº¿æ€§æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é‡å»ºæ–¹æ³•ï¼Œåªéœ€ä¸€è½®æ— æ¡ä»¶é‡‡æ ·å’Œï¼ˆéƒ¨åˆ†ï¼‰åè½¬æ‰©æ•£æ¨¡å‹ã€‚åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼Œå¯¹æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ç†è®ºåˆ†æã€‚å¯¹å›¾åƒæ•°æ®é›†çš„ä¸åŒéçº¿æ€§æµ‹é‡æ¨¡å‹è¿›è¡Œçš„æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸ç«äº‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°æ›´å‡†ç¡®çš„é‡å»ºï¼ŒåŒæ—¶å¤§å¤§å‡å°‘ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°çš„ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰èƒ½é«˜æ•ˆç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡å›¾åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºä¿¡å·æ¢å¤çš„å¼ºå¤§å…ˆéªŒï¼Œèƒ½æ˜¾è‘—æé«˜é‡å»ºä¿¡å·è´¨é‡ã€‚</li>
<li>ç ”ç©¶å…³æ³¨ä½¿ç”¨æ‰©æ•£æ¨¡å‹å®ç°åŠå‚æ•°å•ç´¢å¼•æ¨¡å‹çš„ç²¾ç¡®æ¢å¤ï¼Œé€‚ç”¨äºå…·æœ‰é—´æ–­å’ŒæœªçŸ¥é“¾æ¥å‡½æ•°çš„éçº¿æ€§æ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ç§æœ‰æ•ˆçš„é‡å»ºæ–¹æ³•ï¼Œåªéœ€ä¸€è½®æ— æ¡ä»¶é‡‡æ ·å’Œï¼ˆéƒ¨åˆ†ï¼‰åè½¬æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼Œå¯¹æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ç†è®ºåˆ†æã€‚</li>
<li>æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°é‡å»ºä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef1a15b37e78b85e764d95d92497e52f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-with-Classifier-Free-Gibbs-like-Guidance"><a href="#Conditional-Diffusion-Models-with-Classifier-Free-Gibbs-like-Guidance" class="headerlink" title="Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance"></a>Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance</h2><p><strong>Authors:Badr Moufad, Yazid Janati, Alain Durmus, Ahmed Ghorbel, Eric Moulines, Jimmy Olsson</strong></p>
<p>Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero â€“ where the data distribution is tilted by a power $w \gt 1$ of the conditional distribution. We identify the missing component: a R&#39;enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at <a target="_blank" rel="noopener" href="https://github.com/yazidjanati/cfgig">https://github.com/yazidjanati/cfgig</a> </p>
<blockquote>
<p>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºæ”¹è¿›æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„çº¿æ€§ç»„åˆè¾“å‡ºæŠ€æœ¯çš„æŠ€æœ¯ã€‚é€šè¿‡å°†æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„å»å™ªå™¨ç»“åˆèµ·æ¥ä½¿ç”¨ï¼ŒCFGåœ¨æé«˜è§†è§‰è´¨é‡çš„åŒæ—¶ä¹Ÿèƒ½æ”¹å–„ä¸æç¤ºçš„å¯¹é½æ€§ã€‚ç„¶è€Œï¼ŒCFGå¾€å¾€å‡å°‘äº†æ ·æœ¬çš„å¤šæ ·æ€§ï¼Œå› æ­¤åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´äº§ç”Ÿäº†æŒ‘æˆ˜æ€§çš„æƒè¡¡é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤ä¸ªé‡è¦çš„è´¡çŒ®ã€‚é¦–å…ˆï¼ŒCFGé€šå¸¸å¹¶ä¸å¯¹åº”ä¸€ä¸ªå®šä¹‰è‰¯å¥½çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ã€‚å°¤å…¶é‡è¦çš„æ˜¯ï¼Œä¸å¸¸è¯†ç›¸åï¼Œå½“å™ªå£°æ°´å¹³æ¥è¿‘é›¶æ—¶ï¼ŒCFGå¹¶ä¸ä¼šä»ç›®æ ‡åˆ†å¸ƒä¸­ç”Ÿæˆæ ·æœ¬ï¼Œè¿™ä¸ªåˆ†å¸ƒä¸æ¡ä»¶åˆ†å¸ƒçš„å¹‚æ¬¡æ–¹$w \gt 1$ç›¸å¯¹åº”ã€‚æˆ‘ä»¬ç¡®å®šäº†ç¼ºå¤±çš„éƒ¨åˆ†ï¼šä¸€ä¸ªèµ·ç€æ’æ–¥åŠ›ä½œç”¨çš„RÃ©nyiæ•£åº¦é¡¹æ˜¯å¿…éœ€çš„ï¼Œç”¨æ¥çº æ­£CFGå¹¶ä½¿å…¶ä¸é€‚å½“çš„DDMä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ä¸ªä¿®æ­£é¡¹åœ¨å™ªå£°æ°´å¹³è¾ƒä½æ—¶ä¼šæ¶ˆå¤±ã€‚å…¶æ¬¡ï¼ŒåŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼Gibbsçš„é‡‡æ ·ç¨‹åºï¼Œä»æ‰€éœ€çš„å€¾æ–œåˆ†å¸ƒä¸­ç»˜åˆ¶æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•ä»¥æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­çš„åˆå§‹æ ·æœ¬å¼€å§‹ï¼Œä¸å¸¦CFGè¿›è¡Œè¿­ä»£æ”¹è¿›ï¼Œä»è€Œä¿æŒäº†å¤šæ ·æ€§å¹¶é€æ­¥æé«˜äº†æ ·æœ¬è´¨é‡ã€‚æˆ‘ä»¬åœ¨å›¾åƒå’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨æ‰€æœ‰è€ƒè™‘çš„æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºCFGã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yazidjanati/cfgig%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yazidjanati/cfgigæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21101v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Classifier-Free Guidanceï¼ˆCFGï¼‰åœ¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡çº¿æ€§ç»„åˆæ¡ä»¶å’Œæ— æ¡ä»¶å»å™ªå™¨çš„è¾“å‡ºæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚è™½ç„¶CFGèƒ½æé«˜è§†è§‰è´¨é‡å’Œä¸æç¤ºçš„å¯¹é½åº¦ï¼Œä½†å®ƒå¾€å¾€å‡å°‘äº†æ ·æœ¬çš„å¤šæ ·æ€§ï¼Œéœ€è¦åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´åšå‡ºæƒè¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åšå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ã€‚é¦–å…ˆï¼ŒCFGå¹¶ä¸å¯¹åº”äºä¸€ä¸ªå®šä¹‰æ˜ç¡®çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ã€‚é€šè¿‡æ·±å…¥çš„åˆ†æï¼Œæœ¬æ–‡ç¡®å®šäº†ç¼ºå¤±çš„ç»„ä»¶ï¼šä¸€ä¸ªèµ·åˆ°æ’æ–¥ä½œç”¨çš„RÃ©nyiæ•£åº¦é¡¹ï¼Œè¿™æ˜¯ä¿®æ­£CFGå¹¶ä½¿å…¶ä¸é€‚å½“çš„DDMä¸€è‡´çš„å¿…è¦æ¡ä»¶ã€‚å…¶æ¬¡ï¼ŒåŸºäºè¿™ä¸€è§è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§Gibbsæ ·å¼çš„é‡‡æ ·ç¨‹åºï¼Œä»æ‰€éœ€çš„å€¾æ–œåˆ†å¸ƒä¸­ç»˜åˆ¶æ ·æœ¬ã€‚è¯¥æ–¹æ³•ä»æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­çš„æ— CFGåˆå§‹æ ·æœ¬å¼€å§‹ï¼Œè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ—¢ä¿æŒå¤šæ ·æ€§åˆé€æ­¥æé«˜æ ·æœ¬è´¨é‡ã€‚åœ¨å›¾åƒå’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºCFGã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Classifier-Free Guidanceï¼ˆCFGï¼‰ç”¨äºæé«˜æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>CFGå¹¶ä¸ç›´æ¥å¯¹åº”äºä¸€ä¸ªæ˜ç¡®çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ã€‚</li>
<li>è¯†åˆ«å‡ºç¼ºå¤±çš„ç»„ä»¶ï¼šä¸€ä¸ªèµ·åˆ°æ’æ–¥ä½œç”¨çš„RÃ©nyiæ•£åº¦é¡¹ï¼Œç”¨äºä¿®æ­£CFGå¹¶ä½¿å…¶ä¸DDMä¸€è‡´ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºGibbsé‡‡æ ·çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æ‰€éœ€çš„å€¾æ–œåˆ†å¸ƒä¸­ç»˜åˆ¶æ ·æœ¬ï¼Œæ—¢æé«˜æ ·æœ¬è´¨é‡åˆä¿æŒå¤šæ ·æ€§ã€‚</li>
<li>ä¿®æ­£åçš„æ–¹æ³•åœ¨å›¾åƒå’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºCFGã€‚</li>
<li>ç›¸å…³çš„ä»£ç å·²å®ç°å¹¶å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0a8b1b1e4cf13545069a90f50c173ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d197480f6d425a2571641823a62d1dfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-659446cecdf88f3d84f2ca899714353d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FeatInv-Spatially-resolved-mapping-from-feature-space-to-input-space-using-conditional-diffusion-models"><a href="#FeatInv-Spatially-resolved-mapping-from-feature-space-to-input-space-using-conditional-diffusion-models" class="headerlink" title="FeatInv: Spatially resolved mapping from feature space to input space   using conditional diffusion models"></a>FeatInv: Spatially resolved mapping from feature space to input space   using conditional diffusion models</h2><p><strong>Authors:Nils Neukirch, Johanna Vielhaben, Nils Strodthoff</strong></p>
<p>Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models. </p>
<blockquote>
<p>å†…éƒ¨è¡¨å¾å¯¹äºç†è§£æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚å…¶å±æ€§å’Œæ¨ç†æ¨¡å¼ï¼‰è‡³å…³é‡è¦ï¼Œä½†ä»éš¾ä»¥è§£é‡Šã€‚ä»ç‰¹å¾ç©ºé—´åˆ°è¾“å…¥ç©ºé—´çš„æ˜ å°„æœ‰åŠ©äºè§£é‡Šå‰è€…ï¼Œè€Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²—ç•¥çš„è¿‘ä¼¼ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹â€”â€”ä¸€ä¸ªä»¥ç©ºé—´è§£æç‰¹å¾å›¾ä¸ºæ¡ä»¶çš„é¢„è®­ç»ƒé«˜ä¿çœŸæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ¦‚ç‡æ–¹å¼å­¦ä¹ è¿™ç§æ˜ å°„ã€‚æˆ‘ä»¬é€šè¿‡å„ç§é¢„è®­ç»ƒå›¾åƒåˆ†ç±»å™¨çš„å®ä¾‹è¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œè¿™äº›åˆ†ç±»å™¨æ¶µç›–äº†ä»å·ç§¯ç¥ç»ç½‘ç»œåˆ°è§†è§‰å˜æ¢å™¨çš„èŒƒå›´ï¼Œæ˜¾ç¤ºå‡ºå‡ºè‰²çš„é‡å»ºèƒ½åŠ›ã€‚é€šè¿‡å®šæ€§å’Œç¨³å¥æ€§åˆ†æï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¯èƒ½çš„åº”ç”¨ï¼Œä¾‹å¦‚åœ¨è¾“å…¥ç©ºé—´ä¸­å¯è§†åŒ–æ¦‚å¿µå¯¼å‘æˆ–ç ”ç©¶ç‰¹å¾ç©ºé—´çš„å¤åˆæ€§è´¨ã€‚è¯¥æ–¹æ³•åœ¨æ”¹è¿›è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­ç‰¹å¾ç©ºé—´çš„ç†è§£æ–¹é¢å…·æœ‰å¹¿é˜”æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21032v1">PDF</a> 15 pages, 10 figures, code is available at   <a target="_blank" rel="noopener" href="https://github.com/AI4HealthUOL/FeatInv">https://github.com/AI4HealthUOL/FeatInv</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œå†…éƒ¨è¡¨å¾çš„é‡è¦æ€§ï¼Œå½“å‰å¯¹äºæ˜ å°„ä»ç‰¹å¾ç©ºé—´åˆ°è¾“å…¥ç©ºé—´çš„è§£é‡Šæ–¹æ³•ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ˜ å°„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡ç©ºé—´è§£æçš„ç‰¹å¾æ˜ å°„æ¡ä»¶é¢„è®­ç»ƒçš„é«˜ä¿çœŸæ‰©æ•£æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸åŒçš„é¢„è®­ç»ƒå›¾åƒåˆ†ç±»å™¨ä¸­çš„åº”ç”¨è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚CNNå’ŒViTsç­‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•éªŒè¯äº†å…¶å¯è¡Œæ€§å’Œç¨³å¥æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¯è§†åŒ–è¾“å…¥ç©ºé—´ä¸­çš„æ¦‚å¿µå¯¼å‘ä»¥åŠæ¢ç©¶ç‰¹å¾ç©ºé—´çš„å¤åˆæ€§è´¨ç­‰åº”ç”¨ã€‚è¯¥æ–¹æ³•å…·æœ‰æé«˜è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ç†è§£çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…éƒ¨è¡¨å¾å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œçš„ç†è§£å’Œæ¢ç©¶è‡³å…³é‡è¦ï¼Œå®ƒåæ˜ äº†æ¨¡å‹çš„æ€§è´¨å’Œæ¨ç†æ¨¡å¼ã€‚</li>
<li>ä»ç‰¹å¾ç©ºé—´åˆ°è¾“å…¥ç©ºé—´çš„æ˜ å°„å¯¹äºè§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†å¾ˆé‡è¦ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºç²—ç•¥çš„è¿‘ä¼¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ˜ å°„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„é«˜ä¿çœŸæ‰©æ•£æ¨¡å‹è¿›è¡Œæ˜ å°„ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§é¢„è®­ç»ƒå›¾åƒåˆ†ç±»å™¨ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é‡å»ºèƒ½åŠ›ï¼ŒåŒ…æ‹¬CNNå’ŒViTsç­‰ã€‚</li>
<li>é€šè¿‡å®šæ€§æ¯”è¾ƒå’Œç¨³å¥æ€§åˆ†æéªŒè¯äº†æ–¹æ³•çš„å¯è¡Œæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¯è§†åŒ–è¾“å…¥ç©ºé—´ä¸­çš„æ¦‚å¿µå¯¼å‘ä»¥åŠæ¢ç©¶ç‰¹å¾ç©ºé—´çš„å¤åˆæ€§è´¨çš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-435eca30c42c3ca4c42b056184235115.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e3c25c6c7e5d6bbf40da8d41899a59f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c13028d122fc6c2cae53ff459054c109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb36a23c44f74ab098547c4a10169371.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Facial-Attribute-Based-Text-Guided-Face-Anonymization"><a href="#Facial-Attribute-Based-Text-Guided-Face-Anonymization" class="headerlink" title="Facial Attribute Based Text Guided Face Anonymization"></a>Facial Attribute Based Text Guided Face Anonymization</h2><p><strong>Authors:Mustafa Ä°zzet MuÅŸtu, HazÄ±m Kemal Ekenel</strong></p>
<p>The increasing prevalence of computer vision applications necessitates handling vast amounts of visual data, often containing personal information. While this technology offers significant benefits, it should not compromise privacy. Data privacy regulations emphasize the need for individual consent for processing personal data, hindering researchersâ€™ ability to collect high-quality datasets containing the faces of the individuals. This paper presents a deep learning-based face anonymization pipeline to overcome this challenge. Unlike most of the existing methods, our method leverages recent advancements in diffusion-based inpainting models, eliminating the need for training Generative Adversarial Networks. The pipeline employs a three-stage approach: face detection with RetinaNet, feature extraction with VGG-Face, and realistic face generation using the state-of-the-art BrushNet diffusion model. BrushNet utilizes the entire image, face masks, and text prompts specifying desired facial attributes like age, ethnicity, gender, and expression. This enables the generation of natural-looking images with unrecognizable individuals, facilitating the creation of privacy-compliant datasets for computer vision research. </p>
<blockquote>
<p>éšç€è®¡ç®—æœºè§†è§‰åº”ç”¨çš„æ—¥ç›Šæ™®åŠï¼Œéœ€è¦å¤„ç†åŒ…å«ä¸ªäººä¿¡æ¯çš„æµ·é‡è§†è§‰æ•°æ®ã€‚è™½ç„¶è¿™é¡¹æŠ€æœ¯å¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼Œä½†å®ƒä¸åº”è¯¥æŸå®³éšç§ã€‚æ•°æ®éšç§è§„å®šå¼ºè°ƒå¤„ç†ä¸ªäººæ•°æ®æ—¶éœ€è¦ä¸ªäººåŒæ„ï¼Œè¿™é˜»ç¢äº†ç ”ç©¶äººå‘˜æ”¶é›†åŒ…å«ä¸ªäººé¢éƒ¨çš„é«˜è´¨é‡æ•°æ®é›†çš„èƒ½åŠ›ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨åŒ¿ååŒ–ç®¡é“ã€‚ä¸å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºäºæ‰©æ•£çš„ä¿®å¤æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæ— éœ€è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚è¯¥ç®¡é“é‡‡ç”¨ä¸‰é˜¶æ®µæ–¹æ³•ï¼šä½¿ç”¨RetinaNetè¿›è¡Œé¢éƒ¨æ£€æµ‹ï¼Œä½¿ç”¨VGG-Faceè¿›è¡Œç‰¹å¾æå–ï¼Œä»¥åŠä½¿ç”¨æœ€å…ˆè¿›çš„BrushNetæ‰©æ•£æ¨¡å‹è¿›è¡Œé€¼çœŸçš„é¢éƒ¨ç”Ÿæˆã€‚BrushNetåˆ©ç”¨æ•´ä¸ªå›¾åƒã€é¢éƒ¨é®æŒ¡å’Œæ–‡æœ¬æç¤ºæ¥æŒ‡å®šæ‰€éœ€çš„é¢éƒ¨å±æ€§ï¼Œå¦‚å¹´é¾„ã€ç§æ—ã€æ€§åˆ«å’Œè¡¨æƒ…ã€‚è¿™èƒ½å¤Ÿç”Ÿæˆçœ‹ä¼¼è‡ªç„¶ä½†æ— æ³•è¯†åˆ«ä¸ªäººçš„å›¾åƒï¼Œä¸ºè®¡ç®—æœºè§†è§‰ç ”ç©¶åˆ›å»ºç¬¦åˆéšç§è¦æ±‚çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21002v1">PDF</a> 6 pages, 5 figures, published in the Proceedings of the Joint   visuAAL-GoodBrother Conference on Trustworthy Video- and Audio-Based   Assistive Technologies</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨åŒ¿ååŒ–ç®¡é“ï¼Œä»¥åº”å¯¹è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­æ¶‰åŠéšç§ä¿æŠ¤çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ‰©æ•£åŸºå»å™ªæ¨¡å‹ï¼Œæ— éœ€è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ŒåŒ…æ‹¬é¢éƒ¨æ£€æµ‹ã€ç‰¹å¾æå–å’ŒåŸºäºBrushNetæ‰©æ•£æ¨¡å‹çš„é€¼çœŸé¢éƒ¨ç”Ÿæˆä¸‰ä¸ªé˜¶æ®µã€‚èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸å¯è¯†åˆ«ä¸ªä½“çš„è‡ªç„¶å›¾åƒï¼Œä¸ºè®¡ç®—æœºè§†è§‰ç ”ç©¶åˆ›å»ºç¬¦åˆéšç§ä¿æŠ¤è¦æ±‚çš„æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰åº”ç”¨éœ€è¦å¤„ç†å¤§é‡åŒ…å«ä¸ªäººä¿¡æ¯çš„è§†è§‰æ•°æ®ã€‚</li>
<li>éšç§ä¿æŠ¤æ˜¯å¤„ç†ä¸ªäººæ•°æ®çš„å…³é”®é—®é¢˜ï¼Œæ•°æ®éšç§æ³•è§„å¼ºè°ƒä¸ªäººåŒæ„çš„å¿…è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸ªäººæ•°æ®éšç§æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨åŒ¿ååŒ–ç®¡é“æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£åŸºå»å™ªæ¨¡å‹ï¼Œä¸éœ€è¦è®­ç»ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚</li>
<li>ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šä½¿ç”¨RetinaNetè¿›è¡Œé¢éƒ¨æ£€æµ‹ï¼Œä½¿ç”¨VGG-Faceè¿›è¡Œç‰¹å¾æå–ï¼Œä»¥åŠä½¿ç”¨BrushNetæ‰©æ•£æ¨¡å‹è¿›è¡Œé€¼çœŸçš„é¢éƒ¨ç”Ÿæˆã€‚</li>
<li>BrushNetåˆ©ç”¨æ•´ä¸ªå›¾åƒã€é¢éƒ¨æ©ç å’ŒæŒ‡å®šé¢éƒ¨å±æ€§çš„æ–‡æœ¬æç¤ºï¼Œå¦‚å¹´é¾„ã€ç§æ—ã€æ€§åˆ«å’Œè¡¨æƒ…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec9df831c36b9ed7f37861c98ab60073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a65ae49479a688b418e0779865c847a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc22bc4c056942e8872babd6caa79f05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ec25d3c050d86231022a2203ee877bf.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generative-Image-Compression-by-Estimating-Gradients-of-the-Rate-variable-Feature-Distribution"><a href="#Generative-Image-Compression-by-Estimating-Gradients-of-the-Rate-variable-Feature-Distribution" class="headerlink" title="Generative Image Compression by Estimating Gradients of the   Rate-variable Feature Distribution"></a>Generative Image Compression by Estimating Gradients of the   Rate-variable Feature Distribution</h2><p><strong>Authors:Minghao Han, Weiyi You, Jinhua Zhang, Leheng Zhang, Ce Zhu, Shuhang Gu</strong></p>
<p>While learned image compression (LIC) focuses on efficient data transmission, generative image compression (GIC) extends this framework by integrating generative modeling to produce photo-realistic reconstructed images. In this paper, we propose a novel diffusion-based generative modeling framework tailored for generative image compression. Unlike prior diffusion-based approaches that indirectly exploit diffusion modeling, we reinterpret the compression process itself as a forward diffusion path governed by stochastic differential equations (SDEs). A reverse neural network is trained to reconstruct images by reversing the compression process directly, without requiring Gaussian noise initialization. This approach achieves smooth rate adjustment and photo-realistic reconstructions with only a minimal number of sampling steps. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing generative image compression approaches across a range of metrics, including perceptual distortion, statistical fidelity, and no-reference quality assessments. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å›¾åƒå‹ç¼©ï¼ˆLICï¼‰ä¾§é‡äºé«˜æ•ˆæ•°æ®ä¼ è¾“ï¼Œè€Œç”Ÿæˆå¼å›¾åƒå‹ç¼©ï¼ˆGICï¼‰åˆ™é€šè¿‡é›†æˆç”Ÿæˆæ¨¡å‹æ¥äº§ç”Ÿé€¼çœŸçš„é‡å»ºå›¾åƒï¼Œä»è€Œæ‰©å±•äº†è¿™ä¸€æ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“ä¸ºç”Ÿæˆå¼å›¾åƒå‹ç¼©è®¾è®¡ã€‚ä¸ä»¥å¾€åŸºäºæ‰©æ•£çš„æ–¹æ³•é—´æ¥åˆ©ç”¨æ‰©æ•£å»ºæ¨¡ä¸åŒï¼Œæˆ‘ä»¬å°†å‹ç¼©è¿‡ç¨‹æœ¬èº«é‡æ–°è§£é‡Šä¸ºå—éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰æ§åˆ¶çš„æ­£å‘æ‰©æ•£è·¯å¾„ã€‚è®­ç»ƒä¸€ä¸ªåå‘ç¥ç»ç½‘ç»œç›´æ¥åè½¬å‹ç¼©è¿‡ç¨‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œæ— éœ€é«˜æ–¯å™ªå£°åˆå§‹åŒ–ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å¹³æ»‘çš„é€Ÿç‡è°ƒæ•´ï¼Œåªéœ€æå°‘çš„é‡‡æ ·æ­¥éª¤å°±èƒ½å®ç°é€¼çœŸçš„é‡å»ºã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ„ŸçŸ¥å¤±çœŸã€ç»Ÿè®¡ä¿çœŸå’Œæ— å‚è€ƒè´¨é‡è¯„ä¼°ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šï¼Œéƒ½ä¼˜äºç°æœ‰çš„ç”Ÿæˆå¼å›¾åƒå‹ç¼©æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20984v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒå‹ç¼©æ¡†æ¶ã€‚ä¸åŒäºé—´æ¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆå‰æ–¹æ³•ï¼Œæœ¬æ–‡å°†å‹ç¼©è¿‡ç¨‹æœ¬èº«é‡æ–°è§£é‡Šä¸ºç”±éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰æ§åˆ¶çš„æ­£å‘æ‰©æ•£è·¯å¾„ã€‚é€šè¿‡è®­ç»ƒåå‘ç¥ç»ç½‘ç»œç›´æ¥åè½¬å‹ç¼©è¿‡ç¨‹ï¼Œæ— éœ€é«˜æ–¯å™ªå£°åˆå§‹åŒ–ï¼Œå®ç°å¹³æ»‘ç‡è°ƒæ•´å’Œé€¼çœŸçš„å›¾åƒé‡å»ºï¼Œé‡‡æ ·æ­¥éª¤æ•°é‡æœ€å°åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å¤±çœŸã€ç»Ÿè®¡ä¿çœŸåº¦å’Œæ— å‚è€ƒè´¨é‡è¯„ä¼°ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰ç”Ÿæˆå›¾åƒå‹ç¼©æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒå‹ç¼©æ¡†æ¶ï¼Œç»“åˆäº†å›¾åƒå‹ç¼©å’Œç”Ÿæˆå»ºæ¨¡ã€‚</li>
<li>ä¸åŒäºå…¶ä»–æ–¹æ³•ï¼Œæœ¬æ–‡ç›´æ¥å°†å‹ç¼©è¿‡ç¨‹è§£é‡Šä¸ºæ­£å‘æ‰©æ•£è·¯å¾„ï¼Œå—éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰æ§åˆ¶ã€‚</li>
<li>ä½¿ç”¨åå‘ç¥ç»ç½‘ç»œåè½¬å‹ç¼©è¿‡ç¨‹ï¼Œæ— éœ€é«˜æ–¯å™ªå£°åˆå§‹åŒ–ï¼Œå®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„å›¾åƒé‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç‡çš„å¹³æ»‘è°ƒæ•´ï¼Œå¯ä»¥åœ¨ä¸åŒå‹ç¼©ç‡ä¸‹ä¿æŒå›¾åƒè´¨é‡ã€‚</li>
<li>ä¸ç°æœ‰ç”Ÿæˆå›¾åƒå‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å¤±çœŸã€ç»Ÿè®¡ä¿çœŸåº¦å’Œæ— å‚è€ƒè´¨é‡è¯„ä¼°ç­‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–é‡‡æ ·æ­¥éª¤æ•°é‡ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒå‹ç¼©çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e825e2bc9426af4ff23c7c64d559e5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80729718b6cbadcc9b04b02b388b0948.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfce13a4b5a2f1bfd5c0df48fdbba7b5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DreamBoothDPO-Improving-Personalized-Generation-using-Direct-Preference-Optimization"><a href="#DreamBoothDPO-Improving-Personalized-Generation-using-Direct-Preference-Optimization" class="headerlink" title="DreamBoothDPO: Improving Personalized Generation using Direct Preference   Optimization"></a>DreamBoothDPO: Improving Personalized Generation using Direct Preference   Optimization</h2><p><strong>Authors:Shamil Ayupov, Maksim Nakhodnov, Anastasia Yaschenko, Andrey Kuznetsov, Aibek Alanov</strong></p>
<p>Personalized diffusion models have shown remarkable success in Text-to-Image (T2I) generation by enabling the injection of user-defined concepts into diverse contexts. However, balancing concept fidelity with contextual alignment remains a challenging open problem. In this work, we propose an RL-based approach that leverages the diverse outputs of T2I models to address this issue. Our method eliminates the need for human-annotated scores by generating a synthetic paired dataset for DPO-like training using external quality metrics. These better-worse pairs are specifically constructed to improve both concept fidelity and prompt adherence. Moreover, our approach supports flexible adjustment of the trade-off between image fidelity and textual alignment. Through multi-step training, our approach outperforms a naive baseline in convergence speed and output quality. We conduct extensive qualitative and quantitative analysis, demonstrating the effectiveness of our method across various architectures and fine-tuning techniques. The source code can be found at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/DreamBoothDPO">https://github.com/ControlGenAI/DreamBoothDPO</a>. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æˆåŠŸï¼Œé€šè¿‡å°†ç”¨æˆ·å®šä¹‰çš„æ¦‚å¿µæ³¨å…¥åˆ°ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ã€‚ç„¶è€Œï¼Œåœ¨æ¦‚å¿µä¿çœŸåº¦å’Œä¸Šä¸‹æ–‡å¯¹é½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨T2Iæ¨¡å‹çš„å¤šæ ·åŒ–è¾“å‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç”Ÿæˆä¸€ä¸ªåˆæˆé…å¯¹æ•°æ®é›†ï¼Œç”¨äºDPOç±»ä¼¼è®­ç»ƒçš„å¤–éƒ¨è´¨é‡æŒ‡æ ‡è¯„ä¼°ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººå·¥æ ‡æ³¨åˆ†æ•°çš„éœ€æ±‚ã€‚è¿™äº›ä¼˜åŠ£å¯¹ç‰¹åœ°è¢«æ„å»ºç”¨æ¥æ”¹è¿›æ¦‚å¿µä¿çœŸåº¦å’Œæç¤ºç¬¦åˆåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒçµæ´»åœ°è°ƒæ•´å›¾åƒä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡å¤šæ­¥è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç®€å•åŸºçº¿åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¸Šè¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šæ€§å’Œå®šé‡åˆ†æï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ¶æ„å’Œå¾®è°ƒæŠ€æœ¯ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/DreamBoothDPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ControlGenAI/DreamBoothDPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20975v1">PDF</a> The first two authors contributed equally. The source code can be   found at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/DreamBoothDPO">https://github.com/ControlGenAI/DreamBoothDPO</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æˆåŠŸåº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå¹³è¡¡æ¦‚å¿µä¿çœŸä¸ä¸Šä¸‹æ–‡å¯¹é½çš„éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤šæ ·è¾“å‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆé…å¯¹æ•°æ®é›†è¿›è¡Œç±»ä¼¼DPOçš„è®­ç»ƒï¼Œæ”¹è¿›äº†æ¦‚å¿µä¿çœŸå’Œæç¤ºéµå¾ªã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ”¯æŒåœ¨å›¾åƒä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½ä¹‹é—´çµæ´»è°ƒæ•´æƒè¡¡ã€‚é€šè¿‡å¤šæ­¥è®­ç»ƒï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¸Šè¶…è¶Šäº†ç®€å•åŸºçº¿ï¼Œå¹¶åœ¨å„ç§æ¶æ„å’Œå¾®è°ƒæŠ€æœ¯æ–¹é¢å±•ç¤ºäº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å±•ç°å‡ºè‰²æˆåŠŸã€‚</li>
<li>å¹³è¡¡æ¦‚å¿µä¿çœŸä¸ä¸Šä¸‹æ–‡å¯¹é½æ˜¯å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆé…å¯¹æ•°æ®é›†è¿›è¡ŒDPOå¼è®­ç»ƒï¼Œæ”¹è¿›æ¦‚å¿µä¿çœŸå’Œæç¤ºéµå¾ªã€‚</li>
<li>æ–¹æ³•æ”¯æŒåœ¨å›¾åƒä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½ä¹‹é—´çµæ´»è°ƒæ•´æƒè¡¡ã€‚</li>
<li>é€šè¿‡å¤šæ­¥è®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b51d38dc8ea47866dba320f173695f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8541d440af923a222bf565a5eb4d9010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b740bd3a6263f59fde4e242748da6a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21abc3e4354e63a354d13464f89005a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe81dfc7f0bb2a0f03593c5f56fb613a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OrienText-Surface-Oriented-Textual-Image-Generation"><a href="#OrienText-Surface-Oriented-Textual-Image-Generation" class="headerlink" title="OrienText: Surface Oriented Textual Image Generation"></a>OrienText: Surface Oriented Textual Image Generation</h2><p><strong>Authors:Shubham Singh Paliwal, Arushi Jain, Monika Sharma, Vikram Jamwal, Lovekesh Vig</strong></p>
<p>Textual content in images is crucial in e-commerce sectors, particularly in marketing campaigns, product imaging, advertising, and the entertainment industry. Current text-to-image (T2I) generation diffusion models, though proficient at producing high-quality images, often struggle to incorporate text accurately onto complex surfaces with varied perspectives, such as angled views of architectural elements like buildings, banners, or walls. In this paper, we introduce the Surface Oriented Textual Image Generation (OrienText) method, which leverages region-specific surface normals as conditional input to T2I generation diffusion model. Our approach ensures accurate rendering and correct orientation of the text within the image context. We demonstrate the effectiveness of the OrienText method on a self-curated dataset of images and compare it against the existing textual image generation methods. </p>
<blockquote>
<p>å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹åœ¨ç”µå­å•†åŠ¡é¢†åŸŸè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è¥é”€ã€äº§å“å›¾åƒã€å¹¿å‘Šå’Œå¨±ä¹è¡Œä¸šã€‚å½“å‰çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹è™½ç„¶æ“…é•¿ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å°†æ–‡æœ¬å‡†ç¡®èå…¥å…·æœ‰ä¸åŒè§†è§’çš„å¤æ‚è¡¨é¢æ—¶å¾€å¾€é‡åˆ°å›°éš¾ï¼Œä¾‹å¦‚å»ºç­‘ç‰©çš„å€¾æ–œè§†å›¾ã€æ¨ªå¹…æˆ–å¢™å£ç­‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘è¡¨é¢çš„æ–‡æœ¬å›¾åƒç”Ÿæˆï¼ˆOrienTextï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŒºåŸŸç‰¹å®šçš„è¡¨é¢æ³•çº¿ä½œä¸ºæ¡ä»¶è¾“å…¥åº”ç”¨äºT2Iç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†å›¾åƒä¸Šä¸‹æ–‡ä¸­æ–‡æœ¬çš„å‡†ç¡®æ¸²æŸ“å’Œæ­£ç¡®æ–¹å‘ã€‚æˆ‘ä»¬åœ¨è‡ªå·±æ•´ç†çš„å›¾åƒæ•°æ®é›†ä¸Šå±•ç¤ºäº†OrienTextæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰çš„æ–‡æœ¬å›¾åƒç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20958v1">PDF</a> 4 pages, SIGGRAPH Asia 2024 Technical Communications</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬å†…å®¹ä¸­çš„å›¾åƒåœ¨ç”µå­å•†åŠ¡é¢†åŸŸå°¤ä¸ºå…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨è¥é”€ã€äº§å“å½±åƒã€å¹¿å‘Šå’Œå¨±ä¹äº§ä¸šä¸­ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å°†æ–‡æœ¬å‡†ç¡®åœ°èå…¥åˆ°å…·æœ‰ä¸åŒè§†è§’çš„å¤æ‚è¡¨é¢ä¸Šæ—¶ï¼Œå¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å»ºç­‘å…ƒç´ ã€æ¨ªå¹…æˆ–å¢™å£çš„å€¾æ–œè§†å›¾ç­‰ã€‚æœ¬æ–‡ä»‹ç»äº†é¢å‘è¡¨é¢çš„æ–‡æœ¬å›¾åƒç”Ÿæˆï¼ˆOrienTextï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŒºåŸŸç‰¹å®šçš„è¡¨é¢æ³•çº¿ä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç¡®ä¿æ–‡æœ¬åœ¨å›¾åƒä¸Šä¸‹æ–‡ä¸­çš„å‡†ç¡®æ¸²æŸ“å’Œæ­£ç¡®æ–¹å‘ã€‚æˆ‘ä»¬åœ¨è‡ªåˆ¶çš„å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†OrienTextæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰çš„æ–‡æœ¬å›¾åƒç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å†…å®¹åœ¨å›¾åƒä¸­çš„å‡†ç¡®æ€§å¯¹ç”µå­å•†åŠ¡é¢†åŸŸè‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨è¥é”€ã€äº§å“å½±åƒã€å¹¿å‘Šå’Œå¨±ä¹äº§ä¸šä¸­ã€‚</li>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§’çš„æ–‡æœ¬é›†æˆæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>OrienTextæ–¹æ³•åˆ©ç”¨åŒºåŸŸç‰¹å®šçš„è¡¨é¢æ³•çº¿ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œä»¥æé«˜æ–‡æœ¬åœ¨å›¾åƒä¸­çš„æ¸²æŸ“å‡†ç¡®æ€§ã€‚</li>
<li>OrienTextæ–¹æ³•ç¡®ä¿äº†æ–‡æœ¬åœ¨å›¾åƒä¸Šä¸‹æ–‡ä¸­çš„æ­£ç¡®æ–¹å‘å’Œå‡†ç¡®æ¸²æŸ“ã€‚</li>
<li>è®ºæ–‡åœ¨è‡ªåˆ¶çš„å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†OrienTextæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>OrienTextæ–¹æ³•ä¸ç°æœ‰çš„æ–‡æœ¬å›¾åƒç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨å¤æ‚è¡¨é¢ä¸Šçš„è¡¨ç°å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c84d79cbdf7d32ebe172c424e6b75371.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423b96b19b79a98dc5a6068fef5cba53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9909a9f8a4709d5202f766f14e6ce05f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e469fccb6f02b41f1fcd5aa64b8b2c74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-990a2eb45a7174677ecc1043438b3864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bdcf1c8edca6f407dc1bb79b3f7e2aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9797897063bd422e14e0eda140bbb7a0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Create-Anything-Anywhere-Layout-Controllable-Personalized-Diffusion-Model-for-Multiple-Subjects"><a href="#Create-Anything-Anywhere-Layout-Controllable-Personalized-Diffusion-Model-for-Multiple-Subjects" class="headerlink" title="Create Anything Anywhere: Layout-Controllable Personalized Diffusion   Model for Multiple Subjects"></a>Create Anything Anywhere: Layout-Controllable Personalized Diffusion   Model for Multiple Subjects</h2><p><strong>Authors:Wei Li, Hebei Li, Yansong Peng, Siying Wu, Yueyi Zhang, Xiaoyan Sun</strong></p>
<p>Diffusion models have significantly advanced text-to-image generation, laying the foundation for the development of personalized generative frameworks. However, existing methods lack precise layout controllability and overlook the potential of dynamic features of reference subjects in improving fidelity. In this work, we propose Layout-Controllable Personalized Diffusion (LCP-Diffusion) model, a novel framework that integrates subject identity preservation with flexible layout guidance in a tuning-free approach. Our model employs a Dynamic-Static Complementary Visual Refining module to comprehensively capture the intricate details of reference subjects, and introduces a Dual Layout Control mechanism to enforce robust spatial control across both training and inference stages. Extensive experiments validate that LCP-Diffusion excels in both identity preservation and layout controllability. To the best of our knowledge, this is a pioneering work enabling users to â€œcreate anything anywhereâ€. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºä¸ªæ€§åŒ–ç”Ÿæˆæ¡†æ¶çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç²¾ç¡®çš„å¸ƒå±€å¯æ§æ€§ï¼Œå¹¶å¿½è§†äº†å‚è€ƒä¸»é¢˜åŠ¨æ€ç‰¹å¾åœ¨æé«˜ä¿çœŸåº¦æ–¹é¢çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¸ƒå±€å¯æ§ä¸ªæ€§åŒ–æ‰©æ•£ï¼ˆLCP-Diffusionï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œä»¥æ— è°ƒæ•´çš„æ–¹å¼å°†ä¸»é¢˜èº«ä»½ä¿ç•™ä¸çµæ´»å¸ƒå±€æŒ‡å¯¼ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨åŠ¨æ€é™æ€äº’è¡¥è§†è§‰ç»†åŒ–æ¨¡å—ï¼Œå…¨é¢æ•æ‰å‚è€ƒä¸»é¢˜çš„å¤æ‚ç»†èŠ‚ï¼Œå¹¶å¼•å…¥åŒé‡å¸ƒå±€æ§åˆ¶æœºåˆ¶ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå®æ–½ç¨³å¥çš„ç©ºé—´æ§åˆ¶ã€‚å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼ŒLCP-Diffusionåœ¨èº«ä»½ä¿ç•™å’Œå¸ƒå±€æ§åˆ¶æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ä¸€é¡¹å¼€åˆ›æ€§å·¥ä½œï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿâ€œåœ¨ä»»ä½•åœ°æ–¹åˆ›å»ºä»»ä½•å†…å®¹â€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20909v1">PDF</a> ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºä¸ªæ€§åŒ–ç”Ÿæˆæ¡†æ¶çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç²¾ç¡®çš„å¸ƒå±€å¯æ§æ€§ï¼Œå¿½è§†äº†å‚è€ƒä¸»ä½“åŠ¨æ€ç‰¹å¾åœ¨æé«˜ä¿çœŸåº¦æ–¹é¢çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†Layout-Controllable Personalized Diffusionï¼ˆLCP-Diffusionï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆä¸»ä½“èº«ä»½ä¿ç•™ä¸çµæ´»å¸ƒå±€æŒ‡å¯¼çš„å…¨æ–°æ¡†æ¶ï¼Œé‡‡ç”¨æ— è°ƒæ•´çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹é€šè¿‡åŠ¨æ€é™æ€äº’è¡¥è§†è§‰ç»†åŒ–æ¨¡å—å…¨é¢æ•æ‰å‚è€ƒä¸»ä½“çš„ç»†èŠ‚ï¼Œå¹¶å¼•å…¥åŒé‡å¸ƒå±€æ§åˆ¶æœºåˆ¶ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå®ç°ç¨³å¥çš„ç©ºé—´æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒLCP-Diffusionåœ¨èº«ä»½ä¿ç•™å’Œå¸ƒå±€å¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™æ˜¯ä¸€é¡¹å¼€åˆ›æ€§å·¥ä½œï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿâ€œåœ¨ä»»ä½•åœ°æ–¹åˆ›å»ºä»»ä½•å†…å®¹â€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¸ºä¸ªæ€§åŒ–ç”Ÿæˆæ¡†æ¶å¥ å®šåŸºç¡€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹ç²¾ç¡®çš„å¸ƒå±€å¯æ§æ€§ã€‚</li>
<li>LCP-Diffusionæ¨¡å‹ç»“åˆäº†ä¸»ä½“èº«ä»½ä¿ç•™ä¸çµæ´»å¸ƒå±€æŒ‡å¯¼ã€‚</li>
<li>åŠ¨æ€é™æ€äº’è¡¥è§†è§‰ç»†åŒ–æ¨¡å—å…¨é¢æ•æ‰å‚è€ƒä¸»ä½“çš„ç»†èŠ‚ã€‚</li>
<li>åŒé‡å¸ƒå±€æ§åˆ¶æœºåˆ¶åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå®ç°ç¨³å¥çš„ç©ºé—´æ§åˆ¶ã€‚</li>
<li>LCP-Diffusionæ¨¡å‹åœ¨èº«ä»½ä¿ç•™å’Œå¸ƒå±€å¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d528573934fb1e6672a5d185e61c877.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c48268dc31a1cffa787322651333db31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-554ea3826a12a2f0af540875421ffe9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1f9420d7a489de5915d84cbe93e0cfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0adf6b0abd96aefad7e6ec3888ae4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438f4b03299e50bf4e47d044855df6da.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models"><a href="#Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models" class="headerlink" title="Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models"></a>Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models</h2><p><strong>Authors:Yang Zheng, Wen Li, Zhaoqiang Liu</strong></p>
<p>Inverse problems (IPs) involve reconstructing signals from noisy observations. Traditional approaches often rely on handcrafted priors, which can fail to capture the complexity of real-world data. The advent of pre-trained generative models has introduced new paradigms, offering improved reconstructions by learning rich priors from data. Among these, diffusion models (DMs) have emerged as a powerful framework, achieving remarkable reconstruction performance across numerous IPs. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approach under appropriate conditions and validate its superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers. </p>
<blockquote>
<p>é€†é—®é¢˜ï¼ˆIPsï¼‰æ¶‰åŠä»å™ªå£°è§‚å¯Ÿä¸­é‡å»ºä¿¡å·ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™å¯èƒ½æ— æ³•æ•æ‰çœŸå®ä¸–ç•Œæ•°æ®çš„å¤æ‚æ€§ã€‚é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„å‡ºç°å¼•å…¥äº†æ–°çš„èŒƒå¼ï¼Œé€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œæä¾›äº†æ”¹è¿›çš„é‡æ„ã€‚å…¶ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ¡†æ¶è„±é¢–è€Œå‡ºï¼Œåœ¨å¤šä¸ªIPsä¸­å®ç°äº†æ˜¾è‘—çš„é‡æ„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDMçš„æ–¹æ³•ç»å¸¸é¢ä¸´è®¡ç®—éœ€æ±‚å¤§ã€æ”¶æ•›æ€§ä¸ä½³ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºæœ€è¿‘çš„å·¥ä½œDMPlugï¼ˆWang et al., 2024ï¼‰æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•ï¼šDMILOå’ŒDMILO-PGDï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ç§æ–¹æ³•DMILOé‡‡ç”¨ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰æ¥ç¼“è§£DMPlugå›ºæœ‰çš„å†…å­˜è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–åå·®ï¼Œæˆ‘ä»¬æ‰©å¤§äº†DMçš„èŒƒå›´ï¼Œèƒ½å¤Ÿæ¢ç´¢å¯èƒ½ä½äºæ‰©æ•£æ¨¡å‹èŒƒå›´ä¹‹å¤–çš„æ½œåœ¨ä¿¡å·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†DMILO-PGDï¼Œå®ƒå°†ILOä¸æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ç›¸ç»“åˆï¼Œä»è€Œé™ä½äº†æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£çš„é£é™©ã€‚æˆ‘ä»¬åœ¨é€‚å½“æ¡ä»¶ä¸‹ç›´è§‚åœ°åˆ†æäº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯¹å„ç§å›¾åƒæ•°æ®é›†çš„å¤§é‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¿™äº›å®éªŒæ¶µç›–äº†çº¿æ€§å’Œéçº¿æ€§IPsã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†DMILOå’ŒDMILO-PGDåœ¨è§£å†³åŸºäºDMçš„IPæ±‚è§£å™¨ä¸­çš„å¸¸è§æŒ‘æˆ˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20789v1">PDF</a> ICML 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†é€†é—®é¢˜ä¸­çš„æœ€æ–°è¿›å±•ã€‚ä¼ ç»Ÿçš„é€†å¤„ç†æ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾è®¡çš„å…ˆéªŒçŸ¥è¯†ï¼Œéš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚éšç€é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„å‡ºç°ï¼Œæ–°çš„æ–¹æ³•å¦‚æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»åœ¨è®¸å¤šé€†é—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„é‡å»ºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DMæ–¹æ³•å¸¸å¸¸é¢ä¸´è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§ä¸ä½³çš„é—®é¢˜ã€‚æœ¬æ–‡åŸºäºDMPlugå·¥ä½œæå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•DMILOå’ŒDMILO-PGDæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚DMILOé€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰å‡è½»DMPlugçš„å†…å­˜è´Ÿæ‹…ï¼Œå¹¶å¼•å…¥ç¨€ç–åå·®æ¥æ‰©å±•DMçš„èŒƒå›´ã€‚è€ŒDMILO-PGDåˆ™ç»“åˆäº†ILOå’ŒæŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ï¼Œé™ä½äº†æ¬¡ä¼˜æ”¶æ•›çš„é£é™©ã€‚æœ¬æ–‡åœ¨å¤šç§å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºæ‰€ææ–¹æ³•åœ¨è§£å†³çº¿æ€§åŠéçº¿æ€§é€†é—®é¢˜æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†é€†é—®é¢˜ä¸­å±•ç°å‡ºå¼ºå¤§çš„é‡å»ºæ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•é¢ä¸´è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§ä¸ä½³çš„æŒ‘æˆ˜ã€‚</li>
<li>DMILOæ–¹æ³•é€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–å’Œå¼•å…¥ç¨€ç–åå·®æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DMILO-PGDç»“åˆäº†ä¸­é—´å±‚ä¼˜åŒ–å’ŒæŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼Œé™ä½æ¬¡ä¼˜æ”¶æ•›é£é™©ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„ä¸¤ç§æ–°æ–¹æ³•åœ¨å¤šç§å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•å¯¹è§£å†³çº¿æ€§åŠéçº¿æ€§é€†é—®é¢˜å‡æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a43a5cb097281eca9370e96338946c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8440b4d1ceaf394f64337d49e0ff6204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-608c1736489c1294ff5674ac5aeca7bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f249d2a2a4131a2012a96ee62e78474.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LeDiFlow-Learned-Distribution-guided-Flow-Matching-to-Accelerate-Image-Generation"><a href="#LeDiFlow-Learned-Distribution-guided-Flow-Matching-to-Accelerate-Image-Generation" class="headerlink" title="LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image   Generation"></a>LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image   Generation</h2><p><strong>Authors:Pascal Zwick, Nils Friederich, Maximilian Beichter, Lennart Hilbert, Ralf Mikut, Oliver Bringmann</strong></p>
<p>Enhancing the efficiency of high-quality image generation using Diffusion Models (DMs) is a significant challenge due to the iterative nature of the process. Flow Matching (FM) is emerging as a powerful generative modeling paradigm based on a simulation-free training objective instead of a score-based one used in DMs. Typical FM approaches rely on a Gaussian distribution prior, which induces curved, conditional probability paths between the prior and target data distribution. These curved paths pose a challenge for the Ordinary Differential Equation (ODE) solver, requiring a large number of inference calls to the flow prediction network. To address this issue, we present Learned Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for training FM-based image generation models using a better-suited prior distribution learned via a regression-based auxiliary model. By initializing the ODE solver with a prior closer to the target data distribution, LeDiFlow enables the learning of more computationally tractable probability paths. These paths directly translate to fewer solver steps needed for high-quality image generation at inference time. Our method utilizes a State-Of-The-Art (SOTA) transformer architecture combined with latent space sampling and can be trained on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably outperforms the respective FM baselines. For instance, when operating directly on pixels, our model accelerates inference by up to 3.75x compared to the corresponding pixel-space baseline. Simultaneously, our latent FM model enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy (CMMD) metric against its respective baseline. </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æé«˜é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„æ•ˆç‡æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¯¥è¿‡ç¨‹å…·æœ‰è¿­ä»£æ€§ã€‚æµåŒ¹é…ï¼ˆFMï¼‰æ­£å´­éœ²å¤´è§’ï¼Œæˆä¸ºä¸€ç§åŸºäºæ¨¡æ‹Ÿè‡ªç”±è®­ç»ƒç›®æ ‡çš„å¼ºå¤§ç”Ÿæˆå»ºæ¨¡èŒƒå¼ï¼Œè€Œä¸æ˜¯DMsä¸­ä½¿ç”¨çš„åŸºäºè¯„åˆ†çš„ç›®æ ‡ã€‚å…¸å‹çš„FMæ–¹æ³•ä¾èµ–äºé«˜æ–¯åˆ†å¸ƒå…ˆéªŒï¼Œè¿™ä¼šåœ¨å…ˆéªŒå’Œç›®æ ‡æ•°æ®åˆ†å¸ƒä¹‹é—´äº§ç”Ÿå¼¯æ›²çš„æ¡ä»¶æ¦‚ç‡è·¯å¾„ã€‚è¿™äº›å¼¯æ›²çš„è·¯å¾„å¯¹å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨æ„æˆäº†æŒ‘æˆ˜ï¼Œéœ€è¦å¤šæ¬¡è°ƒç”¨æµé¢„æµ‹ç½‘ç»œçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå­¦ä¹ åˆ†å¸ƒå¼•å¯¼æµåŒ¹é…ï¼ˆLeDiFlowï¼‰çš„FMå›¾åƒç”Ÿæˆæ¨¡å‹çš„æ–°å‹å¯æ‰©å±•æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨é€šè¿‡åŸºäºå›å½’çš„è¾…åŠ©æ¨¡å‹å­¦ä¹ çš„æ›´é€‚åˆçš„å…ˆéªŒåˆ†å¸ƒã€‚é€šè¿‡ç”¨æ›´æ¥è¿‘ç›®æ ‡æ•°æ®åˆ†å¸ƒçš„å…ˆéªŒåˆå§‹åŒ–ODEæ±‚è§£å™¨ï¼ŒLeDiFlowèƒ½å¤Ÿå­¦ä¹ æ›´æ˜“äºè®¡ç®—çš„æ¦‚ç‡è·¯å¾„ã€‚è¿™äº›è·¯å¾„ç›´æ¥è½¬åŒ–ä¸ºåœ¨æ¨ç†æ—¶é«˜è´¨é‡å›¾åƒç”Ÿæˆæ‰€éœ€çš„æ›´å°‘çš„æ±‚è§£å™¨æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æœ€å…ˆè¿›çš„å˜å‹å™¨æ¶æ„ï¼Œç»“åˆæ½œåœ¨ç©ºé—´é‡‡æ ·ï¼Œå¯ä»¥åœ¨æ¶ˆè´¹è€…å·¥ä½œç«™ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å®è¯è¡¨æ˜ï¼ŒLeDiFlowæ˜¾è‘—ä¼˜äºå„è‡ªçš„FMåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œå½“ç›´æ¥åœ¨åƒç´ ä¸Šæ“ä½œæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†æ¨ç†åŠ é€Ÿäº†é«˜è¾¾3.75å€ï¼Œç›¸å¯¹äºç›¸åº”çš„åƒç´ ç©ºé—´åŸºçº¿ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ½œåœ¨FMæ¨¡å‹åœ¨CLIPæœ€å¤§å¹³å‡å·®å¼‚ï¼ˆCMMDï¼‰æŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†å›¾åƒè´¨é‡1.32å€ï¼Œè¶…è¿‡äº†å…¶ç›¸åº”çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆç‡é«˜è´¨é‡å›¾åƒç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå…¶è¿­ä»£æ€§è´¨ã€‚æµåŒ¹é…ï¼ˆFMï¼‰ä½œä¸ºä¸€ç§åŸºäºæ¨¡æ‹Ÿçš„æ— è®­ç»ƒç›®æ ‡çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼æ­£åœ¨å…´èµ·ï¼Œç›¸å¯¹äºæ‰©æ•£æ¨¡å‹ä½¿ç”¨çš„åŸºäºåˆ†æ•°çš„è®­ç»ƒç›®æ ‡ï¼Œå®ƒå…·æœ‰æ½œåŠ›ã€‚å…¸å‹çš„FMæ–¹æ³•ä¾èµ–äºé«˜æ–¯åˆ†å¸ƒå…ˆéªŒï¼Œè¿™ä¼šåœ¨å…ˆéªŒå’Œç›®æ ‡æ•°æ®åˆ†å¸ƒä¹‹é—´äº§ç”Ÿå¼¯æ›²çš„æ¡ä»¶æ¦‚ç‡è·¯å¾„ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå­¦ä¹ åˆ†å¸ƒå¼•å¯¼æµåŒ¹é…ï¼ˆLeDiFlowï¼‰çš„è§£å†³æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè®­ç»ƒåŸºäºFMçš„å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨æ›´é€‚åˆçš„å…ˆéªŒåˆ†å¸ƒæ¥æé«˜æ•ˆç‡ã€‚LeDiFlowä½¿ç”¨å›å½’è¾…åŠ©æ¨¡å‹æ¥å­¦ä¹ å…ˆéªŒåˆ†å¸ƒï¼Œé€šè¿‡ä½¿ODEæ±‚è§£å™¨çš„åˆå§‹çŠ¶æ€æ›´æ¥è¿‘ç›®æ ‡æ•°æ®åˆ†å¸ƒï¼Œä»è€Œå­¦ä¹ æ›´æ˜“äºè®¡ç®—çš„æ¦‚ç‡è·¯å¾„ã€‚è¿™äº›è·¯å¾„ç›´æ¥è½¬åŒ–ä¸ºåœ¨æ¨ç†æ—¶ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ‰€éœ€çš„æ›´å°‘æ±‚è§£å™¨æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æœ€å…ˆè¿›çš„è½¬æ¢å™¨æ¶æ„å’Œæ½œåœ¨ç©ºé—´é‡‡æ ·ï¼Œå¯ä»¥åœ¨æ¶ˆè´¹è€…å·¥ä½œç«™ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒLeDiFlowæ˜¾è‘—ä¼˜äºç›¸åº”çš„FMåŸºçº¿ã€‚ä¾‹å¦‚ï¼Œç›´æ¥åœ¨åƒç´ ä¸Šæ“ä½œæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†æ¨ç†é€Ÿåº¦æé«˜äº†3.75å€ï¼›è€Œåœ¨æ½œåœ¨ç©ºé—´ä¸Šæ“ä½œæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æé«˜äº†å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµåŒ¹é…ï¼ˆFMï¼‰æ˜¯ä¸€ç§æ–°å…´çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼ŒåŸºäºæ¨¡æ‹Ÿæ— è®­ç»ƒç›®æ ‡ï¼Œæ½œåŠ›å·¨å¤§ã€‚</li>
<li>å…¸å‹çš„FMæ–¹æ³•ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒå…ˆéªŒï¼Œå¯¼è‡´ä¸ç›®è§£å…ˆéªŒçš„åˆ†å¸ƒä¹‹é—´å‡ºç°å¼¯æ›²çš„æ¡ä»¶æ¦‚ç‡è·¯å¾„é—®é¢˜ã€‚</li>
<li>LeDiFlowä½œä¸ºä¸€ç§æ–°å‹å¯æ‰©å±•æ–¹æ³•ï¼Œé‡‡ç”¨å›å½’è¾…åŠ©æ¨¡å‹å­¦ä¹ æ›´é€‚åˆçš„å…ˆéªŒåˆ†å¸ƒæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LeDiFlowé€šè¿‡ä½¿ODEæ±‚è§£å™¨çš„åˆå§‹çŠ¶æ€æ›´æ¥è¿‘ç›®æ ‡æ•°æ®åˆ†å¸ƒï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å¹¶å‡å°‘äº†æ¨ç†æ—¶é—´æ‰€éœ€çš„æ±‚è§£å™¨æ­¥éª¤ã€‚</li>
<li>LeDiFlowç»“åˆäº†æœ€å…ˆè¿›çš„è½¬æ¢å™¨æ¶æ„å’Œæ½œåœ¨ç©ºé—´é‡‡æ ·æŠ€æœ¯ã€‚</li>
<li>LeDiFlowæ˜¾è‘—ä¼˜äºç°æœ‰çš„FMåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e89ee2e10b1f55f41e95eee2431f07bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59eaa6e1b9c244bb10f5e0c080c2da05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c0607e193136c93cb75631bbac4eb6f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SAIL-Self-supervised-Albedo-Estimation-from-Real-Images-with-a-Latent-Diffusion-Model"><a href="#SAIL-Self-supervised-Albedo-Estimation-from-Real-Images-with-a-Latent-Diffusion-Model" class="headerlink" title="SAIL: Self-supervised Albedo Estimation from Real Images with a Latent   Diffusion Model"></a>SAIL: Self-supervised Albedo Estimation from Real Images with a Latent   Diffusion Model</h2><p><strong>Authors:Hala Djeghim, Nathan Piasco, Luis RoldÃ£o, Moussab Bennehar, Dzmitry Tsishkou, CÃ©line Loscos, DÃ©sirÃ© SidibÃ©</strong></p>
<p>Intrinsic image decomposition aims at separating an image into its underlying albedo and shading components, isolating the base color from lighting effects to enable downstream applications such as virtual relighting and scene editing. Despite the rise and success of learning-based approaches, intrinsic image decomposition from real-world images remains a significant challenging task due to the scarcity of labeled ground-truth data. Most existing solutions rely on synthetic data as supervised setups, limiting their ability to generalize to real-world scenes. Self-supervised methods, on the other hand, often produce albedo maps that contain reflections and lack consistency under different lighting conditions. To address this, we propose SAIL, an approach designed to estimate albedo-like representations from single-view real-world images. We repurpose the prior knowledge of a latent diffusion model for unconditioned scene relighting as a surrogate objective for albedo estimation. To extract the albedo, we introduce a novel intrinsic image decomposition fully formulated in the latent space. To guide the training of our latent diffusion model, we introduce regularization terms that constrain both the lighting-dependent and independent components of our latent image decomposition. SAIL predicts stable albedo under varying lighting conditions and generalizes to multiple scenes, using only unlabeled multi-illumination data available online. </p>
<blockquote>
<p>å†…åœ¨å›¾åƒåˆ†è§£æ—¨åœ¨å°†å›¾åƒåˆ†ç¦»ä¸ºå…¶åŸºæœ¬çš„æè´¨åå°„ï¼ˆalbedoï¼‰å’Œé˜´å½±ç»„ä»¶ï¼Œä»è€Œå°†åŸºç¡€é¢œè‰²ä»å…‰ç…§æ•ˆæœä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»¥æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œä¾‹å¦‚è™šæ‹Ÿç…§æ˜å’Œåœºæ™¯ç¼–è¾‘ã€‚å°½ç®¡åŸºäºå­¦ä¹ çš„æ–¹æ³•çš„å…´èµ·å¹¶å–å¾—äº†æˆåŠŸï¼Œä½†ä»çœŸå®ä¸–ç•Œçš„å›¾åƒä¸­è¿›è¡Œå†…åœ¨å›¾åƒåˆ†è§£ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºç¼ºä¹æ ‡è®°çš„çœŸå®æ•°æ®ã€‚ç°æœ‰çš„å¤§å¤šæ•°è§£å†³æ–¹æ¡ˆä¾èµ–äºåˆæˆæ•°æ®ä½œä¸ºç›‘ç£è®¾ç½®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹çœŸå®åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œè‡ªç›‘ç£çš„æ–¹æ³•é€šå¸¸äº§ç”Ÿçš„æè´¨åå°„å›¾åŒ…å«åå°„ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹ç¼ºä¹ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAILï¼ˆä¸€ç§ä»å•è§†å›¾çœŸå®ä¸–ç•Œå›¾åƒä¼°è®¡ç±»ä¼¼æè´¨åå°„çš„è¡¨ç¤ºæ–¹æ³•ï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä½œä¸ºæ— æ¡ä»¶åœºæ™¯ç…§æ˜ä»£ç†ç›®æ ‡æ¥è¿›è¡Œæè´¨åå°„ä¼°è®¡ã€‚ä¸ºäº†æå–æè´¨åå°„ï¼Œæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥äº†ä¸€ç§æ–°å‹å†…åœ¨å›¾åƒåˆ†è§£æ–¹æ³•ã€‚ä¸ºäº†å¼•å¯¼æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œä»¥çº¦æŸæ½œåœ¨å›¾åƒåˆ†è§£çš„å…‰ç…§ä¾èµ–å’Œç‹¬ç«‹ç»„ä»¶ã€‚SAILå¯ä»¥åœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹é¢„æµ‹ç¨³å®šçš„æè´¨åå°„ï¼Œå¹¶é€‚ç”¨äºå¤šä¸ªåœºæ™¯ï¼Œä»…ä½¿ç”¨åœ¨çº¿å¯ç”¨çš„æ— æ ‡ç­¾å¤šç…§æ˜æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19751v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hala-djeghim.github.io/SAIL/">https://hala-djeghim.github.io/SAIL/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SAILæ–¹æ³•ï¼Œæ—¨åœ¨ä»å•è§†è§’çœŸå®å›¾åƒä¸­ä¼°è®¡å‡ºç±»ä¼¼æè´¨çš„è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä½œä¸ºæ— æ¡ä»¶åœºæ™¯è¡¥å…‰çš„æ›¿ä»£ç›®æ ‡æ¥è¿›è¡Œæè´¨ä¼°è®¡ã€‚é€šè¿‡å¼•å…¥æ–°çš„å†…åœ¨å›¾åƒåˆ†è§£æ–¹æ³•ï¼Œå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­å®Œæˆåˆ†è§£ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥æ­£åˆ™åŒ–é¡¹æ¥çº¦æŸæ½œåœ¨å›¾åƒåˆ†è§£ä¸­çš„å…‰ç…§ä¾èµ–å’Œç‹¬ç«‹æˆåˆ†ï¼Œä»è€ŒæŒ‡å¯¼æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒã€‚SAILèƒ½å¤Ÿåœ¨ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹é¢„æµ‹ç¨³å®šçš„æè´¨ï¼Œå¹¶å¯ä»¥æ¨å¹¿åˆ°å¤šä¸ªåœºæ™¯ï¼Œä»…ä½¿ç”¨åœ¨çº¿å¯ç”¨çš„æ— æ ‡ç­¾å¤šå…‰ç…§æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…åœ¨å›¾åƒåˆ†è§£æ—¨åœ¨åˆ†ç¦»å›¾åƒçš„åº•å±‚æè´¨å’Œé˜´å½±æˆåˆ†ï¼Œä¸ºè™šæ‹Ÿè¡¥å…‰å’Œåœºæ™¯ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨æä¾›åŸºç¡€ã€‚</li>
<li>å­¦ä¹ æ–¹æ³•è™½ç„¶æˆåŠŸï¼Œä½†ç”±äºç¼ºä¹çœŸå®æ•°æ®çš„æ ‡ç­¾ï¼Œä»çœŸå®å›¾åƒä¸­è¿›è¡Œå†…åœ¨å›¾åƒåˆ†è§£ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå¤§å¤šä¾èµ–äºåˆæˆæ•°æ®ä½œä¸ºç›‘ç£è®¾ç½®ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è‡ªç›‘ç£æ–¹æ³•äº§ç”Ÿçš„æè´¨å›¾é€šå¸¸åŒ…å«åå°„ï¼Œå¹¶ä¸”åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹ç¼ºä¹ä¸€è‡´æ€§ã€‚</li>
<li>SAILæ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†æ¥ä¼°è®¡ä»å•è§†è§’çœŸå®å›¾åƒä¸­å¾—å‡ºçš„æè´¨è¡¨ç¤ºã€‚</li>
<li>SAILåœ¨æ½œåœ¨ç©ºé—´ä¸­å®Œæˆå†…åœ¨å›¾åƒåˆ†è§£ï¼Œå¹¶é€šè¿‡å¼•å…¥æ­£åˆ™åŒ–é¡¹æ¥çº¦æŸå…¶å…‰ç…§ä¾èµ–å’Œç‹¬ç«‹æˆåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b59fecf785599edca6708fadc5fd31e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f9306f36afdb3823d1b41ada2fd5b34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab5eaa3042e0ab2b19c9bf867d934ef9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Regularized-Personalization-of-Text-to-Image-Diffusion-Models-without-Distributional-Drift"><a href="#Regularized-Personalization-of-Text-to-Image-Diffusion-Models-without-Distributional-Drift" class="headerlink" title="Regularized Personalization of Text-to-Image Diffusion Models without   Distributional Drift"></a>Regularized Personalization of Text-to-Image Diffusion Models without   Distributional Drift</h2><p><strong>Authors:Gihoon Kim, Hyungjin Park, Taesup Kim</strong></p>
<p>Personalization using text-to-image diffusion models involves adapting a pretrained model to novel subjects with only a few image examples. This task presents a fundamental challenge, as the model must not only learn the new subject effectively but also preserve its ability to generate diverse and coherent outputs across a wide range of prompts. In other words, successful personalization requires integrating new concepts without forgetting previously learned generative capabilities. Forgetting denotes unintended distributional drift, where the modelâ€™s output distribution deviates from that of the original pretrained model. In this paper, we provide an analysis of this issue and identify a mismatch between standard training objectives and the goals of personalization. To address this, we propose a new training objective based on a Lipschitz-bounded formulation that explicitly constrains deviation from the pretrained distribution. Our method provides improved control over distributional drift and performs well even in data-scarce scenarios. Experimental results demonstrate that our approach consistently outperforms existing personalization methods, achieving higher CLIP-T, CLIP-I, and DINO scores. </p>
<blockquote>
<p>ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–æ¶‰åŠä½¿ç”¨ä»…å°‘æ•°å›¾åƒç¤ºä¾‹æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ä»¥è¿›è¡Œæ–°å‹ä¸»é¢˜å¤„ç†ã€‚æ­¤ä»»åŠ¡å‘ˆç°äº†ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹ä¸ä»…éœ€è¦æœ‰æ•ˆåœ°å­¦ä¹ æ–°ä¸»é¢˜ï¼Œè€Œä¸”è¿˜è¦ä¿æŒå…¶åœ¨å¹¿æ³›æç¤ºä¸­ç”Ÿæˆå¤šæ ·åŒ–å’Œè¿è´¯è¾“å‡ºçš„èƒ½åŠ›ã€‚æ¢å¥è¯è¯´ï¼ŒæˆåŠŸçš„ä¸ªæ€§åŒ–è¦æ±‚é›†æˆæ–°æ¦‚å¿µè€Œä¸ä¼šå¿˜è®°å…ˆå‰å­¦ä¹ çš„ç”Ÿæˆèƒ½åŠ›ã€‚é—å¿˜è¡¨ç¤ºæ„å¤–çš„åˆ†å¸ƒæ¼‚ç§»ï¼Œå³æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåç¦»åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æ­¤é—®é¢˜è¿›è¡Œäº†åˆ†æï¼Œå¹¶ç¡®å®šäº†æ ‡å‡†è®­ç»ƒç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLipschitzç•Œå®šçš„æ–°è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡æ˜¾å¼çº¦æŸäº†é¢„è®­ç»ƒåˆ†å¸ƒçš„åå·®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ§åˆ¶åˆ†å¸ƒæ¼‚ç§»æ–¹é¢æä¾›äº†æ›´å¥½çš„æ§åˆ¶ï¼Œå³ä½¿åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„CLIP-Tã€CLIP-Iå’ŒDINOåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19519v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–é€‚åº”æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ¨¡å‹ä¸ä»…é«˜æ•ˆå­¦ä¹ æ–°ä¸»é¢˜ï¼Œè¿˜è¦ä¿æŒå¯¹å„ç§æç¤ºç”Ÿæˆå¤šæ ·åŒ–å’Œè¿è´¯æ€§è¾“å‡ºçš„èƒ½åŠ›ã€‚ä¸ªäººåŒ–è¿‡ç¨‹ä¸­ä¼šå‡ºç°æ„å¤–åˆ†å¸ƒæ¼‚ç§»çš„é—®é¢˜ï¼Œå³æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåç¦»åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒã€‚æœ¬æ–‡åˆ†æè¿™ä¸€é—®é¢˜ï¼Œå‘ç°æ ‡å‡†è®­ç»ƒç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLipschitzçº¦æŸçš„æ–°è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡èƒ½å¤Ÿæ˜ç¡®æ§åˆ¶åˆ†å¸ƒåç¦»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä¸€è‡´ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¾¾åˆ°æ›´é«˜çš„CLIP-Tã€CLIP-Iå’ŒDINOè¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–é€‚åº”æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦æ¨¡å‹å­¦ä¹ æ–°ä¸»é¢˜å¹¶ä¿æŒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­å­˜åœ¨åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ï¼Œå³æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåç¦»é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†æ ‡å‡†è®­ç»ƒç›®æ ‡ä¸ä¸ªæ€§åŒ–ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLipschitzçº¦æŸçš„æ–°è®­ç»ƒç›®æ ‡ï¼Œä»¥æ§åˆ¶åˆ†å¸ƒåç¦»ã€‚</li>
<li>æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒè¯„ä¼°ä¸­ä¸€è‡´ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efa0175e72c595051cd5b39cae664512.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c84c63b7632631afe1598f4f66bf5907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc99a82243ff3b3b47e7e67131fb3af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48dd73ed0fb7b79abe12b9ef4dd2023e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Training-free-Stylized-Text-to-Image-Generation-with-Fast-Inference"><a href="#Training-free-Stylized-Text-to-Image-Generation-with-Fast-Inference" class="headerlink" title="Training-free Stylized Text-to-Image Generation with Fast Inference"></a>Training-free Stylized Text-to-Image Generation with Fast Inference</h2><p><strong>Authors:Xin Ma, Yaohui Wang, Xinyuan Chen, Tien-Tsin Wong, Cunjian Chen</strong></p>
<p>Although diffusion models exhibit impressive generative capabilities, existing methods for stylized image generation based on these models often require textual inversion or fine-tuning with style images, which is time-consuming and limits the practical applicability of large-scale diffusion models. To address these challenges, we propose a novel stylized image generation method leveraging a pre-trained large-scale diffusion model without requiring fine-tuning or any additional optimization, termed as OmniPainter. Specifically, we exploit the self-consistency property of latent consistency models to extract the representative style statistics from reference style images to guide the stylization process. Additionally, we then introduce the norm mixture of self-attention, which enables the model to query the most relevant style patterns from these statistics for the intermediate output content features. This mechanism also ensures that the stylized results align closely with the distribution of the reference style images. Our qualitative and quantitative experimental results demonstrate that the proposed method outperforms state-of-the-art approaches. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åŸºäºè¿™äº›æ¨¡å‹çš„é£æ ¼åŒ–å›¾åƒç”Ÿæˆç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦æ–‡æœ¬åè½¬æˆ–ä½¿ç”¨é£æ ¼å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œè¿™æ—¢è€—æ—¶åˆé™åˆ¶äº†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„å®é™…åº”ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é£æ ¼åŒ–å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒæˆ–ä»»ä½•é¢å¤–ä¼˜åŒ–ï¼Œè¢«ç§°ä¸ºOmniPainterã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹çš„è‡ªæ´½æ€§å±æ€§ï¼Œä»å‚è€ƒé£æ ¼å›¾åƒä¸­æå–ä»£è¡¨æ€§é£æ ¼ç»Ÿè®¡ä¿¡æ¯æ¥æŒ‡å¯¼é£æ ¼åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„èŒƒæ•°æ··åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºä¸­é—´è¾“å‡ºå†…å®¹ç‰¹å¾æŸ¥è¯¢æœ€ç›¸å…³çš„é£æ ¼æ¨¡å¼ã€‚è¿™ç§æœºåˆ¶è¿˜ç¡®ä¿é£æ ¼åŒ–ç»“æœä¸å‚è€ƒé£æ ¼å›¾åƒçš„åˆ†å¸ƒç´§å¯†å¯¹é½ã€‚æˆ‘ä»¬çš„å®šæ€§å’Œå®šé‡å®éªŒç»“æœéƒ½è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19063v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://maxin-cn.github.io/omnipainter_project">https://maxin-cn.github.io/omnipainter_project</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼åŒ–å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒæˆ–å…¶ä»–é¢å¤–ä¼˜åŒ–ï¼Œç§°ä¸ºOmniPainterã€‚å®ƒåˆ©ç”¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹çš„è‡ªæ´½æ€§ï¼Œä»å‚è€ƒé£æ ¼å›¾åƒä¸­æå–ä»£è¡¨æ€§é£æ ¼ç»Ÿè®¡ä¿¡æ¯æ¥æŒ‡å¯¼é£æ ¼åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„èŒƒæ•°æ··åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºä¸­é—´è¾“å‡ºå†…å®¹ç‰¹å¾æŸ¥è¯¢æœ€ç›¸å…³çš„é£æ ¼æ¨¡å¼ã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„é£æ ¼åŒ–ç»“æœä¸å‚è€ƒé£æ ¼å›¾åƒåˆ†å¸ƒç´§å¯†å¯¹é½ï¼Œå®éªŒè¯æ˜å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniPainteråˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼åŒ–å›¾åƒç”Ÿæˆï¼Œæ— éœ€é¢å¤–ä¼˜åŒ–æˆ–å¾®è°ƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹çš„è‡ªæ´½æ€§æå–å‚è€ƒé£æ ¼å›¾åƒä¸­çš„ä»£è¡¨æ€§é£æ ¼ç»Ÿè®¡ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥èŒƒæ•°æ··åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸæŸ¥è¯¢æœ€ç›¸å…³çš„é£æ ¼æ¨¡å¼ä¸ä¸­é—´è¾“å‡ºå†…å®¹ç‰¹å¾ç›¸åŒ¹é…ã€‚</li>
<li>OmniPainterç¡®ä¿é£æ ¼åŒ–ç»“æœä¸å‚è€ƒé£æ ¼å›¾åƒåˆ†å¸ƒç´§å¯†å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜OmniPainteråœ¨é£æ ¼åŒ–å›¾åƒç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¤§å¤§å‡å°‘äº†ç”Ÿæˆé£æ ¼åŒ–å›¾åƒæ‰€éœ€çš„æ—¶é—´å’Œå¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea46b9c6e3bba5c3d8ca0e6052498fa2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ff3fa92579c0b91b99c27b345dadf1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f448c7fa51f90e2320973a7c113bdbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b3525ab395c53a84839b1378c8e1ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5cdad68eef07607f87e7ab6e94af08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b70f68fae8086f15b6351f5a57258f42.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Restoring-Real-World-Images-with-an-Internal-Detail-Enhancement-Diffusion-Model"><a href="#Restoring-Real-World-Images-with-an-Internal-Detail-Enhancement-Diffusion-Model" class="headerlink" title="Restoring Real-World Images with an Internal Detail Enhancement   Diffusion Model"></a>Restoring Real-World Images with an Internal Detail Enhancement   Diffusion Model</h2><p><strong>Authors:Peng Xiao, Hongbo Zhao, Yijun Wang, Jianxin Lin</strong></p>
<p>Restoring real-world degraded images, such as old photographs or low-resolution images, presents a significant challenge due to the complex, mixed degradations they exhibit, such as scratches, color fading, and noise. Recent data-driven approaches have struggled with two main challenges: achieving high-fidelity restoration and providing object-level control over colorization. While diffusion models have shown promise in generating high-quality images with specific controls, they often fail to fully preserve image details during restoration. In this work, we propose an internal detail-preserving diffusion model for high-fidelity restoration of real-world degraded images. Our method utilizes a pre-trained Stable Diffusion model as a generative prior, eliminating the need to train a model from scratch. Central to our approach is the Internal Image Detail Enhancement (IIDE) technique, which directs the diffusion model to preserve essential structural and textural information while mitigating degradation effects. The process starts by mapping the input image into a latent space, where we inject the diffusion denoising process with degradation operations that simulate the effects of various degradation factors. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art models in both qualitative assessments and perceptual quantitative evaluations. Additionally, our approach supports text-guided restoration, enabling object-level colorization control that mimics the expertise of professional photo editing. </p>
<blockquote>
<p>æ¢å¤çœŸå®ä¸–ç•Œä¸­çš„é€€åŒ–å›¾åƒï¼Œå¦‚æ—§ç…§ç‰‡æˆ–ä½åˆ†è¾¨ç‡å›¾åƒï¼Œæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬è¡¨ç°å‡ºçš„å¤æ‚æ··åˆé€€åŒ–ï¼Œå¦‚åˆ’ç—•ã€é¢œè‰²è¤ªè‰²å’Œå™ªå£°ã€‚æœ€è¿‘çš„æ•°æ®é©±åŠ¨æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå®ç°é«˜ä¿çœŸæ¢å¤å’Œæä¾›é¢œè‰²åŒ–çš„å¯¹è±¡çº§æ§åˆ¶ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰ç‰¹å®šæ§åˆ¶çš„é«˜è´¨é‡å›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æ¢å¤è¿‡ç¨‹ä¸­æ— æ³•å®Œå…¨ä¿ç•™å›¾åƒç»†èŠ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…éƒ¨ç»†èŠ‚ä¿ç•™çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé«˜ä¿çœŸåœ°æ¢å¤çœŸå®ä¸–ç•Œçš„é€€åŒ–å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å†…éƒ¨å›¾åƒç»†èŠ‚å¢å¼ºï¼ˆIIDEï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æŒ‡å¯¼æ‰©æ•£æ¨¡å‹åœ¨ä¿ç•™åŸºæœ¬ç»“æ„å’Œçº¹ç†ä¿¡æ¯çš„åŒæ—¶ï¼Œå‡è½»é€€åŒ–æ•ˆæœã€‚è¯¥è¿‡ç¨‹é¦–å…ˆå°†é€šè¿‡è¾“å…¥å›¾åƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œç„¶åæˆ‘ä»¬åœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­æ³¨å…¥é€€åŒ–æ“ä½œï¼Œä»¥æ¨¡æ‹Ÿå„ç§é€€åŒ–å› ç´ çš„å½±å“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„æ¢å¤ï¼Œèƒ½å¤Ÿå®ç°å¯¹è±¡çº§çš„é¢œè‰²æ§åˆ¶ï¼Œæ¨¡æ‹Ÿä¸“ä¸šç…§ç‰‡ç¼–è¾‘çš„ä¸“ä¸šæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18674v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå†…éƒ¨ç»†èŠ‚ä¿ç•™çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ¢å¤çœŸå®ä¸–ç•Œé€€åŒ–å›¾åƒã€‚è¯¥ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡å†…éƒ¨å›¾åƒç»†èŠ‚å¢å¼ºï¼ˆIIDEï¼‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¿ç•™é‡è¦çš„ç»“æ„å’Œçº¹ç†ä¿¡æ¯ï¼ŒåŒæ—¶å‡è½»é€€åŒ–æ•ˆæœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œæ„ŸçŸ¥å®šé‡è¯„ä¼°æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„æ¢å¤ï¼Œèƒ½å¤Ÿå®ç°å¯¹è±¡çº§åˆ«çš„è‰²å½©åŒ–æ§åˆ¶ï¼Œæ¨¡æ‹Ÿä¸“ä¸šç…§ç‰‡ç¼–è¾‘çš„ä¸“é•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå†…éƒ¨ç»†èŠ‚ä¿ç•™çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ¢å¤çœŸå®ä¸–ç•Œé€€åŒ–å›¾åƒã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä½œä¸ºç”Ÿæˆå…ˆéªŒï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>é€šè¿‡IIDEæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä¿ç•™é‡è¦çš„ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚</li>
<li>å‡è½»äº†å›¾åƒé€€åŒ–æ•ˆæœï¼Œå¦‚åˆ’ç—•ã€é¢œè‰²è¤ªè‰²å’Œå™ªéŸ³ã€‚</li>
<li>åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„æ¢å¤ï¼Œå®ç°å¯¹è±¡çº§åˆ«çš„è‰²å½©åŒ–æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8489a1a5181c1ca7fac2981e33c252dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf2bf47c840b7a9de6a3bd50efbf9605.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e8a09590423e9518b2d80daac23591b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-044c78c0f1d81881735d92f65ad13e51.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ç²¾ç¡®å…¨é¢çš„ä¸´åºŠè¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ‰©æ•£æ¨¡å‹æ˜¯åˆæˆæ‰€éœ€åŒ»å­¦å›¾åƒçš„ä¸€ç§å¼ºå¤§ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»å­˜åœ¨å› é«˜é¢‘ä¿¡æ¯è¿‡åº¦æ‹Ÿåˆè€Œå¯¼è‡´è§£å‰–ç»“æ„å¤±çœŸä»¥åŠä½é¢‘ä¿¡æ¯å‡å¼±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†æŒ‡å¯¼çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾æ¥æå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é¢‘ç‡ï¼Œä»¥å¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆçš„é«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚ä¸ºäº†å…‹æœä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´å·¨å¤§å·®å¼‚æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èåˆäº†æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ä¸è§†è§‰ç‰¹å¾ï¼Œä»¥ä¿ƒè¿›å‡†ç¡®è§£å‰–ç»“æ„çš„ç”Ÿæˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v2">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆåœ¨ä¸´åºŠåŒ»å­¦è¯Šæ–­ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„è§£å‰–ç»“æ„æ‰­æ›²é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾ï¼Œæå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œè®¾è®¡åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—è‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºäº†çŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œèåˆäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œä»¥ç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ï¼Œè§£å†³äº†ä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´çš„å·®å¼‚é—®é¢˜ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„å®šæ€§å®šé‡æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„å…³é”®ä½œç”¨ä»¥åŠæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆæˆä¸­çš„å¼ºå¤§ç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„è§£å‰–ç»“æ„æ‰­æ›²é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºé«˜é¢‘ä¿¡æ¯çš„è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯çš„å‡å¼±ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹ç‰¹å¾ï¼Œæå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚</li>
<li>è®¾è®¡äº†åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œè‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œä»¥å¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚</li>
<li>æ„å»ºçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œèåˆè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œè§£å†³ä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´çš„å·®å¼‚é—®é¢˜ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e2f95b72251a596f9478be8a8bbaf6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85c531eab5646c4c5753970e1ad9ca85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5c9fbf2f2d2e8b959698c952e87886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16d0ee28d053b37657d39b8c1df380c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469a4ebf7d5195a3575d68266a67bc32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65e76e642496b8d2899d05866e3f3785.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation"><a href="#One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation" class="headerlink" title="One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation"></a>One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation</h2><p><strong>Authors:Daniil Selikhanovych, David Li, Aleksei Leonov, Nikita Gushchin, Sergei Kushneriuk, Alexander Filippov, Evgeny Burnaev, Iaroslav Koshelev, Alexander Korotin</strong></p>
<p>Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å¤„ç†ä¸­èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„è§†è§‰æ•ˆæœï¼Œä½†éœ€è¦æ˜‚è´µçš„è®¡ç®—æˆæœ¬ã€‚å°½ç®¡å·²ç»å¼€å‘äº†å‡ ç§æ–¹æ³•æ¥åŠ é€ŸåŸºäºæ‰©æ•£çš„SRæ¨¡å‹ï¼Œä½†ä¸€äº›æ–¹æ³•ï¼ˆä¾‹å¦‚SinSRï¼‰æ— æ³•äº§ç”Ÿé€¼çœŸçš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œè€Œå…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚OSEDiffï¼‰å¯èƒ½ä¼šè™šæ„ä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RSDï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é¡¶çº§åŸºäºæ‰©æ•£çš„SRæ¨¡å‹ä¹‹ä¸€ResShiftçš„æ–°å‹è’¸é¦æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œæ¥äº§ç”Ÿå›¾åƒï¼Œè¿™äº›å›¾åƒä¼šé€šè¿‡ä¸€ä¸ªåœ¨æ–°å›¾åƒä¸Šè®­ç»ƒçš„å‡å†’ResShiftæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ç›¸å»åˆã€‚RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„è’¸é¦æ–¹æ³•å¯ä»¥è¶…è¶Šç”¨äºResShiftçš„å…¶ä»–åŸºäºè’¸é¦çš„æ–¹æ³•â€”â€”SinSRâ€”â€”ä½¿å…¶ä¸æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„SRè’¸é¦æ–¹æ³•ç›¸åª²ç¾ã€‚ä¸åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„SRæ–¹æ³•ç›¸æ¯”ï¼ŒRSDäº§ç”Ÿçš„æ„ŸçŸ¥è´¨é‡å…·æœ‰ç«äº‰åŠ›ï¼Œä¸ºé€€åŒ–è¾“å…¥å›¾åƒæä¾›äº†æ›´å¥½çš„å›¾åƒå¯¹é½ï¼Œå¹¶ä¸”éœ€è¦æ›´å°‘çš„å‚æ•°å’ŒGPUå†…å­˜ã€‚æˆ‘ä»¬åœ¨å„ç§çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šæä¾›äº†å®éªŒç»“æœï¼ŒåŒ…æ‹¬RealSRã€RealSet65ã€DRealSRã€ImageNetå’ŒDIV2Kã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13358v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å¤„ç†ä¸­èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å°½ç®¡å·²æœ‰å¤šç§åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚SinSRæ— æ³•äº§ç”ŸçœŸå®çš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œè€ŒOSEDiffä¼šäº§ç”Ÿä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è’¸é¦æ–¹æ³•RSDï¼Œç”¨äºé¡¶çº§çš„æ‰©æ•£æ¨¡å‹ResShiftã€‚è¯¥æ–¹æ³•åŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œäº§ç”Ÿå›¾åƒï¼Œä½¿åŸºäºè¿™äº›å›¾åƒè®­ç»ƒçš„æ–°å‡ResShiftæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ã€‚RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è’¸é¦æ–¹æ³•èƒ½è¶…è¶ŠSinSRç­‰å…¶å®ƒè’¸é¦æ–¹æ³•ï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³çš„æ‰©æ•£SRæ¨¡å‹è¡¨ç°ã€‚ä¸åŸºäºé¢„è®­ç»ƒæ–‡æœ¬å›¾åƒçš„SRæ–¹æ³•ç›¸æ¯”ï¼ŒRSDæ„ŸçŸ¥è´¨é‡é«˜ï¼Œæ›´è´´åˆè¾“å…¥çš„ä½åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶ä¸”éœ€è¦çš„å‚æ•°å’ŒGPUå†…å­˜æ›´å°ã€‚å®éªŒæ•ˆæœåœ¨ä¸åŒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡å¤„ç†ä¸­èƒ½äº§ç”Ÿé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå¦‚æ— æ³•äº§ç”ŸçœŸå®ç»†èŠ‚æˆ–äº§ç”Ÿä¸å­˜åœ¨çš„ç»“æ„ã€‚</li>
<li>RSDæ˜¯ä¸€ç§æ–°å‹çš„è’¸é¦æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–é¡¶çº§æ‰©æ•£æ¨¡å‹ResShiftçš„è¡¨ç°ã€‚</li>
<li>RSDåŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œäº§ç”Ÿå›¾åƒï¼Œä½¿æ–°å‡ResShiftæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ã€‚</li>
<li>RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>RSDçš„è’¸é¦æ–¹æ³•èƒ½è¶…è¶Šå…¶ä»–è’¸é¦æ–¹æ³•ï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³çš„æ‰©æ•£SRæ¨¡å‹è¡¨ç°ã€‚</li>
<li>ä¸å…¶ä»–SRæ–¹æ³•ç›¸æ¯”ï¼ŒRSDå…·æœ‰æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡ã€æ›´è´´åˆè¾“å…¥çš„ä½åˆ†è¾¨ç‡å›¾åƒï¼Œä¸”å‚æ•°å’ŒGPUå†…å­˜éœ€æ±‚è¾ƒå°ã€‚å®éªŒæ•ˆæœåœ¨ä¸åŒæ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75fd41ccc3fd38d2b21093baadc7edc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b214adae8e15c29c83b11784caeedb99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ea25c8fbf4d7d3cde97c27f141bb480.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Lunguage A Benchmark for Structured and Sequential Chest X-ray   Interpretation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a6a9a0c91a1ca27d2a35d3db6ec07a48.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Structure from Collision
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
