<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  M3S-UPD Efficient Multi-Stage Self-Supervised Learning for Fine-Grained   Encrypted Traffic Classification with Unknown Pattern Discovery">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-676919b1f8c980b3dd2b7d823b878945.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    55 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="M3S-UPD-Efficient-Multi-Stage-Self-Supervised-Learning-for-Fine-Grained-Encrypted-Traffic-Classification-with-Unknown-Pattern-Discovery"><a href="#M3S-UPD-Efficient-Multi-Stage-Self-Supervised-Learning-for-Fine-Grained-Encrypted-Traffic-Classification-with-Unknown-Pattern-Discovery" class="headerlink" title="M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained   Encrypted Traffic Classification with Unknown Pattern Discovery"></a>M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained   Encrypted Traffic Classification with Unknown Pattern Discovery</h2><p><strong>Authors:Yali Yuan, Yu Huang, Xingjian Zeng, Hantao Mei, Guang Cheng</strong></p>
<p>The growing complexity of encrypted network traffic presents dual challenges for modern network management: accurate multiclass classification of known applications and reliable detection of unknown traffic patterns. Although deep learning models show promise in controlled environments, their real-world deployment is hindered by data scarcity, concept drift, and operational constraints. This paper proposes M3S-UPD, a novel Multi-Stage Self-Supervised Unknown-aware Packet Detection framework that synergistically integrates semi-supervised learning with representation analysis. Our approach eliminates artificial segregation between classification and detection tasks through a four-phase iterative process: 1) probabilistic embedding generation, 2) clustering-based structure discovery, 3) distribution-aligned outlier identification, and 4) confidence-aware model updating. Key innovations include a self-supervised unknown detection mechanism that requires neither synthetic samples nor prior knowledge, and a continuous learning architecture that is resistant to performance degradation. Experimental results show that M3S-UPD not only outperforms existing methods on the few-shot encrypted traffic classification task, but also simultaneously achieves competitive performance on the zero-shot unknown traffic discovery task. </p>
<blockquote>
<p>ä¸æ–­å¢é•¿çš„åŠ å¯†ç½‘ç»œæµé‡å¤æ‚æ€§ç»™ç°ä»£ç½‘ç»œç®¡ç†å¸¦æ¥äº†åŒé‡æŒ‘æˆ˜ï¼šå‡†ç¡®çš„å¤šç±»å·²çŸ¥åº”ç”¨ç¨‹åºåˆ†ç±»å’Œå¯é çš„æœªçŸ¥æµé‡æ¨¡å¼æ£€æµ‹ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å—æ§ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­å—åˆ°æ•°æ®ç¨€ç¼ºã€æ¦‚å¿µæ¼‚ç§»å’Œæ“ä½œçº¦æŸçš„é˜»ç¢ã€‚æœ¬æ–‡æå‡ºäº†M3S-UPDï¼Œä¸€ç§æ–°é¢–çš„å¤šé˜¶æ®µè‡ªç›‘ç£æœªçŸ¥æ„ŸçŸ¥æ•°æ®åŒ…æ£€æµ‹æ¡†æ¶ï¼ŒååŒæ•´åˆäº†åŠç›‘ç£å­¦ä¹ ä¸è¡¨ç¤ºåˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å››é˜¶æ®µè¿­ä»£è¿‡ç¨‹æ¶ˆé™¤äº†åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡ä¹‹é—´äººä¸ºçš„åˆ†å‰²ï¼š1ï¼‰æ¦‚ç‡åµŒå…¥ç”Ÿæˆï¼Œ2ï¼‰åŸºäºèšç±»çš„ç»“æ„å‘ç°ï¼Œ3ï¼‰åˆ†å¸ƒå¯¹é½å¼‚å¸¸å€¼è¯†åˆ«ï¼Œä»¥åŠ4ï¼‰åŸºäºä¿¡å¿ƒçš„æ¨¡å‹æ›´æ–°ã€‚å…³é”®åˆ›æ–°åŒ…æ‹¬æ— éœ€åˆæˆæ ·æœ¬æˆ–å…ˆéªŒçŸ¥è¯†çš„è‡ªç›‘ç£æœªçŸ¥æ£€æµ‹æœºåˆ¶å’ŒæŠµæŠ—æ€§èƒ½é€€åŒ–çš„æŒç»­å­¦ä¹ æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM3S-UPDä¸ä»…åœ¨å°‘æ•°åŠ å¯†æµé‡åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨é›¶æ ·æœ¬æœªçŸ¥æµé‡å‘ç°ä»»åŠ¡ä¸Šä¹Ÿå®ç°äº†ç«äº‰æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹ç°ä»£ç½‘ç»œç®¡ç†æ‰€é¢ä¸´çš„å·²çŸ¥åº”ç”¨å¤šç±»åˆ†ç±»å’ŒæœªçŸ¥æµé‡æ¨¡å¼æ£€æµ‹åŒé‡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šé˜¶æ®µè‡ªç›‘ç£æœªçŸ¥æ„ŸçŸ¥æ•°æ®åŒ…æ£€æµ‹æ¡†æ¶M3S-UPDã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŠç›‘ç£å­¦ä¹ ä¸è¡¨ç¤ºåˆ†æï¼Œå®ç°äº†æ— éœ€äººå·¥åˆ†å‰²åˆ†ç±»ä¸æ£€æµ‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬è‡ªç›‘ç£æœªçŸ¥æ£€æµ‹æœºåˆ¶å’ŒæŠ—æ€§èƒ½ä¸‹é™çš„è¿ç»­å­¦ä¹ æ¶æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3S-UPDä¸ä»…åœ¨å°‘æ ·æœ¬åŠ å¯†æµé‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨é›¶æ ·æœ¬æœªçŸ¥æµé‡å‘ç°ä»»åŠ¡ä¸Šä¹Ÿå…·å¤‡ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M3S-UPDæ¡†æ¶è§£å†³äº†ç°ä»£ç½‘ç»œç®¡ç†ä¸­åŠ å¯†ç½‘ç»œæµé‡åˆ†ç±»å’ŒæœªçŸ¥æµé‡æ£€æµ‹çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŠç›‘ç£å­¦ä¹ ä¸è¡¨ç¤ºåˆ†æï¼Œæ¶ˆé™¤äº†åˆ†ç±»å’Œæ£€æµ‹ä»»åŠ¡çš„äººå·¥åˆ†å‰²ã€‚</li>
<li>M3S-UPDé‡‡ç”¨å››é˜¶æ®µè¿­ä»£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¦‚ç‡åµŒå…¥ç”Ÿæˆã€åŸºäºèšç±»çš„ç»“æ„å‘ç°ã€åˆ†å¸ƒå¯¹é½çš„å¼‚å¸¸å€¼è¯†åˆ«å’Œä¿¡å¿ƒæ„ŸçŸ¥æ¨¡å‹æ›´æ–°ã€‚</li>
<li>æ¡†æ¶çš„å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬è‡ªç›‘ç£æœªçŸ¥æ£€æµ‹æœºåˆ¶å’ŒæŠ—æ€§èƒ½ä¸‹é™çš„è¿ç»­å­¦ä¹ æ¶æ„ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3S-UPDåœ¨å°‘æ ·æœ¬åŠ å¯†æµé‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>M3S-UPDåœ¨é›¶æ ·æœ¬æœªçŸ¥æµé‡å‘ç°ä»»åŠ¡ä¸Šå…·å¤‡ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€åˆæˆæ ·æœ¬å’Œå…ˆéªŒçŸ¥è¯†ï¼Œå³å¯å®ç°æœªçŸ¥æ£€æµ‹æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a75dcc950c72c20d88042cda41323c86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-825ac38f7e1f1c2a00eba19347d47a9c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning"><a href="#Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning" class="headerlink" title="Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning"></a>Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning</h2><p><strong>Authors:Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah</strong></p>
<p>Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the languageâ€™s low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies. </p>
<blockquote>
<p>è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMath Word Problems, MWPsï¼‰ä»ç„¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå­ŸåŠ æ‹‰è¯­çš„èµ„æºç›¸å¯¹è¾ƒå°‘ä»¥åŠéœ€è¦å¤šæ­¥éª¤æ¨ç†ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜æ—¶é‡åˆ°å›°éš¾ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºä¹‹å‰æ²¡æœ‰äººå¯¹å­ŸåŠ æ‹‰è¯­æ•°æ®é›†è¿›è¡Œè¿™æ–¹é¢çš„æ ‡æ³¨å·¥ä½œã€‚è¿™ä¸€ç©ºç™½é™åˆ¶äº†å­ŸåŠ æ‹‰æ•°å­¦æ¨ç†çš„è¿›æ­¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«8792ä¸ªå¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ä»¥åŠæ‰‹åŠ¨ç¼–å†™çš„é€æ­¥è§£ç­”ã€‚æˆ‘ä»¬è®¾è®¡è¿™ä¸ªæ•°æ®é›†æ˜¯ä¸ºäº†æ”¯æŒåœ¨è¯­è¨€å­¦è¡¨è¾¾ä¸è¶³çš„æƒ…å¢ƒä¸­è¿›è¡Œæ¨ç†è¯„ä¼°æ¨¡å‹çš„å‘å±•ã€‚ä½¿ç”¨SOMADHANæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-3.5 Turboã€LLaMAç³»åˆ—æ¨¡å‹ã€Deepseekå’ŒQwenç­‰ï¼Œé€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºä»¥åŠæœ‰æ— æ€ç»´é“¾ï¼ˆChain of Thought, CoTï¼‰æ¨ç†çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚æ€ç»´é“¾æç¤ºæ³•å§‹ç»ˆæé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­ã€‚LLaMA-3.3 70Båœ¨å°‘æ ·æœ¬æ€ç»´é“¾æç¤ºä¸‹å–å¾—äº†88%çš„æœ€é«˜å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•æ¥æœ‰æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›é«˜è´¨é‡æ¨ç†æ•°æ®é›†å’Œè§£å†³å¤æ‚æ•°å­¦æ–‡å­—é¢˜çš„å¯æ‰©å±•æ¡†æ¶æ¥å¡«è¡¥å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„ç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„å…¬å¹³ç ”ç©¶ï¼Œå¹¶æå‡æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21354v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰çš„æŒ‘æˆ˜ï¼Œç”±äºå­ŸåŠ æ‹‰è¯­çš„ä½èµ„æºçŠ¶æ€å’Œå¤šæ­¥éª¤æ¨ç†çš„éœ€æ±‚ï¼Œç°æœ‰çš„æ¨¡å‹éš¾ä»¥å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«8792ä¸ªå¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜åŠå…¶é€æ­¥è§£å†³æ–¹æ¡ˆã€‚ä½¿ç”¨SOMADHANæ•°æ®é›†ï¼Œç ”ç©¶è€…è¯„ä¼°äº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°ï¼Œå‘ç°é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚LLaMA-3.3 70Båœ¨å°‘æ ·æœ¬CoTæç¤ºä¸‹å–å¾—äº†æœ€é«˜çš„88%å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•é«˜æ•ˆå¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿…é€Ÿé€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ã€‚æ­¤ç ”ç©¶å¡«è¡¥äº†å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„å…³é”®ç©ºç™½ï¼Œä¸ºå¤æ‚æ•°å­¦æ–‡å­—é¢˜çš„å¤„ç†æä¾›äº†é«˜è´¨é‡æ¨ç†æ•°æ®é›†å’Œå¯æ‰©å±•æ¡†æ¶ã€‚æ—¨åœ¨æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„ç ”ç©¶å…¬å¹³æ€§å’Œæå‡æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜æ˜¯NLPé¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä½èµ„æºçŠ¶æ€å’Œå¤šæ­¥éª¤æ¨ç†éœ€æ±‚å¯¼è‡´ç°æœ‰æ¨¡å‹éš¾ä»¥åº”å¯¹ã€‚</li>
<li>åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«å¤æ‚å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜åŠå…¶é€æ­¥è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒæ¨ç†è¯„ä¼°ä¸æ¨¡å‹å‘å±•ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>LLaMA-3.3 70Båœ¨å°‘æ ·æœ¬CoTæç¤ºä¸‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º88%ã€‚</li>
<li>åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œèƒ½é«˜æ•ˆå¾®è°ƒæ¨¡å‹ä»¥é€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶å¡«è¡¥äº†å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„ç©ºç™½ï¼Œæä¾›äº†é«˜è´¨é‡æ¨ç†æ•°æ®é›†å’Œå¤æ‚æ•°å­¦æ–‡å­—é¢˜çš„è§£å†³æ–¹æ¡ˆæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aedb13d51ece17fd5fda0c1ad861e90e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee0512f5ac535ccfc71236625e198cd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-133b208ecfed867a54569cb3c1d5d2fc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Is-Hyperbolic-Space-All-You-Need-for-Medical-Anomaly-Detection"><a href="#Is-Hyperbolic-Space-All-You-Need-for-Medical-Anomaly-Detection" class="headerlink" title="Is Hyperbolic Space All You Need for Medical Anomaly Detection?"></a>Is Hyperbolic Space All You Need for Medical Anomaly Detection?</h2><p><strong>Authors:Alvaro Gonzalez-Jimenez, Simone Lionetti, Ludovic Amruthalingam, Philippe Gottfrois, Fabian GrÃ¶ger, Marc Pouly, Alexander A. Navarini</strong></p>
<p>Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at <a target="_blank" rel="noopener" href="https://hyperbolic-anomalies.github.io/">https://hyperbolic-anomalies.github.io</a> </p>
<blockquote>
<p>åŒ»å­¦å¼‚å¸¸æ£€æµ‹å·²æˆä¸ºè§£å†³æ•°æ®å¯ç”¨æ€§å’Œæ ‡ç­¾çº¦æŸæŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿæ–¹æ³•ä»æ¬§å‡ é‡Œå¾—ç©ºé—´çš„é¢„è®­ç»ƒç½‘ç»œçš„ä¸åŒå±‚æ¬¡ä¸­æå–ç‰¹å¾ï¼›ç„¶è€Œï¼Œæ¬§å‡ é‡Œå¾—è¡¨ç¤ºæ³•æ— æ³•æœ‰æ•ˆåœ°æ•è·è¿™äº›ç‰¹å¾å†…çš„å±‚æ¬¡å…³ç³»ï¼Œå¯¼è‡´å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„æ–¹æ³•ï¼Œå°†ç‰¹å¾è¡¨ç¤ºæŠ•å½±åˆ°åŒæ›²ç©ºé—´ï¼ŒåŸºäºç½®ä¿¡åº¦æ°´å¹³è¿›è¡Œèšåˆï¼Œå¹¶å°†æ ·æœ¬åˆ†ç±»ä¸ºæ­£å¸¸æˆ–å¼‚å¸¸ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŒæ›²ç©ºé—´å§‹ç»ˆä¼˜äºåŸºäºæ¬§å‡ é‡Œå¾—å¾·çš„æ¡†æ¶ï¼Œåœ¨å¤šä¸ªåŒ»å­¦åŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯åœ¨å›¾åƒè¿˜æ˜¯åƒç´ çº§åˆ«ï¼Œéƒ½å®ç°äº†æ›´é«˜çš„AUROCåˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åŒæ›²ç©ºé—´å¯¹å‚æ•°å˜åŒ–çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¥åº·å›¾åƒç¨€ç¼ºçš„å°‘é‡åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŒæ›²ç©ºé—´ä½œä¸ºåŒ»å­¦å¼‚å¸¸æ£€æµ‹çš„æœ‰åŠ›æ›¿ä»£æ–¹æ³•çš„æ½œåŠ›ã€‚é¡¹ç›®ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://hyperbolic-anomalies.github.io/">https://hyperbolic-anomalies.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21228v1">PDF</a> Provisionally Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´æå–é¢„è®­ç»ƒç½‘ç»œçš„ä¸åŒå±‚ç‰¹å¾ï¼Œä½†æ— æ³•æœ‰æ•ˆæ•æ‰è¿™äº›ç‰¹å¾ä¸­çš„å±‚æ¬¡å…³ç³»ï¼Œå¯¼è‡´å¼‚å¸¸æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•æ–¹æ³•ï¼Œå°†ç‰¹å¾è¡¨ç¤ºæŠ•å½±åˆ°åŒæ›²ç©ºé—´ï¼ŒåŸºäºç½®ä¿¡åº¦è¿›è¡Œèšåˆï¼Œå¹¶åˆ†ç±»ä¸ºæ­£å¸¸æˆ–å¼‚å¸¸æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒåŒæ›²ç©ºé—´åœ¨å¤šä¸ªåŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºåŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„æ¡†æ¶ï¼Œå¹¶åœ¨å›¾åƒå’Œåƒç´ çº§åˆ«å‡è·å¾—æ›´é«˜çš„AUROCåˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ˜¾ç¤ºåŒæ›²ç©ºé—´å¯¹å‚æ•°å˜åŒ–å…·æœ‰éŸ§æ€§ï¼Œå¹¶åœ¨å¥åº·å›¾åƒç¨€ç¼ºçš„å°‘æ•°åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™ä¸ºåŒæ›²ç©ºé—´åœ¨åŒ»ç–—å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›æä¾›äº†æœ‰åŠ›è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»ç–—å¼‚å¸¸æ£€æµ‹é¢ä¸´æ•°æ®å¯ç”¨æ€§å’Œæ ‡ç­¾çº¦æŸçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´æå–ç‰¹å¾ï¼Œä½†æ— æ³•æœ‰æ•ˆæ•æ‰å±‚æ¬¡å…³ç³»ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†ç‰¹å¾è¡¨ç¤ºæŠ•å½±åˆ°åŒæ›²ç©ºé—´è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åŒæ›²ç©ºé—´åœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæ¬§å‡ é‡Œå¾—ç©ºé—´æ–¹æ³•ã€‚</li>
<li>åŒæ›²ç©ºé—´æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„AUROCåˆ†æ•°ï¼Œé€‚ç”¨äºå›¾åƒå’Œåƒç´ çº§åˆ«æ£€æµ‹ã€‚</li>
<li>åŒæ›²ç©ºé—´å¯¹å‚æ•°å˜åŒ–å…·æœ‰éŸ§æ€§ï¼Œå¹¶åœ¨å°‘æ•°åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5c945587768a7b1720127d17b51ac91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4f1f2a39b936bd9057532639beaa9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7901f193ab10a7610071fd82f595c16d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00647e82ee299db2530585dddef4c51b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Predict-Audio-Effects-Parameters-from-Natural-Language"><a href="#Can-Large-Language-Models-Predict-Audio-Effects-Parameters-from-Natural-Language" class="headerlink" title="Can Large Language Models Predict Audio Effects Parameters from Natural   Language?"></a>Can Large Language Models Predict Audio Effects Parameters from Natural   Language?</h2><p><strong>Authors:Seungheon Doh, Junghyun Koo, Marco A. MartÃ­nez-RamÃ­rez, Wei-Hsiang Liao, Juhan Nam, Yuki Mitsufuji</strong></p>
<p>In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation. We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship between timbre semantics and audio effects in music production. To enhance performance, we introduce three types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous optimization approaches, offering competitive performance in translating natural language descriptions to appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving the way for more intuitive and accessible music production tools. </p>
<blockquote>
<p>åœ¨éŸ³ä¹åˆ¶ä½œä¸­ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æ“ä½œéŸ³é¢‘æ•ˆæœï¼ˆFxï¼‰å‚æ•°å…·æœ‰é™ä½éä¸“å®¶æŠ€æœ¯å£å’çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†LLM2Fxæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›´æ¥ä»æ–‡æœ¬æè¿°é¢„æµ‹Fxå‚æ•°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ˜ å°„åˆ°ç›¸åº”çš„Fxå‚æ•°æ¥è§£å†³æ–‡æœ¬åˆ°æ•ˆæœå‚æ•°é¢„æµ‹ï¼ˆText2Fxï¼‰ä»»åŠ¡ï¼Œè¿™äº›å‚æ•°ç”¨äºå‡è¡¡å’Œæ··å“ã€‚æˆ‘ä»¬è¯æ˜äº†LLMèƒ½å¤Ÿä»¥é›¶æ ·æœ¬çš„æ–¹å¼ç”ŸæˆFxå‚æ•°ï¼Œè¿™é˜æ˜äº†éŸ³ä¹åˆ¶ä½œä¸­éŸ³è‰²è¯­ä¹‰å’ŒéŸ³é¢‘æ•ˆæœä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§ç±»å‹çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼šéŸ³é¢‘æ•°å­—ä¿¡å·å¤„ç†ï¼ˆDSPï¼‰ç‰¹å¾ã€DSPå‡½æ•°ä»£ç å’Œå°‘é‡ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„Fxå‚æ•°ç”Ÿæˆä¼˜äºä»¥å‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨å°†è‡ªç„¶è¯­è¨€æè¿°ç¿»è¯‘æˆé€‚å½“çš„Fxè®¾ç½®æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒLLMå¯ä»¥ä½œä¸ºéŸ³é¢‘åˆ¶ä½œçš„æ–‡æœ¬é©±åŠ¨ç•Œé¢ï¼Œä¸ºæ›´ç›´è§‚å’Œå¯è®¿é—®çš„éŸ³ä¹åˆ¶ä½œå·¥å…·é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20770v1">PDF</a> Submitted to WASPAA 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€æè¿°é¢„æµ‹éŸ³é¢‘æ•ˆæœå‚æ•°å…·æœ‰é™ä½éŸ³ä¹åˆ¶ä½œä¸­éä¸“å®¶æŠ€æœ¯é—¨æ§›çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†LLM2Fxæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›´æ¥æ ¹æ®æ–‡æœ¬æè¿°é¢„æµ‹éŸ³é¢‘æ•ˆæœå‚æ•°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå’Œå¾®è°ƒã€‚æœ¬ç ”ç©¶é€šè¿‡æ˜ å°„è‡ªç„¶è¯­è¨€æè¿°åˆ°å‡è¡¡å’Œæ··å“çš„ç›¸åº”æ•ˆæœå‚æ•°ï¼Œè§£å†³äº†æ–‡æœ¬åˆ°æ•ˆæœå‚æ•°é¢„æµ‹ï¼ˆText2Fxï¼‰çš„ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥åœ¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹ç”Ÿæˆæ•ˆæœå‚æ•°ï¼Œæ­ç¤ºäº†éŸ³ä¹åˆ¶ä½œä¸­éŸ³è‰²è¯­ä¹‰å’ŒéŸ³é¢‘æ•ˆæœä¹‹é—´çš„å…³ç³»ã€‚å¼•å…¥ä¸‰ç§ç±»å‹çš„ä¸Šä¸‹æ–‡å®ä¾‹â€”â€”éŸ³é¢‘æ•°å­—ä¿¡å·å¤„ç†ï¼ˆDSPï¼‰ç‰¹å¾ã€DSPå‡½æ•°ä»£ç å’Œå°‘é‡å®ä¾‹â€”â€”ä»¥å¢å¼ºæ€§èƒ½ã€‚ä¸å…ˆå‰çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºLLMçš„æ•ˆæœå‚æ•°ç”Ÿæˆè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºé€‚å½“çš„éŸ³æ•ˆè®¾ç½®æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒLLMå¯ä½œä¸ºéŸ³é¢‘åˆ¶ä½œçš„æ–‡æœ¬é©±åŠ¨ç•Œé¢ï¼Œä¸ºéŸ³ä¹åˆ¶ä½œå·¥å…·æä¾›æ›´ç›´è§‚å’Œå¯è®¿é—®çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM2Fxæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„æµ‹éŸ³é¢‘æ•ˆæœï¼ˆFxï¼‰å‚æ•°ï¼Œé™ä½éä¸“å®¶åœ¨éŸ³ä¹åˆ¶ä½œä¸­çš„æŠ€æœ¯å£å’ã€‚</li>
<li>æ–‡æœ¬ç›´æ¥é¢„æµ‹Fxå‚æ•°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå’Œå¾®è°ƒã€‚</li>
<li>é€šè¿‡æ˜ å°„è‡ªç„¶è¯­è¨€å’Œå‡è¡¡ã€æ··å“æ•ˆæœå‚æ•°çš„å¯¹åº”å…³ç³»æ¥è§£å†³Text2Fxä»»åŠ¡ã€‚</li>
<li>LLMèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ç”ŸæˆFxå‚æ•°ï¼Œæ­ç¤ºéŸ³è‰²è¯­ä¹‰ä¸éŸ³é¢‘æ•ˆæœçš„å…³ç³»ã€‚</li>
<li>å¼•å…¥DSPç‰¹å¾ã€DSPå‡½æ•°ä»£ç å’Œå°‘é‡å®ä¾‹ä¸‰ç§ç±»å‹çš„ä¸Šä¸‹æ–‡å®ä¾‹ä»¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>åŸºäºLLMçš„Fxå‚æ•°ç”Ÿæˆæ–¹æ³•ä¼˜äºå…ˆå‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºé€‚å½“çš„éŸ³æ•ˆè®¾ç½®æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f473304c60702e0c11a8d7bbcdf5cea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fef49d857bc763ddbab7d6478e9db5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9cc9b74c613f98a187d094ef381be2a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/">https://github.com/roboflow/rf100-vl/</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡åœ¨äº’è”ç½‘è§„æ¨¡çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¸¸è§å¯¹è±¡ï¼ˆå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººï¼‰ä¸Šå®ç°äº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»ç„¶éš¾ä»¥æ¨å¹¿åˆ°å…¶é¢„è®­ç»ƒä¸­æ²¡æœ‰å‡ºç°çš„ç±»åˆ«ã€ä»»åŠ¡ä»¥åŠæˆåƒæ¨¡å¼ã€‚æˆ‘ä»¬ä¸»å¼ ä¸åº”ä»…ä»…é€šè¿‡æ›´å¤šçš„è§†è§‰æ•°æ®é‡æ–°è®­ç»ƒVLMsï¼Œè€Œåº”è¯¥é€šè¿‡åŒ…å«å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œæ–‡æœ¬æè¿°çš„æ³¨é‡ŠæŒ‡ä»¤æ¥å¯¹é½VLMsä»¥ç†è§£æ–°æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŒ…å«ä¸€ç™¾ä¸ªå¤šæ¨¡æ€å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«çš„æ¦‚å¿µåœ¨VLMé¢„è®­ç»ƒä¸­å¹¶ä¸å¸¸è§ã€‚æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€åŠç›‘ç£å’Œå®Œå…¨ç›‘ç£ç¯å¢ƒä¸‹çš„è¯„ä¼°ï¼Œå…è®¸è·¨æ•°æ®ä½“åˆ¶è¿›è¡Œæ¯”è¾ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒGroundingDINOå’ŒQwen2.5-VLè¿™æ ·çš„VLMåœ¨Roboflow100-VLä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œè¿™è¯æ˜äº†è¿›è¡Œå°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl/%E5%92%8Chttps://universe.roboflow.com/rf100-vl/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/roboflow/rf100-vl/å’Œhttps://universe.roboflow.com/rf100-vl/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v1">PDF</a> The first two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒäºäº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½å‡ºè‰²ï¼Œå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ³›åŒ–åˆ°éå¸¸è§ç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€å¥—åŒ…å«100ä¸ªå¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«å„ç§ä¸å¸¸è§äºVLMé¢„è®­ç»ƒçš„æ¦‚å¿µã€‚æˆ‘ä»¬è¯„ä¼°äº†åœ¨ä¸åŒè®¾ç½®ä¸‹çš„å…ˆè¿›æ¨¡å‹ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°æ ·æœ¬æ¬¡ã€åŠç›‘ç£å’Œå…¨ç›‘ç£è®¾ç½®ï¼Œä»¥ä¾¿åœ¨ä¸åŒæ•°æ®ä½“ç³»ä¸­è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨Roboflow100-VLä¸­çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šï¼ŒVLMsçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œè¿™å‡¸æ˜¾äº†å°‘é‡æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>æœ€å…ˆè¿›çš„VLMsåœ¨æ³›åŒ–åˆ°éå¸¸è§ç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>Roboflow100-VLæ˜¯ä¸€ä¸ªåŒ…å«å¤šæ ·æ¦‚å¿µçš„å¤§è§„æ¨¡å¤šæ¨¡æ€å¯¹è±¡æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>åœ¨Roboflow100-VLçš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šï¼ŒVLMsçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>éœ€è¦é€šè¿‡å°‘é‡æ ·æœ¬è¿›è¡Œæ¦‚å¿µå¯¹é½æ¥æå‡VLMsçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æä¾›äº†åœ¨ä¸åŒè®¾ç½®ä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35d539151c8afd89e447f714e321beb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8394e86845eed7bebbdef61422acc1ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2501852af6f26aee3424672c34f1df2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b0dc21e3f7427ccfe67cbcfda2a1ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2b3d326cac1f3f3218c2a08b0071c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06acb2189e042fb7632afe402431899a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbf81dcf18b5d3d0835653a620c72c7f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Time-Series-Generation-Under-Data-Scarcity-A-Unified-Generative-Modeling-Approach"><a href="#Time-Series-Generation-Under-Data-Scarcity-A-Unified-Generative-Modeling-Approach" class="headerlink" title="Time Series Generation Under Data Scarcity: A Unified Generative   Modeling Approach"></a>Time Series Generation Under Data Scarcity: A Unified Generative   Modeling Approach</h2><p><strong>Authors:Tal Gonen, Itai Pemper, Ilan Naiman, Nimrod Berman, Omri Azencot</strong></p>
<p>Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at <a target="_blank" rel="noopener" href="https://github.com/azencot-group/ImagenFew">https://github.com/azencot-group/ImagenFew</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—ç”Ÿæˆå»ºæ¨¡æ˜¯æ—¶é—´åºåˆ—åˆ†æä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹ã€‚å°½ç®¡æœ€è¿‘ç”Ÿæˆæ¨¡å‹æœ‰æ‰€è¿›å±•ï¼Œä½†æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹çš„è¡¨ç°çš„ç»¼åˆç†è§£ä»ç„¶ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡è¿›è¡Œå¤§è§„æ¨¡ç ”ç©¶ï¼Œè¯„ä¼°æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­é¢†å…ˆçš„ç”Ÿæˆæ¨¡å‹ï¼Œæ­ç¤ºäº†å…¨æ•°æ®å’Œæ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨å°‘æ•°å‡ ä¸ªç¤ºä¾‹å°±èƒ½åˆæˆè·¨ä¸åŒé¢†åŸŸçš„é«˜ä¿çœŸæ—¶é—´åºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§é‡ã€å¼‚è´¨çš„æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¯æ¨å¹¿çš„æ—¶é—´è¡¨ç¤ºã€‚å®ƒè¿›ä¸€æ­¥èå…¥äº†æ¶æ„åˆ›æ–°ï¼Œå¦‚åŠ¨æ€å·ç§¯å±‚ï¼Œä»¥å®ç°çµæ´»çš„é€šé“é€‚é…å’Œæ•°æ®é›†ä»¤ç‰Œæ¡ä»¶ï¼Œä»¥è¿›è¡Œé¢†åŸŸæ„ŸçŸ¥ç”Ÿæˆã€‚æ— éœ€å¤§é‡ç›‘ç£ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°äº†æœ€æ–°æ€§èƒ½â€”â€”åœ¨å¹¿æ³›çš„å„ç§å­é›†å¤§å°ä¸Šè¶…è¶Šäº†ç‰¹å®šé¢†åŸŸçš„åŸºçº¿ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå³ä½¿åœ¨å®Œæ•´æ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸Šï¼Œå®ƒä¹Ÿè¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ï¼Œè¿™çªæ˜¾äº†é¢„è®­ç»ƒå’Œè·¨åŸŸæ¨å¹¿çš„åŠ›é‡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½é¼“åŠ±ç¤¾åŒºé‡æ–°å…³æ³¨æ—¶é—´åºåˆ—ç ”ç©¶ä¸­çš„å°æ ·ç”Ÿæˆå»ºæ¨¡ä½œä¸ºå…³é”®é—®é¢˜ï¼Œå¹¶è¿½æ±‚èƒ½é«˜æ•ˆè·¨åŸŸæ‰©å±•çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/azencot-group/ImagenFew">https://github.com/azencot-group/ImagenFew</a>å¤„è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20446v1">PDF</a> The first two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚æ–‡ç« é€šè¿‡å¤§è§„æ¨¡å®éªŒè¯„ä¼°äº†ä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œå‘ç°äº†å…¨æ•°æ®ä¸æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„æ€§èƒ½å·®è·ã€‚ä¸ºç¼©å°è¿™ä¸€å·®è·ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹åˆæˆé«˜è´¨é‡çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ é€šç”¨æ—¶é—´åºåˆ—è¡¨ç¤ºï¼Œå¹¶å¼•å…¥åŠ¨æ€å·ç§¯å±‚å’Œæ•°æ®é›†æ ‡è®°æ¡ä»¶ç­‰åˆ›æ–°æ¶æ„ï¼Œä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„æ•°æ®ç”Ÿæˆã€‚åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³åœ¨å…¨æ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†æ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œçªæ˜¾äº†é¢„è®­ç»ƒå’Œè·¨åŸŸæ³›åŒ–çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜å¯¹æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹çš„æ€§èƒ½è¿›è¡Œäº†å¤§è§„æ¨¡ç ”ç©¶ã€‚</li>
<li>å‘ç°äº†åœ¨å…¨æ•°æ®ä¸æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„ç”Ÿæˆæ¨¡å‹æ€§èƒ½å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ï¼Œå¯åœ¨ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹åˆæˆé«˜è´¨é‡çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ é€šç”¨æ—¶é—´åºåˆ—è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹å¼•å…¥åŠ¨æ€å·ç§¯å±‚å’Œæ•°æ®é›†æ ‡è®°æ¡ä»¶ç­‰åˆ›æ–°æ¶æ„ä»¥é€‚åº”ä¸åŒé¢†åŸŸçš„æ•°æ®ç”Ÿæˆã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½è¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
<li>é¢„è®­ç»ƒå’Œè·¨åŸŸæ³›åŒ–çš„é‡è¦æ€§åœ¨ç ”ç©¶ä¸­å¾—åˆ°äº†çªæ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b47f2268e5d12a43752eeecf04e8f00e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12b7a8b4ab01abe592de1c89ea22890c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46886ddb544c5556421320d207d310a2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling"><a href="#Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling" class="headerlink" title="Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling"></a>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling</h2><p><strong>Authors:Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi</strong></p>
<p>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual&#x2F;textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘å·²è¢«çº³å…¥å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å°‘æ•°é•œå¤´ã€å¼±ç›‘ç£åˆ†ç±»å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„æŒ‘æˆ˜ã€‚ä¸€ç§å…³é”®è¶‹åŠ¿æ˜¯åˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æ›´å¥½åœ°è¡¨ç¤ºå±‚æ¬¡åŒ–çš„ç»„ç»‡ç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šï¼ˆ1ï¼‰åŒä¸€æ¨¡æ€å†…ä¸åŒå°ºåº¦ï¼ˆä¾‹å¦‚ï¼Œ5å€å’Œ20å€ï¼‰ä¹‹é—´äº¤äº’çš„å»ºæ¨¡ä¸è¶³ï¼›ï¼ˆ2ï¼‰åŒä¸€å°ºåº¦ä¸Šè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å¯¹é½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†HiVE-MILï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€å›¾ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç²—ï¼ˆ5å€ï¼‰å’Œç»†ï¼ˆ20å€ï¼‰è§†è§‰&#x2F;æ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„çˆ¶å­é“¾æ¥ï¼Œä»¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼Œä»¥åŠï¼ˆ2ï¼‰åŒä¸€å°ºåº¦ä¸Šçš„è§†è§‰å’Œæ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„å¼‚è´¨å†…å°ºåº¦è¾¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒHiVE-MILé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æ¶ˆé™¤å¼±ç›¸å…³çš„è¡¥ä¸-æ–‡æœ¬å¯¹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å±‚æ¬¡å¯¹æ¯”æŸå¤±ï¼Œä»¥å¯¹é½å„å°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚åœ¨TCGAä¹³è…ºç™Œã€è‚ºç™Œå’Œè‚¾ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHiVE-MILå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„MILå’Œæœ€æ–°çš„åŸºäºVLMçš„MILæ–¹æ³•ï¼Œåœ¨16é•œå¤´è®¾ç½®ä¸‹å®è§‚F1æé«˜äº†é«˜è¾¾4.1%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºä»æœ‰é™çš„ç—…ç†æ•°æ®ä¸­å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•å­¦ä¹ çš„ä»·å€¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bryanwong17/HiVE-MILæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17982v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èå…¥å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å°‘é‡å¼±ç›‘ç£åˆ†ç±»é—®é¢˜åœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸Šçš„åº”ç”¨ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œå¦‚è·¨å°ºåº¦å†…æ¨¡æ€äº¤äº’ä¸è¶³å’Œè§†è§‰ä¸æ–‡æœ¬æ¨¡æ€é—´çš„ä¸å¯¹é½é—®é¢˜ï¼Œæå‡ºäº†HiVE-MILæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç»Ÿä¸€å›¾æ¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µæ–‡æœ¬å¼•å¯¼çš„åŠ¨æ€è¿‡æ»¤æœºåˆ¶å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiVE-MILåœ¨å®è§‚F1å¾—åˆ†ä¸Šä¼˜äºä¼ ç»ŸMILå’ŒåŸºäºVLMçš„MILæ–¹æ³•ï¼Œæœ€é«˜æå‡å¯è¾¾4.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsä¸MILç»“åˆåº”ç”¨äºè§£å†³å°‘é‡å¼±ç›‘ç£åˆ†ç±»çš„WSIé—®é¢˜ã€‚</li>
<li>HiVE-MILæ¡†æ¶å¼•å…¥å¤šå°ºåº¦ä¿¡æ¯ï¼Œæ„å»ºç»Ÿä¸€å›¾æ¥æ•æ‰å±‚æ¬¡å…³ç³»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§æ˜¯è·¨å°ºåº¦å†…æ¨¡æ€äº¤äº’ä¸è¶³å’Œè§†è§‰ä¸æ–‡æœ¬æ¨¡æ€é—´çš„ä¸å¯¹é½ã€‚</li>
<li>HiVE-MILé‡‡ç”¨ä¸¤é˜¶æ®µæ–‡æœ¬å¼•å¯¼çš„åŠ¨æ€è¿‡æ»¤æœºåˆ¶å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºHiVE-MILåœ¨å¤šç§ç™Œç—‡æ•°æ®é›†ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ä»£ç å…¬å¼€å¯ä¾›ä¸‹è½½å‚è€ƒå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2ce53dbb405815ffa10afd6fc6670347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97dcfab98be7f160215b7823cf69fa16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9450b1afc48fc6fa3f8e9bbffc4e4c52.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learning-Policy-Committees-for-Effective-Personalization-in-MDPs-with-Diverse-Tasks"><a href="#Learning-Policy-Committees-for-Effective-Personalization-in-MDPs-with-Diverse-Tasks" class="headerlink" title="Learning Policy Committees for Effective Personalization in MDPs with   Diverse Tasks"></a>Learning Policy Committees for Effective Personalization in MDPs with   Diverse Tasks</h2><p><strong>Authors:Luise Ge, Michael Lanier, Anindya Sarkar, Bengisu Guresti, Chongjie Zhang, Yevgeniy Vorobeychik</strong></p>
<p>Many dynamic decision problems, such as robotic control, involve a series of tasks, many of which are unknown at training time. Typical approaches for these problems, such as multi-task and meta reinforcement learning, do not generalize well when the tasks are diverse. On the other hand, approaches that aim to tackle task diversity, such as using task embedding as policy context and task clustering, typically lack performance guarantees and require a large number of training tasks. To address these challenges, we propose a novel approach for learning a policy committee that includes at least one near-optimal policy with high probability for tasks encountered during execution. While we show that this problem is in general inapproximable, we present two practical algorithmic solutions. The first yields provable approximation and task sample complexity guarantees when tasks are low-dimensional (the best we can do due to inapproximability), whereas the second is a general and practical gradient-based approach. In addition, we provide a provable sample complexity bound for few-shot learning. Our experiments on MuJoCo and Meta-World show that the proposed approach outperforms state-of-the-art multi-task, meta-, and task clustering baselines in training, generalization, and few-shot learning, often by a large margin. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/CERL-WUSTL/PACMAN">https://github.com/CERL-WUSTL/PACMAN</a>. </p>
<blockquote>
<p>åœ¨è®¸å¤šåŠ¨æ€å†³ç­–é—®é¢˜ï¼Œå¦‚æœºå™¨äººæ§åˆ¶ä¸­ï¼Œæ¶‰åŠä¸€ç³»åˆ—ä»»åŠ¡ï¼Œå…¶ä¸­è®¸å¤šä»»åŠ¡åœ¨è®­ç»ƒæ—¶æœªçŸ¥ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜çš„å…¸å‹æ–¹æ³•ï¼Œå¦‚å¤šä»»åŠ¡å­¦ä¹ å’Œå…ƒå¼ºåŒ–å­¦ä¹ ï¼Œå½“ä»»åŠ¡å¤šæ ·æ—¶å¹¶ä¸é€šç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œæ—¨åœ¨è§£å†³ä»»åŠ¡å¤šæ ·æ€§çš„æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨ä»»åŠ¡åµŒå…¥ä½œä¸ºç­–ç•¥ä¸Šä¸‹æ–‡å’Œä»»åŠ¡èšç±»ï¼Œé€šå¸¸ç¼ºä¹æ€§èƒ½ä¿è¯ï¼Œéœ€è¦å¤§é‡è®­ç»ƒä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å­¦ä¹ ç­–ç•¥å§”å‘˜ä¼šçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¤§æ¦‚ç‡åœ°åŒ…æ‹¬åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°ä»»åŠ¡çš„è‡³å°‘ä¸€ä¸ªè¿‘ä¼˜ç­–ç•¥ã€‚è™½ç„¶æˆ‘ä»¬å‘ç°è¿™ä¸ªé—®é¢˜é€šå¸¸ä¸å¯è¿‘ä¼¼è§£å†³ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å®ç”¨çš„ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚ç¬¬ä¸€ç§æ–¹æ³•åœ¨ä»»åŠ¡ä¸ºä½ç»´æ—¶èƒ½äº§ç”Ÿå¯è¯æ˜è¿‘ä¼¼å’Œä»»åŠ¡æ ·æœ¬å¤æ‚æ€§ä¿è¯ï¼ˆè¿™æ˜¯æˆ‘ä»¬èƒ½åšåˆ°çš„æœ€å¥½å› ä¸ºä¸å¯è¿‘ä¼¼æ€§ï¼‰ï¼Œè€Œç¬¬äºŒç§æ˜¯ä¸€ç§é€šç”¨ä¸”å®ç”¨çš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºå°æ ·æœ¬å­¦ä¹ æä¾›äº†å¯è¯æ˜æ ·æœ¬å¤æ‚æ€§ç•Œé™ã€‚æˆ‘ä»¬åœ¨MuJoCoå’ŒMeta-Worldä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è®­ç»ƒã€æ³›åŒ–å’Œå°æ ·å­¦ä¹ æ–¹é¢å‡ä¼˜äºæœ€æ–°çš„å¤šä»»åŠ¡ã€å…ƒå­¦ä¹ å’Œä»»åŠ¡èšç±»åŸºçº¿ï¼Œé€šå¸¸å…·æœ‰è¾ƒå¤§ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CERL-WUSTL/PACMAN%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CERL-WUSTL/PACMANä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01885v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³åŠ¨æ€å†³ç­–é—®é¢˜çš„æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºä»»åŠ¡å¤šæ ·æ€§å’ŒæœªçŸ¥ä»»åŠ¡çš„æƒ…å†µã€‚é€šè¿‡æ„å»ºæ”¿ç­–å§”å‘˜ä¼šï¼Œè¯¥æ–¹æ³•èƒ½ç¡®ä¿åœ¨æ‰§è¡Œæ—¶é‡åˆ°çš„ä»»åŠ¡ä¸­æœ‰è‡³å°‘ä¸€ä¸ªè¿‘ä¼˜æ”¿ç­–çš„æ¦‚ç‡è¾ƒé«˜ã€‚è™½ç„¶è¯¥é—®é¢˜ä¸€èˆ¬ä¸å¯è¿‘ä¼¼ï¼Œä½†æä¾›äº†ä¸¤ç§å®ç”¨çš„ç®—æ³•è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨MuJoCoå’ŒMeta-Worldçš„å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³åŠ¨æ€å†³ç­–é—®é¢˜çš„æ–¹æ³•ï¼Œé€‚ç”¨äºä»»åŠ¡å¤šæ ·æ€§å’ŒæœªçŸ¥ä»»åŠ¡çš„ç¯å¢ƒã€‚</li>
<li>é€šè¿‡æ„å»ºæ”¿ç­–å§”å‘˜ä¼šï¼Œç¡®ä¿åœ¨æ‰§è¡Œæ—¶é‡åˆ°çš„ä»»ä½•ä»»åŠ¡éƒ½æœ‰è‡³å°‘ä¸€ä¸ªè¿‘ä¼˜æ”¿ç­–ã€‚</li>
<li>è™½ç„¶è¯¥é—®é¢˜ä¸€èˆ¬ä¸å¯è¿‘ä¼¼ï¼Œä½†æä¾›äº†ä¸¤ç§å®ç”¨çš„ç®—æ³•è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­ä¸€ç§åœ¨ä»»åŠ¡ä½ç»´åº¦æƒ…å†µä¸‹å¯æä¾›å¯è¯æ˜çš„è¿‘ä¼¼å’Œä»»åŠ¡æ ·æœ¬å¤æ‚æ€§ä¿è¯ã€‚</li>
<li>å¦ä¸€ç§ç®—æ³•æ˜¯ä¸€ç§é€šç”¨ä¸”å®ç”¨çš„åŸºäºæ¢¯åº¦çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¸ºå°‘æ ·æœ¬å­¦ä¹ æä¾›äº†å¯è¯æ˜çš„æ ·æœ¬å¤æ‚æ€§ç•Œé™ã€‚</li>
<li>åœ¨MuJoCoå’ŒMeta-Worldçš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºå¤šä»»åŠ¡ã€å…ƒå­¦ä¹ å’Œä»»åŠ¡èšç±»ç­‰ç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è®­ç»ƒã€æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a6b772381f0a750ee5b89f1b24932ed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-676919b1f8c980b3dd2b7d823b878945.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation"><a href="#Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation" class="headerlink" title="Scalable Model Merging with Progressive Layer-wise Distillation"></a>Scalable Model Merging with Progressive Layer-wise Distillation</h2><p><strong>Authors:Jing Xu, Jiazheng Li, Jingzhao Zhang</strong></p>
<p>Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performance. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods, ProDistill achieves state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill. </p>
<blockquote>
<p>æ¨¡å‹èåˆæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼ï¼Œå¯ä»¥æ•´åˆå¤šä¸ªå¾®è°ƒæ¨¡å‹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œèåˆæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æˆ–åªæœ‰å°‘é‡æ•°æ®å¯ç”¨çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡è¯æ˜æ•°æ®æ— å…³ç®—æ³•å¯èƒ½å…·æœ‰ä»»æ„å·®çš„æœ€åæƒ…å†µæ€§èƒ½æ¥å¼ºè°ƒç‰¹å®šé¢†åŸŸæ•°æ®å¯¹æ¨¡å‹èåˆçš„å¿…è¦æ€§ã€‚åŸºäºè¿™ä¸€ç†è®ºè§è§£ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨¡å‹èåˆä¸è’¸é¦ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å°‘é‡æ•°æ®èåˆç®—æ³•ProDistillï¼ˆæ¸è¿›é€å±‚è’¸é¦ï¼‰ã€‚ä¸æ™®éçš„è§‚ç‚¹ç›¸åï¼Œæˆ‘ä»¬è®¤ä¸ºé€å±‚è®­ç»ƒå¹¶ä¸ä¼šæŸå®³æ€§èƒ½ï¼Œåè€Œå±•ç¤ºé€å±‚æ•™å¸ˆå­¦ç”Ÿè’¸é¦ä¸ä»…æé«˜äº†å¯æ‰©å±•æ€§ï¼Œè¿˜æé«˜äº†æ¨¡å‹èåˆæ€§èƒ½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å°‘é‡æ•°æ®èåˆæ–¹æ³•ç›¸æ¯”ï¼ŒProDistillå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†6.14%å’Œ6.61%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å®éªŒæ‰©å±•åˆ°äº†è¶…è¿‡10Bå‚æ•°çš„æ¨¡å‹ä¸Šï¼Œå±•ç¤ºäº†ProDistillçš„å‡ºè‰²å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12706v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹èåˆæ˜¯é›†æˆå¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†èåˆæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ•°æ®æˆ–å°‘æ•°æ®æƒ…å†µä¸‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–å…ˆå¼ºè°ƒé¢†åŸŸç‰¹å®šæ•°æ®å¯¹æ¨¡å‹èåˆçš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜æ•°æ®æ— å…³ç®—æ³•çš„æœ€åæƒ…å†µæ€§èƒ½å¯èƒ½ä»»æ„å·®ã€‚åŸºäºæ­¤ç†è®ºè§è§£ï¼Œæœ¬æ–‡æ¢ç´¢æ¨¡å‹èåˆä¸è’¸é¦çš„å…³ç³»ï¼Œæå‡ºä¸€ç§æ–°å‹çš„å°æ ·æœ¬èåˆç®—æ³•ProDistillï¼ˆæ¸è¿›é€å±‚è’¸é¦ï¼‰ã€‚ä¸åŒäºæ™®éè®¤çŸ¥ï¼Œé€å±‚è®­ç»ƒå¹¶ä¸ä¼šæŸå®³æ€§èƒ½ï¼Œåè€Œæ˜¾ç¤ºé€å±‚æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ä¸ä»…æé«˜å¯ä¼¸ç¼©æ€§ï¼Œè¿˜èƒ½æ”¹å–„æ¨¡å‹èåˆæ€§èƒ½ã€‚åœ¨å®éªŒä¸­è¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰çš„å°æ ·æœ¬èåˆæ–¹æ³•ï¼ŒProDistillè¡¨ç°è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ€§èƒ½åˆ†åˆ«æé«˜é«˜è¾¾6.14%å’Œ6.61%ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åœ¨è¶…è¿‡10Bå‚æ•°çš„æ¨¡å‹ä¸Šè¡¨ç°å“è¶Šçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹èåˆæ˜¯é›†æˆå¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å­˜åœ¨æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</li>
<li>é¢†åŸŸç‰¹å®šæ•°æ®å¯¹æ¨¡å‹èåˆè‡³å…³é‡è¦ï¼Œæ•°æ®æ— å…³ç®—æ³•çš„æœ€åæƒ…å†µæ€§èƒ½å¯èƒ½å¾ˆå·®ã€‚</li>
<li>æ¸è¿›é€å±‚è’¸é¦ï¼ˆProDistillï¼‰æ˜¯ä¸€ç§æ–°å‹çš„å°æ ·æœ¬èåˆç®—æ³•ï¼Œé€šè¿‡æ¢ç´¢æ¨¡å‹èåˆä¸è’¸é¦çš„å…³ç³»æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>é€å±‚æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ä¸ä»…ä¸æŸå®³æ€§èƒ½ï¼Œåè€Œèƒ½æ”¹å–„æ¨¡å‹èåˆæ€§èƒ½ï¼Œå¹¶æé«˜ç®—æ³•çš„å¯ä¼¸ç¼©æ€§ã€‚</li>
<li>ProDistillåœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>ProDistillåœ¨å¤§å‹æ¨¡å‹ï¼ˆè¶…è¿‡10Bå‚æ•°ï¼‰ä¸Šå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d353f9d290297180a1b37104454d77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269ae7119204a2f43b23894f0fbe6289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b434c37df4bd2082478cc8617c7a658a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives"><a href="#More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives" class="headerlink" title="More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives"></a>More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives</h2><p><strong>Authors:Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Jian Luan, Shuo Shang, Xiuying Chen, Rui Yan</strong></p>
<p>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as ICL demonstrations increase from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce \textit{DrICL}, a novel optimization method that enhances model performance through \textit{Differentiated} and \textit{Reweighting} objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the \textit{Many-Shot ICL Benchmark} (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes. Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and dataset hoping to facilitate further research in many-shot ICL\footnote{<a target="_blank" rel="noopener" href="https://github.com/xiaoqzhwhu/DrICL%7D">https://github.com/xiaoqzhwhu/DrICL}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸éœ€æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ“…é•¿äºå°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚ç„¶è€Œï¼Œéšç€ICLæ¼”ç¤ºä»å°‘æ•°å¢åŠ åˆ°å¤šæ•°ï¼Œæ€§èƒ½å¾€å¾€è¾¾åˆ°å¹³å°æœŸå¹¶æœ€ç»ˆä¸‹é™ã€‚æˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´è¿™ä¸€è¶‹åŠ¿çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼šæ¬¡ä¼˜çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ä¼˜åŒ–ç›®æ ‡å’Œå¢é‡æ•°æ®å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<em>DrICL</em>ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡<em>å·®å¼‚åŒ–</em>å’Œ<em>é‡æƒ</em>ç›®æ ‡å¢å¼ºæ¨¡å‹æ€§èƒ½çš„æ–°å‹ä¼˜åŒ–æ–¹æ³•ã€‚å…¨å±€ä¸Šï¼ŒDrICLåˆ©ç”¨å·®å¼‚åŒ–å­¦ä¹ æ¥ä¼˜åŒ–NLLç›®æ ‡ï¼Œç¡®ä¿å¤šæ­¥æ€§èƒ½è¶…è¶Šé›¶æ­¥æ°´å¹³ã€‚å±€éƒ¨ä¸Šï¼Œå®ƒé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å€Ÿé‰´ç´¯ç§¯ä¼˜åŠ¿æ¥åŠ¨æ€è°ƒæ•´å¤šæ­¥æ¼”ç¤ºçš„æƒé‡ï¼Œä»è€Œå‡è½»å™ªå£°æ•°æ®çš„å½±å“ã€‚ç”±äºæ²¡æœ‰åŒ…å«å¤šç§å¤šæ­¥åˆ†å¸ƒçš„å¤šä»»åŠ¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†<em>å¤šæ­¥ICLåŸºå‡†</em>ï¼ˆICL-50ï¼‰â€”â€”ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«50ä¸ªä»»åŠ¡ï¼Œæ¶µç›–ä»1åˆ°350æ­¥çš„å°„å‡»æ¬¡æ•°ï¼Œåºåˆ—ä¸­çš„ä»¤ç‰Œæ•°é«˜è¾¾8000ä¸ªï¼Œæ—¢å¯ç”¨äºå¾®è°ƒä¹Ÿå¯ç”¨äºè¯„ä¼°ç›®çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨DrICLå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡çš„å¤šæ­¥è®¾ç½®ä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç å’Œæ•°æ®é›†ï¼Œå¸Œæœ›èƒ½è¿›ä¸€æ­¥æ¨åŠ¨å¤šæ­¥ICLçš„ç ”ç©¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/xiaoqzhwhu/DrICL%EF%BC%89%E3%80%82">https://github.com/xiaoqzhwhu/DrICLï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04070v3">PDF</a> 14 pages, 8 figures, 11 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘é‡åœºæ™¯å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œåˆ†æäº†å¯¼è‡´æ€§èƒ½ä¸‹é™çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºDrICLçš„ä¼˜åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚DrICLé€šè¿‡å·®å¼‚åŒ–çš„å­¦ä¹ æ–¹æ³•ä¼˜åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ç›®æ ‡ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å¤šåœºæ™¯æ¼”ç¤ºçš„æƒé‡æ¥å‡å°‘æ•°æ®å™ªå£°çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šåœºæ™¯å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼ˆICL-50ï¼‰ï¼Œç”¨äºè¯„ä¼°å’Œä¼˜åŒ–LLMåœ¨å¤šåœºæ™¯å­¦ä¹ ä¸­çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨DrICLä¼˜åŒ–çš„LLMåœ¨å„ç§ä»»åŠ¡çš„å¤šåœºæ™¯è®¾ç½®ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å°‘é‡åœºæ™¯å­¦ä¹ ï¼ˆICLï¼‰ä¸­é¢ä¸´æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› åŒ…æ‹¬è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ä¼˜åŒ–ç›®æ ‡çš„ä¸ä½³å’Œå¢é‡æ•°æ®å™ªå£°ã€‚</li>
<li>å¼•å…¥DrICLä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å·®å¼‚åŒ–çš„å­¦ä¹ æ–¹æ³•å’ŒåŠ¨æ€è°ƒæ•´æ¼”ç¤ºæƒé‡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DrICLèƒ½åœ¨å…¨çƒèŒƒå›´å†…ä¼˜åŒ–NLLç›®æ ‡ï¼Œæé«˜å¤šåœºæ™¯å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>å»ºç«‹äº†å¤šåœºæ™¯å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼ˆICL-50ï¼‰ï¼Œæ¶µç›–ä»1åˆ°350çš„ä¸åŒåœºæ™¯æ•°é‡ï¼Œç”¨äºè¯„ä¼°å’Œæ¨åŠ¨LLMåœ¨å¤šåœºæ™¯å­¦ä¹ ä¸­çš„ç ”ç©¶ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨DrICLä¼˜åŒ–çš„LLMåœ¨å¤šåœºæ™¯è®¾ç½®ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è·¨åŸŸåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-def9c9f6993c96097ca92ac3aa053ae5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9e8e2e9a38b401521e2fb3f00ab4c53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c62698a9a1c91857d9a3eb70f5f302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e37d4c6ae9011be13cd5ccaa43883b08.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Aggregation-Artifacts-in-Subjective-Tasks-Collapse-Large-Language-Modelsâ€™-Posteriors"><a href="#Aggregation-Artifacts-in-Subjective-Tasks-Collapse-Large-Language-Modelsâ€™-Posteriors" class="headerlink" title="Aggregation Artifacts in Subjective Tasks Collapse Large Language   Modelsâ€™ Posteriors"></a>Aggregation Artifacts in Subjective Tasks Collapse Large Language   Modelsâ€™ Posteriors</h2><p><strong>Authors:Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</strong></p>
<p>In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than â€œlearningâ€ to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å·²æˆä¸ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡Œè‡ªç„¶è¯­è¨€ä»»åŠ¡çš„ä¸»è¦æ–¹æ³•ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„çŸ¥è¯†å¯¹äºè¿™ç§å°æ ·æœ¬èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä¸ºæ¨¡å‹æä¾›ä»»åŠ¡å…ˆéªŒã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒICLä¸»è¦ä¾èµ–äºè·å–ä»»åŠ¡å…ˆéªŒï¼Œè€Œä¸æ˜¯â€œå­¦ä¹ â€æ‰§è¡Œä»»åŠ¡ã€‚è¿™ç§å±€é™æ€§åœ¨æƒ…æ„Ÿã€é“å¾·ç­‰å¤æ‚çš„ä¸»è§‚é¢†åŸŸå°¤ä¸ºæ˜æ˜¾ï¼Œå…ˆéªŒçŸ¥è¯†ä¼šæ˜¾è‘—å½±å“åéªŒé¢„æµ‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥è¿™æ˜¯å¦æ˜¯ç”±äºç›¸åº”æ•°æ®é›†ä¸­ä½¿ç”¨çš„èšåˆæ–¹æ³•å¯¼è‡´çš„ï¼Œå°è¯•ç»“åˆä½åŒæ„ç‡ã€åˆ†æ•£çš„æ³¨é‡Šå¯èƒ½ä¼šå¯¼è‡´æ³¨é‡Šäº§ç”Ÿçš„ä¼ªåƒï¼Œåœ¨æç¤ºä¸­äº§ç”Ÿæœ‰å®³çš„å™ªå£°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºåˆé€‚ã€å®šé‡çš„LLMå…ˆéªŒåº¦é‡æ¥è¯„ä¼°å¯¹ç‰¹å®šæ³¨é‡Šå™¨çš„åéªŒåè§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œèšåˆæ˜¯ä¸»è§‚ä»»åŠ¡å»ºæ¨¡ä¸­çš„æ··æ·†å› ç´ ï¼Œå¹¶ä¸»å¼ é‡ç‚¹å…³æ³¨å¯¹ä¸ªä½“çš„å»ºæ¨¡ã€‚ç„¶è€Œï¼Œèšåˆå¹¶ä¸èƒ½å®Œå…¨è§£é‡ŠICLä¸æœ€æ–°æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œæ„å‘³ç€æ­¤ç±»ä»»åŠ¡ä¸­çš„å…¶ä»–å› ç´ ä¹Ÿå¯¼è‡´äº†è§‚å¯Ÿåˆ°çš„ç°è±¡ã€‚æœ€åï¼Œé€šè¿‡å¯¹æ³¨é‡Šå™¨çº§åˆ«çš„æ ‡ç­¾è¿›è¡Œä¸¥æ ¼ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å°‘æ•°æ³¨é‡Šå™¨ä¸LLMæ›´å¥½åœ°å¯¹é½å¹¶æ”¾å¤§å…¶è§‚ç‚¹æ˜¯å¯èƒ½çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13776v4">PDF</a> 16 pages, 12 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†å®Œæˆè‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚ä½†ç ”ç©¶å‘ç°ï¼ŒICLä¸»è¦ä¾èµ–æ£€ç´¢ä»»åŠ¡å…ˆéªŒçŸ¥è¯†è€Œéâ€œå­¦ä¹ â€æ‰§è¡Œä»»åŠ¡ã€‚åœ¨æƒ…æ„Ÿã€é“å¾·ç­‰å¤æ‚ä¸»è§‚é¢†åŸŸï¼Œå…ˆéªŒçŸ¥è¯†å¯¹åéªŒé¢„æµ‹å½±å“æ˜¾è‘—ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ•°æ®é›†èšåˆæ–¹å¼æ˜¯å¦å¯¼è‡´äº†è¿™ä¸€ç°è±¡ï¼Œå‘ç°å°è¯•åˆå¹¶ä½ä¸€è‡´æ€§ã€åˆ†æ•£çš„æ³¨é‡Šå¯èƒ½å¯¼è‡´æç¤ºä¸­çš„æœ‰å®³å™ªå£°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹LLMå…ˆéªŒçŸ¥è¯†çš„å®šé‡è¯„ä¼°ï¼Œå‘ç°èšåˆæ˜¯ä¸»è§‚ä»»åŠ¡å»ºæ¨¡ä¸­çš„å¹²æ‰°å› ç´ ï¼Œä¸»å¼ é‡ç‚¹å»ºæ¨¡ä¸ªä½“è€Œéæ•´ä½“ã€‚ä½†èšåˆå¹¶ä¸èƒ½å®Œå…¨è§£é‡ŠICLä¸æœ€æ–°æŠ€æœ¯ä¹‹é—´çš„å·®è·ï¼Œè¯´æ˜è¿˜æœ‰å…¶ä»–å› ç´ å½±å“äº†ä»»åŠ¡è¡¨ç°ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶æ³¨é‡Šè€…å±‚é¢çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å‘ç°å°‘æ•°æ³¨é‡Šè€…çš„è§‚ç‚¹ä¸LLMæ›´ä¸€è‡´ï¼Œä¸”ä»–ä»¬çš„è§‚ç‚¹ä¼šè¢«è¿›ä¸€æ­¥æ”¾å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICL ä¸»è¦ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†æ¥å®Œæˆè‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>åœ¨å¤æ‚ä¸»è§‚é¢†åŸŸï¼Œå…ˆéªŒçŸ¥è¯†å¯¹åéªŒé¢„æµ‹çš„å½±å“æ˜¾è‘—ã€‚</li>
<li>æ•°æ®é›†çš„èšåˆæ–¹å¼å¯èƒ½å¯¼è‡´æ³¨é‡Šä¸­çš„æœ‰å®³å™ªå£°ï¼Œå½±å“ä»»åŠ¡è¡¨ç°ã€‚</li>
<li>èšåˆæ˜¯ä¸»è§‚ä»»åŠ¡å»ºæ¨¡ä¸­çš„å¹²æ‰°å› ç´ ï¼Œåº”æ›´æ³¨é‡å»ºæ¨¡ä¸ªä½“ã€‚</li>
<li>é™¤äº†èšåˆå› ç´ ï¼Œå…¶ä»–å› ç´ ä¹Ÿå½±å“äº†ICLçš„è¡¨ç°ï¼Œä»éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å°‘æ•°æ³¨é‡Šè€…çš„è§‚ç‚¹ä¸LLMæ›´ä¸€è‡´ï¼Œä»–ä»¬çš„è§‚ç‚¹åœ¨æ¨¡å‹ä¸­çš„å½±å“å¯èƒ½ä¼šè¢«æ”¾å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48f4427e904b6babf8666eab98676356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea476b55613b4b8cfe6ac91c0d847156.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa137b1732bdc12bced30de02e261e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77c7eebb2a86fccc78d221fd28cc4a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63541f6606e0f528601e2a68b0ec509.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="V-RoAst-Visual-Road-Assessment-Can-VLM-be-a-Road-Safety-Assessor-Using-the-iRAP-Standard"><a href="#V-RoAst-Visual-Road-Assessment-Can-VLM-be-a-Road-Safety-Assessor-Using-the-iRAP-Standard" class="headerlink" title="V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using   the iRAP Standard?"></a>V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using   the iRAP Standard?</h2><p><strong>Authors:Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, James Haworth, Xinglei Wang, Kerkritt Sriroongvikrai, Nicola Christie, Ilya Ilyankou, Meihui Wang, Huanfa Chen</strong></p>
<p>Road traffic crashes result in millions of deaths annually and significant economic burdens, particularly on Low- and Middle-Income Countries (LMICs). Road safety assessments traditionally rely on human-labelled data, which is labour-intensive and time-consuming. While Convolutional Neural Networks (CNNs) have advanced automated road safety assessments, they typically demand large labelled datasets and often require fine-tuning for each new geographic context. This study explores whether Vision Language Models (VLMs) with zero-shot capability can overcome these limitations to serve as effective road safety assessors using the International Road Assessment Programme (iRAP) standard. Our approach, V-RoAst (Visual question answering for Road Assessment), leverages advanced VLMs, such as Gemini-1.5-flash and GPT-4o-mini, to analyse road safety attributes without requiring any labelled training data. By optimising prompt engineering and utilising crowdsourced imagery from Mapillary, V-RoAst provides a scalable, cost-effective, and automated solution for global road safety assessments. Preliminary results show that while VLMs achieve lower performance than CNN-based models, they are capable of Visual Question Answering (VQA) and show potential in predicting star ratings from crowdsourced imagery. However, their performance is poor when key visual features are absent in the imagery, emphasising the need for human labelling to address these gaps. Advancements in VLMs, alongside in-context learning such as chain-of-thought and few-shot learning, and parameter-efficient fine-tuning, present opportunities for improvement, making VLMs promising tools for road assessment tasks. Designed for resource-constrained stakeholders, this framework holds the potential to save lives and reduce economic burdens worldwide. Code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/PongNJ/V-RoAst">https://github.com/PongNJ/V-RoAst</a>. </p>
<blockquote>
<p>é“è·¯äº¤é€šäº‹æ•…æ¯å¹´é€ æˆæ•°ç™¾ä¸‡äººæ­»äº¡å’Œå·¨å¤§çš„ç»æµè´Ÿæ‹…ï¼Œç‰¹åˆ«æ˜¯ç»™ä¸­ä½æ”¶å…¥å›½å®¶ï¼ˆLMICsï¼‰å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é“è·¯å®‰å…¨è¯„ä¼°ä¸»è¦ä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ•°æ®ï¼Œè¿™æ—¢è€—æ—¶åˆè€—åŠ›ã€‚å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²ç»æ¨åŠ¨äº†é“è·¯å®‰å…¨è¯„ä¼°çš„è‡ªåŠ¨åŒ–ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶ä¸”é’ˆå¯¹æ¯ä¸ªæ–°çš„åœ°ç†èƒŒæ™¯éƒ½éœ€è¦è¿›è¡Œå¾®è°ƒã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å…·æœ‰é›¶æ ·æœ¬èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¯å¦èƒ½å¤Ÿå…‹æœè¿™äº›é™åˆ¶ï¼Œä»¥å›½é™…é“è·¯è¯„ä¼°è®¡åˆ’ï¼ˆiRAPï¼‰æ ‡å‡†ä½œä¸ºæœ‰æ•ˆçš„é“è·¯å®‰å…¨è¯„ä¼°å·¥å…·ã€‚æˆ‘ä»¬çš„æ–¹æ³•V-RoAstï¼ˆç”¨äºé“è·¯è¯„ä¼°çš„è§†è§‰é—®ç­”ï¼‰ï¼Œåˆ©ç”¨å…ˆè¿›çš„VLMsæŠ€æœ¯ï¼Œå¦‚Gemini-1.5-flashå’ŒGPT-4o-miniï¼Œåˆ†æé“è·¯å®‰å…¨å±æ€§è€Œæ— éœ€ä»»ä½•æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡ä¼˜åŒ–æç¤ºå·¥ç¨‹å¹¶åˆ©ç”¨æ¥è‡ªMapillaryçš„ä¼—æºå›¾åƒæ•°æ®ï¼ŒV-RoAstæä¾›äº†ä¸€ä¸ªå¯è§„æ¨¡åŒ–ã€ç»æµé«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„å…¨çƒé“è·¯å®‰å…¨è¯„ä¼°è§£å†³æ–¹æ¡ˆã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè™½ç„¶VLMsçš„æ€§èƒ½ä½äºCNNæ¨¡å‹ï¼Œä½†å®ƒä»¬èƒ½å¤Ÿè¿›è¡Œè§†è§‰é—®ç­”å¹¶æ˜¾ç¤ºå‡ºä»ä¼—æºå›¾åƒé¢„æµ‹æ˜Ÿçº§è¯„åˆ†çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å›¾åƒä¸­ç¼ºå°‘å…³é”®è§†è§‰ç‰¹å¾æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½è¾ƒå·®ï¼Œè¿™å¼ºè°ƒäº†è¿›è¡Œäººå·¥æ ‡æ³¨ä»¥å¼¥è¡¥è¿™äº›ä¸è¶³çš„å¿…è¦æ€§ã€‚éšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä»¥åŠä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆå¦‚æ€ç»´é“¾å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒç­‰æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºæ”¹è¿›æä¾›äº†æœºä¼šï¼Œä½¿VLMsæˆä¸ºé“è·¯è¯„ä¼°ä»»åŠ¡çš„æœ‰å‰é€”çš„å·¥å…·ã€‚è¿™ä¸€æ¡†æ¶æ—¨åœ¨ä¸ºèµ„æºæœ‰é™çš„åˆ©ç›Šç›¸å…³è€…è®¾è®¡ï¼Œå…·æœ‰æŒ½æ•‘ç”Ÿå‘½å’Œå‡å°‘å…¨çƒç»æµè´Ÿæ‹…çš„æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/PongNJ/V-RoAst%E3%80%82">https://github.com/PongNJ/V-RoAstã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10872v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œé“è·¯å®‰å…¨è¯„ä¼°çš„å¯è¡Œæ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºV-RoAstçš„æ–¹æ³•ï¼Œåˆ©ç”¨å…ˆè¿›çš„VLMsåˆ†æé“è·¯å®‰å…¨å±æ€§ï¼Œæ— éœ€æ ‡è®°è®­ç»ƒæ•°æ®ã€‚é€šè¿‡ä¼˜åŒ–æç¤ºå·¥ç¨‹å’Œåˆ©ç”¨Mapillaryçš„ä¼—æºå›¾åƒï¼ŒV-RoAstæä¾›äº†å¯è§„æ¨¡åŒ–ã€æˆæœ¬ä½å»‰ã€è‡ªåŠ¨åŒ–çš„å…¨çƒé“è·¯å®‰å…¨è¯„ä¼°è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶VLMsçš„æ€§èƒ½ä½äºCNNæ¨¡å‹ï¼Œä½†åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’ŒåŸºäºä¼—æºå›¾åƒé¢„æµ‹æ˜Ÿçº§è¯„åˆ†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“è·¯å®‰å…¨è¯„ä¼°ä¼ ç»Ÿä¸Šä¾èµ–äººå·¥æ ‡è®°æ•°æ®ï¼Œæ—¢è€—æ—¶åˆåŠ³åŠ›å¯†é›†ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²ç”¨äºè‡ªåŠ¨åŒ–é“è·¯å®‰å…¨è¯„ä¼°ï¼Œä½†éœ€å¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œä¸”å¯¹æ–°åœ°ç†ä¸Šä¸‹æ–‡éœ€å¾®è°ƒã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é“è·¯å®‰å…¨è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºV-RoAstçš„æ–¹æ³•ã€‚</li>
<li>V-RoAståˆ©ç”¨VLMsåˆ†æé“è·¯å®‰å…¨å±æ€§ï¼Œæ— éœ€æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œæä¾›å¯è§„æ¨¡åŒ–ã€è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>VLMsåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’ŒåŸºäºä¼—æºå›¾åƒé¢„æµ‹æ˜Ÿçº§è¯„åˆ†æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>VLMsæ€§èƒ½åœ¨å…³é”®è§†è§‰ç‰¹å¾ç¼ºå¤±æ—¶è¡¨ç°è¾ƒå·®ï¼Œéœ€äººå·¥æ ‡è®°å¡«è¡¥ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfd5d0bc980d42a71657bcec3f331b5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b470323261dcf3ba2ab4b1693f67a34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b40c38a04a120733e7909f81fcc99071.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Query-Performance-Prediction-using-Relevance-Judgments-Generated-by-Large-Language-Models"><a href="#Query-Performance-Prediction-using-Relevance-Judgments-Generated-by-Large-Language-Models" class="headerlink" title="Query Performance Prediction using Relevance Judgments Generated by   Large Language Models"></a>Query Performance Prediction using Relevance Judgments Generated by   Large Language Models</h2><p><strong>Authors:Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke</strong></p>
<p>Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of predicting the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels. This also allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We predict an itemâ€™s relevance by using open-source large language models (LLMs) to ensure scientific reproducibility. We face two main challenges: (i) excessive computational costs of judging an entire corpus for predicting a metric considering recall, and (ii) limited performance in prompting open-source LLMs in a zero-&#x2F;few-shot manner. To solve the challenges, we devise an approximation strategy to predict an IR measure considering recall and propose to fine-tune open-source LLMs using human-labeled relevance judgments. Experiments on the TREC 2019 to 2022 deep learning tracks and CAsT-19 and 20 datasets show that QPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural rankers. </p>
<blockquote>
<p>æŸ¥è¯¢æ€§èƒ½é¢„æµ‹ï¼ˆQPPï¼‰æ—¨åœ¨ä¼°è®¡æœç´¢ç³»ç»Ÿå¯¹æŸ¥è¯¢çš„æ£€ç´¢è´¨é‡ï¼Œè€Œæ— éœ€äººå·¥ç›¸å…³æ€§åˆ¤æ–­ã€‚ä»¥å¾€çš„QPPæ–¹æ³•é€šå¸¸è¿”å›ä¸€ä¸ªå•ä¸€æ ‡é‡å€¼ï¼Œå¹¶ä¸è¦æ±‚é¢„æµ‹å€¼è¿‘ä¼¼ç‰¹å®šçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è¯„ä¼°æªæ–½ï¼Œè¿™å¯¼è‡´äº†ä¸€äº›ç¼ºç‚¹ï¼šï¼ˆiï¼‰ä¸€ä¸ªå•ä¸€æ ‡é‡ä¸è¶³ä»¥å‡†ç¡®ä»£è¡¨ä¸åŒçš„IRè¯„ä¼°æªæ–½ï¼Œå°¤å…¶æ˜¯å½“æŒ‡æ ‡ä¹‹é—´ç›¸å…³æ€§ä¸é«˜æ—¶ï¼›ï¼ˆiiï¼‰å•ä¸€æ ‡é‡é™åˆ¶äº†QPPæ–¹æ³•çš„å¯è§£é‡Šæ€§ï¼Œå› ä¸ºä»…ä½¿ç”¨æ ‡é‡ä¸è¶³ä»¥è§£é‡ŠQPPç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ï¼ˆQPP-GenREï¼‰çš„QPPæ¡†æ¶ã€‚å®ƒå°†QPPåˆ†è§£ä¸ºé¢„æµ‹æ’ååˆ—è¡¨ä¸­æ¯ä¸ªé¡¹ç›®å¯¹ç»™å®šæŸ¥è¯¢çš„ç›¸å…³æ€§çš„ç‹¬ç«‹å­ä»»åŠ¡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä½œä¸ºä¼ªæ ‡ç­¾æ¥é¢„æµ‹ä»»ä½•IRè¯„ä¼°æªæ–½ã€‚è¿™è¿˜å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£é‡Šé¢„æµ‹çš„IRè¯„ä¼°æªæ–½ï¼Œå¹¶è¯†åˆ«ã€è·Ÿè¸ªå’Œçº æ­£ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä¸­çš„é”™è¯¯ï¼Œä»¥æé«˜QPPè´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é¢„æµ‹é¡¹ç›®çš„ç›¸å…³æ€§ï¼Œä»¥ç¡®ä¿ç§‘å­¦å¯é‡å¤æ€§ã€‚æˆ‘ä»¬é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆiï¼‰è€ƒè™‘åˆ°å¬å›ç‡é¢„æµ‹æŒ‡æ ‡è€Œåˆ¤æ–­æ•´ä¸ªè¯­æ–™åº“çš„è®¡ç®—æˆæœ¬è¿‡é«˜ï¼›ï¼ˆiiï¼‰åœ¨é›¶&#x2F;å°‘é•œå¤´æƒ…å†µä¸‹æç¤ºå¼€æºLLMæ—¶çš„æ€§èƒ½æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ç§è¿‘ä¼¼ç­–ç•¥æ¥é¢„æµ‹è€ƒè™‘å¬å›çš„IRæªæ–½ï¼Œå¹¶æè®®ä½¿ç”¨äººç±»æ ‡æ³¨çš„ç›¸å…³æ€§åˆ¤æ–­å¯¹å¼€æºLLMè¿›è¡Œå¾®è°ƒã€‚åœ¨TREC 2019è‡³2022æ·±åº¦å­¦ä¹ è½¨è¿¹ä»¥åŠCAsT-19å’Œ20æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQPP-GenREåœ¨è¯æ±‡å’Œç¥ç»æ’åå™¨æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„QPPè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01012v3">PDF</a> Accepted by ACM Transactions on Information Systems (TOIS)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æŸ¥è¯¢æ€§èƒ½é¢„æµ‹ï¼ˆQPPï¼‰çš„æ–°æ¡†æ¶â€”â€”QPP-GenREã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¼ ç»ŸQPPæ–¹æ³•è¿”å›å•ä¸€æ ‡é‡å€¼å¸¦æ¥çš„é—®é¢˜ï¼Œå¦‚æ— æ³•å‡†ç¡®ä»£è¡¨ä¸åŒçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è¯„ä¼°æŒ‡æ ‡å’Œè§£é‡Šæ€§ä¸è¶³ã€‚QPP-GenREé€šè¿‡åˆ†è§£QPPä¸ºé¢„æµ‹ç»™å®šæŸ¥è¯¢ä¸‹æ’ååˆ—è¡¨ä¸­æ¯ä¸ªé¡¹ç›®ç›¸å…³æ€§çš„ç‹¬ç«‹å­ä»»åŠ¡ï¼Œä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä½œä¸ºä¼ªæ ‡ç­¾æ¥é¢„æµ‹ä»»ä½•IRè¯„ä¼°æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸è§£é‡Šé¢„æµ‹çš„IRè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è¯†åˆ«ã€è·Ÿè¸ªå’Œçº æ­£ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä¸­çš„é”™è¯¯ä»¥æé«˜QPPè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢æ€§èƒ½é¢„æµ‹ï¼ˆQPPï¼‰æ—¨åœ¨ä¼°è®¡æœç´¢ç³»ç»Ÿå¯¹æŸ¥è¯¢çš„æ£€ç´¢è´¨é‡ï¼Œæ— éœ€äººå·¥ç›¸å…³æ€§åˆ¤æ–­ã€‚</li>
<li>ä¼ ç»ŸQPPæ–¹æ³•è¿”å›å•ä¸€æ ‡é‡å€¼ï¼Œå­˜åœ¨ä¸è¶³ï¼Œå¦‚ä¸èƒ½å‡†ç¡®ä»£è¡¨ä¸åŒçš„IRè¯„ä¼°æŒ‡æ ‡å’Œè§£é‡Šæ€§ä¸è¶³ã€‚</li>
<li>QPP-GenREæ¡†æ¶ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œèƒ½é¢„æµ‹ä»»ä½•IRè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>QPP-GenREå…è®¸è§£é‡Šé¢„æµ‹çš„IRè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è¯†åˆ«ã€è·Ÿè¸ªå’Œçº æ­£ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­ä¸­çš„é”™è¯¯ã€‚</li>
<li>QPP-GenREé€šè¿‡ä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„æµ‹é¡¹ç›®çš„ç›¸å…³æ€§ï¼Œç¡®ä¿ç§‘å­¦å¯é‡å¤æ€§ã€‚</li>
<li>QPP-GenREé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåˆ¤æ–­æ•´ä¸ªè¯­æ–™åº“ä»¥é¢„æµ‹æŒ‡æ ‡çš„è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä»¥åŠåœ¨é›¶&#x2F;å°‘é•œå¤´æƒ…å†µä¸‹æç¤ºå¼€æºLLMsçš„æ€§èƒ½æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.01012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2e25969df59c6fb651759cb0f12e405.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16d0ee28d053b37657d39b8c1df380c7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-426c7f27e4ef6d0c57b2bcacc4c994a2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
