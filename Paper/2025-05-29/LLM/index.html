<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="Silence-is-Not-Consensus-Disrupting-Agreement-Bias-in-Multi-Agent-LLMs-via-Catfish-Agent-for-Clinical-Decision-Making"><a href="#Silence-is-Not-Consensus-Disrupting-Agreement-Bias-in-Multi-Agent-LLMs-via-Catfish-Agent-for-Clinical-Decision-Making" class="headerlink" title="Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making"></a>Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making</h2><p><strong>Authors:Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng</strong></p>
<p>Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the &#96;&#96;catfish effectâ€™â€™ in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&amp;A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠé—®é¢˜å›ç­”æ–¹é¢å±•ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œæœ€è¿‘çš„å¤šä»£ç†æ¡†æ¶é€šè¿‡ååŒæ¨ç†è¿›ä¸€æ­¥æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºäº†ä¸€ä¸ªåå¤å‡ºç°çš„é—®é¢˜ï¼Œå³é™é»˜åè®®ï¼ˆSilent Agreementï¼‰ï¼Œä»£ç†åœ¨æ²¡æœ‰è¶³å¤Ÿçš„å…³é”®åˆ†æçš„æƒ…å†µä¸‹è¿‡æ—©åœ°è¾¾æˆè¯Šæ–­å…±è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æˆ–æ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µï¼Œå³â€œçŒ«é±¼ä»£ç†â€ï¼ˆCatfish Agentï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§’è‰²ç‰¹æ®ŠåŒ–çš„LLMè®¾è®¡ï¼Œæ—¨åœ¨æ³¨å…¥ç»“æ„åŒ–å¼‚è®®å’Œåå¯¹é™é»˜åè®®ã€‚è¯¥è®¾è®¡å—åˆ°ç»„ç»‡å¿ƒç†å­¦ä¸­çš„â€œçŒ«é±¼æ•ˆåº”â€çš„å¯å‘ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ–°å…´å…±è¯†æ¥åˆºæ¿€æ›´æ·±å…¥çš„æ¨ç†ã€‚æˆ‘ä»¬åˆ¶å®šäº†ä¸¤ç§æœºåˆ¶æ¥é¼“åŠ±æœ‰æ•ˆå’Œæƒ…å¢ƒæ„ŸçŸ¥å¹²é¢„ï¼šï¼ˆiï¼‰ä¸€ç§åŸºäºæ¡ˆä¾‹éš¾åº¦çš„å¤æ‚æ€§æ„ŸçŸ¥å¹²é¢„ï¼Œå¯è°ƒèŠ‚ä»£ç†å‚ä¸åº¦ï¼›ï¼ˆiiï¼‰ä¸€ç§å¹³è¡¡æ‰¹è¯„ä¸åˆä½œçš„è¯­æ°”æ ¡å‡†å¹²é¢„ã€‚åœ¨ä¹ä¸ªåŒ»å­¦é—®ç­”å’Œä¸‰ä¸ªåŒ»å­¦è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå•ä»£ç†å’Œå¤šä»£ç†LLMæ¡†æ¶ï¼ŒåŒ…æ‹¬é¢†å…ˆçš„å•†ä¸šæ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒDeepSeek-R1ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠé—®ç­”ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¤šä»£ç†æ¡†æ¶èƒ½è¿›ä¸€æ­¥æå‡è¯Šæ–­å‡†ç¡®æ€§ã€‚ä½†å­˜åœ¨â€œé™é»˜åè®®â€é—®é¢˜ï¼Œå³ä»£ç†åœ¨ç¼ºä¹å……åˆ†åˆ†æçš„æƒ…å†µä¸‹è¿‡æ—©è¾¾æˆè¯Šæ–­å…±è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æˆ–æ¨¡ç³Šæ¡ˆä¾‹ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºâ€œçŒ«é±¼ä»£ç†â€æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡çš„è§’è‰²å‹LLMï¼Œæ—¨åœ¨æ³¨å…¥ç»“æ„åŒ–å¼‚è®®å¹¶æŒ‘æˆ˜é™é»˜åè®®ã€‚å—ç»„ç»‡å¿ƒç†å­¦ä¸­çš„â€œçŒ«é±¼æ•ˆåº”â€å¯å‘ï¼ŒçŒ«é±¼ä»£ç†æ—¨åœ¨åˆºæ¿€æ›´æ·±å…¥çš„æ€è€ƒå¹¶æŒ‘æˆ˜ç°æœ‰å…±è¯†ã€‚æˆ‘ä»¬åˆ¶å®šä¸¤ç§æœºåˆ¶æ¥é¼“åŠ±æœ‰æ•ˆä¸”è¯­å¢ƒæ„ŸçŸ¥çš„å¹²é¢„ï¼šï¼ˆä¸€ï¼‰å¤æ‚æƒ…å†µæ„ŸçŸ¥å¹²é¢„ï¼Œæ ¹æ®æ¡ˆä¾‹éš¾åº¦è°ƒæ•´ä»£ç†å‚ä¸åº¦ï¼›ï¼ˆäºŒï¼‰è¯­æ°”æ ¡å‡†å¹²é¢„ï¼Œæ—¨åœ¨å¹³è¡¡æ‰¹è¯„ä¸åä½œã€‚åœ¨ä¹ä¸ªåŒ»ç–—é—®ç­”å’Œä¸‰ä¸ªåŒ»ç–—è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒç»­ä¼˜äºå•ä»£ç†å’Œå¤šä»£ç†LLMæ¡†æ¶ï¼ŒåŒ…æ‹¬é¢†å…ˆçš„å•†ä¸šæ¨¡å‹å¦‚GPT-4oå’ŒDeepSeek-R1ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ä¸´åºŠé—®ç­”ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¤šä»£ç†æ¡†æ¶å¯è¿›ä¸€æ­¥æå‡è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>å­˜åœ¨â€œé™é»˜åè®®â€é—®é¢˜ï¼Œå³ä»£ç†è¿‡æ—©è¾¾æˆè¯Šæ–­å…±è¯†ï¼Œéœ€å¼•å…¥æ–°ç­–ç•¥åº”å¯¹ã€‚</li>
<li>å¼•å…¥â€œçŒ«é±¼ä»£ç†â€æ¦‚å¿µï¼Œä¸ºLLMæ³¨å…¥ç»“æ„åŒ–å¼‚è®®ï¼ŒæŒ‘æˆ˜é™é»˜åè®®ã€‚</li>
<li>çŒ«é±¼ä»£ç†è®¾è®¡å—ç»„ç»‡å¿ƒç†å­¦ä¸­çš„â€œçŒ«é±¼æ•ˆåº”â€å¯å‘ï¼Œæ—¨åœ¨åˆºæ¿€æ›´æ·±å…¥çš„æ€è€ƒã€‚</li>
<li>é€šè¿‡ä¸¤ç§æœºåˆ¶æ¥é¼“åŠ±æœ‰æ•ˆä¸”è¯­å¢ƒæ„ŸçŸ¥çš„å¹²é¢„ï¼šå¤æ‚æƒ…å†µæ„ŸçŸ¥å¹²é¢„å’Œè¯­æ°”æ ¡å‡†å¹²é¢„ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒçŒ«é±¼ä»£ç†æ–¹æ³•ä¼˜äºç°æœ‰LLMæ¡†æ¶ï¼ŒåŒ…æ‹¬å•†ä¸šé¢†å…ˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73d59cff8df0521b54720239d5b9457b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23eed9d80a5e3c34b6013f41ddd37082.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae0839f202633dad06695c9ea30859a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cdb58fd46d9e09baa04621ed04c3e6c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adversarial-Attacks-against-Closed-Source-MLLMs-via-Feature-Optimal-Alignment"><a href="#Adversarial-Attacks-against-Closed-Source-MLLMs-via-Feature-Optimal-Alignment" class="headerlink" title="Adversarial Attacks against Closed-Source MLLMs via Feature Optimal   Alignment"></a>Adversarial Attacks against Closed-Source MLLMs via Feature Optimal   Alignment</h2><p><strong>Authors:Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, Yang Liu</strong></p>
<p>Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIPâ€™s [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at <a target="_blank" rel="noopener" href="https://github.com/jiaxiaojunQAQ/FOA-Attack">https://github.com/jiaxiaojunQAQ/FOA-Attack</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶å®¹æ˜“å—åˆ°å¯è½¬ç§»å¯¹æŠ—æ ·æœ¬çš„å¨èƒã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¯¹æŠ—æ ·æœ¬ä¸ç›®æ ‡æ ·æœ¬ä¹‹é—´çš„å…¨å±€ç‰¹å¾ï¼ˆå¦‚CLIPçš„[CLS]ä»¤ç‰Œï¼‰æ¥å®ç°æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†è¡¥ä¸ä»¤ç‰Œä¸­ä¸°å¯Œçš„å±€éƒ¨ä¿¡æ¯ã€‚è¿™å¯¼è‡´äº†å¯¹å‡†ä¸ä½³å’Œå¯è½¬ç§»æ€§æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å°é—­æºæ¨¡å‹çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æœ€ä¼˜å¯¹é½çš„æœ‰é’ˆå¯¹æ€§çš„å¯è½¬ç§»å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºFOA-Attackï¼Œä»¥æé«˜å¯¹æŠ—è½¬ç§»èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å…¨å±€å±‚é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„å…¨å±€ç‰¹å¾æŸå¤±ï¼Œä»¥å¯¹é½å¯¹æŠ—æ ·æœ¬ä¸ç›®æ ‡æ ·æœ¬çš„ç²—ç²’åº¦ç‰¹å¾ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œè€ƒè™‘åˆ°Transformerå†…ä¸°å¯Œçš„å±€éƒ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬åˆ©ç”¨èšç±»æŠ€æœ¯æå–ç´§å‡‘çš„å±€éƒ¨æ¨¡å¼ï¼Œä»¥å‡è½»å†—ä½™çš„å±€éƒ¨ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯¹æŠ—æ ·æœ¬å’Œç›®æ ‡æ ·æœ¬ä¹‹é—´çš„å±€éƒ¨ç‰¹å¾å¯¹é½åˆ¶å®šä¸ºæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºå±€éƒ¨èšç±»æœ€ä¼˜ä¼ è¾“æŸå¤±æ¥ç²¾ç»†è°ƒæ•´ç»†ç²’åº¦ç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŠ¨æ€é›†æˆæ¨¡å‹åŠ æƒç­–ç•¥ï¼Œä»¥åœ¨ç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ—¶è‡ªé€‚åº”åœ°å¹³è¡¡å¤šä¸ªæ¨¡å‹çš„å½±å“ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜å¯è½¬ç§»æ€§ã€‚åœ¨å¤šç§æ¨¡å‹ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è½¬ç§»åˆ°å°é—­æºMLLMsæ—¶ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jiaxiaojunQAQ/FOA-Attack%E3%80%82">https://github.com/jiaxiaojunQAQ/FOA-Attackã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21494v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰¹å¾æœ€ä¼˜å¯¹é½çš„é’ˆå¯¹æ€§å¯è½¬ç§»å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼ˆFOA-Attackï¼‰ï¼Œæ—¨åœ¨æé«˜å¯¹æŠ—æ ·æœ¬çš„è¿ç§»èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å…¨çƒå±‚é¢å¼•å…¥åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„å…¨å±€ç‰¹å¾æŸå¤±ï¼Œä»¥å¯¹é½å¯¹æŠ—æ ·æœ¬ä¸ç›®æ ‡æ ·æœ¬çš„ç²—ç²’åº¦ç‰¹å¾ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Transformerä¸­çš„ä¸°å¯Œå±€éƒ¨è¡¨ç¤ºï¼Œé€šè¿‡èšç±»æŠ€æœ¯æå–ç´§å‡‘çš„å±€éƒ¨æ¨¡å¼ï¼Œç¼“è§£å†—ä½™çš„å±€éƒ¨ç‰¹å¾ã€‚ç„¶åï¼Œå°†å¯¹æŠ—æ ·æœ¬ä¸é¶æ ‡æ ·æœ¬ä¹‹é—´çš„å±€éƒ¨ç‰¹å¾å¯¹é½å…¬å¼åŒ–ä¸ºæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºå±€éƒ¨èšç±»æœ€ä¼˜ä¼ è¾“æŸå¤±ä»¥ä¼˜åŒ–ç»†ç²’åº¦ç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŠ¨æ€é›†æˆæ¨¡å‹åŠ æƒç­–ç•¥ï¼Œä»¥åœ¨ç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ—¶è‡ªé€‚åº”åœ°å¹³è¡¡å¤šä¸ªæ¨¡å‹çš„å½±å“ï¼Œè¿›ä¸€æ­¥æé«˜è¿ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é’ˆå¯¹å°é—­æºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸Šè¡¨ç°æ›´å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°å¯è½¬ç§»å¯¹æŠ—æ ·æœ¬çš„æ”»å‡»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å…¨å±€ç‰¹å¾å¯¹é½è¿›è¡Œé’ˆå¯¹æ€§æ”»å‡»ï¼Œä½†å¿½ç•¥äº†å±€éƒ¨ä¿¡æ¯ã€‚</li>
<li>FOA-Attackæ–¹æ³•å¼•å…¥å…¨å±€ç‰¹å¾æŸå¤±å’ŒåŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„å±€éƒ¨ç‰¹å¾å¯¹é½ï¼Œä»¥æé«˜å¯¹æŠ—è¿ç§»èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨Transformerä¸­çš„ä¸°å¯Œå±€éƒ¨è¡¨ç¤ºå’Œèšç±»æŠ€æœ¯æ¥æå–ç´§å‡‘çš„å±€éƒ¨æ¨¡å¼ã€‚</li>
<li>å°†å±€éƒ¨ç‰¹å¾å¯¹é½å…¬å¼åŒ–ä¸ºæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºå±€éƒ¨èšç±»æœ€ä¼˜ä¼ è¾“æŸå¤±ã€‚</li>
<li>åŠ¨æ€é›†æˆæ¨¡å‹åŠ æƒç­–ç•¥èƒ½è‡ªé€‚åº”å¹³è¡¡å¤šä¸ªæ¨¡å‹çš„å½±å“ï¼Œè¿›ä¸€æ­¥æé«˜è¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a146e19e730eb612d3c07ee9b9a2d757.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f93cc9714d49b25f11bff1d7e523d78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3da8e6b47caa53b44a1413b3056ca8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reinforcing-General-Reasoning-without-Verifiers"><a href="#Reinforcing-General-Reasoning-without-Verifiers" class="headerlink" title="Reinforcing General Reasoning without Verifiers"></a>Reinforcing General Reasoning without Verifiers</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du</strong></p>
<p>The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree">https://github.com/sail-sg/VeriFree</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé€šè¿‡ä½¿ç”¨DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŒƒå¼è½¬å˜åœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»…é™äºåŸºäºè§„åˆ™çš„ç­”æ¡ˆéªŒè¯å¯è¡Œçš„ä»»åŠ¡ï¼Œå¹¶ä¸èƒ½è‡ªç„¶åœ°æ‰©å±•åˆ°ç°å®ä¸–ç•Œé¢†åŸŸï¼Œå¦‚åŒ–å­¦ã€åŒ»ç–—ã€å·¥ç¨‹ã€æ³•å¾‹ã€ç”Ÿç‰©ã€å•†ä¸šå’Œç»æµã€‚ç›®å‰çš„å®ç”¨è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨é¢å¤–çš„LLMä½œä¸ºåŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œä½†è¿™å¸¦æ¥äº†å¯¹å¼ºå¤§çš„éªŒè¯å™¨LLMçš„ä¾èµ–ã€å¥–åŠ±ä½œå¼Šçš„è„†å¼±æ€§ä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­åœ¨å†…å­˜ä¸­ç»´æŠ¤éªŒè¯å™¨æ¨¡å‹çš„å®è·µè´Ÿæ‹…ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜å¹¶å°†DeepSeek-R1-Zeroé£æ ¼çš„è®­ç»ƒæ‰©å±•åˆ°ä¸€èˆ¬æ¨ç†é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œå®ƒç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œè€Œæ˜¯ä½¿ç”¨RLç›´æ¥æœ€å¤§åŒ–ç”Ÿæˆå‚è€ƒç­”æ¡ˆçš„æ¦‚ç‡ã€‚æˆ‘ä»¬å°†VeriFreeä¸åŸºäºéªŒè¯å™¨çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯æ˜å…¶åœ¨å·¨å¤§çš„å®ç”¨æ€§å¥½å¤„å’Œå‡å°‘çš„è®¡ç®—éœ€æ±‚ä¹‹å¤–ï¼Œåœ¨MMLU-Proã€GPQAã€SuperGPQAå’Œæ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ä¸­ï¼ŒVeriFreeä¸åŸºäºéªŒè¯å™¨çš„æ–¹æ³•ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šå®ƒä»¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å¯¹è¿™ç§æ–¹æ³•æä¾›äº†è§è§£ï¼šä½œä¸ºå°†ç­–ç•¥å’Œéšå¼éªŒè¯å™¨ä¼˜é›…åœ°é›†æˆåœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œä»¥åŠä½œä¸ºä¸€ç§å˜åˆ†ä¼˜åŒ–æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/VeriFreeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºDeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ­¤æ–¹æ³•ä»…é™äºè§„åˆ™å¯éªŒè¯çš„ä»»åŠ¡ï¼Œéš¾ä»¥è‡ªç„¶æ‰©å±•åˆ°åŒ–å­¦ã€åŒ»ç–—ã€å·¥ç¨‹ã€æ³•å¾‹ã€ç”Ÿç‰©ã€å•†ä¸šå’Œç»æµç­‰çœŸå®ä¸–ç•Œé¢†åŸŸã€‚ä¸ºè§£å†³æ­¤é—®é¢˜å¹¶æ‰©å±•DeepSeek-R1-Zeroé£æ ¼çš„è®­ç»ƒè‡³é€šç”¨æ¨ç†é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œè¯¥æ–¹æ³•ç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œè½¬è€Œä½¿ç”¨RLç›´æ¥æœ€å¤§åŒ–ç”Ÿæˆå‚è€ƒç­”æ¡ˆçš„æ¦‚ç‡ã€‚å¯¹æ¯”éªŒè¯å™¨æ–¹æ³•ï¼ŒVeriFreeå…·æœ‰æ˜¾è‘—çš„å®é™…æ•ˆç›Šå’Œæ›´ä½çš„è®¡ç®—è¦æ±‚ï¼Œå¹¶åœ¨MMLU-Proã€GPQAã€SuperGPQAå’Œæ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ åœ¨LLMè®­ç»ƒä¸­æ¨åŠ¨äº†ä»£ç å’Œæ•°å­¦æ¨ç†çš„è¿›æ­¥ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å±€é™äºè§„åˆ™å¯éªŒè¯çš„ä»»åŠ¡ï¼Œéš¾ä»¥åº”ç”¨äºçœŸå®ä¸–ç•Œé¢†åŸŸã€‚</li>
<li>VeriFreeæ–¹æ³•ç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥æœ€å¤§åŒ–ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ã€‚</li>
<li>VeriFreeä¸éªŒè¯å™¨æ–¹æ³•ç›¸æ¯”å…·æœ‰å®é™…æ•ˆç›Šå’Œæ›´ä½çš„è®¡ç®—è¦æ±‚ã€‚</li>
<li>VeriFreeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>VeriFreeä½œä¸ºä¸€ç§å°†ç­–ç•¥å’Œéšå¼éªŒè¯å™¨ä¼˜é›…åœ°é›†æˆåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œå…·æœ‰å¤šç§ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abd1833621d3fc574f509bb61a4501f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d03a7ead67cfc16078c3ed181410e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f79b9549ae46abbbebf4c12cb745df2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners"><a href="#Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners" class="headerlink" title="Are Language Models Consequentialist or Deontological Moral Reasoners?"></a>Are Language Models Consequentialist or Deontological Moral Reasoners?</h2><p><strong>Authors:Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard SchÃ¶lkopf, Zhijing Jin</strong></p>
<p>As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens">https://github.com/keenansamway/moral-lens</a> . </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åŒ»ç–—ä¿å¥ã€æ³•å¾‹å’Œæ²»ç†ç­‰é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œç†è§£å®ƒä»¬å¦‚ä½•å¤„ç†é“å¾·ä¸Šå¤æ‚çš„åœºæ™¯å˜å¾—è‡³å…³é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“å¾·åˆ¤æ–­ä¸Šï¼Œè€Œå¿½è§†äº†å…¶æ½œåœ¨çš„é“å¾·æ¨ç†è¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¯¹LLMæä¾›çš„é“å¾·æ¨ç†è½¨è¿¹çš„å¤§è§„æ¨¡åˆ†æã€‚æ­¤å¤–ï¼Œä¸åŒäºä¹‹å‰çš„ç ”ç©¶ä»…ä»å°‘æ•°é“å¾·å›°å¢ƒä¸­æ¨æ–­ç»“æœï¼Œæˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨è¶…è¿‡600ä¸ªä¸åŒçš„ç”µè½¦é—®é¢˜ä½œä¸ºæ¢é’ˆï¼Œä»¥æ­ç¤ºä¸åŒLLMå†…éƒ¨å‡ºç°çš„æ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬ä»‹ç»å¹¶æµ‹è¯•äº†ä¸€ç§é“å¾·ç†ç”±çš„åˆ†ç±»æ³•ï¼Œæ ¹æ®ä¸¤ç§ä¸»è¦çš„è§„èŒƒæ€§ä¼¦ç†ç†è®ºï¼šç»“æœè®ºå’Œä¹‰åŠ¡è®ºï¼Œç³»ç»Ÿåœ°åˆ†ç±»æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMçš„æ€ç»´é“¾å¾€å¾€å€¾å‘äºåŸºäºé“å¾·ä¹‰åŠ¡çš„ä¹‰åŠ¡è®ºåŸåˆ™ï¼Œè€ŒåæœŸçš„è§£é‡Šåˆ™æ˜¾è‘—è½¬å‘å¼ºè°ƒå®ç”¨æ€§çš„ç»“æœä¸»ä¹‰ç†ç”±ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºç†è§£LLMå¦‚ä½•å¤„ç†å¹¶é˜è¿°é“å¾·è€ƒé‡æä¾›äº†åŸºç¡€ï¼Œè¿™æ˜¯å°†LLMå®‰å…¨ã€å¯è§£é‡Šåœ°éƒ¨ç½²äºé«˜é£é™©å†³ç­–åˆ¶å®šç¯å¢ƒä¸­çš„å…³é”®ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/keenansamway/moral-lensè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é“å¾·æ¨ç†æ—¶çš„å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶é€šè¿‡å¯¹è¶…è¿‡600ä¸ªä¸åŒçš„ç”µè½¦é—®é¢˜è¿›è¡Œåˆ†æï¼Œå‘ç°LLMçš„æ¨ç†é“¾å€¾å‘äºåŸºäºé“å¾·ä¹‰åŠ¡çš„å¾·æ€§è®ºåŸåˆ™ï¼Œè€ŒåæœŸçš„è§£é‡Šåˆ™æ˜æ˜¾è½¬å‘å¼ºè°ƒå®ç”¨ä¸»ä¹‰çš„ç»“æœè®ºç†ç”±ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£LLMå¦‚ä½•å¤„ç†é“å¾·è€ƒé‡æä¾›äº†åŸºç¡€ï¼Œå¯¹äºåœ¨å®‰å…¨ä¸”å¯è§£é‡Šçš„ç¯å¢ƒä¸­éƒ¨ç½²LLMè¿›è¡Œé«˜é£é™©å†³ç­–åˆ¶å®šå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼¦ç†å¤æ‚åœºæ™¯ä¸­çš„å¤„ç†ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—ä¿å¥ã€æ³•å¾‹å’Œæ²»ç†ç­‰é¢†åŸŸã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨LLMçš„é“å¾·åˆ¤æ–­ï¼Œè€Œæœ¬ç ”ç©¶åˆ™ç€é‡äºåˆ†æå…¶é“å¾·æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡åˆ†æè¶…è¿‡600ä¸ªä¸åŒçš„ç”µè½¦é—®é¢˜ï¼Œå‘ç°LLMçš„æ¨ç†é“¾å€¾å‘äºå¾·æ€§è®ºåŸåˆ™ã€‚</li>
<li>LLMçš„åæœŸè§£é‡Šåå‘ç»“æœè®ºç†ç”±ï¼Œå¼ºè°ƒå®ç”¨æ€§ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸€ä¸ªåˆ†ç±»é“å¾·æ¨ç†ç—•è¿¹çš„æ¡†æ¶ï¼Œæ ¹æ®ä¸¤ç§ä¸»è¦çš„ä¼¦ç†ç†è®ºï¼šç»“æœè®ºå’Œå¾·æ€§è®ºè¿›è¡Œç³»ç»Ÿåˆ†ç±»ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç†è§£LLMå¦‚ä½•å¤„ç†é“å¾·è€ƒé‡å¥ å®šäº†åŸºç¡€ï¼Œæ˜¯å‘å®‰å…¨ä¸”å¯è§£é‡Šçš„é«˜é£é™©å†³ç­–åˆ¶å®šç¯å¢ƒéƒ¨ç½²LLMçš„é‡è¦æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81c82638e8b82dcc7462f7876531149e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5310537ab54ab5b6f5dc826993a05f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a3141b963ab23c4f36dc44d93b7b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd5bc52df8235fdfb5574679b7d8270.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Policy-Optimized-Text-to-Image-Pipeline-Design"><a href="#Policy-Optimized-Text-to-Image-Pipeline-Design" class="headerlink" title="Policy Optimized Text-to-Image Pipeline Design"></a>Policy Optimized Text-to-Image Pipeline Design</h2><p><strong>Authors:Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor</strong></p>
<p>Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯å·²ç»å‘å±•è¶…è¶Šäº†å•ä¸€çš„å•ä½“æ¨¡å‹ï¼Œå˜ä¸ºäº†å¤æ‚çš„å¤šç»„ä»¶ç®¡é“ã€‚è¿™äº›ç®¡é“ç»“åˆäº†ç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆå™¨ã€é€‚é…å™¨ã€ä¸Šé‡‡æ ·å—ç”šè‡³ç¼–è¾‘æ­¥éª¤ï¼Œå¯¼è‡´å›¾åƒè´¨é‡å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æœ‰æ•ˆè®¾è®¡éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘çš„æ–¹æ³•æ˜¾ç¤ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹çš„å¸Œæœ›ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ï¼šä»ä½¿ç”¨æ•°ç™¾ä¸ªé¢„å®šä¹‰ç®¡é“ç”Ÿæˆå›¾åƒæ‰€éœ€çš„å·¨å¤§è®¡ç®—é‡ï¼Œä»¥åŠåœ¨è®°å¿†è®­ç»ƒæ ·æœ¬ä¹‹å¤–çš„æ¨å¹¿èƒ½åŠ›è¾ƒå·®ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°å‹æ¡†æ¶æ¥è§£å†³è¿™äº›æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ç»„å¥–åŠ±æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»æç¤º-å·¥ä½œæµç¨‹ç»„åˆä¸­é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†æ˜‚è´µçš„å›¾åƒç”Ÿæˆéœ€æ±‚ã€‚ç„¶åæˆ‘ä»¬å®ç°äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹çš„å·¥ä½œæµè¯æ±‡è®­ç»ƒï¼Œç„¶åæ˜¯åŸºäºGRPOçš„ä¼˜åŒ–ï¼Œå¼•å¯¼æ¨¡å‹æœç€æ€§èƒ½æ›´é«˜çš„å·¥ä½œæµç©ºé—´åŒºåŸŸå‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼çš„æå‡æŠ€æœ¯ï¼Œæ²¿ç€åˆå§‹æ¨¡å‹å’ŒGRPOè°ƒæ•´æ¨¡å‹ä¹‹é—´çš„è·¯å¾„è¿›è¡Œæ¨æ¼”ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¾“å‡ºè´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—çš„å¯¹æ¯”éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜å®ƒå¯ä»¥æˆåŠŸåˆ›å»ºå…·æœ‰æ›´é«˜å¤šæ ·æ€§çš„æ–°æµç¨‹ï¼Œå¹¶äº§ç”Ÿä¼˜äºç°æœ‰åŸºå‡†çº¿çš„å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21478v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¦æ¨¡èªè¨€æ¨¡å‹è¾…åŠ©çš„æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚é’ˆå¯¹å½“å‰å¤šç»„ä»¶ç®¡é“è®¾è®¡çš„éœ€æ±‚ä¸“ä¸šåŒ–é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¥–åŠ±æ¨¡å‹æ¥è§£å†³ç°æœ‰æ¨¡å‹ç”Ÿæˆç®¡é“æ•ˆç‡ä½çš„é—®é¢˜ã€‚ç›´æ¥åˆ©ç”¨å¥–åŠ±æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œå‡å°‘è®­ç»ƒä¸­çš„å›¾åƒç”Ÿæˆæˆæœ¬ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¼•å…¥æ— åˆ†ç±»å™¨æŒ‡å¯¼å¢å¼ºæŠ€æœ¯ï¼Œæé«˜è¾“å‡ºè´¨é‡ã€‚ç›¸è¾ƒäºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºæ›´å…·å¤šæ ·æ€§å’Œä¼˜è´¨æ€§çš„å›¾åƒç”Ÿæˆæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆå·²å‘å±•è‡³å¤æ‚çš„å¤šç»„ä»¶ç®¡é“ç³»ç»Ÿï¼Œç»“åˆç²¾ç»†è°ƒæ•´ç”Ÿæˆå™¨ã€é€‚é…å™¨ã€ä¸Šé‡‡æ ·å—å’Œç¼–è¾‘æ­¥éª¤ï¼Œæ˜¾è‘—æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–è®¾è®¡æµç¨‹å­˜åœ¨è®¡ç®—éœ€æ±‚å¤§åŠæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„ä¸¤å¤§å±€é™ã€‚</li>
<li>å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œæœ‰æ•ˆé™ä½æˆæœ¬å’Œè®¡ç®—éœ€æ±‚ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥åŒ…å«åˆå§‹å·¥ä½œæµç¨‹è¯æ±‡è®­ç»ƒå’ŒåŸºäºGRPOçš„ä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼çš„å¢å¼ºæŠ€æœ¯æå‡è¾“å‡ºè´¨é‡ã€‚</li>
<li>ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºæ›´åŠ å¤šæ ·åŒ–å’Œä¼˜è´¨åŒ–çš„å›¾åƒç”Ÿæˆæµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7493c31768c123ea46735d176ea4d438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08509a719874ee7319d6eaaa84ad7682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ace6180579b47e9f0e9f74d668599c7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model"><a href="#Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model" class="headerlink" title="Hume: Introducing System-2 Thinking in Visual-Language-Action Model"></a>Hume: Introducing System-2 Thinking in Visual-Language-Action Model</h2><p><strong>Authors:Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li</strong></p>
<p>Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments. </p>
<blockquote>
<p>åœ¨å¤„ç†ç‰©ç†ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œäººç±»ä¼šåœ¨å®é™…è¡ŒåŠ¨ä¹‹å‰è¿›è¡Œç¼“æ…¢æ€è€ƒã€‚æœ€è¿‘ï¼Œè¿™ç§æ€è€ƒèŒƒå¼åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­—é¢†åŸŸè§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç¼“æ…¢æ€è€ƒåœ¨æœºå™¨äººåŸºç¡€æ¨¡å‹ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Humeï¼šä¸€ç§å…·æœ‰ä»·å€¼å¼•å¯¼çš„ç³»ç»Ÿ2æ€è€ƒçº§è”åŠ¨ä½œå»å™ªçš„åŒç³»ç»Ÿè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ¢ç´¢è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åœ¨çµå·§æœºå™¨äººæ§åˆ¶æ–¹é¢çš„äººç±»æ€è€ƒèƒ½åŠ›ã€‚Humeçš„ç³»ç»Ÿ2é€šè¿‡æ‰©å±•è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„éª¨å¹²ç½‘ç»œï¼Œé‡‡ç”¨æ–°å‹çš„ä»·å€¼æŸ¥è¯¢å¤´æ¥ä¼°è®¡é¢„æµ‹åŠ¨ä½œçš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œä»è€Œå®ç°ä»·å€¼å¼•å¯¼çš„æ€è€ƒã€‚ä»·å€¼å¼•å¯¼çš„æ€è€ƒæ˜¯é€šè¿‡å¤šæ¬¡é‡‡æ ·å¤šä¸ªåŠ¨ä½œå€™é€‰è€…ï¼Œå¹¶æ ¹æ®çŠ¶æ€-åŠ¨ä½œä»·å€¼é€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ¥è¿›è¡Œã€‚Humeçš„ç³»ç»Ÿ1æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ååº”å‹è§†è¿åŠ¨ç­–ç•¥ï¼Œå®ƒæ¥æ”¶ç³»ç»Ÿ2é€‰æ‹©çš„åŠ¨ä½œï¼Œå¹¶æ‰§è¡Œçº§è”åŠ¨ä½œå»å™ªï¼Œä»¥å®ç°çµå·§çš„æœºå™¨äººæ§åˆ¶ã€‚åœ¨éƒ¨ç½²æ—¶ï¼Œç³»ç»Ÿ2ä»¥è¾ƒä½é¢‘ç‡è¿›è¡Œä»·å€¼å¼•å¯¼æ€è€ƒï¼Œè€Œç³»ç»Ÿ1åˆ™å¼‚æ­¥æ¥æ”¶ç³»ç»Ÿ2é€‰æ‹©çš„åŠ¨ä½œå€™é€‰è€…ï¼Œå¹¶å®æ—¶é¢„æµ‹æµç•…çš„åŠ¨ä½œã€‚æˆ‘ä»¬å±•ç¤ºï¼ŒHumeåœ¨å¤šä¸ªä»¿çœŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21432v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨ç‰©ç†ä¸–ç•Œä¸­å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œäººç±»åœ¨å®è·µè¡ŒåŠ¨å‰ä¼šè¿›è¡Œæ…¢æ€è€ƒçš„æ¨¡å¼ã€‚è¿‘æœŸï¼Œè¿™ç§æ€è€ƒæ¨¡å¼åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³æ•°å­—é¢†åŸŸçš„å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹äºæœºå™¨äººåŸºç¡€æ¨¡å‹ä¸ç‰©ç†ä¸–ç•Œçš„äº¤äº’è€Œè¨€ï¼Œæ…¢æ€è€ƒçš„å·¨å¤§æ½œåŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†Humeï¼šä¸€ç§å…·æœ‰ä»·å€¼å¼•å¯¼çš„ç³»ç»Ÿ2æ€ç»´çš„åŒç³»ç»Ÿè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ¢ç´¢è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„ç±»äººæ€è€ƒèƒ½åŠ›åœ¨çµå·§æœºå™¨äººæ§åˆ¶æ–¹é¢çš„åº”ç”¨ã€‚Humeçš„ç³»ç»Ÿ2é€šè¿‡å¢åŠ ä¸€ä¸ªä»·å€¼æŸ¥è¯¢å¤´æ¥ä¼°è®¡é¢„æµ‹åŠ¨ä½œçš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œä»è€Œå®ç°ä»·å€¼å¼•å¯¼çš„æ€è€ƒã€‚åœ¨éƒ¨ç½²æ—¶ï¼Œç³»ç»Ÿ2ä»¥è¾ƒä½é¢‘ç‡è¿›è¡Œä»·å€¼å¼•å¯¼çš„æ€è€ƒï¼Œè€Œç³»ç»Ÿ1åˆ™å¼‚æ­¥æ¥æ”¶ç³»ç»Ÿ2é€‰æ‹©çš„åŠ¨ä½œå€™é€‰è€…å¹¶å®æ—¶é¢„æµ‹æµç•…åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼ŒHumeåœ¨å¤šä¸ªä»¿çœŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººéƒ¨ç½²ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åœ¨å¤„ç†ç‰©ç†ä¸–ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œä¼šå…ˆè¿›è¡Œæ…¢æ€è€ƒå†è¡ŒåŠ¨ï¼Œè¿™ä¸€æ¨¡å¼æœ€è¿‘åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¾—åˆ°äº†æå‡ã€‚</li>
<li>è™½ç„¶æ…¢æ€è€ƒåœ¨æ•°å­—é¢†åŸŸçš„å¤æ‚ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨æœºå™¨äººåŸºç¡€æ¨¡å‹ä¸ç‰©ç†ä¸–ç•Œçš„äº¤äº’ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚</li>
<li>Humeæ˜¯ä¸€ä¸ªå…·æœ‰ä»·å€¼å¼•å¯¼çš„ç³»ç»Ÿ2æ€ç»´çš„åŒç³»ç»Ÿè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨æ¢ç´¢è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„ç±»äººæ€è€ƒèƒ½åŠ›åœ¨çµå·§æœºå™¨äººæ§åˆ¶æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>Humeçš„ç³»ç»Ÿ2é€šè¿‡ä»·å€¼æŸ¥è¯¢å¤´æ¥ä¼°è®¡é¢„æµ‹åŠ¨ä½œçš„çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œå®ç°ä»·å€¼å¼•å¯¼çš„æ€è€ƒæ¨¡å¼ã€‚</li>
<li>ç³»ç»Ÿ1æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ååº”è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œå®ƒæ¥æ”¶ç³»ç»Ÿ2é€‰æ‹©çš„åŠ¨ä½œå¹¶è¿›è¡Œçº§è”åŠ¨ä½œå»å™ªï¼Œä»¥å®ç°çµå·§çš„æœºå™¨äººæ§åˆ¶ã€‚</li>
<li>åœ¨éƒ¨ç½²æ—¶ï¼Œç³»ç»Ÿ2ä»¥è¾ƒä½é¢‘ç‡è¿›è¡Œä»·å€¼å¼•å¯¼çš„æ€è€ƒï¼Œè€Œç³»ç»Ÿ1åˆ™èƒ½å®æ—¶é¢„æµ‹æµç•…åŠ¨ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8e6ba546a7424f8ddb85e55dc59626e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc2407ea840b86bc927803ac4a12c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350e445309cd68d9210fe3c424dc0167.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diagnosing-and-Resolving-Cloud-Platform-Instability-with-Multi-modal-RAG-LLMs"><a href="#Diagnosing-and-Resolving-Cloud-Platform-Instability-with-Multi-modal-RAG-LLMs" class="headerlink" title="Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG   LLMs"></a>Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG   LLMs</h2><p><strong>Authors:Yifan Wang, Kenneth P. Birman</strong></p>
<p>Todayâ€™s cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives. </p>
<blockquote>
<p>ä»Šå¤©çš„äº‘æ‰˜ç®¡åº”ç”¨ç¨‹åºå’ŒæœåŠ¡æ˜¯å¤æ‚çš„ç³»ç»Ÿï¼Œæ€§èƒ½å’ŒåŠŸèƒ½ä¸ç¨³å®šå¯èƒ½æœ‰æ•°åæˆ–æ•°ç™¾ä¸ªæ½œåœ¨çš„æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œé€šè¿‡ç»“åˆç°ä»£äººå·¥æ™ºèƒ½å·¥å…·çš„æ¨¡å¼åŒ¹é…èƒ½åŠ›å’Œè‡ªç„¶çš„å¤šæ¨¡æ€RAG LLMç•Œé¢ï¼Œå¯ä»¥ç®€åŒ–é—®é¢˜è¯†åˆ«å’Œè§£å†³æ–¹æ¡ˆã€‚ARCAæ˜¯ä¸€ä¸ªé’ˆå¯¹è¿™ä¸€é¢†åŸŸçš„æ–°å¤šæ¨¡æ€RAG LLMç³»ç»Ÿã€‚é€æ­¥è¯„ä¼°è¡¨æ˜ï¼ŒARCAä¼˜äºæœ€æ–°æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21419v1">PDF</a> Published in EuroMLSys2025</p>
<p><strong>Summary</strong>:<br>ç°ä»£äº‘åº”ç”¨å’ŒæœåŠ¡çš„å¤æ‚æ€§å¯¼è‡´äº†æ€§èƒ½å’ŒåŠŸèƒ½é—®é¢˜è¯Šæ–­å›°éš¾ã€‚ç ”ç©¶å‡è®¾åˆ©ç”¨ç°ä»£äººå·¥æ™ºèƒ½å·¥å…·çš„åŒ¹é…æ¨¡å¼å’Œè‡ªç„¶è¯­è¨€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å£ç›¸ç»“åˆï¼Œå¯ç®€åŒ–é—®é¢˜è¯†åˆ«å’Œè§£å†³æ–¹æ¡ˆã€‚ARCAæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€LLMç³»ç»Ÿï¼Œä¸“é—¨ä¸ºæ­¤é¢†åŸŸè®¾è®¡ã€‚é€æ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒARCAä¼˜äºç°æœ‰æœ€ä½³æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>äº‘åº”ç”¨å’ŒæœåŠ¡çš„å¤æ‚æ€§å¯¼è‡´æ€§èƒ½æˆ–åŠŸèƒ½é—®é¢˜è¯Šæ–­å›°éš¾ã€‚</li>
<li>ç»“åˆäººå·¥æ™ºèƒ½å·¥å…·çš„åŒ¹é…æ¨¡å¼å’Œè‡ªç„¶è¯­è¨€å¤šæ¨¡æ€LLMæ¥å£å¯ä»¥ç®€åŒ–é—®é¢˜è¯†åˆ«å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ARCAæ˜¯ä¸€ç§é’ˆå¯¹è¿™ä¸€é¢†åŸŸçš„æ–°å‹å¤šæ¨¡æ€LLMç³»ç»Ÿã€‚</li>
<li>ARCAé€šè¿‡é€æ­¥è¯„ä¼°è¯æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰æœ€ä½³æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ARCAç³»ç»Ÿåˆ©ç”¨æ¨¡å¼åŒ¹é…èƒ½åŠ›è¿›è¡Œé—®é¢˜è¯†åˆ«ã€‚</li>
<li>ARCAé‡‡ç”¨çš„è‡ªç„¶è¯­è¨€å¤šæ¨¡æ€æ¥å£æœ‰åŠ©äºæ›´ç®€å•åœ°è§£å†³é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f10ef6aaa8a0d33ee9f738ae791e76c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df5d05eef47f9f1a633797ddff6b412a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation"><a href="#RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation" class="headerlink" title="RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"></a>RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</h2><p><strong>Authors:Xiao Liu, Da Yin, Zirui Wu, Yansong Feng</strong></p>
<p>Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the modelsâ€™ internal knowledge and would fail in domains beyond the LLMsâ€™ knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning. </p>
<blockquote>
<p>å·¥å…·å¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½æœ‰å¯ç”¨çš„å·¥å…·ã€‚åœ¨æ²¡æœ‰é¢„è®¾å·¥å…·çš„æƒ…å†µä¸‹ï¼Œæ—©æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†æŒ‡å¯¼LLMè‡ªè¡Œç”Ÿæˆå·¥å…·çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶ä¸”åœ¨LLMçŸ¥è¯†èŒƒå›´ä¹‹å¤–çš„é¢†åŸŸä¼šå¤±æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RefToolï¼Œè¿™æ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼çš„è‡ªåŠ¨å·¥å…·åˆ›å»ºæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»“æ„åŒ–å¤–éƒ¨ææ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰ã€‚RefToolç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šï¼ˆ1ï¼‰å·¥å…·åˆ›å»ºï¼Œå…¶ä¸­LLMæ ¹æ®å‚è€ƒå†…å®¹ç”Ÿæˆå¯æ‰§è¡Œå·¥å…·ï¼Œä½¿ç”¨ç¤ºä¾‹è¿›è¡ŒéªŒè¯ï¼Œå¹¶æŒ‰å±‚æ¬¡ç»“æ„å°†å®ƒä»¬ç»„ç»‡åˆ°å·¥å…·ç®±ä¸­ï¼›ï¼ˆ2ï¼‰å·¥å…·åˆ©ç”¨ï¼Œå…¶ä¸­LLMæµè§ˆå·¥å…·ç®±ç»“æ„ï¼Œé€‰æ‹©å¹¶åº”ç”¨é€‚å½“çš„å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚åœ¨å› æœã€ç‰©ç†å’ŒåŒ–å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRefToolåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰çš„å·¥å…·åˆ›å»ºå’Œé¢†åŸŸç‰¹å®šæ¨ç†æ–¹æ³•é«˜å‡º11.3%ï¼ŒåŒæ—¶æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯å¹¿æ³›æ¨å¹¿ã€‚åˆ†æè¡¨æ˜ï¼Œä»¥å‚è€ƒä¸ºåŸºç¡€çš„å·¥å…·åˆ›å»ºäº§ç”Ÿäº†å‡†ç¡®å’Œå¿ è¯šçš„å·¥å…·ï¼Œåˆ†å±‚ç»“æ„æœ‰åŠ©äºæœ‰æ•ˆçš„å·¥å…·é€‰æ‹©ã€‚RefToolä½¿LLMèƒ½å¤Ÿå…‹æœçŸ¥è¯†å±€é™æ€§ï¼Œè¯æ˜äº†å°†å·¥å…·åˆ›å»ºä»¥å¤–éƒ¨å‚è€ƒä¸ºåŸºç¡€å¯¹äºå¢å¼ºå’Œé€šç”¨æ¨ç†çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21413v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/xxxiaol/RefTool">https://github.com/xxxiaol/RefTool</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›å¢å¼ºé—®é¢˜ã€‚å½“æ²¡æœ‰å¯ç”¨çš„é¢„è®¾å·¥å…·æ—¶ï¼ŒLLMéœ€è‡ªè¡Œç”Ÿæˆå·¥å…·ï¼Œä½†è¿™ç§æ–¹æ³•å—é™äºæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œå¯¹äºè¶…å‡ºå…¶çŸ¥è¯†èŒƒç•´çš„é¢†åŸŸå°†å¤±æ•ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†RefToolæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–å¤–éƒ¨èµ„æ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰è¿›è¡Œè‡ªåŠ¨å·¥å…·åˆ›å»ºã€‚RefToolåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå·¥å…·åˆ›å»ºå’Œå·¥å…·åˆ©ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒRefToolåœ¨å› æœã€ç‰©ç†å’ŒåŒ–å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®åº¦æ¯”ç°æœ‰å·¥å…·åˆ›å»ºå’Œé¢†åŸŸç‰¹å®šæ¨ç†æ–¹æ³•é«˜å‡º11.3%ï¼ŒåŒæ—¶æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯å¹¿æ³›é€šç”¨ã€‚è¿™è¡¨æ˜å°†å·¥å…·åˆ›å»ºæ ¹æ¤äºå¤–éƒ¨å‚è€ƒä¸­å¯¹äºå¢å¼ºå’Œé€šç”¨æ¨ç†çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚é—®é¢˜æ±‚è§£ä¸­å¯é€šè¿‡å·¥å…·å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨æ²¡æœ‰é¢„è®¾å·¥å…·çš„æƒ…å†µä¸‹ï¼ŒLLMå¯è‡ªè¡Œç”Ÿæˆå·¥å…·ï¼Œä½†å—é™äºå…¶å†…éƒ¨çŸ¥è¯†ã€‚</li>
<li>RefToolæ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–å¤–éƒ¨èµ„æ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰è¿›è¡Œè‡ªåŠ¨å·¥å…·åˆ›å»ºã€‚</li>
<li>RefToolåŒ…å«å·¥å…·åˆ›å»ºå’Œå·¥å…·åˆ©ç”¨ä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒRefToolåœ¨å¤šä¸ªé¢†åŸŸæµ‹è¯•ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>RefToolé€šè¿‡æ ¹æ¤äºå¤–éƒ¨å‚è€ƒçš„å·¥å…·åˆ›å»ºï¼Œäº§ç”Ÿå‡†ç¡®ä¸”å¿ å®çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-565ff994cf9e3608f1a45c7d54192219.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad7bcf12572c4bec174366f2c6639fa5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ff673cc55f6d6d92eadb47d9967ed88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce15a60ee0afeef1a17e1de7fe7165d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EquAct-An-SE-3-Equivariant-Multi-Task-Transformer-for-Open-Loop-Robotic-Manipulation"><a href="#EquAct-An-SE-3-Equivariant-Multi-Task-Transformer-for-Open-Loop-Robotic-Manipulation" class="headerlink" title="EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop   Robotic Manipulation"></a>EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop   Robotic Manipulation</h2><p><strong>Authors:Xupeng Zhu, Yu Qi, Yizhe Zhu, Robin Walters, Robert Platt</strong></p>
<p>Transformer architectures can effectively learn language-conditioned, multi-task 3D open-loop manipulation policies from demonstrations by jointly processing natural language instructions and 3D observations. However, although both the robot policy and language instructions inherently encode rich 3D geometric structures, standard transformers lack built-in guarantees of geometric consistency, often resulting in unpredictable behavior under SE(3) transformations of the scene. In this paper, we leverage SE(3) equivariance as a key structural property shared by both policy and language, and propose EquAct-a novel SE(3)-equivariant multi-task transformer. EquAct is theoretically guaranteed to be SE(3) equivariant and consists of two key components: (1) an efficient SE(3)-equivariant point cloud-based U-net with spherical Fourier features for policy reasoning, and (2) SE(3)-invariant Feature-wise Linear Modulation (iFiLM) layers for language conditioning. To evaluate its spatial generalization ability, we benchmark EquAct on 18 RLBench simulation tasks with both SE(3) and SE(2) scene perturbations, and on 4 physical tasks. EquAct performs state-of-the-art across these simulation and physical tasks. </p>
<blockquote>
<p>Transformeræ¶æ„å¯ä»¥é€šè¿‡è”åˆå¤„ç†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œ3Dè§‚å¯Ÿç»“æœï¼Œæœ‰æ•ˆåœ°ä»æ¼”ç¤ºä¸­å­¦ä¹ è¯­è¨€æ¡ä»¶åŒ–çš„å¤šä»»åŠ¡3Då¼€ç¯æ“ä½œç­–ç•¥ã€‚ç„¶è€Œï¼Œå°½ç®¡æœºå™¨äººç­–ç•¥å’Œè¯­è¨€æŒ‡ä»¤æœ¬èº«å°±ç¼–ç äº†ä¸°å¯Œçš„3Då‡ ä½•ç»“æ„ï¼Œä½†æ ‡å‡†Transformerç¼ºä¹å†…ç½®çš„å‡ ä½•ä¸€è‡´æ€§ä¿è¯ï¼Œé€šå¸¸åœ¨åœºæ™¯çš„SE(3)è½¬æ¢ä¸‹è¡¨ç°å‡ºä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨SE(3)ç­‰ä»·æ€§ä½œä¸ºç­–ç•¥å’Œè¯­è¨€çš„å…±åŒå…³é”®ç»“æ„å±æ€§ï¼Œå¹¶æå‡ºEquActâ€”â€”ä¸€ç§æ–°å‹çš„SE(3)ç­‰ä»·å¤šä»»åŠ¡Transformerã€‚EquActåœ¨ç†è®ºä¸Šä¿è¯äº†SE(3)çš„ç­‰ä»·æ€§ï¼Œå¹¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸€ç§é«˜æ•ˆçš„åŸºäºç‚¹äº‘çš„SE(3)ç­‰ä»·U-netç½‘ç»œï¼Œå…·æœ‰çƒå½¢å‚…é‡Œå¶ç‰¹å¾ç”¨äºç­–ç•¥æ¨ç†ï¼›ï¼ˆ2ï¼‰ç”¨äºè¯­è¨€è°ƒèŠ‚çš„SE(3)ä¸å˜ç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼ˆiFiLMï¼‰å±‚ã€‚ä¸ºäº†è¯„ä¼°å…¶ç©ºé—´æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨å¸¦æœ‰SE(3)å’ŒSE(2)åœºæ™¯æ‰°åŠ¨çš„18ä¸ªRLBenchä»¿çœŸä»»åŠ¡ä»¥åŠ4ä¸ªå®é™…ä»»åŠ¡ä¸Šæµ‹è¯•äº†EquActçš„æ€§èƒ½ã€‚åœ¨ä»¿çœŸå’Œå®é™…ä»»åŠ¡ä¸­ï¼ŒEquActå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21351v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ¶æ„åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œä¸‰ç»´è§‚å¯Ÿè”åˆå¤„ç†æ¥å­¦ä¹ è¯­è¨€æ¡ä»¶ä¸‹çš„å¤šä»»åŠ¡ä¸‰ç»´å¼€æ”¾å¾ªç¯æ“ä½œç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æœºå™¨äººç­–ç•¥å’Œè¯­è¨€æŒ‡ä»¤æœ¬èº«å°±åŒ…å«äº†ä¸°å¯Œçš„ä¸‰ç»´å‡ ä½•ç»“æ„ï¼Œä½†æ ‡å‡†Transformerç¼ºä¹å†…ç½®å‡ ä½•ä¸€è‡´æ€§ä¿è¯ï¼Œè¿™ä¼šå¯¼è‡´åœ¨SE(3)åœºæ™¯å˜æ¢ä¸‹è¡Œä¸ºä¸å¯é¢„æµ‹ã€‚å› æ­¤ï¼Œæœ¬æ–‡åˆ©ç”¨SE(3)ç­‰ä»·æ€§ä½œä¸ºç­–ç•¥å’Œè¯­è¨€çš„å…±äº«å…³é”®ç»“æ„ç‰¹æ€§ï¼Œæå‡ºä¸€ç§æ–°å‹çš„SE(3)ç­‰ä»·å¤šä»»åŠ¡Transformeræ¨¡å‹EquActã€‚EquActåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä¸€æ˜¯ç”¨äºç­–ç•¥æ¨ç†çš„åŸºäºç‚¹äº‘çš„SE(3)ç­‰ä»·U-netï¼ŒäºŒæ˜¯ç”¨äºè¯­è¨€è°ƒèŠ‚çš„SE(3)ä¸å˜ç‰¹å¾çº¿æ€§è°ƒåˆ¶å±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒEquActåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸Šå…·å¤‡å“è¶Šçš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ï¼Œæ— è®ºæ˜¯åœ¨SE(3)è¿˜æ˜¯SE(2)åœºæ™¯æ‰°åŠ¨ä¸‹å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Transformeræ¶æ„èƒ½é€šè¿‡å­¦ä¹ è¯­è¨€æ¡ä»¶ä¸‹çš„å¤šä»»åŠ¡ä¸‰ç»´æ“ä½œç­–ç•¥æ¥å¤„ç†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œä¸‰ç»´è§‚å¯Ÿè”åˆæ•°æ®ã€‚</li>
<li>å°½ç®¡Transformeræœ‰èƒ½åŠ›å¤„ç†ä¸°å¯Œä¿¡æ¯ï¼Œä½†å®ƒä»¬ç¼ºä¹å†…ç½®å‡ ä½•ä¸€è‡´æ€§ä¿è¯ã€‚</li>
<li>SE(3)ç­‰ä»·æ€§åœ¨æœºå™¨äººæ“ä½œå’ŒæŒ‡ä»¤ä¸­éƒ½å­˜åœ¨ä¸”æ˜¯é‡è¦çš„ã€‚</li>
<li>EquActæ˜¯ä¸€ä¸ªæ–°å‹SE(3)ç­‰ä»·å¤šä»»åŠ¡Transformeræ¨¡å‹ï¼Œç»“åˆäº†åŸºäºç‚¹äº‘çš„U-netå’Œç‰¹å¾çº¿æ€§è°ƒåˆ¶å±‚æ¥ç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>EquActå…·å¤‡å‡ºè‰²çš„ç©ºé—´æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸­é¢„æµ‹å‡†ç¡®çš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e64aca19b524ea3acb908e64aef572f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d036b21493a4d4422919d26a39b0b0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9926641681499b5d6754d371fe491c5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b2e3fad42898c6fb16b1d76781d7176.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DynamicVL-Benchmarking-Multimodal-Large-Language-Models-for-Dynamic-City-Understanding"><a href="#DynamicVL-Benchmarking-Multimodal-Large-Language-Models-for-Dynamic-City-Understanding" class="headerlink" title="DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic   City Understanding"></a>DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic   City Understanding</h2><p><strong>Authors:Weihao Xuan, Junjue Wang, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, Naoto Yokoya</strong></p>
<p>Multimodal large language models have demonstrated remarkable capabilities in visual understanding, but their application to long-term Earth observation analysis remains limited, primarily focusing on single-temporal or bi-temporal imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework for analyzing long-term urban dynamics through remote sensing imagery. Our suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42 megacities in the U.S. from 2005 to 2023, organized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding tasks, from fundamental change detection (pixel-level) to quantitative analyses (regional-level) and comprehensive urban narratives (scene-level), capturing diverse urban dynamics including expansion&#x2F;transformation patterns, disaster assessment, and environmental challenges. We evaluate 17 state-of-the-art multimodal large language models and reveal their limitations in long-term temporal understanding and quantitative analysis. These challenges motivate the creation of DVL-Instruct, a specialized instruction-tuning dataset designed to enhance modelsâ€™ capabilities in multi-temporal Earth observation. Building upon this dataset, we develop DVLChat, a baseline model capable of both image-level question-answering and pixel-level segmentation, facilitating a comprehensive understanding of city dynamics through language interactions. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸåœ°çƒè§‚æµ‹åˆ†ææ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œä¸»è¦é›†ä¸­åœ¨å•æ—¶æ€æˆ–åŒæ—¶æ€å½±åƒä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DVL-Suiteï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡é¥æ„Ÿå½±åƒåˆ†æé•¿æœŸåŸå¸‚åŠ¨æ€çš„å…¨é¢æ¡†æ¶ã€‚æˆ‘ä»¬çš„å¥—ä»¶åŒ…å«2005å¹´è‡³2023å¹´æœŸé—´ç¾å›½42ä¸ªç‰¹å¤§åŸå¸‚çš„15,063å¼ é«˜åˆ†è¾¨ç‡ï¼ˆ1.0ç±³ï¼‰å¤šæ—¶ç›¸å›¾åƒï¼Œåˆ†ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šDVL-Benchå’ŒDVL-Instructã€‚DVL-BenchåŒ…å«ä¸ƒä¸ªåŸå¸‚ç†è§£ä»»åŠ¡ï¼Œä»åŸºæœ¬çš„æ£€æµ‹å˜åŒ–ï¼ˆåƒç´ çº§ï¼‰åˆ°å®šé‡åˆ†æï¼ˆåŒºåŸŸçº§ï¼‰å’Œç»¼åˆåŸå¸‚å™äº‹ï¼ˆåœºæ™¯çº§ï¼‰ï¼Œæ•æ‰åŒ…æ‹¬æ‰©å¼ &#x2F;è½¬å‹æ¨¡å¼ã€ç¾å®³è¯„ä¼°å’Œç¯å¢ƒæŒ‘æˆ˜åœ¨å†…çš„å„ç§åŸå¸‚åŠ¨æ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†17ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬åœ¨é•¿æœŸæ—¶é—´ç†è§£å’Œå®šé‡åˆ†ææ–¹é¢çš„å±€é™æ€§ã€‚è¿™äº›æŒ‘æˆ˜ä¿ƒä½¿æˆ‘ä»¬åˆ›å»ºäº†DVL-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹åœ¨å¤šæ—¶æ€åœ°çƒè§‚æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†DVLChatï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå›¾åƒçº§åˆ«çš„é—®ç­”å’Œåƒç´ çº§åˆ«çš„åˆ†å‰²ï¼Œé€šè¿‡è¯­è¨€äº¤äº’ä¿ƒè¿›å¯¹åŸå¸‚åŠ¨æ€çš„å…¨é¢ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21076v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†æé•¿æœŸåœ°çƒè§‚æµ‹æ•°æ®æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶é’ˆå¯¹è¿™ä¸€é—®é¢˜æå‡ºäº†DVL-Suiteæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ç”¨äºåˆ†æé•¿æœŸåŸå¸‚åŠ¨æ€çš„é«˜åˆ†è¾¨ç‡å¤šæ—¶æ€å›¾åƒï¼Œç”±DVL-Benchå’ŒDVL-Instructä¸¤éƒ¨åˆ†ç»„æˆã€‚å‰è€…æä¾›äº†ä¸€ç³»åˆ—åŸå¸‚ç†è§£ä»»åŠ¡ï¼Œåè€…åˆ™è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä»¥å¢å¼ºæ¨¡å‹åœ¨å¤šæ—¶æ€åœ°çƒè§‚æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œå¼€å‘å‡ºäº†èƒ½å¤Ÿè¿›è¡Œå›¾åƒçº§é—®ç­”å’Œåƒç´ çº§åˆ†å‰²çš„DVLChatåŸºçº¿æ¨¡å‹ï¼Œæœ‰åŠ©äºå…¨é¢ç†è§£åŸå¸‚åŠ¨æ€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨é•¿æœŸåœ°çƒè§‚æµ‹æ•°æ®åˆ†ææ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>å¼•å…¥DVL-Suiteæ¡†æ¶ï¼Œæ—¨åœ¨åˆ†æé•¿æœŸåŸå¸‚åŠ¨æ€ã€‚</li>
<li>DVL-SuiteåŒ…å«é«˜åˆ†è¾¨ç‡å¤šæ—¶æ€å›¾åƒï¼Œè¦†ç›–ç¾å›½42ä¸ªè¶…å¤§åŸå¸‚ä»2005å¹´åˆ°2023å¹´çš„æ•°æ®ã€‚</li>
<li>DVL-BenchåŒ…å«ä»åŸºç¡€å˜åŒ–æ£€æµ‹åˆ°åŒºåŸŸå®šé‡åˆ†æå’ŒåŸå¸‚ç»¼åˆå™äº‹ç­‰å¤šä¸ªä»»åŠ¡ï¼Œæ•æ‰å¤šæ ·çš„åŸå¸‚åŠ¨æ€ã€‚</li>
<li>å¯¹å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨é•¿æœŸæ—¶é—´ç†è§£å’Œå®šé‡åˆ†ææ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>ä¸ºå¢å¼ºæ¨¡å‹åœ¨å¤šæ—¶æ€åœ°çƒè§‚æµ‹æ–¹é¢çš„èƒ½åŠ›ï¼Œåˆ›å»ºäº†DVL-Instructè¿™ä¸€ä¸“ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5850b785a0f23f8f805fd359c8abd1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef34caaa2f3f7cf1a85db95f4a45f337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b1f94fa97f8d50f0d38a72f83f196e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d14d02e7e3d27668dd9627279ccce42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d412c08984be6088e49f63e5497f39f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-364546124933f612d2b3a787d043e772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e0cf1f84d92f307c57a98a208d8a4e3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Respond-to-Change-with-Constancy-Instruction-tuning-with-LLM-for-Non-I-I-D-Network-Traffic-Classification"><a href="#Respond-to-Change-with-Constancy-Instruction-tuning-with-LLM-for-Non-I-I-D-Network-Traffic-Classification" class="headerlink" title="Respond to Change with Constancy: Instruction-tuning with LLM for   Non-I.I.D. Network Traffic Classification"></a>Respond to Change with Constancy: Instruction-tuning with LLM for   Non-I.I.D. Network Traffic Classification</h2><p><strong>Authors:Xinjie Lin, Gang Xiong, Gaopeng Gou, Wenqi Dong, Jing Yu, Zhen Li, Wei Xia</strong></p>
<p>Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooLâ€™s effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooLâ€™s instruction tuning approach. </p>
<blockquote>
<p>åŠ å¯†æµé‡åˆ†ç±»åœ¨ç½‘ç»œå®‰å…¨ä¸­æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒåŸå› åœ¨äºéœ€è¦ä»å†…å®¹æ— å…³çš„æµé‡æ•°æ®ä¸­æå–å‡ºç¨³å¥çš„ç‰¹å¾ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´çš„å…³é”®é—®é¢˜åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ç”±äºä¾èµ–å°é—­ä¸–ç•Œå‡è®¾è€Œå¯¼è‡´çš„åˆ†å¸ƒæ¼‚ç§»ï¼Œé™åˆ¶äº†é€‚åº”ç°å®ä¸–ç•Œå˜åŒ–æ¨¡å¼çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºæˆ–ä¸å¯ç”¨åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æä¾›å¯æ¨å¹¿è‡³å¹¿æ³›ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆæ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå¹¶åœ¨å¤šä¸ªä¸“ä¸šé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œå…¶åœ¨æµé‡åˆ†æä¸­çš„æœ‰æ•ˆæ€§ä»å—åˆ°é€‚åº”æµé‡é¢†åŸŸç‹¬ç‰¹è¦æ±‚æ–¹é¢çš„æŒ‘æˆ˜çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æµé‡è¡¨ç¤ºæ¨¡å‹ï¼Œåä¸ºåŠ å¯†æµé‡å‡ºåˆ†å¸ƒæŒ‡ä»¤è°ƒæ•´ä¸LLMï¼ˆETooLï¼‰ï¼Œå®ƒé€šè¿‡è‡ªç›‘ç£æŒ‡ä»¤è°ƒæ•´èŒƒå¼å°†LLMä¸æµé‡ç»“æ„çŸ¥è¯†ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶å»ºç«‹äº†æ–‡æœ¬ä¿¡æ¯ä¸æµé‡äº¤äº’ä¹‹é—´çš„è¿æ¥ã€‚ETooLåœ¨ç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬æµé‡åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´ç¨³å¥çš„åˆ†ç±»æ€§èƒ½å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨F1åˆ†æ•°ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼šAPP53ï¼ˆI.I.D.ï¼‰æé«˜åˆ°93.19%ï¼ˆ6.62%ï¼‰å’Œ92.11%ï¼ˆ4.19%ï¼‰ï¼ŒAPP53ï¼ˆO.O.D.ï¼‰æé«˜åˆ°74.88%ï¼ˆ18.17%ï¼‰å’Œ72.13%ï¼ˆ15.15%ï¼‰ï¼Œä»¥åŠISCX-Botnetï¼ˆO.O.D.ï¼‰æé«˜åˆ°95.03%ï¼ˆ9.16%ï¼‰å’Œ81.95%ï¼ˆ12.08%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†NETDæµé‡æ•°æ®é›†ï¼Œä»¥æ”¯æŒåŠ¨æ€åˆ†å¸ƒåç§»ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥éªŒè¯ETooLåœ¨ä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ETooLæŒ‡ä»¤è°ƒæ•´æ–¹æ³•åœ¨æé«˜æ•ˆç‡æ–¹é¢çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20866v1">PDF</a> IEEE Transactions on Information Forensics and Security (TIFS) camera   ready, 15 pages, 6 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ç½‘ç»œåŠ å¯†æµé‡åˆ†ç±»é—®é¢˜çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åˆ†å¸ƒæ¼‚ç§»å’Œä¾èµ–æ ‡ç­¾æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºETooLçš„æ–°å‹æµé‡è¡¨ç¤ºæ¨¡å‹ã€‚ETooLé€šè¿‡è‡ªç›‘ç£æŒ‡ä»¤è°ƒæ•´èŒƒå¼å°†LLMä¸æµé‡ç»“æ„çŸ¥è¯†ç›¸ç»“åˆï¼Œå»ºç«‹æ–‡æœ¬ä¿¡æ¯ä¸æµé‡äº¤äº’ä¹‹é—´çš„è”ç³»ã€‚åœ¨ç›‘ç£å’Œé›¶æ ·æœ¬æµé‡åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒETooLè¡¨ç°å‡ºæ›´ç¨³å¥çš„åˆ†ç±»æ€§èƒ½å’Œä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„F1åˆ†æ•°æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†ç”¨äºæ”¯æŒåŠ¨æ€åˆ†å¸ƒè½¬ç§»çš„NETDæµé‡æ•°æ®é›†ï¼Œå¹¶éªŒè¯äº†ETooLåœ¨ä¸åŒåˆ†å¸ƒæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ å¯†æµé‡åˆ†ç±»æ˜¯ç½‘ç»œå®‰å…¨çš„é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦ä»å†…å®¹æ— å…³çš„æµé‡æ•°æ®ä¸­æå–ç¨³å¥ç‰¹å¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´åˆ†å¸ƒæ¼‚ç§»å’Œä¾èµ–æ ‡ç­¾æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­å±•ç°å‡ºå¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</li>
<li>ETooLæ¨¡å‹é€šè¿‡è‡ªç›‘ç£æŒ‡ä»¤è°ƒæ•´èŒƒå¼æ•´åˆLLMå’Œæµé‡ç»“æ„çŸ¥è¯†ã€‚</li>
<li>ETooLåœ¨æµé‡åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ETooLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„F1åˆ†æ•°æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8dc23243c0489195a53f1125953afd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-715ebd06b6db2c9abca45e9edae5919b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ceca2e62709e1846f109ea979fe5039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78dba9ebbb3c4ef630560918e982748.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leaner-Transformers-More-Heads-Less-Depth"><a href="#Leaner-Transformers-More-Heads-Less-Depth" class="headerlink" title="Leaner Transformers: More Heads, Less Depth"></a>Leaner Transformers: More Heads, Less Depth</h2><p><strong>Authors:Hemanth Saratchandran, Damien Teney, Simon Lucey</strong></p>
<p>Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that â€œbigger means betterâ€, leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark). </p>
<blockquote>
<p>Transformeré€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¤§å‹æ•°æ®é›†ä¸­çš„å¤æ‚æ¨¡å¼ï¼Œé‡å¡‘äº†æœºå™¨å­¦ä¹ ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ€§èƒ½ã€‚è¿™ä¸€æˆåŠŸä¹Ÿå‚¬ç”Ÿäº†ä¸€ç§â€œè¶Šå¤§è¶Šå¥½â€çš„ä¿¡å¿µï¼Œå¯¼è‡´æ¨¡å‹è§„æ¨¡ä¸æ–­å¢é•¿ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†è¿™ä¸€ç†å¿µï¼Œè¡¨æ˜è®¸å¤šç°æœ‰Transformerå¯èƒ½ä¸å¿…è¦åœ°è¿‡å¤§ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé‡æ–°å®šä¹‰äº†å¤šå¤´æ³¨æ„åŠ›è§’è‰²çš„ç†è®ºåŸç†ã€‚å¤šå¤´çš„ä¸€ä¸ªé‡è¦ä¼˜ç‚¹æ˜¯æ”¹å–„æ³¨æ„åŠ›å—çš„æ¡ä»¶ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç†è®ºè§è§£ï¼Œé‡æ–°è®¾è®¡äº†æµè¡Œçš„æ¶æ„ï¼Œå¢åŠ äº†å¤´éƒ¨çš„æ•°é‡ã€‚åœ¨å®è·µä¸­ï¼Œæ¡ä»¶æ”¹å–„çš„æ˜¾è‘—è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œå¯ä»¥å‡å°æ¨¡å‹æ·±åº¦ï¼ŒåŒæ—¶å‡å°‘é«˜è¾¾30%~50%çš„å‚æ•°æ•°é‡ï¼Œå¹¶ä¿æŒå‡†ç¡®æ€§ã€‚åœ¨å„ç§è§„æ¨¡çš„åŸºäºTransformerçš„æ¶æ„ä¸­ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆImageNet-1kï¼‰ä»¥åŠè¯­è¨€å’Œåºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼ˆGLUEåŸºå‡†æµ‹è¯•ã€TinyStorieså’ŒLong-Range ArenaåŸºå‡†æµ‹è¯•ï¼‰ä¸Šè·å¾—äº†ä¸€è‡´çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20802v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è½¬æ¢å™¨é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¤§å‹æ•°æ®é›†ä¸­çš„å¤æ‚æ¨¡å¼ï¼Œä»è€Œé‡å¡‘äº†æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™ä¸€æˆåŠŸä¿ƒä½¿äººä»¬è®¤ä¸ºâ€œè¶Šå¤§è¶Šå¥½â€ï¼Œå¯¼è‡´æ¨¡å‹è§„æ¨¡ä¸æ–­å¢å¤§ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†è¿™ä¸€ç†å¿µï¼Œè¡¨æ˜è®¸å¤šç°æœ‰è½¬æ¢å™¨å¯èƒ½ä¸å¿…è¦åœ°è¿‡å¤§ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé‡æ–°å®šä¹‰äº†å¤šå¤´æ³¨æ„åŠ›ä½œç”¨çš„ç†è®ºåŸåˆ™ã€‚å¤šå¤´çš„ä¸€ä¸ªé‡è¦å¥½å¤„åœ¨äºæ”¹å–„äº†æ³¨æ„åŠ›çš„æ¡ä»¶ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç†è®ºè§è§£ï¼Œé‡æ–°è®¾è®¡äº†å…·æœ‰å¢åŠ çš„å¤´æ•°çš„æµè¡Œæ¶æ„ã€‚æ¡ä»¶çš„æ”¹å–„åœ¨å®è·µä¸­è¯æ˜æ˜¯å¦‚æ­¤é‡è¦ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹æ·±åº¦ï¼ŒåŒæ—¶å‡å°‘é«˜è¾¾30-50%çš„å‚æ•°æ•°é‡è€Œä¿æŒå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆImageNet-1kï¼‰ã€è¯­è¨€åŠåºåˆ—å»ºæ¨¡ï¼ˆGLUEåŸºå‡†æµ‹è¯•ã€TinyStorieså’ŒLong-Range ArenaåŸºå‡†æµ‹è¯•ä¸­çš„ä»»åŠ¡ï¼‰çš„å„ç§è§„æ¨¡çš„åŸºäºè½¬æ¢å™¨çš„æ¶æ„ä¸­éƒ½è·å¾—äº†ä¸€è‡´çš„å¥½å¤„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è½¬æ¢å™¨é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¤æ‚æ¨¡å¼ï¼Œæå‡äº†æœºå™¨å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>â€œè¶Šå¤§è¶Šå¥½â€çš„ç†å¿µå¯¼è‡´æ¨¡å‹è§„æ¨¡ä¸æ–­å¢å¤§ã€‚</li>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†ç°æœ‰è½¬æ¢å™¨å¯èƒ½ä¸å¿…è¦è¿‡å¤§çš„è§‚ç‚¹ã€‚</li>
<li>å‘ç°å¤šå¤´æ³¨æ„åŠ›å¯ä»¥æ”¹å–„æ³¨æ„åŠ›çš„æ¡ä»¶ã€‚</li>
<li>é‡æ–°è®¾è®¡äº†å…·æœ‰å¢åŠ å¤´æ•°çš„æµè¡Œæ¶æ„ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹æ·±åº¦å¯ä»¥å‡å°‘ï¼ŒåŒæ—¶å‡å°‘é«˜è¾¾30-50%çš„å‚æ•°æ•°é‡è€Œä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡å’ŒåŸºäºè½¬æ¢å™¨çš„æ¶æ„ä¸­éƒ½è·å¾—äº†ä¸€è‡´çš„å¥½å¤„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43713a3c0843206bf6ff589101ada908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38fe184411e6005e651dfcc345af1a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9424e3e87349da687cc62920ceb0225b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf69b2d2b59445e0dbb72124d48b92d5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Transformation-from-Natural-Language-to-Signal-Temporal-Logic-Using-LLMs-with-Diverse-External-Knowledge"><a href="#Enhancing-Transformation-from-Natural-Language-to-Signal-Temporal-Logic-Using-LLMs-with-Diverse-External-Knowledge" class="headerlink" title="Enhancing Transformation from Natural Language to Signal Temporal Logic   Using LLMs with Diverse External Knowledge"></a>Enhancing Transformation from Natural Language to Signal Temporal Logic   Using LLMs with Diverse External Knowledge</h2><p><strong>Authors:Yue Fang, Zhi Jin, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan</strong></p>
<p>Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets. </p>
<blockquote>
<p>æ—¶é—´é€»è¾‘ï¼ˆTLï¼‰ï¼Œç‰¹åˆ«æ˜¯ä¿¡å·æ—¶é—´é€»è¾‘ï¼ˆSTLï¼‰ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„å½¢å¼åŒ–è§„èŒƒï¼Œä½¿å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç­‰ç½‘ç»œç‰©ç†ç³»ç»Ÿä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚è‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è½¬æ¢ä¸ºSTLæ˜¯ä¸€ç§å…‹æœæ‰‹åŠ¨è½¬æ¢é™åˆ¶çš„å¸å¼•äººçš„æ–¹æ³•ï¼Œæ‰‹åŠ¨è½¬æ¢è€—æ—¶ä¸”æ˜“å‡ºé”™ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®é›†ç¼ºä¹ï¼Œè‡ªåŠ¨è½¬æ¢ç›®å‰é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSTL-Diversity-Enhancedï¼ˆSTL-DivEnï¼‰çš„NL-STLæ•°æ®é›†ï¼ŒåŒ…å«16,000ä¸ªå¯Œå«å¤šç§æ¨¡å¼çš„æ ·æœ¬ã€‚ä¸ºäº†å¼€å‘è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬é¦–å…ˆæ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªå°è§„æ¨¡çš„NL-STLå¯¹ç§å­é›†ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡èšç±»ç¡®å®šä»£è¡¨æ€§ç¤ºä¾‹ï¼Œå¹¶ç”¨äºæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé¢å¤–çš„NL-STLå¯¹ã€‚æœ€åï¼Œé€šè¿‡ä¸¥æ ¼çš„åŸºäºè§„åˆ™çš„è¿‡æ»¤å™¨å’Œäººå·¥éªŒè¯ï¼Œç¡®ä¿å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†çŸ¥è¯†å¼•å¯¼STLè½¬æ¢ï¼ˆKGSTï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSTLçš„æ–°å‹æ–¹æ³•ï¼ŒåŸºäºå¤–éƒ¨çŸ¥è¯†é‡‡ç”¨å…ˆç”Ÿæˆåç²¾ç»†åŒ–çš„è¿‡ç¨‹ã€‚ç»Ÿè®¡åˆ†æè¡¨æ˜ï¼ŒSTL-DivEnæ•°æ®é›†æ¯”ç°æœ‰çš„NL-STLæ•°æ®é›†æ›´å…·å¤šæ ·æ€§ã€‚è€Œä¸”ï¼ŒåŸºäºæŒ‡æ ‡å’Œäººå·¥è¯„ä¼°éƒ½è¡¨æ˜ï¼Œæˆ‘ä»¬çš„KGSTæ–¹æ³•åœ¨STL-DivEnå’ŒDeepSTLæ•°æ®é›†ä¸Šçš„è½¬æ¢ç²¾åº¦è¶…è¿‡äº†åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20658v1">PDF</a> 13 pages, 5 figures, published to ACL</p>
<p><strong>Summary</strong></p>
<p>è‡ªç„¶è¯­è¨€ä¸ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰ä¹‹é—´çš„è‡ªåŠ¨è½¬æ¢æŠ€æœ¯èƒ½å¤Ÿå…‹æœæ‰‹åŠ¨è½¬æ¢æ–¹æ³•çš„å±€é™ï¼Œå¦‚è€—æ—¶ä¸æ˜“å‡ºé”™ç­‰é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ•°æ®é›†ï¼Œè‡ªåŠ¨è½¬æ¢æŠ€æœ¯é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºSTL-DivEnçš„NL-STLæ•°æ®é›†ï¼ŒåŒ…å«16,000ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å¤šæ ·çš„æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºçŸ¥è¯†çš„STLè½¬æ¢ï¼ˆKGSTï¼‰æ¡†æ¶ï¼Œå°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºSTLã€‚ç»Ÿè®¡åˆ†ææ˜¾ç¤ºï¼ŒSTL-DivEnæ•°æ®é›†å…·æœ‰æ›´é«˜çš„å¤šæ ·æ€§ï¼Œå¹¶ä¸”ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒKGSTåœ¨STL-DivEnå’ŒDeepSTLæ•°æ®é›†ä¸Šçš„è½¬æ¢å‡†ç¡®æ€§æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Temporal Logicï¼ˆç‰¹åˆ«æ˜¯STLï¼‰åœ¨è‡ªä¸»é©¾é©¶å’Œæœºå™¨äººç­‰è·¨ç‰©ç†ç³»ç»Ÿä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ä»·å€¼ã€‚</li>
<li>è‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è½¬æ¢ä¸ºSTLå¯æœ‰æ•ˆå…‹æœæ‰‹åŠ¨è½¬æ¢æ–¹æ³•å­˜åœ¨çš„è€—æ—¶ä¸æ˜“é”™ç¼ºé™·ã€‚</li>
<li>ç¼ºä¹æ•°æ®é›†æ˜¯å½“å‰è‡ªåŠ¨è½¬æ¢æŠ€æœ¯é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>STL-DivEnæ•°æ®é›†åŒ…å«ä¸°å¯Œçš„å¤šæ ·æ¨¡å¼æ ·æœ¬ï¼Œå…±è®¡åŒ…å«è¶…è¿‡ä¸€å®šæ•°é‡çš„æ ·æœ¬æ•°æ®ï¼Œèƒ½ä¸ºæ­¤é—®é¢˜æä¾›æ›´å…¨é¢æ·±å…¥çš„ç ”ç©¶èµ„æ–™ã€‚</li>
<li>KGSTæ¡†æ¶èƒ½æœ‰æ•ˆå®ç°è‡ªç„¶è¯­è¨€åˆ°STLçš„è½¬æ¢ã€‚</li>
<li>STL-DivEnæ•°æ®é›†ç›¸è¾ƒäºç°æœ‰NL-STLæ•°æ®é›†å±•ç°å‡ºæ›´é«˜çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbf7fd60c2b529998c87f9721aa93dbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b83f03b54c2a0f303f2caceddb0b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6276847648861f2c394a49037c3fe11.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-shot-Entropy-Minimization"><a href="#One-shot-Entropy-Minimization" class="headerlink" title="One-shot Entropy Minimization"></a>One-shot Entropy Minimization</h2><p><strong>Authors:Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai</strong></p>
<p>We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zitian-gao/one-shot-em">https://github.com/zitian-gao/one-shot-em</a>. </p>
<blockquote>
<p>æˆ‘ä»¬è®­ç»ƒäº†13440ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°ç†µæœ€å°åŒ–åªéœ€ä¸€ä¸ªæœªæ ‡è®°çš„æ•°æ®å’Œ10æ­¥ä¼˜åŒ–ï¼Œå°±èƒ½å®ç°ä¸åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ä¸­æ•°åƒä¸ªæ•°æ®å’Œç²¾å¿ƒè®¾è®¡å¥–åŠ±æ‰€è·å¾—çš„æ€§èƒ½æ”¹è¿›ç›¸å½“ç”šè‡³æ›´é«˜çš„æ€§èƒ½æ”¹è¿›ã€‚è¿™ä¸€ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœå¯èƒ½ä¼šä¿ƒä½¿äººä»¬é‡æ–°æ€è€ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåèŒƒå¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zitian-gao/one-shot-em%E4%B8%AD%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/zitian-gao/one-shot-emä¸­æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20282v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬è®­ç»ƒäº†13440ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°ç†µæœ€å°åŒ–ä»…éœ€ä¸€ä¸ªæœªæ ‡è®°æ•°æ®ç‚¹å’Œ10æ­¥ä¼˜åŒ–ï¼Œå³å¯å®ç°ä¸åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ æ•°åƒä¸ªæ•°æ®å’Œç²¾å¿ƒè®¾è®¡å¥–åŠ±æ‰€è·å¾—æ€§èƒ½çš„æ”¹è¿›ç›¸å½“ç”šè‡³æ›´é«˜çš„æ€§èƒ½ã€‚è¿™ä¸€ç»“æœå¯èƒ½ä¼šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åæœŸè®­ç»ƒèŒƒå¼äº§ç”Ÿåæ€æ€§æ€è€ƒã€‚æˆ‘ä»¬çš„ä»£ç åœ¨GitHubä¸Šå¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>è®­ç»ƒäº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ•°é‡è¾¾13,440ä¸ªã€‚</li>
<li>ç†µæœ€å°åŒ–æ–¹æ³•è¡¨ç°å‡ºä»¤äººç©ç›®çš„ç»“æœã€‚</li>
<li>ä»…éœ€å•ä¸ªæœªæ ‡è®°æ•°æ®å’Œ10æ­¥ä¼˜åŒ–ï¼Œå°±èƒ½å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•æ€§èƒ½ä¸åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶æŒ‘æˆ˜äº†ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒèŒƒå¼ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯èƒ½å¯¼è‡´å¯¹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ–°æ€è€ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9db172948c5bdc25ecbab54f50e22c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc6a9a90865a2914717c82763111fd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-100130e3caadf74877730fe07b190ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e0fa75a4bc443c2c103073338d604a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2448b5cafd3c167a317aaa0d1c19c17.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="When-Two-LLMs-Debate-Both-Think-Theyâ€™ll-Win"><a href="#When-Two-LLMs-Debate-Both-Think-Theyâ€™ll-Win" class="headerlink" title="When Two LLMs Debate, Both Think Theyâ€™ll Win"></a>When Two LLMs Debate, Both Think Theyâ€™ll Win</h2><p><strong>Authors:Pradyumna Shyama Prasad, Minh Nhat Nguyen</strong></p>
<p>Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed &gt;&#x3D;75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: modelsâ€™ private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLM outputs are deployed without careful review in assistant roles or agentic settings. </p>
<blockquote>
<p>åœ¨é¢å¯¹å¯¹ç«‹è§‚ç‚¹æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¦å‡†ç¡®åœ°è°ƒæ•´å…¶ä¿¡å¿ƒï¼Ÿæˆ‘ä»¬åŸºäºä¹‹å‰å¯¹åŸºäºé™æ€äº‹å®é—®é¢˜çš„æ ¡å‡†æµ‹é‡ç ”ç©¶ï¼Œåœ¨åŠ¨æ€ã€å¯¹æŠ—æ€§çš„è¾©è®ºç¯å¢ƒä¸­è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬ç‹¬ç‰¹åœ°ç»“åˆäº†ä¸¤ä¸ªç°å®å› ç´ ï¼šï¼ˆaï¼‰å¤šè½®å½¢å¼è¦æ±‚æ¨¡å‹éšç€æ–°ä¿¡æ¯çš„å‡ºç°è€Œæ›´æ–°ä¿¡å¿µï¼›ï¼ˆbï¼‰é›¶å’Œç»“æ„æ§åˆ¶ä»»åŠ¡ç›¸å…³çš„ä¸ç¡®å®šæ€§ï¼Œå› ä¸ºç›¸äº’çš„é«˜ä¿¡å¿ƒä¸»å¼ æ„å‘³ç€ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ã€‚æˆ‘ä»¬ç»„ç»‡äº†10ä¸ªæœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹å‚ä¸çš„ä¸‰è½®æ”¿ç­–è¾©è®ºï¼Œå…±è¿›è¡Œ60åœºè¾©è®ºï¼Œæ¯è½®ç»“æŸåæ¨¡å‹ç§ä¸‹è¯„ä¼°å…¶è·èƒœçš„ä¿¡å¿ƒï¼ˆ0-100ï¼‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°äº”ç§ä»¤äººæ‹…å¿§çš„æ¨¡å¼ï¼šï¼ˆ1ï¼‰ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼šè¾©è®ºå¼€å§‹æ—¶æ¨¡å‹çš„å¹³å‡åˆå§‹ä¿¡å¿ƒä¸º72.9%ï¼Œè€Œç†æ€§åŸºçº¿ä¸º50%ã€‚ï¼ˆ2ï¼‰ä¿¡å¿ƒå‡çº§ï¼šè¾©è®ºè¿›å±•æ—¶ï¼Œè¾©æ‰‹å¹¶æœªå‡å°‘ä¿¡å¿ƒï¼Œåè€Œå¢åŠ äº†è·èƒœæ¦‚ç‡ï¼Œè‡³ç»ˆè½®å¹³å‡è¾¾åˆ°83%ã€‚ï¼ˆ3ï¼‰ç›¸äº’é«˜ä¼°ï¼šåœ¨61.7%çš„è¾©è®ºä¸­ï¼ŒåŒæ–¹åŒæ—¶å£°ç§°èƒœåˆ©æ¦‚ç‡å¤§äºç­‰äº75%ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„ä¸å¯èƒ½ã€‚ï¼ˆ4ï¼‰æŒç»­çš„è‡ªæˆ‘è¾©è®ºåè§ï¼šæ¨¡å‹åœ¨ä¸ç›¸åŒå‰¯æœ¬çš„è¾©è®ºä¸­ä¿¡å¿ƒä»64.1%å¢åŠ åˆ°75.2%ï¼›å³ä½¿æ˜ç¡®å‘ŠçŸ¥ä»–ä»¬è·èƒœå‡ ç‡æ­£å¥½æ˜¯50%ï¼Œä¿¡å¿ƒä»ç„¶ä¸Šå‡ï¼ˆä»50.0%ä¸Šå‡åˆ°57.1%ï¼‰ã€‚ï¼ˆ5ï¼‰ä¸ä¸€è‡´çš„ç§äººæ¨ç†ï¼šæ¨¡å‹çš„ç§ä¸‹ç¬”è®°æƒ³æ³•æœ‰æ—¶ä¸å…¶å…¬å¼€çš„ä¿¡å¿ƒè¯„åˆ†ä¸ä¸€è‡´ï¼Œè¿™å¼•å‘äº†å…³äºæ€è€ƒè¿‡ç¨‹å¯é æ€§çš„æ‹…å¿§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåœ¨åŠ¨æ€å¤šè½®ä»»åŠ¡ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹å‡†ç¡®è‡ªæˆ‘è¯„ä¼°æˆ–æ›´æ–°å…¶ä¿¡å¿µçš„èƒ½åŠ›ï¼›è¿™æ˜¯ä¸€ä¸ªä»¤äººæ‹…å¿§çš„é—®é¢˜ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºåœ¨æ²¡æœ‰ä»”ç»†å®¡æŸ¥çš„æƒ…å†µä¸‹è¢«éƒ¨ç½²åœ¨åŠ©ç†è§’è‰²æˆ–ä»£ç†ç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19184v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨åŠ¨æ€å¤šè½®è¾©è®ºç¯å¢ƒä¸­çš„è¡¨ç°å­˜åœ¨è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°äº”å¤§é—®é¢˜ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ã€ä¿¡å¿ƒé€’å¢ã€åŒæ–¹äº’ç›¸é«˜ä¼°ã€æŒç»­è‡ªæˆ‘è¾©è®ºåè§ä»¥åŠç§äººæ¨ç†ä¸ä¸€è‡´ç­‰ã€‚è¿™äº›é—®é¢˜è¡¨æ˜LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­ç¼ºä¹å‡†ç¡®è‡ªæˆ‘è¯„ä¼°æˆ–æ›´æ–°ä¿¡å¿µçš„èƒ½åŠ›ï¼Œå¯¹äºå…¶åœ¨åŠ©ç†è§’è‰²æˆ–ä»£ç†ç¯å¢ƒä¸­çš„éƒ¨ç½²åº”ç”¨æ„æˆäº†é‡å¤§æ‹…å¿§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¡¨ç°å‡ºç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼Œåˆå§‹ä¿¡å¿ƒå¹³å‡å€¼ä¸º72.9%ï¼Œè¿œé«˜äºç†æ€§åŸºçº¿å€¼50%ã€‚</li>
<li>åœ¨è¾©è®ºè¿‡ç¨‹ä¸­ï¼ŒLLMsçš„ä¿¡å¿ƒä¸æ˜¯é™ä½è€Œæ˜¯é€’å¢ï¼Œæœ€ç»ˆè½®æ¬¡å¹³å‡è¾¾åˆ°83%ã€‚</li>
<li>åœ¨è¶…è¿‡ä¸€åŠçš„è¾©è®ºä¸­ï¼ŒåŒæ–¹åŒæ—¶å£°ç§°èƒœåˆ©æ¦‚ç‡å¤§äºæˆ–ç­‰äº75%ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„ä¸å¯èƒ½ç°è±¡ã€‚</li>
<li>åœ¨è‡ªæˆ‘è¾©è®ºåœºæ™¯ä¸­ï¼ŒLLMsçš„ä¿¡å¿ƒè¿›ä¸€æ­¥å¢åŠ ï¼Œå³ä½¿è¢«å‘ŠçŸ¥è·èƒœå‡ ç‡æ˜¯ç²¾ç¡®çš„50%ï¼Œä¿¡å¿ƒä»ç„¶ä¸Šå‡ã€‚</li>
<li>LLMsçš„ç§äººæ¨ç†ä¸å…¶å…¬å¼€ä¿¡å¿ƒè¯„çº§æœ‰æ—¶å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¼•å‘å¯¹è¿è´¯æ€§æ¨ç†çš„ç–‘è™‘ã€‚</li>
<li>LLMsåœ¨åŠ¨æ€ã€å¤šè½®ä»»åŠ¡ä¸­ç¼ºä¹å‡†ç¡®è‡ªæˆ‘è¯„ä¼°æˆ–æ›´æ–°ä¿¡å¿µçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9880e37152e66f0c9d8ab88135d9861b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-407e38dccdb8542ada5658d3b74f4239.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d2bdd9758b5d64689e9d5a53f8bb548.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Exact-Expressive-Power-of-Transformers-with-Padding"><a href="#Exact-Expressive-Power-of-Transformers-with-Padding" class="headerlink" title="Exact Expressive Power of Transformers with Padding"></a>Exact Expressive Power of Transformers with Padding</h2><p><strong>Authors:William Merrill, Ashish Sabharwal</strong></p>
<p>Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformerâ€™s expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformersâ€™ expressive power: with polylogarithmic looping, padded transformers converge to the class $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} &#x3D; \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought. </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´æ˜¯åŸºäºè½¬æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜è®¡ç®—èƒ½åŠ›çš„è‡ªç„¶æ¨ç†æ—¶é—´æ–¹æ³•ï¼Œä½†ä¼šä»˜å‡ºé¡ºåºè§£ç çš„ä»£ä»·ã€‚æ˜¯å¦å­˜åœ¨ä¸€ç§æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥æ‰©å±•è½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›è€Œä¸å¢åŠ å‚æ•°ï¼Ÿæˆ‘ä»¬å°†å¡«å……æ ‡è®°ä½œä¸ºå¹¶è¡Œæµ‹è¯•æ—¶é—´è®¡ç®—çš„è½¬æ¢å™¨å½¢å¼ã€‚æˆ‘ä»¬è¯æ˜äº†ç¡¬å¹³å‡æ³¨æ„åŠ›ã€å¸¦æœ‰å¤šé¡¹å¼å¡«å……çš„æ©ç é¢„èŒƒè½¬æ¢å™¨ç²¾ç¡®åœ°æ”¶æ•›äºæå¯å¹¶è¡ŒåŒ–é—®é¢˜çš„ç±»åˆ«$\sf TC^0$ã€‚è™½ç„¶å·²çŸ¥$\sf TC^0$çš„ä¸Šç•Œï¼Œä½†è¯æ˜åŒ¹é…çš„ä¸‹ç•Œä¸€ç›´éš¾ä»¥æ‰æ‘¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–°åˆ†ææ­ç¤ºäº†å¡«å……è½¬æ¢å™¨çš„ç²¾ç¡®æ‰©å±•èƒ½åŠ›ï¼Œå½“å®ƒä¸å¦ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—ç›¸ç»“åˆæ—¶ï¼Œå³é€šè¿‡å¾ªç¯åŠ¨æ€å¢åŠ æ·±åº¦ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®æ˜¯å±•ç¤ºå¡«å……å¦‚ä½•å¸®åŠ©å¼•å…¥å®Œæ•´é—®é¢˜å’Œå½’çº¦çš„æ¦‚å¿µï¼Œè¿™äº›ä¸€ç›´æ˜¯ç»å…¸å¤æ‚æ€§ç†è®ºçš„æ ¸å¿ƒã€‚å€ŸåŠ©è¿™ä¸ªæ–°å·¥å…·ï¼Œæˆ‘ä»¬è¯æ˜äº†å¸¦æœ‰è¾“å…¥é•¿åº¦ä¸ºnçš„å¯¹æ•°dæ¬¡å¾ªç¯çš„å¡«å……è½¬æ¢å™¨ç²¾ç¡®åœ°è¯†åˆ«å‡ºä¸­ç­‰å¹¶è¡ŒåŒ–é—®é¢˜çš„ç±»åˆ«$\sf TC^d$ã€‚å› æ­¤ï¼Œå¡«å……å’Œå¾ªç¯ä¸€èµ·ç³»ç»Ÿåœ°æ‰©å±•äº†è½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼šå…·æœ‰å¤šé¡¹å¼å¯¹æ•°å¾ªç¯çš„å¡«å……è½¬æ¢å™¨æ”¶æ•›åˆ°ç±»åˆ«$\sf NC$ï¼Œè¿™æ˜¯åœ¨ä¸å¤±å»å¹¶è¡Œæ€§çš„æƒ…å†µä¸‹æ‰€èƒ½æœŸæœ›çš„æœ€ä½³ç»“æœï¼ˆé™¤é$\sf NC &#x3D; \sf P$ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿›ä¸€æ­¥é¼“åŠ±æ¢ç´¢å¡«å……å’Œå¾ªç¯ä½œä¸ºå¯å¹¶è¡ŒåŒ–çš„æ›¿ä»£é“¾æ€ç»´çš„æ–¹æ³•ã€‚â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18948v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¢åŠ åŸºäºè½¬æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—èƒ½åŠ›çš„è‡ªç„¶æ¨ç†æ—¶é—´æ–¹æ³•é“¾å¼æ€ç»´ï¼Œå¹¶åˆ†æå…¶é¢ä¸´çš„é¡ºåºè§£ç æˆæœ¬é—®é¢˜ã€‚æœ¬æ–‡è€ƒè™‘åœ¨æµ‹è¯•é˜¶æ®µé€šè¿‡å¹¶è¡ŒåŒ–è®¡ç®—æ–¹å¼åˆ©ç”¨å¸¦æœ‰å¡«å……ç¬¦å·çš„è½¬æ¢å™¨ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¹³å‡ç¡¬æ³¨æ„åŠ›ã€å¸¦æœ‰å¤šé¡¹å¼å¡«å……çš„æ©ç é¢„æ ‡å‡†åŒ–è½¬æ¢å™¨å¯ä»¥ç²¾ç¡®è§£å†³æå¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»åˆ«TC^0ã€‚æ­¤å¤–ï¼Œç»“åˆå¦ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—æ–¹å¼â€”â€”åŠ¨æ€å¢åŠ æ·±åº¦å¾ªç¯ï¼Œæœ¬æ–‡æ­ç¤ºäº†å¡«å……è½¬æ¢å™¨ç²¾ç¡®æ‰©å±•èƒ½åŠ›çš„å…·ä½“è¡¨ç°ã€‚æ–‡ç« çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºå±•ç¤ºäº†å¡«å……å¦‚ä½•å¸®åŠ©å¸¦æ¥ä¼ ç»Ÿå¤æ‚æ€§ç†è®ºåŸºçŸ³çš„å®Œå…¨é—®é¢˜å’Œå‡å°‘çš„æ¦‚å¿µï¼Œå¹¶å°†å®ƒä»¬åº”ç”¨äºè½¬æ¢å™¨çš„æ­£å¼ç ”ç©¶ã€‚é€šè¿‡å¡«å……å’Œå¾ªç¯ç»“åˆï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°æ‰©å±•äº†è½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼šåœ¨è¾“å…¥é•¿åº¦ä¸ºnçš„æƒ…å†µä¸‹ï¼Œå…·æœ‰å¯¹æ•°æ·±åº¦å¾ªç¯çš„å¡«å……è½¬æ¢å™¨èƒ½å¤Ÿç²¾ç¡®åœ°è¯†åˆ«å‡ºé€‚åº¦å¹¶è¡ŒåŒ–é—®é¢˜ç±»åˆ«TC^dã€‚å› æ­¤ï¼Œå¡«å……å’Œå¾ªç¯ä½œä¸ºå¯å¹¶è¡ŒåŒ–çš„æ›¿ä»£é“¾å¼æ€ç»´çš„æ–¹æ³•å€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é“¾å¼æ€ç»´æ˜¯ä¸€ç§ç”¨äºå¢åŠ åŸºäºè½¬æ¢å™¨çš„LLMçš„è®¡ç®—èƒ½åŠ›çš„æ–¹æ³•ï¼Œä½†å®ƒä»¥é¡ºåºè§£ç æˆæœ¬ä¸ºä»£ä»·ã€‚</li>
<li>å¸¦æœ‰å¡«å……ç¬¦å·çš„è½¬æ¢å™¨å¯ä½œä¸ºæµ‹è¯•é˜¶æ®µçš„ä¸€ç§å¹¶è¡ŒåŒ–è®¡ç®—å½¢å¼ã€‚</li>
<li>å¹³å‡ç¡¬æ³¨æ„åŠ›ã€å¸¦æœ‰å¤šé¡¹å¼å¡«å……çš„æ©ç é¢„æ ‡å‡†åŒ–è½¬æ¢å™¨å¯ä»¥ç²¾ç¡®è§£å†³æå¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»åˆ«TC^0ã€‚</li>
<li>ç»“åˆåŠ¨æ€å¢åŠ æ·±åº¦å¾ªç¯çš„å¡«å……è½¬æ¢å™¨èƒ½å¤Ÿç²¾ç¡®åœ°æ‰©å±•å…¶èƒ½åŠ›ï¼Œè§£å†³é€‚åº¦å¹¶è¡ŒåŒ–é—®é¢˜ç±»åˆ«TC^dã€‚</li>
<li>å¡«å……å¸®åŠ©å¼•å…¥ä¼ ç»Ÿå¤æ‚æ€§ç†è®ºä¸­çš„å®Œå…¨é—®é¢˜å’Œå‡å°‘çš„æ¦‚å¿µï¼Œå¹¶å°†å…¶åº”ç”¨äºè½¬æ¢å™¨çš„æ­£å¼ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ç»“åˆå¡«å……å’Œå¾ªç¯ï¼Œè½¬æ¢å™¨çš„è¡¨è¾¾èƒ½åŠ›å¾—åˆ°ç³»ç»Ÿæ‰©å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14e60179d6617371d199d0aab84d4985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5be8500752ca8ba832a2afc43c653e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Modelsâ€™ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillationâ€™s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åŸºäºè§„åˆ™çš„RLèƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰é‡å¤§å½±å“ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚å‡è®¾åˆ†æè¡¨æ˜ï¼ŒSFTæ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚åœ¨æˆ‘ä»¬çš„åˆ†ææŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å°è§„æ¨¡è’¸é¦å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ï¼Œè’¸é¦æ¥æºäºRLè®­ç»ƒçš„ç­–ç•¥ã€‚åœ¨Knight &amp; Knaveå’ŒMATHæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å†è’¸é¦çš„æƒŠäººæ•ˆç‡ï¼šå†è’¸é¦æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬å’Œè®¡ç®—å°±èƒ½è¾¾åˆ°RLæ€§èƒ½ã€‚ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚å› æ­¤ï¼Œåœ¨K&amp;Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åªç”¨1Kä¸ªSFTæ ·æœ¬çš„å†è’¸é¦Qwen2.5-1.5Bæ¨¡å‹è¶…è¶Šäº†DeepSeek-V3-0324ã€‚åœ¨MATHä¸Šï¼Œä½¿ç”¨å†è’¸é¦çš„500ä¸ªæ ·æœ¬å¯¹Qwen2.5-1.5Bè¿›è¡Œå¾®è°ƒï¼Œå¯ä¸æ²¡æœ‰RLçš„æŒ‡ä»¤è°ƒä¼˜å˜ä½“ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v2">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong></p>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°å°è§„æ¨¡çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹RLæœ‰é‡è¦å½±å“ï¼Œä½†æ•ˆç‡ä¸é«˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åˆ†ææ¡†æ¶ï¼Œé€šè¿‡è¡¡é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶å‡è¡¨æ˜ï¼ŒSFTçš„æ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæå‡ºäº†é‡è’¸é¦æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°è§„æ¨¡çš„è’¸é¦å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»RLè®­ç»ƒçš„å†³ç­–ä¸­å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œé‡è’¸é¦æ¨¡å‹åœ¨æ ·æœ¬æ•°é‡å’Œè®¡ç®—é‡è¿œå°äºRLçš„æƒ…å†µä¸‹è¾¾åˆ°ä¸å…¶ç›¸è¿‘çš„æ€§èƒ½ã€‚ç»éªŒéªŒè¯æ˜¾ç¤ºæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æå‡çš„è‰¯å¥½æŒ‡æ ‡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°è§„æ¨¡çš„æœ‰ç›‘ç£å¾®è°ƒå¯¹å¼ºåŒ–å­¦ä¹ æœ‰é‡è¦å½±å“ï¼Œä½†å…¶æ•ˆç‡æœ‰å¾…æé«˜ã€‚</li>
<li>é€šè¿‡åˆ†ææ¡†æ¶å’Œæ ·æœ¬æ•ˆåº”è¡¡é‡ï¼Œå¯¹SFTå’ŒRLçš„æ•ˆç‡è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>æ ·æœ¬æ•ˆåº”æ˜¯è¯„ä¼°æ€§èƒ½æå‡çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>é‡è’¸é¦æŠ€æœ¯é€šè¿‡å°è§„æ¨¡çš„è’¸é¦ä»RLè®­ç»ƒçš„å†³ç­–ä¸­å­¦ä¹ ï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜é‡è’¸é¦æ¨¡å‹åœ¨æ ·æœ¬å’Œè®¡ç®—éœ€æ±‚æ–¹é¢ä¼˜äºRLã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da9dc98994f59ee1c05654847fe410ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the modelâ€™s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>æ€ç»´é“¾æ¨ç†å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§é¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä¸€ç›´è¢«é™åˆ¶åœ¨æ–‡æœ¬ç©ºé—´å†…ï¼Œé™åˆ¶äº†å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨è¿™ä¸€æ–°é¢–æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œä¾‹å¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥æ£€æŸ¥ã€è´¨ç–‘å’Œä»è§†è§‰è¯æ®ä¸­è¿›è¡Œæ¨æ–­ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åœ¨VLMä¸­åŸ¹å…»è¿™ç§åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´ç€æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æŠµè§¦ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°å‹è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚å€ŸåŠ©è¿™äº›è§†è§‰æ“ä½œï¼ŒVLMå¯ä»¥ä¸å¤æ‚çš„è§†è§‰è¾“å…¥è¿›è¡Œäº¤äº’ï¼Œå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼Œä»¥ä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹å®ç°äº†V* benchçš„84%ã€TallyQA-Complexçš„74%å’ŒInfographicsVQAçš„84%ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹æ‰€å–å¾—çš„æœ€é«˜ç²¾åº¦ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Pixel-Reasoner/">https://tiger-ai-lab.github.io/Pixel-Reasoner/</a>,   Hands-on Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner">https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é“¾å¼æ€ç»´æ¨ç†å·²æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä»…é™äºæ–‡æœ¬ç©ºé—´ï¼Œä½¿å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—é™ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨æ­¤æ–°æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œå¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥æ£€æŸ¥ã€è¯¢é—®å’Œæ¨æ–­è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åŸ¹å…»VLMä¸­çš„åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æ¥å—åº¦ä½ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°çš„è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚è¿™äº›è§†è§‰æ“ä½œä½¿VLMèƒ½å¤Ÿä¸å¤æ‚è§†è§‰è¾“å…¥ï¼ˆå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼‰è¿›è¡Œäº¤äº’ï¼Œä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨V*åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°84%ã€TallyQA-Complexä¸Šè¾¾åˆ°74%ã€InfographicsVQAä¸Šè¾¾åˆ°84%ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹ä¸­æ€§èƒ½æœ€é«˜çš„ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é“¾å¼æ€ç»´æ¨ç†å·²æå‡LLMæ€§èƒ½ï¼Œä½†å±€é™äºæ–‡æœ¬ç©ºé—´ï¼Œåœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å…¥åƒç´ ç©ºé—´æ¨ç†æ¦‚å¿µï¼Œä½¿VLMå…·å¤‡ç›´æ¥å¤„ç†è§†è§‰è¯æ®çš„èƒ½åŠ›ã€‚</li>
<li>VLMé¢ä¸´åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°è§†è§‰æ“ä½œæ¥å—åº¦ä½çš„æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ç†Ÿæ‚‰è§†è§‰æ“ä½œï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¹³è¡¡åƒç´ ç©ºé—´å’Œæ–‡æœ¬æ¨ç†çš„æ¢ç´¢ã€‚</li>
<li>VLMèƒ½é€šè¿‡å¤æ‚è§†è§‰è¾“å…¥ä¸»åŠ¨æ”¶é›†ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—æé«˜VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>7Bæ¨¡å‹åœ¨å¤šä¸ªæµ‹è¯•ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šå…ˆå‰å¼€æºæ¨¡å‹çš„æœ€é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b8da341c6535770c2f92380e0df79d31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b1106d4ee92d009b154b01b7626b30.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs"><a href="#STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs" class="headerlink" title="STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"></a>STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</h2><p><strong>Authors:Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1â€™s anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1">https://github.com/zongzhao23/STAR-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢å´è¿œè¿œè½åäºäººç±»ã€‚æˆ‘ä»¬é€šè¿‡è½¬æ¢é©±åŠ¨è§†è§‰æ¨ç†ï¼ˆTVRï¼‰æ¥ç ”ç©¶è¿™ä¸€å·®è·ï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸åŒçš„è§‚ç‚¹ä¸‹è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡è½¬æ¢ã€‚è™½ç„¶ä¼ ç»Ÿçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ— æ³•åœ¨è·¨è§†å›¾è®¾ç½®ä¸­ç”Ÿæˆè¿è´¯çš„æ¨ç†è·¯å¾„ï¼Œè€Œç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ™é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œæ¢ç´¢ç¼“æ…¢çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†STAR-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå•é˜¶æ®µRLèŒƒå¼å’Œé’ˆå¯¹TVRé‡èº«å®šåˆ¶çš„ç²¾ç»†å¥–åŠ±æœºåˆ¶çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒSTAR-R1å¥–åŠ±éƒ¨åˆ†æ­£ç¡®æ€§ï¼ŒåŒæ—¶æƒ©ç½šè¿‡åº¦æšä¸¾å’Œè¢«åŠ¨ä¸ä½œä¸ºï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSTAR-R1åœ¨æ‰€æœ‰11é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œåœ¨è·¨è§†å›¾åœºæ™¯ä¸­æ¯”SFTé«˜å‡º23%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†STAR-R1çš„äººç±»è¡Œä¸ºç‰¹å¾ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ”¹è¿›ç©ºé—´æ¨ç†æ–¹é¢æ¯”è¾ƒæ‰€æœ‰å¯¹è±¡çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæ¨è¿›MLLMså’Œæ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†å…³é”®è§è§£ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/zongzhao23/STAR-R1ä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è§†å›¾è½¬æ¢é©±åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶æå‡ºäº†åä¸ºSTAR-R1çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ èŒƒå¼å’Œé’ˆå¯¹è§†è§‰æ¨ç†ä»»åŠ¡çš„ç²¾ç»†å¥–åŠ±æœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆæ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTAR-R1åœ¨æ‰€æœ‰11é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œåœ¨è·¨è§†å›¾åœºæ™¯ä¸‹è¾ƒä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•æé«˜äº†23%ã€‚è¿™ä¸ºæ¨è¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œæ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è§†å›¾è½¬æ¢é©±åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨ä¸äººç±»æ˜¾è‘—å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶STAR-R1ï¼Œç»“åˆäº†å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ èŒƒå¼å’Œç²¾ç»†å¥–åŠ±æœºåˆ¶ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>STAR-R1é€šè¿‡å¥–åŠ±éƒ¨åˆ†æ­£ç¡®æ€§å’Œæƒ©ç½šè¿‡åº¦æšä¸¾åŠè¢«åŠ¨è¡Œä¸ºï¼Œå®ç°äº†é«˜æ•ˆæ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTAR-R1åœ¨è·¨è§†å›¾åœºæ™¯ä¸‹çš„æ€§èƒ½è¾ƒä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•æé«˜äº†23%ï¼Œè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>STAR-R1å±•ç°å‡ºç±»äººè¡Œä¸ºç‰¹å¾ï¼Œå¹¶å…·å¤‡æ¯”è¾ƒæ‰€æœ‰å¯¹è±¡ä»¥æ”¹å–„ç©ºé—´æ¨ç†çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ä¸ºæ¨è¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œæ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†å…³é”®è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e615d0760922e8863e3f12672d2c0fc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3175477c62e1924b40260c938fd5bb51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64d507ff00417d60ec92ecb7b99014a1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šé€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒæŒç»­å—ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç›´æ¥è°ƒæ•´INSTRUCTï¼ˆå³æŒ‡ä»¤è°ƒæ•´ï¼‰æ¨¡å‹é€šå¸¸åªä¼šå¸¦æ¥å¾®å°çš„æ”¹è¿›ï¼Œç”šè‡³ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›INSTRUCTå˜ä½“çš„åŸºç¡€é…å¥—BASEæ¨¡å‹åŒ…å«é«˜åº¦ç›¸ä¼¼çš„æƒé‡å€¼ï¼ˆä¾‹å¦‚Llama 3.1 8Bå¹³å‡ä¸åˆ°2%ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Shadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨ç›¸åº”çš„BASEæ¨¡å‹æ¥è°ƒæ•´INSTRUCTæ¨¡å‹ã€‚å…³é”®æ€è·¯æ˜¯å¯¹BASEæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„Shadow-FTä¸ä¼šå¼•å…¥é¢å¤–çš„å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸»æµçš„LLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¦‚Qwen 3å’ŒLlama 3ç³»åˆ—ï¼Œå¹¶åœ¨æ¶µç›–ç¼–ç ã€æ¨ç†å’Œæ•°å­¦ä»»åŠ¡çš„19ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShadow-FTæŒç»­ä¼˜äºä¼ ç»Ÿçš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆçš„è°ƒæ•´æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒShadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç›¸ç»“åˆã€‚ç›¸å…³ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT">Github</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v2">PDF</a> 19 pages, 10 tables, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è¿›ä¸€æ­¥å¾®è°ƒå¯ä»¥è·å¾—æŒç»­çš„æ”¶ç›Šï¼Œä½†ç›´æ¥å¯¹INSTRUCTæ¨¡å‹è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ç”šè‡³é€€æ­¥ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„Shadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹åº”çš„BASEæ¨¡å‹æ¥ä¼˜åŒ–INSTRUCTæ¨¡å‹çš„å¾®è°ƒã€‚é€šè¿‡å¾®è°ƒBASEæ¨¡å‹ï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ä¸Šï¼Œå®ç°äº†æ— éœ€é¢å¤–å‚æ•°ã€æ˜“äºå®æ–½ï¼Œå¹¶æ˜¾è‘—æé«˜æ€§èƒ½çš„æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒShadow-FTåœ¨ä¸»æµLLMæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬Qwen 3å’ŒLlama 3ç³»åˆ—ï¼Œæ¶µç›–ç¼–ç¨‹ã€æ¨ç†å’Œæ•°å­¦ä»»åŠ¡çš„19ä¸ªåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡è¿›ä¸€æ­¥å¾®è°ƒå¯ä»¥è·å¾—æ€§èƒ½æå‡ã€‚</li>
<li>ç›´æ¥å¯¹INSTRUCTæ¨¡å‹å¾®è°ƒå¯èƒ½å¯¼è‡´æœ‰é™æå‡æˆ–æ€§èƒ½é€€æ­¥ã€‚</li>
<li>æå‡ºçš„Shadow-FTæ¡†æ¶åˆ©ç”¨BASEæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>Shadow-FTä¸å¢åŠ é¢å¤–å‚æ•°ï¼Œæ˜“äºå®æ–½ã€‚</li>
<li>Shadow-FTæ˜¾è‘—æé«˜äº†LLMçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜Shadow-FTåœ¨å¤šä¸ªä¸»æµLLMæ¨¡å‹å’Œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Shadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d43d8b4e19d7a5c7707c84fbdf855d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171f6aafd714b9fe49957d79b1830234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3402b8f88c324bb04351c6a1c0aa18d1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-426c7f27e4ef6d0c57b2bcacc4c994a2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Reinforcing General Reasoning without Verifiers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
