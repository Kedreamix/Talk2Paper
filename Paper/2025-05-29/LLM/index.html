<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-29-更新"><a href="#2025-05-29-更新" class="headerlink" title="2025-05-29 更新"></a>2025-05-29 更新</h1><h2 id="Silence-is-Not-Consensus-Disrupting-Agreement-Bias-in-Multi-Agent-LLMs-via-Catfish-Agent-for-Clinical-Decision-Making"><a href="#Silence-is-Not-Consensus-Disrupting-Agreement-Bias-in-Multi-Agent-LLMs-via-Catfish-Agent-for-Clinical-Decision-Making" class="headerlink" title="Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making"></a>Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making</h2><p><strong>Authors:Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng</strong></p>
<p>Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the &#96;&#96;catfish effect’’ in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&amp;A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1. </p>
<blockquote>
<p>大型语言模型（LLM）在临床问题回答方面展现出了强大的潜力，最近的多代理框架通过协同推理进一步提高了诊断的准确性。然而，我们识别出了一个反复出现的问题，即静默协议（Silent Agreement），代理在没有足够的关键分析的情况下过早地达成诊断共识，特别是在复杂或模糊的情况下。我们提出了一个新的概念，即“猫鱼代理”（Catfish Agent），这是一种角色特殊化的LLM设计，旨在注入结构化异议和反对静默协议。该设计受到组织心理学中的“猫鱼效应”的启发，旨在挑战新兴共识来刺激更深入的推理。我们制定了两种机制来鼓励有效和情境感知干预：（i）一种基于案例难度的复杂性感知干预，可调节代理参与度；（ii）一种平衡批评与合作的语气校准干预。在九个医学问答和三个医学视觉问答基准测试上的评估表明，我们的方法始终优于单代理和多代理LLM框架，包括领先的商业模型，如GPT-4o和DeepSeek-R1。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在临床问答中展现出巨大潜力，多代理框架能进一步提升诊断准确性。但存在“静默协议”问题，即代理在缺乏充分分析的情况下过早达成诊断共识，特别是在复杂或模糊案例中。为此，我们提出“猫鱼代理”概念，这是一种专门设计的角色型LLM，旨在注入结构化异议并挑战静默协议。受组织心理学中的“猫鱼效应”启发，猫鱼代理旨在刺激更深入的思考并挑战现有共识。我们制定两种机制来鼓励有效且语境感知的干预：（一）复杂情况感知干预，根据案例难度调整代理参与度；（二）语气校准干预，旨在平衡批评与协作。在九个医疗问答和三个医疗视觉问答基准测试上的评估显示，我们的方法持续优于单代理和多代理LLM框架，包括领先的商业模型如GPT-4o和DeepSeek-R1。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在临床问答中展现出强大的潜力，多代理框架可进一步提升诊断准确性。</li>
<li>存在“静默协议”问题，即代理过早达成诊断共识，需引入新策略应对。</li>
<li>引入“猫鱼代理”概念，为LLM注入结构化异议，挑战静默协议。</li>
<li>猫鱼代理设计受组织心理学中的“猫鱼效应”启发，旨在刺激更深入的思考。</li>
<li>通过两种机制来鼓励有效且语境感知的干预：复杂情况感知干预和语气校准干预。</li>
<li>评估显示，猫鱼代理方法优于现有LLM框架，包括商业领先模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73d59cff8df0521b54720239d5b9457b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23eed9d80a5e3c34b6013f41ddd37082.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae0839f202633dad06695c9ea30859a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cdb58fd46d9e09baa04621ed04c3e6c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adversarial-Attacks-against-Closed-Source-MLLMs-via-Feature-Optimal-Alignment"><a href="#Adversarial-Attacks-against-Closed-Source-MLLMs-via-Feature-Optimal-Alignment" class="headerlink" title="Adversarial Attacks against Closed-Source MLLMs via Feature Optimal   Alignment"></a>Adversarial Attacks against Closed-Source MLLMs via Feature Optimal   Alignment</h2><p><strong>Authors:Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, Yang Liu</strong></p>
<p>Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP’s [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at <a target="_blank" rel="noopener" href="https://github.com/jiaxiaojunQAQ/FOA-Attack">https://github.com/jiaxiaojunQAQ/FOA-Attack</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）仍然容易受到可转移对抗样本的威胁。虽然现有方法通常通过对抗样本与目标样本之间的全局特征（如CLIP的[CLS]令牌）来实现有针对性的攻击，但它们往往忽略了补丁令牌中丰富的局部信息。这导致了对准不佳和可转移性有限，特别是在封闭源模型的情况下。为了解决这一局限性，我们提出了一种基于特征最优对齐的有针对性的可转移对抗攻击方法，称为FOA-Attack，以提高对抗转移能力。具体而言，在全局层面，我们引入了一种基于余弦相似度的全局特征损失，以对齐对抗样本与目标样本的粗粒度特征。在局部层面，考虑到Transformer内丰富的局部表示，我们利用聚类技术提取紧凑的局部模式，以减轻冗余的局部特征。然后，我们将对抗样本和目标样本之间的局部特征对齐制定为最优传输（OT）问题，并提出局部聚类最优传输损失来精细调整细粒度特征对齐。此外，我们还提出了一种动态集成模型加权策略，以在生成对抗样本时自适应地平衡多个模型的影响，从而进一步提高可转移性。在多种模型上进行的大量实验表明，所提方法具有优越性，尤其是在转移到封闭源MLLMs时，超过了最先进的方法。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/jiaxiaojunQAQ/FOA-Attack%E3%80%82">https://github.com/jiaxiaojunQAQ/FOA-Attack。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21494v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于特征最优对齐的针对性可转移对抗攻击方法（FOA-Attack），旨在提高对抗样本的迁移能力。该方法在全球层面引入基于余弦相似性的全局特征损失，以对齐对抗样本与目标样本的粗粒度特征。同时，利用Transformer中的丰富局部表示，通过聚类技术提取紧凑的局部模式，缓解冗余的局部特征。然后，将对抗样本与靶标样本之间的局部特征对齐公式化为最优传输（OT）问题，并提出局部聚类最优传输损失以优化细粒度特征对齐。此外，还提出了一种动态集成模型加权策略，以在生成对抗样本时自适应地平衡多个模型的影响，进一步提高迁移性。实验表明，该方法在多种模型上均优于现有方法，特别是在针对封闭源的多模态大型语言模型（MLLMs）上表现更出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）容易受到可转移对抗样本的攻击。</li>
<li>现有方法主要通过全局特征对齐进行针对性攻击，但忽略了局部信息。</li>
<li>FOA-Attack方法引入全局特征损失和基于余弦相似性的局部特征对齐，以提高对抗迁移能力。</li>
<li>利用Transformer中的丰富局部表示和聚类技术来提取紧凑的局部模式。</li>
<li>将局部特征对齐公式化为最优传输（OT）问题，并提出局部聚类最优传输损失。</li>
<li>动态集成模型加权策略能自适应平衡多个模型的影响，进一步提高迁移性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a146e19e730eb612d3c07ee9b9a2d757.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f93cc9714d49b25f11bff1d7e523d78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3da8e6b47caa53b44a1413b3056ca8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reinforcing-General-Reasoning-without-Verifiers"><a href="#Reinforcing-General-Reasoning-without-Verifiers" class="headerlink" title="Reinforcing General Reasoning without Verifiers"></a>Reinforcing General Reasoning without Verifiers</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du</strong></p>
<p>The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree">https://github.com/sail-sg/VeriFree</a>. </p>
<blockquote>
<p>最近，通过使用DeepSeek-R1-Zero风格的强化学习（RL）在可验证奖励上进行训练，大型语言模型（LLM）的范式转变在代码和数学推理方面取得了令人印象深刻的进展。然而，这种方法仅限于基于规则的答案验证可行的任务，并不能自然地扩展到现实世界领域，如化学、医疗、工程、法律、生物、商业和经济。目前的实用解决方案是使用额外的LLM作为基于模型的验证器，但这带来了对强大的验证器LLM的依赖、奖励作弊的脆弱性以及训练过程中在内存中维护验证器模型的实践负担等问题。为了解决这一问题并将DeepSeek-R1-Zero风格的训练扩展到一般推理领域，我们提出了一种无验证器的方法（VeriFree），它绕过答案验证，而是使用RL直接最大化生成参考答案的概率。我们将VeriFree与基于验证器的方法进行比较，并证明其在巨大的实用性好处和减少的计算需求之外，在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试上的评估中，VeriFree与基于验证器的方法相匹配甚至超越它们。此外，我们从多个角度对这种方法提供了见解：作为将策略和隐式验证器优雅地集成在一个统一模型中的方法，以及作为一种变分优化方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/VeriFree找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于DeepSeek-R1-Zero风格的强化学习（RL）在可验证奖励下训练大型语言模型（LLM），在代码和数学推理方面取得了显著进展。然而，此方法仅限于规则可验证的任务，难以自然扩展到化学、医疗、工程、法律、生物、商业和经济等真实世界领域。为解决此问题并扩展DeepSeek-R1-Zero风格的训练至通用推理领域，我们提出了无需验证器的方法（VeriFree），该方法绕过答案验证，转而使用RL直接最大化生成参考答案的概率。对比验证器方法，VeriFree具有显著的实际效益和更低的计算要求，并在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1-Zero风格的强化学习在LLM训练中推动了代码和数学推理的进步。</li>
<li>当前方法主要局限于规则可验证的任务，难以应用于真实世界领域。</li>
<li>VeriFree方法绕过答案验证，使用强化学习直接最大化生成正确答案的概率。</li>
<li>VeriFree与验证器方法相比具有实际效益和更低的计算要求。</li>
<li>VeriFree在多个基准测试中表现优异。</li>
<li>VeriFree作为一种将策略和隐式验证器优雅地集成在统一模型中的方法，具有多种优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abd1833621d3fc574f509bb61a4501f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d03a7ead67cfc16078c3ed181410e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f79b9549ae46abbbebf4c12cb745df2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners"><a href="#Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners" class="headerlink" title="Are Language Models Consequentialist or Deontological Moral Reasoners?"></a>Are Language Models Consequentialist or Deontological Moral Reasoners?</h2><p><strong>Authors:Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin</strong></p>
<p>As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens">https://github.com/keenansamway/moral-lens</a> . </p>
<blockquote>
<p>随着人工智能系统在医疗保健、法律和治理等领域的应用日益广泛，理解它们如何处理道德上复杂的场景变得至关重要。以往的研究主要集中在大型语言模型（LLM）的道德判断上，而忽视了其潜在的道德推理过程。相比之下，我们专注于对LLM提供的道德推理轨迹的大规模分析。此外，不同于之前的研究仅从少数道德困境中推断结果，我们的研究利用超过600个不同的电车问题作为探针，以揭示不同LLM内部出现的推理模式。我们介绍并测试了一种道德理由的分类法，根据两种主要的规范性伦理理论：结果论和义务论，系统地分类推理轨迹。我们的分析表明，LLM的思维链往往倾向于基于道德义务的义务论原则，而后期的解释则显著转向强调实用性的结果主义理由。我们的框架为理解LLM如何处理并阐述道德考量提供了基础，这是将LLM安全、可解释地部署于高风险决策制定环境中的关键一步。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/keenansamway/moral-lens获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）在处理道德推理时的内在机制。研究通过对超过600个不同的电车问题进行分析，发现LLM的推理链倾向于基于道德义务的德性论原则，而后期的解释则明显转向强调实用主义的结果论理由。该研究为理解LLM如何处理道德考量提供了基础，对于在安全且可解释的环境中部署LLM进行高风险决策制定具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在伦理复杂场景中的处理理解至关重要，尤其是在医疗保健、法律和治理等领域。</li>
<li>此前的研究主要关注LLM的道德判断，而本研究则着重于分析其道德推理过程。</li>
<li>通过分析超过600个不同的电车问题，发现LLM的推理链倾向于德性论原则。</li>
<li>LLM的后期解释偏向结果论理由，强调实用性。</li>
<li>研究提供了一个分类道德推理痕迹的框架，根据两种主要的伦理理论：结果论和德性论进行系统分类。</li>
<li>该研究为理解LLM如何处理道德考量奠定了基础，是向安全且可解释的高风险决策制定环境部署LLM的重要步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81c82638e8b82dcc7462f7876531149e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5310537ab54ab5b6f5dc826993a05f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a3141b963ab23c4f36dc44d93b7b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd5bc52df8235fdfb5574679b7d8270.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Policy-Optimized-Text-to-Image-Pipeline-Design"><a href="#Policy-Optimized-Text-to-Image-Pipeline-Design" class="headerlink" title="Policy Optimized Text-to-Image Pipeline Design"></a>Policy Optimized Text-to-Image Pipeline Design</h2><p><strong>Authors:Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor</strong></p>
<p>Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines. </p>
<blockquote>
<p>文本到图像生成技术已经发展超越了单一的单体模型，变为了复杂的多组件管道。这些管道结合了精细调整的生成器、适配器、上采样块甚至编辑步骤，导致图像质量得到了显著改善。然而，它们的有效设计需要大量的专业知识。最近的方法显示通过大型语言模型（LLM）自动化这一过程的希望，但它们存在两个关键的局限性：从使用数百个预定义管道生成图像所需的巨大计算量，以及在记忆训练样本之外的推广能力较差。我们引入了一种基于强化学习的新型框架来解决这些效率低下的问题。我们的方法首先训练一组奖励模型，这些模型能够直接从提示-工作流程组合中预测图像质量分数，从而在训练过程中消除了昂贵的图像生成需求。然后我们实现了两阶段训练策略：初始的工作流词汇训练，然后是基于GRPO的优化，引导模型朝着性能更高的工作流空间区域发展。此外，我们采用了一种基于无分类器引导的提升技术，沿着初始模型和GRPO调整模型之间的路径进行推演，进一步提高了输出质量。我们通过一系列的对比验证了我们的方法，结果表明它可以成功创建具有更高多样性的新流程，并产生优于现有基准线的图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21478v1">PDF</a> </p>
<p><strong>Summary</strong><br>大規模語言模型辅助的文本转图像生成框架。针对当前多组件管道设计的需求专业化问题，通过引入强化学习优化奖励模型来解决现有模型生成管道效率低的问题。直接利用奖励模型预测图像质量分数，减少训练中的图像生成成本。采用两阶段训练策略，引入无分类器指导增强技术，提高输出质量。相较于现有基线模型，新方法能够创建更具多样性和优质性的图像生成流程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转图像生成已发展至复杂的多组件管道系统，结合精细调整生成器、适配器、上采样块和编辑步骤，显著提高图像质量。</li>
<li>当前自动化设计流程存在计算需求大及泛化能力不足的两大局限。</li>
<li>引入基于强化学习的奖励模型预测图像质量分数，有效降低成本和计算需求。</li>
<li>两阶段训练策略包含初始工作流程词汇训练和基于GRPO的优化。</li>
<li>采用无分类器指导的增强技术提升输出质量。</li>
<li>与现有基线相比，新方法能够创建更加多样化和优质化的图像生成流程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7493c31768c123ea46735d176ea4d438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08509a719874ee7319d6eaaa84ad7682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ace6180579b47e9f0e9f74d668599c7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model"><a href="#Hume-Introducing-System-2-Thinking-in-Visual-Language-Action-Model" class="headerlink" title="Hume: Introducing System-2 Thinking in Visual-Language-Action Model"></a>Hume: Introducing System-2 Thinking in Visual-Language-Action Model</h2><p><strong>Authors:Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, Xuelong Li</strong></p>
<p>Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments. </p>
<blockquote>
<p>在处理物理世界的复杂任务时，人类会在实际行动之前进行缓慢思考。最近，这种思考范式在推动大型语言模型（LLM）在数字领域解决复杂任务方面取得了显著进展。然而，缓慢思考在机器人基础模型与物理世界交互中的潜力尚未得到广泛探索。在这项工作中，我们提出了Hume：一种具有价值引导的系统2思考级联动作去噪的双系统视觉语言动作（VLA）模型，探索视觉语言动作模型在灵巧机器人控制方面的人类思考能力。Hume的系统2通过扩展视觉语言动作模型的骨干网络，采用新型的价值查询头来估计预测动作的状态-动作价值，从而实现价值引导的思考。价值引导的思考是通过多次采样多个动作候选者，并根据状态-动作价值选择其中一个来进行。Hume的系统1是一个轻量级的反应型视运动策略，它接收系统2选择的动作，并执行级联动作去噪，以实现灵巧的机器人控制。在部署时，系统2以较低频率进行价值引导思考，而系统1则异步接收系统2选择的动作候选者，并实时预测流畅的动作。我们展示，Hume在多个仿真基准测试和真实机器人部署中，表现优于现有的最先进的视觉语言动作模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21432v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在物理世界中处理复杂任务时，人类在实践行动前会进行慢思考的模式。近期，这种思考模式在提升大型语言模型（LLM）解决数字领域的复杂任务方面取得了显著进展。然而，对于机器人基础模型与物理世界的交互而言，慢思考的巨大潜力尚未得到广泛探索。本文提出了Hume：一种具有价值引导的系统2思维的双系统视觉语言动作（VLA）模型，探索视觉语言动作模型的类人思考能力在灵巧机器人控制方面的应用。Hume的系统2通过增加一个价值查询头来估计预测动作的状态-动作价值，从而实现价值引导的思考。在部署时，系统2以较低频率进行价值引导的思考，而系统1则异步接收系统2选择的动作候选者并实时预测流畅动作。实验表明，Hume在多个仿真基准测试和真实机器人部署中的表现均优于现有的最先进的视觉语言动作模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类在处理物理世界的复杂任务时，会先进行慢思考再行动，这一模式最近在大型语言模型（LLM）中得到了提升。</li>
<li>虽然慢思考在数字领域的复杂任务中取得了进展，但在机器人基础模型与物理世界的交互中的应用尚未得到广泛探索。</li>
<li>Hume是一个具有价值引导的系统2思维的双系统视觉语言动作（VLA）模型，旨在探索视觉语言动作模型的类人思考能力在灵巧机器人控制方面的应用。</li>
<li>Hume的系统2通过价值查询头来估计预测动作的状态-动作价值，实现价值引导的思考模式。</li>
<li>系统1是一个轻量级的反应视觉运动策略，它接收系统2选择的动作并进行级联动作去噪，以实现灵巧的机器人控制。</li>
<li>在部署时，系统2以较低频率进行价值引导的思考，而系统1则能实时预测流畅动作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21432">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8e6ba546a7424f8ddb85e55dc59626e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc2407ea840b86bc927803ac4a12c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350e445309cd68d9210fe3c424dc0167.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diagnosing-and-Resolving-Cloud-Platform-Instability-with-Multi-modal-RAG-LLMs"><a href="#Diagnosing-and-Resolving-Cloud-Platform-Instability-with-Multi-modal-RAG-LLMs" class="headerlink" title="Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG   LLMs"></a>Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG   LLMs</h2><p><strong>Authors:Yifan Wang, Kenneth P. Birman</strong></p>
<p>Today’s cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives. </p>
<blockquote>
<p>今天的云托管应用程序和服务是复杂的系统，性能和功能不稳定可能有数十或数百个潜在的根本原因。我们的假设是，通过结合现代人工智能工具的模式匹配能力和自然的多模态RAG LLM界面，可以简化问题识别和解决方案。ARCA是一个针对这一领域的新多模态RAG LLM系统。逐步评估表明，ARCA优于最新替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21419v1">PDF</a> Published in EuroMLSys2025</p>
<p><strong>Summary</strong>:<br>现代云应用和服务的复杂性导致了性能和功能问题诊断困难。研究假设利用现代人工智能工具的匹配模式和自然语言多模态大型语言模型（LLM）接口相结合，可简化问题识别和解决方案。ARCA是一种新型的多模态LLM系统，专门为此领域设计。逐步评估显示，ARCA优于现有最佳替代方案。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>云应用和服务的复杂性导致性能或功能问题诊断困难。</li>
<li>结合人工智能工具的匹配模式和自然语言多模态LLM接口可以简化问题识别和解决方案。</li>
<li>ARCA是一种针对这一领域的新型多模态LLM系统。</li>
<li>ARCA通过逐步评估证明其性能优于现有最佳替代方案。</li>
<li>ARCA系统利用模式匹配能力进行问题识别。</li>
<li>ARCA采用的自然语言多模态接口有助于更简单地解决问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f10ef6aaa8a0d33ee9f738ae791e76c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df5d05eef47f9f1a633797ddff6b412a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation"><a href="#RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation" class="headerlink" title="RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"></a>RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</h2><p><strong>Authors:Xiao Liu, Da Yin, Zirui Wu, Yansong Feng</strong></p>
<p>Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models’ internal knowledge and would fail in domains beyond the LLMs’ knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning. </p>
<blockquote>
<p>工具可以增强大型语言模型（LLM）在复杂问题解决任务中的推理能力，但并非所有任务都有可用的工具。在没有预设工具的情况下，早期的工作已经探索了指导LLM自行生成工具的方法。然而，这些方法很大程度上依赖于模型的内部知识，并且在LLM知识范围之外的领域会失效。为了解决这一局限性，我们提出了RefTool，这是一个参考引导的自动工具创建框架，它利用结构化外部材料（如教科书）。RefTool由两个模块组成：（1）工具创建，其中LLM根据参考内容生成可执行工具，使用示例进行验证，并按层次结构将它们组织到工具箱中；（2）工具利用，其中LLM浏览工具箱结构，选择并应用适当的工具来解决问题。在因果、物理和化学基准测试上的实验表明，RefTool在平均准确率上比现有的工具创建和领域特定推理方法高出11.3%，同时成本效益高且可广泛推广。分析表明，以参考为基础的工具创建产生了准确和忠诚的工具，分层结构有助于有效的工具选择。RefTool使LLM能够克服知识局限性，证明了将工具创建以外部参考为基础对于增强和通用推理的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21413v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/xxxiaol/RefTool">https://github.com/xxxiaol/RefTool</a></p>
<p><strong>Summary</strong></p>
<p>本摘要探讨了大型语言模型（LLM）在复杂问题解决任务中的推理能力增强问题。当没有可用的预设工具时，LLM需自行生成工具，但这种方法受限于模型的内部知识，对于超出其知识范畴的领域将失效。为此，本文提出了RefTool框架，该框架利用结构化外部资料（如教科书）进行自动工具创建。RefTool包含两个模块：工具创建和工具利用。实验表明，RefTool在因果、物理和化学基准测试上的平均准确度比现有工具创建和领域特定推理方法高出11.3%，同时成本效益高且可广泛通用。这表明将工具创建根植于外部参考中对于增强和通用推理的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂问题求解中可通过工具增强推理能力。</li>
<li>在没有预设工具的情况下，LLM可自行生成工具，但受限于其内部知识。</li>
<li>RefTool框架利用结构化外部资料（如教科书）进行自动工具创建。</li>
<li>RefTool包含工具创建和工具利用两个模块。</li>
<li>实验显示，RefTool在多个领域测试中表现出较高的准确性和通用性。</li>
<li>RefTool通过根植于外部参考的工具创建，产生准确且忠实的工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-565ff994cf9e3608f1a45c7d54192219.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad7bcf12572c4bec174366f2c6639fa5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ff673cc55f6d6d92eadb47d9967ed88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce15a60ee0afeef1a17e1de7fe7165d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EquAct-An-SE-3-Equivariant-Multi-Task-Transformer-for-Open-Loop-Robotic-Manipulation"><a href="#EquAct-An-SE-3-Equivariant-Multi-Task-Transformer-for-Open-Loop-Robotic-Manipulation" class="headerlink" title="EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop   Robotic Manipulation"></a>EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop   Robotic Manipulation</h2><p><strong>Authors:Xupeng Zhu, Yu Qi, Yizhe Zhu, Robin Walters, Robert Platt</strong></p>
<p>Transformer architectures can effectively learn language-conditioned, multi-task 3D open-loop manipulation policies from demonstrations by jointly processing natural language instructions and 3D observations. However, although both the robot policy and language instructions inherently encode rich 3D geometric structures, standard transformers lack built-in guarantees of geometric consistency, often resulting in unpredictable behavior under SE(3) transformations of the scene. In this paper, we leverage SE(3) equivariance as a key structural property shared by both policy and language, and propose EquAct-a novel SE(3)-equivariant multi-task transformer. EquAct is theoretically guaranteed to be SE(3) equivariant and consists of two key components: (1) an efficient SE(3)-equivariant point cloud-based U-net with spherical Fourier features for policy reasoning, and (2) SE(3)-invariant Feature-wise Linear Modulation (iFiLM) layers for language conditioning. To evaluate its spatial generalization ability, we benchmark EquAct on 18 RLBench simulation tasks with both SE(3) and SE(2) scene perturbations, and on 4 physical tasks. EquAct performs state-of-the-art across these simulation and physical tasks. </p>
<blockquote>
<p>Transformer架构可以通过联合处理自然语言指令和3D观察结果，有效地从演示中学习语言条件化的多任务3D开环操作策略。然而，尽管机器人策略和语言指令本身就编码了丰富的3D几何结构，但标准Transformer缺乏内置的几何一致性保证，通常在场景的SE(3)转换下表现出不可预测的行为。在本文中，我们利用SE(3)等价性作为策略和语言的共同关键结构属性，并提出EquAct——一种新型的SE(3)等价多任务Transformer。EquAct在理论上保证了SE(3)的等价性，并包含两个关键组件：（1）一种高效的基于点云的SE(3)等价U-net网络，具有球形傅里叶特征用于策略推理；（2）用于语言调节的SE(3)不变特征线性调制（iFiLM）层。为了评估其空间泛化能力，我们在带有SE(3)和SE(2)场景扰动的18个RLBench仿真任务以及4个实际任务上测试了EquAct的性能。在仿真和实际任务中，EquAct均表现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21351v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了Transformer架构在通过自然语言指令和三维观察联合处理来学习语言条件下的多任务三维开放循环操作策略方面的有效性。然而，尽管机器人策略和语言指令本身就包含了丰富的三维几何结构，但标准Transformer缺乏内置几何一致性保证，这会导致在SE(3)场景变换下行为不可预测。因此，本文利用SE(3)等价性作为策略和语言的共享关键结构特性，提出一种新型的SE(3)等价多任务Transformer模型EquAct。EquAct包括两个关键组成部分：一是用于策略推理的基于点云的SE(3)等价U-net，二是用于语言调节的SE(3)不变特征线性调制层。实验表明，EquAct在模拟任务上具备卓越的空间泛化能力，无论是在SE(3)还是SE(2)场景扰动下均表现出卓越性能。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Transformer架构能通过学习语言条件下的多任务三维操作策略来处理自然语言指令和三维观察联合数据。</li>
<li>尽管Transformer有能力处理丰富信息，但它们缺乏内置几何一致性保证。</li>
<li>SE(3)等价性在机器人操作和指令中都存在且是重要的。</li>
<li>EquAct是一个新型SE(3)等价多任务Transformer模型，结合了基于点云的U-net和特征线性调制层来确保几何一致性。</li>
<li>EquAct具备出色的空间泛化能力，可以在不同场景中预测准确的行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0e64aca19b524ea3acb908e64aef572f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d036b21493a4d4422919d26a39b0b0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9926641681499b5d6754d371fe491c5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b2e3fad42898c6fb16b1d76781d7176.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DynamicVL-Benchmarking-Multimodal-Large-Language-Models-for-Dynamic-City-Understanding"><a href="#DynamicVL-Benchmarking-Multimodal-Large-Language-Models-for-Dynamic-City-Understanding" class="headerlink" title="DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic   City Understanding"></a>DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic   City Understanding</h2><p><strong>Authors:Weihao Xuan, Junjue Wang, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, Naoto Yokoya</strong></p>
<p>Multimodal large language models have demonstrated remarkable capabilities in visual understanding, but their application to long-term Earth observation analysis remains limited, primarily focusing on single-temporal or bi-temporal imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework for analyzing long-term urban dynamics through remote sensing imagery. Our suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42 megacities in the U.S. from 2005 to 2023, organized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding tasks, from fundamental change detection (pixel-level) to quantitative analyses (regional-level) and comprehensive urban narratives (scene-level), capturing diverse urban dynamics including expansion&#x2F;transformation patterns, disaster assessment, and environmental challenges. We evaluate 17 state-of-the-art multimodal large language models and reveal their limitations in long-term temporal understanding and quantitative analysis. These challenges motivate the creation of DVL-Instruct, a specialized instruction-tuning dataset designed to enhance models’ capabilities in multi-temporal Earth observation. Building upon this dataset, we develop DVLChat, a baseline model capable of both image-level question-answering and pixel-level segmentation, facilitating a comprehensive understanding of city dynamics through language interactions. </p>
<blockquote>
<p>多模态大型语言模型在视觉理解方面表现出了显著的能力，但它们在长期地球观测分析方面的应用仍然有限，主要集中在单时态或双时态影像上。为了解决这一差距，我们引入了DVL-Suite，这是一个通过遥感影像分析长期城市动态的全面框架。我们的套件包含2005年至2023年期间美国42个特大城市的15,063张高分辨率（1.0米）多时相图像，分为两个组成部分：DVL-Bench和DVL-Instruct。DVL-Bench包含七个城市理解任务，从基本的检测变化（像素级）到定量分析（区域级）和综合城市叙事（场景级），捕捉包括扩张&#x2F;转型模式、灾害评估和环境挑战在内的各种城市动态。我们评估了17个最先进的多模态大型语言模型，并揭示了它们在长期时间理解和定量分析方面的局限性。这些挑战促使我们创建了DVL-Instruct，这是一个专门设计的指令微调数据集，旨在增强模型在多时态地球观测方面的能力。基于该数据集，我们开发了DVLChat，这是一个基线模型，能够进行图像级别的问答和像素级别的分割，通过语言交互促进对城市动态的全面理解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21076v1">PDF</a> </p>
<p><strong>Summary</strong>：本文介绍了多模态大型语言模型在分析长期地球观测数据方面的局限性，并针对这一问题提出了DVL-Suite框架。该框架包含用于分析长期城市动态的高分辨率多时态图像，由DVL-Bench和DVL-Instruct两部分组成。前者提供了一系列城市理解任务，后者则设计了一个专门的指令调整数据集以增强模型在多时态地球观测方面的能力。基于该数据集，开发出了能够进行图像级问答和像素级分割的DVLChat基线模型，有助于全面理解城市动态。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型语言模型在视觉理解方面表现出卓越的能力，但在长期地球观测数据分析方面的应用仍然有限。</li>
<li>引入DVL-Suite框架，旨在分析长期城市动态。</li>
<li>DVL-Suite包含高分辨率多时态图像，覆盖美国42个超大城市从2005年到2023年的数据。</li>
<li>DVL-Bench包含从基础变化检测到区域定量分析和城市综合叙事等多个任务，捕捉多样的城市动态。</li>
<li>对当前的多模态大型语言模型进行了评估，揭示了它们在长期时间理解和定量分析方面的局限性。</li>
<li>为增强模型在多时态地球观测方面的能力，创建了DVL-Instruct这一专用指令调整数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5850b785a0f23f8f805fd359c8abd1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef34caaa2f3f7cf1a85db95f4a45f337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b1f94fa97f8d50f0d38a72f83f196e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d14d02e7e3d27668dd9627279ccce42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d412c08984be6088e49f63e5497f39f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-364546124933f612d2b3a787d043e772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e0cf1f84d92f307c57a98a208d8a4e3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Respond-to-Change-with-Constancy-Instruction-tuning-with-LLM-for-Non-I-I-D-Network-Traffic-Classification"><a href="#Respond-to-Change-with-Constancy-Instruction-tuning-with-LLM-for-Non-I-I-D-Network-Traffic-Classification" class="headerlink" title="Respond to Change with Constancy: Instruction-tuning with LLM for   Non-I.I.D. Network Traffic Classification"></a>Respond to Change with Constancy: Instruction-tuning with LLM for   Non-I.I.D. Network Traffic Classification</h2><p><strong>Authors:Xinjie Lin, Gang Xiong, Gaopeng Gou, Wenqi Dong, Jing Yu, Zhen Li, Wei Xia</strong></p>
<p>Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooL’s effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooL’s instruction tuning approach. </p>
<blockquote>
<p>加密流量分类在网络安全中是一项极具挑战性的任务，原因在于需要从内容无关的流量数据中提取出稳健的特征。现有方法面临的关键问题包括：（1）由于依赖封闭世界假设而导致的分布漂移，限制了适应现实世界变化模式的能力；（2）对标记数据的依赖限制了其在标记数据稀缺或不可用场景下的适用性。大型语言模型（LLM）在提供可推广至广泛任务的解决方案方面展现出显著潜力，并在多个专业领域取得了显著成功。然而，其在流量分析中的有效性仍受到适应流量领域独特要求方面的挑战的限制。在本文中，我们介绍了一种新型的流量表示模型，名为加密流量出分布指令调整与LLM（ETooL），它通过自监督指令调整范式将LLM与流量结构知识相结合。该框架建立了文本信息与流量交互之间的连接。ETooL在监督学习和零样本流量分类任务中表现出更稳健的分类性能和更好的泛化能力。值得注意的是，它在F1分数上取得了显著改进：APP53（I.I.D.）提高到93.19%（6.62%）和92.11%（4.19%），APP53（O.O.D.）提高到74.88%（18.17%）和72.13%（15.15%），以及ISCX-Botnet（O.O.D.）提高到95.03%（9.16%）和81.95%（12.08%）。此外，我们还构建了NETD流量数据集，以支持动态分布偏移，并使用它来验证ETooL在不同分布条件下的有效性。我们还评估了ETooL指令调整方法在提高效率方面的成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20866v1">PDF</a> IEEE Transactions on Information Forensics and Security (TIFS) camera   ready, 15 pages, 6 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用大型语言模型（LLM）解决网络加密流量分类问题的新方法。针对现有方法面临的挑战，如分布漂移和依赖标签数据的问题，提出了一种名为ETooL的新型流量表示模型。ETooL通过自监督指令调整范式将LLM与流量结构知识相结合，建立文本信息与流量交互之间的联系。在监督和零样本流量分类任务中，ETooL表现出更稳健的分类性能和优越的泛化能力，并在多个数据集上实现了显著的F1分数改进。此外，还构建了用于支持动态分布转移的NETD流量数据集，并验证了ETooL在不同分布条件下的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>加密流量分类是网络安全的重大挑战，需要从内容无关的流量数据中提取稳健特征。</li>
<li>现有方法面临分布漂移和依赖标签数据的问题。</li>
<li>大型语言模型（LLM）在广泛的任务中展现出可推广的解决方案的潜力。</li>
<li>ETooL模型通过自监督指令调整范式整合LLM和流量结构知识。</li>
<li>ETooL在流量分类任务中表现出优异的性能和泛化能力。</li>
<li>ETooL在多个数据集上的F1分数有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20866">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a8dc23243c0489195a53f1125953afd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-715ebd06b6db2c9abca45e9edae5919b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ceca2e62709e1846f109ea979fe5039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78dba9ebbb3c4ef630560918e982748.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leaner-Transformers-More-Heads-Less-Depth"><a href="#Leaner-Transformers-More-Heads-Less-Depth" class="headerlink" title="Leaner Transformers: More Heads, Less Depth"></a>Leaner Transformers: More Heads, Less Depth</h2><p><strong>Authors:Hemanth Saratchandran, Damien Teney, Simon Lucey</strong></p>
<p>Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that “bigger means better”, leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark). </p>
<blockquote>
<p>Transformer通过利用注意力机制捕捉大型数据集中的复杂模式，重塑了机器学习，从而大大提高了性能。这一成功也催生了一种“越大越好”的信念，导致模型规模不断增长。本文挑战了这一理念，表明许多现有Transformer可能不必要地过大。我们发现了一个重新定义了多头注意力角色的理论原理。多头的一个重要优点是改善注意力块的条件。我们利用这一理论见解，重新设计了流行的架构，增加了头部的数量。在实践中，条件改善的显著证明了这一点，可以减小模型深度，同时减少高达30%~50%的参数数量，并保持准确性。在各种规模的基于Transformer的架构中，我们在计算机视觉（ImageNet-1k）以及语言和序列建模任务（GLUE基准测试、TinyStories和Long-Range Arena基准测试）上获得了一致的好处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20802v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>转换器通过利用注意力机制捕捉大型数据集中的复杂模式，从而重塑了机器学习领域，并显著提高了性能。这一成功促使人们认为“越大越好”，导致模型规模不断增大。本文挑战了这一理念，表明许多现有转换器可能不必要地过大。我们发现了一个重新定义了多头注意力作用的理论原则。多头的一个重要好处在于改善了注意力的条件。我们利用这一理论见解，重新设计了具有增加的头数的流行架构。条件的改善在实践中证明是如此重要，可以减少模型深度，同时减少高达30-50%的参数数量而保持准确性。我们在计算机视觉（ImageNet-1k）、语言及序列建模（GLUE基准测试、TinyStories和Long-Range Arena基准测试中的任务）的各种规模的基于转换器的架构中都获得了一致的好处。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>转换器通过注意力机制捕捉复杂模式，提升了机器学习的性能。</li>
<li>“越大越好”的理念导致模型规模不断增大。</li>
<li>本文挑战了现有转换器可能不必要过大的观点。</li>
<li>发现多头注意力可以改善注意力的条件。</li>
<li>重新设计了具有增加头数的流行架构，显著改善了模型的性能。</li>
<li>模型深度可以减少，同时减少高达30-50%的参数数量而保持准确性。</li>
<li>在多种任务和基于转换器的架构中都获得了一致的好处。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-43713a3c0843206bf6ff589101ada908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38fe184411e6005e651dfcc345af1a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9424e3e87349da687cc62920ceb0225b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf69b2d2b59445e0dbb72124d48b92d5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Transformation-from-Natural-Language-to-Signal-Temporal-Logic-Using-LLMs-with-Diverse-External-Knowledge"><a href="#Enhancing-Transformation-from-Natural-Language-to-Signal-Temporal-Logic-Using-LLMs-with-Diverse-External-Knowledge" class="headerlink" title="Enhancing Transformation from Natural Language to Signal Temporal Logic   Using LLMs with Diverse External Knowledge"></a>Enhancing Transformation from Natural Language to Signal Temporal Logic   Using LLMs with Diverse External Knowledge</h2><p><strong>Authors:Yue Fang, Zhi Jin, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan</strong></p>
<p>Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets. </p>
<blockquote>
<p>时间逻辑（TL），特别是信号时间逻辑（STL），能够实现精确的形式化规范，使其在自动驾驶和机器人等网络物理系统中得到广泛应用。自动将自然语言（NL）转换为STL是一种克服手动转换限制的吸引人的方法，手动转换耗时且易出错。然而，由于数据集缺乏，自动转换目前面临重大挑战，尚未被完全探索。在本文中，我们提出了一个名为STL-Diversity-Enhanced（STL-DivEn）的NL-STL数据集，包含16,000个富含多种模式的样本。为了开发该数据集，我们首先手动创建一个小规模的NL-STL对种子集。接下来，通过聚类确定代表性示例，并用于指导大型语言模型（LLM）生成额外的NL-STL对。最后，通过严格的基于规则的过滤器和人工验证，确保多样性和准确性。此外，我们介绍了知识引导STL转换（KGST）框架，这是一种将自然语言转换为STL的新型方法，基于外部知识采用先生成后精细化的过程。统计分析表明，STL-DivEn数据集比现有的NL-STL数据集更具多样性。而且，基于指标和人工评估都表明，我们的KGST方法在STL-DivEn和DeepSTL数据集上的转换精度超过了基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20658v1">PDF</a> 13 pages, 5 figures, published to ACL</p>
<p><strong>Summary</strong></p>
<p>自然语言与信号时序逻辑（STL）之间的自动转换技术能够克服手动转换方法的局限，如耗时与易出错等问题。然而，由于缺乏数据集，自动转换技术面临挑战。本研究提出了一个名为STL-DivEn的NL-STL数据集，包含16,000个样本，涵盖多样的模式。此外，研究引入了基于知识的STL转换（KGST）框架，将自然语言转化为STL。统计分析显示，STL-DivEn数据集具有更高的多样性，并且与基准模型相比，KGST在STL-DivEn和DeepSTL数据集上的转换准确性更高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Temporal Logic（特别是STL）在自主驾驶和机器人等跨物理系统中有着广泛应用价值。</li>
<li>自动将自然语言（NL）转换为STL可有效克服手动转换方法存在的耗时与易错缺陷。</li>
<li>缺乏数据集是当前自动转换技术面临的主要挑战之一。</li>
<li>STL-DivEn数据集包含丰富的多样模式样本，共计包含超过一定数量的样本数据，能为此问题提供更全面深入的研究资料。</li>
<li>KGST框架能有效实现自然语言到STL的转换。</li>
<li>STL-DivEn数据集相较于现有NL-STL数据集展现出更高的多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20658">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbf7fd60c2b529998c87f9721aa93dbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b83f03b54c2a0f303f2caceddb0b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6276847648861f2c394a49037c3fe11.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-shot-Entropy-Minimization"><a href="#One-shot-Entropy-Minimization" class="headerlink" title="One-shot Entropy Minimization"></a>One-shot Entropy Minimization</h2><p><strong>Authors:Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai</strong></p>
<p>We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/zitian-gao/one-shot-em">https://github.com/zitian-gao/one-shot-em</a>. </p>
<blockquote>
<p>我们训练了13440个大型语言模型，发现熵最小化只需一个未标记的数据和10步优化，就能实现与基于规则强化学习中数千个数据和精心设计奖励所获得的性能改进相当甚至更高的性能改进。这一令人印象深刻的结果可能会促使人们重新思考大型语言模型的训练后范式。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zitian-gao/one-shot-em%E4%B8%AD%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/zitian-gao/one-shot-em中查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20282v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong>：我们训练了13440个大型语言模型，发现熵最小化仅需一个未标记数据点和10步优化，即可实现与基于规则强化学习数千个数据和精心设计奖励所获得性能的改进相当甚至更高的性能。这一结果可能会对大型语言模型的后期训练范式产生反思性思考。我们的代码在GitHub上可用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>训练了大规模语言模型数量达13,440个。</li>
<li>熵最小化方法表现出令人瞩目的结果。</li>
<li>仅需单个未标记数据和10步优化，就能实现显著的性能提升。</li>
<li>该方法性能与基于规则强化学习相比具有竞争力。</li>
<li>该研究挑战了现有的大型语言模型训练范式。</li>
<li>研究结果可能导致对语言模型训练的新思考。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9db172948c5bdc25ecbab54f50e22c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc6a9a90865a2914717c82763111fd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-100130e3caadf74877730fe07b190ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e0fa75a4bc443c2c103073338d604a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2448b5cafd3c167a317aaa0d1c19c17.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="When-Two-LLMs-Debate-Both-Think-They’ll-Win"><a href="#When-Two-LLMs-Debate-Both-Think-They’ll-Win" class="headerlink" title="When Two LLMs Debate, Both Think They’ll Win"></a>When Two LLMs Debate, Both Think They’ll Win</h2><p><strong>Authors:Pradyumna Shyama Prasad, Minh Nhat Nguyen</strong></p>
<p>Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed &gt;&#x3D;75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models’ private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLM outputs are deployed without careful review in assistant roles or agentic settings. </p>
<blockquote>
<p>在面对对立观点时，大型语言模型（LLMs）能否准确地调整其信心？我们基于之前对基于静态事实问题的校准测量研究，在动态、对抗性的辩论环境中评估大型语言模型（LLM）。我们独特地结合了两个现实因素：（a）多轮形式要求模型随着新信息的出现而更新信念；（b）零和结构控制任务相关的不确定性，因为相互的高信心主张意味着系统性过度自信。我们组织了10个最前沿的大型语言模型参与的三轮政策辩论，共进行60场辩论，每轮结束后模型私下评估其获胜的信心（0-100）。我们观察到五种令人担忧的模式：（1）系统性过度自信：辩论开始时模型的平均初始信心为72.9%，而理性基线为50%。（2）信心升级：辩论进展时，辩手并未减少信心，反而增加了获胜概率，至终轮平均达到83%。（3）相互高估：在61.7%的辩论中，双方同时声称胜利概率大于等于75%，这是一个逻辑上的不可能。（4）持续的自我辩论偏见：模型在与相同副本的辩论中信心从64.1%增加到75.2%；即使明确告知他们获胜几率正好是50%，信心仍然上升（从50.0%上升到57.1%）。（5）不一致的私人推理：模型的私下笔记想法有时与其公开的信心评分不一致，这引发了关于思考过程可靠性的担忧。这些结果表明，在动态多轮任务中，大型语言模型缺乏准确自我评估或更新其信念的能力；这是一个令人担忧的问题，因为大型语言模型的输出在没有仔细审查的情况下被部署在助理角色或代理环境中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19184v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMs在动态多轮辩论环境中的表现存在过度自信问题。研究中观察到五大问题，包括系统性过度自信、信心递增、双方互相高估、持续自我辩论偏见以及私人推理不一致等。这些问题表明LLMs在动态环境中缺乏准确自我评估或更新信念的能力，对于其在助理角色或代理环境中的部署应用构成了重大担忧。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs表现出系统性过度自信，初始信心平均值为72.9%，远高于理性基线值50%。</li>
<li>在辩论过程中，LLMs的信心不是降低而是递增，最终轮次平均达到83%。</li>
<li>在超过一半的辩论中，双方同时声称胜利概率大于或等于75%，这是一个逻辑上的不可能现象。</li>
<li>在自我辩论场景中，LLMs的信心进一步增加，即使被告知获胜几率是精确的50%，信心仍然上升。</li>
<li>LLMs的私人推理与其公开信心评级有时存在不一致，引发对连贯性推理的疑虑。</li>
<li>LLMs在动态、多轮任务中缺乏准确自我评估或更新信念的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9880e37152e66f0c9d8ab88135d9861b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-407e38dccdb8542ada5658d3b74f4239.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d2bdd9758b5d64689e9d5a53f8bb548.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Exact-Expressive-Power-of-Transformers-with-Padding"><a href="#Exact-Expressive-Power-of-Transformers-with-Padding" class="headerlink" title="Exact Expressive Power of Transformers with Padding"></a>Exact Expressive Power of Transformers with Padding</h2><p><strong>Authors:William Merrill, Ashish Sabharwal</strong></p>
<p>Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer’s expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers’ expressive power: with polylogarithmic looping, padded transformers converge to the class $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} &#x3D; \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought. </p>
<blockquote>
<p>“链式思维是基于转换器的大型语言模型（LLM）提高计算能力的自然推理时间方法，但会付出顺序解码的代价。是否存在一种更有效的方法来扩展转换器的表达能力而不增加参数？我们将填充标记作为并行测试时间计算的转换器形式。我们证明了硬平均注意力、带有多项式填充的掩码预范转换器精确地收敛于极可并行化问题的类别$\sf TC^0$。虽然已知$\sf TC^0$的上界，但证明匹配的下界一直难以捉摸。此外，我们的新分析揭示了填充转换器的精确扩展能力，当它与另一种推理时间计算相结合时，即通过循环动态增加深度。我们的核心技术贡献是展示填充如何帮助引入完整问题和归约的概念，这些一直是经典复杂性理论的核心。借助这个新工具，我们证明了带有输入长度为n的对数d次循环的填充转换器精确地识别出中等并行化问题的类别$\sf TC^d$。因此，填充和循环一起系统地扩展了转换器的表达能力：具有多项式对数循环的填充转换器收敛到类别$\sf NC$，这是在不失去并行性的情况下所能期望的最佳结果（除非$\sf NC &#x3D; \sf P$）。因此，我们的研究结果进一步鼓励探索填充和循环作为可并行化的替代链思维的方法。”</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18948v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文主要探讨了增加基于转换器的大型语言模型（LLM）的计算能力的自然推理时间方法链式思维，并分析其面临的顺序解码成本问题。本文考虑在测试阶段通过并行化计算方式利用带有填充符号的转换器。研究结果显示，使用平均硬注意力、带有多项式填充的掩码预标准化转换器可以精确解决极可并行化问题类别TC^0。此外，结合另一种推理时间计算方式——动态增加深度循环，本文揭示了填充转换器精确扩展能力的具体表现。文章的核心技术贡献在于展示了填充如何帮助带来传统复杂性理论基石的完全问题和减少的概念，并将它们应用于转换器的正式研究。通过填充和循环结合，本文系统地扩展了转换器的表达能力：在输入长度为n的情况下，具有对数深度循环的填充转换器能够精确地识别出适度并行化问题类别TC^d。因此，填充和循环作为可并行化的替代链式思维的方法值得进一步探索。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>链式思维是一种用于增加基于转换器的LLM的计算能力的方法，但它以顺序解码成本为代价。</li>
<li>带有填充符号的转换器可作为测试阶段的一种并行化计算形式。</li>
<li>平均硬注意力、带有多项式填充的掩码预标准化转换器可以精确解决极可并行化问题类别TC^0。</li>
<li>结合动态增加深度循环的填充转换器能够精确地扩展其能力，解决适度并行化问题类别TC^d。</li>
<li>填充帮助引入传统复杂性理论中的完全问题和减少的概念，并将其应用于转换器的正式研究。</li>
<li>通过结合填充和循环，转换器的表达能力得到系统扩展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-14e60179d6617371d199d0aab84d4985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5be8500752ca8ba832a2afc43c653e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Models’ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillation’s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>R1风格的强化学习（RL）显著增强了大型语言模型的推理能力，但基于规则的RL背后的机制仍然不清楚。我们发现小规模SFT对RL有重大影响，但效率较低。为了解释我们的观察，我们提出了一个分析框架，通过测量样本效应比较SFT和RL的效率。假设分析表明，SFT效率受限于训练数据。在我们的分析指导下，我们提出了再蒸馏技术，这是一种通过小规模蒸馏对预训练模型进行微调的技术，蒸馏来源于RL训练的策略。在Knight &amp; Knave和MATH数据集上的实验证明了再蒸馏的惊人效率：再蒸馏模型使用较少的样本和计算就能达到RL性能。经验验证表明，样本效应是性能改进的良好指标。因此，在K&amp;K数据集上，我们只用1K个SFT样本的再蒸馏Qwen2.5-1.5B模型超越了DeepSeek-V3-0324。在MATH上，使用再蒸馏的500个样本对Qwen2.5-1.5B进行微调，可与没有RL的指令调优变体相匹配。我们的工作解释了R1风格RL中的几个有趣现象，揭示了其经验成功的机制。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v2">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong></p>
<p>R1风格的强化学习（RL）能显著提升大型语言模型的推理能力，但其背后的机制尚不清楚。研究发现小规模的有监督微调（SFT）对RL有重要影响，但效率不高。为此，提出了分析框架，通过衡量样本效应来比较SFT和RL的效率。理论分析和实证研究均表明，SFT的效率受限于训练数据。基于这些分析，提出了重蒸馏技术，它通过小规模的蒸馏对预训练模型进行微调，从RL训练的决策中学习。实验表明，重蒸馏模型在样本数量和计算量远小于RL的情况下达到与其相近的性能。经验验证显示样本效应是性能提升的良好指标。最终，我们的工作解释了R1风格RL中的几个有趣现象，揭示了其经验成功的机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1风格的强化学习能够显著增强大型语言模型的推理能力。</li>
<li>小规模的有监督微调对强化学习有重要影响，但其效率有待提高。</li>
<li>通过分析框架和样本效应衡量，对SFT和RL的效率进行了比较。</li>
<li>样本效应是评估性能提升的重要指标。</li>
<li>重蒸馏技术通过小规模的蒸馏从RL训练的决策中学习，提高了模型的效率。</li>
<li>实验证明重蒸馏模型在样本和计算需求方面优于RL。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da9dc98994f59ee1c05654847fe410ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model’s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>思维链推理已经显著提高了大型语言模型（LLM）在各种领域的性能。然而，这种推理过程一直被限制在文本空间内，限制了其在视觉密集型任务中的有效性。为了解决这一局限性，我们引入了像素空间推理的概念。在这一新颖框架下，视觉语言模型（VLM）配备了一系列视觉推理操作，例如放大和选择帧。这些操作使VLM能够直接检查、质疑和从视觉证据中进行推断，从而提高视觉任务的推理保真度。在VLM中培养这种像素空间推理能力面临着显著挑战，包括模型的初始能力不均衡和对新引入的像素空间操作的抵触。我们通过两阶段训练方法来解决这些挑战。第一阶段通过合成推理轨迹进行指令调整，使模型熟悉新型视觉操作。接下来，强化学习（RL）阶段利用基于好奇心的奖励方案来平衡像素空间推理和文本推理之间的探索。借助这些视觉操作，VLM可以与复杂的视觉输入进行交互，如信息丰富的图像或视频，以主动收集必要信息。我们证明，该方法显著提高了VLM在多种视觉推理基准测试上的性能。我们的7B模型实现了V* bench的84%、TallyQA-Complex的74%和InfographicsVQA的84%，这是迄今为止任何开源模型所取得的最高精度。这些结果突显了像素空间推理的重要性以及我们框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Pixel-Reasoner/">https://tiger-ai-lab.github.io/Pixel-Reasoner/</a>,   Hands-on Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner">https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner</a></p>
<p><strong>摘要</strong></p>
<p>链式思维推理已显著提升了大型语言模型（LLM）在各领域的性能。然而，这种推理过程仅限于文本空间，使其在视觉密集型任务中的有效性受限。为解决这一局限性，我们引入像素空间推理的概念。在此新框架下，视觉语言模型（VLM）配备了一系列视觉推理操作，如放大和选择帧。这些操作使VLM能够直接检查、询问和推断视觉证据，从而提高视觉任务的推理保真度。培养VLM中的像素空间推理能力带来了显著挑战，包括模型的初始能力不均衡和对新引入的像素空间操作的接受度低。我们通过两阶段训练方法来应对这些挑战。第一阶段通过合成推理轨迹进行指令调整，使模型熟悉新的视觉操作。接下来，强化学习（RL）阶段利用基于好奇心的奖励方案来平衡像素空间推理和文本推理之间的探索。这些视觉操作使VLM能够与复杂视觉输入（如信息丰富的图像或视频）进行交互，主动收集必要信息。我们的方法显著提高了VLM在多种视觉推理基准测试上的性能。我们的7B模型在V*基准测试上达到84%、TallyQA-Complex上达到74%、InfographicsVQA上达到84%，成为迄今为止任何开源模型中性能最高的。这些结果突显了像素空间推理的重要性以及我们框架的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>链式思维推理已提升LLM性能，但局限于文本空间，在视觉任务中的效果有限。</li>
<li>引入像素空间推理概念，使VLM具备直接处理视觉证据的能力。</li>
<li>VLM面临初始能力不均衡和对新视觉操作接受度低的挑战。</li>
<li>采用两阶段训练法：第一阶段通过指令调整模型熟悉视觉操作，第二阶段利用强化学习平衡像素空间和文本推理的探索。</li>
<li>VLM能通过复杂视觉输入主动收集信息。</li>
<li>方法显著提高VLM在多种视觉推理基准测试上的性能。</li>
<li>7B模型在多个测试上达到或超越先前开源模型的最高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b8da341c6535770c2f92380e0df79d31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b1106d4ee92d009b154b01b7626b30.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs"><a href="#STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs" class="headerlink" title="STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"></a>STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</h2><p><strong>Authors:Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1’s anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1">https://github.com/zongzhao23/STAR-R1</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在多种任务中展现出卓越的能力，但在空间推理方面却远远落后于人类。我们通过转换驱动视觉推理（TVR）来研究这一差距，这是一项具有挑战性的任务，要求在不同的观点下识别图像中的对象转换。虽然传统的有监督微调（SFT）无法在跨视图设置中生成连贯的推理路径，而稀疏奖励强化学习（RL）则面临效率低下和探索缓慢的问题。为了解决这些局限性，我们提出了STAR-R1，这是一个结合单阶段RL范式和针对TVR量身定制的精细奖励机制的新框架。具体来说，STAR-R1奖励部分正确性，同时惩罚过度枚举和被动不作为，从而实现有效的探索和精确推理。综合评估表明，STAR-R1在所有11项指标上均达到最新技术性能水平，在跨视图场景中比SFT高出23%。进一步的分析揭示了STAR-R1的人类行为特征，并突出了其在改进空间推理方面比较所有对象的独特能力。我们的工作为推进MLLMs和推理模型的研究提供了关键见解。代码、模型权重和数据将在<a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/zongzhao23/STAR-R1上公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了多模态大型语言模型在跨视图转换驱动视觉推理任务上的性能差距，并提出了名为STAR-R1的新型框架。该框架结合了单阶段强化学习范式和针对视觉推理任务的精细奖励机制，实现了高效探索和精确推理。实验结果显示，STAR-R1在所有11项指标上均达到最佳性能，在跨视图场景下较传统监督微调方法提高了23%。这为推进多模态语言模型和推理模型的研究提供了重要见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在跨视图转换驱动视觉推理任务上仍存在与人类显著差距。</li>
<li>提出了一种新型框架STAR-R1，结合了单阶段强化学习范式和精细奖励机制，以解决现有方法的局限性。</li>
<li>STAR-R1通过奖励部分正确性和惩罚过度枚举及被动行为，实现了高效探索和精确推理。</li>
<li>实验结果显示，STAR-R1在跨视图场景下的性能较传统监督微调方法提高了23%，达到最佳性能。</li>
<li>STAR-R1展现出类人行为特征，并具备比较所有对象以改善空间推理的独特能力。</li>
<li>研究为推进多模态语言模型和推理模型的研究提供了关键见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e615d0760922e8863e3f12672d2c0fc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3175477c62e1924b40260c938fd5bb51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64d507ff00417d60ec92ecb7b99014a1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务上通过进一步的微调持续受益。然而，我们观察到直接调整INSTRUCT（即指令调整）模型通常只会带来微小的改进，甚至会导致性能下降。值得注意的是，这些INSTRUCT变体的基础配套BASE模型包含高度相似的权重值（例如Llama 3.1 8B平均不到2%）。因此，我们提出了一种新的Shadow-FT框架，利用相应的BASE模型来调整INSTRUCT模型。关键思路是对BASE模型进行微调，然后将学习到的权重更新直接应用到INSTRUCT模型。我们提出的Shadow-FT不会引入额外的参数，易于实现，并能显著提高性能。我们在主流的LLM上进行了广泛的实验，如Qwen 3和Llama 3系列，并在涵盖编码、推理和数学任务的19个基准测试上对其进行了评估。实验结果表明，Shadow-FT持续优于传统的全参数和参数高效的调整方法。进一步的分析表明，Shadow-FT可应用于多模态大型语言模型（MLLMs）并与直接偏好优化（DPO）相结合。相关代码和权重可在<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT">Github</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v2">PDF</a> 19 pages, 10 tables, 6 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过进一步微调可以获得持续的收益，但直接对INSTRUCT模型进行微调会导致性能提升有限甚至退步。本研究提出一种新型的Shadow-FT框架，利用对应的BASE模型来优化INSTRUCT模型的微调。通过微调BASE模型，然后将学习到的权重更新直接应用到INSTRUCT模型上，实现了无需额外参数、易于实施，并显著提高性能的效果。实验证明，Shadow-FT在主流LLM模型上表现优异，包括Qwen 3和Llama 3系列，涵盖编程、推理和数学任务的19个基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs通过进一步微调可以获得性能提升。</li>
<li>直接对INSTRUCT模型微调可能导致有限提升或性能退步。</li>
<li>提出的Shadow-FT框架利用BASE模型进行微调。</li>
<li>Shadow-FT不增加额外参数，易于实施。</li>
<li>Shadow-FT显著提高了LLM的性能。</li>
<li>实验证明Shadow-FT在多个主流LLM模型和任务上表现优异。</li>
<li>Shadow-FT可应用于多模态大型语言模型（MLLMs）并与直接偏好优化（DPO）结合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d43d8b4e19d7a5c7707c84fbdf855d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171f6aafd714b9fe49957d79b1830234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3402b8f88c324bb04351c6a1c0aa18d1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-426c7f27e4ef6d0c57b2bcacc4c994a2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-29  Reinforcing General Reasoning without Verifiers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
