<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-29  Lunguage A Benchmark for Structured and Sequential Chest X-ray   Interpretation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-29-更新"><a href="#2025-05-29-更新" class="headerlink" title="2025-05-29 更新"></a>2025-05-29 更新</h1><h2 id="Lunguage-A-Benchmark-for-Structured-and-Sequential-Chest-X-ray-Interpretation"><a href="#Lunguage-A-Benchmark-for-Structured-and-Sequential-Chest-X-ray-Interpretation" class="headerlink" title="Lunguage: A Benchmark for Structured and Sequential Chest X-ray   Interpretation"></a>Lunguage: A Benchmark for Structured and Sequential Chest X-ray   Interpretation</h2><p><strong>Authors:Jong Hak Moon, Geon Choi, Paloma Rabaey, Min Gwan Kim, Hyuk Gi Hong, Jung-Oh Lee, Hangyul Yoon, Eun Woo Doe, Jiyoun Kim, Harshita Sharma, Daniel C. Castro, Javier Alvarez-Valle, Edward Choi</strong></p>
<p>Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/SuperSupermoon/Lunguage">https://github.com/SuperSupermoon/Lunguage</a> </p>
<blockquote>
<p>放射学报告传达详细的临床观察，并捕捉随时间演变的诊断推理。然而，现有的评估方法仅限于单一报告设置，并依赖于粗糙的指标，无法捕捉精细的临床语义和时间依赖关系。我们介绍了LUNGUAGE，这是一个用于结构化放射学报告生成的基准数据集，支持单报告评估和多研究中的纵向患者水平评估。它包含1473份经过专家审查的胸部X射线报告注释，其中80份包含纵向注释，以捕捉疾病进展和研究间间隔，也经过专家审查。使用这个基准数据集，我们开发了一个两阶段框架，将生成的报告转换为精细的、与模式对齐的结构化表示形式，以实现纵向解释。我们还提出了LUNGUAGESCORE这一可解释的指标，该指标可以在实体、关系和属性层面比较结构化输出，同时建模患者时间线上的时间一致性。这些贡献建立了首个基准数据集、结构化框架和顺序放射学报告的评估指标，实证结果表明LUNGUAGESCORE有效地支持结构化报告评估。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/SuperSupermoon/Lunguage">https://github.com/SuperSupermoon/Lunguage</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了LUNGUAGE数据集，该数据集用于结构化放射学报告生成的评价，支持单报告评价和跨多研究的纵向病人级评估。该数据集包含经过专家审核的胸部X射线报告，其中部分报告包含疾病进展和跨研究时间间隔的纵向注释。文章还提出了一种两阶段框架和LUNGUAGESCORE评价指标，用于对生成的报告进行精细结构化表示，并在实体、关系和属性级别进行比较，同时建立患者时间线的时序一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射学报告包含详细的临床观察和诊断推理，但现有的评估方法仅限于单报告设置，并依赖于无法捕获细微临床语义和时序依赖性的粗略指标。</li>
<li>引入LUNGUAGE数据集，该数据集包含经过专家审核的胸部X射线报告，并支持单报告评价和跨研究的纵向病人级评估。</li>
<li>数据集中部分报告包含描述疾病进展和跨研究时间间隔的纵向注释，这些注释经过专家审核。</li>
<li>提出一种两阶段框架，将生成的报告转化为精细的结构化表示，以支持纵向解释。</li>
<li>引入LUNGUAGESCORE评价指标，该指标在实体、关系和属性级别比较结构化输出，并建模患者时间线的时序一致性。</li>
<li>这是首个针对序列放射学报告的结构化数据集、构建框架和评价指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-515d291d749bd4d15cbcc9b98a12e48f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1b6e857b57dfcbf4915c4f819c8cf16.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Identifying-Compton-thick-AGNs-with-Machine-learning-algorithm-in-Chandra-Deep-Field-South"><a href="#Identifying-Compton-thick-AGNs-with-Machine-learning-algorithm-in-Chandra-Deep-Field-South" class="headerlink" title="Identifying Compton-thick AGNs with Machine learning algorithm in   Chandra Deep Field-South"></a>Identifying Compton-thick AGNs with Machine learning algorithm in   Chandra Deep Field-South</h2><p><strong>Authors:Rui Zhang, Xiaotong Guo, Qiusheng Gu, Guanwen Fang, Jun Xu, Hai-Cheng Feng, Yongyun Chen, Rui Li, Nan Ding, Hongtao Wang</strong></p>
<p>Compton-thick active galactic nuclei (CT-AGNs), which are defined by column density $\mathrm{N_H} \geqslant 1.5 \times 10^{24} \ \mathrm{cm}^{-2}$, emit feeble X-ray radiation, even undetectable by X-ray instruments. Despite this, the X-ray emissions from CT-AGNs are believed to be a substantial contributor to the cosmic X-ray background (CXB). According to synthesis models of AGNs, CT-AGNs are expected to make up a significant fraction of the AGN population, likely around 30% or more. However, only $\sim$11% of AGNs have been identified as CT-AGNs in the Chandra Deep Field-South (CDFS). To identify hitherto unknown CT-AGNs in the field, we used a Random Forest algorithm for identifying them. First, we build a secure classified subset of 210 AGNs to train and evaluate our algorithm. Our algorithm achieved an accuracy rate of 90% on the test set after training. Then, we applied our algorithm to an additional subset of 254 AGNs, successfully identifying 67 CT-AGNs within this group. This result significantly increased the fraction of CT-AGNs in the CDFS, which is closer to the theoretical predictions of the CXB. Finally, we compared the properties of host galaxies between CT-AGNs and non-CT-AGNs and found that the host galaxies of CT-AGNs exhibit higher levels of star formation activity. </p>
<blockquote>
<p>康普顿厚活动星系核（CT-AGNs），以其列密度$\mathrm{N_H} \geqslant 1.5 \times 10^{24} \ \mathrm{cm}^{-2}$为特征，发出微弱的X射线辐射，甚至无法被X射线仪器探测到。尽管如此，CT-AGNs的X射线发射被认为是宇宙X射线背景（CXB）的重要组成部分。根据活动星系核的合成模型，CT-AGNs预计占活动星系核人口的很大一部分，可能达到30%或更多。然而，在钱德拉南深场（CDFS）中，只有约11%的活动星系核被识别为CT-AGNs。为了识别领域中的未知CT-AGNs，我们采用了随机森林算法进行识别。首先，我们建立了一个包含210个安全分类的活动星系核子集来训练和评估我们的算法。我们的算法在测试集上达到了90%的准确率。然后，我们将算法应用于额外的254个活动星系核子集，成功识别出其中的67个CT-AGNs。这一结果显著增加了CDFS中CT-AGNs的比例，更接近CXB的理论预测。最后，我们比较了CT-AGNs和非CT-AGNs宿主星系的属性，发现CT-AGNs的宿主星系表现出更高的恒星形成活性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21105v1">PDF</a> 12 pages, 6 figures, 2 Tables. Accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>本文介绍了高柱密度活动星系核（CT-AGNs）的X-ray辐射特性及其对宇宙X-ray背景（CXB）的贡献。虽然CT-AGNs的X-ray辐射微弱，甚至无法被X-ray仪器探测到，但它们被认为是CXB的重要组成部分。通过运用随机森林算法，成功识别出更多的CT-AGNs，并发现其宿主星系具有较高的恒星形成活性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-AGNs的X-ray辐射特性微弱，但仍然对宇宙X-ray背景（CXB）有显著贡献。</li>
<li>合成模型预测CT-AGNs在AGN总体中占相当大的比例，可能达到或超过30%。</li>
<li>通过随机森林算法，提高了对CT-AGNs的识别率，准确率达到了90%。</li>
<li>在CDFS中成功识别出的CT-AGNs数量增加，更接近理论预测。</li>
<li>与非CT-AGNs相比，CT-AGNs的宿主星系表现出更高的恒星形成活性。</li>
<li>训练集和测试集的选择对算法的性能至关重要。需要选择适当的子集来训练和评估算法以确保准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce7e5134ba91a74a4167bb75c2397da8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9811b67d43fe7678bfe962ff56ea0cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-383df6d79ae20eebe38b36d2a304a377.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="All-optical-discrete-illumination-based-compressed-ultrafast-photography"><a href="#All-optical-discrete-illumination-based-compressed-ultrafast-photography" class="headerlink" title="All-optical discrete illumination-based compressed ultrafast photography"></a>All-optical discrete illumination-based compressed ultrafast photography</h2><p><strong>Authors:Long Cheng, Dalong Qi, Jiali Yao, Ning Xu, Chengyu Zhou, Wenzhang Lin, Yu He, Zhen Pan, Yunhua Yao, Lianzhong Deng, Yuecheng Shen, Zhenrong Sun, Shian Zhang</strong></p>
<p>Snapshot ultrafast optical imaging (SUOI) plays a vital role in capturing complex transient events in real time, with significant implications for both fundamental science and practical applications. As an outstanding talent in SUOI, compressed ultrafast photography (CUP) has demonstrated remarkable frame rate reaching trillions of frames per second and hundreds of sequence depth. Nevertheless, as CUP relies on streak cameras, the system’s imaging fidelity suffers from an inevitable limitation induced by the charge coupling artifacts in a streak camera. Moreover, although advanced image reconstruction algorithms have improved the recovered scenes, its high compression ratio still causes a compromise in image quality. To address these challenges, we propose a novel approach termed all-optical discrete illumination compressed ultrafast photography (AOD-CUP), which employs a free-space angular-chirp-enhanced delay (FACED) technique to temporally stretch femtosecond pulses and achieves discrete illumination for dynamic scenes. With its distinctive system architecture, AOD-CUP features adjustable frame numbers and flexible inter-frame intervals ranging from picoseconds to nanoseconds, thereby achieving high-fidelity ultrafast imaging in a snapshot. Experimental results demonstrate the system’s superior dynamic spatial resolution and its capability to visualize ultrafast phenomena with complex spatial details, such as stress wave propagation in LiF crystals and air plasma channel formation. These results highlight the potential of AOD-CUP for high-fidelity, real-time ultrafast imaging, which provides an unprecedented tool for advancing the frontiers of ultrafast science. </p>
<blockquote>
<p>快照超高速光学成像（SUOI）在实时捕捉复杂的瞬态事件方面起着至关重要的作用，对基础科学和实际应用都有重要意义。作为SUOI领域的杰出人才，压缩超高速摄影（CUP）已经实现了高达每秒数万亿帧的帧率以及数百序列深度。然而，由于CUP依赖于条纹相机，系统的成像保真度受到条纹相机中电荷耦合伪影引起的不可避免的限制。尽管先进的图像重建算法提高了恢复的场景质量，但其较高的压缩比仍然会导致图像质量的妥协。为了解决这些挑战，我们提出了一种新的方法，称为全光学离散照明压缩超高速摄影（AOD-CUP），该方法采用自由空间角啁啾增强延迟（FACED）技术来延长飞秒脉冲的时间，并为动态场景实现离散照明。凭借独特的系统架构，AOD-CUP具有可调整的帧数和灵活的帧间间隔，范围从皮秒到纳秒，从而在一次快照中实现高保真超高速成像。实验结果证明了系统出色的动态空间分辨率以及其在可视化具有复杂空间细节的超快现象方面的能力，如LiF晶体中的应力波传播和空气等离子体通道的形成。这些结果突显了AOD-CUP在高保真、实时超高速成像方面的潜力，为超快科学的前沿发展提供了前所未有的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21086v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>快照超高速光学成像（SUOI）在实时捕捉复杂瞬态事件方面发挥着至关重要的作用，对基础科学和实际应用都具有重要意义。压缩超高速摄影（CUP）作为SUOI的杰出人才，帧率高达每秒数万亿帧，序列深度达数百个。然而，由于CUP依赖于条纹相机，系统成像保真度受到条纹相机中电荷耦合伪影的固有限制的影响。尽管先进的图像重建算法改善了恢复的场景，但其高压缩比仍然会影响图像质量。为了解决这些挑战，我们提出了一种名为全光学离散照明压缩超高速摄影（AOD-CUP）的新方法，该方法采用自由空间角啁��ーム增强延迟（FACED）技术，对飞秒脉冲进行时间拉伸，实现动态场景的离散照明。凭借独特系统架构，AOD-CUP具有可调帧数和灵活的帧间间隔，范围从皮秒到纳秒，从而实现高保真超高速快照成像。实验结果证明了系统出色的动态空间分辨率和可视化复杂空间细节的超高现象的能力，如LiF晶体中的应力波传播和空气等离子体通道的形成。这些结果突显了AOD-CUP在高保真、实时超高速成像方面的潜力，为超高速科学领域的发展提供了前所未有的工具。</p>
<p><strong>要点</strong></p>
<ol>
<li>SUOI在捕捉复杂瞬态事件方面至关重要，对科学和应用都有重要意义。</li>
<li>CUP作为SUOI的杰出代表，具有极高的帧率和序列深度。<br>3.条纹相机带来的电荷耦合伪影限制了CUP的成像质量。</li>
<li>提出AOD-CUP新方法，采用FACED技术实现离散照明和灵活帧间间隔。</li>
<li>AOD-CUP具有可调帧数，可实现高动态空间分辨率的超高速成像。</li>
<li>实验结果证明AOD-CUP在可视化复杂空间细节的超高现象方面的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21086">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e1fdaa60f175bfcd8f46d23b0580512.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-241c9906a4c171fab5729b683b463558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aa75b1849b7d023d968654b6939c61d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cardiac-Digital-Twins-at-Scale-from-MRI-Open-Tools-and-Representative-Models-from-55000-UK-Biobank-Participants"><a href="#Cardiac-Digital-Twins-at-Scale-from-MRI-Open-Tools-and-Representative-Models-from-55000-UK-Biobank-Participants" class="headerlink" title="Cardiac Digital Twins at Scale from MRI: Open Tools and Representative   Models from ~55000 UK Biobank Participants"></a>Cardiac Digital Twins at Scale from MRI: Open Tools and Representative   Models from ~55000 UK Biobank Participants</h2><p><strong>Authors:Devran Ugurlu, Shuang Qian, Elliot Fairweather, Charlene Mauger, Bram Ruijsink, Laura Dal Toso, Yu Deng, Marina Strocchi, Reza Razavi, Alistair Young, Pablo Lamata, Steven Niederer, Martin Bishop</strong></p>
<p>A cardiac digital twin is a virtual replica of a patient’s heart for screening, diagnosis, prognosis, risk assessment, and treatment planning of cardiovascular diseases. This requires an anatomically accurate patient-specific 3D structural representation of the heart, suitable for electro-mechanical simulations or study of disease mechanisms. However, generation of cardiac digital twins at scale is demanding and there are no public repositories of models across demographic groups. We describe an automatic open-source pipeline for creating patient-specific left and right ventricular meshes from cardiovascular magnetic resonance images, its application to a large cohort of ~55000 participants from UK Biobank, and the construction of the most comprehensive cohort of adult heart models to date, comprising 1423 representative meshes across sex (male, female), body mass index (range: 16 - 42 kg&#x2F;m$^2$) and age (range: 49 - 80 years). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025">https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025</a> , and pre-trained networks, representative volumetric meshes with fibers and UVCs will be made available soon. </p>
<blockquote>
<p>心脏数字孪生体是指患者心脏的虚拟副本，用于心血管疾病的筛查、诊断、预后评估、风险评估和治疗计划。这需要患者心脏的具体解剖结构的三维结构表示，适用于机电模拟或疾病机理研究。然而，大规模生成心脏数字孪生体是有挑战性的，并且目前没有跨人口群体的模型公共仓库。我们描述了一个自动开源管道，用于从心血管磁共振图像创建患者特定的左心室和右心室网格，将其应用于来自英国生物银行的约55000名参与者的研究，并构建了迄今为止最全面的成人心脏模型队列，包括按性别（男性、女性）、体重指数（范围：16-42 kg&#x2F;m²）和年龄（范围：49-80岁）划分的代表性网格共1423个。我们的代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025">https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025</a>，并且预训练网络和具有代表性的体积网格（带有纤维和紫外线杀菌剂）将很快提供使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这是一项关于心脏数字双胞胎的研究，该研究利用心血管磁共振图像创建患者特定的左心室和右心室网格，并应用于大规模的UK Biobank参与者群体，构建了迄今为止最全面的成人心脏模型。该研究提供了自动开源管道，并创建了包含性别、体重指数和年龄等跨人口特征的模型库。这些模型可用于心血管疾病筛查、诊断、预后评估和治疗计划。其代码和资源库公开，方便研究者获取和应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心脏数字双胞胎是一种虚拟患者心脏复制品，主要用于心血管疾病的筛查、诊断、预后评估和治疗计划。</li>
<li>需要解剖学准确的患者特定心脏三维结构表示，适用于机电模拟或疾病机理研究。</li>
<li>自动开源管道被开发出来，用于从心血管磁共振图像创建患者特定的左心室和右心室网格。</li>
<li>该管道成功应用于大规模的UK Biobank参与者群体，创建了迄今为止最全面的成人心脏模型。</li>
<li>模型考虑了性别、体重指数和年龄等多种因素，使其成为跨人口特征的模型库。这对于各种类型的心血管疾病研究具有重要价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21019">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a85fe368173805908404b3518d0a5e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88ee780562c3087f39afc38d9b0ba240.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99c918af5d4b45da5ccfb675234d481e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TumorHoPe2-An-updated-database-for-Tumor-Homing-Peptides"><a href="#TumorHoPe2-An-updated-database-for-Tumor-Homing-Peptides" class="headerlink" title="TumorHoPe2: An updated database for Tumor Homing Peptides"></a>TumorHoPe2: An updated database for Tumor Homing Peptides</h2><p><strong>Authors:Diksha Kashyap, Devanshi Gupta, Naman Kumar Mehta, Gajendra P. S. Raghava</strong></p>
<p>Addressing the growing need for organized data on tumor homing peptides (THPs), we present TumorHoPe2, a manually curated database offering extensive details on experimentally validated THPs. This represents a significant update to TumorHoPe, originally developed by our group in 2012. TumorHoPe2 now contains 1847 entries, representing 1297 unique tumor homing peptides, a substantial expansion from the 744 entries in its predecessor. For each peptide, the database provides critical information, including sequence, terminal or chemical modifications, corresponding cancer cell lines, and specific tumor types targeted. The database compiles data from two primary sources: phage display libraries, which are commonly used to identify peptide ligands targeting tumor-specific markers, and synthetic peptides, which are chemically modified to enhance properties such as stability, binding affinity, and specificity. Our dataset includes 594 chemically modified peptides, with 255 having N-terminal and 195 C-terminal modifications. These THPs have been validated against 172 cancer cell lines and demonstrate specificity for 37 distinct tumor types. To maximize utility for the research community, TumorHoPe2 is equipped with intuitive tools for data searching, filtering, and analysis, alongside a RESTful API for efficient programmatic access and integration into bioinformatics pipelines. It is freely available at <a target="_blank" rel="noopener" href="https://webs.iiitd.edu.in/raghava/tumorhope2/">https://webs.iiitd.edu.in/raghava/tumorhope2/</a> </p>
<blockquote>
<p>为了应对肿瘤归巢肽（THPs）有序数据不断增长的需求，我们推出了TumorHoPe2，这是一个手动整理数据库，提供了经过实验验证的THPs的详细信息。这是对2012年由我们团队开发的TumorHoPe的重大更新。TumorHoPe2目前包含1847个条目，代表有1297个独特的肿瘤归巢肽，较前一代的744个条目有了实质性的扩展。对于每个肽，数据库提供了关键信息，包括序列、末端或化学修饰、相应的癌细胞株以及特定的靶向肿瘤类型。数据库整理了两种主要来源的数据：常用于识别针对肿瘤特异性标记物的肽配体的噬菌体展示库，以及通过化学修饰增强稳定性、结合亲和力和特异性的合成肽。我们的数据集包含594个化学修饰的肽，其中255个具有N端修饰和195个具有C端修饰。这些THPs已经通过针对172种癌细胞株的验证，显示出对37种不同肿瘤类型的特异性。为了最大化对研究群体的效用，TumorHoPe2配备了用于数据搜索、过滤和分析的直观工具，以及用于高效编程访问和整合到生物信息学流程的RESTful API。它可在<a target="_blank" rel="noopener" href="https://webs.iiitd.edu.in/raghava/tumorhope2/%E5%85%8D%E8%B4%B9%E8%AE%BF%E9%97%AE%E3%80%82">https://webs.iiitd.edu.in/raghava/tumorhope2/免费访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20913v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>肿瘤HoPe数据库的新版本——TumorHoPe2现已推出，包含经过实验验证的肿瘤归巢肽（THP）的详细信息。此数据库包含大量关于肿瘤归巢肽的信息，包括序列、终端或化学修饰、相应的癌细胞株以及特定肿瘤类型等。相比之前的版本，新数据库扩充了内容，并提供了更直观的工具进行数据搜索、筛选和分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TumorHoPe2是TumorHoPe数据库的升级版，包含更多关于肿瘤归巢肽（THP）的信息。</li>
<li>数据库包含1847个条目，代表1297个独特的肿瘤归巢肽。</li>
<li>数据库数据主要来自噬菌体展示库和合成肽两种来源。</li>
<li>数据库中包含594个经过化学修饰的肽，其中255个具有N末端和195个具有C末端修饰。</li>
<li>这些THPs已针对172种癌细胞株进行验证，并显示出对37种不同肿瘤的特异性。</li>
<li>TumorHoPe2提供了直观的工具进行数据搜索、筛选和分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1abe6c653423c465a0a7ce536b099995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88e8873d95b4611af68deb99a08e47db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Highly-Enhanced-robust-room-temperature-ferromagnetism-in-CVD-grown-nano-dimensional-MoS2-flakes-by-modifying-edges-and-defect-engineering"><a href="#Highly-Enhanced-robust-room-temperature-ferromagnetism-in-CVD-grown-nano-dimensional-MoS2-flakes-by-modifying-edges-and-defect-engineering" class="headerlink" title="Highly Enhanced robust room temperature ferromagnetism in CVD-grown   nano-dimensional MoS2 flakes by modifying edges and defect engineering"></a>Highly Enhanced robust room temperature ferromagnetism in CVD-grown   nano-dimensional MoS2 flakes by modifying edges and defect engineering</h2><p><strong>Authors:Sharmistha Dey, Nahid Chaudhary, Ulrich Kentsch, Rajendra Singh, Pankaj Srivastava, Santanu Ghosh</strong></p>
<p>The alterations in the magnetic properties and electronic structure of chemical vapor deposition (CVD) grown nano-dimensional molybdenum disulfide (MoS2) after low energy ion irradiation are thoroughly investigated. The formation of pure hexagonal 2-H phase has been identified by Raman spectroscopy and X-ray diffraction (XRD). The pristine samples are irradiated by Argon (Ar) ions with low energy at different fluences. A comprehensive analysis of Raman spectroscopy data manifests the formation of lattice defects like S-vacancies across the samples after irradiation. Triangular-flake formation in the pristine sample is confirmed by field emission scanning electron microscopy (FESEM) images. After increasing irradiation fluences the big flakes commenced to fragment into smaller ones enhancing the number of edge-terminated structures. The electron probe microanalyzer (EPMA) analysis verifies the absence of any magnetic impurity. Rutherford backscattering spectrometry (RBS) and X-ray photoelectron spectroscopy (XPS) study confirm the formation of S-vacancies after irradiation. The pristine sample exhibits diamagnetic behavior at room temperature. The saturation magnetization value increases with increasing the ion irradiation fluences, and the sample irradiated with 1e15 ions&#x2F;cm2 demonstrates the highest magnetization value of 4.18 emu&#x2F;g. The impact of edge-terminated structure and point defects like S-vacancies to induce room-temperature ferromagnetism (RTFM) is thoroughly examined. </p>
<blockquote>
<p>采用化学气相沉积（CVD）生长的纳米级二硫化钼（MoS2）在低能量离子辐射后的磁性和电子结构变化得到了深入研究。通过拉曼光谱和X射线衍射（XRD）确定了纯六角形2-H相的形成。原始样品受到不同流强的氩（Ar）离子低能量辐射。拉曼光谱数据的综合分析表明，辐射后在样品中形成了晶格缺陷，如硫空位。场发射扫描电子显微镜（FESEM）图像证实了原始样品中三角形片状结构的形成。随着辐射流强的增加，大片状结构开始碎裂成较小的片状结构，增加了边缘终止结构的数量。电子探针微分析仪（EPMA）分析验证了无磁性杂质的存在。卢瑟福背散射光谱仪（RBS）和X射线光电子光谱仪（XPS）的研究证实了硫空位在辐射后的形成。原始样品在室温下表现出抗磁性行为。随着离子辐射流强的增加，饱和磁化强度值增加，辐照离子数为1e15离子&#x2F;cm2的样品表现出最高的磁化强度值为4.18emu&#x2F;g。边缘终止结构和硫空位等点缺陷对诱导室温铁磁性（RTFM）的影响得到了彻底检查。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20695v1">PDF</a> </p>
<p><strong>Summary</strong><br>     化学气相沉积法制备的纳米级二硫化钼在受到低能量离子辐射后，其磁性和电子结构发生变化。研究通过拉曼光谱和X射线衍射确认形成了纯净的六角形2-H相，并发现离子辐射后产生了硫空位等晶格缺陷。随着辐射剂量的增加，大晶片开始碎裂成更小的晶片，增加了边缘终止结构数量。电子探针微分析仪分析确认无磁性杂质。硫空位形成的原因通过卢瑟福背散射光谱和X射线光电子光谱研究得到证实。样品的饱和磁化强度随着离子辐射剂量的增加而增加，受到离子辐射后的样品显示出最高的磁化强度为每克样品产生4.18emu的磁化强度。文中详细研究了边缘终止结构和硫空位等点缺陷对室温铁磁性的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CVD生长的纳米级二硫化钼在经过低能量离子辐射后，磁性和电子结构发生变化。</li>
<li>确认了纯净的六角形2-H相的形成。</li>
<li>拉曼光谱分析表明离子辐射后产生了硫空位等晶格缺陷。</li>
<li>随着辐射剂量的增加，大晶片碎裂成更小的晶片，边缘终止结构数量增加。</li>
<li>无磁性杂质的存在得到电子探针微分析仪的验证。</li>
<li>研究确认了硫空位在离子辐射后的形成机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20695">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ee3e96980d92dc881264fb0adfb4c9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53ee72e46cafe0d2463d391385e75682.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MetaWriter-Personalized-Handwritten-Text-Recognition-Using-Meta-Learned-Prompt-Tuning"><a href="#MetaWriter-Personalized-Handwritten-Text-Recognition-Using-Meta-Learned-Prompt-Tuning" class="headerlink" title="MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned   Prompt Tuning"></a>MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned   Prompt Tuning</h2><p><strong>Authors:Wenhao Gu, Li Gu, Ching Yee Suen, Yang Wang</strong></p>
<p>Recent advancements in handwritten text recognition (HTR) have enabled the effective conversion of handwritten text to digital formats. However, achieving robust recognition across diverse writing styles remains challenging. Traditional HTR methods lack writer-specific personalization at test time due to limitations in model architecture and training strategies. Existing attempts to bridge this gap, through gradient-based meta-learning, still require labeled examples and suffer from parameter-inefficient fine-tuning, leading to substantial computational and memory overhead. To overcome these challenges, we propose an efficient framework that formulates personalization as prompt tuning, incorporating an auxiliary image reconstruction task with a self-supervised loss to guide prompt adaptation with unlabeled test-time examples. To ensure self-supervised loss effectively minimizes text recognition error, we leverage meta-learning to learn the optimal initialization of the prompts. As a result, our method allows the model to efficiently capture unique writing styles by updating less than 1% of its parameters and eliminating the need for time-intensive annotation processes. We validate our approach on the RIMES and IAM Handwriting Database benchmarks, where it consistently outperforms previous state-of-the-art methods while using 20x fewer parameters. We believe this represents a significant advancement in personalized handwritten text recognition, paving the way for more reliable and practical deployment in resource-constrained scenarios. </p>
<blockquote>
<p>手写文本识别（HTR）的最新进展已经实现了手写文本向数字格式的有效转换。然而，实现在各种书写风格中的稳健识别仍然具有挑战性。传统HTR方法由于在模型架构和训练策略上的局限性，在测试时缺乏针对特定作者的个性化。现有的一些尝试通过梯度基础的元学习（Meta-Learning）来弥补这一差距，但依旧需要标注样本，并且在精细调整参数时效率低下，导致计算与内存负担巨大。为了克服这些挑战，我们提出了一个高效的框架，将个性化表述为提示调整（Prompt Tuning），并结合一个辅助图像重建任务与自我监督损失来指导无标签测试样本的提示适应。为了确保自我监督损失能有效地最小化文本识别错误，我们利用元学习来学习提示的最优初始化。因此，我们的方法允许模型通过更新不到1%的参数来有效地捕捉独特的书写风格，并且无需耗时的人工标注过程。我们在RIMES和IAM手写数据库基准测试上验证了我们的方法，它始终优于之前的最先进方法，同时使用的参数少了20倍。我们相信这代表了个性化手写文本识别的重大进展，为资源受限的场景中更可靠、更实际的部署铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20513v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>手写文本识别（HTR）技术的最新进展实现了手写文本向数字格式的有效转换，但实现跨不同书写风格的稳健识别仍然具有挑战性。针对传统HTR方法缺乏测试时的个性化且存在模型架构和训练策略的局限性，以及现有尝试解决此问题的梯度元学习方法需要标注示例和参数调整不够高效的问题，本文提出了一种有效的框架。该框架将个性化表述为提示调整，并结合辅助图像重建任务和自监督损失来引导无标签测试例的提示适应。为确保自监督损失最小化文本识别误差，本文利用元学习来学习提示的最佳初始化。该方法允许模型通过更新不到1%的参数来高效捕捉独特的书写风格，并省去耗时标注过程。在RIMES和IAM手写数据库基准测试中，该方法持续优于先前最先进的方法，同时使用参数更少20倍。这标志着个性化手写文本识别的重大进展，为资源受限场景中更可靠、更实用的部署铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期手写文本识别（HTR）技术的进展实现了手写文本到数字格式的转换，但识别不同书写风格的挑战仍然存在。</li>
<li>传统HTR方法因模型架构和训练策略的局限性，缺乏测试时的个性化。</li>
<li>现有尝试通过梯度元学习弥补这一差距的方法仍需要标注示例，且参数调整不够高效。</li>
<li>本文提出的框架将个性化表述为提示调整，结合辅助图像重建任务和自监督损失引导无标签测试例的提示适应。</li>
<li>利用自监督损失和元学习来优化提示的初始化和提高模型性能。</li>
<li>该方法能高效捕捉独特书写风格，更新参数不到1%，并免去耗时标注过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-142d01941fef69c6e840e7456f7e9ab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-671be78fb0d25d58a689ab490794b8b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b189d0e0b58cf392e480855397e0b24d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b37902bd61ac95a42e4fcefe8cea8c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09331fd4a8127ca1b13930490dba794.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologists’-Diagnostic-Logic"><a href="#CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologists’-Diagnostic-Logic" class="headerlink" title="CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologists’ Diagnostic   Logic"></a>CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologists’ Diagnostic   Logic</h2><p><strong>Authors:Yuxuan Sun, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Bowen Ding, Tao Lin, Lin Yang</strong></p>
<p>Recent advances in computational pathology have led to the emergence of numerous foundation models. However, these approaches fail to replicate the diagnostic process of pathologists, as they either simply rely on general-purpose encoders with multi-instance learning for classification or directly apply multimodal models to generate reports from images. A significant limitation is their inability to emulate the diagnostic logic employed by pathologists, who systematically examine slides at low magnification for overview before progressively zooming in on suspicious regions to formulate comprehensive diagnoses. To address this gap, we introduce CPathAgent, an innovative agent-based model that mimics pathologists’ reasoning processes by autonomously executing zoom-in&#x2F;out and navigation operations across pathology images based on observed visual features. To achieve this, we develop a multi-stage training strategy unifying patch-level, region-level, and whole-slide capabilities within a single model, which is essential for mimicking pathologists, who require understanding and reasoning capabilities across all three scales. This approach generates substantially more detailed and interpretable diagnostic reports compared to existing methods, particularly for huge region understanding. Additionally, we construct an expert-validated PathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical intermediate scale between patches and whole slides, as diagnosticians typically examine several key regions rather than entire slides at once. Extensive experiments demonstrate that CPathAgent consistently outperforms existing approaches across three scales of benchmarks, validating the effectiveness of our agent-based diagnostic approach and highlighting a promising direction for the future development of computational pathology. </p>
<blockquote>
<p>近期计算病理学领域的进展催生了许多基础模型。然而，这些方法无法复制病理医生的诊断过程，因为它们要么简单地依赖于具有多实例学习功能的一般编码器进行分类，要么直接将多模式模型应用于图像生成报告。一个显著的局限性在于，它们无法模拟病理医生的诊断逻辑，即系统性地在低倍镜下观察切片以获取概览，然后逐步放大可疑区域以做出全面的诊断。为了解决这一差距，我们引入了CPathAgent，这是一个基于创新的智能体模型，通过自主执行放大&#x2F;缩小和导航操作来模拟病理医生的推理过程，在病理图像上基于观察到的视觉特征进行跨尺度操作。为了实现这一点，我们开发了一种多阶段训练策略，在一个单一模型中统一了补丁级别、区域级别和整个幻灯片级别的能力，这对于模拟病理医生来说至关重要，他们需要在这三个尺度上具备理解和推理能力。该方法生成的诊断报告比现有方法更为详细和可解释，特别是对于巨大区域的理解。此外，我们构建了经过专家验证的PathMMU-HR$^{2}$，这是首个针对巨大区域分析的基准测试，是补丁和整个幻灯片之间的一个重要中间尺度，因为诊断通常需要对几个关键区域而非整个幻灯片进行检查。大量实验表明，CPathAgent在三个尺度的基准测试中始终优于现有方法，验证了我们的基于智能体的诊断方法的有效性，并指出了计算病理学未来发展的有前途的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20510v1">PDF</a> 49 pages, 33 figures</p>
<p><strong>Summary</strong><br>     近期计算病理学领域的进展催生出众多基础模型，但现有方法无法复制病理医师的诊断过程。为此，我们提出CPathAgent，基于视觉特征进行自主缩放和导航操作，模拟病理医师的推理过程。通过多阶段训练策略，将斑块、区域和整个幻灯片的能力统一于单一模型中，生成更详细、可解释的诊断报告。同时，我们建立了专家验证的PathMMU-HR$^{2}$基准测试，用于评估巨大区域的分析能力。实验证明，CPathAgent在三个基准测试中均表现优异，验证了其诊断效果和前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算病理学领域涌现出众多基础模型，但现有方法无法完全模拟病理医师的诊断过程。</li>
<li>CPathAgent通过自主执行缩放和导航操作，模拟病理医师的推理过程。</li>
<li>多阶段训练策略实现了斑块、区域和整个幻灯片的统一能力模型，提高诊断的全面性。</li>
<li>CPathAgent生成的诊断报告更加详细和可解释。</li>
<li>PathMMU-HR$^{2}$基准测试的建立，填补了巨大区域分析领域的空白。</li>
<li>CPathAgent在三个基准测试中表现优异，验证了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-62c1f2a9d98cc8cd0d32145d20c79f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ce77d019e9bd3555f5aebd3d5828e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab2e2af72b08abb4296db7ba6b21ca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e6a2e279a4338bd1bf7ab91868f38c9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation"><a href="#CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation" class="headerlink" title="CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation"></a>CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation</h2><p><strong>Authors:Jiong Wu, Yang Xing, Boxiao Yu, Wei Shao, Kuang Gong</strong></p>
<p>Most publicly available medical segmentation datasets are only partially labeled, with annotations provided for a subset of anatomical structures. When multiple datasets are combined for training, this incomplete annotation poses challenges, as it limits the model’s ability to learn shared anatomical representations among datasets. Furthermore, vision-only frameworks often fail to capture complex anatomical relationships and task-specific distinctions, leading to reduced segmentation accuracy and poor generalizability to unseen datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven Segmentation Network (CDPDNet), which combined a self-supervised vision transformer with CLIP-based text embedding and introduced task-specific text prompts to tackle these challenges. Specifically, the framework was constructed upon a convolutional neural network (CNN) and incorporated DINOv2 to extract both fine-grained and global visual features, which were then fused using a multi-head cross-attention module to overcome the limited long-range modeling capability of CNNs. In addition, CLIP-derived text embeddings were projected into the visual space to help model complex relationships among organs and tumors. To further address the partial label challenge and enhance inter-task discriminative capability, a Text-based Task Prompt Generation (TTPG) module that generated task-specific prompts was designed to guide the segmentation. Extensive experiments on multiple medical imaging datasets demonstrated that CDPDNet consistently outperformed existing state-of-the-art segmentation methods. Code and pretrained model are available at: <a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git">https://github.com/wujiong-hub/CDPDNet.git</a>. </p>
<blockquote>
<p>大部分公开可用的医学分割数据集仅部分标注，只为部分解剖结构提供注释。当多个数据集组合进行训练时，这种不完全的注释带来了挑战，因为它限制了模型在数据集之间学习共享解剖表征的能力。此外，仅依赖视觉的框架往往无法捕捉复杂的解剖关系和任务特定区别，导致分割准确度降低，对未见数据集的泛化能力较差。在这项研究中，我们提出了一种新颖的CLIP-DINO Prompt-Driven Segmentation Network（CDPDNet），它将自我监督的视觉变压器与CLIP基于的文本嵌入相结合，并引入了任务特定的文本提示来解决这些挑战。具体来说，该框架建立在卷积神经网络（CNN）之上，融入了DINOv2以提取精细粒度和全局视觉特征，然后使用多头交叉注意模块融合这些特征，以克服CNN有限的长期建模能力。此外，CLIP衍生的文本嵌入被投射到视觉空间中，以帮助建模器官和肿瘤之间的复杂关系。为了进一步解决部分标签挑战并增强任务间的判别能力，设计了一个基于文本的任务提示生成（TTPG）模块，以生成特定任务的提示来指导分割。在多个医学影像数据集上的广泛实验表明，CDPDNet持续优于现有的最先进的分割方法。代码和预先训练好的模型可在：<a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wujiong-hub/CDPDNet.git找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18958v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的CLIP-DINO Prompt驱动分割网络（CDPDNet），结合自监督视觉变压器与CLIP文本嵌入技术，通过特定任务文本提示来解决医学图像分割中面临的标注不完整和复杂解剖关系捕捉难题。CDPDNet采用CNN结合DINOv2技术提取精细粒度和全局视觉特征，并利用多头交叉注意力模块进行特征融合，以克服CNN的长期建模能力限制。同时，利用CLIP衍生的文本嵌入将文本信息投影到视觉空间，帮助模型理解器官和肿瘤之间的复杂关系。为解决部分标注问题并增强任务间的判别能力，设计了一种基于文本的任务提示生成（TTPG）模块来指导分割。在多个医学图像数据集上的实验表明，CDPDNet显著优于现有最先进的分割方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDPDNet结合自监督视觉变压器和CLIP文本嵌入技术，解决医学图像分割中的标注不完整和复杂解剖关系捕捉难题。</li>
<li>CDPDNet采用CNN和DINOv2技术提取视觉特征，实现精细粒度和全局特征的融合。</li>
<li>多头交叉注意力模块克服CNN的长期建模能力限制。</li>
<li>CLIP衍生的文本嵌入帮助模型理解器官和肿瘤之间的复杂关系。</li>
<li>引入基于文本的任务提示生成（TTPG）模块，解决部分标注问题并增强任务间的判别能力。</li>
<li>在多个医学图像数据集上，CDPDNet的实验结果显著优于现有最先进的分割方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b394b118a8230f95539df4b78bc578f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4cd7466255c92803b98e69df89abf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f929c714444911899e5fbbfda0c1eace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6c352b327d1eb756a35dec0df24319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4b7b4a0c050de83ddb270cfd1e0edf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TAGS-3D-Tumor-Adaptive-Guidance-for-SAM"><a href="#TAGS-3D-Tumor-Adaptive-Guidance-for-SAM" class="headerlink" title="TAGS: 3D Tumor-Adaptive Guidance for SAM"></a>TAGS: 3D Tumor-Adaptive Guidance for SAM</h2><p><strong>Authors:Sirui Li, Linkai Peng, Zheyuan Zhang, Gorkem Durak, Ulas Bagci</strong></p>
<p>Foundation models (FMs) such as CLIP and SAM have recently shown great promise in image segmentation tasks, yet their adaptation to 3D medical imaging-particularly for pathology detection and segmentation-remains underexplored. A critical challenge arises from the domain gap between natural images and medical volumes: existing FMs, pre-trained on 2D data, struggle to capture 3D anatomical context, limiting their utility in clinical applications like tumor segmentation. To address this, we propose an adaptation framework called TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D medical tasks through multi-prompt fusion. By preserving most of the pre-trained weights, our approach enhances SAM’s spatial feature extraction using CLIP’s semantic insights and anatomy-specific prompts. Extensive experiments on three open-source tumor segmentation datasets prove that our model surpasses the state-of-the-art medical image segmentation models (+46.88% over nnUNet), interactive segmentation frameworks, and other established medical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and SAM-B (at least +13% over them). This highlights the robustness and adaptability of our proposed framework across diverse medical segmentation tasks. </p>
<blockquote>
<p>最近，CLIP和SAM等基础模型（FMs）在图像分割任务中显示出巨大的潜力，然而它们在适应三维医学成像方面的应用，特别是在病理检测和分割方面仍然被研究得不够深入。自然图像和医学体积之间存在领域差距的问题由此产生了一个关键挑战：现有的在二维数据上预训练的FMs在捕获三维解剖上下文方面存在困难，这在肿瘤分割等临床应用中限制了它们的实用性。为了解决这个问题，我们提出了一种名为TAGS的适应框架：SAM的肿瘤自适应指导。通过多提示融合，我们的框架解锁了二维FMs在三维医学任务中的应用。通过保留大部分预训练权重，我们的方法利用CLIP的语义洞察力和解剖特定提示，增强了SAM的空间特征提取能力。在三个开源肿瘤分割数据集上的大量实验证明，我们的模型超越了最先进的医学图像分割模型（相对于nnUNet提高+46.88%），交互式分割框架和其他既定的医学FMs，包括SAM-Med2D、SAM-Med3D、SegVol、Universal、3D-Adapter和SAM-B（至少相对于它们提高+13%）。这突显了我们提出的框架在不同医学分割任务中的稳健性和适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自然图像领域表现良好的Foundation模型（FMs）在3D医学成像上的应用挑战。通过提出名为TAGS的适应框架，实现了对SAM模型的优化，使其在肿瘤分割等临床应用中表现更好。该框架通过多提示融合技术，利用CLIP的语义提示和解剖特定提示，提高SAM的空间特征提取能力。在三个开源肿瘤分割数据集上的实验证明，该模型优于现有医学图像分割模型和其他建立的医学FMs。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Foundation模型（FMs）在图像分割任务中展现出巨大潜力，但在3D医学成像上的适应度仍需提高。</li>
<li>现有FMs面临从自然图像到医学体积数据的领域差距问题，难以捕捉3D解剖上下文信息。</li>
<li>提出的TAGS框架成功解锁了SAM模型在3D医学任务中的潜力，通过多提示融合技术提高其空间特征提取能力。</li>
<li>TAGS框架保留了大部分预训练权重，并结合了CLIP的语义提示和解剖特定提示。</li>
<li>在三个开源肿瘤分割数据集上的实验表明，TAGS框架表现超越现有医学图像分割模型和其他医学FMs。</li>
<li>TAGS框架的鲁棒性和适应性在不同医学分割任务中得到了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d77591b5f5b6401cb770f7ba47422045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910dad623cc0ec6c2fba1754b8b336d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f899a68ef50954d3826c884603992d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d61fd7cb3d78b3240ddd7eded60efb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities"></a>Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities</h2><p><strong>Authors:Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</strong></p>
<p>Existing methods for multimodal MRI segmentation with missing modalities typically assume that all MRI modalities are available during training. However, in clinical practice, some modalities may be missing due to the sequential nature of MRI acquisition, leading to performance degradation. Furthermore, retraining models to accommodate newly available modalities can be inefficient and may cause overfitting, potentially compromising previously learned knowledge. To address these challenges, we propose Replay-based Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to enable the segmentation model to learn from newly acquired MRI modalities without forgetting previously learned information. To enhance segmentation performance across diverse patient scenarios, we introduce the Cross-Patient Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture high-order associations between patients. Additionally, we incorporate Tversky-Aware Contrastive (TAC) loss to effectively mitigate information imbalance both across and within different modalities. Extensive experiments on the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art methods, achieving an improvement of over 2% in the Dice Similarity Coefficient across various tumor regions. </p>
<blockquote>
<p>现有的多模态MRI分割缺失模态的方法通常假设在训练期间所有MRI模态都是可用的。然而，在临床实践中，由于MRI采集的序列性质，某些模态可能会缺失，导致性能下降。此外，为了容纳新可用的模态而重新训练模型可能效率低下，并可能导致过拟合，从而可能损害之前学到的知识。为了应对这些挑战，我们提出了基于重播的超图域增量学习（ReHyDIL）方法，用于具有缺失模态的脑肿瘤分割。ReHyDIL利用域增量学习（DIL）使分割模型能够从新获取的MRI模态中学习，而不会忘记之前学到的信息。为了提高不同患者场景下的分割性能，我们引入了跨患者超图分割网络（CHSNet），该网络利用超图来捕捉患者之间的高阶关联。此外，我们结合了Tversky感知对比（TAC）损失，以有效地缓解不同模态之间和内部的信息不平衡问题。在BraTS2019数据集上的广泛实验表明，ReHyDIL优于最新方法，在各种肿瘤区域的Dice相似系数上提高了超过2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16809v2">PDF</a> MICCAI 2025 Early Accept. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/reeive/ReHyDIL">https://github.com/reeive/ReHyDIL</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于回放和超图域的增量学习（ReHyDIL）方法，用于处理MRI图像中缺失模态的情况下的脑肿瘤分割问题。该方法结合了域增量学习（DIL）技术，使得分割模型能够在没有忘记先前知识的情况下学习新获取的MRI模态。通过引入跨患者超图分割网络（CHSNet）和Tversky感知对比损失（TAC loss），提高了不同患者场景下的分割性能。在BraTS2019数据集上的实验表明，ReHyDIL在Dice相似系数上较现有方法提高了超过2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReHyDIL方法能够处理MRI图像中缺失模态的情况，提高脑肿瘤分割的准确性。</li>
<li>ReHyDIL结合了域增量学习（DIL）技术，使得模型能够学习新获取的MRI模态而不忘记先前知识。</li>
<li>CHSNet网络通过利用超图捕捉患者之间的高阶关联，提高了分割性能。</li>
<li>Tversky感知对比损失（TAC loss）有效地缓解了不同模态间和内部的信息不平衡问题。</li>
<li>ReHyDIL在BraTS2019数据集上的实验表现优于现有方法，Dice相似系数提高了超过2%。</li>
<li>该方法能够应对MRI图像获取过程中的序贯性导致的模态缺失问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-03f82c8c72814176e72332d1ae103a44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce3e84b95fc3b900a06a1f9ac5ab1b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd688d21eddf9e330de941f127023ccb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation"><a href="#Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation" class="headerlink" title="Auto-nnU-Net: Towards Automated Medical Image Segmentation"></a>Auto-nnU-Net: Towards Automated Medical Image Segmentation</h2><p><strong>Authors:Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer</strong></p>
<p>Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/automl/AutoNNUnet">https://github.com/automl/AutoNNUnet</a>. </p>
<blockquote>
<p>医学图像分割（MIS）包括从骨骼到器官分割等多样化的任务，每个任务在寻找最佳分割模型时都面临自己的挑战。目前最先进的与AutoML相关的MIS框架nnU-Net能够自动化模型配置的许多方面，但仍然受到固定超参数和启发式设计选择的限制。作为MIS的全自动机器学习（AutoML）框架，我们提出了Auto-nnU-Net，这是一种新型的nnU-Net变体，能够实现超参数优化（HPO）、神经网络架构搜索（NAS）和分层NAS（HNAS）。此外，我们提出了正则化PriorBand，以平衡模型精度与训练所需的计算资源，解决现实医学环境中经常面临的资源约束问题，这些问题限制了广泛训练程序的可行性。我们在经过充分验证的医学分割十项全能赛的多个人MIS数据集上评估了我们的方法，分析了AutoML技术对分割性能、计算效率和模型设计选择的影响。结果表明，我们的AutoML方法在10个数据集中的6个数据集上显著提高了nnU-Net的分割性能，在其他数据集上表现相当，同时保持了实际的资源需求。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/automl/AutoNNUnet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/automl/AutoNNUnet获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16561v3">PDF</a> 31 pages, 19 figures. Accepted for publication at AutoML 2025</p>
<p><strong>摘要</strong><br>    提出一种全自动医学图像分割框架Auto-nnU-Net，基于nnU-Net进行超参数优化、神经网络架构搜索和分层架构搜索，以提高模型在多种医学图像分割任务上的性能。同时引入Regularized PriorBand方法平衡模型精度与训练所需计算资源，以应对实际医疗环境中资源限制的问题。在Medical Segmentation Decathlon多个数据集上进行的实验表明，该AutoML方法可显著提高nnU-Net在6个数据集上的分割性能，并在其他数据集上表现相当，同时满足实际资源需求。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Auto-nnU-Net框架扩展了nnU-Net，增加了超参数优化、神经网络架构搜索和分层架构搜索功能，适用于多种医学图像分割任务。</li>
<li>Regularized PriorBand方法平衡了模型精度和计算资源需求，适应了医学环境中资源限制的挑战。</li>
<li>相较于nnU-Net，Auto-nnU-Net在多数医学图像分割数据集上表现出更高的性能。</li>
<li>新型框架能够自动化配置模型，降低了手动调整超参数的需求。</li>
<li>该框架的实用性和高效性满足实际医学图像分割的需求。</li>
<li>Auto-nnU-Net的代码已公开发布，便于其他研究者使用和改进。</li>
<li>研究结果对医学图像分割的自动化和性能提升有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28b9beff93368d180f3e8819a1c8a072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-113f352dd289a97cd261c9c75b9d8d7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95c23babfb91541184b1a3629f20407c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation"><a href="#HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation" class="headerlink" title="HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation"></a>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation</h2><p><strong>Authors:Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong</strong></p>
<p>Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities’ anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via <a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR">https://github.com/JeMing-creater/HWA-UNETR</a>. </p>
<blockquote>
<p>在胃癌病灶分析的背景下，多模态医学图像分割面临着巨大的挑战。这种临床背景的特点是缺乏独立的多模态数据集，以及必须将本质上不对齐的模式融合起来的迫切需求。因此，算法受限于在近似数据上进行训练，并依赖于应用迁移，导致资源消耗巨大，分析精度可能下降。为了应对这些挑战，我们做出了两大贡献：首先，我们公开发布了GCM 2025数据集，这是首个大规模、开源的胃癌多模态MRI扫描数据集，包含来自500名患者的专业注释FS-T2W、CE-T1W和ADC图像。其次，我们引入了HWA-UNETR，这是一种新型的3D分割框架，采用可学习的窗口聚合层构成原始HWA块，以在不同模态的解剖结构之间建立动态特征对应关系，并利用创新的tri-orientated融合mamba机制进行上下文建模和捕捉长期空间依赖性。在我们的GCM 2025数据集和公开的BraTS 2021数据集上的大量实验验证了我们的框架性能，证明新方法在Dice得分上最多提高了1.68%，同时保持了稳健性。数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETR公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10464v3">PDF</a> This work has been provisionally accepted for MICCAI 2025</p>
<p><strong>摘要</strong><br>    胃癌病灶分析的多模态医学图像分割面临挑战，包括缺乏独立多模态数据集和需要融合固有错位模态的必要性。为此，本研究贡献了两个重要成果：一是公开了GCM 2025数据集，该数据集为首个公开的大规模胃癌多模态MRI扫描数据集，包含来自500名患者的专业注释FS-T2W、CE-T1W和ADC图像；二是引入了HWA-UNETR，一种新型的3D分割框架，采用可学习的窗口聚合层建立不同模态解剖结构之间的动态特征对应关系，并利用创新的三角融合策略进行上下文建模和捕捉长期空间依赖性。在GCM 2025数据集和公开BraTS 2021数据集上的实验验证了该框架的性能，新方法的Dice得分率提高了高达1.68%，同时保持了稳健性。数据集和代码已公开在<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E3%80%82">https://github.com/JeMing-creater/HWA-UNETR。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态医学图像分割在胃癌病灶分析中面临挑战，主要由于缺乏独立多模态数据集和需要融合不同模态的必要性。</li>
<li>公开了GCM 2025数据集，为胃癌多模态MRI扫描的首个大规模、开源数据集。</li>
<li>引入了HWA-UNETR框架，该框架采用可学习的窗口聚合层建立不同模态之间的动态特征对应关系。</li>
<li>HWA-UNETR框架采用创新的三角融合策略进行上下文建模和捕捉长期空间依赖性。</li>
<li>在GCM 2025和BraTS 2021数据集上的实验验证了HWA-UNETR框架的性能优越性。</li>
<li>新的方法相对于现有方法在Dice得分率上有所提高，最高达到1.68%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-56ca283db7146e0c0f781725e2c0cdac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85a41fcd5f13056645a134422a164310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ab7b06bb46c9d77884911ec3cf98ae.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection"><a href="#Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection" class="headerlink" title="Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection"></a>Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection</h2><p><strong>Authors:Amy Rafferty, Rishi Ramaesh, Ajitha Rajan</strong></p>
<p>Deep learning models have shown promise in lung pathology detection from chest X-rays, but widespread clinical adoption remains limited due to opaque model decision-making. In prior work, we introduced ClinicXAI, a human-centric, expert-guided concept bottleneck model (CBM) designed for interpretable lung cancer diagnosis. We now extend that approach and present XpertXAI, a generalizable expert-driven model that preserves human-interpretable clinical concepts while scaling to detect multiple lung pathologies. Using a high-performing InceptionV3-based classifier and a public dataset of chest X-rays with radiology reports, we compare XpertXAI against leading post-hoc explainability methods and an unsupervised CBM, XCBs. We assess explanations through comparison with expert radiologist annotations and medical ground truth. Although XpertXAI is trained for multiple pathologies, our expert validation focuses on lung cancer. We find that existing techniques frequently fail to produce clinically meaningful explanations, omitting key diagnostic features and disagreeing with radiologist judgments. XpertXAI not only outperforms these baselines in predictive accuracy but also delivers concept-level explanations that better align with expert reasoning. While our focus remains on explainability in lung cancer detection, this work illustrates how human-centric model design can be effectively extended to broader diagnostic contexts - offering a scalable path toward clinically meaningful explainable AI in medical diagnostics. </p>
<blockquote>
<p>深度学习模型在胸部X光片中检测肺部病变方面显示出巨大潜力，但由于模型决策不透明，其在临床上的广泛应用仍然有限。先前，我们引入了ClinicXAI，这是一种以人类为中心、专家指导的概念瓶颈模型（CBM），旨在实现可解释的肺癌诊断。现在我们扩展了该方法，并推出了XpertXAI，这是一种通用、专家驱动型的模型，既保留了可解释的临床概念，又能扩展到检测多种肺部病变。我们使用高性能的InceptionV3分类器和一个带有放射学报告的公共胸部X光数据集，将XpertXAI与领先的事后解释方法以及无监督的CBM（XCBs）进行比较。我们通过专家放射科医生注释和医学真实值对其进行评估。尽管XpertXAI经过多种病变的训练，但我们的专家验证主要集中在肺癌上。我们发现现有的技术往往无法产生具有临床意义的解释，忽略了关键的诊断特征，并与放射科医生的判断存在分歧。XpertXAI不仅在预测准确性方面超越这些基线，而且提供了与专家推理更相符的概念层面的解释。虽然我们的重点仍然是肺癌检测中的可解释性，但这项工作说明了以人类为中心的设计如何有效地扩展到更广泛的诊断环境，为医学诊断中的可解释人工智能提供了可扩展的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09755v2">PDF</a> </p>
<p><strong>Summary</strong><br>    深度学习模型在胸部X光片中检测肺部病变具有潜力，但临床应用广泛采用仍有限，主要由于模型决策不透明。本文扩展先前工作，提出XpertXAI，一种通用专家驱动模型，旨在保留人类可解释的临床概念，同时扩展检测多种肺部病变。通过与领先的后验解释方法和无监督CBM比较评估，以及专家放射科医生注释和医学真实情况验证评估解释结果。尽管XpertXAI针对多种病变进行训练，但专家验证侧重于肺癌。研究发现现有技术难以产生临床上有意义的解释，忽略关键诊断特征并与放射科医生判断存在分歧。XpertXAI不仅预测准确率高于基线，而且提供与专家推理更一致的概念级解释。虽然我们的重点仍然是肺癌检测的可解释性，但这项工作表明以人为中心的设计模型如何有效地扩展到更广泛的诊断情境，为临床上有意义的可解释人工智能诊断提供了可扩展路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在胸部X光肺部病变检测中有潜力，但临床应用受限，主要由于决策不透明。</li>
<li>介绍XpertXAI模型：一种专家驱动模型，旨在实现可解释的肺癌诊断，同时能够检测多种肺部病变。</li>
<li>通过与现有技术比较评估XpertXAI性能，发现现有技术难以产生临床意义的解释。</li>
<li>XpertXAI不仅预测准确率高，而且提供与专家推理更一致的概念级解释。</li>
<li>XpertXAI通过保留人类可解释的临床概念实现解释性增强。</li>
<li>工作展示人为中心的设计模型如何扩展到更广泛的诊断情境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7c8ca1c09ca0cf96cbae73b1d37d6303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04edd3a2425877ee8a7de7b606daa2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88477cb86d423ef7b4d61fb3acace7c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a36c6b91cf135fed68bec7d5d06a8322.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89f48ee1d6aa8edb5eeb123935fff8df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98fc825e71211d41d027322ebc34fc47.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Describe-Anything-in-Medical-Images"><a href="#Describe-Anything-in-Medical-Images" class="headerlink" title="Describe Anything in Medical Images"></a>Describe Anything in Medical Images</h2><p><strong>Authors:Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Lin Zhao, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, Tianming Liu, Min Xu</strong></p>
<p>Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM’s superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration. </p>
<blockquote>
<p>局部图像标注已经取得了显著的进步，如“描述任何事物模型”（DAM）等模型，能够在没有明确的区域文本监督的情况下，生成详细的区域特定描述。然而，这种能力尚未广泛应用于医疗成像等特定领域，诊断解读依赖于微妙的区域发现而非全局理解。为了弥补这一差距，我们提出了MedDAM，这是第一个利用大型视觉语言模型进行医学图像区域特定标注的全面框架。MedDAM采用医学专家设计的针对特定成像模式的提示，并建立了一个稳健的评估基准，包括定制评估协议、数据预处理管道和专用QA模板库。该基准不仅评估MedDAM，还评估其他可适应的大型视觉语言模型，通过属性级别验证任务关注临床事实性，从而避免了医学数据集中地面真实区域标题对缺失的问题。在VinDr-CXR、LIDC-IDRI和SkinCon数据集上的大量实验表明，MedDAM在任务上优于领先的对等模型（包括GPT-4o、Claude 3.7 Sonnet、LLaMA-3.2 Vision、Qwen2.5-VL、GPT-4Rol和OMG-LLaVA），这凸显了区域级别语义对齐在医学图像理解中的重要性，并确立了MedDAM作为临床视觉语言集成的有前途的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05804v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对医学图像的区域特定描述问题，提出MedDAM框架，利用大型视觉语言模型生成详细的区域特定描述，通过医学专家设计的提示和定制评估协议等建立稳健的评估基准，并在实际医学数据集上验证其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedDAM是首个针对医学图像区域特定描述的综合框架，利用大型视觉语言模型生成详细的区域特定描述。</li>
<li>MedDAM通过医学专家设计的提示，适应特定成像模式，建立稳健的评估基准。</li>
<li>评估基准包括定制的评估协议、数据预处理管道和专用问答模板库。</li>
<li>MedDAM侧重于临床事实性，通过属性级别的验证任务来评估模型性能。</li>
<li>MedDAM在VinDr-CXR、LIDC-IDRI和SkinCon数据集上的实验结果表明其优越性。</li>
<li>实验结果揭示了区域级别语义对齐在医学图像理解中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7d403b6b7e3f64ddfca44b5276d6dbb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c5645c201ae936baa89a72807b8918.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-148c5ba40ff16dbda09e506fb74c63fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bab9d231cd26097297f1e380bc7bd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017280bf5c0106a392b503294a6fd28f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>多模态医学图像在精确全面的临床诊断中起着至关重要的作用。扩散模型是合成所需医学图像的一种强大策略。然而，现有方法仍然存在因高频信息过拟合和低频信息减弱导致的解剖结构扭曲问题。因此，我们提出了一种基于动态频率平衡和知识引导的新方法。具体来说，我们首先通过利用小波变换分解模型的关键特征来提取低频和高频成分。然后，设计了一个动态频率平衡模块，该模块可以自适应地调整频率，以增强全局低频特征和有效的高频细节，同时抑制高频噪声。为了克服不同医学模态之间巨大差异所带来的挑战，我们构建了一个知识引导机制，该机制将来自视觉语言模型的先验临床知识与视觉特征相结合，有助于生成准确的解剖结构。在多个数据集上的实验评估表明，所提出的方法在定性和定量评估方面都取得了显著的改进，验证了其有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v2">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>基于动态频率平衡和知识引导的方法在多模态医学图像合成中具有重要作用，解决了现有方法存在的解剖结构扭曲问题。该方法通过小波变换提取高低频成分，并设计动态频率平衡模块自适应调整频率，增强全局低频特征和有效高频细节，同时抑制高频噪声。此外，通过构建知识引导机制融合临床先验知识，生成准确的解剖结构。实验评估表明，该方法在多个数据集上实现了显著的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态医学图像在临床诊断中具有重要作用。</li>
<li>扩散模型是合成医学图像的有效策略。</li>
<li>现有方法存在解剖结构扭曲的问题，主要是由于高频信息的过度拟合和低频信息的减弱。</li>
<li>提出的基于动态频率平衡和知识引导的新方法旨在解决这一问题。</li>
<li>方法通过小波变换提取高低频成分，并使用动态频率平衡模块增强低频特征和有效高频细节。</li>
<li>知识引导机制融合了临床先验知识，有助于生成准确的解剖结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0e2f95b72251a596f9478be8a8bbaf6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85c531eab5646c4c5753970e1ad9ca85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5c9fbf2f2d2e8b959698c952e87886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d0ee28d053b37657d39b8c1df380c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-469a4ebf7d5195a3575d68266a67bc32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65e76e642496b8d2899d05866e3f3785.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy"><a href="#TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy" class="headerlink" title="TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy"></a>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy</h2><p><strong>Authors:Yiling Wang, Elia Lombardo, Adrian Thummerer, Tom Blöcker, Yu Fan, Yue Zhao, Christianna Iris Papadopoulou, Coen Hurkmans, Rob H. N. Tijssen, Pia A. W. Görts, Shyama U. Tetar, Davide Cusumano, Martijn P. W. Intven, Pim Borman, Marco Riboldi, Denis Dudáš, Hilary Byrne, Lorenzo Placidi, Marco Fusella, Michael Jameson, Miguel Palacios, Paul Cobussen, Tobias Finazzi, Cornelis J. A. Haasbeek, Paul Keall, Christopher Kurz, Guillaume Landry, Matteo Maspero</strong></p>
<p>Purpose: Magnetic resonance imaging (MRI) to visualize anatomical motion is becoming increasingly important when treating cancer patients with radiotherapy. Hybrid MRI-linear accelerator (MRI-linac) systems allow real-time motion management during irradiation. This paper presents a multi-institutional real-time MRI time series dataset from different MRI-linac vendors. The dataset is designed to support developing and evaluating real-time tumor localization (tracking) algorithms for MRI-guided radiotherapy within the TrackRAD2025 challenge (<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/">https://trackrad2025.grand-challenge.org/</a>).   Acquisition and validation methods: The dataset consists of sagittal 2D cine MRIs in 585 patients from six centers (3 Dutch, 1 German, 1 Australian, and 1 Chinese). Tumors in the thorax, abdomen, and pelvis acquired on two commercially available MRI-linacs (0.35 T and 1.5 T) were included. For 108 cases, irradiation targets or tracking surrogates were manually segmented on each temporal frame. The dataset was randomly split into a public training set of 527 cases (477 unlabeled and 50 labeled) and a private testing set of 58 cases (all labeled).   Data Format and Usage Notes: The data is publicly available under the TrackRAD2025 collection: <a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539">https://doi.org/10.57967/hf/4539</a>. Both the images and segmentations for each patient are available in metadata format.   Potential Applications: This novel clinical dataset will enable the development and evaluation of real-time tumor localization algorithms for MRI-guided radiotherapy. By enabling more accurate motion management and adaptive treatment strategies, this dataset has the potential to advance the field of radiotherapy significantly. </p>
<blockquote>
<p>目的：在治疗癌症患者时进行放射治疗时，利用磁共振成像（MRI）来可视化解剖运动变得越来越重要。混合MRI-直线加速器（MRI-linac）系统可在照射过程中进行实时运动管理。本文介绍了一个多机构实时MRI时间序列数据集，该数据集来自不同的MRI-linac供应商。该数据集旨在支持开发并评估MRI引导放射治疗的实时肿瘤定位（跟踪）算法，以应对TrackRAD2025挑战（<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/%EF%BC%89%E3%80%82">https://trackrad2025.grand-challenge.org/）。</a></p>
</blockquote>
<p>采集和验证方法：数据集包含来自6个中心（荷兰3个，德国1个，澳大利亚1个，中国1个）的585例患者的矢状面2D电影MRI。胸、腹和骨盆部位的肿瘤是在两台商用MRI-linac（0.35T和1.5T）上获得的。在108个病例中，每个时间帧上都手动分割了照射目标或跟踪代理。数据集被随机分成527例公共训练集（477例未标记和50例已标记）和58例私有测试集（均已标记）。</p>
<p>数据格式和使用注意事项：数据可在TrackRAD2025收藏中公开获取：<a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539%E3%80%82%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%84%E5%9B%BE%E5%83%8F%E5%92%8C%E5%88%86%E6%AE%B5%E9%83%BD%E5%8F%AF%E7%94%A8%E5%85%83%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E6%9F%A5%E7%9C%8B%E3%80%82">https://doi.org/10.57967/hf/4539。每个患者的图像和分段都可用元数据格式查看。</a></p>
<p>潜在应用：这个新型临床数据集将能够开发和评估MRI引导放射治疗的实时肿瘤定位算法。通过实现更精确的运动管理和自适应治疗方案，该数据集有望推动放射治疗领域的发展。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19119v2">PDF</a> 10 pages, 5 figures, 2 tables; submitted to Medical Physics,   tentatively accepted</p>
<p><strong>Summary</strong><br>磁共振成像（MRI）在癌症患者放疗治疗中用于可视化解剖运动日益重要。本文介绍了一个多机构实时MRI时间序列数据集，该数据集来自不同的MRI-直线加速器（MRI-linac）供应商，旨在支持开发并评估用于MRI引导放疗的实时肿瘤定位（跟踪）算法。数据集包含来自六个中心的585名患者的矢状面2D电影MRI，涵盖了胸部、腹部和骨盆的肿瘤。此数据集已公开，并可用于TrackRAD2025挑战赛（<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/%EF%BC%89%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9C%89%E6%9C%9B%E4%BF%83%E8%BF%9B%E5%AE%9E%E6%97%B6%E8%82%BF%E7%98%A4%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%BC%80%E5%8F%91%E5%92%8C%E8%AF%84%E4%BC%B0%EF%BC%8C%E9%80%9A%E8%BF%87%E6%9B%B4%E7%B2%BE%E7%A1%AE%E7%9A%84%E8%BF%90%E5%8A%A8%E7%AE%A1%E7%90%86%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E6%B2%BB%E7%96%97%E6%96%B9%E6%A1%88%EF%BC%8C%E4%B8%BA%E6%94%BE%E7%96%97%E9%A2%86%E5%9F%9F%E5%B8%A6%E6%9D%A5%E6%98%BE%E8%91%97%E8%BF%9B%E5%B1%95%E3%80%82">https://trackrad2025.grand-challenge.org/）。该数据集有望促进实时肿瘤定位算法的开发和评估，通过更精确的运动管理和自适应治疗方案，为放疗领域带来显著进展。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实时MRI时间序列数据集支持开发并评估用于MRI引导放疗的肿瘤定位算法。</li>
<li>数据集包含来自不同MRI-linac供应商的实时数据，涵盖胸部、腹部和骨盆的肿瘤。</li>
<li>数据集包含公开可用的训练集和测试集，分为有标签和无标签的数据。</li>
<li>数据集具有潜力推动放疗领域的发展，通过更精确的运动管理和自适应治疗方案改善治疗效果。</li>
<li>数据集可用于验证和改进肿瘤运动跟踪算法的性能和准确性。</li>
<li>此数据集是首个多机构合作创建的大规模实时MRI数据集，具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19119">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a76b9e93ef13273798906b478c28c7d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9236fd355aa01f6c7e9526c44ab0c3f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a20a738135024c911b7f29a0cf0c5bf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b66b8b634d47f0db8f14cdc65f473eb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UltraBones100k-A-reliable-automated-labeling-method-and-large-scale-dataset-for-ultrasound-based-bone-surface-extraction"><a href="#UltraBones100k-A-reliable-automated-labeling-method-and-large-scale-dataset-for-ultrasound-based-bone-surface-extraction" class="headerlink" title="UltraBones100k: A reliable automated labeling method and large-scale   dataset for ultrasound-based bone surface extraction"></a>UltraBones100k: A reliable automated labeling method and large-scale   dataset for ultrasound-based bone surface extraction</h2><p><strong>Authors:Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd Viehöfer, Stephan Wirth, Lilian Calvet, Philipp Fürnstahl</strong></p>
<p>Ultrasound-based bone surface segmentation is crucial in computer-assisted orthopedic surgery. However, ultrasound images have limitations, including a low signal-to-noise ratio, and acoustic shadowing, which make interpretation difficult. Existing deep learning models for bone segmentation rely primarily on costly manual labeling by experts, limiting dataset size and model generalizability. Additionally, the complexity of ultrasound physics and acoustic shadow makes the images difficult for humans to interpret, leading to incomplete labels in anechoic regions and limiting model performance. To advance ultrasound bone segmentation and establish effective model benchmarks, larger and higher-quality datasets are needed.   We propose a methodology for collecting ex-vivo ultrasound datasets with automatically generated bone labels, including anechoic regions. The proposed labels are derived by accurately superimposing tracked bone CT models onto the tracked ultrasound images. These initial labels are refined to account for ultrasound physics. A clinical evaluation is conducted by an expert physician specialized on orthopedic sonography to assess the quality of the generated bone labels. A neural network for bone segmentation is trained on the collected dataset and its predictions are compared to expert manual labels, evaluating accuracy, completeness, and F1-score.   We collected the largest known dataset of 100k ultrasound images of human lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank test with Bonferroni correction confirmed that the bone alignment after our method significantly improved the quality of bone labeling (p &lt; 0.001). The model trained on UltraBones100k consistently outperforms manual labeling in all metrics, particularly in low-intensity regions (320% improvement in completeness at a distance threshold of 0.5 mm). </p>
<blockquote>
<p>基于超声的骨骼表面分割在计算机辅助骨科手术中至关重要。然而，超声图像存在信号噪声比低和声影等局限性，使得解读困难。现有的骨骼分割深度学习模型主要依赖于专家昂贵的手动标注，这限制了数据集的大小和模型的通用性。此外，超声物理和声影的复杂性使得图像对人类来说难以解读，导致无声区域的标签不完整并限制了模型性能。为了推进超声骨分割并建立有效的模型基准，需要更大、更高质量的数据集。我们提出了一种收集离体超声数据集的方法，其中包括自动生成的骨骼标签和无声区域。这些标签是通过准确地将追踪的骨骼CT模型叠加到追踪的超声图像上而得出的。这些初始标签经过调整以考虑超声物理特性。由专门研究骨科超声的专家医生进行临床评估，以评估生成的骨骼标签的质量。在收集的数据集上训练了一个骨骼分割神经网络，并将其预测结果与专家手动标签进行比较，评估准确性、完整性和F1分数。我们收集了已知最大的包含骨骼标签的10万张人类下肢超声图像数据集，称为UltraBones100k。采用威尔科克斯符号秩检验进行Bonferroni校正后证实，使用我们的方法后，骨骼对齐显著提高了骨骼标签的质量（p &lt; 0.001）。在UltraBones100k上训练的模型在所有指标上都一致优于手动标注，特别是在低强度区域（在距离阈值为0.5毫米的情况下，完整性提高了320%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03783v3">PDF</a> </p>
<p><strong>摘要</strong><br>    超声骨表面分割在计算机辅助骨科手术中至关重要。然而，由于超声图像存在信号噪声比较低、声影等局限性，使得解读困难。现有的深度学习骨分割模型主要依赖专家昂贵的手动标注，限制了数据集大小和模型的泛化能力。为推进超声骨分割并建立有效的模型基准，需要更大、更高质量的数据集。我们提出了一种收集离体超声数据集的方法，可以自动生成骨标签，包括无声区标签。标签是通过将追踪的骨CT模型精确叠加到追踪的超声图像上而派生出来的。这些初始标签经过了超声物理学的修正。由专家医生对生成的骨标签质量进行评估。在收集的数据集上训练了一个用于骨分割的神经网络，并将其预测结果与专家手动标签进行比较，评估了准确性、完整性和F1分数。我们收集了已知最大的包含人类下肢骨骼标签的超声图像数据集UltraBones100k。通过Wilcoxon符号秩检验和Bonferroni校正证实，我们的方法显著提高了骨标签的质量（p &lt; 0.001）。在UltraBones100k上训练的模型在所有指标上都优于手动标注，特别是在低强度区域（在距离阈值为0.5毫米的情况下，完整性提高了320%）。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>超声骨表面分割在骨科手术中非常重要，但超声图像解读存在困难。</li>
<li>当前深度学习模型受限于手动标注，影响数据集大小和模型泛化能力。</li>
<li>提出了一种自动生成骨标签的方法，适用于离体超声数据集。</li>
<li>通过将追踪的骨CT模型叠加到超声图像上生成初始标签，并进行超声物理学修正。</li>
<li>进行了临床评估，证实了生成的骨标签质量显著提高。</li>
<li>在收集的大型数据集UltraBones100k上训练的模型表现优于手动标注。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577ee87f914d2a9b569b5a52f86bf342.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PUSSM-Point-Cloud-Upsampling-as-Implicit-Statistical-Shape-Model"><a href="#PUSSM-Point-Cloud-Upsampling-as-Implicit-Statistical-Shape-Model" class="headerlink" title="PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model"></a>PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model</h2><p><strong>Authors:Tongxu Zhang, Bei Wang</strong></p>
<p>This paper proposes a framework for high-fidelity reconstruction of pelvic structures by integrating medical image segmentation and point cloud upsampling. By point cloud upsampling to learn shape priors from MedShapePelvic without requiring landmarks or PCA, our method functions as an implicit statistical shape model. Evaluations on Pelvic1k show significant improvements in surface quality and anatomical accuracy. This approach is generalizable and applicable to other skeletal regions. </p>
<blockquote>
<p>本文提出一个通过整合医学图像分割和点云上采样进行骨盆结构高保真重建的框架。通过点云上采样从MedShapePelvic中学习形状先验知识，而无需地标或主成分分析，我们的方法可作为隐式统计形状模型。在Pelvic1k上的评估显示，表面质量和解剖精度有了显著提高。此方法是通用的，适用于其他骨骼区域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16716v3">PDF</a> 14 pages, 4 figures</p>
<p><strong>Summary</strong><br>本文提出了一种结合医学图像分割和点云上采样技术的骨盆结构高保真重建框架。通过点云上采样，从MedShapePelvic中学习形状先验，无需地标或PCA，该方法可作为隐式统计形状模型。在Pelvic1k上的评估显示，表面质量和解剖精度得到显著提高。该方法可推广应用于其他骨骼区域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>骨盆结构的高保真重建框架结合了医学图像分割和点云上采样技术。</li>
<li>通过点云上采样学习形状先验，无需使用地标或PCA技术。</li>
<li>该方法可作为隐式统计形状模型。</li>
<li>在Pelvic1k上的评估表明，该方法的表面质量和解剖精度显著提高。</li>
<li>该方法具有推广性，可应用于其他骨骼区域的重建。</li>
<li>该框架对于医学图像分析和处理具有重要的应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a0fd4b52ce607b09b55a3b2eaef4682c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f8517f0a301f19cb35a0377ab8bae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17913b850528ee98d312d3783099c475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc8ab7688631ba96752c9197204ef3a7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging"><a href="#Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging" class="headerlink" title="Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging"></a>Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging</h2><p><strong>Authors:Jarett Dewbury, Chi-en Amy Tai, Alexander Wong</strong></p>
<p>Prostate cancer (PCa) is the most prevalent cancer among men in the United States, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000 total deaths in 2024. Traditional screening methods such as prostate-specific antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in diagnosis, but have faced limitations in specificity and generalizability. In this paper, we explore the potential of enhancing PCa gland segmentation using a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$). We employ several state-of-the-art deep learning models, including U-Net, SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment prostate glands from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \pm 0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82 \pm 2.0$), offered a favorable balance between accuracy and computational efficiency. Our findings demonstrate the potential of deep learning models in improving prostate gland segmentation using CDI$^s$ to enhance PCa management and clinical support. </p>
<blockquote>
<p>前列腺癌（PCa）是美国男性最常见的癌症，2024年将近有30万例病例，占所有诊断的29%，以及导致3万5千人死亡。传统的筛查方法，如前列腺特异性抗原（PSA）检测和磁共振成像（MRI），在诊断中至关重要，但在特异性和普及性方面存在局限性。在本文中，我们探讨了使用一种名为合成相关扩散成像（CDI$^s$）的新型MRI模式提高前列腺癌腺体分割的潜力。我们采用了先进的深度学习模型，包括U-Net、SegResNet、Swin UNETR、Attention U-Net和LightM-UNet，对来自200名CDI$^s$患者的前列腺腺体进行分割。我们发现SegResNet的分割性能最佳，Dice-Sorensen系数（DSC）为$76.68 \pm 0.8$。值得注意的是，虽然Attention U-Net的准确度稍低（DSC $74.82 \pm 2.0$），但在准确性与计算效率之间达到了有利的平衡。我们的研究结果表明，深度学习模型在利用CDI$^s$提高前列腺癌腺体分割方面具有潜力，可改善前列腺癌管理和临床支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09185v2">PDF</a> 8 pages, 2 figures, to be published in Studies in Computational   Intelligence. This paper introduces Cancer-Net PCa-Seg, a comprehensive   evaluation of deep learning models for prostate cancer segmentation using   synthetic correlated diffusion imaging (CDI$^s$). We benchmark five   state-of-the-art architectures: U-Net, SegResNet, Swin UNETR, Attention   U-Net, and LightM-UNet</p>
<p><strong>Summary</strong><br>前列腺癌是美国男性最常见的癌症之一，针对其诊断方法如前列腺特异性抗原检测和磁共振成像存在局限性。本研究探讨了使用合成相关扩散成像（CDI$^s$）这一新型MRI模式在前列腺癌腺体分割方面的潜力。采用多种先进的深度学习模型，发现SegResNet模型分割性能最优，Dice系数为$76.68 \pm 0.8$。Attention U-Net模型虽准确性稍低，但计算效率较高，为前列腺癌管理和临床支持提供了新的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>前列腺癌是美国男性最常见的癌症之一，预计到2024年将导致近3万例死亡。</li>
<li>传统的前列腺癌诊断方法如PSA测试和MRI存在局限性。</li>
<li>合成相关扩散成像（CDI$^s$）作为一种新型的MRI模态被研究用于增强前列腺癌腺体的分割。</li>
<li>深度学习模型在前列腺癌腺体分割中显示出潜力，其中SegResNet模型表现最佳。</li>
<li>Attention U-Net模型在计算效率和准确性之间达到了较好的平衡。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa50a45c0d07e82a1ffd6dfe2e1714bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0cfd36a497309a846da241d173813f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db6d0c08df0c7edfddccb4e7c7e4475.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8de2ae4d28c9195f7096460090da067c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-05-29  ArVoice A Multi-Speaker Dataset for Arabic Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea46b9c6e3bba5c3d8ca0e6052498fa2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-29  Be Decisive Noise-Induced Layouts for Multi-Subject Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
