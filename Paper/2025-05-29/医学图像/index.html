<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Lunguage A Benchmark for Structured and Sequential Chest X-ray   Interpretation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="Lunguage-A-Benchmark-for-Structured-and-Sequential-Chest-X-ray-Interpretation"><a href="#Lunguage-A-Benchmark-for-Structured-and-Sequential-Chest-X-ray-Interpretation" class="headerlink" title="Lunguage: A Benchmark for Structured and Sequential Chest X-ray   Interpretation"></a>Lunguage: A Benchmark for Structured and Sequential Chest X-ray   Interpretation</h2><p><strong>Authors:Jong Hak Moon, Geon Choi, Paloma Rabaey, Min Gwan Kim, Hyuk Gi Hong, Jung-Oh Lee, Hangyul Yoon, Eun Woo Doe, Jiyoun Kim, Harshita Sharma, Daniel C. Castro, Javier Alvarez-Valle, Edward Choi</strong></p>
<p>Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/SuperSupermoon/Lunguage">https://github.com/SuperSupermoon/Lunguage</a> </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šä¼ è¾¾è¯¦ç»†çš„ä¸´åºŠè§‚å¯Ÿï¼Œå¹¶æ•æ‰éšæ—¶é—´æ¼”å˜çš„è¯Šæ–­æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä»…é™äºå•ä¸€æŠ¥å‘Šè®¾ç½®ï¼Œå¹¶ä¾èµ–äºç²—ç³™çš„æŒ‡æ ‡ï¼Œæ— æ³•æ•æ‰ç²¾ç»†çš„ä¸´åºŠè¯­ä¹‰å’Œæ—¶é—´ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬ä»‹ç»äº†LUNGUAGEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†ï¼Œæ”¯æŒå•æŠ¥å‘Šè¯„ä¼°å’Œå¤šç ”ç©¶ä¸­çš„çºµå‘æ‚£è€…æ°´å¹³è¯„ä¼°ã€‚å®ƒåŒ…å«1473ä»½ç»è¿‡ä¸“å®¶å®¡æŸ¥çš„èƒ¸éƒ¨Xå°„çº¿æŠ¥å‘Šæ³¨é‡Šï¼Œå…¶ä¸­80ä»½åŒ…å«çºµå‘æ³¨é‡Šï¼Œä»¥æ•æ‰ç–¾ç—…è¿›å±•å’Œç ”ç©¶é—´é—´éš”ï¼Œä¹Ÿç»è¿‡ä¸“å®¶å®¡æŸ¥ã€‚ä½¿ç”¨è¿™ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†ç”Ÿæˆçš„æŠ¥å‘Šè½¬æ¢ä¸ºç²¾ç»†çš„ã€ä¸æ¨¡å¼å¯¹é½çš„ç»“æ„åŒ–è¡¨ç¤ºå½¢å¼ï¼Œä»¥å®ç°çºµå‘è§£é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LUNGUAGESCOREè¿™ä¸€å¯è§£é‡Šçš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡å¯ä»¥åœ¨å®ä½“ã€å…³ç³»å’Œå±æ€§å±‚é¢æ¯”è¾ƒç»“æ„åŒ–è¾“å‡ºï¼ŒåŒæ—¶å»ºæ¨¡æ‚£è€…æ—¶é—´çº¿ä¸Šçš„æ—¶é—´ä¸€è‡´æ€§ã€‚è¿™äº›è´¡çŒ®å»ºç«‹äº†é¦–ä¸ªåŸºå‡†æ•°æ®é›†ã€ç»“æ„åŒ–æ¡†æ¶å’Œé¡ºåºæ”¾å°„å­¦æŠ¥å‘Šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®è¯ç»“æœè¡¨æ˜LUNGUAGESCOREæœ‰æ•ˆåœ°æ”¯æŒç»“æ„åŒ–æŠ¥å‘Šè¯„ä¼°ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/SuperSupermoon/Lunguage">https://github.com/SuperSupermoon/Lunguage</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LUNGUAGEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”¨äºç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„è¯„ä»·ï¼Œæ”¯æŒå•æŠ¥å‘Šè¯„ä»·å’Œè·¨å¤šç ”ç©¶çš„çºµå‘ç—…äººçº§è¯„ä¼°ã€‚è¯¥æ•°æ®é›†åŒ…å«ç»è¿‡ä¸“å®¶å®¡æ ¸çš„èƒ¸éƒ¨Xå°„çº¿æŠ¥å‘Šï¼Œå…¶ä¸­éƒ¨åˆ†æŠ¥å‘ŠåŒ…å«ç–¾ç—…è¿›å±•å’Œè·¨ç ”ç©¶æ—¶é—´é—´éš”çš„çºµå‘æ³¨é‡Šã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶å’ŒLUNGUAGESCOREè¯„ä»·æŒ‡æ ‡ï¼Œç”¨äºå¯¹ç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œç²¾ç»†ç»“æ„åŒ–è¡¨ç¤ºï¼Œå¹¶åœ¨å®ä½“ã€å…³ç³»å’Œå±æ€§çº§åˆ«è¿›è¡Œæ¯”è¾ƒï¼ŒåŒæ—¶å»ºç«‹æ‚£è€…æ—¶é—´çº¿çš„æ—¶åºä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„å­¦æŠ¥å‘ŠåŒ…å«è¯¦ç»†çš„ä¸´åºŠè§‚å¯Ÿå’Œè¯Šæ–­æ¨ç†ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä»…é™äºå•æŠ¥å‘Šè®¾ç½®ï¼Œå¹¶ä¾èµ–äºæ— æ³•æ•è·ç»†å¾®ä¸´åºŠè¯­ä¹‰å’Œæ—¶åºä¾èµ–æ€§çš„ç²—ç•¥æŒ‡æ ‡ã€‚</li>
<li>å¼•å…¥LUNGUAGEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç»è¿‡ä¸“å®¶å®¡æ ¸çš„èƒ¸éƒ¨Xå°„çº¿æŠ¥å‘Šï¼Œå¹¶æ”¯æŒå•æŠ¥å‘Šè¯„ä»·å’Œè·¨ç ”ç©¶çš„çºµå‘ç—…äººçº§è¯„ä¼°ã€‚</li>
<li>æ•°æ®é›†ä¸­éƒ¨åˆ†æŠ¥å‘ŠåŒ…å«æè¿°ç–¾ç—…è¿›å±•å’Œè·¨ç ”ç©¶æ—¶é—´é—´éš”çš„çºµå‘æ³¨é‡Šï¼Œè¿™äº›æ³¨é‡Šç»è¿‡ä¸“å®¶å®¡æ ¸ã€‚</li>
<li>æå‡ºä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†ç”Ÿæˆçš„æŠ¥å‘Šè½¬åŒ–ä¸ºç²¾ç»†çš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œä»¥æ”¯æŒçºµå‘è§£é‡Šã€‚</li>
<li>å¼•å…¥LUNGUAGESCOREè¯„ä»·æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åœ¨å®ä½“ã€å…³ç³»å’Œå±æ€§çº§åˆ«æ¯”è¾ƒç»“æ„åŒ–è¾“å‡ºï¼Œå¹¶å»ºæ¨¡æ‚£è€…æ—¶é—´çº¿çš„æ—¶åºä¸€è‡´æ€§ã€‚</li>
<li>è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹åºåˆ—æ”¾å°„å­¦æŠ¥å‘Šçš„ç»“æ„åŒ–æ•°æ®é›†ã€æ„å»ºæ¡†æ¶å’Œè¯„ä»·æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-515d291d749bd4d15cbcc9b98a12e48f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1b6e857b57dfcbf4915c4f819c8cf16.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Identifying-Compton-thick-AGNs-with-Machine-learning-algorithm-in-Chandra-Deep-Field-South"><a href="#Identifying-Compton-thick-AGNs-with-Machine-learning-algorithm-in-Chandra-Deep-Field-South" class="headerlink" title="Identifying Compton-thick AGNs with Machine learning algorithm in   Chandra Deep Field-South"></a>Identifying Compton-thick AGNs with Machine learning algorithm in   Chandra Deep Field-South</h2><p><strong>Authors:Rui Zhang, Xiaotong Guo, Qiusheng Gu, Guanwen Fang, Jun Xu, Hai-Cheng Feng, Yongyun Chen, Rui Li, Nan Ding, Hongtao Wang</strong></p>
<p>Compton-thick active galactic nuclei (CT-AGNs), which are defined by column density $\mathrm{N_H} \geqslant 1.5 \times 10^{24} \ \mathrm{cm}^{-2}$, emit feeble X-ray radiation, even undetectable by X-ray instruments. Despite this, the X-ray emissions from CT-AGNs are believed to be a substantial contributor to the cosmic X-ray background (CXB). According to synthesis models of AGNs, CT-AGNs are expected to make up a significant fraction of the AGN population, likely around 30% or more. However, only $\sim$11% of AGNs have been identified as CT-AGNs in the Chandra Deep Field-South (CDFS). To identify hitherto unknown CT-AGNs in the field, we used a Random Forest algorithm for identifying them. First, we build a secure classified subset of 210 AGNs to train and evaluate our algorithm. Our algorithm achieved an accuracy rate of 90% on the test set after training. Then, we applied our algorithm to an additional subset of 254 AGNs, successfully identifying 67 CT-AGNs within this group. This result significantly increased the fraction of CT-AGNs in the CDFS, which is closer to the theoretical predictions of the CXB. Finally, we compared the properties of host galaxies between CT-AGNs and non-CT-AGNs and found that the host galaxies of CT-AGNs exhibit higher levels of star formation activity. </p>
<blockquote>
<p>åº·æ™®é¡¿åšæ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆCT-AGNsï¼‰ï¼Œä»¥å…¶åˆ—å¯†åº¦$\mathrm{N_H} \geqslant 1.5 \times 10^{24} \ \mathrm{cm}^{-2}$ä¸ºç‰¹å¾ï¼Œå‘å‡ºå¾®å¼±çš„Xå°„çº¿è¾å°„ï¼Œç”šè‡³æ— æ³•è¢«Xå°„çº¿ä»ªå™¨æ¢æµ‹åˆ°ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒCT-AGNsçš„Xå°„çº¿å‘å°„è¢«è®¤ä¸ºæ˜¯å®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æ ¹æ®æ´»åŠ¨æ˜Ÿç³»æ ¸çš„åˆæˆæ¨¡å‹ï¼ŒCT-AGNsé¢„è®¡å æ´»åŠ¨æ˜Ÿç³»æ ¸äººå£çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼Œå¯èƒ½è¾¾åˆ°30%æˆ–æ›´å¤šã€‚ç„¶è€Œï¼Œåœ¨é’±å¾·æ‹‰å—æ·±åœºï¼ˆCDFSï¼‰ä¸­ï¼Œåªæœ‰çº¦11%çš„æ´»åŠ¨æ˜Ÿç³»æ ¸è¢«è¯†åˆ«ä¸ºCT-AGNsã€‚ä¸ºäº†è¯†åˆ«é¢†åŸŸä¸­çš„æœªçŸ¥CT-AGNsï¼Œæˆ‘ä»¬é‡‡ç”¨äº†éšæœºæ£®æ—ç®—æ³•è¿›è¡Œè¯†åˆ«ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŒ…å«210ä¸ªå®‰å…¨åˆ†ç±»çš„æ´»åŠ¨æ˜Ÿç³»æ ¸å­é›†æ¥è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„ç®—æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†90%çš„å‡†ç¡®ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç®—æ³•åº”ç”¨äºé¢å¤–çš„254ä¸ªæ´»åŠ¨æ˜Ÿç³»æ ¸å­é›†ï¼ŒæˆåŠŸè¯†åˆ«å‡ºå…¶ä¸­çš„67ä¸ªCT-AGNsã€‚è¿™ä¸€ç»“æœæ˜¾è‘—å¢åŠ äº†CDFSä¸­CT-AGNsçš„æ¯”ä¾‹ï¼Œæ›´æ¥è¿‘CXBçš„ç†è®ºé¢„æµ‹ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†CT-AGNså’ŒéCT-AGNså®¿ä¸»æ˜Ÿç³»çš„å±æ€§ï¼Œå‘ç°CT-AGNsçš„å®¿ä¸»æ˜Ÿç³»è¡¨ç°å‡ºæ›´é«˜çš„æ’æ˜Ÿå½¢æˆæ´»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21105v1">PDF</a> 12 pages, 6 figures, 2 Tables. Accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é«˜æŸ±å¯†åº¦æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆCT-AGNsï¼‰çš„X-rayè¾å°„ç‰¹æ€§åŠå…¶å¯¹å®‡å®™X-rayèƒŒæ™¯ï¼ˆCXBï¼‰çš„è´¡çŒ®ã€‚è™½ç„¶CT-AGNsçš„X-rayè¾å°„å¾®å¼±ï¼Œç”šè‡³æ— æ³•è¢«X-rayä»ªå™¨æ¢æµ‹åˆ°ï¼Œä½†å®ƒä»¬è¢«è®¤ä¸ºæ˜¯CXBçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡è¿ç”¨éšæœºæ£®æ—ç®—æ³•ï¼ŒæˆåŠŸè¯†åˆ«å‡ºæ›´å¤šçš„CT-AGNsï¼Œå¹¶å‘ç°å…¶å®¿ä¸»æ˜Ÿç³»å…·æœ‰è¾ƒé«˜çš„æ’æ˜Ÿå½¢æˆæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-AGNsçš„X-rayè¾å°„ç‰¹æ€§å¾®å¼±ï¼Œä½†ä»ç„¶å¯¹å®‡å®™X-rayèƒŒæ™¯ï¼ˆCXBï¼‰æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</li>
<li>åˆæˆæ¨¡å‹é¢„æµ‹CT-AGNsåœ¨AGNæ€»ä½“ä¸­å ç›¸å½“å¤§çš„æ¯”ä¾‹ï¼Œå¯èƒ½è¾¾åˆ°æˆ–è¶…è¿‡30%ã€‚</li>
<li>é€šè¿‡éšæœºæ£®æ—ç®—æ³•ï¼Œæé«˜äº†å¯¹CT-AGNsçš„è¯†åˆ«ç‡ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†90%ã€‚</li>
<li>åœ¨CDFSä¸­æˆåŠŸè¯†åˆ«å‡ºçš„CT-AGNsæ•°é‡å¢åŠ ï¼Œæ›´æ¥è¿‘ç†è®ºé¢„æµ‹ã€‚</li>
<li>ä¸éCT-AGNsç›¸æ¯”ï¼ŒCT-AGNsçš„å®¿ä¸»æ˜Ÿç³»è¡¨ç°å‡ºæ›´é«˜çš„æ’æ˜Ÿå½¢æˆæ´»æ€§ã€‚</li>
<li>è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é€‰æ‹©å¯¹ç®—æ³•çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚éœ€è¦é€‰æ‹©é€‚å½“çš„å­é›†æ¥è®­ç»ƒå’Œè¯„ä¼°ç®—æ³•ä»¥ç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce7e5134ba91a74a4167bb75c2397da8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9811b67d43fe7678bfe962ff56ea0cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-383df6d79ae20eebe38b36d2a304a377.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="All-optical-discrete-illumination-based-compressed-ultrafast-photography"><a href="#All-optical-discrete-illumination-based-compressed-ultrafast-photography" class="headerlink" title="All-optical discrete illumination-based compressed ultrafast photography"></a>All-optical discrete illumination-based compressed ultrafast photography</h2><p><strong>Authors:Long Cheng, Dalong Qi, Jiali Yao, Ning Xu, Chengyu Zhou, Wenzhang Lin, Yu He, Zhen Pan, Yunhua Yao, Lianzhong Deng, Yuecheng Shen, Zhenrong Sun, Shian Zhang</strong></p>
<p>Snapshot ultrafast optical imaging (SUOI) plays a vital role in capturing complex transient events in real time, with significant implications for both fundamental science and practical applications. As an outstanding talent in SUOI, compressed ultrafast photography (CUP) has demonstrated remarkable frame rate reaching trillions of frames per second and hundreds of sequence depth. Nevertheless, as CUP relies on streak cameras, the systemâ€™s imaging fidelity suffers from an inevitable limitation induced by the charge coupling artifacts in a streak camera. Moreover, although advanced image reconstruction algorithms have improved the recovered scenes, its high compression ratio still causes a compromise in image quality. To address these challenges, we propose a novel approach termed all-optical discrete illumination compressed ultrafast photography (AOD-CUP), which employs a free-space angular-chirp-enhanced delay (FACED) technique to temporally stretch femtosecond pulses and achieves discrete illumination for dynamic scenes. With its distinctive system architecture, AOD-CUP features adjustable frame numbers and flexible inter-frame intervals ranging from picoseconds to nanoseconds, thereby achieving high-fidelity ultrafast imaging in a snapshot. Experimental results demonstrate the systemâ€™s superior dynamic spatial resolution and its capability to visualize ultrafast phenomena with complex spatial details, such as stress wave propagation in LiF crystals and air plasma channel formation. These results highlight the potential of AOD-CUP for high-fidelity, real-time ultrafast imaging, which provides an unprecedented tool for advancing the frontiers of ultrafast science. </p>
<blockquote>
<p>å¿«ç…§è¶…é«˜é€Ÿå…‰å­¦æˆåƒï¼ˆSUOIï¼‰åœ¨å®æ—¶æ•æ‰å¤æ‚çš„ç¬æ€äº‹ä»¶æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¯¹åŸºç¡€ç§‘å­¦å’Œå®é™…åº”ç”¨éƒ½æœ‰é‡è¦æ„ä¹‰ã€‚ä½œä¸ºSUOIé¢†åŸŸçš„æ°å‡ºäººæ‰ï¼Œå‹ç¼©è¶…é«˜é€Ÿæ‘„å½±ï¼ˆCUPï¼‰å·²ç»å®ç°äº†é«˜è¾¾æ¯ç§’æ•°ä¸‡äº¿å¸§çš„å¸§ç‡ä»¥åŠæ•°ç™¾åºåˆ—æ·±åº¦ã€‚ç„¶è€Œï¼Œç”±äºCUPä¾èµ–äºæ¡çº¹ç›¸æœºï¼Œç³»ç»Ÿçš„æˆåƒä¿çœŸåº¦å—åˆ°æ¡çº¹ç›¸æœºä¸­ç”µè·è€¦åˆä¼ªå½±å¼•èµ·çš„ä¸å¯é¿å…çš„é™åˆ¶ã€‚å°½ç®¡å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•æé«˜äº†æ¢å¤çš„åœºæ™¯è´¨é‡ï¼Œä½†å…¶è¾ƒé«˜çš„å‹ç¼©æ¯”ä»ç„¶ä¼šå¯¼è‡´å›¾åƒè´¨é‡çš„å¦¥åã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå…¨å…‰å­¦ç¦»æ•£ç…§æ˜å‹ç¼©è¶…é«˜é€Ÿæ‘„å½±ï¼ˆAOD-CUPï¼‰ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è‡ªç”±ç©ºé—´è§’å•å•¾å¢å¼ºå»¶è¿Ÿï¼ˆFACEDï¼‰æŠ€æœ¯æ¥å»¶é•¿é£ç§’è„‰å†²çš„æ—¶é—´ï¼Œå¹¶ä¸ºåŠ¨æ€åœºæ™¯å®ç°ç¦»æ•£ç…§æ˜ã€‚å‡­å€Ÿç‹¬ç‰¹çš„ç³»ç»Ÿæ¶æ„ï¼ŒAOD-CUPå…·æœ‰å¯è°ƒæ•´çš„å¸§æ•°å’Œçµæ´»çš„å¸§é—´é—´éš”ï¼ŒèŒƒå›´ä»çš®ç§’åˆ°çº³ç§’ï¼Œä»è€Œåœ¨ä¸€æ¬¡å¿«ç…§ä¸­å®ç°é«˜ä¿çœŸè¶…é«˜é€Ÿæˆåƒã€‚å®éªŒç»“æœè¯æ˜äº†ç³»ç»Ÿå‡ºè‰²çš„åŠ¨æ€ç©ºé—´åˆ†è¾¨ç‡ä»¥åŠå…¶åœ¨å¯è§†åŒ–å…·æœ‰å¤æ‚ç©ºé—´ç»†èŠ‚çš„è¶…å¿«ç°è±¡æ–¹é¢çš„èƒ½åŠ›ï¼Œå¦‚LiFæ™¶ä½“ä¸­çš„åº”åŠ›æ³¢ä¼ æ’­å’Œç©ºæ°”ç­‰ç¦»å­ä½“é€šé“çš„å½¢æˆã€‚è¿™äº›ç»“æœçªæ˜¾äº†AOD-CUPåœ¨é«˜ä¿çœŸã€å®æ—¶è¶…é«˜é€Ÿæˆåƒæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¶…å¿«ç§‘å­¦çš„å‰æ²¿å‘å±•æä¾›äº†å‰æ‰€æœªæœ‰çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21086v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¿«ç…§è¶…é«˜é€Ÿå…‰å­¦æˆåƒï¼ˆSUOIï¼‰åœ¨å®æ—¶æ•æ‰å¤æ‚ç¬æ€äº‹ä»¶æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¯¹åŸºç¡€ç§‘å­¦å’Œå®é™…åº”ç”¨éƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚å‹ç¼©è¶…é«˜é€Ÿæ‘„å½±ï¼ˆCUPï¼‰ä½œä¸ºSUOIçš„æ°å‡ºäººæ‰ï¼Œå¸§ç‡é«˜è¾¾æ¯ç§’æ•°ä¸‡äº¿å¸§ï¼Œåºåˆ—æ·±åº¦è¾¾æ•°ç™¾ä¸ªã€‚ç„¶è€Œï¼Œç”±äºCUPä¾èµ–äºæ¡çº¹ç›¸æœºï¼Œç³»ç»Ÿæˆåƒä¿çœŸåº¦å—åˆ°æ¡çº¹ç›¸æœºä¸­ç”µè·è€¦åˆä¼ªå½±çš„å›ºæœ‰é™åˆ¶çš„å½±å“ã€‚å°½ç®¡å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•æ”¹å–„äº†æ¢å¤çš„åœºæ™¯ï¼Œä½†å…¶é«˜å‹ç¼©æ¯”ä»ç„¶ä¼šå½±å“å›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå…¨å…‰å­¦ç¦»æ•£ç…§æ˜å‹ç¼©è¶…é«˜é€Ÿæ‘„å½±ï¼ˆAOD-CUPï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è‡ªç”±ç©ºé—´è§’å•ï¿½ï¿½ãƒ¼ãƒ å¢å¼ºå»¶è¿Ÿï¼ˆFACEDï¼‰æŠ€æœ¯ï¼Œå¯¹é£ç§’è„‰å†²è¿›è¡Œæ—¶é—´æ‹‰ä¼¸ï¼Œå®ç°åŠ¨æ€åœºæ™¯çš„ç¦»æ•£ç…§æ˜ã€‚å‡­å€Ÿç‹¬ç‰¹ç³»ç»Ÿæ¶æ„ï¼ŒAOD-CUPå…·æœ‰å¯è°ƒå¸§æ•°å’Œçµæ´»çš„å¸§é—´é—´éš”ï¼ŒèŒƒå›´ä»çš®ç§’åˆ°çº³ç§’ï¼Œä»è€Œå®ç°é«˜ä¿çœŸè¶…é«˜é€Ÿå¿«ç…§æˆåƒã€‚å®éªŒç»“æœè¯æ˜äº†ç³»ç»Ÿå‡ºè‰²çš„åŠ¨æ€ç©ºé—´åˆ†è¾¨ç‡å’Œå¯è§†åŒ–å¤æ‚ç©ºé—´ç»†èŠ‚çš„è¶…é«˜ç°è±¡çš„èƒ½åŠ›ï¼Œå¦‚LiFæ™¶ä½“ä¸­çš„åº”åŠ›æ³¢ä¼ æ’­å’Œç©ºæ°”ç­‰ç¦»å­ä½“é€šé“çš„å½¢æˆã€‚è¿™äº›ç»“æœçªæ˜¾äº†AOD-CUPåœ¨é«˜ä¿çœŸã€å®æ—¶è¶…é«˜é€Ÿæˆåƒæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¶…é«˜é€Ÿç§‘å­¦é¢†åŸŸçš„å‘å±•æä¾›äº†å‰æ‰€æœªæœ‰çš„å·¥å…·ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>SUOIåœ¨æ•æ‰å¤æ‚ç¬æ€äº‹ä»¶æ–¹é¢è‡³å…³é‡è¦ï¼Œå¯¹ç§‘å­¦å’Œåº”ç”¨éƒ½æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>CUPä½œä¸ºSUOIçš„æ°å‡ºä»£è¡¨ï¼Œå…·æœ‰æé«˜çš„å¸§ç‡å’Œåºåˆ—æ·±åº¦ã€‚<br>3.æ¡çº¹ç›¸æœºå¸¦æ¥çš„ç”µè·è€¦åˆä¼ªå½±é™åˆ¶äº†CUPçš„æˆåƒè´¨é‡ã€‚</li>
<li>æå‡ºAOD-CUPæ–°æ–¹æ³•ï¼Œé‡‡ç”¨FACEDæŠ€æœ¯å®ç°ç¦»æ•£ç…§æ˜å’Œçµæ´»å¸§é—´é—´éš”ã€‚</li>
<li>AOD-CUPå…·æœ‰å¯è°ƒå¸§æ•°ï¼Œå¯å®ç°é«˜åŠ¨æ€ç©ºé—´åˆ†è¾¨ç‡çš„è¶…é«˜é€Ÿæˆåƒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜AOD-CUPåœ¨å¯è§†åŒ–å¤æ‚ç©ºé—´ç»†èŠ‚çš„è¶…é«˜ç°è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e1fdaa60f175bfcd8f46d23b0580512.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-241c9906a4c171fab5729b683b463558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aa75b1849b7d023d968654b6939c61d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cardiac-Digital-Twins-at-Scale-from-MRI-Open-Tools-and-Representative-Models-from-55000-UK-Biobank-Participants"><a href="#Cardiac-Digital-Twins-at-Scale-from-MRI-Open-Tools-and-Representative-Models-from-55000-UK-Biobank-Participants" class="headerlink" title="Cardiac Digital Twins at Scale from MRI: Open Tools and Representative   Models from ~55000 UK Biobank Participants"></a>Cardiac Digital Twins at Scale from MRI: Open Tools and Representative   Models from ~55000 UK Biobank Participants</h2><p><strong>Authors:Devran Ugurlu, Shuang Qian, Elliot Fairweather, Charlene Mauger, Bram Ruijsink, Laura Dal Toso, Yu Deng, Marina Strocchi, Reza Razavi, Alistair Young, Pablo Lamata, Steven Niederer, Martin Bishop</strong></p>
<p>A cardiac digital twin is a virtual replica of a patientâ€™s heart for screening, diagnosis, prognosis, risk assessment, and treatment planning of cardiovascular diseases. This requires an anatomically accurate patient-specific 3D structural representation of the heart, suitable for electro-mechanical simulations or study of disease mechanisms. However, generation of cardiac digital twins at scale is demanding and there are no public repositories of models across demographic groups. We describe an automatic open-source pipeline for creating patient-specific left and right ventricular meshes from cardiovascular magnetic resonance images, its application to a large cohort of ~55000 participants from UK Biobank, and the construction of the most comprehensive cohort of adult heart models to date, comprising 1423 representative meshes across sex (male, female), body mass index (range: 16 - 42 kg&#x2F;m$^2$) and age (range: 49 - 80 years). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025">https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025</a> , and pre-trained networks, representative volumetric meshes with fibers and UVCs will be made available soon. </p>
<blockquote>
<p>å¿ƒè„æ•°å­—å­ªç”Ÿä½“æ˜¯æŒ‡æ‚£è€…å¿ƒè„çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…çš„ç­›æŸ¥ã€è¯Šæ–­ã€é¢„åè¯„ä¼°ã€é£é™©è¯„ä¼°å’Œæ²»ç–—è®¡åˆ’ã€‚è¿™éœ€è¦æ‚£è€…å¿ƒè„çš„å…·ä½“è§£å‰–ç»“æ„çš„ä¸‰ç»´ç»“æ„è¡¨ç¤ºï¼Œé€‚ç”¨äºæœºç”µæ¨¡æ‹Ÿæˆ–ç–¾ç—…æœºç†ç ”ç©¶ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡ç”Ÿæˆå¿ƒè„æ•°å­—å­ªç”Ÿä½“æ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå¹¶ä¸”ç›®å‰æ²¡æœ‰è·¨äººå£ç¾¤ä½“çš„æ¨¡å‹å…¬å…±ä»“åº“ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ä¸ªè‡ªåŠ¨å¼€æºç®¡é“ï¼Œç”¨äºä»å¿ƒè¡€ç®¡ç£å…±æŒ¯å›¾åƒåˆ›å»ºæ‚£è€…ç‰¹å®šçš„å·¦å¿ƒå®¤å’Œå³å¿ƒå®¤ç½‘æ ¼ï¼Œå°†å…¶åº”ç”¨äºæ¥è‡ªè‹±å›½ç”Ÿç‰©é“¶è¡Œçš„çº¦55000åå‚ä¸è€…çš„ç ”ç©¶ï¼Œå¹¶æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„æˆäººå¿ƒè„æ¨¡å‹é˜Ÿåˆ—ï¼ŒåŒ…æ‹¬æŒ‰æ€§åˆ«ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰ã€ä½“é‡æŒ‡æ•°ï¼ˆèŒƒå›´ï¼š16-42 kg&#x2F;mÂ²ï¼‰å’Œå¹´é¾„ï¼ˆèŒƒå›´ï¼š49-80å²ï¼‰åˆ’åˆ†çš„ä»£è¡¨æ€§ç½‘æ ¼å…±1423ä¸ªã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025">https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025</a>ï¼Œå¹¶ä¸”é¢„è®­ç»ƒç½‘ç»œå’Œå…·æœ‰ä»£è¡¨æ€§çš„ä½“ç§¯ç½‘æ ¼ï¼ˆå¸¦æœ‰çº¤ç»´å’Œç´«å¤–çº¿æ€èŒå‰‚ï¼‰å°†å¾ˆå¿«æä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€é¡¹å…³äºå¿ƒè„æ•°å­—åŒèƒèƒçš„ç ”ç©¶ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨å¿ƒè¡€ç®¡ç£å…±æŒ¯å›¾åƒåˆ›å»ºæ‚£è€…ç‰¹å®šçš„å·¦å¿ƒå®¤å’Œå³å¿ƒå®¤ç½‘æ ¼ï¼Œå¹¶åº”ç”¨äºå¤§è§„æ¨¡çš„UK Biobankå‚ä¸è€…ç¾¤ä½“ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„æˆäººå¿ƒè„æ¨¡å‹ã€‚è¯¥ç ”ç©¶æä¾›äº†è‡ªåŠ¨å¼€æºç®¡é“ï¼Œå¹¶åˆ›å»ºäº†åŒ…å«æ€§åˆ«ã€ä½“é‡æŒ‡æ•°å’Œå¹´é¾„ç­‰è·¨äººå£ç‰¹å¾çš„æ¨¡å‹åº“ã€‚è¿™äº›æ¨¡å‹å¯ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…ç­›æŸ¥ã€è¯Šæ–­ã€é¢„åè¯„ä¼°å’Œæ²»ç–—è®¡åˆ’ã€‚å…¶ä»£ç å’Œèµ„æºåº“å…¬å¼€ï¼Œæ–¹ä¾¿ç ”ç©¶è€…è·å–å’Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„æ•°å­—åŒèƒèƒæ˜¯ä¸€ç§è™šæ‹Ÿæ‚£è€…å¿ƒè„å¤åˆ¶å“ï¼Œä¸»è¦ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…çš„ç­›æŸ¥ã€è¯Šæ–­ã€é¢„åè¯„ä¼°å’Œæ²»ç–—è®¡åˆ’ã€‚</li>
<li>éœ€è¦è§£å‰–å­¦å‡†ç¡®çš„æ‚£è€…ç‰¹å®šå¿ƒè„ä¸‰ç»´ç»“æ„è¡¨ç¤ºï¼Œé€‚ç”¨äºæœºç”µæ¨¡æ‹Ÿæˆ–ç–¾ç—…æœºç†ç ”ç©¶ã€‚</li>
<li>è‡ªåŠ¨å¼€æºç®¡é“è¢«å¼€å‘å‡ºæ¥ï¼Œç”¨äºä»å¿ƒè¡€ç®¡ç£å…±æŒ¯å›¾åƒåˆ›å»ºæ‚£è€…ç‰¹å®šçš„å·¦å¿ƒå®¤å’Œå³å¿ƒå®¤ç½‘æ ¼ã€‚</li>
<li>è¯¥ç®¡é“æˆåŠŸåº”ç”¨äºå¤§è§„æ¨¡çš„UK Biobankå‚ä¸è€…ç¾¤ä½“ï¼Œåˆ›å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„æˆäººå¿ƒè„æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è€ƒè™‘äº†æ€§åˆ«ã€ä½“é‡æŒ‡æ•°å’Œå¹´é¾„ç­‰å¤šç§å› ç´ ï¼Œä½¿å…¶æˆä¸ºè·¨äººå£ç‰¹å¾çš„æ¨¡å‹åº“ã€‚è¿™å¯¹äºå„ç§ç±»å‹çš„å¿ƒè¡€ç®¡ç–¾ç—…ç ”ç©¶å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a85fe368173805908404b3518d0a5e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88ee780562c3087f39afc38d9b0ba240.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99c918af5d4b45da5ccfb675234d481e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TumorHoPe2-An-updated-database-for-Tumor-Homing-Peptides"><a href="#TumorHoPe2-An-updated-database-for-Tumor-Homing-Peptides" class="headerlink" title="TumorHoPe2: An updated database for Tumor Homing Peptides"></a>TumorHoPe2: An updated database for Tumor Homing Peptides</h2><p><strong>Authors:Diksha Kashyap, Devanshi Gupta, Naman Kumar Mehta, Gajendra P. S. Raghava</strong></p>
<p>Addressing the growing need for organized data on tumor homing peptides (THPs), we present TumorHoPe2, a manually curated database offering extensive details on experimentally validated THPs. This represents a significant update to TumorHoPe, originally developed by our group in 2012. TumorHoPe2 now contains 1847 entries, representing 1297 unique tumor homing peptides, a substantial expansion from the 744 entries in its predecessor. For each peptide, the database provides critical information, including sequence, terminal or chemical modifications, corresponding cancer cell lines, and specific tumor types targeted. The database compiles data from two primary sources: phage display libraries, which are commonly used to identify peptide ligands targeting tumor-specific markers, and synthetic peptides, which are chemically modified to enhance properties such as stability, binding affinity, and specificity. Our dataset includes 594 chemically modified peptides, with 255 having N-terminal and 195 C-terminal modifications. These THPs have been validated against 172 cancer cell lines and demonstrate specificity for 37 distinct tumor types. To maximize utility for the research community, TumorHoPe2 is equipped with intuitive tools for data searching, filtering, and analysis, alongside a RESTful API for efficient programmatic access and integration into bioinformatics pipelines. It is freely available at <a target="_blank" rel="noopener" href="https://webs.iiitd.edu.in/raghava/tumorhope2/">https://webs.iiitd.edu.in/raghava/tumorhope2/</a> </p>
<blockquote>
<p>ä¸ºäº†åº”å¯¹è‚¿ç˜¤å½’å·¢è‚½ï¼ˆTHPsï¼‰æœ‰åºæ•°æ®ä¸æ–­å¢é•¿çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TumorHoPe2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨æ•´ç†æ•°æ®åº“ï¼Œæä¾›äº†ç»è¿‡å®éªŒéªŒè¯çš„THPsçš„è¯¦ç»†ä¿¡æ¯ã€‚è¿™æ˜¯å¯¹2012å¹´ç”±æˆ‘ä»¬å›¢é˜Ÿå¼€å‘çš„TumorHoPeçš„é‡å¤§æ›´æ–°ã€‚TumorHoPe2ç›®å‰åŒ…å«1847ä¸ªæ¡ç›®ï¼Œä»£è¡¨æœ‰1297ä¸ªç‹¬ç‰¹çš„è‚¿ç˜¤å½’å·¢è‚½ï¼Œè¾ƒå‰ä¸€ä»£çš„744ä¸ªæ¡ç›®æœ‰äº†å®è´¨æ€§çš„æ‰©å±•ã€‚å¯¹äºæ¯ä¸ªè‚½ï¼Œæ•°æ®åº“æä¾›äº†å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬åºåˆ—ã€æœ«ç«¯æˆ–åŒ–å­¦ä¿®é¥°ã€ç›¸åº”çš„ç™Œç»†èƒæ ªä»¥åŠç‰¹å®šçš„é¶å‘è‚¿ç˜¤ç±»å‹ã€‚æ•°æ®åº“æ•´ç†äº†ä¸¤ç§ä¸»è¦æ¥æºçš„æ•°æ®ï¼šå¸¸ç”¨äºè¯†åˆ«é’ˆå¯¹è‚¿ç˜¤ç‰¹å¼‚æ€§æ ‡è®°ç‰©çš„è‚½é…ä½“çš„å™¬èŒä½“å±•ç¤ºåº“ï¼Œä»¥åŠé€šè¿‡åŒ–å­¦ä¿®é¥°å¢å¼ºç¨³å®šæ€§ã€ç»“åˆäº²å’ŒåŠ›å’Œç‰¹å¼‚æ€§çš„åˆæˆè‚½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«594ä¸ªåŒ–å­¦ä¿®é¥°çš„è‚½ï¼Œå…¶ä¸­255ä¸ªå…·æœ‰Nç«¯ä¿®é¥°å’Œ195ä¸ªå…·æœ‰Cç«¯ä¿®é¥°ã€‚è¿™äº›THPså·²ç»é€šè¿‡é’ˆå¯¹172ç§ç™Œç»†èƒæ ªçš„éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºå¯¹37ç§ä¸åŒè‚¿ç˜¤ç±»å‹çš„ç‰¹å¼‚æ€§ã€‚ä¸ºäº†æœ€å¤§åŒ–å¯¹ç ”ç©¶ç¾¤ä½“çš„æ•ˆç”¨ï¼ŒTumorHoPe2é…å¤‡äº†ç”¨äºæ•°æ®æœç´¢ã€è¿‡æ»¤å’Œåˆ†æçš„ç›´è§‚å·¥å…·ï¼Œä»¥åŠç”¨äºé«˜æ•ˆç¼–ç¨‹è®¿é—®å’Œæ•´åˆåˆ°ç”Ÿç‰©ä¿¡æ¯å­¦æµç¨‹çš„RESTful APIã€‚å®ƒå¯åœ¨<a target="_blank" rel="noopener" href="https://webs.iiitd.edu.in/raghava/tumorhope2/%E5%85%8D%E8%B4%B9%E8%AE%BF%E9%97%AE%E3%80%82">https://webs.iiitd.edu.in/raghava/tumorhope2/å…è´¹è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20913v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‚¿ç˜¤HoPeæ•°æ®åº“çš„æ–°ç‰ˆæœ¬â€”â€”TumorHoPe2ç°å·²æ¨å‡ºï¼ŒåŒ…å«ç»è¿‡å®éªŒéªŒè¯çš„è‚¿ç˜¤å½’å·¢è‚½ï¼ˆTHPï¼‰çš„è¯¦ç»†ä¿¡æ¯ã€‚æ­¤æ•°æ®åº“åŒ…å«å¤§é‡å…³äºè‚¿ç˜¤å½’å·¢è‚½çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬åºåˆ—ã€ç»ˆç«¯æˆ–åŒ–å­¦ä¿®é¥°ã€ç›¸åº”çš„ç™Œç»†èƒæ ªä»¥åŠç‰¹å®šè‚¿ç˜¤ç±»å‹ç­‰ã€‚ç›¸æ¯”ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œæ–°æ•°æ®åº“æ‰©å……äº†å†…å®¹ï¼Œå¹¶æä¾›äº†æ›´ç›´è§‚çš„å·¥å…·è¿›è¡Œæ•°æ®æœç´¢ã€ç­›é€‰å’Œåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TumorHoPe2æ˜¯TumorHoPeæ•°æ®åº“çš„å‡çº§ç‰ˆï¼ŒåŒ…å«æ›´å¤šå…³äºè‚¿ç˜¤å½’å·¢è‚½ï¼ˆTHPï¼‰çš„ä¿¡æ¯ã€‚</li>
<li>æ•°æ®åº“åŒ…å«1847ä¸ªæ¡ç›®ï¼Œä»£è¡¨1297ä¸ªç‹¬ç‰¹çš„è‚¿ç˜¤å½’å·¢è‚½ã€‚</li>
<li>æ•°æ®åº“æ•°æ®ä¸»è¦æ¥è‡ªå™¬èŒä½“å±•ç¤ºåº“å’Œåˆæˆè‚½ä¸¤ç§æ¥æºã€‚</li>
<li>æ•°æ®åº“ä¸­åŒ…å«594ä¸ªç»è¿‡åŒ–å­¦ä¿®é¥°çš„è‚½ï¼Œå…¶ä¸­255ä¸ªå…·æœ‰Næœ«ç«¯å’Œ195ä¸ªå…·æœ‰Cæœ«ç«¯ä¿®é¥°ã€‚</li>
<li>è¿™äº›THPså·²é’ˆå¯¹172ç§ç™Œç»†èƒæ ªè¿›è¡ŒéªŒè¯ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¹37ç§ä¸åŒè‚¿ç˜¤çš„ç‰¹å¼‚æ€§ã€‚</li>
<li>TumorHoPe2æä¾›äº†ç›´è§‚çš„å·¥å…·è¿›è¡Œæ•°æ®æœç´¢ã€ç­›é€‰å’Œåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1abe6c653423c465a0a7ce536b099995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88e8873d95b4611af68deb99a08e47db.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Highly-Enhanced-robust-room-temperature-ferromagnetism-in-CVD-grown-nano-dimensional-MoS2-flakes-by-modifying-edges-and-defect-engineering"><a href="#Highly-Enhanced-robust-room-temperature-ferromagnetism-in-CVD-grown-nano-dimensional-MoS2-flakes-by-modifying-edges-and-defect-engineering" class="headerlink" title="Highly Enhanced robust room temperature ferromagnetism in CVD-grown   nano-dimensional MoS2 flakes by modifying edges and defect engineering"></a>Highly Enhanced robust room temperature ferromagnetism in CVD-grown   nano-dimensional MoS2 flakes by modifying edges and defect engineering</h2><p><strong>Authors:Sharmistha Dey, Nahid Chaudhary, Ulrich Kentsch, Rajendra Singh, Pankaj Srivastava, Santanu Ghosh</strong></p>
<p>The alterations in the magnetic properties and electronic structure of chemical vapor deposition (CVD) grown nano-dimensional molybdenum disulfide (MoS2) after low energy ion irradiation are thoroughly investigated. The formation of pure hexagonal 2-H phase has been identified by Raman spectroscopy and X-ray diffraction (XRD). The pristine samples are irradiated by Argon (Ar) ions with low energy at different fluences. A comprehensive analysis of Raman spectroscopy data manifests the formation of lattice defects like S-vacancies across the samples after irradiation. Triangular-flake formation in the pristine sample is confirmed by field emission scanning electron microscopy (FESEM) images. After increasing irradiation fluences the big flakes commenced to fragment into smaller ones enhancing the number of edge-terminated structures. The electron probe microanalyzer (EPMA) analysis verifies the absence of any magnetic impurity. Rutherford backscattering spectrometry (RBS) and X-ray photoelectron spectroscopy (XPS) study confirm the formation of S-vacancies after irradiation. The pristine sample exhibits diamagnetic behavior at room temperature. The saturation magnetization value increases with increasing the ion irradiation fluences, and the sample irradiated with 1e15 ions&#x2F;cm2 demonstrates the highest magnetization value of 4.18 emu&#x2F;g. The impact of edge-terminated structure and point defects like S-vacancies to induce room-temperature ferromagnetism (RTFM) is thoroughly examined. </p>
<blockquote>
<p>é‡‡ç”¨åŒ–å­¦æ°”ç›¸æ²‰ç§¯ï¼ˆCVDï¼‰ç”Ÿé•¿çš„çº³ç±³çº§äºŒç¡«åŒ–é’¼ï¼ˆMoS2ï¼‰åœ¨ä½èƒ½é‡ç¦»å­è¾å°„åçš„ç£æ€§å’Œç”µå­ç»“æ„å˜åŒ–å¾—åˆ°äº†æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡æ‹‰æ›¼å…‰è°±å’ŒXå°„çº¿è¡å°„ï¼ˆXRDï¼‰ç¡®å®šäº†çº¯å…­è§’å½¢2-Hç›¸çš„å½¢æˆã€‚åŸå§‹æ ·å“å—åˆ°ä¸åŒæµå¼ºçš„æ°©ï¼ˆArï¼‰ç¦»å­ä½èƒ½é‡è¾å°„ã€‚æ‹‰æ›¼å…‰è°±æ•°æ®çš„ç»¼åˆåˆ†æè¡¨æ˜ï¼Œè¾å°„ååœ¨æ ·å“ä¸­å½¢æˆäº†æ™¶æ ¼ç¼ºé™·ï¼Œå¦‚ç¡«ç©ºä½ã€‚åœºå‘å°„æ‰«æç”µå­æ˜¾å¾®é•œï¼ˆFESEMï¼‰å›¾åƒè¯å®äº†åŸå§‹æ ·å“ä¸­ä¸‰è§’å½¢ç‰‡çŠ¶ç»“æ„çš„å½¢æˆã€‚éšç€è¾å°„æµå¼ºçš„å¢åŠ ï¼Œå¤§ç‰‡çŠ¶ç»“æ„å¼€å§‹ç¢è£‚æˆè¾ƒå°çš„ç‰‡çŠ¶ç»“æ„ï¼Œå¢åŠ äº†è¾¹ç¼˜ç»ˆæ­¢ç»“æ„çš„æ•°é‡ã€‚ç”µå­æ¢é’ˆå¾®åˆ†æä»ªï¼ˆEPMAï¼‰åˆ†æéªŒè¯äº†æ— ç£æ€§æ‚è´¨çš„å­˜åœ¨ã€‚å¢ç‘Ÿç¦èƒŒæ•£å°„å…‰è°±ä»ªï¼ˆRBSï¼‰å’ŒXå°„çº¿å…‰ç”µå­å…‰è°±ä»ªï¼ˆXPSï¼‰çš„ç ”ç©¶è¯å®äº†ç¡«ç©ºä½åœ¨è¾å°„åçš„å½¢æˆã€‚åŸå§‹æ ·å“åœ¨å®¤æ¸©ä¸‹è¡¨ç°å‡ºæŠ—ç£æ€§è¡Œä¸ºã€‚éšç€ç¦»å­è¾å°„æµå¼ºçš„å¢åŠ ï¼Œé¥±å’Œç£åŒ–å¼ºåº¦å€¼å¢åŠ ï¼Œè¾ç…§ç¦»å­æ•°ä¸º1e15ç¦»å­&#x2F;cm2çš„æ ·å“è¡¨ç°å‡ºæœ€é«˜çš„ç£åŒ–å¼ºåº¦å€¼ä¸º4.18emu&#x2F;gã€‚è¾¹ç¼˜ç»ˆæ­¢ç»“æ„å’Œç¡«ç©ºä½ç­‰ç‚¹ç¼ºé™·å¯¹è¯±å¯¼å®¤æ¸©é“ç£æ€§ï¼ˆRTFMï¼‰çš„å½±å“å¾—åˆ°äº†å½»åº•æ£€æŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20695v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ–å­¦æ°”ç›¸æ²‰ç§¯æ³•åˆ¶å¤‡çš„çº³ç±³çº§äºŒç¡«åŒ–é’¼åœ¨å—åˆ°ä½èƒ½é‡ç¦»å­è¾å°„åï¼Œå…¶ç£æ€§å’Œç”µå­ç»“æ„å‘ç”Ÿå˜åŒ–ã€‚ç ”ç©¶é€šè¿‡æ‹‰æ›¼å…‰è°±å’ŒXå°„çº¿è¡å°„ç¡®è®¤å½¢æˆäº†çº¯å‡€çš„å…­è§’å½¢2-Hç›¸ï¼Œå¹¶å‘ç°ç¦»å­è¾å°„åäº§ç”Ÿäº†ç¡«ç©ºä½ç­‰æ™¶æ ¼ç¼ºé™·ã€‚éšç€è¾å°„å‰‚é‡çš„å¢åŠ ï¼Œå¤§æ™¶ç‰‡å¼€å§‹ç¢è£‚æˆæ›´å°çš„æ™¶ç‰‡ï¼Œå¢åŠ äº†è¾¹ç¼˜ç»ˆæ­¢ç»“æ„æ•°é‡ã€‚ç”µå­æ¢é’ˆå¾®åˆ†æä»ªåˆ†æç¡®è®¤æ— ç£æ€§æ‚è´¨ã€‚ç¡«ç©ºä½å½¢æˆçš„åŸå› é€šè¿‡å¢ç‘Ÿç¦èƒŒæ•£å°„å…‰è°±å’ŒXå°„çº¿å…‰ç”µå­å…‰è°±ç ”ç©¶å¾—åˆ°è¯å®ã€‚æ ·å“çš„é¥±å’Œç£åŒ–å¼ºåº¦éšç€ç¦»å­è¾å°„å‰‚é‡çš„å¢åŠ è€Œå¢åŠ ï¼Œå—åˆ°ç¦»å­è¾å°„åçš„æ ·å“æ˜¾ç¤ºå‡ºæœ€é«˜çš„ç£åŒ–å¼ºåº¦ä¸ºæ¯å…‹æ ·å“äº§ç”Ÿ4.18emuçš„ç£åŒ–å¼ºåº¦ã€‚æ–‡ä¸­è¯¦ç»†ç ”ç©¶äº†è¾¹ç¼˜ç»ˆæ­¢ç»“æ„å’Œç¡«ç©ºä½ç­‰ç‚¹ç¼ºé™·å¯¹å®¤æ¸©é“ç£æ€§çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CVDç”Ÿé•¿çš„çº³ç±³çº§äºŒç¡«åŒ–é’¼åœ¨ç»è¿‡ä½èƒ½é‡ç¦»å­è¾å°„åï¼Œç£æ€§å’Œç”µå­ç»“æ„å‘ç”Ÿå˜åŒ–ã€‚</li>
<li>ç¡®è®¤äº†çº¯å‡€çš„å…­è§’å½¢2-Hç›¸çš„å½¢æˆã€‚</li>
<li>æ‹‰æ›¼å…‰è°±åˆ†æè¡¨æ˜ç¦»å­è¾å°„åäº§ç”Ÿäº†ç¡«ç©ºä½ç­‰æ™¶æ ¼ç¼ºé™·ã€‚</li>
<li>éšç€è¾å°„å‰‚é‡çš„å¢åŠ ï¼Œå¤§æ™¶ç‰‡ç¢è£‚æˆæ›´å°çš„æ™¶ç‰‡ï¼Œè¾¹ç¼˜ç»ˆæ­¢ç»“æ„æ•°é‡å¢åŠ ã€‚</li>
<li>æ— ç£æ€§æ‚è´¨çš„å­˜åœ¨å¾—åˆ°ç”µå­æ¢é’ˆå¾®åˆ†æä»ªçš„éªŒè¯ã€‚</li>
<li>ç ”ç©¶ç¡®è®¤äº†ç¡«ç©ºä½åœ¨ç¦»å­è¾å°„åçš„å½¢æˆæœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ee3e96980d92dc881264fb0adfb4c9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53ee72e46cafe0d2463d391385e75682.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MetaWriter-Personalized-Handwritten-Text-Recognition-Using-Meta-Learned-Prompt-Tuning"><a href="#MetaWriter-Personalized-Handwritten-Text-Recognition-Using-Meta-Learned-Prompt-Tuning" class="headerlink" title="MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned   Prompt Tuning"></a>MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned   Prompt Tuning</h2><p><strong>Authors:Wenhao Gu, Li Gu, Ching Yee Suen, Yang Wang</strong></p>
<p>Recent advancements in handwritten text recognition (HTR) have enabled the effective conversion of handwritten text to digital formats. However, achieving robust recognition across diverse writing styles remains challenging. Traditional HTR methods lack writer-specific personalization at test time due to limitations in model architecture and training strategies. Existing attempts to bridge this gap, through gradient-based meta-learning, still require labeled examples and suffer from parameter-inefficient fine-tuning, leading to substantial computational and memory overhead. To overcome these challenges, we propose an efficient framework that formulates personalization as prompt tuning, incorporating an auxiliary image reconstruction task with a self-supervised loss to guide prompt adaptation with unlabeled test-time examples. To ensure self-supervised loss effectively minimizes text recognition error, we leverage meta-learning to learn the optimal initialization of the prompts. As a result, our method allows the model to efficiently capture unique writing styles by updating less than 1% of its parameters and eliminating the need for time-intensive annotation processes. We validate our approach on the RIMES and IAM Handwriting Database benchmarks, where it consistently outperforms previous state-of-the-art methods while using 20x fewer parameters. We believe this represents a significant advancement in personalized handwritten text recognition, paving the way for more reliable and practical deployment in resource-constrained scenarios. </p>
<blockquote>
<p>æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†æ‰‹å†™æ–‡æœ¬å‘æ•°å­—æ ¼å¼çš„æœ‰æ•ˆè½¬æ¢ã€‚ç„¶è€Œï¼Œå®ç°åœ¨å„ç§ä¹¦å†™é£æ ¼ä¸­çš„ç¨³å¥è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»ŸHTRæ–¹æ³•ç”±äºåœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä¸Šçš„å±€é™æ€§ï¼Œåœ¨æµ‹è¯•æ—¶ç¼ºä¹é’ˆå¯¹ç‰¹å®šä½œè€…çš„ä¸ªæ€§åŒ–ã€‚ç°æœ‰çš„ä¸€äº›å°è¯•é€šè¿‡æ¢¯åº¦åŸºç¡€çš„å…ƒå­¦ä¹ ï¼ˆMeta-Learningï¼‰æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½†ä¾æ—§éœ€è¦æ ‡æ³¨æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨ç²¾ç»†è°ƒæ•´å‚æ•°æ—¶æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—ä¸å†…å­˜è´Ÿæ‹…å·¨å¤§ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œå°†ä¸ªæ€§åŒ–è¡¨è¿°ä¸ºæç¤ºè°ƒæ•´ï¼ˆPrompt Tuningï¼‰ï¼Œå¹¶ç»“åˆä¸€ä¸ªè¾…åŠ©å›¾åƒé‡å»ºä»»åŠ¡ä¸è‡ªæˆ‘ç›‘ç£æŸå¤±æ¥æŒ‡å¯¼æ— æ ‡ç­¾æµ‹è¯•æ ·æœ¬çš„æç¤ºé€‚åº”ã€‚ä¸ºäº†ç¡®ä¿è‡ªæˆ‘ç›‘ç£æŸå¤±èƒ½æœ‰æ•ˆåœ°æœ€å°åŒ–æ–‡æœ¬è¯†åˆ«é”™è¯¯ï¼Œæˆ‘ä»¬åˆ©ç”¨å…ƒå­¦ä¹ æ¥å­¦ä¹ æç¤ºçš„æœ€ä¼˜åˆå§‹åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ¨¡å‹é€šè¿‡æ›´æ–°ä¸åˆ°1%çš„å‚æ•°æ¥æœ‰æ•ˆåœ°æ•æ‰ç‹¬ç‰¹çš„ä¹¦å†™é£æ ¼ï¼Œå¹¶ä¸”æ— éœ€è€—æ—¶çš„äººå·¥æ ‡æ³¨è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨RIMESå’ŒIAMæ‰‹å†™æ•°æ®åº“åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒå§‹ç»ˆä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°å°‘äº†20å€ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä»£è¡¨äº†ä¸ªæ€§åŒ–æ‰‹å†™æ–‡æœ¬è¯†åˆ«çš„é‡å¤§è¿›å±•ï¼Œä¸ºèµ„æºå—é™çš„åœºæ™¯ä¸­æ›´å¯é ã€æ›´å®é™…çš„éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20513v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•å®ç°äº†æ‰‹å†™æ–‡æœ¬å‘æ•°å­—æ ¼å¼çš„æœ‰æ•ˆè½¬æ¢ï¼Œä½†å®ç°è·¨ä¸åŒä¹¦å†™é£æ ¼çš„ç¨³å¥è¯†åˆ«ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹ä¼ ç»ŸHTRæ–¹æ³•ç¼ºä¹æµ‹è¯•æ—¶çš„ä¸ªæ€§åŒ–ä¸”å­˜åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„å±€é™æ€§ï¼Œä»¥åŠç°æœ‰å°è¯•è§£å†³æ­¤é—®é¢˜çš„æ¢¯åº¦å…ƒå­¦ä¹ æ–¹æ³•éœ€è¦æ ‡æ³¨ç¤ºä¾‹å’Œå‚æ•°è°ƒæ•´ä¸å¤Ÿé«˜æ•ˆçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¸ªæ€§åŒ–è¡¨è¿°ä¸ºæç¤ºè°ƒæ•´ï¼Œå¹¶ç»“åˆè¾…åŠ©å›¾åƒé‡å»ºä»»åŠ¡å’Œè‡ªç›‘ç£æŸå¤±æ¥å¼•å¯¼æ— æ ‡ç­¾æµ‹è¯•ä¾‹çš„æç¤ºé€‚åº”ã€‚ä¸ºç¡®ä¿è‡ªç›‘ç£æŸå¤±æœ€å°åŒ–æ–‡æœ¬è¯†åˆ«è¯¯å·®ï¼Œæœ¬æ–‡åˆ©ç”¨å…ƒå­¦ä¹ æ¥å­¦ä¹ æç¤ºçš„æœ€ä½³åˆå§‹åŒ–ã€‚è¯¥æ–¹æ³•å…è®¸æ¨¡å‹é€šè¿‡æ›´æ–°ä¸åˆ°1%çš„å‚æ•°æ¥é«˜æ•ˆæ•æ‰ç‹¬ç‰¹çš„ä¹¦å†™é£æ ¼ï¼Œå¹¶çœå»è€—æ—¶æ ‡æ³¨è¿‡ç¨‹ã€‚åœ¨RIMESå’ŒIAMæ‰‹å†™æ•°æ®åº“åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨å‚æ•°æ›´å°‘20å€ã€‚è¿™æ ‡å¿—ç€ä¸ªæ€§åŒ–æ‰‹å†™æ–‡æœ¬è¯†åˆ«çš„é‡å¤§è¿›å±•ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸­æ›´å¯é ã€æ›´å®ç”¨çš„éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰æŠ€æœ¯çš„è¿›å±•å®ç°äº†æ‰‹å†™æ–‡æœ¬åˆ°æ•°å­—æ ¼å¼çš„è½¬æ¢ï¼Œä½†è¯†åˆ«ä¸åŒä¹¦å†™é£æ ¼çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚</li>
<li>ä¼ ç»ŸHTRæ–¹æ³•å› æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„å±€é™æ€§ï¼Œç¼ºä¹æµ‹è¯•æ—¶çš„ä¸ªæ€§åŒ–ã€‚</li>
<li>ç°æœ‰å°è¯•é€šè¿‡æ¢¯åº¦å…ƒå­¦ä¹ å¼¥è¡¥è¿™ä¸€å·®è·çš„æ–¹æ³•ä»éœ€è¦æ ‡æ³¨ç¤ºä¾‹ï¼Œä¸”å‚æ•°è°ƒæ•´ä¸å¤Ÿé«˜æ•ˆã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å°†ä¸ªæ€§åŒ–è¡¨è¿°ä¸ºæç¤ºè°ƒæ•´ï¼Œç»“åˆè¾…åŠ©å›¾åƒé‡å»ºä»»åŠ¡å’Œè‡ªç›‘ç£æŸå¤±å¼•å¯¼æ— æ ‡ç­¾æµ‹è¯•ä¾‹çš„æç¤ºé€‚åº”ã€‚</li>
<li>åˆ©ç”¨è‡ªç›‘ç£æŸå¤±å’Œå…ƒå­¦ä¹ æ¥ä¼˜åŒ–æç¤ºçš„åˆå§‹åŒ–å’Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½é«˜æ•ˆæ•æ‰ç‹¬ç‰¹ä¹¦å†™é£æ ¼ï¼Œæ›´æ–°å‚æ•°ä¸åˆ°1%ï¼Œå¹¶å…å»è€—æ—¶æ ‡æ³¨è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-142d01941fef69c6e840e7456f7e9ab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-671be78fb0d25d58a689ab490794b8b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b189d0e0b58cf392e480855397e0b24d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b37902bd61ac95a42e4fcefe8cea8c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09331fd4a8127ca1b13930490dba794.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologistsâ€™-Diagnostic-Logic"><a href="#CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologistsâ€™-Diagnostic-Logic" class="headerlink" title="CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologistsâ€™ Diagnostic   Logic"></a>CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologistsâ€™ Diagnostic   Logic</h2><p><strong>Authors:Yuxuan Sun, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Bowen Ding, Tao Lin, Lin Yang</strong></p>
<p>Recent advances in computational pathology have led to the emergence of numerous foundation models. However, these approaches fail to replicate the diagnostic process of pathologists, as they either simply rely on general-purpose encoders with multi-instance learning for classification or directly apply multimodal models to generate reports from images. A significant limitation is their inability to emulate the diagnostic logic employed by pathologists, who systematically examine slides at low magnification for overview before progressively zooming in on suspicious regions to formulate comprehensive diagnoses. To address this gap, we introduce CPathAgent, an innovative agent-based model that mimics pathologistsâ€™ reasoning processes by autonomously executing zoom-in&#x2F;out and navigation operations across pathology images based on observed visual features. To achieve this, we develop a multi-stage training strategy unifying patch-level, region-level, and whole-slide capabilities within a single model, which is essential for mimicking pathologists, who require understanding and reasoning capabilities across all three scales. This approach generates substantially more detailed and interpretable diagnostic reports compared to existing methods, particularly for huge region understanding. Additionally, we construct an expert-validated PathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical intermediate scale between patches and whole slides, as diagnosticians typically examine several key regions rather than entire slides at once. Extensive experiments demonstrate that CPathAgent consistently outperforms existing approaches across three scales of benchmarks, validating the effectiveness of our agent-based diagnostic approach and highlighting a promising direction for the future development of computational pathology. </p>
<blockquote>
<p>è¿‘æœŸè®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„è¿›å±•å‚¬ç”Ÿäº†è®¸å¤šåŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ³•å¤åˆ¶ç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­è¿‡ç¨‹ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆç®€å•åœ°ä¾èµ–äºå…·æœ‰å¤šå®ä¾‹å­¦ä¹ åŠŸèƒ½çš„ä¸€èˆ¬ç¼–ç å™¨è¿›è¡Œåˆ†ç±»ï¼Œè¦ä¹ˆç›´æ¥å°†å¤šæ¨¡å¼æ¨¡å‹åº”ç”¨äºå›¾åƒç”ŸæˆæŠ¥å‘Šã€‚ä¸€ä¸ªæ˜¾è‘—çš„å±€é™æ€§åœ¨äºï¼Œå®ƒä»¬æ— æ³•æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­é€»è¾‘ï¼Œå³ç³»ç»Ÿæ€§åœ°åœ¨ä½å€é•œä¸‹è§‚å¯Ÿåˆ‡ç‰‡ä»¥è·å–æ¦‚è§ˆï¼Œç„¶åé€æ­¥æ”¾å¤§å¯ç–‘åŒºåŸŸä»¥åšå‡ºå…¨é¢çš„è¯Šæ–­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CPathAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåˆ›æ–°çš„æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé€šè¿‡è‡ªä¸»æ‰§è¡Œæ”¾å¤§&#x2F;ç¼©å°å’Œå¯¼èˆªæ“ä½œæ¥æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„æ¨ç†è¿‡ç¨‹ï¼Œåœ¨ç—…ç†å›¾åƒä¸ŠåŸºäºè§‚å¯Ÿåˆ°çš„è§†è§‰ç‰¹å¾è¿›è¡Œè·¨å°ºåº¦æ“ä½œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€äº†è¡¥ä¸çº§åˆ«ã€åŒºåŸŸçº§åˆ«å’Œæ•´ä¸ªå¹»ç¯ç‰‡çº§åˆ«çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºæ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿæ¥è¯´è‡³å…³é‡è¦ï¼Œä»–ä»¬éœ€è¦åœ¨è¿™ä¸‰ä¸ªå°ºåº¦ä¸Šå…·å¤‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„è¯Šæ–­æŠ¥å‘Šæ¯”ç°æœ‰æ–¹æ³•æ›´ä¸ºè¯¦ç»†å’Œå¯è§£é‡Šï¼Œç‰¹åˆ«æ˜¯å¯¹äºå·¨å¤§åŒºåŸŸçš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç»è¿‡ä¸“å®¶éªŒè¯çš„PathMMU-HR$^{2}$ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å·¨å¤§åŒºåŸŸåˆ†æçš„åŸºå‡†æµ‹è¯•ï¼Œæ˜¯è¡¥ä¸å’Œæ•´ä¸ªå¹»ç¯ç‰‡ä¹‹é—´çš„ä¸€ä¸ªé‡è¦ä¸­é—´å°ºåº¦ï¼Œå› ä¸ºè¯Šæ–­é€šå¸¸éœ€è¦å¯¹å‡ ä¸ªå…³é”®åŒºåŸŸè€Œéæ•´ä¸ªå¹»ç¯ç‰‡è¿›è¡Œæ£€æŸ¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCPathAgentåœ¨ä¸‰ä¸ªå°ºåº¦çš„åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„åŸºäºæ™ºèƒ½ä½“çš„è¯Šæ–­æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºäº†è®¡ç®—ç—…ç†å­¦æœªæ¥å‘å±•çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20510v1">PDF</a> 49 pages, 33 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸè®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„è¿›å±•å‚¬ç”Ÿå‡ºä¼—å¤šåŸºç¡€æ¨¡å‹ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•å¤åˆ¶ç—…ç†åŒ»å¸ˆçš„è¯Šæ–­è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºCPathAgentï¼ŒåŸºäºè§†è§‰ç‰¹å¾è¿›è¡Œè‡ªä¸»ç¼©æ”¾å’Œå¯¼èˆªæ“ä½œï¼Œæ¨¡æ‹Ÿç—…ç†åŒ»å¸ˆçš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†æ–‘å—ã€åŒºåŸŸå’Œæ•´ä¸ªå¹»ç¯ç‰‡çš„èƒ½åŠ›ç»Ÿä¸€äºå•ä¸€æ¨¡å‹ä¸­ï¼Œç”Ÿæˆæ›´è¯¦ç»†ã€å¯è§£é‡Šçš„è¯Šæ–­æŠ¥å‘Šã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸“å®¶éªŒè¯çš„PathMMU-HR$^{2}$åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å·¨å¤§åŒºåŸŸçš„åˆ†æèƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒCPathAgentåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶è¯Šæ–­æ•ˆæœå’Œå‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦é¢†åŸŸæ¶Œç°å‡ºä¼—å¤šåŸºç¡€æ¨¡å‹ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•å®Œå…¨æ¨¡æ‹Ÿç—…ç†åŒ»å¸ˆçš„è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>CPathAgenté€šè¿‡è‡ªä¸»æ‰§è¡Œç¼©æ”¾å’Œå¯¼èˆªæ“ä½œï¼Œæ¨¡æ‹Ÿç—…ç†åŒ»å¸ˆçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°äº†æ–‘å—ã€åŒºåŸŸå’Œæ•´ä¸ªå¹»ç¯ç‰‡çš„ç»Ÿä¸€èƒ½åŠ›æ¨¡å‹ï¼Œæé«˜è¯Šæ–­çš„å…¨é¢æ€§ã€‚</li>
<li>CPathAgentç”Ÿæˆçš„è¯Šæ–­æŠ¥å‘Šæ›´åŠ è¯¦ç»†å’Œå¯è§£é‡Šã€‚</li>
<li>PathMMU-HR$^{2}$åŸºå‡†æµ‹è¯•çš„å»ºç«‹ï¼Œå¡«è¡¥äº†å·¨å¤§åŒºåŸŸåˆ†æé¢†åŸŸçš„ç©ºç™½ã€‚</li>
<li>CPathAgentåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-62c1f2a9d98cc8cd0d32145d20c79f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ce77d019e9bd3555f5aebd3d5828e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab2e2af72b08abb4296db7ba6b21ca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e6a2e279a4338bd1bf7ab91868f38c9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation"><a href="#CDPDNet-Integrating-Text-Guidance-with-Hybrid-Vision-Encoders-for-Medical-Image-Segmentation" class="headerlink" title="CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation"></a>CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for   Medical Image Segmentation</h2><p><strong>Authors:Jiong Wu, Yang Xing, Boxiao Yu, Wei Shao, Kuang Gong</strong></p>
<p>Most publicly available medical segmentation datasets are only partially labeled, with annotations provided for a subset of anatomical structures. When multiple datasets are combined for training, this incomplete annotation poses challenges, as it limits the modelâ€™s ability to learn shared anatomical representations among datasets. Furthermore, vision-only frameworks often fail to capture complex anatomical relationships and task-specific distinctions, leading to reduced segmentation accuracy and poor generalizability to unseen datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven Segmentation Network (CDPDNet), which combined a self-supervised vision transformer with CLIP-based text embedding and introduced task-specific text prompts to tackle these challenges. Specifically, the framework was constructed upon a convolutional neural network (CNN) and incorporated DINOv2 to extract both fine-grained and global visual features, which were then fused using a multi-head cross-attention module to overcome the limited long-range modeling capability of CNNs. In addition, CLIP-derived text embeddings were projected into the visual space to help model complex relationships among organs and tumors. To further address the partial label challenge and enhance inter-task discriminative capability, a Text-based Task Prompt Generation (TTPG) module that generated task-specific prompts was designed to guide the segmentation. Extensive experiments on multiple medical imaging datasets demonstrated that CDPDNet consistently outperformed existing state-of-the-art segmentation methods. Code and pretrained model are available at: <a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git">https://github.com/wujiong-hub/CDPDNet.git</a>. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†å…¬å¼€å¯ç”¨çš„åŒ»å­¦åˆ†å‰²æ•°æ®é›†ä»…éƒ¨åˆ†æ ‡æ³¨ï¼Œåªä¸ºéƒ¨åˆ†è§£å‰–ç»“æ„æä¾›æ³¨é‡Šã€‚å½“å¤šä¸ªæ•°æ®é›†ç»„åˆè¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™ç§ä¸å®Œå…¨çš„æ³¨é‡Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒé™åˆ¶äº†æ¨¡å‹åœ¨æ•°æ®é›†ä¹‹é—´å­¦ä¹ å…±äº«è§£å‰–è¡¨å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»…ä¾èµ–è§†è§‰çš„æ¡†æ¶å¾€å¾€æ— æ³•æ•æ‰å¤æ‚çš„è§£å‰–å…³ç³»å’Œä»»åŠ¡ç‰¹å®šåŒºåˆ«ï¼Œå¯¼è‡´åˆ†å‰²å‡†ç¡®åº¦é™ä½ï¼Œå¯¹æœªè§æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„CLIP-DINO Prompt-Driven Segmentation Networkï¼ˆCDPDNetï¼‰ï¼Œå®ƒå°†è‡ªæˆ‘ç›‘ç£çš„è§†è§‰å˜å‹å™¨ä¸CLIPåŸºäºçš„æ–‡æœ¬åµŒå…¥ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„æ–‡æœ¬æç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹‹ä¸Šï¼Œèå…¥äº†DINOv2ä»¥æå–ç²¾ç»†ç²’åº¦å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œç„¶åä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„æ¨¡å—èåˆè¿™äº›ç‰¹å¾ï¼Œä»¥å…‹æœCNNæœ‰é™çš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥è¢«æŠ•å°„åˆ°è§†è§‰ç©ºé—´ä¸­ï¼Œä»¥å¸®åŠ©å»ºæ¨¡å™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³éƒ¨åˆ†æ ‡ç­¾æŒ‘æˆ˜å¹¶å¢å¼ºä»»åŠ¡é—´çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—ï¼Œä»¥ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æç¤ºæ¥æŒ‡å¯¼åˆ†å‰²ã€‚åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDPDNetæŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/wujiong-hub/CDPDNet.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wujiong-hub/CDPDNet.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18958v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„CLIP-DINO Prompté©±åŠ¨åˆ†å‰²ç½‘ç»œï¼ˆCDPDNetï¼‰ï¼Œç»“åˆè‡ªç›‘ç£è§†è§‰å˜å‹å™¨ä¸CLIPæ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡æ–‡æœ¬æç¤ºæ¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´çš„æ ‡æ³¨ä¸å®Œæ•´å’Œå¤æ‚è§£å‰–å…³ç³»æ•æ‰éš¾é¢˜ã€‚CDPDNeté‡‡ç”¨CNNç»“åˆDINOv2æŠ€æœ¯æå–ç²¾ç»†ç²’åº¦å’Œå…¨å±€è§†è§‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œç‰¹å¾èåˆï¼Œä»¥å…‹æœCNNçš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›é™åˆ¶ã€‚åŒæ—¶ï¼Œåˆ©ç”¨CLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥å°†æ–‡æœ¬ä¿¡æ¯æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£å™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä¸ºè§£å†³éƒ¨åˆ†æ ‡æ³¨é—®é¢˜å¹¶å¢å¼ºä»»åŠ¡é—´çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—æ¥æŒ‡å¯¼åˆ†å‰²ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDPDNetæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CDPDNetç»“åˆè‡ªç›‘ç£è§†è§‰å˜å‹å™¨å’ŒCLIPæ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ ‡æ³¨ä¸å®Œæ•´å’Œå¤æ‚è§£å‰–å…³ç³»æ•æ‰éš¾é¢˜ã€‚</li>
<li>CDPDNeté‡‡ç”¨CNNå’ŒDINOv2æŠ€æœ¯æå–è§†è§‰ç‰¹å¾ï¼Œå®ç°ç²¾ç»†ç²’åº¦å’Œå…¨å±€ç‰¹å¾çš„èåˆã€‚</li>
<li>å¤šå¤´äº¤å‰æ³¨æ„åŠ›æ¨¡å—å…‹æœCNNçš„é•¿æœŸå»ºæ¨¡èƒ½åŠ›é™åˆ¶ã€‚</li>
<li>CLIPè¡ç”Ÿçš„æ–‡æœ¬åµŒå…¥å¸®åŠ©æ¨¡å‹ç†è§£å™¨å®˜å’Œè‚¿ç˜¤ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚</li>
<li>å¼•å…¥åŸºäºæ–‡æœ¬çš„ä»»åŠ¡æç¤ºç”Ÿæˆï¼ˆTTPGï¼‰æ¨¡å—ï¼Œè§£å†³éƒ¨åˆ†æ ‡æ³¨é—®é¢˜å¹¶å¢å¼ºä»»åŠ¡é—´çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šï¼ŒCDPDNetçš„å®éªŒç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b394b118a8230f95539df4b78bc578f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4cd7466255c92803b98e69df89abf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f929c714444911899e5fbbfda0c1eace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6c352b327d1eb756a35dec0df24319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4b7b4a0c050de83ddb270cfd1e0edf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TAGS-3D-Tumor-Adaptive-Guidance-for-SAM"><a href="#TAGS-3D-Tumor-Adaptive-Guidance-for-SAM" class="headerlink" title="TAGS: 3D Tumor-Adaptive Guidance for SAM"></a>TAGS: 3D Tumor-Adaptive Guidance for SAM</h2><p><strong>Authors:Sirui Li, Linkai Peng, Zheyuan Zhang, Gorkem Durak, Ulas Bagci</strong></p>
<p>Foundation models (FMs) such as CLIP and SAM have recently shown great promise in image segmentation tasks, yet their adaptation to 3D medical imaging-particularly for pathology detection and segmentation-remains underexplored. A critical challenge arises from the domain gap between natural images and medical volumes: existing FMs, pre-trained on 2D data, struggle to capture 3D anatomical context, limiting their utility in clinical applications like tumor segmentation. To address this, we propose an adaptation framework called TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D medical tasks through multi-prompt fusion. By preserving most of the pre-trained weights, our approach enhances SAMâ€™s spatial feature extraction using CLIPâ€™s semantic insights and anatomy-specific prompts. Extensive experiments on three open-source tumor segmentation datasets prove that our model surpasses the state-of-the-art medical image segmentation models (+46.88% over nnUNet), interactive segmentation frameworks, and other established medical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and SAM-B (at least +13% over them). This highlights the robustness and adaptability of our proposed framework across diverse medical segmentation tasks. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒCLIPå’ŒSAMç­‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨é€‚åº”ä¸‰ç»´åŒ»å­¦æˆåƒæ–¹é¢çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…ç†æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢ä»ç„¶è¢«ç ”ç©¶å¾—ä¸å¤Ÿæ·±å…¥ã€‚è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦ä½“ç§¯ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·çš„é—®é¢˜ç”±æ­¤äº§ç”Ÿäº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç°æœ‰çš„åœ¨äºŒç»´æ•°æ®ä¸Šé¢„è®­ç»ƒçš„FMsåœ¨æ•è·ä¸‰ç»´è§£å‰–ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™åœ¨è‚¿ç˜¤åˆ†å‰²ç­‰ä¸´åºŠåº”ç”¨ä¸­é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºTAGSçš„é€‚åº”æ¡†æ¶ï¼šSAMçš„è‚¿ç˜¤è‡ªé€‚åº”æŒ‡å¯¼ã€‚é€šè¿‡å¤šæç¤ºèåˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶è§£é”äº†äºŒç»´FMsåœ¨ä¸‰ç»´åŒ»å­¦ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é€šè¿‡ä¿ç•™å¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨CLIPçš„è¯­ä¹‰æ´å¯ŸåŠ›å’Œè§£å‰–ç‰¹å®šæç¤ºï¼Œå¢å¼ºäº†SAMçš„ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¼€æºè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆç›¸å¯¹äºnnUNetæé«˜+46.88%ï¼‰ï¼Œäº¤äº’å¼åˆ†å‰²æ¡†æ¶å’Œå…¶ä»–æ—¢å®šçš„åŒ»å­¦FMsï¼ŒåŒ…æ‹¬SAM-Med2Dã€SAM-Med3Dã€SegVolã€Universalã€3D-Adapterå’ŒSAM-Bï¼ˆè‡³å°‘ç›¸å¯¹äºå®ƒä»¬æé«˜+13%ï¼‰ã€‚è¿™çªæ˜¾äº†æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨ä¸åŒåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸè¡¨ç°è‰¯å¥½çš„Foundationæ¨¡å‹ï¼ˆFMsï¼‰åœ¨3DåŒ»å­¦æˆåƒä¸Šçš„åº”ç”¨æŒ‘æˆ˜ã€‚é€šè¿‡æå‡ºåä¸ºTAGSçš„é€‚åº”æ¡†æ¶ï¼Œå®ç°äº†å¯¹SAMæ¨¡å‹çš„ä¼˜åŒ–ï¼Œä½¿å…¶åœ¨è‚¿ç˜¤åˆ†å‰²ç­‰ä¸´åºŠåº”ç”¨ä¸­è¡¨ç°æ›´å¥½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæç¤ºèåˆæŠ€æœ¯ï¼Œåˆ©ç”¨CLIPçš„è¯­ä¹‰æç¤ºå’Œè§£å‰–ç‰¹å®šæç¤ºï¼Œæé«˜SAMçš„ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¼€æºè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å’Œå…¶ä»–å»ºç«‹çš„åŒ»å­¦FMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Foundationæ¨¡å‹ï¼ˆFMsï¼‰åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨3DåŒ»å­¦æˆåƒä¸Šçš„é€‚åº”åº¦ä»éœ€æé«˜ã€‚</li>
<li>ç°æœ‰FMsé¢ä¸´ä»è‡ªç„¶å›¾åƒåˆ°åŒ»å­¦ä½“ç§¯æ•°æ®çš„é¢†åŸŸå·®è·é—®é¢˜ï¼Œéš¾ä»¥æ•æ‰3Dè§£å‰–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„TAGSæ¡†æ¶æˆåŠŸè§£é”äº†SAMæ¨¡å‹åœ¨3DåŒ»å­¦ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡å¤šæç¤ºèåˆæŠ€æœ¯æé«˜å…¶ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>TAGSæ¡†æ¶ä¿ç•™äº†å¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶ç»“åˆäº†CLIPçš„è¯­ä¹‰æç¤ºå’Œè§£å‰–ç‰¹å®šæç¤ºã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¼€æºè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTAGSæ¡†æ¶è¡¨ç°è¶…è¶Šç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å’Œå…¶ä»–åŒ»å­¦FMsã€‚</li>
<li>TAGSæ¡†æ¶çš„é²æ£’æ€§å’Œé€‚åº”æ€§åœ¨ä¸åŒåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d77591b5f5b6401cb770f7ba47422045.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910dad623cc0ec6c2fba1754b8b336d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f899a68ef50954d3826c884603992d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d61fd7cb3d78b3240ddd7eded60efb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities"></a>Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities</h2><p><strong>Authors:Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</strong></p>
<p>Existing methods for multimodal MRI segmentation with missing modalities typically assume that all MRI modalities are available during training. However, in clinical practice, some modalities may be missing due to the sequential nature of MRI acquisition, leading to performance degradation. Furthermore, retraining models to accommodate newly available modalities can be inefficient and may cause overfitting, potentially compromising previously learned knowledge. To address these challenges, we propose Replay-based Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to enable the segmentation model to learn from newly acquired MRI modalities without forgetting previously learned information. To enhance segmentation performance across diverse patient scenarios, we introduce the Cross-Patient Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture high-order associations between patients. Additionally, we incorporate Tversky-Aware Contrastive (TAC) loss to effectively mitigate information imbalance both across and within different modalities. Extensive experiments on the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art methods, achieving an improvement of over 2% in the Dice Similarity Coefficient across various tumor regions. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€MRIåˆ†å‰²ç¼ºå¤±æ¨¡æ€çš„æ–¹æ³•é€šå¸¸å‡è®¾åœ¨è®­ç»ƒæœŸé—´æ‰€æœ‰MRIæ¨¡æ€éƒ½æ˜¯å¯ç”¨çš„ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­ï¼Œç”±äºMRIé‡‡é›†çš„åºåˆ—æ€§è´¨ï¼ŒæŸäº›æ¨¡æ€å¯èƒ½ä¼šç¼ºå¤±ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®¹çº³æ–°å¯ç”¨çš„æ¨¡æ€è€Œé‡æ–°è®­ç»ƒæ¨¡å‹å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œå¹¶å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»è€Œå¯èƒ½æŸå®³ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé‡æ’­çš„è¶…å›¾åŸŸå¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰æ–¹æ³•ï¼Œç”¨äºå…·æœ‰ç¼ºå¤±æ¨¡æ€çš„è„‘è‚¿ç˜¤åˆ†å‰²ã€‚ReHyDILåˆ©ç”¨åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰ä½¿åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿä»æ–°è·å–çš„MRIæ¨¡æ€ä¸­å­¦ä¹ ï¼Œè€Œä¸ä¼šå¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¿¡æ¯ã€‚ä¸ºäº†æé«˜ä¸åŒæ‚£è€…åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨è¶…å›¾æ¥æ•æ‰æ‚£è€…ä¹‹é—´çš„é«˜é˜¶å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†Tverskyæ„ŸçŸ¥å¯¹æ¯”ï¼ˆTACï¼‰æŸå¤±ï¼Œä»¥æœ‰æ•ˆåœ°ç¼“è§£ä¸åŒæ¨¡æ€ä¹‹é—´å’Œå†…éƒ¨çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReHyDILä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å„ç§è‚¿ç˜¤åŒºåŸŸçš„Diceç›¸ä¼¼ç³»æ•°ä¸Šæé«˜äº†è¶…è¿‡2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16809v2">PDF</a> MICCAI 2025 Early Accept. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/reeive/ReHyDIL">https://github.com/reeive/ReHyDIL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›æ”¾å’Œè¶…å›¾åŸŸçš„å¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰æ–¹æ³•ï¼Œç”¨äºå¤„ç†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹çš„è„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰æŠ€æœ¯ï¼Œä½¿å¾—åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰å¿˜è®°å…ˆå‰çŸ¥è¯†çš„æƒ…å†µä¸‹å­¦ä¹ æ–°è·å–çš„MRIæ¨¡æ€ã€‚é€šè¿‡å¼•å…¥è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰å’ŒTverskyæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼ˆTAC lossï¼‰ï¼Œæé«˜äº†ä¸åŒæ‚£è€…åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReHyDILåœ¨Diceç›¸ä¼¼ç³»æ•°ä¸Šè¾ƒç°æœ‰æ–¹æ³•æé«˜äº†è¶…è¿‡2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReHyDILæ–¹æ³•èƒ½å¤Ÿå¤„ç†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µï¼Œæé«˜è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>ReHyDILç»“åˆäº†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰æŠ€æœ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ–°è·å–çš„MRIæ¨¡æ€è€Œä¸å¿˜è®°å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>CHSNetç½‘ç»œé€šè¿‡åˆ©ç”¨è¶…å›¾æ•æ‰æ‚£è€…ä¹‹é—´çš„é«˜é˜¶å…³è”ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>Tverskyæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼ˆTAC lossï¼‰æœ‰æ•ˆåœ°ç¼“è§£äº†ä¸åŒæ¨¡æ€é—´å’Œå†…éƒ¨çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ReHyDILåœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒDiceç›¸ä¼¼ç³»æ•°æé«˜äº†è¶…è¿‡2%ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåº”å¯¹MRIå›¾åƒè·å–è¿‡ç¨‹ä¸­çš„åºè´¯æ€§å¯¼è‡´çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03f82c8c72814176e72332d1ae103a44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce3e84b95fc3b900a06a1f9ac5ab1b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd688d21eddf9e330de941f127023ccb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation"><a href="#Auto-nnU-Net-Towards-Automated-Medical-Image-Segmentation" class="headerlink" title="Auto-nnU-Net: Towards Automated Medical Image Segmentation"></a>Auto-nnU-Net: Towards Automated Medical Image Segmentation</h2><p><strong>Authors:Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer</strong></p>
<p>Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/automl/AutoNNUnet">https://github.com/automl/AutoNNUnet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆMISï¼‰åŒ…æ‹¬ä»éª¨éª¼åˆ°å™¨å®˜åˆ†å‰²ç­‰å¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åœ¨å¯»æ‰¾æœ€ä½³åˆ†å‰²æ¨¡å‹æ—¶éƒ½é¢ä¸´è‡ªå·±çš„æŒ‘æˆ˜ã€‚ç›®å‰æœ€å…ˆè¿›çš„ä¸AutoMLç›¸å…³çš„MISæ¡†æ¶nnU-Netèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ¨¡å‹é…ç½®çš„è®¸å¤šæ–¹é¢ï¼Œä½†ä»ç„¶å—åˆ°å›ºå®šè¶…å‚æ•°å’Œå¯å‘å¼è®¾è®¡é€‰æ‹©çš„é™åˆ¶ã€‚ä½œä¸ºMISçš„å…¨è‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Auto-nnU-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„nnU-Netå˜ä½“ï¼Œèƒ½å¤Ÿå®ç°è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œåˆ†å±‚NASï¼ˆHNASï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ­£åˆ™åŒ–PriorBandï¼Œä»¥å¹³è¡¡æ¨¡å‹ç²¾åº¦ä¸è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œè§£å†³ç°å®åŒ»å­¦ç¯å¢ƒä¸­ç»å¸¸é¢ä¸´çš„èµ„æºçº¦æŸé—®é¢˜ï¼Œè¿™äº›é—®é¢˜é™åˆ¶äº†å¹¿æ³›è®­ç»ƒç¨‹åºçš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬åœ¨ç»è¿‡å……åˆ†éªŒè¯çš„åŒ»å­¦åˆ†å‰²åé¡¹å…¨èƒ½èµ›çš„å¤šä¸ªäººMISæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåˆ†æäº†AutoMLæŠ€æœ¯å¯¹åˆ†å‰²æ€§èƒ½ã€è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è®¾è®¡é€‰æ‹©çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AutoMLæ–¹æ³•åœ¨10ä¸ªæ•°æ®é›†ä¸­çš„6ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†nnU-Netçš„åˆ†å‰²æ€§èƒ½ï¼Œåœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒäº†å®é™…çš„èµ„æºéœ€æ±‚ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/automl/AutoNNUnet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/automl/AutoNNUnetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16561v3">PDF</a> 31 pages, 19 figures. Accepted for publication at AutoML 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§å…¨è‡ªåŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶Auto-nnU-Netï¼ŒåŸºäºnnU-Netè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å’Œåˆ†å±‚æ¶æ„æœç´¢ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åŒæ—¶å¼•å…¥Regularized PriorBandæ–¹æ³•å¹³è¡¡æ¨¡å‹ç²¾åº¦ä¸è®­ç»ƒæ‰€éœ€è®¡ç®—èµ„æºï¼Œä»¥åº”å¯¹å®é™…åŒ»ç–—ç¯å¢ƒä¸­èµ„æºé™åˆ¶çš„é—®é¢˜ã€‚åœ¨Medical Segmentation Decathlonå¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥AutoMLæ–¹æ³•å¯æ˜¾è‘—æé«˜nnU-Netåœ¨6ä¸ªæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶æ»¡è¶³å®é™…èµ„æºéœ€æ±‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Auto-nnU-Netæ¡†æ¶æ‰©å±•äº†nnU-Netï¼Œå¢åŠ äº†è¶…å‚æ•°ä¼˜åŒ–ã€ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å’Œåˆ†å±‚æ¶æ„æœç´¢åŠŸèƒ½ï¼Œé€‚ç”¨äºå¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>Regularized PriorBandæ–¹æ³•å¹³è¡¡äº†æ¨¡å‹ç²¾åº¦å’Œè®¡ç®—èµ„æºéœ€æ±‚ï¼Œé€‚åº”äº†åŒ»å­¦ç¯å¢ƒä¸­èµ„æºé™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>ç›¸è¾ƒäºnnU-Netï¼ŒAuto-nnU-Netåœ¨å¤šæ•°åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ–°å‹æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åŒ–é…ç½®æ¨¡å‹ï¼Œé™ä½äº†æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°çš„éœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å®ç”¨æ€§å’Œé«˜æ•ˆæ€§æ»¡è¶³å®é™…åŒ»å­¦å›¾åƒåˆ†å‰²çš„éœ€æ±‚ã€‚</li>
<li>Auto-nnU-Netçš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„è‡ªåŠ¨åŒ–å’Œæ€§èƒ½æå‡æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28b9beff93368d180f3e8819a1c8a072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-113f352dd289a97cd261c9c75b9d8d7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95c23babfb91541184b1a3629f20407c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation"><a href="#HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation" class="headerlink" title="HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation"></a>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation</h2><p><strong>Authors:Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong</strong></p>
<p>Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalitiesâ€™ anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via <a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR">https://github.com/JeMing-creater/HWA-UNETR</a>. </p>
<blockquote>
<p>åœ¨èƒƒç™Œç—…ç¶åˆ†æçš„èƒŒæ™¯ä¸‹ï¼Œå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™ç§ä¸´åºŠèƒŒæ™¯çš„ç‰¹ç‚¹æ˜¯ç¼ºä¹ç‹¬ç«‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥åŠå¿…é¡»å°†æœ¬è´¨ä¸Šä¸å¯¹é½çš„æ¨¡å¼èåˆèµ·æ¥çš„è¿«åˆ‡éœ€æ±‚ã€‚å› æ­¤ï¼Œç®—æ³•å—é™äºåœ¨è¿‘ä¼¼æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¾èµ–äºåº”ç”¨è¿ç§»ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œåˆ†æç²¾åº¦å¯èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤å¤§è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†GCM 2025æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ã€å¼€æºçš„èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†HWA-UNETRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚æ„æˆåŸå§‹HWAå—ï¼Œä»¥åœ¨ä¸åŒæ¨¡æ€çš„è§£å‰–ç»“æ„ä¹‹é—´å»ºç«‹åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„tri-orientatedèåˆmambaæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•æ‰é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æˆ‘ä»¬çš„GCM 2025æ•°æ®é›†å’Œå…¬å¼€çš„BraTS 2021æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶æ€§èƒ½ï¼Œè¯æ˜æ–°æ–¹æ³•åœ¨Diceå¾—åˆ†ä¸Šæœ€å¤šæé«˜äº†1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10464v3">PDF</a> This work has been provisionally accepted for MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    èƒƒç™Œç—…ç¶åˆ†æçš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†å’Œéœ€è¦èåˆå›ºæœ‰é”™ä½æ¨¡æ€çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶è´¡çŒ®äº†ä¸¤ä¸ªé‡è¦æˆæœï¼šä¸€æ˜¯å…¬å¼€äº†GCM 2025æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸ºé¦–ä¸ªå…¬å¼€çš„å¤§è§„æ¨¡èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒï¼›äºŒæ˜¯å¼•å…¥äº†HWA-UNETRï¼Œä¸€ç§æ–°å‹çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å»ºç«‹ä¸åŒæ¨¡æ€è§£å‰–ç»“æ„ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„ä¸‰è§’èåˆç­–ç•¥è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•æ‰é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚åœ¨GCM 2025æ•°æ®é›†å’Œå…¬å¼€BraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æ€§èƒ½ï¼Œæ–°æ–¹æ³•çš„Diceå¾—åˆ†ç‡æé«˜äº†é«˜è¾¾1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨èƒƒç™Œç—…ç¶åˆ†æä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†å’Œéœ€è¦èåˆä¸åŒæ¨¡æ€çš„å¿…è¦æ€§ã€‚</li>
<li>å…¬å¼€äº†GCM 2025æ•°æ®é›†ï¼Œä¸ºèƒƒç™Œå¤šæ¨¡æ€MRIæ‰«æçš„é¦–ä¸ªå¤§è§„æ¨¡ã€å¼€æºæ•°æ®é›†ã€‚</li>
<li>å¼•å…¥äº†HWA-UNETRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å»ºç«‹ä¸åŒæ¨¡æ€ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ã€‚</li>
<li>HWA-UNETRæ¡†æ¶é‡‡ç”¨åˆ›æ–°çš„ä¸‰è§’èåˆç­–ç•¥è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•æ‰é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>åœ¨GCM 2025å’ŒBraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†HWA-UNETRæ¡†æ¶çš„æ€§èƒ½ä¼˜è¶Šæ€§ã€‚</li>
<li>æ–°çš„æ–¹æ³•ç›¸å¯¹äºç°æœ‰æ–¹æ³•åœ¨Diceå¾—åˆ†ç‡ä¸Šæœ‰æ‰€æé«˜ï¼Œæœ€é«˜è¾¾åˆ°1.68%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-56ca283db7146e0c0f781725e2c0cdac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85a41fcd5f13056645a134422a164310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ab7b06bb46c9d77884911ec3cf98ae.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection"><a href="#Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection" class="headerlink" title="Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection"></a>Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection</h2><p><strong>Authors:Amy Rafferty, Rishi Ramaesh, Ajitha Rajan</strong></p>
<p>Deep learning models have shown promise in lung pathology detection from chest X-rays, but widespread clinical adoption remains limited due to opaque model decision-making. In prior work, we introduced ClinicXAI, a human-centric, expert-guided concept bottleneck model (CBM) designed for interpretable lung cancer diagnosis. We now extend that approach and present XpertXAI, a generalizable expert-driven model that preserves human-interpretable clinical concepts while scaling to detect multiple lung pathologies. Using a high-performing InceptionV3-based classifier and a public dataset of chest X-rays with radiology reports, we compare XpertXAI against leading post-hoc explainability methods and an unsupervised CBM, XCBs. We assess explanations through comparison with expert radiologist annotations and medical ground truth. Although XpertXAI is trained for multiple pathologies, our expert validation focuses on lung cancer. We find that existing techniques frequently fail to produce clinically meaningful explanations, omitting key diagnostic features and disagreeing with radiologist judgments. XpertXAI not only outperforms these baselines in predictive accuracy but also delivers concept-level explanations that better align with expert reasoning. While our focus remains on explainability in lung cancer detection, this work illustrates how human-centric model design can be effectively extended to broader diagnostic contexts - offering a scalable path toward clinically meaningful explainable AI in medical diagnostics. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸­æ£€æµ‹è‚ºéƒ¨ç—…å˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å†³ç­–ä¸é€æ˜ï¼Œå…¶åœ¨ä¸´åºŠä¸Šçš„å¹¿æ³›åº”ç”¨ä»ç„¶æœ‰é™ã€‚å…ˆå‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ClinicXAIï¼Œè¿™æ˜¯ä¸€ç§ä»¥äººç±»ä¸ºä¸­å¿ƒã€ä¸“å®¶æŒ‡å¯¼çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰ï¼Œæ—¨åœ¨å®ç°å¯è§£é‡Šçš„è‚ºç™Œè¯Šæ–­ã€‚ç°åœ¨æˆ‘ä»¬æ‰©å±•äº†è¯¥æ–¹æ³•ï¼Œå¹¶æ¨å‡ºäº†XpertXAIï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨ã€ä¸“å®¶é©±åŠ¨å‹çš„æ¨¡å‹ï¼Œæ—¢ä¿ç•™äº†å¯è§£é‡Šçš„ä¸´åºŠæ¦‚å¿µï¼Œåˆèƒ½æ‰©å±•åˆ°æ£€æµ‹å¤šç§è‚ºéƒ¨ç—…å˜ã€‚æˆ‘ä»¬ä½¿ç”¨é«˜æ€§èƒ½çš„InceptionV3åˆ†ç±»å™¨å’Œä¸€ä¸ªå¸¦æœ‰æ”¾å°„å­¦æŠ¥å‘Šçš„å…¬å…±èƒ¸éƒ¨Xå…‰æ•°æ®é›†ï¼Œå°†XpertXAIä¸é¢†å…ˆçš„äº‹åè§£é‡Šæ–¹æ³•ä»¥åŠæ— ç›‘ç£çš„CBMï¼ˆXCBsï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬é€šè¿‡ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæ³¨é‡Šå’ŒåŒ»å­¦çœŸå®å€¼å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚å°½ç®¡XpertXAIç»è¿‡å¤šç§ç—…å˜çš„è®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„ä¸“å®¶éªŒè¯ä¸»è¦é›†ä¸­åœ¨è‚ºç™Œä¸Šã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„æŠ€æœ¯å¾€å¾€æ— æ³•äº§ç”Ÿå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§£é‡Šï¼Œå¿½ç•¥äº†å…³é”®çš„è¯Šæ–­ç‰¹å¾ï¼Œå¹¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„åˆ¤æ–­å­˜åœ¨åˆ†æ­§ã€‚XpertXAIä¸ä»…åœ¨é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šè¿™äº›åŸºçº¿ï¼Œè€Œä¸”æä¾›äº†ä¸ä¸“å®¶æ¨ç†æ›´ç›¸ç¬¦çš„æ¦‚å¿µå±‚é¢çš„è§£é‡Šã€‚è™½ç„¶æˆ‘ä»¬çš„é‡ç‚¹ä»ç„¶æ˜¯è‚ºç™Œæ£€æµ‹ä¸­çš„å¯è§£é‡Šæ€§ï¼Œä½†è¿™é¡¹å·¥ä½œè¯´æ˜äº†ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è®¾è®¡å¦‚ä½•æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­ç¯å¢ƒï¼Œä¸ºåŒ»å­¦è¯Šæ–­ä¸­çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æä¾›äº†å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09755v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸­æ£€æµ‹è‚ºéƒ¨ç—…å˜å…·æœ‰æ½œåŠ›ï¼Œä½†ä¸´åºŠåº”ç”¨å¹¿æ³›é‡‡ç”¨ä»æœ‰é™ï¼Œä¸»è¦ç”±äºæ¨¡å‹å†³ç­–ä¸é€æ˜ã€‚æœ¬æ–‡æ‰©å±•å…ˆå‰å·¥ä½œï¼Œæå‡ºXpertXAIï¼Œä¸€ç§é€šç”¨ä¸“å®¶é©±åŠ¨æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ç•™äººç±»å¯è§£é‡Šçš„ä¸´åºŠæ¦‚å¿µï¼ŒåŒæ—¶æ‰©å±•æ£€æµ‹å¤šç§è‚ºéƒ¨ç—…å˜ã€‚é€šè¿‡ä¸é¢†å…ˆçš„åéªŒè§£é‡Šæ–¹æ³•å’Œæ— ç›‘ç£CBMæ¯”è¾ƒè¯„ä¼°ï¼Œä»¥åŠä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæ³¨é‡Šå’ŒåŒ»å­¦çœŸå®æƒ…å†µéªŒè¯è¯„ä¼°è§£é‡Šç»“æœã€‚å°½ç®¡XpertXAIé’ˆå¯¹å¤šç§ç—…å˜è¿›è¡Œè®­ç»ƒï¼Œä½†ä¸“å®¶éªŒè¯ä¾§é‡äºè‚ºç™Œã€‚ç ”ç©¶å‘ç°ç°æœ‰æŠ€æœ¯éš¾ä»¥äº§ç”Ÿä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„è§£é‡Šï¼Œå¿½ç•¥å…³é”®è¯Šæ–­ç‰¹å¾å¹¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿåˆ¤æ–­å­˜åœ¨åˆ†æ­§ã€‚XpertXAIä¸ä»…é¢„æµ‹å‡†ç¡®ç‡é«˜äºåŸºçº¿ï¼Œè€Œä¸”æä¾›ä¸ä¸“å®¶æ¨ç†æ›´ä¸€è‡´çš„æ¦‚å¿µçº§è§£é‡Šã€‚è™½ç„¶æˆ‘ä»¬çš„é‡ç‚¹ä»ç„¶æ˜¯è‚ºç™Œæ£€æµ‹çš„å¯è§£é‡Šæ€§ï¼Œä½†è¿™é¡¹å·¥ä½œè¡¨æ˜ä»¥äººä¸ºä¸­å¿ƒçš„è®¾è®¡æ¨¡å‹å¦‚ä½•æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­æƒ…å¢ƒï¼Œä¸ºä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½è¯Šæ–­æä¾›äº†å¯æ‰©å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰è‚ºéƒ¨ç—…å˜æ£€æµ‹ä¸­æœ‰æ½œåŠ›ï¼Œä½†ä¸´åºŠåº”ç”¨å—é™ï¼Œä¸»è¦ç”±äºå†³ç­–ä¸é€æ˜ã€‚</li>
<li>ä»‹ç»XpertXAIæ¨¡å‹ï¼šä¸€ç§ä¸“å®¶é©±åŠ¨æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å¯è§£é‡Šçš„è‚ºç™Œè¯Šæ–­ï¼ŒåŒæ—¶èƒ½å¤Ÿæ£€æµ‹å¤šç§è‚ºéƒ¨ç—…å˜ã€‚</li>
<li>é€šè¿‡ä¸ç°æœ‰æŠ€æœ¯æ¯”è¾ƒè¯„ä¼°XpertXAIæ€§èƒ½ï¼Œå‘ç°ç°æœ‰æŠ€æœ¯éš¾ä»¥äº§ç”Ÿä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚</li>
<li>XpertXAIä¸ä»…é¢„æµ‹å‡†ç¡®ç‡é«˜ï¼Œè€Œä¸”æä¾›ä¸ä¸“å®¶æ¨ç†æ›´ä¸€è‡´çš„æ¦‚å¿µçº§è§£é‡Šã€‚</li>
<li>XpertXAIé€šè¿‡ä¿ç•™äººç±»å¯è§£é‡Šçš„ä¸´åºŠæ¦‚å¿µå®ç°è§£é‡Šæ€§å¢å¼ºã€‚</li>
<li>å·¥ä½œå±•ç¤ºäººä¸ºä¸­å¿ƒçš„è®¾è®¡æ¨¡å‹å¦‚ä½•æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­æƒ…å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c8ca1c09ca0cf96cbae73b1d37d6303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04edd3a2425877ee8a7de7b606daa2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88477cb86d423ef7b4d61fb3acace7c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a36c6b91cf135fed68bec7d5d06a8322.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89f48ee1d6aa8edb5eeb123935fff8df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98fc825e71211d41d027322ebc34fc47.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Describe-Anything-in-Medical-Images"><a href="#Describe-Anything-in-Medical-Images" class="headerlink" title="Describe Anything in Medical Images"></a>Describe Anything in Medical Images</h2><p><strong>Authors:Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Lin Zhao, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, Tianming Liu, Min Xu</strong></p>
<p>Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAMâ€™s superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration. </p>
<blockquote>
<p>å±€éƒ¨å›¾åƒæ ‡æ³¨å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¦‚â€œæè¿°ä»»ä½•äº‹ç‰©æ¨¡å‹â€ï¼ˆDAMï¼‰ç­‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®çš„åŒºåŸŸæ–‡æœ¬ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸç‰¹å®šæè¿°ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›å°šæœªå¹¿æ³›åº”ç”¨äºåŒ»ç–—æˆåƒç­‰ç‰¹å®šé¢†åŸŸï¼Œè¯Šæ–­è§£è¯»ä¾èµ–äºå¾®å¦™çš„åŒºåŸŸå‘ç°è€Œéå…¨å±€ç†è§£ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MedDAMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåŒºåŸŸç‰¹å®šæ ‡æ³¨çš„å…¨é¢æ¡†æ¶ã€‚MedDAMé‡‡ç”¨åŒ»å­¦ä¸“å®¶è®¾è®¡çš„é’ˆå¯¹ç‰¹å®šæˆåƒæ¨¡å¼çš„æç¤ºï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬å®šåˆ¶è¯„ä¼°åè®®ã€æ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸“ç”¨QAæ¨¡æ¿åº“ã€‚è¯¥åŸºå‡†ä¸ä»…è¯„ä¼°MedDAMï¼Œè¿˜è¯„ä¼°å…¶ä»–å¯é€‚åº”çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å±æ€§çº§åˆ«éªŒè¯ä»»åŠ¡å…³æ³¨ä¸´åºŠäº‹å®æ€§ï¼Œä»è€Œé¿å…äº†åŒ»å­¦æ•°æ®é›†ä¸­åœ°é¢çœŸå®åŒºåŸŸæ ‡é¢˜å¯¹ç¼ºå¤±çš„é—®é¢˜ã€‚åœ¨VinDr-CXRã€LIDC-IDRIå’ŒSkinConæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedDAMåœ¨ä»»åŠ¡ä¸Šä¼˜äºé¢†å…ˆçš„å¯¹ç­‰æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claude 3.7 Sonnetã€LLaMA-3.2 Visionã€Qwen2.5-VLã€GPT-4Rolå’ŒOMG-LLaVAï¼‰ï¼Œè¿™å‡¸æ˜¾äº†åŒºåŸŸçº§åˆ«è¯­ä¹‰å¯¹é½åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ç¡®ç«‹äº†MedDAMä½œä¸ºä¸´åºŠè§†è§‰è¯­è¨€é›†æˆçš„æœ‰å‰é€”çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05804v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒçš„åŒºåŸŸç‰¹å®šæè¿°é—®é¢˜ï¼Œæå‡ºMedDAMæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸç‰¹å®šæè¿°ï¼Œé€šè¿‡åŒ»å­¦ä¸“å®¶è®¾è®¡çš„æç¤ºå’Œå®šåˆ¶è¯„ä¼°åè®®ç­‰å»ºç«‹ç¨³å¥çš„è¯„ä¼°åŸºå‡†ï¼Œå¹¶åœ¨å®é™…åŒ»å­¦æ•°æ®é›†ä¸ŠéªŒè¯å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedDAMæ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåŒºåŸŸç‰¹å®šæè¿°çš„ç»¼åˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„åŒºåŸŸç‰¹å®šæè¿°ã€‚</li>
<li>MedDAMé€šè¿‡åŒ»å­¦ä¸“å®¶è®¾è®¡çš„æç¤ºï¼Œé€‚åº”ç‰¹å®šæˆåƒæ¨¡å¼ï¼Œå»ºç«‹ç¨³å¥çš„è¯„ä¼°åŸºå‡†ã€‚</li>
<li>è¯„ä¼°åŸºå‡†åŒ…æ‹¬å®šåˆ¶çš„è¯„ä¼°åè®®ã€æ•°æ®é¢„å¤„ç†ç®¡é“å’Œä¸“ç”¨é—®ç­”æ¨¡æ¿åº“ã€‚</li>
<li>MedDAMä¾§é‡äºä¸´åºŠäº‹å®æ€§ï¼Œé€šè¿‡å±æ€§çº§åˆ«çš„éªŒè¯ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>MedDAMåœ¨VinDr-CXRã€LIDC-IDRIå’ŒSkinConæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>å®éªŒç»“æœæ­ç¤ºäº†åŒºåŸŸçº§åˆ«è¯­ä¹‰å¯¹é½åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d403b6b7e3f64ddfca44b5276d6dbb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c5645c201ae936baa89a72807b8918.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-148c5ba40ff16dbda09e506fb74c63fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bab9d231cd26097297f1e380bc7bd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017280bf5c0106a392b503294a6fd28f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-via-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation via Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ç²¾ç¡®å…¨é¢çš„ä¸´åºŠè¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ‰©æ•£æ¨¡å‹æ˜¯åˆæˆæ‰€éœ€åŒ»å­¦å›¾åƒçš„ä¸€ç§å¼ºå¤§ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶å­˜åœ¨å› é«˜é¢‘ä¿¡æ¯è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯å‡å¼±å¯¼è‡´çš„è§£å‰–ç»“æ„æ‰­æ›²é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡åˆ©ç”¨å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾æ¥æå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é¢‘ç‡ï¼Œä»¥å¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆçš„é«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚ä¸ºäº†å…‹æœä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´å·¨å¤§å·®å¼‚æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ä¸è§†è§‰ç‰¹å¾ç›¸ç»“åˆï¼Œæœ‰åŠ©äºç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v2">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å­˜åœ¨çš„è§£å‰–ç»“æ„æ‰­æ›²é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°æ³¢å˜æ¢æå–é«˜ä½é¢‘æˆåˆ†ï¼Œå¹¶è®¾è®¡åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—è‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ„å»ºçŸ¥è¯†å¼•å¯¼æœºåˆ¶èåˆä¸´åºŠå…ˆéªŒçŸ¥è¯†ï¼Œç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜¯åˆæˆåŒ»å­¦å›¾åƒçš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨è§£å‰–ç»“æ„æ‰­æ›²çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºé«˜é¢‘ä¿¡æ¯çš„è¿‡åº¦æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯çš„å‡å¼±ã€‚</li>
<li>æå‡ºçš„åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–°æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å°æ³¢å˜æ¢æå–é«˜ä½é¢‘æˆåˆ†ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—å¢å¼ºä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ã€‚</li>
<li>çŸ¥è¯†å¼•å¯¼æœºåˆ¶èåˆäº†ä¸´åºŠå…ˆéªŒçŸ¥è¯†ï¼Œæœ‰åŠ©äºç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e2f95b72251a596f9478be8a8bbaf6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85c531eab5646c4c5753970e1ad9ca85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd5c9fbf2f2d2e8b959698c952e87886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d0ee28d053b37657d39b8c1df380c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-469a4ebf7d5195a3575d68266a67bc32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65e76e642496b8d2899d05866e3f3785.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy"><a href="#TrackRAD2025-challenge-dataset-Real-time-tumor-tracking-for-MRI-guided-radiotherapy" class="headerlink" title="TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy"></a>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided   radiotherapy</h2><p><strong>Authors:Yiling Wang, Elia Lombardo, Adrian Thummerer, Tom BlÃ¶cker, Yu Fan, Yue Zhao, Christianna Iris Papadopoulou, Coen Hurkmans, Rob H. N. Tijssen, Pia A. W. GÃ¶rts, Shyama U. Tetar, Davide Cusumano, Martijn P. W. Intven, Pim Borman, Marco Riboldi, Denis DudÃ¡Å¡, Hilary Byrne, Lorenzo Placidi, Marco Fusella, Michael Jameson, Miguel Palacios, Paul Cobussen, Tobias Finazzi, Cornelis J. A. Haasbeek, Paul Keall, Christopher Kurz, Guillaume Landry, Matteo Maspero</strong></p>
<p>Purpose: Magnetic resonance imaging (MRI) to visualize anatomical motion is becoming increasingly important when treating cancer patients with radiotherapy. Hybrid MRI-linear accelerator (MRI-linac) systems allow real-time motion management during irradiation. This paper presents a multi-institutional real-time MRI time series dataset from different MRI-linac vendors. The dataset is designed to support developing and evaluating real-time tumor localization (tracking) algorithms for MRI-guided radiotherapy within the TrackRAD2025 challenge (<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/">https://trackrad2025.grand-challenge.org/</a>).   Acquisition and validation methods: The dataset consists of sagittal 2D cine MRIs in 585 patients from six centers (3 Dutch, 1 German, 1 Australian, and 1 Chinese). Tumors in the thorax, abdomen, and pelvis acquired on two commercially available MRI-linacs (0.35 T and 1.5 T) were included. For 108 cases, irradiation targets or tracking surrogates were manually segmented on each temporal frame. The dataset was randomly split into a public training set of 527 cases (477 unlabeled and 50 labeled) and a private testing set of 58 cases (all labeled).   Data Format and Usage Notes: The data is publicly available under the TrackRAD2025 collection: <a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539">https://doi.org/10.57967/hf/4539</a>. Both the images and segmentations for each patient are available in metadata format.   Potential Applications: This novel clinical dataset will enable the development and evaluation of real-time tumor localization algorithms for MRI-guided radiotherapy. By enabling more accurate motion management and adaptive treatment strategies, this dataset has the potential to advance the field of radiotherapy significantly. </p>
<blockquote>
<p>ç›®çš„ï¼šåœ¨æ²»ç–—ç™Œç—‡æ‚£è€…æ—¶è¿›è¡Œæ”¾å°„æ²»ç–—æ—¶ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ¥å¯è§†åŒ–è§£å‰–è¿åŠ¨å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ··åˆMRI-ç›´çº¿åŠ é€Ÿå™¨ï¼ˆMRI-linacï¼‰ç³»ç»Ÿå¯åœ¨ç…§å°„è¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶è¿åŠ¨ç®¡ç†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæœºæ„å®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªä¸åŒçš„MRI-linacä¾›åº”å•†ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒå¼€å‘å¹¶è¯„ä¼°MRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„å®æ—¶è‚¿ç˜¤å®šä½ï¼ˆè·Ÿè¸ªï¼‰ç®—æ³•ï¼Œä»¥åº”å¯¹TrackRAD2025æŒ‘æˆ˜ï¼ˆ<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/%EF%BC%89%E3%80%82">https://trackrad2025.grand-challenge.org/ï¼‰ã€‚</a></p>
</blockquote>
<p>é‡‡é›†å’ŒéªŒè¯æ–¹æ³•ï¼šæ•°æ®é›†åŒ…å«æ¥è‡ª6ä¸ªä¸­å¿ƒï¼ˆè·å…°3ä¸ªï¼Œå¾·å›½1ä¸ªï¼Œæ¾³å¤§åˆ©äºš1ä¸ªï¼Œä¸­å›½1ä¸ªï¼‰çš„585ä¾‹æ‚£è€…çš„çŸ¢çŠ¶é¢2Dç”µå½±MRIã€‚èƒ¸ã€è…¹å’Œéª¨ç›†éƒ¨ä½çš„è‚¿ç˜¤æ˜¯åœ¨ä¸¤å°å•†ç”¨MRI-linacï¼ˆ0.35Tå’Œ1.5Tï¼‰ä¸Šè·å¾—çš„ã€‚åœ¨108ä¸ªç—…ä¾‹ä¸­ï¼Œæ¯ä¸ªæ—¶é—´å¸§ä¸Šéƒ½æ‰‹åŠ¨åˆ†å‰²äº†ç…§å°„ç›®æ ‡æˆ–è·Ÿè¸ªä»£ç†ã€‚æ•°æ®é›†è¢«éšæœºåˆ†æˆ527ä¾‹å…¬å…±è®­ç»ƒé›†ï¼ˆ477ä¾‹æœªæ ‡è®°å’Œ50ä¾‹å·²æ ‡è®°ï¼‰å’Œ58ä¾‹ç§æœ‰æµ‹è¯•é›†ï¼ˆå‡å·²æ ‡è®°ï¼‰ã€‚</p>
<p>æ•°æ®æ ¼å¼å’Œä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼šæ•°æ®å¯åœ¨TrackRAD2025æ”¶è—ä¸­å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://doi.org/10.57967/hf/4539%E3%80%82%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%84%E5%9B%BE%E5%83%8F%E5%92%8C%E5%88%86%E6%AE%B5%E9%83%BD%E5%8F%AF%E7%94%A8%E5%85%83%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E6%9F%A5%E7%9C%8B%E3%80%82">https://doi.org/10.57967/hf/4539ã€‚æ¯ä¸ªæ‚£è€…çš„å›¾åƒå’Œåˆ†æ®µéƒ½å¯ç”¨å…ƒæ•°æ®æ ¼å¼æŸ¥çœ‹ã€‚</a></p>
<p>æ½œåœ¨åº”ç”¨ï¼šè¿™ä¸ªæ–°å‹ä¸´åºŠæ•°æ®é›†å°†èƒ½å¤Ÿå¼€å‘å’Œè¯„ä¼°MRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„å®æ—¶è‚¿ç˜¤å®šä½ç®—æ³•ã€‚é€šè¿‡å®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨ç®¡ç†å’Œè‡ªé€‚åº”æ²»ç–—æ–¹æ¡ˆï¼Œè¯¥æ•°æ®é›†æœ‰æœ›æ¨åŠ¨æ”¾å°„æ²»ç–—é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19119v2">PDF</a> 10 pages, 5 figures, 2 tables; submitted to Medical Physics,   tentatively accepted</p>
<p><strong>Summary</strong><br>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ç™Œç—‡æ‚£è€…æ”¾ç–—æ²»ç–—ä¸­ç”¨äºå¯è§†åŒ–è§£å‰–è¿åŠ¨æ—¥ç›Šé‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæœºæ„å®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªä¸åŒçš„MRI-ç›´çº¿åŠ é€Ÿå™¨ï¼ˆMRI-linacï¼‰ä¾›åº”å•†ï¼Œæ—¨åœ¨æ”¯æŒå¼€å‘å¹¶è¯„ä¼°ç”¨äºMRIå¼•å¯¼æ”¾ç–—çš„å®æ—¶è‚¿ç˜¤å®šä½ï¼ˆè·Ÿè¸ªï¼‰ç®—æ³•ã€‚æ•°æ®é›†åŒ…å«æ¥è‡ªå…­ä¸ªä¸­å¿ƒçš„585åæ‚£è€…çš„çŸ¢çŠ¶é¢2Dç”µå½±MRIï¼Œæ¶µç›–äº†èƒ¸éƒ¨ã€è…¹éƒ¨å’Œéª¨ç›†çš„è‚¿ç˜¤ã€‚æ­¤æ•°æ®é›†å·²å…¬å¼€ï¼Œå¹¶å¯ç”¨äºTrackRAD2025æŒ‘æˆ˜èµ›ï¼ˆ<a target="_blank" rel="noopener" href="https://trackrad2025.grand-challenge.org/%EF%BC%89%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9C%89%E6%9C%9B%E4%BF%83%E8%BF%9B%E5%AE%9E%E6%97%B6%E8%82%BF%E7%98%A4%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%BC%80%E5%8F%91%E5%92%8C%E8%AF%84%E4%BC%B0%EF%BC%8C%E9%80%9A%E8%BF%87%E6%9B%B4%E7%B2%BE%E7%A1%AE%E7%9A%84%E8%BF%90%E5%8A%A8%E7%AE%A1%E7%90%86%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E6%B2%BB%E7%96%97%E6%96%B9%E6%A1%88%EF%BC%8C%E4%B8%BA%E6%94%BE%E7%96%97%E9%A2%86%E5%9F%9F%E5%B8%A6%E6%9D%A5%E6%98%BE%E8%91%97%E8%BF%9B%E5%B1%95%E3%80%82">https://trackrad2025.grand-challenge.org/ï¼‰ã€‚è¯¥æ•°æ®é›†æœ‰æœ›ä¿ƒè¿›å®æ—¶è‚¿ç˜¤å®šä½ç®—æ³•çš„å¼€å‘å’Œè¯„ä¼°ï¼Œé€šè¿‡æ›´ç²¾ç¡®çš„è¿åŠ¨ç®¡ç†å’Œè‡ªé€‚åº”æ²»ç–—æ–¹æ¡ˆï¼Œä¸ºæ”¾ç–—é¢†åŸŸå¸¦æ¥æ˜¾è‘—è¿›å±•ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶MRIæ—¶é—´åºåˆ—æ•°æ®é›†æ”¯æŒå¼€å‘å¹¶è¯„ä¼°ç”¨äºMRIå¼•å¯¼æ”¾ç–—çš„è‚¿ç˜¤å®šä½ç®—æ³•ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒMRI-linacä¾›åº”å•†çš„å®æ—¶æ•°æ®ï¼Œæ¶µç›–èƒ¸éƒ¨ã€è…¹éƒ¨å’Œéª¨ç›†çš„è‚¿ç˜¤ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å…¬å¼€å¯ç”¨çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œåˆ†ä¸ºæœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®ã€‚</li>
<li>æ•°æ®é›†å…·æœ‰æ½œåŠ›æ¨åŠ¨æ”¾ç–—é¢†åŸŸçš„å‘å±•ï¼Œé€šè¿‡æ›´ç²¾ç¡®çš„è¿åŠ¨ç®¡ç†å’Œè‡ªé€‚åº”æ²»ç–—æ–¹æ¡ˆæ”¹å–„æ²»ç–—æ•ˆæœã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºéªŒè¯å’Œæ”¹è¿›è‚¿ç˜¤è¿åŠ¨è·Ÿè¸ªç®—æ³•çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ­¤æ•°æ®é›†æ˜¯é¦–ä¸ªå¤šæœºæ„åˆä½œåˆ›å»ºçš„å¤§è§„æ¨¡å®æ—¶MRIæ•°æ®é›†ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a76b9e93ef13273798906b478c28c7d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9236fd355aa01f6c7e9526c44ab0c3f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a20a738135024c911b7f29a0cf0c5bf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b66b8b634d47f0db8f14cdc65f473eb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UltraBones100k-A-reliable-automated-labeling-method-and-large-scale-dataset-for-ultrasound-based-bone-surface-extraction"><a href="#UltraBones100k-A-reliable-automated-labeling-method-and-large-scale-dataset-for-ultrasound-based-bone-surface-extraction" class="headerlink" title="UltraBones100k: A reliable automated labeling method and large-scale   dataset for ultrasound-based bone surface extraction"></a>UltraBones100k: A reliable automated labeling method and large-scale   dataset for ultrasound-based bone surface extraction</h2><p><strong>Authors:Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd ViehÃ¶fer, Stephan Wirth, Lilian Calvet, Philipp FÃ¼rnstahl</strong></p>
<p>Ultrasound-based bone surface segmentation is crucial in computer-assisted orthopedic surgery. However, ultrasound images have limitations, including a low signal-to-noise ratio, and acoustic shadowing, which make interpretation difficult. Existing deep learning models for bone segmentation rely primarily on costly manual labeling by experts, limiting dataset size and model generalizability. Additionally, the complexity of ultrasound physics and acoustic shadow makes the images difficult for humans to interpret, leading to incomplete labels in anechoic regions and limiting model performance. To advance ultrasound bone segmentation and establish effective model benchmarks, larger and higher-quality datasets are needed.   We propose a methodology for collecting ex-vivo ultrasound datasets with automatically generated bone labels, including anechoic regions. The proposed labels are derived by accurately superimposing tracked bone CT models onto the tracked ultrasound images. These initial labels are refined to account for ultrasound physics. A clinical evaluation is conducted by an expert physician specialized on orthopedic sonography to assess the quality of the generated bone labels. A neural network for bone segmentation is trained on the collected dataset and its predictions are compared to expert manual labels, evaluating accuracy, completeness, and F1-score.   We collected the largest known dataset of 100k ultrasound images of human lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank test with Bonferroni correction confirmed that the bone alignment after our method significantly improved the quality of bone labeling (p &lt; 0.001). The model trained on UltraBones100k consistently outperforms manual labeling in all metrics, particularly in low-intensity regions (320% improvement in completeness at a distance threshold of 0.5 mm). </p>
<blockquote>
<p>åŸºäºè¶…å£°çš„éª¨éª¼è¡¨é¢åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒå­˜åœ¨ä¿¡å·å™ªå£°æ¯”ä½å’Œå£°å½±ç­‰å±€é™æ€§ï¼Œä½¿å¾—è§£è¯»å›°éš¾ã€‚ç°æœ‰çš„éª¨éª¼åˆ†å‰²æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸»è¦ä¾èµ–äºä¸“å®¶æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†æ•°æ®é›†çš„å¤§å°å’Œæ¨¡å‹çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¶…å£°ç‰©ç†å’Œå£°å½±çš„å¤æ‚æ€§ä½¿å¾—å›¾åƒå¯¹äººç±»æ¥è¯´éš¾ä»¥è§£è¯»ï¼Œå¯¼è‡´æ— å£°åŒºåŸŸçš„æ ‡ç­¾ä¸å®Œæ•´å¹¶é™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†æ¨è¿›è¶…å£°éª¨åˆ†å‰²å¹¶å»ºç«‹æœ‰æ•ˆçš„æ¨¡å‹åŸºå‡†ï¼Œéœ€è¦æ›´å¤§ã€æ›´é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¶é›†ç¦»ä½“è¶…å£°æ•°æ®é›†çš„æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬è‡ªåŠ¨ç”Ÿæˆçš„éª¨éª¼æ ‡ç­¾å’Œæ— å£°åŒºåŸŸã€‚è¿™äº›æ ‡ç­¾æ˜¯é€šè¿‡å‡†ç¡®åœ°å°†è¿½è¸ªçš„éª¨éª¼CTæ¨¡å‹å åŠ åˆ°è¿½è¸ªçš„è¶…å£°å›¾åƒä¸Šè€Œå¾—å‡ºçš„ã€‚è¿™äº›åˆå§‹æ ‡ç­¾ç»è¿‡è°ƒæ•´ä»¥è€ƒè™‘è¶…å£°ç‰©ç†ç‰¹æ€§ã€‚ç”±ä¸“é—¨ç ”ç©¶éª¨ç§‘è¶…å£°çš„ä¸“å®¶åŒ»ç”Ÿè¿›è¡Œä¸´åºŠè¯„ä¼°ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„éª¨éª¼æ ‡ç­¾çš„è´¨é‡ã€‚åœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªéª¨éª¼åˆ†å‰²ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶é¢„æµ‹ç»“æœä¸ä¸“å®¶æ‰‹åŠ¨æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’ŒF1åˆ†æ•°ã€‚æˆ‘ä»¬æ”¶é›†äº†å·²çŸ¥æœ€å¤§çš„åŒ…å«éª¨éª¼æ ‡ç­¾çš„10ä¸‡å¼ äººç±»ä¸‹è‚¢è¶…å£°å›¾åƒæ•°æ®é›†ï¼Œç§°ä¸ºUltraBones100kã€‚é‡‡ç”¨å¨å°”ç§‘å…‹æ–¯ç¬¦å·ç§©æ£€éªŒè¿›è¡ŒBonferroniæ ¡æ­£åè¯å®ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åï¼Œéª¨éª¼å¯¹é½æ˜¾è‘—æé«˜äº†éª¨éª¼æ ‡ç­¾çš„è´¨é‡ï¼ˆp &lt; 0.001ï¼‰ã€‚åœ¨UltraBones100kä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¸€è‡´ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å¼ºåº¦åŒºåŸŸï¼ˆåœ¨è·ç¦»é˜ˆå€¼ä¸º0.5æ¯«ç±³çš„æƒ…å†µä¸‹ï¼Œå®Œæ•´æ€§æé«˜äº†320%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03783v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è¶…å£°éª¨è¡¨é¢åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè¶…å£°å›¾åƒå­˜åœ¨ä¿¡å·å™ªå£°æ¯”è¾ƒä½ã€å£°å½±ç­‰å±€é™æ€§ï¼Œä½¿å¾—è§£è¯»å›°éš¾ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ éª¨åˆ†å‰²æ¨¡å‹ä¸»è¦ä¾èµ–ä¸“å®¶æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œé™åˆ¶äº†æ•°æ®é›†å¤§å°å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ¨è¿›è¶…å£°éª¨åˆ†å‰²å¹¶å»ºç«‹æœ‰æ•ˆçš„æ¨¡å‹åŸºå‡†ï¼Œéœ€è¦æ›´å¤§ã€æ›´é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¶é›†ç¦»ä½“è¶…å£°æ•°æ®é›†çš„æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆéª¨æ ‡ç­¾ï¼ŒåŒ…æ‹¬æ— å£°åŒºæ ‡ç­¾ã€‚æ ‡ç­¾æ˜¯é€šè¿‡å°†è¿½è¸ªçš„éª¨CTæ¨¡å‹ç²¾ç¡®å åŠ åˆ°è¿½è¸ªçš„è¶…å£°å›¾åƒä¸Šè€Œæ´¾ç”Ÿå‡ºæ¥çš„ã€‚è¿™äº›åˆå§‹æ ‡ç­¾ç»è¿‡äº†è¶…å£°ç‰©ç†å­¦çš„ä¿®æ­£ã€‚ç”±ä¸“å®¶åŒ»ç”Ÿå¯¹ç”Ÿæˆçš„éª¨æ ‡ç­¾è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªç”¨äºéª¨åˆ†å‰²çš„ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶é¢„æµ‹ç»“æœä¸ä¸“å®¶æ‰‹åŠ¨æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°äº†å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’ŒF1åˆ†æ•°ã€‚æˆ‘ä»¬æ”¶é›†äº†å·²çŸ¥æœ€å¤§çš„åŒ…å«äººç±»ä¸‹è‚¢éª¨éª¼æ ‡ç­¾çš„è¶…å£°å›¾åƒæ•°æ®é›†UltraBones100kã€‚é€šè¿‡Wilcoxonç¬¦å·ç§©æ£€éªŒå’ŒBonferroniæ ¡æ­£è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†éª¨æ ‡ç­¾çš„è´¨é‡ï¼ˆp &lt; 0.001ï¼‰ã€‚åœ¨UltraBones100kä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å¼ºåº¦åŒºåŸŸï¼ˆåœ¨è·ç¦»é˜ˆå€¼ä¸º0.5æ¯«ç±³çš„æƒ…å†µä¸‹ï¼Œå®Œæ•´æ€§æé«˜äº†320%ï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¶…å£°éª¨è¡¨é¢åˆ†å‰²åœ¨éª¨ç§‘æ‰‹æœ¯ä¸­éå¸¸é‡è¦ï¼Œä½†è¶…å£°å›¾åƒè§£è¯»å­˜åœ¨å›°éš¾ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å—é™äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œå½±å“æ•°æ®é›†å¤§å°å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆéª¨æ ‡ç­¾çš„æ–¹æ³•ï¼Œé€‚ç”¨äºç¦»ä½“è¶…å£°æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å°†è¿½è¸ªçš„éª¨CTæ¨¡å‹å åŠ åˆ°è¶…å£°å›¾åƒä¸Šç”Ÿæˆåˆå§‹æ ‡ç­¾ï¼Œå¹¶è¿›è¡Œè¶…å£°ç‰©ç†å­¦ä¿®æ­£ã€‚</li>
<li>è¿›è¡Œäº†ä¸´åºŠè¯„ä¼°ï¼Œè¯å®äº†ç”Ÿæˆçš„éª¨æ ‡ç­¾è´¨é‡æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨æ”¶é›†çš„å¤§å‹æ•°æ®é›†UltraBones100kä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f5337319b388f01414425415e9f8148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577ee87f914d2a9b569b5a52f86bf342.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PUSSM-Point-Cloud-Upsampling-as-Implicit-Statistical-Shape-Model"><a href="#PUSSM-Point-Cloud-Upsampling-as-Implicit-Statistical-Shape-Model" class="headerlink" title="PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model"></a>PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model</h2><p><strong>Authors:Tongxu Zhang, Bei Wang</strong></p>
<p>This paper proposes a framework for high-fidelity reconstruction of pelvic structures by integrating medical image segmentation and point cloud upsampling. By point cloud upsampling to learn shape priors from MedShapePelvic without requiring landmarks or PCA, our method functions as an implicit statistical shape model. Evaluations on Pelvic1k show significant improvements in surface quality and anatomical accuracy. This approach is generalizable and applicable to other skeletal regions. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªé€šè¿‡æ•´åˆåŒ»å­¦å›¾åƒåˆ†å‰²å’Œç‚¹äº‘ä¸Šé‡‡æ ·è¿›è¡Œéª¨ç›†ç»“æ„é«˜ä¿çœŸé‡å»ºçš„æ¡†æ¶ã€‚é€šè¿‡ç‚¹äº‘ä¸Šé‡‡æ ·ä»MedShapePelvicä¸­å­¦ä¹ å½¢çŠ¶å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ— éœ€åœ°æ ‡æˆ–ä¸»æˆåˆ†åˆ†æï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä½œä¸ºéšå¼ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ã€‚åœ¨Pelvic1kä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¡¨é¢è´¨é‡å’Œè§£å‰–ç²¾åº¦æœ‰äº†æ˜¾è‘—æé«˜ã€‚æ­¤æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œé€‚ç”¨äºå…¶ä»–éª¨éª¼åŒºåŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16716v3">PDF</a> 14 pages, 4 figures</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåŒ»å­¦å›¾åƒåˆ†å‰²å’Œç‚¹äº‘ä¸Šé‡‡æ ·æŠ€æœ¯çš„éª¨ç›†ç»“æ„é«˜ä¿çœŸé‡å»ºæ¡†æ¶ã€‚é€šè¿‡ç‚¹äº‘ä¸Šé‡‡æ ·ï¼Œä»MedShapePelvicä¸­å­¦ä¹ å½¢çŠ¶å…ˆéªŒï¼Œæ— éœ€åœ°æ ‡æˆ–PCAï¼Œè¯¥æ–¹æ³•å¯ä½œä¸ºéšå¼ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ã€‚åœ¨Pelvic1kä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¡¨é¢è´¨é‡å’Œè§£å‰–ç²¾åº¦å¾—åˆ°æ˜¾è‘—æé«˜ã€‚è¯¥æ–¹æ³•å¯æ¨å¹¿åº”ç”¨äºå…¶ä»–éª¨éª¼åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éª¨ç›†ç»“æ„çš„é«˜ä¿çœŸé‡å»ºæ¡†æ¶ç»“åˆäº†åŒ»å­¦å›¾åƒåˆ†å‰²å’Œç‚¹äº‘ä¸Šé‡‡æ ·æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡ç‚¹äº‘ä¸Šé‡‡æ ·å­¦ä¹ å½¢çŠ¶å…ˆéªŒï¼Œæ— éœ€ä½¿ç”¨åœ°æ ‡æˆ–PCAæŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºéšå¼ç»Ÿè®¡å½¢çŠ¶æ¨¡å‹ã€‚</li>
<li>åœ¨Pelvic1kä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„è¡¨é¢è´¨é‡å’Œè§£å‰–ç²¾åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ¨å¹¿æ€§ï¼Œå¯åº”ç”¨äºå…¶ä»–éª¨éª¼åŒºåŸŸçš„é‡å»ºã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹äºåŒ»å­¦å›¾åƒåˆ†æå’Œå¤„ç†å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0fd4b52ce607b09b55a3b2eaef4682c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f8517f0a301f19cb35a0377ab8bae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17913b850528ee98d312d3783099c475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc8ab7688631ba96752c9197204ef3a7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging"><a href="#Cancer-Net-PCa-Seg-Benchmarking-Deep-Learning-Models-for-Prostate-Cancer-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging" class="headerlink" title="Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging"></a>Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate   Cancer Segmentation Using Synthetic Correlated Diffusion Imaging</h2><p><strong>Authors:Jarett Dewbury, Chi-en Amy Tai, Alexander Wong</strong></p>
<p>Prostate cancer (PCa) is the most prevalent cancer among men in the United States, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000 total deaths in 2024. Traditional screening methods such as prostate-specific antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in diagnosis, but have faced limitations in specificity and generalizability. In this paper, we explore the potential of enhancing PCa gland segmentation using a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$). We employ several state-of-the-art deep learning models, including U-Net, SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment prostate glands from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \pm 0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82 \pm 2.0$), offered a favorable balance between accuracy and computational efficiency. Our findings demonstrate the potential of deep learning models in improving prostate gland segmentation using CDI$^s$ to enhance PCa management and clinical support. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ˜¯ç¾å›½ç”·æ€§æœ€å¸¸è§çš„ç™Œç—‡ï¼Œ2024å¹´å°†è¿‘æœ‰30ä¸‡ä¾‹ç—…ä¾‹ï¼Œå æ‰€æœ‰è¯Šæ–­çš„29%ï¼Œä»¥åŠå¯¼è‡´3ä¸‡5åƒäººæ­»äº¡ã€‚ä¼ ç»Ÿçš„ç­›æŸ¥æ–¹æ³•ï¼Œå¦‚å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸï¼ˆPSAï¼‰æ£€æµ‹å’Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œåœ¨è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨ç‰¹å¼‚æ€§å’Œæ™®åŠæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨ä¸€ç§åä¸ºåˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰çš„æ–°å‹MRIæ¨¡å¼æé«˜å‰åˆ—è…ºç™Œè…ºä½“åˆ†å‰²çš„æ½œåŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬U-Netã€SegResNetã€Swin UNETRã€Attention U-Netå’ŒLightM-UNetï¼Œå¯¹æ¥è‡ª200åCDI$^s$æ‚£è€…çš„å‰åˆ—è…ºè…ºä½“è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬å‘ç°SegResNetçš„åˆ†å‰²æ€§èƒ½æœ€ä½³ï¼ŒDice-Sorensenç³»æ•°ï¼ˆDSCï¼‰ä¸º$76.68 \pm 0.8$ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶Attention U-Netçš„å‡†ç¡®åº¦ç¨ä½ï¼ˆDSC $74.82 \pm 2.0$ï¼‰ï¼Œä½†åœ¨å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ©ç”¨CDI$^s$æé«˜å‰åˆ—è…ºç™Œè…ºä½“åˆ†å‰²æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯æ”¹å–„å‰åˆ—è…ºç™Œç®¡ç†å’Œä¸´åºŠæ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09185v2">PDF</a> 8 pages, 2 figures, to be published in Studies in Computational   Intelligence. This paper introduces Cancer-Net PCa-Seg, a comprehensive   evaluation of deep learning models for prostate cancer segmentation using   synthetic correlated diffusion imaging (CDI$^s$). We benchmark five   state-of-the-art architectures: U-Net, SegResNet, Swin UNETR, Attention   U-Net, and LightM-UNet</p>
<p><strong>Summary</strong><br>å‰åˆ—è…ºç™Œæ˜¯ç¾å›½ç”·æ€§æœ€å¸¸è§çš„ç™Œç—‡ä¹‹ä¸€ï¼Œé’ˆå¯¹å…¶è¯Šæ–­æ–¹æ³•å¦‚å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸæ£€æµ‹å’Œç£å…±æŒ¯æˆåƒå­˜åœ¨å±€é™æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰è¿™ä¸€æ–°å‹MRIæ¨¡å¼åœ¨å‰åˆ—è…ºç™Œè…ºä½“åˆ†å‰²æ–¹é¢çš„æ½œåŠ›ã€‚é‡‡ç”¨å¤šç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå‘ç°SegResNetæ¨¡å‹åˆ†å‰²æ€§èƒ½æœ€ä¼˜ï¼ŒDiceç³»æ•°ä¸º$76.68 \pm 0.8$ã€‚Attention U-Netæ¨¡å‹è™½å‡†ç¡®æ€§ç¨ä½ï¼Œä½†è®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œä¸ºå‰åˆ—è…ºç™Œç®¡ç†å’Œä¸´åºŠæ”¯æŒæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å‰åˆ—è…ºç™Œæ˜¯ç¾å›½ç”·æ€§æœ€å¸¸è§çš„ç™Œç—‡ä¹‹ä¸€ï¼Œé¢„è®¡åˆ°2024å¹´å°†å¯¼è‡´è¿‘3ä¸‡ä¾‹æ­»äº¡ã€‚</li>
<li>ä¼ ç»Ÿçš„å‰åˆ—è…ºç™Œè¯Šæ–­æ–¹æ³•å¦‚PSAæµ‹è¯•å’ŒMRIå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰ä½œä¸ºä¸€ç§æ–°å‹çš„MRIæ¨¡æ€è¢«ç ”ç©¶ç”¨äºå¢å¼ºå‰åˆ—è…ºç™Œè…ºä½“çš„åˆ†å‰²ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å‰åˆ—è…ºç™Œè…ºä½“åˆ†å‰²ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå…¶ä¸­SegResNetæ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>Attention U-Netæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾åˆ°äº†è¾ƒå¥½çš„å¹³è¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa50a45c0d07e82a1ffd6dfe2e1714bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0cfd36a497309a846da241d173813f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db6d0c08df0c7edfddccb4e7c7e4475.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8de2ae4d28c9195f7096460090da067c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  ArVoice A Multi-Speaker Dataset for Arabic Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea46b9c6e3bba5c3d8ca0e6052498fa2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Be Decisive Noise-Induced Layouts for Multi-Subject Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
