<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Reinforcing General Reasoning without Verifiers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="Reinforcing-General-Reasoning-without-Verifiers"><a href="#Reinforcing-General-Reasoning-without-Verifiers" class="headerlink" title="Reinforcing General Reasoning without Verifiers"></a>Reinforcing General Reasoning without Verifiers</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du</strong></p>
<p>The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree">https://github.com/sail-sg/VeriFree</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåˆ©ç”¨DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ä¸Šè¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒçš„èŒƒå¼è½¬å˜ï¼Œåœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»…é™äºåŸºäºè§„åˆ™çš„ç­”æ¡ˆéªŒè¯å¯è¡Œçš„ä»»åŠ¡ï¼Œå¹¶ä¸èƒ½è‡ªç„¶åœ°æ‰©å±•åˆ°ç°å®ä¸–ç•Œé¢†åŸŸï¼Œå¦‚åŒ–å­¦ã€åŒ»ç–—ã€å·¥ç¨‹ã€æ³•å¾‹ã€ç”Ÿç‰©ã€å•†ä¸šå’Œç»æµå­¦ã€‚ç›®å‰çš„å®ç”¨è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨é¢å¤–çš„LLMä½œä¸ºæ¨¡å‹éªŒè¯å™¨ï¼Œä½†è¿™å¸¦æ¥äº†å¯¹å¼ºå¤§çš„éªŒè¯å™¨LLMçš„ä¾èµ–ã€å¥–åŠ±ç ´è§£çš„æ˜“æ„Ÿæ€§ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨å†…å­˜ä¸­ç»´æŠ¤éªŒè¯å™¨æ¨¡å‹çš„å®è·µè´Ÿæ‹…ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶å°†DeepSeek-R1-Zeroé£æ ¼çš„åŸ¹è®­æ‰©å±•åˆ°ä¸€èˆ¬æ¨ç†é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œå®ƒç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œè€Œæ˜¯ä½¿ç”¨RLç›´æ¥æœ€å¤§åŒ–ç”Ÿæˆå‚è€ƒç­”æ¡ˆçš„æ¦‚ç‡ã€‚æˆ‘ä»¬å°†VeriFreeä¸åŸºäºéªŒè¯å™¨çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯æ˜VeriFreeé™¤äº†å…·æœ‰æ˜¾è‘—çš„å®è·µæ•ˆç›Šå’Œå‡å°‘è®¡ç®—éœ€æ±‚å¤–ï¼Œåœ¨MMLU-Proã€GPQAã€SuperGPQAå’Œæ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•çš„å…¨é¢è¯„ä¼°ä¸­ï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºéªŒè¯å™¨çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å¯¹è¿™ç§æ–¹æ³•è¿›è¡Œäº†æ·±å…¥æ´å¯Ÿï¼šä½œä¸ºå°†ç­–ç•¥å’Œéšå¼éªŒè¯å™¨æ•´åˆåˆ°ç»Ÿä¸€æ¨¡å‹ä¸­çš„å·§å¦™é›†æˆï¼Œä»¥åŠä½œä¸ºä¸€ç§å˜åˆ†ä¼˜åŒ–æ–¹æ³•ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree%E3%80%82">https://github.com/sail-sg/VeriFreeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ä¸‹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œè¯¥æ–¹æ³•åœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»…é™äºè§„åˆ™å¯éªŒè¯çš„ä»»åŠ¡ï¼Œæ— æ³•è‡ªç„¶æ‰©å±•åˆ°åŒ–å­¦ã€åŒ»ç–—ã€å·¥ç¨‹ã€æ³•å¾‹ã€ç”Ÿç‰©ã€å•†ä¸šå’Œç»æµç­‰çœŸå®ä¸–ç•Œé¢†åŸŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜å¹¶æ‰©å±•DeepSeek-R1-Zeroé£æ ¼çš„è®­ç»ƒè‡³é€šç”¨æ¨ç†é¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§æ— éœ€éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œè¯¥æ–¹æ³•ç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æœ€å¤§åŒ–ç”Ÿæˆå‚è€ƒç­”æ¡ˆçš„æ¦‚ç‡ã€‚å¯¹æ¯”åŸºäºéªŒè¯å™¨çš„æ–¹æ³•ï¼ŒVeriFreeä¸ä»…åœ¨å®é™…åº”ç”¨å’Œè®¡ç®—éœ€æ±‚æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œè€Œä¸”åœ¨å¹¿æ³›çš„è¯„ä¼°ä¸­ç”šè‡³è¶…è¶Šäº†åŸºäºéªŒè¯å™¨çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ åœ¨å¯éªŒè¯å¥–åŠ±ä¸‹å–å¾—äº†åœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å±€é™äºè§„åˆ™å¯éªŒè¯çš„ä»»åŠ¡ï¼Œéš¾ä»¥åº”ç”¨äºçœŸå®ä¸–ç•Œé¢†åŸŸå¦‚åŒ–å­¦ã€åŒ»ç–—ç­‰ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†æ— éœ€éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥æœ€å¤§åŒ–ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ã€‚</li>
<li>VeriFreeä¸åŸºäºéªŒè¯å™¨çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰å®é™…åº”ç”¨å’Œè®¡ç®—éœ€æ±‚ä¸Šçš„ä¼˜åŠ¿ï¼Œå¹¶åœ¨å¹¿æ³›çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>VeriFreeä½œä¸ºä¸€ç§ä¼˜é›…åœ°å°†ç­–ç•¥å’Œéšå¼éªŒè¯å™¨é›†æˆåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨æ€§ã€‚</li>
<li>VeriFreeæä¾›äº†ä¸€ç§å˜åˆ†ä¼˜åŒ–æ–¹æ³•çš„è§†è§’ï¼Œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abd1833621d3fc574f509bb61a4501f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d03a7ead67cfc16078c3ed181410e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f79b9549ae46abbbebf4c12cb745df2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners"><a href="#Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners" class="headerlink" title="Are Language Models Consequentialist or Deontological Moral Reasoners?"></a>Are Language Models Consequentialist or Deontological Moral Reasoners?</h2><p><strong>Authors:Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard SchÃ¶lkopf, Zhijing Jin</strong></p>
<p>As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens">https://github.com/keenansamway/moral-lens</a> . </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åŒ»ç–—ä¿å¥ã€æ³•å¾‹å’Œæ²»ç†ç­‰é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œç†è§£å®ƒä»¬å¦‚ä½•å¤„ç†é“å¾·ä¸Šå¤æ‚çš„åœºæ™¯å˜å¾—è‡³å…³é‡è¦ã€‚ä»¥å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“å¾·åˆ¤æ–­ï¼Œè€Œéå…¶æ½œåœ¨çš„é“å¾·æ¨ç†è¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¯¹LLMæä¾›çš„é“å¾·æ¨ç†è½¨è¿¹çš„å¤§è§„æ¨¡åˆ†æã€‚æ­¤å¤–ï¼Œä¸åŒäºä¹‹å‰çš„ç ”ç©¶ä»…ä»å°‘æ•°é“å¾·å›°å¢ƒä¸­æ¨æ–­ç»“æœï¼Œæˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨è¶…è¿‡600ä¸ªä¸åŒçš„ç”µè½¦é—®é¢˜ä½œä¸ºæ¢é’ˆï¼Œä»¥æ­ç¤ºä¸åŒLLMå†…éƒ¨å‡ºç°çš„æ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬ä»‹ç»å¹¶æµ‹è¯•äº†ä¸€ç§é“å¾·ç†ç”±çš„åˆ†ç±»æ³•ï¼Œæ ¹æ®ä¸¤ç§ä¸»è¦çš„è§„èŒƒæ€§ä¼¦ç†ç†è®ºï¼Œå³åæœä¸»ä¹‰å’Œé“ä¹‰è®ºæ¥ç³»ç»Ÿåœ°åˆ†ç±»æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMçš„æ€ç»´é“¾å¾€å¾€å€¾å‘äºåŸºäºé“å¾·ä¹‰åŠ¡çš„é“ä¹‰è®ºåŸåˆ™ï¼Œè€ŒåæœŸçš„è§£é‡Šåˆ™æ˜¾è‘—è½¬å‘å¼ºè°ƒå®ç”¨æ€§çš„åæœä¸»ä¹‰ç†ç”±ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºç†è§£LLMå¦‚ä½•å¤„ç†å¹¶é˜è¿°é“å¾·è€ƒé‡æä¾›äº†åŸºç¡€ï¼Œè¿™æ˜¯å°†LLMå®‰å…¨ä¸”å¯è§£é‡Šåœ°éƒ¨ç½²äºé«˜é£é™©å†³ç­–ç¯å¢ƒçš„é‡è¦æ­¥éª¤ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/keenansamway/moral-lensè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼¦ç†å¤æ‚åœºæ™¯ä¸­çš„é“å¾·æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å¯¹è¶…è¿‡600ä¸ªä¸åŒçš„ç”µè½¦é—®é¢˜çš„åˆ†æï¼Œä½œè€…å‘ç°LLMçš„æ€è€ƒé“¾å¾€å¾€å€¾å‘äºåŸºäºé“å¾·ä¹‰åŠ¡çš„åŸåˆ™ï¼Œè€ŒåæœŸçš„è§£é‡Šåˆ™æ˜æ˜¾è½¬å‘å¼ºè°ƒå®ç”¨ä¸»ä¹‰çš„åæœä¸»ä¹‰ç†ç”±ã€‚è¿™ä¸ºç†è§£LLMså¦‚ä½•å¤„ç†å¹¶è¡¨è¾¾ä¼¦ç†è€ƒè™‘æä¾›äº†åŸºç¡€ï¼Œæ˜¯å®‰å…¨ä¸”å¯è§£é‡Šåœ°åœ¨é«˜é£é™©å†³ç­–ç¯å¢ƒä¸­éƒ¨ç½²LLMsçš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼¦ç†å¤æ‚åœºæ™¯ä¸­çš„é“å¾·æ¨ç†è‡³å…³é‡è¦ã€‚</li>
<li>LLMçš„æ€è€ƒé“¾å€¾å‘äºåŸºäºé“å¾·ä¹‰åŠ¡çš„åŸåˆ™ï¼ˆdeontological principlesï¼‰ã€‚</li>
<li>ä¸æ—©æœŸç ”ç©¶ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶ä½¿ç”¨äº†è¶…è¿‡600ä¸ªç”µè½¦é—®é¢˜æ¥æ­ç¤ºä¸åŒLLMä¸­çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>å¼•å…¥å¹¶æµ‹è¯•äº†é“å¾·ç†ç”±çš„åˆ†ç±»æ³•ï¼Œä»¥ç³»ç»Ÿåœ°æ ¹æ®ä¸¤ç§ä¸»è¦çš„ä¼¦ç†ç†è®ºï¼ˆåæœä¸»ä¹‰å’Œé“ä¹‰è®ºï¼‰å¯¹æ¨ç†è½¨è¿¹è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åæœŸçš„è§£é‡Šæ˜æ˜¾è½¬å‘å¼ºè°ƒå®ç”¨ä¸»ä¹‰çš„åæœä¸»ä¹‰ç†ç”±ï¼ˆconsequentialist rationalesï¼‰ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªç†è§£LLMå¦‚ä½•å¤„ç†å¹¶è¡¨è¾¾ä¼¦ç†è€ƒè™‘çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81c82638e8b82dcc7462f7876531149e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5310537ab54ab5b6f5dc826993a05f40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5a3141b963ab23c4f36dc44d93b7b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd5bc52df8235fdfb5574679b7d8270.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Policy-Optimized-Text-to-Image-Pipeline-Design"><a href="#Policy-Optimized-Text-to-Image-Pipeline-Design" class="headerlink" title="Policy Optimized Text-to-Image Pipeline Design"></a>Policy Optimized Text-to-Image Pipeline Design</h2><p><strong>Authors:Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor</strong></p>
<p>Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå·²ç»å‘å±•è¶…è¶Šäº†å•ä¸€çš„æ•´ä½“æ¨¡å‹ï¼Œå½¢æˆäº†å¤æ‚çš„å¤šç»„ä»¶ç®¡é“ã€‚è¿™äº›ç®¡é“ç»“åˆäº†ç²¾ç»†è°ƒæ•´çš„ç”Ÿæˆå™¨ã€é€‚é…å™¨ã€æ”¾å¤§å—ç”šè‡³ç¼–è¾‘æ­¥éª¤ï¼Œå¯¼è‡´å›¾åƒè´¨é‡å¾—åˆ°æ˜¾è‘—æ”¹å–„ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æœ‰æ•ˆè®¾è®¡éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘çš„æ–¹æ³•åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ï¼šä¸€æ˜¯ä»æ•°ç™¾ä¸ªé¢„å®šä¹‰ç®¡é“ç”Ÿæˆå›¾åƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼ŒäºŒæ˜¯å¯¹è®°å¿†è®­ç»ƒç¤ºä¾‹ä¹‹å¤–çš„æ¨å¹¿èƒ½åŠ›è¾ƒå·®ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶æ¥è§£å†³è¿™äº›ä½æ•ˆé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹é›†åˆï¼Œèƒ½å¤Ÿç›´æ¥ä»æç¤º-å·¥ä½œæµç¨‹ç»„åˆä¸­é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œä»è€Œæ¶ˆé™¤è®­ç»ƒæœŸé—´æ˜‚è´µçš„å›¾åƒç”Ÿæˆéœ€æ±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹å·¥ä½œæµç¨‹è¯æ±‡è®­ç»ƒï¼Œç„¶åæ˜¯åŸºäºGRPOçš„ä¼˜åŒ–ï¼Œå¼•å¯¼æ¨¡å‹å‘æ€§èƒ½æ›´é«˜çš„å·¥ä½œæµç¨‹ç©ºé—´åŒºåŸŸå‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºæ— åˆ†ç±»å™¨æŒ‡å¯¼çš„å¢å¼ºæŠ€æœ¯ï¼Œæ²¿ç€åˆå§‹æ¨¡å‹å’ŒGRPOè°ƒæ•´æ¨¡å‹ä¹‹é—´çš„è·¯å¾„è¿›è¡Œæ¨æ–­ï¼Œè¿›ä¸€æ­¥æé«˜è¾“å‡ºè´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—å¯¹æ¯”éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜å®ƒæˆåŠŸåœ°åˆ›å»ºäº†å…·æœ‰æ›´é«˜å¤šæ ·æ€§çš„æ–°æµç¨‹ï¼Œå¹¶å¯¼è‡´å›¾åƒè´¨é‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸçš„å‘å±•ï¼ŒåŒ…æ‹¬å¤šç»„ä»¶ç®¡é“ã€ç²¾ç»†çš„ç”Ÿæˆå™¨ã€é€‚é…å™¨å’Œæ”¾å¤§å—ç­‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æœ‰æ•ˆè®¾è®¡éœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†ã€‚æœ€æ–°æ–¹æ³•è™½è¯•å›¾é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä½†ä»å­˜åœ¨è®¡ç®—éœ€æ±‚å¤§ã€æ³›åŒ–èƒ½åŠ›å·®çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒä¸€ç»„å¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ï¼Œä»è€Œå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›¾åƒç”Ÿæˆæˆæœ¬ã€‚åŒæ—¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåˆå§‹å·¥ä½œæµç¨‹è¯æ±‡è®­ç»ƒï¼Œæ¥ç€æ˜¯GPROä¼˜åŒ–ï¼Œæœ€åå¼•å…¥ä¸€ç§åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯çš„å¢å¼ºæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜è¾“å‡ºè´¨é‡ã€‚éªŒè¯äº†æ–°æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºæ›´å…·å¤šæ ·æ€§å’Œé«˜è´¨é‡å›¾åƒçš„å·¥ä½œæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå·²ä»å•ä¸€æ¨¡å‹æ¼”å˜ä¸ºå¤æ‚çš„å¤šç»„ä»¶ç®¡é“ï¼ŒåŒ…æ‹¬ç²¾ç»†çš„ç”Ÿæˆå™¨ã€é€‚é…å™¨å’Œæ”¾å¤§å—ç­‰ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•å­˜åœ¨è®¡ç®—éœ€æ±‚å¤§ã€æ³›åŒ–èƒ½åŠ›å·®çš„å±€é™æ€§ã€‚</li>
<li>æ–°æ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°æ¥å‡å°‘è®­ç»ƒæˆæœ¬ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåˆå§‹å·¥ä½œæµç¨‹è¯æ±‡è®­ç»ƒï¼Œç„¶åæ˜¯GRPOä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯çš„å¢å¼ºæŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜è¾“å‡ºè´¨é‡ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºå…·æœ‰å¤šæ ·æ€§å’Œé«˜è´¨é‡å›¾åƒçš„æ–°å·¥ä½œæµç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7493c31768c123ea46735d176ea4d438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08509a719874ee7319d6eaaa84ad7682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ace6180579b47e9f0e9f74d668599c7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PropMolFlow-Property-guided-Molecule-Generation-with-Geometry-Complete-Flow-Matching"><a href="#PropMolFlow-Property-guided-Molecule-Generation-with-Geometry-Complete-Flow-Matching" class="headerlink" title="PropMolFlow: Property-guided Molecule Generation with Geometry-Complete   Flow Matching"></a>PropMolFlow: Property-guided Molecule Generation with Geometry-Complete   Flow Matching</h2><p><strong>Authors:Cheng Zeng, Jirui Jin, George Karypis, Mark Transtrum, Ellad B. Tadmor, Richard G. Hennig, Adrian Roitberg, Stefano Martiniani, Mingjie Liu</strong></p>
<p>Molecule generation is advancing rapidly in chemical discovery and drug design. Flow matching methods have recently set the state of the art (SOTA) in unconditional molecule generation, surpassing score-based diffusion models. However, diffusion models still lead in property-guided generation. In this work, we introduce PropMolFlow, a novel approach for property-guided molecule generation based on geometry-complete SE(3)-equivariant flow matching. Integrating five different property embedding methods with a Gaussian expansion of scalar properties, PropMolFlow outperforms previous SOTA diffusion models in conditional molecule generation across various properties while preserving the stability and validity of the generated molecules, consistent with its unconditional counterpart. Additionally, it enables faster inference with significantly fewer time steps compared to baseline models. We highlight the importance of validating the properties of generated molecules through DFT calculations performed at the same level of theory as the training data. Specifically, our analysis identifies properties that require DFT validation and others where a pretrained SE(3) geometric vector perceptron regressors provide sufficiently accurate predictions on generated molecules. Furthermore, we introduce a new property metric designed to assess the modelâ€™s ability to propose molecules with underrepresented property values, assessing its capacity for out-of-distribution generalization. Our findings reveal shortcomings in existing structural metrics, which mistakenly validate open-shell molecules or molecules with invalid valence-charge configurations, underscoring the need for improved evaluation frameworks. Overall, this work paves the way for developing targeted property-guided generation methods, enhancing the design of molecular generative models for diverse applications. </p>
<blockquote>
<p>åˆ†å­ç”Ÿæˆåœ¨åŒ–å­¦å‘ç°å’Œè¯ç‰©è®¾è®¡é¢†åŸŸæ­£è¿…é€Ÿæ¨è¿›ã€‚æœ€è¿‘ï¼Œæµé‡åŒ¹é…æ–¹æ³•åœ¨æ— æ¡ä»¶åˆ†å­ç”Ÿæˆé¢†åŸŸå·²ç»å¤„äºæœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼ˆSOTAï¼‰ï¼Œè¶…è¶Šäº†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹åœ¨å±æ€§å¯¼å‘ç”Ÿæˆæ–¹é¢ä»å ä¼˜åŠ¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PropMolFlowï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå‡ ä½•å®Œå…¨SE(3)ç­‰ä»·æµåŒ¹é…çš„æ–°é¢–å±æ€§å¯¼å‘åˆ†å­ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡å°†äº”ç§ä¸åŒçš„å±æ€§åµŒå…¥æ–¹æ³•ä¸æ ‡é‡å±æ€§çš„é«˜æ–¯æ‰©å±•ç›¸ç»“åˆï¼ŒPropMolFlowåœ¨å¤šç§å±æ€§çš„æœ‰æ¡ä»¶åˆ†å­ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„SOTAæ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆåˆ†å­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸å…¶æ— æ¡ä»¶å¯¹åº”ç‰©ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ›´å¿«çš„æ¨ç†ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ—¶é—´æ­¥éª¤ã€‚æˆ‘ä»¬å¼ºè°ƒé€šè¿‡ä¸å®æ–½è®­ç»ƒæ•°æ®ç›¸åŒç†è®ºæ°´å¹³çš„DFTè®¡ç®—éªŒè¯ç”Ÿæˆåˆ†å­å±æ€§çš„é‡è¦æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„åˆ†æç¡®å®šäº†éœ€è¦è¿›è¡ŒDFTéªŒè¯çš„å±æ€§ï¼Œä»¥åŠå…¶ä»–ä½¿ç”¨é¢„è®­ç»ƒçš„SE(3)å‡ ä½•å‘é‡æ„ŸçŸ¥å™¨å›å½’å™¨å¯¹ç”Ÿæˆçš„åˆ†å­æä¾›è¶³å¤Ÿå‡†ç¡®é¢„æµ‹çš„å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å±æ€§æŒ‡æ ‡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹æå‡ºå…·æœ‰ä»£è¡¨æ€§ä¸è¶³çš„å±æ€§å€¼çš„åˆ†å­çš„èƒ½åŠ›ï¼Œè¯„ä¼°å…¶åç¦»åˆ†å¸ƒæ³›åŒ–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰ç»“æ„æŒ‡æ ‡çš„ä¸è¶³ï¼Œè¿™äº›æŒ‡æ ‡é”™è¯¯åœ°éªŒè¯äº†å¼€å£³åˆ†å­æˆ–å…·æœ‰æ— æ•ˆä»·ç”µè·é…ç½®çš„åˆ†å­ï¼Œå¼ºè°ƒäº†å¯¹æ”¹è¿›è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¼€å‘æœ‰é’ˆå¯¹æ€§çš„å±æ€§å¯¼å‘ç”Ÿæˆæ–¹æ³•é“ºå¹³äº†é“è·¯ï¼Œå¢å¼ºäº†åˆ†å­ç”Ÿæˆæ¨¡å‹åœ¨å„ç§åº”ç”¨ä¸­çš„è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21469v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ–å­¦å‘ç°å’Œè¯ç‰©è®¾è®¡ä¸­ï¼Œåˆ†å­ç”Ÿæˆé¢†åŸŸçš„æ–°è¿›å±•ã€‚æ–‡ç« æå‡ºä¸€ç§æ–°å‹çš„æ–¹æ³•PropMolFlowï¼ŒåŸºäºå‡ ä½•å®Œå…¨SE(3)ç­‰ä»·æµåŒ¹é…ï¼Œç”¨äºå±æ€§å¯¼å‘çš„åˆ†å­ç”Ÿæˆã€‚æ­¤æ–¹æ³•åœ¨èåˆäº”ç§å±æ€§åµŒå…¥æ–¹æ³•å’Œæ ‡é‡å±æ€§çš„é«˜æ–¯æ‰©å±•åï¼Œåœ¨æ¡ä»¶åˆ†å­ç”Ÿæˆçš„å„ç§å±æ€§ä¸Šè¶…è¶Šäº†å½“å‰æœ€ä½³æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆåˆ†å­çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”çš„æ—¶é—´æ­¥éª¤ã€‚æ–‡ç« å¼ºè°ƒäº†é€šè¿‡DFTè®¡ç®—éªŒè¯ç”Ÿæˆåˆ†å­å±æ€§çš„é‡è¦æ€§ï¼Œå¹¶åˆ†æäº†å“ªäº›å±æ€§éœ€è¦DFTéªŒè¯ï¼Œå“ªäº›å±æ€§å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„SE(3)å‡ ä½•å‘é‡æ„ŸçŸ¥å›å½’å™¨æä¾›è¶³å¤Ÿå‡†ç¡®çš„é¢„æµ‹ã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å±æ€§æŒ‡æ ‡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹æå‡ºå…·æœ‰ä»£è¡¨æ€§å±æ€§å€¼çš„åˆ†å­çš„èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰ç»“æ„æŒ‡æ ‡çš„ä¸è¶³ï¼Œå¼ºè°ƒäº†éœ€è¦æ”¹è¿›è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ†å­ç”Ÿæˆé¢†åŸŸæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶åœ¨åŒ–å­¦å‘ç°å’Œè¯ç‰©è®¾è®¡ä¸­ã€‚</li>
<li>PropMolFlowæ˜¯ä¸€ç§æ–°çš„å±æ€§å¯¼å‘çš„åˆ†å­ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºå‡ ä½•å®Œå…¨SE(3)ç­‰ä»·æµåŒ¹é…ã€‚</li>
<li>PropMolFlowåœ¨æ¡ä»¶åˆ†å­ç”Ÿæˆçš„å„ç§å±æ€§ä¸Šè¶…è¶Šäº†å½“å‰æœ€ä½³æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>PropMolFlowèƒ½æ›´å¿«åœ°è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ—¶é—´æ­¥éª¤ã€‚</li>
<li>DFTè®¡ç®—å¯¹äºéªŒè¯ç”Ÿæˆåˆ†å­çš„å±æ€§è‡³å…³é‡è¦ã€‚</li>
<li>éœ€è¦åŒºåˆ†å“ªäº›å±æ€§éœ€è¦DFTéªŒè¯ï¼Œå“ªäº›å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„SE(3)å‡ ä½•å‘é‡æ„ŸçŸ¥å›å½’å™¨è¿›è¡Œé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6355fb4bf4e3d348062441e7c810a53d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3074ea071efeaa8aea5d7f16e01d78f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Accelerating-Diffusion-Language-Model-Inference-via-Efficient-KV-Caching-and-Guided-Diffusion"><a href="#Accelerating-Diffusion-Language-Model-Inference-via-Efficient-KV-Caching-and-Guided-Diffusion" class="headerlink" title="Accelerating Diffusion Language Model Inference via Efficient KV Caching   and Guided Diffusion"></a>Accelerating Diffusion Language Model Inference via Efficient KV Caching   and Guided Diffusion</h2><p><strong>Authors:Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta</strong></p>
<p>Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver up to a 34x end-to-end speedup without compromising accuracy. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains. </p>
<blockquote>
<p>æ‰©æ•£è¯­è¨€æ¨¡å‹å…·æœ‰å¹¶è¡Œä»¤ç‰Œç”Ÿæˆå’Œå†…åœ¨åŒå‘æ€§çš„ä¼˜ç‚¹ï¼Œä¸è‡ªå›å½’æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä»¬æä¾›äº†æ›´é«˜æ•ˆå’Œå¼ºå¤§çš„åºåˆ—å»ºæ¨¡ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚Dream 7Bã€LLaDA 8Bï¼‰å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚å°½ç®¡å®ƒä»¬åœ¨è´¨é‡ä¸Šå¯ä»¥ä¸è§„æ¨¡ç›¸ä¼¼çš„è‡ªå›å½’æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen2.5 7Bã€Llama3 8Bï¼‰ç›¸åŒ¹é…ï¼Œä½†ç”±äºå®ƒä»¬çš„è¿­ä»£é™å™ªéœ€è¦å¤§é‡å®Œæ•´åºåˆ—çš„å‰å‘ä¼ é€’è¿‡ç¨‹ï¼Œå¯¼è‡´äº†è®¡ç®—æˆæœ¬é«˜å’Œå»¶è¿Ÿé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥æç¤ºé•¿ä»¥åŠé•¿æœŸä¸Šä¸‹æ–‡åœºæ™¯ä¸­æ›´ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œå¹¶è¡Œä»¤ç‰Œç”Ÿæˆä¼šå¯¼è‡´ä»¤ç‰Œä¸ä¸€è‡´é—®é¢˜ï¼Œå½“å‰é‡‡æ ·å¯å‘å¼ç­–ç•¥åœ¨å‡å°‘é™å™ªæ­¥éª¤æ—¶ä¼šå‡ºç°æ˜¾è‘—çš„è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ç§æ— éœ€è®­ç»ƒçš„æŠ€æœ¯æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFreeCacheçš„é”®å€¼ï¼ˆKVï¼‰è¿‘ä¼¼ç¼“å­˜æŠ€æœ¯ï¼Œå®ƒå¯ä»¥åœ¨é™å™ªæ­¥éª¤ä¸­é‡å¤ä½¿ç”¨ç¨³å®šçš„KVæŠ•å½±ï¼Œæœ‰æ•ˆé™ä½æ‰©æ•£è¯­è¨€æ¨¡å‹æ¨ç†çš„è®¡ç®—æˆæœ¬ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†Guided Diffusionæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç›‘ç£ä»¤ç‰Œå»æ©ç æ–¹æ³•ï¼Œä½¿ç”¨è½»é‡çº§é¢„è®­ç»ƒè‡ªå›å½’æ¨¡å‹è¿›è¡Œç›‘ç£ï¼Œå¤§å¤§é™ä½äº†é™å™ªè¿­ä»£æ¬¡æ•°ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è´¨é‡ã€‚æˆ‘ä»¬åœ¨å¼€æºæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ç»„åˆæ–¹æ³•åœ¨ä¸æŸå®³å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è¾¾34å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹é¦–æ¬¡å®ç°äº†ä¸å¹¿æ³›é‡‡ç”¨çš„è‡ªå›å½’æ¨¡å‹ç›¸å½“çš„ç”šè‡³æ›´å¿«çš„å»¶è¿Ÿæ—¶é—´ã€‚æˆ‘ä»¬çš„å·¥ä½œæˆåŠŸä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨è·¨ä¸åŒé¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDiffusion Language Modelsï¼Œç®€ç§°DLMsï¼‰çš„ç‰¹ç‚¹åŠå…¶åœ¨åºåˆ—å»ºæ¨¡ä¸­çš„ä¼˜åŠ¿ã€‚å°½ç®¡å®ƒä»¬å…·å¤‡å¹¶è¡Œæ ‡è®°ç”Ÿæˆå’Œå›ºæœ‰çš„åŒå‘æ€§ï¼Œä½†ä¸ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§è®­ç»ƒæ— å…³çš„æŠ€æœ¯ï¼šFreeCacheå’ŒGuided Diffusionã€‚å‰è€…é€šè¿‡é‡ç”¨ç¨³å®šçš„é”®å€¼æŠ•å½±æ¥å‡å°‘è®¡ç®—æˆæœ¬ï¼Œåè€…ä½¿ç”¨è½»é‡çº§çš„é¢„è®­ç»ƒè‡ªå›å½’æ¨¡å‹æ¥ç›‘ç£æ ‡è®°å»æ©ç è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘å»å™ªè¿­ä»£æ¬¡æ•°è€Œä¸æŸå¤±è´¨é‡ã€‚ç»“åˆè¿™ä¸¤ç§æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾34å€ç«¯åˆ°ç«¯çš„åŠ é€Ÿï¼Œä½¿æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å»¶è¿Ÿä¸å¹¿æ³›é‡‡ç”¨çš„è‡ªå›å½’æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¿«ã€‚è¿™ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨è·¨ä¸åŒé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ›´å¹¿é˜”çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹å…·å¤‡å¹¶è¡Œæ ‡è®°ç”Ÿæˆå’Œå›ºæœ‰çš„åŒå‘æ€§ï¼Œå¯å®ç°æ›´é«˜æ•ˆå’Œå¼ºå¤§çš„åºåˆ—å»ºæ¨¡ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºè¿­ä»£å»å™ªéœ€è¦å¤šæ¬¡å…¨åºåˆ—å‰å‘ä¼ é€’ã€‚</li>
<li>FreeCacheæŠ€æœ¯é€šè¿‡é‡ç”¨ç¨³å®šçš„é”®å€¼æŠ•å½±æ¥å‡å°‘è®¡ç®—æˆæœ¬ï¼Œæé«˜æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>Guided Diffusionæ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹æ¥ç›‘ç£æ ‡è®°å»æ©ç è¿‡ç¨‹ï¼Œå‡å°‘å»å™ªè¿­ä»£æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒè´¨é‡ã€‚</li>
<li>ç»“åˆä¸¤ç§æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾34å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä½¿æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å»¶è¿Ÿä¸è‡ªå›å½’æ¨¡å‹ç›¸å½“ã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ”¹è¿›ä¸ºå…¶åœ¨è·¨ä¸åŒé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ›´å¹¿é˜”çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fefbd53508a7785a06c003231275da77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c83ee5751308cea2e39b50ebfd85f3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32a0073bd3b3cdfe5461c1fe27d06721.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e844680aa78cddbc863be1dce62cc21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584784fa475132d3dd9b379809876baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9996f9810e0e6771e1e9f25044eeeec7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation"><a href="#RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation" class="headerlink" title="RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"></a>RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</h2><p><strong>Authors:Xiao Liu, Da Yin, Zirui Wu, Yansong Feng</strong></p>
<p>Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the modelsâ€™ internal knowledge and would fail in domains beyond the LLMsâ€™ knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning. </p>
<blockquote>
<p>å·¥å…·å¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„ä»»åŠ¡éƒ½æœ‰å¯ç”¨çš„å·¥å…·ã€‚åœ¨æ²¡æœ‰é¢„è®¾å·¥å…·çš„æƒ…å†µä¸‹ï¼Œæ—©æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»æ¢ç´¢äº†æŒ‡å¯¼LLMè‡ªè¡Œç”Ÿæˆå·¥å…·çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶ä¸”åœ¨LLMçŸ¥è¯†èŒƒå›´ä¹‹å¤–çš„é¢†åŸŸä¼šå¤±æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RefToolï¼Œè¿™æ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼çš„è‡ªåŠ¨å·¥å…·åˆ›å»ºæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»“æ„åŒ–çš„å¤–éƒ¨ææ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰ã€‚RefToolç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šï¼ˆ1ï¼‰å·¥å…·åˆ›å»ºï¼Œå…¶ä¸­LLMæ ¹æ®å‚è€ƒå†…å®¹ç”Ÿæˆå¯æ‰§è¡Œå·¥å…·ï¼Œä½¿ç”¨ç¤ºä¾‹è¿›è¡ŒéªŒè¯ï¼Œå¹¶å°†å…¶æŒ‰å±‚æ¬¡ç»“æ„ç»„ç»‡æˆå·¥å…·ç®±ï¼›ï¼ˆ2ï¼‰å·¥å…·åˆ©ç”¨ï¼Œå…¶ä¸­LLMæµè§ˆå·¥å…·ç®±ç»“æ„ï¼Œé€‰æ‹©å¹¶åº”ç”¨é€‚å½“çš„å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚åœ¨å› æœã€ç‰©ç†å’ŒåŒ–å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRefToolåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰çš„å·¥å…·åˆ›å»ºå’Œé¢†åŸŸç‰¹å®šæ¨ç†æ–¹æ³•é«˜å‡º11.3%ï¼ŒåŒæ—¶æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯å¹¿æ³›æ¨å¹¿ã€‚åˆ†æè¡¨æ˜ï¼Œä»¥å‚è€ƒä¸ºåŸºç¡€çš„å·¥å…·åˆ›å»ºäº§ç”Ÿäº†å‡†ç¡®å’Œå¿ è¯šçš„å·¥å…·ï¼Œå±‚æ¬¡ç»“æ„æœ‰åŠ©äºæœ‰æ•ˆçš„å·¥å…·é€‰æ‹©ã€‚RefToolä½¿LLMèƒ½å¤Ÿå…‹æœçŸ¥è¯†å±€é™æ€§ï¼Œè¯æ˜äº†ä»¥å¤–éƒ¨å‚è€ƒä¸ºåŸºç¡€çš„å·¥å…·åˆ›å»ºå¯¹äºå¢å¼ºå’Œé€šç”¨æ¨ç†çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21413v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/xxxiaol/RefTool">https://github.com/xxxiaol/RefTool</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRefToolçš„å‚è€ƒå¼•å¯¼æ¡†æ¶ï¼Œç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è‡ªåŠ¨ç”Ÿæˆå·¥å…·ã€‚è¯¥æ¡†æ¶å€ŸåŠ©ç»“æ„åŒ–å¤–éƒ¨ææ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰æ¥åˆ›å»ºå·¥å…·ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶åœ¨å› æœã€ç‰©ç†å’ŒåŒ–å­¦ç­‰é¢†åŸŸçš„æ•ˆæœä¼˜äºç°æœ‰å·¥å…·åˆ›å»ºå’Œç‰¹å®šé¢†åŸŸæ¨ç†æ–¹æ³•ã€‚RefToolè§£å†³äº†åœ¨ç¼ºä¹é¢„å…ˆå®šä¹‰çš„å·¥å…·æ—¶LLMçš„çŸ¥è¯†å±€é™æ€§é—®é¢˜ï¼Œé€šè¿‡å‚è€ƒå†…å®¹ç”Ÿæˆå¯æ‰§è¡Œå·¥å…·ï¼Œå¹¶ä»¥å±‚æ¬¡ç»“æ„ç»„ç»‡å·¥å…·ç®±ï¼Œä½¿LLMèƒ½å¤Ÿé€‰æ‹©å¹¶åº”ç”¨é€‚å½“çš„å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RefToolæ˜¯ä¸€ä¸ªå‚è€ƒå¼•å¯¼æ¡†æ¶ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆå·¥å…·ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–å¤–éƒ¨ææ–™ï¼ˆå¦‚æ•™ç§‘ä¹¦ï¼‰æ¥åˆ›å»ºå·¥å…·ã€‚</li>
<li>RefToolåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šå·¥å…·åˆ›å»ºå’Œå·¥å…·åˆ©ç”¨ã€‚</li>
<li>å·¥å…·åˆ›å»ºæ¨¡å—ä½¿LLMä»å‚è€ƒå†…å®¹ç”Ÿæˆå·¥å…·ï¼Œå¹¶ç”¨ç¤ºä¾‹éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œä»¥å±‚æ¬¡ç»“æ„ç»„ç»‡å·¥å…·ç®±ã€‚</li>
<li>å·¥å…·åˆ©ç”¨æ¨¡å—ä½¿LLMèƒ½å¤Ÿé€‰æ‹©å¹¶åº”ç”¨é€‚å½“çš„å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRefToolåœ¨å› æœã€ç‰©ç†å’ŒåŒ–å­¦ç­‰é¢†åŸŸä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜11.3%ã€‚</li>
<li>RefToolè§£å†³äº†LLMçš„çŸ¥è¯†å±€é™æ€§é—®é¢˜ï¼Œé€šè¿‡å‚è€ƒå†…å®¹ç”Ÿæˆå·¥å…·å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-565ff994cf9e3608f1a45c7d54192219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7bcf12572c4bec174366f2c6639fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff673cc55f6d6d92eadb47d9967ed88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce15a60ee0afeef1a17e1de7fe7165d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Structured-Unplugged-Approach-for-Foundational-AI-Literacy-in-Primary-Education"><a href="#A-Structured-Unplugged-Approach-for-Foundational-AI-Literacy-in-Primary-Education" class="headerlink" title="A Structured Unplugged Approach for Foundational AI Literacy in Primary   Education"></a>A Structured Unplugged Approach for Foundational AI Literacy in Primary   Education</h2><p><strong>Authors:Maria Cristina Carrisi, Mirko Marras, Sara Vergallo</strong></p>
<p>Younger generations are growing up in a world increasingly shaped by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them. However, education in this field often emphasizes tool-based learning, prioritizing usage over understanding the underlying concepts. This lack of knowledge leaves non-experts, especially children, prone to misconceptions, unrealistic expectations, and difficulties in recognizing biases and stereotypes. In this paper, we propose a structured and replicable teaching approach that fosters foundational AI literacy in primary students, by building upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI. To assess the effectiveness of our approach, we conducted an empirical study with thirty-one fifth-grade students across two classes, evaluating their progress through a post-test and a satisfaction survey. Our results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations. Moreover, the approach proved engaging, with students particularly enjoying activities that linked AI concepts to real-world reasoning. Materials: <a target="_blank" rel="noopener" href="https://github.com/tail-unica/ai-literacy-primary-ed">https://github.com/tail-unica/ai-literacy-primary-ed</a>. </p>
<blockquote>
<p>å¹´è½»ä¸€ä»£æ­£åœ¨ä¸€ä¸ªç”±æ™ºèƒ½æŠ€æœ¯æ—¥ç›Šå¡‘é€ çš„ä¸–ç•Œä¸­æˆé•¿ï¼Œå› æ­¤æ—©æœŸçš„äººå·¥æ™ºèƒ½ç´ å…»å¯¹äºå‘å±•æ‰¹åˆ¤æ€§ç†è§£å’Œé©¾é©­è¿™äº›æŠ€æœ¯çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„æ•™è‚²å¾€å¾€å¼ºè°ƒåŸºäºå·¥å…·çš„å­¦ä¹ ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨è€Œéç†è§£åŸºæœ¬æ¦‚å¿µã€‚è¿™ç§çŸ¥è¯†çš„ç¼ºä¹ä½¿å¾—éä¸“å®¶ï¼Œå°¤å…¶æ˜¯å„¿ç«¥ï¼Œå®¹æ˜“å—åˆ°è¯¯è§£ã€äº§ç”Ÿä¸åˆ‡å®é™…çš„æœŸæœ›ï¼Œå¹¶ä¸”åœ¨è¯†åˆ«åè§å’Œåˆ»æ¿å°è±¡æ—¶é‡åˆ°å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“æ„åŒ–ä¸”å¯å¤åˆ¶çš„æ•™å­¦æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ ¸å¿ƒæ•°å­¦å…ƒç´ çš„åŸºç¡€ä¸Šï¼Œç»“åˆå°å­¦è¯¾ç¨‹ä¸­çš„ç´§å¯†ç›¸å…³å’Œæ„Ÿå…´è¶£çš„å†…å®¹ï¼Œä¿ƒè¿›å°å­¦ç”Ÿçš„äººå·¥æ™ºèƒ½ç´ å…»å‘å±•ï¼Œä»¥åŠ å¼ºæ¦‚å¿µåŒ–ã€æ•°æ®è¡¨ç¤ºã€åˆ†ç±»æ¨ç†å’Œäººå·¥æ™ºèƒ½è¯„ä¼°ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹ä¸¤ä¸ªç­çº§çš„ä¸‰åä¸€åäº”å¹´çº§å­¦ç”Ÿè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œé€šè¿‡è¯¾åæµ‹è¯•å’Œæ»¡æ„åº¦è°ƒæŸ¥æ¥è¯„ä¼°ä»–ä»¬çš„è¿›æ­¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå­¦ç”Ÿåœ¨æœ¯è¯­ç†è§£å’Œä½¿ç”¨ã€ç‰¹å¾æè¿°ã€é€»è¾‘æ¨ç†å’Œè¯„ä¼°æŠ€èƒ½æ–¹é¢æœ‰æ‰€æé«˜ï¼Œå­¦ç”Ÿå¯¹å†³ç­–è¿‡ç¨‹åŠå…¶å±€é™æ€§æœ‰äº†æ›´æ·±çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¯æ˜æ˜¯å¸å¼•äººçš„ï¼Œå­¦ç”Ÿç‰¹åˆ«å–œæ¬¢å°†äººå·¥æ™ºèƒ½æ¦‚å¿µä¸ç°å®ä¸–ç•Œçš„æ¨ç†è”ç³»èµ·æ¥çš„æ´»åŠ¨ã€‚ææ–™ï¼š<a target="_blank" rel="noopener" href="https://github.com/tail-unica/ai-literacy-primary-ed">é“¾æ¥</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21398v1">PDF</a> Under review</p>
<p><strong>Summary</strong>ï¼šå¹´è½»ä¸€ä»£æ­£ç”Ÿæ´»åœ¨ä¸€ä¸ªæ™ºèƒ½æŠ€æœ¯æ—¥ç›Šæ™®åŠçš„ä¸–ç•Œï¼Œæ—©æœŸçš„äººå·¥æ™ºèƒ½ç´ å…»å¯¹äºåŸ¹å…»æ‰¹åˆ¤ç†è§£å’Œåº”å¯¹è¿™äº›æŠ€æœ¯çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ•™è‚²é¢†åŸŸå¾€å¾€æ³¨é‡å·¥å…·æ€§å­¦ä¹ ï¼Œä¼˜å…ˆè€ƒè™‘ä½¿ç”¨è€Œéç†è§£åŸºç¡€æ¦‚å¿µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–ã€å¯å¤åˆ¶çš„æ•™å­¦æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°å­¦æ•°å­¦çš„æ ¸å¿ƒå…ƒç´ æ¥åŸ¹å…»å°å­¦ç”Ÿçš„äººå·¥æ™ºèƒ½ç´ å…»ï¼Œå¼ºåŒ–æ¦‚å¿µåŒ–ã€æ•°æ®è¡¨ç¤ºã€åˆ†ç±»æ¨ç†å’Œäººå·¥æ™ºèƒ½è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹ä¸¤ä¸ªç­çº§å…±31åäº”å¹´çº§å­¦ç”Ÿè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œé€šè¿‡æµ‹è¯•å’Œåæµ‹è¯•æ»¡æ„åº¦è°ƒæŸ¥æ¥è¯„ä¼°å…¶æ•ˆæœã€‚ç»“æœæ˜¾ç¤ºå­¦ç”Ÿåœ¨æœ¯è¯­ç†è§£å’Œä½¿ç”¨ã€ç‰¹å¾æè¿°ã€é€»è¾‘æ¨ç†å’Œè¯„ä¼°æŠ€èƒ½æ–¹é¢æœ‰æ‰€æé«˜ï¼Œå¯¹å†³ç­–è¿‡ç¨‹åŠå…¶å±€é™æ€§æœ‰äº†æ›´æ·±çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å—åˆ°å­¦ç”Ÿçš„æ¬¢è¿ï¼Œå°¤å…¶æ˜¯é‚£äº›å°†äººå·¥æ™ºèƒ½æ¦‚å¿µä¸ç°å®ç”Ÿæ´»ç›¸ç»“åˆçš„æ´»åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ—©æœŸAIç´ å…»æ•™è‚²å¯¹å¹´è½»ä¸€ä»£ç†è§£å’Œåº”å¯¹æ™ºèƒ½æŠ€æœ¯è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•™è‚²é¢†åŸŸåœ¨AIæ•™è‚²ä¸Šè¿‡äºå¼ºè°ƒå·¥å…·ä½¿ç”¨ï¼Œå¿½è§†åŸºç¡€æ¦‚å¿µçš„ç†è§£ã€‚</li>
<li>æè®®é‡‡ç”¨ç»“æ„åŒ–ã€å¯å¤åˆ¶çš„æ•™å­¦æ–¹æ³•åŸ¹å…»å°å­¦ç”ŸAIç´ å…»ã€‚</li>
<li>é€šè¿‡å°å­¦æ•°å­¦çš„æ ¸å¿ƒå…ƒç´ æ¥æ•™æˆAIï¼Œå¼ºåŒ–æ¦‚å¿µåŒ–ã€æ•°æ®è¡¨ç¤ºã€åˆ†ç±»æ¨ç†å’ŒAIè¯„ä¼°æŠ€èƒ½ã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜è¯¥æ–¹æ³•èƒ½æé«˜å­¦ç”ŸAIæœ¯è¯­çš„ç†è§£å’Œä½¿ç”¨ã€ç‰¹å¾æè¿°ã€é€»è¾‘æ¨ç†å’Œè¯„ä¼°æŠ€èƒ½ã€‚</li>
<li>å­¦ç”Ÿèƒ½æ›´æ·±å…¥åœ°ç†è§£AIå†³ç­–è¿‡ç¨‹åŠå…¶å±€é™æ€§ã€‚</li>
<li>è¯¥æ•™å­¦æ–¹æ³•å—åˆ°å­¦ç”Ÿçš„æ¬¢è¿ï¼Œå°¤å…¶æ˜¯ä¸ç°å®ç”Ÿæ´»ç›¸ç»“åˆçš„æ´»åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7752b8edb622bc84b0c289f2940dfb74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53254af06a4822439f2a78c6da9a39a3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-Holmes-Can-MLLM-Think-Like-Holmes-for-Complex-Video-Reasoning"><a href="#Video-Holmes-Can-MLLM-Think-Like-Holmes-for-Complex-Video-Reasoning" class="headerlink" title="Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?"></a>Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a â€œHolmes-testâ€ for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in <a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes">https://github.com/TencentARC/Video-Holmes</a>. </p>
<blockquote>
<p>æœ€è¿‘æŠ¥é“äº†è®¤çŸ¥æ¨ç†ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒåœ¨å¢å¼ºå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†é¢‘æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•ã€‚è¿™ä¸€è¿›å±•è‡ªç„¶å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šè¿™äº›æ¨¡å‹èƒ½å¦ä»¥ä¸äººç±»ä¸“å®¶ç›¸å½“çš„æ–¹å¼è¿›è¡Œå¤æ‚çš„è§†é¢‘æ¨ç†ï¼Ÿç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°è§†è§‰æ„ŸçŸ¥å’Œå®šä½èƒ½åŠ›ï¼Œé—®é¢˜å¯ä»¥åŸºäºæ˜ç¡®çš„æç¤ºæˆ–å­¤ç«‹çš„è§†è§‰çº¿ç´¢æ¥å›ç­”ã€‚è¿™æ ·çš„åŸºå‡†æµ‹è¯•å¹¶ä¸èƒ½å®Œå…¨æ•æ‰åˆ°ç°å®ä¸–ç•Œæ¨ç†çš„å¤æ‚æ€§ï¼Œå…¶ä¸­äººç±»å¿…é¡»åœ¨å¾—å‡ºç»“è®ºä¹‹å‰ç§¯æå¯»æ‰¾ã€æ•´åˆå’Œåˆ†æå¤šä¸ªçº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Video-HolmesåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä»¥ç¦å°”æ‘©æ–¯ä¾¦æ¢çš„æ¨ç†è¿‡ç¨‹ä¸ºçµæ„Ÿï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚Video-HolmesåŒ…å«ä»ç²¾å¿ƒæŒ‘é€‰çš„270éƒ¨æ‚¬ç–‘çŸ­ç‰‡ä¸­è¡ç”Ÿå‡ºçš„1837ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†ä¸ƒä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡é¦–å…ˆè¯†åˆ«ç”µå½±ä¸­çš„å…³é”®äº‹ä»¶å’Œå› æœå…³ç³»ï¼Œç„¶åè®¾è®¡é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹ä¸»åŠ¨å®šä½å¹¶è¿æ¥æ•£å¸ƒåœ¨ä¸åŒè§†é¢‘ç‰‡æ®µä¸­çš„å¤šä¸ªç›¸å…³è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„MLLMsçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•´åˆä¿¡æ¯å’Œç»å¸¸é—æ¼å…³é”®çº¿ç´¢æ–¹é¢é‡åˆ°äº†å¾ˆå¤§çš„å›°éš¾ã€‚ä¾‹å¦‚ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹Gemini-2.5 Proå‡†ç¡®ç‡ä»…ä¸º45%ï¼Œå¤§å¤šæ•°æ¨¡å‹çš„å¾—åˆ†ä½äº40%ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡Video-Holmesä½œä¸ºå¤šæ¨¡æ€æ¨ç†çš„â€œç¦å°”æ‘©æ–¯æµ‹è¯•â€ï¼Œæ¿€åŠ±æ¨¡å‹åƒäººç±»ä¸€æ ·è¿›è¡Œæ¨ç†ï¼Œå¹¶å¼ºè°ƒè¿™ä¸€é¢†åŸŸçš„æŒç»­æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/TencentARC/Video-Holmeså‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21374v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes">https://github.com/TencentARC/Video-Holmes</a></p>
<p><strong>Summary</strong><br>è§†é¢‘æ¨ç†èƒ½åŠ›åœ¨å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­é€æ¸å—åˆ°å…³æ³¨ã€‚ä¸ºè¯„ä¼°æ¨¡å‹çš„å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›ï¼Œæ¨å‡ºäº†Video-HolmesåŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•ä»270éƒ¨æ‚¬ç–‘çŸ­ç‰‡ä¸­æ‰‹åŠ¨æ ‡æ³¨äº†å…³é”®äº‹ä»¶å’Œå› æœå…³ç³»ï¼Œè®¾è®¡äº†åŒ…å«ä¸ƒå¤§ä»»åŠ¡çš„é—®é¢˜é›†ã€‚å°½ç®¡æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•´åˆä¿¡æ¯å’Œå¯»æ‰¾å…³é”®çº¿ç´¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º45%ï¼Œå¤§å¤šæ•°æ¨¡å‹å¾—åˆ†ä½äº40%ã€‚Video-Holmesæ—¨åœ¨ä¸ºå¤šåª’ä½“æ¨ç†æä¾›ä¸€ä¸ªâ€œéœå§†æ–¯æµ‹è¯•â€ï¼Œå¼ºè°ƒæ¨¡å‹éœ€è¦åƒäººç±»ä¸€æ ·è¿›è¡Œæ¨ç†ï¼Œå¹¶çªå‡ºè¿™ä¸€é¢†åŸŸçš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-Holmesæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æµ‹è¯•åŒ…å«ä»æ‚¬ç–‘çŸ­ç‰‡ä¸­æ‰‹åŠ¨æ ‡æ³¨çš„ä¸ƒå¤§ä»»åŠ¡ï¼Œæ¶‰åŠå…³é”®äº‹ä»¶å’Œå› æœå…³ç³»çš„è¯†åˆ«ã€‚</li>
<li>å°½ç®¡æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•´åˆä¿¡æ¯å’Œå¯»æ‰¾å…³é”®çº¿ç´¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç›®å‰æœ€å¥½çš„æ¨¡å‹åœ¨Video-Holmesä¸Šçš„å‡†ç¡®ç‡ä¸º45%ï¼Œå¤§å¤šæ•°æ¨¡å‹å¾—åˆ†ä½äºæ­¤ã€‚</li>
<li>Video-Holmesæ—¨åœ¨ä½œä¸ºå¤šåª’ä½“æ¨ç†çš„â€œéœå§†æ–¯æµ‹è¯•â€ï¼Œé¼“åŠ±æ¨¡å‹åƒäººç±»ä¸€æ ·è¿›è¡Œæ¨ç†ã€‚</li>
<li>æ­¤åŸºå‡†æµ‹è¯•çš„å‘å¸ƒå¼ºè°ƒäº†å¤šåª’ä½“æ¨ç†é¢†åŸŸçš„æŒ‘æˆ˜å’Œå‘å±•éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b74c3036d99eab20c0ce9dd320234e5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf35c51100e2720e4073b5506e2ec078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01145b8139b58441f4073c336c97f759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e5a424d1f878d726fcfdece58832aae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning"><a href="#Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning" class="headerlink" title="Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning"></a>Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning</h2><p><strong>Authors:Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah</strong></p>
<p>Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the languageâ€™s low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies. </p>
<blockquote>
<p>è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰ä»ç„¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå­ŸåŠ æ‹‰è¯­çš„èµ„æºç›¸å¯¹è¾ƒå°‘ä»¥åŠéœ€è¦å¤šæ­¥éª¤æ¨ç†ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜æ—¶é¢ä¸´å›°éš¾ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºä¹‹å‰æ²¡æœ‰äººå¯¹å­ŸåŠ æ‹‰æ•°æ®é›†è¿›è¡Œæ ‡æ³¨æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚è¿™ä¸€å·®è·é™åˆ¶äº†å­ŸåŠ æ‹‰æ•°å­¦æ¨ç†çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«8792ä¸ªå¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ä»¥åŠæ‰‹åŠ¨ç¼–å†™çš„é€æ­¥è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è®¾è®¡è¿™ä¸ªæ•°æ®é›†æ˜¯ä¸ºäº†æ”¯æŒåœ¨è¯­è¨€è¡¨ç¤ºä¸è¶³çš„æƒ…å¢ƒä¸­è¿›è¡Œæ¨ç†è¯„ä¼°å¯¼å‘å’Œæ¨¡å‹å¼€å‘ã€‚ä½¿ç”¨SOMADHANæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-3.5 Turboã€LLaMAç³»åˆ—æ¨¡å‹ã€Deepseekå’ŒQwenç­‰ï¼Œé€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºä»¥åŠæœ‰æ— æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œæµ‹è¯•ã€‚æ€ç»´é“¾æç¤ºä¸€è‡´åœ°æé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­ã€‚LLaMA-3.3 70Båœ¨å°‘æ ·æœ¬æ€ç»´é“¾æç¤ºä¸‹å–å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°88%ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯æ¥é«˜æ•ˆå¾®è°ƒæ¨¡å‹ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿé€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜å¹¶å…·æœ‰æœ€å°çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›é«˜è´¨é‡æ¨ç†æ•°æ®é›†å’Œè§£å†³å¤æ‚æ•°å­¦æ–‡å­—é¢˜çš„å¯æ‰©å±•æ¡†æ¶æ¥å¡«è¡¥å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„ç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„å…¬å¹³ç ”ç©¶ï¼Œå¹¶æå‡æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21354v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç”±äºå­ŸåŠ æ‹‰è¯­çš„èµ„æºåŒ®ä¹å’Œå¤šæ­¥éª¤æ¨ç†çš„éœ€æ±‚ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥åº”å¯¹å¤æ‚é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«8792ä¸ªå¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜å’Œæ‰‹åŠ¨ç¼–å†™çš„é€æ­¥è§£ç­”ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒè¯­è¨€ä»£è¡¨æ€§ä¸è¶³çš„æƒ…å¢ƒä¸­çš„æ¨ç†è¯„ä¼°æ¨¡å‹å‘å±•ã€‚é€šè¿‡ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨è§£å†³éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„é—®é¢˜æ—¶è¡¨ç°å“è¶Šã€‚LLaMA-3.3 70Bé€šè¿‡æœ‰é™çš„è®¡ç®—æˆæœ¬ä½¿ç”¨LoRAæŠ€æœ¯å®ç°è¾ƒé«˜çš„å‡†ç¡®åº¦ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†å­ŸåŠ æ‹‰è¯­è‡ªç„¶è¯­è¨€å¤„ç†çš„ç©ºç™½ï¼Œæä¾›äº†é«˜è´¨é‡çš„æ¨ç†æ•°æ®é›†å’Œå¤æ‚çš„æ–‡å­—é¢˜è§£å†³æ¡†æ¶ã€‚ç›®æ ‡æ˜¯æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„å‡è¡¡ç ”ç©¶ï¼Œæé«˜æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜å¤„ç†æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå­ŸåŠ æ‹‰è¯­çš„èµ„æºåŒ®ä¹å’Œå¤šæ­¥éª¤æ¨ç†éœ€æ±‚ã€‚</li>
<li>SOMADHANæ•°æ®é›†åŒ…å«å¤æ‚å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜åŠå…¶æ‰‹åŠ¨é€æ­¥è§£ç­”ï¼Œæ—¨åœ¨æ”¯æŒè¯­è¨€ä»£è¡¨æ€§ä¸è¶³çš„æƒ…å¢ƒä¸­çš„æ¨ç†è¯„ä¼°æ¨¡å‹å‘å±•ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨è§£å†³éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„é—®é¢˜æ—¶è¡¨ç°å“è¶Šã€‚</li>
<li>LLaMA-3.3 70Bé€šè¿‡ç»“åˆCoTæç¤ºå’ŒLoRAæŠ€æœ¯å®ç°è¾ƒé«˜çš„å‡†ç¡®åº¦ã€‚</li>
<li>LoRAæŠ€æœ¯ç”¨äºé«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>è¯¥ç ”ç©¶å¡«è¡¥äº†å­ŸåŠ æ‹‰è¯­è‡ªç„¶è¯­è¨€å¤„ç†çš„ç©ºç™½ï¼Œæä¾›äº†ä¸€ä¸ªé‡è¦çš„æ•°æ®é›†å’Œæ¡†æ¶æ¥è§£å†³å¤æ‚çš„æ•°å­¦æ–‡å­—é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aedb13d51ece17fd5fda0c1ad861e90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee0512f5ac535ccfc71236625e198cd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133b208ecfed867a54569cb3c1d5d2fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MME-Reasoning-A-Comprehensive-Benchmark-for-Logical-Reasoning-in-MLLMs"><a href="#MME-Reasoning-A-Comprehensive-Benchmark-for-Logical-Reasoning-in-MLLMs" class="headerlink" title="MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs"></a>MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs</h2><p><strong>Authors:Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, Xiangyu Yue</strong></p>
<p>Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as &#96;&#96;thinking modeâ€™â€™ and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities. </p>
<blockquote>
<p>é€»è¾‘æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œä¹Ÿæ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ ¸å¿ƒèƒ½åŠ›ã€‚å°½ç®¡å¤šæ¨¡æ€æ¨ç†å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹æ˜ç¡®çš„é€»è¾‘æ¨ç†ç±»å‹åˆ†ç±»å’Œå¯¹æ¨ç†çš„æ¸…æ™°ç†è§£ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å…¨é¢è¯„ä¼°å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MME-Reasoningï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsæ¨ç†èƒ½åŠ›çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå…¶é—®é¢˜æ¶µç›–äº†ä¸‰ç§æ¨ç†ç±»å‹ï¼ˆå³å½’çº³æ¨ç†ã€æ¼”ç»æ¨ç†å’Œæº¯å› æ¨ç†ï¼‰ã€‚æˆ‘ä»¬ç²¾å¿ƒç­›é€‰æ•°æ®ï¼Œä»¥ç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½èƒ½æœ‰æ•ˆåœ°è¯„ä¼°æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯æ„ŸçŸ¥èƒ½åŠ›æˆ–çŸ¥è¯†å¹¿åº¦ï¼Œå¹¶æ‰©å±•è¯„ä¼°åè®®ä»¥æ¶µç›–å¯¹ä¸åŒé—®é¢˜çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“è¿›è¡Œå…¨é¢çš„é€»è¾‘æ¨ç†èƒ½åŠ›è¯„ä¼°æ—¶ï¼Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨é‡å¤§å±€é™æ€§ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMsåœ¨ç»¼åˆé€»è¾‘æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿæœ‰é™ï¼Œä¸åŒæ¨ç†ç±»å‹ä¹‹é—´çš„æ€§èƒ½ä¸å¹³è¡¡ç°è±¡å°¤ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä¸€äº›æ™®éè®¤ä¸ºèƒ½å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼ˆå¦‚â€œæ€è€ƒæ¨¡å¼â€å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼‰è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒé€»è¾‘æ¨ç†åœºæ™¯ä¸­çš„å…³é”®å±€é™æ€§å’Œæ€§èƒ½ä¸å¹³è¡¡é—®é¢˜ï¼Œä¸ºç†è§£å’Œè¯„ä¼°æ¨ç†èƒ½åŠ›æä¾›äº†å…¨é¢å’Œç³»ç»Ÿçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21327v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ¨å‡ºä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†â€”â€”MME-Reasoningï¼Œæ¶µç›–äº†å½’çº³ã€æ¼”ç»å’Œæº¯å› ä¸‰ç§æ¨ç†ç±»å‹çš„é—®é¢˜ã€‚å¯¹ç°æœ‰MLLMsçš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°ä»å­˜åœ¨æ˜¾è‘—å±€é™å’Œæ€§èƒ½ä¸å‡è¡¡çš„é—®é¢˜ã€‚æ·±å…¥åˆ†æäº†ä¸€äº›æå‡æ¨ç†èƒ½åŠ›çš„å¸¸ç”¨æ–¹æ³•ï¼Œå¦‚â€œæ€è€ƒæ¨¡å¼â€å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ç­‰ã€‚æ­¤ç ”ç©¶æ­ç¤ºäº†MLLMsåœ¨å¤šæ ·é€»è¾‘æ¨ç†åœºæ™¯ä¸‹çš„å…³é”®å±€é™å’Œæ€§èƒ½ä¸å‡è¡¡é—®é¢˜ï¼Œä¸ºç†è§£å’Œè¯„ä¼°æ¨ç†èƒ½åŠ›æä¾›äº†å…¨é¢ç³»ç»Ÿçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MME-ReasoningåŸºå‡†è¢«æ¨å‡ºï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å½’çº³ã€æ¼”ç»å’Œæº¯å› æ¨ç†ã€‚</li>
<li>ç°æœ‰MLLMsåœ¨é€»è¾‘æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>å³ä½¿åœ¨æœ€å…ˆè¿›çš„MLLMsä¸­ï¼Œæ¨ç†èƒ½åŠ›çš„è¡¨ç°ä¹Ÿä¸å‡è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒç±»å‹çš„æ¨ç†ä¹‹é—´ã€‚</li>
<li>æ·±å…¥åˆ†æäº†å¢å¼ºMLLMsæ¨ç†èƒ½åŠ›çš„å¸¸ç”¨æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†MLLMsåœ¨å¤šæ ·é€»è¾‘æ¨ç†åœºæ™¯ä¸‹çš„å…³é”®å±€é™ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºç†è§£å’Œè¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›æä¾›äº†å…¨é¢ç³»ç»Ÿçš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-062f31e6a2d802f0f12b925f44d51d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f750f2c2170d08eb8ed45191b54bd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2297c412eb5f01e954eca1ffe3d1c438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78de4d409a6aa5e88dc60841a785cb33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604e0a40b4c1af0065f68093e765485e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8cce7ac762bdcef4255b6286266451.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="rStar-Coder-Scaling-Competitive-Code-Reasoning-with-a-Large-Scale-Verified-Dataset"><a href="#rStar-Coder-Scaling-Competitive-Code-Reasoning-with-a-Large-Scale-Verified-Dataset" class="headerlink" title="rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale   Verified Dataset"></a>rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale   Verified Dataset</h2><p><strong>Authors:Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, Mao Yang</strong></p>
<p>Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at <a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar">https://github.com/microsoft/rStar</a>. </p>
<blockquote>
<p>æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä»£ç æ¨ç†èƒ½åŠ›ï¼Œä»æ ¹æœ¬ä¸Šå—é™äºé«˜è´¨é‡æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œå°¤å…¶æ˜¯é‚£äº›å…·æœ‰å¯éªŒè¯çš„è¾“å…¥è¾“å‡ºæµ‹è¯•ç”¨ä¾‹ï¼Œè¿™äº›ç”¨ä¾‹å¯¹äºå¤§è§„æ¨¡ä¸¥æ ¼è§£å†³æ–¹æ¡ˆéªŒè¯æ˜¯å¿…ä¸å¯å°‘çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†rStar-Coderï¼Œå®ƒé€šè¿‡æ„å»ºåŒ…å«41.8ä¸‡ä¸ªç«èµ›çº§ä»£ç é—®é¢˜ã€58ä¸‡ä¸ªé•¿æœŸæ¨ç†è§£å†³æ–¹æ¡ˆä»¥åŠä¸åŒéš¾åº¦ä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹çš„å¤§è§„æ¨¡éªŒè¯æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†LLMçš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚è¿™æ˜¯é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®å®ç°çš„ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬ç²¾é€‰ç«èµ›ç¼–ç¨‹ä»£ç é—®é¢˜å’Œæ ‡å‡†è§£å†³æ–¹æ¡ˆï¼Œåˆæˆæ–°çš„å¯è§£å†³é—®é¢˜ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯é çš„è¾“å…¥è¾“å‡ºæµ‹è¯•ç”¨ä¾‹åˆæˆç®¡é“ï¼Œå°†ç”Ÿæˆè§£è€¦ä¸ºä¸‰æ­¥è¾“å…¥ç”Ÿæˆæ–¹æ³•å’Œæœ‰æ•ˆçš„è¾“å‡ºæ ‡ç­¾äº’éªŒè¯æœºåˆ¶ï¼›ï¼ˆ3ï¼‰æˆ‘ä»¬ä¸ºé—®é¢˜å¢åŠ äº†é«˜è´¨é‡ã€æµ‹è¯•ç”¨ä¾‹éªŒè¯çš„é•¿æœŸæ¨ç†è§£å†³æ–¹æ¡ˆã€‚åœ¨Qwenæ¨¡å‹ï¼ˆ1.5B-14Bï¼‰ä¸Šçš„å¹¿æ³›å®éªŒï¼Œä»¥åŠå„ç§ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒrStar-Coderæ•°æ®é›†å…·æœ‰å“è¶Šæ€§ï¼Œå…¶æ€§èƒ½é¢†å…ˆï¼Œç”šè‡³åœ¨ä¸€äº›è¾ƒå°çš„æ¨¡å‹å¤§å°æ–¹é¢ä¹Ÿè¾¾åˆ°äº†å‰æ²¿æ¨ç†LLMçš„æ°´å¹³ã€‚åœ¨LiveCodeBenchä¸Šï¼ŒrStar-Coderå°†Qwen2.5-7Bçš„æ€§èƒ½ä»17.4%æé«˜åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„57.3%ï¼Œå°†Qwen2.5-14Bçš„æ€§èƒ½ä»23.3%æé«˜åˆ°62.5%ï¼Œè¶…è¿‡äº†o3-mini (low) 3.1%ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç¾å›½è®¡ç®—å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­ï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹å¹³å‡pass@1å‡†ç¡®ç‡è¾¾åˆ°äº†16.15%ï¼Œè¶…è¶Šäº†å‰æ²¿çš„QWQ-32Bã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/microsoft/rStarä¸Šå‘å¸ƒã€‚</a> </p>
</blockquote>
<p><strong>ç®€åŒ–ç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21297v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¾®è½¯æ¨å‡ºçš„rStar-Coderæ•°æ®é›†ï¼Œå®ƒé€šè¿‡æ„å»ºå¤§è§„æ¨¡éªŒè¯æ•°æ®é›†æ˜¾è‘—æé«˜äº†LLMçš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚æ•°æ®é›†åŒ…å«41.8ä¸‡ä¸ªç«èµ›çº§åˆ«çš„ä»£ç é—®é¢˜ï¼Œä»¥åŠä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹å’ŒéªŒè¯è¿‡çš„é•¿æ¨ç†è§£å†³æ–¹æ¡ˆã€‚rStar-Coderé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®å®ç°è¿™ä¸€ç›®æ ‡ï¼šä»ç«äº‰æ€§ç¼–ç¨‹ä¸­ç­›é€‰é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆåˆæˆæ–°é—®é¢˜ï¼›å¼•å…¥å¯é çš„è¾“å…¥-è¾“å‡ºæµ‹è¯•ç”¨ä¾‹åˆæˆç®¡é“ï¼›ä¸ºé—®é¢˜æä¾›é«˜è´¨é‡ã€æµ‹è¯•æ¡ˆä¾‹éªŒè¯çš„é•¿æ¨ç†è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒrStar-Coderæ•°æ®é›†åœ¨å¤šä¸ªä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rStar-Coderé€šè¿‡æ„å»ºå¤§è§„æ¨¡éªŒè¯æ•°æ®é›†æå‡äº†LLMçš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡41.8ä¸‡ä¸ªç«èµ›çº§åˆ«çš„ä»£ç é—®é¢˜ï¼Œæ¶µç›–ä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹å’ŒéªŒè¯è¿‡çš„é•¿æ¨ç†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>rStar-Coderé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒæ–¹æ³•å®ç°æ€§èƒ½æå‡ï¼šç­›é€‰ç«èµ›çº§ç¼–ç¨‹é—®é¢˜ã€åˆæˆæ–°çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶ä¸ºè¿™äº›é—®é¢˜æä¾›å¯é éªŒè¯çš„é•¿æ¨ç†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒrStar-Coderåœ¨å„ç§ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆã€‚åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒæ¯”æœ€å‰æ²¿çš„æ¨ç†LLMè¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚å®ƒå°¤å…¶èƒ½æ˜¾è‘—æé«˜å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨LiveCodeBenchä¸Šï¼Œæ¨¡å‹æ€§èƒ½ä»ä¸åˆ°å››åˆ†ä¹‹ä¸€æé«˜åˆ°è¶…è¿‡ä¸€åŠã€‚åœ¨ç¾å›½è®¡ç®—å¥¥æ—åŒ¹ç«èµ›ä¸Šï¼Œä¸€ä¸ªä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¶…è¿‡ç™¾åˆ†ä¹‹åå…­ç‚¹ä¸€äº”ï¼Œè¶…è¿‡äº†æœ€å‰æ²¿çš„å¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6fe01f5c03ac19337d89f54f52a62aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a24ccbbe969d374d8d12b6be39fdead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be7d0c9c5bb6dd84f3a071a355bdc2f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2619d813959d9af86a69315fc4ae7ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d331655456756f0d5bf2afdbfc2c649.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Walk-Before-You-Run-Concise-LLM-Reasoning-via-Reinforcement-Learning"><a href="#Walk-Before-You-Run-Concise-LLM-Reasoning-via-Reinforcement-Learning" class="headerlink" title="Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning"></a>Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning</h2><p><strong>Authors:Mingyang Song, Mao Zheng</strong></p>
<p>As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the modelâ€™s reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the â€œwalk before you runâ€ principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks. </p>
<blockquote>
<p>éšç€æµ‹è¯•æ—¶ç¼©æ”¾æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•çš„æ ¸å¿ƒç ”ç©¶å‰æ²¿ï¼Œå½“ä»£å…ˆè¿›çš„åè®­ç»ƒæ–¹æ³•è®ºè¶Šæ¥è¶Šå…³æ³¨æ‰©å±•é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰å“åº”çš„ç”Ÿæˆé•¿åº¦ï¼Œä»¥æé«˜å‘DeepSeek R1ç­‰æ€§èƒ½çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¸­å­˜åœ¨ä¸€ç§æŒç»­è¿‡åº¦æ€è€ƒçš„ç°è±¡ï¼Œè¡¨ç°ä¸ºé•¿CoTå“åº”ä¸­çš„è¿‡åº¦å†—ä½™æˆ–é‡å¤æ€è€ƒæ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨LLMä¸­å®ç°ç®€æ´æ¨ç†ï¼Œåä¸ºConciseRã€‚å…·ä½“æ¥è¯´ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ­¥éª¤ï¼Œæ—¨åœ¨é€šè¿‡å¸¦æœ‰å‰ªè¾‘æ›´é«˜å’ŒåŠ¨æ€é‡‡æ ·ç»„ä»¶çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO++ï¼‰æ¥æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ­¥éª¤ï¼Œé€šè¿‡é•¿åº¦æ„ŸçŸ¥ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆL-GRPOï¼‰æ˜ç¡®å¼ºè°ƒç®€æ´æ€§å¹¶æé«˜æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒConciseRä»…åœ¨æ ·æœ¬æ‰€æœ‰rolloutséƒ½æ­£ç¡®çš„æƒ…å†µä¸‹ä¼˜åŒ–å“åº”é•¿åº¦ï¼Œéµå¾ªâ€œèµ°åœ¨ä½ è·‘ä¹‹å‰â€çš„åŸåˆ™ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ConciseRæ¨¡å‹ç”Ÿæˆäº†æ›´ç®€æ´çš„CoTæ¨ç†å“åº”ï¼Œåœ¨AIME 2024ã€MATH-500ã€AMC 2023ã€Minervaå’ŒOlympiadç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¶…è¶Šäº†æ— å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸‹çš„æœ€æ–°å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21178v1">PDF</a> Ongoing Work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•ä¸­ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯çš„é‡è¦æ€§ã€‚é’ˆå¯¹å½“å‰å…ˆè¿›çš„è®­ç»ƒåæ–¹æ³•åœ¨æé«˜æ¨ç†èƒ½åŠ›æ—¶äº§ç”Ÿçš„è¿‡åº¦æ€è€ƒç°è±¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ConciseRã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO++ï¼‰æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡é•¿åº¦æ„ŸçŸ¥é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆL-GRPOï¼‰ä¿ƒè¿›ç®€æ´æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConciseRæ¨¡å‹åœ¨AIME 2024ã€MATH-500ã€AMC 2023ã€Minervaå’ŒOlympiadç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€è¿‘çš„å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¸­ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯æ˜¯å…³é”®çš„ç ”ç©¶å‰æ²¿ã€‚</li>
<li>å½“ä»£å…ˆè¿›çš„è®­ç»ƒåæ–¹æ³•è‡´åŠ›äºé€šè¿‡æ‰©å±•ç”Ÿæˆé•¿åº¦æ¥æé«˜Chain-of-Thoughtï¼ˆCoTï¼‰å“åº”çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨CoTå“åº”ä¸­å‡ºç°äº†è¿‡åº¦æ€è€ƒç°è±¡ï¼Œè¡¨ç°ä¸ºå†—ä½™æˆ–é‡å¤æ€è€ƒæ¨¡å¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ConciseRæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ConciseRåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µä¿ƒè¿›ç®€æ´æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ConciseRåªåœ¨æ‰€æœ‰æ ·æœ¬çš„æ‰€æœ‰rolloutæ­£ç¡®åä¼˜åŒ–å“åº”é•¿åº¦ï¼Œéµå¾ªâ€œå…ˆèµ°å†è·‘â€çš„åŸåˆ™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17f8ccd85b55cfeb130f0150b9a031fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f990dd9561de9b7962f4eeb4ddbdf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44258cc4f9135acea1f9506ae9911a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7696d5d7ea24a341649f090568a9581.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-Think-But-Not-In-Your-Flow-Reasoning-Level-Personalization-for-Black-Box-Large-Language-Models"><a href="#LLMs-Think-But-Not-In-Your-Flow-Reasoning-Level-Personalization-for-Black-Box-Large-Language-Models" class="headerlink" title="LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for   Black-Box Large Language Models"></a>LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for   Black-Box Large Language Models</h2><p><strong>Authors:Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, Dongha Lee</strong></p>
<p>Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMsâ€“served via APIs without access to model internalsâ€“are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the modelâ€™s reasoning process with a userâ€™s personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå¹¶ç°åœ¨å¹¿æ³›åº”ç”¨äºå®é™…ä¸–ç•Œåº”ç”¨ã€‚å…¶ä¸­ï¼Œé»‘ç›’LLMï¼ˆé€šè¿‡APIæä¾›æœåŠ¡ï¼Œæ— æ³•è®¿é—®æ¨¡å‹å†…éƒ¨ï¼‰ç”±äºå…¶å¯æ‰©å±•æ€§å’Œæ˜“äºéƒ¨ç½²çš„ç‰¹ç‚¹è€Œå°¤å…¶å æ®ä¸»å¯¼åœ°ä½ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿå¿½ç•¥ä¸ªäººåå¥½å’Œæ¨ç†é£æ ¼çš„é€šç”¨å“åº”ã€‚è¿™å¼•å‘äº†äººä»¬å¯¹é»‘ç›’LLMä¸ªæ€§åŒ–çš„å…´è¶£ä¸æ–­å¢é•¿ï¼Œå…¶ç›®æ ‡æ˜¯é’ˆå¯¹ç”¨æˆ·ç‰¹å®šä¸Šä¸‹æ–‡å®šåˆ¶æ¨¡å‹è¾“å‡ºï¼Œè€Œä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºå“åº”çº§ä¸ªæ€§åŒ–ï¼Œè¯•å›¾åŒ¹é…æœ€ç»ˆè¾“å‡ºï¼Œè€Œæ²¡æœ‰å¯¹ä¸ªäººçš„æ€è€ƒè¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RPMï¼Œä¸€ä¸ªç”¨äºæ¨ç†çº§ä¸ªæ€§åŒ–çš„æ¡†æ¶ï¼Œå®ƒå°†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸ç”¨æˆ·ä¸ªæ€§åŒ–çš„é€»è¾‘å¯¹é½ã€‚RPMé¦–å…ˆé€šè¿‡ä»ç”¨æˆ·å†å²ä¸­æå–å’Œåˆ†ç»„å½±å“å“åº”çš„ç‰¹å¾æ¥æ„å»ºç”¨æˆ·ç‰¹å®šçš„ç»Ÿè®¡å› ç´ ã€‚ç„¶åï¼Œå®ƒå»ºç«‹åæ˜ è¿™äº›å› å­å¦‚ä½•åœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨çš„ä¸ªæ€§åŒ–æ¨ç†è·¯å¾„ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒRPMé€šè¿‡ç‰¹å¾çº§åˆ«çš„ç›¸ä¼¼æ€§æ£€ç´¢ä¸æ–°æŸ¥è¯¢ç›¸åŒ¹é…çš„æ¨ç†å¯¹é½ç¤ºä¾‹ï¼Œå¹¶åœ¨ç»“æ„åŒ–å› ç´ å’Œæ£€ç´¢åˆ°çš„æ¨ç†è·¯å¾„çš„æ¡ä»¶ä¸‹è¿›è¡Œæ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéµå¾ªç”¨æˆ·ç‰¹å®šçš„æ¨ç†è½¨è¿¹ã€‚è¿™ç§æ¨ç†çº§ä¸ªæ€§åŒ–é€šè¿‡ç»“æ„åŒ–ä¿¡æ¯å°†æ¨¡å‹è¾“å‡ºæ ¹æ¤äºç”¨æˆ·ç‰¹å®šçš„é€»è¾‘ä¸­ï¼Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRPMå§‹ç»ˆä¼˜äºå“åº”çº§ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œè¯æ˜äº†åœ¨é»‘ç›’LLMä¸­è¿›è¡Œæ¨ç†çº§ä¸ªæ€§åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ã€‚å…¶ä¸­ï¼Œé»‘ç›’LLMï¼ˆé€šè¿‡APIæä¾›æœåŠ¡ï¼Œæ— æ³•è®¿é—®æ¨¡å‹å†…éƒ¨ï¼‰å› å…¶å¯æ‰©å±•æ€§å’Œæ˜“äºéƒ¨ç½²çš„ç‰¹ç‚¹å°¤å…¶å æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸äº§ç”Ÿå¿½ç•¥ä¸ªäººåå¥½å’Œæ¨ç†é£æ ¼çš„é€šç”¨å“åº”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†RPMæ¡†æ¶ï¼Œå®ç°æ¨ç†çº§ä¸ªæ€§åŒ–ï¼Œä½¿æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸ç”¨æˆ·ä¸ªæ€§åŒ–é€»è¾‘ç›¸åŒ¹é…ã€‚RPMé€šè¿‡æå–å’Œåˆ†ç»„ç”¨æˆ·å†å²ä¸­çš„å“åº”å½±å“å› ç´ æ¥æ„å»ºç”¨æˆ·ç‰¹å®šå› ç´ ï¼Œå¹¶å»ºç«‹åæ˜ è¿™äº›å› ç´ å¦‚ä½•åœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨çš„ä¸ªæ€§åŒ–æ¨ç†è·¯å¾„ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒRPMé€šè¿‡ç‰¹å¾çº§åˆ«ç›¸ä¼¼æ€§æ£€ç´¢ä¸æ¨ç†ç›¸åŒ¹é…çš„ç¤ºä¾‹ï¼Œå¹¶åœ¨ç»“æ„åŒ–å› ç´ å’Œæ£€ç´¢åˆ°çš„æ¨ç†è·¯å¾„ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½¿æ¨¡å‹éµå¾ªç”¨æˆ·ç‰¹å®šçš„æ¨ç†è½¨è¿¹ã€‚è¿™ç§æ¨ç†çº§ä¸ªæ€§åŒ–é€šè¿‡ç»“æ„åŒ–ä¿¡æ¯æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä½¿æ¨¡å‹è¾“å‡ºæ ¹æ¤äºç”¨æˆ·ç‰¹å®šé€»è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å¹¿æ³›åº”ç”¨ã€‚</li>
<li>é»‘ç›’LLMå› å…¶å¯æ‰©å±•æ€§å’Œæ˜“äºéƒ¨ç½²è€Œå°¤ä¸ºå—æ¬¢è¿ï¼Œä½†ç¼ºä¹ä¸ªæ€§åŒ–å“åº”ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å“åº”çº§ä¸ªæ€§åŒ–ï¼Œå°è¯•åŒ¹é…æœ€ç»ˆè¾“å‡ºï¼Œä½†æœªå»ºæ¨¡ä¸ªäººæ€ç»´è¿‡ç¨‹ã€‚</li>
<li>æå‡ºRPMæ¡†æ¶å®ç°æ¨ç†çº§ä¸ªæ€§åŒ–ï¼Œä¸ç”¨æˆ·çš„ä¸ªæ€§åŒ–é€»è¾‘ç›¸åŒ¹é…ã€‚</li>
<li>RPMæ„å»ºç”¨æˆ·ç‰¹å®šå› ç´ å¹¶å»ºç«‹ä¸ªæ€§åŒ–æ¨ç†è·¯å¾„ï¼Œåæ˜ ç”¨æˆ·å†å²ä¸­çš„å“åº”å½±å“å› ç´ å’Œä¸Šä¸‹æ–‡ä½¿ç”¨æ–¹å¼ã€‚</li>
<li>RPMé€šè¿‡ç‰¹å¾çº§åˆ«ç›¸ä¼¼æ€§æ£€ç´¢æ¨ç†åŒ¹é…çš„ç¤ºä¾‹ï¼Œå¹¶åœ¨ç»“æ„åŒ–å› ç´ å’Œæ£€ç´¢åˆ°çš„æ¨ç†è·¯å¾„ä¸Šè¿›è¡Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a3733893091275df5028f5b72420025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1521ddac4df3960b0637c7bf407a7c87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bc16abc5a04b39fb723ca11837e6b89.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Large-Language-Model-Inference-with-Neural-Block-Linearization"><a href="#Efficient-Large-Language-Model-Inference-with-Neural-Block-Linearization" class="headerlink" title="Efficient Large Language Model Inference with Neural Block Linearization"></a>Efficient Large Language Model Inference with Neural Block Linearization</h2><p><strong>Authors:Mete Erdogan, Francesco Tonin, Volkan Cevher</strong></p>
<p>The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ¨ç†éœ€æ±‚åœ¨å…¶éƒ¨ç½²è¿‡ç¨‹ä¸­å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¥ç»å—çº¿æ€§åŒ–ï¼ˆNBLï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç”¨åŸºäºçº¿æ€§æœ€å°å‡æ–¹è¯¯å·®ä¼°è®¡çš„çº¿æ€§è¿‘ä¼¼å€¼æ›¿æ¢è‡ªæ³¨æ„åŠ›å±‚æ¥åŠ é€ŸTransformeræ¨¡å‹çš„æ¨ç†ã€‚NBLåˆ©ç”¨å…¸å‹ç›¸å…³æ€§åˆ†ææ¥è®¡ç®—è¿‘ä¼¼è¯¯å·®çš„ç†è®ºä¸Šé™ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªç•Œé™ä½œä¸ºæ›¿ä»£æ ‡å‡†ï¼Œé€‰æ‹©çº¿æ€§åŒ–è¯¯å·®æœ€ä½çš„LLMå±‚ã€‚NBLå¯ä»¥é«˜æ•ˆåº”ç”¨äºé¢„è®­ç»ƒçš„LLMï¼Œæ— éœ€å¾®è°ƒã€‚åœ¨å®éªŒä¸­ï¼ŒNBLåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„è®¡ç®—é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›ã€‚ä¾‹å¦‚ï¼Œå°†NBLåº”ç”¨äºDeepSeek-R1-Distill-Llama-8Bçš„12ä¸ªè‡ªæ³¨æ„åŠ›å±‚ï¼Œåœ¨ä»…ç‰ºç‰²ä¸åˆ°1%å‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†32%ï¼Œä½¿å…¶æˆä¸ºæé«˜LLMæ¨ç†æ•ˆç‡çš„ä¸€ç§çµæ´»ä¸”å…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œå—çº¿æ€§åŒ–ï¼ˆNBLï¼‰æ˜¯ä¸€ç§ç”¨äºåŠ é€ŸåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚å®ƒé€šè¿‡åˆ©ç”¨çº¿æ€§æœ€å°å‡æ–¹è¯¯å·®ä¼°è®¡å™¨äº§ç”Ÿçš„çº¿æ€§è¿‘ä¼¼æ›¿æ¢è‡ªæ³¨æ„åŠ›å±‚æ¥å®ç°æ¨ç†åŠ é€Ÿã€‚NBLä½¿ç”¨å…¸å‹ç›¸å…³æ€§åˆ†æè®¡ç®—è¿‘ä¼¼è¯¯å·®çš„ç†è®ºä¸Šé™ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºæ›¿æ¢æ ‡å‡†ï¼Œé€‰æ‹©çº¿æ€§åŒ–è¯¯å·®æœ€ä½çš„LLMå±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒNBLåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼Œå°†NBLåº”ç”¨äºDeepSeek-R1-Distill-Llama-8Bçš„12ä¸ªè‡ªæ³¨æ„åŠ›å±‚ï¼Œå¯åœ¨ä»…æŸå¤±ä¸åˆ°1%å‡†ç¡®ç‡çš„æƒ…å†µä¸‹æé«˜32%çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¥ç»ç½‘ç»œå—çº¿æ€§åŒ–ï¼ˆNBLï¼‰æ—¨åœ¨åŠ é€ŸåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>NBLé€šè¿‡ç”¨çº¿æ€§æœ€å°å‡æ–¹è¯¯å·®ä¼°è®¡äº§ç”Ÿçš„çº¿æ€§è¿‘ä¼¼æ›¿æ¢è‡ªæ³¨æ„åŠ›å±‚æ¥å®ç°æ¨ç†åŠ é€Ÿã€‚</li>
<li>NBLä½¿ç”¨å…¸å‹ç›¸å…³æ€§åˆ†æè®¡ç®—ç†è®ºä¸Šçš„è¿‘ä¼¼è¯¯å·®ä¸Šé™ï¼Œä½œä¸ºé€‰æ‹©æ›¿æ¢å±‚çš„æ ‡å‡†ã€‚</li>
<li>NBLå¯é«˜æ•ˆåº”ç”¨äºé¢„è®­ç»ƒLLMï¼Œæ— éœ€å¾®è°ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒNBLåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„è®¡ç®—é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›ã€‚</li>
<li>åœ¨DeepSeek-R1-Distill-Llama-8Bæ¨¡å‹ä¸­åº”ç”¨NBLï¼Œå¯æé«˜32%çš„æ¨ç†é€Ÿåº¦ï¼Œä»…æŸå¤±ä¸åˆ°1%çš„å‡†ç¡®ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67a3fc631e80a55cf8b1a59a809caf68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10b77bb3310666f6dcd53cffcaae86a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93cb873dd5ef89d79d26e717f56d0e6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3306165221fc39ab37f32e3b939f8ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a34df7e3921e03ee9ed027887148d69.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Def-DTS-Deductive-Reasoning-for-Open-domain-Dialogue-Topic-Segmentation"><a href="#Def-DTS-Deductive-Reasoning-for-Open-domain-Dialogue-Topic-Segmentation" class="headerlink" title="Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation"></a>Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation</h2><p><strong>Authors:Seungmin Lee, Yongsang Yoo, Minhwa Jung, Min Song</strong></p>
<p>Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS. </p>
<blockquote>
<p>å¯¹è¯ä¸»é¢˜åˆ†æ®µï¼ˆDTSï¼‰æ—¨åœ¨å°†å¯¹è¯åˆ’åˆ†ä¸ºè¿è´¯çš„æ®µè½ã€‚DTSåœ¨å„ç§NLPä¸‹æ¸¸ä»»åŠ¡ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†å®ƒä¹Ÿé¢ä¸´ç€ä¸€äº›é•¿æœŸå­˜åœ¨çš„é—®é¢˜ï¼šæ•°æ®çŸ­ç¼ºã€æ ‡ç­¾æ¨¡ç³Šä»¥åŠæœ€è¿‘æå‡ºçš„è§£å†³æ–¹æ¡ˆçš„å¤æ‚æ€§ä¸æ–­å¢åŠ ã€‚å¦ä¸€æ–¹é¢ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ¨ç†ç­–ç•¥æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬å¾ˆå°‘è¢«åº”ç”¨äºDTSã€‚æœ¬æ–‡ä»‹ç»äº†Def-DTSï¼šåŸºäºå¼€æ”¾åŸŸå¯¹è¯ä¸»é¢˜çš„å½’çº³æ¨ç†åˆ†æ®µæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºLLMçš„å¤šæ­¥å½’çº³æ¨ç†æ¥æé«˜DTSçš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ä¸­é—´ç»“æœè¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç»“æ„åŒ–æç¤ºæ–¹æ³•è¿›è¡ŒåŒå‘ä¸Šä¸‹æ–‡æ‘˜è¦ã€è¯è¯­æ„å›¾åˆ†ç±»å’Œå½’çº³ä¸»é¢˜è½¬ç§»æ£€æµ‹ã€‚åœ¨æ„å›¾åˆ†ç±»è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé¢†åŸŸæ— å…³çš„å¯¹è¯æ„å›¾åˆ†ç±»çš„å¯æ¨å¹¿æ„å›¾åˆ—è¡¨ã€‚åœ¨å„ç§å¯¹è¯åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒDef-DTSå§‹ç»ˆä¼˜äºä¼ ç»Ÿå’Œæœ€æ–°æ–¹æ³•ï¼Œæ¯ä¸ªå­ä»»åŠ¡éƒ½å¯¹æ€§èƒ½æå‡æœ‰æ‰€è´¡çŒ®ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡å°‘ç¬¬äºŒç±»é”™è¯¯æ–¹é¢ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†è‡ªåŠ¨æ ‡ç­¾çš„æ½œåŠ›ï¼Œå¼ºè°ƒLLMæ¨ç†æŠ€æœ¯åœ¨DTSä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21033v1">PDF</a> 19 pages, 3 figures, Accepted to Findings of the ACL 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†Def-DTSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¼”ç»æ¨ç†ï¼Œæ—¨åœ¨æé«˜å¯¹è¯ä¸»é¢˜åˆ†å‰²çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–æç¤ºè¿›è¡ŒåŒå‘ä¸Šä¸‹æ–‡æ‘˜è¦ã€è¯è¯­æ„å›¾åˆ†ç±»å’Œæ¼”ç»ä¸»é¢˜è½¬ç§»æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDef-DTSåœ¨å¤šç§å¯¹è¯åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºä¼ ç»Ÿå’Œæœ€æ–°çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å‡å°‘äº†ç¬¬äºŒç±»é”™è¯¯çš„å‡ºç°ã€‚åŒæ—¶æ¢è®¨äº†è‡ªåŠ¨æ ‡æ³¨çš„æ½œåŠ›ï¼Œå¼ºè°ƒLLMæ¨ç†æŠ€æœ¯åœ¨DTSä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Def-DTSæ–¹æ³•æ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¼”ç»æ¨ç†æé«˜å¯¹è¯ä¸»é¢˜åˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–æç¤ºè¿›è¡ŒåŒå‘ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œæœ‰åŠ©äºç†è§£å¯¹è¯çš„è¿è´¯æ€§å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚</li>
<li>Def-DTSåŒ…æ‹¬è¯è¯­æ„å›¾åˆ†ç±»çš„ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†é€šç”¨çš„æ„å›¾åˆ—è¡¨ï¼Œç”¨äºéç‰¹å®šé¢†åŸŸçš„å¯¹è¯æ„å›¾åˆ†ç±»ã€‚</li>
<li>æ¼”ç»ä¸»é¢˜è½¬ç§»æ£€æµ‹æ˜¯Def-DTSçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæœ‰åŠ©äºå‡†ç¡®è¯†åˆ«å¯¹è¯ä¸»é¢˜çš„è½¬å˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDef-DTSåœ¨å¤šç§å¯¹è¯åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¼ ç»Ÿå’Œæœ€æ–°çš„æ–¹æ³•ã€‚</li>
<li>Def-DTSé€šè¿‡å‡å°‘ç¬¬äºŒç±»é”™è¯¯ï¼Œæé«˜äº†å¯¹è¯ä¸»é¢˜åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c71bf57d5cf44744b9dcf424373dbcaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26ca14a337329cb98efdaea65a2593ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819d5efdc144be34311fc6a378eef33a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06e0b091c9f50f6524da8eb9f4a78edc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e697fe3ff619bb9c795d2acfdca7ba01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d1853595dd997ca37ca4684fa3d347.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reason-Align-Respond-Aligning-LLM-Reasoning-with-Knowledge-Graphs-for-KGQA"><a href="#Reason-Align-Respond-Aligning-LLM-Reasoning-with-Knowledge-Graphs-for-KGQA" class="headerlink" title="Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for   KGQA"></a>Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for   KGQA</h2><p><strong>Authors:Xiangqing Shen, Fanfan Wang, Rui Xia</strong></p>
<p>LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¸¸å¸¸å‡ºç°å¹»è§‰ï¼Œç¼ºä¹å¯é çš„çš„äº‹å®åŸºç¡€ã€‚åŒæ—¶ï¼ŒçŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æä¾›äº†ç»“æ„åŒ–çš„äº‹å®çŸ¥è¯†ï¼Œä½†ç¼ºä¹LLMsçš„çµæ´»æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-Align-Respondï¼ˆRARï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ•´åˆäº†LLMæ¨ç†å’ŒçŸ¥è¯†å›¾è°±ï¼Œç”¨äºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šç”Ÿæˆç±»ä¼¼äººç±»çš„æ¨ç†é“¾çš„æ¨ç†å™¨ï¼Œå°†è¿™äº›é“¾æ˜ å°„åˆ°æœ‰æ•ˆçš„çŸ¥è¯†å›¾è°±è·¯å¾„çš„å¯¹é½å™¨ï¼Œä»¥åŠåˆæˆæœ€ç»ˆç­”æ¡ˆçš„å“åº”å™¨ã€‚æˆ‘ä»¬å°†è¿™ä¸ªè¿‡ç¨‹åˆ¶å®šä¸ºæ¦‚ç‡æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥ç®—æ³•é€šè¿‡è¿­ä»£æ”¹è¿›æ¨ç†é“¾å’ŒçŸ¥è¯†è·¯å¾„ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†RARçš„æœ‰æ•ˆæ€§ï¼Œåœ¨WebQSPå’ŒCWQä¸Šçš„å‘½ä¸­ç‡å¾—åˆ†åˆ†åˆ«ä¸º93.3%å’Œ91.0%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚äººç±»è¯„ä¼°è¯å®ï¼ŒRARç”Ÿæˆçš„æ¨ç†é“¾é«˜è´¨é‡ä¸”å¯è§£é‡Šæ€§å¼ºï¼Œä¸çŸ¥è¯†å›¾è°±è·¯å¾„å¾ˆå¥½åœ°å¯¹é½ã€‚æ­¤å¤–ï¼ŒRARå…·æœ‰è¾ƒå¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20971v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsä¸çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ç»“åˆå±•ç°å‡ºè‰²æ€§èƒ½ã€‚æ–°æ¡†æ¶Reason-Align-Respondï¼ˆRARï¼‰æ•´åˆäº†LLMæ¨ç†ä¸çŸ¥è¯†å›¾è°±ï¼Œç”¨äºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ã€‚RARåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šç”Ÿæˆäººç±»å¼æ¨ç†é“¾çš„Reasonerï¼Œå°†è¿™äº›é“¾æ˜ å°„åˆ°æœ‰æ•ˆKGè·¯å¾„çš„Alignerï¼Œä»¥åŠåˆæˆæœ€ç»ˆç­”æ¡ˆçš„Respondã€‚æ­¤è¿‡ç¨‹è¢«å…¬å¼åŒ–ä¸ºæ¦‚ç‡æ¨¡å‹å¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œæé«˜äº†æ€§èƒ½å¹¶å±•ç¤ºå‡ºå¼ºæ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒè¡¨æ˜RARè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsä¸çŸ¥è¯†å›¾è°±ç»“åˆèƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚</li>
<li>RARæ¡†æ¶æ•´åˆäº†LLMæ¨ç†ä¸çŸ¥è¯†å›¾è°±ç”¨äºKGQAä»»åŠ¡ã€‚</li>
<li>RARåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šReasonerï¼ŒAlignerå’ŒResponderã€‚</li>
<li>RARé‡‡ç”¨æ¦‚ç‡æ¨¡å‹å¹¶ä½¿ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>RARå®ç°é«˜æ°´å¹³æ€§èƒ½ï¼Œè¾¾åˆ°WebQSPå’ŒCWQåŸºå‡†æµ‹è¯•çš„æœ€ä¼˜æ°´å¹³ã€‚</li>
<li>RARç”Ÿæˆé«˜è´¨é‡ã€å¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œä¸äººç±»çŸ¥è¯†å›¾è°±è·¯å¾„å¯¹é½è‰¯å¥½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c76d03befd9b7706fae6c7b885661816.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6faffb4a0674f383477c77a94c61b5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3701fc66f2d03e4ce67c4d6b335f44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca26a6948ebee23d7fb996c6579190a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cross-from-Left-to-Right-Brain-Adaptive-Text-Dreamer-for-Vision-and-Language-Navigation"><a href="#Cross-from-Left-to-Right-Brain-Adaptive-Text-Dreamer-for-Vision-and-Language-Navigation" class="headerlink" title="Cross from Left to Right Brain: Adaptive Text Dreamer for   Vision-and-Language Navigation"></a>Cross from Left to Right Brain: Adaptive Text Dreamer for   Vision-and-Language Navigation</h2><p><strong>Authors:Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li</strong></p>
<p>Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{<a target="_blank" rel="noopener" href="https://github.com/zhangpingrui/Adaptive-Text-Dreamer%7D%7Bhere%7D">https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}</a>. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„æƒ…å†µä¸‹éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå¯¼èˆªï¼Œè¿™ä½¿å¾—æ„ŸçŸ¥ä¸è¯­è¨€çš„å¯¹é½å˜å¾—å›°éš¾ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡æƒ³è±¡æœªæ¥çš„åœºæ™¯æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬ä¾èµ–äºåŸºäºè§†è§‰çš„åˆæˆï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œç»†èŠ‚å†—ä½™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è¯­è¨€å½¢å¼æ¥é€‚åº”æ€§åœ°æƒ³è±¡å…³é”®ç¯å¢ƒè¯­ä¹‰ï¼Œä»è€Œå®ç°æ›´å¯é å’Œé«˜æ•ˆçš„ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”æ–‡æœ¬æ¢¦æƒ³å®¶ï¼ˆATDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŒåˆ†æ”¯è‡ªæˆ‘å¼•å¯¼æƒ³è±¡ç­–ç•¥ã€‚ATDçš„è®¾è®¡å…·æœ‰äººç±»å·¦å³è„‘æ¶æ„ï¼Œå…¶ä¸­å·¦è„‘ä¸“æ³¨äºé€»è¾‘æ•´åˆï¼Œå³è„‘è´Ÿè´£å¯¹æœªæ¥åœºæ™¯è¿›è¡Œæƒ³è±¡é¢„æµ‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªå¾®è°ƒäº†å·¦å³è„‘ä¸­çš„Q-formerï¼Œä»¥æœ‰æ•ˆæ¿€æ´»LLMä¸­çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œä»è€Œåœ¨å¯¼èˆªè¿‡ç¨‹ä¸­å®ç°é€»è¾‘å’Œæƒ³è±¡çš„åŠ¨æ€æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§äº¤å‰äº¤äº’æœºåˆ¶æ¥è§„èŒƒæƒ³è±¡è¾“å‡ºå¹¶å°†å…¶æ³¨å…¥å¯¼èˆªä¸“å®¶æ¨¡å—ï¼Œä½¿ATDèƒ½å¤Ÿè”åˆåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›å’Œå¯¼èˆªæ¨¡å‹çš„ä¸“é•¿ã€‚æˆ‘ä»¬åœ¨R2RåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒATDä»¥è¾ƒå°‘çš„å‚æ•°å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç è¯¦è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhangpingrui/Adaptive-Text-Dreamer">https://github.com/zhangpingrui/Adaptive-Text-Dreamer</a>ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç¿»è¯‘</strong>ï¼ˆæ›´ç®€æ´çš„ç‰ˆæœ¬ï¼‰ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20897v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬è®¨è®ºäº†Vision-and-Language Navigationçš„é—®é¢˜ï¼ŒåŒ…æ‹¬é€šè¿‡è¯­è¨€æŒ‡å¯¼è¿›è¡Œå¯¼èˆªçš„éš¾åº¦å’ŒæŒ‘æˆ˜ã€‚æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”æ–‡æœ¬æ¢¦æƒ³å®¶æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºè¯­è¨€å½¢å¼çš„æœªæ¥åœºæ™¯æƒ³è±¡ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬å’Œå†—ä½™ç»†èŠ‚ã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªæˆ‘å¼•å¯¼ç­–ç•¥ç”Ÿæˆå·¦å³è„‘ååŒå·¥ä½œå®ç°é€»è¾‘æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ã€‚åœ¨R2RåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚å…·ä½“ä¿¡æ¯å¯è®¿é—®æä¾›çš„é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision-and-Language Navigationé¢ä¸´æ„ŸçŸ¥ä¸è¯­è¨€å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>è‡ªé€‚åº”æ–‡æœ¬æ¢¦æƒ³å®¶æ¨¡å‹é€šè¿‡åŸºäºè¯­è¨€å½¢å¼çš„æœªæ¥åœºæ™¯æƒ³è±¡è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯è‡ªæˆ‘å¼•å¯¼ç­–ç•¥ï¼Œæ¨¡æ‹Ÿäººç±»å·¦å³è„‘åŠŸèƒ½å®ç°é€»è¾‘æ¨ç†å’Œæƒ³è±¡ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº¤å‰äº’åŠ¨æœºåˆ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯¼èˆªæ¨¡å‹çš„ä¸“é•¿ã€‚</li>
<li>åœ¨R2RåŸºå‡†æµ‹è¯•ä¸­ï¼Œè‡ªé€‚åº”æ–‡æœ¬æ¢¦æƒ³å®¶æ¨¡å‹æ€§èƒ½è¾¾åˆ°æœ€ä½³æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ef9068995f70446cb1de55746ac5d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f77dfa94c1ecef5ba5ff08ac6a4cffc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b6c15b1fd6e7e3bc4e633f3031cea8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Step-Wise-Formal-Verification-for-LLM-Based-Mathematical-Problem-Solving"><a href="#Step-Wise-Formal-Verification-for-LLM-Based-Mathematical-Problem-Solving" class="headerlink" title="Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving"></a>Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving</h2><p><strong>Authors:Kuo Zhou, Lu Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated formidable capabilities in solving mathematical problems, yet they may still commit logical reasoning and computational errors during the problem-solving process. Thus, this paper proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for formally verifying the correctness of the solutions generated by large language models. Our framework first utilizes a Formalizer which employs an LLM to translate a natural language solution into a formal context. Afterward, our Critic (which integrates various external tools such as a Computer Algebra System and an SMT solver) evaluates the correctness of each statement within the formal context, and when a statement is incorrect, our Critic provides corrective feedback. We empirically investigate the effectiveness of MATH-VF in two scenarios: 1) Verification: MATH-VF is utilized to determine the correctness of a solution to a given problem. 2) Refinement: When MATH-VF identifies errors in the solution generated by an LLM-based solution generator for a given problem, it submits the corrective suggestions proposed by the Critic to the solution generator to regenerate the solution. We evaluate our framework on widely used mathematical benchmarks: MATH500 and ProcessBench, demonstrating the superiority of our approach over existing approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ä»å¯èƒ½å‡ºç°é€»è¾‘æ¨ç†å’Œè®¡ç®—é”™è¯¯ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºMATH-VFçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªæ ¼å¼åŒ–å™¨å’Œä¸€ä¸ªæ‰¹åˆ¤å®¶ï¼Œç”¨äºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆä½¿ç”¨ä¸€ä¸ªæ ¼å¼åŒ–å™¨ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†è‡ªç„¶è¯­è¨€è§£å†³æ–¹æ¡ˆè½¬æ¢ä¸ºæ­£å¼è¯­å¢ƒã€‚éšåï¼Œæˆ‘ä»¬çš„æ‰¹åˆ¤å®¶ï¼ˆé›†æˆäº†å„ç§å¤–éƒ¨å·¥å…·ï¼Œå¦‚è®¡ç®—æœºä»£æ•°ç³»ç»Ÿå’ŒSMTæ±‚è§£å™¨ï¼‰è¯„ä¼°æ­£å¼è¯­å¢ƒä¸­æ¯ä¸ªè¯­å¥çš„æ­£ç¡®æ€§ï¼Œå¹¶åœ¨è¯­å¥é”™è¯¯æ—¶æä¾›åé¦ˆã€‚æˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†MATH-VFåœ¨ä¸¤ç§åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼š1ï¼‰éªŒè¯ï¼šä½¿ç”¨MATH-VFç¡®å®šç»™å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚2ï¼‰ç²¾è¿›ï¼šå½“MATH-VFè¯†åˆ«å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç»™å®šé—®é¢˜è§£å†³æ–¹æ¡ˆä¸­çš„é”™è¯¯æ—¶ï¼Œå®ƒå°†æ‰¹åˆ¤å®¶æå‡ºçš„ä¿®æ­£å»ºè®®æäº¤ç»™è§£å†³æ–¹æ¡ˆç”Ÿæˆå™¨ä»¥é‡æ–°ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°å­¦åŸºå‡†æµ‹è¯•MATH500å’ŒProcessBenchä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨é€»è¾‘æ¨ç†å’Œè®¡ç®—è¿‡ç¨‹ä¸­ä»å¯èƒ½å‡ºç°é”™è¯¯ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºMATH-VFçš„æ¡†æ¶ï¼ŒåŒ…æ‹¬å½¢å¼åŒ–å™¨å’Œè¯„å®¡å™¨ï¼Œç”¨äºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚è¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨å½¢å¼åŒ–å™¨å°†è‡ªç„¶è¯­è¨€è§£å†³æ–¹æ¡ˆè½¬åŒ–ä¸ºæ­£å¼è¯­å¢ƒï¼Œç„¶åè¯„å®¡å™¨ç»“åˆè®¡ç®—æœºä»£æ•°ç³»ç»Ÿå’ŒSMTæ±‚è§£å™¨ç­‰å¤–éƒ¨å·¥å…·è¯„ä¼°æ­£å¼è¯­å¢ƒä¸­çš„æ¯ä¸ªé™ˆè¿°ï¼Œä¸€æ—¦å‘ç°é”™è¯¯ï¼Œå³æä¾›åé¦ˆã€‚æœ¬æ–‡é€šè¿‡éªŒè¯å’Œç»†åŒ–ä¸¤ä¸ªåœºæ™¯å®è¯ç ”ç©¶äº†MATH-VFçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨MATH500å’ŒProcessBenchç­‰å¸¸ç”¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å­˜åœ¨é€»è¾‘æ¨ç†å’Œè®¡ç®—é”™è¯¯ã€‚</li>
<li>MATH-VFæ¡†æ¶åŒ…æ‹¬å½¢å¼åŒ–å™¨å’Œè¯„å®¡å™¨ï¼Œç”¨äºéªŒè¯LLMç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>å½¢å¼åŒ–å™¨å°†è‡ªç„¶è¯­è¨€è§£å†³æ–¹æ¡ˆè½¬åŒ–ä¸ºæ­£å¼è¯­å¢ƒã€‚</li>
<li>è¯„å®¡å™¨ç»“åˆå¤–éƒ¨å·¥å…·å¦‚è®¡ç®—æœºä»£æ•°ç³»ç»Ÿå’ŒSMTæ±‚è§£å™¨è¯„ä¼°é™ˆè¿°çš„æ­£ç¡®æ€§ï¼Œå¹¶æä¾›åé¦ˆã€‚</li>
<li>MATH-VFæ¡†æ¶åœ¨éªŒè¯å’Œç»†åŒ–ä¸¤ä¸ªåœºæ™¯ä¸­å…·æœ‰æœ‰æ•ˆæ€§ã€‚</li>
<li>MATH-VFæ¡†æ¶åœ¨MATH500å’ŒProcessBenchç­‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºæå‡LLMåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f652be7fe4dee68fdebff1b65c568e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-439d46378e7ff452a7687b0098fe4070.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac12f69efc9d11e79da1fa1bbdd0c127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3518f30459c20a82036d7312eba3d4fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7a03df0f2448a2aa975bf0cec38a7f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Reinforced-Informativeness-Optimization-for-Long-Form-Retrieval-Augmented-Generation"><a href="#Reinforced-Informativeness-Optimization-for-Long-Form-Retrieval-Augmented-Generation" class="headerlink" title="Reinforced Informativeness Optimization for Long-Form   Retrieval-Augmented Generation"></a>Reinforced Informativeness Optimization for Long-Form   Retrieval-Augmented Generation</h2><p><strong>Authors:Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang</strong></p>
<p>Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/RioRAG">https://github.com/RUCAIBox/RioRAG</a>. </p>
<blockquote>
<p>é•¿é—®ç­”ï¼ˆLFQAï¼‰ä¸ºå¤§è¯­è¨€æ¨¡å‹å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¦æ±‚åˆæˆè¿è´¯çš„ã€æ®µè½é•¿åº¦çš„ç­”æ¡ˆã€‚è™½ç„¶åŸºäºæ£€ç´¢çš„ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨å…³é”®æ–¹é¢å­˜åœ¨å±€é™ï¼šé«˜è´¨é‡è®­ç»ƒæ•°æ®ç¼ºä¹ç”¨äºé•¿å½¢å¼ç”Ÿæˆï¼Œæ‰©å±•è¾“å‡ºä¸­çš„å¹»æƒ³é£é™©ç´¯ç§¯ï¼Œä»¥åŠç¼ºä¹å¯é çš„è¯„ä»·æŒ‡æ ‡æ¥è¯„ä¼°äº‹å®å®Œæ•´æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RioRAGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–ä¿¡æ¯ä¼˜åŒ–æ¨è¿›é•¿å½¢å¼RAGã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªåŸºæœ¬åˆ›æ–°æ¥è§£å†³æ ¸å¿ƒæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¼ºåŒ–ä¿¡æ¯ä¼˜åŒ–çš„RLè®­ç»ƒèŒƒå¼ï¼Œç›´æ¥ä¼˜åŒ–ä¿¡æ¯æ€§ï¼Œæœ‰æ•ˆè§£å†³ä¼ ç»ŸRAGç³»ç»Ÿä¸­çš„æ…¢æ€è€ƒç¼ºé™·ï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥ç‰‡æ®µä¸ºä¸­å¿ƒçš„åˆ†å±‚å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ç²¾ç¡®è¯„ä¼°é•¿å½¢å¼ç­”æ¡ˆï¼šä»æ¯ä¸ªæºç½‘é¡µä¸­æå–ç‰‡æ®µã€æ„å»ºç‰‡æ®µå£°æ˜æ¸…å•ã€æ ¹æ®äº‹å®ä¸€è‡´æ€§è®¡ç®—å¥–åŠ±ã€‚åœ¨LongFactå’ŒRAGCheckerä¸¤ä¸ªLFQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/RioRAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/RioRAGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é•¿ç¯‡é—®ç­”ï¼ˆLFQAï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦åˆæˆè¿è´¯çš„ã€æ®µè½é•¿åº¦çš„ç­”æ¡ˆã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰ç ”ç©¶é¢ä¸´å…³é”®é™åˆ¶ï¼šé«˜è´¨é‡è®­ç»ƒæ•°æ®ç¼ºä¹ã€é•¿è¾“å‡ºä¸­çš„è™šæ„é£é™©ä»¥åŠç¼ºä¹å¯é çš„è¯„ä»·æŒ‡æ ‡æ¥è¯„ä¼°äº‹å®å®Œæ•´æ€§ã€‚æœ¬æ–‡æå‡ºRioRAGï¼Œä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–ä¿¡æ¯ä¼˜åŒ–æ¥æ¨è¿›é•¿ç¯‡RAGã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªåŸºæœ¬åˆ›æ–°ç‚¹æ¥è§£å†³æ ¸å¿ƒæŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¼ºåŒ–ä¿¡æ¯ä¼˜åŒ–çš„RLè®­ç»ƒèŒƒå¼ï¼Œç›´æ¥ä¼˜åŒ–ä¿¡æ¯æ€§ï¼Œæœ‰æ•ˆè§£å†³ä¼ ç»ŸRAGç³»ç»Ÿä¸­çš„æ…¢æ€è€ƒç¼ºé™·ï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£æ•°æ®ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥ç‰‡æ®µä¸ºä¸­å¿ƒçš„åˆ†å±‚å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ç²¾ç¡®è¯„ä¼°é•¿ç¯‡ç­”æ¡ˆï¼šä»æ¯ä¸ªæºç½‘é¡µä¸­æå–ç‰‡æ®µã€æ„å»ºç‰‡æ®µå£°æ˜æ¸…å•ã€æ ¹æ®äº‹å®å¯¹é½è®¡ç®—å¥–åŠ±ã€‚åœ¨LongFactå’ŒRAGCheckerä¸¤ä¸ªLFQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿ç¯‡é—®ç­”ï¼ˆLFQAï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æå‡ºç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦ç”Ÿæˆè¿è´¯çš„ã€æ®µè½é•¿åº¦çš„ç­”æ¡ˆã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿè™½ä¸ºæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å­˜åœ¨é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¼ºä¹ã€é•¿è¾“å‡ºä¸­çš„è™šæ„é£é™©åŠè¯„ä»·æŒ‡æ ‡ç¼ºå¤±ç­‰é™åˆ¶ã€‚</li>
<li>RioRAGæ¡†æ¶é€šè¿‡å¼ºåŒ–ä¿¡æ¯ä¼˜åŒ–è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒèŒƒå¼å’Œåˆ†å±‚å¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>RLè®­ç»ƒèŒƒå¼ç›´æ¥ä¼˜åŒ–ä¿¡æ¯æ€§ï¼Œæå‡ä¼ ç»ŸRAGç³»ç»Ÿçš„æ€§èƒ½ï¼Œé™ä½å¯¹æ˜‚è´µç›‘ç£æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>åˆ†å±‚å¥–åŠ±å»ºæ¨¡æ–¹æ³•é€šè¿‡æå–ã€æ„å»ºå’Œè®¡ç®—å¥–åŠ±ä¸‰ä¸ªé˜¶æ®µç²¾ç¡®è¯„ä¼°é•¿ç¯‡ç­”æ¡ˆã€‚</li>
<li>åœ¨LongFactå’ŒRAGCheckeråŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†RioRAGæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c28f3c2ea39d7590877a0af029bc117c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d80eb2d8c3fdc9753ca28e9b6c88df.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Understand-Think-and-Answer-Advancing-Visual-Reasoning-with-Large-Multimodal-Models"><a href="#Understand-Think-and-Answer-Advancing-Visual-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Understand, Think, and Answer: Advancing Visual Reasoning with Large   Multimodal Models"></a>Understand, Think, and Answer: Advancing Visual Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang</strong></p>
<p>Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at <a target="_blank" rel="noopener" href="https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R">https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R</a> soon. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æœ€è¿‘åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡å’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç†è§£æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ•´åˆç”¨äºç»„åˆæ¨ç†çš„å…ˆè¿›ã€ç‰¹å®šä»»åŠ¡åŠŸèƒ½æ–¹é¢å¸¸å¸¸ä¸è¶³ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬æˆä¸ºçœŸæ­£æœ‰èƒ½åŠ›çš„ä¸€èˆ¬è§†è§‰æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œä½¿LMMèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨å…¶å†…åœ¨èƒ½åŠ›ï¼ˆä¾‹å¦‚å®šä½å’Œè§†è§‰ç†è§£èƒ½åŠ›ï¼‰æ¥è§£å†³å¤æ‚çš„ç»„åˆé—®é¢˜ã€‚ä¸åŒäºä¹‹å‰çš„æ·å¾„å­¦ä¹ æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§ç±»ä¼¼äººç±»çš„â€œç†è§£-æ€è€ƒ-å›ç­”â€è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œæ— éœ€å¤šæ¬¡æ¨ç†æˆ–å¤–éƒ¨å·¥å…·ã€‚è¿™ç§è®¾è®¡ç¼©å°äº†åŸºæœ¬è§†è§‰èƒ½åŠ›å’Œé€šç”¨é—®ç­”ä¹‹é—´çš„å·®è·ï¼Œé¼“åŠ±LMMä¸ºå¤æ‚çš„è§†è§‰æ¨ç†ç”Ÿæˆå¿ å®å¯é çš„ç­”æ¡ˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œäº†åŒ…å«ä¸€èˆ¬åœºæ™¯å’Œæ–‡æœ¬ä¸°å¯Œåœºæ™¯çš„è§†è§‰æŒ‡ä»¤æ ·æœ¬ï¼Œæ¶‰åŠå¤šç§åŸºæœ¬è§†è§‰èƒ½åŠ›ã€‚æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹Griffon-Rå…·æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨ç†è§£ã€è‡ªæˆ‘æ€è€ƒå’Œæ¨ç†å›ç­”çš„èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGriffon-Rä¸ä»…åœ¨åŒ…æ‹¬VSRå’ŒCLEVRçš„å¤æ‚è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†è¿›å±•ï¼Œè€Œä¸”åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å¢å¼ºäº†å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¦‚MMBenchå’ŒScienceQAã€‚æ•°æ®ã€æ¨¡å‹å’Œä»£ç å°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jefferyZhan/Griffon/tree/master/Griffon-Rå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20753v1">PDF</a> Tech report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œè§†è§‰ä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°ï¼Œä½†å…¶åœ¨ç»„åˆæ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œä½¿LMMsèƒ½å¤Ÿè§£å†³å¤æ‚çš„ç»„åˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨å…¶å†…åœ¨èƒ½åŠ›ï¼ˆå¦‚æ¥åœ°å’Œè§†è§‰ç†è§£èƒ½åŠ›ï¼‰ã€‚è¯¥æ–¹æ³•ä¸åŒäºä»¥å¾€çš„æ·å¾„å­¦ä¹ æœºåˆ¶ï¼Œå¼•å…¥äººç±»èˆ¬çš„ç†è§£ã€æ€è€ƒå’Œå›ç­”è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹ä¸€æ¬¡å‰å‘ä¼ é€’å³å¯å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œæ— éœ€å¤šæ¬¡æ¨ç†æˆ–å¤–éƒ¨å·¥å…·ã€‚åŒæ—¶ï¼Œä¸ºäº†è®­ç»ƒæ¨¡å‹Griffon-Rï¼Œæ”¶é›†äº†33.4ä¸‡ä»½è§†è§‰æŒ‡ä»¤æ ·æœ¬ï¼Œæ¶µç›–ä¸€èˆ¬åœºæ™¯å’Œæ–‡æœ¬ä¸°å¯Œåœºæ™¯ï¼Œæ¶‰åŠå¤šç§åŸºç¡€è§†è§‰èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGriffon-Råœ¨å¤æ‚è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº†å¤šæ¨¡æ€èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç»„åˆæ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§ç»Ÿä¸€çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œä½¿LMMsèƒ½å¤Ÿè§£å†³å¤æ‚çš„ç»„åˆé—®é¢˜ã€‚</li>
<li>è¯¥æœºåˆ¶ä¸åŒäºä»¥å¾€çš„æ·å¾„å­¦ä¹ ï¼Œå¼•å…¥äººç±»èˆ¬çš„ç†è§£ã€æ€è€ƒå’Œå›ç­”è¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹Griffon-Ré€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œæ— éœ€å¤šæ¬¡æ¨ç†æˆ–å¤–éƒ¨å·¥å…·ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒGriffon-Rï¼Œæ”¶é›†äº†æ¶µç›–å¤šç§åœºæ™¯çš„33.4ä¸‡ä»½è§†è§‰æŒ‡ä»¤æ ·æœ¬ã€‚</li>
<li>å®éªŒè¡¨æ˜Griffon-Råœ¨å¤æ‚è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef2da0a02124cf75f121bb28ab44163a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59849f247b07428f9b3f1393a15a28fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba677c25b5bb8cc8fe24ec34124e63ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99d471b2b51219ed86a86fb4ae1f05cc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-42553c2c44a96e95084b67babc2c1747.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-28  Absolute Coordinates Make Motion Generation Easy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
