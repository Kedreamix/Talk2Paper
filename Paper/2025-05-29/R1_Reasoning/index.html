<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-29  Reinforcing General Reasoning without Verifiers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-29-更新"><a href="#2025-05-29-更新" class="headerlink" title="2025-05-29 更新"></a>2025-05-29 更新</h1><h2 id="Reinforcing-General-Reasoning-without-Verifiers"><a href="#Reinforcing-General-Reasoning-without-Verifiers" class="headerlink" title="Reinforcing General Reasoning without Verifiers"></a>Reinforcing General Reasoning without Verifiers</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du</strong></p>
<p>The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree">https://github.com/sail-sg/VeriFree</a>. </p>
<blockquote>
<p>最近，利用DeepSeek-R1-Zero风格的强化学习（RL）在可验证奖励上进行大规模语言模型（LLM）训练的范式转变，在代码和数学推理方面取得了令人印象深刻的进展。然而，这种方法仅限于基于规则的答案验证可行的任务，并不能自然地扩展到现实世界领域，如化学、医疗、工程、法律、生物、商业和经济学。目前的实用解决方案是使用额外的LLM作为模型验证器，但这带来了对强大的验证器LLM的依赖、奖励破解的易感性以及在训练过程中在内存中维护验证器模型的实践负担等问题。为了解决这一问题，并将DeepSeek-R1-Zero风格的培训扩展到一般推理领域，我们提出了一种无需验证器的方法（VeriFree），它绕过答案验证，而是使用RL直接最大化生成参考答案的概率。我们将VeriFree与基于验证器的方法进行比较，并证明VeriFree除了具有显著的实践效益和减少计算需求外，在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试的全面评估中，甚至超越了基于验证器的方法。此外，我们从多个角度对这种方法进行了深入洞察：作为将策略和隐式验证器整合到统一模型中的巧妙集成，以及作为一种变分优化方法。代码可访问 <a target="_blank" rel="noopener" href="https://github.com/sail-sg/VeriFree%E3%80%82">https://github.com/sail-sg/VeriFree。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用DeepSeek-R1-Zero风格的强化学习（RL）在可验证奖励下训练大型语言模型（LLM）的最新进展，该方法在代码和数学推理方面取得了令人印象深刻的进步。然而，这种方法仅限于规则可验证的任务，无法自然扩展到化学、医疗、工程、法律、生物、商业和经济等真实世界领域。为解决这一问题并扩展DeepSeek-R1-Zero风格的训练至通用推理领域，提出了一种无需验证器的方法（VeriFree），该方法绕过答案验证，直接使用强化学习最大化生成参考答案的概率。对比基于验证器的方法，VeriFree不仅在实际应用和计算需求方面具有显著优势，而且在广泛的评估中甚至超越了基于验证器的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）使用DeepSeek-R1-Zero风格的强化学习在可验证奖励下取得了在代码和数学推理方面的显著进步。</li>
<li>当前方法主要局限于规则可验证的任务，难以应用于真实世界领域如化学、医疗等。</li>
<li>为解决此问题，提出了无需验证器的方法（VeriFree），通过强化学习直接最大化生成正确答案的概率。</li>
<li>VeriFree与基于验证器的方法相比，具有实际应用和计算需求上的优势，并在广泛的评估中表现出卓越性能。</li>
<li>VeriFree作为一种优雅地将策略和隐式验证器集成在统一模型中的方法，具有很高的实用性。</li>
<li>VeriFree提供了一种变分优化方法的视角，为语言模型训练提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abd1833621d3fc574f509bb61a4501f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d03a7ead67cfc16078c3ed181410e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f79b9549ae46abbbebf4c12cb745df2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners"><a href="#Are-Language-Models-Consequentialist-or-Deontological-Moral-Reasoners" class="headerlink" title="Are Language Models Consequentialist or Deontological Moral Reasoners?"></a>Are Language Models Consequentialist or Deontological Moral Reasoners?</h2><p><strong>Authors:Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin</strong></p>
<p>As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens">https://github.com/keenansamway/moral-lens</a> . </p>
<blockquote>
<p>随着人工智能系统在医疗保健、法律和治理等领域的应用日益广泛，理解它们如何处理道德上复杂的场景变得至关重要。以前的研究主要关注大型语言模型（LLM）的道德判断，而非其潜在的道德推理过程。相比之下，我们专注于对LLM提供的道德推理轨迹的大规模分析。此外，不同于之前的研究仅从少数道德困境中推断结果，我们的研究利用超过600个不同的电车问题作为探针，以揭示不同LLM内部出现的推理模式。我们介绍并测试了一种道德理由的分类法，根据两种主要的规范性伦理理论，即后果主义和道义论来系统地分类推理轨迹。我们的分析表明，LLM的思维链往往倾向于基于道德义务的道义论原则，而后期的解释则显著转向强调实用性的后果主义理由。我们的框架为理解LLM如何处理并阐述道德考量提供了基础，这是将LLM安全且可解释地部署于高风险决策环境的重要步骤。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/keenansamway/moral-lens%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/keenansamway/moral-lens获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在伦理复杂场景中的道德推理过程。通过对超过600个不同的电车问题的分析，作者发现LLM的思考链往往倾向于基于道德义务的原则，而后期的解释则明显转向强调实用主义的后果主义理由。这为理解LLMs如何处理并表达伦理考虑提供了基础，是安全且可解释地在高风险决策环境中部署LLMs的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在伦理复杂场景中的道德推理至关重要。</li>
<li>LLM的思考链倾向于基于道德义务的原则（deontological principles）。</li>
<li>与早期研究相比，本研究使用了超过600个电车问题来揭示不同LLM中的推理模式。</li>
<li>引入并测试了道德理由的分类法，以系统地根据两种主要的伦理理论（后果主义和道义论）对推理轨迹进行分类。</li>
<li>后期的解释明显转向强调实用主义的后果主义理由（consequentialist rationales）。</li>
<li>该研究提供了一个理解LLM如何处理并表达伦理考虑的基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81c82638e8b82dcc7462f7876531149e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5310537ab54ab5b6f5dc826993a05f40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5a3141b963ab23c4f36dc44d93b7b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd5bc52df8235fdfb5574679b7d8270.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Policy-Optimized-Text-to-Image-Pipeline-Design"><a href="#Policy-Optimized-Text-to-Image-Pipeline-Design" class="headerlink" title="Policy Optimized Text-to-Image Pipeline Design"></a>Policy Optimized Text-to-Image Pipeline Design</h2><p><strong>Authors:Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor</strong></p>
<p>Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines. </p>
<blockquote>
<p>文本到图像生成已经发展超越了单一的整体模型，形成了复杂的多组件管道。这些管道结合了精细调整的生成器、适配器、放大块甚至编辑步骤，导致图像质量得到显著改善。然而，它们的有效设计需要大量的专业知识。最近的方法在通过大型语言模型（LLM）自动化这个过程方面显示出希望，但它们存在两个关键的局限性：一是从数百个预定义管道生成图像需要大量的计算资源，二是对记忆训练示例之外的推广能力较差。我们引入了一种基于强化学习的新框架来解决这些低效问题。我们的方法首先训练一个奖励模型集合，能够直接从提示-工作流程组合中预测图像质量分数，从而消除训练期间昂贵的图像生成需求。然后，我们采用两阶段训练策略：初始工作流程词汇训练，然后是基于GRPO的优化，引导模型向性能更高的工作流程空间区域发展。此外，我们采用了一种基于无分类器指导的增强技术，沿着初始模型和GRPO调整模型之间的路径进行推断，进一步提高输出质量。我们通过一系列对比验证了我们的方法，结果表明它成功地创建了具有更高多样性的新流程，并导致图像质量优于现有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了文本到图像生成领域的发展，包括多组件管道、精细的生成器、适配器和放大块等。然而，它们的有效设计需要大量专业知识。最新方法虽试图通过大型语言模型自动化这一过程，但仍存在计算需求大、泛化能力差的局限性。为此，研究团队提出了一种基于强化学习的新框架来解决这些问题。该框架通过训练一组奖励模型来预测图像质量分数，从而减少训练过程中的图像生成成本。同时采用两阶段训练策略，初始工作流程词汇训练，接着是GPRO优化，最后引入一种基于无分类器引导技术的增强技术，进一步提高输出质量。验证了新方法能够创建更具多样性和高质量图像的工作流程。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文本到图像生成已从单一模型演变为复杂的多组件管道，包括精细的生成器、适配器和放大块等。</li>
<li>现有自动化方法存在计算需求大、泛化能力差的局限性。</li>
<li>新框架采用强化学习来解决这些问题，通过训练奖励模型预测图像质量分数来减少训练成本。</li>
<li>采用两阶段训练策略：初始工作流程词汇训练，然后是GRPO优化。</li>
<li>引入基于无分类器引导技术的增强技术，进一步提高输出质量。</li>
<li>新方法能够创建具有多样性和高质量图像的新工作流程。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7493c31768c123ea46735d176ea4d438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08509a719874ee7319d6eaaa84ad7682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ace6180579b47e9f0e9f74d668599c7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PropMolFlow-Property-guided-Molecule-Generation-with-Geometry-Complete-Flow-Matching"><a href="#PropMolFlow-Property-guided-Molecule-Generation-with-Geometry-Complete-Flow-Matching" class="headerlink" title="PropMolFlow: Property-guided Molecule Generation with Geometry-Complete   Flow Matching"></a>PropMolFlow: Property-guided Molecule Generation with Geometry-Complete   Flow Matching</h2><p><strong>Authors:Cheng Zeng, Jirui Jin, George Karypis, Mark Transtrum, Ellad B. Tadmor, Richard G. Hennig, Adrian Roitberg, Stefano Martiniani, Mingjie Liu</strong></p>
<p>Molecule generation is advancing rapidly in chemical discovery and drug design. Flow matching methods have recently set the state of the art (SOTA) in unconditional molecule generation, surpassing score-based diffusion models. However, diffusion models still lead in property-guided generation. In this work, we introduce PropMolFlow, a novel approach for property-guided molecule generation based on geometry-complete SE(3)-equivariant flow matching. Integrating five different property embedding methods with a Gaussian expansion of scalar properties, PropMolFlow outperforms previous SOTA diffusion models in conditional molecule generation across various properties while preserving the stability and validity of the generated molecules, consistent with its unconditional counterpart. Additionally, it enables faster inference with significantly fewer time steps compared to baseline models. We highlight the importance of validating the properties of generated molecules through DFT calculations performed at the same level of theory as the training data. Specifically, our analysis identifies properties that require DFT validation and others where a pretrained SE(3) geometric vector perceptron regressors provide sufficiently accurate predictions on generated molecules. Furthermore, we introduce a new property metric designed to assess the model’s ability to propose molecules with underrepresented property values, assessing its capacity for out-of-distribution generalization. Our findings reveal shortcomings in existing structural metrics, which mistakenly validate open-shell molecules or molecules with invalid valence-charge configurations, underscoring the need for improved evaluation frameworks. Overall, this work paves the way for developing targeted property-guided generation methods, enhancing the design of molecular generative models for diverse applications. </p>
<blockquote>
<p>分子生成在化学发现和药物设计领域正迅速推进。最近，流量匹配方法在无条件分子生成领域已经处于最新技术状态（SOTA），超越了基于分数的扩散模型。然而，扩散模型在属性导向生成方面仍占优势。在这项工作中，我们介绍了PropMolFlow，这是一种基于几何完全SE(3)等价流匹配的新颖属性导向分子生成方法。通过将五种不同的属性嵌入方法与标量属性的高斯扩展相结合，PropMolFlow在多种属性的有条件分子生成方面超越了之前的SOTA扩散模型，同时保持了生成分子的稳定性和有效性，与其无条件对应物相一致。此外，与基准模型相比，它实现了更快的推理，并且显著减少了时间步骤。我们强调通过与实施训练数据相同理论水平的DFT计算验证生成分子属性的重要性。具体来说，我们的分析确定了需要进行DFT验证的属性，以及其他使用预训练的SE(3)几何向量感知器回归器对生成的分子提供足够准确预测的属性。此外，我们引入了一个新的属性指标，旨在评估模型提出具有代表性不足的属性值的分子的能力，评估其偏离分布泛化的能力。我们的研究结果揭示了现有结构指标的不足，这些指标错误地验证了开壳分子或具有无效价电荷配置的分子，强调了对改进评估框架的需求。总体而言，这项工作为开发有针对性的属性导向生成方法铺平了道路，增强了分子生成模型在各种应用中的设计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21469v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了在化学发现和药物设计中，分子生成领域的新进展。文章提出一种新型的方法PropMolFlow，基于几何完全SE(3)等价流匹配，用于属性导向的分子生成。此方法在融合五种属性嵌入方法和标量属性的高斯扩展后，在条件分子生成的各种属性上超越了当前最佳扩散模型，同时保持了生成分子的稳定性和有效性。此外，它实现了更快的推理速度，并且显著减少了与基线模型相比的时间步骤。文章强调了通过DFT计算验证生成分子属性的重要性，并分析了哪些属性需要DFT验证，哪些属性可以使用预训练的SE(3)几何向量感知回归器提供足够准确的预测。该研究还引入了一个新的属性指标，旨在评估模型提出具有代表性属性值的分子的能力，并揭示了现有结构指标的不足，强调了需要改进评估框架。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>分子生成领域正在快速发展，尤其在化学发现和药物设计中。</li>
<li>PropMolFlow是一种新的属性导向的分子生成方法，基于几何完全SE(3)等价流匹配。</li>
<li>PropMolFlow在条件分子生成的各种属性上超越了当前最佳扩散模型。</li>
<li>PropMolFlow能更快地进行推理，并且显著减少了时间步骤。</li>
<li>DFT计算对于验证生成分子的属性至关重要。</li>
<li>需要区分哪些属性需要DFT验证，哪些可以使用预训练的SE(3)几何向量感知回归器进行预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21469">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6355fb4bf4e3d348062441e7c810a53d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3074ea071efeaa8aea5d7f16e01d78f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Accelerating-Diffusion-Language-Model-Inference-via-Efficient-KV-Caching-and-Guided-Diffusion"><a href="#Accelerating-Diffusion-Language-Model-Inference-via-Efficient-KV-Caching-and-Guided-Diffusion" class="headerlink" title="Accelerating Diffusion Language Model Inference via Efficient KV Caching   and Guided Diffusion"></a>Accelerating Diffusion Language Model Inference via Efficient KV Caching   and Guided Diffusion</h2><p><strong>Authors:Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta</strong></p>
<p>Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver up to a 34x end-to-end speedup without compromising accuracy. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains. </p>
<blockquote>
<p>扩散语言模型具有并行令牌生成和内在双向性的优点，与自回归方法相比，它们提供了更高效和强大的序列建模。然而，最先进的扩散模型（例如Dream 7B、LLaDA 8B）存在推理速度慢的问题。尽管它们在质量上可以与规模相似的自回归模型（例如Qwen2.5 7B、Llama3 8B）相匹配，但由于它们的迭代降噪需要大量完整序列的前向传递过程，导致了计算成本高和延迟问题，尤其是在输入提示长以及长期上下文场景中更为突出。此外，并行令牌生成会导致令牌不一致问题，当前采样启发式策略在减少降噪步骤时会出现显著的质量下降。我们采用两种无需训练的技术来解决这些局限性。首先，我们提出了一种名为FreeCache的键值（KV）近似缓存技术，它可以在降噪步骤中重复使用稳定的KV投影，有效降低扩散语言模型推理的计算成本。其次，我们引入了Guided Diffusion方法，这是一种无需训练的监督令牌去掩码方法，使用轻量级预训练自回归模型进行监督，大大降低了降噪迭代次数，同时不牺牲质量。我们在开源推理基准测试上进行了广泛评估，我们的组合方法在不损害准确性的情况下实现了高达34倍的端到端加速。扩散语言模型首次实现了与广泛采用的自回归模型相当的甚至更快的延迟时间。我们的工作成功为扩散语言模型在跨不同领域的更广泛应用铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散语言模型（Diffusion Language Models，简称DLMs）的特点及其在序列建模中的优势。尽管它们具备并行标记生成和固有的双向性，但与现有的自回归模型相比，它们在推理过程中存在速度慢的问题。为解决这些问题，本文提出了两种训练无关的技术：FreeCache和Guided Diffusion。前者通过重用稳定的键值投影来减少计算成本，后者使用轻量级的预训练自回归模型来监督标记去掩码过程，从而减少去噪迭代次数而不损失质量。结合这两种方法，实现了高达34倍端到端的加速，使扩散语言模型的延迟与广泛采用的自回归模型相当甚至更快。这为扩散语言模型在跨不同领域的应用提供了更广阔的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散语言模型具备并行标记生成和固有的双向性，可实现更高效和强大的序列建模。</li>
<li>当前扩散模型在推理过程中存在速度慢的问题，主要由于迭代去噪需要多次全序列前向传递。</li>
<li>FreeCache技术通过重用稳定的键值投影来减少计算成本，提高扩散模型的推理速度。</li>
<li>Guided Diffusion方法使用预训练的自回归模型来监督标记去掩码过程，减少去噪迭代次数，同时保持质量。</li>
<li>结合两种方法，实现了高达34倍的端到端加速，使扩散语言模型的延迟与自回归模型相当。</li>
<li>扩散语言模型的改进为其在跨不同领域的应用提供了更广阔的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fefbd53508a7785a06c003231275da77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c83ee5751308cea2e39b50ebfd85f3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32a0073bd3b3cdfe5461c1fe27d06721.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e844680aa78cddbc863be1dce62cc21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584784fa475132d3dd9b379809876baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9996f9810e0e6771e1e9f25044eeeec7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation"><a href="#RefTool-Enhancing-Model-Reasoning-with-Reference-Guided-Tool-Creation" class="headerlink" title="RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation"></a>RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</h2><p><strong>Authors:Xiao Liu, Da Yin, Zirui Wu, Yansong Feng</strong></p>
<p>Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models’ internal knowledge and would fail in domains beyond the LLMs’ knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning. </p>
<blockquote>
<p>工具可以增强大型语言模型（LLM）在复杂问题解决任务中的推理能力，但并不是所有的任务都有可用的工具。在没有预设工具的情况下，早期的研究工作已经探索了指导LLM自行生成工具的方法。然而，这些方法很大程度上依赖于模型的内部知识，并且在LLM知识范围之外的领域会失效。为了解决这一局限性，我们提出了RefTool，这是一个参考引导的自动工具创建框架，它利用结构化的外部材料（如教科书）。RefTool由两个模块组成：（1）工具创建，其中LLM根据参考内容生成可执行工具，使用示例进行验证，并将其按层次结构组织成工具箱；（2）工具利用，其中LLM浏览工具箱结构，选择并应用适当的工具来解决问题。在因果、物理和化学基准测试上的实验表明，RefTool在平均准确率上比现有的工具创建和领域特定推理方法高出11.3%，同时成本效益高且可广泛推广。分析表明，以参考为基础的工具创建产生了准确和忠诚的工具，层次结构有助于有效的工具选择。RefTool使LLM能够克服知识局限性，证明了以外部参考为基础的工具创建对于增强和通用推理的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21413v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/xxxiaol/RefTool">https://github.com/xxxiaol/RefTool</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为RefTool的参考引导框架，用于在大型语言模型（LLM）中自动生成工具。该框架借助结构化外部材料（如教科书）来创建工具，并通过实验证明其在因果、物理和化学等领域的效果优于现有工具创建和特定领域推理方法。RefTool解决了在缺乏预先定义的工具时LLM的知识局限性问题，通过参考内容生成可执行工具，并以层次结构组织工具箱，使LLM能够选择并应用适当的工具来解决问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RefTool是一个参考引导框架，用于大型语言模型（LLM）自动生成工具。</li>
<li>该框架通过结构化外部材料（如教科书）来创建工具。</li>
<li>RefTool包括两个模块：工具创建和工具利用。</li>
<li>工具创建模块使LLM从参考内容生成工具，并用示例验证其有效性，以层次结构组织工具箱。</li>
<li>工具利用模块使LLM能够选择并应用适当的工具来解决问题。</li>
<li>实验表明，RefTool在因果、物理和化学等领域优于现有方法，平均准确度提高11.3%。</li>
<li>RefTool解决了LLM的知识局限性问题，通过参考内容生成工具增强了推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-565ff994cf9e3608f1a45c7d54192219.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7bcf12572c4bec174366f2c6639fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff673cc55f6d6d92eadb47d9967ed88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce15a60ee0afeef1a17e1de7fe7165d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Structured-Unplugged-Approach-for-Foundational-AI-Literacy-in-Primary-Education"><a href="#A-Structured-Unplugged-Approach-for-Foundational-AI-Literacy-in-Primary-Education" class="headerlink" title="A Structured Unplugged Approach for Foundational AI Literacy in Primary   Education"></a>A Structured Unplugged Approach for Foundational AI Literacy in Primary   Education</h2><p><strong>Authors:Maria Cristina Carrisi, Mirko Marras, Sara Vergallo</strong></p>
<p>Younger generations are growing up in a world increasingly shaped by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them. However, education in this field often emphasizes tool-based learning, prioritizing usage over understanding the underlying concepts. This lack of knowledge leaves non-experts, especially children, prone to misconceptions, unrealistic expectations, and difficulties in recognizing biases and stereotypes. In this paper, we propose a structured and replicable teaching approach that fosters foundational AI literacy in primary students, by building upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI. To assess the effectiveness of our approach, we conducted an empirical study with thirty-one fifth-grade students across two classes, evaluating their progress through a post-test and a satisfaction survey. Our results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations. Moreover, the approach proved engaging, with students particularly enjoying activities that linked AI concepts to real-world reasoning. Materials: <a target="_blank" rel="noopener" href="https://github.com/tail-unica/ai-literacy-primary-ed">https://github.com/tail-unica/ai-literacy-primary-ed</a>. </p>
<blockquote>
<p>年轻一代正在一个由智能技术日益塑造的世界中成长，因此早期的人工智能素养对于发展批判性理解和驾驭这些技术的能力至关重要。然而，该领域的教育往往强调基于工具的学习，优先考虑使用而非理解基本概念。这种知识的缺乏使得非专家，尤其是儿童，容易受到误解、产生不切实际的期望，并且在识别偏见和刻板印象时遇到困难。在本文中，我们提出了一种结构化且可复制的教学方法，通过在核心数学元素的基础上，结合小学课程中的紧密相关和感兴趣的内容，促进小学生的人工智能素养发展，以加强概念化、数据表示、分类推理和人工智能评估。为了评估我们方法的有效性，我们对两个班级的三十一名五年级学生进行了实证研究，通过课后测试和满意度调查来评估他们的进步。我们的结果表明，学生在术语理解和使用、特征描述、逻辑推理和评估技能方面有所提高，学生对决策过程及其局限性有了更深的理解。此外，该方法证明是吸引人的，学生特别喜欢将人工智能概念与现实世界的推理联系起来的活动。材料：<a target="_blank" rel="noopener" href="https://github.com/tail-unica/ai-literacy-primary-ed">链接</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21398v1">PDF</a> Under review</p>
<p><strong>Summary</strong>：年轻一代正生活在一个智能技术日益普及的世界，早期的人工智能素养对于培养批判理解和应对这些技术的能力至关重要。然而，教育领域往往注重工具性学习，优先考虑使用而非理解基础概念。本文提出了一种结构化、可复制的教学方法，旨在通过小学数学的核心元素来培养小学生的人工智能素养，强化概念化、数据表示、分类推理和人工智能评估。我们对两个班级共31名五年级学生进行了实证研究，通过测试和后测试满意度调查来评估其效果。结果显示学生在术语理解和使用、特征描述、逻辑推理和评估技能方面有所提高，对决策过程及其局限性有了更深的理解。此外，该方法受到学生的欢迎，尤其是那些将人工智能概念与现实生活相结合的活动。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>早期AI素养教育对年轻一代理解和应对智能技术至关重要。</li>
<li>当前教育领域在AI教育上过于强调工具使用，忽视基础概念的理解。</li>
<li>提议采用结构化、可复制的教学方法培养小学生AI素养。</li>
<li>通过小学数学的核心元素来教授AI，强化概念化、数据表示、分类推理和AI评估技能。</li>
<li>实证研究证明该方法能提高学生AI术语的理解和使用、特征描述、逻辑推理和评估技能。</li>
<li>学生能更深入地理解AI决策过程及其局限性。</li>
<li>该教学方法受到学生的欢迎，尤其是与现实生活相结合的活动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21398">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7752b8edb622bc84b0c289f2940dfb74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53254af06a4822439f2a78c6da9a39a3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-Holmes-Can-MLLM-Think-Like-Holmes-for-Complex-Video-Reasoning"><a href="#Video-Holmes-Can-MLLM-Think-Like-Holmes-for-Complex-Video-Reasoning" class="headerlink" title="Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?"></a>Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a “Holmes-test” for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in <a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes">https://github.com/TencentARC/Video-Holmes</a>. </p>
<blockquote>
<p>最近报道了认知推理（CoT）和强化学习（RL）后训练在增强大型多模态语言模型（MLLMs）的视频推理能力方面的进展。这一进展自然引发了一个问题：这些模型能否以与人类专家相当的方式进行复杂的视频推理？然而，现有的视频基准测试主要评估视觉感知和定位能力，问题可以基于明确的提示或孤立的视觉线索来回答。这样的基准测试并不能完全捕捉到现实世界推理的复杂性，其中人类必须在得出结论之前积极寻找、整合和分析多个线索。为了解决这一问题，我们推出了Video-Holmes基准测试，该测试以福尔摩斯侦探的推理过程为灵感，旨在评估大型多模态语言模型的复杂视频推理能力。Video-Holmes包含从精心挑选的270部悬疑短片中衍生出的1837个问题，涵盖了七个精心设计的任务。每个任务首先识别电影中的关键事件和因果关系，然后设计问题，要求模型主动定位并连接散布在不同视频片段中的多个相关视觉线索。我们对最先进的MLLMs的综合评估表明，虽然这些模型在视觉感知方面表现出色，但在整合信息和经常遗漏关键线索方面遇到了很大的困难。例如，表现最佳的模型Gemini-2.5 Pro准确率仅为45%，大多数模型的得分低于40%。我们希望通过Video-Holmes作为多模态推理的“福尔摩斯测试”，激励模型像人类一样进行推理，并强调这一领域的持续挑战。该基准测试已在<a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/TencentARC/Video-Holmes发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21374v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://github.com/TencentARC/Video-Holmes">https://github.com/TencentARC/Video-Holmes</a></p>
<p><strong>Summary</strong><br>视频推理能力在多媒体大型语言模型（MLLMs）中逐渐受到关注。为评估模型的复杂视频推理能力，推出了Video-Holmes基准测试。该测试从270部悬疑短片中手动标注了关键事件和因果关系，设计了包含七大任务的问题集。尽管模型在视觉感知方面表现出色，但在整合信息和寻找关键线索方面存在困难。最好的模型准确率仅为45%，大多数模型得分低于40%。Video-Holmes旨在为多媒体推理提供一个“霍姆斯测试”，强调模型需要像人类一样进行推理，并突出这一领域的挑战。该基准测试已在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-Holmes是一个旨在评估多媒体大型语言模型（MLLMs）复杂视频推理能力的基准测试。</li>
<li>测试包含从悬疑短片中手动标注的七大任务，涉及关键事件和因果关系的识别。</li>
<li>尽管模型在视觉感知方面表现出色，但在整合信息和寻找关键线索方面存在困难。</li>
<li>目前最好的模型在Video-Holmes上的准确率为45%，大多数模型得分低于此。</li>
<li>Video-Holmes旨在作为多媒体推理的“霍姆斯测试”，鼓励模型像人类一样进行推理。</li>
<li>此基准测试的发布强调了多媒体推理领域的挑战和发展需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21374">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b74c3036d99eab20c0ce9dd320234e5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf35c51100e2720e4073b5506e2ec078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01145b8139b58441f4073c336c97f759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e5a424d1f878d726fcfdece58832aae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab56b09036ffabbfaebc607658af746f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning"><a href="#Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning" class="headerlink" title="Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning"></a>Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning</h2><p><strong>Authors:Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah</strong></p>
<p>Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language’s low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies. </p>
<blockquote>
<p>解决孟加拉数学文字题（MWPs）仍然是自然语言处理（NLP）领域的一个主要挑战，这是由于孟加拉语的资源相对较少以及需要多步骤推理。现有模型在处理复杂的孟加拉数学文字题时面临困难，很大程度上是因为之前没有人对孟加拉数据集进行标注来解决此任务。这一差距限制了孟加拉数学推理的进展。为了解决这个问题，我们创建了SOMADHAN数据集，其中包含8792个复杂的孟加拉数学文字题以及手动编写的逐步解决方案。我们设计这个数据集是为了支持在语言表示不足的情境中进行推理评估导向和模型开发。使用SOMADHAN数据集，我们评估了一系列大型语言模型（LLM），包括GPT-4o、GPT-3.5 Turbo、LLaMA系列模型、Deepseek和Qwen等，通过零样本和少样本提示以及有无思维链（CoT）推理进行测试。思维链提示一致地提高了性能，尤其是在需要多步骤逻辑的任务中。LLaMA-3.3 70B在少样本思维链提示下取得了最高的准确率，达到88%。我们还应用了低秩适应（LoRA）技术来高效微调模型，使它们能够适应孟加拉数学文字题并具有最小的计算成本。我们的工作通过提供高质量推理数据集和解决复杂数学文字题的可扩展框架来填补孟加拉NLP领域的空白。我们的目标是推动低资源语言的公平研究，并提升教育和语言技术中的推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21354v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了解决孟加拉数学文字题（MWPs）的挑战，指出由于孟加拉语的资源匮乏和多步骤推理的需求，现有模型难以应对复杂问题。为应对这一挑战，创建了SOMADHAN数据集，包含8792个复杂的孟加拉数学文字题和手动编写的逐步解答。该数据集旨在支持语言代表性不足的情境中的推理评估模型发展。通过一系列大型语言模型的评估，发现链式思维（CoT）提示在解决需要多步骤逻辑的问题时表现卓越。LLaMA-3.3 70B通过有限的计算成本使用LoRA技术实现较高的准确度。该研究填补了孟加拉语自然语言处理的空白，提供了高质量的推理数据集和复杂的文字题解决框架。目标是推动低资源语言的均衡研究，提高教育和语言技术的推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>孟加拉数学文字题处理是自然语言处理中的一大挑战，主要由于孟加拉语的资源匮乏和多步骤推理需求。</li>
<li>SOMADHAN数据集包含复杂孟加拉数学文字题及其手动逐步解答，旨在支持语言代表性不足的情境中的推理评估模型发展。</li>
<li>链式思维（CoT）提示在解决需要多步骤逻辑的问题时表现卓越。</li>
<li>LLaMA-3.3 70B通过结合CoT提示和LoRA技术实现较高的准确度。</li>
<li>LoRA技术用于高效地微调模型，使其能够适应孟加拉数学文字题，同时保持较低的计算成本。</li>
<li>该研究填补了孟加拉语自然语言处理的空白，提供了一个重要的数据集和框架来解决复杂的数学文字题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21354">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aedb13d51ece17fd5fda0c1ad861e90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee0512f5ac535ccfc71236625e198cd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133b208ecfed867a54569cb3c1d5d2fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MME-Reasoning-A-Comprehensive-Benchmark-for-Logical-Reasoning-in-MLLMs"><a href="#MME-Reasoning-A-Comprehensive-Benchmark-for-Logical-Reasoning-in-MLLMs" class="headerlink" title="MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs"></a>MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs</h2><p><strong>Authors:Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, Xiangyu Yue</strong></p>
<p>Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as &#96;&#96;thinking mode’’ and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities. </p>
<blockquote>
<p>逻辑推理是人类智能的一个基本方面，也是多模态大型语言模型（MLLMs）的核心能力。尽管多模态推理取得了重大进展，但由于缺乏明确的逻辑推理类型分类和对推理的清晰理解，现有的基准测试未能全面评估它们的推理能力。为了解决这些问题，我们推出了MME-Reasoning，这是一个旨在评估MLLMs推理能力的全面基准测试，其问题涵盖了三种推理类型（即归纳推理、演绎推理和溯因推理）。我们精心筛选数据，以确保每个问题都能有效地评估推理能力，而不是感知能力或知识广度，并扩展评估协议以涵盖对不同问题的评估。我们的评估显示，当进行全面的逻辑推理能力评估时，最先进的多模态大型语言模型存在重大局限性。即使是最先进的MLLMs在综合逻辑推理方面的表现也有限，不同推理类型之间的性能不平衡现象尤为突出。此外，我们对一些普遍认为能增强推理能力的方法（如“思考模式”和基于规则的强化学习）进行了深入分析。这些发现突显了当前多模态大型语言模型在不同逻辑推理场景中的关键局限性和性能不平衡问题，为理解和评估推理能力提供了全面和系统的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21327v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>推出一个评估多模态大型语言模型（MLLMs）推理能力的新基准——MME-Reasoning，涵盖了归纳、演绎和溯因三种推理类型的问题。对现有MLLMs的推理能力进行了全面评估，发现仍存在显著局限和性能不均衡的问题。深入分析了一些提升推理能力的常用方法，如“思考模式”和基于规则的强化学习等。此研究揭示了MLLMs在多样逻辑推理场景下的关键局限和性能不均衡问题，为理解和评估推理能力提供了全面系统的见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>MME-Reasoning基准被推出，旨在全面评估MLLMs的推理能力，包括归纳、演绎和溯因推理。</li>
<li>现有MLLMs在逻辑推理能力方面存在显著局限性。</li>
<li>即使在最先进的MLLMs中，推理能力的表现也不均衡，特别是在不同类型的推理之间。</li>
<li>深入分析了增强MLLMs推理能力的常用方法。</li>
<li>研究揭示了MLLMs在多样逻辑推理场景下的关键局限。</li>
<li>此研究为理解和评估MLLMs的推理能力提供了全面系统的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-062f31e6a2d802f0f12b925f44d51d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f750f2c2170d08eb8ed45191b54bd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2297c412eb5f01e954eca1ffe3d1c438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78de4d409a6aa5e88dc60841a785cb33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604e0a40b4c1af0065f68093e765485e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8cce7ac762bdcef4255b6286266451.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="rStar-Coder-Scaling-Competitive-Code-Reasoning-with-a-Large-Scale-Verified-Dataset"><a href="#rStar-Coder-Scaling-Competitive-Code-Reasoning-with-a-Large-Scale-Verified-Dataset" class="headerlink" title="rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale   Verified Dataset"></a>rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale   Verified Dataset</h2><p><strong>Authors:Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, Mao Yang</strong></p>
<p>Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at <a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar">https://github.com/microsoft/rStar</a>. </p>
<blockquote>
<p>推动大型语言模型（LLM）中的代码推理能力，从根本上受限于高质量数据集的稀缺性，尤其是那些具有可验证的输入输出测试用例，这些用例对于大规模严格解决方案验证是必不可少的。我们引入了rStar-Coder，它通过构建包含41.8万个竞赛级代码问题、58万个长期推理解决方案以及不同难度丰富的测试用例的大规模验证数据集，显著提高了LLM的代码推理能力。这是通过三个核心贡献实现的：（1）我们精选竞赛编程代码问题和标准解决方案，合成新的可解决问题；（2）我们引入了一个可靠的输入输出测试用例合成管道，将生成解耦为三步输入生成方法和有效的输出标签互验证机制；（3）我们为问题增加了高质量、测试用例验证的长期推理解决方案。在Qwen模型（1.5B-14B）上的广泛实验，以及各种代码推理基准测试表明，rStar-Coder数据集具有卓越性，其性能领先，甚至在一些较小的模型大小方面也达到了前沿推理LLM的水平。在LiveCodeBench上，rStar-Coder将Qwen2.5-7B的性能从17.4%提高到令人印象深刻的57.3%，将Qwen2.5-14B的性能从23.3%提高到62.5%，超过了o3-mini (low) 3.1%。在更具挑战性的美国计算奥林匹克竞赛中，我们的7B模型平均pass@1准确率达到了16.15%，超越了前沿的QWQ-32B。代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/microsoft/rStar上发布。</a> </p>
</blockquote>
<p><strong>简化翻译</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21297v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了微软推出的rStar-Coder数据集，它通过构建大规模验证数据集显著提高了LLM的代码推理能力。数据集包含41.8万个竞赛级别的代码问题，以及丰富的测试用例和验证过的长推理解决方案。rStar-Coder通过三个核心贡献实现这一目标：从竞争性编程中筛选问题和解决方案合成新问题；引入可靠的输入-输出测试用例合成管道；为问题提供高质量、测试案例验证的长推理解决方案。实验证明，rStar-Coder数据集在多个代码推理基准测试中表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rStar-Coder通过构建大规模验证数据集提升了LLM的代码推理能力。</li>
<li>数据集包含超过41.8万个竞赛级别的代码问题，涵盖丰富的测试用例和验证过的长推理解决方案。</li>
<li>rStar-Coder通过三个核心方法实现性能提升：筛选竞赛级编程问题、合成新的测试用例，并为这些问题提供可靠验证的长推理解决方案。</li>
<li>实验表明，rStar-Coder在各种代码推理基准测试中表现领先。在特定基准测试中，它比最前沿的推理LLM表现更为出色。它尤其能显著提高小型模型的性能。例如，在LiveCodeBench上，模型性能从不到四分之一提高到超过一半。在美国计算奥林匹竞赛上，一个中等规模的模型表现出强大的性能，平均准确率超过百分之十六点一五，超过了最前沿的大型模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e6fe01f5c03ac19337d89f54f52a62aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a24ccbbe969d374d8d12b6be39fdead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be7d0c9c5bb6dd84f3a071a355bdc2f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2619d813959d9af86a69315fc4ae7ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d331655456756f0d5bf2afdbfc2c649.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Walk-Before-You-Run-Concise-LLM-Reasoning-via-Reinforcement-Learning"><a href="#Walk-Before-You-Run-Concise-LLM-Reasoning-via-Reinforcement-Learning" class="headerlink" title="Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning"></a>Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning</h2><p><strong>Authors:Mingyang Song, Mao Zheng</strong></p>
<p>As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model’s reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the “walk before you run” principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks. </p>
<blockquote>
<p>随着测试时缩放成为大型语言模型（LLM）发展的核心研究前沿，当代先进的后训练方法论越来越关注扩展长思维链（CoT）响应的生成长度，以提高向DeepSeek R1等性能的推理能力。然而，最近的研究表明，最先进的推理模型中存在一种持续过度思考的现象，表现为长CoT响应中的过度冗余或重复思考模式。为了解决这一问题，本文提出了一种简单有效的两阶段强化学习框架，用于在LLM中实现简洁推理，名为ConciseR。具体来说，第一阶段使用更多的训练步骤，旨在通过带有剪辑更高和动态采样组件的群组相对策略优化（GRPO++）来激励模型的推理能力；第二阶段使用较少的训练步骤，通过长度感知群组相对策略优化（L-GRPO）明确强调简洁性并提高效率。值得注意的是，ConciseR仅在样本所有rollouts都正确的情况下优化响应长度，遵循“走在你跑之前”的原则。广泛的实验结果表明，我们的ConciseR模型生成了更简洁的CoT推理响应，在AIME 2024、MATH-500、AMC 2023、Minerva和Olympiad等多个基准测试中，超越了无强化学习范式下的最新先进推理模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21178v1">PDF</a> Ongoing Work</p>
<p><strong>Summary</strong></p>
<p>本文探讨了在大型语言模型（LLM）发展中，测试时缩放技术的重要性。针对当前先进的训练后方法在提高推理能力时产生的过度思考现象，本文提出了一种简单有效的两阶段强化学习框架ConciseR。第一阶段通过集团相对策略优化（GRPO++）激励模型的推理能力，第二阶段通过长度感知集团相对策略优化（L-GRPO）促进简洁性和效率。实验结果表明，ConciseR模型在AIME 2024、MATH-500、AMC 2023、Minerva和Olympiad等多个基准测试中优于最近的先进推理模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的发展中，测试时缩放技术是关键的研究前沿。</li>
<li>当代先进的训练后方法致力于通过扩展生成长度来提高Chain-of-Thought（CoT）响应的推理能力。</li>
<li>在CoT响应中出现了过度思考现象，表现为冗余或重复思考模式。</li>
<li>提出了一种新的强化学习框架ConciseR来解决这一问题。</li>
<li>ConciseR分为两个阶段：第一阶段激励模型的推理能力，第二阶段促进简洁性和效率。</li>
<li>ConciseR只在所有样本的所有rollout正确后优化响应长度，遵循“先走再跑”的原则。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17f8ccd85b55cfeb130f0150b9a031fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47f990dd9561de9b7962f4eeb4ddbdf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44258cc4f9135acea1f9506ae9911a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7696d5d7ea24a341649f090568a9581.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-Think-But-Not-In-Your-Flow-Reasoning-Level-Personalization-for-Black-Box-Large-Language-Models"><a href="#LLMs-Think-But-Not-In-Your-Flow-Reasoning-Level-Personalization-for-Black-Box-Large-Language-Models" class="headerlink" title="LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for   Black-Box Large Language Models"></a>LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for   Black-Box Large Language Models</h2><p><strong>Authors:Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, Dongha Lee</strong></p>
<p>Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMs–served via APIs without access to model internals–are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the model’s reasoning process with a user’s personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）最近在多种自然语言任务中取得了令人印象深刻的性能，并现在广泛应用于实际世界应用。其中，黑盒LLM（通过API提供服务，无法访问模型内部）由于其可扩展性和易于部署的特点而尤其占据主导地位。尽管这些模型具有强大的能力，但它们通常会产生忽略个人偏好和推理风格的通用响应。这引发了人们对黑盒LLM个性化的兴趣不断增长，其目标是针对用户特定上下文定制模型输出，而不修改模型参数。然而，现有方法主要侧重于响应级个性化，试图匹配最终输出，而没有对个人的思考过程进行建模。为了解决这一局限性，我们提出了RPM，一个用于推理级个性化的框架，它将模型的推理过程与用户个性化的逻辑对齐。RPM首先通过从用户历史中提取和分组影响响应的特征来构建用户特定的统计因素。然后，它建立反映这些因子如何在上下文中使用的个性化推理路径。在推理阶段，RPM通过特征级别的相似性检索与新查询相匹配的推理对齐示例，并在结构化因素和检索到的推理路径的条件下进行推理，使模型能够遵循用户特定的推理轨迹。这种推理级个性化通过结构化信息将模型输出根植于用户特定的逻辑中，提高了预测准确性和可解释性。在多种任务上的广泛实验表明，RPM始终优于响应级个性化方法，证明了在黑盒LLM中进行推理级个性化的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多种自然语言任务中表现出卓越性能，并在实际应用中广泛使用。其中，黑盒LLM（通过API提供服务，无法访问模型内部）因其可扩展性和易于部署的特点尤其占据主导地位。然而，这些模型通常产生忽略个人偏好和推理风格的通用响应。为解决这一问题，提出了RPM框架，实现推理级个性化，使模型推理过程与用户个性化逻辑相匹配。RPM通过提取和分组用户历史中的响应影响因素来构建用户特定因素，并建立反映这些因素如何在上下文中使用的个性化推理路径。在推理阶段，RPM通过特征级别相似性检索与推理相匹配的示例，并在结构化因素和检索到的推理路径上进行推理，使模型遵循用户特定的推理轨迹。这种推理级个性化通过结构化信息提高预测准确性和可解释性，使模型输出根植于用户特定逻辑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自然语言任务中表现优异，并在实际应用中广泛应用。</li>
<li>黑盒LLM因其可扩展性和易于部署而尤为受欢迎，但缺乏个性化响应。</li>
<li>现有方法主要关注响应级个性化，尝试匹配最终输出，但未建模个人思维过程。</li>
<li>提出RPM框架实现推理级个性化，与用户的个性化逻辑相匹配。</li>
<li>RPM构建用户特定因素并建立个性化推理路径，反映用户历史中的响应影响因素和上下文使用方式。</li>
<li>RPM通过特征级别相似性检索推理匹配的示例，并在结构化因素和检索到的推理路径上进行推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0a3733893091275df5028f5b72420025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1521ddac4df3960b0637c7bf407a7c87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bc16abc5a04b39fb723ca11837e6b89.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Large-Language-Model-Inference-with-Neural-Block-Linearization"><a href="#Efficient-Large-Language-Model-Inference-with-Neural-Block-Linearization" class="headerlink" title="Efficient Large Language Model Inference with Neural Block Linearization"></a>Efficient Large Language Model Inference with Neural Block Linearization</h2><p><strong>Authors:Mete Erdogan, Francesco Tonin, Volkan Cevher</strong></p>
<p>The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）的高推理需求在其部署过程中带来了很大的挑战。为了解决这一问题，我们引入了神经块线性化（NBL）这一新型框架，通过用基于线性最小均方误差估计的线性近似值替换自注意力层来加速Transformer模型的推理。NBL利用典型相关性分析来计算近似误差的理论上限。然后，我们将这个界限作为替代标准，选择线性化误差最低的LLM层。NBL可以高效应用于预训练的LLM，无需微调。在实验中，NBL在多个推理基准测试中实现了显著的计算速度提升，同时保持了竞争力。例如，将NBL应用于DeepSeek-R1-Distill-Llama-8B的12个自注意力层，在仅牺牲不到1%准确率的情况下，推理速度提高了32%，使其成为提高LLM推理效率的一种灵活且具前景的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>神经网络块线性化（NBL）是一种用于加速基于Transformer的大型语言模型（LLM）推理的新型框架。它通过利用线性最小均方误差估计器产生的线性近似替换自注意力层来实现推理加速。NBL使用典型相关性分析计算近似误差的理论上限，并以此作为替换标准，选择线性化误差最低的LLM层。实验表明，NBL在保持竞争力的同时显著提高了计算速度。例如，将NBL应用于DeepSeek-R1-Distill-Llama-8B的12个自注意力层，可在仅损失不到1%准确率的情况下提高32%的推理速度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经网络块线性化（NBL）旨在加速基于Transformer的大型语言模型（LLM）的推理过程。</li>
<li>NBL通过用线性最小均方误差估计产生的线性近似替换自注意力层来实现推理加速。</li>
<li>NBL使用典型相关性分析计算理论上的近似误差上限，作为选择替换层的标准。</li>
<li>NBL可高效应用于预训练LLM，无需微调。</li>
<li>实验表明，NBL在多个推理基准测试中实现了显著的计算速度提升，同时保持了竞争力。</li>
<li>在DeepSeek-R1-Distill-Llama-8B模型中应用NBL，可提高32%的推理速度，仅损失不到1%的准确率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-67a3fc631e80a55cf8b1a59a809caf68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10b77bb3310666f6dcd53cffcaae86a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93cb873dd5ef89d79d26e717f56d0e6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3306165221fc39ab37f32e3b939f8ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a34df7e3921e03ee9ed027887148d69.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Def-DTS-Deductive-Reasoning-for-Open-domain-Dialogue-Topic-Segmentation"><a href="#Def-DTS-Deductive-Reasoning-for-Open-domain-Dialogue-Topic-Segmentation" class="headerlink" title="Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation"></a>Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation</h2><p><strong>Authors:Seungmin Lee, Yongsang Yoo, Minhwa Jung, Min Song</strong></p>
<p>Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS. </p>
<blockquote>
<p>对话主题分段（DTS）旨在将对话划分为连贯的段落。DTS在各种NLP下游任务中扮演着至关重要的角色，但它也面临着一些长期存在的问题：数据短缺、标签模糊以及最近提出的解决方案的复杂性不断增加。另一方面，尽管大型语言模型（LLM）和推理策略有所进展，但它们很少被应用于DTS。本文介绍了Def-DTS：基于开放域对话主题的归纳推理分段方法，该方法利用基于LLM的多步归纳推理来提高DTS的性能，并使用中间结果进行案例研究。我们的方法采用结构化提示方法进行双向上下文摘要、话语意图分类和归纳主题转移检测。在意图分类过程中，我们提出了用于领域无关的对话意图分类的可推广意图列表。在各种对话场景中的实验表明，Def-DTS始终优于传统和最新方法，每个子任务都对性能提升有所贡献，特别是在减少第二类错误方面。我们还探索了自动标签的潜力，强调LLM推理技术在DTS中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21033v1">PDF</a> 19 pages, 3 figures, Accepted to Findings of the ACL 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了Def-DTS方法，该方法利用基于大型语言模型的多步演绎推理，旨在提高对话主题分割的性能。该方法通过结构化提示进行双向上下文摘要、话语意图分类和演绎主题转移检测。实验表明，Def-DTS在多种对话场景中始终优于传统和最新的方法，特别是减少了第二类错误的出现。同时探讨了自动标注的潜力，强调LLM推理技术在DTS中的重要性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Def-DTS方法旨在利用大型语言模型的多步演绎推理提高对话主题分割的性能。</li>
<li>该方法通过结构化提示进行双向上下文摘要，有助于理解对话的连贯性和上下文关系。</li>
<li>Def-DTS包括话语意图分类的任务，并提出了通用的意图列表，用于非特定领域的对话意图分类。</li>
<li>演绎主题转移检测是Def-DTS的重要组成部分，有助于准确识别对话主题的转变。</li>
<li>实验结果显示，Def-DTS在多种对话场景中表现优异，优于传统和最新的方法。</li>
<li>Def-DTS通过减少第二类错误，提高了对话主题分割的准确性和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21033">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c71bf57d5cf44744b9dcf424373dbcaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26ca14a337329cb98efdaea65a2593ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819d5efdc144be34311fc6a378eef33a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06e0b091c9f50f6524da8eb9f4a78edc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e697fe3ff619bb9c795d2acfdca7ba01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d1853595dd997ca37ca4684fa3d347.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reason-Align-Respond-Aligning-LLM-Reasoning-with-Knowledge-Graphs-for-KGQA"><a href="#Reason-Align-Respond-Aligning-LLM-Reasoning-with-Knowledge-Graphs-for-KGQA" class="headerlink" title="Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for   KGQA"></a>Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for   KGQA</h2><p><strong>Authors:Xiangqing Shen, Fanfan Wang, Rui Xia</strong></p>
<p>LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference. </p>
<blockquote>
<p>大型语言模型（LLMs）在复杂的推理任务中表现出了显著的能力，但它们常常出现幻觉，缺乏可靠的的事实基础。同时，知识图谱（KGs）提供了结构化的事实知识，但缺乏LLMs的灵活推理能力。在本文中，我们提出了Reason-Align-Respond（RAR）框架，该框架系统地整合了LLM推理和知识图谱，用于知识图谱问答（KGQA）。我们的方法由三个关键组件组成：生成类似人类的推理链的推理器，将这些链映射到有效的知识图谱路径的对齐器，以及合成最终答案的响应器。我们将这个过程制定为概率模型，并使用期望最大化算法进行优化，该算法通过迭代改进推理链和知识路径。在多个基准测试上的大量实验证明了RAR的有效性，在WebQSP和CWQ上的命中率得分分别为93.3%和91.0%，达到了最先进的性能。人类评估证实，RAR生成的推理链高质量且可解释性强，与知识图谱路径很好地对齐。此外，RAR具有较强的零样本泛化能力，并在推理过程中保持了计算效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20971v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMs与知识图谱（KGs）结合展现出色性能。新框架Reason-Align-Respond（RAR）整合了LLM推理与知识图谱，用于知识图谱问答（KGQA）。RAR包括三个关键组件：生成人类式推理链的Reasoner，将这些链映射到有效KG路径的Aligner，以及合成最终答案的Respond。此过程被公式化为概率模型并使用期望最大化算法进行优化，提高了性能并展示出强泛化能力和计算效率。实验表明RAR表现优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs与知识图谱结合能够提高性能。</li>
<li>RAR框架整合了LLM推理与知识图谱用于KGQA任务。</li>
<li>RAR包含三个关键组件：Reasoner，Aligner和Responder。</li>
<li>RAR采用概率模型并使用期望最大化算法优化过程。</li>
<li>RAR实现高水平性能，达到WebQSP和CWQ基准测试的最优水平。</li>
<li>RAR生成高质量、可解释的推理链，与人类知识图谱路径对齐良好。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20971">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c76d03befd9b7706fae6c7b885661816.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6faffb4a0674f383477c77a94c61b5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3701fc66f2d03e4ce67c4d6b335f44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca26a6948ebee23d7fb996c6579190a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Cross-from-Left-to-Right-Brain-Adaptive-Text-Dreamer-for-Vision-and-Language-Navigation"><a href="#Cross-from-Left-to-Right-Brain-Adaptive-Text-Dreamer-for-Vision-and-Language-Navigation" class="headerlink" title="Cross from Left to Right Brain: Adaptive Text Dreamer for   Vision-and-Language Navigation"></a>Cross from Left to Right Brain: Adaptive Text Dreamer for   Vision-and-Language Navigation</h2><p><strong>Authors:Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li</strong></p>
<p>Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \href{<a target="_blank" rel="noopener" href="https://github.com/zhangpingrui/Adaptive-Text-Dreamer%7D%7Bhere%7D">https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}</a>. </p>
<blockquote>
<p>视觉与语言导航（VLN）要求智能体在部分可观察性的情况下遵循自然语言指令进行导航，这使得感知与语言的对齐变得困难。最近的方法通过想象未来的场景来缓解这一问题，但它们依赖于基于视觉的合成，导致计算成本高昂和细节冗余。为此，我们提出通过语言形式来适应性地想象关键环境语义，从而实现更可靠和高效的策略。具体来说，我们引入了一种新型的自适应文本梦想家（ATD），这是一种基于大型语言模型（LLM）的双分支自我引导想象策略。ATD的设计具有人类左右脑架构，其中左脑专注于逻辑整合，右脑负责对未来场景进行想象预测。为了实现这一点，我们只微调了左右脑中的Q-former，以有效激活LLM中的领域特定知识，从而在导航过程中实现逻辑和想象的动态更新。此外，我们引入了一种交叉交互机制来规范想象输出并将其注入导航专家模块，使ATD能够联合利用LLM的推理能力和导航模型的专长。我们在R2R基准测试上进行了大量实验，ATD以较少的参数实现了最先进的性能。代码详见：<a target="_blank" rel="noopener" href="https://github.com/zhangpingrui/Adaptive-Text-Dreamer">https://github.com/zhangpingrui/Adaptive-Text-Dreamer</a>。</p>
</blockquote>
<p><strong>简化翻译</strong>（更简洁的版本）：</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20897v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本讨论了Vision-and-Language Navigation的问题，包括通过语言指导进行导航的难度和挑战。提出了一个自适应文本梦想家模型来解决这一问题，采用基于语言形式的未来场景想象，减少了计算成本和冗余细节。该模型通过自我引导策略生成左右脑协同工作实现逻辑推理和想象能力。在R2R基准测试中取得了最佳性能。具体信息可访问提供的链接。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision-and-Language Navigation面临感知与语言对齐的挑战。</li>
<li>自适应文本梦想家模型通过基于语言形式的未来场景想象解决此问题。</li>
<li>模型采用双分支自我引导策略，模拟人类左右脑功能实现逻辑推理和想象。</li>
<li>模型引入交叉互动机制，结合大型语言模型的推理能力和导航模型的专长。</li>
<li>在R2R基准测试中，自适应文本梦想家模型性能达到最佳水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6ef9068995f70446cb1de55746ac5d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f77dfa94c1ecef5ba5ff08ac6a4cffc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b6c15b1fd6e7e3bc4e633f3031cea8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Step-Wise-Formal-Verification-for-LLM-Based-Mathematical-Problem-Solving"><a href="#Step-Wise-Formal-Verification-for-LLM-Based-Mathematical-Problem-Solving" class="headerlink" title="Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving"></a>Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving</h2><p><strong>Authors:Kuo Zhou, Lu Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated formidable capabilities in solving mathematical problems, yet they may still commit logical reasoning and computational errors during the problem-solving process. Thus, this paper proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for formally verifying the correctness of the solutions generated by large language models. Our framework first utilizes a Formalizer which employs an LLM to translate a natural language solution into a formal context. Afterward, our Critic (which integrates various external tools such as a Computer Algebra System and an SMT solver) evaluates the correctness of each statement within the formal context, and when a statement is incorrect, our Critic provides corrective feedback. We empirically investigate the effectiveness of MATH-VF in two scenarios: 1) Verification: MATH-VF is utilized to determine the correctness of a solution to a given problem. 2) Refinement: When MATH-VF identifies errors in the solution generated by an LLM-based solution generator for a given problem, it submits the corrective suggestions proposed by the Critic to the solution generator to regenerate the solution. We evaluate our framework on widely used mathematical benchmarks: MATH500 and ProcessBench, demonstrating the superiority of our approach over existing approaches. </p>
<blockquote>
<p>大型语言模型（LLM）在解决数学问题方面表现出了强大的能力，但在问题解决过程中仍可能出现逻辑推理和计算错误。因此，本文提出了一个名为MATH-VF的框架，该框架包括一个格式化器和一个批判家，用于验证大型语言模型生成的解决方案的正确性。我们的框架首先使用一个格式化器，它利用大型语言模型将自然语言解决方案转换为正式语境。随后，我们的批判家（集成了各种外部工具，如计算机代数系统和SMT求解器）评估正式语境中每个语句的正确性，并在语句错误时提供反馈。我们通过实证研究验证了MATH-VF在两种场景中的有效性：1）验证：使用MATH-VF确定给定问题的解决方案是否正确。2）精进：当MATH-VF识别出大型语言模型生成的给定问题解决方案中的错误时，它将批判家提出的修正建议提交给解决方案生成器以重新生成解决方案。我们在广泛使用的数学基准测试MATH500和ProcessBench上评估了我们的框架，证明了我们的方法优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在解决数学问题方面展现出强大能力，但在逻辑推理和计算过程中仍可能出现错误。本文提出一个名为MATH-VF的框架，包括形式化器和评审器，用于验证大型语言模型生成的解决方案的正确性。该框架首先使用形式化器将自然语言解决方案转化为正式语境，然后评审器结合计算机代数系统和SMT求解器等外部工具评估正式语境中的每个陈述，一旦发现错误，即提供反馈。本文通过验证和细化两个场景实证研究了MATH-VF的有效性，并在MATH500和ProcessBench等常用数学基准测试上展示了其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在解决数学问题方面展现出强大能力，但存在逻辑推理和计算错误。</li>
<li>MATH-VF框架包括形式化器和评审器，用于验证LLM生成的解决方案的正确性。</li>
<li>形式化器将自然语言解决方案转化为正式语境。</li>
<li>评审器结合外部工具如计算机代数系统和SMT求解器评估陈述的正确性，并提供反馈。</li>
<li>MATH-VF框架在验证和细化两个场景中具有有效性。</li>
<li>MATH-VF框架在MATH500和ProcessBench等数学基准测试上表现出优越性。</li>
<li>该框架有助于提升LLM在解决数学问题时的准确性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1f652be7fe4dee68fdebff1b65c568e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-439d46378e7ff452a7687b0098fe4070.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac12f69efc9d11e79da1fa1bbdd0c127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3518f30459c20a82036d7312eba3d4fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7a03df0f2448a2aa975bf0cec38a7f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Reinforced-Informativeness-Optimization-for-Long-Form-Retrieval-Augmented-Generation"><a href="#Reinforced-Informativeness-Optimization-for-Long-Form-Retrieval-Augmented-Generation" class="headerlink" title="Reinforced Informativeness Optimization for Long-Form   Retrieval-Augmented Generation"></a>Reinforced Informativeness Optimization for Long-Form   Retrieval-Augmented Generation</h2><p><strong>Authors:Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang</strong></p>
<p>Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/RioRAG">https://github.com/RUCAIBox/RioRAG</a>. </p>
<blockquote>
<p>长问答（LFQA）为大语言模型带来了独特的挑战，要求合成连贯的、段落长度的答案。虽然基于检索的生成（RAG）系统已成为一种有前景的解决方案，但现有研究在关键方面存在局限：高质量训练数据缺乏用于长形式生成，扩展输出中的幻想风险累积，以及缺乏可靠的评价指标来评估事实完整性。在本文中，我们提出了RioRAG，这是一种新型强化学习（RL）框架，通过强化信息优化推进长形式RAG。我们的方法引入了两个基本创新来解决核心挑战。首先，我们开发了一种强化信息优化的RL训练范式，直接优化信息性，有效解决传统RAG系统中的慢思考缺陷，无需昂贵的监督数据。其次，我们提出了一种以片段为中心的分层奖励建模方法，通过三个阶段的过程精确评估长形式答案：从每个源网页中提取片段、构建片段声明清单、根据事实一致性计算奖励。在LongFact和RAGChecker两个LFQA基准测试上的大量实验证明了所提方法的有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/RioRAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/RioRAG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在长篇问答（LFQA）中，大型语言模型面临独特挑战，需要合成连贯的、段落长度的答案。虽然检索增强生成（RAG）系统已成为一种有前景的解决方案，但现有研究面临关键限制：高质量训练数据缺乏、长输出中的虚构风险以及缺乏可靠的评价指标来评估事实完整性。本文提出RioRAG，一种新型的强化学习（RL）框架，通过强化信息优化来推进长篇RAG。我们的方法引入了两个基本创新点来解决核心挑战：首先，我们开发了一种强化信息优化的RL训练范式，直接优化信息性，有效解决传统RAG系统中的慢思考缺陷，无需昂贵的监督数据；其次，我们提出了一种以片段为中心的分层奖励建模方法，通过三个阶段的过程精确评估长篇答案：从每个源网页中提取片段、构建片段声明清单、根据事实对齐计算奖励。在LongFact和RAGChecker两个LFQA基准测试上的实验证明了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长篇问答（LFQA）对大型语言模型提出独特挑战，需要生成连贯的、段落长度的答案。</li>
<li>检索增强生成（RAG）系统虽为有前途的解决方案，但存在高质量训练数据缺乏、长输出中的虚构风险及评价指标缺失等限制。</li>
<li>RioRAG框架通过强化信息优化解决这些挑战，引入强化学习（RL）训练范式和分层奖励建模方法。</li>
<li>RL训练范式直接优化信息性，提升传统RAG系统的性能，降低对昂贵监督数据的依赖。</li>
<li>分层奖励建模方法通过提取、构建和计算奖励三个阶段精确评估长篇答案。</li>
<li>在LongFact和RAGChecker基准测试上的实验证明了RioRAG框架的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c28f3c2ea39d7590877a0af029bc117c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d80eb2d8c3fdc9753ca28e9b6c88df.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Understand-Think-and-Answer-Advancing-Visual-Reasoning-with-Large-Multimodal-Models"><a href="#Understand-Think-and-Answer-Advancing-Visual-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Understand, Think, and Answer: Advancing Visual Reasoning with Large   Multimodal Models"></a>Understand, Think, and Answer: Advancing Visual Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang</strong></p>
<p>Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at <a target="_blank" rel="noopener" href="https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R">https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R</a> soon. </p>
<blockquote>
<p>大型多模态模型（LMM）最近在视觉语言任务和以视觉为中心的任务上都表现出了显著的理解性能。然而，它们在整合用于组合推理的先进、特定任务功能方面常常不足，这阻碍了它们成为真正有能力的一般视觉模型。为了解决这一问题，我们提出了一种统一的视觉推理机制，使LMM能够通过利用其内在能力（例如定位和视觉理解能力）来解决复杂的组合问题。不同于之前的捷径学习机制，我们的方法引入了一种类似人类的“理解-思考-回答”过程，使模型能够在单次前向传递中完成所有步骤，无需多次推理或外部工具。这种设计缩小了基本视觉能力和通用问答之间的差距，鼓励LMM为复杂的视觉推理生成忠实可靠的答案。同时，我们精心制作了包含一般场景和文本丰富场景的视觉指令样本，涉及多种基本视觉能力。我们训练的模型Griffon-R具有端到端自动理解、自我思考和推理回答的能力。综合实验表明，Griffon-R不仅在包括VSR和CLEVR的复杂视觉推理基准测试中取得了进展，而且在各种基准测试中增强了多模态能力，如MMBench和ScienceQA。数据、模型和代码将很快在<a target="_blank" rel="noopener" href="https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20753v1">PDF</a> Tech report</p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型多模态模型（LMMs）在视觉理解和视觉任务上的出色表现，但其在组合推理方面存在不足。为此，提出了一种统一的视觉推理机制，使LMMs能够解决复杂的组合问题，并利用其内在能力（如接地和视觉理解能力）。该方法不同于以往的捷径学习机制，引入人类般的理解、思考和回答过程，使模型一次前向传递即可完成所有步骤，无需多次推理或外部工具。同时，为了训练模型Griffon-R，收集了33.4万份视觉指令样本，涵盖一般场景和文本丰富场景，涉及多种基础视觉能力。实验表明，Griffon-R在复杂视觉推理基准测试上取得了先进性能，并增强了多模态能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）在视觉理解和任务上表现优异，但在组合推理方面存在不足。</li>
<li>提出一种统一的视觉推理机制，使LMMs能够解决复杂的组合问题。</li>
<li>该机制不同于以往的捷径学习，引入人类般的理解、思考和回答过程。</li>
<li>模型Griffon-R通过一次前向传递完成所有步骤，无需多次推理或外部工具。</li>
<li>为了训练Griffon-R，收集了涵盖多种场景的33.4万份视觉指令样本。</li>
<li>实验表明Griffon-R在复杂视觉推理基准测试上取得了先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef2da0a02124cf75f121bb28ab44163a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59849f247b07428f9b3f1393a15a28fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba677c25b5bb8cc8fe24ec34124e63ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99d471b2b51219ed86a86fb4ae1f05cc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-985e4ebb2bd7ddb1f91621b542159685.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-29  Silence is Not Consensus Disrupting Agreement Bias in Multi-Agent LLMs   via Catfish Agent for Clinical Decision Making
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-28/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-42553c2c44a96e95084b67babc2c1747.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-05-28  Absolute Coordinates Make Motion Generation Easy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
