<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Towards Robust Automated Perceptual Voice Quality Assessment with Deep   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-27a4eb1027100c021e210d447c2a5f1f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-29-æ›´æ–°"><a href="#2025-05-29-æ›´æ–°" class="headerlink" title="2025-05-29 æ›´æ–°"></a>2025-05-29 æ›´æ–°</h1><h2 id="Towards-Robust-Automated-Perceptual-Voice-Quality-Assessment-with-Deep-Learning"><a href="#Towards-Robust-Automated-Perceptual-Voice-Quality-Assessment-with-Deep-Learning" class="headerlink" title="Towards Robust Automated Perceptual Voice Quality Assessment with Deep   Learning"></a>Towards Robust Automated Perceptual Voice Quality Assessment with Deep   Learning</h2><p><strong>Authors:Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Objective: Perceptual voice quality assessment plays a critical role in diagnosing and monitoring voice disorders by providing standardized evaluation of vocal function. Traditionally, this process relies on expert raters utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are inherently subjective and susceptible to inter-rater variability, motivating the need for automated and objective assessment methods. Methods: We propose Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to capture high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we present VOQANet+, which integrates handcrafted acoustic features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings. Results: Sentence-based input yields stronger performance than vowel-based input, especially at the patient level. VOQANet consistently outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even better and maintains robustness under noisy conditions. Conclusion: Combining SFM embeddings with domain-informed acoustic features improves interpretability and resilience. Significance: VOQANet+ shows strong potential for deployment in real-world and telehealth settings, addressing the limitations of subjective perceptual assessments with an interpretable and noise-resilient solution. </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ„ŸçŸ¥è¯­éŸ³è´¨é‡è¯„ä¼°åœ¨é€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°è¯­éŸ³åŠŸèƒ½æ¥è¯Šæ–­å’Œç›‘æµ‹è¯­éŸ³éšœç¢æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºä¸“å®¶è¯„ä¼°è€…ä½¿ç”¨æ ‡å‡†é‡è¡¨ï¼Œå¦‚è¯­éŸ³å…±è¯†æ„ŸçŸ¥è¯„ä¼°ï¼ˆCAPE-Vï¼‰å’Œç­‰çº§ã€ç²—ç³™åº¦ã€æ°”æ¯ã€è™šå¼±å’Œç´§å¼ åº¦ï¼ˆGRBASï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œå®¹æ˜“å‡ºç°è¯„ä¼°è€…é—´å˜å¼‚ï¼Œè¿™æ¿€å‘äº†å¯¹è‡ªåŠ¨åŒ–å’Œå®¢è§‚è¯„ä¼°æ–¹æ³•çš„éœ€æ±‚ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­éŸ³è´¨é‡è¯„ä¼°ç½‘ç»œï¼ˆVOQANetï¼‰ï¼Œè¯¥ç½‘ç»œå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ä»åŸå§‹è¯­éŸ³ä¸­æ•è·é«˜çº§å£°éŸ³å’ŒéŸµå¾‹ä¿¡æ¯ã€‚ä¸ºäº†æé«˜ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VOQANet+ï¼Œå®ƒå°†æ‰‹å·¥åˆ¶ä½œçš„å£°å­¦ç‰¹å¾ï¼ˆå¦‚æŠ–åŠ¨ã€é¢¤æŠ–å’Œè°æ³¢ä¸å™ªå£°æ¯”ï¼ˆHNRï¼‰ï¼‰ä¸SFMåµŒå…¥ç›¸ç»“åˆã€‚ç»“æœï¼šåŸºäºå¥å­çš„è¾“å…¥æ¯”åŸºäºå…ƒéŸ³çš„è¾“å…¥è¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‚£è€…å±‚é¢ã€‚VOQANetåœ¨RMSEå’ŒPCCæ–¹é¢çš„è¡¨ç°ä¸€ç›´ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè€ŒVOQANet+è¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨å˜ˆæ‚æ¡ä»¶ä¸‹ä¿æŒç¨³å¥æ€§ã€‚ç»“è®ºï¼šå°†SFMåµŒå…¥ä¸é¢†åŸŸçŸ¥è¯†é©±åŠ¨çš„å£°å­¦ç‰¹å¾ç›¸ç»“åˆï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚æ„ä¹‰ï¼šVOQANet+åœ¨ç°å®ä¸–ç•Œå’Œè¿œç¨‹åŒ»ç–—ç¯å¢ƒä¸­å…·æœ‰å¾ˆå¼ºçš„éƒ¨ç½²æ½œåŠ›ï¼Œä¸ºè§£å†³ä¸»è§‚æ„ŸçŸ¥è¯„ä¼°çš„å±€é™æ€§æä¾›äº†ä¸€ç§å¯è§£é‡Šå’Œå™ªå£°é¡½å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21356v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è¯­éŸ³è´¨é‡è¯„ä¼°ç½‘ç»œï¼ˆVOQANetï¼‰ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°å—“éŸ³åŠŸèƒ½ï¼Œå¯¹å—“éŸ³éšœç¢çš„è¯Šæ–­å’Œç›‘æµ‹èµ·åˆ°å…³é”®ä½œç”¨ã€‚ç ”ç©¶å¼•å…¥è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰æ•æ‰åŸå§‹è¯­éŸ³ä¸­çš„é«˜çº§å£°å­¦ç‰¹å¾ï¼Œå¹¶é›†æˆæ‰‹å·¥åˆ¶ä½œçš„å£°å­¦ç‰¹å¾ï¼Œå¦‚æŠ–åŠ¨ã€é—ªçƒå’Œè°æ³¢å™ªå£°æ¯”ï¼ˆHNRï¼‰ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºå¥å­çš„è¾“å…¥ç›¸æ¯”åŸºäºå…ƒéŸ³çš„è¾“å…¥æ€§èƒ½æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‚£è€…å±‚é¢ã€‚VOQANetåœ¨RMSEå’ŒPCCä¸ŠæŒç»­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè€Œç»“åˆé¢†åŸŸçŸ¥è¯†çš„VOQANet+ï¼ˆé›†æˆå£°å­¦ç‰¹å¾çš„ç‰ˆæœ¬ï¼‰è¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨å™ªå£°ç¯å¢ƒä¸‹ä¿æŒç¨³å¥æ€§ã€‚ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–å’Œå®¢è§‚è¯„ä¼°è¯­éŸ³è´¨é‡æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œå…·æœ‰åœ¨çœŸå®ä¸–ç•Œå’Œè¿œç¨‹åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³è´¨é‡è¯„ä¼°åœ¨å—“éŸ³éšœç¢çš„è¯Šæ–­å’Œç›‘æµ‹ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ä¾èµ–äºä¸“å®¶è¯„ä¼°è€…ä½¿ç”¨æ ‡å‡†é‡è¡¨ï¼Œå­˜åœ¨ä¸»è§‚æ€§å’Œè¯„åˆ†è€…é—´å˜å¼‚æ€§çš„ç¼ºç‚¹ã€‚</li>
<li>æå‡ºçš„VOQANetåˆ©ç”¨æ·±åº¦å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œå¼•å…¥SFMæ•æ‰é«˜çº§å£°å­¦ç‰¹å¾ã€‚</li>
<li>ç»“åˆæ‰‹å·¥åˆ¶ä½œçš„å£°å­¦ç‰¹å¾ï¼ˆå¦‚æŠ–åŠ¨ã€é—ªçƒå’ŒHNRï¼‰å¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åŸºäºå¥å­çš„è¾“å…¥åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºäºå…ƒéŸ³çš„è¾“å…¥ï¼Œå°¤å…¶åœ¨æ‚£è€…å±‚é¢è¡¨ç°æ›´ä½³ã€‚</li>
<li>VOQANetåœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-153990689a22df8599d743083ab74f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6077b431d84a52a6d06c68889d5118e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12f403025e6a79995f21db489a6a0d85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70830002732b0f1fc897180e1da153d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a12ca3c686294871280b7182b1709d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f0431d5f0e781cddca7c724319c7bea.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PSRB-A-Comprehensive-Benchmark-for-Evaluating-Persian-ASR-Systems"><a href="#PSRB-A-Comprehensive-Benchmark-for-Evaluating-Persian-ASR-Systems" class="headerlink" title="PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems"></a>PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems</h2><p><strong>Authors:Nima Sedghiyeh, Sara Sadeghi, Reza Khodadadi, Farzin Kashani, Omid Aghdaei, Somayeh Rahimi, Mohammad Sadegh Safari</strong></p>
<p>Although Automatic Speech Recognition (ASR) systems have become an integral part of modern technology, their evaluation remains challenging, particularly for low-resource languages such as Persian. This paper introduces Persian Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to address this gap by incorporating diverse linguistic and acoustic conditions. We evaluate ten ASR systems, including state-of-the-art commercial and open-source models, to examine performance variations and inherent biases. Additionally, we conduct an in-depth analysis of Persian ASR transcriptions, identifying key error types and proposing a novel metric that weights substitution errors. This metric enhances evaluation robustness by reducing the impact of minor and partial errors, thereby improving the precision of performance assessment. Our findings indicate that while ASR models generally perform well on standard Persian, they struggle with regional accents, childrenâ€™s speech, and specific linguistic challenges. These results highlight the necessity of fine-tuning and incorporating diverse, representative training datasets to mitigate biases and enhance overall ASR performance. PSRB provides a valuable resource for advancing ASR research in Persian and serves as a framework for developing benchmarks in other low-resource languages. A subset of the PSRB dataset is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PartAI/PSRB">https://huggingface.co/datasets/PartAI/PSRB</a>. </p>
<blockquote>
<p>å°½ç®¡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå·²æˆä¸ºç°ä»£æŠ€æœ¯çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ³¢æ–¯è¯­è¿™æ ·çš„èµ„æºè´«ä¹çš„è¯­è¨€è€Œè¨€ã€‚æœ¬æ–‡ä»‹ç»äº†æ³¢æ–¯è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆPSRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡èå…¥å¤šç§è¯­è¨€å’Œå£°å­¦æ¡ä»¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬è¯„ä¼°äº†10ä¸ªASRç³»ç»Ÿï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„å•†ä¸šå’Œå¼€æºæ¨¡å‹ï¼Œä»¥æ£€æŸ¥æ€§èƒ½å˜åŒ–å’Œå›ºæœ‰åè§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ³¢æ–¯è¯­ASRè½¬å½•è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œç¡®å®šäº†ä¸»è¦çš„é”™è¯¯ç±»å‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†ï¼Œè¯¥æ ‡å‡†å¯¹æ›¿æ¢é”™è¯¯è¿›è¡ŒåŠ æƒã€‚æ­¤åº¦é‡æ ‡å‡†é€šè¿‡å‡å°‘å¾®å°å’Œå±€éƒ¨é”™è¯¯çš„å½±å“ï¼Œæé«˜äº†è¯„ä¼°çš„ç¨³å¥æ€§ï¼Œä»è€Œæé«˜äº†æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ASRæ¨¡å‹åœ¨æ ‡å‡†æ³¢æ–¯è¯­ä¸Šçš„è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨åŒºåŸŸæ€§å£éŸ³ã€å„¿ç«¥è¯­è¨€å’Œç‰¹å®šè¯­è¨€æŒ‘æˆ˜æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¯¹ç»†å¾®è°ƒæ•´ä»¥åŠå¼•å…¥å¤šæ ·åŒ–å’Œå…·æœ‰ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®é›†çš„éœ€è¦ï¼Œä»¥å‡è½»åè§å¹¶å¢å¼ºASRçš„æ•´ä½“æ€§èƒ½ã€‚PSRBä¸ºæ³¢æ–¯è¯­ASRç ”ç©¶çš„å‘å±•æä¾›äº†å®è´µçš„èµ„æºï¼Œå¹¶ä¸ºå…¶ä»–èµ„æºè´«ä¹çš„è¯­è¨€å¼€å‘åŸºå‡†æµ‹è¯•æä¾›äº†æ¡†æ¶ã€‚PSRBæ•°æ®é›†çš„ä¸€ä¸ªå­é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PartAI/PSRB%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/PartAI/PSRBå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21230v1">PDF</a> 25 pages, 7 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†æ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆPSRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å¯¹ä½èµ„æºè¯­è¨€ï¼ˆå¦‚æ³¢æ–¯è¯­ï¼‰çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è¯„ä¼°æŒ‘æˆ˜ã€‚æ–‡ç« è¯„ä»·äº†åç§æ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„å•†ç”¨å’Œå¼€æºæ¨¡å‹ï¼Œå¹¶å¯¹å®ƒä»¬çš„æ€§èƒ½å˜åŒ–å’Œå›ºæœ‰åè§è¿›è¡Œäº†æ·±å…¥çš„åˆ†æã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹æ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«è½¬å½•çš„æ·±å…¥åˆ†æï¼Œæ–‡ç« ç¡®å®šäº†ä¸»è¦é”™è¯¯ç±»å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æƒé‡æ›¿ä»£é”™è¯¯è¯„ä»·æŒ‡æ ‡ã€‚è¿™ä¸€æŒ‡æ ‡é™ä½äº†è½»å¾®å’Œå±€éƒ¨é”™è¯¯çš„å½±å“ï¼Œæé«˜äº†æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ASRæ¨¡å‹åœ¨æ ‡å‡†æ³¢æ–¯è¯­ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒºåŸŸæ€§å£éŸ³ã€å„¿ç«¥è¯­éŸ³å’Œç‰¹å®šè¯­è¨€æŒ‘æˆ˜æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚è¿™å¼ºè°ƒäº†å¾®è°ƒå’Œä½¿ç”¨å¤šæ ·åŒ–ã€å…·æœ‰ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®é›†æ¥å‡è½»åè§å’Œæé«˜æ•´ä½“ASRæ€§èƒ½çš„å¿…è¦æ€§ã€‚PSRBä¸ºæ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶çš„å‘å±•æä¾›äº†å®è´µçš„èµ„æºï¼Œå¹¶ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€å¼€å‘åŸºå‡†æµ‹è¯•æä¾›äº†æ¡†æ¶ã€‚éƒ¨åˆ†PSRBæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PartAI/PSRB%e5%85%AC%e5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/PartAI/PSRBå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆPSRBï¼‰æ˜¯è§£å†³ä½èµ„æºè¯­è¨€è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè¯„ä¼°æŒ‘æˆ˜çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>PSRBè¯„ä»·äº†åç§æ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†åŒºåŸŸæ€§å£éŸ³ã€å„¿ç«¥è¯­éŸ³å’Œç‰¹å®šè¯­è¨€æŒ‘æˆ˜æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æƒé‡æ›¿ä»£é”™è¯¯è¯„ä»·æŒ‡æ ‡ï¼Œä»¥æé«˜æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>PSRBæ•°æ®é›†éƒ¨åˆ†å…¬å¼€å¯ç”¨ï¼Œä¸ºæ³¢æ–¯è¯­è¯­éŸ³è¯†åˆ«ç ”ç©¶çš„å‘å±•æä¾›äº†å®è´µèµ„æºã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒå’Œä½¿ç”¨å¤šæ ·åŒ–ã€å…·æœ‰ä»£è¡¨æ€§çš„è®­ç»ƒæ•°æ®é›†åœ¨æ”¹å–„ASRæ€§èƒ½æ–¹é¢çš„å¿…è¦æ€§ã€‚</li>
<li>PSRBä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€å¼€å‘åŸºå‡†æµ‹è¯•æä¾›äº†æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06305c89284387e7e9779ee66b9661ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a970fcde2c384a8bb6167a99d2715df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3310a1f456c959465bfe76211a7cdb6e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Model-as-Loss-A-Self-Consistent-Training-Paradigm"><a href="#Model-as-Loss-A-Self-Consistent-Training-Paradigm" class="headerlink" title="Model as Loss: A Self-Consistent Training Paradigm"></a>Model as Loss: A Self-Consistent Training Paradigm</h2><p><strong>Authors:Saisamarth Rajesh Phaye, Milos Cernak, Andrew Harper</strong></p>
<p>Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.   The Model as Loss paradigm leverages the encoderâ€™s task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoderâ€™s learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œæ—¶é—´æˆ–é¢‘ç‡åŸŸæŸå¤±ï¼‰æˆ–æ·±åº¦ç‰¹å¾æŸå¤±ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨WavLMæˆ–wav2vecï¼‰ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰åˆ°å¯¹æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦çš„ç»†å¾®ä¿¡å·å±æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ¨¡å‹å³æŸå¤±â€ï¼ˆModel as Lossï¼‰è¿™ä¸€æ–°å‹è®­ç»ƒèŒƒå¼ï¼Œå®ƒåˆ©ç”¨åŒä¸€æ¨¡å‹çš„ç¼–ç å™¨ä½œä¸ºæŸå¤±å‡½æ•°æ¥æŒ‡å¯¼è®­ç»ƒã€‚ â€œæ¨¡å‹å³æŸå¤±â€èŒƒå¼åˆ©ç”¨ç¼–ç å™¨çš„ä»»åŠ¡ç‰¹å®šç‰¹å¾ç©ºé—´ï¼Œä¼˜åŒ–è§£ç å™¨ä»¥äº§ç”Ÿä¸æ¸…æ´ä¿¡å·çš„æ„ŸçŸ¥å’Œä»»åŠ¡ç›¸å…³ç‰¹æ€§ä¸€è‡´çš„è¾“å‡ºã€‚é€šè¿‡åˆ©ç”¨ç¼–ç å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†æ¸…æ´å‚è€ƒè¯­éŸ³å’Œå¢å¼ºæ¨¡å‹è¾“å‡ºä¹‹é—´çš„è‡ªæˆ‘ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†è¯­éŸ³å¢å¼ºåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºé¢„è®­ç»ƒçš„æ·±åº¦ç‰¹å¾æŸå¤±ï¼Œæä¾›äº†æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œå¯¹åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21156v1">PDF</a> Accepted in Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¼ ç»Ÿçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æŸå¤±å‡½æ•°æˆ–æ·±åº¦ç‰¹å¾æŸå¤±ï¼Œè¿™äº›å¸¸å¸¸éš¾ä»¥æ•æ‰åˆ°ç»†å¾®çš„ä¿¡å·ç‰¹å¾ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºâ€œæ¨¡å‹ä½œä¸ºæŸå¤±â€çš„æ–°è®­ç»ƒèŒƒå¼ï¼Œå®ƒåˆ©ç”¨åŒä¸€æ¨¡å‹çš„ç¼–ç å™¨ä½œä¸ºæŸå¤±å‡½æ•°æ¥æŒ‡å¯¼è®­ç»ƒã€‚è¯¥èŒƒå¼åˆ©ç”¨ç¼–ç å™¨çš„ä»»åŠ¡ç‰¹å®šç‰¹å¾ç©ºé—´ï¼Œä¼˜åŒ–è§£ç å™¨ä»¥äº§ç”Ÿä¸æ¸…æ´ä¿¡å·çš„æ„ŸçŸ¥å’Œä»»åŠ¡ç›¸å…³ç‰¹æ€§ä¸€è‡´çš„è¾“å‡ºã€‚é€šè¿‡ä½¿ç”¨ç¼–ç å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†æ¸…æ´å‚è€ƒè¯­éŸ³å’Œå¢å¼ºæ¨¡å‹è¾“å‡ºä¹‹é—´çš„è‡ªæˆ‘ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†è¯­éŸ³å¢å¼ºåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºé¢„è®­ç»ƒçš„æ·±åº¦ç‰¹å¾æŸå¤±ï¼Œæä¾›äº†æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œå¯¹åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä¼ ç»Ÿè¯­éŸ³å¢å¼ºæ–¹æ³•ä¾èµ–æ‰‹å·¥æŸå¤±å‡½æ•°æˆ–æ·±åº¦ç‰¹å¾æŸå¤±ï¼Œå­˜åœ¨æ€§èƒ½å±€é™ã€‚</li>
<li>æå‡ºâ€œæ¨¡å‹ä½œä¸ºæŸå¤±â€çš„æ–°è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨åŒä¸€æ¨¡å‹çš„ç¼–ç å™¨ä½œä¸ºæŸå¤±å‡½æ•°ã€‚</li>
<li>è¯¥èŒƒå¼åˆ©ç”¨ç¼–ç å™¨çš„ä»»åŠ¡ç‰¹å®šç‰¹å¾ç©ºé—´ï¼Œä¼˜åŒ–è§£ç å™¨è¾“å‡ºä¸æ¸…æ´ä¿¡å·çš„æ„ŸçŸ¥å’Œä»»åŠ¡ç›¸å…³ç‰¹æ€§ä¸€è‡´ã€‚</li>
<li>ç¼–ç å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾ä½œä¸ºæŸå¤±å‡½æ•°ç¡®ä¿äº†æ¸…æ´å‚è€ƒè¯­éŸ³å’Œå¢å¼ºæ¨¡å‹è¾“å‡ºçš„è‡ªæˆ‘ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨æ ‡å‡†è¯­éŸ³å¢å¼ºåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºé¢„è®­ç»ƒçš„æ·±åº¦ç‰¹å¾æŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fab8f6917cc5b88c7990828848c6a27b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61367db879f5c1fc24f189101b30e06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31c2aa4ac5da2e3892e1ce3a82cab79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb52fdbf7bce3402d941b63a07121c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-986204ce58621dd8b8de1639d7182c1f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis"><a href="#Leveraging-LLM-and-Self-Supervised-Training-Models-for-Speech-Recognition-in-Chinese-Dialects-A-Comparative-Analysis" class="headerlink" title="Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis"></a>Leveraging LLM and Self-Supervised Training Models for Speech   Recognition in Chinese Dialects: A Comparative Analysis</h2><p><strong>Authors:Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei</strong></p>
<p>Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre- training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research </p>
<blockquote>
<p>å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“å·²ç»æ˜¾è‘—æé«˜äº†è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®çš„ç›¸å¯¹ç¨€ç¼ºï¼Œä¸­æ–‡å£éŸ³å’Œæ–¹è¨€ä»ç„¶æ˜¯å¤§å¤šæ•°è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æŒ‘æˆ˜ã€‚æœ€è¿‘è‡ªç›‘ç£å­¦ä¹ çš„è¿›æ­¥è¡¨æ˜ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“åˆï¼Œå¯ä»¥åœ¨èµ„æºç¨€ç¼ºçš„æƒ…å†µä¸‹æœ‰æ•ˆå¢å¼ºASRçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ—¨åœ¨ç ”ç©¶è¿™ç§èŒƒå¼åœ¨ä¸­æ–‡æ–¹è¨€ä¸­çš„åº”ç”¨æ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨30ä¸‡å°æ—¶çš„æ— æ ‡ç­¾æ–¹è¨€å’Œå¸¦å£éŸ³çš„è¯­éŸ³æ•°æ®ä¸Šé¢„è®­ç»ƒäº†ä¸€ä¸ªData2vec2æ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ä¸ª4ä¸‡å°æ—¶çš„ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œå¯¹é½è®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œä¸åŒçš„æŠ•å½±å™¨å’ŒLLMå¯¹æ™®é€šè¯ã€æ–¹è¨€å’Œå¸¦å£éŸ³è¯­éŸ³è¯†åˆ«æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ–¹è¨€æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬Kespeechã€‚æˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„å·¥ä½œä»¥ä¿ƒè¿›å¯å¤åˆ¶çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21138v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“å·²ç»æ˜¾è‘—æé«˜äº†è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚ä½†ç”±äºä¸­æ–‡æ–¹è¨€å£éŸ³æ•°æ®ç›¸å¯¹ç¨€ç¼ºï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç›‘ç£é¢„è®­ç»ƒå¯æœ‰æ•ˆæé«˜ä½èµ„æºåœºæ™¯çš„ASRæ€§èƒ½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨è¿™ä¸€æ¨¡å¼åœ¨ä¸­æ–‡æ–¹è¨€å£éŸ³ä¸Šçš„åº”ç”¨æ•ˆæœã€‚æˆ‘ä»¬é‡‡ç”¨Data2vec2æ¨¡å‹åœ¨æœªç»æ ‡æ³¨çš„æ–¹è¨€å£éŸ³è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨æ ‡æ³¨æ•°æ®é›†è¿›è¡Œå¯¹é½è®­ç»ƒã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¨¡å¼åœ¨å¤šç§æ–¹è¨€å£éŸ³æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šè¡¨ç°ï¼ŒåŒ…æ‹¬Kespeechæ•°æ®é›†ã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„ç ”ç©¶æˆæœä»¥ä¿ƒè¿›å¯å¤ç°æ€§ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è®­ç»ƒè¯­æ–™åº“å¢å¼ºäº†ASRæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸­æ–‡æ–¹è¨€å’Œå£éŸ³çš„æ•°æ®ç¨€ç¼ºæ˜¯ASRæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒå’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½èµ„æºåœºæ™¯çš„ASRæ€§èƒ½æå‡ä¸Šæ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨æ¢è®¨è¿™ä¸€æ¨¡å¼åœ¨ä¸­æ–‡æ–¹è¨€å£éŸ³ä¸Šçš„åº”ç”¨æ•ˆæœã€‚</li>
<li>é‡‡ç”¨Data2vec2æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æ–¹è¨€å£éŸ³è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œå¯¹é½è®­ç»ƒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºè¯¥æ¨¡å¼åœ¨å¤šç§æ–¹è¨€å£éŸ³æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0340842806d0798a241350017c40b823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013d37df0e3f2cb6fe97a767a5f31427.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c83d5dfb5fb3af0f0b09f6cda952ff6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1339c33604d6c4e21e782d07e92a7a65.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multimodal-Assessment-of-Speech-Impairment-in-ALS-Using-Audio-Visual-and-Machine-Learning-Approaches"><a href="#Multimodal-Assessment-of-Speech-Impairment-in-ALS-Using-Audio-Visual-and-Machine-Learning-Approaches" class="headerlink" title="Multimodal Assessment of Speech Impairment in ALS Using Audio-Visual and   Machine Learning Approaches"></a>Multimodal Assessment of Speech Impairment in ALS Using Audio-Visual and   Machine Learning Approaches</h2><p><strong>Authors:Francesco Pierotti, Andrea Bandini</strong></p>
<p>The analysis of speech in individuals with amyotrophic lateral sclerosis is a powerful tool to support clinicians in the assessment of bulbar dysfunction. However, current methods used in clinical practice consist of subjective evaluations or expensive instrumentation. This study investigates different approaches combining audio-visual analysis and machine learning to predict the speech impairment evaluation performed by clinicians. Using a small dataset of acoustic and kinematic features extracted from audio and video recordings of speech tasks, we trained and tested some regression models. The best performance was achieved using the extreme boosting machine regressor with multimodal features, which resulted in a root mean squared error of 0.93 on a scale ranging from 5 to 25. Results suggest that integrating audio-video analysis enhances speech impairment assessment, providing an objective tool for early detection and monitoring of bulbar dysfunction, also in home settings. </p>
<blockquote>
<p>é’ˆå¯¹æ‚£æœ‰è‚Œèç¼©ä¾§ç´¢ç¡¬åŒ–ç—‡ï¼ˆALSï¼‰çš„ä¸ªä½“çš„è¨€è¯­åˆ†ææ˜¯æ”¯æŒä¸´åºŠåŒ»ç”Ÿè¯„ä¼°çƒéƒ¨åŠŸèƒ½éšœç¢çš„æœ‰åŠ›å·¥å…·ã€‚ç„¶è€Œï¼Œç›®å‰ä¸´åºŠå®è·µä¸­ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬ä¸»è§‚è¯„ä¼°æˆ–æ˜‚è´µçš„ä»ªå™¨ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ç»“åˆè§†å¬åˆ†æå’Œæœºå™¨å­¦ä¹ çš„ä¸åŒæ–¹æ³•ï¼Œä»¥é¢„æµ‹ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œçš„è¨€è¯­éšœç¢è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨ä»è¯­éŸ³ä»»åŠ¡çš„éŸ³é¢‘å’Œè§†é¢‘è®°å½•ä¸­æå–çš„å°‘é‡å£°å­¦å’Œè¿åŠ¨å­¦ç‰¹å¾æ•°æ®é›†æ¥è®­ç»ƒå’Œæµ‹è¯•ä¸€äº›å›å½’æ¨¡å‹ã€‚ä½¿ç”¨æç«¯å¢å¼ºæœºå›å½’å™¨è¿›è¡Œå¤šæ¨¡å¼ç‰¹å¾æ—¶è·å¾—æœ€ä½³æ€§èƒ½ï¼Œåœ¨5åˆ°25çš„èŒƒå›´å†…å¾—å‡ºçš„å‡æ–¹æ ¹è¯¯å·®ä¸º0.93ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆéŸ³è§†é¢‘åˆ†æå¯æé«˜è¨€è¯­éšœç¢è¯„ä¼°æ°´å¹³ï¼Œä¸ºåœ¨å®¶åº­ç¯å¢ƒä¸­æ—©æœŸæ£€æµ‹å’Œç›‘æµ‹çƒéƒ¨åŠŸèƒ½éšœç¢æä¾›å®¢è§‚å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21093v1">PDF</a> Submitted to Interspeech</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ç»“åˆè§†å¬åˆ†æä¸æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥é¢„æµ‹ä¸´åºŠåŒ»ç”Ÿå¯¹è¿åŠ¨ç¥ç»å…ƒç—…æ‚£è€…çš„è¨€è¯­éšœç¢è¯„ä¼°ã€‚é€šè¿‡å¯¹éŸ³é¢‘å’Œè§†é¢‘è®°å½•ä¸­æå–çš„å£°å­¦å’Œè¿åŠ¨ç‰¹å¾è¿›è¡Œå›å½’æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ï¼Œä½¿ç”¨æç«¯å¢å¼ºæœºå›å½’å™¨å’Œå¤šæ¨¡å¼ç‰¹å¾å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œåœ¨5è‡³25çš„èŒƒå›´å†…å‡æ–¹æ ¹è¯¯å·®ä¸º0.93ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆè§†å¬åˆ†æèƒ½å¤Ÿæé«˜è¨€è¯­éšœç¢è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œä¸ºæ—©æœŸå‘ç°å’Œç›‘æµ‹è¿åŠ¨ç¥ç»å…ƒç—…æ‚£è€…çš„è¨€è¯­éšœç¢æä¾›å®¢è§‚å·¥å…·ï¼Œä¹Ÿé€‚ç”¨äºå®¶åº­ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æè¿åŠ¨ç¥ç»å…ƒç—…æ‚£è€…çš„è¨€è¯­å¯¹äºä¸´åºŠåŒ»ç”Ÿè¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰ä¸´åºŠå®è·µä¸­ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸»è§‚è¯„ä»·å’Œæ˜‚è´µçš„ä»ªå™¨æ£€æµ‹ã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆäº†è§†å¬åˆ†æå’Œæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¨€è¯­éšœç¢è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æç«¯å¢å¼ºæœºå›å½’å™¨å’Œå¤šæ¨¡å¼ç‰¹å¾ï¼Œå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è§†å¬åˆ†æç»“åˆçš„æ–¹æ³•åœ¨è¯„ä¼°è¿åŠ¨ç¥ç»å…ƒç—…æ‚£è€…çš„è¨€è¯­éšœç¢æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä¸ºæ—©æœŸå‘ç°å’Œç›‘æµ‹è¿åŠ¨ç¥ç»å…ƒç—…æ‚£è€…çš„è¨€è¯­éšœç¢æä¾›å®¢è§‚å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc9069b769d0fcef8dbaa02c2becef1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a2cc09aeb6fa074eda76efed3917367.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Study-of-Lightweight-Transformer-Architectures-for-Single-Channel-Speech-Enhancement"><a href="#Study-of-Lightweight-Transformer-Architectures-for-Single-Channel-Speech-Enhancement" class="headerlink" title="Study of Lightweight Transformer Architectures for Single-Channel Speech   Enhancement"></a>Study of Lightweight Transformer Architectures for Single-Channel Speech   Enhancement</h2><p><strong>Authors:Haixin Zhao, Nilesh Madhu</strong></p>
<p>In speech enhancement, achieving state-of-the-art (SotA) performance while adhering to the computational constraints on edge devices remains a formidable challenge. Networks integrating stacked temporal and spectral modelling effectively leverage improved architectures such as transformers; however, they inevitably incur substantial computational complexity and model expansion. Through systematic ablation analysis on transformer-based temporal and spectral modelling, we demonstrate that the architecture employing streamlined Frequency-Time-Frequency (FTF) stacked transformers efficiently learns global dependencies within causal context, while avoiding considerable computational demands. Utilising discriminators in training further improves learning efficacy and enhancement without introducing additional complexity during inference. The proposed lightweight, causal, transformer-based architecture with adversarial training (LCT-GAN) yields SoTA performance on instrumental metrics among contemporary lightweight models, but with far less overhead. Compared to DeepFilterNet2, the LCT-GAN only requires 6% of the parameters, at similar complexity and performance. Against CCFNet+(Lite), LCT-GAN saves 9% in parameters and 10% in multiply-accumulate operations yet yielding improved performance. Further, the LCT-GAN even outperforms more complex, common baseline models on widely used test datasets. </p>
<blockquote>
<p>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼Œåœ¨æ»¡è¶³è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—çº¦æŸçš„åŒæ—¶å®ç°æœ€æ–°å‰æ²¿æŠ€æœ¯ï¼ˆSotAï¼‰æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç½‘ç»œé›†æˆäº†å †å çš„æ—¶é—´æ¨¡å‹å’Œå…‰è°±æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†æ”¹è¿›çš„ç»“æ„å¦‚å˜å‹å™¨ï¼›ç„¶è€Œï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´å¤§é‡çš„è®¡ç®—å¤æ‚æ€§å’Œæ¨¡å‹æ‰©å±•ã€‚é€šè¿‡å¯¹åŸºäºå˜å‹å™¨çš„æ—¶åºå’Œå…‰è°±å»ºæ¨¡è¿›è¡Œç³»ç»Ÿæ¶ˆèåˆ†æï¼Œæˆ‘ä»¬è¯æ˜é‡‡ç”¨ç®€åŒ–å‹é¢‘æ—¶é¢‘ï¼ˆFTFï¼‰å †å å˜å‹å™¨çš„æ¶æ„èƒ½å¤Ÿåœ¨å› æœä¸Šä¸‹æ–‡ä¸­æœ‰æ•ˆåœ°å­¦ä¹ å…¨å±€ä¾èµ–æ€§ï¼ŒåŒæ—¶é¿å…äº†å·¨å¤§çš„è®¡ç®—éœ€æ±‚ã€‚åœ¨è®­ç»ƒä¸­åˆ©ç”¨é‰´åˆ«å™¨è¿›ä¸€æ­¥æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œå¢å¼ºäº†å­¦ä¹ æ•ˆæœï¼Œè€Œä¸ä¼šå¼•å…¥æ¨ç†è¿‡ç¨‹ä¸­çš„é¢å¤–å¤æ‚æ€§ã€‚æ‰€æå‡ºçš„åŸºäºè½»é‡çº§ã€å› æœæ€§ã€å¯¹æŠ—è®­ç»ƒçš„å˜å‹å™¨æ¶æ„ï¼ˆLCT-GANï¼‰åœ¨å½“ä»£è½»é‡çº§æ¨¡å‹ä¸­è·å¾—äº†ä»ªå™¨æŒ‡æ ‡çš„å‰æ²¿æ€§èƒ½ï¼Œä½†å¼€é”€å¤§å¤§é™ä½ã€‚ä¸DeepFilterNet2ç›¸æ¯”ï¼ŒLCT-GANä»…éœ€è¦6%çš„å‚æ•°ï¼Œåœ¨å¤æ‚æ€§å’Œæ€§èƒ½ä¸Šç›¸ä¼¼ã€‚ä¸CCFNet+ï¼ˆLiteï¼‰ç›¸æ¯”ï¼ŒLCT-GANèŠ‚çœäº†9%çš„å‚æ•°å’Œ10%çš„ä¹˜ç§¯ç´¯æ“ä½œï¼ŒåŒæ—¶å´å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒLCT-GANç”šè‡³åœ¨å¹¿æ³›ä½¿ç”¨çš„æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæ›´å¤æ‚ã€å¸¸è§çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21057v1">PDF</a> Accepted by EUSIPCO 2025</p>
<p><strong>Summary</strong><br>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼Œå°½ç®¡å­˜åœ¨è®¸å¤šåŸºäºç¥ç»ç½‘ç»œçš„æŠ€æœ¯èƒ½å¤Ÿåˆ©ç”¨å…ˆè¿›æ¶æ„ï¼ˆå¦‚å˜å‹å™¨ï¼‰å®ç°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSotAï¼‰çš„æ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°è®¡ç®—çº¦æŸçš„åŒæ—¶ä¿æŒé«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ€§åˆ†æå˜å‹å™¨åœ¨æ—¶åºå’Œé¢‘è°±å»ºæ¨¡ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§é‡‡ç”¨æµçº¿åŒ–é¢‘æ—¶é¢‘ï¼ˆFTFï¼‰å †å å˜å‹å™¨æ¶æ„çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å› æœä¸Šä¸‹æ–‡ä¸­é«˜æ•ˆå­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶é¿å…è®¡ç®—å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥é‰´åˆ«å™¨æé«˜äº†å­¦ä¹ æ•ˆç‡å¹¶å¢å¼ºäº†æ•ˆæœï¼Œä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šå¢åŠ é¢å¤–çš„å¤æ‚æ€§ã€‚æ‰€æå‡ºåŸºäºå¯¹æŠ—æ€§è®­ç»ƒçš„è½»é‡çº§å› æœå˜å‹å™¨æ¶æ„ï¼ˆLCT-GANï¼‰åœ¨ç°ä»£è½»é‡çº§æ¨¡å‹ä¸­å®ç°äº†é¢†å…ˆæ°´å¹³çš„è¡¨ç°ã€‚ä¸DeepFilterNet2ç›¸æ¯”ï¼ŒLCT-GANçš„å‚æ•°éœ€æ±‚ä»…å å…¶6%ï¼Œåœ¨å¤æ‚æ€§å’Œæ€§èƒ½ä¸Šä¸ä¹‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚ç›¸è¾ƒäºæ›´å¤æ‚çš„åŸºçº¿æ¨¡å‹CCFNet+ï¼ˆLiteï¼‰ï¼ŒLCT-GANåœ¨å‚æ•°å’Œä¹˜ç§¯ç´¯åŠ è¿ç®—ä¸Šåˆ†åˆ«èŠ‚çœäº†9%å’Œ10%ï¼ŒåŒæ—¶è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼Œå®ç°è®¡ç®—çº¦æŸä¸‹çš„æœ€æ–°æŠ€æœ¯æ°´å¹³æ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„é¢‘æ—¶é¢‘ï¼ˆFTFï¼‰å †å æ¶æ„èƒ½é«˜æ•ˆå­¦ä¹ å…¨å±€ä¾èµ–å…³ç³»ï¼Œå¹¶å‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>åœ¨è®­ç»ƒä¸­åŠ å…¥é‰´åˆ«å™¨æœ‰åŠ©äºæé«˜å­¦ä¹ æ•ˆç‡å¹¶å¢å¼ºæ•ˆæœï¼Œä¸”ä¸å½±å“æ¨ç†é˜¶æ®µçš„å¤æ‚æ€§ã€‚</li>
<li>LCT-GANæ¶æ„åœ¨ç°ä»£è½»é‡çº§æ¨¡å‹ä¸­è¡¨ç°å‡ºé¢†å…ˆæ°´å¹³ã€‚</li>
<li>ä¸DeepFilterNet2ç›¸æ¯”ï¼ŒLCT-GANåœ¨å‚æ•°éœ€æ±‚ä¸Šå¤§å¹…é™ä½ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„å¤æ‚æ€§å’Œæ€§èƒ½ã€‚</li>
<li>LCT-GANç›¸è¾ƒäºæ›´å¤æ‚çš„åŸºçº¿æ¨¡å‹CCFNet+ï¼ˆLiteï¼‰åœ¨å‚æ•°å’Œè¿ç®—æ•ˆç‡ä¸Šæœ‰æ‰€æ”¹å–„ï¼ŒåŒæ—¶æ€§èƒ½æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a216b1021c1565444082ac6896829b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b15651a44dc2e6760938da3ab4e6752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb024422676c5d1327ad25fe939ae456.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52aeeca61701d9b88b76291bcbc82061.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afbcf7d4baad47a04d5b899fb8454339.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Visual-Cues-Enhance-Predictive-Turn-Taking-for-Two-Party-Human-Interaction"><a href="#Visual-Cues-Enhance-Predictive-Turn-Taking-for-Two-Party-Human-Interaction" class="headerlink" title="Visual Cues Enhance Predictive Turn-Taking for Two-Party Human   Interaction"></a>Visual Cues Enhance Predictive Turn-Taking for Two-Party Human   Interaction</h2><p><strong>Authors:Sam Oâ€™Connor Russell, Naomi Harte</strong></p>
<p>Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold&#x2F;shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available. </p>
<blockquote>
<p>ä¼šè¯äº¤æ›¿æ˜¯å¤šæ¨¡å¼çš„ã€‚é¢„æµ‹ä¼šè¯äº¤æ›¿æ¨¡å‹ï¼ˆPTTMsï¼‰ä¿ƒè¿›äº†äººç±»ä¸æœºå™¨äººçš„è‡ªç„¶äº¤äº’ï¼Œä½†å¤§å¤šæ•°ä»…ä¾èµ–äºè¯­éŸ³ã€‚æˆ‘ä»¬å¼•å…¥äº†MM-VAPï¼ˆå¤šæ¨¡æ€é¢„æµ‹ä¼šè¯äº¤æ›¿æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†è¯­éŸ³ä¸è§†è§‰çº¿ç´¢ï¼ˆåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿æ€å’Œç›®å…‰æ³¨è§†ï¼‰çš„å¤šæ¨¡å¼é¢„æµ‹ä¼šè¯äº¤æ›¿æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°å®ƒåœ¨è§†é¢‘ä¼šè®®äº¤äº’ä¸­è¶…è¿‡äº†å½“å‰é¢†å…ˆçš„ä»…ä¾èµ–éŸ³é¢‘çš„æ¨¡å‹ï¼ˆä¿æŒ&#x2F;è½¬æ¢é¢„æµ‹å‡†ç¡®ç‡ä¸º84% vs. 79%ï¼‰ã€‚ä¸ä»¥å¾€å°†æ‰€æœ‰ä¿æŒå’Œè½¬å˜æ±‡æ€»çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬æŒ‰ä¸åŒä¼šè¯é—´çš„æ²‰é»˜æ—¶é•¿è¿›è¡Œåˆ†ç»„ã€‚è¿™è¡¨æ˜é€šè¿‡åŒ…å«è§†è§‰ç‰¹å¾ï¼ŒMM-VAPåœ¨æ‰€æœ‰å‘è¨€è€…è¿‡æ¸¡æ—¶é•¿ä¸­ï¼Œéƒ½è¶…è¿‡äº†ä»…ä¾èµ–éŸ³é¢‘çš„é¢„æµ‹ä¼šè¯äº¤æ›¿æ¨¡å‹çš„æœ€ä½³è¡¨ç°ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„åˆ‡é™¤ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºé¢éƒ¨è¡¨æƒ…ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ€å¤§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œå½“å¯¹è¯è€…å¯ä»¥ç›¸äº’çœ‹è§æ—¶ï¼Œè§†è§‰çº¿ç´¢å¯¹äºä¼šè¯äº¤æ›¿è‡³å…³é‡è¦ï¼Œå› æ­¤å¿…é¡»åŒ…å«ä»¥è¿›è¡Œå‡†ç¡®çš„ä¼šè¯äº¤æ›¿é¢„æµ‹ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†è‡ªåŠ¨è¯­éŸ³å¯¹é½åœ¨PTTMè®­ç»ƒä¸­çš„é€‚ç”¨æ€§ï¼Œä½¿ç”¨ç”µè¯è¯­éŸ³ã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†é¦–ä¸ªå¯¹å¤šæ¨¡å¼PTTMçš„å…¨é¢åˆ†æã€‚æˆ‘ä»¬è®¨è®ºäº†æœªæ¥å·¥ä½œçš„å¯ç¤ºå¹¶ä½¿æ‰€æœ‰ä»£ç å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€é¢„æµ‹æ€§å¯¹è¯æ¨¡å‹MM-VAPï¼Œå®ƒç»“åˆäº†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿æ€å’Œç›®å…‰ç­‰å¤šæ¨¡æ€ä¿¡æ¯æ¥è¿›è¡Œå¯¹è¯ä¸­çš„å‘è¨€è€…åˆ‡æ¢é¢„æµ‹ã€‚ç›¸è¾ƒäºä»…ä¾èµ–è¯­éŸ³çš„æ¨¡å‹ï¼ŒMM-VAPåœ¨è§†é¢‘ä¼šè®®äº¤äº’ä¸­è¡¨ç°æ›´ä½³ï¼Œé¢„æµ‹å‡†ç¡®ç‡æé«˜äº†ä»ç™¾åˆ†ä¹‹å‡ åˆ°ç™¾åˆ†ä¹‹å‡ åä¸ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒè§†è§‰ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°é¢éƒ¨è¡¨æƒ…ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ€å¤§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¿˜éªŒè¯äº†åˆ©ç”¨ç”µè¯è¯­éŸ³è¿›è¡Œè‡ªåŠ¨è¯­éŸ³å¯¹é½ä»¥è®­ç»ƒé¢„æµ‹æ€§å¯¹è¯æ¨¡å‹çš„å¯è¡Œæ€§ã€‚æ­¤é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€é¢„æµ‹æ€§å¯¹è¯æ¨¡å‹çš„é¦–ä¸ªç»¼åˆåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MM-VAPæ˜¯ä¸€ç§å¤šæ¨¡æ€é¢„æµ‹æ€§å¯¹è¯æ¨¡å‹ï¼Œèåˆäº†è¯­éŸ³ã€é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿æ€å’Œç›®å…‰ç­‰å¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>MM-VAPåœ¨è§†é¢‘ä¼šè®®äº¤äº’ä¸­çš„è¡¨ç°ä¼˜äºä»…ä¾èµ–è¯­éŸ³çš„æ¨¡å‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
<li>ç›¸è¾ƒäºå…ˆå‰çš„ç ”ç©¶ï¼ŒMM-VAPèƒ½å¤Ÿæ ¹æ®å‘è¨€è€…è½¬æ¢çš„æŒç»­æ—¶é—´åˆ†ç»„è¿›è¡Œé¢„æµ‹ï¼Œæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ç ”ç©¶ä¸åŒè§†è§‰ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°é¢éƒ¨è¡¨æƒ…ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ€å¤§ã€‚</li>
<li>å½“å¯¹è¯è€…èƒ½å¤Ÿç›¸äº’çœ‹è§æ—¶ï¼Œè§†è§‰çº¿ç´¢å¯¹äºå¯¹è¯ä¸­çš„å‘è¨€è€…åˆ‡æ¢é¢„æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥ç ”ç©¶éªŒè¯äº†åˆ©ç”¨ç”µè¯è¯­éŸ³è¿›è¡Œè‡ªåŠ¨è¯­éŸ³å¯¹é½ä»¥è®­ç»ƒé¢„æµ‹æ€§å¯¹è¯æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6a00a2835c4c2186006e2d18691a135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c734948d35e468222ec1f415f294056.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81c13f5413d4f82a27ad632576dfd0de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac17ebba9927e0b3652bd046f7272635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aea1834256d3f33d9e35bf978c7beff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf9d46960f1cf28053fdbfae9379114b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a180a621629f0d30b9c911ef89f05aa6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity"><a href="#ClearSphere-Multi-Earphone-Synergy-for-Enhanced-Conversational-Clarity" class="headerlink" title="ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity"></a>ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity</h2><p><strong>Authors:Lixing He</strong></p>
<p>In crowded places such as conferences, background noise, overlapping voices, and lively interactions make it difficult to have clear conversations. This situation often worsens the phenomenon known as â€œcocktail party deafness.â€ We present ClearSphere, the collaborative system that enhances speech at the conversation level with multi-earphones. Real-time conversation enhancement requires a holistic modeling of all the members in the conversation, and an effective way to extract the speech from the mixture. ClearSphere bridges the acoustic sensor system and state-of-the-art deep learning for target speech extraction by making two key contributions: 1) a conversation-driven network protocol, and 2) a robust target conversation extraction model. Our networking protocol enables mobile, infrastructure-free coordination among earphone devices. Our conversation extraction model can leverage the relay audio in a bandwidth-efficient way. ClearSphere is evaluated in both real-world experiments and simulations. Results show that our conversation network obtains more than 90% accuracy in group formation, improves the speech quality by up to 8.8 dB over state-of-the-art baselines, and demonstrates real-time performance on a mobile device. In a user study with 20 participants, ClearSphere has a much higher score than baseline with good usability. </p>
<blockquote>
<p>åœ¨ä¼šè®®ç­‰æ‹¥æŒ¤åœºæ‰€ï¼ŒèƒŒæ™¯å™ªéŸ³ã€å£°éŸ³é‡å ä»¥åŠæ´»è·ƒçš„äº’åŠ¨ä½¿å¾—è¿›è¡Œæ¸…æ™°çš„å¯¹è¯å˜å¾—å›°éš¾ã€‚è¿™ç§æƒ…å†µå¾€å¾€ä¼šåŠ å‰§æ‰€è°“çš„â€œé¸¡å°¾é…’ä¼šè€³è‹â€ç°è±¡ã€‚æˆ‘ä»¬æ¨å‡ºäº†ClearSphereï¼Œè¿™æ˜¯ä¸€æ¬¾åä½œç³»ç»Ÿï¼Œé€šè¿‡å¤šè€³æœºåœ¨å¯¹è¯çº§åˆ«ä¸Šå¢å¼ºè¯­éŸ³ã€‚å®æ—¶å¯¹è¯å¢å¼ºéœ€è¦å¯¹æ‰€æœ‰å¯¹è¯æˆå‘˜è¿›è¡Œæ•´ä½“å»ºæ¨¡ï¼Œä»¥åŠä»æ··åˆè¯­éŸ³ä¸­æœ‰æ•ˆæå–è¯­éŸ³çš„æœ‰æ•ˆæ–¹æ³•ã€‚ClearSphereé€šè¿‡ä¸¤é¡¹å…³é”®è´¡çŒ®â€”â€”1ï¼‰å¯¹è¯é©±åŠ¨çš„ç½‘ç»œåè®®å’Œ2ï¼‰ç¨³å¥çš„ç›®æ ‡å¯¹è¯æå–æ¨¡å‹ï¼Œæ¶è®¾äº†å£°å­¦ä¼ æ„Ÿå™¨ç³»ç»Ÿå’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ ç›®æ ‡è¯­éŸ³æå–ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„ç½‘ç»œåè®®å®ç°äº†è€³æœºè®¾å¤‡ä¹‹é—´æ— éœ€åŸºç¡€è®¾æ–½çš„ç§»åŠ¨åè°ƒã€‚æˆ‘ä»¬çš„å¯¹è¯æå–æ¨¡å‹èƒ½å¤Ÿä»¥å¸¦å®½æœ‰æ•ˆçš„æ–¹å¼åˆ©ç”¨ä¸­ç»§éŸ³é¢‘ã€‚ClearSphereåœ¨çœŸå®å®éªŒå’Œæ¨¡æ‹Ÿä¸­éƒ½å¾—åˆ°äº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¯¹è¯ç½‘ç»œåœ¨ç¾¤ä½“å½¢æˆæ–¹é¢è¾¾åˆ°äº†è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼Œåœ¨æœ€æ–°åŸºçº¿çš„åŸºç¡€ä¸Šæé«˜äº†é«˜è¾¾8.8åˆ†è´çš„è¯­éŸ³è´¨é‡ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ€§èƒ½ã€‚åœ¨20åå‚ä¸è€…çš„ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒClearSphereçš„å¾—åˆ†è¿œé«˜äºåŸºçº¿ï¼Œå…·æœ‰è‰¯å¥½çš„å¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21004v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ¸…æ™°çƒä½“ç³»ç»Ÿæ˜¯ä¸€æ¬¾é¢å‘å¯¹è¯åœºæ™¯çš„è¯­éŸ³å¢å¼ºåä½œç³»ç»Ÿï¼Œé€šè¿‡ç§»åŠ¨è®¾å¤‡å®ç°å¯¹è¯æˆå‘˜é—´çš„å®æ—¶äº¤æµã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯æå–ç›®æ ‡è¯­éŸ³ï¼Œå¹¶å¼•å…¥æ–°å‹ç½‘ç»œåè®®ï¼Œä»¥æ”¹å–„èƒŒæ™¯å™ªéŸ³å’Œå¤šäººå¯¹è¯æ—¶çš„å¬åŠ›é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ç¾¤ä½“è¯†åˆ«ä¸Šæœ‰è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼Œç›¸å¯¹äºå…¶ä»–åŸºå‡†æ–¹æ¡ˆæ”¹å–„äº†è¯­éŸ³è´¨é‡è¾¾8.8åˆ†è´ï¼ŒåŒæ—¶å±•ç¤ºäº†å‡ºè‰²çš„å®æ—¶æ€§èƒ½å’Œä½¿ç”¨ä½“éªŒã€‚è¿™é¡¹åˆ›æ–°å°†æœ‰åŠ©äºæ”¹å–„å¤šäººåœºæ™¯ä¸‹çš„å¬åŠ›å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æ¸…æ™°çƒä½“ç³»ç»Ÿé‡‡ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯æå‡è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚</li>
<li>ç³»ç»Ÿå¼•å…¥æ–°å‹ç½‘ç»œåè®®ï¼Œå®ç°ç§»åŠ¨è®¾å¤‡é—´çš„å®æ—¶äº¤æµåä½œã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ç¾¤ä½“è¯†åˆ«ä¸Šæœ‰è¶…è¿‡90%çš„å‡†ç¡®ç‡ã€‚</li>
<li>ä¸å…¶ä»–åŸºå‡†æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯­éŸ³è´¨é‡æé«˜äº†é«˜è¾¾8.8åˆ†è´ã€‚</li>
<li>ç³»ç»Ÿå±•ç¤ºäº†å‡ºè‰²çš„å®æ—¶æ€§èƒ½å’Œä½¿ç”¨ä½“éªŒã€‚</li>
<li>è¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆæ”¹å–„å¤šäººåœºæ™¯ä¸‹çš„å¬åŠ›å›°éš¾é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea565b9b741da56f0c7ae60b17a205ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b5133471ce67e201a3acb693274887.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2830a5db60319238678645f41c23db77.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REWIND-Speech-Time-Reversal-for-Enhancing-Speaker-Representations-in-Diffusion-based-Voice-Conversion"><a href="#REWIND-Speech-Time-Reversal-for-Enhancing-Speaker-Representations-in-Diffusion-based-Voice-Conversion" class="headerlink" title="REWIND: Speech Time Reversal for Enhancing Speaker Representations in   Diffusion-based Voice Conversion"></a>REWIND: Speech Time Reversal for Enhancing Speaker Representations in   Diffusion-based Voice Conversion</h2><p><strong>Authors:Ishan D. Biyani, Nirmesh J. Shah, Ashishkumar P. Gudmalwar, Pankaj Wasnik, Rajiv R. Shah</strong></p>
<p>Speech time reversal refers to the process of reversing the entire speech signal in time, causing it to play backward. Such signals are completely unintelligible since the fundamental structures of phonemes and syllables are destroyed. However, they still retain tonal patterns that enable perceptual speaker identification despite losing linguistic content. In this paper, we propose leveraging speaker representations learned from time reversed speech as an augmentation strategy to enhance speaker representation. Notably, speaker and language disentanglement in voice conversion (VC) is essential to accurately preserve a speakerâ€™s unique vocal traits while minimizing interference from linguistic content. The effectiveness of the proposed approach is evaluated in the context of state-of-the-art diffusion-based VC models. Experimental results indicate that the proposed approach significantly improves speaker similarity-related scores while maintaining high speech quality. </p>
<blockquote>
<p>è¯­éŸ³æ—¶é—´åè½¬æ˜¯æŒ‡å°†æ•´ä¸ªè¯­éŸ³ä¿¡å·åœ¨æ—¶é—´ä¸Šå€’è½¬ï¼Œä½¿å…¶å€’æ”¾ã€‚ç”±äºéŸ³ç´ å’ŒéŸ³èŠ‚çš„åŸºæœ¬ç»“æ„è¢«ç ´åï¼Œè¿™ç§ä¿¡å·æ˜¯å®Œå…¨ä¸å¯ç†è§£çš„ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶ä¿ç•™èƒ½å¤Ÿæ„ŸçŸ¥è¯´è¯äººèº«ä»½çš„è¯­è°ƒæ¨¡å¼ï¼Œå°½ç®¡å¤±å»äº†è¯­è¨€å†…å®¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ä»æ—¶é—´åè½¬è¯­éŸ³ä¸­å­¦ä¹ åˆ°çš„è¯´è¯äººè¡¨å¾ä½œä¸ºä¸€ç§å¢å¼ºç­–ç•¥ï¼Œä»¥æé«˜è¯´è¯äººè¡¨å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ä¸­ï¼Œè¯´è¯äººå’Œè¯­è¨€çš„å»è€¦æ˜¯å…³é”®ï¼Œè¿™æ ·å¯ä»¥å‡†ç¡®ä¿ç•™è¯´è¯äººç‹¬ç‰¹çš„å—“éŸ³ç‰¹å¾ï¼ŒåŒæ—¶æœ€å°åŒ–è¯­è¨€å†…å®¹çš„å¹²æ‰°ã€‚æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§åœ¨åŸºäºæœ€å…ˆè¿›çš„æ‰©æ•£çš„VCæ¨¡å‹èƒŒæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è¯­éŸ³è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ä¸è¯´è¯äººç›¸ä¼¼æ€§ç›¸å…³çš„åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20756v1">PDF</a> Accepted in INTERSPEECH 2025</p>
<p><strong>Summary</strong>ï¼š<br>è¯­éŸ³æ—¶é—´åè½¬æ˜¯æŒ‡å°†æ•´ä¸ªè¯­éŸ³ä¿¡å·åœ¨æ—¶é—´ä¸Šè¿›è¡Œåè½¬ï¼Œä½¿å…¶å‘åæ’­æ”¾ã€‚è™½ç„¶è¿™ç§ä¿¡å·æ— æ³•ç†è§£ï¼Œä½†å®ƒä»¬ä»ç„¶ä¿ç•™äº†éŸ³è°ƒçš„æ¨¡å¼ï¼Œå¯ä»¥è¿›è¡Œè¯´è¯è€…æ„ŸçŸ¥è¯†åˆ«ï¼Œå°½ç®¡å¤±å»äº†è¯­è¨€å†…å®¹ã€‚æœ¬æ–‡æå‡ºäº†åˆ©ç”¨ä»æ—¶é—´åè½¬è¯­éŸ³ä¸­å­¦ä¹ åˆ°çš„è¯´è¯è€…è¡¨å¾ä½œä¸ºå¢å¼ºç­–ç•¥ï¼Œä»¥æé«˜è¯´è¯è€…è¡¨å¾ã€‚åœ¨åŸºäºæ‰©æ•£çš„è¯­éŸ³è½¬æ¢æ¨¡å‹ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è¯­éŸ³è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ä¸è¯´è¯è€…ç›¸ä¼¼æ€§ç›¸å…³çš„åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³æ—¶é—´åè½¬æ˜¯æŒ‡å°†è¯­éŸ³ä¿¡å·åå‘æ’­æ”¾ï¼Œå¯¼è‡´ä¿¡å·ä¸å¯ç†è§£ï¼Œä½†ä»å¯è¯†åˆ«è¯´è¯è€…ã€‚</li>
<li>æ—¶é—´åè½¬çš„è¯­éŸ³ä¿ç•™äº†éŸ³è°ƒçš„æ¨¡å¼ï¼Œä½¿å¾—è¯´è¯è€…èƒ½å¤Ÿè¢«è¯†åˆ«ã€‚</li>
<li>è¯´è¯è€…å’Œè¯­è¨€çš„åˆ†ç¦»åœ¨è¯­éŸ³è½¬æ¢ä¸­æ˜¯å…³é”®ï¼Œæ—¨åœ¨å‡†ç¡®ä¿ç•™è¯´è¯è€…çš„ç‹¬ç‰¹å—“éŸ³ç‰¹å¾ï¼ŒåŒæ—¶å°½é‡å‡å°‘è¯­è¨€å†…å®¹çš„å¹²æ‰°ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ä»æ—¶é—´åè½¬è¯­éŸ³ä¸­å­¦ä¹ åˆ°çš„è¯´è¯è€…è¡¨å¾ä½œä¸ºå¢å¼ºç­–ç•¥çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åŸºäºæ‰©æ•£çš„è¯­éŸ³è½¬æ¢æ¨¡å‹ä¸­è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯´è¯è€…ç›¸ä¼¼æ€§ç›¸å…³çš„åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1703655a37d2feee491bab13640f8bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b52cdd3e00f38c7ca9a13465326f0cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488c606425ab791aaad075a834251463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78241af0f60f0b026e4a0f1a5b88b6ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97361c2dbb94cb47af4c50872f96a33a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14bb0a9b135858638072dd9f6a220ac7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50535ca8024965d73559d86a4adcb2e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PromptEVC-Controllable-Emotional-Voice-Conversion-with-Natural-Language-Prompts"><a href="#PromptEVC-Controllable-Emotional-Voice-Conversion-with-Natural-Language-Prompts" class="headerlink" title="PromptEVC: Controllable Emotional Voice Conversion with Natural Language   Prompts"></a>PromptEVC: Controllable Emotional Voice Conversion with Natural Language   Prompts</h2><p><strong>Authors:Tianhua Qi, Shiyan Wang, Cheng Lu, Tengfei Song, Hao Yang, Zhanglin Wu, Wenming Zheng</strong></p>
<p>Controllable emotional voice conversion (EVC) aims to manipulate emotional expressions to increase the diversity of synthesized speech. Existing methods typically rely on predefined labels, reference audios, or prespecified factor values, often overlooking individual differences in emotion perception and expression. In this paper, we introduce PromptEVC that utilizes natural language prompts for precise and flexible emotion control. To bridge text descriptions with emotional speech, we propose emotion descriptor and prompt mapper to generate fine-grained emotion embeddings, trained jointly with reference embeddings. To enhance naturalness, we present a prosody modeling and control pipeline that adjusts the rhythm based on linguistic content and emotional cues. Additionally, a speaker encoder is incorporated to preserve identity. Experimental results demonstrate that PromptEVC outperforms state-of-the-art controllable EVC methods in emotion conversion, intensity control, mixed emotion synthesis, and prosody manipulation. Speech samples are available at <a target="_blank" rel="noopener" href="https://jeremychee4.github.io/PromptEVC/">https://jeremychee4.github.io/PromptEVC/</a>. </p>
<blockquote>
<p>å¯æ§æƒ…æ„Ÿè¯­éŸ³è½¬æ¢ï¼ˆEVCï¼‰æ—¨åœ¨é€šè¿‡æ“çºµæƒ…æ„Ÿè¡¨è¾¾æ¥å¢åŠ åˆæˆè¯­éŸ³çš„å¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„æ ‡ç­¾ã€å‚è€ƒéŸ³é¢‘æˆ–é¢„å…ˆè®¾å®šçš„å› å­å€¼ï¼Œå¾€å¾€å¿½è§†äº†æƒ…æ„Ÿæ„ŸçŸ¥å’Œè¡¨è¾¾çš„ä¸ªä½“å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PromptEVCï¼Œå®ƒåˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ¥è¿›è¡Œç²¾ç¡®å’Œçµæ´»çš„æƒ…ç»ªæ§åˆ¶ã€‚ä¸ºäº†æ¶èµ·æ–‡æœ¬æè¿°ä¸æƒ…æ„Ÿè¯­éŸ³ä¹‹é—´çš„æ¡¥æ¢ï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…æ„Ÿæè¿°ç¬¦å’Œæç¤ºæ˜ å°„å™¨ï¼Œä»¥ç”Ÿæˆç²¾ç»†çš„æƒ…ç»ªåµŒå…¥ï¼Œä¸å‚è€ƒåµŒå…¥è”åˆè®­ç»ƒã€‚ä¸ºäº†æé«˜è‡ªç„¶åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­è°ƒå»ºæ¨¡å’Œæ§åˆ¶æµç¨‹ï¼Œè¯¥æµç¨‹æ ¹æ®è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿçº¿ç´¢è°ƒæ•´èŠ‚å¥ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†è¯´è¯äººç¼–ç å™¨ä»¥ä¿ç•™èº«ä»½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æƒ…æ„Ÿè½¬æ¢ã€å¼ºåº¦æ§åˆ¶ã€æ··åˆæƒ…æ„Ÿåˆæˆå’Œè¯­è°ƒæ“çºµæ–¹é¢ï¼ŒPromptEVCä¼˜äºæœ€æ–°çš„å¯æ§EVCæ–¹æ³•ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://jeremychee4.github.io/PromptEVC/%E6%89%BE%E5%88%B0%E3%80%82">https://jeremychee4.github.io/PromptEVC/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20678v1">PDF</a> Accepted to INTERSPEECH2025</p>
<p><strong>Summary</strong><br>æƒ…æ„Ÿå¯æ§çš„è¯­éŸ³è½¬æ¢ï¼ˆEVCï¼‰æ—¨åœ¨é€šè¿‡æ“çºµæƒ…æ„Ÿè¡¨è¾¾æ¥å¢åŠ åˆæˆè¯­éŸ³çš„å¤šæ ·æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„è®¾æ ‡ç­¾ã€å‚è€ƒéŸ³é¢‘æˆ–é¢„è®¾å› å­å€¼ï¼Œå¾€å¾€å¿½ç•¥äº†æƒ…æ„Ÿæ„ŸçŸ¥å’Œè¡¨è¾¾çš„ä¸ªä½“å·®å¼‚ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç²¾ç¡®çµæ´»çš„æƒ…æ„Ÿæ§åˆ¶çš„PromptEVCæ–¹æ³•ã€‚ä¸ºäº†å°†æ–‡æœ¬æè¿°ä¸æƒ…æ„Ÿè¯­éŸ³ç›¸ç»“åˆï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…ç»ªæè¿°ç¬¦å’Œæç¤ºæ˜ å°„å™¨æ¥ç”Ÿæˆç²¾ç»†çš„æƒ…ç»ªåµŒå…¥ï¼Œä¸å‚è€ƒåµŒå…¥è”åˆè®­ç»ƒã€‚ä¸ºäº†æé«˜è‡ªç„¶åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éŸµå¾‹å»ºæ¨¡å’Œæ§åˆ¶ç®¡é“ï¼Œæ ¹æ®è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿçº¿ç´¢è°ƒæ•´èŠ‚å¥ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†è¯´è¯äººç¼–ç å™¨ä»¥ä¿ç•™èº«ä»½ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptEVCåœ¨æƒ…æ„Ÿè½¬æ¢ã€å¼ºåº¦æ§åˆ¶ã€æ··åˆæƒ…æ„Ÿåˆæˆå’ŒéŸµå¾‹æ“æ§æ–¹é¢ä¼˜äºç°æœ‰çš„å¯æ§EVCæ–¹æ³•ã€‚è¯­éŸ³æ ·æœ¬å¯åœ¨<a href="https://jeremychee4%20.github.io/PromptEVC/%E6%89%BE%E5%88%B0%E3%80%82">https://jeremychee4.github.io/PromptEVC/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PromptEVCæ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œæƒ…æ„Ÿæ§åˆ¶çš„ç²¾ç¡®å’Œçµæ´»è½¬æ¢ã€‚</li>
<li>é€šè¿‡æƒ…ç»ªæè¿°ç¬¦å’Œæç¤ºæ˜ å°„å™¨ç”Ÿæˆç²¾ç»†æƒ…ç»ªåµŒå…¥ï¼Œä¸å‚è€ƒåµŒå…¥è”åˆè®­ç»ƒã€‚</li>
<li>æå‡ºäº†éŸµå¾‹å»ºæ¨¡å’Œæ§åˆ¶ç®¡é“ï¼Œæ ¹æ®è¯­è¨€å†…å®¹å’Œæƒ…æ„Ÿçº¿ç´¢è°ƒæ•´èŠ‚å¥ï¼Œå¢å¼ºè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
<li>èå…¥äº†è¯´è¯äººç¼–ç å™¨ä»¥ä¿ç•™èº«ä»½ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptEVCåœ¨æƒ…æ„Ÿè½¬æ¢ã€å¼ºåº¦æ§åˆ¶ç­‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªåœ¨çº¿è¯­éŸ³æ ·æœ¬åº“ä¾›å…¬ä¼—è®¿é—®ï¼Œå±•ç¤ºäº†ç ”ç©¶æˆæœçš„å®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8d785a97adc8b2c540e11195931d16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7dae50187e25ab49e8c8eaa1c680429.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-210adc9f1beacab427942a3c287bfa6d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Robust-fine-tuning-of-speech-recognition-models-via-model-merging-application-to-disordered-speech"><a href="#Robust-fine-tuning-of-speech-recognition-models-via-model-merging-application-to-disordered-speech" class="headerlink" title="Robust fine-tuning of speech recognition models via model merging:   application to disordered speech"></a>Robust fine-tuning of speech recognition models via model merging:   application to disordered speech</h2><p><strong>Authors:Alexandre Ducorroy, Rachid Riad</strong></p>
<p>Automatic Speech Recognition (ASR) has advanced with Speech Foundation Models (SFMs), yet performance degrades on dysarthric speech due to variability and limited data. This study as part of the submission to the Speech Accessibility challenge, explored model merging to improve ASR generalization using Whisper as the base SFM. We compared fine-tuning with single-trajectory merging, combining models from one fine-tuning path, and multi-run merging, merging independently trained models. Our best multi-run merging approach achieved a 12% relative decrease of WER over classic fine-tuning, and a 16.2% relative decrease on long-form audios, a major loss contributor in dysarthric ASR. Merging more and more models led to continuous gains, remained effective in low-data regimes, and generalized across model architectures. These results highlight model merging as an easily replicable adaptation method that consistently improves ASR without additional inference cost or hyperparameter tuning. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰éšç€è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œä½†ç”±äºå˜å¼‚æ€§å’Œæ•°æ®æœ‰é™ï¼Œå®ƒåœ¨æ„éŸ³éšœç¢è¯­éŸ³ä¸Šçš„æ€§èƒ½ä¼šä¸‹é™ã€‚æœ¬ç ”ç©¶ä½œä¸ºæäº¤ç»™è¯­éŸ³å¯è®¿é—®æ€§æŒ‘æˆ˜çš„ä¸€éƒ¨åˆ†ï¼Œæ¢ç´¢äº†é€šè¿‡æ¨¡å‹åˆå¹¶ä½¿ç”¨whisperä½œä¸ºåŸºæœ¬SFMæ¥æé«˜ASRæ³›åŒ–çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å¾®è°ƒä¸å•è½¨è¿¹åˆå¹¶ï¼Œå³æ¥è‡ªä¸€æ¡å¾®è°ƒè·¯å¾„çš„æ¨¡å‹åˆå¹¶ï¼Œä»¥åŠå¤šè¿è¡Œåˆå¹¶ï¼Œå³åˆå¹¶ç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬æœ€å¥½çš„å¤šè¿è¡Œåˆå¹¶æ–¹æ³•å®ç°äº†ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ¯”ç»å…¸å¾®è°ƒé™ä½12%ï¼Œåœ¨é•¿éŸ³é¢‘ä¸Šé™ä½äº†16.2%ï¼Œè€Œé•¿éŸ³é¢‘æ˜¯æ„éŸ³éšœç¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ä¸»è¦æŸå¤±æ¥æºã€‚åˆå¹¶è¶Šæ¥è¶Šå¤šçš„æ¨¡å‹å¸¦æ¥äº†æŒç»­çš„æ”¶ç›Šï¼Œåœ¨ä½æ•°æ®ç¯å¢ƒä¸­ä»ç„¶æœ‰æ•ˆï¼Œå¹¶ä¸”é€‚ç”¨äºå„ç§æ¨¡å‹æ¶æ„ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æ¨¡å‹åˆå¹¶ä½œä¸ºä¸€ç§æ˜“äºå¤åˆ¶çš„é€‚åº”æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬æˆ–è¶…å‚æ•°è°ƒæ•´çš„æƒ…å†µä¸‹ï¼ŒæŒç»­æé«˜ASRæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20477v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡æ¨¡å‹èåˆæé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å‘éŸ³éšœç¢äººå£«çš„è¯­éŸ³ã€‚ä½¿ç”¨Whisperä½œä¸ºåŸºç¡€è¯­éŸ³æ¨¡å‹ï¼Œé€šè¿‡å•ä¸€è½¨è¿¹èåˆã€å•è·¯å¾„å¾®è°ƒæ¨¡å‹èåˆä»¥åŠå¤šè·¯å¾„ç‹¬ç«‹è®­ç»ƒæ¨¡å‹èåˆçš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚å…¶ä¸­å¤šè·¯å¾„èåˆæ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•å–å¾—äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç›¸å¯¹ä¸‹é™12%ï¼Œåœ¨é•¿ç¯‡éŸ³é¢‘ä¸Šçš„ç›¸å¯¹ä¸‹é™æ›´æ˜¯è¾¾åˆ°äº†æƒŠäººçš„16.2%ã€‚æ¨¡å‹èåˆåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ä¾ç„¶æœ‰æ•ˆï¼Œä¸”å¯è·¨æ¨¡å‹æ¶æ„æ¨å¹¿ï¼Œæˆä¸ºä¸€ç§æ˜“äºå¤åˆ¶ä¸”ç¨³å®šçš„é€‚åº”ç­–ç•¥ï¼ŒæŒç»­æé«˜ASRæ€§èƒ½ï¼Œä¸”æ— éœ€é¢å¤–æ¨ç†æˆæœ¬å’Œè¶…å‚æ•°è°ƒæ•´ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ¨¡å‹èåˆåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­ç”¨äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é’ˆå¯¹å‘éŸ³éšœç¢äººå£«çš„è¯­éŸ³è¿›è¡Œäº†æ¨¡å‹èåˆçš„æ¢ç´¢ã€‚</li>
<li>ä½¿ç”¨Whisperä½œä¸ºåŸºç¡€è¯­éŸ³æ¨¡å‹è¿›è¡Œè¯•éªŒã€‚</li>
<li>å¤šè·¯å¾„èåˆæ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•å–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚</li>
<li>æ¨¡å‹èåˆåœ¨é•¿ç¯‡éŸ³é¢‘ä¸Šçš„æ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>æ¨¡å‹èåˆåœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ä¾ç„¶æœ‰æ•ˆï¼Œå¹¶å¯ä»¥è·¨æ¨¡å‹æ¶æ„æ¨å¹¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d57e98885f3696191369d63887f8663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6126b2b1fbccfe8ac6777345e1807aaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfa725ad4ef9cf5b16f24c316bee67a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a1ce5e590f4130d85a59bb01ebb4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a0cebb30d52d380fc6bee0ffd771149.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition"><a href="#In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition" class="headerlink" title="In-context Language Learning for Endangered Languages in Speech   Recognition"></a>In-context Language Learning for Endangered Languages in Speech   Recognition</h2><p><strong>Authors:Zhaolin Li, Jan Niehues</strong></p>
<p>With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. </p>
<blockquote>
<p>å…¨ä¸–ç•Œå¤§çº¦æœ‰7000ç§è¯­è¨€ï¼Œè€Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åªæ”¯æŒä¸€å°éƒ¨åˆ†ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMå¯ä»¥åœ¨æ²¡æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ä¸ºæŸäº›ä»»åŠ¡å­¦ä¹ æ–°çš„è¯­è¨€ã€‚æˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°è¯­éŸ³è¯†åˆ«é¢†åŸŸï¼Œç ”ç©¶LLMæ˜¯å¦å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥å­¦ä¹ æœªè§è¿‡çš„ä½èµ„æºè¯­è¨€ã€‚æˆ‘ä»¬åœ¨å››ç§å¤šæ ·çš„æ¿’å±è¯­è¨€ä¸Šå¯¹å®éªŒè¿›è¡Œäº†æµ‹è¯•ï¼Œè¿™äº›è¯­è¨€å¹¶æœªç”¨äºè®­ç»ƒLLMï¼Œæˆ‘ä»¬å‘ç°æä¾›ç›¸å…³çš„æ–‡æœ¬æ ·æœ¬å¯ä»¥æé«˜è¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜åŸºäºæ¦‚ç‡çš„æ–¹æ³•åœ¨è¯­è¨€å­¦ä¹ æ–¹é¢çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åŸºäºæŒ‡ä»¤çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿LLMèƒ½å¤Ÿå®ç°çš„ASRæ€§èƒ½å¯ä»¥ä¸ä¸“é—¨é’ˆå¯¹è¿™äº›è¯­è¨€è®­ç»ƒçš„ä¸“ç”¨è¯­è¨€æ¨¡å‹ç›¸å½“ç”šè‡³è¶…è¶Šï¼ŒåŒæ—¶ä¿ç•™LLMçš„åŸå§‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶èƒ½å¤Ÿæ”¯æŒå¤šç§è¯­è¨€ï¼Œä½†ä»…é™äºä¸€å°éƒ¨åˆ†å·²çŸ¥è¯­è¨€ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†LLMsåœ¨è¯­éŸ³è¾¨è¯†æ–¹é¢èƒ½å¦å­¦ä¹ æœªçŸ¥ã€ä½èµ„æºçš„è¯­è¨€ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æä¾›æ›´ä¸ºç›¸å…³çš„æ–‡æœ¬æ ·æœ¬åï¼ŒLLMsåœ¨è¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½æœ‰æ‰€æå‡ã€‚æ­¤å¤–ï¼Œæ¦‚ç‡æ€§æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„æŒ‡ä»¤æ€§æ–¹æ³•ï¼Œåœ¨è¯­è¨€å­¦ä¹ ä¸Šè¡¨ç°æ›´ä¼˜ã€‚åŒæ—¶ï¼ŒICLä½¿å¾—LLMsçš„ASRæ€§èƒ½å¯ä¸é’ˆå¯¹è¿™äº›è¯­è¨€ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šå®ƒä»¬ï¼ŒåŒæ—¶ä¿ç•™äº†LLMsçš„åŸå§‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›®å‰ä»…æ”¯æŒä¸€å°éƒ¨åˆ†è¯­è¨€ï¼Œå¯¹äºæœªçŸ¥çš„ä½èµ„æºè¯­è¨€çš„å­¦ä¹ èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼ŒLLMsèƒ½å¤Ÿå­¦ä¹ å¹¶è¯†åˆ«æœªå—è¿‡è®­ç»ƒçš„è¯­è¨€ã€‚</li>
<li>æä¾›æ›´å¤šç›¸å…³çš„æ–‡æœ¬æ ·æœ¬å¯ä»¥æå‡LLMsåœ¨è¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>åœ¨è¯­è¨€å­¦ä¹ ä¸Šï¼Œæ¦‚ç‡æ€§æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„æŒ‡ä»¤æ€§æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒLLMsçš„ASRæ€§èƒ½å¯ä»¥è¾¾åˆ°ä¸ä¸“é—¨é’ˆå¯¹è¿™äº›è¯­è¨€è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ç”šè‡³è¶…è¶Šçš„æ°´å¹³ã€‚</li>
<li>LLMsåœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­èƒ½å¤Ÿä¿æŒå…¶åŸæœ‰çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4693e55d904af4767449fcaaf7d7d8db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a481ae22b6b4a7dbf577fac08bd0f10c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524f4f6d3eecffdf8692b71393ebec5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829fb446143474eb917e94b751fbab0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95834bfc7bc495782b9a4895d28942c9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection"><a href="#AmpleHate-Amplifying-the-Attention-for-Versatile-Implicit-Hate-Detection" class="headerlink" title="AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection"></a>AmpleHate: Amplifying the Attention for Versatile Implicit Hate   Detection</h2><p><strong>Authors:Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han</strong></p>
<p>Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness. </p>
<blockquote>
<p>éšæ™¦ä»‡æ¨è¨€è®ºçš„æ£€æµ‹é¢‡å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒçš„éšè”½æ€§ï¼Œå¹¶ä¸”ä¾èµ–äºè¯­å¢ƒè§£è¯»è€Œéæ˜æ˜¾çš„å†’çŠ¯æ€§è¯æ±‡ã€‚å½“å‰çš„æ–¹æ³•ä¾èµ–äºå¯¹æ¯”å­¦ä¹ ï¼Œå·²è¯æ˜åœ¨åŒºåˆ†ä»‡æ¨å’Œéä»‡æ¨å¥å­æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œäººç±»æ£€æµ‹éšæ™¦ä»‡æ¨è¨€è®ºæ˜¯å…ˆè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œç„¶åè§£é‡Šè¿™äº›ç›®æ ‡æ˜¯å¦‚ä½•ä¸å‘¨å›´çš„ä¸Šä¸‹æ–‡ç›¸å…³è”çš„ã€‚å—è¿™ç§æ¨ç†è¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†AmpleHateï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†è¿›è¡Œéšæ™¦ä»‡æ¨æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚AmpleHateä½¿ç”¨é¢„è®­ç»ƒçš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹æ¥è¯†åˆ«æ˜ç¡®çš„ç›®æ ‡ï¼Œå¹¶é€šè¿‡[CLS]ä»¤ç‰Œè·å–éšæ™¦ç›®æ ‡ä¿¡æ¯ã€‚å®ƒè®¡ç®—æ˜ç¡®ã€éšæ™¦ç›®æ ‡ä¸å¥å­ä¸Šä¸‹æ–‡ä¹‹é—´çš„åŸºäºæ³¨æ„åŠ›çš„å…³ç³»ï¼Œç„¶åå°†è¿™äº›å…³ç³»å‘é‡ç›´æ¥æ³¨å…¥æœ€ç»ˆçš„å¥å­è¡¨ç¤ºä¸­ã€‚è¿™æ”¾å¤§äº†ç¡®å®šéšæ™¦ä»‡æ¨æ—¶çš„ç›®æ ‡-ä¸Šä¸‹æ–‡å…³ç³»çš„å…³é”®ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒAmpleHateè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡è¶…å‡ºå¯¹æ¯”å­¦ä¹ åŸºå‡†82.14%ï¼Œå¹¶ä¸”å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒAmpleHateäº§ç”Ÿçš„æ³¨æ„åŠ›æ¨¡å¼ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œçªå‡ºäº†å…¶å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19528v2">PDF</a> 13 pages, 4 figures, Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éšæ€§ä»‡æ¨è¨€è®ºæ£€æµ‹çš„æ–°æ–¹æ³•AmpleHateã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯†åˆ«æ˜¾å¼ç›®æ ‡ï¼Œå¹¶é€šè¿‡[CLS]ä»¤ç‰Œæ•è·éšæ€§ç›®æ ‡ä¿¡æ¯ã€‚å®ƒè®¡ç®—æ˜¾å¼ã€éšæ€§ç›®æ ‡ä¸å¥å­ä¸Šä¸‹æ–‡ä¹‹é—´çš„æ³¨æ„åŠ›å…³ç³»ï¼Œç„¶åå°†è¿™äº›å…³ç³»å‘é‡ç›´æ¥æ³¨å…¥æœ€ç»ˆçš„å¥å­è¡¨ç¤ºä¸­ï¼Œä»è€Œæ”¾å¤§ç›®æ ‡ä¸Šä¸‹æ–‡å…³ç³»çš„å…³é”®ä¿¡å·ï¼Œä»¥ç¡®å®šéšæ€§ä»‡æ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒAmpleHateå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æ¯”å¯¹æ¯”å­¦ä¹ åŸºçº¿é«˜å‡º82.14%ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšæ€§ä»‡æ¨è¨€è®ºæ£€æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› å…¶å¾®å¦™æ€§å’Œå¯¹ä¸Šä¸‹æ–‡è§£è¯»çš„ä¾èµ–ï¼Œè€Œéæ˜æ˜¾çš„å†’çŠ¯æ€§è¯æ±‡ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨åŒºåˆ†ä»‡æ¨å’Œéä»‡æ¨å¥å­æ–¹é¢æœ‰æ•ˆã€‚</li>
<li>äººç±»æ£€æµ‹éšæ€§ä»‡æ¨è¨€è®ºé¦–å…ˆè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šç›®æ ‡ï¼Œç„¶åè§£è¯»è¿™äº›ç›®æ ‡ä¸å…¶ä¸Šä¸‹æ–‡çš„å…³ç³»ã€‚</li>
<li>AmpleHateæ–¹æ³•è®¾è®¡æ—¨åœ¨æ¨¡ä»¿äººç±»æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºéšæ€§ä»‡æ¨æ£€æµ‹ã€‚</li>
<li>AmpleHateä½¿ç”¨é¢„è®­ç»ƒçš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯†åˆ«æ˜¾å¼ç›®æ ‡ï¼Œå¹¶é€šè¿‡[CLS]ä»¤ç‰Œæ•è·éšæ€§ç›®æ ‡ä¿¡æ¯ã€‚</li>
<li>AmpleHateè®¡ç®—ç›®æ ‡ï¼ˆæ˜¾æ€§å’Œéšæ€§ï¼‰ä¸å¥å­ä¸Šä¸‹æ–‡ä¹‹é—´çš„æ³¨æ„åŠ›å…³ç³»ï¼Œå¹¶å°†è¿™äº›å…³ç³»æ³¨å…¥å¥å­è¡¨ç¤ºä¸­ï¼Œä»¥å¼ºåŒ–éšæ€§ä»‡æ¨çš„åˆ¤å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14d5c21861a8b01f35235e0aaf765314.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49e983d6bc3ca2daa2d582f60538f1d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea743330f209fb7503c044ab5cb61344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86e5eb9e0cd9c490823477cb0659ab0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching"><a href="#FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching" class="headerlink" title="FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching"></a>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</h2><p><strong>Authors:Ziqian Wang, Zikai Liu, Xinfa Zhu, Yike Zhu, Mingshuai Liu, Jun Chen, Longshuai Xiao, Chao Weng, Lei Xie</strong></p>
<p>Generative models have excelled in audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a conditional flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speechâ€™s temporal-spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outperforms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé‡‡ç”¨äº†è¯­è¨€æ¨¡å‹ã€æ‰©æ•£å’ŒæµåŒ¹é…ç­‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ç”Ÿæˆæ–¹æ³•é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„æ–¹æ³•é­å—é‡åŒ–æŸå¤±ï¼Œå¯¼è‡´è¯´è¯äººç›¸ä¼¼æ€§å’Œå¯æ‡‚åº¦å—æŸï¼Œè€Œæ‰©æ•£æ¨¡å‹åˆ™éœ€è¦å¤æ‚çš„è®­ç»ƒå’Œè¾ƒé«˜çš„æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FlowSEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºSEçš„åŸºäºæµåŒ¹é…çš„æ–¹æ³•ã€‚æµåŒ¹é…åœ¨å•æ¬¡ä¼ é€’ä¸­å­¦ä¹ å™ªå£°å’Œå¹²å‡€è¯­éŸ³åˆ†å¸ƒä¹‹é—´çš„è¿ç»­è½¬æ¢ï¼Œåœ¨ä¿æŒé«˜è´¨é‡é‡å»ºçš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†å»¶è¿Ÿã€‚å…·ä½“æ¥è¯´ï¼ŒFlowSEåœ¨å¸¦æœ‰å™ªå£°çš„æ¢…å°”é¢‘è°±å’Œå¯é€‰å­—ç¬¦åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥åœ°é¢çœŸå®æ¢…å°”é¢‘è°±ä½œä¸ºç›‘ç£æ¥ä¼˜åŒ–æ¡ä»¶æµåŒ¹é…æŸå¤±ã€‚å®ƒéšå¼åœ°å­¦ä¹ è¯­éŸ³çš„æ—¶ç©ºç»“æ„ä»¥åŠæ–‡æœ¬ä¸è¯­éŸ³çš„å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒFlowSEå¯ä»¥å¸¦æœ‰æˆ–ä¸å¸¦æ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ“ä½œï¼Œåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹éƒ½å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œåœ¨æœ‰æ–‡å­—ç¨¿çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æ”¹è¿›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlowSEæ˜¾è‘—ä¼˜äºæœ€æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºåŸºäºç”Ÿæˆçš„SEå»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œå¹¶å±•ç¤ºäº†æµåŒ¹é…åœ¨è¯¥é¢†åŸŸçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ã€é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å’ŒéŸ³é¢‘æ ·æœ¬å¯ä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19476v2">PDF</a> Accepted to InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæµåŒ¹é…ï¼ˆFlowSEï¼‰çš„è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚FlowSEé€šè¿‡å­¦ä¹ å’Œè½¬æ¢å™ªå£°å’Œæ¸…æ´è¯­éŸ³åˆ†å¸ƒä¹‹é—´çš„è¿ç»­è½¬æ¢ï¼Œå®ç°äº†é«˜è´¨é‡çš„è¯­éŸ³é‡å»ºï¼ŒåŒæ—¶é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚è¯¥æ¨¡å‹é€šè¿‡è®­ç»ƒå™ªå£°æ¢…å°”é¢‘è°±å’Œå¯é€‰å­—ç¬¦åºåˆ—ï¼Œä¼˜åŒ–æ¡ä»¶æµåŒ¹é…æŸå¤±ï¼Œå¹¶éšå¼å­¦ä¹ è¯­éŸ³çš„æ—¶ç©ºè°±ç»“æ„å’Œæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒFlowSEåœ¨æœ‰æ— æ–‡æœ¬ä¿¡æ¯çš„æƒ…å†µä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æä¾›è½¬å½•æ—¶è¿›ä¸€æ­¥æ”¹è¿›ã€‚å®ƒæ˜¾è‘—ä¼˜äºå½“å‰ä¸»æµçš„ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºåŸºäºç”Ÿæˆçš„è¯­éŸ³å¢å¼ºæä¾›äº†æ–°çš„èŒƒä¾‹ï¼Œå¹¶å±•ç¤ºäº†æµåŒ¹é…åœ¨è¯¥é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹ã€æ‰©æ•£å’ŒæµåŒ¹é…ç­‰æ–¹æ³•ã€‚</li>
<li>ç°æœ‰è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰çš„ç”Ÿæˆæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è¯­è¨€æ¨¡å‹çš„é‡åŒ–æŸå¤±å’Œæ‰©æ•£æ¨¡å‹çš„å¤æ‚è®­ç»ƒå’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>FlowSEæ˜¯ä¸€ä¸ªåŸºäºæµåŒ¹é…çš„SEæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ å’Œè½¬æ¢å™ªå£°å’Œæ¸…æ´è¯­éŸ³åˆ†å¸ƒä¹‹é—´çš„è¿ç»­è½¬æ¢æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>FlowSEé€šè¿‡è®­ç»ƒå™ªå£°æ¢…å°”é¢‘è°±å’Œå­—ç¬¦åºåˆ—ä¼˜åŒ–æ¡ä»¶æµåŒ¹é…æŸå¤±ï¼Œéšå¼å­¦ä¹ è¯­éŸ³çš„æ—¶ç©ºè°±ç»“æ„å’Œæ–‡æœ¬è¯­éŸ³å¯¹é½ã€‚</li>
<li>FlowSEåœ¨æœ‰æ— æ–‡æœ¬ä¿¡æ¯çš„æƒ…å†µä¸‹å‡å¯è¿è¡Œï¼Œä¸”åœ¨æä¾›è½¬å½•æ—¶æ€§èƒ½æ›´ä½³ã€‚</li>
<li>FlowSEæ˜¾è‘—ä¼˜äºå½“å‰ä¸»æµçš„ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºåŸºäºç”Ÿæˆçš„è¯­éŸ³å¢å¼ºæä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e85cdf25ab89eecb8bf3f3349182e241.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fea74b2eb2ceff0e0a40bee92bd325b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d25fc3fa38e551dd3675fd2b7dd5a878.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training"><a href="#CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training" class="headerlink" title="CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training"></a>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training</h2><p><strong>Authors:Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye</strong></p>
<p>In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3">https://funaudiollm.github.io/cosyvoice3</a>. </p>
<blockquote>
<p>åœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„æµå¼è¯­éŸ³åˆæˆæ¨¡å‹CosyVoice 2ï¼Œå®ƒé›†æˆäº†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºåˆ†å—çš„æµåŒ¹é…ï¼ˆFMï¼‰æ¨¡å‹ï¼Œå®ç°äº†ä½å»¶è¿Ÿçš„åŒæµè¯­éŸ³åˆæˆå’Œäººè´¨é‡ç›¸å½“çš„æ•ˆæœã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼ŒCosyVoice 2åœ¨è¯­è¨€è¦†ç›–ã€é¢†åŸŸå¤šæ ·æ€§ã€æ•°æ®é‡ã€æ–‡æœ¬æ ¼å¼å’Œè®­ç»ƒåæŠ€æœ¯æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CosyVoice 3ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¹è¿›å‹çš„æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é‡å¤–é›¶å¯åŠ¨å¤šè¯­ç§è¯­éŸ³åˆæˆï¼Œåœ¨å†…å®¹ä¸€è‡´æ€§ã€æ¼”è®²è€…ç›¸ä¼¼æ€§å’ŒéŸµå¾‹è‡ªç„¶æ€§æ–¹é¢è¶…è¿‡äº†å…¶å‰èº«ã€‚CosyVoice 3çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š1ï¼‰ä¸€ç§æ–°å‹è¯­éŸ³æ ‡è®°å™¨ï¼Œé€šè¿‡ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒå¼€å‘ï¼Œæ—¨åœ¨æé«˜éŸµå¾‹çš„è‡ªç„¶æ€§ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€è¯­è¨€è¯†åˆ«ã€éŸ³é¢‘äº‹ä»¶æ£€æµ‹å’Œè¯´è¯äººåˆ†æã€‚2ï¼‰ä¸€ä¸ªæ–°çš„å¯å¾®å¥–åŠ±æ¨¡å‹ï¼Œé€‚ç”¨äºè®­ç»ƒåçš„åº”ç”¨ï¼Œä¸ä»…é€‚ç”¨äºCosyVoice 3ï¼Œè€Œä¸”é€‚ç”¨äºå…¶ä»–åŸºäºLLMçš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚3ï¼‰æ•°æ®é›†å¤§å°æ‰©å±•ï¼šè®­ç»ƒæ•°æ®ä»ä¸€ä¸‡å°æ—¶æ‰©å±•åˆ°ä¸€ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–9ç§è¯­è¨€å’Œ18ç§ä¸­æ–‡æ–¹è¨€ï¼Œè·¨è¶Šå„ç§é¢†åŸŸå’Œæ–‡æœ¬æ ¼å¼ã€‚4ï¼‰æ¨¡å‹å¤§å°æ‰©å±•ï¼šæ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œç”±äºæ¨¡å‹å®¹é‡æ›´å¤§ï¼Œæˆ‘ä»¬çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•æ€§èƒ½å¾—åˆ°æå‡ã€‚è¿™äº›è¿›å±•å¯¹é‡å¤–è¯­éŸ³åˆæˆçš„è¿›æ­¥åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬é¼“åŠ±è¯»è€…é€šè¿‡<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3%E8%81%86%E5%90%AC%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice3è†å¬æ¼”ç¤ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17589v2">PDF</a> Preprint, work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CosyVoice 3æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸Šä¸€ä»£è¯­éŸ³åˆæˆæ¨¡å‹CosyVoice 2çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œæ—¨åœ¨å®ç°é›¶æ ·æœ¬å¤šè¯­ç§é‡å¤–è¯­éŸ³åˆæˆã€‚CosyVoice 3åœ¨å†…å®¹ä¸€è‡´æ€§ã€æ¼”è®²è€…ç›¸ä¼¼æ€§å’Œè¯­è°ƒè‡ªç„¶æ€§æ–¹é¢è¶…è¶Šäº†å…¶å‰èº«ã€‚ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šä¸€ã€é‡‡ç”¨æ–°å‹è¯­éŸ³åˆ†è¯å™¨ï¼Œé€šè¿‡åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€è¯­è¨€è¯†åˆ«ã€éŸ³é¢‘äº‹ä»¶æ£€æµ‹å’Œè¯´è¯äººåˆ†æåœ¨å†…çš„ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒæ¥æé«˜è¯­è°ƒè‡ªç„¶åº¦ï¼›äºŒã€å¼€å‘æ–°çš„å¯å¾®å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºåæœŸè®­ç»ƒï¼Œä¸ä»…é€‚ç”¨äºCosyVoice 3ï¼Œä¹Ÿé€‚ç”¨äºå…¶ä»–åŸºäºLLMçš„è¯­éŸ³åˆæˆæ¨¡å‹ï¼›ä¸‰ã€æ•°æ®é›†å¤§å°æ‰©å±•ï¼Œè®­ç»ƒæ•°æ®ä»ä¸€ä¸‡å°æ—¶æ‰©å¤§åˆ°ä¸€ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–9ç§è¯­è¨€å’Œ18ç§ä¸­æ–‡æ–¹è¨€ï¼Œä»¥åŠå¤šç§é¢†åŸŸå’Œæ–‡æœ¬æ ¼å¼ï¼›å››ã€æ¨¡å‹å¤§å°æ‰©å±•ï¼Œæ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œç”±äºæ¨¡å‹å®¹é‡æ›´å¤§ï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¢å¼ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CosyVoice 3æ¨¡å‹è¢«ä»‹ç»ä¸ºä¸€ç§æ”¹è¿›çš„é›¶æ ·æœ¬å¤šè¯­ç§é‡å¤–è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œè¶…è¶Šäº†å…ˆå‰çš„CosyVoice 2æ¨¡å‹ã€‚</li>
<li>CosyVoice 3é‡‡ç”¨äº†æ–°å‹è¯­éŸ³åˆ†è¯å™¨ï¼Œé€šè¿‡ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒæé«˜è¯­è°ƒè‡ªç„¶åº¦ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„å¯å¾®å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºåæœŸè®­ç»ƒï¼Œé€‚ç”¨äºå¤šç§è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li>
<li>è®­ç»ƒæ•°æ®é›†å¤§å°ä»åä¸‡å°æ—¶æ‰©å±•è‡³ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–å¤šç§è¯­è¨€å’Œä¸­æ–‡æ–¹è¨€ã€é¢†åŸŸåŠæ–‡æœ¬æ ¼å¼ã€‚</li>
<li>æ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œæå‡äº†åœ¨å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚</li>
<li>è¿™äº›æ”¹è¿›å¯¹é‡å¤–è¯­éŸ³åˆæˆé¢†åŸŸæœ‰æ˜¾è‘—è´¡çŒ®ã€‚</li>
<li>è¯»è€…å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3%E8%81%86%E5%90%ACDemo%E3%80%82">https://funaudiollm.github.io/cosyvoice3è†å¬Demoã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7352d250abd85e3ba1e29ce146bf593.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a151dcf188a216bc252c52f1eeb9500f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-becc3688ac33167ec55c47a327468b56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c3b320b2d2bf7d701dc185ecaa1938c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance"><a href="#X-ARES-A-Comprehensive-Framework-for-Assessing-Audio-Encoder-Performance" class="headerlink" title="X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance"></a>X-ARES: A Comprehensive Framework for Assessing Audio Encoder   Performance</h2><p><strong>Authors:Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, Jian Luan</strong></p>
<p>We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a novel open-source benchmark designed to systematically assess audio encoder performance across diverse domains. By encompassing tasks spanning speech, environmental sounds, and music, X-ARES provides two evaluation approaches for evaluating audio representations: linear fine-tuning and unparameterized evaluation. The framework includes 22 distinct tasks that cover essential aspects of audio processing, from speech recognition and emotion detection to sound event classification and music genre identification. Our extensive evaluation of state-of-the-art audio encoders reveals significant performance variations across different tasks and domains, highlighting the complexity of general audio representation learning. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†X-ARESï¼ˆæ‰©å±•éŸ³é¢‘è¡¨ç¤ºå’Œè¯„ä¼°å¥—ä»¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¼€æºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒé¢†åŸŸéŸ³é¢‘ç¼–ç å™¨çš„æ€§èƒ½ã€‚X-ARESæ¶µç›–äº†è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ç­‰ä»»åŠ¡ï¼Œä¸ºè¯„ä¼°éŸ³é¢‘è¡¨ç¤ºæä¾›äº†ä¸¤ç§è¯„ä¼°æ–¹æ³•ï¼šçº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ¶µç›–éŸ³é¢‘å¤„ç†å„ä¸ªæ–¹é¢çš„22ä¸ªä¸åŒä»»åŠ¡ï¼Œä»è¯­éŸ³è¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹åˆ°å£°éŸ³äº‹ä»¶åˆ†ç±»å’ŒéŸ³ä¹é£æ ¼è¯†åˆ«ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„éŸ³é¢‘ç¼–ç å™¨çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œè¿™çªå‡ºäº†é€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16369v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>X-ARESæ˜¯ä¸€ä¸ªç”¨äºå…¨é¢è¯„ä¼°éŸ³é¢‘ç¼–ç å™¨æ€§èƒ½çš„æ–°å¼€æ”¾æºä»£ç åŸºå‡†æµ‹è¯•å¹³å°ã€‚å®ƒæ¶µç›–äº†æ¶µç›–è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ç­‰é¢†åŸŸçš„ä»»åŠ¡ï¼Œæä¾›äº†çº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ä¸¤ç§è¯„ä¼°éŸ³é¢‘è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ¶µç›–éŸ³é¢‘å¤„ç†å„ä¸ªæ–¹é¢çš„22ä¸ªä¸åŒä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿæ£€æµ‹ã€å£°éŸ³äº‹ä»¶åˆ†ç±»å’ŒéŸ³ä¹æµæ´¾è¯†åˆ«ç­‰ã€‚å¯¹æœ€å…ˆè¿›çš„éŸ³é¢‘ç¼–ç å™¨çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œè¿™çªå‡ºäº†é€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-ARESæ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å®ƒæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°éŸ³é¢‘ç¼–ç å™¨åœ¨å¤šæ ·åŒ–é¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>X-ARESæä¾›äº†çº¿æ€§å¾®è°ƒå’Œæ— å‚æ•°è¯„ä¼°ä¸¤ç§è¯„ä¼°éŸ³é¢‘è¡¨ç¤ºçš„æ–¹æ³•ã€‚</li>
<li>è¯¥å¹³å°æ¶µç›–äº†è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ç­‰é¢†åŸŸçš„ä»»åŠ¡ã€‚</li>
<li>æ¡†æ¶åŒ…å«æ¶µç›–éŸ³é¢‘å¤„ç†å„ä¸ªæ–¹é¢çš„22ä¸ªä»»åŠ¡ã€‚</li>
<li>å¯¹éŸ³é¢‘ç¼–ç å™¨çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3aafb319a302061fa2c11263ee372b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f487d64a504192774a4ba644053fa077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-701069cc0f0b87a96d423c67478de3a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35d2479d211a526218f3d83bb87f4ce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c13c9fb47b5d8934925001fb3ea299b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis"><a href="#TCSinger-2-Customizable-Multilingual-Zero-shot-Singing-Voice-Synthesis" class="headerlink" title="TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis"></a>TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao</strong></p>
<p>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks. Singing voice samples are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/">https://aaronz345.github.io/TCSinger2Demo/</a>. </p>
<blockquote>
<p>å¯å®šåˆ¶çš„å¤šè¯­è¨€é›¶æ ·æœ¬æ­Œå£°åˆæˆï¼ˆSVSï¼‰åœ¨éŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ç­‰æ–¹é¢å…·æœ‰å„ç§æ½œåœ¨åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SVSæ¨¡å‹è¿‡äºä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶å¯¼è‡´éŸ³ç´ å’ŒéŸ³ç¬¦ä¹‹é—´çš„è¿‡æ¸¡ä¸è‡ªç„¶ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜ç¼ºä¹é€šè¿‡ä¸åŒæç¤ºè¿›è¡Œæœ‰æ•ˆçš„å¤šçº§é£æ ¼æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TCSinger 2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸åŒæç¤ºè¿›è¡Œé£æ ¼è½¬æ¢å’Œé£æ ¼æ§åˆ¶çš„å¤šä»»åŠ¡å¤šè¯­è¨€é›¶æ ·æœ¬SVSæ¨¡å‹ã€‚TCSinger 2ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼š1ï¼‰æ¨¡ç³Šè¾¹ç•Œå†…å®¹ï¼ˆBBCï¼‰ç¼–ç å™¨ï¼Œå®ƒé¢„æµ‹æŒç»­æ—¶é—´ï¼Œæ‰©å±•å†…å®¹åµŒå…¥ï¼Œå¹¶å¯¹è¾¹ç•Œåº”ç”¨æ©ç ä»¥å®ç°å¹³æ»‘è¿‡æ¸¡ã€‚2ï¼‰è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä»æ­Œå£°ã€è¯­éŸ³å’Œæ–‡æœ¬æç¤ºä¸­æå–å¯¹é½è¡¨ç¤ºã€‚3ï¼‰åŸºäºæµçš„è‡ªå®šä¹‰è½¬æ¢å™¨ï¼Œåˆ©ç”¨Cus-MOEå’ŒF0ç›‘ç£ï¼Œæé«˜ç”Ÿæˆæ­Œå£°çš„åˆæˆè´¨é‡å’Œé£æ ¼å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSinger 2åœ¨å¤šä¸ªç›¸å…³ä»»åŠ¡çš„ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­Œå£°æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://aaronz345.github.io/TCSinger2Demo/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14910v2">PDF</a> Accepted by Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šä»»åŠ¡çš„å¤šè¯­ç§é›¶æ ·æœ¬æ¼”å”±å£°éŸ³åˆæˆï¼ˆSVSï¼‰æ¨¡å‹TCSinger 2ï¼Œå…·å¤‡é£æ ¼è¿ç§»å’ŒåŸºäºä¸åŒæç¤ºçš„é£æ ¼æ§åˆ¶åŠŸèƒ½ï¼Œå¯å¹¿æ³›åº”ç”¨äºéŸ³ä¹åˆ›ä½œå’ŒçŸ­è§†é¢‘é…éŸ³ã€‚å®ƒé€šè¿‡æ¨¡ç³Šè¾¹ç•Œå†…å®¹ç¼–ç å™¨ã€è‡ªå®šä¹‰éŸ³é¢‘ç¼–ç å™¨å’ŒåŸºäºæµçš„è‡ªå®šä¹‰è½¬æ¢å™¨ç­‰æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰SVSæ¨¡å‹è¿‡åº¦ä¾èµ–éŸ³ç´ å’ŒéŸ³ç¬¦è¾¹ç•Œæ³¨é‡Šã€ç¼ºä¹å¤šå±‚æ¬¡é£æ ¼æ§åˆ¶ç­‰é—®é¢˜ï¼Œæé«˜äº†é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œåˆæˆæ¼”å”±å£°éŸ³çš„è´¨æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Customizable multilingual zero-shot singing voice synthesis (SVS) has potential applications in music composition and short video dubbing.</li>
<li>Existing SVS models rely heavily on phoneme and note boundary annotations, limiting their performance in zero-shot scenarios.</li>
<li>TCSinger 2 model is introduced to overcome these challenges, with three key modules: Blurred Boundary Content Encoder, Custom Audio Encoder, and Flow-based Custom Transformer.</li>
<li>The Blurred Boundary Content Encoder enables smooth transitions between phonemes and notes.</li>
<li>The Custom Audio Encoder uses contrastive learning for effective representations from singing, speech, and textual prompts.</li>
<li>The Flow-based Custom Transformer leverages Cus-MOE with F0 supervision to enhance synthesis quality and style modeling.</li>
<li>Experimental results show that TCSinger 2 outperforms baseline models in multiple related tasks, and singing voice samples are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/TCSinger2Demo/">https://aaronz345.github.io/TCSinger2Demo/</a>.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14e5544bbcf104bc071282cfbc66fda2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27a4eb1027100c021e210d447c2a5f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99887b38acacc46968bc74cb19d1cd6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e608cb554b61d11570a3fefe79d414fd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>é’ˆå¯¹å‘éŸ³å›°éš¾è€…ä½¿ç”¨çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨éè‹±è¯­è¯­è¨€çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è‹±æ–‡å‘éŸ³å›°éš¾è¯­éŸ³ï¼ˆUASpeechï¼‰è¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ï¼Œç„¶åå°†æ­¤æ¨¡å‹åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³å›°éš¾è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®éšåè¢«ç”¨äºå¾®è°ƒå¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³å›°éš¾è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³å›°éš¾çš„ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v2">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éè‹±è¯­è¯­è¨€çš„å‘éŸ³éšœç¢è¯­éŸ³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜åœ¨è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰ä¸Šå¾®è°ƒäº†ä¸€ä¸ªè¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç¼–ç è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹æ‰­æ›²ã€‚ç„¶åï¼Œä»–ä»¬ä½¿ç”¨è¯¥æ¨¡å‹å°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­çš„å‘éŸ³éšœç¢è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºå¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶è½¬æ¢è¯´è¯äººå’ŒéŸµå¾‹çš„è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³çš„ASRé¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åœ¨è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ä¸Šå¾®è°ƒä¸€ä¸ªåŒæ—¶ç¼–ç è¯´è¯äººç‰¹æ€§å’ŒéŸµå¾‹æ‰­æ›²çš„è¯­éŸ³è½¬æ¢æ¨¡å‹æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨è¯¥æ¨¡å‹å°†å¥åº·éè‹±è¯­è¯­éŸ³è½¬æ¢ä¸ºéè‹±è¯­çš„å‘éŸ³éšœç¢è¯­éŸ³ï¼Œç”Ÿæˆçš„æ•°æ®ç”¨äºè¿›ä¸€æ­¥å¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç»“åˆè¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—æé«˜äº†ASRæ€§èƒ½ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„ASRå¢å¼ºæŠ€æœ¯å’Œç°æˆçš„å¤šè¯­è¨€ASRæ¨¡å‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¯å®äº†è¯¥æ¨¡å‹æ¨¡æ‹Ÿçš„å‘éŸ³éšœç¢è¯­éŸ³çš„çœŸå®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d674a6a9a988e7dc28dc3076a278e24c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851cefed71dd03055dee202aa3b93374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bff05fd4865b97d0af86cd3b7475e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Multimodal-Information-Based-Speech-Processing-MISP-2025-Challenge-Audio-Visual-Diarization-and-Recognition"><a href="#The-Multimodal-Information-Based-Speech-Processing-MISP-2025-Challenge-Audio-Visual-Diarization-and-Recognition" class="headerlink" title="The Multimodal Information Based Speech Processing (MISP) 2025   Challenge: Audio-Visual Diarization and Recognition"></a>The Multimodal Information Based Speech Processing (MISP) 2025   Challenge: Audio-Visual Diarization and Recognition</h2><p><strong>Authors:Ming Gao, Shilong Wu, Hang Chen, Jun Du, Chin-Hui Lee, Shinji Watanabe, Jingdong Chen, Siniscalchi Sabato Marco, Odette Scharenborg</strong></p>
<p>Meetings are a valuable yet challenging scenario for speech applications due to complex acoustic conditions. This paper summarizes the outcomes of the MISP 2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal, multi-device meeting transcription by incorporating video modality alongside audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR). We present the challengeâ€™s objectives, tasks, dataset, baseline systems, and solutions proposed by participants. The best-performing systems achieved significant improvements over the baseline: the top AVSD model achieved a Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the best AVDR system achieved a concatenated minimum-permutation Character Error Rate (cpCER) of 11.56%, improving by 72.49%. </p>
<blockquote>
<p>ä¼šè®®å¯¹äºè¯­éŸ³åº”ç”¨æ¥è¯´æ˜¯ä¸€ä¸ªå®è´µä¸”å……æ»¡æŒ‘æˆ˜çš„åœºæ™¯ï¼Œç”±äºå­˜åœ¨å¤æ‚çš„å£°å­¦æ¡ä»¶ã€‚æœ¬æ–‡æ€»ç»“äº†MISP 2025æŒ‘æˆ˜çš„æˆæœï¼Œè¯¥æŒ‘æˆ˜äºInterspeech 2025ä¸¾åŠï¼Œä¾§é‡äºç»“åˆè§†é¢‘æ¨¡å¼çš„å¤šæ¨¡æ€ã€å¤šè®¾å¤‡ä¼šè®®è½¬å½•ã€‚ä»»åŠ¡åŒ…æ‹¬éŸ³é¢‘-è§†é¢‘è¯´è¯äººå®šä½ï¼ˆAVSDï¼‰ã€éŸ³é¢‘-è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’ŒéŸ³é¢‘-è§†é¢‘å®šä½ä¸è¯†åˆ«ï¼ˆAVDRï¼‰ã€‚æˆ‘ä»¬ä»‹ç»äº†æŒ‘æˆ˜çš„ç›®æ ‡ã€ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†ç³»ç»Ÿä»¥åŠå‚ä¸è€…æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿçš„æ€§èƒ½å®ç°äº†æ˜¾è‘—æé«˜ï¼šæœ€ä½³AVSDæ¨¡å‹çš„è¯´è¯äººåˆ†å‰²é”™è¯¯ç‡ï¼ˆDERï¼‰è¾¾åˆ°äº†8.09%ï¼Œæé«˜äº†7.43%ï¼›æœ€ä½³AVSRç³»ç»Ÿçš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰è¾¾åˆ°äº†9.48%ï¼Œæé«˜äº†10.62%ï¼›æœ€ä½³AVDRç³»ç»Ÿçš„ä¸²è”æœ€å°æ’åˆ—å­—ç¬¦é”™è¯¯ç‡ï¼ˆcpCERï¼‰è¾¾åˆ°äº†11.56%ï¼Œæé«˜äº†72.49%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13971v2">PDF</a> Accepted by Interspeech 2025. Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ€»ç»“äº†MISP 2025æŒ‘æˆ˜çš„æˆæœï¼Œè¯¥æŒ‘æˆ˜èšç„¦äºå¤šæ¨¡æ€ã€å¤šè®¾å¤‡çš„ä¼šè®®è½¬å½•æŠ€æœ¯ï¼Œç»“åˆäº†è§†é¢‘æ¨¡æ€ä¸éŸ³é¢‘ã€‚æŒ‘æˆ˜ä»»åŠ¡åŒ…æ‹¬éŸ³é¢‘è§†è§‰è¯´è¯äººè¯†åˆ«ï¼ˆAVSDï¼‰ã€éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’ŒéŸ³é¢‘è§†è§‰è¯´è¯äººè¯†åˆ«å’Œè¯­éŸ³è¯†åˆ«ï¼ˆAVDRï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†æŒ‘æˆ˜çš„ç›®æ ‡ã€ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†ç³»ç»Ÿå’Œå‚ä¸è€…æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚æœ€ä½³ç³»ç»Ÿè¡¨ç°æ˜¾è‘—ä¼˜äºåŸºå‡†ç³»ç»Ÿï¼Œå…¶ä¸­AVSDæ¨¡å‹æœ€ä½³ç»“æœçš„DERä¸º8.09%ï¼ŒAVSRç³»ç»Ÿçš„CERä¸º9.48%ï¼Œè€ŒAVDRç³»ç»Ÿçš„cpCERä¸ºæœ€ä½å€¼ã€‚è¿™äº›æ”¹è¿›è¯æ˜äº†å¤šæ¨¡æ€æŠ€æœ¯åœ¨ä¼šè®®åœºæ™¯çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹æ‘˜è¦ï¼š</p>
<ul>
<li>MISP 2025æŒ‘æˆ˜åœ¨ä¼šè®®åœºæ™¯çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸Šè¿›è¡Œäº†å¤šæ–¹é¢çš„å°è¯•å’Œç ”ç©¶ã€‚è¿™æ¶‰åŠè§†é¢‘ä¸éŸ³é¢‘ç»“åˆçš„è¯†åˆ«æŠ€æœ¯ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°è§£æå’Œè§£é‡Šè¯­éŸ³ä¿¡å·åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å˜åŒ–ã€‚</li>
<li>è¯¥æŒ‘æˆ˜åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šéŸ³é¢‘è§†è§‰è¯´è¯äººè¯†åˆ«ï¼ˆAVSDï¼‰ã€éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œç»“åˆäºŒè€…çš„è¯†åˆ«ä»»åŠ¡ï¼ˆAVDRï¼‰ã€‚è¿™ä¸‰ä¸ªä»»åŠ¡éƒ½å¯¹è¯†åˆ«å¤æ‚æ€§å’Œå‡†ç¡®åº¦æå‡ºäº†è¾ƒé«˜è¦æ±‚ã€‚</li>
<li>æŒ‘æˆ˜çš„ç›®æ ‡æ˜¯æ¨è¿›å¤šæ¨¡æ€å’Œå¤šè®¾å¤‡åœ¨ä¼šè®®åœºæ™¯çš„åº”ç”¨èƒ½åŠ›ï¼Œè¿™åŒ…æ‹¬äº†å¤æ‚å£°å­¦æ¡ä»¶ä¸‹çš„å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†æŠ€æœ¯ã€‚</li>
<li>æœ€ä½³ç³»ç»Ÿè¡¨ç°æ˜¾è‘—ä¼˜äºåŸºå‡†ç³»ç»Ÿï¼Œè¿™è¯æ˜äº†å¤šæ¨¡æ€æŠ€æœ¯åœ¨ä¼šè®®åœºæ™¯çš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚å…·ä½“æ¥è¯´ï¼ŒAVSDæ¨¡å‹åœ¨è¯´è¯äººè¯†åˆ«ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè€ŒAVSRç³»ç»Ÿåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚åŒæ—¶ï¼ŒAVDRç³»ç»Ÿçš„æ€§èƒ½æå‡æœ€ä¸ºæ˜¾è‘—ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ae83125e4f971ef6d1633ee45c97627d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17607389671e8b796c4b75f1b7ea4ae0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e4ec8a20dd9c07acc03934a81d075dc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding"><a href="#U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding" class="headerlink" title="U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding"></a>U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding</h2><p><strong>Authors:Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie</strong></p>
<p>The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/">https://github.com/Honee-W/U-SAM/</a>). </p>
<blockquote>
<p>éŸ³é¢‘ä»»åŠ¡æ–‡æœ¬ç”ŸæˆèŒƒå¼ä¸ºç»Ÿä¸€éŸ³é¢‘ç†è§£å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å®ç°ä¸åŒç±»å‹éŸ³é¢‘çš„å…¨é¢ç†è§£æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³ã€ä¸€èˆ¬éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹äº¤å‰ç†µæŸå¤±çš„è¿‡åº¦ä¾èµ–å¾€å¾€ä¼šå¯¼è‡´å¯¹é½ä¸è¶³ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±å¹³ç­‰å¯¹å¾…æ‰€æœ‰ä»¤ç‰Œï¼Œå¹¶ä¸”æ— æ³•å¤„ç†å†—ä½™çš„éŸ³é¢‘ç‰¹å¾ï¼Œä»è€Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½è¾ƒå¼±ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†U-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†é’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ä¸“ä¸šç¼–ç å™¨ï¼Œä»¥åŠä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚U-SAMé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±ä»ªè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼ŒåŠ¨æ€è·¯ç”±å’Œé›†æˆé¢†åŸŸç‰¹å®šçš„ç¼–ç å™¨è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜é‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶çº æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒU-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæ–°å…´èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶æ³›åŒ–æ½œåŠ›ã€‚ä»£ç å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/%EF%BC%89%E3%80%82">https://github.com/Honee-W/U-SAM/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13880v3">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹éŸ³é¢‘ä»»åŠ¡çš„æ–°å‹æ–‡æœ¬ç”ŸæˆèŒƒå¼ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨ç»Ÿä¸€éŸ³é¢‘ç†è§£æ–¹é¢çš„æ–°å¯èƒ½æ€§ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä¸åŒç±»å‹çš„éŸ³é¢‘ï¼ˆå¦‚è¯­éŸ³ã€é€šç”¨éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶åŸºäºäº¤å‰ç†µæŸå¤±çš„å•ä¸€å¯¹é½æ–¹æ³•å¸¸å¸¸éš¾ä»¥åº”å¯¹å¤æ‚å¤šå˜çš„éŸ³é¢‘ç‰¹å¾ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½æ€§èƒ½å—é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†U-SAMé«˜çº§éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é›†æˆäº†é’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ä¸“é—¨ç¼–ç å™¨ä»¥åŠé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚U-SAMåˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±ä»ªè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼ŒåŠ¨æ€è·¯ç”±å’Œé›†æˆç‰¹å®šé¢†åŸŸçš„ç¼–ç å™¨è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€å­¦ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œæ ¡æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒU-SAMåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒU-SAMåœ¨æœªè§ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåœ¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ä»»åŠ¡æ–‡æœ¬ç”ŸæˆèŒƒå¼ä¸ºç»Ÿä¸€éŸ³é¢‘ç†è§£å¸¦æ¥äº†æ–°æœºä¼šã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šæ ·éŸ³é¢‘ç±»å‹æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>U-SAMæ¨¡å‹é›†æˆäº†ä¸“é—¨ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>U-SAMé‡‡ç”¨æ··åˆä¸“å®¶æŠ•å½±ä»ªè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆã€‚</li>
<li>U-SAMå¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>U-SAMåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>U-SAMåœ¨æœªè§ä»»åŠ¡ä¸Šå±•ç°å‡ºæ³›åŒ–æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c58b37e8d24632519093c85e3fcd3bfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b367bb8b913388b6cde7df16fdc82eee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-145be9705a0d75da8c744da23ca6faa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e0a1c73fd5d5e2a42a987fcd7cee72.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-855b5ea0fddf5b3fac68205bef55358c.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Towards Generalized Proactive Defense against Face Swapping with   Contour-Hybrid Watermark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62a1deddab7d7a2e78d281e1ec1d7145.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Contrastive Desensitization Learning for Cross Domain Face Forgery   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
