<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  VoiceAssistant-Eval Benchmarking AI Assistants across Listening,   Speaking, and Viewing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-30-æ›´æ–°"><a href="#2025-09-30-æ›´æ–°" class="headerlink" title="2025-09-30 æ›´æ–°"></a>2025-09-30 æ›´æ–°</h1><h2 id="VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing"><a href="#VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing" class="headerlink" title="VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,   Speaking, and Viewing"></a>VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,   Speaking, and Viewing</h2><p><strong>Authors:Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li</strong></p>
<p>The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systemsâ€™ capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at <a target="_blank" rel="noopener" href="https://mathllm.github.io/VoiceAssistantEval/">https://mathllm.github.io/VoiceAssistantEval/</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€ç³»ç»Ÿçš„èƒ½åŠ›å¢é•¿å¼•å‘äº†äººä»¬å¯¹ä»¥è¯­éŸ³ä¸ºä¸»çš„AIåŠ©æ‰‹çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸è¶³ä»¥è¯„ä¼°è¿™äº›ç³»ç»Ÿçš„å…¨éƒ¨èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†VoiceAssistant-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°AIåŠ©æ‰‹åœ¨å¬ã€è¯´ã€çœ‹æ–¹é¢çš„èƒ½åŠ›ã€‚VoiceAssistant-EvalåŒ…å«10,497ä¸ªç²¾é€‰æ ·æœ¬ï¼Œæ¶µç›–13ä¸ªä»»åŠ¡ç±»åˆ«ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ç”¨äºå¬åŠ›çš„è‡ªç„¶å£°éŸ³ã€éŸ³ä¹å’Œå£è¯­å¯¹è¯ï¼›ç”¨äºè¯´è¯çš„å¤šè½®å¯¹è¯ã€è§’è‰²æ‰®æ¼”å’Œå„ç§åœºæ™¯ï¼›ä»¥åŠç”¨äºè§‚çœ‹çš„é«˜åº¦å¤šæ ·åŒ–çš„å›¾åƒã€‚ä¸ºäº†è¯æ˜å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¯¹21ä¸ªå¼€æºæ¨¡å‹å’ŒGPT-4o-Audioè¿›è¡Œäº†è¯„ä¼°ï¼Œæµ‹é‡äº†å›å¤å†…å®¹å’Œè¯­éŸ³çš„è´¨é‡ï¼Œä»¥åŠå®ƒä»¬çš„ä¸€è‡´æ€§ã€‚ç»“æœæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼š(1)ä¸“æœ‰æ¨¡å‹å¹¶ä¸æ™®éä¼˜äºå¼€æºæ¨¡å‹ï¼›(2)å¤§å¤šæ•°æ¨¡å‹åœ¨è¯´è¯ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éŸ³é¢‘ç†è§£ä¸Šè½åï¼›(3)è®¾è®¡ç²¾è‰¯çš„å°å‹æ¨¡å‹å¯ä»¥ä¸å¤§å‹æ¨¡å‹ç›¸ç«äº‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸­å‹Step-Audio-2-miniï¼ˆ7Bï¼‰çš„å¬åŠ›å‡†ç¡®æ€§æ˜¯LLaMA-Omni2-32B-Bilingualçš„ä¸¤å€å¤šã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼šå¯¹äºå½“å‰æ¨¡å‹æ¥è¯´ï¼Œè·¨æ¨¡æ€ï¼ˆéŸ³é¢‘åŠ è§†è§‰ï¼‰è¾“å…¥å’Œè§’è‰²æ‰®æ¼”è¯­éŸ³æ¨¡ä»¿ä»»åŠ¡å¾ˆå›°éš¾ï¼Œè€Œä¸”åœ¨ç¨³å¥æ€§å’Œå®‰å…¨å¯¹é½æ–¹é¢ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚VoiceAssistant-Evalè¯†åˆ«äº†è¿™äº›å·®è·ï¼Œå¹¶ä¸ºè¯„ä¼°å’ŒæŒ‡å¯¼ä¸‹ä¸€ä»£AIåŠ©æ‰‹çš„å‘å±•å»ºç«‹äº†ä¸¥æ ¼æ¡†æ¶ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://mathllm.github.io/VoiceAssistantEval/%E4%B8%8A%E5%B8%A%E8%BE%83%E5%B0%91%E7%9A%84%E5%9C%BA%E6%99%AF%E4%B8%8B%E5%BC%80%E5%A7%8B%E5%9B%9E%E7%AD%94%E9%92%88%E5%AF%B9%E5%A4%9A%E5%9C%BA%E6%99%AF%E6%8F%90%E9%97%AE%E7%9A%84%E6%94%AF%E6%8C%81%E6%83%85%E5%86%B5%E5%92%8C%E5%AD%98%E5%9C%A8%E7%9A%84%E9%99%90%E5%88%B6%E5%81%9A%E4%BA%86%E8%AF%B4%E6%98%8E%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E6%96%87%E7%AB%A0%E8%BF%98%E8%AE%A8%E8%AE%BA%E4%BA%86%E8%BF%99%E4%BA%9B%E5%B7%A5%E5%85%B7%E5%9C%A8%E5%BA%94%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%8C%91%E6%88%98%E6%97%B6%E7%9A%84%E4%BC%98%E5%8A%BF%E5%92%8C%E4%B8%8D%E8%B6%B3%E3%80%82">https://mathllm.github.io/VoiceAssistantEval/ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22651v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åŠ©æ‰‹è¯„ä¼°åŸºå‡†ï¼ˆVoiceAssistant-Evalï¼‰çš„æå‡ºï¼Œè¯¥åŸºå‡†æ—¨åœ¨å…¨é¢è¯„ä¼°è¯­éŸ³åŠ©æ‰‹åœ¨å¬ã€è¯´ã€è§†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ¶µç›–å¤šç§ä»»åŠ¡ç±»åˆ«çš„10,497ä¸ªæ ·æœ¬ï¼Œå¯¹å¼€æºæ¨¡å‹å’ŒGPT-4o-Audioè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å…³é”®æŒ‘æˆ˜åœ¨äºå¤šæ¨¡æ€è¾“å…¥ã€è§’è‰²è¯­éŸ³æ¨¡ä»¿ä»»åŠ¡å¯¹äºå½“å‰æ¨¡å‹ä»ç„¶å…·æœ‰éš¾åº¦ï¼Œä¸”ç¨³å¥æ€§å’Œå®‰å…¨å¯¹é½ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚VoiceAssistant-Evalä¸ºè¯„ä¼°å’ŒæŒ‡å¯¼ä¸‹ä¸€ä»£è¯­éŸ³åŠ©æ‰‹çš„å‘å±•æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åŠ©æ‰‹è¯„ä¼°åŸºå‡†ï¼ˆVoiceAssistant-Evalï¼‰æ˜¯ä¸€ä¸ªç”¨äºå…¨é¢è¯„ä¼°è¯­éŸ³åŠ©æ‰‹èƒ½åŠ›çš„å·¥å…·ï¼Œæ¶µç›–å¬ã€è¯´ã€è§†ä¸‰ä¸ªæ–¹é¢ã€‚</li>
<li>ç°æœ‰è¯­éŸ³åŠ©æ‰‹åœ¨å¤šæ¨¡æ€ï¼ˆéŸ³é¢‘+è§†è§‰ï¼‰è¾“å…¥å’Œè§’è‰²æ¨¡ä»¿ä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼€æºæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹è¡¨ç°ä¸ä¸€ï¼ŒæŸäº›å¼€æºæ¨¡å‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¤§å¤šæ•°æ¨¡å‹æ“…é•¿è¯´è¯ä»»åŠ¡ï¼Œä½†åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>è®¾è®¡è‰¯å¥½çš„å°å‹æ¨¡å‹å¯ä»¥ä¸å¤§å‹æ¨¡å‹ç«äº‰ï¼Œå¦‚Step-Audio-2-miniï¼ˆ7Bï¼‰åœ¨å¬åŠ›å‡†ç¡®æ€§ä¸Šè¶…è¶ŠLLaMA-Omni2-32B-Bilingualã€‚</li>
<li>è¯­éŸ³åŠ©æ‰‹åœ¨ç¨³å¥æ€§å’Œå®‰å…¨å¯¹é½æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs"><a href="#Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs" class="headerlink" title="Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal   LLMs"></a>Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal   LLMs</h2><p><strong>Authors:Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch</strong></p>
<p>Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension â€“ whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated â€“ has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation. </p>
<blockquote>
<p>äººç±»èƒ½å¦è¯†åˆ«AIç”Ÿæˆçš„ï¼ˆè™šå‡ï¼‰è§†é¢‘å¹¶æä¾›æ˜ç¡®çš„åŸå› ï¼Ÿå°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†ä¸€ä¸ªå…³é”®æ–¹é¢â€”â€”äººç±»æ˜¯å¦èƒ½åœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­æ£€æµ‹æ·±è¿¹ï¼Œå³æ—¶ç©ºå®šä½çš„è§†è§‰ä¼ªè¿¹ï¼Œè¿™äº›ä¼ªè¿¹æ­ç¤ºäº†è§†é¢‘æ˜¯æœºå™¨ç”Ÿæˆçš„â€”â€”ä¸€ç›´è¢«å¿½è§†ã€‚æˆ‘ä»¬å¼•å…¥äº†DeeptraceRewardï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç²¾ç»†ç²’åº¦çš„ã€ç©ºé—´å’Œæ—¶é—´æ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸ºè§†é¢‘ç”Ÿæˆå¥–åŠ±æ ‡æ³¨äº†äººç±»æ„ŸçŸ¥çš„è™šå‡ç—•è¿¹ã€‚è¯¥æ•°æ®é›†åŒ…å«3300ä¸ªé«˜è´¨é‡ç”Ÿæˆè§†é¢‘çš„4300ä¸ªè¯¦ç»†æ ‡æ³¨ã€‚æ¯ä¸ªæ ‡æ³¨éƒ½æä¾›äº†è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œç¡®å®šäº†åŒ…å«æ„ŸçŸ¥è½¨è¿¹çš„è¾¹ç•Œæ¡†åŒºåŸŸï¼Œå¹¶æ ‡è®°äº†ç²¾ç¡®çš„å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³ã€‚æˆ‘ä»¬å°†è¿™äº›æ³¨é‡Šæ•´åˆä¸º9å¤§ç±»æ·±è¿¹ï¼Œè¿™äº›è¿¹è±¡ä¼šå¯¼è‡´äººç±»å°†è§†é¢‘è¯†åˆ«ä¸ºAIç”Ÿæˆçš„ï¼Œå¹¶è®­ç»ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ¨¡ä»¿äººç±»çš„åˆ¤æ–­å’Œå®šä½ã€‚åœ¨DeeptraceRewardä¸Šï¼Œæˆ‘ä»¬çš„7Bå¥–åŠ±æ¨¡å‹åœ¨è™šå‡çº¿ç´¢è¯†åˆ«ã€å®šä½å’Œè§£ç­‰æ–¹é¢å¹³å‡ä¼˜äºGPT-5è¾¾34.7%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªæŒç»­çš„éš¾åº¦æ¢¯åº¦ï¼šäºŒå…ƒè™šå‡ä¸ç°å®åˆ†ç±»æ¯”ç²¾ç»†ç²’åº¦çš„æ·±è¿¹æ£€æµ‹è¦å®¹æ˜“å¾—å¤šï¼›åœ¨åè€…ä¸­ï¼Œä»è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆæœ€å®¹æ˜“ï¼‰åˆ°ç©ºé—´å®šä½å†åˆ°æ—¶é—´æ ‡ç­¾ï¼ˆæœ€éš¾ï¼‰æ€§èƒ½é€æ¸ä¸‹é™ã€‚é€šè¿‡çªå‡ºäººç±»æ„ŸçŸ¥çš„æ·±è¿¹ï¼ŒDeeptraceRewardä¸ºå…·æœ‰ç¤¾ä¼šæ„è¯†å’Œå¯ä¿¡åº¦çš„è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸¥æ ¼çš„æµ‹è¯•å¹³å°å’Œè®­ç»ƒä¿¡å·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22646v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://deeptracereward.github.io/">https://deeptracereward.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DeeptraceRewardæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘ç”Ÿæˆå¥–åŠ±çš„ç²¾ç»†ç²’åº¦ã€æ—¶ç©ºæ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†äººç±»æ„ŸçŸ¥åˆ°çš„è§†é¢‘ä¸­çš„è™šå‡ç—•è¿¹ï¼Œæ¶µç›–å¹¿æ³›çš„é«˜è´¨é‡ç”Ÿæˆè§†é¢‘ï¼Œå¹¶å¯¹æ¯ä¸ªæ ‡æ³¨è¿›è¡Œäº†è¯¦ç»†çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€è¾¹ç•Œæ¡†å®šä½å’Œç²¾ç¡®çš„æ—¶é—´æˆ³æ ‡è®°ã€‚æ–‡ç« è¿˜ä»‹ç»äº†è®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ¨¡ä»¿äººç±»çš„åˆ¤æ–­å’Œå®šä½èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨DeeptraceRewardä¸Šçš„æ€§èƒ½ä¼˜äºGPT-5ã€‚æ­¤å¤–ï¼Œæ–‡ç« ä¹ŸæŒ‡å‡ºé‰´å®šéš¾åº¦ç­‰çº§å‘ˆç°ä¸€è‡´è¶‹åŠ¿ï¼šä¸çœŸå®è§†é¢‘çš„äºŒå…ƒé‰´å®šç›¸å¯¹ç®€å•ï¼Œè€Œåœ¨ç²¾ç»†ç²’åº¦çš„æ·±åº¦ä¼ªé€ ç—•è¿¹æ£€æµ‹ä¸­ï¼Œä»è‡ªç„¶è¯­è¨€è§£é‡Šåˆ°ç©ºé—´å®šä½å†åˆ°æ—¶é—´æ ‡ç­¾çš„éš¾åº¦é€æ¸ä¸Šå‡ã€‚æ•´ä½“è€Œè¨€ï¼ŒDeeptraceRewardæ•°æ®é›†ä¸ºç¤¾ä¼šæ„è¯†å¼ºçš„è§†é¢‘ç”Ÿæˆæä¾›ä¸¥æ ¼æµ‹è¯•å’Œè®­ç»ƒä¿¡å·ï¼Œæ˜¯æµ‹è¯•æ¨¡å‹ç¨³å®šæ€§å’Œå…¬æ­£æ€§çš„æœ‰æ•ˆå·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®é›†DeeptraceRewardï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹è§†é¢‘ç”Ÿæˆå¥–åŠ±çš„ç²¾ç»†ç²’åº¦è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äººç±»æ„ŸçŸ¥åˆ°çš„è§†é¢‘ä¸­çš„è™šå‡ç—•è¿¹æ ‡æ³¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯¦ç»†çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€è¾¹ç•Œæ¡†å®šä½å’Œç²¾ç¡®æ—¶é—´æˆ³æ ‡è®°ã€‚</li>
<li>è®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹å¯ä»¥æ¨¡ä»¿äººç±»çš„åˆ¤æ–­å’Œå®šä½èƒ½åŠ›ï¼Œæ€§èƒ½ä¼˜äºGPT-5ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†é‰´å®šéš¾åº¦ç­‰çº§çš„ä¸€è‡´è¶‹åŠ¿ï¼Œä»äºŒå…ƒé‰´å®šåˆ°ç²¾ç»†ç²’åº¦çš„æ·±åº¦ä¼ªé€ ç—•è¿¹æ£€æµ‹éš¾åº¦é€æ¸å¢åŠ ã€‚å…¶ä¸­è‡ªç„¶è¯­è¨€è§£é‡Šç›¸å¯¹å®¹æ˜“ï¼Œè€Œæ—¶ç©ºå®šä½æœ€éš¾ã€‚</li>
<li>DeeptraceRewardæ•°æ®é›†æ˜¯ç¤¾ä¼šæ„è¯†å¼ºçš„è§†é¢‘ç”Ÿæˆçš„æµ‹è¯•å¹³å°ï¼Œå¯¹äºè®­ç»ƒç¨³å¥ä¸”å…¬æ­£çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ã€‚æ•°æ®é›†èƒ½å¤Ÿæä¾›ç¤¾ä¼šè®¤å¯çš„åŸºå‡†æµ‹è¯•å’Œæ ‡å‡†ã€‚é€šè¿‡æµ‹è¯•æ¨¡å‹çš„å‡†ç¡®æ€§æ¥è¯„ä¼°å…¶å¯¹äººç±»æ„ŸçŸ¥çš„æ·±åº¦ä¼ªé€ ç—•è¿¹çš„è¯†åˆ«èƒ½åŠ›ã€‚è¿™å¯¹äºç¡®ä¿è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„å¯é æ€§å’Œå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚åŒæ—¶ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ä»¥ä¸ºè®¾è®¡æ›´ä¸ºå…¬æ­£çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œå‡å°‘æ·±åº¦ä¼ªé€ ç­‰æŠ€æœ¯åœ¨ç¤¾äº¤ç¯å¢ƒä¸­çš„åº”ç”¨å¸¦æ¥çš„æ½œåœ¨é£é™©å’Œé—®é¢˜ã€‚åŒæ—¶æ•°æ®é›†çš„å»ºç«‹è¿˜è¿›ä¸€æ­¥æ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„å‘å±•ï¼Œå¯¹äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½å’Œå‘å±•æ–¹å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ•°æ®é›†çš„è®¾è®¡æ€è·¯å’Œå®éªŒæ–¹æ³•ä¹Ÿä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’Œå¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning"><a href="#WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning" class="headerlink" title="WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning"></a>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning</h2><p><strong>Authors:Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the modelâ€™s website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ç³»ç»Ÿå·²åœ¨ä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºä¸¥é‡ä¾èµ–äºè§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ä»»åŠ¡ï¼ˆå¦‚ç½‘ç«™ä»£ç åº“ç”Ÿæˆï¼‰ï¼Œå½“å‰çš„ä»£ç ä»£ç†ä»…ä¾é ç®€å•çš„ä»£ç æ‰§è¡Œè¿›è¡Œåé¦ˆå’ŒéªŒè¯ã€‚è¿™ç§æ–¹æ³•æ— æ³•æ•æ‰ç”Ÿæˆä»£ç çš„å®é™…è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç«™ç”Ÿæˆä»£ç†â€”â€”WebGen-Agentï¼Œå®ƒåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆæ¥è¿­ä»£ç”Ÿæˆå’Œå®Œå–„ç½‘ç«™ä»£ç åº“ã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå…³äºç½‘ç«™æˆªå›¾å’ŒGUIä»£ç†æµ‹è¯•çš„è¯¦ç»†ä¸”æè¿°æ€§çš„æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼Œä»¥åŠé‡åŒ–å…¶è´¨é‡çš„åˆ†æ•°ã€‚æˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°è¿›ä¸€æ­¥ä¸å›æº¯å’Œé€‰æ‹©æœ€ä½³æœºåˆ¶ç›¸ç»“åˆï¼Œå¢å¼ºäº†ä»£ç†çš„æ€§èƒ½ã€‚åˆ©ç”¨WebGen-Agentå·¥ä½œæµç¨‹ä¸­å›ºæœ‰çš„ç²¾ç¡®è§†è§‰åˆ†æ•°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†\textit{å¸¦æœ‰æˆªå›¾å’ŒGUIä»£ç†åé¦ˆçš„Step-GRPO}ï¼Œä»¥æé«˜LLMä½œä¸ºWebGen-Agentæ¨ç†å¼•æ“çš„èƒ½åŠ›ã€‚å°†æ¯ä¸€æ­¥çš„æˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°ä½œä¸ºStep-GRPOä¸­çš„å¥–åŠ±ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯†é›†ä¸”å¯é çš„è¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œè¿™æœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹ç”Ÿæˆç½‘ç«™çš„èƒ½åŠ›ã€‚åœ¨WebGen-Benchæ•°æ®é›†ä¸Šï¼ŒWebGen-Agentå°†Claude-3.5-Sonnetçš„å‡†ç¡®æ€§ä»26.4%æé«˜åˆ°51.9%ï¼Œå¤–è§‚è¯„åˆ†ä»3.0æé«˜åˆ°3.9ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›ä»£ç†ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„Step-GRPOè®­ç»ƒæ–¹æ³•å°†Qwen2.5-Coder-7B-Instructçš„å‡†ç¡®æ€§ä»38.9%æé«˜åˆ°45.4%ï¼Œå¤–è§‚è¯„åˆ†ä»3.4æé«˜åˆ°3.7ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22644v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„Agentç³»ç»Ÿå·²ç»åœ¨ä»“åº“çº§çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚ä½†å¯¹äºä¾èµ–è§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ç½‘ç«™ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå½“å‰ä»£ç Agentä»…ä¾èµ–ç®€å•çš„ä»£ç æ‰§è¡Œè¿›è¡Œåé¦ˆå’ŒéªŒè¯ï¼Œæ— æ³•æ•æ‰ç”Ÿæˆä»£ç çš„å®é™…è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†WebGen-Agentï¼Œä¸€ä¸ªåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆæ¥è¿­ä»£ç”Ÿæˆå’Œç²¾ç‚¼ç½‘ç«™ä»£ç åº“çš„æ–°å‹ç½‘ç«™ç”ŸæˆAgentã€‚ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼Œä»¥åŠä¸æˆªå›¾å’ŒGUI-agentæµ‹è¯•ç›¸å…³çš„å¾—åˆ†ï¼ŒWebGen-Agentæé«˜äº†æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†ç»“åˆæˆªå›¾å’ŒGUI-agentåé¦ˆçš„Step-GRPOæ–¹æ³•ï¼Œæé«˜äº†LLMä½œä¸ºWebGen-Agentæ¨ç†å¼•æ“çš„èƒ½åŠ›ã€‚åœ¨WebGen-Benchæ•°æ®é›†ä¸Šï¼ŒWebGen-Agentçš„å¼•å…¥ä½¿Claude-3.5-Sonnetçš„å‡†ç¡®ç‡ä»26.4%æé«˜åˆ°51.9%ï¼Œå¤–è§‚å¾—åˆ†ä»3.0æé«˜åˆ°3.9ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„Agentç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒStep-GRPOè®­ç»ƒæ–¹æ³•çš„é‡‡ç”¨ä¹Ÿæé«˜äº†Qwen2.5-Coder-7B-Instructçš„å‡†ç¡®ç‡å’Œå¤–è§‚å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„Agentç³»ç»Ÿåœ¨ä»“åº“çº§ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¾èµ–è§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ç½‘ç«™ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>WebGen-Agentåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆæ¥è¿­ä»£ç”Ÿæˆå’Œç²¾ç‚¼ç½‘ç«™ä»£ç åº“ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>WebGen-Agentç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼Œä»¥åŠä¸æˆªå›¾å’ŒGUI-agentæµ‹è¯•ç›¸å…³çš„å¾—åˆ†ã€‚</li>
<li>Step-GRPOæ–¹æ³•ç»“åˆæˆªå›¾å’ŒGUI-agentåé¦ˆï¼Œæé«˜äº†LLMä½œä¸ºWebGen-Agentæ¨ç†å¼•æ“çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨WebGen-Benchæ•°æ®é›†ä¸Šï¼ŒWebGen-Agentæ˜¾è‘—æé«˜äº†ç°æœ‰Agentç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>WebGen-Agentçš„å¼•å…¥æ˜¾è‘—æé«˜äº†ç½‘ç«™ç”Ÿæˆçš„å‡†ç¡®ç‡å’Œå¤–è§‚å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning"><a href="#UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning" class="headerlink" title="UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning"></a>UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning</h2><p><strong>Authors:Hongyu Chen, Guangrun Wang</strong></p>
<p>Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨å®ä½“ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§å’Œå¯æ‰§è¡Œæ€§ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†ä½¿ç”¨åœºæ™¯æˆ–é€»è¾‘å›¾çš„ç»“æ„åŒ–CoTï¼Œä½†è¿™äº›æ–¹æ³•ä»å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ï¼šå®ƒä»¬åªæ¨¡æ‹Ÿä½é˜¶å…³ç³»ï¼Œç¼ºä¹ç»§æ‰¿æˆ–è¡Œä¸ºæŠ½è±¡ç­‰ç»“æ„ï¼Œå¹¶ä¸”ä¸æä¾›ç”¨äºé¡ºåºæˆ–æ¡ä»¶è§„åˆ’çš„æ ‡å‡†åŒ–è¯­ä¹‰ã€‚æˆ‘ä»¬æå‡ºUML-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„åŒ–æ¨ç†å’Œè§„åˆ’æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰ç”Ÿæˆç¬¦å·åŒ–CoTå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚UMLç±»å›¾æ•è·ç»„åˆå¯¹è±¡è¯­ä¹‰ï¼Œè€Œæ´»åŠ¨å›¾æ¨¡æ‹Ÿè¿‡ç¨‹æ§åˆ¶æµã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“å°†ç›‘ç£å¾®è°ƒä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆï¼ŒåŒ…æ‹¬ä»…ä»ç­”æ¡ˆæ•°æ®ä¸­å­¦ä¹ å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨æ–°çš„æ‚ä¹±æˆ¿é—´æ¸…æ´åœºæ™¯åŸºå‡†æµ‹è¯•MRoom-3 ç»“ä¸­è¿›è¡Œè¯„ä¼°ã€‚UML-CoTåœ¨å¯è§£é‡Šæ€§ã€è§„åˆ’è¿è´¯æ€§å’Œæ‰§è¡ŒæˆåŠŸæ–¹é¢ä¼˜äºéç»“æ„åŒ–CoTï¼Œçªæ˜¾äº†UMLä½œä¸ºæ›´å…·è¡¨ç°åŠ›å’Œå¯æ“ä½œæ€§çš„ç»“æ„åŒ–æ¨ç†å½¢å¼ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ”¹å–„äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬çš„ç‰¹æ€§é™åˆ¶äº†å…¶åœ¨å®ä½“ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§å’Œå¯æ‰§è¡Œæ€§ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰æ„å»ºç»“æ„åŒ–CoTå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’çš„UML-CoTæ¡†æ¶ã€‚é€šè¿‡UMLç±»å›¾æ•æ‰ç»„æˆå¯¹è±¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œæ´»åŠ¨å›¾åˆ™æ¨¡æ‹Ÿç¨‹åºæ§åˆ¶æµã€‚ç»“åˆç›‘ç£å¾®è°ƒä¸ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡ä»…ç­”æ¡ˆæ•°æ®è¿›è¡Œå¥–åŠ±å­¦ä¹ ã€‚åœ¨æ–°çš„æ··ä¹±æˆ¿é—´æ¸…æ´åœºæ™¯åŸºå‡†æµ‹è¯•MRoom-30kä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUML-CoTåœ¨å¯è§£é‡Šæ€§ã€è§„åˆ’è¿è´¯æ€§å’Œæ‰§è¡ŒæˆåŠŸç‡æ–¹é¢ä¼˜äºéç»“æ„åŒ–CoTï¼Œå‡¸æ˜¾å‡ºUMLä½œä¸ºä¸€ç§æ›´å…·è¡¨ç°åŠ›å’Œå¯æ“ä½œæ€§çš„ç»“æ„åŒ–æ¨ç†å½¢å¼çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†é¢ä¸´å¯è§£é‡Šæ€§å’Œæ‰§è¡Œæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç»“æ„åŒ–çš„CoTæ–¹æ³•å¦‚åœºæ™¯æˆ–é€»è¾‘å›¾å­˜åœ¨å±€é™æ€§ï¼Œä»…å»ºæ¨¡ä½é˜¶å…³ç³»ï¼Œç¼ºä¹å¦‚ç»§æ‰¿å’Œè¡Œä¸ºæŠ½è±¡ç­‰æ„é€ ï¼Œä»¥åŠä¸ºé¡ºåºæˆ–æ¡ä»¶è§„åˆ’æä¾›æ ‡å‡†åŒ–è¯­ä¹‰çš„ç¼ºå¤±ã€‚</li>
<li>æå‡ºäº†UML-CoTæ¡†æ¶ï¼Œåˆ©ç”¨ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰ç”Ÿæˆç¬¦å·CoTå’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ã€‚</li>
<li>UMLç±»å›¾ç”¨äºæ•æ‰ç»„æˆå¯¹è±¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œæ´»åŠ¨å›¾åˆ™æ¨¡æ‹Ÿç¨‹åºæ§åˆ¶æµã€‚</li>
<li>ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“åŒ…æ‹¬ç›‘ç£å¾®è°ƒä¸ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚</li>
<li>å¥–åŠ±å­¦ä¹ æ˜¯é€šè¿‡ä»…ç­”æ¡ˆæ•°æ®è¿›è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ArabJobs-A-Multinational-Corpus-of-Arabic-Job-Ads"><a href="#ArabJobs-A-Multinational-Corpus-of-Arabic-Job-Ads" class="headerlink" title="ArabJobs: A Multinational Corpus of Arabic Job Ads"></a>ArabJobs: A Multinational Corpus of Arabic Job Ads</h2><p><strong>Authors:Mo El-Haj</strong></p>
<p>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/drelhaj/ArabJobs">https://github.com/drelhaj/ArabJobs</a>. </p>
<blockquote>
<p>ArabJobsæ˜¯ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„é˜¿æ‹‰ä¼¯è¯­æ‹›è˜å¹¿å‘Šè¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æ”¶é›†äº†æ¥è‡ªåŸƒåŠã€çº¦æ—¦ã€æ²™ç‰¹é˜¿æ‹‰ä¼¯å’Œé˜¿æ‹‰ä¼¯è”åˆé…‹é•¿å›½çš„æ‹›è˜å¹¿å‘Šã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡8500ä¸ªå¸–å­å’Œè¶…è¿‡55ä¸‡ä¸ªå•è¯ï¼Œåæ˜ äº†é˜¿æ‹‰ä¼¯åŠ³åŠ¨åŠ›å¸‚åœºçš„è¯­è¨€ã€åœ°åŸŸå’Œç¤¾ä¼šç»æµå·®å¼‚ã€‚æˆ‘ä»¬å¯¹æ€§åˆ«ä»£è¡¨å’ŒèŒä¸šç»“æ„è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¼ºè°ƒäº†å¹¿å‘Šä¸­çš„æ–¹è¨€å·®å¼‚ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœºä¼šã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå·¥èµ„ä¼°ç®—å’ŒèŒä¸šç±»åˆ«å½’ä¸€åŒ–çš„åº”ç”¨ç¨‹åºï¼Œä»¥åŠç”¨äºæ€§åˆ«åè§æ£€æµ‹å’ŒèŒä¸šåˆ†ç±»çš„åŸºå‡†ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒArabJobså¯¹äºæ³¨é‡å…¬å¹³æ€§çš„é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†å’ŒåŠ³åŠ¨åŠ›å¸‚åœºç ”ç©¶å…·æœ‰å®ç”¨ä»·å€¼ã€‚æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/drelhaj/ArabJobs%E3%80%82">https://github.com/drelhaj/ArabJobsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯å°±ä¸šè¯­æ–™åº“ï¼ˆArabJobsï¼‰æ˜¯ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„é˜¿æ‹‰ä¼¯è¯­æ‹›è˜å¹¿å‘Šé›†åˆï¼Œæ¶µç›–äº†åŸƒåŠã€çº¦æ—¦ã€æ²™ç‰¹é˜¿æ‹‰ä¼¯å’Œé˜¿è”é…‹çš„æ•°æ®ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡8,500æ¡æ‹›è˜å¹¿å‘Šå’Œè¶…è¿‡55ä¸‡ä¸ªå•è¯ï¼Œå±•ç°äº†é˜¿æ‹‰ä¼¯åŠ³åŠ¨åŠ›å¸‚åœºçš„è¯­è¨€ã€åœ°åŸŸå’Œç¤¾ä¼šç»æµå·®å¼‚ã€‚é€šè¿‡å¯¹æ€§åˆ«ä»£è¡¨å’ŒèŒä¸šç»“æ„çš„åˆ†æï¼Œä»¥åŠå¹¿å‘Šä¸­æ–¹è¨€å·®å¼‚çš„å¼ºè°ƒï¼Œè¯¥æ•°æ®é›†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœºä¼šã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†ç”¨äºè–ªé…¬ä¼°ç®—å’ŒèŒä¸šç±»åˆ«å½’ä¸€åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œä»¥åŠç”¨äºæ€§åˆ«åè§æ£€æµ‹å’ŒèŒä¸šåˆ†ç±»çš„åŸºå‡†ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé˜¿æ‹‰ä¼¯å°±ä¸šè¯­æ–™åº“å¯¹å…¬å¹³æ„ŸçŸ¥çš„é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†å’ŒåŠ³åŠ¨åŠ›å¸‚åœºç ”ç©¶å…·æœ‰å®ç”¨æ€§ã€‚æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯å°±ä¸šè¯­æ–™åº“ï¼ˆArabJobsï¼‰æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡8,500æ¡æ‹›è˜å¹¿å‘Šçš„å…¬å¼€æ•°æ®é›†ï¼Œæ¶‰åŠåŸƒåŠã€çº¦æ—¦ã€æ²™ç‰¹é˜¿æ‹‰ä¼¯å’Œé˜¿è”é…‹çš„é˜¿æ‹‰ä¼¯è¯­æ‹›è˜å¹¿å‘Šã€‚</li>
<li>æ•°æ®é›†å±•ç¤ºäº†é˜¿æ‹‰ä¼¯åŠ³åŠ¨åŠ›å¸‚åœºçš„è¯­è¨€ã€åœ°åŸŸå’Œç¤¾ä¼šç»æµå·®å¼‚ã€‚</li>
<li>åˆ†æäº†æ•°æ®é›†ä¸­çš„æ€§åˆ«ä»£è¡¨å’ŒèŒä¸šç»“æ„ï¼Œå¼ºè°ƒäº†å¹¿å‘Šä¸­çš„æ–¹è¨€å·®å¼‚ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœºä¼šã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è–ªé…¬ä¼°ç®—å’ŒèŒä¸šç±»åˆ«å½’ä¸€åŒ–çš„åº”ç”¨å±•ç¤ºã€‚</li>
<li>æä¾›äº†ç”¨äºæ€§åˆ«åè§æ£€æµ‹å’ŒèŒä¸šåˆ†ç±»çš„åŸºå‡†ä»»åŠ¡ã€‚</li>
<li>é˜¿æ‹‰ä¼¯å°±ä¸šè¯­æ–™åº“å¯¹å…¬å¹³æ„ŸçŸ¥çš„é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†å’ŒåŠ³åŠ¨åŠ›å¸‚åœºç ”ç©¶å…·æœ‰å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Detection-of-Context-Grounded-Hallucinations-Using-LLMs"><a href="#Fine-Grained-Detection-of-Context-Grounded-Hallucinations-Using-LLMs" class="headerlink" title="Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs"></a>Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs</h2><p><strong>Authors:Yehonatan Pesiakhovsky, Zorik Gekhman, Yosi Mass, Liat Ein-Dor, Roi Reichart</strong></p>
<p>Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmarkâ€™s difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the modelâ€™s parametric knowledge. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡ç›¸å…³çš„å¹»è§‰æ˜¯æ¨¡å‹è¾“å‡ºä¸­åŒ…å«æ— æ³•æ ¹æ®æºæ–‡æœ¬è¿›è¡ŒéªŒè¯çš„ä¿¡æ¯çš„æƒ…å†µã€‚æˆ‘ä»¬ç ”ç©¶LLMåœ¨æ­¤ç±»å¹»è§‰å®šä½æ–¹é¢çš„é€‚ç”¨æ€§ï¼Œä½œä¸ºç°æœ‰å¤æ‚è¯„ä¼°æµç¨‹çš„æ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨ç¼ºä¹ç”¨äºå…ƒè¯„ä¼°å¹»è§‰å®šä½çš„åŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹LLMçš„åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬å¯¹1000å¤šä¸ªç¤ºä¾‹å…·æœ‰æŒ‘æˆ˜æ€§çš„äººå·¥æ ‡æ³¨ã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ¶å®šäº†åŸºäºLLMçš„è¯„ä¼°åè®®ï¼Œå¹¶é€šè¿‡äººå·¥è¯„ä¼°éªŒè¯äº†å…¶è´¨é‡ã€‚ç”±äºç°æœ‰çš„å¹»è§‰è¡¨ç¤ºé™åˆ¶äº†å¯ä»¥è¡¨è¾¾çš„é”™è¯¯ç±»å‹ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç”±å½¢å¼æ–‡æœ¬æè¿°çš„æ–°è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥æ•è·æ‰€æœ‰å¯èƒ½çš„é”™è¯¯èŒƒå›´ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œè¯„ä¼°äº†å››ä¸ªå¤§è§„æ¨¡LLMï¼Œçªå‡ºäº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ï¼Œå› ä¸ºæœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°ä»…ä¸º0.67ã€‚é€šè¿‡è®¤çœŸåˆ†æï¼Œæˆ‘ä»¬æä¾›äº†å®Œæˆæ­¤ä»»åŠ¡çš„æœ€ä½³æç¤ºç­–ç•¥ï¼Œå¹¶ç¡®å®šäº†ä½¿LLMé¢ä¸´æŒ‘æˆ˜çš„ä¸»è¦å› ç´ ï¼šï¼ˆ1ï¼‰å°½ç®¡å·²æŒ‡ç¤ºä»…æ£€æŸ¥è¾“å‡ºä¸­çš„äº‹å®ï¼Œä½†ä»é”™è¯¯åœ°å°†ç¼ºå¤±çš„ç»†èŠ‚æ ‡è®°ä¸ºä¸ä¸€è‡´ï¼›ï¼ˆ2ï¼‰è¾“å‡ºä¸­åŒ…å«äº‹å®æ­£ç¡®çš„ä¿¡æ¯ä½†ç”±äºä¸æ¨¡å‹çš„å‚æ•°çŸ¥è¯†å¯¹é½è€Œç¼ºå¤±æºæ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„è¾“å‡ºä¸­å­˜åœ¨æ— æ³•éªŒè¯æºæ–‡æœ¬çš„ä¿¡æ¯ï¼Œå³ä¸Šä¸‹æ–‡ç›¸å…³çš„å¹»è§‰ã€‚ç ”ç©¶LLMåœ¨å®šä½æ­¤ç±»å¹»è§‰æ–¹é¢çš„é€‚ç”¨æ€§ï¼Œä½œä¸ºç°æœ‰å¤æ‚è¯„ä¼°ç®¡é“çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨æ²¡æœ‰ä¸ºå¹»è§‰å®šä½è¿›è¡Œå…ƒè¯„ä»·çš„åŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹LLMçš„åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠè¶…è¿‡1000ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¾‹å­çš„äººç±»æ³¨é‡Šã€‚è¯¥åŸºå‡†æµ‹è¯•è¿˜åŒ…å«åŸºäºLLMçš„è¯„ä¼°åè®®ï¼Œå¹¶é€šè¿‡äººç±»è¯„ä¼°éªŒè¯å…¶è´¨é‡ã€‚ç”±äºç°æœ‰çš„å¹»è§‰è¡¨ç¤ºé™åˆ¶äº†å¯è¡¨è¾¾é”™è¯¯ç±»å‹ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªç”±å½¢å¼æ–‡æœ¬æè¿°çš„æ–°è¡¨ç¤ºå½¢å¼ï¼Œä»¥æ•æ‰å¯èƒ½å‡ºç°çš„æ‰€æœ‰é”™è¯¯ã€‚æˆ‘ä»¬å¯¹å››é¡¹å¤§è§„æ¨¡LLMè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œçªå‡ºè¯¥åŸºå‡†æµ‹è¯•çš„å›°éš¾æ€§ï¼Œå› ä¸ºæœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°ä»…ä¸º0.67ã€‚é€šè¿‡ä»”ç»†åˆ†æï¼Œæˆ‘ä»¬æä¾›äº†å…³äºä»»åŠ¡çš„æœ€ä¼˜æç¤ºç­–ç•¥ï¼Œå¹¶ç¡®å®šäº†ä½¿LLMé¢ä¸´æŒ‘æˆ˜çš„ä¸»è¦å› ç´ ï¼šï¼ˆ1ï¼‰é”™è¯¯åœ°å°†ç¼ºå¤±çš„ç»†èŠ‚æ ‡è®°ä¸ºä¸ä¸€è‡´çš„å€¾å‘ï¼Œå°½ç®¡æŒ‡ä»¤è¦æ±‚ä»…æ£€æŸ¥è¾“å‡ºä¸­çš„äº‹å®ï¼›ï¼ˆ2ï¼‰å¯¹äºå› å‚æ•°çŸ¥è¯†ä¸æºæ–‡æœ¬ä¸ç¬¦è€Œå¯¼è‡´çš„è¾“å‡ºä¸­åŒ…å«çš„äº‹å®ä¸Šæ­£ç¡®çš„ä¿¡æ¯â€”â€”å› æ­¤æ— æ³•éªŒè¯â€”â€”çš„å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¼šäº§ç”Ÿä¸Šä¸‹æ–‡ç›¸å…³çš„å¹»è§‰ï¼Œå³æ¨¡å‹è¾“å‡ºä¸­åŒ…å«æ— æ³•éªŒè¯æºæ–‡æœ¬çš„ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶äº†LLMsåœ¨å®šä½è¿™ç±»å¹»è§‰æ–¹é¢çš„é€‚ç”¨æ€§ï¼Œå¹¶ä½œä¸ºä¸€ç§å®ç”¨çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªé’ˆå¯¹LLMsçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡1000ä¸ªä¾‹å­çš„äººç±»æ³¨é‡Šï¼Œä»¥è¯„ä¼°å¹»è§‰å®šä½ã€‚</li>
<li>æå‡ºäº†åŸºäºè‡ªç”±å½¢å¼æ–‡æœ¬æè¿°çš„æ–°è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æ›´å…¨é¢åœ°æ•æ‰é”™è¯¯ç±»å‹ã€‚</li>
<li>æœ€ä½³LLMæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„F1åˆ†æ•°ä¸º0.67ï¼Œè¡¨æ˜ä»»åŠ¡çš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>LLMsåœ¨å®šä½å¹»è§‰æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼šé”™è¯¯åœ°å°†ç¼ºå¤±çš„ç»†èŠ‚æ ‡è®°ä¸ºä¸ä¸€è‡´ï¼Œä»¥åŠå¤„ç†å› æ¨¡å‹å‚æ•°çŸ¥è¯†ä¸æºæ–‡æœ¬ä¸ç¬¦è€Œå¯¼è‡´çš„ä¸å¯éªŒè¯çš„è¾“å‡ºä¿¡æ¯ã€‚</li>
<li>é€šè¿‡åˆ†æï¼Œæä¾›äº†é’ˆå¯¹è¯¥ä»»åŠ¡çš„æœ€ä¼˜æç¤ºç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time"><a href="#Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time" class="headerlink" title="Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time"></a>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time</h2><p><strong>Authors:Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</strong></p>
<p>Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„é¢å¤–è®¡ç®—ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹æ¶æ„çš„ä½œç”¨ã€‚åœ¨ä¸»æµçš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰LLMä¸­ï¼Œæˆ‘ä»¬å‘ç°æ”¹å˜æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¼šäº§ç”Ÿå…·æœ‰ç¨³å®šå‡†ç¡®æ€§çš„äº’è¡¥è§£å†³æ–¹æ¡ˆé›†ï¼Œæ­ç¤ºäº†ä¸€ç§æ–°çš„ä¸”å°šæœªè¢«å……åˆ†æ¢ç´¢çš„å¤šæ ·æ€§æ¥æºã€‚å—è¿™ä¸€è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ä¸“å®¶æ¿€æ´»æå‡ä¸ºæœç´¢ç©ºé—´çš„å¯æ§ç»´åº¦çš„TTSç­–ç•¥ã€‚DESé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨æ€MoEï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æˆæœ¬ï¼›ï¼ˆ2ï¼‰ä¸“å®¶é…ç½®ç»§æ‰¿ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨ç†è·¯å¾„å†…ä¿æŒä¸€è‡´çš„ä¸“å®¶æ•°é‡ï¼ŒåŒæ—¶åœ¨å„è¿è¡Œä¹‹é—´å˜åŒ–ä¸“å®¶æ•°é‡ï¼Œä»è€Œåœ¨æœç´¢è¿‡ç¨‹ä¸­å¹³è¡¡ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚åœ¨MoEæ¶æ„ã€éªŒè¯å™¨å’Œæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå³æ•°å­¦ã€ä»£ç å’ŒçŸ¥è¯†ï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDESå¯é åœ°ä¼˜äºTTSåŸºçº¿ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–æˆæœ¬ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†DESä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ¶æ„æ„ŸçŸ¥TTSå½¢å¼ï¼Œå±•ç¤ºäº†ç°ä»£LLMä¸­çš„ç»“æ„çµæ´»æ€§å¦‚ä½•æ¨åŠ¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22572v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„é¢å¤–è®¡ç®—æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½è§†äº†æ¨¡å‹æ¶æ„çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ç­–ç•¥ï¼Œå°†ä¸“å®¶æ¿€æ´»ä½œä¸ºæœç´¢ç©ºé—´çš„å¯æ§ç»´åº¦ã€‚DESåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€MoEå’Œä¸“å®¶é…ç½®ç»§æ‰¿ã€‚åŠ¨æ€MoEèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æˆæœ¬ï¼›ä¸“å®¶é…ç½®ç»§æ‰¿åˆ™èƒ½åœ¨æ¨ç†è·¯å¾„ä¸­ä¿æŒä¸€è‡´çš„ä¸“å®¶æ•°é‡ï¼ŒåŒæ—¶åœ¨å„è¿è¡Œä¸­å˜åŒ–ä¸“å®¶æ•°é‡ï¼Œä»è€Œå¹³è¡¡ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDESåœ¨MoEæ¶æ„ã€éªŒè¯å™¨å’Œæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚æ•°å­¦ã€ä»£ç å’ŒçŸ¥è¯†ï¼‰ä¸Šå¯é åœ°ä¼˜äºTTSåŸºçº¿ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰TTSæ–¹æ³•ä¸»è¦å…³æ³¨è¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½è§†äº†æ¨¡å‹æ¶æ„çš„é‡è¦æ€§ã€‚</li>
<li>åŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ç­–ç•¥æå‡ºå°†ä¸“å®¶æ¿€æ´»ä½œä¸ºæœç´¢ç©ºé—´çš„å¯æ§ç»´åº¦ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DESåŒ…æ‹¬åŠ¨æ€MoEå’Œä¸“å®¶é…ç½®ç»§æ‰¿ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>åŠ¨æ€MoEèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æˆæœ¬ã€‚</li>
<li>ä¸“å®¶é…ç½®ç»§æ‰¿èƒ½å¤Ÿå¹³è¡¡æ¨ç†è·¯å¾„çš„ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-Augmented-and-Fair-Machine-Learning-Framework-for-University-Admission-Prediction"><a href="#LLM-Augmented-and-Fair-Machine-Learning-Framework-for-University-Admission-Prediction" class="headerlink" title="LLM-Augmented and Fair Machine Learning Framework for University   Admission Prediction"></a>LLM-Augmented and Fair Machine Learning Framework for University   Admission Prediction</h2><p><strong>Authors:Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Dahlia Mansoor, Wathiq Mansoor</strong></p>
<p>Universities face surging applications and heightened expectations for fairness, making accurate admission prediction increasingly vital. This work presents a comprehensive framework that fuses machine learning, deep learning, and large language model techniques to combine structured academic and demographic variables with unstructured text signals. Drawing on more than 2,000 student records, the study benchmarks logistic regression, Naive Bayes, random forests, deep neural networks, and a stacked ensemble. Logistic regression offers a strong, interpretable baseline at 89.5% accuracy, while the stacked ensemble achieves the best performance at 91.0%, with Naive Bayes and random forests close behind. To probe text integration, GPT-4-simulated evaluations of personal statements are added as features, yielding modest gains but demonstrating feasibility for authentic essays and recommendation letters. Transparency is ensured through feature-importance visualizations and fairness audits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11% gap by parental education, underscoring the need for continued monitoring. The framework is interpretable, fairness-aware, and deployable. </p>
<blockquote>
<p>å¤§å­¦é¢ä¸´ç€ç”³è¯·äººæ•°æ¿€å¢å’Œå¯¹å…¬å¹³æ€§è¶Šæ¥è¶Šé«˜çš„æœŸæœ›ï¼Œå› æ­¤å‡†ç¡®çš„æ‹›ç”Ÿé¢„æµ‹å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œèåˆäº†æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œå°†ç»“æ„åŒ–çš„å­¦æœ¯å’Œäººå£ç»Ÿè®¡å˜é‡ä¸éç»“æ„åŒ–çš„æ–‡æœ¬ä¿¡å·ç›¸ç»“åˆã€‚è¯¥ç ”ç©¶ä»¥è¶…è¿‡2000ä»½å­¦ç”Ÿè®°å½•ä¸ºåŸºå‡†ï¼Œå¯¹æ¯”äº†é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ã€éšæœºæ£®æ—ã€æ·±åº¦ç¥ç»ç½‘ç»œå’Œå †å é›†æˆç­‰æ–¹æ³•ã€‚é€»è¾‘å›å½’æä¾›äº†å¼ºå¤§ä¸”å¯è§£é‡Šçš„åŸºçº¿ï¼Œå‡†ç¡®ç‡ä¸º89.5%ï¼Œè€Œå †å é›†æˆå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º91.0%ï¼Œæœ´ç´ è´å¶æ–¯å’Œéšæœºæ£®æ—ç´§éšå…¶åã€‚ä¸ºäº†æ¢ç©¶æ–‡æœ¬é›†æˆï¼Œå¢åŠ äº†GPT-4æ¨¡æ‹Ÿçš„ä¸ªäººé™ˆè¿°è¯„ä¼°ç‰¹å¾ï¼Œè™½ç„¶åªå¸¦æ¥äº†é€‚åº¦çš„æå‡ï¼Œä½†è¯æ˜äº†åœ¨çœŸå®ä½œæ–‡å’Œæ¨èä¿¡ä¸­çš„å¯è¡Œæ€§ã€‚é€šè¿‡ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å’Œå…¬å¹³æ€§å®¡è®¡æ¥ç¡®ä¿é€æ˜åº¦ã€‚å®¡è®¡ç»“æœæ˜¾ç¤ºå­˜åœ¨9%çš„æ€§åˆ«å·®è·ï¼ˆç”·æ€§67%å¯¹å¥³æ€§76%ï¼‰å’Œ11%çš„çˆ¶ç³»æ•™è‚²å·®è·ï¼Œè¿™çªæ˜¾äº†æŒç»­ç›‘æµ‹çš„å¿…è¦æ€§ã€‚è¯¥æ¡†æ¶å…·æœ‰å¯è§£é‡Šæ€§ã€å…¬å¹³æ„è¯†å’Œå¯éƒ¨ç½²æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22560v1">PDF</a> The 9th International Symposium on Multidisciplinary Studies and   Innovative Technologies (ISMSIT)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œèåˆæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œç»“åˆç»“æ„åŒ–å­¦æœ¯å’Œäººå£ç»Ÿè®¡å˜é‡ä¸æ— ç»“æ„åŒ–æ–‡æœ¬ä¿¡å·ï¼Œä»¥è¿›è¡Œå‡†ç¡®çš„å¤§å­¦å…¥å­¦é¢„æµ‹ã€‚ç ”ç©¶ä½¿ç”¨è¶…è¿‡2000åå­¦ç”Ÿè®°å½•ï¼Œå¯¹æ¯”äº†é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ã€éšæœºæ£®æ—ã€æ·±åº¦ç¥ç»ç½‘ç»œå’Œå †å é›†æˆç­‰æ–¹æ³•ã€‚é€»è¾‘å›å½’æä¾›89.5%å‡†ç¡®ç‡çš„å¼ºå¤§å¯è§£é‡ŠåŸºçº¿ï¼Œè€Œå †å é›†æˆå–å¾—æœ€ä½³æ€§èƒ½ä¸º91.0%ï¼Œæœ´ç´ è´å¶æ–¯å’Œéšæœºæ£®æ—è¡¨ç°æ¬¡ä¹‹ã€‚åŒæ—¶åˆ©ç”¨GPT-4æ¨¡æ‹Ÿä¸ªäººé™ˆè¿°è¯„ä¼°ä»¥å¢å¼ºæ–‡æœ¬é›†æˆèƒ½åŠ›ã€‚é€æ˜åº¦é€šè¿‡ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å’Œå…¬å¹³æ€§å®¡è®¡å¾—åˆ°ä¿éšœã€‚å®¡è®¡æ˜¾ç¤ºå­˜åœ¨æ€§åˆ«å·®è·å’Œçˆ¶æ¯æ•™è‚²å·®è·ï¼Œçªæ˜¾æŒç»­ç›‘æµ‹çš„å¿…è¦æ€§ã€‚æ­¤æ¡†æ¶å…·æœ‰å¯è§£é‡Šæ€§ã€å…¬å¹³æ€§å’Œå¯éƒ¨ç½²æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ¡†æ¶ç»“åˆäº†æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œç”¨äºæé«˜å¤§å­¦å…¥å­¦é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è¶…è¿‡2000åå­¦ç”Ÿè®°å½•è¿›è¡Œå®è¯ç ”ç©¶ï¼Œå¯¹æ¯”äº†å¤šç§é¢„æµ‹æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>é€»è¾‘å›å½’æä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„åŸºçº¿ï¼Œè€Œå †å é›†æˆæ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨GPT-4æ¨¡æ‹Ÿè¯„ä¼°æ–‡æœ¬é›†æˆèƒ½åŠ›ï¼Œå¢å¼ºäº†é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å…·æœ‰é€æ˜åº¦ï¼Œé€šè¿‡ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å±•ç¤ºã€‚</li>
<li>å…¬å¹³æ€§å®¡è®¡æ˜¾ç¤ºå­˜åœ¨æ€§åˆ«å’Œçˆ¶æ¯æ•™è‚²çš„å·®è·ï¼Œå¼ºè°ƒæŒç»­ç›‘æµ‹çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models"><a href="#StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models" class="headerlink" title="StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models"></a>StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models</h2><p><strong>Authors:Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge</strong></p>
<p>Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿ç­¹å­¦ï¼ˆORï¼‰é—®é¢˜æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ä½œä¸ºè¿ç­¹é—®é¢˜ä¸Šè®­ç»ƒLLMçš„å¼ºå¤§èŒƒå¼ï¼Œä½†ç°æœ‰å·¥ä½œé€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚é¦–å…ˆï¼Œç»“æœå¥–åŠ±ä¼šå—åˆ°å­¦åˆ†åˆ†é…é—®é¢˜çš„å½±å“ï¼Œå…¶ä¸­æ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆå¯èƒ½ä¼šå¼ºåŒ–é”™è¯¯çš„æ¨ç†ã€‚å…¶æ¬¡ï¼Œä¼ ç»Ÿçš„åˆ¤åˆ«è¿‡ç¨‹ç›‘ç£æ˜¯çŸ­è§†çš„ï¼Œæ— æ³•å…¨é¢è¯„ä¼°è¿ç­¹å»ºæ¨¡çš„ç›¸äº’ä¾èµ–æ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†StepORLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªè¿›åŒ–æ¡†æ¶ï¼Œå…·æœ‰ç”Ÿæˆè¿‡ç¨‹ç›‘ç£ã€‚StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼Œå…¶ä¸­ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆGenPRMï¼‰å¯ä»¥ç›¸äº’è¿­ä»£æ”¹è¿›ã€‚è¿™ä¸ªå¾ªç¯æ˜¯ç”±åŒé‡åé¦ˆæœºåˆ¶é©±åŠ¨çš„ï¼šæ¥è‡ªå¤–éƒ¨æ±‚è§£å™¨çš„åŸºäºç»“æœçš„ç¡®å®šæ€§éªŒè¯ï¼Œä»¥åŠæ¥è‡ªGenPRMçš„ç»†è‡´å…¨é¢è¿‡ç¨‹è¯„ä¼°ã€‚ç»“åˆä¿¡å·ç”¨äºé€šè¿‡åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆW-DPOï¼‰å¯¹é½ç­–ç•¥ï¼ŒåŒæ—¶æ”¹è¿›GenPRMã€‚æˆ‘ä»¬å¾—åˆ°çš„8Bå‚æ•°StepORLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€è‡ªä¸»æ–¹æ³•å’Œä¸“ç”¨åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿå……å½“å¼ºå¤§ä¸”é€šç”¨çš„è¿‡ç¨‹éªŒè¯å™¨ï¼Œå¤§å¤§æé«˜äº†æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹å’Œå…¶ä»–ç°æœ‰LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿ç­¹å­¦ï¼ˆORï¼‰é—®é¢˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚å¼ºåŒ–å­¦ä¹ æ˜¯è®­ç»ƒLLMè§£å†³ORé—®é¢˜çš„å¼ºå¤§èŒƒå¼ï¼Œä½†ç°æœ‰ç ”ç©¶é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šæˆæœå¥–åŠ±å­˜åœ¨ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œå³æ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆå¯èƒ½å¼ºåŒ–é”™è¯¯çš„æ¨ç†ï¼›ä¼ ç»Ÿåˆ¤åˆ«è¿‡ç¨‹ç›‘ç£å­˜åœ¨è§†é‡ç‹­éš˜é—®é¢˜ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ORå»ºæ¨¡çš„ç›¸äº’ä¾èµ–æ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†StepORLMï¼Œä¸€ç§æ–°å‹è‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œå…·æœ‰ç”Ÿæˆè¿‡ç¨‹ç›‘ç£åŠŸèƒ½ã€‚StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼Œç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆGenPRMï¼‰ç›¸äº’è¿­ä»£æ”¹è¿›ã€‚è¯¥å¾ªç¯ç”±åŒé‡åé¦ˆæœºåˆ¶é©±åŠ¨ï¼šæ¥è‡ªå¤–éƒ¨æ±‚è§£å™¨çš„ç¡®å®šæ€§ã€ç»“æœä¸ºåŸºç¡€çš„éªŒè¯ï¼Œä»¥åŠæ¥è‡ªGenPRMçš„å¾®å¦™ã€å…¨é¢è¿‡ç¨‹è¯„ä¼°ã€‚ç»“åˆä¿¡å·ç”¨äºé€šè¿‡åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆW-DPOï¼‰å¯¹é½ç­–ç•¥ï¼Œå¹¶åŒæ—¶ä¼˜åŒ–GenPRMã€‚æˆ‘ä»¬çš„8Bå‚æ•°StepORLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€æ™ºèƒ½æ–¹æ³•å’Œä¸“ç”¨åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿå……å½“å¼ºå¤§è€Œé€šç”¨çš„è¿‡ç¨‹éªŒè¯å™¨ï¼Œå¤§å¹…æå‡æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹å’Œå…¶ä»–ç°æœ‰LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºè§£å†³ORé—®é¢˜çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§£å†³ORé—®é¢˜æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šæˆæœå¥–åŠ±çš„ä¿¡ç”¨åˆ†é…é—®é¢˜å’Œä¼ ç»Ÿåˆ¤åˆ«è¿‡ç¨‹ç›‘ç£çš„è§†é‡ç‹­éš˜é—®é¢˜ã€‚</li>
<li>StepORLMæ¡†æ¶é€šè¿‡å¼•å…¥ç”Ÿæˆè¿‡ç¨‹ç›‘ç£æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼ŒåŒ…æ‹¬ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è¿­ä»£æ”¹è¿›ã€‚</li>
<li>åŒé‡åé¦ˆæœºåˆ¶åŒ…æ‹¬å¤–éƒ¨æ±‚è§£å™¨çš„ç»“æœéªŒè¯å’ŒGenPRMçš„è¿‡ç¨‹è¯„ä¼°ã€‚</li>
<li>æå‡ºçš„8Bå‚æ•°StepORLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Linear-Causal-Representation-Learning-by-Topological-Ordering-Pruning-and-Disentanglement"><a href="#Linear-Causal-Representation-Learning-by-Topological-Ordering-Pruning-and-Disentanglement" class="headerlink" title="Linear Causal Representation Learning by Topological Ordering, Pruning,   and Disentanglement"></a>Linear Causal Representation Learning by Topological Ordering, Pruning,   and Disentanglement</h2><p><strong>Authors:Hao Chen, Lin Liu, Yu Guang Wang</strong></p>
<p>Causal representation learning (CRL) has garnered increasing interests from the causal inference and artificial intelligence community, due to its capability of disentangling potentially complex data-generating mechanism into causally interpretable latent features, by leveraging the heterogeneity of modern datasets. In this paper, we further contribute to the CRL literature, by focusing on the stylized linear structural causal model over the latent features and assuming a linear mixing function that maps latent features to the observed data or measurements. Existing linear CRL methods often rely on stringent assumptions, such as accessibility to single-node interventional data or restrictive distributional constraints on latent features and exogenous measurement noise. However, these prerequisites can be challenging to satisfy in certain scenarios. In this work, we propose a novel linear CRL algorithm that, unlike most existing linear CRL methods, operates under weaker assumptions about environment heterogeneity and data-generating distributions while still recovering latent causal features up to an equivalence class. We further validate our new algorithm via synthetic experiments and an interpretability analysis of large language models (LLMs), demonstrating both its superiority over competing methods in finite samples and its potential in integrating causality into AI. </p>
<blockquote>
<p>å› æœè¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰å› å…¶èƒ½å¤Ÿåˆ©ç”¨ç°ä»£æ•°æ®é›†çš„å¼‚è´¨æ€§ï¼Œå°†æ½œåœ¨å¤æ‚çš„æ•°æ®ç”Ÿæˆæœºåˆ¶è½¬åŒ–ä¸ºå› æœå¯è§£é‡Šçš„æ½œåœ¨ç‰¹å¾ï¼Œè€Œå—åˆ°å› æœæ¨ç†å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„å¹¿æ³›å…³æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºæ½œåœ¨ç‰¹å¾çš„çº¿æ€§ç»“æ„åŒ–å› æœæ¨¡å‹ï¼Œå¹¶å‡è®¾å­˜åœ¨ä¸€ä¸ªçº¿æ€§æ··åˆå‡½æ•°ï¼Œå°†æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°è§‚æµ‹æ•°æ®æˆ–æµ‹é‡å€¼ã€‚ç°æœ‰çš„çº¿æ€§CRLæ–¹æ³•é€šå¸¸ä¾èµ–äºä¸¥æ ¼çš„å‡è®¾ï¼Œå¦‚å¯è®¿é—®å•ä¸ªèŠ‚ç‚¹çš„å¹²é¢„æ•°æ®æˆ–å¯¹æ½œåœ¨ç‰¹å¾å’Œå¤–åœ¨æµ‹é‡å™ªå£°çš„åˆ†å¸ƒçº¦æŸã€‚ç„¶è€Œï¼Œåœ¨æŸäº›åœºæ™¯ä¸­ï¼Œè¿™äº›å…ˆå†³æ¡ä»¶å¯èƒ½éš¾ä»¥æ»¡è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„çº¿æ€§CRLç®—æ³•ï¼Œä¸å¤§å¤šæ•°ç°æœ‰çº¿æ€§CRLæ–¹æ³•ä¸åŒï¼Œå®ƒåœ¨ç¯å¢ƒå¼‚è´¨æ€§å’Œæ•°æ®ç”Ÿæˆåˆ†å¸ƒçš„å‡è®¾æ›´å¼±çš„æƒ…å†µä¸‹ï¼Œä»ç„¶èƒ½å¤Ÿæ¢å¤ç­‰ä»·ç±»ä¸­çš„æ½œåœ¨å› æœç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åˆæˆå®éªŒå’Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ€§åˆ†æéªŒè¯äº†æˆ‘ä»¬çš„æ–°ç®—æ³•ï¼Œè¯æ˜äº†å®ƒåœ¨æœ‰é™æ ·æœ¬ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œå¹¶ä¸”åœ¨å°†å› æœæ€§èå…¥äººå·¥æ™ºèƒ½ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å› æœè¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰é¢†åŸŸï¼Œé’ˆå¯¹çº¿æ€§ç»“æ„å› æœæ¨¡å‹è¿›è¡Œç ”ç©¶ã€‚ç°æœ‰çº¿æ€§CRLæ–¹æ³•ä¾èµ–ä¸¥æ ¼å‡è®¾ï¼Œå¦‚å•èŠ‚ç‚¹å¹²é¢„æ•°æ®çš„å¯è®¿é—®æ€§æˆ–å¯¹æ½œåœ¨ç‰¹å¾å’Œå¤–æ¥æµ‹é‡å™ªå£°çš„åˆ†å¸ƒçº¦æŸã€‚ç„¶è€Œï¼Œè¿™äº›å…ˆå†³æ¡ä»¶åœ¨æŸäº›åœºæ™¯ä¸‹å¯èƒ½éš¾ä»¥æ»¡è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çº¿æ€§CRLç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç¯å¢ƒå¼‚è´¨æ€§å’Œæ•°æ®ç”Ÿæˆåˆ†å¸ƒæ–¹é¢çš„å‡è®¾è¾ƒå¼±ï¼Œä½†ä»èƒ½æ¢å¤ç­‰ä»·ç±»ä¸­çš„æ½œåœ¨å› æœç‰¹å¾ã€‚é€šè¿‡åˆæˆå®éªŒå’Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ€§åˆ†æï¼ŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨æœ‰é™æ ·æœ¬ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œä»¥åŠå…¶åœ¨äººå·¥æ™ºèƒ½ä¸­èå…¥å› æœå…³ç³»çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å› æœè¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰èƒ½è§£æå¤æ‚æ•°æ®ç”Ÿæˆæœºåˆ¶ä¸ºå› æœå¯è§£é‡Šçš„æ½œåœ¨ç‰¹å¾ï¼Œåˆ©ç”¨ç°ä»£æ•°æ®é›†çš„å¼‚è´¨æ€§ã€‚</li>
<li>ç°æœ‰çº¿æ€§CRLæ–¹æ³•é€šå¸¸ä¾èµ–ä¸¥æ ¼å‡è®¾ï¼Œå¦‚å•èŠ‚ç‚¹å¹²é¢„æ•°æ®çš„å¯è®¿é—®æ€§å’Œæ½œåœ¨ç‰¹å¾åˆ†å¸ƒçš„é™åˆ¶æ€§çº¦æŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çº¿æ€§CRLç®—æ³•ï¼Œå¯åœ¨ç¯å¢ƒå¼‚è´¨æ€§å’Œæ•°æ®ç”Ÿæˆåˆ†å¸ƒæ–¹é¢çš„å‡è®¾è¾ƒå¼±çš„æƒ…å†µä¸‹è¿è¡Œã€‚</li>
<li>æ–°ç®—æ³•èƒ½æ¢å¤ç­‰ä»·ç±»ä¸­çš„æ½œåœ¨å› æœç‰¹å¾ã€‚</li>
<li>é€šè¿‡åˆæˆå®éªŒï¼ŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨æœ‰é™æ ·æœ¬ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¯è§£é‡Šæ€§åˆ†æï¼Œå±•ç¤ºäº†è¯¥ç®—æ³•åœ¨äººå·¥æ™ºèƒ½ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22553v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22553v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JanusVLN-Decoupling-Semantics-and-Spatiality-with-Dual-Implicit-Memory-for-Vision-Language-Navigation"><a href="#JanusVLN-Decoupling-Semantics-and-Spatiality-with-Dual-Implicit-Memory-for-Vision-Language-Navigation" class="headerlink" title="JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation"></a>JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation</h2><p><strong>Authors:Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei</strong></p>
<p>Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brainâ€™s semantic understanding and the right brainâ€™s spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: <a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/">https://miv-xjtu.github.io/JanusVLN.github.io/</a>. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªè¦æ±‚ä¸€ä¸ªå®ä½“ä»£ç†åœ¨æœªçŸ¥çš„é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œè¿ç»­è§†é¢‘æµçš„å¼•å¯¼ä¸‹è¿›è¡Œå¯¼èˆªã€‚è§†è§‰ä¸è¯­è¨€å¯¼èˆªçš„è¿‘æœŸè¿›å±•å¾—ç›Šäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¯­ä¹‰ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„è¯­ä¹‰è®°å¿†ï¼Œå¦‚å»ºç«‹æ–‡æœ¬è®¤çŸ¥åœ°å›¾æˆ–å­˜å‚¨å†å²è§†è§‰å¸§ã€‚è¿™ç§æ–¹æ³•å­˜åœ¨ç©ºé—´ä¿¡æ¯ä¸¢å¤±ã€è®¡ç®—å†—ä½™å’Œå†…å­˜è†¨èƒ€ç­‰é—®é¢˜ï¼Œé˜»ç¢äº†é«˜æ•ˆå¯¼èˆªã€‚å—åˆ°äººç±»å¯¼èˆªä¸­éšå¼åœºæ™¯è¡¨ç¤ºçš„å¯å‘ï¼Œç±»ä¼¼äºå·¦è„‘çš„è¯­ä¹‰ç†è§£å’Œå³è„‘çš„ç©ºé—´è®¤çŸ¥ï¼Œæˆ‘ä»¬æå‡ºäº†JanusVLNï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰ä¸è¯­è¨€å¯¼èˆªæ¡†æ¶ï¼Œå…·æœ‰åŒéšç¥ç»è®°å¿†ï¼Œå°†ç©ºé—´å‡ ä½•è®°å¿†å’Œè§†è§‰è¯­ä¹‰è®°å¿†å»ºæ¨¡ä¸ºå•ç‹¬ã€ç´§å‡‘å’Œå›ºå®šå¤§å°çš„ç¥ç»è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é¦–å…ˆæ‰©å±•äº†MLLMï¼Œä»¥èå…¥ç©ºé—´å‡ ä½•ç¼–ç å™¨çš„3Då…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå¢å¼ºä»…åŸºäºRGBè¾“å…¥çš„æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæ„å»ºç©ºé—´å‡ ä½•å’Œè§†è§‰è¯­ä¹‰ç¼–ç å™¨çš„å†å²é”®å€¼ç¼“å­˜ä½œä¸ºåŒéšè®°å¿†ã€‚é€šè¿‡ä»…ä¿ç•™åˆå§‹å’Œæ»‘åŠ¨çª—å£ä¸­çš„ä»¤ç‰Œé”®å€¼ï¼Œé¿å…äº†å†—ä½™è®¡ç®—ï¼Œå®ç°äº†é«˜æ•ˆå¢é‡æ›´æ–°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJanusVLNåœ¨è¶…è¿‡20ç§æœ€æ–°æ–¹æ³•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä¸ä½¿ç”¨å¤šç§æ•°æ®ç±»å‹ä½œä¸ºè¾“å…¥çš„æ–¹æ³•ç›¸æ¯”ï¼ŒæˆåŠŸç‡æé«˜äº†10.5-35.5%ï¼›ä¸ä½¿ç”¨æ›´å¤šRGBè®­ç»ƒæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œæé«˜äº†3.6-10.8%ã€‚è¿™è¡¨æ˜æ‰€æå‡ºçš„åŒéšç¥ç»è®°å¿†ä½œä¸ºä¸€ç§æ–°èŒƒå¼ï¼Œä¸ºæœªæ¥çš„è§†è§‰ä¸è¯­è¨€å¯¼èˆªç ”ç©¶æ¢ç´¢äº†æœ‰å¸Œæœ›çš„æ–°æ–¹å‘ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/%E3%80%82">https://miv-xjtu.github.io/JanusVLN.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22548v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/">https://miv-xjtu.github.io/JanusVLN.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision-and-Language Navigationï¼ˆVLNï¼‰çš„æ–°æ¡†æ¶JanusVLNã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒéšç¥ç»å†…å­˜ï¼Œå°†ç©ºé—´å‡ ä½•å’Œè§†è§‰è¯­ä¹‰å†…å­˜å»ºæ¨¡ä¸ºå•ç‹¬çš„ã€ç´§å‡‘çš„ã€å›ºå®šå¤§å°çš„ç¥ç»è¡¨ç¤ºã€‚å®ƒç»“åˆäº†Multimodal Large Language Modelï¼ˆMLLMï¼‰å’Œ3Då…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†ä»…åŸºäºRGBè¾“å…¥çš„æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒJanusVLNåœ¨VLNä»»åŠ¡ä¸Šå®ç°äº†è¶…è¶Š20ç§æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æœªæ¥VLNç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-and-Language Navigationï¼ˆVLNï¼‰éœ€è¦æ™ºèƒ½ä½“é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œè¿ç»­è§†é¢‘æµåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªã€‚</li>
<li>è¿‘æœŸVLNçš„è¿›å±•å¾—ç›ŠäºMultimodal Large Language Modelsï¼ˆMLLMï¼‰çš„å¼ºå¤§è¯­ä¹‰ç†è§£ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šå¸¸ä¾èµ–æ˜¾å¼è¯­ä¹‰è®°å¿†ï¼Œå¦‚æ„å»ºæ–‡æœ¬è®¤çŸ¥åœ°å›¾æˆ–å­˜å‚¨å†å²è§†è§‰å¸§ï¼Œä½†å­˜åœ¨ç©ºé—´ä¿¡æ¯æŸå¤±ã€è®¡ç®—å†—ä½™å’Œè®°å¿†è†¨èƒ€ç­‰é—®é¢˜ã€‚</li>
<li>JanusVLNæ¡†æ¶é‡‡ç”¨åŒéšç¥ç»è®°å¿†ï¼Œå¯¹ç©ºé—´å‡ ä½•å’Œè§†è§‰è¯­ä¹‰å†…å­˜è¿›è¡Œå»ºæ¨¡ï¼Œé¿å…å†—ä½™è®¡ç®—å¹¶å®ç°é«˜æ•ˆå¢é‡æ›´æ–°ã€‚</li>
<li>JanusVLNç»“åˆ3Då…ˆéªŒçŸ¥è¯†æé«˜æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>JanusVLNå®ç°äº†è¶…è¶Š20ç§æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨VLNä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="The-InviTE-Corpus-Annotating-Invectives-in-Tudor-English-Texts-for-Computational-Modeling"><a href="#The-InviTE-Corpus-Annotating-Invectives-in-Tudor-English-Texts-for-Computational-Modeling" class="headerlink" title="The InviTE Corpus: Annotating Invectives in Tudor English Texts for   Computational Modeling"></a>The InviTE Corpus: Annotating Invectives in Tudor English Texts for   Computational Modeling</h2><p><strong>Authors:Sophie Spliethoff, Sanne Hoeken, Silke Schwandt, Sina ZarrieÃŸ, Ã–zge AlaÃ§am</strong></p>
<p>In this paper, we aim at the application of Natural Language Processing (NLP) techniques to historical research endeavors, particularly addressing the study of religious invectives in the context of the Protestant Reformation in Tudor England. We outline a workflow spanning from raw data, through pre-processing and data selection, to an iterative annotation process. As a result, we introduce the InviTE corpus â€“ a corpus of almost 2000 Early Modern English (EModE) sentences, which are enriched with expert annotations regarding invective language throughout 16th-century England. Subsequently, we assess and compare the performance of fine-tuned BERT-based models and zero-shot prompted instruction-tuned large language models (LLMs), which highlights the superiority of models pre-trained on historical data and fine-tuned to invective detection. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯åº”ç”¨äºå†å²ç ”ç©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éƒ½é“ç‹æœè‹±æ ¼å…°æ—¶ä»£å®—æ•™è¯½è°¤çš„ç ”ç©¶ã€‚æˆ‘ä»¬æ¦‚è¿°äº†ä¸€ä¸ªä»åŸå§‹æ•°æ®å¼€å§‹ï¼Œç»è¿‡é¢„å¤„ç†å’Œæ•°æ®é€‰æ‹©ï¼Œå†åˆ°è¿­ä»£æ³¨é‡Šè¿‡ç¨‹çš„å·¥ä½œæµç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†InviTEè¯­æ–™åº“â€”â€”ä¸€ä¸ªåŒ…å«è¿‘ä¸¤åƒä¸ªæ—©æœŸç°ä»£è‹±è¯­ï¼ˆEModEï¼‰å¥å­çš„è¯­æ–™åº“ï¼Œè¿™äº›å¥å­éƒ½ç»è¿‡äº†å…³äºåå…­ä¸–çºªè‹±æ ¼å…°è¯½è°¤è¯­è¨€çš„ä¸“å®¶æ³¨é‡Šã€‚éšåï¼Œæˆ‘ä»¬è¯„ä¼°å’Œæ¯”è¾ƒäº†ç»è¿‡å¾®è°ƒåŸºäºBERTçš„æ¨¡å‹å’Œé›¶æ ·æœ¬æç¤ºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†é¢„å…ˆåœ¨å†å²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¹¶é’ˆå¯¹è¯½è°¤æ£€æµ‹è¿›è¡Œå¾®è°ƒæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22345v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åœ¨å†å²ç ”ç©¶é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç ”ç©¶éƒ½é“ç‹æœè‹±æ ¼å…°æ—¶æœŸçš„å®—æ•™è¯½è°¤è¯­æ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« æè¿°äº†ä¸€ä¸ªä»åŸå§‹æ•°æ®åˆ°é¢„å¤„ç†ã€æ•°æ®é€‰æ‹©ï¼Œå†åˆ°è¿­ä»£æ ‡æ³¨çš„å·¥ä½œæµç¨‹ï¼Œå¹¶ä»‹ç»äº†InviTEè¯­æ–™åº“çš„å¼€å‘ä¸åº”ç”¨ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è¯„ä¼°äº†å¾®è°ƒåçš„BERTæ¨¡å‹å’Œé›¶æ ·æœ¬æç¤ºæŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†å¯¹å†å²æ•°æ®è¿›è¡Œé¢„è®­ç»ƒå’Œé’ˆå¯¹è¯½è°¤è¯­æ£€æµ‹è¿›è¡Œå¾®è°ƒçš„é‡è¦æ€§ã€‚æ€»ä¹‹ï¼Œæ–‡ç« å±•ç°äº†ä¸€é¡¹åˆ›æ–°çš„èåˆå¤šå­¦ç§‘ç ”ç©¶æ–¹æ³•çš„åº”ç”¨ï¼Œè¯¥ç ”ç©¶æ–¹æ³•å€ŸåŠ©æœºå™¨å­¦ä¹ å·¥å…·è§£å†³äº†å†å²ä¸ŠçœŸå®é—®é¢˜çš„å›°éš¾æ€§æŒ‘æˆ˜ï¼Œä¸ä»…èƒ½å¸®åŠ©å­¦è€…è§£å†³æŒ‘æˆ˜æ›´å¯ä¿ƒè¿›å­¦æœ¯äº¤æµå’Œåˆ›æ–°ç ”ç©¶çš„èåˆå’Œè¿›ä¸€æ­¥å‘å±•ã€‚è¿™ç¯‡æ–‡ç« è¿˜å°†NLPåœ¨å†å²æ–‡çŒ®é¢†åŸŸä¸­çš„æœªæ¥è¶‹åŠ¿å±•æœ›ç»™äºˆå±•ç°å’Œå¯è¿ªã€‚ä¸€ä¸ªé‡ç‚¹æ˜¯é€šè¿‡ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥å¼€å‘è¯­æ–™åº“ï¼Œå¹¶è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨å¤„ç†å†å²è¯­è¨€æ–¹é¢çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹çš„åº”ç”¨æœ‰åŠ©äºæ­ç¤ºå†å²ä¸Šè¯­è¨€ä½¿ç”¨çš„å¤æ‚æ€§å’Œæ¼”å˜è¿‡ç¨‹ã€‚æ•´ä½“æ¥çœ‹æœ¬æ–‡é¢å‘ç›¸å…³é¢†åŸŸç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªå¯¹äºå¦‚ä½•å¤„ç†è¯­è¨€æ ‡æ³¨å¹¶è®¾è®¡è¯„ä¼°æµç¨‹çš„å»ºè®®æ–¹æ¡ˆã€‚æ­¤å¤–ä¹Ÿæä¾›äº†å®è´µçš„è¯­æ–™åº“èµ„æºï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æå¤§çš„ä¾¿åˆ©ã€‚è¯¥ç ”ç©¶çš„æˆåŠŸå®è·µå¯¹äºæœªæ¥è·¨å­¦ç§‘åˆä½œä»¥åŠæœºå™¨å­¦ä¹ åœ¨å†å²æ–‡çŒ®ç ”ç©¶ä¸­çš„åº”ç”¨å…·æœ‰æ·±è¿œå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¢«åº”ç”¨äºå†å²ç ”ç©¶é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯éƒ½é“ç‹æœè‹±æ ¼å…°æ—¶æœŸçš„å®—æ•™è¯½è°¤è¯­ç ”ç©¶ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºInviTEçš„è¯­æ–™åº“ï¼ŒåŒ…å«è¿‘2000ä¸ªæ—©æœŸç°ä»£è‹±è¯­çš„å¥å­ï¼Œå¹¶è¿›è¡Œäº†å…³äºè¯½è°¤è¯­è¨€çš„ä¸“å®¶æ ‡æ³¨ã€‚</li>
<li>è¯„ä¼°äº†å¾®è°ƒåçš„BERTæ¨¡å‹å’Œé›¶æ ·æœ¬æç¤ºæŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚</li>
<li>å¼ºè°ƒäº†é¢„è®­ç»ƒåœ¨å†å²æ•°æ®ä¸Šçš„æ¨¡å‹ä»¥åŠå¯¹è¯½è°¤è¯­æ£€æµ‹è¿›è¡Œå¾®è°ƒçš„é‡è¦æ€§ã€‚</li>
<li>NLPæŠ€æœ¯åœ¨å†å²æ–‡çŒ®ç ”ç©¶ä¸­çš„åº”ç”¨æœ‰åŠ©äºæ­ç¤ºå†å²ä¸Šè¯­è¨€ä½¿ç”¨çš„å¤æ‚æ€§å’Œæ¼”å˜è¿‡ç¨‹ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¤„ç†è¯­è¨€æ ‡æ³¨å’Œè¯„ä¼°æµç¨‹çš„å»ºè®®æ–¹æ¡ˆï¼Œä¸ºç›¸å…³é¢†åŸŸç ”ç©¶è€…æä¾›äº†å®è´µçš„è¯­æ–™åº“èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Teaching-Transformers-to-Solve-Combinatorial-Problems-through-Efficient-Trial-Error"><a href="#Teaching-Transformers-to-Solve-Combinatorial-Problems-through-Efficient-Trial-Error" class="headerlink" title="Teaching Transformers to Solve Combinatorial Problems through Efficient   Trial &amp; Error"></a>Teaching Transformers to Solve Combinatorial Problems through Efficient   Trial &amp; Error</h2><p><strong>Authors:Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos</strong></p>
<p>Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç»„åˆé—®é¢˜ï¼ˆå¦‚å¯æ»¡è¶³æ€§é—®é¢˜ã€æ—…è¡Œå•†é—®é¢˜ï¼Œç”šè‡³åŸºæœ¬ç®—æœ¯ï¼‰æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§è§£å†³NPç±»é—®é¢˜çš„æ–°å‹æ–¹æ³•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä¸“æ³¨äºå…¸å‹çš„æ•°ç‹¬ä»»åŠ¡ï¼Œä¸å…ˆå‰çš„ç¥ç»ç¬¦å·æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ˆ99%ï¼‰ã€‚ä¸ä»¥å¾€ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ™®é€šçš„ä»…è§£ç å™¨è½¬æ¢å™¨ï¼ˆGPT-2ï¼‰ï¼Œæ— éœ€ä½¿ç”¨å¤–éƒ¨å·¥å…·æˆ–å‡½æ•°è°ƒç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç®€å•æ•°ç‹¬è§„åˆ™çš„æ¨¡ä»¿å­¦ä¹ ä¸åŒ…å«çŒœæµ‹å’Œå›æº¯çš„æ˜¾å¼æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰æ¢ç´¢ç­–ç•¥ç›¸ç»“åˆã€‚è¶…è¶Šæ¨¡ä»¿å­¦ä¹ ï¼Œæˆ‘ä»¬åŠªåŠ›å‡å°‘çŒœæµ‹æ¬¡æ•°ï¼Œç›´è‡³æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å¯¹è¿™ä¸€è®¾ç½®è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†æï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºMin-Sum Set Coverçš„ä¸Šä¸‹æ–‡å˜ä½“ï¼Œè¿™æ˜¯ç®—æ³•å’Œéšæœºä¼˜åŒ–ä¸­çš„ä¸€ä¸ªå¹¿å—å…³æ³¨çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22023v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£å†³ç»„åˆé—®é¢˜ï¼ˆå¦‚å¯æ»¡è¶³æ€§é—®é¢˜ã€æ—…è¡Œå•†é—®é¢˜ç­‰ï¼‰æˆ–åŸºæœ¬ç®—æœ¯é—®é¢˜æ—¶ä»å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç§æ–°æ–¹æ³•è§£å†³NPç±»é—®é¢˜ï¼Œå¹¶ä»¥ç»å…¸çš„æ•°ç‹¬ä»»åŠ¡ä¸ºç„¦ç‚¹ï¼Œå®ç°äº†ç›¸è¾ƒäºå…ˆå‰ç¥ç»ç¬¦å·æ–¹æ³•çš„æœ€æ–°å‡†ç¡®åº¦ï¼ˆ99%ï¼‰ã€‚ä¸åŒäºä»¥å¾€ä½¿ç”¨å®šåˆ¶æ¶æ„çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…é‡‡ç”¨é€šç”¨è§£ç å™¨è½¬æ¢å™¨ï¼ˆGPT-2ï¼‰ï¼Œæ— éœ€å¤–éƒ¨å·¥å…·æˆ–å‡½æ•°è°ƒç”¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ•°ç‹¬ç®€å•è§„åˆ™çš„å­¦ä¹ æ¨¡ä»¿ä¸åŒ…å«å¯å‘å¼çŒœæµ‹å’Œå›æº¯çš„æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰æ¢ç´¢ç­–ç•¥ã€‚é™¤äº†å­¦ä¹ æ¨¡ä»¿ï¼Œæˆ‘ä»¬è¿˜åŠ›æ±‚å‡å°‘çŒœæµ‹æ¬¡æ•°ç›´è‡³æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å¯¹æ­¤è®¾ç½®è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†æï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºç®—æ³•å’Œéšæœºä¼˜åŒ–ä¸­ç ”ç©¶çš„Min-Sum Set Coveré—®é¢˜çš„ä¸Šä¸‹æ–‡å˜ä½“ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³ç»„åˆé—®é¢˜ï¼ˆå¦‚Sudokuï¼‰å’ŒåŸºæœ¬ç®—æœ¯é—®é¢˜ä¸Šå­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡ä¸€ç§æ–°çš„æ–¹æ³•è§£å†³äº†NPç±»é—®é¢˜ï¼Œå–å¾—äº†åœ¨æ•°ç‹¬ä»»åŠ¡ä¸Šçš„æœ€æ–°å‡†ç¡®åº¦ï¼ˆ99%ï¼‰ã€‚</li>
<li>ä¸å…ˆå‰ä½¿ç”¨å®šåˆ¶æ¶æ„çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡çš„æ–¹æ³•ä»…é‡‡ç”¨GPT-2è§£ç å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ•°ç‹¬è§„åˆ™çš„å­¦ä¹ æ¨¡ä»¿ä¸æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰ç­–ç•¥ã€‚</li>
<li>é™¤äº†å­¦ä¹ æ¨¡ä»¿ï¼Œè¯¥æ–¹æ³•è¿˜æ³¨é‡å‡å°‘çŒœæµ‹æ¬¡æ•°ç›´è‡³æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬æ–‡è®¾ç½®ä¸Min-Sum Set Coveré—®é¢˜æœ‰å¯†åˆ‡è”ç³»ï¼Œä¸ºç®—æ³•å’Œéšæœºä¼˜åŒ–é¢†åŸŸæä¾›äº†æ–°è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Low-Rank-Adaptation-with-Structured-Nonlinear-Transformations"><a href="#Enhancing-Low-Rank-Adaptation-with-Structured-Nonlinear-Transformations" class="headerlink" title="Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations"></a>Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations</h2><p><strong>Authors:Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒçš„çº¿æ€§æ€§è´¨é™åˆ¶äº†å…¶è¡¨ç°åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†LoRANï¼Œè¿™æ˜¯LoRAçš„éçº¿æ€§æ‰©å±•ï¼Œå®ƒå°†å¯¹ä½ç§©æ›´æ–°è¿›è¡Œè½»é‡çº§è½¬æ¢ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºæ­£å¼¦å‡½æ•°çš„æ¿€æ´»å‡½æ•°Sinterï¼Œå®ƒåœ¨ä¸å¢åŠ å‚æ•°è®¡æ•°çš„æƒ…å†µä¸‹æ·»åŠ äº†ç»“æ„åŒ–æ‰°åŠ¨ã€‚è·¨è¶Šæ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒLoRANåœ¨QLoRAä¸Šè¡¨ç°æŒç»­ä¼˜å¼‚ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒSinterä¼˜äºæ ‡å‡†æ¿€æ´»å‡½æ•°ï¼Œå¦‚Sigmoidã€ReLUå’ŒTanhï¼Œçªå‡ºäº†æ¿€æ´»è®¾è®¡åœ¨ä½ç§©è°ƒæ•´ä¸­çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21870v1">PDF</a> This manuscript has been submitted to IEEE Journal of Selected Topics   in Signal Processing (JSTSP) for review. Until the moment I submitted the   manuscript to arXiv, we havenâ€™t received any review comments from JSTSP</p>
<p><strong>Summary</strong></p>
<p>LoRAæ–¹æ³•æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä½†å…¶çº¿æ€§ç‰¹æ€§é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoRANï¼Œè¿™æ˜¯LoRAçš„éçº¿æ€§æ‰©å±•ï¼Œé€šè¿‡å¯¹ä½ç§©æ›´æ–°è¿›è¡Œè½»é‡çº§è½¬æ¢æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºæ­£å¼¦å‡½æ•°çš„æ¿€æ´»å‡½æ•°Sinterï¼Œå®ƒå¢åŠ äº†ç»“æ„åŒ–æ‰°åŠ¨ï¼ŒåŒæ—¶ä¸å¢åŠ å‚æ•°æ•°é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒLoRANåœ¨æ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºQLoRAã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒSinteråœ¨æ ‡å‡†æ¿€æ´»å‡½æ•°å¦‚Sigmoidã€ReLUå’ŒTanhä¸­å…·æœ‰ä¼˜è¶Šæ€§ï¼Œçªæ˜¾äº†æ¿€æ´»è®¾è®¡åœ¨ä½ç§©è°ƒæ•´ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>LoRANæ˜¯LoRAçš„éçº¿æ€§æ‰©å±•ï¼Œé€šè¿‡ä½ç§©æ›´æ–°çš„è½»é‡çº§è½¬æ¢æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Sinteræ˜¯ä¸€ç§åŸºäºæ­£å¼¦å‡½æ•°çš„æ¿€æ´»å‡½æ•°ï¼Œèƒ½å¤Ÿå¢åŠ ç»“æ„åŒ–æ‰°åŠ¨ä¸”ä¸ä¼šå¢åŠ å‚æ•°æ•°é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLoRANåœ¨æ‘˜è¦å’Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºQLoRAã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†Sinteråœ¨æ ‡å‡†æ¿€æ´»å‡½æ•°ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>æ¿€æ´»è®¾è®¡åœ¨ä½ç§©è°ƒæ•´ä¸­éå¸¸é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DiTraj-training-free-trajectory-control-for-video-diffusion-transformer"><a href="#DiTraj-training-free-trajectory-control-for-video-diffusion-transformer" class="headerlink" title="DiTraj: training-free trajectory control for video diffusion transformer"></a>DiTraj: training-free trajectory control for video diffusion transformer</h2><p><strong>Authors:Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su, Zhicheng Zhao</strong></p>
<p>Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the objectâ€™s trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokensâ€™ position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¸”å¸¦æœ‰3Då…¨æ³¨æ„åŠ›ã€‚è½¨è¿¹æ§åˆ¶ä»£è¡¨äº†å¯æ§è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªç”¨æˆ·å‹å¥½å‹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡è®­ç»ƒèµ„æºï¼Œè¦ä¹ˆä¸“é—¨ä¸ºU-Netè®¾è®¡ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨DiTçš„å“è¶Šæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹DiTæå‡ºäº†DiTrajï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ— éœ€è®­ç»ƒçš„è½¨è¿¹æ§åˆ¶æ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œä¸ºäº†æ³¨å…¥ç‰©ä½“çš„è½¨è¿¹ï¼Œæˆ‘ä»¬æå‡ºå‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼ï¼šæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†ç”¨æˆ·æä¾›çš„æç¤ºè½¬æ¢ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼Œåˆ†åˆ«æŒ‡å¯¼è§†é¢‘ä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç”Ÿæˆã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†æäº†3Då…¨æ³¨æ„åŠ›ï¼Œå¹¶æ¢ç´¢äº†ä»¤ç‰Œé—´æ³¨æ„åŠ›å¾—åˆ†ä¸ä½ç½®åµŒå…¥ä¹‹é—´çš„ç´§å¯†å…³è”ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å¸§æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰ã€‚é€šè¿‡ä»…ä¿®æ”¹å‰æ™¯ä»¤ç‰Œçš„ä½ç½®åµŒå…¥ï¼ŒSTD-RoPEæ¶ˆé™¤äº†è·¨å¸§çš„ç©ºé—´å·®å¼‚ï¼ŒåŠ å¼ºäº†å®ƒä»¬ä¹‹é—´çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†è½¨è¿¹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å¯†åº¦å®ç°äº†3Dæ„ŸçŸ¥çš„è½¨è¿¹æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘è´¨é‡å’Œè½¨è¿¹å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21839v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºDiffusion Transformersï¼ˆDiTï¼‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨3Då…¨æ³¨æ„åŠ›ä¸‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚é’ˆå¯¹å¯æ§è§†é¢‘ç”Ÿæˆä¸­çš„è½¨è¿¹æ§åˆ¶ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å…è®­ç»ƒæ¡†æ¶DiTrajï¼Œé€‚ç”¨äºDiTã€‚é€šè¿‡å‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼ã€è·¨å¸§æ—¶ç©ºè§£è€¦3D-RoPEï¼ˆSTD-RoPEï¼‰ç­‰æ–¹æ³•ï¼Œå®ç°äº†é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆå’Œç²¾ç¡®çš„è½¨è¿¹æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiT) åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è½¨è¿¹æ§åˆ¶æ–¹æ³•å­˜åœ¨èµ„æºéœ€æ±‚å¤§æˆ–ä¸“ä¸ºU-Netè®¾è®¡çš„é—®é¢˜ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨DiTçš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹DiTçš„å…è®­ç»ƒæ¡†æ¶DiTrajï¼Œç”¨äºè½¨è¿¹æ§åˆ¶ã€‚</li>
<li>é€šè¿‡å‰æ™¯èƒŒæ™¯åˆ†ç¦»æŒ‡å¯¼ï¼Œå°†ç”¨æˆ·æç¤ºåˆ†ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æç¤ºï¼ŒæŒ‡å¯¼è§†é¢‘ç”Ÿæˆã€‚</li>
<li>è·¨å¸§æ—¶ç©ºè§£è€¦çš„3D-RoPEï¼ˆSTD-RoPEï¼‰æ–¹æ³•é€šè¿‡ä¿®æ”¹å‰æ™¯æ ‡è®°çš„ä½ç½®åµŒå…¥ï¼Œæé«˜äº†è½¨è¿¹æ§åˆ¶æ•ˆæœã€‚</li>
<li>é€šè¿‡è°ƒèŠ‚ä½ç½®åµŒå…¥çš„å¯†åº¦å®ç°3Dæ„ŸçŸ¥è½¨è¿¹æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learning-to-Summarize-by-Learning-to-Quiz-Adversarial-Agentic-Collaboration-for-Long-Document-Summarization"><a href="#Learning-to-Summarize-by-Learning-to-Quiz-Adversarial-Agentic-Collaboration-for-Long-Document-Summarization" class="headerlink" title="Learning to Summarize by Learning to Quiz: Adversarial Agentic   Collaboration for Long Document Summarization"></a>Learning to Summarize by Learning to Quiz: Adversarial Agentic   Collaboration for Long Document Summarization</h2><p><strong>Authors:Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</strong></p>
<p>Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality. </p>
<blockquote>
<p>é•¿æ–‡æ¡£æ‘˜è¦å¯¹äºå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†è¶…é•¿æ–‡æ¡£æ—¶é€šå¸¸ä¼šé¢ä¸´ä¿¡æ¯ä¸¢å¤±ã€äº‹å®ä¸ä¸€è‡´å’Œè¿è´¯æ€§é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†SummQï¼Œä¸€ç§æ–°å‹å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªäº’è¡¥é¢†åŸŸä¸­çš„æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œæ™ºèƒ½æ¥è§£å†³è¿™äº›å±€é™æ€§ï¼šæ‘˜è¦å’Œæµ‹éªŒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ‘˜è¦ç”Ÿæˆå™¨å’Œè¯„å®¡è€…è¿›è¡Œåä½œä»¥åˆ›å»ºå’Œè¯„ä¼°å…¨é¢çš„æ‘˜è¦ï¼Œè€Œæµ‹éªŒç”Ÿæˆå™¨å’Œè¯„å®¡è€…åˆ™åˆ›å»ºç†è§£é—®é¢˜ï¼Œä½œä¸ºæ‘˜è¦è¿‡ç¨‹çš„æŒç»­è´¨é‡æ£€æŸ¥ã€‚è¿™ç§å¯¹æŠ—æ€§åŠ¨æ€é€šè¿‡è€ƒè¯•æ™ºèƒ½ä½“å¾—åˆ°äº†åŠ å¼ºï¼ŒéªŒè¯ç”Ÿæˆçš„æ‘˜è¦æ˜¯å¦åŒ…å«å›ç­”æµ‹éªŒé—®é¢˜çš„å¿…è¦ä¿¡æ¯ï¼Œä½¿å¤šé¢å‘çš„åé¦ˆæœºåˆ¶æˆä¸ºå¯èƒ½ï¼Œå®ç°è¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„é•¿æ–‡æ¡£æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸Šå¯¹SummQè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ROUGEå’ŒBERTScoreæŒ‡æ ‡ä»¥åŠLLMä½œä¸ºæ³•å®˜å’Œäººç±»è¯„ä¼°ä¸­ï¼Œéƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»¼åˆåˆ†ææ­ç¤ºäº†å¤šæ™ºèƒ½ä½“åä½œåŠ¨æ€çš„æœ‰æ•ˆæ€§ã€ä¸åŒæ™ºèƒ½ä½“é…ç½®çš„å½±å“ä»¥åŠæµ‹éªŒæœºåˆ¶çš„å½±å“ã€‚è¿™é¡¹å·¥ä½œä¸ºé•¿æ–‡æ¡£æ‘˜è¦å»ºç«‹äº†æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨å¯¹æŠ—æ€§æ™ºèƒ½åä½œæ¥æé«˜æ‘˜è¦è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20900v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£æ‘˜è¦æ—¶é¢ä¸´ä¿¡æ¯ä¸¢å¤±ã€äº‹å®é”™è¯¯å’Œè¿è´¯æ€§é—®é¢˜ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSummQï¼Œä¸€ç§æ–°å‹å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨æ™ºèƒ½ä½“åœ¨æ‘˜è¦å’Œé—®ç­”ä¸¤ä¸ªé¢†åŸŸçš„åä½œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ‘˜è¦ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œä»¥åŠç”Ÿæˆé—®é¢˜å’Œè¯„ä¼°é—®é¢˜çš„æ™ºèƒ½ä½“ï¼Œå…±åŒåä½œå¹¶æŒç»­æ£€æŸ¥æ‘˜è¦è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSummQåœ¨å¹¿æ³›ä½¿ç”¨çš„é•¿æ–‡æ¡£æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºé•¿æ–‡æ¡£æ‘˜è¦æä¾›äº†ä¸€ç§æ–°çš„å¯¹æŠ—æ€§æ™ºèƒ½ä½“åä½œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£æ‘˜è¦æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ä¿¡æ¯ä¸¢å¤±ã€äº‹å®é”™è¯¯å’Œè¿è´¯æ€§é—®é¢˜ã€‚</li>
<li>SummQæ˜¯ä¸€ç§æ–°å‹å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>SummQåŒ…æ‹¬æ‘˜è¦ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œä»¥åŠç”Ÿæˆå’Œç†è§£é—®é¢˜çš„æ™ºèƒ½ä½“ï¼Œå…±åŒåä½œè¿›è¡Œæ‘˜è¦çš„è´¨é‡æ£€æŸ¥ã€‚</li>
<li>SummQåœ¨å¹¿æ³›ä½¿ç”¨çš„é•¿æ–‡æ¡£æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“åä½œåŠ¨æ€åœ¨SummQä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>SummQçš„ç ”ç©¶åˆ†æäº†ä¸åŒæ™ºèƒ½ä½“é…ç½®å’Œé—®ç­”æœºåˆ¶çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLMs-in-Web-API-Integration-Tasks"><a href="#Benchmarking-LLMs-in-Web-API-Integration-Tasks" class="headerlink" title="Benchmarking LLMs in Web API Integration Tasks"></a>Benchmarking LLMs in Web API Integration Tasks</h2><p><strong>Authors:Daniel Maninger, Leon Chemnitz, Amir Molzam Sharifloo, Jannis Brugger, Mira Mezini</strong></p>
<p>API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models (LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models were able to solve more than 40% of the tasks. </p>
<blockquote>
<p>APIé›†æˆæ˜¯æˆ‘ä»¬æ•°å­—åŸºç¡€è®¾æ–½çš„æ ¸å¿ƒï¼Œä½¿å¾—è½¯ä»¶ç³»ç»Ÿèƒ½å¤Ÿè¿æ¥å’Œäº¤äº’ã€‚ç„¶è€Œï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œç¼–å†™æˆ–ç”Ÿæˆæ­£ç¡®çš„ä»£ç æ¥è°ƒç”¨APIï¼Œç‰¹åˆ«æ˜¯Web APIï¼Œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­å˜å¾—æµè¡Œèµ·æ¥ï¼Œä½†å®ƒä»¬è‡ªåŠ¨åŒ–ç”ŸæˆWeb APIé›†æˆä»£ç çš„æœ‰æ•ˆæ€§å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é›†å’Œè¯„ä¼°æµç¨‹ï¼Œæ—¨åœ¨è¯„ä¼°LLMç”ŸæˆWeb APIè°ƒç”¨ä»£ç çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¼€æºLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç”ŸæˆAPIè°ƒç”¨æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´å‡ºç°äº†è™šæ„çš„ç«¯ç‚¹ã€å‚æ•°ä½¿ç”¨ä¸æ­£ç¡®å’Œå…¶ä»–é”™è¯¯ã€‚ç»è¿‡è¯„ä¼°çš„å¼€æºæ¨¡å‹æ²¡æœ‰èƒ½å¤Ÿå®Œæˆè¶…è¿‡40%çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20172v2">PDF</a> To be published in Proceedings of 2nd ACM International Conference on   AI-powered Software, Benchmark &amp; Dataset Track (AIware â€˜25); updated paper   title and affiliations</p>
<p><strong>Summary</strong></p>
<p>APIæ•´åˆæ˜¯æ•°å­—åŸºç¡€è®¾æ–½çš„æ ¸å¿ƒï¼Œå®ƒè®©è½¯ä»¶ç³»ç»Ÿèƒ½å¤Ÿè¿æ¥å’Œäº¤äº’ã€‚ç„¶è€Œï¼Œå¦‚å¤šé¡¹ç ”ç©¶æ‰€ç¤ºï¼Œç¼–å†™æˆ–ç”Ÿæˆæ­£ç¡®çš„ä»£ç ä»¥è°ƒç”¨APIï¼Œç‰¹åˆ«æ˜¯Web APIï¼Œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†å®ƒä»¬è‡ªåŠ¨ç”ŸæˆWeb APIé›†æˆä»£ç çš„æ•ˆæœå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—æ•°æ®é›†å’Œè¯„ä¼°æµç¨‹ï¼Œæ—¨åœ¨è¯„ä¼°LLMç”ŸæˆWeb APIè°ƒç”¨ä»£ç çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªå¼€æºLLMè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç”ŸæˆAPIè°ƒç”¨å­˜åœ¨å¾ˆå¤§æŒ‘æˆ˜ï¼Œä¼šå‡ºç°è™šæ„çš„ç«¯ç‚¹ã€å‚æ•°ä½¿ç”¨é”™è¯¯ç­‰é”™è¯¯ã€‚æ²¡æœ‰ä»»ä½•ä¸€ä¸ªè¯„ä¼°çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿå®Œæˆè¶…è¿‡40%çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APIæ•´åˆæ˜¯æ•°å­—åŸºç¡€è®¾æ–½çš„æ ¸å¿ƒã€‚</li>
<li>ç¼–å†™æˆ–ç”Ÿæˆæ­£ç¡®çš„APIè°ƒç”¨ä»£ç æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ç”ŸæˆWeb APIé›†æˆä»£ç æ–¹é¢çš„æ•ˆæœå°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç”ŸæˆAPIè°ƒç”¨ä»£ç å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚è™šæ„ç«¯ç‚¹å’Œå‚æ•°ä½¿ç”¨é”™è¯¯ç­‰ã€‚</li>
<li>ç›®å‰æ²¡æœ‰å¼€æºLLMèƒ½å¤Ÿè§£å†³è¶…è¿‡40%çš„APIè°ƒç”¨ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›LLMåœ¨APIè°ƒç”¨ä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="EigenTrack-Spectral-Activation-Feature-Tracking-for-Hallucination-and-Out-of-Distribution-Detection-in-LLMs-and-VLMs"><a href="#EigenTrack-Spectral-Activation-Feature-Tracking-for-Hallucination-and-Out-of-Distribution-Detection-in-LLMs-and-VLMs" class="headerlink" title="EigenTrack: Spectral Activation Feature Tracking for Hallucination and   Out-of-Distribution Detection in LLMs and VLMs"></a>EigenTrack: Spectral Activation Feature Tracking for Hallucination and   Out-of-Distribution Detection in LLMs and VLMs</h2><p><strong>Authors:Davide Ettori, Nastaran Darabi, Sina Tayebati, Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, Amit Ranjan Trivedi</strong></p>
<p>Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å…·æœ‰å¹¿æ³›çš„å®ç”¨æ€§ï¼Œä½†ä»å®¹æ˜“å‡ºé”™ï¼Œå‡ºç°å¹»è§‰å’Œè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰è¯¯å·®ã€‚æˆ‘ä»¬æå‡ºäº†EigenTrackï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šçš„å®æ—¶æ£€æµ‹å™¨ï¼Œå®ƒåˆ©ç”¨éšè—æ¿€æ´»çš„è°±å‡ ä½•å’Œæ¨¡å‹åŠ¨æ€çš„å…¨å±€ç´§å‡‘ç­¾åã€‚é€šè¿‡å°†ç†µã€ç‰¹å¾å€¼é—´éš™å’ŒKLæ•£åº¦ç­‰åæ–¹å·®è°±ç»Ÿè®¡ä¿¡æ¯ä»éšæœºåŸºçº¿æµå…¥è½»é‡çº§å¾ªç¯åˆ†ç±»å™¨ä¸­ï¼ŒEigenTrackè·Ÿè¸ªè¡¨ç¤ºç»“æ„çš„ä¸´æ—¶å˜åŒ–ï¼Œä»è€Œåœ¨è¡¨é¢é”™è¯¯å‡ºç°ä¹‹å‰é¢„æµ‹å¹»è§‰å’ŒOODæ¼‚ç§»ã€‚ä¸åŒäºé»‘ç®±å’Œç°ç®±æ–¹æ³•ï¼Œå®ƒåªéœ€è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’è€Œæ— éœ€é‡æ–°é‡‡æ ·ã€‚ä¸å…¶ä»–ç°æœ‰çš„ç™½ç®±æ£€æµ‹å™¨ä¸åŒï¼Œå®ƒä¿ç•™äº†æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œèšåˆå…¨å±€ä¿¡å·ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„å‡†ç¡®æ€§å»¶è¿Ÿæƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15735v2">PDF</a> 5 pages, submitted to ICASSP 2026, September 2025</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¹¿æ³›çš„å®ç”¨æ€§ï¼Œä½†ä»å®¹æ˜“å‡ºé”™å‡ºç°å¹»è§‰å’Œè„±ç¦»åˆ†å¸ƒã€‚æœ¬æ–‡æå‡ºEigenTrackå®æ—¶æ£€æµ‹å™¨ï¼Œå…¶ä½¿ç”¨éšè—æ¿€æ´»çš„è°±å‡ ä½•ç‰¹æ€§æ¥ç›‘æµ‹æ¨¡å‹çš„åŠ¨æ€å˜åŒ–ã€‚é€šè¿‡å®æ—¶è®¡ç®—åæ–¹å·®è°±ç»Ÿè®¡é‡ï¼Œå¦‚ç†µã€ç‰¹å¾å€¼é—´éš™å’ŒKLæ•£åº¦ç­‰ï¼ŒEigenTrackèƒ½å¤Ÿè·Ÿè¸ªè¡¨ç¤ºç»“æ„ä¸­çš„æ—¶é—´å˜åŒ–ï¼Œé¢„æµ‹æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„å¹»è§‰å’Œè„±ç¦»åˆ†å¸ƒçš„é”™è¯¯ã€‚æ­¤æ–¹æ³•ä¸éœ€è¦å¤æ‚çš„é‡é‡‡æ ·è¿‡ç¨‹ï¼Œä¸”èƒ½å¤Ÿæä¾›å¯è§£é‡Šçš„ç²¾åº¦å»¶è¿Ÿæƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>LLMså…·æœ‰å¹¿æ³›çš„å®ç”¨æ€§ï¼Œä½†å­˜åœ¨å¹»è§‰å’Œè„±ç¦»åˆ†å¸ƒçš„é”™è¯¯é—®é¢˜ã€‚</li>
<li>EigenTrackæ˜¯ä¸€ç§å®æ—¶æ£€æµ‹å™¨ï¼Œç”¨äºé¢„æµ‹LLMsä¸­çš„å¹»è§‰å’ŒOODé”™è¯¯ã€‚</li>
<li>EigenTrackåˆ©ç”¨éšè—æ¿€æ´»çš„è°±å‡ ä½•ç‰¹æ€§æ¥ç›‘æµ‹æ¨¡å‹åŠ¨æ€å˜åŒ–ã€‚</li>
<li>EigenTrackä½¿ç”¨åæ–¹å·®è°±ç»Ÿè®¡é‡æ¥è·Ÿè¸ªè¡¨ç¤ºç»“æ„ä¸­çš„æ—¶é—´å˜åŒ–ã€‚</li>
<li>EigenTrackä¸éœ€è¦å¤æ‚çš„é‡é‡‡æ ·è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæä¾›å¯è§£é‡Šçš„ç²¾åº¦å»¶è¿Ÿæƒè¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAIâ€™s-Latest-Open-Source-Models"><a href="#Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAIâ€™s-Latest-Open-Source-Models" class="headerlink" title="Is GPT-OSS Good? A Comprehensive Evaluation of OpenAIâ€™s Latest Open   Source Models"></a>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAIâ€™s Latest Open   Source Models</h2><p><strong>Authors:Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song</strong></p>
<p>In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at the \href{<a target="_blank" rel="noopener" href="https://ai-agent-lab.github.io/gpt-oss%7D%7BProject">https://ai-agent-lab.github.io/gpt-oss}{Project</a> Webpage}. </p>
<blockquote>
<p>åœ¨2025å¹´8æœˆï¼ŒOpenAIå‘å¸ƒäº†GPT-OSSæ¨¡å‹ï¼Œè¿™æ˜¯è‡ª2019å¹´GPT-2ä»¥æ¥å…¶é¦–æ¬¡å¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹æƒé‡ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªæ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰120Bå’Œ20Bçš„å‚æ•°ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§å˜ä½“ï¼Œä¸å…­ç§å½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ï¼Œè¿™äº›æ¨¡å‹å‚æ•°èŒƒå›´ä»14.7Båˆ°235Bï¼Œä»£è¡¨äº†å¯†é›†å’Œç¨€ç–ä¸¤ç§è®¾è®¡ï¼Œè·¨è¶Šåä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸€èˆ¬çŸ¥è¯†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šè¯­è¨€ç†è§£å’Œå¯¹è¯èƒ½åŠ›ç­‰æ–¹é¢ã€‚æ‰€æœ‰æ¨¡å‹éƒ½åœ¨æ ‡å‡†åŒ–æ¨ç†è®¾ç½®ä¸‹ä»¥éé‡åŒ–å½¢å¼è¿›è¡Œæµ‹è¯•ï¼Œä½¿ç”¨McNemarsæµ‹è¯•å’Œæ•ˆåº”é‡åˆ†æè¿›è¡Œç»Ÿè®¡éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡gpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œå¦‚åœ¨HumanEvalå’ŒMMLUä¸Šï¼Œä½†å…¶æ‰€éœ€çš„å†…å­˜å’Œæ¯å“åº”èƒ½è€—è¦å°‘å¾—å¤šã€‚è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å½“å‰å¼€æºç¯å¢ƒä¸­çš„æ€»ä½“è¡¨ç°å±äºä¸­ç­‰æ°´å¹³ï¼Œåœ¨ä»£ç ç”Ÿæˆæ–¹é¢ç›¸å¯¹è¾ƒå¼ºï¼Œè€Œåœ¨å¤šè¯­è¨€ä»»åŠ¡æ–¹é¢å­˜åœ¨æ˜æ˜¾å¼±ç‚¹ã€‚è¿™äº›å‘ç°æä¾›äº†å®è¯è¯æ®ï¼Œè¡¨æ˜åœ¨ç¨€ç–æ¶æ„ä¸­è¿›è¡Œæ‰©å±•å¯èƒ½ä¸ä¼šäº§ç”Ÿæ¯”ä¾‹çš„æ€§èƒ½æå‡ï¼Œè¿™å¼ºè°ƒäº†è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥çš„éœ€è¦ï¼Œå¹¶ä¸ºæœªæ¥å¼€æºéƒ¨ç½²æä¾›æ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©ä¿¡æ¯ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œè¯„ä¼°è„šæœ¬å¯åœ¨é¡¹ç›®ç½‘é¡µ[<a target="_blank" rel="noopener" href="https://ai-agent-lab.github.io/gpt-oss]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ai-agent-lab.github.io/gpt-oss]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12461v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GPT-OSSæ¨¡å‹æ˜¯OpenAIåœ¨2025å¹´8æœˆå‘å¸ƒçš„å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«ä¸¤ä¸ªæ··åˆä¸“å®¶æ¶æ„çš„ç‰ˆæœ¬ï¼Œå‚æ•°åˆ†åˆ«ä¸º120Bå’Œ20Bã€‚åœ¨æ¶µç›–é€šç”¨çŸ¥è¯†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šè¯­ç§ç†è§£å’Œå¯¹è¯èƒ½åŠ›ç­‰æ–¹é¢çš„åä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸å…­ä¸ªå½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒGPT-OSSçš„è¡¨ç°ä¸­è§„ä¸­çŸ©ã€‚å°¤å…¶æ˜¯gpt-oss-20Båœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°½ç®¡æ‰€éœ€çš„å†…å­˜å’Œèƒ½æºå“åº”æ›´å°‘ã€‚è¿™äº›å‘ç°è¡¨æ˜åœ¨ç¨€ç–æ¶æ„ä¸­è¿›è¡Œæ‰©å±•å¯èƒ½ä¸ä¼šäº§ç”Ÿæ¯”ä¾‹çš„æ€§èƒ½æå‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥å’Œæ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenAIå‘å¸ƒäº†GPT-OSSæ¨¡å‹ï¼ŒåŒ…å«ä¸¤ä¸ªæ··åˆä¸“å®¶æ¶æ„çš„ç‰ˆæœ¬ï¼Œå‚æ•°åˆ†åˆ«ä¸º120Bå’Œ20Bã€‚</li>
<li>GPT-OSSæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸å…­ä¸ªå½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ã€‚</li>
<li>gpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œä¸”æ‰€éœ€å†…å­˜å’Œèƒ½æºå“åº”æ›´å°‘ã€‚</li>
<li>GPT-OSSæ¨¡å‹çš„æ•´ä½“è¡¨ç°å±äºä¸­æ¸¸æ°´å¹³ï¼Œåœ¨ä»£ç ç”Ÿæˆæ–¹é¢ç›¸å¯¹è¾ƒå¼ºï¼Œåœ¨å¤šè¯­ç§ä»»åŠ¡æ–¹é¢å­˜åœ¨æ˜æ˜¾å¼±ç‚¹ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ç¨€ç–æ¶æ„ä¸­æ‰©å±•å¯èƒ½ä¸ä¼šäº§ç”Ÿæ¯”ä¾‹çš„æ€§èƒ½æå‡ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥å’Œæ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©æ¥æé«˜æœªæ¥å¼€æºéƒ¨ç½²çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning"><a href="#Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning" class="headerlink" title="Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning"></a>Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning</h2><p><strong>Authors:Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang</strong></p>
<p>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. We propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language. Furthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language. To close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks, highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. <a target="_blank" rel="noopener" href="https://jd730.github.io/projects/M2A_GeoFact-X">https://jd730.github.io/projects/M2A_GeoFact-X</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦ã€äº‹å®é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œç„¶è€Œå®ƒä»¬åœ¨ä¸åŒè¯­è¨€ä¸­è¿›è¡Œè¿™äº›ä»»åŠ¡çš„èƒ½åŠ›ä»ç„¶æœ‰å¾…å‘å±•ã€‚ç‰¹åˆ«æ˜¯å¯¹äºæ–¯ç“¦å¸Œé‡Œè¯­æˆ–æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€ï¼ŒLLMç»å¸¸ä¼šè¯¯è§£æç¤ºæˆ–é»˜è®¤ä»¥è‹±è¯­è¿›è¡Œæ¨ç†ã€‚è¿™ç§å¯¹é«˜èµ„æºè¯­è¨€çš„éšæ€§åè§ä¼šæŸå®³äº‹å®å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¿¡ä»»ã€‚æˆ‘ä»¬æå‡ºM2Aï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†å¤šå°ºåº¦å¤šè¯­è¨€å¯¹é½å’Œæœºå™¨ç¿»è¯‘é—®é¢˜ä¸Šçš„è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ï¼Œè®­ç»ƒæ¨¡å‹ç›´æ¥åœ¨ç›®æ ‡è¯­è¨€ä¸­è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•åªè¯„ä¼°æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†æ¨ç†æ˜¯å¦å‘ç”Ÿåœ¨ç›®æ ‡è¯­è¨€ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoFact-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåœ°ç†çš„å¤šè¯­è¨€äº‹å®æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„æ¨ç†è½¨è¿¹ï¼šè‹±è¯­ã€å°åœ°è¯­ã€æ—¥è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ³°è¯­ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒM2Aåœ¨æ•°å­¦å’Œäº‹å®æ¨ç†ä»»åŠ¡ä¸­éƒ½æ˜¾è‘—æé«˜äº†å¤šè¯­è¨€æ¨ç†çš„ä¿çœŸåº¦ï¼Œå¼ºè°ƒæ¨ç†æ„ŸçŸ¥çš„å¤šè¯­è¨€å¼ºåŒ–å­¦ä¹ å¯¹äºç¨³å¥çš„è·¨è¯­è¨€æ³›åŒ–è‡³å…³é‡è¦ã€‚è¯¦æƒ…è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://jd730.github.io/projects/M2A_GeoFact-%E2%AD%BE">https://jd730.github.io/projects/M2A_GeoFact-X</a>ï¼ˆæ­¤å¤„å·²å°†ç½‘é¡µé“¾æ¥è¿›è¡Œæ ¼å¼åŒ–å¤„ç†ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05418v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦ã€äº‹å®é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ä¸åŒè¯­è¨€çš„ä»»åŠ¡æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾…å‘å±•ã€‚å¯¹äºæ–¯ç“¦å¸Œé‡Œè¯­æˆ–æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€ï¼ŒLLMå¸¸å¸¸è¯¯è§£æç¤ºæˆ–é»˜è®¤ä»¥è‹±è¯­è¿›è¡Œæ¨ç†ã€‚è¿™ç§å¯¹é«˜èµ„æºè¯­è¨€çš„éšæ€§åè§ä¼šå½±å“äº‹å®å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºM2Aæ–¹æ³•ï¼Œç»“åˆå¤šå°ºåº¦å¤šè¯­è¨€å¯¹é½å’Œæœºå™¨ç¿»è¯‘é—®é¢˜ä¸Šçš„è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ï¼Œè®­ç»ƒæ¨¡å‹ç›´æ¥åœ¨ç›®æ ‡è¯­è¨€ä¸­å‡†ç¡®æ¨ç†ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†äº†æ¨ç†æ˜¯å¦å‘ç”Ÿåœ¨ç›®æ ‡è¯­è¨€ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GeoFact-Xï¼Œä¸€ä¸ªåŸºäºåœ°ç†çš„å¤šè¯­è¨€äº‹å®æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„æ¨ç†è½¨è¿¹ï¼šè‹±è¯­ã€å°åœ°è¯­ã€æ—¥è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ³°è¯­ã€‚ç»“æœæ˜¾ç¤ºï¼ŒM2Aæ˜¾è‘—æé«˜äº†å¤šè¯­è¨€æ¨ç†çš„å¿ å®åº¦ï¼Œåœ¨æ•°å­¦å’Œäº‹å®æ¨ç†ä»»åŠ¡ä¸­å°¤ä¸ºæ˜¾è‘—ï¼Œå¼ºè°ƒäº†æ¨ç†æ„ŸçŸ¥çš„å¤šè¯­è¨€å¼ºåŒ–å­¦ä¹ å¯¹äºç¨³å¥çš„è·¨è¯­è¨€æ³›åŒ–è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ•°å­¦ã€äº‹å®é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸæœ‰å‡ºè‰²è¡¨ç°ï¼Œä½†åœ¨ä¸åŒè¯­è¨€çš„æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>LLMå¯¹ä½èµ„æºè¯­è¨€å­˜åœ¨è¯¯è§£æç¤ºå’Œé»˜è®¤è‹±è¯­æ¨ç†çš„åå‘ï¼Œå½±å“äº‹å®å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ã€‚</li>
<li>æå‡ºäº†M2Aæ–¹æ³•ï¼Œç»“åˆå¤šå°ºåº¦å¤šè¯­è¨€å¯¹é½å’Œæœºå™¨ç¿»è¯‘é—®é¢˜ä¸Šçš„è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ï¼Œæé«˜æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†äº†æ¨ç†æ˜¯å¦å‘ç”Ÿåœ¨ç›®æ ‡è¯­è¨€ä¸­ã€‚</li>
<li>å¼•å…¥äº†GeoFact-XåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§è¯­è¨€çš„æ¨ç†è½¨è¿¹ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>M2Aæ–¹æ³•æ˜¾è‘—æé«˜äº†å¤šè¯­è¨€æ¨ç†çš„å¿ å®åº¦ï¼Œåœ¨æ•°å­¦å’Œäº‹å®æ¨ç†ä»»åŠ¡ä¸­æ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Agent/2509.22596v1/page_2_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Effective Policy Learning for Multi-Agent Online Coordination Beyond   Submodular Objectives
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_R1_Reasoning/2509.22572v1/page_5_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  WebGen-Agent Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
