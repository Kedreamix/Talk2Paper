<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-09-30  VoiceAssistant-Eval Benchmarking AI Assistants across Listening,   Speaking, and Viewing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-30-更新"><a href="#2025-09-30-更新" class="headerlink" title="2025-09-30 更新"></a>2025-09-30 更新</h1><h2 id="VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing"><a href="#VoiceAssistant-Eval-Benchmarking-AI-Assistants-across-Listening-Speaking-and-Viewing" class="headerlink" title="VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,   Speaking, and Viewing"></a>VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,   Speaking, and Viewing</h2><p><strong>Authors:Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li</strong></p>
<p>The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems’ capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at <a target="_blank" rel="noopener" href="https://mathllm.github.io/VoiceAssistantEval/">https://mathllm.github.io/VoiceAssistantEval/</a> . </p>
<blockquote>
<p>大型语言模型和跨模态系统的能力增长引发了人们对以语音为主的AI助手的兴趣。然而，现有的基准测试不足以评估这些系统的全部能力。我们介绍了VoiceAssistant-Eval，这是一个全面的基准测试，旨在评估AI助手在听、说、看方面的能力。VoiceAssistant-Eval包含10,497个精选样本，涵盖13个任务类别。这些任务包括用于听力的自然声音、音乐和口语对话；用于说话的多轮对话、角色扮演和各种场景；以及用于观看的高度多样化的图像。为了证明其实用性，我们对21个开源模型和GPT-4o-Audio进行了评估，测量了回复内容和语音的质量，以及它们的一致性。结果揭示了三个关键发现：(1)专有模型并不普遍优于开源模型；(2)大多数模型在说话任务上表现优异，但在音频理解上落后；(3)设计精良的小型模型可以与大型模型相竞争。值得注意的是，中型Step-Audio-2-mini（7B）的听力准确性是LLaMA-Omni2-32B-Bilingual的两倍多。然而，仍然存在挑战：对于当前模型来说，跨模态（音频加视觉）输入和角色扮演语音模仿任务很困难，而且在稳健性和安全对齐方面仍存在很大差距。VoiceAssistant-Eval识别了这些差距，并为评估和指导下一代AI助手的发展建立了严格框架。代码和数据将在<a target="_blank" rel="noopener" href="https://mathllm.github.io/VoiceAssistantEval/%E4%B8%8A%E5%B8%A%E8%BE%83%E5%B0%91%E7%9A%84%E5%9C%BA%E6%99%AF%E4%B8%8B%E5%BC%80%E5%A7%8B%E5%9B%9E%E7%AD%94%E9%92%88%E5%AF%B9%E5%A4%9A%E5%9C%BA%E6%99%AF%E6%8F%90%E9%97%AE%E7%9A%84%E6%94%AF%E6%8C%81%E6%83%85%E5%86%B5%E5%92%8C%E5%AD%98%E5%9C%A8%E7%9A%84%E9%99%90%E5%88%B6%E5%81%9A%E4%BA%86%E8%AF%B4%E6%98%8E%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E6%96%87%E7%AB%A0%E8%BF%98%E8%AE%A8%E8%AE%BA%E4%BA%86%E8%BF%99%E4%BA%9B%E5%B7%A5%E5%85%B7%E5%9C%A8%E5%BA%94%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%8C%91%E6%88%98%E6%97%B6%E7%9A%84%E4%BC%98%E5%8A%BF%E5%92%8C%E4%B8%8D%E8%B6%B3%E3%80%82">https://mathllm.github.io/VoiceAssistantEval/上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22651v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音助手评估基准（VoiceAssistant-Eval）的提出，该基准旨在全面评估语音助手在听、说、视方面的能力。通过涵盖多种任务类别的10,497个样本，对开源模型和GPT-4o-Audio进行了评估，发现关键挑战在于多模态输入、角色语音模仿任务对于当前模型仍然具有难度，且稳健性和安全对齐仍存在显著差距。VoiceAssistant-Eval为评估和指导下一代语音助手的发展提供了严格框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音助手评估基准（VoiceAssistant-Eval）是一个用于全面评估语音助手能力的工具，涵盖听、说、视三个方面。</li>
<li>现有语音助手在多模态（音频+视觉）输入和角色模仿任务上存在挑战。</li>
<li>开源模型和专有模型表现不一，某些开源模型表现优异。</li>
<li>大多数模型擅长说话任务，但在音频理解方面存在不足。</li>
<li>设计良好的小型模型可以与大型模型竞争，如Step-Audio-2-mini（7B）在听力准确性上超越LLaMA-Omni2-32B-Bilingual。</li>
<li>语音助手在稳健性和安全对齐方面仍存在显著差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22651v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs"><a href="#Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs" class="headerlink" title="Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal   LLMs"></a>Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal   LLMs</h2><p><strong>Authors:Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch</strong></p>
<p>Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension – whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated – has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation. </p>
<blockquote>
<p>人类能否识别AI生成的（虚假）视频并提供明确的原因？尽管视频生成模型已经迅速发展，但一个关键方面——人类是否能在生成的视频中检测深迹，即时空定位的视觉伪迹，这些伪迹揭示了视频是机器生成的——一直被忽视。我们引入了DeeptraceReward，这是第一个精细粒度的、空间和时间感知的基准测试，它为视频生成奖励标注了人类感知的虚假痕迹。该数据集包含3300个高质量生成视频的4300个详细标注。每个标注都提供了自然语言解释，确定了包含感知轨迹的边界框区域，并标记了精确的开始和结束时间戳。我们将这些注释整合为9大类深迹，这些迹象会导致人类将视频识别为AI生成的，并训练多模态语言模型作为奖励模型，以模仿人类的判断和定位。在DeeptraceReward上，我们的7B奖励模型在虚假线索识别、定位和解等方面平均优于GPT-5达34.7%。有趣的是，我们观察到了一个持续的难度梯度：二元虚假与现实分类比精细粒度的深迹检测要容易得多；在后者中，从自然语言解释（最容易）到空间定位再到时间标签（最难）性能逐渐下降。通过突出人类感知的深迹，DeeptraceReward为具有社会意识和可信度的视频生成提供了严格的测试平台和训练信号。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22646v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://deeptracereward.github.io/">https://deeptracereward.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了DeeptraceReward数据集，这是首个针对视频生成奖励的精细粒度、时空感知的基准测试。该数据集包含了人类感知到的视频中的虚假痕迹，涵盖广泛的高质量生成视频，并对每个标注进行了详细的自然语言解释、边界框定位和精确的时间戳标记。文章还介绍了训练的多模态语言模型作为奖励模型，该模型可以模仿人类的判断和定位能力。实验表明，该模型在DeeptraceReward上的性能优于GPT-5。此外，文章也指出鉴定难度等级呈现一致趋势：与真实视频的二元鉴定相对简单，而在精细粒度的深度伪造痕迹检测中，从自然语言解释到空间定位再到时间标签的难度逐渐上升。整体而言，DeeptraceReward数据集为社会意识强的视频生成提供严格测试和训练信号，是测试模型稳定性和公正性的有效工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新的数据集DeeptraceReward，用于评估模型对视频生成奖励的精细粒度识别能力。</li>
<li>数据集包含人类感知到的视频中的虚假痕迹标注信息，包括详细的自然语言解释、边界框定位和精确时间戳标记。</li>
<li>训练的多模态语言模型作为奖励模型可以模仿人类的判断和定位能力，性能优于GPT-5。</li>
<li>文章指出了鉴定难度等级的一致趋势，从二元鉴定到精细粒度的深度伪造痕迹检测难度逐渐增加。其中自然语言解释相对容易，而时空定位最难。</li>
<li>DeeptraceReward数据集是社会意识强的视频生成的测试平台，对于训练稳健且公正的视频生成模型至关重要。数据集能够提供社会认可的基准测试和标准。通过测试模型的准确性来评估其对人类感知的深度伪造痕迹的识别能力。这对于确保视频生成技术的可靠性和可信度至关重要。同时，该数据集还可以为设计更为公正的视频生成模型提供指导，减少深度伪造等技术在社交环境中的应用带来的潜在风险和问题。同时数据集的建立还进一步推动了视频生成领域的发展，对于评估视频生成模型的性能和发展方向具有重要意义。数据集的设计思路和实验方法也为未来研究提供了有价值的参考和启示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22646v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning"><a href="#WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning" class="headerlink" title="WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning"></a>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning</h2><p><strong>Authors:Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model’s website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7. </p>
<blockquote>
<p>由大型语言模型（LLM）驱动的智能系统已在仓库级别的代码生成任务上表现出令人印象深刻的性能。然而，对于严重依赖于视觉效果和用户交互反馈的任务（如网站代码库生成），当前的代码代理仅依靠简单的代码执行进行反馈和验证。这种方法无法捕捉生成代码的实际质量。在本文中，我们提出了一种新型网站生成代理——WebGen-Agent，它利用全面、多层次的视觉反馈来迭代生成和完善网站代码库。通过视觉语言模型（VLM）生成关于网站截图和GUI代理测试的详细且描述性的文本描述和建议，以及量化其质量的分数。截图和GUI代理分数进一步与回溯和选择最佳机制相结合，增强了代理的性能。利用WebGen-Agent工作流程中固有的精确视觉分数，我们进一步引入了\textit{带有截图和GUI代理反馈的Step-GRPO}，以提高LLM作为WebGen-Agent推理引擎的能力。将每一步的截图和GUI代理分数作为Step-GRPO中的奖励，我们提供了一个密集且可靠的过程监督信号，这有效地提高了模型生成网站的能力。在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确性从26.4%提高到51.9%，外观评分从3.0提高到3.9，超越了之前的先进代理系统。此外，我们的Step-GRPO训练方法将Qwen2.5-Coder-7B-Instruct的准确性从38.9%提高到45.4%，外观评分从3.4提高到3.7。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22644v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>基于大语言模型的Agent系统已经在仓库级的代码生成任务上展现了令人印象深刻的表现。但对于依赖视觉效果和用户交互反馈的网站代码生成任务，当前代码Agent仅依赖简单的代码执行进行反馈和验证，无法捕捉生成代码的实际质量。为此，本文提出了WebGen-Agent，一个利用全面、多层次的视觉反馈来迭代生成和精炼网站代码库的新型网站生成Agent。结合视觉语言模型生成详细的文本描述和建议，以及与截图和GUI-agent测试相关的得分，WebGen-Agent提高了性能。同时，本文还介绍了结合截图和GUI-agent反馈的Step-GRPO方法，提高了LLM作为WebGen-Agent推理引擎的能力。在WebGen-Bench数据集上，WebGen-Agent的引入使Claude-3.5-Sonnet的准确率从26.4%提高到51.9%，外观得分从3.0提高到3.9，超越了现有的最先进的Agent系统。此外，Step-GRPO训练方法的采用也提高了Qwen2.5-Coder-7B-Instruct的准确率和外观得分。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>基于大语言模型的Agent系统在仓库级代码生成任务上表现出色，但在依赖视觉效果和用户交互反馈的网站代码生成任务上存在不足。</li>
<li>WebGen-Agent利用全面、多层次的视觉反馈来迭代生成和精炼网站代码库，提高了性能。</li>
<li>WebGen-Agent结合视觉语言模型生成详细的文本描述和建议，以及与截图和GUI-agent测试相关的得分。</li>
<li>Step-GRPO方法结合截图和GUI-agent反馈，提高了LLM作为WebGen-Agent推理引擎的能力。</li>
<li>在WebGen-Bench数据集上，WebGen-Agent显著提高了现有Agent系统的性能。</li>
<li>WebGen-Agent的引入显著提高了网站生成的准确率和外观得分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22644v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning"><a href="#UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning" class="headerlink" title="UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning"></a>UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning</h2><p><strong>Authors:Hongyu Chen, Guangrun Wang</strong></p>
<p>Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism. </p>
<blockquote>
<p>链式思维（CoT）提示改进大型语言模型（LLM）的推理能力，但其依赖于非结构化文本，限制了其在实体任务中的可解释性和可执行性。先前的工作已经探索了使用场景或逻辑图的结构化CoT，但这些方法仍存在根本性限制：它们只模拟低阶关系，缺乏继承或行为抽象等结构，并且不提供用于顺序或条件规划的标准化语义。我们提出UML-CoT，这是一个结构化推理和规划框架，它利用统一建模语言（UML）生成符号化CoT和可执行行动计划。UML类图捕获组合对象语义，而活动图模拟过程控制流。我们的三阶段训练管道将监督微调与群体相对策略优化（GRPO）相结合，包括仅从答案数据中学习奖励。我们在新的杂乱房间清洁场景基准测试MRoom-3 结中进行评估。UML-CoT在可解释性、规划连贯性和执行成功方面优于非结构化CoT，突显了UML作为更具表现力和可操作性的结构化推理形式系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）中的链式思维（CoT）提示改善了模型的推理能力，但其依赖于非结构化文本的特性限制了其在实体任务中的可解释性和可执行性。本研究提出利用统一建模语言（UML）构建结构化CoT和可执行行动计划的UML-CoT框架。通过UML类图捕捉组成对象的语义信息，而活动图则模拟程序控制流。结合监督微调与相对组策略优化（GRPO）的三阶段训练管道，通过仅答案数据进行奖励学习。在新的混乱房间清洁场景基准测试MRoom-30k上的评估显示，UML-CoT在可解释性、规划连贯性和执行成功率方面优于非结构化CoT，凸显出UML作为一种更具表现力和可操作性的结构化推理形式的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>链式思维（CoT）在大型语言模型（LLM）中增强了推理能力，但面临可解释性和执行性的挑战。</li>
<li>现有结构化的CoT方法如场景或逻辑图存在局限性，仅建模低阶关系，缺乏如继承和行为抽象等构造，以及为顺序或条件规划提供标准化语义的缺失。</li>
<li>提出了UML-CoT框架，利用统一建模语言（UML）生成符号CoT和可执行行动计划。</li>
<li>UML类图用于捕捉组成对象的语义信息，而活动图则模拟程序控制流。</li>
<li>三阶段训练管道包括监督微调与相对组策略优化（GRPO）。</li>
<li>奖励学习是通过仅答案数据进行。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22628v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ArabJobs-A-Multinational-Corpus-of-Arabic-Job-Ads"><a href="#ArabJobs-A-Multinational-Corpus-of-Arabic-Job-Ads" class="headerlink" title="ArabJobs: A Multinational Corpus of Arabic Job Ads"></a>ArabJobs: A Multinational Corpus of Arabic Job Ads</h2><p><strong>Authors:Mo El-Haj</strong></p>
<p>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/drelhaj/ArabJobs">https://github.com/drelhaj/ArabJobs</a>. </p>
<blockquote>
<p>ArabJobs是一个公开可用的阿拉伯语招聘广告语料库，该语料库收集了来自埃及、约旦、沙特阿拉伯和阿拉伯联合酋长国的招聘广告。数据集包含超过8500个帖子和超过55万个单词，反映了阿拉伯劳动力市场的语言、地域和社会经济差异。我们对性别代表和职业结构进行了分析，并强调了广告中的方言差异，这为未来的研究提供了机会。我们还展示了使用大型语言模型进行工资估算和职业类别归一化的应用程序，以及用于性别偏见检测和职业分类的基准任务。研究结果表明，ArabJobs对于注重公平性的阿拉伯语自然语言处理和劳动力市场研究具有实用价值。数据集已在GitHub上公开提供：<a target="_blank" rel="noopener" href="https://github.com/drelhaj/ArabJobs%E3%80%82">https://github.com/drelhaj/ArabJobs。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>阿拉伯就业语料库（ArabJobs）是一个公开可用的阿拉伯语招聘广告集合，涵盖了埃及、约旦、沙特阿拉伯和阿联酋的数据。该数据集包含超过8,500条招聘广告和超过55万个单词，展现了阿拉伯劳动力市场的语言、地域和社会经济差异。通过对性别代表和职业结构的分析，以及广告中方言差异的强调，该数据集为未来的研究提供了机会。此外，还展示了用于薪酬估算和职业类别归一化的大型语言模型的应用，以及用于性别偏见检测和职业分类的基准任务。研究结果表明，阿拉伯就业语料库对公平感知的阿拉伯语自然语言处理和劳动力市场研究具有实用性。数据集已在GitHub上公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>阿拉伯就业语料库（ArabJobs）是一个包含超过8,500条招聘广告的公开数据集，涉及埃及、约旦、沙特阿拉伯和阿联酋的阿拉伯语招聘广告。</li>
<li>数据集展示了阿拉伯劳动力市场的语言、地域和社会经济差异。</li>
<li>分析了数据集中的性别代表和职业结构，强调了广告中的方言差异，为未来研究提供了机会。</li>
<li>利用大型语言模型进行了薪酬估算和职业类别归一化的应用展示。</li>
<li>提供了用于性别偏见检测和职业分类的基准任务。</li>
<li>阿拉伯就业语料库对公平感知的阿拉伯语自然语言处理和劳动力市场研究具有实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22589v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Detection-of-Context-Grounded-Hallucinations-Using-LLMs"><a href="#Fine-Grained-Detection-of-Context-Grounded-Hallucinations-Using-LLMs" class="headerlink" title="Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs"></a>Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs</h2><p><strong>Authors:Yehonatan Pesiakhovsky, Zorik Gekhman, Yosi Mass, Liat Ein-Dor, Roi Reichart</strong></p>
<p>Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmark’s difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the model’s parametric knowledge. </p>
<blockquote>
<p>上下文相关的幻觉是模型输出中包含无法根据源文本进行验证的信息的情况。我们研究LLM在此类幻觉定位方面的适用性，作为现有复杂评估流程的更实用的替代方案。在缺乏用于元评估幻觉定位的基准测试的情况下，我们构建了一个针对LLM的基准测试，其中包括对1000多个示例具有挑战性的人工标注。除了基准测试外，我们还制定了基于LLM的评估协议，并通过人工评估验证了其质量。由于现有的幻觉表示限制了可以表达的错误类型，因此我们提出了一种基于自由形式文本描述的新表示方法，可以捕获所有可能的错误范围。我们进行了全面的研究，评估了四个大规模LLM，突出了基准测试的困难，因为最佳模型的F1分数仅为0.67。通过认真分析，我们提供了完成此任务的最佳提示策略，并确定了使LLM面临挑战的主要因素：（1）尽管已指示仅检查输出中的事实，但仍错误地将缺失的细节标记为不一致；（2）输出中包含事实正确的信息但由于与模型的参数知识对齐而缺失源文本中的信息，因此无法验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）产生的输出中存在无法验证源文本的信息，即上下文相关的幻觉。研究LLM在定位此类幻觉方面的适用性，作为现有复杂评估管道的实用替代方案。在没有为幻觉定位进行元评价的基准测试的情况下，我们构建了一个针对LLM的基准测试，涉及超过1000个具有挑战性的例子的人类注释。该基准测试还包含基于LLM的评估协议，并通过人类评估验证其质量。由于现有的幻觉表示限制了可表达错误类型，因此我们提出了一种基于自由形式文本描述的新表示形式，以捕捉可能出现的所有错误。我们对四项大规模LLM进行了全面的研究，突出该基准测试的困难性，因为最佳模型的F1分数仅为0.67。通过仔细分析，我们提供了关于任务的最优提示策略，并确定了使LLM面临挑战的主要因素：（1）错误地将缺失的细节标记为不一致的倾向，尽管指令要求仅检查输出中的事实；（2）对于因参数知识与源文本不符而导致的输出中包含的事实上正确的信息——因此无法验证——的困难。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs会产生上下文相关的幻觉，即模型输出中包含无法验证源文本的信息。</li>
<li>研究了LLMs在定位这类幻觉方面的适用性，并作为一种实用的评估方法。</li>
<li>创建了一个针对LLMs的基准测试，包含超过1000个例子的人类注释，以评估幻觉定位。</li>
<li>提出了基于自由形式文本描述的新表示方法，以更全面地捕捉错误类型。</li>
<li>最佳LLM模型在基准测试中的F1分数为0.67，表明任务的挑战性。</li>
<li>LLMs在定位幻觉时面临的主要挑战包括：错误地将缺失的细节标记为不一致，以及处理因模型参数知识与源文本不符而导致的不可验证的输出信息。</li>
<li>通过分析，提供了针对该任务的最优提示策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22582v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time"><a href="#Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time" class="headerlink" title="Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time"></a>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time</h2><p><strong>Authors:Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</strong></p>
<p>Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning. </p>
<blockquote>
<p>测试时缩放（TTS）通过推理过程中的额外计算，增强了大型语言模型（LLM）的推理能力。然而，现有方法主要依赖于输出级别的采样，而忽略了模型架构的作用。在主流的专家混合（MoE）LLM中，我们发现改变激活的专家数量会产生具有稳定准确性的互补解决方案集，揭示了一种新的且尚未被充分探索的多样性来源。受这一观察结果的启发，我们提出了动态专家搜索（DES），这是一种将专家激活提升为搜索空间的可控维度的TTS策略。DES集成了两个关键组件：（1）动态MoE，它能够在推理过程中直接控制专家数量，生成多样化的推理轨迹，无需额外成本；（2）专家配置继承，它能够在推理路径内保持一致的专家数量，同时在各运行之间变化专家数量，从而在搜索过程中平衡稳定性和多样性。在MoE架构、验证器和推理基准测试（即数学、代码和知识）上的广泛实验表明，DES可靠地优于TTS基线，提高了准确性和稳定性，且无需额外成本。这些结果强调了DES作为一种实用且可扩展的架构感知TTS形式，展示了现代LLM中的结构灵活性如何推动推理能力的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22572v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>测试时缩放（TTS）通过推理过程中的额外计算来提升大型语言模型（LLM）的推理能力。然而，现有方法主要依赖于输出级别的采样，而忽视了模型架构的作用。本文提出动态专家搜索（DES）策略，将专家激活作为搜索空间的可控维度。DES包括两个关键组件：动态MoE和专家配置继承。动态MoE能够在推理过程中直接控制专家数量，生成多样化的推理轨迹，无需额外成本；专家配置继承则能在推理路径中保持一致的专家数量，同时在各运行中变化专家数量，从而平衡稳定性和多样性。实验表明，DES在MoE架构、验证器和推理基准测试（如数学、代码和知识）上可靠地优于TTS基线，提高了准确性和稳定性，且无需额外成本。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>测试时缩放（TTS）可以增强大型语言模型（LLM）的推理能力。</li>
<li>现有TTS方法主要关注输出级别的采样，而忽视了模型架构的重要性。</li>
<li>动态专家搜索（DES）策略提出将专家激活作为搜索空间的可控维度，以提高推理能力。</li>
<li>DES包括动态MoE和专家配置继承两个关键组件。</li>
<li>动态MoE能够在推理过程中直接控制专家数量，生成多样化轨迹，无需额外成本。</li>
<li>专家配置继承能够平衡推理路径的稳定性和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22572">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22572v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-Augmented-and-Fair-Machine-Learning-Framework-for-University-Admission-Prediction"><a href="#LLM-Augmented-and-Fair-Machine-Learning-Framework-for-University-Admission-Prediction" class="headerlink" title="LLM-Augmented and Fair Machine Learning Framework for University   Admission Prediction"></a>LLM-Augmented and Fair Machine Learning Framework for University   Admission Prediction</h2><p><strong>Authors:Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Dahlia Mansoor, Wathiq Mansoor</strong></p>
<p>Universities face surging applications and heightened expectations for fairness, making accurate admission prediction increasingly vital. This work presents a comprehensive framework that fuses machine learning, deep learning, and large language model techniques to combine structured academic and demographic variables with unstructured text signals. Drawing on more than 2,000 student records, the study benchmarks logistic regression, Naive Bayes, random forests, deep neural networks, and a stacked ensemble. Logistic regression offers a strong, interpretable baseline at 89.5% accuracy, while the stacked ensemble achieves the best performance at 91.0%, with Naive Bayes and random forests close behind. To probe text integration, GPT-4-simulated evaluations of personal statements are added as features, yielding modest gains but demonstrating feasibility for authentic essays and recommendation letters. Transparency is ensured through feature-importance visualizations and fairness audits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11% gap by parental education, underscoring the need for continued monitoring. The framework is interpretable, fairness-aware, and deployable. </p>
<blockquote>
<p>大学面临着申请人数激增和对公平性越来越高的期望，因此准确的招生预测变得至关重要。本研究提出了一个综合框架，融合了机器学习、深度学习和大型语言模型技术，将结构化的学术和人口统计变量与非结构化的文本信号相结合。该研究以超过2000份学生记录为基准，对比了逻辑回归、朴素贝叶斯、随机森林、深度神经网络和堆叠集成等方法。逻辑回归提供了强大且可解释的基线，准确率为89.5%，而堆叠集成取得了最佳性能，准确率为91.0%，朴素贝叶斯和随机森林紧随其后。为了探究文本集成，增加了GPT-4模拟的个人陈述评估特征，虽然只带来了适度的提升，但证明了在真实作文和推荐信中的可行性。通过特征重要性可视化和公平性审计来确保透明度。审计结果显示存在9%的性别差距（男性67%对女性76%）和11%的父系教育差距，这突显了持续监测的必要性。该框架具有可解释性、公平意识和可部署性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22560v1">PDF</a> The 9th International Symposium on Multidisciplinary Studies and   Innovative Technologies (ISMSIT)</p>
<p><strong>Summary</strong></p>
<p>本文提出一个综合框架，融合机器学习、深度学习和大型语言模型技术，结合结构化学术和人口统计变量与无结构化文本信号，以进行准确的大学入学预测。研究使用超过2000名学生记录，对比了逻辑回归、朴素贝叶斯、随机森林、深度神经网络和堆叠集成等方法。逻辑回归提供89.5%准确率的强大可解释基线，而堆叠集成取得最佳性能为91.0%，朴素贝叶斯和随机森林表现次之。同时利用GPT-4模拟个人陈述评估以增强文本集成能力。透明度通过特征重要性可视化和公平性审计得到保障。审计显示存在性别差距和父母教育差距，突显持续监测的必要性。此框架具有可解释性、公平性和可部署性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的框架结合了机器学习、深度学习和大型语言模型技术，用于提高大学入学预测的准确性。</li>
<li>通过使用超过2000名学生记录进行实证研究，对比了多种预测方法的性能。</li>
<li>逻辑回归提供了一个可解释的基线，而堆叠集成方法取得了最佳性能。</li>
<li>利用GPT-4模拟评估文本集成能力，增强了预测模型的性能。</li>
<li>模型具有透明度，通过特征重要性可视化展示。</li>
<li>公平性审计显示存在性别和父母教育的差距，强调持续监测的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22560">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22560v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models"><a href="#StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models" class="headerlink" title="StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models"></a>StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models</h2><p><strong>Authors:Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge</strong></p>
<p>Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）在解决运筹学（OR）问题方面展现出有前景的能力。虽然强化学习作为运筹问题上训练LLM的强大范式，但现有工作通常面临两个主要局限性。首先，结果奖励会受到学分分配问题的影响，其中正确的最终答案可能会强化错误的推理。其次，传统的判别过程监督是短视的，无法全面评估运筹建模的相互依赖步骤。为此，我们引入了StepORLM，这是一种新型的自进化框架，具有生成过程监督。StepORLM的核心是一个协同进化循环，其中策略模型和生成过程奖励模型（GenPRM）可以相互迭代改进。这个循环是由双重反馈机制驱动的：来自外部求解器的基于结果的确定性验证，以及来自GenPRM的细致全面过程评估。结合信号用于通过加权直接偏好优化（W-DPO）对齐策略，同时改进GenPRM。我们得到的8B参数StepORLM在六个基准测试上达到了最新水平，显著优于更大的通用模型、自主方法和专用基准测试。此外，协同进化的GenPRM能够充当强大且通用的过程验证器，大大提高了我们自己的模型和其他现有LLM的推理扩展性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在解决运筹学（OR）问题方面展现出巨大潜力。强化学习是训练LLM解决OR问题的强大范式，但现有研究面临两个主要局限：成果奖励存在信用分配问题，即正确的最终答案可能强化错误的推理；传统判别过程监督存在视野狭隘问题，无法全面评估OR建模的相互依赖步骤。为此，我们提出了StepORLM，一种新型自我进化的框架，具有生成过程监督功能。StepORLM的核心是一个协同进化循环，策略模型和生成过程奖励模型（GenPRM）相互迭代改进。该循环由双重反馈机制驱动：来自外部求解器的确定性、结果为基础的验证，以及来自GenPRM的微妙、全面过程评估。结合信号用于通过加权直接偏好优化（W-DPO）对齐策略，并同时优化GenPRM。我们的8B参数StepORLM在六个基准测试上达到最新水平，显著优于更大的通用模型、智能方法和专用基准测试。此外，协同进化的GenPRM能够充当强大而通用的过程验证器，大幅提升我们自己的模型和其他现有LLM的推理扩展性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs展现出解决OR问题的潜力。</li>
<li>现有强化学习方法在解决OR问题时存在两个主要局限：成果奖励的信用分配问题和传统判别过程监督的视野狭隘问题。</li>
<li>StepORLM框架通过引入生成过程监督来解决这些问题。</li>
<li>StepORLM的核心是一个协同进化循环，包括策略模型和生成过程奖励模型的迭代改进。</li>
<li>双重反馈机制包括外部求解器的结果验证和GenPRM的过程评估。</li>
<li>提出的8B参数StepORLM在多个基准测试上表现出卓越性能，优于其他模型和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22558">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22558v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Linear-Causal-Representation-Learning-by-Topological-Ordering-Pruning-and-Disentanglement"><a href="#Linear-Causal-Representation-Learning-by-Topological-Ordering-Pruning-and-Disentanglement" class="headerlink" title="Linear Causal Representation Learning by Topological Ordering, Pruning,   and Disentanglement"></a>Linear Causal Representation Learning by Topological Ordering, Pruning,   and Disentanglement</h2><p><strong>Authors:Hao Chen, Lin Liu, Yu Guang Wang</strong></p>
<p>Causal representation learning (CRL) has garnered increasing interests from the causal inference and artificial intelligence community, due to its capability of disentangling potentially complex data-generating mechanism into causally interpretable latent features, by leveraging the heterogeneity of modern datasets. In this paper, we further contribute to the CRL literature, by focusing on the stylized linear structural causal model over the latent features and assuming a linear mixing function that maps latent features to the observed data or measurements. Existing linear CRL methods often rely on stringent assumptions, such as accessibility to single-node interventional data or restrictive distributional constraints on latent features and exogenous measurement noise. However, these prerequisites can be challenging to satisfy in certain scenarios. In this work, we propose a novel linear CRL algorithm that, unlike most existing linear CRL methods, operates under weaker assumptions about environment heterogeneity and data-generating distributions while still recovering latent causal features up to an equivalence class. We further validate our new algorithm via synthetic experiments and an interpretability analysis of large language models (LLMs), demonstrating both its superiority over competing methods in finite samples and its potential in integrating causality into AI. </p>
<blockquote>
<p>因果表示学习（CRL）因其能够利用现代数据集的异质性，将潜在复杂的数据生成机制转化为因果可解释的潜在特征，而受到因果推理和人工智能领域的广泛关注。在本文中，我们关注于潜在特征的线性结构化因果模型，并假设存在一个线性混合函数，将潜在特征映射到观测数据或测量值。现有的线性CRL方法通常依赖于严格的假设，如可访问单个节点的干预数据或对潜在特征和外在测量噪声的分布约束。然而，在某些场景中，这些先决条件可能难以满足。在这项工作中，我们提出了一种新的线性CRL算法，与大多数现有线性CRL方法不同，它在环境异质性和数据生成分布的假设更弱的情况下，仍然能够恢复等价类中的潜在因果特征。我们进一步通过合成实验和对大型语言模型（LLM）的可解释性分析验证了我们的新算法，证明了它在有限样本中优于竞争对手，并且在将因果性融入人工智能中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注因果表示学习（CRL）领域，针对线性结构因果模型进行研究。现有线性CRL方法依赖严格假设，如单节点干预数据的可访问性或对潜在特征和外来测量噪声的分布约束。然而，这些先决条件在某些场景下可能难以满足。本文提出了一种新型线性CRL算法，该算法在环境异质性和数据生成分布方面的假设较弱，但仍能恢复等价类中的潜在因果特征。通过合成实验和对大型语言模型（LLM）的可解释性分析，验证了该算法在有限样本上的优越性，以及其在人工智能中融入因果关系的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>因果表示学习（CRL）能解析复杂数据生成机制为因果可解释的潜在特征，利用现代数据集的异质性。</li>
<li>现有线性CRL方法通常依赖严格假设，如单节点干预数据的可访问性和潜在特征分布的限制性约束。</li>
<li>本文提出了一种新型线性CRL算法，可在环境异质性和数据生成分布方面的假设较弱的情况下运行。</li>
<li>新算法能恢复等价类中的潜在因果特征。</li>
<li>通过合成实验，验证了该算法在有限样本上的优越性。</li>
<li>对大型语言模型（LLM）进行可解释性分析，展示了该算法在人工智能中的潜在应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22553v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22553v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JanusVLN-Decoupling-Semantics-and-Spatiality-with-Dual-Implicit-Memory-for-Vision-Language-Navigation"><a href="#JanusVLN-Decoupling-Semantics-and-Spatiality-with-Dual-Implicit-Memory-for-Vision-Language-Navigation" class="headerlink" title="JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation"></a>JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation</h2><p><strong>Authors:Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei</strong></p>
<p>Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain’s semantic understanding and the right brain’s spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: <a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/">https://miv-xjtu.github.io/JanusVLN.github.io/</a>. </p>
<blockquote>
<p>视觉与语言导航要求一个实体代理在未知的通过自然语言指令和连续视频流的引导下进行导航。视觉与语言导航的近期进展得益于多模态大型语言模型的强大语义理解。然而，这些方法通常依赖于明确的语义记忆，如建立文本认知地图或存储历史视觉帧。这种方法存在空间信息丢失、计算冗余和内存膨胀等问题，阻碍了高效导航。受到人类导航中隐式场景表示的启发，类似于左脑的语义理解和右脑的空间认知，我们提出了JanusVLN，这是一种新型的视觉与语言导航框架，具有双隐神经记忆，将空间几何记忆和视觉语义记忆建模为单独、紧凑和固定大小的神经表示。该框架首先扩展了MLLM，以融入空间几何编码器的3D先验知识，从而增强仅基于RGB输入的模型的空间推理能力。然后，构建空间几何和视觉语义编码器的历史键值缓存作为双隐记忆。通过仅保留初始和滑动窗口中的令牌键值，避免了冗余计算，实现了高效增量更新。大量实验表明，JanusVLN在超过20种最新方法上表现出卓越的性能。例如，与使用多种数据类型作为输入的方法相比，成功率提高了10.5-35.5%；与使用更多RGB训练数据的方法相比，提高了3.6-10.8%。这表明所提出的双隐神经记忆作为一种新范式，为未来的视觉与语言导航研究探索了有希望的新方向。我们的项目页面：<a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/%E3%80%82">https://miv-xjtu.github.io/JanusVLN.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22548v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://miv-xjtu.github.io/JanusVLN.github.io/">https://miv-xjtu.github.io/JanusVLN.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision-and-Language Navigation（VLN）的新框架JanusVLN。该框架采用双隐神经内存，将空间几何和视觉语义内存建模为单独的、紧凑的、固定大小的神经表示。它结合了Multimodal Large Language Model（MLLM）和3D先验知识，提高了仅基于RGB输入的模型的空间推理能力。实验表明，JanusVLN在VLN任务上实现了超越20种最新方法的性能，展示了其在未来VLN研究中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-and-Language Navigation（VLN）需要智能体通过自然语言指令和连续视频流在未见过的环境中进行导航。</li>
<li>近期VLN的进展得益于Multimodal Large Language Models（MLLM）的强大语义理解。</li>
<li>当前方法通常依赖显式语义记忆，如构建文本认知地图或存储历史视觉帧，但存在空间信息损失、计算冗余和记忆膨胀等问题。</li>
<li>JanusVLN框架采用双隐神经记忆，对空间几何和视觉语义内存进行建模，避免冗余计算并实现高效增量更新。</li>
<li>JanusVLN结合3D先验知识提高模型的空间推理能力。</li>
<li>JanusVLN实现了超越20种最新方法的性能，表明其在VLN任务上的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22548">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22548v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="The-InviTE-Corpus-Annotating-Invectives-in-Tudor-English-Texts-for-Computational-Modeling"><a href="#The-InviTE-Corpus-Annotating-Invectives-in-Tudor-English-Texts-for-Computational-Modeling" class="headerlink" title="The InviTE Corpus: Annotating Invectives in Tudor English Texts for   Computational Modeling"></a>The InviTE Corpus: Annotating Invectives in Tudor English Texts for   Computational Modeling</h2><p><strong>Authors:Sophie Spliethoff, Sanne Hoeken, Silke Schwandt, Sina Zarrieß, Özge Alaçam</strong></p>
<p>In this paper, we aim at the application of Natural Language Processing (NLP) techniques to historical research endeavors, particularly addressing the study of religious invectives in the context of the Protestant Reformation in Tudor England. We outline a workflow spanning from raw data, through pre-processing and data selection, to an iterative annotation process. As a result, we introduce the InviTE corpus – a corpus of almost 2000 Early Modern English (EModE) sentences, which are enriched with expert annotations regarding invective language throughout 16th-century England. Subsequently, we assess and compare the performance of fine-tuned BERT-based models and zero-shot prompted instruction-tuned large language models (LLMs), which highlights the superiority of models pre-trained on historical data and fine-tuned to invective detection. </p>
<blockquote>
<p>本文旨在将自然语言处理（NLP）技术应用于历史研究领域，特别是针对都铎王朝英格兰时代宗教诽谤的研究。我们概述了一个从原始数据开始，经过预处理和数据选择，再到迭代注释过程的工作流程。因此，我们引入了InviTE语料库——一个包含近两千个早期现代英语（EModE）句子的语料库，这些句子都经过了关于十六世纪英格兰诽谤语言的专家注释。随后，我们评估和比较了经过微调基于BERT的模型和零样本提示指令调整的大型语言模型（LLM）的性能，这突显了预先在历史数据上进行训练并针对诽谤检测进行微调模型的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22345v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本主要介绍了自然语言处理技术在历史研究领域的应用，特别是在研究都铎王朝英格兰时期的宗教诽谤语方面的应用。文章描述了一个从原始数据到预处理、数据选择，再到迭代标注的工作流程，并介绍了InviTE语料库的开发与应用。同时，文章还评估了微调后的BERT模型和零样本提示指令训练的大型语言模型（LLM）的性能，强调了对历史数据进行预训练和针对诽谤语检测进行微调的重要性。总之，文章展现了一项创新的融合多学科研究方法的应用，该研究方法借助机器学习工具解决了历史上真实问题的困难性挑战，不仅能帮助学者解决挑战更可促进学术交流和创新研究的融合和进一步发展。这篇文章还将NLP在历史文献领域中的未来趋势展望给予展现和启迪。一个重点是通过使用机器学习技术来开发语料库，并评估不同模型在处理历史语言方面的性能。这些模型的应用有助于揭示历史上语言使用的复杂性和演变过程。整体来看本文面向相关领域研究者提出了一个对于如何处理语言标注并设计评估流程的建议方案。此外也提供了宝贵的语料库资源，为后续研究提供了极大的便利。该研究的成功实践对于未来跨学科合作以及机器学习在历史文献研究中的应用具有深远影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言处理技术被应用于历史研究领域，特别是都铎王朝英格兰时期的宗教诽谤语研究。</li>
<li>开发了一个名为InviTE的语料库，包含近2000个早期现代英语的句子，并进行了关于诽谤语言的专家标注。</li>
<li>评估了微调后的BERT模型和零样本提示指令训练的大型语言模型（LLM）的性能。</li>
<li>强调了预训练在历史数据上的模型以及对诽谤语检测进行微调的重要性。</li>
<li>NLP技术在历史文献研究中的应用有助于揭示历史上语言使用的复杂性和演变过程。</li>
<li>该研究提供了一个处理语言标注和评估流程的建议方案，为相关领域研究者提供了宝贵的语料库资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22345">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22345v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Teaching-Transformers-to-Solve-Combinatorial-Problems-through-Efficient-Trial-Error"><a href="#Teaching-Transformers-to-Solve-Combinatorial-Problems-through-Efficient-Trial-Error" class="headerlink" title="Teaching Transformers to Solve Combinatorial Problems through Efficient   Trial &amp; Error"></a>Teaching Transformers to Solve Combinatorial Problems through Efficient   Trial &amp; Error</h2><p><strong>Authors:Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos</strong></p>
<p>Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization. </p>
<blockquote>
<p>尽管大型语言模型（LLM）在各种语言任务中表现出色，但在组合问题（如可满足性问题、旅行商问题，甚至基本算术）方面仍存在困难。我们通过一种解决NP类问题的新型方法来解决这一差距。我们专注于典型的数独任务，与先前的神经符号方法相比，实现了最先进的准确性（99%）。与以往使用自定义架构的工作不同，我们的方法采用普通的仅解码器转换器（GPT-2），无需使用外部工具或函数调用。我们的方法将简单数独规则的模仿学习与包含猜测和回溯的显式深度优先搜索（DFS）探索策略相结合。超越模仿学习，我们努力减少猜测次数，直至找到解决方案。我们对这一设置进行了严格的分析，将其形式化为Min-Sum Set Cover的上下文变体，这是算法和随机优化中的一个广受关注的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22023v1">PDF</a> </p>
<p><strong>Summary</strong>：尽管大型语言模型（LLM）在各种语言任务上表现出色，但在解决组合问题（如可满足性问题、旅行商问题等）或基本算术问题时仍存在困难。本文通过一种新方法解决NP类问题，并以经典的数独任务为焦点，实现了相较于先前神经符号方法的最新准确度（99%）。不同于以往使用定制架构的方法，我们的方法仅采用通用解码器转换器（GPT-2），无需外部工具或函数调用。该方法结合了数独简单规则的学习模仿与包含启发式猜测和回溯的深度优先搜索（DFS）探索策略。除了学习模仿，我们还力求减少猜测次数直至找到解决方案。本文对此设置进行了严格的分析，将其形式化为算法和随机优化中研究的Min-Sum Set Cover问题的上下文变体。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型（LLM）在解决组合问题（如Sudoku）和基本算术问题上存在困难。</li>
<li>本文通过一种新的方法解决了NP类问题，取得了在数独任务上的最新准确度（99%）。</li>
<li>与先前使用定制架构的方法不同，本文的方法仅采用GPT-2解码器。</li>
<li>该方法结合了数独规则的学习模仿与深度优先搜索（DFS）策略。</li>
<li>除了学习模仿，该方法还注重减少猜测次数直至找到解决方案。</li>
<li>本文设置与Min-Sum Set Cover问题有密切联系，为算法和随机优化领域提供了新视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22023">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.22023v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Low-Rank-Adaptation-with-Structured-Nonlinear-Transformations"><a href="#Enhancing-Low-Rank-Adaptation-with-Structured-Nonlinear-Transformations" class="headerlink" title="Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations"></a>Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations</h2><p><strong>Authors:Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning. </p>
<blockquote>
<p>低秩适应（LoRA）是一种广泛应用于大型语言模型的参数高效微调方法。然而，它的线性性质限制了其表现力。我们提出了LoRAN，这是LoRA的非线性扩展，它将对低秩更新进行轻量级转换。我们还介绍了基于正弦函数的激活函数Sinter，它在不增加参数计数的情况下添加了结构化扰动。跨越摘要和分类任务的实验表明，LoRAN在QLoRA上表现持续优异。消融研究表明，Sinter优于标准激活函数，如Sigmoid、ReLU和Tanh，突出了激活设计在低秩调整中的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21870v1">PDF</a> This manuscript has been submitted to IEEE Journal of Selected Topics   in Signal Processing (JSTSP) for review. Until the moment I submitted the   manuscript to arXiv, we haven’t received any review comments from JSTSP</p>
<p><strong>Summary</strong></p>
<p>LoRA方法是一种广泛应用于大型语言模型的参数高效微调方法，但其线性特性限制了表达能力。为此，我们提出了LoRAN，这是LoRA的非线性扩展，通过对低秩更新进行轻量级转换来提高模型性能。此外，我们还引入了基于正弦函数的激活函数Sinter，它增加了结构化扰动，同时不增加参数数量。实验表明，LoRAN在摘要和分类任务上均优于QLoRA。消融研究表明，Sinter在标准激活函数如Sigmoid、ReLU和Tanh中具有优越性，突显了激活设计在低秩调整中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRA是一种参数高效的微调方法，广泛应用于大型语言模型。</li>
<li>LoRAN是LoRA的非线性扩展，通过低秩更新的轻量级转换提高模型性能。</li>
<li>Sinter是一种基于正弦函数的激活函数，能够增加结构化扰动且不会增加参数数量。</li>
<li>实验表明，LoRAN在摘要和分类任务上的性能优于QLoRA。</li>
<li>消融研究证明了Sinter在标准激活函数中的优越性。</li>
<li>激活设计在低秩调整中非常重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21870">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21870v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DiTraj-training-free-trajectory-control-for-video-diffusion-transformer"><a href="#DiTraj-training-free-trajectory-control-for-video-diffusion-transformer" class="headerlink" title="DiTraj: training-free trajectory control for video diffusion transformer"></a>DiTraj: training-free trajectory control for video diffusion transformer</h2><p><strong>Authors:Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su, Zhicheng Zhao</strong></p>
<p>Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object’s trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens’ position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability. </p>
<blockquote>
<p>基于扩散转换器（DiT）的视频生成模型具有强大的生成能力，并且带有3D全注意力。轨迹控制代表了可控视频生成领域的一个用户友好型任务。然而，现有方法要么需要大量训练资源，要么专门为U-Net设计，没有充分利用DiT的卓越性能。为了解决这些问题，我们针对DiT提出了DiTraj，这是一个简单有效的无需训练的轨迹控制框架，用于文本到视频的生成。具体来说，首先，为了注入物体的轨迹，我们提出前景背景分离指导：我们使用大型语言模型（LLM）将用户提供的提示转换为前景和背景提示，分别指导视频中的前景和背景区域的生成。接着，我们分析了3D全注意力，并探索了令牌间注意力得分与位置嵌入之间的紧密关联。基于此，我们提出了跨帧时空解耦的3D-RoPE（STD-RoPE）。通过仅修改前景令牌的位置嵌入，STD-RoPE消除了跨帧的空间差异，加强了它们之间的跨帧注意力，从而提高了轨迹控制。此外，我们通过调节位置嵌入的密度实现了3D感知的轨迹控制。大量实验表明，我们的方法在视频质量和轨迹可控性方面都优于之前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21839v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Diffusion Transformers（DiT）的视频生成模型在3D全注意力下的强大生成能力。针对可控视频生成中的轨迹控制任务，提出了一种简单有效的免训练框架DiTraj，适用于DiT。通过前景背景分离指导、跨帧时空解耦3D-RoPE（STD-RoPE）等方法，实现了高质量的视频生成和精确的轨迹控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiT) 在视频生成领域具有强大的生成能力。</li>
<li>现有轨迹控制方法存在资源需求大或专为U-Net设计的问题，未能充分利用DiT的优势。</li>
<li>提出了一种针对DiT的免训练框架DiTraj，用于轨迹控制。</li>
<li>通过前景背景分离指导，将用户提示分为前景和背景提示，指导视频生成。</li>
<li>跨帧时空解耦的3D-RoPE（STD-RoPE）方法通过修改前景标记的位置嵌入，提高了轨迹控制效果。</li>
<li>通过调节位置嵌入的密度实现3D感知轨迹控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21839">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.21839v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learning-to-Summarize-by-Learning-to-Quiz-Adversarial-Agentic-Collaboration-for-Long-Document-Summarization"><a href="#Learning-to-Summarize-by-Learning-to-Quiz-Adversarial-Agentic-Collaboration-for-Long-Document-Summarization" class="headerlink" title="Learning to Summarize by Learning to Quiz: Adversarial Agentic   Collaboration for Long Document Summarization"></a>Learning to Summarize by Learning to Quiz: Adversarial Agentic   Collaboration for Long Document Summarization</h2><p><strong>Authors:Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</strong></p>
<p>Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality. </p>
<blockquote>
<p>长文档摘要对于当前的大型语言模型（LLM）来说仍然是一个巨大的挑战，因为现有的方法在处理超长文档时通常会面临信息丢失、事实不一致和连贯性问题。我们提出了SummQ，一种新型对抗性多智能体框架，它通过两个互补领域中的智能体之间的协作智能来解决这些局限性：摘要和测验。我们的方法采用摘要生成器和评审者进行协作以创建和评估全面的摘要，而测验生成器和评审者则创建理解问题，作为摘要过程的持续质量检查。这种对抗性动态通过考试智能体得到了加强，验证生成的摘要是否包含回答测验问题的必要信息，使多面向的反馈机制成为可能，实现迭代优化。我们在三个广泛使用的长文档摘要基准测试上对SummQ进行了评估。实验结果表明，我们的框架在ROUGE和BERTScore指标以及LLM作为法官和人类评估中，都显著优于现有的最新方法。我们的综合分析揭示了多智能体协作动态的有效性、不同智能体配置的影响以及测验机制的影响。这项工作为长文档摘要建立了新的方法，使用对抗性智能协作来提高摘要质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20900v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>当前大型语言模型在处理长文档摘要时面临信息丢失、事实错误和连贯性问题等挑战。为此，我们提出SummQ，一种新型对抗性多智能体框架，通过专门智能体在摘要和问答两个领域的协作来解决这些问题。该框架包括摘要生成器和评估器，以及生成问题和评估问题的智能体，共同协作并持续检查摘要质量。实验结果表明，SummQ在广泛使用的长文档摘要基准测试中显著优于现有方法。该工作为长文档摘要提供了一种新的对抗性智能体协作方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前大型语言模型在处理长文档摘要时存在挑战，如信息丢失、事实错误和连贯性问题。</li>
<li>SummQ是一种新型对抗性多智能体框架，旨在解决这些挑战。</li>
<li>SummQ包括摘要生成器和评估器，以及生成和理解问题的智能体，共同协作进行摘要的质量检查。</li>
<li>SummQ在广泛使用的长文档摘要基准测试中表现优异。</li>
<li>多智能体协作动态在SummQ中起到了关键作用。</li>
<li>SummQ的研究分析了不同智能体配置和问答机制的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20900v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLMs-in-Web-API-Integration-Tasks"><a href="#Benchmarking-LLMs-in-Web-API-Integration-Tasks" class="headerlink" title="Benchmarking LLMs in Web API Integration Tasks"></a>Benchmarking LLMs in Web API Integration Tasks</h2><p><strong>Authors:Daniel Maninger, Leon Chemnitz, Amir Molzam Sharifloo, Jannis Brugger, Mira Mezini</strong></p>
<p>API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models (LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models were able to solve more than 40% of the tasks. </p>
<blockquote>
<p>API集成是我们数字基础设施的核心，使得软件系统能够连接和交互。然而，许多研究表明，编写或生成正确的代码来调用API，特别是Web API，是一项挑战。虽然大型语言模型（LLM）在软件开发中变得流行起来，但它们自动化生成Web API集成代码的有效性尚未被探索。为了解决这个问题，我们提出了一种数据集和评估流程，旨在评估LLM生成Web API调用代码的能力。我们在多个开源LLM上的实验表明，生成API调用构成了重大挑战，导致出现了虚构的端点、参数使用不正确和其他错误。经过评估的开源模型没有能够完成超过40%的任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20172v2">PDF</a> To be published in Proceedings of 2nd ACM International Conference on   AI-powered Software, Benchmark &amp; Dataset Track (AIware ‘25); updated paper   title and affiliations</p>
<p><strong>Summary</strong></p>
<p>API整合是数字基础设施的核心，它让软件系统能够连接和交互。然而，如多项研究所示，编写或生成正确的代码以调用API，特别是Web API，是一项挑战。虽然大型语言模型（LLM）在软件开发中很受欢迎，但它们自动生成Web API集成代码的效果尚未被探索。为了解决这个问题，我们提出了一套数据集和评估流程，旨在评估LLM生成Web API调用代码的能力。我们对多个开源LLM进行的实验表明，生成API调用存在很大挑战，会出现虚构的端点、参数使用错误等错误。没有任何一个评估的开源模型能够完成超过40%的任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>API整合是数字基础设施的核心。</li>
<li>编写或生成正确的API调用代码是一项挑战。</li>
<li>大型语言模型（LLM）在自动化生成Web API集成代码方面的效果尚未被充分探索。</li>
<li>生成API调用代码存在挑战，如虚构端点和参数使用错误等。</li>
<li>目前没有开源LLM能够解决超过40%的API调用代码生成任务。</li>
<li>需要进一步研究和改进LLM在API调用代码生成方面的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.20172v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="EigenTrack-Spectral-Activation-Feature-Tracking-for-Hallucination-and-Out-of-Distribution-Detection-in-LLMs-and-VLMs"><a href="#EigenTrack-Spectral-Activation-Feature-Tracking-for-Hallucination-and-Out-of-Distribution-Detection-in-LLMs-and-VLMs" class="headerlink" title="EigenTrack: Spectral Activation Feature Tracking for Hallucination and   Out-of-Distribution Detection in LLMs and VLMs"></a>EigenTrack: Spectral Activation Feature Tracking for Hallucination and   Out-of-Distribution Detection in LLMs and VLMs</h2><p><strong>Authors:Davide Ettori, Nastaran Darabi, Sina Tayebati, Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, Amit Ranjan Trivedi</strong></p>
<p>Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs. </p>
<blockquote>
<p>大型语言模型（LLM）虽然具有广泛的实用性，但仍容易出错，出现幻觉和超出分布（OOD）误差。我们提出了EigenTrack，这是一种可解释的实时检测器，它利用隐藏激活的谱几何和模型动态的全局紧凑签名。通过将熵、特征值间隙和KL散度等协方差谱统计信息从随机基线流入轻量级循环分类器中，EigenTrack跟踪表示结构的临时变化，从而在表面错误出现之前预测幻觉和OOD漂移。不同于黑箱和灰箱方法，它只需进行一次前向传递而无需重新采样。与其他现有的白箱检测器不同，它保留了时间上下文，聚合全局信号，并提供可解释的准确性延迟权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15735v2">PDF</a> 5 pages, submitted to ICASSP 2026, September 2025</p>
<p><strong>Summary</strong>：大型语言模型（LLMs）具有广泛的实用性，但仍容易出错出现幻觉和脱离分布。本文提出EigenTrack实时检测器，其使用隐藏激活的谱几何特性来监测模型的动态变化。通过实时计算协方差谱统计量，如熵、特征值间隙和KL散度等，EigenTrack能够跟踪表示结构中的时间变化，预测模型可能产生的幻觉和脱离分布的错误。此方法不需要复杂的重采样过程，且能够提供可解释的精度延迟权衡。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>LLMs具有广泛的实用性，但存在幻觉和脱离分布的错误问题。</li>
<li>EigenTrack是一种实时检测器，用于预测LLMs中的幻觉和OOD错误。</li>
<li>EigenTrack利用隐藏激活的谱几何特性来监测模型动态变化。</li>
<li>EigenTrack使用协方差谱统计量来跟踪表示结构中的时间变化。</li>
<li>EigenTrack不需要复杂的重采样过程，能够提供可解释的精度延迟权衡。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2509.15735v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAI’s-Latest-Open-Source-Models"><a href="#Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAI’s-Latest-Open-Source-Models" class="headerlink" title="Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open   Source Models"></a>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open   Source Models</h2><p><strong>Authors:Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song</strong></p>
<p>In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. More details and evaluation scripts are available at the \href{<a target="_blank" rel="noopener" href="https://ai-agent-lab.github.io/gpt-oss%7D%7BProject">https://ai-agent-lab.github.io/gpt-oss}{Project</a> Webpage}. </p>
<blockquote>
<p>在2025年8月，OpenAI发布了GPT-OSS模型，这是自2019年GPT-2以来其首次开放的大型语言模型权重。该模型包含两个混合专家架构，具有120B和20B的参数。我们评估了这两种变体，与六种当代开源大型语言模型进行了对比，这些模型参数范围从14.7B到235B，代表了密集和稀疏两种设计，跨越十个基准测试，涵盖一般知识、数学推理、代码生成、多语言理解和对话能力等方面。所有模型都在标准化推理设置下以非量化形式进行测试，使用McNemars测试和效应量分析进行统计验证。结果表明，尽管gpt-oss-20B在多个基准测试上表现优于gpt-oss-120B，如在HumanEval和MMLU上，但其所需的内存和每响应能耗要少得多。这两个模型在当前开源环境中的总体表现属于中等水平，在代码生成方面相对较强，而在多语言任务方面存在明显弱点。这些发现提供了实证证据，表明在稀疏架构中进行扩展可能不会产生比例的性能提升，这强调了进一步优化策略的需要，并为未来开源部署提供更有效的模型选择信息。更多详细信息和评估脚本可在项目网页[<a target="_blank" rel="noopener" href="https://ai-agent-lab.github.io/gpt-oss]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ai-agent-lab.github.io/gpt-oss]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12461v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GPT-OSS模型是OpenAI在2025年8月发布的开放权重大型语言模型，包含两个混合专家架构的版本，参数分别为120B和20B。在涵盖通用知识、数学推理、代码生成、多语种理解和对话能力等方面的十个基准测试中，与六个当代开源大型语言模型相比，GPT-OSS的表现中规中矩。尤其是gpt-oss-20B在某些基准测试上表现优异，尽管所需的内存和能源响应更少。这些发现表明在稀疏架构中进行扩展可能不会产生比例的性能提升，需要进一步优化策略和更有效的模型选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenAI发布了GPT-OSS模型，包含两个混合专家架构的版本，参数分别为120B和20B。</li>
<li>GPT-OSS模型在多个基准测试中进行了评估，与六个当代开源大型语言模型相比。</li>
<li>gpt-oss-20B在多个基准测试上表现优于gpt-oss-120B，且所需内存和能源响应更少。</li>
<li>GPT-OSS模型的整体表现属于中游水平，在代码生成方面相对较强，在多语种任务方面存在明显弱点。</li>
<li>研究结果表明，在稀疏架构中扩展可能不会产生比例的性能提升。</li>
<li>需要进一步优化策略和更有效的模型选择来提高未来开源部署的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12461">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2508.12461v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning"><a href="#Learn-Globally-Speak-Locally-Bridging-the-Gaps-in-Multilingual-Reasoning" class="headerlink" title="Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning"></a>Learn Globally, Speak Locally: Bridging the Gaps in Multilingual   Reasoning</h2><p><strong>Authors:Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang</strong></p>
<p>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. We propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language. Furthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language. To close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks, highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. <a target="_blank" rel="noopener" href="https://jd730.github.io/projects/M2A_GeoFact-X">https://jd730.github.io/projects/M2A_GeoFact-X</a> </p>
<blockquote>
<p>大型语言模型（LLM）在数学、事实问答和代码生成等领域取得了强大的性能，然而它们在不同语言中进行这些任务的能力仍然有待发展。特别是对于斯瓦希里语或泰语等低资源语言，LLM经常会误解提示或默认以英语进行推理。这种对高资源语言的隐性偏见会损害事实准确性、可解释性和信任。我们提出M2A，这是一种新颖的方法，它结合了多尺度多语言对齐和机器翻译问题上的语言一致性奖励，训练模型直接在目标语言中进行准确推理。此外，现有的多语言基准测试只评估最终答案，忽视推理是否发生在目标语言中。为了弥补这一差距，我们引入了GeoFact-X，这是一个基于地理的多语言事实推理基准测试，包含五种语言的推理轨迹：英语、印地语、日语、斯瓦希里语和泰语。我们的结果表明，M2A在数学和事实推理任务中都显著提高了多语言推理的保真度，强调推理感知的多语言强化学习对于稳健的跨语言泛化至关重要。详情访问：<a target="_blank" rel="noopener" href="https://jd730.github.io/projects/M2A_GeoFact-%E2%AD%BE">https://jd730.github.io/projects/M2A_GeoFact-X</a>（此处已将网页链接进行格式化处理）</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05418v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在数学、事实问答和代码生成等领域表现出强大的性能，但在不同语言的任务推理能力方面仍有待发展。对于斯瓦希里语或泰语等低资源语言，LLM常常误解提示或默认以英语进行推理。这种对高资源语言的隐性偏见会影响事实准确性、可解释性和信任度。为此，我们提出M2A方法，结合多尺度多语言对齐和机器翻译问题上的语言一致性奖励，训练模型直接在目标语言中准确推理。此外，现有的多语言基准测试只关注最终答案，忽视了推理是否发生在目标语言中。为了弥补这一差距，我们推出了GeoFact-X，一个基于地理的多语言事实推理基准测试，包含五种语言的推理轨迹：英语、印地语、日语、斯瓦希里语和泰语。结果显示，M2A显著提高了多语言推理的忠实度，在数学和事实推理任务中尤为显著，强调了推理感知的多语言强化学习对于稳健的跨语言泛化至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在数学、事实问答和代码生成等领域有出色表现，但在不同语言的推理能力方面存在不足。</li>
<li>LLM对低资源语言存在误解提示和默认英语推理的偏向，影响事实准确性、可解释性和信任度。</li>
<li>提出了M2A方法，结合多尺度多语言对齐和机器翻译问题上的语言一致性奖励，提高模型在目标语言的推理能力。</li>
<li>现有的多语言基准测试主要关注最终答案，忽视了推理是否发生在目标语言中。</li>
<li>引入了GeoFact-X基准测试，包含多种语言的推理轨迹，以评估模型在多语言环境下的推理能力。</li>
<li>M2A方法显著提高了多语言推理的忠实度，在数学和事实推理任务中效果尤为显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_LLM/2507.05418v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Agent/2509.22596v1/page_2_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-30  Effective Policy Learning for Multi-Agent Online Coordination Beyond   Submodular Objectives
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_R1_Reasoning/2509.22572v1/page_5_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-30  WebGen-Agent Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
