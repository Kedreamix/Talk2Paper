<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-30  Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-30-更新"><a href="#2025-09-30-更新" class="headerlink" title="2025-09-30 更新"></a>2025-09-30 更新</h1><h2 id="Training-Free-Synthetic-Data-Generation-with-Dual-IP-Adapter-Guidance"><a href="#Training-Free-Synthetic-Data-Generation-with-Dual-IP-Adapter-Guidance" class="headerlink" title="Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance"></a>Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance</h2><p><strong>Authors:Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour, Vicky Kalogeiton</strong></p>
<p>Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks. </p>
<blockquote>
<p>图像分类任务中由于可用标签样本数量有限，少量样本情况下的图像分类仍然面临挑战。尽管已有研究尝试通过文本到图像的扩散模型生成合成训练数据，但它们通常需要大量的模型微调或外部信息源。我们提出了一种无需训练的新方法，称为DIPSY，它通过IP-Adapter实现图像到图像的转换，仅使用可用的少量样本即可生成具有高度辨别力的合成图像。DIPSY引入了三个关键创新点：（1）扩展的无分类器引导方案，实现对正负面图像条件的独立控制；（2）基于类别相似性的采样策略，用于识别有效的对比样本；（3）无需模型微调或外部字幕标注和过滤的简单有效的流程。在十个基准数据集上的实验表明，我们的方法达到了最新或相当的性能水平，同时消除了对生成模型适应性的需求或对外部工具进行字幕生成和图像过滤的依赖。我们的结果突显了在生成具有类别区分性的特征时，特别是进行精细分类任务时，利用正负面引导的双图像提示的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22635v1">PDF</a> BMVC 2025. Project page:   <a target="_blank" rel="noopener" href="https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/">https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种称为DIPSY的新型免训练图像分类方法，它利用IP-Adapter进行图像到图像的翻译，仅使用有限的样本生成具有高度区分性的合成图像。DIPSY引入了三项关键创新：扩展的无分类器引导方案、基于类别相似性的采样策略以及无需模型微调或外部描述的简单有效流程。实验证明，该方法在多个基准数据集上实现了最新或相当的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DIPSY是一种新型免训练图像分类方法。</li>
<li>利用IP-Adapter进行图像到图像的翻译，生成合成图像。</li>
<li>引入扩展的无分类器引导方案，实现正负面图像条件的独立控制。</li>
<li>采用基于类别相似性的采样策略，有效识别对比样本。</li>
<li>无需模型微调或外部描述，流程简单有效。</li>
<li>在多个基准数据集上实现最新或相当的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.22635v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.22635v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.22635v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transport-Based-Mean-Flows-for-Generative-Modeling"><a href="#Transport-Based-Mean-Flows-for-Generative-Modeling" class="headerlink" title="Transport Based Mean Flows for Generative Modeling"></a>Transport Based Mean Flows for Generative Modeling</h2><p><strong>Authors:Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil Kolouri</strong></p>
<p>Flow-matching generative models have emerged as a powerful paradigm for continuous data generation, achieving state-of-the-art results across domains such as images, 3D shapes, and point clouds. Despite their success, these models suffer from slow inference due to the requirement of numerous sequential sampling steps. Recent work has sought to accelerate inference by reducing the number of sampling steps. In particular, Mean Flows offer a one-step generation approach that delivers substantial speedups while retaining strong generative performance. Yet, in many continuous domains, Mean Flows fail to faithfully approximate the behavior of the original multi-step flow-matching process. In this work, we address this limitation by incorporating optimal transport-based sampling strategies into the Mean Flow framework, enabling one-step generators that better preserve the fidelity and diversity of the original multi-step flow process. Experiments on controlled low-dimensional settings and on high-dimensional tasks such as image generation, image-to-image translation, and point cloud generation demonstrate that our approach achieves superior inference accuracy in one-step generative modeling. </p>
<blockquote>
<p>流匹配生成模型已经作为连续数据生成的强大范式出现，在图像、3D形状和点云等领域取得了最先进的成果。尽管这些模型取得了成功，但由于需要大量的连续采样步骤，它们面临着推理速度慢的问题。最近的工作试图通过减少采样步骤来加速推理。特别是，Mean Flow提供了一种一步生成方法，在保持强大的生成性能的同时，提供了实质性的加速。然而，在许多连续领域，Mean Flow无法忠实地逼近原始多步流匹配过程的行为。在这项工作中，我们通过将基于最优传输的采样策略融入Mean Flow框架来解决这一限制，使一步生成器能更好地保持原始多步流过程的保真度和多样性。在受控的低维设置和高维任务（如图像生成、图像到图像的翻译和点云生成）上的实验表明，我们的方法在一步生成模型中实现了更高的推理准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>流匹配生成模型已成为连续数据生成的强大范式，在图像、3D形状和点云等领域取得了最新成果。尽管这些模型取得了成功，但由于需要大量的顺序采样步骤，推理速度较慢。最近的工作试图通过减少采样步骤来加速推理。特别是，Mean Flows 提供了一种一步生成方法，在保持强大的生成性能的同时实现了实质性的加速。然而，在许多连续领域，Mean Flows 无法忠实模拟原始多步流匹配过程的行为。在本研究中，我们通过将基于最优传输的采样策略融入 Mean Flow 框架，解决这一问题，使一步生成器更好地保留原始多步流过程的保真度和多样性。实验表明，我们的方法在一步生成建模中实现了更高的推理准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流匹配生成模型已成为连续数据生成的强大工具，在多域（如图像、3D形状和点云）表现出卓越性能。</li>
<li>这些模型通常由于需要多个顺序采样步骤而导致推理速度慢。</li>
<li>Mean Flows 作为一种一步生成方法实现了推理的实质性加速，但仍无法完全模拟多步流匹配过程的行为。</li>
<li>将最优传输采样策略融入 Mean Flow 框架能解决此问题。</li>
<li>此新方法在一步生成建模中提高了推理准确性。</li>
<li>实验在受控的低维环境和复杂的高维任务（如图像生成、图像到图像转换和点云生成）中验证了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.22592v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.22592v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Dynamic-Novel-View-Synthesis-in-High-Dynamic-Range"><a href="#Dynamic-Novel-View-Synthesis-in-High-Dynamic-Range" class="headerlink" title="Dynamic Novel View Synthesis in High Dynamic Range"></a>Dynamic Novel View Synthesis in High Dynamic Range</h2><p><strong>Authors:Kaixuan Zhang, Zhipeng Xiong, Minxian Li, Mingwu Ren, Jiankang Deng, Xiatian Zhu</strong></p>
<p>High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension &#96;&#96;Dynamic’’ emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released. </p>
<blockquote>
<p>高动态范围新颖视图合成（HDR NVS）旨在从在常规成像条件下捕获的低动态范围（LDR）训练图像中学习HDR 3D模型。当前的方法主要关注静态场景，隐含地假设所有场景元素都是静止的且非活着的。然而，现实世界中的场景通常包含动态元素，例如移动物体、变化的照明条件和其他临时事件，从而呈现出更具挑战性的场景。为了弥补这一差距，我们提出了一个更现实的问题，称为高动态范围动态新颖视图合成（HDR DNVS），其中“动态”这个额外的维度强调了同时建模时间辐射变化的必要性，以及与LDR和HDR之间复杂3D转换的精致性。为了应对这一复杂且交织的挑战，我们引入了HDR-4DGS，这是一种基于高斯喷绘的架构，具有创新性的动态色调映射模块，该模块显式连接HDR和LDR域，通过根据时间维度上不断发展的辐射分布动态地适应色调映射功能，从而保持时间辐射一致性。因此，HDR-4DGS实现了时间辐射一致性和空间精确的颜色翻译，可以实现从任意观点和时间实例进行逼真的HDR渲染。大量实验表明，HDR-4DGS在定量性能和视觉保真度方面都超越了现有的最新方法。源代码将发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了HDR动态新视角合成技术（HDR DNVS），该技术解决了HDR新视角合成中遇到的动态场景问题。通过引入HDR-4DGS架构，实现了动态场景下的HDR渲染，同时保证了时间辐射一致性和空间色彩准确性。该技术在实验上超越了现有技术，提高了视觉效果和性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HDR动态新视角合成技术（HDR DNVS）解决了现有HDR模型无法处理动态场景的问题。</li>
<li>HDR-4DGS架构是基于高斯斑贴图的模型，包含动态调色映射模块，该模块实现了HDR和LDR域之间的明确连接。</li>
<li>HDR-4DGS通过动态调整调色映射函数，根据时间维度上的辐射分布变化，保持了时间辐射的一致性。</li>
<li>HDR-4DGS实现了空间精确的色彩翻译，使得从任意视角和时间点进行的光照真实感的HDR渲染成为可能。</li>
<li>大量实验证明，HDR-4DGS在定量性能和视觉保真度上均超越了现有技术。</li>
<li>HDR-4DGS技术可以应用于需要处理动态场景的HDR渲染，如虚拟现实、电影后期制作等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21853v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21853v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21853v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Causal-Abstraction-Inference-under-Lossy-Representations"><a href="#Causal-Abstraction-Inference-under-Lossy-Representations" class="headerlink" title="Causal Abstraction Inference under Lossy Representations"></a>Causal Abstraction Inference under Lossy Representations</h2><p><strong>Authors:Kevin Xia, Elias Bareinboim</strong></p>
<p>The study of causal abstractions bridges two integral components of human intelligence: the ability to determine cause and effect, and the ability to interpret complex patterns into abstract concepts. Formally, causal abstraction frameworks define connections between complicated low-level causal models and simple high-level ones. One major limitation of most existing definitions is that they are not well-defined when considering lossy abstraction functions in which multiple low-level interventions can have different effects while mapping to the same high-level intervention (an assumption called the abstract invariance condition). In this paper, we introduce a new type of abstractions called projected abstractions that generalize existing definitions to accommodate lossy representations. We show how to construct a projected abstraction from the low-level model and how it translates equivalent observational, interventional, and counterfactual causal queries from low to high-level. Given that the true model is rarely available in practice we prove a new graphical criteria for identifying and estimating high-level causal queries from limited low-level data. Finally, we experimentally show the effectiveness of projected abstraction models in high-dimensional image settings. </p>
<blockquote>
<p>关于因果抽象的研究是人类智力的两个核心组成部分之间的桥梁：确定因果关系的能力和将复杂模式解释为抽象概念的能力。在形式上，因果抽象框架定义了复杂低层次因果模型与简单高层次模型之间的联系。现有定义的一个主要局限性在于，在考虑有损抽象函数时，它们定义得并不明确，在有损抽象函数中，多个低层次的干预措施可能会产生不同的效果，而同时映射到同一高层次干预措施上（这被称为抽象不变条件假设）。在本文中，我们引入了一种新的抽象类型，称为投影抽象，它将现有定义推广到容纳有损表示。我们展示了如何从低层次模型构建投影抽象，以及它如何将等效的观察、干预和假设因果查询从低层次翻译到高层次。鉴于在实践中真实模型很少可用，我们证明了一种新的图形标准，用于从有限的低层次数据中识别和估计高层次因果查询。最后，我们通过实验展示了投影抽象模型在高维图像设置中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21607v1">PDF</a> 35 pages, 8 figures, published at ICML 2025</p>
<p><strong>摘要</strong></p>
<p>该研究探讨了因果抽象如何桥梁人类智力的两个核心组成部分：因果关系的判断能力与将复杂模式解读为抽象概念的能力。因果抽象框架正式定义了复杂低层次因果模型与简单高层次模型之间的联系。现有定义的一个主要局限是，在考虑有损抽象函数时，它们并不明确，其中多个低层次的干预可能会产生不同的效果，同时映射到同一高层次干预（称为抽象不变条件）。本文介绍了一种新的抽象类型——投影抽象，它将现有定义推广到适应有损表示。我们展示了如何从低层次模型构建投影抽象，以及它如何将等效的观察、干预和反事实因果查询从低层次转换为高层次。鉴于实践中真实模型很少可用，我们证明了一种新的图形标准，用于从有限的低层次数据中识别和估计高层次因果查询。最后，我们在高维图像环境中实验性地展示了投影抽象模型的有效性。</p>
<p><strong>要点</strong></p>
<ol>
<li>因果抽象是人类智力的重要体现，连接因果关系判断与复杂模式向抽象概念的解读。</li>
<li>现有的因果抽象框架主要面临在处理有损抽象函数时的定义不明确问题。</li>
<li>论文提出了一种新的投影抽象类型，旨在解决现有定义的局限并适应有损表示。</li>
<li>论文介绍了如何构建投影抽象，并阐述了其在不同层次间的因果查询转换功能。</li>
<li>在真实模型难以获取的情况下，论文提出了识别与估计高层次因果查询的新图形标准。</li>
<li>论文强调投影抽象模型在有限低层次数据下的效能，并证明了其适用于高维图像环境的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21607">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21607v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21607v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21607v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Models-Cannot-Plan-but-Can-They-Formalize"><a href="#Vision-Language-Models-Cannot-Plan-but-Can-They-Formalize" class="headerlink" title="Vision Language Models Cannot Plan, but Can They Formalize?"></a>Vision Language Models Cannot Plan, but Can They Formalize?</h2><p><strong>Authors:Muyu He, Yuxi Zheng, Yuchen Liu, Zijian An, Bill Cai, Jiani Huang, Lifeng Zhou, Feng Liu, Ziyang Li, Li Zhang</strong></p>
<p>The advancement of vision language models (VLMs) has empowered embodied agents to accomplish simple multimodal planning tasks, but not long-horizon ones requiring long sequences of actions. In text-only simulations, long-horizon planning has seen significant improvement brought by repositioning the role of LLMs. Instead of directly generating action sequences, LLMs translate the planning domain and problem into a formal planning language like the Planning Domain Definition Language (PDDL), which can call a formal solver to derive the plan in a verifiable manner. In multimodal environments, research on VLM-as-formalizer remains scarce, usually involving gross simplifications such as predefined object vocabulary or overly similar few-shot examples. In this work, we present a suite of five VLM-as-formalizer pipelines that tackle one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those on an existing benchmark while presenting another two that for the first time account for planning with authentic, multi-view, and low-quality images. We conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation. We reveal the bottleneck to be vision rather than language, as VLMs often fail to capture an exhaustive set of necessary object relations. While generating intermediate, textual representations such as captions or scene graphs partially compensate for the performance, their inconsistent gain leaves headroom for future research directions on multimodal planning formalization. </p>
<blockquote>
<p>视觉语言模型（VLMs）的进步使得实体代理能够完成简单的多模态规划任务，但对于需要一长串动作的长远规划任务仍无法完成。在仅文本模拟中，通过改变大型语言模型（LLMs）的角色，长远规划已取得了重大改进。大型语言模型不直接生成动作序列，而是将规划领域和问题翻译成如规划领域定义语言（PDDL）之类的正式规划语言，这样可以调用形式化求解器以可验证的方式推导计划。在多模态环境中，关于VLM-as-formalizer的研究仍然很少，通常涉及过于简化的内容，例如预设的对象词汇或过于相似的少数案例。在这项工作中，我们提出了五个VLM-as-formalizer管道，解决了一次性、开放词汇和多模态的PDDL形式化问题。我们在现有的基准测试上对这些进行了评估，同时首次推出了另外两个基准测试，用于处理真实的、多视角和低质量的图像规划。我们得出结论，VLM-as-formalizer在端到端计划生成方面表现出巨大优势。我们发现的瓶颈在于视觉而非语言，因为视觉语言模型通常无法捕获必要对象关系的完整集合。虽然生成中间文本表示（如字幕或场景图）可以部分弥补性能损失，但其不一致的增益为未来多模态规划形式化的研究方向留下了空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21576v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>VLMS在简单多模态规划任务中表现良好，但在需要长序列动作的长远规划任务中表现不足。文本模拟中，LLMS通过翻译规划领域和问题到形式化规划语言（如PDDL），调用形式化求解器进行可验证的规划，推动了长远规划的发展。多模态环境中VLM作为格式化程序的研究仍缺乏实际数据支持，通常需要预先设定目标对象词汇表或者假设少例案例场景等简化条件。本研究推出五个VLM作为格式化程序流水线，应对单次操作、开放词汇和多模态PDDL格式化的任务，对现有评估标准进行了评估并新提出两个标准来体现面向真实环境的多视角低质量图像的多模态规划的现实问题情况。最终实验结果表明，通过LLMS方式结合的形式化设计性能更佳优于直接进行规划的流水线设计，同时发现瓶颈在于视觉能力而非语言能力，未来研究可通过增强视觉能力来提升多模态规划形式化的性能。</p>
<p><strong>Key Takeaways</strong>: </p>
<ol>
<li>VLM在多模态规划中表现出较好成绩。但仍难以应对涉及复杂长期计划的多步骤行为场景任务的需求问题；相较基于语言处理领域的方案而研发规划的规划模式即依赖预置标签的方法仍然存在局限和不稳定的表现效果问题。本文基于现有的理论基础探索并解决了这一系列的问题，优化了解决方法的可行性；而此难题也被进一步归纳为现有模型中的一个重大短板或未来的工作重点之一。特别是对结构化信息和行动域关系的精准理解。模型未能捕捉到这些关系可能导致预测结果的偏差或误差的增大；文中给出了解决方案是通过中间层，使用语义表示方式间接解决问题同时减少短板，一定程度上起到性能补偿作用但收效不一、依旧存在一定的优化提升空间以及创新尝试机会和空间。后续工作可以从模型内部进行针对性优化或者改进，以提高模型捕捉结构化信息的能力，进一步提升预测性能等方面着手改进和展开相关工作；整体技术仍存在着潜在应用空间。也仍有很多有待探索和发掘技术途径尚未知晓其中具体原因值得进行深入探索和专门研究的实际问题发生技术链条和价值网定位分析与未来发展推广技术等领域可以继续提升对多层次数据进行有机融合的应用等复杂数据相关的实验设计与技术研究进一步提升科研应用能力也是一条十分有发展潜力和提升应用价值的新思路和前景可能和发展趋势值得期待。文中通过提出一系列新的方法和流程实现了一定程度的突破和进展为后续研究提供了参考和借鉴价值较高的思路和方向指引作用；在构建全新的大规模现实多场景智能化精准规划与决策领域开拓更多的技术应用潜力大有可为 。与此同时带来的安全和风险可控等保证作用有待进一步深入研究探索解决之道以保障人工智能技术的健康发展以及未来智能社会的和谐稳定与可持续发展趋势 。本文的工作旨在通过不断的研究与探索为解决上述挑战和问题贡献更多的创新思路和技术方案。为实现更智能、更高效、更安全的智能化决策系统提供强有力的技术支持和保障作用 。同时本文的研究成果也为相关领域的研究提供了宝贵的借鉴经验和思路指引 ，并为后续的进一步研究和拓展奠定了坚实的基础 。总结本文主要总结了提出一系列的算法架构并开展实验研究及实验结果展示给出解决此类问题可能的方法和研究方向分析进一步开拓技术应用领域并进行优化拓展和应用探索有着深远影响与贡献以及潜在的学术价值和广泛的实际应用价值方面可能继续带来更多的颠覆性技术和理念以及前景无限的可持续发展潜力和可能研究方向进行挖掘并进一步研究具有重要意义以及巨大的发展前景和商业潜力领域和应用价值体现有较高的重要性和价值。但其在结构、形式以及具体内容表达等方面仍然有一定的提升空间对这类问题需要未来继续探索提出更有效的研究方法和方案来提高对技术内容的解读能力以及推进科研领域不断进步和创新发展进程和提高实际运用的可靠性有效性实用性和经济价值等综合效能领域拓展的设想和实践设想和价值探索方面的无限潜力不断得到实现并最终展现出更高质量和影响力的科技成果与应用实践前景体现出广阔的前景空间和经济社会发展领域得到更广泛的应用和推广实践当中 。期望未来能够在该领域取得更大的突破和进展为相关领域的发展做出更大的贡献 。总的来说该论文研究内容丰富具有一定的创新性具有重要的理论和实践价值 。未来可以进一步深入研究探索新的方法和应用拓展该领域的研究边界 。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21576v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21576v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21576v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.21576v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Draw-In-Mind-Rebalancing-Designer-Painter-Roles-in-Unified-Multimodal-Models-Benefits-Image-Editing"><a href="#Draw-In-Mind-Rebalancing-Designer-Painter-Roles-in-Unified-Multimodal-Models-Benefits-Image-Editing" class="headerlink" title="Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal   Models Benefits Image Editing"></a>Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal   Models Benefits Image Editing</h2><p><strong>Authors:Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou</strong></p>
<p>In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I&#x2F;Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models are available at <a target="_blank" rel="noopener" href="https://github.com/showlab/DIM">https://github.com/showlab/DIM</a>. </p>
<blockquote>
<p>近年来，将多模态理解和生成集成到一个单一统一模型中已成为一种有前途的范式。虽然这一方法在文本到图像（T2I）生成方面取得了很好的效果，但在精确图像编辑方面仍面临挑战。我们将这一局限性归因于责任划分的不平衡。理解模块主要充当将用户指令编码为语义条件的翻译器，而生成模块必须同时充当设计师和画家，推断原始布局，识别目标编辑区域，并呈现新内容。这种不平衡有些反直觉，因为理解模块通常使用更多关于复杂推理任务的数据进行训练，相比之下生成模块则较少。为了解决这个问题，我们引入了Draw-In-Mind（DIM）数据集，它包含两个互补的子集：（i）DIM-T2I，包含14M长上下文图像文本对，用于增强复杂指令理解；（ii）DIM-Edit，由GPT-4o生成的包含233K思维链的想象图像，作为图像编辑的明确设计蓝图。我们通过轻量级的两层MLP将冷冻的Qwen2.5-VL-3B与可训练的SANA1.5-1.6B连接起来，并在提出的DIM数据集上进行训练，得到DIM-4.6B-T2I&#x2F;Edit模型。尽管其参数规模适中，但DIM-4.6B-Edit在ImgEdit和GEdit-Bench基准测试中实现了SOTA或具有竞争力的性能表现，超越了诸如UniWorld-V1和Step1X-Edit等大型模型。这些发现表明，明确地将设计责任分配给理解模块对于图像编辑具有显著益处。我们的数据集和模型可以在<a target="_blank" rel="noopener" href="https://github.com/showlab/DIM%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/showlab/DIM处获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01986v2">PDF</a> Tech Report</p>
<p><strong>Summary</strong><br>     近年，构建多模态理解与生成统一模型成为热门方向，在文本到图像生成方面表现优异，但在图像编辑上仍有局限。为解决理解模块与生成模块职责不平衡的问题，我们引入Draw-In-Mind数据集，分为DIM-T2I和DIM-Edit两个子集，前者强化复杂指令理解，后者提供图像编辑的明确设计蓝图。通过连接两个模型并训练在DIM数据集上，我们的模型在ImgEdit和GEdit-Bench基准测试中达到或保持领先水平，证明明确分配设计责任给理解模块对图像编辑有显著益处。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态理解与生成统一模型在文本到图像生成领域表现优秀，但在图像编辑方面存在挑战。</li>
<li>理解模块和生成模块职责不平衡是限制模型在图像编辑任务上表现的主要原因之一。</li>
<li>Draw-In-Mind数据集的引入是为了解决职责不平衡问题，分为用于增强复杂指令理解的DIM-T2I和提供图像编辑设计蓝图的DIM-Edit两个子集。</li>
<li>通过连接两个模型并训练在DIM数据集上，得到的DIM-4.6B-T2I&#x2F;Edit模型在基准测试中表现优秀。</li>
<li>该研究证明了明确分配设计责任给理解模块对图像编辑的重要性。</li>
<li>所提出的方法和数据集在ImgEdit和GEdit-Bench基准测试中达到了先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01986">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.01986v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.01986v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.01986v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2509.01986v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TAMMs-Temporal-Aware-Multimodal-Model-for-Satellite-Image-Change-Understanding-and-Forecasting"><a href="#TAMMs-Temporal-Aware-Multimodal-Model-for-Satellite-Image-Change-Understanding-and-Forecasting" class="headerlink" title="TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change   Understanding and Forecasting"></a>TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change   Understanding and Forecasting</h2><p><strong>Authors:Zhongbin Guo, Yuhao Wang, Ping Jian, Chengzhi Li, Xinyue Chen, Zhen Yang, Ertai E</strong></p>
<p>Temporal Change Description (TCD) and Future Satellite Image Forecasting (FSIF) are critical, yet historically disjointed tasks in Satellite Image Time Series (SITS) analysis. Both are fundamentally limited by the common challenge of modeling long-range temporal dynamics. To explore how to improve the performance of methods on both tasks simultaneously by enhancing long-range temporal understanding capabilities, we introduce TAMMs, the first unified framework designed to jointly perform TCD and FSIF within a single MLLM-diffusion architecture. TAMMs introduces two key innovations: Temporal Adaptation Modules (TAM) enhance frozen MLLM’s ability to comprehend long-range dynamics, and Semantic-Fused Control Injection (SFCI) mechanism translates this change understanding into fine-grained generative control. This synergistic design makes the understanding from the TCD task to directly inform and improve the consistency of the FSIF task. Extensive experiments demonstrate TAMMs significantly outperforms state-of-the-art specialist baselines on both tasks. </p>
<blockquote>
<p>时间变化描述（TCD）和未来卫星图像预测（FSIF）是卫星图像时间序列（SITS）分析中的关键任务，但它们在历史上是相互分离的任务。两者的根本局限性在于模拟长期时间动态的共同挑战。为了探索如何通过提高长期时间理解的能力，同时提高这两种任务方法的性能，我们引入了TAMMs，这是一个统一框架，旨在在一个MLLM扩散架构内同时执行TCD和FSIF。TAMMs引入了两个关键创新点：时间适应模块（TAM）增强了冻结的MLLM对长期动态的理解能力，语义融合控制注入（SFCI）机制将这种变化理解转化为精细的生成控制。这种协同设计使得从TCD任务的理解能够直接为FSIF任务提供信息并提高其一致性。大量实验表明，在两项任务上，TAMMs都显著优于最新的专业基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18862v2">PDF</a> Submitted to The Fourteenth International Conference on Learning   Representations (ICLR 2026). Our dataset can be found at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IceInPot/TAMMs">https://huggingface.co/datasets/IceInPot/TAMMs</a></p>
<p><strong>Summary</strong></p>
<p>卫星图像时间序列（SITS）分析中的时间变化描述（TCD）和未来卫星图像预测（FSIF）是两个关键但历史分离的任务。两者都受到模拟长期动态变化挑战的限制。为探索如何通过增强长期时间理解同时提高两个任务的方法性能，我们引入了TAMMs，这是首个联合执行这两个任务的统一框架，使用单一MLLM扩散架构。TAMMs包括两大创新点：时序适配模块（TAM）增强了冻结的MLLM对长期动态的理解能力；语义融合控制注入（SFCI）机制将这种变化理解转化为精细的生成控制。这种协同设计使得从TCD任务的理解直接促进FSIF任务的连贯性。大量实验证明，在两项任务上，TAMMs显著优于最新专业基线。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Temporal Change Description (TCD) 和 Future Satellite Image Forecasting (FSIF) 是卫星图像时间序列分析中的两个关键任务。</li>
<li>两个任务都面临模拟长期动态变化的挑战。<br>*TAMMs是首个统一框架，能在单一MLLM扩散架构中联合执行这两个任务。<br>*TAMMs包括两大创新点：时序适配模块（TAM）增强长期动态理解；语义融合控制注入（SFCI）机制将变化理解转化为生成控制。<br>*协同设计使得从TCD任务的理解提高FSIF任务的连贯性。<br>*大量实验证明，在两项任务上，TAMMs显著优于最新专业基线。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2506.18862v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Forward-only-Diffusion-Probabilistic-Models"><a href="#Forward-only-Diffusion-Probabilistic-Models" class="headerlink" title="Forward-only Diffusion Probabilistic Models"></a>Forward-only Diffusion Probabilistic Models</h2><p><strong>Authors:Ziwei Luo, Fredrik K. Gustafsson, Jens Sjölund, Thomas B. Schön</strong></p>
<p>This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model, despite its simplicity, achieves state-of-the-art performance on various image restoration tasks. Its general applicability on image-conditioned generation is also demonstrated via qualitative results on image-to-image translation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD">https://github.com/Algolzw/FoD</a>. </p>
<blockquote>
<p>本文提出了一种只前向扩散（FoD）的生成建模方法。与传统的依赖于耦合的前向和后向扩散方案的扩散模型相比，FoD通过单一的前向扩散过程直接学习数据生成，从而构建了一个简单而高效的生成框架。FoD的核心是一个与状态相关的随机微分方程，该方程在漂移和扩散函数中均涉及均值回复项。这种均值回复属性保证了向清洁数据的收敛，自然地模拟了源分布和目标分布之间的随机插值。更重要的是，FoD具有分析可行性，并使用简单的随机流匹配目标进行训练，从而在推理过程中实现了几步非马尔可夫链采样。尽管FoD模型简单，但在各种图像恢复任务上却取得了最先进的性能。通过对图像到图像翻译的定性结果，也证明了其在图像条件生成上的普遍适用性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Algolzw/FoD上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16733v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://algolzw.github.io/fod">https://algolzw.github.io/fod</a></p>
<p><strong>Summary</strong><br>     此工作提出了一种仅前向扩散（FoD）的生成建模方法。与传统的采用耦合的前向-后向扩散方案的扩散模型不同，FoD通过单一的前向扩散过程直接学习数据生成，提供了一个简洁而高效的生成框架。FoD的核心是一个涉及均值回归项的态依赖随机微分方程，该方程在漂移和扩散函数中均有体现。均值回归属性保证了向清洁数据的收敛，自然地模拟了源分布和目标分布之间的随机插值。更重要的是，FoD具有分析上的可行性，并使用简单的随机流匹配目标进行训练，使得在推理过程中实现了几步非马尔可夫链采样。尽管其简洁性，FoD在各种图像恢复任务上达到了最先进的性能，并且在图像到图像的转换上通过定性结果证明了其普遍适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>此工作提出了一个名为FoD的仅前向扩散方法，用于生成建模。</li>
<li>传统扩散模型通常使用前向和后向扩散的耦合方案，而FoD通过单一的前向扩散过程进行数据处理生成。</li>
<li>FoD的核心是一个态依赖随机微分方程，包含均值回归项，保证数据收敛的清洁性。</li>
<li>均值回归属性自然地模拟了源分布和目标分布之间的随机插值过程。</li>
<li>FoD具有分析上的可行性，并使用简单的随机流匹配目标进行训练。</li>
<li>在推理过程中，FoD实现了非马尔可夫链的简短采样步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2505.16733v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2505.16733v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DanceText-A-Training-Free-Layered-Framework-for-Controllable-Multilingual-Text-Transformation-in-Images"><a href="#DanceText-A-Training-Free-Layered-Framework-for-Controllable-Multilingual-Text-Transformation-in-Images" class="headerlink" title="DanceText: A Training-Free Layered Framework for Controllable   Multilingual Text Transformation in Images"></a>DanceText: A Training-Free Layered Framework for Controllable   Multilingual Text Transformation in Images</h2><p><strong>Authors:Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Rizwan Qureshi, Shaina Raza, Aman Chadha, Yong Xiang, Zhixiang Chen</strong></p>
<p>We present DanceText, a training-free framework for multilingual text editing in images, designed to support complex geometric transformations and achieve seamless foreground-background integration. While diffusion-based generative models have shown promise in text-guided image synthesis, they often lack controllability and fail to preserve layout consistency under non-trivial manipulations such as rotation, translation, scaling, and warping. To address these limitations, DanceText introduces a layered editing strategy that separates text from the background, allowing geometric transformations to be performed in a modular and controllable manner. A depth-aware module is further proposed to align appearance and perspective between the transformed text and the reconstructed background, enhancing photorealism and spatial consistency. Importantly, DanceText adopts a fully training-free design by integrating pretrained modules, allowing flexible deployment without task-specific fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that our method achieves superior performance in visual quality, especially under large-scale and complex transformation scenarios. Code is avaible at <a target="_blank" rel="noopener" href="https://github.com/YuZhenyuLindy/DanceText.git">https://github.com/YuZhenyuLindy/DanceText.git</a>. </p>
<blockquote>
<p>我们推出了DanceText，这是一个用于图像中多语言文本编辑的无训练框架，旨在支持复杂的几何变换，并实现无缝的前景-背景集成。虽然基于扩散的生成模型在文本引导的图像合成中显示出潜力，但它们往往缺乏可控性，在非平凡操作（例如旋转，平移，缩放和弯曲）下无法保持布局一致性。为了解决这些局限性，DanceText引入了一种分层编辑策略，将文本与背景分开，以模块化和可控的方式执行几何变换。还提出了深度感知模块，以在变换的文本和重建的背景之间对齐外观和透视，增强逼真度和空间一致性。重要的是，DanceText通过集成预训练模块采用了完全无训练的设计，无需特定任务的微调即可灵活部署。在AnyWord-3M基准测试的大量实验表明，我们的方法在视觉质量方面实现了卓越的性能，尤其是在大规模和复杂变换场景下。代码可在<a target="_blank" rel="noopener" href="https://github.com/YuZhenyuLindy/DanceText.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuZhenyuLindy/DanceText.git上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14108v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DanceText是一个无需训练的多语种图像文本编辑框架，支持复杂几何变换，实现无缝前景背景融合。它采用分层编辑策略和深度感知模块，提高几何变换的模块性和可控性，增强文本与背景融合的光照真实感和空间一致性。DanceText通过集成预训练模块实现完全无训练设计，无需针对特定任务进行微调，即可灵活部署。在AnyWord-3M基准测试上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DanceText是一个多语种图像文本编辑框架，无需训练。</li>
<li>支持复杂几何变换，如旋转、平移、缩放和扭曲。</li>
<li>框架采用分层编辑策略，将文本与背景分离，实现模块化可控的几何变换。</li>
<li>引入深度感知模块，提高变换文本与重建背景之间的外观和视角对齐。</li>
<li>增强了光照真实感和空间一致性。</li>
<li>通过集成预训练模块，实现完全无训练设计，灵活部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14108">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_I2I Translation/2504.14108v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_视频理解/2509.21451v1/page_5_0.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-09-30  VideoJudge Bootstrapping Enables Scalable Supervision of   MLLM-as-a-Judge for Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Few-Shot/2410.03039v3/page_3_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-30  Death of the Novel(ty) Beyond n-Gram Novelty as a Metric for Textual   Creativity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
