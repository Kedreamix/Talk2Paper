<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  From Coarse to Fine Recursive Audio-Visual Semantic Enhancement for   Speech Separation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-670c254014ec77c8f14fd2873cb62fc1')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    44 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-30-æ›´æ–°"><a href="#2025-09-30-æ›´æ–°" class="headerlink" title="2025-09-30 æ›´æ–°"></a>2025-09-30 æ›´æ–°</h1><h2 id="From-Coarse-to-Fine-Recursive-Audio-Visual-Semantic-Enhancement-for-Speech-Separation"><a href="#From-Coarse-to-Fine-Recursive-Audio-Visual-Semantic-Enhancement-for-Speech-Separation" class="headerlink" title="From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation"></a>From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation</h2><p><strong>Authors:Ke Xue, Rongfei Fan,  Lixin, Dawei Zhao, Chao Zhu, Han Hu</strong></p>
<p>Audio-visual speech separation aims to isolate each speakerâ€™s clean voice from mixtures by leveraging visual cues such as lip movements and facial features. While visual information provides complementary semantic guidance, existing methods often underexploit its potential by relying on static visual representations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine Network that introduces a recursive semantic enhancement paradigm for more effective separation. CSFNet operates in two stages: (1) Coarse Separation, where a first-pass estimation reconstructs a coarse audio waveform from the mixture and visual input; and (2) Fine Separation, where the coarse audio is fed back into an audio-visual speech recognition (AVSR) model together with the visual stream. This recursive process produces more discriminative semantic representations, which are then used to extract refined audio. To further exploit these semantics, we design a speaker-aware perceptual fusion block to encode speaker identity across modalities, and a multi-range spectro-temporal separation network to capture both local and global time-frequency patterns. Extensive experiments on three benchmark datasets and two noisy datasets show that CSFNet achieves state-of-the-art (SOTA) performance, with substantial coarse-to-fine improvements, validating the necessity and effectiveness of our recursive semantic enhancement framework. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ†ç¦»æ—¨åœ¨åˆ©ç”¨è§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨ç‰¹å¾ï¼‰æ¥ä»æ··åˆå£°éŸ³ä¸­éš”ç¦»æ¯ä¸ªè¯´è¯äººçš„æ¸…æ™°å£°éŸ³ã€‚è™½ç„¶è§†è§‰ä¿¡æ¯æä¾›äº†è¡¥å……è¯­ä¹‰æŒ‡å¯¼ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¸¸é€šè¿‡ä¾èµ–é™æ€è§†è§‰è¡¨ç¤ºè€Œæœªèƒ½å……åˆ†å‘æ˜å…¶æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CSFNetï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç²—ç•¥åˆ°åˆ†ç¦»çš„ç²¾ç»†ç½‘ç»œï¼Œå®ƒå¼•å…¥äº†ä¸€ç§é€’å½’è¯­ä¹‰å¢å¼ºèŒƒå¼ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„åˆ†ç¦»ã€‚CSFNetåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰ç²—ç•¥åˆ†ç¦»ï¼Œç¬¬ä¸€é˜¶æ®µä¼°è®¡ä»æ··åˆå£°éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºç²—ç•¥çš„éŸ³é¢‘æ³¢å½¢ï¼›2ï¼‰ç²¾ç»†åˆ†ç¦»ï¼Œå°†ç²—ç•¥çš„éŸ³é¢‘åé¦ˆè¾“å…¥åˆ°è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹ä»¥åŠè§†è§‰æµä¸­ã€‚è¿™ç§é€’å½’è¿‡ç¨‹äº§ç”Ÿäº†æ›´å…·åŒºåˆ†æ€§çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åç”¨äºæå–ç²¾ç‚¼çš„éŸ³é¢‘ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ©ç”¨è¿™äº›è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥èåˆå—ï¼Œç”¨äºç¼–ç è·¨æ¨¡æ€çš„è¯´è¯äººèº«ä»½ï¼Œä»¥åŠä¸€ä¸ªå¤šèŒƒå›´çš„æ—¶é¢‘åˆ†ç¦»ç½‘ç»œï¼Œç”¨äºæ•æ‰å±€éƒ¨å’Œå…¨å±€çš„æ—¶é—´-é¢‘ç‡æ¨¡å¼ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªå™ªå£°æ•°æ®é›†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCSFNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ç²—ç•¥åˆ°ç²¾ç»†çš„æ”¹è¿›ä¸Šæœ‰æ˜¾è‘—çš„æå‡ï¼ŒéªŒè¯äº†æˆ‘ä»¬é€’å½’è¯­ä¹‰å¢å¼ºæ¡†æ¶çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22425v1">PDF</a> </p>
<p><strong>Summary</strong><br>éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åˆ©ç”¨è§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨ç‰¹å¾ï¼‰æ¥åˆ†ç¦»æ··åˆè¯­éŸ³ä¸­çš„æ¯ä¸ªå‘è¨€äººçš„æ¸…æ™°å£°éŸ³ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯çš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†CSFNetâ€”â€”ä¸€ä¸ªä»ç²—åˆ°ç»†åˆ†ç¦»ç½‘ç»œï¼Œé‡‡ç”¨é€’å½’è¯­ä¹‰å¢å¼ºèŒƒå¼æ¥å®ç°æ›´æœ‰æ•ˆçš„åˆ†ç¦»ã€‚CSFNetåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç²—åˆ†ç¦»é˜¶æ®µï¼Œé€šè¿‡åˆæ­¥ä¼°ç®—ä»æ··åˆè¯­éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºç²—ç•¥çš„éŸ³é¢‘æ³¢å½¢ï¼›ç²¾ç»†åˆ†ç¦»é˜¶æ®µï¼Œå°†ç²—ç•¥çš„éŸ³é¢‘åé¦ˆåˆ°éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹ï¼Œä¸è§†è§‰æµä¸€èµ·ç”Ÿæˆæ›´å…·é‰´åˆ«åŠ›çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç”¨äºæå–ç²¾ç»†çš„éŸ³é¢‘ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›è¯­ä¹‰ä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥èåˆæ¨¡å—å’Œå¤šèŒƒå›´è°±æ—¶é—´åˆ†ç¦»ç½‘ç»œæ¥æ•æ‰è·¨æ¨¡æ€çš„å±€éƒ¨å’Œå…¨å±€æ—¶é—´é¢‘ç‡æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒCSFNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒéªŒè¯äº†é€’å½’è¯­ä¹‰å¢å¼ºæ¡†æ¶çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åˆ©ç”¨è§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨ç‰¹å¾ï¼‰è¿›è¡Œè¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªå……åˆ†å‘æ˜è§†è§‰ä¿¡æ¯çš„æ½œåŠ›ã€‚</li>
<li>CSFNeté‡‡ç”¨é€’å½’è¯­ä¹‰å¢å¼ºèŒƒå¼ï¼Œåˆ†ä¸ºç²—åˆ†ç¦»å’Œç²¾ç»†åˆ†ç¦»ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>ç²—åˆ†ç¦»é˜¶æ®µé€šè¿‡åˆæ­¥ä¼°ç®—ä»æ··åˆè¯­éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºç²—ç•¥éŸ³é¢‘æ³¢å½¢ã€‚</li>
<li>ç²¾ç»†åˆ†ç¦»é˜¶æ®µåˆ©ç”¨é€’å½’è¿‡ç¨‹ç”Ÿæˆæ›´å…·é‰´åˆ«åŠ›çš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>è®¾è®¡äº†æ„ŸçŸ¥èåˆæ¨¡å—ä»¥åˆ©ç”¨è·¨æ¨¡æ€çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c52a7cadf6b56b7da69601ac2524c91" align="middle">
<img src="https://pic1.zhimg.com/v2-cc10bb5c0c2129cea54cc55dfa8e4965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db80c6fe284afacf1e089e2eb630bf50" align="middle">
<img src="https://picx.zhimg.com/v2-1ae29a7ddbe8ba01544a4528b7242145" align="middle">
<img src="https://picx.zhimg.com/v2-299d94445e5781a444fea7203b86dc69" align="middle">
<img src="https://picx.zhimg.com/v2-cb007e37dc53180526b7fd8c144a6d24.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Comprehend-and-Talk-Text-to-Speech-Synthesis-via-Dual-Language-Modeling"><a href="#Comprehend-and-Talk-Text-to-Speech-Synthesis-via-Dual-Language-Modeling" class="headerlink" title="Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling"></a>Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling</h2><p><strong>Authors:Junjie Cao, Yichen Han, Ruonan Zhang, Xiaoyang Hao, Hongxiang Li, Shuaijiang Zhao, Yue Liu, Xiao-Ping Zhng</strong></p>
<p>Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an <code>Understand-then-Generate&#39;&#39; dual-Transformer architecture that decouples comprehension from rendering. An initial </code>Understandingâ€™â€™ Transformer models the cross-modal relationship between text and the audioâ€™s semantic tokens to form a high-level utterance plan. A subsequent &#96;&#96;Generationâ€™â€™ Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. </p>
<blockquote>
<p>ç°æœ‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨å›å½’ï¼ˆARï¼‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè™½ç„¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å“è´¨ï¼Œä½†ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åŸºäºLLMçš„èŒƒå¼çš„åŸºç¡€æ˜¯å°†è¿ç»­çš„è¯­éŸ³æ³¢å½¢ç¦»æ•£åŒ–ä¸ºä¸€ç³»åˆ—ç¦»æ•£ä»¤ç‰Œï¼Œé€šè¿‡ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨å®ç°ã€‚ç„¶è€Œï¼Œå•ä¸€ç¼–ç æœ¬å»ºæ¨¡é€‚åˆæ–‡æœ¬LLMï¼Œä½†å­˜åœ¨ä¿¡æ¯æŸå¤±è¾ƒå¤§çš„é—®é¢˜ï¼›é€šè¿‡æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰ç”Ÿæˆçš„åˆ†å±‚å£°å­¦ä»¤ç‰Œé€šå¸¸ç¼ºä¹æ˜ç¡®çš„è¯­ä¹‰ç»“æ„ï¼Œç»™æ¨¡å‹å¸¦æ¥äº†æ²‰é‡çš„å­¦ä¹ è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨å›å½’è¿‡ç¨‹æœ¬è´¨ä¸Šå®¹æ˜“å—åˆ°è¯¯å·®ç´¯ç§¯çš„å½±å“ï¼Œå¯èƒ½ä¼šé™ä½ç”Ÿæˆç¨³å®šæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CaT-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç¨³å¥å’Œè¯­ä¹‰åŒ–é›¶æ‹åˆæˆçš„å…¨æ–°æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†S3Codecï¼Œè¿™æ˜¯ä¸€ç§åˆ†è£‚RVQç¼–è§£ç å™¨ï¼Œå®ƒé€šè¿‡æ¥è‡ªæœ€æ–°è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è¯­ä¹‰è’¸é¦å°†å…¶ä¸»è¦ç¼–è§£ç å™¨ä¸­æ³¨å…¥æ˜ç¡®çš„è¯­è¨€ç‰¹å¾ï¼Œæä¾›ç»“æ„åŒ–è¡¨ç¤ºï¼Œä»è€Œç®€åŒ–å­¦ä¹ ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§â€œç†è§£ç„¶åç”Ÿæˆâ€çš„åŒTransformeræ¶æ„ï¼Œå°†ç†è§£ä¸æ¸²æŸ“è§£è€¦ã€‚æœ€åˆçš„â€œç†è§£â€Transformerå¯¹æ–‡æœ¬å’ŒéŸ³é¢‘è¯­ä¹‰ä»¤ç‰Œä¹‹é—´çš„è·¨æ¨¡æ€å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å½¢æˆé«˜çº§è¯è¯­è®¡åˆ’ã€‚éšåçš„â€œç”Ÿæˆâ€Transformeråˆ™æ‰§è¡Œæ­¤è®¡åˆ’ï¼Œè‡ªåŠ¨å›å½’åœ°åˆæˆåˆ†å±‚å£°å­¦ä»¤ç‰Œã€‚æœ€åï¼Œä¸ºäº†æé«˜ç”Ÿæˆçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ©ç éŸ³é¢‘å¹¶è¡Œæ¨ç†ï¼ˆMAPIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å‡ ä¹æ— éœ€å‚æ•°çš„æ¨ç†ç­–ç•¥ï¼Œå¯ä»¥åŠ¨æ€æŒ‡å¯¼è§£ç è¿‡ç¨‹ï¼Œå‡å°‘å±€éƒ¨é”™è¯¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22062v1">PDF</a> conference paper about TTS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨å›å½’ï¼ˆARï¼‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè™½ç„¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œä½†ä»é¢ä¸´å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„TTSæ¡†æ¶CaT-TTSï¼Œä»¥å®ç°ç¨³å¥ä¸”è¯­ä¹‰åŒ–çš„é›¶æ ·æœ¬åˆæˆã€‚é€šè¿‡å¼•å…¥S3Codecç¼–ç å™¨å’Œâ€œç†è§£åç”Ÿæˆâ€åŒTransformeræ¶æ„ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œè§£å†³äº†ä¼ ç»ŸLLMæ¨¡å‹åœ¨ä¿¡æ¯æŸå¤±å’Œé”™è¯¯ç´¯ç§¯ç­‰æ–¹é¢çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œè¿˜å¼•å…¥äº†Masked Audio Parallel Inferenceï¼ˆMAPIï¼‰ç­–ç•¥ä»¥æé«˜ç”Ÿæˆç¨³å®šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„TTSç³»ç»Ÿåœ¨æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æŸå¤±å’Œé”™è¯¯ç´¯ç§¯æ–¹é¢ã€‚</li>
<li>S3Codecç¼–ç å™¨é€šè¿‡è¯­ä¹‰è’¸é¦æŠ€æœ¯æ³¨å…¥æ˜¾å¼è¯­è¨€ç‰¹å¾ï¼Œæä¾›äº†ç»“æ„åŒ–çš„è¡¨ç¤ºä»¥ç®€åŒ–å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>â€œç†è§£åç”Ÿæˆâ€åŒTransformeræ¶æ„å°†ç†è§£å’Œæ¸²æŸ“è¿‡ç¨‹è§£è€¦ï¼Œå½¢æˆé«˜çº§è¯è¯­è®¡åˆ’å¹¶æ‰§è¡Œã€‚</li>
<li>CaT-TTSæ¡†æ¶é€šè¿‡å¼•å…¥S3Codecç¼–ç å™¨å’ŒåŒTransformeræ¶æ„ï¼Œå®ç°äº†ç¨³å¥ä¸”è¯­ä¹‰åŒ–çš„é›¶æ ·æœ¬åˆæˆã€‚</li>
<li>Masked Audio Parallel Inferenceï¼ˆMAPIï¼‰ç­–ç•¥èƒ½åŠ¨æ€å¼•å¯¼è§£ç è¿‡ç¨‹ï¼Œå‡å°‘å±€éƒ¨é”™è¯¯ï¼Œæé«˜ç”Ÿæˆç¨³å®šæ€§ã€‚</li>
<li>åˆ†è£‚RVQç¼–ç å™¨çš„ä½¿ç”¨æœ‰æ•ˆè§£å†³äº†å•ä¸€ç¼–ç ä¹¦åœ¨ä¿¡æ¯è¡¨è¾¾å’Œçµæ´»æ€§æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆä»¥ä¸ŠæŠ€æœ¯ï¼ŒCaT-TTSæ¡†æ¶æœ‰æœ›æé«˜TTSç³»ç»Ÿçš„æ€§èƒ½å¹¶å…‹æœç°æœ‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-821b0a2ee5f764bf107f34fbe736c85a" align="middle">
<img src="https://pic1.zhimg.com/v2-5356647aa1b9972f1ee004b862dc2ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-891bc7cd2fc1a606f38558852114be48" align="middle">
<img src="https://picx.zhimg.com/v2-d5adbdfee431fd4ecb0553547272c81d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Decoding-Deception-Understanding-Automatic-Speech-Recognition-Vulnerabilities-in-Evasion-and-Poisoning-Attacks"><a href="#Decoding-Deception-Understanding-Automatic-Speech-Recognition-Vulnerabilities-in-Evasion-and-Poisoning-Attacks" class="headerlink" title="Decoding Deception: Understanding Automatic Speech Recognition   Vulnerabilities in Evasion and Poisoning Attacks"></a>Decoding Deception: Understanding Automatic Speech Recognition   Vulnerabilities in Evasion and Poisoning Attacks</h2><p><strong>Authors:Aravindhan G, Yuvaraj Govindarajulu, Parin Shah</strong></p>
<p>Recent studies have demonstrated the vulnerability of Automatic Speech Recognition systems to adversarial examples, which can deceive these systems into misinterpreting input speech commands. While previous research has primarily focused on white-box attacks with constrained optimizations, and transferability based black-box attacks against commercial Automatic Speech Recognition devices, this paper explores cost efficient white-box attack and non transferability black-box adversarial attacks on Automatic Speech Recognition systems, drawing insights from approaches such as Fast Gradient Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper includes how poisoning attack can degrade the performances of state-of-the-art models leading to misinterpretation of audio signals. Through experimentation and analysis, we illustrate how hybrid models can generate subtle yet impactful adversarial examples with very little perturbation having Signal Noise Ratio of 35dB that can be generated within a minute. These vulnerabilities of state-of-the-art open source model have practical security implications, and emphasize the need for adversarial security. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå®¹æ˜“å—åˆ°å¯¹æŠ—æ ·æœ¬çš„å¨èƒï¼Œè¿™äº›å¯¹æŠ—æ ·æœ¬å¯ä»¥æ¬ºéª—ç³»ç»Ÿè¯¯è§£è¾“å…¥çš„è¯­éŸ³å‘½ä»¤ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å…·æœ‰çº¦æŸä¼˜åŒ–çš„ç™½ç›’æ”»å‡»å’ŒåŸºäºè¿ç§»æ€§çš„é»‘ç›’æ”»å‡»å•†ä¸šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¾å¤‡ï¼Œä½†æœ¬æ–‡æ¢è®¨äº†æˆæœ¬æ•ˆç›Šé«˜çš„ç™½ç›’æ”»å‡»å’Œéè¿ç§»æ€§çš„é»‘ç›’å¯¹æŠ—æ”»å‡»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œä»å¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•å’Œé›¶é˜¶ä¼˜åŒ–ç­‰æ–¹æ³•ä¸­æ±²å–çµæ„Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–°é¢–ä¹‹å¤„åœ¨äºï¼Œå¦‚ä½•ä½¿æ¯’ç‰©æ”»å‡»èƒ½é™ä½æœ€å…ˆè¿›çš„æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯¼è‡´å¯¹éŸ³é¢‘ä¿¡å·çš„è¯¯è§£ã€‚é€šè¿‡è¯•éªŒå’Œåˆ†æï¼Œæˆ‘ä»¬è¯´æ˜äº†æ··åˆæ¨¡å‹å¦‚ä½•ç”Ÿæˆç»†å¾®ä½†å½±å“é‡å¤§çš„å¯¹æŠ—æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬çš„æ‰°åŠ¨éå¸¸å°ï¼Œä¿¡å·å™ªå£°æ¯”ä¸º35åˆ†è´ï¼Œä¸”å¯åœ¨ä¸€åˆ†é’Ÿå†…ç”Ÿæˆã€‚è¿™äº›æœ€æ–°å¼€æºæ¨¡å‹çš„æ¼æ´å¯¹å®é™…å®‰å…¨æœ‰é‡å¤§å½±å“ï¼Œå¹¶å¼ºè°ƒäº†å¯¹æŠ—æ€§å®‰å…¨çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22060v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ˜“å—å¯¹æŠ—æ ·æœ¬æ”»å‡»ï¼Œè¿™äº›å¯¹æŠ—æ ·æœ¬èƒ½è¯¯å¯¼ç³»ç»Ÿè¯¯è§£è¯­éŸ³æŒ‡ä»¤ã€‚æœ¬æ–‡æ¢è®¨äº†æˆæœ¬æ•ˆç›Šé«˜çš„ç™½ç›’æ”»å‡»å’Œéè¿ç§»æ€§é»‘ç›’å¯¹æŠ—æ”»å‡»ï¼Œå¹¶ä»å¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•å’Œé›¶é˜¶ä¼˜åŒ–ç­‰æ–¹æ³•ä¸­æ±²å–çµæ„Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†ä¸­æ¯’æ”»å‡»å¦‚ä½•å¯¼è‡´å…ˆè¿›æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œè¯¯è§£éŸ³é¢‘ä¿¡å·ã€‚å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œæ··åˆæ¨¡å‹å¯ä»¥ç”Ÿæˆå…·æœ‰å¾ˆå°æ‰°åŠ¨ä½†å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå½±å“æ˜¾è‘—çš„å¯¹æŠ—æ ·æœ¬ï¼Œä¿¡å™ªæ¯”é«˜è¾¾35åˆ†è´ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€ä¸€åˆ†é’Ÿã€‚è¿™äº›å…ˆè¿›å¼€æºæ¨¡å‹çš„æ¼æ´å…·æœ‰å®é™…çš„å®‰å…¨å½±å“ï¼Œå¹¶å¼ºè°ƒå¯¹æŠ—æ€§å®‰å…¨çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ˜“å—å¯¹æŠ—æ ·æœ¬æ”»å‡»ï¼Œå¯èƒ½è¯¯å¯¼ç³»ç»Ÿè¯¯è§£è¯­éŸ³æŒ‡ä»¤ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†æˆæœ¬æ•ˆç›Šé«˜çš„ç™½ç›’æ”»å‡»å’Œéè¿ç§»æ€§é»‘ç›’å¯¹æŠ—æ”»å‡»ã€‚</li>
<li>ä¸­æ¯’æ”»å‡»å¯å¯¼è‡´å…ˆè¿›æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œç”šè‡³è¯¯è§£éŸ³é¢‘ä¿¡å·ã€‚</li>
<li>æ··åˆæ¨¡å‹èƒ½ç”Ÿæˆå…·æœ‰å¾®å°æ‰°åŠ¨çš„å¯¹æŠ—æ ·æœ¬ï¼Œä½†å¯¹ç³»ç»Ÿå½±å“æ˜¾è‘—ã€‚</li>
<li>ä¿¡å™ªæ¯”é«˜è¾¾35åˆ†è´çš„å¯¹æŠ—æ ·æœ¬å¯åœ¨ä¸€åˆ†é’Ÿå†…ç”Ÿæˆã€‚</li>
<li>å…ˆè¿›å¼€æºæ¨¡å‹çš„æ¼æ´å…·æœ‰å®é™…çš„å®‰å…¨å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d5571f7d2da81fa9ad45b52b6b518659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ed1f12eb8300d03d8657d6bc5c33ec5" align="middle">
<img src="https://pica.zhimg.com/v2-7618d0fb17135c321ec721d6a1db01f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40761333f622d9c580bd7d8cd824dea6" align="middle">
<img src="https://pica.zhimg.com/v2-3e76032cabb3e0d3608b3ca12040a7ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0f8e14e9b6d5ff807f4fd7418a85636.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FastEnhancer-Speed-Optimized-Streaming-Neural-Speech-Enhancement"><a href="#FastEnhancer-Speed-Optimized-Streaming-Neural-Speech-Enhancement" class="headerlink" title="FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement"></a>FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement</h2><p><strong>Authors:Sunghwan Ahn, Jinmo Han, Beom Jun Woo, Nam Soo Kim</strong></p>
<p>Streaming speech enhancement is a crucial task for real-time applications such as online meetings, smart home appliances, and hearing aids. Deep neural network-based approaches achieve exceptional performance while demanding substantial computational resources. Although recent neural speech enhancement models have succeeded in reducing the number of parameters and multiply-accumulate operations, their sophisticated architectures often introduce significant processing latency on common hardware. In this work, we propose FastEnhancer, a streaming neural speech enhancement model designed explicitly to minimize real-world latency. It features a simple encoder-decoder structure with efficient RNNFormer blocks. Evaluations on various objective metrics show that FastEnhancer achieves state-of-the-art speech quality and intelligibility while simultaneously demonstrating the fastest processing speed on a single CPU thread. Code and pre-trained weights are publicly available (<a target="_blank" rel="noopener" href="https://github.com/aask1357/fastenhancer">https://github.com/aask1357/fastenhancer</a>). </p>
<blockquote>
<p>æµå¼è¯­éŸ³å¢å¼ºæ˜¯å®æ—¶åº”ç”¨ï¼ˆå¦‚åœ¨çº¿ä¼šè®®ã€æ™ºèƒ½å®¶ç”µå’ŒåŠ©å¬å™¨ï¼‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†å“è¶Šçš„è¡¨ç°ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚å°½ç®¡æœ€è¿‘çš„ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºæ¨¡å‹æˆåŠŸå‡å°‘äº†å‚æ•°å’Œä¹˜ç§¯ç´¯æ“ä½œçš„æ•°é‡ï¼Œä½†å®ƒä»¬å¤æ‚çš„æ¶æ„é€šå¸¸åœ¨å¸¸è§ç¡¬ä»¶ä¸Šå¼•å…¥æ˜¾è‘—çš„å¤„ç†å»¶è¿Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FastEnhancerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºæœ€å°åŒ–ç°å®ä¸–ç•Œå»¶è¿Ÿçš„æµå¼ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºæ¨¡å‹ã€‚å®ƒé‡‡ç”¨ç®€å•çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œå¹¶é…å¤‡äº†é«˜æ•ˆçš„RNNFormerå—ã€‚åœ¨å¤šç§å®¢è§‚æŒ‡æ ‡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFastEnhanceråœ¨è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶åœ¨å•ä¸ªCPUçº¿ç¨‹ä¸Šå±•ç¤ºäº†æœ€å¿«çš„å¤„ç†é€Ÿåº¦ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å¯å…¬å¼€è·å–ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/aask1357/fastenhancer%EF%BC%89%E3%80%82">https://github.com/aask1357/fastenhancerï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21867v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å®æ—¶è¯­éŸ³å¢å¼ºå¯¹äºåœ¨çº¿ä¼šè®®ã€æ™ºèƒ½å®¶ç”µå’ŒåŠ©å¬å™¨ç­‰å®æ—¶åº”ç”¨è‡³å…³é‡è¦ã€‚åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚FastEnhanceræ˜¯ä¸€ä¸ªä¸“ä¸ºå‡å°‘å®é™…å»¶è¿Ÿè€Œè®¾è®¡çš„æµå¼ç¥ç»è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œå…·æœ‰ç®€å•çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œé‡‡ç”¨é«˜æ•ˆçš„RNNFormerå—ã€‚å®ƒåœ¨å•çº¿ç¨‹CPUä¸Šå®ç°äº†æœ€å¿«çš„å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶è¯­éŸ³å¢å¼ºå¯¹äºåœ¨çº¿ä¼šè®®ã€æ™ºèƒ½å®¶ç”µå’ŒåŠ©å¬å™¨ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>FastEnhanceræ¨¡å‹ä¸“ä¸ºå‡å°‘å®é™…å»¶è¿Ÿè€Œè®¾è®¡ï¼Œé€‚ç”¨äºæµå¼å¤„ç†ã€‚</li>
<li>FastEnhancerå…·æœ‰ç®€å•çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œé‡‡ç”¨é«˜æ•ˆçš„RNNFormerå—ã€‚</li>
<li>FastEnhanceråœ¨å•çº¿ç¨‹CPUä¸Šå®ç°äº†æœ€å¿«çš„å¤„ç†é€Ÿåº¦ã€‚</li>
<li>FastEnhancerè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d92ec7d40494ccedce85d0e83aedd35f" align="middle">
<img src="https://pic1.zhimg.com/v2-051615f336441efad34b6ffa39325d20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9409d123084b7b81d6df364ba58ca7f6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Sound-Audio-Chain-of-Thought-Enables-Multimodal-Reasoning-in-Large-Audio-Language-Models"><a href="#Thinking-with-Sound-Audio-Chain-of-Thought-Enables-Multimodal-Reasoning-in-Large-Audio-Language-Models" class="headerlink" title="Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning   in Large Audio-Language Models"></a>Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning   in Large Audio-Language Models</h2><p><strong>Authors:Zhen Xiong, Yujun Cai, Zhecheng Li, Junsong Yuan, Yiwei Wang</strong></p>
<p>Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q&amp;A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce Thinking-with-Sound (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively think with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct MELD-Hard1k, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than $50%$ compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain $24.73%$ absolute accuracy, with improvements scaling consistently up to $36.61%$ for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems. </p>
<blockquote>
<p>æœ€æ–°çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨å„ç§éŸ³é¢‘ç†è§£ä»»åŠ¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘é—®ç­”ï¼‰ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚çš„å£°å­¦åœºæ™¯ä¸­çš„æŒ‘æˆ˜æ€§éŸ³é¢‘æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚è¿™äº›æƒ…å†µå°†å¤§å¤§å—ç›Šäºä½¿ç”¨å™ªå£°æŠ‘åˆ¶ã€éŸ³æºåˆ†ç¦»å’Œç²¾ç¡®æ—¶é—´å¯¹é½ç­‰å£°å­¦å·¥å…·ï¼Œä½†å½“å‰çš„LALMsæ— æ³•è®¿é—®è¿™äº›å·¥å…·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Thinking-with-Soundï¼ˆTwSï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆè¯­è¨€æ¨ç†å’Œå³æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä¸ºLALMsé…å¤‡éŸ³é¢‘è®¤çŸ¥ã€‚ä¸åŒäºå°†éŸ³é¢‘è§†ä¸ºé™æ€è¾“å…¥çš„æ–¹æ³•ï¼ŒTwSä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä¸éŸ³é¢‘ä¿¡å·è¿›è¡Œäº¤äº’æ€è€ƒï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨ç†è¿›è¡Œæ•°å€¼åˆ†æå’Œæ•°å­—æ“ä½œã€‚ä¸ºäº†è¯„ä¼°è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†MELD-Hard1kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é²æ£’æ€§åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¼•å…¥å„ç§å£°å­¦æ‰°åŠ¨æ¥åˆ›å»ºã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LALMsåœ¨MELD-Hard1kä¸Šé­å—äº†æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œä¸å¹²å‡€éŸ³é¢‘ç›¸æ¯”ï¼Œå‡†ç¡®ç‡ä¸‹é™äº†è¶…è¿‡50%ã€‚TwSåœ¨é²æ£’æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼šå°å‹æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†24.73%ï¼Œå¤§å‹æ¨¡å‹çš„å‡†ç¡®ç‡åˆ™ç¨³å®šæé«˜è‡³36.61%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘è®¤çŸ¥å¯ä»¥åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜é²æ£’æ€§ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥çš„éŸ³é¢‘ç†è§£ç³»ç»Ÿæ‰“å¼€äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21749v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€è¿‘çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨å¤šç§éŸ³é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘é—®ç­”ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é¢å¯¹å¤æ‚å£°å­¦åœºæ™¯ä¸­çš„æŒ‘æˆ˜éŸ³é¢‘æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æ˜æ˜¾å±€é™ã€‚è¿™äº›æƒ…å†µå°†æå¤§å—ç›Šäºä½¿ç”¨å™ªå£°æŠ‘åˆ¶ã€å£°æºåˆ†ç¦»å’Œç²¾ç¡®æ—¶é—´å¯¹é½ç­‰å£°å­¦å·¥å…·ï¼Œä½†å½“å‰çš„LALMæ— æ³•è®¿é—®è¿™äº›å·¥å…·ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæˆ‘ä»¬æå‡ºThinking-with-Soundï¼ˆTwSï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­è¨€æ¨ç†å’Œå³æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä¸ºLALMé…å¤‡éŸ³é¢‘è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚ä¸åŒäºå°†éŸ³é¢‘è§†ä¸ºé™æ€è¾“å…¥ç°æœ‰æ–¹æ³•ï¼ŒTwSä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä¸éŸ³é¢‘ä¿¡å·æ€è€ƒï¼Œæ‰§è¡Œæ•°å€¼åˆ†æå’Œæ•°å­—æ“ä½œé€šè¿‡å¤šæ¨¡æ€æ¨ç†ã€‚ä¸ºè¯„ä¼°æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†MELD-Hard1kæ–°é²æ£’æ€§åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¼•å…¥å„ç§å£°å­¦æ‰°åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LALMåœ¨MELD-Hard1kä¸Šè¡¨ç°å‰§çƒˆä¸‹é™ï¼Œä¸å¹²å‡€éŸ³é¢‘ç›¸æ¯”ï¼Œå‡†ç¡®æ€§ä¸‹é™è¶…è¿‡50%ã€‚TwSåœ¨é²æ£’æ€§æ–¹é¢å–å¾—å®è´¨æ€§æ”¹è¿›ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼šå°å‹æ¨¡å‹è·å¾—24.73%çš„ç»å¯¹å‡†ç¡®æ€§æå‡ï¼Œä¸”æ”¹è¿›åœ¨å¤§å‹æ¨¡å‹ä¸­ä¸€ç›´æ‰©å¤§åˆ°36.61%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘è®¤çŸ¥æ¨ç†å¯æ˜¾è‘—æé«˜é²æ£’æ€§è€Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œä¸ºå¼€å‘æ›´ç¨³å¥çš„éŸ³é¢‘ç†è§£ç³»ç»Ÿæ‰“å¼€äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨éŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚å£°å­¦åœºæ™¯ä¸­çš„éŸ³é¢‘æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>ç°æœ‰æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨å™ªå£°æŠ‘åˆ¶ã€å£°æºåˆ†ç¦»å’Œç²¾ç¡®æ—¶é—´å¯¹é½ç­‰å£°å­¦å·¥å…·ã€‚</li>
<li>Thinking-with-Soundï¼ˆTwSï¼‰æ¡†æ¶ç»“åˆäº†è¯­è¨€æ¨ç†å’Œå³æ—¶éŸ³é¢‘åŸŸåˆ†æï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä¸éŸ³é¢‘ä¿¡å·äº’åŠ¨ã€‚</li>
<li>TwSé€šè¿‡å¤šæ¨¡æ€æ¨ç†æ‰§è¡Œæ•°å€¼åˆ†æå’Œæ•°å­—æ“ä½œã€‚</li>
<li>MELD-Hard1kåŸºå‡†æµ‹è¯•æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é¢ä¸´å£°å­¦æ‰°åŠ¨æ—¶çš„æ€§èƒ½å±€é™ã€‚</li>
<li>TwSæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3687729224a50b72276d26ed39b6bf9d" align="middle">
<img src="https://picx.zhimg.com/v2-d0194696994647ac259ae80b98f2d132" align="middle">
<img src="https://pica.zhimg.com/v2-b82c50d6a575cff70f289f3694328020.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Align2Speak-Improving-TTS-for-Low-Resource-Languages-via-ASR-Guided-Online-Preference-Optimization"><a href="#Align2Speak-Improving-TTS-for-Low-Resource-Languages-via-ASR-Guided-Online-Preference-Optimization" class="headerlink" title="Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided   Online Preference Optimization"></a>Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided   Online Preference Optimization</h2><p><strong>Authors:Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Roy Fejgin, Ryan Langman, Mikyas Desta, Leili Tavabi, Jason Li</strong></p>
<p>Developing high-quality text-to-speech (TTS) systems for low-resource languages is challenging due to the scarcity of paired text and speech data. In contrast, automatic speech recognition (ASR) models for such languages are often more accessible, owing to large-scale multilingual pre-training efforts. We propose a framework based on Group Relative Policy Optimization (GRPO) to adapt an autoregressive, multilingual TTS model to new languages. Our method first establishes a language-agnostic foundation for TTS synthesis by training a multilingual baseline with International Phonetic Alphabet (IPA) tokens. Next, we fine-tune this model on limited paired data of the new languages to capture the target languageâ€™s prosodic features. Finally, we apply GRPO to optimize the model using only unpaired text and speaker prompts, guided by a multi-objective reward from pretrained ASR, speaker verification, and audio quality estimation models. Experiments demonstrate that this pipeline produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone. Furthermore, our GRPO-based framework also improves TTS performance in high-resource languages, surpassing offline alignment methods such as Direct Preference Optimization (DPO) yielding superior intelligibility, speaker similarity, and audio quality. </p>
<blockquote>
<p>é’ˆå¯¹ä½èµ„æºè¯­è¨€çš„ä¼˜è´¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å‘å±•é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé…å¯¹æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®çš„ç¨€ç¼ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºå¤§è§„æ¨¡çš„å¤šè¯­è¨€é¢„è®­ç»ƒåŠªåŠ›ï¼Œæ­¤ç±»è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹é€šå¸¸æ›´å®¹æ˜“è·å¾—ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ¡†æ¶ï¼Œä»¥é€‚åº”æ–°çš„å¤šè¯­è¨€è‡ªå›å½’TTSæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡ä¸å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰ç¬¦å·ä¸€èµ·è®­ç»ƒå¤šè¯­è¨€åŸºçº¿ï¼Œä¸ºTTSåˆæˆå»ºç«‹ä¸€ç§è¯­è¨€æ— å…³çš„åŸºç¡€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨æ–°è¯­è¨€çš„æœ‰é™é…å¯¹æ•°æ®ä¸Šå¾®è°ƒæ­¤æ¨¡å‹ï¼Œä»¥æ•è·ç›®æ ‡è¯­è¨€çš„éŸµå¾‹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨GRPOä»…ä½¿ç”¨æœªé…å¯¹çš„æ–‡æœ¬å’Œæ¼”è®²æç¤ºæ¥ä¼˜åŒ–æ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ASRã€è¯´è¯äººéªŒè¯å’ŒéŸ³é¢‘è´¨é‡ä¼°è®¡æ¨¡å‹çš„å¤šç›®æ ‡å¥–åŠ±è¿›è¡Œå¼•å¯¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®¡é“åœ¨èµ„æºè´«ä¹çš„è¯­è¨€ä¸­äº§ç”Ÿå¯ç†è§£çš„ã€ä¸è¯´è¯äººä¸€è‡´çš„è¯­éŸ³ï¼Œæ˜æ˜¾ä¼˜äºå•çº¯çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºGRPOçš„æ¡†æ¶è¿˜æé«˜äº†é«˜èµ„æºè¯­è¨€çš„TTSæ€§èƒ½ï¼Œè¶…è¶Šäº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰ç¦»çº¿å¯¹é½æ–¹æ³•ï¼Œäº§ç”Ÿäº†æ›´å¥½çš„å¯ç†è§£æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21718v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGroup Relative Policy Optimization (GRPO)çš„æ–°æ¡†æ¶ï¼Œç”¨äºé€‚åº”ä½èµ„æºè¯­è¨€çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚è¯¥æ¡†æ¶é¦–å…ˆå»ºç«‹ä¸€ä¸ªå¤šè¯­è¨€åŸºçº¿æ¨¡å‹ï¼Œä»¥å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰ä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œå½¢æˆè¯­è¨€æ— å…³çš„åŸºç¡€ã€‚ç„¶åï¼Œåœ¨æ–°è¯­è¨€çš„æœ‰é™é…å¯¹æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæ•æ‰ç›®æ ‡è¯­è¨€çš„éŸµå¾‹ç‰¹å¾ã€‚æœ€åï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººéªŒè¯å’ŒéŸ³é¢‘è´¨é‡ä¼°è®¡æ¨¡å‹çš„å¤šå…ƒç›®æ ‡å¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç®¡é“åœ¨ä½èµ„æºè¯­è¨€ä¸­ç”Ÿæˆäº†å¯ç†è§£çš„ã€å…·æœ‰è¯´è¯äººç‰¹è‰²çš„è¯­éŸ³ï¼Œæ˜æ˜¾ä¼˜äºä»…è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒåŸºäºGRPOçš„æ¡†æ¶åœ¨é«˜èµ„æºè¯­è¨€ä¸­ä¹Ÿèƒ½æé«˜TTSæ€§èƒ½ï¼Œè¶…è¶Šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰ç¦»çº¿å¯¹é½æ–¹æ³•ï¼Œå¸¦æ¥æ›´é«˜çš„å¯ç†è§£æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºGroup Relative Policy Optimization (GRPO)çš„æ¡†æ¶ï¼Œç”¨äºä½èµ„æºè¯­è¨€çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡å»ºç«‹å¤šè¯­è¨€åŸºçº¿æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰ä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œå½¢æˆè¯­è¨€æ— å…³çš„åŸºç¡€ã€‚</li>
<li>åˆ©ç”¨æœ‰é™çš„é…å¯¹æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•æ‰ç›®æ ‡è¯­è¨€çš„éŸµå¾‹ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€è¯´è¯äººéªŒè¯å’ŒéŸ³é¢‘è´¨é‡ä¼°è®¡æ¨¡å‹ä½œä¸ºå¤šå…ƒç›®æ ‡å¥–åŠ±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½ç”Ÿæˆå¯ç†è§£çš„ã€å…·æœ‰è¯´è¯äººç‰¹è‰²çš„è¯­éŸ³ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨é«˜èµ„æºè¯­è¨€ä¸­ä¹Ÿèƒ½æé«˜TTSæ€§èƒ½ï¼Œè¶…è¶Šäº†ç¦»çº¿å¯¹é½æ–¹æ³•å¦‚Direct Preference Optimization (DPO)ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6b5c43704a64dd7a50c181c720d9e2c" align="middle">
<img src="https://picx.zhimg.com/v2-3ce0e7b0c70f9121b0fe793905ee00f6" align="middle">
<img src="https://picx.zhimg.com/v2-db2fc50a670cf665e8debd74954a1d3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da6e73540d28fc38ba50fbf9d0c843e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8b78ef6dcc3a27d4d97ff490c592b7d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HuLA-Prosody-Aware-Anti-Spoofing-with-Multi-Task-Learning-for-Expressive-and-Emotional-Synthetic-Speech"><a href="#HuLA-Prosody-Aware-Anti-Spoofing-with-Multi-Task-Learning-for-Expressive-and-Emotional-Synthetic-Speech" class="headerlink" title="HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for   Expressive and Emotional Synthetic Speech"></a>HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for   Expressive and Emotional Synthetic Speech</h2><p><strong>Authors:Aurosweta Mahapatra, Ismail Rasim Ulgen, Berrak Sisman</strong></p>
<p>Current anti-spoofing systems remain vulnerable to expressive and emotional synthetic speech, since they rarely leverage prosody as a discriminative cue. Prosody is central to human expressiveness and emotion, and humans instinctively use prosodic cues such as F0 patterns and voiced&#x2F;unvoiced structure to distinguish natural from synthetic speech. In this paper, we propose HuLA, a two-stage prosody-aware multi-task learning framework for spoof detection. In Stage 1, a self-supervised learning (SSL) backbone is trained on real speech with auxiliary tasks of F0 prediction and voiced&#x2F;unvoiced classification, enhancing its ability to capture natural prosodic variation similar to human perceptual learning. In Stage 2, the model is jointly optimized for spoof detection and prosody tasks on both real and synthetic data, leveraging prosodic awareness to detect mismatches between natural and expressive synthetic speech. Experiments show that HuLA consistently outperforms strong baselines on challenging out-of-domain dataset, including expressive, emotional, and cross-lingual attacks. These results demonstrate that explicit prosodic supervision, combined with SSL embeddings, substantially improves robustness against advanced synthetic speech attacks. </p>
<blockquote>
<p>å½“å‰çš„åæ¬ºéª—ç³»ç»Ÿåœ¨é¢å¯¹è¡¨è¾¾æ€§å’Œæƒ…æ„Ÿæ€§åˆæˆè¯­éŸ³æ—¶ä»ç„¶å®¹æ˜“å—åˆ°å½±å“ï¼Œå› ä¸ºå®ƒä»¬å¾ˆå°‘åˆ©ç”¨éŸµå¾‹ä½œä¸ºåˆ¤åˆ«çº¿ç´¢ã€‚éŸµå¾‹åœ¨äººç±»è¡¨è¾¾å’Œæƒ…æ„Ÿä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œäººç±»æœ¬èƒ½åœ°ä½¿ç”¨è¯¸å¦‚F0æ¨¡å¼å’Œæœ‰å£°æ— å£°ç»“æ„ç­‰éŸµå¾‹çº¿ç´¢æ¥åŒºåˆ†è‡ªç„¶è¯­éŸ³å’Œåˆæˆè¯­éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HuLAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸµå¾‹æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¬ºéª—æ£€æµ‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡çœŸå®è¯­éŸ³æ•°æ®è®­ç»ƒè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸»å¹²ç½‘ç»œï¼Œå¹¶è¾…ä»¥F0é¢„æµ‹å’Œæœ‰å£°æ— å£°åˆ†ç±»çš„è¾…åŠ©ä»»åŠ¡ï¼Œå¢å¼ºäº†å…¶æ•æ‰è‡ªç„¶éŸµå¾‹å˜åŒ–çš„èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»æ„ŸçŸ¥å­¦ä¹ ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹å¯¹çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„æ¬ºéª—æ£€æµ‹å’ŒéŸµå¾‹ä»»åŠ¡è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œåˆ©ç”¨éŸµå¾‹æ„ŸçŸ¥æ¥æ£€æµ‹è‡ªç„¶è¯­éŸ³å’Œè¡¨è¾¾æ€§åˆæˆè¯­éŸ³ä¹‹é—´çš„ä¸åŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŒ…æ‹¬è¡¨è¾¾æ€§ã€æƒ…æ„Ÿæ€§å’Œè·¨è¯­è¨€æ”»å‡»åœ¨å†…çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šï¼ŒHuLAå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ç³»ç»Ÿã€‚è¿™äº›ç»“æœè¯æ˜ï¼Œæ˜ç¡®çš„éŸµå¾‹ç›‘ç£ä¸SSLåµŒå…¥ç›¸ç»“åˆï¼Œå¤§å¤§æé«˜äº†å¯¹é«˜çº§åˆæˆè¯­éŸ³æ”»å‡»çš„é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21676v1">PDF</a> Submitted to IEEE Transactions on Affective Computing</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºHuLAçš„ä¸¤é˜¶æ®µéŸµå¾‹æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæŠ—æ¬ºéª—æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è®­ç»ƒçœŸå®è¯­éŸ³çš„æ¨¡å‹ï¼Œå¹¶è¾…ä»¥F0é¢„æµ‹å’Œå£°éŸ³åˆ†ç±»ä»»åŠ¡ï¼Œå¢å¼ºäº†æ•æ‰è‡ªç„¶éŸµå¾‹å˜åŒ–çš„èƒ½åŠ›ã€‚éšåï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šè”åˆä¼˜åŒ–æ¬ºéª—æ£€æµ‹å’ŒéŸµå¾‹ä»»åŠ¡ï¼Œåˆ©ç”¨éŸµå¾‹æ„ŸçŸ¥æ¥æ£€æµ‹è‡ªç„¶å’Œåˆæˆè¯­éŸ³ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒè¡¨æ˜ï¼ŒHuLAåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨åŸŸæ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜éŸµå¾‹æ„ŸçŸ¥å¯¹äºè¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾å’Œè·¨è¯­è¨€æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚æ­¤ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç»“åˆéŸµå¾‹æ„ŸçŸ¥ç›‘ç£å’ŒSSLåµŒå…¥ï¼Œå¯æ˜¾è‘—æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åæ¬ºéª—ç³»ç»Ÿå®¹æ˜“å—åˆ°è¡¨è¾¾å’Œæƒ…æ„Ÿåˆæˆè¯­éŸ³çš„æ”»å‡»ï¼Œå› ä¸ºå®ƒä»¬å¾ˆå°‘åˆ©ç”¨éŸµå¾‹ä½œä¸ºè¾¨åˆ«çº¿ç´¢ã€‚</li>
<li>éŸµå¾‹åœ¨äººç±»è¡¨è¾¾å’Œæƒ…æ„Ÿä¸­èµ·å…³é”®ä½œç”¨ï¼Œäººç±»æœ¬èƒ½åœ°ä½¿ç”¨éŸµå¾‹çº¿ç´¢æ¥åŒºåˆ†è‡ªç„¶å’Œåˆæˆè¯­éŸ³ã€‚</li>
<li>HuLAæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸµå¾‹æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¬ºéª—æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è®­ç»ƒæ¨¡å‹ï¼Œè¾…ä»¥F0é¢„æµ‹å’Œå£°éŸ³åˆ†ç±»ä»»åŠ¡ï¼Œå¢å¼ºæ•æ‰è‡ªç„¶éŸµå¾‹å˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šè”åˆä¼˜åŒ–æ¬ºéª—æ£€æµ‹å’ŒéŸµå¾‹ä»»åŠ¡ï¼Œåˆ©ç”¨éŸµå¾‹æ„ŸçŸ¥æ¥æ£€æµ‹è‡ªç„¶å’Œåˆæˆè¯­éŸ³ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>å®éªŒè¯æ˜HuLAåœ¨æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æƒ…æ„Ÿè¡¨è¾¾å’Œè·¨è¯­è¨€æ”»å‡»æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24f682f01c4fbcbc530fce852db27503" align="middle">
<img src="https://picx.zhimg.com/v2-a8507d0691c0e5435f375441c397c50f" align="middle">
<img src="https://picx.zhimg.com/v2-670c254014ec77c8f14fd2873cb62fc1" align="middle">
<img src="https://picx.zhimg.com/v2-a8e5c770117da76b0cd6b10f540874d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab304ccb05d65efdc4151ebc8a9d76cd" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Shortcut-Flow-Matching-for-Speech-Enhancement-Step-Invariant-flows-via-single-stage-training"><a href="#Shortcut-Flow-Matching-for-Speech-Enhancement-Step-Invariant-flows-via-single-stage-training" class="headerlink" title="Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via   single stage training"></a>Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via   single stage training</h2><p><strong>Authors:Naisong Zhou, Saisamarth Rajesh Phaye, Milos Cernak, Tijana Stojkovic, Andy Pearce, Andrea Cavallaro, Andy Harper</strong></p>
<p>Diffusion-based generative models have achieved state-of-the-art performance for perceptual quality in speech enhancement (SE). However, their iterative nature requires numerous Neural Function Evaluations (NFEs), posing a challenge for real-time applications. On the contrary, flow matching offers a more efficient alternative by learning a direct vector field, enabling high-quality synthesis in just a few steps using deterministic ordinary differential equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech Enhancement (SFMSE), a novel approach that trains a single, step-invariant model. By conditioning the velocity field on the target time step during a one-stage training process, SFMSE can perform single, few, or multi-step denoising without any architectural changes or fine-tuning. Our results demonstrate that a single-step SFMSE inference achieves a real-time factor (RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable to a strong diffusion baseline requiring 60 NFEs. This work also provides an empirical analysis of the role of stochasticity in training and inference, bridging the gap between high-quality generative SE and low-latency constraints. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰çš„æ„ŸçŸ¥è´¨é‡æ–¹é¢å·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¿­ä»£æ€§è´¨éœ€è¦å¤§é‡çš„ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œè¿™ä¸ºå…¶åœ¨å®æ—¶åº”ç”¨æ–¹é¢å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæµåŒ¹é…æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡å­¦ä¹ ä¸€ä¸ªç›´æ¥çš„å‘é‡åœºï¼Œä½¿ç”¨ç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨ï¼Œèƒ½å¤Ÿåœ¨å‡ æ­¥å†…å®ç°é«˜è´¨é‡åˆæˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­éŸ³å¢å¼ºçš„å¿«æ·æ–¹å¼æµåŒ¹é…ï¼ˆSFMSEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè®­ç»ƒä¸€ä¸ªå•ä¸€ã€æ­¥éª¤ä¸å˜å‹çš„æ¨¡å‹ã€‚é€šè¿‡åœ¨å•é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ä¸­ä»¥ç›®æ ‡æ—¶é—´æ­¥é•¿ä½œä¸ºæ¡ä»¶ï¼ŒSFMSEå¯ä»¥æ‰§è¡Œå•æ­¥ã€å°‘æ­¥æˆ–å¤šæ­¥å»å™ªï¼Œæ— éœ€è¿›è¡Œä»»ä½•ç»“æ„æ›´æ”¹æˆ–å¾®è°ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå•æ­¥SFMSEæ¨ç†åœ¨æ¶ˆè´¹è€…GPUä¸Šå®ç°äº†å®æ—¶å› å­ï¼ˆRTFï¼‰ä¸º0.013ï¼ŒåŒæ—¶æä¾›äº†ä¸éœ€è¦60ä¸ªNFEçš„å¼ºå¤§æ‰©æ•£åŸºå‡†ç›¸å½“çš„å¯æ„ŸçŸ¥è´¨é‡ã€‚è¿™é¡¹å·¥ä½œè¿˜å¯¹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éšæœºæ€§çš„ä½œç”¨è¿›è¡Œäº†å®è¯åˆ†æï¼Œç¼©å°äº†é«˜è´¨é‡ç”ŸæˆSEå’Œä½å»¶è¿Ÿçº¦æŸä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21522v1">PDF</a> 5 pages, 2 figures, submitted to ICASSP2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³å¢å¼ºä»»åŠ¡çš„æ–°å‹æ–¹æ³•â€”â€”Shortcut Flow Matching for Speech Enhancementï¼ˆSFMSEï¼‰ã€‚è¯¥æ³•åˆ©ç”¨ç›´æ¥çŸ¢é‡åœºå­¦ä¹ ï¼Œèƒ½åœ¨å°‘é‡æ­¥éª¤å†…å®ç°é«˜è´¨é‡è¯­éŸ³åˆæˆï¼Œä¸”é€šè¿‡æ™®é€šå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨å®ç°ã€‚SFMSEè®­ç»ƒå•ä¸€ã€æ­¥éª¤ä¸å˜çš„æ¨¡å‹ï¼Œèƒ½åœ¨å•æ­¥ã€å°‘æ­¥æˆ–å¤šæ­¥å»å™ªä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€æ¶æ„è°ƒæ•´æˆ–å¾®è°ƒã€‚åœ¨æ¶ˆè´¹è€…GPUä¸Šï¼Œå•æ­¥SFMSEæ¨ç†å®ç°å®æ—¶å› å­ï¼ˆRTFï¼‰ä¸º0.013ï¼Œæ„ŸçŸ¥è´¨é‡ä¸éœ€è¦60æ¬¡NFEçš„æ‰©æ•£åŸºçº¿ç›¸å½“ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿè¿›è¡Œäº†å…³äºè®­ç»ƒä¸­éšæœºæ€§ä½œç”¨çš„å®è¯åˆ†æï¼Œç¼©å°äº†é«˜è´¨é‡ç”Ÿæˆå¼SEå’Œä½å»¶è¿Ÿçº¦æŸä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion-based generative modelsåœ¨è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿­ä»£æ€§è´¨éœ€è¦å¤§é‡Neural Function Evaluations (NFEs)ï¼Œä¸åˆ©äºå®æ—¶åº”ç”¨ã€‚</li>
<li>Flow matchingä½œä¸ºä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å­¦ä¹ çŸ¢é‡åœºå®ç°é«˜è´¨é‡åˆæˆï¼Œä½¿ç”¨ç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨ï¼Œä»…éœ€è¦å°‘é‡æ­¥éª¤ã€‚</li>
<li>Shortcut Flow Matching for Speech Enhancement (SFMSE)æ˜¯æ–°å‹æ–¹æ³•ï¼Œè®­ç»ƒå•ä¸€ã€æ­¥éª¤ä¸å˜çš„æ¨¡å‹ï¼Œå¯åœ¨ä¸åŒå»å™ªæ­¥éª¤ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€‚</li>
<li>å•æ­¥SFMSEæ¨ç†å®ç°å®æ—¶å› å­ï¼ˆRTFï¼‰ä¸º0.013ï¼Œæ„ŸçŸ¥è´¨é‡ä¸éœ€è¦å¤šæ¬¡è¿­ä»£çš„æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>SFMSEæ¡¥æ¥äº†é«˜è´¨é‡ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºå’Œä½å»¶è¿Ÿçº¦æŸä¹‹é—´çš„å·®è·ã€‚</li>
<li>å®è¯åˆ†æè¡¨æ˜è®­ç»ƒä¸­éšæœºæ€§ä½œç”¨å¯¹æ¨¡å‹æ€§èƒ½æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1ec7cc3053c1a5dd9727838f7854c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c6faacd7906f85334d68b85234503e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58013a9173b3800d47c128581daff0e0" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition"><a href="#MMedFD-A-Real-world-Healthcare-Benchmark-for-Multi-turn-Full-Duplex-Automatic-Speech-Recognition" class="headerlink" title="MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition"></a>MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex   Automatic Speech Recognition</h2><p><strong>Authors:Hongzhao Chen, XiaoYang Wang, Jing Lan, Hexiao Ding, Yufeng Jiang, MingHui Yang, DanHui Xu, Jun Luo, Nga-Chun Ng, Gerald W. Y. Cheng, Yunlin Mao, Jung Sun Yoo</strong></p>
<p>Automatic speech recognition (ASR) in clinical dialogue demands robustness to full-duplex interaction, speaker overlap, and low-latency constraints, yet open benchmarks remain scarce. We present MMedFD, the first real-world Chinese healthcare ASR corpus designed for multi-turn, full-duplex settings. Captured from a deployed AI assistant, the dataset comprises 5,805 annotated sessions with synchronized user and mixed-channel views, RTTM&#x2F;CTM timing, and role labels. We introduce a model-agnostic pipeline for streaming segmentation, speaker attribution, and dialogue memory, and fine-tune Whisper-small on role-concatenated audio for long-context recognition. ASR evaluation includes WER, CER, and HC-WER, which measures concept-level accuracy across healthcare settings. LLM-generated responses are assessed using rubric-based and pairwise protocols. MMedFD establishes a reproducible framework for benchmarking streaming ASR and end-to-end duplex agents in healthcare deployment. The dataset and related resources are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD">https://github.com/Kinetics-JOJO/MMedFD</a> </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ä¸´åºŠå¯¹è¯ä¸­éœ€è¦åº”å¯¹å…¨åŒå·¥äº¤äº’ã€è¯´è¯äººé‡å å’Œä½å»¶è¿Ÿçº¦æŸçš„ç¨³å¥æ€§æŒ‘æˆ˜ï¼Œç„¶è€Œå¯ç”¨çš„å¼€æ”¾åŸºå‡†æµ‹è¯•ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æ¨å‡ºäº†MMedFDï¼Œè¿™æ˜¯é¦–ä¸ªä¸ºå¤šè½®ã€å…¨åŒå·¥ç¯å¢ƒè®¾è®¡çš„ç°å®ä¸–ç•Œä¸­æ–‡åŒ»ç–—ASRè¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†æ¥è‡ªéƒ¨ç½²çš„AIåŠ©æ‰‹ï¼ŒåŒ…å«5805ä¸ªå·²æ ‡æ³¨ä¼šè¯ï¼Œå…·å¤‡åŒæ­¥çš„ç”¨æˆ·å’Œæ··åˆé€šé“è§†å›¾ã€RTTM&#x2F;CTMæ—¶é—´ä»¥åŠè§’è‰²æ ‡ç­¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†ï¼Œå¹¶å¯¹ç”¨äºé•¿ä¸Šä¸‹æ–‡è¯†åˆ«çš„è§’è‰²ä¸²è”éŸ³é¢‘è¿›è¡Œå¾®è°ƒWhisper-smallã€‚ASRè¯„ä¼°åŒ…æ‹¬è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’ŒåŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„æ¦‚å¿µçº§å‡†ç¡®ç‡çš„HC-WERã€‚é€šè¿‡åŸºäºè§„åˆ™å’Œæˆå¯¹åè®®è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å“åº”ã€‚MMedFDä¸ºåŒ»ç–—ä¿å¥éƒ¨ç½²ä¸­çš„æµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†çš„åŸºå‡†æµ‹è¯•å»ºç«‹äº†å¯é‡å¤ä½¿ç”¨çš„æ¡†æ¶ã€‚æ•°æ®é›†å’Œç›¸å…³èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/MMedFD%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Kinetics-JOJO/MMedFDå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19817v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸´åºŠå¯¹è¯ä¸­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å…¨åŒå·¥äº¤äº’ã€è¯´è¯äººé‡å å’Œä½å»¶è¿Ÿçº¦æŸã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MMedFDï¼Œé¦–ä¸ªä¸ºå¤šè½®ã€å…¨åŒå·¥è®¾ç½®è®¾è®¡çš„ç°å®ä¸–ç•Œä¸­æ±‰è¯­åŒ»ç–—ASRè¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»éƒ¨ç½²çš„AIåŠ©ç†æ•è·çš„5805ä¸ªæ³¨é‡Šä¼šè¯ï¼Œå…·æœ‰åŒæ­¥çš„ç”¨æˆ·å’Œæ··åˆé€šé“è§†å›¾ã€RTTM&#x2F;CTMå®šæ—¶å’Œè§’è‰²æ ‡ç­¾ã€‚ä½œè€…è¿˜ä»‹ç»äº†ä¸€ç§æ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†ï¼Œå¹¶ä½¿ç”¨è§’è‰²è¿æ¥çš„éŸ³é¢‘å¯¹Whisperå°å‹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°é•¿ä¸Šä¸‹æ–‡è¯†åˆ«ã€‚ASRè¯„ä¼°åŒ…æ‹¬å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ã€å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’Œæ¦‚å¿µçº§åˆ«é”™è¯¯ç‡ï¼ˆHC-WERï¼‰ï¼Œåè€…ç”¨äºè¡¡é‡åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„å‡†ç¡®æ€§ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™å’Œæˆå¯¹åè®®è¿›è¡Œè¯„ä¼°ã€‚MMedFDä¸ºåŒ»ç–—ä¿å¥éƒ¨ç½²ä¸­çš„æµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†æä¾›äº†å¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚æ•°æ®é›†å’Œç›¸å…³èµ„æºå¯åœ¨å…¬å¼€è®¿é—®çš„é“¾æ¥ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMedFDæ˜¯é¦–ä¸ªä¸ºä¸´åºŠå¯¹è¯è®¾è®¡çš„æ±‰è¯­åŒ»ç–—ASRè¯­æ–™åº“ï¼Œé€‚ç”¨äºå¤šè½®ã€å…¨åŒå·¥è®¾ç½®ã€‚</li>
<li>æ•°æ®é›†åŒ…å«5805ä¸ªæ³¨é‡Šä¼šè¯ï¼Œå…·å¤‡å¤šç§é‡è¦ç‰¹æ€§ï¼Œå¦‚åŒæ­¥çš„ç”¨æˆ·å’Œæ··åˆé€šé“è§†å›¾ã€RTTM&#x2F;CTMå®šæ—¶ä»¥åŠè§’è‰²æ ‡ç­¾ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œç”¨äºæµå¼åˆ†å‰²ã€è¯´è¯äººå½’å±å’Œå¯¹è¯è®°å¿†å¤„ç†ã€‚</li>
<li>é‡‡ç”¨æ¦‚å¿µçº§åˆ«é”™è¯¯ç‡ï¼ˆHC-WERï¼‰æ¥è¡¡é‡åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„ASRå‡†ç¡®æ€§ã€‚</li>
<li>MMedFDä¸ºæµå¼ASRå’Œç«¯åˆ°ç«¯åŒå·¥ä»£ç†æä¾›äº†å¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚</li>
<li>æ•°æ®é›†å’Œç›¸å…³èµ„æºå¯å…¬å¼€è®¿é—®ï¼Œé“¾æ¥å·²æä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9349c34f1ffa67c14e77340d51b233cb" align="middle">
<img src="https://picx.zhimg.com/v2-b983579fa26d909bdc522d9682fa28c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2baccfe49782b965e476dc3169fdb0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6beef4378d14dd1f730282eb41d1423" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Audio-Super-Resolution-with-Latent-Bridge-Models"><a href="#Audio-Super-Resolution-with-Latent-Bridge-Models" class="headerlink" title="Audio Super-Resolution with Latent Bridge Models"></a>Audio Super-Resolution with Latent Bridge Models</h2><p><strong>Authors:Chang Li, Zehua Chen, Liyuan Wang, Jun Zhu</strong></p>
<p>Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at <a target="_blank" rel="noopener" href="https://audiolbm.github.io/">https://AudioLBM.github.io/</a>. </p>
<blockquote>
<p>éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å³å°†ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰æ³¢å½¢ä¸Šé‡‡æ ·åˆ°é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ç‰ˆæœ¬ï¼Œæœ€è¿‘å·²ç»é€šè¿‡æ‰©æ•£å’Œæ¡¥æ¢æ¨¡å‹è¿›è¡Œäº†æ¢ç´¢ã€‚è€Œä¹‹å‰çš„æ–¹æ³•ç”±äºå…¶ç¼ºä¹ä¿¡æ¯ç”Ÿæˆå…ˆéªŒï¼Œå¾€å¾€é­å—æ¬¡ä¼˜ä¸Šé‡‡æ ·è´¨é‡çš„å›°æ‰°ã€‚ä¸ºäº†è·å¾—é«˜è´¨é‡éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ½œè—æ¡¥æ¢æ¨¡å‹ï¼ˆLBMsï¼‰ç³»ç»Ÿï¼Œæˆ‘ä»¬å°†éŸ³é¢‘æ³¢å½¢å‹ç¼©åˆ°è¿ç»­æ½œè—ç©ºé—´ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªLBMï¼Œä»¥å¯ç”¨è‡ªç„¶åŒ¹é…LRåˆ°HRä¸Šé‡‡æ ·è¿‡ç¨‹çš„æ½œè—åˆ°æ½œè—ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œå……åˆ†åˆ©ç”¨LRæ³¢å½¢ä¸­åŒ…å«çš„æŒ‡ç¤ºæ€§å…ˆéªŒä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è®­ç»ƒç»“æœï¼Œå³ä½¿é«˜è´¨é‡æ ·æœ¬æœ‰é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥LBMsï¼Œå°†å…ˆéªŒå’Œç›®æ ‡é¢‘ç‡ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œä½¿LBMsèƒ½å¤Ÿåœ¨è®­ç»ƒé˜¶æ®µæ˜¾å¼åœ°å­¦ä¹ ä»»ä½•åˆ°ä»»ä½•çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†çº§è”LBMsï¼Œå¹¶æå‡ºäº†ä¸¤ç§å…ˆéªŒå¢å¼ºç­–ç•¥ã€‚æˆ‘ä»¬é¦–æ¬¡å°è¯•è§£é”è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·ï¼Œèµ‹èƒ½æ— ç¼çº§è”SRè¿‡ç¨‹ï¼Œä¸ºéŸ³é¢‘åæœŸåˆ¶ä½œæä¾›æ›´é«˜çš„çµæ´»æ€§ã€‚åœ¨VCTKã€ESC-50ã€Song-DescriberåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªå†…éƒ¨æµ‹è¯•é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„ä»»ä½•åˆ°48kHz SRæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„å®¢è§‚å’Œæ„ŸçŸ¥è´¨é‡ï¼Œå¹¶é¦–æ¬¡åˆ›é€ äº†ä»»ä½•åˆ°1:åˆ°é«˜éŸ³è´¨çš„è®°å½•SRä¸ºæ¯ç§’é‡‡æ ·ç‡ä¸º192kHzçš„è®°å½•ã€‚æ¼”ç¤ºåœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://audiolbm.github.io/%E3%80%82">https://AudioLBM.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17609v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºæ½œåœ¨æ¡¥æ¨¡å‹ï¼ˆLBMsï¼‰çš„æ–°ç³»ç»Ÿå®ç°éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼Œé€šè¿‡å‹ç¼©éŸ³é¢‘æ³¢å½¢åˆ°è¿ç»­æ½œåœ¨ç©ºé—´ï¼Œè®¾è®¡LBMè¿›è¡Œæ½œåœ¨åˆ°æ½œåœ¨ç”Ÿæˆè¿‡ç¨‹ï¼Œè‡ªç„¶åŒ¹é…LRåˆ°HRçš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚å¼•å…¥é¢‘ç‡æ„ŸçŸ¥LBMsï¼Œåˆ©ç”¨é¢‘ç‡ä¿¡æ¯å¢å¼ºè®­ç»ƒç»“æœã€‚è®¾è®¡çº§è”LBMså’Œä¸¤ç§å…ˆéªŒå¢å¼ºç­–ç•¥ï¼Œå®ç°è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·ï¼Œæä¾›éŸ³é¢‘åæœŸåˆ¶ä½œæ›´é«˜çµæ´»æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„ä»»ä½•åˆ°48kHzçš„SRä¸­å®ç°äº†æœ€å…ˆè¿›çš„å®¢è§‚å’Œæ„ŸçŸ¥è´¨é‡ï¼Œå¹¶åœ¨ä»»ä½•åˆ°192kHzçš„éŸ³é¢‘SRä¸­åˆ›ä¸‹äº†é¦–ä¸ªè®°å½•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ½œåœ¨æ¡¥æ¨¡å‹ï¼ˆLBMsï¼‰çš„éŸ³é¢‘è¶…åˆ†è¾¨ç‡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å‹ç¼©éŸ³é¢‘æ³¢å½¢åˆ°è¿ç»­æ½œåœ¨ç©ºé—´è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LBMsçš„è®¾è®¡èƒ½å¤Ÿè‡ªç„¶åŒ¹é…ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œå……åˆ†åˆ©ç”¨ä½åˆ†è¾¨ç‡æ³¢å½¢ä¸­çš„å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>å¼•å…¥é¢‘ç‡æ„ŸçŸ¥LBMsï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µèƒ½å¤Ÿæ˜¾å¼å­¦ä¹ ä»»ä½•åˆ°ä»»ä½•çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†çº§è”LBMså’Œä¸¤ç§å…ˆéªŒå¢å¼ºç­–ç•¥ï¼Œå®ç°äº†è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·ï¼Œä¸ºéŸ³é¢‘åæœŸåˆ¶ä½œæä¾›äº†æ›´é«˜çš„çµæ´»æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„ä»»ä½•åˆ°48kHzçš„è¶…åˆ†è¾¨ç‡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨ä»»ä½•åˆ°192kHzçš„éŸ³é¢‘è¶…åˆ†è¾¨ç‡ä¸Šåˆ›ä¸‹äº†é¦–ä¸ªè®°å½•ã€‚</li>
<li>æä¾›äº†æ¼”ç¤ºç½‘ç«™é“¾æ¥ï¼Œå¯ä»¥ç›´è§‚åœ°ä½“éªŒè¯¥ç³»ç»Ÿçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f40bcd01d87eedf6c5cc87b43e7af566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c47a2912bf503b062155d30f8cb4cbf7" align="middle">
<img src="https://picx.zhimg.com/v2-2a9191182761fe66bcb724b28e5e248f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21f088fbd5bb25115c5413bb438cdc48.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>é’ˆå¯¹å‘éŸ³éšœç¢è€…çš„è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ï¼Œç„¶åå°†å…¶åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®éšåç”¨äºå¾®è°ƒå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v5">PDF</a> 5 pages, 1 figure, Proceedings of Interspeech</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ï¼Œå¯¹å‘éŸ³éšœç¢çš„è¯­éŸ³è¯†åˆ«ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰çš„è¯­éŸ³è½¬æ¢æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç¼–ç è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å¤±çœŸã€‚æ¥ç€å°†æ­¤æ¨¡å‹åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢æˆéè‹±è¯­çš„å‘éŸ³éšœç¢ç±»è¯­éŸ³ã€‚ç”Ÿæˆçš„è¯­éŸ³æ•°æ®è¢«ç”¨æ¥å¾®è°ƒå¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŒæ—¶è½¬æ¢è¯´è¯äººå’ŒéŸµå¾‹çš„è¯­éŸ³è½¬æ¢æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨è¯†åˆ«å‘éŸ³éšœç¢çš„è¯­éŸ³æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ã€‚</li>
<li>ç ”ç©¶è€…é€šè¿‡å¾®è°ƒä¸€ä¸ªè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³çš„è¯­éŸ³è½¬æ¢æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ä»…èƒ½ç¼–ç è¯´è¯äººçš„ç‰¹æ€§ï¼Œè¿˜èƒ½ç¼–ç éŸµå¾‹å¤±çœŸã€‚</li>
<li>ç ”ç©¶è€…ä½¿ç”¨è¯¥æ¨¡å‹å°†å¥åº·çš„éè‹±è¯­è¯­éŸ³è½¬æ¢æˆéè‹±è¯­çš„å‘éŸ³éšœç¢ç±»è¯­éŸ³ã€‚</li>
<li>ç”Ÿæˆçš„è¯­éŸ³æ•°æ®ç”¨äºå¾®è°ƒå¤šè¯­ç§ASRæ¨¡å‹ï¼Œæé«˜è¯†åˆ«å‘éŸ³éšœç¢è¯­éŸ³çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ASRæ¨¡å‹å’Œå¢å¼ºæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac0fc4c404365339034014bc6ad46c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7e5d7f9d542140dc457cb446cd1bdce" align="middle">
<img src="https://picx.zhimg.com/v2-d2a0a5b07ebf2a1d63cd8c8f6b829ad5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e36eca1ec149499f2a5ad2686326d8" align="middle">
<img src="https://pic1.zhimg.com/v2-205cb5cfc3fefc775dfc640c695d5d30.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84c93a7667de52e2b24a69986c892b9b" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  SpecXNet A Dual-Domain Convolutional Network for Robust Deepfake   Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c1d26ee3f203d013f2b81400c472edda.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  No-Reference Image Contrast Assessment with Customized EfficientNet-B0
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
