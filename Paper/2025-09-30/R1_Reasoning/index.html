<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-30  WebGen-Agent Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-61d527f5dfa72b4247df0be816b64da4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059303&auth_key=1760059303-0-0-473018ebe01c916383ddd07b52c082d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-30-更新"><a href="#2025-09-30-更新" class="headerlink" title="2025-09-30 更新"></a>2025-09-30 更新</h1><h2 id="WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning"><a href="#WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning" class="headerlink" title="WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning"></a>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning</h2><p><strong>Authors:Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model’s website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7. </p>
<blockquote>
<p>由大型语言模型（LLM）驱动的智能系统已在仓库级别的代码生成任务中展现出令人印象深刻的性能。然而，对于高度依赖视觉效果和用户交互反馈的网站代码生成任务，当前代码智能体仅依赖于简单的代码执行进行反馈和验证。这种方法无法捕捉生成代码的实际质量。在本文中，我们提出了WebGen-Agent，这是一种新型网站生成智能体，它利用全面、多层次的视觉反馈来迭代生成和细化网站代码库。通过视觉语言模型（VLM）生成关于网站截图和GUI智能体测试的详细而富有表现力的文本描述和建议，以及量化其质量的分数。截图和GUI智能体的分数进一步与回溯和选择最佳机制相结合，增强了智能体的性能。利用WebGen-Agent工作流程中固有的准确视觉分数，我们还引入了带有截图和GUI智能体反馈的Step-GRPO，以提高LLM作为WebGen-Agent推理引擎的能力。通过将每一步的截图和GUI智能体分数作为Step-GRPO中的奖励，我们提供了密集可靠的流程监督信号，有效提高模型的网站生成能力。在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确性从26.4%提高到51.9%，外观得分从3.0提高到3.9，超越了之前的先进智能体系。此外，我们的Step-GRPO训练方法将Qwen2.5-Coder-7B-Instruct的准确性从38.9%提高到45.4%，外观得分从3.4提高到3.7。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22644v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的代理系统在仓库级别的代码生成任务上表现出强大的性能。然而，对于依赖视觉效果和用户交互反馈的网站代码生成任务，当前代码代理仅依赖于简单的代码执行进行反馈和验证，无法捕捉生成代码的实际质量。为此，本文提出了WebGen-Agent，一个利用全面、多层次的视觉反馈来迭代生成和细化网站代码库的新型网站生成代理。WebGen-Agent通过视觉语言模型（VLM）生成关于网站截图和GUI代理测试的详细文本描述和建议，同时量化其质量。通过集成截图和GUI代理分数以及回溯和选择最佳机制，增强了代理的性能。此外，还介绍了利用WebGen-Agent工作流程中的精确视觉分数改进的Step-GRPO方法，通过每一步的截图和GUI代理分数作为奖励，为Step-GRPO提供了密集可靠的过程监督信号，有效提高模型在网站生成方面的能力。在WebGen-Bench数据集上，WebGen-Agent提高了Claude-3.5-Sonnet的准确性和外观评分，超越了现有最先进的代理系统。同时，我们的Step-GRPO训练方法也提高了Qwen2.5-Coder-7B-Instruct的准确性和外观评分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代理系统在仓库级别的代码生成任务上表现优异，但在依赖视觉效果和用户交互反馈的网站代码生成任务上存在局限性。</li>
<li>WebGen-Agent利用全面、多层次的视觉反馈迭代生成和细化网站代码库，包括通过视觉语言模型生成的详细文本描述和建议。</li>
<li>WebGen-Agent集成了截图和GUI代理分数，增强了性能，并引入了回溯和选择最佳机制。</li>
<li>Step-GRPO方法利用WebGen-Agent工作流程中的精确视觉分数，通过每一步的截图和GUI代理分数作为奖励，提供密集可靠的过程监督信号。</li>
<li>WebGen-Agent在WebGen-Bench数据集上显著提高了代理系统的准确性，特别是针对Claude-3.5-Sonnet的性能提升显著。</li>
<li>Step-GRPO训练方法也提高了模型的准确性和外观评分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d8fa6812c28048cedaf18a377f7020ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059311&auth_key=1760059311-0-0-d407fee8975da04ab00a0aa9a891f19a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8e7ee6f5aeec558da0188a66affc6f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059318&auth_key=1760059318-0-0-edeb709305372e1f10d9cf5d74fd92f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ec6f005265afd95f4fdf4fbb8f5bf56c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059325&auth_key=1760059325-0-0-009f2f92a695f45907f31aa94cec8855&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95a5a9d99dcbca69f8c2278e48bce3a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059332&auth_key=1760059332-0-0-5aad4f32cb2b608c891077f029a47b8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VLA-Reasoner-Empowering-Vision-Language-Action-Models-with-Reasoning-via-Online-Monte-Carlo-Tree-Search"><a href="#VLA-Reasoner-Empowering-Vision-Language-Action-Models-with-Reasoning-via-Online-Monte-Carlo-Tree-Search" class="headerlink" title="VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning   via Online Monte Carlo Tree Search"></a>VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning   via Online Monte Carlo Tree Search</h2><p><strong>Authors:Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang</strong></p>
<p>Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation. </p>
<blockquote>
<p>视觉-语言-动作模型（VLAs）通过扩大模仿学习，在一般的机器人操作任务中取得了强大的性能。然而，现有的VLAs仅限于预测短视的下一个动作，对于长周期轨迹任务则由于增量偏差而面临困难。为了解决这个问题，我们提出了一种名为VLA-Reasoner的插件框架，该框架可以有效地赋予现成的VLAs通过测试时缩放预测未来状态的能力。具体来说，VLA-Reasoner会抽样并展开可能的行动轨迹，其中的行动是作为理由来生成未来状态的世界模型，这使得VLA-Reasoner能够预测并推理潜在的结果并寻找最佳行动。我们进一步利用蒙特卡洛树搜索（MCTS）来提高在大动作空间中的搜索效率，其中逐步的VLA预测作为根种子。同时，我们引入了一种基于核密度估计（KDE）的信心抽样机制，能够在MCTS中进行有效的探索，无需冗余的VLA查询。我们通过离线奖励塑形策略来评估MCTS中的中间状态，为预测的未来打分并纠正长期反馈中的偏差。我们在模拟器和真实世界中都进行了大量实验，证明了我们提出的VLA-Reasoner相较于最先进的VLAs取得了显著的改进。我们的方法揭示了实现可扩展的测试时间计算的机器人操作的潜在途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22643v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>在视觉-语言-动作模型（VLAs）中，通过扩展模仿学习，可以在一般的机器人操作任务中取得强大的性能。但现有的VLAs仅局限于预测短视的下一个动作，难以应对长视规划轨迹的任务。为此，我们提出了一个名为VLA-Reasoner的插件框架，它通过测试时扩展的能力赋予现成的VLAs预见未来状态的能力。该框架通过世界模型生成未来状态，涉及的动作作为解释进行采样和展开可能的行动轨迹。此外，我们利用蒙特卡洛树搜索（MCTS）提高在大动作空间中的搜索效率，以逐步的VLA预测作为根节点。同时，我们引入基于核密度估计（KDE）的信心采样机制，在MCTS中无需冗余的VLA查询即可实现高效探索。通过离线奖励塑形策略评估MCTS的中间状态，以评分预测的未来并纠正长期反馈中的偏差。在模拟器和真实世界的广泛实验表明，我们提出的VLA-Reasoner相较于最先进的VLAs取得了显著改进。我们的方法为机器人操作的测试时计算提供了一个潜在的可行路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA模型在机器人操作任务中表现出强大的性能，但仅限于短视预测。</li>
<li>VLA-Reasoner插件框架赋能VLAs以预见未来状态的能力，通过生成未来状态并展开可能的行动轨迹来解决短视问题。</li>
<li>VLA-Reasoner利用蒙特卡洛树搜索（MCTS）提高在大动作空间中的搜索效率。</li>
<li>基于核密度估计（KDE）的信心采样机制提高了探索效率，无需冗余的VLA查询。</li>
<li>通过离线奖励塑形策略评估中间状态，实现对预测未来的评分和对长期反馈中的偏差的纠正。</li>
<li>在模拟器和真实世界的实验中，VLA-Reasoner相较于最先进的VLAs取得了显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-15219d83569de8e1e90c04fd8d2b0e49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059340&auth_key=1760059340-0-0-6771bc49ab326a2f13aa10a6e0e8991e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05516f5f20537e2925fe470cceb2a40f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059347&auth_key=1760059347-0-0-edc9237b35954a1ee189fe381172a523&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3334f6a310ee26a4cff274b177686255~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059354&auth_key=1760059354-0-0-3d4a58319664f06b97de5a094e7db43f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2670332c0dbc66d94fc0de26045e3035~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059360&auth_key=1760059360-0-0-1700c4d083560cc0bacba2420659565e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c0aea994c4e8cea57bf7ac1a43aa561~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059367&auth_key=1760059367-0-0-18122aba4ca7f3afe15fceb202921a5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Variational-Reasoning-for-Language-Models"><a href="#Variational-Reasoning-for-Language-Models" class="headerlink" title="Variational Reasoning for Language Models"></a>Variational Reasoning for Language Models</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang</strong></p>
<p>We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/variational-reasoning">https://github.com/sail-sg/variational-reasoning</a>. </p>
<blockquote>
<p>我们为语言模型引入了一种变分推理框架，该框架将思考痕迹视为潜在变量，并通过变分推理对其进行优化。我们从证据下界（ELBO）出发，将其扩展为多轨迹目标以获得更紧密界限，并提出了一种正向KL公式，该公式可稳定变分后验的训练。我们进一步表明，拒绝采样微调和二值奖励强化学习，包括GRPO，可以解释为局部正向KL目标，其中模型准确性的隐式加权自然产生于推导中，并揭示了之前未注意到的对简单问题的偏向。我们在广泛的推理任务上对Qwen 2.5和Qwen 3模型家族上进行了实证验证。总的来说，我们的工作提供了一个有原则的概率视角，将变分推理与RL风格的方法统一起来，并为提高语言模型的推理能力提供了稳定的目标。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/sail-sg/variational-reasoning%E3%80%82">https://github.com/sail-sg/variational-reasoning。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22637v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了一种用于语言模型的变分推理框架，该框架将思考痕迹视为潜在变量，并通过变分推理进行优化。从证据下界（ELBO）出发，扩展到多痕迹目标以获取更紧密界限，并提出前向KL公式以稳定变分后验的训练。文本还展示了拒绝采样微调、二元奖励强化学习等方法可以解释为局部前向KL目标，其中模型精度隐式加权自然产生于推导中，揭示了对更简单问题的偏向。文本对Qwen 2.5和Qwen 3模型家族进行了实证验证，涉及广泛的推理任务。总体而言，这项工作提供了一个有原则的概率视角，将变分推理与强化学习风格的方法统一起来，并为提高语言模型的推理能力提供了稳定的目标。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种变分推理框架，将思考痕迹视为潜在变量进行优化。</li>
<li>通过扩展证据下界（ELBO）到多痕迹目标，获得更紧密界限。</li>
<li>提出前向KL公式，稳定变分后验的训练。</li>
<li>拒绝采样微调、二元奖励强化学习等方法可解释为局部前向KL目标。</li>
<li>模型精度隐式加权自然产生于推导中，揭示了对更简单问题的偏向。</li>
<li>在Qwen 2.5和Qwen 3模型家族上进行了实证验证，涉及广泛的推理任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22637">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d4456b1d7566cafa65f9d05587df7a78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059374&auth_key=1760059374-0-0-100e3dae8eea4da6bad76f6caeea023d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4eff3b65907d6b3a4f16698c72cdf5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059382&auth_key=1760059382-0-0-411a827869327c612a9552f644735d9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5436c66055f7a90d1ec6f2a64e56ae7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059389&auth_key=1760059389-0-0-9d76a7d698257444462eb67519c00ab6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning"><a href="#UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning" class="headerlink" title="UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning"></a>UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning</h2><p><strong>Authors:Hongyu Chen, Guangrun Wang</strong></p>
<p>Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism. </p>
<blockquote>
<p>链式思维（CoT）提示可以改善大型语言模型（LLM）的推理能力，但其对无结构文本的依赖限制了其在实体任务中的可解释性和可执行性。早期的工作已经探索了使用场景或逻辑图的结构化CoT，但这些方法仍然存在根本性的局限：它们只建模低阶关系，缺乏继承或行为抽象等构建，并且不提供用于顺序或条件规划的标准化语义。我们提出了UML-CoT，这是一个结构化推理和规划框架，它利用统一建模语言（UML）生成符号化的CoT和可执行的行动计划。UML类图捕获组合对象语义，而活动图对程序控制流进行建模。我们的三阶段训练管道结合了监督微调与群组相对策略优化（GRPO），包括仅从答案数据中学习奖励。我们在MRoom-30k新基准测试上评估了UML-CoT，该测试模拟了杂乱房间清洁场景。UML-CoT在无结构化CoT的可解释性、规划连贯性和执行成功方面表现出优势，突显了UML作为一种更具表现力和可操作性的结构化推理形式系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22628v1">PDF</a> </p>
<p><strong>Summary</strong>：利用统一建模语言（UML）建立结构化推理和规划框架UML-CoT，通过生成符号化链式思维（CoT）和可执行行动计划，改进大型语言模型（LLM）的推理能力。该框架通过UML类图捕捉组合对象语义，通过活动图模拟过程控制流。采用包括奖励学习在内的三阶段训练管道，从仅答案数据中优化相对策略。在全新的房间清洁场景MRoom-30k测试中，UML-CoT在可解释性、规划连贯性和执行成功率方面优于非结构化CoT，凸显了UML作为更具表现力和可操作性的结构化推理形式的重要性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>UML-CoT结合了统一建模语言（UML）和链式思维（CoT），旨在改进大型语言模型（LLM）的推理能力。</li>
<li>通过UML类图和活动图，UML-CoT能够捕捉组合对象语义并模拟过程控制流。</li>
<li>该框架提出了一个三阶段训练管道，包括监督微调以及与Group Relative Policy Optimization (GRPO)结合的奖励学习。</li>
<li>在新的房间清洁场景MRoom-30k测试中，UML-CoT表现出更高的可解释性、规划连贯性和执行成功率。</li>
<li>与传统的非结构化CoT相比，UML-CoT提供了更标准化和可执行的语义理解和行动计划。</li>
<li>UML作为结构化推理形式，具有更高的表达力和可操作性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b318bffb25cf6b3afe9ffcd5c57ea57b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059396&auth_key=1760059396-0-0-d71ec943d7e069e874456932393433ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-303c7973c5eef7a1afdbe0c488e4e685~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059404&auth_key=1760059404-0-0-e098e0215a73bede8707855e9f913f2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ead859e7b28e0c1ad096a9bf2b9376f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059411&auth_key=1760059411-0-0-e333d301f7282d2db42338aaaa28bcb4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75247ff8633f85737f3058ff1cfe2998~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059418&auth_key=1760059418-0-0-0ce322d75bfc5c2dd063fe7a02c2e657&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce93981cde70d784a95ba203177b57b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059424&auth_key=1760059424-0-0-c9fd87e2d5c78a2ce6e11dd8b9b96b31&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time"><a href="#Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time" class="headerlink" title="Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time"></a>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time</h2><p><strong>Authors:Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</strong></p>
<p>Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning. </p>
<blockquote>
<p>测试时缩放（TTS）通过推理过程中的额外计算，增强了大型语言模型（LLM）的推理能力。然而，现有的方法主要依赖于输出级别的采样，而忽视了模型架构的作用。在主流混合专家（MoE）LLM中，我们观察到改变激活的专家数量会产生具有稳定准确率的互补解决方案集，揭示了一种新的且尚未被充分探索的多样性来源。受这一观察结果的启发，我们提出了动态专家搜索（DES），这是一种将专家激活提升为搜索空间的可控维度的TTS策略。DES集成了两个关键组件：（1）动态MoE，它能够在推理过程中直接控制专家数量，生成多样化的推理轨迹，无需额外成本；（2）专家配置继承，它能够在推理路径中保持一致的专家数量，同时在各次运行中变化专家数量，从而在搜索过程中平衡稳定性和多样性。在MoE架构、验证器和推理基准测试（例如数学、代码和知识）上的大量实验表明，DES可靠地优于TTS基线，提高了准确性和稳定性，且无需额外成本。这些结果突出了DES作为一种实用且可扩展的架构感知TTS形式，说明了现代LLM中的结构灵活性如何推动推理的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22572v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>测试时缩放（TTS）通过推理时分配额外的计算资源来提高大型语言模型（LLM）的推理能力。然而，现有方法主要依赖于输出级别的采样，而忽略了模型架构的作用。本文提出动态专家搜索（DES）策略，这是一种将专家激活提升为搜索空间可控维度的TTS策略。DES包括两个关键组件：动态MoE和专家配置继承，能够在推理过程中直接控制专家数量，生成多样化的推理轨迹，同时保持专家数量的一致性，从而平衡稳定性和多样性。实验表明，DES在不需要额外成本的情况下可靠地优于TTS基线，提高了准确性和稳定性。这些结果突显了DES作为实用且可扩展的架构感知TTS形式的优势，展示了现代LLM中结构灵活性对推理的推动作用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>测试时缩放（TTS）增强了大型语言模型（LLM）的推理能力，通过分配额外的计算资源进行优化。</li>
<li>现有方法主要关注输出级别的采样，而忽视模型架构的影响。</li>
<li>本文引入动态专家搜索（DES）策略，将专家激活作为搜索空间的可控维度。</li>
<li>DES包含动态MoE和专家配置继承两个关键组件。</li>
<li>动态MoE允许在推理过程中直接控制专家数量，生成多样化推理轨迹，无需额外成本。</li>
<li>专家配置继承在推理路径中保持专家数量的一致性，平衡稳定性和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22572">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c6eb411d6309d9fa6012c40fb742c8f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059432&auth_key=1760059432-0-0-b8836a434ff9842601a7c5030da8f665&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d84d1070476f742f5eca80724d899519~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059439&auth_key=1760059439-0-0-e0935bd15f23ee57527771f64071c407&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df408d6615225b002a326d82d15e2cf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059446&auth_key=1760059446-0-0-1780d11bfffb9dce90bb05324f3667cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2c75eb1fd550f071622b564bd8bc2a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059453&auth_key=1760059453-0-0-33322557fa2135e8f215dbcb79ba7917&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-61d527f5dfa72b4247df0be816b64da4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059461&auth_key=1760059461-0-0-e880524eae89895c009eebd948b20a92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models"><a href="#StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models" class="headerlink" title="StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models"></a>StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models</h2><p><strong>Authors:Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge</strong></p>
<p>Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs. </p>
<blockquote>
<p>大规模语言模型（LLM）在解决运筹学（OR）问题方面显示出巨大的潜力。虽然强化学习是训练LLM解决OR问题的强大范式，但现有研究通常面临两个主要局限。首先，结果奖励存在信用分配问题，正确的最终答案可能会强化错误的推理。其次，传统的判别过程监督是短视的，无法全面评估OR建模的相互依赖步骤。为此，我们引入了StepORLM，这是一种新型的自进化框架，具有生成过程监督功能。StepORLM的核心是一个协同进化循环，其中策略模型和生成过程奖励模型（GenPRM）相互迭代改进。这个循环由一个双重反馈机制驱动：来自外部求解器的基于结果的确定性验证，以及来自GenPRM的细致全面过程评估。结合这两种信号，通过加权直接偏好优化（W-DPO）来调整策略，同时改进GenPRM。我们得到的8B参数StepORLM在六个基准测试上达到了最新水平，显著优于更大的通用模型、主动方法和专用基准测试。此外，协同进化的GenPRM能够充当强大而通用的过程验证器，大大提高我们自己的模型和其他现有LLM的推理扩展性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在解决运筹学（OR）问题方面展现出巨大潜力。强化学习作为LLM训练的一种强大范式在OR问题上存在两个主要局限。为解决这些问题，本文提出StepORLM，一种新型自我进化的框架，具有生成过程监督功能。StepORLM核心是一个协同进化循环，策略模型和生成过程奖励模型相互迭代改进。该循环由双重反馈机制驱动：来自外部求解器的确定性、结果为基础的验证，以及来自GenPRM的微妙、整体过程评估。结合信号用于通过加权直接偏好优化（W-DPO）对齐策略，同时改进GenPRM。我们的8B参数StepORLM在六个基准测试上达到最新水平，显著优于更大的通用模型、自主方法和专用基准。此外，协同进化的GenPRM能够充当强大而通用的过程验证器，显著提高了我们模型和其他现有LLM的推理扩展性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在解决运筹学（OR）问题方面具有巨大潜力。</li>
<li>强化学习是训练LLM解决OR问题的一种有效方法，但存在两个主要局限：结果奖励中的信用分配问题和传统判别过程监督的近视性。</li>
<li>StepORLM框架通过引入生成过程监督来解决这些问题，具有自我进化的能力。</li>
<li>StepORLM的核心是一个协同进化循环，包括策略模型和生成过程奖励模型的迭代改进。</li>
<li>双重反馈机制包括外部求解器的结果验证和GenPRM的过程评估。</li>
<li>提出的8B参数StepORLM在多个基准测试上表现出最佳性能，优于大型通用模型、自主方法和专用基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22558">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7ff1cf82a7b19fb113fa0d984420e2f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059468&auth_key=1760059468-0-0-92d539ebc5bb1662fa36adb5024a88d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8add0cc7742216d8afd873e354d9b2e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059475&auth_key=1760059475-0-0-ef33cd7b2a1485d684ceb5eefa239b52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7cf587ac5352e33f1b09fe83a5ac0ca4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059482&auth_key=1760059482-0-0-f325392b12bd95f6174f0a9800bb5e4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-657b4744fa6f9fa202a866dd3961ac81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059489&auth_key=1760059489-0-0-3117fa9a2b3282ee0a9c979c820d6a0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc56ec5d52c475253e45cae32ac32a53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059495&auth_key=1760059495-0-0-1683edf29d27b5d1195dd87b4bb2744e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="REMA-A-Unified-Reasoning-Manifold-Framework-for-Interpreting-Large-Language-Model"><a href="#REMA-A-Unified-Reasoning-Manifold-Framework-for-Interpreting-Large-Language-Model" class="headerlink" title="REMA: A Unified Reasoning Manifold Framework for Interpreting Large   Language Model"></a>REMA: A Unified Reasoning Manifold Framework for Interpreting Large   Language Model</h2><p><strong>Authors:Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen</strong></p>
<p>Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model’s layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models. </p>
<blockquote>
<p>理解大型语言模型（LLM）如何进行复杂推理及其失败机制是解释性研究中的一项挑战。为了提供一个可衡量的几何分析视角，我们定义了“推理流形”这一概念，这是一个由与所有正确推理生成相对应的潜在低维几何结构形成的内部表示。这一结构可以概念化为模型学习成功解决给定任务的有效思考路径的体现。基于此概念，我们构建了REMA框架，通过定量比较错误和正确推理样本对应的内部模型表示的空间关系来解释失败的原因。具体来说，REMA首先通过计算每个错误表示与其邻近的正确表示形成的近似流形的k最近邻距离来量化其几何偏差，从而提供一个统一的失败信号。然后，它通过跟踪模型各层中的偏差度量并与来自正确表示的内部波动的基线进行比较，定位这些偏差首次变得显著的发散点，从而确定推理链开始出错的地方。我们在多种语言和跨模态模型和任务上的大量实验证明了推理流形的低维性质以及错误和正确推理表示之间的高可分性。结果也验证了REMA框架在分析推理失败原因方面的有效性。这项研究将抽象的推理失败与可衡量的表示几何偏差联系起来，为深入理解和诊断黑箱模型的内部计算过程提供了新的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22518v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了理解大型语言模型（LLM）进行复杂推理及其失败机制的可衡量几何分析视角。通过引入“推理流形”的概念，即所有正确推理生成对应的潜在低维几何结构，为有效思考路径的体现，进而构建REMA框架。该框架通过定量比较错误和正确推理样本的模型内部表示的空间关系，揭示失败的起源。通过计算每个错误表示与由正确表示形成的近似流形的k近邻距离，REMA提供了一个统一的失败信号。然后，通过跟踪这种偏差度量跨越模型层并与正确表示的基线内部波动进行比较，定位偏差首次变得显著的发散点。在多种语言和跨模态模型及任务上的实验证明了推理流形的低维性质以及错误和正确推理表示之间的高可分离性。结果也验证了REMA框架在分析推理失败原因方面的有效性。该研究将抽象的推理失败与可衡量的几何偏差联系起来，为深入了解和理解黑箱模型的内部计算过程提供了新的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入“推理流形”概念，定义为LLM内部表示形成的低维几何结构，反映模型成功解决给定任务的有效思考路径。</li>
<li>构建REMA框架，通过比较错误和正确推理样本的模型内部表示的空间关系，定量解析失败的起源。</li>
<li>通过计算错误表示与正确表示流形之间的距离，提供统一的失败信号。</li>
<li>定位偏差首次变得显著的发散点，通过跟踪偏差度量在模型各层的变动并与基线进行比较。</li>
<li>实验证明推理流形的低维性质以及错误和正确推理表示之间的高可分离性。</li>
<li>REMA框架在分析推理失败原因方面有效性得到验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-49a82ff01ea94ff7f6a9719c174538ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059503&auth_key=1760059503-0-0-52754fcca4d152e51e40bbb7a36960af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6902b01e6cb7b68efc8678a56d6a448~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059511&auth_key=1760059511-0-0-4f1a862d2f7b86fff2ffc84d872424b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7fb21ae2d2c18c31456948da8cb7ded~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059539&auth_key=1760059539-0-0-585c0a1c27539ff3085067b2de9656e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Estimating-the-Empowerment-of-Language-Model-Agents"><a href="#Estimating-the-Empowerment-of-Language-Model-Agents" class="headerlink" title="Estimating the Empowerment of Language Model Agents"></a>Estimating the Empowerment of Language Model Agents</h2><p><strong>Authors:Jinyeop Song, Jeff Gore, Max Kleiman-Weiner</strong></p>
<p>As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent’s actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings. </p>
<blockquote>
<p>随着语言模型（LM）代理变得更加强大，并获得了更广泛访问真实世界工具的能力，对代理能力的可评估评价框架的需求不断增长。然而，以基准测试为中心的传统评估在设计上成本高昂，需要人类设计师提出有效的任务，以洞察通用模型的能力。在这项工作中，我们提出基于赋能的信息理论评估，即代理行动与未来状态之间的互信息，作为一种评估语言模型代理的开放方法。我们介绍了EELMA（语言模型代理的赋能估计），这是一种通过多轮文本交互来近似有效赋能的算法。我们在语言游戏和规模更大的真实网页浏览场景中验证了EELMA。我们发现赋能与平均任务性能有很强的相关性，并描述了环境复杂性和代理因素（如思维链、模型规模和记忆长度）对估计赋能的影响，以及高赋能状态和行动通常是通用能力的重要时刻。总的来说，这些结果证明了赋能作为评估和监控复杂开放式设置中语言模型代理的通用指标的吸引力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22504v1">PDF</a> 10 pages, 8 figures. Submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于赋能的信息理论评估方法，用于评估语言模型（LM）代理的能力。文章介绍了EELMA算法，可通过多轮文本交互来估算代理的有效赋能。通过语言游戏和规模化的真实网页浏览场景的验证，发现赋能与平均任务性能密切相关。文章还探讨了环境复杂性和代理因素（如思维链、模型规模和记忆长度）对估算赋能的影响，并指出高赋能状态和行动通常是代理通用能力的重要时刻。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种基于赋能的信息理论评估框架，用于评估语言模型代理的能力。</li>
<li>介绍了EELMA算法，通过多轮文本交互估算代理的有效赋能。</li>
<li>验证EELMA在语言游戏和真实网页浏览场景下的有效性。</li>
<li>发现赋能与平均任务性能之间的强相关性。</li>
<li>探讨了环境复杂性和代理因素对估算赋能的影响。</li>
<li>高赋能状态和行动对代理的通用能力至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e8cab989de96a2b6854c5645db433b03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059547&auth_key=1760059547-0-0-195a5202ff11b0ad7031d7101ac9c149&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65b7ff7882f7cd3b4c73811c85f22eb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059555&auth_key=1760059555-0-0-425bde2df02b733e96223369ff1abe98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ede11379322c974d0efc59134f2181d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059562&auth_key=1760059562-0-0-7eed4657dccb59b503e513fb166269fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e218b710b482e121d9794d18759cb3c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059569&auth_key=1760059569-0-0-b753bb3bd68fcb5764c83185ef43bc1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d44d01cf90354dfa911424ade4fb1bec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059577&auth_key=1760059577-0-0-74eda91c380fbe58db69aa5d93c8a647&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13660dcd361c796be371052ac221bc7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059584&auth_key=1760059584-0-0-d2cf4d82bd734bd12ee2ab4adde43e35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Limits-of-Large-Language-Models-in-Multilingual-Legal-Reasoning"><a href="#Evaluating-the-Limits-of-Large-Language-Models-in-Multilingual-Legal-Reasoning" class="headerlink" title="Evaluating the Limits of Large Language Models in Multilingual Legal   Reasoning"></a>Evaluating the Limits of Large Language Models in Multilingual Legal   Reasoning</h2><p><strong>Authors:Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve Gürel</strong></p>
<p>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta’s LLaMA, OpenAI’s ChatGPT, Google’s Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications. </p>
<blockquote>
<p>在这个以大型语言模型（LLM）为主导的时代，了解其能力和局限性，特别是在法律等高风险领域，至关重要。虽然诸如Meta的LLaMA、OpenAI的ChatGPT、Google的Gemini、DeepSeek等新兴的大型语言模型被越来越多地融入法律工作流程中，但它们在多语种、管辖范围广泛以及对立情境下的表现仍然未得到足够的探索。本工作针对LLaMA和Gemini进行了多语言法律和非法律基准的测试评估，并通过字符和单词级别的扰动评估了它们在法律任务中的对抗稳健性。我们采用LLM作为法官的方法来进行符合人类评估。此外，我们推出一个开源的模块化评估管道，旨在支持任何大型语言模型和数据集的多语言、任务多样化的基准测试，特别侧重于法律任务，包括分类、摘要、开放问题和一般推理。我们的研究发现，法律任务对大型语言模型构成了重大挑战，在LEXam等法律推理基准测试上的准确率往往低于50%，而在XNLI等通用任务上的准确率超过70%。此外，虽然英语通常会产生更稳定的结果，但并不总是导致更高的准确性。提示敏感性和对抗性脆弱性也显示会在各种语言中持续存在。最后，我们发现一种语言与英语的句法相似性与其表现之间存在关联。我们还观察到LLaMA比Gemini表现较弱，后者在同一任务上平均优势约为24个百分点。尽管新一代的大型语言模型有所改进，但在关键的多语言法律应用中可靠部署它们仍存在挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22472v1">PDF</a> 39 pages, 36 figures. Code and evaluation pipeline available at   <a target="_blank" rel="noopener" href="https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs">https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在法律领域的应用日益广泛，但其在多语言、司法管辖多样以及对抗性环境下的表现尚待充分探索。本研究对LLaMA和Gemini进行了多语言法律和非法律基准测试，并通过字符和单词级别的扰动评估了它们在法律任务中的对抗性稳健性。研究发现，法律任务对LLMs构成重大挑战，准确率常低于50%，且在跨语言环境下存在提示敏感性和对抗性脆弱性问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在法律领域的应用愈发重要，但其在法律任务中的表现尚待深入研究。</li>
<li>LLMs在多语言环境下的表现，特别是对抗性情境，仍面临诸多挑战。</li>
<li>LLaMA和Gemini在多元法律基准测试中的表现存在差异，Gemini相对更优越。</li>
<li>法律任务的复杂性使得LLMs的准确率常低于50%，尤其是法律推理任务。</li>
<li>在某些情况下，英语并不总是导致更高的准确率。</li>
<li>LLMs对提示的敏感性和对抗性脆弱性在跨语言环境下持续存在。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22472">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2eb79dd91f33416fc2c2b6e2a73f2c7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059591&auth_key=1760059591-0-0-91db375a77297d667df02a9f595e9e8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd2d645efbd0a3f357ba1319a9b1047d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059599&auth_key=1760059599-0-0-826aa5fa65c0592499d5385e359188fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MDAR-A-Multi-scene-Dynamic-Audio-Reasoning-Benchmark"><a href="#MDAR-A-Multi-scene-Dynamic-Audio-Reasoning-Benchmark" class="headerlink" title="MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark"></a>MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</h2><p><strong>Authors:Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at <a target="_blank" rel="noopener" href="https://github.com/luckyerr/MDAR">https://github.com/luckyerr/MDAR</a>. </p>
<blockquote>
<p>从音频中推理的能力，包括语音、副语言线索、环境声音和音乐，对于人工智能代理在真实世界场景中有效地交互至关重要。现有的基准测试主要关注静态或单一场景设置，并没有完全捕捉多说话者、展开的事件和异质音频源交互的场景。为了解决这些挑战，我们引入了MDAR，这是一个用于评估复杂、多场景和动态演化音频推理任务的模型性能的基准测试。MDAR包含3000个精心策划的问题答案对，与各种音频剪辑相关联，涵盖五类复杂推理，涉及三种问题类型。我们在MDAR上评估了26个最先进的音频语言模型，并观察到它们在复杂推理任务上的局限性。在单选题方面，Qwen2.5-Omni（开源）的准确率为76.67%，而GPT-4o Audio（闭源）的准确率为68.47%；然而，在更具挑战性的多选和开放式任务上，GPT-4o Audio显著优于Qwen2.5-Omni。在所有三种问题类型中，没有任何模型的性能达到80%。这些发现强调了MDAR所面临的独特挑战以及其作为推进音频推理研究基准测试的价值。代码和基准测试可在<a target="_blank" rel="noopener" href="https://github.com/luckyerr/MDAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luckyerr/MDAR找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22461v1">PDF</a> 25 pages, 7 figures</p>
<p><strong>Summary</strong><br>    文章强调了在真实世界场景中，AI需要从音频中进行推理的能力至关重要，包括语音、副语言线索、环境声音和音乐。现有基准测试主要集中在静态或单一场景设置上，无法充分捕捉复杂、多场景和动态演变的音频推理任务。为此，文章引入MDAR基准测试，涵盖五大类复杂推理和三种问题类型。对现有先进的音频语言模型进行基准测试发现，它们在复杂推理任务上存在局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI在真实世界场景中的音频推理能力至关重要，包括从语音、副语言线索、环境声音和音乐中推理。</li>
<li>现有基准测试无法充分捕捉复杂、多场景和动态演变的音频推理任务。</li>
<li>MDAR基准测试用于评估模型在复杂、多场景和动态音频推理任务上的性能。</li>
<li>MDAR包含与各种音频剪辑相关的3000个精心策划的问题答案对。</li>
<li>在MDAR基准测试中，现有音频语言模型在复杂推理任务中表现出局限性。</li>
<li>在单项选择题方面，Qwen2.5-Omni表现较好，而GPT-4o Audio在某些更具挑战性的多项选择和开放式任务中表现更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22461">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a9feddee44798a516a028fee1c8d8c5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059608&auth_key=1760059608-0-0-49823cf5a50cd4860af161cd79e097d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc924bd899e89ae40a2c299574255009~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059615&auth_key=1760059615-0-0-e24f2f43659b46584948d61ff5e2072a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07081a99ce87a70868e137283191d684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059622&auth_key=1760059622-0-0-6539209041bb57ee40a974f65151c723&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-774d06abe2dc3a5e8784ce988d605400~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059629&auth_key=1760059629-0-0-d2e144ac77da475b8dd6c40ae2811bdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e58d1a7896b82ec97fad6afb343bc2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059657&auth_key=1760059657-0-0-00f986a181836673622f5656ad99335f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ca8feb11e6b1dad3f523a792e6de51e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059664&auth_key=1760059664-0-0-35d07bdbe1c982db373f04076399e395&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TreeMind-Automatically-Reproducing-Android-Bug-Reports-via-LLM-empowered-Monte-Carlo-Tree-Search"><a href="#TreeMind-Automatically-Reproducing-Android-Bug-Reports-via-LLM-empowered-Monte-Carlo-Tree-Search" class="headerlink" title="TreeMind: Automatically Reproducing Android Bug Reports via   LLM-empowered Monte Carlo Tree Search"></a>TreeMind: Automatically Reproducing Android Bug Reports via   LLM-empowered Monte Carlo Tree Search</h2><p><strong>Authors:Zhengyu Chen, Zhaoyi Meng, Wenxiang Zhao, Wansen Wang, Haoyang Zhao, Jiahao Zhan, Jie Cui, Hong Zhong</strong></p>
<p>Automatically reproducing Android app crashes from textual bug reports is challenging, particularly when the reports are incomplete and the modern UI exhibits high combinatorial complexity. Existing approaches based on reinforcement learning or large language models (LLMs) exhibit limitations in such scenarios. They struggle to infer unobserved steps and reconstruct the underlying user action sequences to navigate the vast UI interaction space, primarily due to limited goal-directed reasoning and planning. We present TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug reproduction. To the best of our knowledge, this is the first work to combine external decision-making with LLM semantic reasoning for reliable bug reproduction. We formulate the reproduction task as a target-driven search problem, leveraging MCTS as the core planning mechanism to iteratively refine action sequences. To enhance MCTS with semantic reasoning, we introduce two LLM-guided agents with distinct roles: Expander generates top-k promising actions based on the current UI state and exploration history, while Simulator estimates the likelihood that each action leads toward successful reproduction. By incorporating multi-modal UI inputs and advanced prompting techniques, TreeMind conducts feedback-aware navigation that identifies missing but essential user actions and incrementally reconstructs the reproduction paths. We evaluate TreeMind on a dataset of 93 real-world Android bug reports from three widely-used benchmarks. Experimental results show that it significantly outperforms four state-of-the-art baselines in reproduction success rate. A real-world case study indicates that integrating LLM reasoning with MCTS-based planning is a compelling direction for automated bug reproduction. </p>
<blockquote>
<p>自动根据文本故障报告重现安卓应用崩溃是有挑战性的，尤其是在报告不完整且现代用户界面具有高组合复杂性的情况下。基于强化学习或大型语言模型（LLM）的现有方法在此类场景中表现出局限性。他们很难推断未观察到的步骤并重建潜在的用户操作序列，以导航广泛的UI交互空间，这主要是因为缺乏目标导向的推理和规划。我们提出了TreeMind，这是一种将LLM与定制蒙特卡洛树搜索（MCTS）算法相结合的新技术，以实现战略性的UI探索，用于故障重现。据我们所知，这是第一个将外部决策制定与LLM语义推理相结合的工作，以实现可靠的故障重现。我们将重现任务制定为目标驱动搜索问题，利用MCTS作为核心规划机制来迭代优化动作序列。为了通过语义推理增强MCTS，我们引入了两个由LLM引导的主体，分别扮演不同的角色：扩展器根据当前UI状态和探索历史生成前k个有前景的动作，而模拟器估计每个动作导致成功重现的可能性。通过结合多模式UI输入和先进的提示技术，TreeMind进行了反馈感知导航，可识别缺失但至关重要的用户操作，并逐步重建重现路径。我们在包含来自三个广泛使用的基准测试的93个真实世界安卓故障报告的数据集上评估了TreeMind。实验结果表明，在重现成功率方面，它显著优于四种最新技术基线。一个真实案例研究表明，将LLM推理与基于MCTS的规划相结合是自动化故障重现的一个令人信服的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22431v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化地从文本bug报告中重现Android应用崩溃具有挑战性，特别是在报告不完整且现代UI具有高度的组合复杂性时。现有的基于强化学习或大型语言模型（LLMs）的方法存在局限性。它们难以推断未观察到的步骤并重建潜在的用户操作序列来导航广阔的UI交互空间，这主要是由于缺乏目标导向的推理和规划。本研究提出了TreeMind技术，该技术将LLMs与定制的蒙特卡洛树搜索（MCTS）算法相结合，以实现战略性的UI探索用于bug重现。这是首次将外部决策与LLM语义推理相结合的工作，以实现可靠的bug重现。我们将重现任务制定为目标驱动搜索问题，利用MCTS作为核心规划机制来迭代地完善动作序列。通过引入两个由LLM引导的agent，以增强MCTS的语义推理能力：Expander根据当前UI状态和探索历史生成有前景的k个动作，而Simulator估计每个动作导致成功重现的可能性。通过结合多模式UI输入和先进的提示技术，TreeMind进行反馈感知导航，确定缺失但关键的用户操作并逐步重建重现路径。在包含来自三个广泛使用的基准测试的93个真实世界Android bug报告的数据集上评估TreeMind。实验结果表明，它在重现成功率上显著优于四种最先进的基线方法。一项真实案例研究表明，将LLM推理与MCTS-based规划相结合是一个很有前景的方向，用于自动化bug重现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化重现Android应用崩溃从文本bug报告中具有挑战性，特别是在处理不完整报告和现代UI的高组合复杂性时。</li>
<li>现有方法（如强化学习和大型语言模型）在推断未观察到的步骤和重建用户操作序列方面存在局限性。</li>
<li>TreeMind技术结合了LLMs和MCTS算法，以战略性地探索UI用于bug重现。</li>
<li>TreeMind通过引入两个LLM引导的agent（Expander和Simulator）来增强MCTS的语义推理能力。</li>
<li>TreeMind通过结合多模式UI输入和先进的提示技术，进行反馈感知导航并重建bug重现路径。</li>
<li>在包含真实世界Android bug报告的数据集上进行的实验表明，TreeMind在重现成功率方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22431">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f2ff6b07f648afbe4a6310dd53ea6008~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059693&auth_key=1760059693-0-0-f638103ffc39d8a49abc371d9f191992&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c4765264368197327fbd8fa2d1473273~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059702&auth_key=1760059702-0-0-aac407a0e4f46d61fdaacc47c56d14f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bb49b96385e1fe0f6367de339b879c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059709&auth_key=1760059709-0-0-2ac1f6883e2a3a73e69e11df3c48d064&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoveFM-R-Advancing-Mobility-Foundation-Models-via-Language-driven-Semantic-Reasoning"><a href="#MoveFM-R-Advancing-Mobility-Foundation-Models-via-Language-driven-Semantic-Reasoning" class="headerlink" title="MoveFM-R: Advancing Mobility Foundation Models via Language-driven   Semantic Reasoning"></a>MoveFM-R: Advancing Mobility Foundation Models via Language-driven   Semantic Reasoning</h2><p><strong>Authors:Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li</strong></p>
<p>Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLM’s reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MoveFM-R-CDE7/">https://anonymous.4open.science/r/MoveFM-R-CDE7/</a>. </p>
<blockquote>
<p>移动基础模型（MFMs）已经推进了人类运动模式的建模，但由于数据规模和语义理解的限制，它们面临上限。虽然大型语言模型（LLMs）提供了强大的语义推理，但它们缺乏生成物理上可行的移动轨迹所需的时空统计的固有理解。为了解决这些差距，我们提出了MoveFM-R，这是一个利用语言驱动的语义推理能力解锁移动基础模型潜力的新型框架。它解决了两个关键挑战：连续地理坐标和离散语言令牌之间的词汇不匹配，以及MFMs的潜在向量和LLMs的语义世界之间的表示差距。MoveFM-R建立在三项核心创新之上：一种语义增强的位置编码，以弥合地理与语言的差距；一种渐进式课程，以使LLM的推理与移动模式对齐；以及一种用于条件轨迹生成交互式自我反思机制。大量实验表明，MoveFM-R显著优于现有的基于MFM和LLM的基线。它还在零样本设置中表现出稳健的泛化能力，并擅长根据自然语言指令生成逼真的轨迹。通过合成MFM的统计能力与LLM的深度语义理解，MoveFM-R开创了一种新范式，使人类移动性的建模更加全面、可解释和强大。MoveFM-R的实现可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MoveFM-R-CDE7/%E5%9C%A8%E7%BA%BF%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/r/MoveFM-R-CDE7/在线访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Mobility Foundation Models（MFMs）在建模人类移动模式方面的进展及其面临的数据规模和语义理解上的局限性。针对这些问题，提出了一种新型框架MoveFM-R，它通过利用语言驱动的语义推理能力，解锁了移动基础模型的全潜力。该框架解决了连续地理坐标和离散语言令牌之间词汇不匹配以及MFMs的潜在向量和LLMs的语义世界之间表示差距等两个关键挑战。MoveFM-R建立在三项核心创新之上：语义增强的位置编码以弥补地理与语言之间的差距，渐进式课程以使LLM的推理与移动模式对齐，以及用于条件轨迹生成交互式自我反思机制。实验表明，MoveFM-R显著优于现有的MFM和LLM基线，在零样本设置中表现出强大的泛化能力，并擅长根据自然语言指令生成逼真的轨迹。它通过合成MFMs的统计能力与LLMs的深度语义理解，开创了一种新的范式，使人类移动性的建模更加全面、可解释和强大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MFMs在建模人类移动模式方面取得进展，但面临数据规模和语义理解的局限性。</li>
<li>MoveFM-R框架旨在解锁MFMs的潜力，结合语言驱动的语义推理能力。</li>
<li>MoveFM-R解决两个关键挑战：地理坐标与语言令牌之间的词汇不匹配以及MFMs与LLMs之间的表示差距。</li>
<li>MoveFM-R建立在三项核心创新上：语义增强的位置编码、渐进式课程对齐和交互式自我反思机制。</li>
<li>MoveFM-R显著优于现有基线，具有强大的泛化能力，并能根据自然语言指令生成真实轨迹。</li>
<li>MoveFM-R合成MFMs的统计能力与LLMs的语义理解，为建模人类移动性开创了新范式。</li>
<li>MoveFM-R框架的实现可在网上获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f6d0183df4484cf41b098bf16cace855~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059716&auth_key=1760059716-0-0-66e5f82c08f99e42be026c7636fa5b15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-beadbfb32dc52bbcabb15e9a9aff94c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059723&auth_key=1760059723-0-0-627227f3ca2ef5849446d5fe6a7a5ccb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CircuitSense-A-Hierarchical-Circuit-System-Benchmark-Bridging-Visual-Comprehension-and-Symbolic-Reasoning-in-Engineering-Design-Process"><a href="#CircuitSense-A-Hierarchical-Circuit-System-Benchmark-Bridging-Visual-Comprehension-and-Symbolic-Reasoning-in-Engineering-Design-Process" class="headerlink" title="CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual   Comprehension and Symbolic Reasoning in Engineering Design Process"></a>CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual   Comprehension and Symbolic Reasoning in Engineering Design Process</h2><p><strong>Authors:Arman Akbari, Jian Gao, Yifei Zou, Mei Yang, Jinru Duan, Dmitrii Torbunov, Yanzhi Wang, Yihui Ren, Xuan Zhang</strong></p>
<p>Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence. </p>
<blockquote>
<p>工程设计通过层次抽象从系统规格到组件实现进行操作，需要在每个层次上结合视觉理解和数学推理。虽然多模态大型语言模型（MLLMs）在自然图像任务上表现出色，但它们在从技术图表中提取数学模型的能力仍待探索。我们提出了CircuitSense，这是一个全面的基准测试，通过8006多个问题评估从组件级原理图到系统级框图的电路理解层次。我们的基准测试独特地考察了完整的工程工作流程：感知、分析和设计，特别是从视觉输入中推导符号方程这一关键但尚未充分探索的能力。我们引入了一个分层的合成生成管道，包括一个基于网格的原理图生成器和一个带有自动派生符号方程标签的框图生成器。对六个最新MLLMs的综合评估，包括封闭源代码和开放源代码模型，揭示了视觉到数学推理的根本局限性。封闭源代码的模型在涉及组件识别和拓扑识别的感知任务上的准确率超过85%，但在符号推导和分析推理方面的性能低于19%，暴露了视觉解析和符号推理之间的关键差距。具有更强符号推理能力的模型在设计任务上的准确性持续更高，证实了数学理解在电路合成中的基础作用，并确立了符号推理作为工程能力的主要指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22339v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>工程设计的层次抽象需要从系统规格到组件实现，每个层次都需要视觉理解与数学推理的结合。当前，多模态大型语言模型（MLLMs）在图像处理任务中表现出色，但其从技术图表中提取数学模型的能力尚未被探索。本研究提出了CircuitSense综合基准测试，该测试通过超过8006个涵盖从组件级原理图到系统级框图的问题，评估电路理解的层次结构。该基准测试独特地考察了完整的工程工作流程：感知、分析和设计，特别是从视觉输入中推导符号方程这一重要但被忽视的能力。本研究引入了一个层次性的合成生成管道，包括基于网格的原理图生成器和带有自动生成的符号方程标签的框图生成器。对六个最先进的MLLMs的综合评估表明，视觉到数学的推理存在根本性局限。封闭源模型在涉及组件识别和拓扑识别的感知任务上的准确率超过85%，但在符号推导和分析推理方面的性能低于19%，暴露了视觉解析和符号推理之间的关键差距。具有更强符号推理能力的模型在设计任务上始终具有更高的准确性，证实了数学理解在工程合成中的基础作用，并确立了符号推理作为工程能力的关键指标。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>工程设计涉及从系统规格到组件实现的层次抽象，需要视觉理解与数学推理的结合。</li>
<li>当前多模态大型语言模型（MLLMs）在电路理解的视觉与数学结合方面能力有限。</li>
<li>CircuitSense基准测试用于评估电路理解的层次结构，涵盖从原理图到系统框图的问题。</li>
<li>基准测试强调完整的工程工作流程，特别是从视觉输入中推导符号方程的能力。</li>
<li>封闭源模型在感知任务上表现良好，但在符号推导和分析推理上存在显著局限。</li>
<li>模型在符号推理方面的能力对于工程设计任务的成功至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d7a3d8b775386d6dc15fc3f740789c69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059730&auth_key=1760059730-0-0-866c454bb546f1421cad12ade59256fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2de8ee66d9d8c6f9f1ab64c10e8c8c22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059738&auth_key=1760059738-0-0-1e8bc343f5c74ab6ea8b28375017cc55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51b8007e35ebbb9175027d20f401912c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059745&auth_key=1760059745-0-0-b04ff5d02b0845fd6baefd7b6eeab022&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56a577ae049a6d0cee821aadd19c57ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059752&auth_key=1760059752-0-0-ab1be7610898174778dac58318aaedfb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RAPID-3-Tri-Level-Reinforced-Acceleration-Policies-for-Diffusion-Transformer"><a href="#RAPID-3-Tri-Level-Reinforced-Acceleration-Policies-for-Diffusion-Transformer" class="headerlink" title="RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer"></a>RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer</h2><p><strong>Authors:Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You</strong></p>
<p>Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model’s distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality. </p>
<blockquote>
<p>扩散Transformer（DiTs）在视觉生成方面表现出色，但仍受到采样速度较慢的阻碍。现有的无训练加速器——步骤减少、特征缓存和稀疏注意力——提高了推理速度，但它们通常依赖于统一的启发式方法或手动设计的适用于所有图像的自适应策略，从而留下了质量提升空间。相比之下，动态神经网络提供了每图像自适应加速，但其高昂的微调成本限制了其更广泛的应用。为了解决这些限制，我们引入了RAPID3：扩散Transformer的三级强化加速策略，该框架实现了对基础生成器的零更新进行图像级加速。具体来说，三个轻量级的策略头——步骤跳过、缓存重用和稀疏注意力——观察当前的去噪状态，并独立决定每个时间步的相应加速。所有策略参数都通过组相对策略优化（GRPO）在线训练，而生成器保持不变。同时，对抗性学习鉴别器增强了奖励信号，通过仅在生成样本接近原始模型分布时提高回报来抑制奖励操纵。在包括Stable Diffusion 3和FLUX在内的最新DiT主干网络上，RAPID3实现了近3倍的快速采样，同时保持有竞争力的生成质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对Diffusion Transformers（DiTs）的加速框架RAPID3，该框架能够在不更新基础生成器的情况下实现图像级别的加速。RAPID3包括三个独立的策略头：Step-Skip、Cache-Reuse和Sparse-Attention，它们根据当前的去噪状态在每个时间步独立决定加速。通过Group Relative Policy Optimization（GRPO）在线训练策略参数，同时采用对抗性学习的方法增强奖励信号，以保持生成样本接近于原始模型分布。RAPID3能够在多个先进的DiT主干网络上实现近3倍的采样速度提升，同时保持竞争力强的生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAPID3是一个针对Diffusion Transformers的加速框架，旨在提高图像生成的速度而不损失质量。</li>
<li>RAPID3包括三个策略头：Step-Skip、Cache-Reuse和Sparse-Attention，它们根据去噪状态进行决策以加速采样过程。</li>
<li>策略参数通过Group Relative Policy Optimization（GRPO）在线训练，而基础生成器保持不变。</li>
<li>RAPID3采用对抗性学习的方法增强奖励信号，以确保生成的样本接近原始模型分布。</li>
<li>RAPID3能够在多个先进的DiT主干网络上实现近3倍的采样速度提升。</li>
<li>RAPID3框架能够应对训练免费加速器的局限性，通过更精细的加速策略提高图像生成效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22323">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-89f100493232136ce1b88f3f506feafe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059760&auth_key=1760059760-0-0-8e7fd65fe4056fb948f21cf5b7251ecf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f89dfa9a2dabcd47308776f608c3d773~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059767&auth_key=1760059767-0-0-d10ca080a4e2549f96851053c5ca85d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76a9646c8341e4d32144b287d07c8fd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059774&auth_key=1760059774-0-0-d629f7b7f6788d57f1899631df6e1d91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rule-Based-Reinforcement-Learning-for-Document-Image-Classification-with-Vision-Language-Models"><a href="#Rule-Based-Reinforcement-Learning-for-Document-Image-Classification-with-Vision-Language-Models" class="headerlink" title="Rule-Based Reinforcement Learning for Document Image Classification with   Vision Language Models"></a>Rule-Based Reinforcement Learning for Document Image Classification with   Vision Language Models</h2><p><strong>Authors:Michael Jungo, Andreas Fischer</strong></p>
<p>Rule-based reinforcement learning has been gaining popularity ever since DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In the domain of document analysis, reinforcement learning is not as prevalent, even though many downstream tasks may benefit from the emerging properties of reinforcement learning, particularly the enhanced reason capabilities. We study the effects of rule-based reinforcement learning with the task of Document Image Classification which is one of the most commonly studied downstream tasks in document analysis. We find that reinforcement learning tends to have better generalisation capabilities to out-of-distritbution data, which we examine in three different scenarios, namely out-of-distribution images, unseen classes and different modalities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune">https://github.com/jungomi/vision-finetune</a>. </p>
<blockquote>
<p>基于规则的强化学习自从DeepSeek-R1通过简单的可验证奖励展示其成功以来，一直受到广泛关注。虽然在文档分析领域，强化学习的普及程度并不高，尽管许多下游任务可能会从强化学习的新兴属性中受益，尤其是增强的推理能力。我们研究了基于规则的强化学习在文档图像分类任务中的应用，这是文档分析中最常见的下游任务之一。我们发现强化学习对于离群数据具有更好的泛化能力，我们在三种不同场景中对此进行了检查，即离群图像、未见类别和不同模式。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jungomi/vision-finetune上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22283v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune">https://github.com/jungomi/vision-finetune</a></p>
<p><strong>Summary</strong>：规则化的强化学习因DeepSeek-R1的成功演示简单可验证奖励而受到广泛关注。在文档分析领域，强化学习尚未普及，尽管许多下游任务可从强化学习的新兴属性中受益，特别是增强推理能力。研究团队以文档图像分类任务为对象研究基于规则的强化学习效果，发现强化学习在三种不同场景下对离群分布数据有更好的泛化能力，包括离群图像、未见类别和不同模态。相关代码已公开在<a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune%E3%80%82">https://github.com/jungomi/vision-finetune。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习通过简单可验证奖励实现成功应用。</li>
<li>文档分析领域强化学习普及程度不高，但具有潜力。</li>
<li>基于规则的强化学习在文档图像分类任务上进行了实证研究。</li>
<li>强化学习在三种不同场景下展现出对离群分布数据的良好泛化能力。</li>
<li>强化学习能够应对如离群图像、未见类别和不同模态等挑战。</li>
<li>相关研究代码已公开在GitHub上供公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6bc721fb57818a57a8737eb716c934b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059781&auth_key=1760059781-0-0-584f5bc8219864ce27667c8fc314e9e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks"><a href="#Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks" class="headerlink" title="Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks"></a>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks</h2><p><strong>Authors:Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang, Yuanyuan Peng, Huan Gao, Mingkun Xu, Shangyang Li</strong></p>
<p>Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at <a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a> as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI. </p>
<blockquote>
<p>近期视觉语言模型（VLMs）的进步在标准医学基准测试中取得了显著的性能，但它们真正的临床推理能力仍不明确。现有数据集主要强调分类准确性，创造了一种评估错觉，即模型虽然表现熟练，但在高风险诊断推理中仍然失败。我们推出了Neural-MedBench，这是一个紧凑而推理密集型的基准测试，专门用于探索神经学多模式临床推理的极限。Neural-MedBench集成了多序列MRI扫描、结构化电子健康记录和临床笔记，包含三个核心任务家族：鉴别诊断、病灶识别和理由生成。为了确保可靠的评估，我们开发了一个混合评分管道，结合了基于LLM的评分者、临床医生验证和语义相似性度量。通过对最先进的VLMs的系统评估，包括GPT-4o、Claude-4和MedGemma，我们观察到与常规数据集相比，性能急剧下降。错误分析表明，推理失败，而不是感知错误，主导了模型的不足。我们的研究结果表明了两轴评估框架的必要性：面向广度的大型数据集用于统计泛化，以及面向深度的紧凑基准测试（如Neural-MedBench）用于推理保真度。我们公开了Neural-MedBench：<a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a>，作为一个开放和可扩展的诊断测试平台，旨在指导未来基准测试的扩展，并能够进行严格而经济实惠的临床可信人工智能评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22258v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong><br>在标准医学基准测试中，视觉语言模型取得了显著的性能提升，但其真正的临床推理能力尚不清楚。为此，我们推出了Neural-MedBench，这是一个紧凑且推理密集型的基准测试，旨在探索神经学中的多模态临床推理的极限。它集成了多序列MRI扫描、结构化电子健康记录和临床笔记，包括三个核心任务家族：鉴别诊断、病灶识别和理由生成。为了确保可靠的评估，我们开发了一个混合评分管道，结合了基于LLM的评分者、临床医生验证和语义相似性度量。通过对最先进的VLMs的系统评估，包括GPT-4o、Claude-4和MedGemma，我们发现与常规数据集相比，性能大幅下降。错误分析表明，推理失败而非感知错误是模型的主要缺陷。我们的研究强调了两轴评估框架的必要性：面向广度的大型数据集用于统计泛化，以及面向深度的紧凑基准测试如Neural-MedBench用于推理保真度。我们公开发布了Neural-MedBench（<a target="_blank" rel="noopener" href="https://neuromedbench.github.io/%EF%BC%89%EF%BC%8C%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%BC%80%E6%94%BE%E5%92%8C%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E8%AF%8A%E6%96%AD%E6%B5%8B%E8%AF%95%E5%B9%B3%E5%8F%B0%EF%BC%8C%E4%B8%BA%E6%9C%AA%E6%9D%A5%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E5%8F%91%E5%B1%95%E6%8F%90%E4%BE%9B%E6%8C%87%E5%AF%BC%EF%BC%8C%E5%B9%B6%E8%83%BD%E8%BF%9B%E8%A1%8C%E4%B8%A5%E6%A0%BC%E4%B8%94%E7%BB%8F%E6%B5%8E%E5%AE%9E%E6%83%A0%E7%9A%84%E4%B8%B4%E5%BA%8A%E4%BF%A1%E4%BB%BB%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AF%84%E4%BC%B0%E3%80%82">https://neuromedbench.github.io/），作为一个开放和可扩展的诊断测试平台，为未来基准测试的发展提供指导，并能进行严格且经济实惠的临床信任人工智能评估。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在医学基准测试上表现卓越，但临床推理能力尚不清楚。</li>
<li>推出Neural-MedBench基准测试，旨在评估多模态临床推理能力。</li>
<li>Neural-MedBench集成了MRI扫描、电子健康记录和临床笔记等多个数据源。</li>
<li>基准测试包括三个核心任务：鉴别诊断、病灶识别和理由生成。</li>
<li>开发了混合评分管道以确保评估的可靠性。</li>
<li>与常规数据集相比，VLMs在Neural-MedBench上的性能显著下降。</li>
<li>错误分析显示推理失败是模型的主要挑战，而不是感知错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-101dbbc008b15e0ecb7719ddd919518f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059788&auth_key=1760059788-0-0-f945bd9ccaf4aa7b986daa979ad75d88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00760c0a6937eb06ae90262c0d12787d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059795&auth_key=1760059795-0-0-ca44f065923afe78db2eef157c2daaad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d732e65fb684b9a920a28b17486960d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059802&auth_key=1760059802-0-0-ee3630c9026c1356224dd3dde08b98a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3a477b11590f393fd02ed9bd0908035~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059809&auth_key=1760059809-0-0-dc1eca7c7e435841dfd79e4ed32a03f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d22a37cba0c3039329a148b6243458a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059815&auth_key=1760059815-0-0-e47eff83d14ce0185e9743558ad240a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ee1e583a2088419b495f2100c61e9ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059822&auth_key=1760059822-0-0-fec89b6680b003ee206c045c1b47c978&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing"><a href="#Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing" class="headerlink" title="Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing"></a>Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing</h2><p><strong>Authors:Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Rajesh Mojumder</strong></p>
<p>This paper presents an evaluation framework for assessing Large Language Models’ (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks. </p>
<blockquote>
<p>本文提出了一个评估框架，用于评估大型语言模型（LLM）在组合优化方面的能力，特别是解决二维装箱问题。我们介绍了一种系统方法，该方法将LLM与进化算法相结合，通过迭代生成和优化启发式解决方案。通过全面的实验对比LLM生成的启发式方法与传统方法（有限首次适应法和混合首次适应法），我们证明了LLM能够生成更高效的解决方案，同时减少计算资源的需求。我们的评估表明，GPT-4o在两次迭代内实现了最优解，平均使用的箱子数量从原来的16个减少到15个，空间利用率也从原来的0.76-0.78提升到现在的0.83。这项研究对于理解LLM在特定领域的评估有着重要的贡献，并建立了评估LLM在组合优化任务中性能的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22255v1">PDF</a> 1 table, 6 figures. 39th Conference on Neural Information Processing   Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM   Lifecycle Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>该文评估了大型语言模型（LLM）在组合优化领域的能力，特别是解决二维装箱问题的方法。文章提出了一种结合LLM和进化算法的系统方法，通过迭代生成和优化启发式解决方案。实验证明，LLM生成的启发式方法比传统方法（如有限首次适配和混合首次适配）更高效，且计算资源消耗更少。评价结果显示，GPT-4o在两次迭代内达到最优解，平均使用箱数从16个减少到15个，空间利用率从0.76-0.78提高到0.83。该研究加深了对LLM在特定领域评价的理解，并为评估其在组合优化任务中的性能建立了基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章评估了大型语言模型（LLM）在解决组合优化问题中的表现，特别是针对二维装箱问题。</li>
<li>提出了一种结合LLM和进化算法的系统方法，用于生成和优化启发式解决方案。</li>
<li>通过实验证明，LLM生成的启发式方法相较于传统方法更为高效，且计算资源消耗更少。</li>
<li>GPT-4o在两次迭代内找到最优解，展现了其高效性能。</li>
<li>研究结果提高了空间利用率，平均使用箱数有所减少。</li>
<li>该研究有助于加深对LLM在特定领域评价的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8eb5498545dffd4406ac9e628e97a22b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059828&auth_key=1760059828-0-0-490c61b79a7350fb01c478371d2faa0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ef43380d407c29ce79b44d190b9cbc5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059835&auth_key=1760059835-0-0-867c34f09d91a9390f06ca768336ce9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de879a991a7537028f14fa0a9616aabb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059842&auth_key=1760059842-0-0-cd38cb567db444818b808c61f6dd662f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ASSESS-A-Semantic-and-Structural-Evaluation-Framework-for-Statement-Similarity"><a href="#ASSESS-A-Semantic-and-Structural-Evaluation-Framework-for-Statement-Similarity" class="headerlink" title="ASSESS: A Semantic and Structural Evaluation Framework for Statement   Similarity"></a>ASSESS: A Semantic and Structural Evaluation Framework for Statement   Similarity</h2><p><strong>Authors:Xiaoyang Liu, Tao Zhu, Zineng Dong, Yuntian Liu, Qingfeng Guo, Zhaoxuan Liu, Yu Chen, Tao Luo</strong></p>
<p>Statement autoformalization, the automated translation of statements from natural language into formal languages, has seen significant advancements, yet the development of automated evaluation metrics remains limited. Existing metrics for formal statement similarity often fail to balance semantic and structural information. String-based approaches capture syntactic structure but ignore semantic meaning, whereas proof-based methods validate semantic equivalence but disregard structural nuances and, critically, provide no graded similarity score in the event of proof failure. To address these issues, we introduce ASSESS (A Semantic and Structural Evaluation Framework for Statement Similarity), which comprehensively integrates semantic and structural information to provide a continuous similarity score. Our framework first transforms formal statements into Operator Trees to capture their syntactic structure and then computes a similarity score using our novel TransTED (Transformation Tree Edit Distance) Similarity metric, which enhances traditional Tree Edit Distance by incorporating semantic awareness through transformations. For rigorous validation, we present EPLA (Evaluating Provability and Likeness for Autoformalization), a new benchmark of 524 expert-annotated formal statement pairs derived from miniF2F and ProofNet, with labels for both semantic provability and structural likeness. Experiments on EPLA demonstrate that TransTED Similarity outperforms existing methods, achieving state-of-the-art accuracy and the highest Kappa coefficient. The benchmark, and implementation code will be made public soon. </p>
<blockquote>
<p>声明自动形式化（将自然语言中的声明自动转换为形式化语言）已经取得了重大进展，但自动评估指标的发展仍然有限。现有的形式化声明相似性度量通常无法平衡语义和结构信息。基于字符串的方法可以捕捉句法结构，但忽略了语义意义，而基于证明的方法验证了语义等价性，但忽略了结构细微差别，并且在证明失败的情况下不提供分级相似性评分。为了解决这些问题，我们引入了ASSESS（声明相似性的语义和结构评估框架），它全面整合语义和结构信息以提供连续的相似性评分。我们的框架首先将形式化声明转换为操作树以捕获其句法结构，然后使用我们新颖的TransTED（转换树编辑距离）相似性度量来计算相似性得分，该度量通过转换融入语义意识，增强了传统的树编辑距离。为了严格验证，我们推出了EPLA（自动形式化的可证明性和相似性评价），这是从miniF2F和ProofNet派生的、带有语义可证明性和结构相似性的专家注释形式化声明对的新基准数据集，包含524对数据点。在EPLA上的实验表明，TransTED相似性度量优于现有方法，达到了最先进的准确性和最高的Kappa系数。基准数据集和实现代码将很快公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22246v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动形式化声明的评估方法的发展状况，针对现有方法的不足，提出了一个新的综合语义和结构信息的评价框架——ASSESS，并引入了TransTED相似度度量方法。该方法通过转换操作树捕捉语句的语法结构，并通过语义感知的转换增强传统的树编辑距离。实验结果表明，TransTED相似度度量方法在EPLA基准测试集上表现优异，准确率和kappa系数均达到最佳水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动形式化声明的评估仍然是研究的热点和难点。现有的评估方法无法平衡语义和结构信息。</li>
<li>ASSESS框架通过结合语义和结构信息来解决现有问题，提供了一种连续的相似度评分方法。它通过操作树来捕捉语法结构。</li>
<li>TransTED相似度度量方法是本文提出的创新方法，通过融入语义感知的转换来增强传统的树编辑距离。</li>
<li>EPLA基准测试集由专家标注的524对形式化声明组成，用于评估自动形式化的可信度与相似性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22246">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ede2327f8ec11b8695c73c76ef12f979~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059849&auth_key=1760059849-0-0-f102c306edb8f81ddd7f9453563472b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b579ec000ca46c0a4acce11c9a309b17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059857&auth_key=1760059857-0-0-d704c04db3c9e46b0de2d96f4ddc8bc4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c85c5fd464bcd2c8d1c63acab75a41bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059863&auth_key=1760059863-0-0-a4683dcc44f6fad6de6bb1d7459b6cec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3c5bd3833ac65ad4b58285988473e53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059869&auth_key=1760059869-0-0-22dbd5e2c4125b1619d8fd548e3a5623&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Thinking-in-Many-Modes-How-Composite-Reasoning-Elevates-Large-Language-Model-Performance-with-Limited-Data"><a href="#Thinking-in-Many-Modes-How-Composite-Reasoning-Elevates-Large-Language-Model-Performance-with-Limited-Data" class="headerlink" title="Thinking in Many Modes: How Composite Reasoning Elevates Large Language   Model Performance with Limited Data"></a>Thinking in Many Modes: How Composite Reasoning Elevates Large Language   Model Performance with Limited Data</h2><p><strong>Authors:Zishan Ahmad, Saisubramaniam Gopalakrishnan</strong></p>
<p>Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities. </p>
<blockquote>
<p>尽管大型语言模型（LLMs）具有显著的能力，但它们依赖于单一、主导的推理范式，这阻碍了它们在需要多种认知策略的复杂问题上的表现。为了解决这个问题，我们引入了复合推理（CR），这是一种新型推理方法，能够赋予LLMs动态探索和结合多种推理风格的能力，如演绎推理、归纳推理和类比推理，以更微妙的方式解决问题。在科学和医学问答基准测试上，我们的方法优于现有的基线方法，如思维链（CoT），并且超越了DeepSeek-R1风格推理（SR）的准确率，同时显示出较高的样本效率和足够的令牌使用效率。值得注意的是，CR能够自适应地强调与领域相适应的推理风格。它在医学问题回答中优先使用类比和演绎推理，但在科学推理中则转向因果、演绎和归纳方法。我们的研究结果表明，通过培养内部推理风格的多样性，LLMs可以获得更稳健、自适应和高效的问题解决能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22224v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在解决复杂问题时受限于单一的预定义推理模式。为解决这一问题，我们提出了复合推理（CR）这一新型推理方法，它能动态探索并结合多种推理风格（如演绎推理、归纳推理和假设推理），从而实现更精细的问题解决。在科研和医疗问答基准测试中，复合推理方法优于现有的思维链（CoT）等基准测试方法，并超越了DeepSeek-R1式推理（SR）的准确率，表现出优越的样本效率和足够的标记词使用效率。特别地，复合推理可以自适应地强调与特定领域适合的推理风格，比如在医疗问答上注重假设推理和演绎推理，而在科学推理上则转向因果、演绎和归纳方法。我们的研究结果表明，通过培养内部推理风格的多样性，LLM可以获得更强大、适应性和高效的问题解决能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM受限于单一预定义推理模式，难以解决需要多种认知策略的复杂问题。</li>
<li>引入复合推理（CR）方法，能结合多种推理风格（如演绎、归纳和假设推理）。</li>
<li>在科研和医疗问答基准测试中，复合推理表现优于现有方法，如思维链（CoT）。</li>
<li>复合推理方法提高了样本效率和标记词使用效率。</li>
<li>复合推理能自适应强调特定领域的推理风格，如医疗领域的假设和演绎推理。</li>
<li>通过培养内部推理风格的多样性，LLM获得更强大、适应性和高效的问题解决能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d096f727fc52acf3347d90a5095a827c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059877&auth_key=1760059877-0-0-a8ff675fcb144505f1132b33be4777c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f63ade4311e94344eac5fbf8d9d5779~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059884&auth_key=1760059884-0-0-2fe1d0539fd7048b15aad8036e5fbffc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0aec70d7c67a500fd071b6e4c2d70351~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059890&auth_key=1760059890-0-0-cb741a3a4c80a3b23f3c8e614d87f69b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-220f139e4f7a49c94ad2d805cf2a75b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059897&auth_key=1760059897-0-0-e091dd8be30000dfc7f09922fc4bda7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Faithful-Reasoning-in-Remote-Sensing-A-Perceptually-Grounded-GeoSpatial-Chain-of-Thought-for-Vision-Language-Models"><a href="#Towards-Faithful-Reasoning-in-Remote-Sensing-A-Perceptually-Grounded-GeoSpatial-Chain-of-Thought-for-Vision-Language-Models" class="headerlink" title="Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded   GeoSpatial Chain-of-Thought for Vision-Language Models"></a>Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded   GeoSpatial Chain-of-Thought for Vision-Language Models</h2><p><strong>Authors:Jiaqi Liu, Lang Sun, Ronghao Fu, Bo Yang</strong></p>
<p>Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model’s reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation. </p>
<blockquote>
<p>遥感中的视觉语言模型（VLMs）往往在复杂的分析任务上表现不佳，这一局限性源于其端到端的训练范式，这种范式跳过了关键的推理步骤，导致输出无法验证。为了解决这一局限性，我们引入了感知基础地理思维链（Geo-CoT），这是一种将遥感分析建模为可验证的多步骤过程的框架。我们通过两阶段对齐策略来灌输这种分析过程，利用Geo-CoT380k，这是第一个大规模的结构化Geo-CoT理由数据集。该策略首先采用监督微调（SFT）来灌输基础认知架构，然后利用组奖励政策优化（GRPO）来完善模型的推理策略，使其朝事实正确性方向发展。由此产生的模型RSThinker不仅输出最终答案，还输出可验证的分析轨迹。这种能力带来了卓越的性能，在一系列任务中显著优于最先进的模型。我们数据集Geo-CoT380k和RSThinker模型的公开发布，为从模糊感知走向结构化、可验证的地球观测推理提供了具体的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在遥感领域中，视觉语言模型（VLMs）在处理复杂分析任务时的局限性，提出了一种名为Perceptually-Grounded Geospatial Chain-of-Thought（Geo-CoT）的框架来解决这一问题。该框架通过两阶段对齐策略，利用Geo-CoT380k数据集，将遥感分析建模为可验证的多步骤过程。首先通过监督微调（SFT）来建立基础认知架构，然后利用Group Reward Policy Optimization（GRPO）来优化模型的推理策略，以实现事实正确性。最终产生的模型RSThinker不仅能给出最终答案，还能提供可验证的分析轨迹。这一能力使其在多种任务上显著优于现有最先进的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在遥感复杂分析任务中存在局限性，需要一种新的框架来解决。</li>
<li>提出了Perceptually-Grounded Geospatial Chain-of-Thought（Geo-CoT）框架，将遥感分析建模为可验证的多步骤过程。</li>
<li>利用两阶段对齐策略和Geo-CoT380k数据集来建立和推广这一框架。</li>
<li>第一阶段通过监督微调（SFT）建立基础认知架构。</li>
<li>第二阶段利用Group Reward Policy Optimization（GRPO）优化模型的推理策略，实现事实正确性。</li>
<li>最终产生的模型RSThinker能同时输出最终答案和可验证的分析轨迹。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ffdd8e66a0e4be00eff8384aa77c658b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059904&auth_key=1760059904-0-0-e5b6c8685d1242c01cab031bc580fb0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c86ad03b5dfddc88026eb30aec3e1e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059912&auth_key=1760059912-0-0-ed8173b62102b1e6ceec0c8cda1c4d65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62a3a2d3f1e62223c1358b21f63b752e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059918&auth_key=1760059918-0-0-f8e8c8525fa5c1d4691c5ffde16b39d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3dc56433121a6951f77df7df7a6be9b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059925&auth_key=1760059925-0-0-bf74d9988d2abf115bc27445113a555b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1a863b482cb97b8b8a21713de2a7dcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059932&auth_key=1760059932-0-0-c11ff892b62671d27d59058fec6f5e60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-df408d6615225b002a326d82d15e2cf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760059939&auth_key=1760059939-0-0-ee94fca5d024c3a3b8a27ac231330e97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-30  VoiceAssistant-Eval Benchmarking AI Assistants across Listening,   Speaking, and Viewing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Talking Head Generation/2509.21465v1/page_3_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-30  Comprehend and Talk Text to Speech Synthesis via Dual Language Modeling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
