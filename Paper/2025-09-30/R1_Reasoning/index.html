<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  WebGen-Agent Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-61d527f5dfa72b4247df0be816b64da4')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-30-æ›´æ–°"><a href="#2025-09-30-æ›´æ–°" class="headerlink" title="2025-09-30 æ›´æ–°"></a>2025-09-30 æ›´æ–°</h1><h2 id="WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning"><a href="#WebGen-Agent-Enhancing-Interactive-Website-Generation-with-Multi-Level-Feedback-and-Step-Level-Reinforcement-Learning" class="headerlink" title="WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning"></a>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level   Feedback and Step-Level Reinforcement Learning</h2><p><strong>Authors:Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the modelâ€™s website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ç³»ç»Ÿå·²åœ¨ä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºé«˜åº¦ä¾èµ–è§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ç½‘ç«™ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå½“å‰ä»£ç æ™ºèƒ½ä½“ä»…ä¾èµ–äºç®€å•çš„ä»£ç æ‰§è¡Œè¿›è¡Œåé¦ˆå’ŒéªŒè¯ã€‚è¿™ç§æ–¹æ³•æ— æ³•æ•æ‰ç”Ÿæˆä»£ç çš„å®é™…è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WebGen-Agentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç½‘ç«™ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œå®ƒåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆæ¥è¿­ä»£ç”Ÿæˆå’Œç»†åŒ–ç½‘ç«™ä»£ç åº“ã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå…³äºç½‘ç«™æˆªå›¾å’ŒGUIæ™ºèƒ½ä½“æµ‹è¯•çš„è¯¦ç»†è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼Œä»¥åŠé‡åŒ–å…¶è´¨é‡çš„åˆ†æ•°ã€‚æˆªå›¾å’ŒGUIæ™ºèƒ½ä½“çš„åˆ†æ•°è¿›ä¸€æ­¥ä¸å›æº¯å’Œé€‰æ‹©æœ€ä½³æœºåˆ¶ç›¸ç»“åˆï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚åˆ©ç”¨WebGen-Agentå·¥ä½œæµç¨‹ä¸­å›ºæœ‰çš„å‡†ç¡®è§†è§‰åˆ†æ•°ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¸¦æœ‰æˆªå›¾å’ŒGUIæ™ºèƒ½ä½“åé¦ˆçš„Step-GRPOï¼Œä»¥æé«˜LLMä½œä¸ºWebGen-Agentæ¨ç†å¼•æ“çš„èƒ½åŠ›ã€‚é€šè¿‡å°†æ¯ä¸€æ­¥çš„æˆªå›¾å’ŒGUIæ™ºèƒ½ä½“åˆ†æ•°ä½œä¸ºStep-GRPOä¸­çš„å¥–åŠ±ï¼Œæˆ‘ä»¬æä¾›äº†å¯†é›†å¯é çš„æµç¨‹ç›‘ç£ä¿¡å·ï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹çš„ç½‘ç«™ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨WebGen-Benchæ•°æ®é›†ä¸Šï¼ŒWebGen-Agentå°†Claude-3.5-Sonnetçš„å‡†ç¡®æ€§ä»26.4%æé«˜åˆ°51.9%ï¼Œå¤–è§‚å¾—åˆ†ä»3.0æé«˜åˆ°3.9ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ™ºèƒ½ä½“ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„Step-GRPOè®­ç»ƒæ–¹æ³•å°†Qwen2.5-Coder-7B-Instructçš„å‡†ç¡®æ€§ä»38.9%æé«˜åˆ°45.4%ï¼Œå¤–è§‚å¾—åˆ†ä»3.4æé«˜åˆ°3.7ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22644v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç³»ç»Ÿåœ¨ä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºä¾èµ–è§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ç½‘ç«™ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå½“å‰ä»£ç ä»£ç†ä»…ä¾èµ–äºç®€å•çš„ä»£ç æ‰§è¡Œè¿›è¡Œåé¦ˆå’ŒéªŒè¯ï¼Œæ— æ³•æ•æ‰ç”Ÿæˆä»£ç çš„å®é™…è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†WebGen-Agentï¼Œä¸€ä¸ªåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆæ¥è¿­ä»£ç”Ÿæˆå’Œç»†åŒ–ç½‘ç«™ä»£ç åº“çš„æ–°å‹ç½‘ç«™ç”Ÿæˆä»£ç†ã€‚WebGen-Agenté€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆå…³äºç½‘ç«™æˆªå›¾å’ŒGUIä»£ç†æµ‹è¯•çš„è¯¦ç»†æ–‡æœ¬æè¿°å’Œå»ºè®®ï¼ŒåŒæ—¶é‡åŒ–å…¶è´¨é‡ã€‚é€šè¿‡é›†æˆæˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°ä»¥åŠå›æº¯å’Œé€‰æ‹©æœ€ä½³æœºåˆ¶ï¼Œå¢å¼ºäº†ä»£ç†çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åˆ©ç”¨WebGen-Agentå·¥ä½œæµç¨‹ä¸­çš„ç²¾ç¡®è§†è§‰åˆ†æ•°æ”¹è¿›çš„Step-GRPOæ–¹æ³•ï¼Œé€šè¿‡æ¯ä¸€æ­¥çš„æˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œä¸ºStep-GRPOæä¾›äº†å¯†é›†å¯é çš„è¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç½‘ç«™ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨WebGen-Benchæ•°æ®é›†ä¸Šï¼ŒWebGen-Agentæé«˜äº†Claude-3.5-Sonnetçš„å‡†ç¡®æ€§å’Œå¤–è§‚è¯„åˆ†ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„ä»£ç†ç³»ç»Ÿã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„Step-GRPOè®­ç»ƒæ–¹æ³•ä¹Ÿæé«˜äº†Qwen2.5-Coder-7B-Instructçš„å‡†ç¡®æ€§å’Œå¤–è§‚è¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç†ç³»ç»Ÿåœ¨ä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¾èµ–è§†è§‰æ•ˆæœå’Œç”¨æˆ·äº¤äº’åé¦ˆçš„ç½‘ç«™ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>WebGen-Agentåˆ©ç”¨å…¨é¢ã€å¤šå±‚æ¬¡çš„è§†è§‰åé¦ˆè¿­ä»£ç”Ÿæˆå’Œç»†åŒ–ç½‘ç«™ä»£ç åº“ï¼ŒåŒ…æ‹¬é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†æ–‡æœ¬æè¿°å’Œå»ºè®®ã€‚</li>
<li>WebGen-Agenté›†æˆäº†æˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°ï¼Œå¢å¼ºäº†æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†å›æº¯å’Œé€‰æ‹©æœ€ä½³æœºåˆ¶ã€‚</li>
<li>Step-GRPOæ–¹æ³•åˆ©ç”¨WebGen-Agentå·¥ä½œæµç¨‹ä¸­çš„ç²¾ç¡®è§†è§‰åˆ†æ•°ï¼Œé€šè¿‡æ¯ä¸€æ­¥çš„æˆªå›¾å’ŒGUIä»£ç†åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œæä¾›å¯†é›†å¯é çš„è¿‡ç¨‹ç›‘ç£ä¿¡å·ã€‚</li>
<li>WebGen-Agentåœ¨WebGen-Benchæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†ä»£ç†ç³»ç»Ÿçš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Claude-3.5-Sonnetçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>Step-GRPOè®­ç»ƒæ–¹æ³•ä¹Ÿæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¤–è§‚è¯„åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8fa6812c28048cedaf18a377f7020ce" align="middle">
<img src="https://picx.zhimg.com/v2-e8e7ee6f5aeec558da0188a66affc6f5" align="middle">
<img src="https://picx.zhimg.com/v2-ec6f005265afd95f4fdf4fbb8f5bf56c" align="middle">
<img src="https://picx.zhimg.com/v2-95a5a9d99dcbca69f8c2278e48bce3a1" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VLA-Reasoner-Empowering-Vision-Language-Action-Models-with-Reasoning-via-Online-Monte-Carlo-Tree-Search"><a href="#VLA-Reasoner-Empowering-Vision-Language-Action-Models-with-Reasoning-via-Online-Monte-Carlo-Tree-Search" class="headerlink" title="VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning   via Online Monte Carlo Tree Search"></a>VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning   via Online Monte Carlo Tree Search</h2><p><strong>Authors:Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang</strong></p>
<p>Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation. </p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAsï¼‰é€šè¿‡æ‰©å¤§æ¨¡ä»¿å­¦ä¹ ï¼Œåœ¨ä¸€èˆ¬çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLAsä»…é™äºé¢„æµ‹çŸ­è§†çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œå¯¹äºé•¿å‘¨æœŸè½¨è¿¹ä»»åŠ¡åˆ™ç”±äºå¢é‡åå·®è€Œé¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºVLA-Reasonerçš„æ’ä»¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°èµ‹äºˆç°æˆçš„VLAsé€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾é¢„æµ‹æœªæ¥çŠ¶æ€çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒVLA-Reasonerä¼šæŠ½æ ·å¹¶å±•å¼€å¯èƒ½çš„è¡ŒåŠ¨è½¨è¿¹ï¼Œå…¶ä¸­çš„è¡ŒåŠ¨æ˜¯ä½œä¸ºç†ç”±æ¥ç”Ÿæˆæœªæ¥çŠ¶æ€çš„ä¸–ç•Œæ¨¡å‹ï¼Œè¿™ä½¿å¾—VLA-Reasonerèƒ½å¤Ÿé¢„æµ‹å¹¶æ¨ç†æ½œåœ¨çš„ç»“æœå¹¶å¯»æ‰¾æœ€ä½³è¡ŒåŠ¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æé«˜åœ¨å¤§åŠ¨ä½œç©ºé—´ä¸­çš„æœç´¢æ•ˆç‡ï¼Œå…¶ä¸­é€æ­¥çš„VLAé¢„æµ‹ä½œä¸ºæ ¹ç§å­ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰çš„ä¿¡å¿ƒæŠ½æ ·æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨MCTSä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¢ç´¢ï¼Œæ— éœ€å†—ä½™çš„VLAæŸ¥è¯¢ã€‚æˆ‘ä»¬é€šè¿‡ç¦»çº¿å¥–åŠ±å¡‘å½¢ç­–ç•¥æ¥è¯„ä¼°MCTSä¸­çš„ä¸­é—´çŠ¶æ€ï¼Œä¸ºé¢„æµ‹çš„æœªæ¥æ‰“åˆ†å¹¶çº æ­£é•¿æœŸåé¦ˆä¸­çš„åå·®ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå™¨å’ŒçœŸå®ä¸–ç•Œä¸­éƒ½è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„VLA-Reasonerç›¸è¾ƒäºæœ€å…ˆè¿›çš„VLAså–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ­ç¤ºäº†å®ç°å¯æ‰©å±•çš„æµ‹è¯•æ—¶é—´è®¡ç®—çš„æœºå™¨äººæ“ä½œçš„æ½œåœ¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22643v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAsï¼‰ä¸­ï¼Œé€šè¿‡æ‰©å±•æ¨¡ä»¿å­¦ä¹ ï¼Œå¯ä»¥åœ¨ä¸€èˆ¬çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚ä½†ç°æœ‰çš„VLAsä»…å±€é™äºé¢„æµ‹çŸ­è§†çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œéš¾ä»¥åº”å¯¹é•¿è§†è§„åˆ’è½¨è¿¹çš„ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºVLA-Reasonerçš„æ’ä»¶æ¡†æ¶ï¼Œå®ƒé€šè¿‡æµ‹è¯•æ—¶æ‰©å±•çš„èƒ½åŠ›èµ‹äºˆç°æˆçš„VLAsé¢„è§æœªæ¥çŠ¶æ€çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸–ç•Œæ¨¡å‹ç”Ÿæˆæœªæ¥çŠ¶æ€ï¼Œæ¶‰åŠçš„åŠ¨ä½œä½œä¸ºè§£é‡Šè¿›è¡Œé‡‡æ ·å’Œå±•å¼€å¯èƒ½çš„è¡ŒåŠ¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æé«˜åœ¨å¤§åŠ¨ä½œç©ºé—´ä¸­çš„æœç´¢æ•ˆç‡ï¼Œä»¥é€æ­¥çš„VLAé¢„æµ‹ä½œä¸ºæ ¹èŠ‚ç‚¹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰çš„ä¿¡å¿ƒé‡‡æ ·æœºåˆ¶ï¼Œåœ¨MCTSä¸­æ— éœ€å†—ä½™çš„VLAæŸ¥è¯¢å³å¯å®ç°é«˜æ•ˆæ¢ç´¢ã€‚é€šè¿‡ç¦»çº¿å¥–åŠ±å¡‘å½¢ç­–ç•¥è¯„ä¼°MCTSçš„ä¸­é—´çŠ¶æ€ï¼Œä»¥è¯„åˆ†é¢„æµ‹çš„æœªæ¥å¹¶çº æ­£é•¿æœŸåé¦ˆä¸­çš„åå·®ã€‚åœ¨æ¨¡æ‹Ÿå™¨å’ŒçœŸå®ä¸–ç•Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„VLA-Reasonerç›¸è¾ƒäºæœ€å…ˆè¿›çš„VLAså–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæœºå™¨äººæ“ä½œçš„æµ‹è¯•æ—¶è®¡ç®—æä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„å¯è¡Œè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä»…é™äºçŸ­è§†é¢„æµ‹ã€‚</li>
<li>VLA-Reasoneræ’ä»¶æ¡†æ¶èµ‹èƒ½VLAsä»¥é¢„è§æœªæ¥çŠ¶æ€çš„èƒ½åŠ›ï¼Œé€šè¿‡ç”Ÿæˆæœªæ¥çŠ¶æ€å¹¶å±•å¼€å¯èƒ½çš„è¡ŒåŠ¨è½¨è¿¹æ¥è§£å†³çŸ­è§†é—®é¢˜ã€‚</li>
<li>VLA-Reasoneråˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æé«˜åœ¨å¤§åŠ¨ä½œç©ºé—´ä¸­çš„æœç´¢æ•ˆç‡ã€‚</li>
<li>åŸºäºæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰çš„ä¿¡å¿ƒé‡‡æ ·æœºåˆ¶æé«˜äº†æ¢ç´¢æ•ˆç‡ï¼Œæ— éœ€å†—ä½™çš„VLAæŸ¥è¯¢ã€‚</li>
<li>é€šè¿‡ç¦»çº¿å¥–åŠ±å¡‘å½¢ç­–ç•¥è¯„ä¼°ä¸­é—´çŠ¶æ€ï¼Œå®ç°å¯¹é¢„æµ‹æœªæ¥çš„è¯„åˆ†å’Œå¯¹é•¿æœŸåé¦ˆä¸­çš„åå·®çš„çº æ­£ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå™¨å’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒVLA-Reasonerç›¸è¾ƒäºæœ€å…ˆè¿›çš„VLAså–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15219d83569de8e1e90c04fd8d2b0e49" align="middle">
<img src="https://picx.zhimg.com/v2-05516f5f20537e2925fe470cceb2a40f" align="middle">
<img src="https://picx.zhimg.com/v2-3334f6a310ee26a4cff274b177686255" align="middle">
<img src="https://picx.zhimg.com/v2-2670332c0dbc66d94fc0de26045e3035" align="middle">
<img src="https://picx.zhimg.com/v2-1c0aea994c4e8cea57bf7ac1a43aa561" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Variational-Reasoning-for-Language-Models"><a href="#Variational-Reasoning-for-Language-Models" class="headerlink" title="Variational Reasoning for Language Models"></a>Variational Reasoning for Language Models</h2><p><strong>Authors:Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang</strong></p>
<p>We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/variational-reasoning">https://github.com/sail-sg/variational-reasoning</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºè¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ç§å˜åˆ†æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ€è€ƒç—•è¿¹è§†ä¸ºæ½œåœ¨å˜é‡ï¼Œå¹¶é€šè¿‡å˜åˆ†æ¨ç†å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä»è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰å‡ºå‘ï¼Œå°†å…¶æ‰©å±•ä¸ºå¤šè½¨è¿¹ç›®æ ‡ä»¥è·å¾—æ›´ç´§å¯†ç•Œé™ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ­£å‘KLå…¬å¼ï¼Œè¯¥å…¬å¼å¯ç¨³å®šå˜åˆ†åéªŒçš„è®­ç»ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ‹’ç»é‡‡æ ·å¾®è°ƒå’ŒäºŒå€¼å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬GRPOï¼Œå¯ä»¥è§£é‡Šä¸ºå±€éƒ¨æ­£å‘KLç›®æ ‡ï¼Œå…¶ä¸­æ¨¡å‹å‡†ç¡®æ€§çš„éšå¼åŠ æƒè‡ªç„¶äº§ç”Ÿäºæ¨å¯¼ä¸­ï¼Œå¹¶æ­ç¤ºäº†ä¹‹å‰æœªæ³¨æ„åˆ°çš„å¯¹ç®€å•é—®é¢˜çš„åå‘ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„æ¨ç†ä»»åŠ¡ä¸Šå¯¹Qwen 2.5å’ŒQwen 3æ¨¡å‹å®¶æ—ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„æ¦‚ç‡è§†è§’ï¼Œå°†å˜åˆ†æ¨ç†ä¸RLé£æ ¼çš„æ–¹æ³•ç»Ÿä¸€èµ·æ¥ï¼Œå¹¶ä¸ºæé«˜è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†ç¨³å®šçš„ç›®æ ‡ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/sail-sg/variational-reasoning%E3%80%82">https://github.com/sail-sg/variational-reasoningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22637v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ç”¨äºè¯­è¨€æ¨¡å‹çš„å˜åˆ†æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ€è€ƒç—•è¿¹è§†ä¸ºæ½œåœ¨å˜é‡ï¼Œå¹¶é€šè¿‡å˜åˆ†æ¨ç†è¿›è¡Œä¼˜åŒ–ã€‚ä»è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰å‡ºå‘ï¼Œæ‰©å±•åˆ°å¤šç—•è¿¹ç›®æ ‡ä»¥è·å–æ›´ç´§å¯†ç•Œé™ï¼Œå¹¶æå‡ºå‰å‘KLå…¬å¼ä»¥ç¨³å®šå˜åˆ†åéªŒçš„è®­ç»ƒã€‚æ–‡æœ¬è¿˜å±•ç¤ºäº†æ‹’ç»é‡‡æ ·å¾®è°ƒã€äºŒå…ƒå¥–åŠ±å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•å¯ä»¥è§£é‡Šä¸ºå±€éƒ¨å‰å‘KLç›®æ ‡ï¼Œå…¶ä¸­æ¨¡å‹ç²¾åº¦éšå¼åŠ æƒè‡ªç„¶äº§ç”Ÿäºæ¨å¯¼ä¸­ï¼Œæ­ç¤ºäº†å¯¹æ›´ç®€å•é—®é¢˜çš„åå‘ã€‚æ–‡æœ¬å¯¹Qwen 2.5å’ŒQwen 3æ¨¡å‹å®¶æ—è¿›è¡Œäº†å®è¯éªŒè¯ï¼Œæ¶‰åŠå¹¿æ³›çš„æ¨ç†ä»»åŠ¡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„æ¦‚ç‡è§†è§’ï¼Œå°†å˜åˆ†æ¨ç†ä¸å¼ºåŒ–å­¦ä¹ é£æ ¼çš„æ–¹æ³•ç»Ÿä¸€èµ·æ¥ï¼Œå¹¶ä¸ºæé«˜è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†ç¨³å®šçš„ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§å˜åˆ†æ¨ç†æ¡†æ¶ï¼Œå°†æ€è€ƒç—•è¿¹è§†ä¸ºæ½œåœ¨å˜é‡è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡æ‰©å±•è¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰åˆ°å¤šç—•è¿¹ç›®æ ‡ï¼Œè·å¾—æ›´ç´§å¯†ç•Œé™ã€‚</li>
<li>æå‡ºå‰å‘KLå…¬å¼ï¼Œç¨³å®šå˜åˆ†åéªŒçš„è®­ç»ƒã€‚</li>
<li>æ‹’ç»é‡‡æ ·å¾®è°ƒã€äºŒå…ƒå¥–åŠ±å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•å¯è§£é‡Šä¸ºå±€éƒ¨å‰å‘KLç›®æ ‡ã€‚</li>
<li>æ¨¡å‹ç²¾åº¦éšå¼åŠ æƒè‡ªç„¶äº§ç”Ÿäºæ¨å¯¼ä¸­ï¼Œæ­ç¤ºäº†å¯¹æ›´ç®€å•é—®é¢˜çš„åå‘ã€‚</li>
<li>åœ¨Qwen 2.5å’ŒQwen 3æ¨¡å‹å®¶æ—ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œæ¶‰åŠå¹¿æ³›çš„æ¨ç†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4456b1d7566cafa65f9d05587df7a78" align="middle">
<img src="https://picx.zhimg.com/v2-a4eff3b65907d6b3a4f16698c72cdf5c" align="middle">
<img src="https://picx.zhimg.com/v2-d5436c66055f7a90d1ec6f2a64e56ae7" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning"><a href="#UML-CoT-Structured-Reasoning-and-Planning-with-Unified-Modeling-Language-for-Robotic-Room-Cleaning" class="headerlink" title="UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning"></a>UML-CoT: Structured Reasoning and Planning with Unified Modeling   Language for Robotic Room Cleaning</h2><p><strong>Authors:Hongyu Chen, Guangrun Wang</strong></p>
<p>Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å¯¹æ— ç»“æ„æ–‡æœ¬çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨å®ä½“ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§å’Œå¯æ‰§è¡Œæ€§ã€‚æ—©æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†ä½¿ç”¨åœºæ™¯æˆ–é€»è¾‘å›¾çš„ç»“æ„åŒ–CoTï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶å­˜åœ¨æ ¹æœ¬æ€§çš„å±€é™ï¼šå®ƒä»¬åªå»ºæ¨¡ä½é˜¶å…³ç³»ï¼Œç¼ºä¹ç»§æ‰¿æˆ–è¡Œä¸ºæŠ½è±¡ç­‰æ„å»ºï¼Œå¹¶ä¸”ä¸æä¾›ç”¨äºé¡ºåºæˆ–æ¡ä»¶è§„åˆ’çš„æ ‡å‡†åŒ–è¯­ä¹‰ã€‚æˆ‘ä»¬æå‡ºäº†UML-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„åŒ–æ¨ç†å’Œè§„åˆ’æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰ç”Ÿæˆç¬¦å·åŒ–çš„CoTå’Œå¯æ‰§è¡Œçš„è¡ŒåŠ¨è®¡åˆ’ã€‚UMLç±»å›¾æ•è·ç»„åˆå¯¹è±¡è¯­ä¹‰ï¼Œè€Œæ´»åŠ¨å›¾å¯¹ç¨‹åºæ§åˆ¶æµè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒåŒ…æ‹¬ä»…ä»ç­”æ¡ˆæ•°æ®ä¸­å­¦ä¹ å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨MRoom-30kæ–°åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†UML-CoTï¼Œè¯¥æµ‹è¯•æ¨¡æ‹Ÿäº†æ‚ä¹±æˆ¿é—´æ¸…æ´åœºæ™¯ã€‚UML-CoTåœ¨æ— ç»“æ„åŒ–CoTçš„å¯è§£é‡Šæ€§ã€è§„åˆ’è¿è´¯æ€§å’Œæ‰§è¡ŒæˆåŠŸæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œçªæ˜¾äº†UMLä½œä¸ºä¸€ç§æ›´å…·è¡¨ç°åŠ›å’Œå¯æ“ä½œæ€§çš„ç»“æ„åŒ–æ¨ç†å½¢å¼ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22628v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰å»ºç«‹ç»“æ„åŒ–æ¨ç†å’Œè§„åˆ’æ¡†æ¶UML-CoTï¼Œé€šè¿‡ç”Ÿæˆç¬¦å·åŒ–é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œå¯æ‰§è¡Œè¡ŒåŠ¨è®¡åˆ’ï¼Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡UMLç±»å›¾æ•æ‰ç»„åˆå¯¹è±¡è¯­ä¹‰ï¼Œé€šè¿‡æ´»åŠ¨å›¾æ¨¡æ‹Ÿè¿‡ç¨‹æ§åˆ¶æµã€‚é‡‡ç”¨åŒ…æ‹¬å¥–åŠ±å­¦ä¹ åœ¨å†…çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œä»ä»…ç­”æ¡ˆæ•°æ®ä¸­ä¼˜åŒ–ç›¸å¯¹ç­–ç•¥ã€‚åœ¨å…¨æ–°çš„æˆ¿é—´æ¸…æ´åœºæ™¯MRoom-30kæµ‹è¯•ä¸­ï¼ŒUML-CoTåœ¨å¯è§£é‡Šæ€§ã€è§„åˆ’è¿è´¯æ€§å’Œæ‰§è¡ŒæˆåŠŸç‡æ–¹é¢ä¼˜äºéç»“æ„åŒ–CoTï¼Œå‡¸æ˜¾äº†UMLä½œä¸ºæ›´å…·è¡¨ç°åŠ›å’Œå¯æ“ä½œæ€§çš„ç»“æ„åŒ–æ¨ç†å½¢å¼çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>UML-CoTç»“åˆäº†ç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼ˆUMLï¼‰å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡UMLç±»å›¾å’Œæ´»åŠ¨å›¾ï¼ŒUML-CoTèƒ½å¤Ÿæ•æ‰ç»„åˆå¯¹è±¡è¯­ä¹‰å¹¶æ¨¡æ‹Ÿè¿‡ç¨‹æ§åˆ¶æµã€‚</li>
<li>è¯¥æ¡†æ¶æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒä»¥åŠä¸Group Relative Policy Optimization (GRPO)ç»“åˆçš„å¥–åŠ±å­¦ä¹ ã€‚</li>
<li>åœ¨æ–°çš„æˆ¿é—´æ¸…æ´åœºæ™¯MRoom-30kæµ‹è¯•ä¸­ï¼ŒUML-CoTè¡¨ç°å‡ºæ›´é«˜çš„å¯è§£é‡Šæ€§ã€è§„åˆ’è¿è´¯æ€§å’Œæ‰§è¡ŒæˆåŠŸç‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„éç»“æ„åŒ–CoTç›¸æ¯”ï¼ŒUML-CoTæä¾›äº†æ›´æ ‡å‡†åŒ–å’Œå¯æ‰§è¡Œçš„è¯­ä¹‰ç†è§£å’Œè¡ŒåŠ¨è®¡åˆ’ã€‚</li>
<li>UMLä½œä¸ºç»“æ„åŒ–æ¨ç†å½¢å¼ï¼Œå…·æœ‰æ›´é«˜çš„è¡¨è¾¾åŠ›å’Œå¯æ“ä½œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b318bffb25cf6b3afe9ffcd5c57ea57b" align="middle">
<img src="https://picx.zhimg.com/v2-303c7973c5eef7a1afdbe0c488e4e685" align="middle">
<img src="https://picx.zhimg.com/v2-9ead859e7b28e0c1ad096a9bf2b9376f" align="middle">
<img src="https://picx.zhimg.com/v2-75247ff8633f85737f3058ff1cfe2998" align="middle">
<img src="https://picx.zhimg.com/v2-ce93981cde70d784a95ba203177b57b3" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time"><a href="#Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time" class="headerlink" title="Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time"></a>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time</h2><p><strong>Authors:Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</strong></p>
<p>Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„é¢å¤–è®¡ç®—ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½è§†äº†æ¨¡å‹æ¶æ„çš„ä½œç”¨ã€‚åœ¨ä¸»æµæ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLMä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ”¹å˜æ¿€æ´»çš„ä¸“å®¶æ•°é‡ä¼šäº§ç”Ÿå…·æœ‰ç¨³å®šå‡†ç¡®ç‡çš„äº’è¡¥è§£å†³æ–¹æ¡ˆé›†ï¼Œæ­ç¤ºäº†ä¸€ç§æ–°çš„ä¸”å°šæœªè¢«å……åˆ†æ¢ç´¢çš„å¤šæ ·æ€§æ¥æºã€‚å—è¿™ä¸€è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ä¸“å®¶æ¿€æ´»æå‡ä¸ºæœç´¢ç©ºé—´çš„å¯æ§ç»´åº¦çš„TTSç­–ç•¥ã€‚DESé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨æ€MoEï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æˆæœ¬ï¼›ï¼ˆ2ï¼‰ä¸“å®¶é…ç½®ç»§æ‰¿ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨ç†è·¯å¾„ä¸­ä¿æŒä¸€è‡´çš„ä¸“å®¶æ•°é‡ï¼ŒåŒæ—¶åœ¨å„æ¬¡è¿è¡Œä¸­å˜åŒ–ä¸“å®¶æ•°é‡ï¼Œä»è€Œåœ¨æœç´¢è¿‡ç¨‹ä¸­å¹³è¡¡ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚åœ¨MoEæ¶æ„ã€éªŒè¯å™¨å’Œæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚æ•°å­¦ã€ä»£ç å’ŒçŸ¥è¯†ï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDESå¯é åœ°ä¼˜äºTTSåŸºçº¿ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–æˆæœ¬ã€‚è¿™äº›ç»“æœçªå‡ºäº†DESä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ¶æ„æ„ŸçŸ¥TTSå½¢å¼ï¼Œè¯´æ˜äº†ç°ä»£LLMä¸­çš„ç»“æ„çµæ´»æ€§å¦‚ä½•æ¨åŠ¨æ¨ç†çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22572v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰é€šè¿‡æ¨ç†æ—¶åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹æ¶æ„çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å°†ä¸“å®¶æ¿€æ´»æå‡ä¸ºæœç´¢ç©ºé—´å¯æ§ç»´åº¦çš„TTSç­–ç•¥ã€‚DESåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€MoEå’Œä¸“å®¶é…ç½®ç»§æ‰¿ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒä¸“å®¶æ•°é‡çš„ä¸€è‡´æ€§ï¼Œä»è€Œå¹³è¡¡ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDESåœ¨ä¸éœ€è¦é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹å¯é åœ°ä¼˜äºTTSåŸºçº¿ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚è¿™äº›ç»“æœçªæ˜¾äº†DESä½œä¸ºå®ç”¨ä¸”å¯æ‰©å±•çš„æ¶æ„æ„ŸçŸ¥TTSå½¢å¼çš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†ç°ä»£LLMä¸­ç»“æ„çµæ´»æ€§å¯¹æ¨ç†çš„æ¨åŠ¨ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¾“å‡ºçº§åˆ«çš„é‡‡æ ·ï¼Œè€Œå¿½è§†æ¨¡å‹æ¶æ„çš„å½±å“ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥åŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰ç­–ç•¥ï¼Œå°†ä¸“å®¶æ¿€æ´»ä½œä¸ºæœç´¢ç©ºé—´çš„å¯æ§ç»´åº¦ã€‚</li>
<li>DESåŒ…å«åŠ¨æ€MoEå’Œä¸“å®¶é…ç½®ç»§æ‰¿ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>åŠ¨æ€MoEå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œç”Ÿæˆå¤šæ ·åŒ–æ¨ç†è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æˆæœ¬ã€‚</li>
<li>ä¸“å®¶é…ç½®ç»§æ‰¿åœ¨æ¨ç†è·¯å¾„ä¸­ä¿æŒä¸“å®¶æ•°é‡çš„ä¸€è‡´æ€§ï¼Œå¹³è¡¡ç¨³å®šæ€§å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6eb411d6309d9fa6012c40fb742c8f6" align="middle">
<img src="https://picx.zhimg.com/v2-d84d1070476f742f5eca80724d899519" align="middle">
<img src="https://picx.zhimg.com/v2-df408d6615225b002a326d82d15e2cf8" align="middle">
<img src="https://picx.zhimg.com/v2-e2c75eb1fd550f071622b564bd8bc2a3" align="middle">
<img src="https://picx.zhimg.com/v2-61d527f5dfa72b4247df0be816b64da4" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models"><a href="#StepORLM-A-Self-Evolving-Framework-With-Generative-Process-Supervision-For-Operations-Research-Language-Models" class="headerlink" title="StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models"></a>StepORLM: A Self-Evolving Framework With Generative Process Supervision   For Operations Research Language Models</h2><p><strong>Authors:Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge</strong></p>
<p>Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿ç­¹å­¦ï¼ˆORï¼‰é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ æ˜¯è®­ç»ƒLLMè§£å†³ORé—®é¢˜çš„å¼ºå¤§èŒƒå¼ï¼Œä½†ç°æœ‰ç ”ç©¶é€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ã€‚é¦–å…ˆï¼Œç»“æœå¥–åŠ±å­˜åœ¨ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œæ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆå¯èƒ½ä¼šå¼ºåŒ–é”™è¯¯çš„æ¨ç†ã€‚å…¶æ¬¡ï¼Œä¼ ç»Ÿçš„åˆ¤åˆ«è¿‡ç¨‹ç›‘ç£æ˜¯çŸ­è§†çš„ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ORå»ºæ¨¡çš„ç›¸äº’ä¾èµ–æ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†StepORLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªè¿›åŒ–æ¡†æ¶ï¼Œå…·æœ‰ç”Ÿæˆè¿‡ç¨‹ç›‘ç£åŠŸèƒ½ã€‚StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼Œå…¶ä¸­ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆGenPRMï¼‰ç›¸äº’è¿­ä»£æ”¹è¿›ã€‚è¿™ä¸ªå¾ªç¯ç”±ä¸€ä¸ªåŒé‡åé¦ˆæœºåˆ¶é©±åŠ¨ï¼šæ¥è‡ªå¤–éƒ¨æ±‚è§£å™¨çš„åŸºäºç»“æœçš„ç¡®å®šæ€§éªŒè¯ï¼Œä»¥åŠæ¥è‡ªGenPRMçš„ç»†è‡´å…¨é¢è¿‡ç¨‹è¯„ä¼°ã€‚ç»“åˆè¿™ä¸¤ç§ä¿¡å·ï¼Œé€šè¿‡åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆW-DPOï¼‰æ¥è°ƒæ•´ç­–ç•¥ï¼ŒåŒæ—¶æ”¹è¿›GenPRMã€‚æˆ‘ä»¬å¾—åˆ°çš„8Bå‚æ•°StepORLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€ä¸»åŠ¨æ–¹æ³•å’Œä¸“ç”¨åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿå……å½“å¼ºå¤§è€Œé€šç”¨çš„è¿‡ç¨‹éªŒè¯å™¨ï¼Œå¤§å¤§æé«˜æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹å’Œå…¶ä»–ç°æœ‰LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿ç­¹å­¦ï¼ˆORï¼‰é—®é¢˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚å¼ºåŒ–å­¦ä¹ ä½œä¸ºLLMè®­ç»ƒçš„ä¸€ç§å¼ºå¤§èŒƒå¼åœ¨ORé—®é¢˜ä¸Šå­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºStepORLMï¼Œä¸€ç§æ–°å‹è‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œå…·æœ‰ç”Ÿæˆè¿‡ç¨‹ç›‘ç£åŠŸèƒ½ã€‚StepORLMæ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼Œç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ç›¸äº’è¿­ä»£æ”¹è¿›ã€‚è¯¥å¾ªç¯ç”±åŒé‡åé¦ˆæœºåˆ¶é©±åŠ¨ï¼šæ¥è‡ªå¤–éƒ¨æ±‚è§£å™¨çš„ç¡®å®šæ€§ã€ç»“æœä¸ºåŸºç¡€çš„éªŒè¯ï¼Œä»¥åŠæ¥è‡ªGenPRMçš„å¾®å¦™ã€æ•´ä½“è¿‡ç¨‹è¯„ä¼°ã€‚ç»“åˆä¿¡å·ç”¨äºé€šè¿‡åŠ æƒç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆW-DPOï¼‰å¯¹é½ç­–ç•¥ï¼ŒåŒæ—¶æ”¹è¿›GenPRMã€‚æˆ‘ä»¬çš„8Bå‚æ•°StepORLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€è‡ªä¸»æ–¹æ³•å’Œä¸“ç”¨åŸºå‡†ã€‚æ­¤å¤–ï¼ŒååŒè¿›åŒ–çš„GenPRMèƒ½å¤Ÿå……å½“å¼ºå¤§è€Œé€šç”¨çš„è¿‡ç¨‹éªŒè¯å™¨ï¼Œæ˜¾è‘—æé«˜äº†æˆ‘ä»¬æ¨¡å‹å’Œå…¶ä»–ç°æœ‰LLMçš„æ¨ç†æ‰©å±•æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿ç­¹å­¦ï¼ˆORï¼‰é—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯è®­ç»ƒLLMè§£å†³ORé—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šç»“æœå¥–åŠ±ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜å’Œä¼ ç»Ÿåˆ¤åˆ«è¿‡ç¨‹ç›‘ç£çš„è¿‘è§†æ€§ã€‚</li>
<li>StepORLMæ¡†æ¶é€šè¿‡å¼•å…¥ç”Ÿæˆè¿‡ç¨‹ç›‘ç£æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå…·æœ‰è‡ªæˆ‘è¿›åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>StepORLMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªååŒè¿›åŒ–å¾ªç¯ï¼ŒåŒ…æ‹¬ç­–ç•¥æ¨¡å‹å’Œç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„è¿­ä»£æ”¹è¿›ã€‚</li>
<li>åŒé‡åé¦ˆæœºåˆ¶åŒ…æ‹¬å¤–éƒ¨æ±‚è§£å™¨çš„ç»“æœéªŒè¯å’ŒGenPRMçš„è¿‡ç¨‹è¯„ä¼°ã€‚</li>
<li>æå‡ºçš„8Bå‚æ•°StepORLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¼˜äºå¤§å‹é€šç”¨æ¨¡å‹ã€è‡ªä¸»æ–¹æ³•å’Œä¸“ç”¨åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ff1cf82a7b19fb113fa0d984420e2f5" align="middle">
<img src="https://picx.zhimg.com/v2-8add0cc7742216d8afd873e354d9b2e3" align="middle">
<img src="https://picx.zhimg.com/v2-7cf587ac5352e33f1b09fe83a5ac0ca4" align="middle">
<img src="https://picx.zhimg.com/v2-657b4744fa6f9fa202a866dd3961ac81" align="middle">
<img src="https://picx.zhimg.com/v2-fc56ec5d52c475253e45cae32ac32a53" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="REMA-A-Unified-Reasoning-Manifold-Framework-for-Interpreting-Large-Language-Model"><a href="#REMA-A-Unified-Reasoning-Manifold-Framework-for-Interpreting-Large-Language-Model" class="headerlink" title="REMA: A Unified Reasoning Manifold Framework for Interpreting Large   Language Model"></a>REMA: A Unified Reasoning Manifold Framework for Interpreting Large   Language Model</h2><p><strong>Authors:Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua Zhao, Linqi Song, Lijie Wen</strong></p>
<p>Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the modelâ€™s layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models. </p>
<blockquote>
<p>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•è¿›è¡Œå¤æ‚æ¨ç†åŠå…¶å¤±è´¥æœºåˆ¶æ˜¯è§£é‡Šæ€§ç ”ç©¶ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†æä¾›ä¸€ä¸ªå¯è¡¡é‡çš„å‡ ä½•åˆ†æè§†è§’ï¼Œæˆ‘ä»¬å®šä¹‰äº†â€œæ¨ç†æµå½¢â€è¿™ä¸€æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸æ‰€æœ‰æ­£ç¡®æ¨ç†ç”Ÿæˆç›¸å¯¹åº”çš„æ½œåœ¨ä½ç»´å‡ ä½•ç»“æ„å½¢æˆçš„å†…éƒ¨è¡¨ç¤ºã€‚è¿™ä¸€ç»“æ„å¯ä»¥æ¦‚å¿µåŒ–ä¸ºæ¨¡å‹å­¦ä¹ æˆåŠŸè§£å†³ç»™å®šä»»åŠ¡çš„æœ‰æ•ˆæ€è€ƒè·¯å¾„çš„ä½“ç°ã€‚åŸºäºæ­¤æ¦‚å¿µï¼Œæˆ‘ä»¬æ„å»ºäº†REMAæ¡†æ¶ï¼Œé€šè¿‡å®šé‡æ¯”è¾ƒé”™è¯¯å’Œæ­£ç¡®æ¨ç†æ ·æœ¬å¯¹åº”çš„å†…éƒ¨æ¨¡å‹è¡¨ç¤ºçš„ç©ºé—´å…³ç³»æ¥è§£é‡Šå¤±è´¥çš„åŸå› ã€‚å…·ä½“æ¥è¯´ï¼ŒREMAé¦–å…ˆé€šè¿‡è®¡ç®—æ¯ä¸ªé”™è¯¯è¡¨ç¤ºä¸å…¶é‚»è¿‘çš„æ­£ç¡®è¡¨ç¤ºå½¢æˆçš„è¿‘ä¼¼æµå½¢çš„kæœ€è¿‘é‚»è·ç¦»æ¥é‡åŒ–å…¶å‡ ä½•åå·®ï¼Œä»è€Œæä¾›ä¸€ä¸ªç»Ÿä¸€çš„å¤±è´¥ä¿¡å·ã€‚ç„¶åï¼Œå®ƒé€šè¿‡è·Ÿè¸ªæ¨¡å‹å„å±‚ä¸­çš„åå·®åº¦é‡å¹¶ä¸æ¥è‡ªæ­£ç¡®è¡¨ç¤ºçš„å†…éƒ¨æ³¢åŠ¨çš„åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œå®šä½è¿™äº›åå·®é¦–æ¬¡å˜å¾—æ˜¾è‘—çš„å‘æ•£ç‚¹ï¼Œä»è€Œç¡®å®šæ¨ç†é“¾å¼€å§‹å‡ºé”™çš„åœ°æ–¹ã€‚æˆ‘ä»¬åœ¨å¤šç§è¯­è¨€å’Œè·¨æ¨¡æ€æ¨¡å‹å’Œä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ¨ç†æµå½¢çš„ä½ç»´æ€§è´¨ä»¥åŠé”™è¯¯å’Œæ­£ç¡®æ¨ç†è¡¨ç¤ºä¹‹é—´çš„é«˜å¯åˆ†æ€§ã€‚ç»“æœä¹ŸéªŒè¯äº†REMAæ¡†æ¶åœ¨åˆ†ææ¨ç†å¤±è´¥åŸå› æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹ç ”ç©¶å°†æŠ½è±¡çš„æ¨ç†å¤±è´¥ä¸å¯è¡¡é‡çš„è¡¨ç¤ºå‡ ä½•åå·®è”ç³»èµ·æ¥ï¼Œä¸ºæ·±å…¥ç†è§£å’Œè¯Šæ–­é»‘ç®±æ¨¡å‹çš„å†…éƒ¨è®¡ç®—è¿‡ç¨‹æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22518v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤æ‚æ¨ç†åŠå…¶å¤±è´¥æœºåˆ¶çš„å¯è¡¡é‡å‡ ä½•åˆ†æè§†è§’ã€‚é€šè¿‡å¼•å…¥â€œæ¨ç†æµå½¢â€çš„æ¦‚å¿µï¼Œå³æ‰€æœ‰æ­£ç¡®æ¨ç†ç”Ÿæˆå¯¹åº”çš„æ½œåœ¨ä½ç»´å‡ ä½•ç»“æ„ï¼Œä¸ºæœ‰æ•ˆæ€è€ƒè·¯å¾„çš„ä½“ç°ï¼Œè¿›è€Œæ„å»ºREMAæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å®šé‡æ¯”è¾ƒé”™è¯¯å’Œæ­£ç¡®æ¨ç†æ ·æœ¬çš„æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„ç©ºé—´å…³ç³»ï¼Œæ­ç¤ºå¤±è´¥çš„èµ·æºã€‚é€šè¿‡è®¡ç®—æ¯ä¸ªé”™è¯¯è¡¨ç¤ºä¸ç”±æ­£ç¡®è¡¨ç¤ºå½¢æˆçš„è¿‘ä¼¼æµå½¢çš„kè¿‘é‚»è·ç¦»ï¼ŒREMAæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å¤±è´¥ä¿¡å·ã€‚ç„¶åï¼Œé€šè¿‡è·Ÿè¸ªè¿™ç§åå·®åº¦é‡è·¨è¶Šæ¨¡å‹å±‚å¹¶ä¸æ­£ç¡®è¡¨ç¤ºçš„åŸºçº¿å†…éƒ¨æ³¢åŠ¨è¿›è¡Œæ¯”è¾ƒï¼Œå®šä½åå·®é¦–æ¬¡å˜å¾—æ˜¾è‘—çš„å‘æ•£ç‚¹ã€‚åœ¨å¤šç§è¯­è¨€å’Œè·¨æ¨¡æ€æ¨¡å‹åŠä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†æ¨ç†æµå½¢çš„ä½ç»´æ€§è´¨ä»¥åŠé”™è¯¯å’Œæ­£ç¡®æ¨ç†è¡¨ç¤ºä¹‹é—´çš„é«˜å¯åˆ†ç¦»æ€§ã€‚ç»“æœä¹ŸéªŒè¯äº†REMAæ¡†æ¶åœ¨åˆ†ææ¨ç†å¤±è´¥åŸå› æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å°†æŠ½è±¡çš„æ¨ç†å¤±è´¥ä¸å¯è¡¡é‡çš„å‡ ä½•åå·®è”ç³»èµ·æ¥ï¼Œä¸ºæ·±å…¥äº†è§£å’Œç†è§£é»‘ç®±æ¨¡å‹çš„å†…éƒ¨è®¡ç®—è¿‡ç¨‹æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥â€œæ¨ç†æµå½¢â€æ¦‚å¿µï¼Œå®šä¹‰ä¸ºLLMå†…éƒ¨è¡¨ç¤ºå½¢æˆçš„ä½ç»´å‡ ä½•ç»“æ„ï¼Œåæ˜ æ¨¡å‹æˆåŠŸè§£å†³ç»™å®šä»»åŠ¡çš„æœ‰æ•ˆæ€è€ƒè·¯å¾„ã€‚</li>
<li>æ„å»ºREMAæ¡†æ¶ï¼Œé€šè¿‡æ¯”è¾ƒé”™è¯¯å’Œæ­£ç¡®æ¨ç†æ ·æœ¬çš„æ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„ç©ºé—´å…³ç³»ï¼Œå®šé‡è§£æå¤±è´¥çš„èµ·æºã€‚</li>
<li>é€šè¿‡è®¡ç®—é”™è¯¯è¡¨ç¤ºä¸æ­£ç¡®è¡¨ç¤ºæµå½¢ä¹‹é—´çš„è·ç¦»ï¼Œæä¾›ç»Ÿä¸€çš„å¤±è´¥ä¿¡å·ã€‚</li>
<li>å®šä½åå·®é¦–æ¬¡å˜å¾—æ˜¾è‘—çš„å‘æ•£ç‚¹ï¼Œé€šè¿‡è·Ÿè¸ªåå·®åº¦é‡åœ¨æ¨¡å‹å„å±‚çš„å˜åŠ¨å¹¶ä¸åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>å®éªŒè¯æ˜æ¨ç†æµå½¢çš„ä½ç»´æ€§è´¨ä»¥åŠé”™è¯¯å’Œæ­£ç¡®æ¨ç†è¡¨ç¤ºä¹‹é—´çš„é«˜å¯åˆ†ç¦»æ€§ã€‚</li>
<li>REMAæ¡†æ¶åœ¨åˆ†ææ¨ç†å¤±è´¥åŸå› æ–¹é¢æœ‰æ•ˆæ€§å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49a82ff01ea94ff7f6a9719c174538ac" align="middle">
<img src="https://picx.zhimg.com/v2-a6902b01e6cb7b68efc8678a56d6a448" align="middle">
<img src="https://picx.zhimg.com/v2-e7fb21ae2d2c18c31456948da8cb7ded" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Estimating-the-Empowerment-of-Language-Model-Agents"><a href="#Estimating-the-Empowerment-of-Language-Model-Agents" class="headerlink" title="Estimating the Empowerment of Language Model Agents"></a>Estimating the Empowerment of Language Model Agents</h2><p><strong>Authors:Jinyeop Song, Jeff Gore, Max Kleiman-Weiner</strong></p>
<p>As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agentâ€™s actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings. </p>
<blockquote>
<p>éšç€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä»£ç†å˜å¾—æ›´åŠ å¼ºå¤§ï¼Œå¹¶è·å¾—äº†æ›´å¹¿æ³›è®¿é—®çœŸå®ä¸–ç•Œå·¥å…·çš„èƒ½åŠ›ï¼Œå¯¹ä»£ç†èƒ½åŠ›çš„å¯è¯„ä¼°è¯„ä»·æ¡†æ¶çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œä»¥åŸºå‡†æµ‹è¯•ä¸ºä¸­å¿ƒçš„ä¼ ç»Ÿè¯„ä¼°åœ¨è®¾è®¡ä¸Šæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦äººç±»è®¾è®¡å¸ˆæå‡ºæœ‰æ•ˆçš„ä»»åŠ¡ï¼Œä»¥æ´å¯Ÿé€šç”¨æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåŸºäºèµ‹èƒ½çš„ä¿¡æ¯ç†è®ºè¯„ä¼°ï¼Œå³ä»£ç†è¡ŒåŠ¨ä¸æœªæ¥çŠ¶æ€ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä½œä¸ºä¸€ç§è¯„ä¼°è¯­è¨€æ¨¡å‹ä»£ç†çš„å¼€æ”¾æ–¹æ³•ã€‚æˆ‘ä»¬ä»‹ç»äº†EELMAï¼ˆè¯­è¨€æ¨¡å‹ä»£ç†çš„èµ‹èƒ½ä¼°è®¡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¤šè½®æ–‡æœ¬äº¤äº’æ¥è¿‘ä¼¼æœ‰æ•ˆèµ‹èƒ½çš„ç®—æ³•ã€‚æˆ‘ä»¬åœ¨è¯­è¨€æ¸¸æˆå’Œè§„æ¨¡æ›´å¤§çš„çœŸå®ç½‘é¡µæµè§ˆåœºæ™¯ä¸­éªŒè¯äº†EELMAã€‚æˆ‘ä»¬å‘ç°èµ‹èƒ½ä¸å¹³å‡ä»»åŠ¡æ€§èƒ½æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œå¹¶æè¿°äº†ç¯å¢ƒå¤æ‚æ€§å’Œä»£ç†å› ç´ ï¼ˆå¦‚æ€ç»´é“¾ã€æ¨¡å‹è§„æ¨¡å’Œè®°å¿†é•¿åº¦ï¼‰å¯¹ä¼°è®¡èµ‹èƒ½çš„å½±å“ï¼Œä»¥åŠé«˜èµ‹èƒ½çŠ¶æ€å’Œè¡ŒåŠ¨é€šå¸¸æ˜¯é€šç”¨èƒ½åŠ›çš„é‡è¦æ—¶åˆ»ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœè¯æ˜äº†èµ‹èƒ½ä½œä¸ºè¯„ä¼°å’Œç›‘æ§å¤æ‚å¼€æ”¾å¼è®¾ç½®ä¸­è¯­è¨€æ¨¡å‹ä»£ç†çš„é€šç”¨æŒ‡æ ‡çš„å¸å¼•åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22504v1">PDF</a> 10 pages, 8 figures. Submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºèµ‹èƒ½çš„ä¿¡æ¯ç†è®ºè¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä»£ç†çš„èƒ½åŠ›ã€‚æ–‡ç« ä»‹ç»äº†EELMAç®—æ³•ï¼Œå¯é€šè¿‡å¤šè½®æ–‡æœ¬äº¤äº’æ¥ä¼°ç®—ä»£ç†çš„æœ‰æ•ˆèµ‹èƒ½ã€‚é€šè¿‡è¯­è¨€æ¸¸æˆå’Œè§„æ¨¡åŒ–çš„çœŸå®ç½‘é¡µæµè§ˆåœºæ™¯çš„éªŒè¯ï¼Œå‘ç°èµ‹èƒ½ä¸å¹³å‡ä»»åŠ¡æ€§èƒ½å¯†åˆ‡ç›¸å…³ã€‚æ–‡ç« è¿˜æ¢è®¨äº†ç¯å¢ƒå¤æ‚æ€§å’Œä»£ç†å› ç´ ï¼ˆå¦‚æ€ç»´é“¾ã€æ¨¡å‹è§„æ¨¡å’Œè®°å¿†é•¿åº¦ï¼‰å¯¹ä¼°ç®—èµ‹èƒ½çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºé«˜èµ‹èƒ½çŠ¶æ€å’Œè¡ŒåŠ¨é€šå¸¸æ˜¯ä»£ç†é€šç”¨èƒ½åŠ›çš„é‡è¦æ—¶åˆ»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºèµ‹èƒ½çš„ä¿¡æ¯ç†è®ºè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä»£ç†çš„èƒ½åŠ›ã€‚</li>
<li>ä»‹ç»äº†EELMAç®—æ³•ï¼Œé€šè¿‡å¤šè½®æ–‡æœ¬äº¤äº’ä¼°ç®—ä»£ç†çš„æœ‰æ•ˆèµ‹èƒ½ã€‚</li>
<li>éªŒè¯EELMAåœ¨è¯­è¨€æ¸¸æˆå’ŒçœŸå®ç½‘é¡µæµè§ˆåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å‘ç°èµ‹èƒ½ä¸å¹³å‡ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚</li>
<li>æ¢è®¨äº†ç¯å¢ƒå¤æ‚æ€§å’Œä»£ç†å› ç´ å¯¹ä¼°ç®—èµ‹èƒ½çš„å½±å“ã€‚</li>
<li>é«˜èµ‹èƒ½çŠ¶æ€å’Œè¡ŒåŠ¨å¯¹ä»£ç†çš„é€šç”¨èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8cab989de96a2b6854c5645db433b03" align="middle">
<img src="https://picx.zhimg.com/v2-65b7ff7882f7cd3b4c73811c85f22eb0" align="middle">
<img src="https://picx.zhimg.com/v2-7ede11379322c974d0efc59134f2181d" align="middle">
<img src="https://picx.zhimg.com/v2-e218b710b482e121d9794d18759cb3c5" align="middle">
<img src="https://picx.zhimg.com/v2-d44d01cf90354dfa911424ade4fb1bec" align="middle">
<img src="https://picx.zhimg.com/v2-13660dcd361c796be371052ac221bc7a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Limits-of-Large-Language-Models-in-Multilingual-Legal-Reasoning"><a href="#Evaluating-the-Limits-of-Large-Language-Models-in-Multilingual-Legal-Reasoning" class="headerlink" title="Evaluating the Limits of Large Language Models in Multilingual Legal   Reasoning"></a>Evaluating the Limits of Large Language Models in Multilingual Legal   Reasoning</h2><p><strong>Authors:Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve GÃ¼rel</strong></p>
<p>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Metaâ€™s LLaMA, OpenAIâ€™s ChatGPT, Googleâ€™s Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications. </p>
<blockquote>
<p>åœ¨è¿™ä¸ªä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä¸»å¯¼çš„æ—¶ä»£ï¼Œäº†è§£å…¶èƒ½åŠ›å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸï¼Œè‡³å…³é‡è¦ã€‚è™½ç„¶è¯¸å¦‚Metaçš„LLaMAã€OpenAIçš„ChatGPTã€Googleçš„Geminiã€DeepSeekç­‰æ–°å…´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¢«è¶Šæ¥è¶Šå¤šåœ°èå…¥æ³•å¾‹å·¥ä½œæµç¨‹ä¸­ï¼Œä½†å®ƒä»¬åœ¨å¤šè¯­ç§ã€ç®¡è¾–èŒƒå›´å¹¿æ³›ä»¥åŠå¯¹ç«‹æƒ…å¢ƒä¸‹çš„è¡¨ç°ä»ç„¶æœªå¾—åˆ°è¶³å¤Ÿçš„æ¢ç´¢ã€‚æœ¬å·¥ä½œé’ˆå¯¹LLaMAå’ŒGeminiè¿›è¡Œäº†å¤šè¯­è¨€æ³•å¾‹å’Œéæ³•å¾‹åŸºå‡†çš„æµ‹è¯•è¯„ä¼°ï¼Œå¹¶é€šè¿‡å­—ç¬¦å’Œå•è¯çº§åˆ«çš„æ‰°åŠ¨è¯„ä¼°äº†å®ƒä»¬åœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„å¯¹æŠ—ç¨³å¥æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•æ¥è¿›è¡Œç¬¦åˆäººç±»è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºä¸€ä¸ªå¼€æºçš„æ¨¡å—åŒ–è¯„ä¼°ç®¡é“ï¼Œæ—¨åœ¨æ”¯æŒä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†çš„å¤šè¯­è¨€ã€ä»»åŠ¡å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«ä¾§é‡äºæ³•å¾‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ‘˜è¦ã€å¼€æ”¾é—®é¢˜å’Œä¸€èˆ¬æ¨ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ³•å¾‹ä»»åŠ¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œåœ¨LEXamç­‰æ³•å¾‹æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡å¾€å¾€ä½äº50%ï¼Œè€Œåœ¨XNLIç­‰é€šç”¨ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡70%ã€‚æ­¤å¤–ï¼Œè™½ç„¶è‹±è¯­é€šå¸¸ä¼šäº§ç”Ÿæ›´ç¨³å®šçš„ç»“æœï¼Œä½†å¹¶ä¸æ€»æ˜¯å¯¼è‡´æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æç¤ºæ•æ„Ÿæ€§å’Œå¯¹æŠ—æ€§è„†å¼±æ€§ä¹Ÿæ˜¾ç¤ºä¼šåœ¨å„ç§è¯­è¨€ä¸­æŒç»­å­˜åœ¨ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ä¸€ç§è¯­è¨€ä¸è‹±è¯­çš„å¥æ³•ç›¸ä¼¼æ€§ä¸å…¶è¡¨ç°ä¹‹é—´å­˜åœ¨å…³è”ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°LLaMAæ¯”Geminiè¡¨ç°è¾ƒå¼±ï¼Œåè€…åœ¨åŒä¸€ä»»åŠ¡ä¸Šå¹³å‡ä¼˜åŠ¿çº¦ä¸º24ä¸ªç™¾åˆ†ç‚¹ã€‚å°½ç®¡æ–°ä¸€ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨å…³é”®çš„å¤šè¯­è¨€æ³•å¾‹åº”ç”¨ä¸­å¯é éƒ¨ç½²å®ƒä»¬ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22472v1">PDF</a> 39 pages, 36 figures. Code and evaluation pipeline available at   <a target="_blank" rel="noopener" href="https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs">https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶åœ¨å¤šè¯­è¨€ã€å¸æ³•ç®¡è¾–å¤šæ ·ä»¥åŠå¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„è¡¨ç°å°šå¾…å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¯¹LLaMAå’ŒGeminiè¿›è¡Œäº†å¤šè¯­è¨€æ³•å¾‹å’Œéæ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡å­—ç¬¦å’Œå•è¯çº§åˆ«çš„æ‰°åŠ¨è¯„ä¼°äº†å®ƒä»¬åœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ³•å¾‹ä»»åŠ¡å¯¹LLMsæ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œå‡†ç¡®ç‡å¸¸ä½äº50%ï¼Œä¸”åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹å­˜åœ¨æç¤ºæ•æ„Ÿæ€§å’Œå¯¹æŠ—æ€§è„†å¼±æ€§é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨æ„ˆå‘é‡è¦ï¼Œä½†å…¶åœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„è¡¨ç°å°šå¾…æ·±å…¥ç ”ç©¶ã€‚</li>
<li>LLMsåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å¯¹æŠ—æ€§æƒ…å¢ƒï¼Œä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>LLaMAå’ŒGeminiåœ¨å¤šå…ƒæ³•å¾‹åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼ŒGeminiç›¸å¯¹æ›´ä¼˜è¶Šã€‚</li>
<li>æ³•å¾‹ä»»åŠ¡çš„å¤æ‚æ€§ä½¿å¾—LLMsçš„å‡†ç¡®ç‡å¸¸ä½äº50%ï¼Œå°¤å…¶æ˜¯æ³•å¾‹æ¨ç†ä»»åŠ¡ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè‹±è¯­å¹¶ä¸æ€»æ˜¯å¯¼è‡´æ›´é«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>LLMså¯¹æç¤ºçš„æ•æ„Ÿæ€§å’Œå¯¹æŠ—æ€§è„†å¼±æ€§åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹æŒç»­å­˜åœ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2eb79dd91f33416fc2c2b6e2a73f2c7f" align="middle">
<img src="https://picx.zhimg.com/v2-dd2d645efbd0a3f357ba1319a9b1047d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MDAR-A-Multi-scene-Dynamic-Audio-Reasoning-Benchmark"><a href="#MDAR-A-Multi-scene-Dynamic-Audio-Reasoning-Benchmark" class="headerlink" title="MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark"></a>MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</h2><p><strong>Authors:Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at <a target="_blank" rel="noopener" href="https://github.com/luckyerr/MDAR">https://github.com/luckyerr/MDAR</a>. </p>
<blockquote>
<p>ä»éŸ³é¢‘ä¸­æ¨ç†çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€å‰¯è¯­è¨€çº¿ç´¢ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ï¼Œå¯¹äºäººå·¥æ™ºèƒ½ä»£ç†åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æœ‰æ•ˆåœ°äº¤äº’è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€æˆ–å•ä¸€åœºæ™¯è®¾ç½®ï¼Œå¹¶æ²¡æœ‰å®Œå…¨æ•æ‰å¤šè¯´è¯è€…ã€å±•å¼€çš„äº‹ä»¶å’Œå¼‚è´¨éŸ³é¢‘æºäº¤äº’çš„åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MDARï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤æ‚ã€å¤šåœºæ™¯å’ŒåŠ¨æ€æ¼”åŒ–éŸ³é¢‘æ¨ç†ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚MDARåŒ…å«3000ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä¸å„ç§éŸ³é¢‘å‰ªè¾‘ç›¸å…³è”ï¼Œæ¶µç›–äº”ç±»å¤æ‚æ¨ç†ï¼Œæ¶‰åŠä¸‰ç§é—®é¢˜ç±»å‹ã€‚æˆ‘ä»¬åœ¨MDARä¸Šè¯„ä¼°äº†26ä¸ªæœ€å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿåˆ°å®ƒä»¬åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ã€‚åœ¨å•é€‰é¢˜æ–¹é¢ï¼ŒQwen2.5-Omniï¼ˆå¼€æºï¼‰çš„å‡†ç¡®ç‡ä¸º76.67%ï¼Œè€ŒGPT-4o Audioï¼ˆé—­æºï¼‰çš„å‡†ç¡®ç‡ä¸º68.47%ï¼›ç„¶è€Œï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å¤šé€‰å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸Šï¼ŒGPT-4o Audioæ˜¾è‘—ä¼˜äºQwen2.5-Omniã€‚åœ¨æ‰€æœ‰ä¸‰ç§é—®é¢˜ç±»å‹ä¸­ï¼Œæ²¡æœ‰ä»»ä½•æ¨¡å‹çš„æ€§èƒ½è¾¾åˆ°80%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†MDARæ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ä»¥åŠå…¶ä½œä¸ºæ¨è¿›éŸ³é¢‘æ¨ç†ç ”ç©¶åŸºå‡†æµ‹è¯•çš„ä»·å€¼ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/luckyerr/MDAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luckyerr/MDARæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22461v1">PDF</a> 25 pages, 7 figures</p>
<p><strong>Summary</strong><br>    æ–‡ç« å¼ºè°ƒäº†åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ï¼ŒAIéœ€è¦ä»éŸ³é¢‘ä¸­è¿›è¡Œæ¨ç†çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€å‰¯è¯­è¨€çº¿ç´¢ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é™æ€æˆ–å•ä¸€åœºæ™¯è®¾ç½®ä¸Šï¼Œæ— æ³•å……åˆ†æ•æ‰å¤æ‚ã€å¤šåœºæ™¯å’ŒåŠ¨æ€æ¼”å˜çš„éŸ³é¢‘æ¨ç†ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥MDARåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº”å¤§ç±»å¤æ‚æ¨ç†å’Œä¸‰ç§é—®é¢˜ç±»å‹ã€‚å¯¹ç°æœ‰å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å‘ç°ï¼Œå®ƒä»¬åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„éŸ³é¢‘æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ä»è¯­éŸ³ã€å‰¯è¯­è¨€çº¿ç´¢ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ä¸­æ¨ç†ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†æ•æ‰å¤æ‚ã€å¤šåœºæ™¯å’ŒåŠ¨æ€æ¼”å˜çš„éŸ³é¢‘æ¨ç†ä»»åŠ¡ã€‚</li>
<li>MDARåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚ã€å¤šåœºæ™¯å’ŒåŠ¨æ€éŸ³é¢‘æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>MDARåŒ…å«ä¸å„ç§éŸ³é¢‘å‰ªè¾‘ç›¸å…³çš„3000ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>åœ¨MDARåŸºå‡†æµ‹è¯•ä¸­ï¼Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå±€é™æ€§ã€‚</li>
<li>åœ¨å•é¡¹é€‰æ‹©é¢˜æ–¹é¢ï¼ŒQwen2.5-Omniè¡¨ç°è¾ƒå¥½ï¼Œè€ŒGPT-4o Audioåœ¨æŸäº›æ›´å…·æŒ‘æˆ˜æ€§çš„å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9feddee44798a516a028fee1c8d8c5e" align="middle">
<img src="https://picx.zhimg.com/v2-cc924bd899e89ae40a2c299574255009" align="middle">
<img src="https://picx.zhimg.com/v2-07081a99ce87a70868e137283191d684" align="middle">
<img src="https://picx.zhimg.com/v2-774d06abe2dc3a5e8784ce988d605400" align="middle">
<img src="https://picx.zhimg.com/v2-1e58d1a7896b82ec97fad6afb343bc2a" align="middle">
<img src="https://picx.zhimg.com/v2-4ca8feb11e6b1dad3f523a792e6de51e" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TreeMind-Automatically-Reproducing-Android-Bug-Reports-via-LLM-empowered-Monte-Carlo-Tree-Search"><a href="#TreeMind-Automatically-Reproducing-Android-Bug-Reports-via-LLM-empowered-Monte-Carlo-Tree-Search" class="headerlink" title="TreeMind: Automatically Reproducing Android Bug Reports via   LLM-empowered Monte Carlo Tree Search"></a>TreeMind: Automatically Reproducing Android Bug Reports via   LLM-empowered Monte Carlo Tree Search</h2><p><strong>Authors:Zhengyu Chen, Zhaoyi Meng, Wenxiang Zhao, Wansen Wang, Haoyang Zhao, Jiahao Zhan, Jie Cui, Hong Zhong</strong></p>
<p>Automatically reproducing Android app crashes from textual bug reports is challenging, particularly when the reports are incomplete and the modern UI exhibits high combinatorial complexity. Existing approaches based on reinforcement learning or large language models (LLMs) exhibit limitations in such scenarios. They struggle to infer unobserved steps and reconstruct the underlying user action sequences to navigate the vast UI interaction space, primarily due to limited goal-directed reasoning and planning. We present TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug reproduction. To the best of our knowledge, this is the first work to combine external decision-making with LLM semantic reasoning for reliable bug reproduction. We formulate the reproduction task as a target-driven search problem, leveraging MCTS as the core planning mechanism to iteratively refine action sequences. To enhance MCTS with semantic reasoning, we introduce two LLM-guided agents with distinct roles: Expander generates top-k promising actions based on the current UI state and exploration history, while Simulator estimates the likelihood that each action leads toward successful reproduction. By incorporating multi-modal UI inputs and advanced prompting techniques, TreeMind conducts feedback-aware navigation that identifies missing but essential user actions and incrementally reconstructs the reproduction paths. We evaluate TreeMind on a dataset of 93 real-world Android bug reports from three widely-used benchmarks. Experimental results show that it significantly outperforms four state-of-the-art baselines in reproduction success rate. A real-world case study indicates that integrating LLM reasoning with MCTS-based planning is a compelling direction for automated bug reproduction. </p>
<blockquote>
<p>è‡ªåŠ¨æ ¹æ®æ–‡æœ¬æ•…éšœæŠ¥å‘Šé‡ç°å®‰å“åº”ç”¨å´©æºƒæ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå°¤å…¶æ˜¯åœ¨æŠ¥å‘Šä¸å®Œæ•´ä¸”ç°ä»£ç”¨æˆ·ç•Œé¢å…·æœ‰é«˜ç»„åˆå¤æ‚æ€§çš„æƒ…å†µä¸‹ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰æ–¹æ³•åœ¨æ­¤ç±»åœºæ™¯ä¸­è¡¨ç°å‡ºå±€é™æ€§ã€‚ä»–ä»¬å¾ˆéš¾æ¨æ–­æœªè§‚å¯Ÿåˆ°çš„æ­¥éª¤å¹¶é‡å»ºæ½œåœ¨çš„ç”¨æˆ·æ“ä½œåºåˆ—ï¼Œä»¥å¯¼èˆªå¹¿æ³›çš„UIäº¤äº’ç©ºé—´ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ç›®æ ‡å¯¼å‘çš„æ¨ç†å’Œè§„åˆ’ã€‚æˆ‘ä»¬æå‡ºäº†TreeMindï¼Œè¿™æ˜¯ä¸€ç§å°†LLMä¸å®šåˆ¶è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç®—æ³•ç›¸ç»“åˆçš„æ–°æŠ€æœ¯ï¼Œä»¥å®ç°æˆ˜ç•¥æ€§çš„UIæ¢ç´¢ï¼Œç”¨äºæ•…éšœé‡ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¤–éƒ¨å†³ç­–åˆ¶å®šä¸LLMè¯­ä¹‰æ¨ç†ç›¸ç»“åˆçš„å·¥ä½œï¼Œä»¥å®ç°å¯é çš„æ•…éšœé‡ç°ã€‚æˆ‘ä»¬å°†é‡ç°ä»»åŠ¡åˆ¶å®šä¸ºç›®æ ‡é©±åŠ¨æœç´¢é—®é¢˜ï¼Œåˆ©ç”¨MCTSä½œä¸ºæ ¸å¿ƒè§„åˆ’æœºåˆ¶æ¥è¿­ä»£ä¼˜åŒ–åŠ¨ä½œåºåˆ—ã€‚ä¸ºäº†é€šè¿‡è¯­ä¹‰æ¨ç†å¢å¼ºMCTSï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªç”±LLMå¼•å¯¼çš„ä¸»ä½“ï¼Œåˆ†åˆ«æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼šæ‰©å±•å™¨æ ¹æ®å½“å‰UIçŠ¶æ€å’Œæ¢ç´¢å†å²ç”Ÿæˆå‰kä¸ªæœ‰å‰æ™¯çš„åŠ¨ä½œï¼Œè€Œæ¨¡æ‹Ÿå™¨ä¼°è®¡æ¯ä¸ªåŠ¨ä½œå¯¼è‡´æˆåŠŸé‡ç°çš„å¯èƒ½æ€§ã€‚é€šè¿‡ç»“åˆå¤šæ¨¡å¼UIè¾“å…¥å’Œå…ˆè¿›çš„æç¤ºæŠ€æœ¯ï¼ŒTreeMindè¿›è¡Œäº†åé¦ˆæ„ŸçŸ¥å¯¼èˆªï¼Œå¯è¯†åˆ«ç¼ºå¤±ä½†è‡³å…³é‡è¦çš„ç”¨æˆ·æ“ä½œï¼Œå¹¶é€æ­¥é‡å»ºé‡ç°è·¯å¾„ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ªä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•çš„93ä¸ªçœŸå®ä¸–ç•Œå®‰å“æ•…éšœæŠ¥å‘Šçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†TreeMindã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é‡ç°æˆåŠŸç‡æ–¹é¢ï¼Œå®ƒæ˜¾è‘—ä¼˜äºå››ç§æœ€æ–°æŠ€æœ¯åŸºçº¿ã€‚ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œå°†LLMæ¨ç†ä¸åŸºäºMCTSçš„è§„åˆ’ç›¸ç»“åˆæ˜¯è‡ªåŠ¨åŒ–æ•…éšœé‡ç°çš„ä¸€ä¸ªä»¤äººä¿¡æœçš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22431v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–åœ°ä»æ–‡æœ¬bugæŠ¥å‘Šä¸­é‡ç°Androidåº”ç”¨å´©æºƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ¥å‘Šä¸å®Œæ•´ä¸”ç°ä»£UIå…·æœ‰é«˜åº¦çš„ç»„åˆå¤æ‚æ€§æ—¶ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚å®ƒä»¬éš¾ä»¥æ¨æ–­æœªè§‚å¯Ÿåˆ°çš„æ­¥éª¤å¹¶é‡å»ºæ½œåœ¨çš„ç”¨æˆ·æ“ä½œåºåˆ—æ¥å¯¼èˆªå¹¿é˜”çš„UIäº¤äº’ç©ºé—´ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹ç›®æ ‡å¯¼å‘çš„æ¨ç†å’Œè§„åˆ’ã€‚æœ¬ç ”ç©¶æå‡ºäº†TreeMindæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†LLMsä¸å®šåˆ¶çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç®—æ³•ç›¸ç»“åˆï¼Œä»¥å®ç°æˆ˜ç•¥æ€§çš„UIæ¢ç´¢ç”¨äºbugé‡ç°ã€‚è¿™æ˜¯é¦–æ¬¡å°†å¤–éƒ¨å†³ç­–ä¸LLMè¯­ä¹‰æ¨ç†ç›¸ç»“åˆçš„å·¥ä½œï¼Œä»¥å®ç°å¯é çš„bugé‡ç°ã€‚æˆ‘ä»¬å°†é‡ç°ä»»åŠ¡åˆ¶å®šä¸ºç›®æ ‡é©±åŠ¨æœç´¢é—®é¢˜ï¼Œåˆ©ç”¨MCTSä½œä¸ºæ ¸å¿ƒè§„åˆ’æœºåˆ¶æ¥è¿­ä»£åœ°å®Œå–„åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡å¼•å…¥ä¸¤ä¸ªç”±LLMå¼•å¯¼çš„agentï¼Œä»¥å¢å¼ºMCTSçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼šExpanderæ ¹æ®å½“å‰UIçŠ¶æ€å’Œæ¢ç´¢å†å²ç”Ÿæˆæœ‰å‰æ™¯çš„kä¸ªåŠ¨ä½œï¼Œè€ŒSimulatorä¼°è®¡æ¯ä¸ªåŠ¨ä½œå¯¼è‡´æˆåŠŸé‡ç°çš„å¯èƒ½æ€§ã€‚é€šè¿‡ç»“åˆå¤šæ¨¡å¼UIè¾“å…¥å’Œå…ˆè¿›çš„æç¤ºæŠ€æœ¯ï¼ŒTreeMindè¿›è¡Œåé¦ˆæ„ŸçŸ¥å¯¼èˆªï¼Œç¡®å®šç¼ºå¤±ä½†å…³é”®çš„ç”¨æˆ·æ“ä½œå¹¶é€æ­¥é‡å»ºé‡ç°è·¯å¾„ã€‚åœ¨åŒ…å«æ¥è‡ªä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•çš„93ä¸ªçœŸå®ä¸–ç•ŒAndroid bugæŠ¥å‘Šçš„æ•°æ®é›†ä¸Šè¯„ä¼°TreeMindã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨é‡ç°æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºå››ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚ä¸€é¡¹çœŸå®æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œå°†LLMæ¨ç†ä¸MCTS-basedè§„åˆ’ç›¸ç»“åˆæ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œç”¨äºè‡ªåŠ¨åŒ–bugé‡ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–é‡ç°Androidåº”ç”¨å´©æºƒä»æ–‡æœ¬bugæŠ¥å‘Šä¸­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸å®Œæ•´æŠ¥å‘Šå’Œç°ä»£UIçš„é«˜ç»„åˆå¤æ‚æ€§æ—¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨æ¨æ–­æœªè§‚å¯Ÿåˆ°çš„æ­¥éª¤å’Œé‡å»ºç”¨æˆ·æ“ä½œåºåˆ—æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>TreeMindæŠ€æœ¯ç»“åˆäº†LLMså’ŒMCTSç®—æ³•ï¼Œä»¥æˆ˜ç•¥æ€§åœ°æ¢ç´¢UIç”¨äºbugé‡ç°ã€‚</li>
<li>TreeMindé€šè¿‡å¼•å…¥ä¸¤ä¸ªLLMå¼•å¯¼çš„agentï¼ˆExpanderå’ŒSimulatorï¼‰æ¥å¢å¼ºMCTSçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TreeMindé€šè¿‡ç»“åˆå¤šæ¨¡å¼UIè¾“å…¥å’Œå…ˆè¿›çš„æç¤ºæŠ€æœ¯ï¼Œè¿›è¡Œåé¦ˆæ„ŸçŸ¥å¯¼èˆªå¹¶é‡å»ºbugé‡ç°è·¯å¾„ã€‚</li>
<li>åœ¨åŒ…å«çœŸå®ä¸–ç•ŒAndroid bugæŠ¥å‘Šçš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTreeMindåœ¨é‡ç°æˆåŠŸç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2ff6b07f648afbe4a6310dd53ea6008" align="middle">
<img src="https://picx.zhimg.com/v2-c4765264368197327fbd8fa2d1473273" align="middle">
<img src="https://picx.zhimg.com/v2-9bb49b96385e1fe0f6367de339b879c1" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoveFM-R-Advancing-Mobility-Foundation-Models-via-Language-driven-Semantic-Reasoning"><a href="#MoveFM-R-Advancing-Mobility-Foundation-Models-via-Language-driven-Semantic-Reasoning" class="headerlink" title="MoveFM-R: Advancing Mobility Foundation Models via Language-driven   Semantic Reasoning"></a>MoveFM-R: Advancing Mobility Foundation Models via Language-driven   Semantic Reasoning</h2><p><strong>Authors:Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li</strong></p>
<p>Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLMâ€™s reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MoveFM-R-CDE7/">https://anonymous.4open.science/r/MoveFM-R-CDE7/</a>. </p>
<blockquote>
<p>ç§»åŠ¨åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰å·²ç»æ¨è¿›äº†äººç±»è¿åŠ¨æ¨¡å¼çš„å»ºæ¨¡ï¼Œä½†ç”±äºæ•°æ®è§„æ¨¡å’Œè¯­ä¹‰ç†è§£çš„é™åˆ¶ï¼Œå®ƒä»¬é¢ä¸´ä¸Šé™ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†å¼ºå¤§çš„è¯­ä¹‰æ¨ç†ï¼Œä½†å®ƒä»¬ç¼ºä¹ç”Ÿæˆç‰©ç†ä¸Šå¯è¡Œçš„ç§»åŠ¨è½¨è¿¹æ‰€éœ€çš„æ—¶ç©ºç»Ÿè®¡çš„å›ºæœ‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MoveFM-Rï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è¯­è¨€é©±åŠ¨çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›è§£é”ç§»åŠ¨åŸºç¡€æ¨¡å‹æ½œåŠ›çš„æ–°å‹æ¡†æ¶ã€‚å®ƒè§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè¿ç»­åœ°ç†åæ ‡å’Œç¦»æ•£è¯­è¨€ä»¤ç‰Œä¹‹é—´çš„è¯æ±‡ä¸åŒ¹é…ï¼Œä»¥åŠMFMsçš„æ½œåœ¨å‘é‡å’ŒLLMsçš„è¯­ä¹‰ä¸–ç•Œä¹‹é—´çš„è¡¨ç¤ºå·®è·ã€‚MoveFM-Rå»ºç«‹åœ¨ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸Šï¼šä¸€ç§è¯­ä¹‰å¢å¼ºçš„ä½ç½®ç¼–ç ï¼Œä»¥å¼¥åˆåœ°ç†ä¸è¯­è¨€çš„å·®è·ï¼›ä¸€ç§æ¸è¿›å¼è¯¾ç¨‹ï¼Œä»¥ä½¿LLMçš„æ¨ç†ä¸ç§»åŠ¨æ¨¡å¼å¯¹é½ï¼›ä»¥åŠä¸€ç§ç”¨äºæ¡ä»¶è½¨è¿¹ç”Ÿæˆäº¤äº’å¼è‡ªæˆ‘åæ€æœºåˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoveFM-Ræ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºMFMå’ŒLLMçš„åŸºçº¿ã€‚å®ƒè¿˜åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ“…é•¿æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé€¼çœŸçš„è½¨è¿¹ã€‚é€šè¿‡åˆæˆMFMçš„ç»Ÿè®¡èƒ½åŠ›ä¸LLMçš„æ·±åº¦è¯­ä¹‰ç†è§£ï¼ŒMoveFM-Rå¼€åˆ›äº†ä¸€ç§æ–°èŒƒå¼ï¼Œä½¿äººç±»ç§»åŠ¨æ€§çš„å»ºæ¨¡æ›´åŠ å…¨é¢ã€å¯è§£é‡Šå’Œå¼ºå¤§ã€‚MoveFM-Rçš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MoveFM-R-CDE7/%E5%9C%A8%E7%BA%BF%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/r/MoveFM-R-CDE7/åœ¨çº¿è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Mobility Foundation Modelsï¼ˆMFMsï¼‰åœ¨å»ºæ¨¡äººç±»ç§»åŠ¨æ¨¡å¼æ–¹é¢çš„è¿›å±•åŠå…¶é¢ä¸´çš„æ•°æ®è§„æ¨¡å’Œè¯­ä¹‰ç†è§£ä¸Šçš„å±€é™æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶MoveFM-Rï¼Œå®ƒé€šè¿‡åˆ©ç”¨è¯­è¨€é©±åŠ¨çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œè§£é”äº†ç§»åŠ¨åŸºç¡€æ¨¡å‹çš„å…¨æ½œåŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†è¿ç»­åœ°ç†åæ ‡å’Œç¦»æ•£è¯­è¨€ä»¤ç‰Œä¹‹é—´è¯æ±‡ä¸åŒ¹é…ä»¥åŠMFMsçš„æ½œåœ¨å‘é‡å’ŒLLMsçš„è¯­ä¹‰ä¸–ç•Œä¹‹é—´è¡¨ç¤ºå·®è·ç­‰ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚MoveFM-Rå»ºç«‹åœ¨ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸Šï¼šè¯­ä¹‰å¢å¼ºçš„ä½ç½®ç¼–ç ä»¥å¼¥è¡¥åœ°ç†ä¸è¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œæ¸è¿›å¼è¯¾ç¨‹ä»¥ä½¿LLMçš„æ¨ç†ä¸ç§»åŠ¨æ¨¡å¼å¯¹é½ï¼Œä»¥åŠç”¨äºæ¡ä»¶è½¨è¿¹ç”Ÿæˆäº¤äº’å¼è‡ªæˆ‘åæ€æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒMoveFM-Ræ˜¾è‘—ä¼˜äºç°æœ‰çš„MFMå’ŒLLMåŸºçº¿ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ“…é•¿æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé€¼çœŸçš„è½¨è¿¹ã€‚å®ƒé€šè¿‡åˆæˆMFMsçš„ç»Ÿè®¡èƒ½åŠ›ä¸LLMsçš„æ·±åº¦è¯­ä¹‰ç†è§£ï¼Œå¼€åˆ›äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œä½¿äººç±»ç§»åŠ¨æ€§çš„å»ºæ¨¡æ›´åŠ å…¨é¢ã€å¯è§£é‡Šå’Œå¼ºå¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MFMsåœ¨å»ºæ¨¡äººç±»ç§»åŠ¨æ¨¡å¼æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†é¢ä¸´æ•°æ®è§„æ¨¡å’Œè¯­ä¹‰ç†è§£çš„å±€é™æ€§ã€‚</li>
<li>MoveFM-Ræ¡†æ¶æ—¨åœ¨è§£é”MFMsçš„æ½œåŠ›ï¼Œç»“åˆè¯­è¨€é©±åŠ¨çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MoveFM-Rè§£å†³ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šåœ°ç†åæ ‡ä¸è¯­è¨€ä»¤ç‰Œä¹‹é—´çš„è¯æ±‡ä¸åŒ¹é…ä»¥åŠMFMsä¸LLMsä¹‹é—´çš„è¡¨ç¤ºå·®è·ã€‚</li>
<li>MoveFM-Rå»ºç«‹åœ¨ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ä¸Šï¼šè¯­ä¹‰å¢å¼ºçš„ä½ç½®ç¼–ç ã€æ¸è¿›å¼è¯¾ç¨‹å¯¹é½å’Œäº¤äº’å¼è‡ªæˆ‘åæ€æœºåˆ¶ã€‚</li>
<li>MoveFM-Ræ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”ŸæˆçœŸå®è½¨è¿¹ã€‚</li>
<li>MoveFM-RåˆæˆMFMsçš„ç»Ÿè®¡èƒ½åŠ›ä¸LLMsçš„è¯­ä¹‰ç†è§£ï¼Œä¸ºå»ºæ¨¡äººç±»ç§»åŠ¨æ€§å¼€åˆ›äº†æ–°èŒƒå¼ã€‚</li>
<li>MoveFM-Ræ¡†æ¶çš„å®ç°å¯åœ¨ç½‘ä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6d0183df4484cf41b098bf16cace855" align="middle">
<img src="https://picx.zhimg.com/v2-beadbfb32dc52bbcabb15e9a9aff94c2" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CircuitSense-A-Hierarchical-Circuit-System-Benchmark-Bridging-Visual-Comprehension-and-Symbolic-Reasoning-in-Engineering-Design-Process"><a href="#CircuitSense-A-Hierarchical-Circuit-System-Benchmark-Bridging-Visual-Comprehension-and-Symbolic-Reasoning-in-Engineering-Design-Process" class="headerlink" title="CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual   Comprehension and Symbolic Reasoning in Engineering Design Process"></a>CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual   Comprehension and Symbolic Reasoning in Engineering Design Process</h2><p><strong>Authors:Arman Akbari, Jian Gao, Yifei Zou, Mei Yang, Jinru Duan, Dmitrii Torbunov, Yanzhi Wang, Yihui Ren, Xuan Zhang</strong></p>
<p>Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence. </p>
<blockquote>
<p>å·¥ç¨‹è®¾è®¡é€šè¿‡å±‚æ¬¡æŠ½è±¡ä»ç³»ç»Ÿè§„æ ¼åˆ°ç»„ä»¶å®ç°è¿›è¡Œæ“ä½œï¼Œéœ€è¦åœ¨æ¯ä¸ªå±‚æ¬¡ä¸Šç»“åˆè§†è§‰ç†è§£å’Œæ•°å­¦æ¨ç†ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶å›¾åƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨ä»æŠ€æœ¯å›¾è¡¨ä¸­æå–æ•°å­¦æ¨¡å‹çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†CircuitSenseï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡8006å¤šä¸ªé—®é¢˜è¯„ä¼°ä»ç»„ä»¶çº§åŸç†å›¾åˆ°ç³»ç»Ÿçº§æ¡†å›¾çš„ç”µè·¯ç†è§£å±‚æ¬¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç‹¬ç‰¹åœ°è€ƒå¯Ÿäº†å®Œæ•´çš„å·¥ç¨‹å·¥ä½œæµç¨‹ï¼šæ„ŸçŸ¥ã€åˆ†æå’Œè®¾è®¡ï¼Œç‰¹åˆ«æ˜¯ä»è§†è§‰è¾“å…¥ä¸­æ¨å¯¼ç¬¦å·æ–¹ç¨‹è¿™ä¸€å…³é”®ä½†å°šæœªå……åˆ†æ¢ç´¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†å±‚çš„åˆæˆç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŸºäºç½‘æ ¼çš„åŸç†å›¾ç”Ÿæˆå™¨å’Œä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨æ´¾ç”Ÿç¬¦å·æ–¹ç¨‹æ ‡ç­¾çš„æ¡†å›¾ç”Ÿæˆå™¨ã€‚å¯¹å…­ä¸ªæœ€æ–°MLLMsçš„ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬å°é—­æºä»£ç å’Œå¼€æ”¾æºä»£ç æ¨¡å‹ï¼Œæ­ç¤ºäº†è§†è§‰åˆ°æ•°å­¦æ¨ç†çš„æ ¹æœ¬å±€é™æ€§ã€‚å°é—­æºä»£ç çš„æ¨¡å‹åœ¨æ¶‰åŠç»„ä»¶è¯†åˆ«å’Œæ‹“æ‰‘è¯†åˆ«çš„æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡85%ï¼Œä½†åœ¨ç¬¦å·æ¨å¯¼å’Œåˆ†ææ¨ç†æ–¹é¢çš„æ€§èƒ½ä½äº19%ï¼Œæš´éœ²äº†è§†è§‰è§£æå’Œç¬¦å·æ¨ç†ä¹‹é—´çš„å…³é”®å·®è·ã€‚å…·æœ‰æ›´å¼ºç¬¦å·æ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨è®¾è®¡ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§æŒç»­æ›´é«˜ï¼Œè¯å®äº†æ•°å­¦ç†è§£åœ¨ç”µè·¯åˆæˆä¸­çš„åŸºç¡€ä½œç”¨ï¼Œå¹¶ç¡®ç«‹äº†ç¬¦å·æ¨ç†ä½œä¸ºå·¥ç¨‹èƒ½åŠ›çš„ä¸»è¦æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22339v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å·¥ç¨‹è®¾è®¡çš„å±‚æ¬¡æŠ½è±¡éœ€è¦ä»ç³»ç»Ÿè§„æ ¼åˆ°ç»„ä»¶å®ç°ï¼Œæ¯ä¸ªå±‚æ¬¡éƒ½éœ€è¦è§†è§‰ç†è§£ä¸æ•°å­¦æ¨ç†çš„ç»“åˆã€‚å½“å‰ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»æŠ€æœ¯å›¾è¡¨ä¸­æå–æ•°å­¦æ¨¡å‹çš„èƒ½åŠ›å°šæœªè¢«æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†CircuitSenseç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•é€šè¿‡è¶…è¿‡8006ä¸ªæ¶µç›–ä»ç»„ä»¶çº§åŸç†å›¾åˆ°ç³»ç»Ÿçº§æ¡†å›¾çš„é—®é¢˜ï¼Œè¯„ä¼°ç”µè·¯ç†è§£çš„å±‚æ¬¡ç»“æ„ã€‚è¯¥åŸºå‡†æµ‹è¯•ç‹¬ç‰¹åœ°è€ƒå¯Ÿäº†å®Œæ•´çš„å·¥ç¨‹å·¥ä½œæµç¨‹ï¼šæ„ŸçŸ¥ã€åˆ†æå’Œè®¾è®¡ï¼Œç‰¹åˆ«æ˜¯ä»è§†è§‰è¾“å…¥ä¸­æ¨å¯¼ç¬¦å·æ–¹ç¨‹è¿™ä¸€é‡è¦ä½†è¢«å¿½è§†çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå±‚æ¬¡æ€§çš„åˆæˆç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬åŸºäºç½‘æ ¼çš„åŸç†å›¾ç”Ÿæˆå™¨å’Œå¸¦æœ‰è‡ªåŠ¨ç”Ÿæˆçš„ç¬¦å·æ–¹ç¨‹æ ‡ç­¾çš„æ¡†å›¾ç”Ÿæˆå™¨ã€‚å¯¹å…­ä¸ªæœ€å…ˆè¿›çš„MLLMsçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè§†è§‰åˆ°æ•°å­¦çš„æ¨ç†å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚å°é—­æºæ¨¡å‹åœ¨æ¶‰åŠç»„ä»¶è¯†åˆ«å’Œæ‹“æ‰‘è¯†åˆ«çš„æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡85%ï¼Œä½†åœ¨ç¬¦å·æ¨å¯¼å’Œåˆ†ææ¨ç†æ–¹é¢çš„æ€§èƒ½ä½äº19%ï¼Œæš´éœ²äº†è§†è§‰è§£æå’Œç¬¦å·æ¨ç†ä¹‹é—´çš„å…³é”®å·®è·ã€‚å…·æœ‰æ›´å¼ºç¬¦å·æ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨è®¾è®¡ä»»åŠ¡ä¸Šå§‹ç»ˆå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¯å®äº†æ•°å­¦ç†è§£åœ¨å·¥ç¨‹åˆæˆä¸­çš„åŸºç¡€ä½œç”¨ï¼Œå¹¶ç¡®ç«‹äº†ç¬¦å·æ¨ç†ä½œä¸ºå·¥ç¨‹èƒ½åŠ›çš„å…³é”®æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å·¥ç¨‹è®¾è®¡æ¶‰åŠä»ç³»ç»Ÿè§„æ ¼åˆ°ç»„ä»¶å®ç°çš„å±‚æ¬¡æŠ½è±¡ï¼Œéœ€è¦è§†è§‰ç†è§£ä¸æ•°å­¦æ¨ç†çš„ç»“åˆã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”µè·¯ç†è§£çš„è§†è§‰ä¸æ•°å­¦ç»“åˆæ–¹é¢èƒ½åŠ›æœ‰é™ã€‚</li>
<li>CircuitSenseåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°ç”µè·¯ç†è§£çš„å±‚æ¬¡ç»“æ„ï¼Œæ¶µç›–ä»åŸç†å›¾åˆ°ç³»ç»Ÿæ¡†å›¾çš„é—®é¢˜ã€‚</li>
<li>åŸºå‡†æµ‹è¯•å¼ºè°ƒå®Œæ•´çš„å·¥ç¨‹å·¥ä½œæµç¨‹ï¼Œç‰¹åˆ«æ˜¯ä»è§†è§‰è¾“å…¥ä¸­æ¨å¯¼ç¬¦å·æ–¹ç¨‹çš„èƒ½åŠ›ã€‚</li>
<li>å°é—­æºæ¨¡å‹åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç¬¦å·æ¨å¯¼å’Œåˆ†ææ¨ç†ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™ã€‚</li>
<li>æ¨¡å‹åœ¨ç¬¦å·æ¨ç†æ–¹é¢çš„èƒ½åŠ›å¯¹äºå·¥ç¨‹è®¾è®¡ä»»åŠ¡çš„æˆåŠŸè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7a3d8b775386d6dc15fc3f740789c69" align="middle">
<img src="https://picx.zhimg.com/v2-2de8ee66d9d8c6f9f1ab64c10e8c8c22" align="middle">
<img src="https://picx.zhimg.com/v2-51b8007e35ebbb9175027d20f401912c" align="middle">
<img src="https://picx.zhimg.com/v2-56a577ae049a6d0cee821aadd19c57ae" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RAPID-3-Tri-Level-Reinforced-Acceleration-Policies-for-Diffusion-Transformer"><a href="#RAPID-3-Tri-Level-Reinforced-Acceleration-Policies-for-Diffusion-Transformer" class="headerlink" title="RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer"></a>RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer</h2><p><strong>Authors:Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You</strong></p>
<p>Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original modelâ€™s distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality. </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTsï¼‰åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å—åˆ°é‡‡æ ·é€Ÿåº¦è¾ƒæ…¢çš„é˜»ç¢ã€‚ç°æœ‰çš„æ— è®­ç»ƒåŠ é€Ÿå™¨â€”â€”æ­¥éª¤å‡å°‘ã€ç‰¹å¾ç¼“å­˜å’Œç¨€ç–æ³¨æ„åŠ›â€”â€”æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç»Ÿä¸€çš„å¯å‘å¼æ–¹æ³•æˆ–æ‰‹åŠ¨è®¾è®¡çš„é€‚ç”¨äºæ‰€æœ‰å›¾åƒçš„è‡ªé€‚åº”ç­–ç•¥ï¼Œä»è€Œç•™ä¸‹äº†è´¨é‡æå‡ç©ºé—´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŠ¨æ€ç¥ç»ç½‘ç»œæä¾›äº†æ¯å›¾åƒè‡ªé€‚åº”åŠ é€Ÿï¼Œä½†å…¶é«˜æ˜‚çš„å¾®è°ƒæˆæœ¬é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†RAPID3ï¼šæ‰©æ•£Transformerçš„ä¸‰çº§å¼ºåŒ–åŠ é€Ÿç­–ç•¥ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹åŸºç¡€ç”Ÿæˆå™¨çš„é›¶æ›´æ–°è¿›è¡Œå›¾åƒçº§åŠ é€Ÿã€‚å…·ä½“æ¥è¯´ï¼Œä¸‰ä¸ªè½»é‡çº§çš„ç­–ç•¥å¤´â€”â€”æ­¥éª¤è·³è¿‡ã€ç¼“å­˜é‡ç”¨å’Œç¨€ç–æ³¨æ„åŠ›â€”â€”è§‚å¯Ÿå½“å‰çš„å»å™ªçŠ¶æ€ï¼Œå¹¶ç‹¬ç«‹å†³å®šæ¯ä¸ªæ—¶é—´æ­¥çš„ç›¸åº”åŠ é€Ÿã€‚æ‰€æœ‰ç­–ç•¥å‚æ•°éƒ½é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨çº¿è®­ç»ƒï¼Œè€Œç”Ÿæˆå™¨ä¿æŒä¸å˜ã€‚åŒæ—¶ï¼Œå¯¹æŠ—æ€§å­¦ä¹ é‰´åˆ«å™¨å¢å¼ºäº†å¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡ä»…åœ¨ç”Ÿæˆæ ·æœ¬æ¥è¿‘åŸå§‹æ¨¡å‹åˆ†å¸ƒæ—¶æé«˜å›æŠ¥æ¥æŠ‘åˆ¶å¥–åŠ±æ“çºµã€‚åœ¨åŒ…æ‹¬Stable Diffusion 3å’ŒFLUXåœ¨å†…çš„æœ€æ–°DiTä¸»å¹²ç½‘ç»œä¸Šï¼ŒRAPID3å®ç°äº†è¿‘3å€çš„å¿«é€Ÿé‡‡æ ·ï¼ŒåŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„ç”Ÿæˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Diffusion Transformersï¼ˆDiTsï¼‰çš„åŠ é€Ÿæ¡†æ¶RAPID3ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸æ›´æ–°åŸºç¡€ç”Ÿæˆå™¨çš„æƒ…å†µä¸‹å®ç°å›¾åƒçº§åˆ«çš„åŠ é€Ÿã€‚RAPID3åŒ…æ‹¬ä¸‰ä¸ªç‹¬ç«‹çš„ç­–ç•¥å¤´ï¼šStep-Skipã€Cache-Reuseå’ŒSparse-Attentionï¼Œå®ƒä»¬æ ¹æ®å½“å‰çš„å»å™ªçŠ¶æ€åœ¨æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹å†³å®šåŠ é€Ÿã€‚é€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨çº¿è®­ç»ƒç­–ç•¥å‚æ•°ï¼ŒåŒæ—¶é‡‡ç”¨å¯¹æŠ—æ€§å­¦ä¹ çš„æ–¹æ³•å¢å¼ºå¥–åŠ±ä¿¡å·ï¼Œä»¥ä¿æŒç”Ÿæˆæ ·æœ¬æ¥è¿‘äºåŸå§‹æ¨¡å‹åˆ†å¸ƒã€‚RAPID3èƒ½å¤Ÿåœ¨å¤šä¸ªå…ˆè¿›çš„DiTä¸»å¹²ç½‘ç»œä¸Šå®ç°è¿‘3å€çš„é‡‡æ ·é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›å¼ºçš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAPID3æ˜¯ä¸€ä¸ªé’ˆå¯¹Diffusion Transformersçš„åŠ é€Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„é€Ÿåº¦è€Œä¸æŸå¤±è´¨é‡ã€‚</li>
<li>RAPID3åŒ…æ‹¬ä¸‰ä¸ªç­–ç•¥å¤´ï¼šStep-Skipã€Cache-Reuseå’ŒSparse-Attentionï¼Œå®ƒä»¬æ ¹æ®å»å™ªçŠ¶æ€è¿›è¡Œå†³ç­–ä»¥åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>ç­–ç•¥å‚æ•°é€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨çº¿è®­ç»ƒï¼Œè€ŒåŸºç¡€ç”Ÿæˆå™¨ä¿æŒä¸å˜ã€‚</li>
<li>RAPID3é‡‡ç”¨å¯¹æŠ—æ€§å­¦ä¹ çš„æ–¹æ³•å¢å¼ºå¥–åŠ±ä¿¡å·ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬æ¥è¿‘åŸå§‹æ¨¡å‹åˆ†å¸ƒã€‚</li>
<li>RAPID3èƒ½å¤Ÿåœ¨å¤šä¸ªå…ˆè¿›çš„DiTä¸»å¹²ç½‘ç»œä¸Šå®ç°è¿‘3å€çš„é‡‡æ ·é€Ÿåº¦æå‡ã€‚</li>
<li>RAPID3æ¡†æ¶èƒ½å¤Ÿåº”å¯¹è®­ç»ƒå…è´¹åŠ é€Ÿå™¨çš„å±€é™æ€§ï¼Œé€šè¿‡æ›´ç²¾ç»†çš„åŠ é€Ÿç­–ç•¥æé«˜å›¾åƒç”Ÿæˆæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89f100493232136ce1b88f3f506feafe" align="middle">
<img src="https://picx.zhimg.com/v2-f89dfa9a2dabcd47308776f608c3d773" align="middle">
<img src="https://picx.zhimg.com/v2-76a9646c8341e4d32144b287d07c8fd1" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rule-Based-Reinforcement-Learning-for-Document-Image-Classification-with-Vision-Language-Models"><a href="#Rule-Based-Reinforcement-Learning-for-Document-Image-Classification-with-Vision-Language-Models" class="headerlink" title="Rule-Based Reinforcement Learning for Document Image Classification with   Vision Language Models"></a>Rule-Based Reinforcement Learning for Document Image Classification with   Vision Language Models</h2><p><strong>Authors:Michael Jungo, Andreas Fischer</strong></p>
<p>Rule-based reinforcement learning has been gaining popularity ever since DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In the domain of document analysis, reinforcement learning is not as prevalent, even though many downstream tasks may benefit from the emerging properties of reinforcement learning, particularly the enhanced reason capabilities. We study the effects of rule-based reinforcement learning with the task of Document Image Classification which is one of the most commonly studied downstream tasks in document analysis. We find that reinforcement learning tends to have better generalisation capabilities to out-of-distritbution data, which we examine in three different scenarios, namely out-of-distribution images, unseen classes and different modalities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune">https://github.com/jungomi/vision-finetune</a>. </p>
<blockquote>
<p>åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è‡ªä»DeepSeek-R1é€šè¿‡ç®€å•çš„å¯éªŒè¯å¥–åŠ±å±•ç¤ºå…¶æˆåŠŸä»¥æ¥ï¼Œä¸€ç›´å—åˆ°å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶åœ¨æ–‡æ¡£åˆ†æé¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ çš„æ™®åŠç¨‹åº¦å¹¶ä¸é«˜ï¼Œå°½ç®¡è®¸å¤šä¸‹æ¸¸ä»»åŠ¡å¯èƒ½ä¼šä»å¼ºåŒ–å­¦ä¹ çš„æ–°å…´å±æ€§ä¸­å—ç›Šï¼Œå°¤å…¶æ˜¯å¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œè¿™æ˜¯æ–‡æ¡£åˆ†æä¸­æœ€å¸¸è§çš„ä¸‹æ¸¸ä»»åŠ¡ä¹‹ä¸€ã€‚æˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ å¯¹äºç¦»ç¾¤æ•°æ®å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§ä¸åŒåœºæ™¯ä¸­å¯¹æ­¤è¿›è¡Œäº†æ£€æŸ¥ï¼Œå³ç¦»ç¾¤å›¾åƒã€æœªè§ç±»åˆ«å’Œä¸åŒæ¨¡å¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jungomi/vision-finetuneä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22283v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune">https://github.com/jungomi/vision-finetune</a></p>
<p><strong>Summary</strong>ï¼šè§„åˆ™åŒ–çš„å¼ºåŒ–å­¦ä¹ å› DeepSeek-R1çš„æˆåŠŸæ¼”ç¤ºç®€å•å¯éªŒè¯å¥–åŠ±è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚åœ¨æ–‡æ¡£åˆ†æé¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ å°šæœªæ™®åŠï¼Œå°½ç®¡è®¸å¤šä¸‹æ¸¸ä»»åŠ¡å¯ä»å¼ºåŒ–å­¦ä¹ çš„æ–°å…´å±æ€§ä¸­å—ç›Šï¼Œç‰¹åˆ«æ˜¯å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿä»¥æ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸ºå¯¹è±¡ç ”ç©¶åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœï¼Œå‘ç°å¼ºåŒ–å­¦ä¹ åœ¨ä¸‰ç§ä¸åŒåœºæ™¯ä¸‹å¯¹ç¦»ç¾¤åˆ†å¸ƒæ•°æ®æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç¦»ç¾¤å›¾åƒã€æœªè§ç±»åˆ«å’Œä¸åŒæ¨¡æ€ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/jungomi/vision-finetune%E3%80%82">https://github.com/jungomi/vision-finetuneã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡ç®€å•å¯éªŒè¯å¥–åŠ±å®ç°æˆåŠŸåº”ç”¨ã€‚</li>
<li>æ–‡æ¡£åˆ†æé¢†åŸŸå¼ºåŒ–å­¦ä¹ æ™®åŠç¨‹åº¦ä¸é«˜ï¼Œä½†å…·æœ‰æ½œåŠ›ã€‚</li>
<li>åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æ¡£å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨ä¸‰ç§ä¸åŒåœºæ™¯ä¸‹å±•ç°å‡ºå¯¹ç¦»ç¾¤åˆ†å¸ƒæ•°æ®çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿåº”å¯¹å¦‚ç¦»ç¾¤å›¾åƒã€æœªè§ç±»åˆ«å’Œä¸åŒæ¨¡æ€ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bc721fb57818a57a8737eb716c934b2" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks"><a href="#Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks" class="headerlink" title="Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks"></a>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks</h2><p><strong>Authors:Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang, Yuanyuan Peng, Huan Gao, Mingkun Xu, Shangyang Li</strong></p>
<p>Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at <a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a> as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥åœ¨æ ‡å‡†åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çœŸæ­£çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚ç°æœ‰æ•°æ®é›†ä¸»è¦å¼ºè°ƒåˆ†ç±»å‡†ç¡®æ€§ï¼Œåˆ›é€ äº†ä¸€ç§è¯„ä¼°é”™è§‰ï¼Œå³æ¨¡å‹è™½ç„¶è¡¨ç°ç†Ÿç»ƒï¼Œä½†åœ¨é«˜é£é™©è¯Šæ–­æ¨ç†ä¸­ä»ç„¶å¤±è´¥ã€‚æˆ‘ä»¬æ¨å‡ºäº†Neural-MedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘è€Œæ¨ç†å¯†é›†å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºæ¢ç´¢ç¥ç»å­¦å¤šæ¨¡å¼ä¸´åºŠæ¨ç†çš„æé™ã€‚Neural-MedBenché›†æˆäº†å¤šåºåˆ—MRIæ‰«æã€ç»“æ„åŒ–ç”µå­å¥åº·è®°å½•å’Œä¸´åºŠç¬”è®°ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡å®¶æ—ï¼šé‰´åˆ«è¯Šæ–­ã€ç—…ç¶è¯†åˆ«å’Œç†ç”±ç”Ÿæˆã€‚ä¸ºäº†ç¡®ä¿å¯é çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ··åˆè¯„åˆ†ç®¡é“ï¼Œç»“åˆäº†åŸºäºLLMçš„è¯„åˆ†è€…ã€ä¸´åºŠåŒ»ç”ŸéªŒè¯å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMsçš„ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬GPT-4oã€Claude-4å’ŒMedGemmaï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸å¸¸è§„æ•°æ®é›†ç›¸æ¯”ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œæ¨ç†å¤±è´¥ï¼Œè€Œä¸æ˜¯æ„ŸçŸ¥é”™è¯¯ï¼Œä¸»å¯¼äº†æ¨¡å‹çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸¤è½´è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ï¼šé¢å‘å¹¿åº¦çš„å¤§å‹æ•°æ®é›†ç”¨äºç»Ÿè®¡æ³›åŒ–ï¼Œä»¥åŠé¢å‘æ·±åº¦çš„ç´§å‡‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Neural-MedBenchï¼‰ç”¨äºæ¨ç†ä¿çœŸåº¦ã€‚æˆ‘ä»¬å…¬å¼€äº†Neural-MedBenchï¼š<a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a>ï¼Œä½œä¸ºä¸€ä¸ªå¼€æ”¾å’Œå¯æ‰©å±•çš„è¯Šæ–­æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨æŒ‡å¯¼æœªæ¥åŸºå‡†æµ‹è¯•çš„æ‰©å±•ï¼Œå¹¶èƒ½å¤Ÿè¿›è¡Œä¸¥æ ¼è€Œç»æµå®æƒ çš„ä¸´åºŠå¯ä¿¡äººå·¥æ™ºèƒ½è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22258v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong><br>åœ¨æ ‡å‡†åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†å…¶çœŸæ­£çš„ä¸´åºŠæ¨ç†èƒ½åŠ›å°šä¸æ¸…æ¥šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Neural-MedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘ä¸”æ¨ç†å¯†é›†å‹çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¢ç´¢ç¥ç»å­¦ä¸­çš„å¤šæ¨¡æ€ä¸´åºŠæ¨ç†çš„æé™ã€‚å®ƒé›†æˆäº†å¤šåºåˆ—MRIæ‰«æã€ç»“æ„åŒ–ç”µå­å¥åº·è®°å½•å’Œä¸´åºŠç¬”è®°ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡å®¶æ—ï¼šé‰´åˆ«è¯Šæ–­ã€ç—…ç¶è¯†åˆ«å’Œç†ç”±ç”Ÿæˆã€‚ä¸ºäº†ç¡®ä¿å¯é çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ··åˆè¯„åˆ†ç®¡é“ï¼Œç»“åˆäº†åŸºäºLLMçš„è¯„åˆ†è€…ã€ä¸´åºŠåŒ»ç”ŸéªŒè¯å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMsçš„ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬GPT-4oã€Claude-4å’ŒMedGemmaï¼Œæˆ‘ä»¬å‘ç°ä¸å¸¸è§„æ•°æ®é›†ç›¸æ¯”ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œæ¨ç†å¤±è´¥è€Œéæ„ŸçŸ¥é”™è¯¯æ˜¯æ¨¡å‹çš„ä¸»è¦ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä¸¤è½´è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ï¼šé¢å‘å¹¿åº¦çš„å¤§å‹æ•°æ®é›†ç”¨äºç»Ÿè®¡æ³›åŒ–ï¼Œä»¥åŠé¢å‘æ·±åº¦çš„ç´§å‡‘åŸºå‡†æµ‹è¯•å¦‚Neural-MedBenchç”¨äºæ¨ç†ä¿çœŸåº¦ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†Neural-MedBenchï¼ˆ<a target="_blank" rel="noopener" href="https://neuromedbench.github.io/%EF%BC%89%EF%BC%8C%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%BC%80%E6%94%BE%E5%92%8C%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E8%AF%8A%E6%96%AD%E6%B5%8B%E8%AF%95%E5%B9%B3%E5%8F%B0%EF%BC%8C%E4%B8%BA%E6%9C%AA%E6%9D%A5%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9A%84%E5%8F%91%E5%B1%95%E6%8F%90%E4%BE%9B%E6%8C%87%E5%AF%BC%EF%BC%8C%E5%B9%B6%E8%83%BD%E8%BF%9B%E8%A1%8C%E4%B8%A5%E6%A0%BC%E4%B8%94%E7%BB%8F%E6%B5%8E%E5%AE%9E%E6%83%A0%E7%9A%84%E4%B8%B4%E5%BA%8A%E4%BF%A1%E4%BB%BB%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AF%84%E4%BC%B0%E3%80%82">https://neuromedbench.github.io/ï¼‰ï¼Œä½œä¸ºä¸€ä¸ªå¼€æ”¾å’Œå¯æ‰©å±•çš„è¯Šæ–­æµ‹è¯•å¹³å°ï¼Œä¸ºæœªæ¥åŸºå‡†æµ‹è¯•çš„å‘å±•æä¾›æŒ‡å¯¼ï¼Œå¹¶èƒ½è¿›è¡Œä¸¥æ ¼ä¸”ç»æµå®æƒ çš„ä¸´åºŠä¿¡ä»»äººå·¥æ™ºèƒ½è¯„ä¼°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œä½†ä¸´åºŠæ¨ç†èƒ½åŠ›å°šä¸æ¸…æ¥šã€‚</li>
<li>æ¨å‡ºNeural-MedBenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Neural-MedBenché›†æˆäº†MRIæ‰«æã€ç”µå­å¥åº·è®°å½•å’Œä¸´åºŠç¬”è®°ç­‰å¤šä¸ªæ•°æ®æºã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šé‰´åˆ«è¯Šæ–­ã€ç—…ç¶è¯†åˆ«å’Œç†ç”±ç”Ÿæˆã€‚</li>
<li>å¼€å‘äº†æ··åˆè¯„åˆ†ç®¡é“ä»¥ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>ä¸å¸¸è§„æ•°æ®é›†ç›¸æ¯”ï¼ŒVLMsåœ¨Neural-MedBenchä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>é”™è¯¯åˆ†ææ˜¾ç¤ºæ¨ç†å¤±è´¥æ˜¯æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜ï¼Œè€Œä¸æ˜¯æ„ŸçŸ¥é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-101dbbc008b15e0ecb7719ddd919518f" align="middle">
<img src="https://picx.zhimg.com/v2-00760c0a6937eb06ae90262c0d12787d" align="middle">
<img src="https://picx.zhimg.com/v2-8d732e65fb684b9a920a28b17486960d" align="middle">
<img src="https://picx.zhimg.com/v2-d3a477b11590f393fd02ed9bd0908035" align="middle">
<img src="https://picx.zhimg.com/v2-3d22a37cba0c3039329a148b6243458a" align="middle">
<img src="https://picx.zhimg.com/v2-3ee1e583a2088419b495f2100c61e9ea" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing"><a href="#Evaluating-LLMs-for-Combinatorial-Optimization-One-Phase-and-Two-Phase-Heuristics-for-2D-Bin-Packing" class="headerlink" title="Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing"></a>Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing</h2><p><strong>Authors:Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Rajesh Mojumder</strong></p>
<p>This paper presents an evaluation framework for assessing Large Language Modelsâ€™ (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»„åˆä¼˜åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è§£å†³äºŒç»´è£…ç®±é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç³»ç»Ÿæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†LLMä¸è¿›åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å…¨é¢çš„å®éªŒå¯¹æ¯”LLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆæœ‰é™é¦–æ¬¡é€‚åº”æ³•å’Œæ··åˆé¦–æ¬¡é€‚åº”æ³•ï¼‰ï¼Œæˆ‘ä»¬è¯æ˜äº†LLMèƒ½å¤Ÿç”Ÿæˆæ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å‡å°‘è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒGPT-4oåœ¨ä¸¤æ¬¡è¿­ä»£å†…å®ç°äº†æœ€ä¼˜è§£ï¼Œå¹³å‡ä½¿ç”¨çš„ç®±å­æ•°é‡ä»åŸæ¥çš„16ä¸ªå‡å°‘åˆ°15ä¸ªï¼Œç©ºé—´åˆ©ç”¨ç‡ä¹Ÿä»åŸæ¥çš„0.76-0.78æå‡åˆ°ç°åœ¨çš„0.83ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºç†è§£LLMåœ¨ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æœ‰ç€é‡è¦çš„è´¡çŒ®ï¼Œå¹¶å»ºç«‹äº†è¯„ä¼°LLMåœ¨ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸­æ€§èƒ½çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22255v1">PDF</a> 1 table, 6 figures. 39th Conference on Neural Information Processing   Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM   Lifecycle Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»„åˆä¼˜åŒ–é¢†åŸŸçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è§£å†³äºŒç»´è£…ç®±é—®é¢˜çš„æ–¹æ³•ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§ç»“åˆLLMå’Œè¿›åŒ–ç®—æ³•çš„ç³»ç»Ÿæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚æœ‰é™é¦–æ¬¡é€‚é…å’Œæ··åˆé¦–æ¬¡é€‚é…ï¼‰æ›´é«˜æ•ˆï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—æ›´å°‘ã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨ä¸¤æ¬¡è¿­ä»£å†…è¾¾åˆ°æœ€ä¼˜è§£ï¼Œå¹³å‡ä½¿ç”¨ç®±æ•°ä»16ä¸ªå‡å°‘åˆ°15ä¸ªï¼Œç©ºé—´åˆ©ç”¨ç‡ä»0.76-0.78æé«˜åˆ°0.83ã€‚è¯¥ç ”ç©¶åŠ æ·±äº†å¯¹LLMåœ¨ç‰¹å®šé¢†åŸŸè¯„ä»·çš„ç†è§£ï¼Œå¹¶ä¸ºè¯„ä¼°å…¶åœ¨ç»„åˆä¼˜åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½å»ºç«‹äº†åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹äºŒç»´è£…ç®±é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆLLMå’Œè¿›åŒ–ç®—æ³•çš„ç³»ç»Ÿæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå’Œä¼˜åŒ–å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜ï¼ŒLLMç”Ÿæˆçš„å¯å‘å¼æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ›´ä¸ºé«˜æ•ˆï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—æ›´å°‘ã€‚</li>
<li>GPT-4oåœ¨ä¸¤æ¬¡è¿­ä»£å†…æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œå±•ç°äº†å…¶é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœæé«˜äº†ç©ºé—´åˆ©ç”¨ç‡ï¼Œå¹³å‡ä½¿ç”¨ç®±æ•°æœ‰æ‰€å‡å°‘ã€‚</li>
<li>è¯¥ç ”ç©¶æœ‰åŠ©äºåŠ æ·±å¯¹LLMåœ¨ç‰¹å®šé¢†åŸŸè¯„ä»·çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8eb5498545dffd4406ac9e628e97a22b" align="middle">
<img src="https://picx.zhimg.com/v2-2ef43380d407c29ce79b44d190b9cbc5" align="middle">
<img src="https://picx.zhimg.com/v2-de879a991a7537028f14fa0a9616aabb" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ASSESS-A-Semantic-and-Structural-Evaluation-Framework-for-Statement-Similarity"><a href="#ASSESS-A-Semantic-and-Structural-Evaluation-Framework-for-Statement-Similarity" class="headerlink" title="ASSESS: A Semantic and Structural Evaluation Framework for Statement   Similarity"></a>ASSESS: A Semantic and Structural Evaluation Framework for Statement   Similarity</h2><p><strong>Authors:Xiaoyang Liu, Tao Zhu, Zineng Dong, Yuntian Liu, Qingfeng Guo, Zhaoxuan Liu, Yu Chen, Tao Luo</strong></p>
<p>Statement autoformalization, the automated translation of statements from natural language into formal languages, has seen significant advancements, yet the development of automated evaluation metrics remains limited. Existing metrics for formal statement similarity often fail to balance semantic and structural information. String-based approaches capture syntactic structure but ignore semantic meaning, whereas proof-based methods validate semantic equivalence but disregard structural nuances and, critically, provide no graded similarity score in the event of proof failure. To address these issues, we introduce ASSESS (A Semantic and Structural Evaluation Framework for Statement Similarity), which comprehensively integrates semantic and structural information to provide a continuous similarity score. Our framework first transforms formal statements into Operator Trees to capture their syntactic structure and then computes a similarity score using our novel TransTED (Transformation Tree Edit Distance) Similarity metric, which enhances traditional Tree Edit Distance by incorporating semantic awareness through transformations. For rigorous validation, we present EPLA (Evaluating Provability and Likeness for Autoformalization), a new benchmark of 524 expert-annotated formal statement pairs derived from miniF2F and ProofNet, with labels for both semantic provability and structural likeness. Experiments on EPLA demonstrate that TransTED Similarity outperforms existing methods, achieving state-of-the-art accuracy and the highest Kappa coefficient. The benchmark, and implementation code will be made public soon. </p>
<blockquote>
<p>å£°æ˜è‡ªåŠ¨å½¢å¼åŒ–ï¼ˆå°†è‡ªç„¶è¯­è¨€ä¸­çš„å£°æ˜è‡ªåŠ¨è½¬æ¢ä¸ºå½¢å¼åŒ–è¯­è¨€ï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å‘å±•ä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„å½¢å¼åŒ–å£°æ˜ç›¸ä¼¼æ€§åº¦é‡é€šå¸¸æ— æ³•å¹³è¡¡è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ã€‚åŸºäºå­—ç¬¦ä¸²çš„æ–¹æ³•å¯ä»¥æ•æ‰å¥æ³•ç»“æ„ï¼Œä½†å¿½ç•¥äº†è¯­ä¹‰æ„ä¹‰ï¼Œè€ŒåŸºäºè¯æ˜çš„æ–¹æ³•éªŒè¯äº†è¯­ä¹‰ç­‰ä»·æ€§ï¼Œä½†å¿½ç•¥äº†ç»“æ„ç»†å¾®å·®åˆ«ï¼Œå¹¶ä¸”åœ¨è¯æ˜å¤±è´¥çš„æƒ…å†µä¸‹ä¸æä¾›åˆ†çº§ç›¸ä¼¼æ€§è¯„åˆ†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ASSESSï¼ˆå£°æ˜ç›¸ä¼¼æ€§çš„è¯­ä¹‰å’Œç»“æ„è¯„ä¼°æ¡†æ¶ï¼‰ï¼Œå®ƒå…¨é¢æ•´åˆè¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ä»¥æä¾›è¿ç»­çš„ç›¸ä¼¼æ€§è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆå°†å½¢å¼åŒ–å£°æ˜è½¬æ¢ä¸ºæ“ä½œæ ‘ä»¥æ•è·å…¶å¥æ³•ç»“æ„ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬æ–°é¢–çš„TransTEDï¼ˆè½¬æ¢æ ‘ç¼–è¾‘è·ç¦»ï¼‰ç›¸ä¼¼æ€§åº¦é‡æ¥è®¡ç®—ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œè¯¥åº¦é‡é€šè¿‡è½¬æ¢èå…¥è¯­ä¹‰æ„è¯†ï¼Œå¢å¼ºäº†ä¼ ç»Ÿçš„æ ‘ç¼–è¾‘è·ç¦»ã€‚ä¸ºäº†ä¸¥æ ¼éªŒè¯ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EPLAï¼ˆè‡ªåŠ¨å½¢å¼åŒ–çš„å¯è¯æ˜æ€§å’Œç›¸ä¼¼æ€§è¯„ä»·ï¼‰ï¼Œè¿™æ˜¯ä»miniF2Få’ŒProofNetæ´¾ç”Ÿçš„ã€å¸¦æœ‰è¯­ä¹‰å¯è¯æ˜æ€§å’Œç»“æ„ç›¸ä¼¼æ€§çš„ä¸“å®¶æ³¨é‡Šå½¢å¼åŒ–å£°æ˜å¯¹çš„æ–°åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«524å¯¹æ•°æ®ç‚¹ã€‚åœ¨EPLAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransTEDç›¸ä¼¼æ€§åº¦é‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæœ€é«˜çš„Kappaç³»æ•°ã€‚åŸºå‡†æ•°æ®é›†å’Œå®ç°ä»£ç å°†å¾ˆå¿«å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22246v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨å½¢å¼åŒ–å£°æ˜çš„è¯„ä¼°æ–¹æ³•çš„å‘å±•çŠ¶å†µï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„ç»¼åˆè¯­ä¹‰å’Œç»“æ„ä¿¡æ¯çš„è¯„ä»·æ¡†æ¶â€”â€”ASSESSï¼Œå¹¶å¼•å…¥äº†TransTEDç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è½¬æ¢æ“ä½œæ ‘æ•æ‰è¯­å¥çš„è¯­æ³•ç»“æ„ï¼Œå¹¶é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„è½¬æ¢å¢å¼ºä¼ ç»Ÿçš„æ ‘ç¼–è¾‘è·ç¦»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransTEDç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•åœ¨EPLAåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡å’Œkappaç³»æ•°å‡è¾¾åˆ°æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨å½¢å¼åŒ–å£°æ˜çš„è¯„ä¼°ä»ç„¶æ˜¯ç ”ç©¶çš„çƒ­ç‚¹å’Œéš¾ç‚¹ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•æ— æ³•å¹³è¡¡è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ã€‚</li>
<li>ASSESSæ¡†æ¶é€šè¿‡ç»“åˆè¯­ä¹‰å’Œç»“æ„ä¿¡æ¯æ¥è§£å†³ç°æœ‰é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§è¿ç»­çš„ç›¸ä¼¼åº¦è¯„åˆ†æ–¹æ³•ã€‚å®ƒé€šè¿‡æ“ä½œæ ‘æ¥æ•æ‰è¯­æ³•ç»“æ„ã€‚</li>
<li>TransTEDç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•æ˜¯æœ¬æ–‡æå‡ºçš„åˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡èå…¥è¯­ä¹‰æ„ŸçŸ¥çš„è½¬æ¢æ¥å¢å¼ºä¼ ç»Ÿçš„æ ‘ç¼–è¾‘è·ç¦»ã€‚</li>
<li>EPLAåŸºå‡†æµ‹è¯•é›†ç”±ä¸“å®¶æ ‡æ³¨çš„524å¯¹å½¢å¼åŒ–å£°æ˜ç»„æˆï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨å½¢å¼åŒ–çš„å¯ä¿¡åº¦ä¸ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ede2327f8ec11b8695c73c76ef12f979" align="middle">
<img src="https://picx.zhimg.com/v2-b579ec000ca46c0a4acce11c9a309b17" align="middle">
<img src="https://picx.zhimg.com/v2-c85c5fd464bcd2c8d1c63acab75a41bc" align="middle">
<img src="https://picx.zhimg.com/v2-f3c5bd3833ac65ad4b58285988473e53" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Thinking-in-Many-Modes-How-Composite-Reasoning-Elevates-Large-Language-Model-Performance-with-Limited-Data"><a href="#Thinking-in-Many-Modes-How-Composite-Reasoning-Elevates-Large-Language-Model-Performance-with-Limited-Data" class="headerlink" title="Thinking in Many Modes: How Composite Reasoning Elevates Large Language   Model Performance with Limited Data"></a>Thinking in Many Modes: How Composite Reasoning Elevates Large Language   Model Performance with Limited Data</h2><p><strong>Authors:Zishan Ahmad, Saisubramaniam Gopalakrishnan</strong></p>
<p>Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºå•ä¸€ã€ä¸»å¯¼çš„æ¨ç†èŒƒå¼ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦å¤šç§è®¤çŸ¥ç­–ç•¥çš„å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤åˆæ¨ç†ï¼ˆCRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨ç†æ–¹æ³•ï¼Œèƒ½å¤Ÿèµ‹äºˆLLMsåŠ¨æ€æ¢ç´¢å’Œç»“åˆå¤šç§æ¨ç†é£æ ¼çš„èƒ½åŠ›ï¼Œå¦‚æ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†å’Œç±»æ¯”æ¨ç†ï¼Œä»¥æ›´å¾®å¦™çš„æ–¹å¼è§£å†³é—®é¢˜ã€‚åœ¨ç§‘å­¦å’ŒåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå¹¶ä¸”è¶…è¶Šäº†DeepSeek-R1é£æ ¼æ¨ç†ï¼ˆSRï¼‰çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºè¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡å’Œè¶³å¤Ÿçš„ä»¤ç‰Œä½¿ç”¨æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCRèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¼ºè°ƒä¸é¢†åŸŸç›¸é€‚åº”çš„æ¨ç†é£æ ¼ã€‚å®ƒåœ¨åŒ»å­¦é—®é¢˜å›ç­”ä¸­ä¼˜å…ˆä½¿ç”¨ç±»æ¯”å’Œæ¼”ç»æ¨ç†ï¼Œä½†åœ¨ç§‘å­¦æ¨ç†ä¸­åˆ™è½¬å‘å› æœã€æ¼”ç»å’Œå½’çº³æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åŸ¹å…»å†…éƒ¨æ¨ç†é£æ ¼çš„å¤šæ ·æ€§ï¼ŒLLMså¯ä»¥è·å¾—æ›´ç¨³å¥ã€è‡ªé€‚åº”å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22224v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶å—é™äºå•ä¸€çš„é¢„å®šä¹‰æ¨ç†æ¨¡å¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤åˆæ¨ç†ï¼ˆCRï¼‰è¿™ä¸€æ–°å‹æ¨ç†æ–¹æ³•ï¼Œå®ƒèƒ½åŠ¨æ€æ¢ç´¢å¹¶ç»“åˆå¤šç§æ¨ç†é£æ ¼ï¼ˆå¦‚æ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†å’Œå‡è®¾æ¨ç†ï¼‰ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„é—®é¢˜è§£å†³ã€‚åœ¨ç§‘ç ”å’ŒåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤åˆæ¨ç†æ–¹æ³•ä¼˜äºç°æœ‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç­‰åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œå¹¶è¶…è¶Šäº†DeepSeek-R1å¼æ¨ç†ï¼ˆSRï¼‰çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ ·æœ¬æ•ˆç‡å’Œè¶³å¤Ÿçš„æ ‡è®°è¯ä½¿ç”¨æ•ˆç‡ã€‚ç‰¹åˆ«åœ°ï¼Œå¤åˆæ¨ç†å¯ä»¥è‡ªé€‚åº”åœ°å¼ºè°ƒä¸ç‰¹å®šé¢†åŸŸé€‚åˆçš„æ¨ç†é£æ ¼ï¼Œæ¯”å¦‚åœ¨åŒ»ç–—é—®ç­”ä¸Šæ³¨é‡å‡è®¾æ¨ç†å’Œæ¼”ç»æ¨ç†ï¼Œè€Œåœ¨ç§‘å­¦æ¨ç†ä¸Šåˆ™è½¬å‘å› æœã€æ¼”ç»å’Œå½’çº³æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åŸ¹å…»å†…éƒ¨æ¨ç†é£æ ¼çš„å¤šæ ·æ€§ï¼ŒLLMå¯ä»¥è·å¾—æ›´å¼ºå¤§ã€é€‚åº”æ€§å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå—é™äºå•ä¸€é¢„å®šä¹‰æ¨ç†æ¨¡å¼ï¼Œéš¾ä»¥è§£å†³éœ€è¦å¤šç§è®¤çŸ¥ç­–ç•¥çš„å¤æ‚é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¤åˆæ¨ç†ï¼ˆCRï¼‰æ–¹æ³•ï¼Œèƒ½ç»“åˆå¤šç§æ¨ç†é£æ ¼ï¼ˆå¦‚æ¼”ç»ã€å½’çº³å’Œå‡è®¾æ¨ç†ï¼‰ã€‚</li>
<li>åœ¨ç§‘ç ”å’ŒåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¤åˆæ¨ç†è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚</li>
<li>å¤åˆæ¨ç†æ–¹æ³•æé«˜äº†æ ·æœ¬æ•ˆç‡å’Œæ ‡è®°è¯ä½¿ç”¨æ•ˆç‡ã€‚</li>
<li>å¤åˆæ¨ç†èƒ½è‡ªé€‚åº”å¼ºè°ƒç‰¹å®šé¢†åŸŸçš„æ¨ç†é£æ ¼ï¼Œå¦‚åŒ»ç–—é¢†åŸŸçš„å‡è®¾å’Œæ¼”ç»æ¨ç†ã€‚</li>
<li>é€šè¿‡åŸ¹å…»å†…éƒ¨æ¨ç†é£æ ¼çš„å¤šæ ·æ€§ï¼ŒLLMè·å¾—æ›´å¼ºå¤§ã€é€‚åº”æ€§å’Œé«˜æ•ˆçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d096f727fc52acf3347d90a5095a827c" align="middle">
<img src="https://picx.zhimg.com/v2-7f63ade4311e94344eac5fbf8d9d5779" align="middle">
<img src="https://picx.zhimg.com/v2-0aec70d7c67a500fd071b6e4c2d70351" align="middle">
<img src="https://picx.zhimg.com/v2-220f139e4f7a49c94ad2d805cf2a75b6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Faithful-Reasoning-in-Remote-Sensing-A-Perceptually-Grounded-GeoSpatial-Chain-of-Thought-for-Vision-Language-Models"><a href="#Towards-Faithful-Reasoning-in-Remote-Sensing-A-Perceptually-Grounded-GeoSpatial-Chain-of-Thought-for-Vision-Language-Models" class="headerlink" title="Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded   GeoSpatial Chain-of-Thought for Vision-Language Models"></a>Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded   GeoSpatial Chain-of-Thought for Vision-Language Models</h2><p><strong>Authors:Jiaqi Liu, Lang Sun, Ronghao Fu, Bo Yang</strong></p>
<p>Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the modelâ€™s reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation. </p>
<blockquote>
<p>é¥æ„Ÿä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾€å¾€åœ¨å¤æ‚çš„åˆ†æä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™ä¸€å±€é™æ€§æºäºå…¶ç«¯åˆ°ç«¯çš„è®­ç»ƒèŒƒå¼ï¼Œè¿™ç§èŒƒå¼è·³è¿‡äº†å…³é”®çš„æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´è¾“å‡ºæ— æ³•éªŒè¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ„ŸçŸ¥åŸºç¡€åœ°ç†æ€ç»´é“¾ï¼ˆGeo-CoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†é¥æ„Ÿåˆ†æå»ºæ¨¡ä¸ºå¯éªŒè¯çš„å¤šæ­¥éª¤è¿‡ç¨‹çš„æ¡†æ¶ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥æ¥çŒè¾“è¿™ç§åˆ†æè¿‡ç¨‹ï¼Œåˆ©ç”¨Geo-CoT380kï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„ç»“æ„åŒ–Geo-CoTç†ç”±æ•°æ®é›†ã€‚è¯¥ç­–ç•¥é¦–å…ˆé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥çŒè¾“åŸºç¡€è®¤çŸ¥æ¶æ„ï¼Œç„¶ååˆ©ç”¨ç»„å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å®Œå–„æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œä½¿å…¶æœäº‹å®æ­£ç¡®æ€§æ–¹å‘å‘å±•ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹RSThinkerä¸ä»…è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œè¿˜è¾“å‡ºå¯éªŒè¯çš„åˆ†æè½¨è¿¹ã€‚è¿™ç§èƒ½åŠ›å¸¦æ¥äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬æ•°æ®é›†Geo-CoT380kå’ŒRSThinkeræ¨¡å‹çš„å…¬å¼€å‘å¸ƒï¼Œä¸ºä»æ¨¡ç³Šæ„ŸçŸ¥èµ°å‘ç»“æ„åŒ–ã€å¯éªŒè¯çš„åœ°çƒè§‚æµ‹æ¨ç†æä¾›äº†å…·ä½“çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨é¥æ„Ÿé¢†åŸŸä¸­ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤æ‚åˆ†æä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPerceptually-Grounded Geospatial Chain-of-Thoughtï¼ˆGeo-CoTï¼‰çš„æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥ï¼Œåˆ©ç”¨Geo-CoT380kæ•°æ®é›†ï¼Œå°†é¥æ„Ÿåˆ†æå»ºæ¨¡ä¸ºå¯éªŒè¯çš„å¤šæ­¥éª¤è¿‡ç¨‹ã€‚é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å»ºç«‹åŸºç¡€è®¤çŸ¥æ¶æ„ï¼Œç„¶ååˆ©ç”¨Group Reward Policy Optimizationï¼ˆGRPOï¼‰æ¥ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œä»¥å®ç°äº‹å®æ­£ç¡®æ€§ã€‚æœ€ç»ˆäº§ç”Ÿçš„æ¨¡å‹RSThinkerä¸ä»…èƒ½ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œè¿˜èƒ½æä¾›å¯éªŒè¯çš„åˆ†æè½¨è¿¹ã€‚è¿™ä¸€èƒ½åŠ›ä½¿å…¶åœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¥æ„Ÿå¤æ‚åˆ†æä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§æ–°çš„æ¡†æ¶æ¥è§£å†³ã€‚</li>
<li>æå‡ºäº†Perceptually-Grounded Geospatial Chain-of-Thoughtï¼ˆGeo-CoTï¼‰æ¡†æ¶ï¼Œå°†é¥æ„Ÿåˆ†æå»ºæ¨¡ä¸ºå¯éªŒè¯çš„å¤šæ­¥éª¤è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥å’ŒGeo-CoT380kæ•°æ®é›†æ¥å»ºç«‹å’Œæ¨å¹¿è¿™ä¸€æ¡†æ¶ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å»ºç«‹åŸºç¡€è®¤çŸ¥æ¶æ„ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨Group Reward Policy Optimizationï¼ˆGRPOï¼‰ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†ç­–ç•¥ï¼Œå®ç°äº‹å®æ­£ç¡®æ€§ã€‚</li>
<li>æœ€ç»ˆäº§ç”Ÿçš„æ¨¡å‹RSThinkerèƒ½åŒæ—¶è¾“å‡ºæœ€ç»ˆç­”æ¡ˆå’Œå¯éªŒè¯çš„åˆ†æè½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffdd8e66a0e4be00eff8384aa77c658b" align="middle">
<img src="https://picx.zhimg.com/v2-c86ad03b5dfddc88026eb30aec3e1e8a" align="middle">
<img src="https://picx.zhimg.com/v2-62a3a2d3f1e62223c1358b21f63b752e" align="middle">
<img src="https://picx.zhimg.com/v2-3dc56433121a6951f77df7df7a6be9b2" align="middle">
<img src="https://picx.zhimg.com/v2-c1a863b482cb97b8b8a21713de2a7dcf" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-df408d6615225b002a326d82d15e2cf8" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  VoiceAssistant-Eval Benchmarking AI Assistants across Listening,   Speaking, and Viewing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1e99f3110d3840fcecdac10c291309b7" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Comprehend and Talk Text to Speech Synthesis via Dual Language Modeling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
