<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-09-30  Dynamic Experts Search Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21801v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-30-更新"><a href="#2025-09-30-更新" class="headerlink" title="2025-09-30 更新"></a>2025-09-30 更新</h1><h2 id="Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time"><a href="#Dynamic-Experts-Search-Enhancing-Reasoning-in-Mixture-of-Experts-LLMs-at-Test-Time" class="headerlink" title="Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time"></a>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time</h2><p><strong>Authors:Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</strong></p>
<p>Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning. </p>
<blockquote>
<p>测试时缩放（TTS）通过推理过程中的额外计算来增强大型语言模型（LLM）的推理能力。然而，现有方法主要依赖于输出级别的采样，而忽略了模型架构的作用。在主流混合专家（MoE）LLM中，我们观察到改变激活的专家数量会产生具有稳定准确率的互补解决方案集，揭示了一种新的且尚未被充分探索的多样性来源。受此观察结果的启发，我们提出了动态专家搜索（DES），这是一种TTS策略，它将专家激活提升为搜索空间的可控维度。DES集成了两个关键组件：（1）动态MoE，它能够在推理过程中直接控制专家数量，生成多样化的推理轨迹，无需额外成本；（2）专家配置继承，它能够在推理路径内保持一致的专家数量，同时在各次运行中变化它们，从而在搜索过程中平衡稳定性和多样性。在MoE架构、验证器和推理基准测试（例如数学、代码和知识）的广泛实验表明，DES可靠地优于TTS基线，提高了准确性和稳定性，且无需额外成本。这些结果强调了DES作为一种实用且可扩展的架构感知TTS形式，展示了现代LLM中的结构灵活性如何推动推理的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22572v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本中提到的Test-Time Scaling（TTS）通过推理时分配额外的计算资源增强了大型语言模型（LLM）的推理能力。然而，现有的方法主要依赖于输出级别的采样，而忽略了模型架构的作用。本文观察到主流混合专家（MoE）LLM中激活的专家数量不同会产生互补的解决方案集，并具有稳定的准确性，揭示了一种新的未被充分探索的多样性来源。基于此观察，本文提出了Dynamic Experts Search（DES）的TTS策略，将专家激活提升为搜索空间的可控维度。DES包括两个关键组件：动态MoE和专家配置继承。动态MoE能够在推理过程中直接控制专家数量，生成多样化的推理轨迹而无需额外成本；专家配置继承则能够在推理路径内保持一致的专家数量，同时在多次运行中变化它们，从而在搜索中平衡稳定性和多样性。实验表明，DES在MoE架构、验证器和推理基准测试（例如数学、代码和知识）上可靠地优于TTS基线，提高了准确性和稳定性且无需额外成本。这些结果突出了DES作为实用且可扩展的架构感知TTS形式的特点，展示了现代LLM中结构灵活性在推理方面的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-Time Scaling (TTS) 通过在推理时分配额外计算资源，提高了大型语言模型（LLMs）的推理能力。</li>
<li>现有TTS方法主要关注输出级别的采样，忽略了模型架构的作用。</li>
<li>在混合专家（MoE）LLMs中，激活的专家数量不同会产生不同的解决方案集，表现出稳定的准确性。</li>
<li>Dynamic Experts Search (DES) 是一种新的TTS策略，将专家激活作为搜索空间的可控维度。</li>
<li>DES包括两个关键组件：动态MoE和专家配置继承，能够在推理过程中直接控制专家数量并生成多样化轨迹，同时保持推理路径内的专家数量一致性。</li>
<li>实验表明DES在多种MoE架构和推理基准测试上优于现有TTS方法，提高了准确性和稳定性，且无需额外成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22572">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22572v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22572v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22572v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22572v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22572v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Kernel-Regression-of-Multi-Way-Data-via-Tensor-Trains-with-Hadamard-Overparametrization-The-Dynamic-Graph-Flow-Case"><a href="#Kernel-Regression-of-Multi-Way-Data-via-Tensor-Trains-with-Hadamard-Overparametrization-The-Dynamic-Graph-Flow-Case" class="headerlink" title="Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard   Overparametrization: The Dynamic Graph Flow Case"></a>Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard   Overparametrization: The Dynamic Graph Flow Case</h2><p><strong>Authors:Duc Thien Nguyen, Konstantinos Slavakis, Eleftherios Kofidis, Dimitris Pados</strong></p>
<p>A regression-based framework for interpretable multi-way data imputation, termed Kernel Regression via Tensor Trains with Hadamard overparametrization (KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting imputation as regression via reproducing kernel Hilbert spaces. Parameter efficiency is achieved through tensors of fixed tensor-train (TT) rank, which reside on low-dimensional Riemannian manifolds, and is further enhanced via Hadamard overparametrization, which promotes sparsity within the TT parameter space. Learning is accomplished by solving a smooth inverse problem posed on the Riemannian manifold of fixed TT-rank tensors. As a representative application, the estimation of dynamic graph flows is considered. In this setting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based (topological) priors via its inverse problem formulation. Numerical tests on real-world graph datasets demonstrate that KReTTaH consistently outperforms state-of-the-art alternatives-including a nonparametric tensor- and a neural-network-based methods-for imputing missing, time-varying edge flows. </p>
<blockquote>
<p>介绍了一种基于回归的可解释多元数据补全框架，称为通过张量列车和哈达玛超参数化实现的核回归（KReTTaH）。KReTTaH采用非参数公式，通过将补全表示为通过再生核希尔伯特空间的回归来实现。通过固定张量列车（TT）秩的张量，它们在低维黎曼流形上实现参数效率，并通过哈达玛超参数化进一步增强，这促进了TT参数空间内的稀疏性。学习是通过解决固定TT秩张量上的黎曼流形上的平滑反问题来实现的。作为一个典型的应用，考虑了动态图流的估计。在这种情况下，KReTTaH通过其反问题公式灵活地融入了基于图的（拓扑）先验知识。对真实世界图数据集进行的数值测试表明，KReTTaH始终优于最新的替代方案——包括一种用于补全缺失的、时间变化的边缘流的非参数张量和基于神经网络的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22197v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于回归的多方式数据填充框架Kernel Regression via Tensor Trains with Hadamard overparametrization（简称KReTTaH）被引入。KReTTaH通过将数据填充作为回归转换为再生核希尔伯特空间的方式实现非参数化公式化表达。它通过在固定张量列车（TT）等级张量上实现的参数效率达到高效状态，在相对简单的低维流形上产生矩阵空间的隐层效应；然后通过哈达玛超参数化进一步增强了其效果，在TT参数空间中促进了稀疏性。学习是通过解决固定TT等级张量上的流形平滑反问题来实现的。作为典型应用，考虑了动态图流的估计问题。在这个场景下，KReTTaH展现出了灵活性，能够很好地通过其反问题形式融入基于图的先验知识。在真实图形数据集上的数值测试表明，相较于最新的非参数张量方法以及神经网络方法，KReTTaH具有出色的缺失、时间动态边缘流预测性能。在补齐缺失数据方面表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KReTTaH是一种基于回归的多方式数据填充框架，通过非参数化公式实现数据填充。</li>
<li>KReTTaH利用固定张量列车（TT）等级张量的参数效率进行运算优化。</li>
<li>哈达玛超参数化增强了KReTTaH的性能，促进了TT参数空间中的稀疏性。</li>
<li>学习过程是通过解决固定TT等级张量上的流形平滑反问题来实现的。</li>
<li>KReTTaH在动态图流估计中具有灵活性，能够融入基于图的先验知识。</li>
<li>在真实图形数据集上的数值测试中，KReTTaH相较于其他方法表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22197">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22197v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22197v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22197v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Semantic-VAE-Semantic-Alignment-Latent-Representation-for-Better-Speech-Synthesis"><a href="#Semantic-VAE-Semantic-Alignment-Latent-Representation-for-Better-Speech-Synthesis" class="headerlink" title="Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech   Synthesis"></a>Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech   Synthesis</h2><p><strong>Authors:Zhikang Niu, Shujie Hu, Jeongsoo Choi, Yushen Chen, Peining Chen, Pengcheng Zhu, Yunting Yang, Bowen Zhang, Jian Zhao, Chunhui Wang, Xie Chen</strong></p>
<p>While mel-spectrograms have been widely utilized as intermediate representations in zero-shot text-to-speech (TTS), their inherent redundancy leads to inefficiency in learning text-speech alignment. Compact VAE-based latent representations have recently emerged as a stronger alternative, but they also face a fundamental optimization dilemma: higher-dimensional latent spaces improve reconstruction quality and speaker similarity, but degrade intelligibility, while lower-dimensional spaces improve intelligibility at the expense of reconstruction fidelity. To overcome this dilemma, we propose Semantic-VAE, a novel VAE framework that utilizes semantic alignment regularization in the latent space. This design alleviates the reconstruction-generation trade-off by capturing semantic structure in high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE significantly improves synthesis quality and training efficiency. When integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker similarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and vanilla acoustic VAE baselines (2.65%, 0.59). We also release the code and models to facilitate further research. </p>
<blockquote>
<p>虽然梅尔频谱图已广泛应用于零样本文本到语音（TTS）的中间表示，但其固有的冗余性导致文本语音对齐学习的不高效。基于紧凑变分自动编码器（VAE）的潜在表示最近作为一种更强的替代方案出现，但它们也面临一个基本的优化困境：高维潜在空间提高了重建质量和说话人相似性，但降低了可懂度，而低维空间提高了可懂度却以牺牲重建保真度为代价。为了克服这一困境，我们提出了Semantic-VAE，这是一种新型VAE框架，在潜在空间中使用语义对齐正则化。这种设计通过捕获高维潜在表示中的语义结构，缓解了重建-生成之间的权衡。大量实验表明，Semantic-VAE显著提高了合成质量和训练效率。当集成到F5-TTS中时，我们的方法在LibriSpeech-PC上实现了2.10%的单词错误率和0.64的说话人相似性，优于基于梅尔的系统（2.23%，0.60）和普通的声学VAE基线（2.65%，0.59）。我们还发布了代码和模型，以促进进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22167v1">PDF</a> Submitted to ICASSP2026</p>
<p><strong>Summary</strong><br>文本主要探讨了在零文本到语音（TTS）转换中，mel-spectrograms作为中间表示形式的广泛应用及其存在的冗余问题。为解决这一问题，研究者提出了一种新型的VAE框架——Semantic-VAE，它通过在高维潜在空间中进行语义对齐正则化，改善了重建生成之间的权衡问题。实验证明，Semantic-VAE能显著提高合成质量和训练效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mel-spectrograms在TTS中的广泛应用存在冗余问题，导致文本语音对齐学习效率低下。</li>
<li>紧凑型VAE基潜在表示作为一种更强大的替代方案出现，但它们面临优化困境：高维潜在空间可提高重建质量和说话人相似性，但会降低可理解性；低维空间则反之。</li>
<li>Semantic-VAE是一种新型的VAE框架，通过在高维潜在空间中进行语义对齐正则化，解决了重建生成之间的权衡问题。</li>
<li>Semantic-VAE显著提高了合成质量和训练效率。</li>
<li>在LibriSpeech-PC上，将Semantic-VAE集成到F5-TTS中，实现了2.10%的WER（词错误率）和0.64的说话人相似性，优于基于mel的系统（2.23%，0.60）和普通的声学VAE基线（2.65%，0.59）。</li>
<li>研究者公开了代码和模型，以方便进一步的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22167v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22167v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22167v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22167v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22167v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Think-Right-Not-More-Test-Time-Scaling-for-Numerical-Claim-Verification"><a href="#Think-Right-Not-More-Test-Time-Scaling-for-Numerical-Claim-Verification" class="headerlink" title="Think Right, Not More: Test-Time Scaling for Numerical Claim   Verification"></a>Think Right, Not More: Test-Time Scaling for Numerical Claim   Verification</h2><p><strong>Authors:Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand</strong></p>
<p>Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/VenkteshV/VerifierFC">https://github.com/VenkteshV/VerifierFC</a> </p>
<blockquote>
<p>事实核查现实世界中的陈述，特别是数字陈述，本质上是复杂的，需要多步骤推理和数值推理来验证陈述的各个方面。尽管大型语言模型（包括推理模型）已经取得了巨大的进步，但它们仍然不足以核查现实世界中需要结合组合和数值推理的陈述。它们无法理解数字方面的细微差别，也容易出现推理漂移问题，即模型无法根据上下文理解多样化的信息，导致误解和推理过程回溯。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22101v1">PDF</a> Accepted to EMNLP 2025, 19 pages</p>
<p><strong>摘要</strong></p>
<p>针对事实核查中的复杂数值主张，特别是需要多步骤推理和数值推理的主张，尽管大型语言模型（LLM）包括推理模型已取得显著进展，但仍存在不足。本工作系统地探索了利用测试时间计算（TTS）对LLM进行扩展，以应对事实核查复杂数值主张的任务。我们训练了一个验证器模型（VERIFIERFC），以在可能的推理路径中导航并选择可能导致正确结论的路径。观察发现，TTS有助于缓解推理漂移问题，显著提高事实核查数值主张的性能。为提高TTS的计算效率，我们引入了一种自适应机制，根据主张的复杂性有选择地进行TTS。该方法实现了比标准TTS高出1.8倍的计算效率，同时在单次主张验证方法上实现了18.8%的性能提升。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>事实核查，尤其是涉及复杂数值主张的核查，需要多步骤和数值推理。</li>
<li>大型语言模型（LLM）在事实核查任务中仍有局限，难以处理组合了组合和数值推理的主张。</li>
<li>测试时间计算（TTS）有助于缓解推理漂移问题，提高事实核查性能。</li>
<li>VERIFIERFC模型能够在可能的推理路径中导航并选择最佳路径。</li>
<li>自适应TTS机制根据主张的复杂性进行选择，提高了计算效率。</li>
<li>与标准TTS相比，自适应机制实现了更高的效率。</li>
<li>与单次主张验证方法相比，性能提升显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22101v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22101v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Comprehend-and-Talk-Text-to-Speech-Synthesis-via-Dual-Language-Modeling"><a href="#Comprehend-and-Talk-Text-to-Speech-Synthesis-via-Dual-Language-Modeling" class="headerlink" title="Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling"></a>Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling</h2><p><strong>Authors:Junjie Cao, Yichen Han, Ruonan Zhang, Xiaoyang Hao, Hongxiang Li, Shuaijiang Zhao, Yue Liu, Xiao-Ping Zhng</strong></p>
<p>Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an <code>Understand-then-Generate&#39;&#39; dual-Transformer architecture that decouples comprehension from rendering. An initial </code>Understanding’’ Transformer models the cross-modal relationship between text and the audio’s semantic tokens to form a high-level utterance plan. A subsequent &#96;&#96;Generation’’ Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. </p>
<blockquote>
<p>基于大型语言模型（LLM）的自动回归（AR）文本到语音（TTS）系统虽然达到了最先进的品质，但仍然面临关键挑战。基于LLM的范式的基础是将连续的语音波形离散化为一系列离散令牌，通过神经音频编解码器实现。然而，单码本建模非常适合文本LLM，但存在信息损失严重的问题；通过残差向量量化（RVQ）通常产生层次化的音频令牌，但往往缺乏明确的语义结构，给模型带来了沉重的学习负担。此外，自动回归过程本质上容易出错累积，可能会降低生成稳定性。为了解决这些局限性，我们提出了Cat-TTS，这是一个用于稳健和语义基础零射击合成的全新框架。首先，我们引入了S3编解码器，这是一种分裂的RVQ编解码器，它通过来自最先进的ASR模型的语义蒸馏将其主要码本注入显式语言特征，提供结构化表示，从而简化学习任务。其次，我们提出了一种“理解然后生成”的双Transformer架构，该架构将理解与渲染解耦。初始的“理解”Transformer对文本和音频语义令牌之间的跨模态关系进行建模，以形成高级话语计划。随后的“生成”Transformer则执行此计划，通过自动回归的方式合成层次化的音频令牌。最后，为了提高生成的稳定性，我们引入了几乎无需参数的Masked Audio Parallel Inference（MAPI）推理策略，该策略动态指导解码过程，缓解局部错误。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22062v1">PDF</a> conference paper about TTS</p>
<p><strong>摘要</strong></p>
<p>基于大型语言模型（LLM）的自动回归（AR）文本到语音（TTS）系统虽然达到了最先进的水平，但仍面临关键挑战。本文提出CaT-TTS框架，通过引入S3Codec和“理解后生成”的双Transformer架构以及带有Masked Audio Parallel Inference（MAPI）的解码过程，实现了稳健且语义基础的零样本合成。S3Codec将显式语言特征注入主要码簿，提供结构化表示以简化学习任务。“理解后生成”架构将理解和渲染过程解耦，先通过“理解”Transformer建模文本与音频语义标记的跨模态关系，形成高级话语计划，然后通过“生成”Transformer执行该计划，自动回归合成层次化的声学标记。最后，通过引入MAPI策略增强生成稳定性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM-based AR-TTS系统虽达到先进质量，但仍面临信息损失、模型学习负担重和错误累积的问题。</li>
<li>引入S3Codec：采用分裂RVQ码组技术，通过来自先进ASR模型的语义蒸馏注入显式语言特征，提供结构化表示。</li>
<li>提出“理解后生成”双Transformer架构：解耦理解和渲染过程，形成高级话语计划并执行。</li>
<li>引入Masked Audio Parallel Inference (MAPI)：一种几乎无需参数的推理策略，可动态指导解码过程，减少局部错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22062">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22062v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22062v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22062v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.22062v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Redefining-Machine-Simultaneous-Interpretation-From-Incremental-Translation-to-Human-Like-Strategies"><a href="#Redefining-Machine-Simultaneous-Interpretation-From-Incremental-Translation-to-Human-Like-Strategies" class="headerlink" title="Redefining Machine Simultaneous Interpretation: From Incremental   Translation to Human-Like Strategies"></a>Redefining Machine Simultaneous Interpretation: From Incremental   Translation to Human-Like Strategies</h2><p><strong>Authors:Qianen Zhang, Satoshi Nakamura</strong></p>
<p>Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional encoder-decoder policies with only READ&#x2F;WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION and PRONOMINALIZATION, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We implement these actions in a decoder-only large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and latency, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60&#x2F;60 English-Chinese and English-German benchmarks show that our framework consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower delay (measured by Average Lagging) compared to reference translations and salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the best overall balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation. </p>
<blockquote>
<p>同时机器翻译（SiMT）需要在严格的实时约束下实现高质量的翻译，而传统的仅使用READ&#x2F;WRITE操作的编码器-解码器策略无法完全满足这一需求。我们扩展了SiMT的动作空间，增加了四种自适应动作：句子切割（SENTENCE_CUT）、省略（DROP）、部分摘要（PARTIAL_SUMMARIZATION）和代词化（PRONOMINALIZATION），这些动作能够在保持语义忠实度的同时，实现实时重组、省略和简化。我们在仅解码器的大型语言模型（LLM）框架中实现这些动作，并通过动作感知提示构建训练参考。为了评估质量和延迟，我们进一步开发了一个延迟感知的文本转语音（TTS）管道，将文本输出映射为具有现实时间安排的语音。在ACL60&#x2F;60英语-中文和英语-德语基准测试上的实验表明，我们的框架在语义指标（如COMET-KIWI）上持续提高，与参考翻译和基于沙拉的基线相比，实现了更低的延迟（通过平均滞后时间测量）。值得注意的是，结合DROP和SENTENCE_CUT动作在流畅性和延迟之间达到了最佳平衡。这些结果证明，丰富基于LLM的SiMT的动作空间为弥合人机解释之间的差距提供了一个有前途的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21801v1">PDF</a> </p>
<p><strong>Summary</strong><br>     同步机器翻译（SiMT）需要在严格的时间约束下提供高质量的翻译，传统的仅使用READ&#x2F;WRITE动作的编码器-解码器策略无法完全满足这一需求。研究团队扩展了SiMT的动作空间，增加了四种自适应动作：SENTENCE_CUT、DROP、PARTIAL_SUMMARIZATION和PRONOMINALIZATION，这些动作能够在保持语义保真度的同时，实现实时重构、省略和简化。研究团队在仅包含解码器的大型语言模型（LLM）框架中实现这些动作，并通过动作感知提示构建训练参考。为了评估翻译质量和延迟，研究团队进一步开发了一个延迟感知的文本转语音（TTS）管道，将文本输出映射为具有现实时间节奏的语音。在ACL60&#x2F;60英语-中文和英语-德语基准测试上的实验表明，该研究框架在语义指标（如COMET-KIWI）上持续提高，与参考翻译和基于沙拉的基线相比，实现了更低的延迟（通过平均滞后测量）。特别是，结合DROP和SENTENCE_CUT动作在流畅性和延迟之间达到了最佳平衡。这一研究结果表明，丰富LLM基于SiMT的动作空间为缩小人机解释之间的差距提供了有前景的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统编码器-解码器策略无法满足严格实时约束下的高质量翻译需求。</li>
<li>研究团队扩展了SiMT的动作空间，包括SENTENCE_CUT、DROP、PARTIAL_SUMMARIZATION和PRONOMINALIZATION等自适应动作。</li>
<li>这些动作能够在保持语义保真度的同时，实现实时重构、省略和简化。</li>
<li>研究团队在大型语言模型框架中实现这些动作并通过动作感知提示构建训练参考。</li>
<li>为评估翻译质量和延迟，开发了一个延迟感知的TTS管道。</li>
<li>在基准测试上，该框架在语义指标上表现优异，并实现较低延迟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21801v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21801v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21801v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Align2Speak-Improving-TTS-for-Low-Resource-Languages-via-ASR-Guided-Online-Preference-Optimization"><a href="#Align2Speak-Improving-TTS-for-Low-Resource-Languages-via-ASR-Guided-Online-Preference-Optimization" class="headerlink" title="Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided   Online Preference Optimization"></a>Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided   Online Preference Optimization</h2><p><strong>Authors:Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Roy Fejgin, Ryan Langman, Mikyas Desta, Leili Tavabi, Jason Li</strong></p>
<p>Developing high-quality text-to-speech (TTS) systems for low-resource languages is challenging due to the scarcity of paired text and speech data. In contrast, automatic speech recognition (ASR) models for such languages are often more accessible, owing to large-scale multilingual pre-training efforts. We propose a framework based on Group Relative Policy Optimization (GRPO) to adapt an autoregressive, multilingual TTS model to new languages. Our method first establishes a language-agnostic foundation for TTS synthesis by training a multilingual baseline with International Phonetic Alphabet (IPA) tokens. Next, we fine-tune this model on limited paired data of the new languages to capture the target language’s prosodic features. Finally, we apply GRPO to optimize the model using only unpaired text and speaker prompts, guided by a multi-objective reward from pretrained ASR, speaker verification, and audio quality estimation models. Experiments demonstrate that this pipeline produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone. Furthermore, our GRPO-based framework also improves TTS performance in high-resource languages, surpassing offline alignment methods such as Direct Preference Optimization (DPO) yielding superior intelligibility, speaker similarity, and audio quality. </p>
<blockquote>
<p>针对低资源语言的优质文本转语音（TTS）系统发展面临挑战，因为文本和语音数据配对稀缺。相比之下，由于大规模多语种预训练的努力，此类语言的自动语音识别（ASR）模型往往更容易获得。我们提出了一种基于组相对策略优化（GRPO）的框架，以适应新的语言的自回归多语种TTS模型。我们的方法首先通过与国际音标（IPA）符号一起训练多语种基线，为TTS合成建立一种语言无关的基础。接下来，我们在新语言的有限配对数据上对此模型进行微调，以捕获目标语言的韵律特征。最后，我们应用GRPO仅使用未配对的文本和说话人提示来优化模型，由预训练的ASR、说话人验证和音频质量估计模型的多目标奖励来指导。实验表明，该管道在资源贫乏的语言中产生可理解的、与说话人一致的语音，明显优于单纯的微调。此外，我们基于GRPO的框架还提高了高资源语言的TTS性能，超越了直接偏好优化（DPO）等离线对齐方法，产生了更高的可理解性、说话人相似性和音频质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21718v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Group Relative Policy Optimization (GRPO)的方法，用于适应新的低资源语言的文本到语音（TTS）系统。该方法首先建立一个跨语言的TTS合成基础，通过国际音标（IPA）符号训练多语言基线模型。然后，在新语言的有限配对数据上微调此模型，以捕获目标语言的韵律特征。最后，利用GRPO优化模型，仅使用未配对的文本和演讲提示，由预训练的自动语音识别（ASR）、说话人验证和音频质量估计模型的多元奖励引导。实验表明，该管道在资源匮乏的语言中产生可理解和具有说话人特色的语音，明显优于仅进行微调的方法。此外，基于GRPO的框架还提高了高资源语言的TTS性能，超越了如Direct Preference Optimization (DPO)的离线对齐方法，在可理解性、说话人相似性和音频质量方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本提出一种基于GRPO的框架来适应低资源语言的TTS系统。</li>
<li>通过IPA符号建立跨语言的TTS基础。</li>
<li>在新语言的有限数据上微调模型以捕获目标语言的韵律特征。</li>
<li>使用GRPO优化模型，结合ASR、说话人验证和音频质量估计模型的多元奖励。</li>
<li>实验证明该框架在资源匮乏的语言中表现优异，显著提高TTS性能。</li>
<li>该框架同样适用于高资源语言，在可理解性、说话人相似性和音频质量上超越DPO方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21718v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21718v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21718v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21718v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.21718v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS"><a href="#SPADE-Structured-Pruning-and-Adaptive-Distillation-for-Efficient-LLM-TTS" class="headerlink" title="SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS"></a>SPADE: Structured Pruning and Adaptive Distillation for Efficient   LLM-TTS</h2><p><strong>Authors:Tan Dat Nguyen, Jaehun Kim, Ji-Hoon Kim, Shukjae Choi, Youshin Lim, Joon Son Chung</strong></p>
<p>The goal of this paper is to introduce SPADE, a framework for Structured Pruning and Adaptive Distillation for Efficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a pruning step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level knowledge distillation to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to 20%, and achieving up to 1.7x faster real-time factor with less than 5% of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/">https://mm.kaist.ac.kr/projects/SPADE/</a>. </p>
<blockquote>
<p>本文的目标是介绍SPADE，这是一个用于基于大型语言模型的文本到语音（LLM-TTS）的结构化剪枝和自适应蒸馏框架。最近的LLM-TTS系统实现了强大的可控性和零样本泛化能力，但它们的参数数量庞大和延迟较高，限制了其在现实世界中的部署。SPADE通过结合（i）以基于词错误率的层重要性指数为指导的剪枝步骤，以去除非必要的Transformer层，以及（ii）多层次知识蒸馏来恢复自回归一致性，解决了这个问题。在零样本基准测试中，SPADE在保持近乎相当的感知质量的同时，将Transformer的深度减半，将VRAM使用量减少了高达20%，并且使用不到5%的原始训练数据实现了高达1.7倍的实时因子。这些结果表明，紧凑的LLM-TTS模型能够在保持自然度和语音者相似性的同时，实现实用的实时语音生成。音频样本可在<a target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/projects/SPADE/%E6%89%BE%E5%88%B0%E3%80%82">https://mm.kaist.ac.kr/projects/SPADE/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20802v2">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SPADE框架，该框架旨在实现基于大型语言模型（LLM）的文本到语音（TTS）的结构化剪枝和自适应蒸馏。SPADE通过结合（i）基于词错误率的层重要性指数引导的剪枝步骤，以去除非必要的Transformer层，（ii）多层次知识蒸馏，以恢复自回归一致性，来解决大型语言模型的高参数计数和高延迟问题。SPADE能够在零样本基准测试中保持近乎相等的感知质量，同时使Transformer深度减半，将VRAM使用量减少高达20%，并以不到原始训练数据5%的数据实现高达1.7倍的实时因子。结果表明，紧凑的LLM-TTS模型可以在保持自然性和语音相似性的同时，实现实用的实时语音生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPADE是一个针对基于大型语言模型的文本到语音转换（TTS）系统的结构化剪枝和自适应蒸馏框架。</li>
<li>SPADE旨在解决大型语言模型高参数计数和高延迟问题，限制其在现实世界的部署。</li>
<li>通过结合剪枝步骤和多层次知识蒸馏，SPADE能够去除非必要的Transformer层并恢复自回归一致性。</li>
<li>在零样本基准测试中，SPADE在保持近乎相等的感知质量的同时，实现了Transformer深度的减半。</li>
<li>SPADE方法降低了VRAM的使用量，提高了实时语音生成的效率。</li>
<li>使用SPADE框架，可以实现使用少于原始训练数据5%的数据达到高达1.7倍的实时因子。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.20802v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.20802v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.20802v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2509.20802v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech"><a href="#CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech" class="headerlink" title="CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech"></a>CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Thomas Thebaud, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak</strong></p>
<p>Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems. </p>
<blockquote>
<p>近年来，生成式人工智能的最新进展为带风格标注的文本到语音合成（CapTTS）领域带来了显著变革。然而，由于缺少标准化、全面的数据集以及基于CapTTS的下游任务研究有限，因此将CapTTS适应于现实应用仍然具有挑战性。为了解决这些差距，我们引入了CapSpeech，这是一个为一系列CapTTS相关任务设计的新基准测试，包括带有声音事件的风格标注文本到语音合成（CapTTS-SE）、口音标注的TTS（AccCapTTS）、情感标注的TTS（EmoCapTTS）以及用于聊天代理的文本到语音合成（AgentTTS）。CapSpeech包含超过10万个机器标注的音频字幕对和近36万个人工标注的音频字幕对。此外，我们还专门收集并记录了由专业配音演员和经验丰富的音频工程师针对AgentTTS和CapTTS-SE任务的数据集。除了数据集之外，我们还使用自回归和非自回归模型在CapSpeech上进行了全面的实验。我们的结果表明，在各种不同的讲话风格中，语音合成具有很高的保真度和高度可理解性。据我们所知，CapSpeech是目前为止为CapTTS相关任务提供全面标注的最大可用数据集。实验和发现进一步为开发CapTTS系统提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02863v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期生成式人工智能的进展为风格标注文本转语音合成（CapTTS）领域带来了重大变革。然而，由于缺乏标准化综合数据集和对基于CapTTS的下游任务的研究有限，将其应用于实际场景仍具挑战。为解决这些问题，我们推出了CapSpeech，一个为CapTTS相关任务设计的新基准，包括带有声音事件的风格标注文本转语音合成（CapTTS-SE）、口音标注TTS（AccCapTTS）、情感标注TTS（EmoCapTTS）和用于聊天代理的文本转语音合成（AgentTTS）。CapSpeech包含超过10亿机器标注的音频字幕对和近百万人类标注的音频字幕对。此外，我们还收集并记录了专业配音演员和音频工程师针对AgentTTS和CapTTS-SE任务的数据集。在CapSpeech上，我们使用了自回归和非自回归模型进行了全面的实验。结果证明了跨多种说话风格的高保真度和高度可理解的语音合成。据我们所知，CapSpeech是目前最大的为CapTTS相关任务提供全面注释的数据集。实验和发现为开发CapTTS系统提供了宝贵的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式人工智能的进步已经显著影响了风格标注文本转语音合成（CapTTS）领域。</li>
<li>CapSpeech数据集的推出填补了CapTTS相关任务的标准化综合数据集的空白。</li>
<li>CapSpeech包含大量机器和人类标注的音频字幕对，适用于多种CapTTS相关任务。</li>
<li>推出新数据集以支持AgentTTS和CapTTS-SE任务的研究。</li>
<li>全面的实验证明了CapSpeech数据集的高质量和有效性。</li>
<li>实验结果展示了跨多种说话风格的高保真度和高度可理解的语音合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_TTS/2506.02863v2/page_4_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Interactive/2406.13144v6/page_0_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-30  VoiceAssistant-Eval Benchmarking AI Assistants across Listening,   Speaking, and Viewing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-253247c48e5f4fe4758606f516fd9b41~resize:0:q75.jpg?source=1f5c5e47&expiration=1759995016&auth_key=1759995016-0-0-29fe2500fd9b953740a2298b6fb0d6be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-30  Orochi Versatile Biomedical Image Processor
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
