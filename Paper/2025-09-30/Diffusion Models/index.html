<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  RefAM Attention Magnets for Zero-Shot Referral Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.20761v5/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-30-æ›´æ–°"><a href="#2025-09-30-æ›´æ–°" class="headerlink" title="2025-09-30 æ›´æ–°"></a>2025-09-30 æ›´æ–°</h1><h2 id="RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation"><a href="#RefAM-Attention-Magnets-for-Zero-Shot-Referral-Segmentation" class="headerlink" title="RefAM: Attention Magnets for Zero-Shot Referral Segmentation"></a>RefAM: Attention Magnets for Zero-Shot Referral Segmentation</h2><p><strong>Authors:Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele</strong></p>
<p>Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components. </p>
<blockquote>
<p>ç›®å‰å¤§å¤šæ•°æŒ‡ä»£åˆ†å‰²çš„æ–¹æ³•åªæœ‰é€šè¿‡å¯¹å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–ç»„åˆï¼Œæ‰èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œè¿™é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒå’Œæ¶æ„ä¿®æ”¹ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§è§„æ¨¡çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å…¶æˆä¸ºé€šç”¨çš„ç‰¹å¾æå–å™¨çš„ç†æƒ³é€‰æ‹©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒç›´æ¥ä»æ‰©æ•£å˜å‹å™¨ä¸­æå–ç‰¹å¾ã€æ³¨æ„åŠ›åˆ†æ•°ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹æˆ–é¢å¤–çš„è®­ç»ƒã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›ç‰¹æ€§ï¼Œæˆ‘ä»¬æ‰©å±•äº†æ¶µç›–å›¾åƒå’Œè§†é¢‘çš„è§†è§‰è¯­è¨€å®šä½ä»»åŠ¡åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåœç”¨è¯å……å½“æ³¨æ„åŠ›ç£é“ï¼šå®ƒä»¬ç§¯ç´¯äº†å¤šä½™çš„æ³¨æ„åŠ›å¹¶ä¸”å¯ä»¥è¢«è¿‡æ»¤ä»¥å‡å°‘å™ªéŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®šäº†åœ¨è¾ƒæ·±å±‚æ¬¡ä¸­å‡ºç°çš„å…¨å±€æ³¨æ„åŠ›æ±‡ç‚¹ï¼ˆGASï¼‰ï¼Œå¹¶è¡¨æ˜å®ƒä»¬å¯ä»¥è¢«å®‰å…¨åœ°æŠ‘åˆ¶æˆ–é‡å®šå‘åˆ°è¾…åŠ©æ ‡è®°ï¼Œä»è€Œäº§ç”Ÿæ›´æ¸…æ™°ã€æ›´å‡†ç¡®çš„å®šä½å›¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ³¨æ„åŠ›å†åˆ†é…ç­–ç•¥ï¼Œå…¶ä¸­æ·»åŠ çš„åœç”¨è¯å°†èƒŒæ™¯æ¿€æ´»åˆ’åˆ†ä¸ºè¾ƒå°çš„é›†ç¾¤ï¼Œäº§ç”Ÿæ›´æ¸…æ™°ã€æ›´å±€éƒ¨çš„çƒ­å›¾ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†RefAMï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ— éœ€è®­ç»ƒçš„å®šä½æ¡†æ¶ï¼Œå®ƒç»“åˆäº†äº¤å‰æ³¨æ„åŠ›å›¾ã€GASå¤„ç†å’Œå†åˆ†é…ã€‚åœ¨æ— é¢„è®¾å‚ç…§çš„å›¾åƒå’Œè§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œæ— éœ€å¾®è°ƒæˆ–é¢å¤–çš„ç»„ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22650v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://refam-diffusion.github.io/">https://refam-diffusion.github.io/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨æ‰©æ•£å˜å‹å™¨çš„ç‰¹å¾ï¼ˆæ³¨æ„åŠ›å¾—åˆ†ï¼‰è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹æˆ–é¢å¤–çš„è®­ç»ƒã€‚ç ”ç©¶çš„å…³é”®è§è§£æ˜¯åœç”¨è¯ä½œä¸ºæ³¨æ„åŠ›ç£é“ç§¯ç´¯è¿‡å¤šçš„æ³¨æ„åŠ›å¹¶å¯ä»¥è¿‡æ»¤ä»¥å‡å°‘å™ªå£°ã€‚é€šè¿‡æŠ‘åˆ¶æˆ–é‡å®šå‘å…¨å±€æ³¨æ„åŠ›æ±‡ç‚¹ï¼ˆGASï¼‰ï¼Œå¾—åˆ°æ›´æ¸…æ™°å‡†ç¡®çš„å®šä½å›¾ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æœªç»è®­ç»ƒçš„å®šä½æ¡†æ¶RefAMï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›å›¾ã€GASå¤„ç†å’Œé‡æ–°åˆ†é…ï¼Œåœ¨é›¶æ ·æœ¬å¼•ç”¨å›¾åƒå’Œè§†é¢‘åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰å¼•ç”¨åˆ†å‰²æ–¹æ³•å¤§å¤šéœ€è¦é€šè¿‡å¾®è°ƒæˆ–ç»„åˆå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥å®ç°é«˜æ€§èƒ½ï¼Œè¿™é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒå’Œæ¶æ„ä¿®æ”¹ã€‚</li>
<li>å¤§è§„æ¨¡ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ç¼–ç ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä½œä¸ºé€šç”¨ç‰¹å¾æå–å™¨ã€‚</li>
<li>åœç”¨è¯åœ¨æ¨¡å‹ä¸­æ‰®æ¼”æ³¨æ„åŠ›ç£é“è§’è‰²ï¼Œç§¯ç´¯è¿‡å¤šæ³¨æ„åŠ›å¹¶å¯ä»¥è¿‡æ»¤ä»¥å‡å°‘å™ªå£°ã€‚</li>
<li>è¯†åˆ«å‡ºå…¨å±€æ³¨æ„åŠ›æ±‡ç‚¹ï¼ˆGASï¼‰ï¼Œå¯ä»¥é€šè¿‡æŠ‘åˆ¶æˆ–é‡å®šå‘æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›é‡æ–°åˆ†é…ç­–ç•¥ï¼Œé€šè¿‡é™„åŠ åœç”¨è¯å°†èƒŒæ™¯æ¿€æ´»åˆ†æˆè¾ƒå°çš„é›†ç¾¤ï¼Œäº§ç”Ÿæ›´æ¸…æ™°ã€æ›´å±€éƒ¨åŒ–çš„çƒ­å›¾ã€‚</li>
<li>åŸºäºä¸Šè¿°å‘ç°ï¼Œå¼€å‘äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å®šä½æ¡†æ¶RefAMï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äº¤å‰æ³¨æ„åŠ›å›¾ã€GASå¤„ç†å’Œé‡æ–°åˆ†é…ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22650v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22650v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22650v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22650v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22650v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Training-Free-Synthetic-Data-Generation-with-Dual-IP-Adapter-Guidance"><a href="#Training-Free-Synthetic-Data-Generation-with-Dual-IP-Adapter-Guidance" class="headerlink" title="Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance"></a>Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance</h2><p><strong>Authors:Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour, Vicky Kalogeiton</strong></p>
<p>Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks. </p>
<blockquote>
<p>å›¾åƒåˆ†ç±»çš„å°‘é‡æ ·æœ¬ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå¯ç”¨çš„æ ‡è®°æ ·æœ¬æœ‰é™ã€‚æœ€è¿‘çš„æ–¹æ³•å·²ç»å°è¯•ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ï¼Œä½†è¿™é€šå¸¸éœ€è¦å¤§é‡çš„æ¨¡å‹å¾®è°ƒæˆ–å¤–éƒ¨ä¿¡æ¯æºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºDIPSYï¼Œå®ƒåˆ©ç”¨IP-Adapterè¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œä»…ä½¿ç”¨å¯ç”¨çš„å°‘é‡æ ·æœ¬ç”Ÿæˆé«˜åº¦åˆ¤åˆ«æ€§çš„åˆæˆå›¾åƒã€‚DIPSYå¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§æ‰©å±•çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ¡ˆï¼Œå®ç°å¯¹æ­£è´Ÿå›¾åƒæ¡ä»¶çš„ç‹¬ç«‹æ§åˆ¶ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºç±»åˆ«ç›¸ä¼¼æ€§çš„é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºè¯†åˆ«æœ‰æ•ˆçš„å¯¹æ¯”ç¤ºä¾‹ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æµç¨‹ï¼Œæ— éœ€æ¨¡å‹å¾®è°ƒæˆ–å¤–éƒ¨å­—å¹•æ ‡æ³¨å’Œè¿‡æ»¤ã€‚åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³æˆ–ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¯¹ç”Ÿæˆæ¨¡å‹é€‚åº”æ€§çš„éœ€æ±‚æˆ–å¯¹ç”Ÿæˆå­—å¹•å’Œå›¾åƒè¿‡æ»¤çš„å¤–éƒ¨å·¥å…·çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åˆ©ç”¨å…·æœ‰æ­£è´Ÿå¼•å¯¼çš„åŒå‘å›¾åƒæç¤ºç”Ÿæˆç±»åˆ«åˆ¤åˆ«ç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç²¾ç»†åˆ†ç±»ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22635v1">PDF</a> BMVC 2025. Project page:   <a target="_blank" rel="noopener" href="https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/">https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„æ–°æ–¹æ³•DIPSYï¼Œåˆ©ç”¨IP-Adapterç”Ÿæˆé«˜åº¦åŒºåˆ†æ€§çš„åˆæˆå›¾åƒï¼Œä»…ä½¿ç”¨æœ‰é™çš„æ ·æœ¬å³å¯å®Œæˆå°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚DIPSYå¼•å…¥ä¸‰é¡¹å…³é”®æŠ€æœ¯ï¼šæ‰©å±•çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ¡ˆï¼ŒåŸºäºç±»åˆ«ç›¸ä¼¼åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œä»¥åŠæ— éœ€æ¨¡å‹å¾®è°ƒæˆ–å¤–éƒ¨æè¿°çš„ç®€å•æœ‰æ•ˆæµç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æˆ–ä¼˜äºç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œæ— éœ€å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œé€‚åº”æˆ–ä¾èµ–å¤–éƒ¨å·¥å…·è¿›è¡Œæè¿°ç”Ÿæˆå’Œå›¾åƒè¿‡æ»¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬å›¾åƒåˆ†ç±»æ–¹æ³•DIPSYï¼ŒåŸºäºå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æŠ€æœ¯IP-Adapterã€‚</li>
<li>DIPSYåˆ©ç”¨æœ‰é™çš„æ ·æœ¬ç”Ÿæˆé«˜åº¦åŒºåˆ†æ€§çš„åˆæˆå›¾åƒã€‚</li>
<li>å¼•å…¥æ‰©å±•çš„æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ¡ˆï¼Œå®ç°å¯¹æ­£è´Ÿé¢å›¾åƒæ¡ä»¶çš„ç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨åŸºäºç±»åˆ«ç›¸ä¼¼åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆé€‰æ‹©å¯¹æ¯”æ ·æœ¬ã€‚</li>
<li>æµç¨‹ç®€å•æœ‰æ•ˆï¼Œæ— éœ€æ¨¡å‹å¾®è°ƒæˆ–å¤–éƒ¨æè¿°ã€è¿‡æ»¤å·¥å…·ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22635v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22635v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22635v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Overclocking-Electrostatic-Generative-Models"><a href="#Overclocking-Electrostatic-Generative-Models" class="headerlink" title="Overclocking Electrostatic Generative Models"></a>Overclocking Electrostatic Generative Models</h2><p><strong>Authors:Daniil Shlenskii, Alexander Korotin</strong></p>
<p>Electrostatic generative models such as PFGM++ have recently emerged as a powerful framework, achieving state-of-the-art performance in image synthesis. PFGM++ operates in an extended data space with auxiliary dimensionality $D$, recovering the diffusion model framework as $D\to\infty$, while yielding superior empirical results for finite $D$. Like diffusion models, PFGM++ relies on expensive ODE simulations to generate samples, making it computationally costly. To address this, we propose Inverse Poisson Flow Matching (IPFM), a novel distillation framework that accelerates electrostatic generative models across all values of $D$. Our IPFM reformulates distillation as an inverse problem: learning a generator whose induced electrostatic field matches that of the teacher. We derive a tractable training objective for this problem and show that, as $D \to \infty$, our IPFM closely recovers Score Identity Distillation (SiD), a recent method for distilling diffusion models. Empirically, our IPFM produces distilled generators that achieve near-teacher or even superior sample quality using only a few function evaluations. Moreover, we observe that distillation converges faster for finite $D$ than in the $D \to \infty$ (diffusion) limit, which is consistent with prior findings that finite-$D$ PFGM++ models exhibit more favorable optimization and sampling properties. </p>
<blockquote>
<p>é™ç”µç”Ÿæˆæ¨¡å‹å¦‚PFGM++æœ€è¿‘ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶å‡ºç°ï¼Œåœ¨å›¾åƒåˆæˆä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚PFGM++åœ¨ä¸€ä¸ªæ‰©å±•çš„æ•°æ®ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå…·æœ‰è¾…åŠ©ç»´åº¦Dï¼Œåœ¨Dè¶‹å‘æ— ç©·å¤§æ—¶æ¢å¤æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè€Œå¯¹äºæœ‰é™çš„Dåˆ™äº§ç”Ÿä¼˜è¶Šçš„å®è¯ç»“æœã€‚ä¸æ‰©æ•£æ¨¡å‹ä¸€æ ·ï¼ŒPFGM++ä¾èµ–äºæ˜‚è´µçš„ODEæ¨¡æ‹Ÿæ¥ç”Ÿæˆæ ·æœ¬ï¼Œä½¿å…¶è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€†PoissonæµåŒ¹é…ï¼ˆIPFMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œå¯ä»¥åŠ é€Ÿæ‰€æœ‰Då€¼ä¸‹çš„é™ç”µç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„IPFMå°†è’¸é¦é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªåé—®é¢˜ï¼šå­¦ä¹ ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå…¶äº§ç”Ÿçš„é™ç”µåœºä¸æ•™å¸ˆçš„é™ç”µåœºç›¸åŒ¹é…ã€‚æˆ‘ä»¬ä¸ºè¿™ä¸ªé—®é¢˜æ¨å¯¼äº†ä¸€ä¸ªå¯è¡Œçš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶è¡¨æ˜å½“Dè¶‹å‘æ— ç©·å¤§æ—¶ï¼Œæˆ‘ä»¬çš„IPFMå‡ ä¹å¯ä»¥æ¢å¤åˆ†æ•°èº«ä»½è’¸é¦ï¼ˆSiDï¼‰â€”â€”ä¸€ç§ç”¨äºè’¸é¦æ‰©æ•£æ¨¡å‹çš„æœ€æ–°æ–¹æ³•ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„IPFMäº§ç”Ÿçš„è’¸é¦ç”Ÿæˆå™¨åªä½¿ç”¨å°‘æ•°å‡ æ¬¡å‡½æ•°è¯„ä¼°å°±èƒ½è¾¾åˆ°æ¥è¿‘æ•™å¸ˆç”šè‡³æ›´å¥½çš„æ ·æœ¬è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¯¹äºæœ‰é™çš„Dï¼Œè’¸é¦çš„æ”¶æ•›é€Ÿåº¦æ¯”Dè¶‹å‘æ— ç©·å¤§ï¼ˆæ‰©æ•£ï¼‰æé™æ—¶æ›´å¿«ï¼Œè¿™ä¸å…ˆå‰çš„å‘ç°ä¸€è‡´ï¼Œå³æœ‰é™Dçš„PFGM++æ¨¡å‹è¡¨ç°å‡ºæ›´æœ‰åˆ©çš„ä¼˜åŒ–å’Œé‡‡æ ·ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22454v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é™ç”µç”Ÿæˆæ¨¡å‹å¦‚PFGM++å·²æˆä¸ºå›¾åƒåˆæˆé¢†åŸŸçš„æœ€å‰æ²¿æ¡†æ¶ã€‚ä¸ºä¼˜åŒ–å…¶è®¡ç®—æˆæœ¬ï¼Œæå‡ºä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶â€”â€”Inverse Poisson Flow Matchingï¼ˆIPFMï¼‰ï¼Œå°†è’¸é¦é‡æ„ä¸ºé€†é—®é¢˜ï¼šå­¦ä¹ ä¸€ä¸ªå…¶æ„Ÿåº”é™ç”µåœºä¸æ•™å¸ˆç›¸åŒ¹é…çš„ç”Ÿæˆå™¨ã€‚å½“$D$è¶‹å‘æ— ç©·æ—¶ï¼ŒIPFMæ¥è¿‘äºScore Identity Distillationï¼ˆSiDï¼‰ï¼Œä¸€ç§ç”¨äºè’¸é¦æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚ç»éªŒè¡¨æ˜ï¼ŒIPFMç”Ÿæˆçš„è’¸é¦å™¨æ ·æœ¬è´¨é‡æ¥è¿‘æ•™å¸ˆç”šè‡³æ›´ä¼˜ï¼Œä»…ä½¿ç”¨å°‘é‡å‡½æ•°è¯„ä¼°ã€‚æ­¤å¤–ï¼Œå¯¹äºæœ‰é™$D$çš„è’¸é¦æ”¶æ•›é€Ÿåº¦æ¯”$D$è¶‹å‘æ— ç©·æ—¶æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é™ç”µç”Ÿæˆæ¨¡å‹å¦‚PFGM++åœ¨å›¾åƒåˆæˆé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>PFGM++åœ¨æ‰©å±•æ•°æ®ç©ºé—´ä¸­ä½¿ç”¨è¾…åŠ©ç»´åº¦$D$ï¼Œå¹¶åœ¨$D$è¶‹å‘æ— ç©·æ—¶æ¢å¤æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>ä¸ºä¼˜åŒ–è®¡ç®—æˆæœ¬ï¼Œæå‡ºäº†Inverse Poisson Flow Matchingï¼ˆIPFMï¼‰è’¸é¦æ¡†æ¶ã€‚</li>
<li>IPFMå°†è’¸é¦é‡æ„ä¸ºé€†é—®é¢˜ï¼Œå­¦ä¹ æ„Ÿåº”é™ç”µåœºä¸æ•™å¸ˆç›¸åŒ¹é…çš„ç”Ÿæˆå™¨ã€‚</li>
<li>å½“$D$è¶‹å‘æ— ç©·æ—¶ï¼ŒIPFMæ¥è¿‘Score Identity Distillationï¼ˆSiDï¼‰ã€‚</li>
<li>IPFMç”Ÿæˆçš„è’¸é¦å™¨æ ·æœ¬è´¨é‡æ¥è¿‘ç”šè‡³ä¼˜äºæ•™å¸ˆæ¨¡å‹ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„å‡½æ•°è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22454v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models"><a href="#HiGS-History-Guided-Sampling-for-Plug-and-Play-Enhancement-of-Diffusion-Models" class="headerlink" title="HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion   Models"></a>HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion   Models</h2><p><strong>Authors:Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber</strong></p>
<p>While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶è¾“å‡ºä»ç„¶å¯èƒ½å‡ºç°ä¸çœŸå®å’Œç¼ºä¹ç»†èŠ‚çš„æƒ…å†µï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°‘çš„ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰æˆ–è¾ƒä½çš„æŒ‡å¯¼å°ºåº¦æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨é‡é‡‡æ ·æŠ€æœ¯çš„æ–°å‹é‡‡æ ·æ–¹æ³•ï¼Œç§°ä¸ºå†å²å¼•å¯¼é‡‡æ ·ï¼ˆHiGSï¼‰ã€‚å®ƒé€šè¿‡é›†æˆæœ€æ–°çš„æ¨¡å‹é¢„æµ‹ç»“æœåˆ°æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­ï¼Œæé«˜äº†æ‰©æ•£é‡‡æ ·çš„è´¨é‡å’Œæ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒHiGSåˆ©ç”¨å½“å‰é¢„æµ‹ä¸è¿‡å»é¢„æµ‹åŠ æƒå¹³å‡å€¼ä¹‹é—´çš„å·®å¼‚æ¥å¼•å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶äº§ç”Ÿæ›´çœŸå®ã€ç»†èŠ‚å’Œç»“æ„æ›´å¥½çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡ ä¹æ²¡æœ‰å¼•å…¥é¢å¤–çš„è®¡ç®—é‡ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¡†æ¶ä¸­ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHiGSåœ¨å¤šç§æ¨¡å‹å’Œæ¶æ„ä¸‹ä»¥åŠä¸åŒçš„é‡‡æ ·é¢„ç®—å’ŒæŒ‡å¯¼å°ºåº¦ä¸‹éƒ½èƒ½æŒç»­æé«˜å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„SiTæ¨¡å‹ï¼ŒHiGSåœ¨ä¸ä½¿ç”¨æŒ‡å¯¼çš„æƒ…å†µä¸‹å®ç°äº†ImageNetç”Ÿæˆçš„æ–°æœ€ä½³FIDåˆ†æ•°ä¸º1.61ï¼Œåˆ†è¾¨ç‡ä¸º256x256ï¼Œä»…ä½¿ç”¨30ä¸ªé‡‡æ ·æ­¥éª¤ï¼ˆè€Œä¸æ˜¯æ ‡å‡†çš„250ä¸ªï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºHiGSä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„å¢å¼ºç‰ˆæ ‡å‡†æ‰©æ•£é‡‡æ ·æ–¹æ³•ï¼Œå¯å®ç°æ›´å¿«ã€æ›´é«˜ä¿çœŸåº¦çš„ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22300v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå†å²å¼•å¯¼é‡‡æ ·ï¼ˆHiGSï¼‰çš„æ–°å‹åŠ¨é‡é‡‡æ ·æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£é‡‡æ ·çš„è´¨é‡å’Œæ•ˆç‡ã€‚HiGSé€šè¿‡å°†å½“å‰é¢„æµ‹ä¸è¿‡å»é¢„æµ‹çš„åŠ æƒå¹³å‡ä¹‹é—´çš„å·®å¼‚çº³å…¥è€ƒè™‘ï¼Œå¼•å¯¼é‡‡æ ·è¿‡ç¨‹ç”Ÿæˆæ›´çœŸå®ã€ç»†èŠ‚å’Œç»“æ„æ›´å¥½çš„å›¾åƒã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„è®¡ç®—ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¡†æ¶ä¸­ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒHiGSåœ¨ä¸åŒæ¨¡å‹å’Œæ¶æ„ä¸‹ï¼Œä»¥åŠä¸åŒçš„é‡‡æ ·é¢„ç®—å’Œå¼•å¯¼å°ºåº¦ä¸‹ï¼Œéƒ½èƒ½æé«˜å›¾åƒè´¨é‡ã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„SiTæ¨¡å‹æ—¶ï¼ŒHiGSåœ¨åªæœ‰30ä¸ªé‡‡æ ·æ­¥éª¤çš„æƒ…å†µä¸‹å®ç°äº†æœªå¼•å¯¼ImageNetç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯æŒ‡æ ‡çš„FID 1.61ï¼ˆè€Œæ ‡å‡†æ­¥éª¤ä¸º250ï¼‰ã€‚å› æ­¤ï¼ŒHiGSè¢«æå‡ºä½œä¸ºä¸€ç§æ ‡å‡†çš„æ‰©æ•£é‡‡æ ·çš„æ’ä»¶å¢å¼ºåŠŸèƒ½ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿«ã€æ›´é«˜ä¿çœŸåº¦çš„ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HiGSæ˜¯ä¸€ç§åŸºäºåŠ¨é‡çš„é‡‡æ ·æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>HiGSé€šè¿‡å°†å½“å‰é¢„æµ‹ä¸è¿‡å»é¢„æµ‹çš„åŠ æƒå¹³å‡ä¹‹é—´çš„å·®å¼‚çº³å…¥è€ƒè™‘ï¼Œå¼•å¯¼é‡‡æ ·è¿‡ç¨‹ç”Ÿæˆæ›´çœŸå®çš„å›¾åƒã€‚</li>
<li>HiGSæ–¹æ³•æ— éœ€é¢å¤–çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¡†æ¶ä¸­ã€‚</li>
<li>HiGSåœ¨å¤šç§æ¨¡å‹å’Œæ¶æ„ä¸‹éƒ½èƒ½æé«˜å›¾åƒè´¨é‡ï¼Œé€‚ç”¨äºä¸åŒçš„é‡‡æ ·é¢„ç®—å’Œå¼•å¯¼å°ºåº¦ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„SiTæ¨¡å‹ï¼ŒHiGSåœ¨åªæœ‰30ä¸ªé‡‡æ ·æ­¥éª¤çš„æƒ…å†µä¸‹å®ç°äº†æœªå¼•å¯¼ImageNetç”Ÿæˆçš„FID 1.61ã€‚</li>
<li>HiGSæé«˜äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦å¹¶æå‡äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22300v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22300v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing"><a href="#FlashEdit-Decoupling-Speed-Structure-and-Semantics-for-Precise-Image-Editing" class="headerlink" title="FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing"></a>FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image   Editing</h2><p><strong>Authors:Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit">https://github.com/JunyiWuCode/FlashEdit</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†æ˜¾è‘—çš„å“è´¨ï¼Œä½†ä»ç„¶å­˜åœ¨å»¶è¿Ÿé—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„è¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FlashEditè¿™ä¸€å…¨æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå®æ—¶å›¾åƒç¼–è¾‘ã€‚å…¶é«˜æ•ˆç‡æ¥æºäºä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯ä¸€æ­¥å¼åè½¬å’Œç¼–è¾‘ï¼ˆOSIEï¼‰ç®¡é“ï¼Œå®ƒç»•è¿‡äº†æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ï¼›äºŒæ˜¯èƒŒæ™¯å±è”½ï¼ˆBG-Shieldï¼‰æŠ€æœ¯ï¼Œå®ƒé€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾æ¥ä¿è¯èƒŒæ™¯ä¿ç•™ï¼›ä¸‰æ˜¯ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›ï¼ˆSSCAï¼‰æœºåˆ¶ï¼Œå®ƒé€šè¿‡æŠ‘åˆ¶è¯­ä¹‰æ³„éœ²åˆ°èƒŒæ™¯æ¥ç¡®ä¿ç²¾ç¡®ã€å±€éƒ¨åŒ–çš„ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlashEditåœ¨ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°0.2ç§’å†…å®Œæˆç¼–è¾‘ï¼Œç›¸è¾ƒäºä¹‹å‰çš„å¤šæ­¥éª¤æ–¹æ³•å®ç°äº†è¶…è¿‡150å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunyiWuCode/FlashEditä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22244v1">PDF</a> Our code will be made publicly available at   <a target="_blank" rel="noopener" href="https://github.com/JunyiWuCode/FlashEdit">https://github.com/JunyiWuCode/FlashEdit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶FlashEditï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå®æ—¶å›¾åƒç¼–è¾‘ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºç»•è¿‡æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ï¼Œé‡‡ç”¨ä¸€æ­¥åè½¬ç¼–è¾‘ç®¡é“ã€èƒŒæ™¯å±è”½æŠ€æœ¯å’Œç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿ç²¾ç¡®å±€éƒ¨ç¼–è¾‘åŒæ—¶ä¿æŒèƒŒæ™¯ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒFlashEditåœ¨ä¿æŒèƒŒæ™¯å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œç¼–è¾‘é€Ÿåº¦è¾¾åˆ°æ¯ç§’0.2å¸§ä»¥å†…ï¼Œç›¸æ¯”ä¹‹å‰çš„å¤šæ­¥éª¤æ–¹æ³•å®ç°äº†è¶…è¿‡150å€çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashEditæ¡†æ¶å®ç°äº†é«˜ä¿çœŸå®æ—¶å›¾åƒç¼–è¾‘ã€‚</li>
<li>é€šè¿‡ä¸€æ­¥åè½¬ç¼–è¾‘ç®¡é“ï¼ˆOSIEï¼‰æé«˜äº†æ•ˆç‡ï¼Œç»•è¿‡æ˜‚è´µçš„è¿­ä»£è¿‡ç¨‹ã€‚</li>
<li>èƒŒæ™¯å±è”½æŠ€æœ¯ï¼ˆBG-Shieldï¼‰ä¿è¯äº†èƒŒæ™¯çš„ä¸€è‡´æ€§ï¼Œåªé€‰æ‹©æ€§åœ°ä¿®æ”¹ç¼–è¾‘åŒºåŸŸå†…çš„ç‰¹å¾ã€‚</li>
<li>ç¨€ç–ç©ºé—´äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSSCAï¼‰ç¡®ä¿äº†ç²¾ç¡®ã€å±€éƒ¨çš„ç¼–è¾‘ï¼ŒæŠ‘åˆ¶äº†è¯­ä¹‰æ³„éœ²åˆ°èƒŒæ™¯ã€‚</li>
<li>å®éªŒè¯æ˜FlashEditåœ¨ä¿æŒèƒŒæ™¯å’Œç»“æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå®ç°äº†å¿«é€Ÿç¼–è¾‘ã€‚</li>
<li>FlashEditç¼–è¾‘é€Ÿåº¦è¾¾åˆ°æ¯ç§’0.2å¸§ä»¥å†…ï¼Œç›¸æ¯”ä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22244v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22244v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22244v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22244v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpecXNet-A-Dual-Domain-Convolutional-Network-for-Robust-Deepfake-Detection"><a href="#SpecXNet-A-Dual-Domain-Convolutional-Network-for-Robust-Deepfake-Detection" class="headerlink" title="SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake   Detection"></a>SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake   Detection</h2><p><strong>Authors:Inzamamul Alam, Md Tanvir Islam, Simon S. Woo</strong></p>
<p>The increasing realism of content generated by GANs and diffusion models has made deepfake detection significantly more challenging. Existing approaches often focus solely on spatial or frequency-domain features, limiting their generalization to unseen manipulations. We propose the Spectral Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)} decomposes features into a local spatial branch for capturing texture-level anomalies and a global spectral branch that employs Fast Fourier Transform to model periodic inconsistencies. This dual-domain formulation allows SpecXNet to jointly exploit localized detail and global structural coherence, which are critical for distinguishing authentic from manipulated images. We also introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically fuses spatial and spectral features in a content-aware manner. Built atop a modified XceptionNet backbone, we embed the DDFC and DFA modules within a separable convolution block. Extensive experiments on multiple deepfake benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly under cross-dataset and unseen manipulation scenarios, while maintaining real-time feasibility. Our results highlight the effectiveness of unified spatial-spectral learning for robust and generalizable deepfake detection. To ensure reproducibility, we released the full code on \href{<a target="_blank" rel="noopener" href="https://github.com/inzamamulDU/SpecXNet%7D%7B/textcolor%7Bblue%7D%7B/textbf%7BGitHub%7D%7D%7D">https://github.com/inzamamulDU/SpecXNet}{\textcolor{blue}{\textbf{GitHub}}}</a>. </p>
<blockquote>
<p>ç”±GANå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å†…å®¹æ—¥ç›Šé€¼çœŸçš„ç°å®æƒ…å†µä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´æ›´å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä¸“æ³¨äºç©ºé—´æˆ–é¢‘åŸŸç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æœªè§æ“ä½œçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è°±äº¤å‰æ³¨æ„åŠ›ç½‘ç»œï¼ˆSpecXNetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨³å¥æ·±åº¦ä¼ªé€ æ£€æµ‹çš„åŒåŸŸæ¶æ„ã€‚å…¶æ ¸å¿ƒ<strong>åŒåŸŸç‰¹å¾è€¦åˆå™¨ï¼ˆDDFCï¼‰</strong>å°†ç‰¹å¾åˆ†è§£ä¸ºå±€éƒ¨ç©ºé—´åˆ†æ”¯ï¼Œç”¨äºæ•æ‰çº¹ç†çº§åˆ«çš„å¼‚å¸¸ï¼Œå’Œå…¨å±€è°±åˆ†æ”¯ï¼Œè¯¥åˆ†æ”¯åˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢æ¥æ¨¡æ‹Ÿå‘¨æœŸæ€§ä¸ä¸€è‡´ã€‚è¿™ç§åŒåŸŸå…¬å¼ä½¿SpecXNetèƒ½å¤Ÿå…±åŒåˆ©ç”¨å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„ä¸€è‡´æ€§ï¼Œè¿™å¯¹äºåŒºåˆ†çœŸå®å›¾åƒå’Œæ“çºµè¿‡çš„å›¾åƒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†<strong>åŒå‚…é‡Œå¶æ³¨æ„åŠ›ï¼ˆDFAï¼‰</strong>æ¨¡å—ï¼Œå®ƒä»¥å†…å®¹æ„ŸçŸ¥çš„æ–¹å¼åŠ¨æ€èåˆç©ºé—´å’Œè°±ç‰¹å¾ã€‚å»ºç«‹åœ¨ä¿®æ”¹åçš„XceptionNetä¸»å¹²ä¹‹ä¸Šï¼Œæˆ‘ä»¬åœ¨å¯åˆ†ç¦»å·ç§¯å—ä¸­åµŒå…¥äº†DDFCå’ŒDFAæ¨¡å—ã€‚åœ¨å¤šä¸ªæ·±åº¦ä¼ªé€ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSpecXNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ•°æ®é›†å’Œæœªè§æ“ä½œåœºæ™¯ä¸­ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶å¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ç»Ÿä¸€çš„ç©ºé—´-è°±å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå¯å®ç°ç¨³å¥ä¸”å¯æ³›åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ä¸ºç¡®ä¿å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬å·²åœ¨GitHubä¸Šå‘å¸ƒäº†å®Œæ•´çš„ä»£ç ï¼š[é“¾æ¥]ï¼ˆå…·ä½“é“¾æ¥éœ€æ›¿æ¢ä¸ºå®é™…çš„GitHubé“¾æ¥åœ°å€ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22070v1">PDF</a> ACM MM Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†æå‡ºçš„Spectral Cross-Attentional Networkï¼ˆSpecXNetï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒåŸŸæ¶æ„ï¼Œé€šè¿‡Dual-Domain Feature Couplerï¼ˆDDFCï¼‰åˆ†è§£ç‰¹å¾ï¼Œå¹¶ç»“åˆDual Fourier Attentionï¼ˆDFAï¼‰æ¨¡å—åŠ¨æ€èåˆç©ºé—´ä¸å…‰è°±ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼ŒSpecXNetåœ¨å¤šä¸ªæ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ•°æ®é›†å’Œæœªè§æ“ä½œåœºæ™¯ä¸‹ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANså’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å†…å®¹è¶Šæ¥è¶ŠçœŸå®ï¼Œä½¿å¾—æ·±åº¦ä¼ªé€ æ£€æµ‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´æˆ–é¢‘ç‡åŸŸç‰¹å¾ï¼Œä½†éš¾ä»¥æ¨å¹¿åˆ°æœªè§æ“ä½œã€‚</li>
<li>SpecXNetæ¨¡å‹æ˜¯ä¸€ä¸ªåŒåŸŸæ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºæ•æ‰çº¹ç†å¼‚å¸¸çš„å±€éƒ¨ç©ºé—´åˆ†æ”¯å’Œåˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢å»ºæ¨¡å‘¨æœŸæ€§ä¸ä¸€è‡´æ€§çš„å…¨å±€å…‰è°±åˆ†æ”¯ã€‚</li>
<li>Dual-Domain Feature Coupler (DDFC) æ˜¯æ ¸å¿ƒæ¨¡å—ï¼Œå…è®¸æ¨¡å‹åŒæ—¶åˆ©ç”¨å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥çš„Dual Fourier Attentionï¼ˆDFAï¼‰æ¨¡å—ä»¥å†…å®¹æ„ŸçŸ¥çš„æ–¹å¼åŠ¨æ€èåˆç©ºé—´å’Œå…‰è°±ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSpecXNetåœ¨å¤šä¸ªæ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è·¨æ•°æ®é›†å’Œæœªè§æ“ä½œåœºæ™¯ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22070v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22070v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.22070v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FailureAtlas-Mapping-the-Failure-Landscape-of-T2I-Models-via-Active-Exploration"><a href="#FailureAtlas-Mapping-the-Failure-Landscape-of-T2I-Models-via-Active-Exploration" class="headerlink" title="FailureAtlas:Mapping the Failure Landscape of T2I Models via Active   Exploration"></a>FailureAtlas:Mapping the Failure Landscape of T2I Models via Active   Exploration</h2><p><strong>Authors:Muxi Chen, Zhaohua Zhang, Chenchen Zhao, Mingyang Chen, Wenyu Jiang, Tianwen Jiang, Jianhuan Zhuo, Yu Tang, Qiuyong Xiao, Jihong Zhang, Qiang Xu</strong></p>
<p>Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at <a target="_blank" rel="noopener" href="https://github.com/cure-lab/FailureAtlas">https://github.com/cure-lab/FailureAtlas</a> </p>
<blockquote>
<p>é™æ€åŸºå‡†æµ‹è¯•ä¸ºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„æ¯”è¾ƒæä¾›äº†å®è´µçš„åŸºçŸ³ã€‚ç„¶è€Œï¼Œå…¶è¢«åŠ¨è®¾è®¡æä¾›äº†æœ‰é™çš„è¯Šæ–­èƒ½åŠ›ï¼Œéš¾ä»¥æ­ç¤ºæ•´ä¸ªç³»ç»Ÿæ•…éšœæ™¯è§‚æˆ–éš”ç¦»å…¶æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬ä¸»å¼ ä¸€ç§äº’è¡¥çš„æ–¹æ³•ï¼šä¸»åŠ¨æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†FailureAtlasï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è‡ªä¸»æ¢ç´¢å’Œå¤§è§„æ¨¡æ˜ å°„T2Iæ¨¡å‹åºå¤§æ•…éšœæ™¯è§‚çš„æ¡†æ¶ã€‚FailureAtlaså°†é”™è¯¯å‘ç°æ„å»ºä¸ºé’ˆå¯¹æœ€å°å¤±è´¥è¯±å¯¼æ¦‚å¿µçš„ç»“æ„åŒ–æœç´¢ã€‚è™½ç„¶è¿™æ˜¯ä¸€ä¸ªè®¡ç®—çˆ†ç‚¸æ€§çš„é—®é¢˜ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨æ–°å‹åŠ é€ŸæŠ€æœ¯ä½¿å…¶æ˜“äºå¤„ç†ã€‚å½“åº”ç”¨äºStable Diffusionæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ­ç¤ºäº†æ•°åä¸‡ä¹‹å‰æœªçŸ¥çš„è¯¯å·®åˆ‡ç‰‡ï¼ˆä»…åœ¨SD1.5ä¸­å°±è¶…è¿‡247,000ä¸ªï¼‰ï¼Œå¹¶é¦–æ¬¡æä¾›äº†å¤§è§„æ¨¡è¯æ®è¡¨æ˜è¿™äº›å¤±è´¥ä¸è®­ç»ƒé›†ä¸­çš„æ•°æ®ç¨€ç¼ºæœ‰å…³ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæœ‰åŸåˆ™ä¸”å¯æ‰©å±•çš„å¼•æ“æ¥è¿›è¡Œæ·±åº¦æ¨¡å‹å®¡è®¡ï¼ŒFailureAtlaså»ºç«‹äº†ä¸€ç§ä»¥è¯Šæ–­ä¸ºå…ˆçš„æ–°æ–¹æ³•æ¥æŒ‡å¯¼å¼€å‘æ›´ç¨³å¥çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cure-lab/FailureAtlas%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cure-lab/FailureAtlasæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºé™æ€åŸºå‡†æµ‹è¯•ä¸ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¯”è¾ƒæä¾›äº†æœ‰ä»·å€¼çš„åŸºç¡€ï¼Œä½†å…¶è¢«åŠ¨è®¾è®¡å¯¼è‡´è¯Šæ–­èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥æ­ç¤ºæ•´ä½“çš„ç³»ç»Ÿæ•…éšœæˆ–å®šä½å…¶æ ¹æœ¬åŸå› ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§äº’è¡¥çš„æ–¹æ³•ï¼šä¸»åŠ¨æ¢ç´¢ï¼Œå¹¶ä»‹ç»äº†FailureAtlasæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è‡ªä¸»æ¢ç´¢å’Œå¤§è§„æ¨¡æ˜ å°„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ•…éšœæ™¯è§‚ã€‚FailureAtlaså°†é”™è¯¯å‘ç°å®šä¹‰ä¸ºå¯»æ‰¾æœ€å°çš„æ•…éšœè¯±å¯¼æ¦‚å¿µçš„ç»“æ„åŒ–æœç´¢ã€‚ä½œè€…é‡‡ç”¨æ–°çš„åŠ é€ŸæŠ€æœ¯ä½¿è¿™ä¸€è®¡ç®—å¯†é›†å‹é—®é¢˜å˜å¾—å¯è¡Œã€‚åœ¨åº”ç”¨äºStable Diffusionæ¨¡å‹æ—¶ï¼Œè¯¥æ–¹æ³•å‘ç°äº†æ•°åä¸‡ä¸ªä¹‹å‰æœªçŸ¥çš„é”™è¯¯åˆ‡ç‰‡ï¼Œå¹¶ä¸ºè¿™äº›å¤±è´¥ä¸è®­ç»ƒé›†ä¸­çš„æ•°æ®ç¨€ç¼ºä¹‹é—´çš„è”ç³»æä¾›äº†å¤§è§„æ¨¡è¯æ®ã€‚é€šè¿‡æä¾›ä¸€ä¸ªæœ‰åŸåˆ™çš„å’Œå¯æ‰©å±•çš„å¼•æ“è¿›è¡Œæ·±åº¦æ¨¡å‹å®¡è®¡ï¼ŒFailureAtlaså»ºç«‹äº†ä¸€ç§æ–°çš„ä»¥è¯Šæ–­ä¸ºä¸»çš„æ–¹æ³•ï¼Œä»¥æŒ‡å¯¼å¼€å‘æ›´ç¨³å¥çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é™æ€åŸºå‡†æµ‹è¯•å¯¹äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å…·æœ‰ä¸€å®šä»·å€¼ï¼Œä½†å…¶è¢«åŠ¨è®¾è®¡é™åˆ¶äº†è¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>ä¸»åŠ¨æ¢ç´¢æ–¹æ³•ä½œä¸ºé™æ€åŸºå‡†æµ‹è¯•çš„è¡¥å……è¢«æå‡ºï¼Œä»¥æ›´å…¨é¢åœ°æ­ç¤ºæ¨¡å‹çš„å¤±è´¥ç‚¹ã€‚</li>
<li>FailureAtlasæ¡†æ¶è¢«ä»‹ç»ä¸ºç¬¬ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å’Œå¤§è§„æ¨¡æ˜ å°„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ•…éšœæ™¯è§‚çš„æ¡†æ¶ã€‚</li>
<li>FailureAtlaså°†é”™è¯¯å‘ç°å®šä¹‰ä¸ºå¯»æ‰¾æœ€å°çš„æ•…éšœè¯±å¯¼æ¦‚å¿µçš„ç»“æ„åŒ–æœç´¢é—®é¢˜ã€‚</li>
<li>ä½œè€…ä½¿ç”¨æ–°çš„åŠ é€ŸæŠ€æœ¯ä½¿ä¸»åŠ¨æ¢ç´¢æˆä¸ºå¯èƒ½ï¼Œè¯¥æŠ€æœ¯åœ¨è®¡ç®—ä¸Šå¯èƒ½æ˜¯å¯†é›†å‹çš„ã€‚</li>
<li>åœ¨åº”ç”¨äºStable Diffusionæ¨¡å‹æ—¶ï¼ŒFailureAtlaså‘ç°äº†å¤§é‡çš„é”™è¯¯åˆ‡ç‰‡ï¼Œå¹¶æä¾›äº†ä¸è®­ç»ƒæ•°æ®ç¨€ç¼ºæ€§ç›¸å…³çš„å¤±è´¥è¯æ®ã€‚</li>
<li>FailureAtlasæä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„å’Œå¯æ‰©å±•çš„å¼•æ“è¿›è¡Œæ·±åº¦æ¨¡å‹å®¡è®¡ï¼Œæ¨åŠ¨å‘å±•æ›´ç¨³å¥çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¼€å‘æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21995v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation"><a href="#Mind-the-Glitch-Visual-Correspondence-for-Detecting-Inconsistencies-in-Subject-Driven-Generation" class="headerlink" title="Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in   Subject-Driven Generation"></a>Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in   Subject-Driven Generation</h2><p><strong>Authors:Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka</strong></p>
<p>We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and visionâ€“language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:<a target="_blank" rel="noopener" href="https://abdo-eldesokey.github.io/mind-the-glitch/">https://abdo-eldesokey.github.io/mind-the-glitch/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ä¸»å¹²ä¸­åˆ†ç¦»è§†è§‰å’Œè¯­ä¹‰ç‰¹å¾ï¼Œä»¥ç±»ä¼¼äºå·²å»ºç«‹çš„è¯­ä¹‰å¯¹åº”çš„æ–¹å¼å®ç°è§†è§‰å¯¹åº”ã€‚è™½ç„¶å·²çŸ¥æ‰©æ•£æ¨¡å‹çš„ä¸»å¹²ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œä½†å®ƒä»¬ä¹Ÿå¿…ç„¶åŒ…å«è§†è§‰ç‰¹å¾ä»¥æ”¯æŒå…¶å›¾åƒåˆæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®é›†ï¼Œéš”ç¦»è¿™äº›è§†è§‰ç‰¹å¾æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œè¯¥ç®¡é“åŸºäºç°æœ‰çš„ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆæ•°æ®é›†æ„å»ºå…·æœ‰æ ‡æ³¨è¯­ä¹‰å’Œè§†è§‰å¯¹åº”çš„å›¾åƒå¯¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯¹æ¯”æ¶æ„æ¥åˆ†ç¦»è¿™ä¸¤ç§ç‰¹å¾ç±»å‹ã€‚åˆ©ç”¨è§£è€¦çš„è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†â€”â€”è§†è§‰è¯­ä¹‰åŒ¹é…ï¼ˆVSMï¼‰ï¼Œç”¨äºé‡åŒ–ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆä¸­çš„è§†è§‰ä¸ä¸€è‡´æ€§ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡åŒ–è§†è§‰ä¸ä¸€è‡´æ€§æ–¹é¢ä¼˜äºåŸºäºå…¨å±€ç‰¹å¾çš„åº¦é‡æ ‡å‡†ï¼Œå¦‚CLIPã€DINOå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶èƒ½å¤Ÿå®ç°ä¸ä¸€è‡´åŒºåŸŸçš„ç©ºé—´å®šä½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§æ—¢æ”¯æŒä¸ä¸€è‡´æ€§çš„é‡åŒ–åˆæ”¯æŒå®šä½çš„ä¸»ä½“é©±åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºæ¨è¿›è¿™é¡¹ä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://abdo-eldesokey.github.io/mind-the-glitch/">https://abdo-eldesokey.github.io/mind-the-glitch/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21989v1">PDF</a> NeurIPS 2025 (Spotlight). Project Page:   <a target="_blank" rel="noopener" href="https://abdo-eldesokey.github.io/mind-the-glitch/">https://abdo-eldesokey.github.io/mind-the-glitch/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ä¸»å¹²æ¥åˆ†ç¦»è§†è§‰å’Œè¯­ä¹‰ç‰¹å¾ï¼Œå®ç°è§†è§‰å¯¹åº”ï¼Œç±»ä¼¼äºå·²ç»å»ºç«‹çš„è¯­ä¹‰å¯¹åº”ã€‚ä¸ºè§£å†³æ²¡æœ‰æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–ç®¡é“ï¼Œæ ¹æ®ç°æœ‰çš„ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ•°æ®é›†æ„å»ºå…·æœ‰æ ‡æ³¨è¯­ä¹‰å’Œè§†è§‰å¯¹åº”çš„å›¾åƒå¯¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯¹æ¯”æ¶æ„æ¥åˆ†ç¦»è¿™ä¸¤ç§ç‰¹å¾ç±»å‹ã€‚åˆ©ç”¨åˆ†ç¦»å‡ºçš„ç‰¹å¾è¡¨ç¤ºï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æŒ‡æ ‡â€”â€”è§†è§‰è¯­ä¹‰åŒ¹é…ï¼ˆVSMï¼‰ï¼Œç”¨äºé‡åŒ–ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆä¸­çš„è§†è§‰ä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡åŒ–è§†è§‰ä¸ä¸€è‡´æ€§æ–¹é¢ä¼˜äºCLIPã€DINOå’Œè§†è§‰è¯­è¨€æ¨¡å‹ç­‰å…¨å±€ç‰¹å¾æŒ‡æ ‡ï¼Œå¹¶å®ç°äº†ä¸ä¸€è‡´åŒºåŸŸçš„ç©ºé—´å®šä½ã€‚è¿™æ˜¯æ”¯æŒé‡åŒ–å®šä½ä¸»é¢˜é©±åŠ¨ç”Ÿæˆä¸­ä¸ä¸€è‡´æ€§çš„é¦–ä¸ªæ–¹æ³•ï¼Œä¸ºæ¨è¿›æ­¤ä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ä¸»å¹²æ¥åˆ†ç¦»è§†è§‰å’Œè¯­ä¹‰ç‰¹å¾ï¼Œå®ç°è§†è§‰å¯¹åº”ã€‚</li>
<li>æå‡ºä¸€ç§è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºå›¾åƒå¯¹ï¼Œå…·æœ‰æ ‡æ³¨è¯­ä¹‰å’Œè§†è§‰å¯¹åº”ï¼ŒåŸºäºç°æœ‰ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ•°æ®é›†ã€‚</li>
<li>è®¾è®¡å¯¹æ¯”æ¶æ„ä»¥æ›´æœ‰æ•ˆåœ°åˆ†ç¦»è§†è§‰å’Œè¯­ä¹‰ç‰¹å¾ç±»å‹ã€‚</li>
<li>å¼•å…¥æ–°çš„æŒ‡æ ‡â€”â€”è§†è§‰è¯­ä¹‰åŒ¹é…ï¼ˆVSMï¼‰ï¼Œç”¨äºé‡åŒ–ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆä¸­çš„è§†è§‰ä¸ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é‡åŒ–è§†è§‰ä¸ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…¨å±€ç‰¹å¾æŒ‡æ ‡ï¼Œå¦‚CLIPã€DINOç­‰ã€‚</li>
<li>å®ç°ä¸ä¸€è‡´åŒºåŸŸçš„ç©ºé—´å®šä½ï¼Œè¿™æ˜¯å…¶ä»–æ–¹æ³•å°šæœªåšåˆ°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21989v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21989v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21989v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21989v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SemanticControl-A-Training-Free-Approach-for-Handling-Loosely-Aligned-Visual-Conditions-in-ControlNet"><a href="#SemanticControl-A-Training-Free-Approach-for-Handling-Loosely-Aligned-Visual-Conditions-in-ControlNet" class="headerlink" title="SemanticControl: A Training-Free Approach for Handling Loosely Aligned   Visual Conditions in ControlNet"></a>SemanticControl: A Training-Free Approach for Handling Loosely Aligned   Visual Conditions in ControlNet</h2><p><strong>Authors:Woosung Joung, Daewon Chae, Jinkyu Kim</strong></p>
<p>ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt-a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings-for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., â€œa human playing guitarâ€ for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., cat playing guitar). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code is available at <a target="_blank" rel="noopener" href="https://mung3477.github.io/semantic-control">https://mung3477.github.io/semantic-control</a>. </p>
<blockquote>
<p>ControlNeté€šè¿‡å¼•å…¥æ·±åº¦æˆ–è¾¹ç¼˜æ˜ å°„ç­‰é¢å¤–çš„è§†è§‰æ¡ä»¶ï¼Œå®ç°äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¯¦ç»†ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºä¸æ–‡æœ¬æç¤ºæŒ‡å®šçš„ç”Ÿæˆç›®æ ‡ç²¾ç¡®å¯¹é½çš„è§†è§‰æ¡ä»¶çš„å­˜åœ¨ï¼Œè¿™ä¸€è¦æ±‚åœ¨å®è·µä¸­ç»å¸¸å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸å¸¸è§æˆ–å¯Œæœ‰æƒ³è±¡åŠ›çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œç”±äºç¼ºä¹åˆé€‚çš„è§†è§‰æ¡ä»¶ï¼Œç”Ÿæˆä¸€å¼ ç‰¹å®šå§¿åŠ¿çš„çŒ«åšé¥­çš„å›¾åƒå¯èƒ½æ˜¯ä¸å¯è¡Œçš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ›´å¸¸è§çš„è®¾ç½®ä¸­å¯ä»¥ç»å¸¸æ‰¾åˆ°ç»“æ„ç›¸ä¼¼çš„çº¿ç´¢â€”â€”ä¾‹å¦‚ï¼Œäººç±»çš„çƒ¹é¥ªå§¿åŠ¿æ˜¯å¹¿æ³›å¯ç”¨çš„ï¼Œå¯ä»¥ä½œä¸ºç²—ç•¥çš„è§†è§‰æŒ‡å—ã€‚é—æ†¾çš„æ˜¯ï¼Œç°æœ‰çš„ControlNetæ¨¡å‹éš¾ä»¥åˆ©ç”¨è¿™ç§æ¾æ•£å¯¹é½çš„è§†è§‰æ¡ä»¶ï¼Œè¿™å¾€å¾€å¯¼è‡´æ–‡æœ¬ä¿çœŸåº¦ä½æˆ–è§†è§‰å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SemanticControlï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯æœ‰æ•ˆåˆ©ç”¨é”™ä½ä½†è¯­ä¹‰ä¸Šç›¸å…³çš„è§†è§‰æ¡ä»¶çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è‡ªé€‚åº”åœ°æŠ‘åˆ¶äº†è§†è§‰æ¡ä»¶çš„å½±å“ï¼Œå½“å…¶ä¸æç¤ºå†²çªæ—¶ï¼ŒåŒæ—¶åŠ å¼ºæ–‡æœ¬çš„æŒ‡å¯¼ã€‚å…³é”®çš„æƒ³æ³•æ˜¯é¦–å…ˆä½¿ç”¨ä¸€ä¸ªä¸è§†è§‰æ¡ä»¶å¯¹é½çš„æ›¿ä»£æç¤ºè¿è¡Œä¸€ä¸ªè¾…åŠ©å»å™ªè¿‡ç¨‹ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ªäººå¼¹å‰ä»–â€å¯¹äºäººç±»å§¿åŠ¿æ¡ä»¶ï¼‰ï¼Œä»¥æå–ä¿¡æ¯ä¸°å¯Œçš„æ³¨æ„åŠ›æ©ç ï¼Œç„¶ååœ¨å®é™…ç›®æ ‡æç¤ºçš„å»å™ªè¿‡ç¨‹ä¸­åˆ©ç”¨è¿™äº›æ©ç ï¼ˆä¾‹å¦‚ï¼Œâ€œçŒ«å¼¹å‰ä»–â€ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¾æ•£å¯¹é½çš„æ¡ä»¶ä¸‹ï¼Œåœ¨å„ç§æ¡ä»¶ä¸‹å‡æé«˜äº†æ€§èƒ½ï¼ŒåŒ…æ‹¬æ·±åº¦å›¾ã€è¾¹ç¼˜å›¾å’Œäººç±»éª¨æ¶ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://mung3477.github.io/semantic-control%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://mung3477.github.io/semantic-controlä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21938v1">PDF</a> BMVC 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ControlNeté€šè¿‡å¼•å…¥æ·±åº¦æˆ–è¾¹ç¼˜æ˜ å°„ç­‰é¢å¤–è§†è§‰æ¡ä»¶ï¼Œå®ç°äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¯¦ç»†ç©ºé—´æ§åˆ¶ã€‚ä½†å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºè§†è§‰æ¡ä»¶ä¸æ–‡æœ¬æç¤ºæŒ‡å®šçš„ç”Ÿæˆç›®æ ‡ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œè¿™åœ¨å®è·µä¸­å°¤å…¶æ˜¯å¯¹äºä¸å¸¸è§æˆ–æƒ³è±¡åœºæ™¯å¸¸å¸¸éš¾ä»¥å®ç°ã€‚ä¸ºè§£å†³ç°æœ‰ControlNetæ¨¡å‹åœ¨å¤„ç†é”™ä½ä½†è¯­ä¹‰ç›¸å…³çš„è§†è§‰æ¡ä»¶æ—¶çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SemanticControlï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨é”™ä½ä½†è¯­ä¹‰ç›¸å…³çš„è§†è§‰æ¡ä»¶ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰æ¡ä»¶ä¸æç¤ºå†²çªæ—¶æŠ‘åˆ¶å…¶å½±å“ï¼ŒåŒæ—¶å¼ºåŒ–æ–‡æœ¬æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¡ä»¶ä¸‹æ€§èƒ½ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ·±åº¦å›¾ã€è¾¹ç¼˜å›¾å’Œäººç±»éª¨æ¶ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ControlNeté€šè¿‡å¼•å…¥é¢å¤–çš„è§†è§‰æ¡ä»¶å¢å¼ºäº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç©ºé—´æ§åˆ¶ï¼Œä½†ä¾èµ–äºè§†è§‰æ¡ä»¶ä¸æ–‡æœ¬æç¤ºçš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>å¯¹äºä¸å¸¸è§æˆ–æƒ³è±¡åœºæ™¯ï¼Œè·å–é€‚åˆçš„è§†è§‰æ¡ä»¶å¯èƒ½å›°éš¾ï¼Œé™åˆ¶äº†ControlNetçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>SemanticControlæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨é”™ä½ä½†è¯­ä¹‰ç›¸å…³çš„è§†è§‰æ¡ä»¶ã€‚</li>
<li>SemanticControlé€šè¿‡è‡ªé€‚åº”åœ°æŠ‘åˆ¶å†²çªçš„è§†è§‰æ¡ä»¶å¹¶å¼ºåŒ–æ–‡æœ¬æŒ‡å¯¼æ¥æ”¹è¿›æ€§èƒ½ã€‚</li>
<li>SemanticControlé€šè¿‡è¾…åŠ©å»å™ªè¿‡ç¨‹æå–ä¿¡æ¯æ€§æ³¨æ„åŠ›æ©è†œï¼Œç„¶ååˆ©ç”¨è¿™äº›æ©è†œè¿›è¡Œå®é™…ç›®æ ‡æç¤ºçš„å»å™ªã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSemanticControlåœ¨å¤šç§æ¡ä»¶ä¸‹æ€§èƒ½ä¼˜è¶Šï¼ŒåŒ…æ‹¬æ·±åº¦å›¾ã€è¾¹ç¼˜å›¾å’Œäººç±»éª¨æ¶ç­‰ã€‚</li>
<li>ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21938v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21938v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21938v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21938v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Discrete-Guidance-Matching-Exact-Guidance-for-Discrete-Flow-Matching"><a href="#Discrete-Guidance-Matching-Exact-Guidance-for-Discrete-Flow-Matching" class="headerlink" title="Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching"></a>Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching</h2><p><strong>Authors:Zhengyan Wan, Yidong Ouyang, Liyan Xie, Fang Fang, Hongyuan Zha, Guang Cheng</strong></p>
<p>Guidance provides a simple and effective framework for posterior sampling by steering the generation process towards the desired distribution. When modeling discrete data, existing approaches mostly focus on guidance with the first-order Taylor approximation to improve the sampling efficiency. However, such an approximation is inappropriate in discrete state spaces since the approximation error could be large. A novel guidance framework for discrete data is proposed to address this problem: We derive the exact transition rate for the desired distribution given a learned discrete flow matching model, leading to guidance that only requires a single forward pass in each sampling step, significantly improving efficiency. This unified novel framework is general enough, encompassing existing guidance methods as special cases, and it can also be seamlessly applied to the masked diffusion model. We demonstrate the effectiveness of our proposed guidance on energy-guided simulations and preference alignment on text-to-image generation and multimodal understanding tasks. The code is available through <a target="_blank" rel="noopener" href="https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main">https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main</a>. </p>
<blockquote>
<p>æŒ‡å¯¼ï¼ˆGuidanceï¼‰é€šè¿‡å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœå‘ç›®æ ‡åˆ†å¸ƒï¼Œä¸ºåé‡‡æ ·æä¾›äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ã€‚åœ¨ç¦»æ•£æ•°æ®å»ºæ¨¡ä¸­ï¼Œç°æœ‰çš„æ–¹æ³•å¤§å¤šé›†ä¸­åœ¨ç”¨ä¸€é˜¶æ³°å‹’è¿‘ä¼¼è¿›è¡Œå¼•å¯¼ä»¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚ç„¶è€Œï¼Œç”±äºåœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿‘ä¼¼è¯¯å·®å¯èƒ½è¾ƒå¤§ï¼Œè¿™ç§è¿‘ä¼¼æ˜¯ä¸æ°å½“çš„ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¦»æ•£æ•°æ®çš„æ–°å‹æŒ‡å¯¼æ¡†æ¶ï¼šæˆ‘ä»¬æ ¹æ®å­¦ä¹ åˆ°çš„ç¦»æ•£æµåŒ¹é…æ¨¡å‹æ¨å¯¼å‡ºç›®æ ‡åˆ†å¸ƒçš„ç²¾ç¡®è½¬ç§»ç‡ï¼Œä»è€Œå®ç°äº†ä¸€ç§æŒ‡å¯¼æ–¹å¼ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­åªéœ€è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œå¤§å¤§æé«˜äº†æ•ˆç‡ã€‚è¿™ç§ç»Ÿä¸€çš„å…¨æ–°æ¡†æ¶è¶³å¤Ÿé€šç”¨ï¼ŒåŒ…å«ç°æœ‰æŒ‡å¯¼æ–¹æ³•ä½œä¸ºç‰¹ä¾‹ï¼Œå¹¶èƒ½æ— ç¼åº”ç”¨äºå¸¦æ©ç çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨èƒ½é‡å¼•å¯¼çš„æ¨¡æ‹Ÿå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­çš„åå¥½å¯¹é½æ–¹é¢éªŒè¯äº†æ‰€æå‡ºæŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/mainè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21912v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„ç¦»æ•£æ•°æ®å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡ä¸ºæœŸæœ›åˆ†å¸ƒæ¨å¯¼ç²¾ç¡®è½¬ç§»ç‡å¹¶ä½¿ç”¨å­¦ä¹ åˆ°çš„ç¦»æ•£æµåŒ¹é…æ¨¡å‹ï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚æ­¤æ¡†æ¶ç»Ÿä¸€å¹¶æ‰©å±•äº†ç°æœ‰å¼•å¯¼æ–¹æ³•ï¼Œå¯æ— ç¼åº”ç”¨äºå¸¦æ©ç çš„æ‰©æ•£æ¨¡å‹ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­è¿›è¡Œäº†æœ‰æ•ˆçš„æ¼”ç¤ºã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/mainè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹ç¦»æ•£æ•°æ®çš„æ–°å‹å¼•å¯¼æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç¦»æ•£æ•°æ®æ—¶çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ¨å¯¼ç²¾ç¡®è½¬ç§»ç‡ï¼Œå®ç°äº†æœŸæœ›åˆ†å¸ƒçš„å¼•å¯¼ï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œæ¶µç›–ç°æœ‰å¼•å¯¼æ–¹æ³•ä½œä¸ºç‰¹ä¾‹ï¼Œå¹¶èƒ½æ— ç¼åº”ç”¨äºå¸¦æ©ç çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨èƒ½é‡å¼•å¯¼æ¨¡æ‹Ÿã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»¥åŠå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¡†æ¶çš„å¼•å…¥æœ‰åŠ©äºæé«˜æ¨¡å‹çš„é‡‡æ ·æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>ä»£ç å·²ç»å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21912v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21912v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2509.21912v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning"><a href="#Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning" class="headerlink" title="Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning"></a>Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning</h2><p><strong>Authors:Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰éšç€æ‰©æ•£æ¨¡å‹çš„æˆåŠŸè€Œè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œç¼–è¾‘æ“ä½œçš„ç®€å•æ˜ç¡®æŒ‡ä»¤ä¸Šï¼Œä¾‹å¦‚æ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–äº¤æ¢å¯¹è±¡ã€‚ä»–ä»¬å¾ˆéš¾å¤„ç†æ›´å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤éœ€è¦æ›´æ·±çš„æ¨ç†æ¥æ¨æ–­å¯èƒ½å‘ç”Ÿçš„è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†åœ¨æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨ç»“æ„ä¸Šï¼Œè¿™äº›æ–¹æ³•ä¹Ÿç¼ºä¹æ”¯æŒæ­¤ç±»æ¨ç†çš„ç²¾ç»†ç»†èŠ‚æå–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠReasonBrainï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å¤šç§åœºæ™¯ä¸­æ‰§è¡Œéšå«å‡è®¾æŒ‡ä»¤çš„æ¨ç†ã€‚Reason50KåŒ…å«è¶…è¿‡50Kä¸ªæ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç¼–è¾‘æŒ‡å¯¼ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†çš„æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—æ¥æ•æ‰æ”¯æŒæŒ‡ä»¤æ¨ç†çš„è¯¦ç»†è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚ä¸ºäº†å‡å°‘è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œå®ƒä½¿ç²¾ç»†çº¿ç´¢ä¸MLLMæ´¾ç”Ÿç‰¹å¾ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œä¸°å¯Œçš„äº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸ŠæŒç»­è¶…è¶Šæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶åœ¨ä¼ ç»Ÿçš„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01908v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰åœ¨æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹å‘å±•è¿…é€Ÿã€‚ç„¶è€Œï¼Œç°æœ‰åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œç®€å•æ˜ç¡®çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå¦‚æ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–äº¤æ¢å¯¹è±¡ã€‚å®ƒä»¬éš¾ä»¥å¤„ç†æ›´å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œéœ€è¦æ›´æ·±çš„æ¨ç†æ¥æ¨æ–­å¯è¡Œçš„è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†å¯¹è®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ”¯æŒæœ‰é™ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„å¤§å‹æ•°æ®é›†ï¼Œä»¥åŠReasonBrainï¼Œä¸€ä¸ªæ—¨åœ¨åœ¨å¤šç§åœºæ™¯ä¸­æ‰§è¡Œéšå«å‡è®¾æŒ‡ä»¤çš„æ–°æ¡†æ¶ã€‚Reason50KåŒ…æ‹¬è¶…è¿‡5ä¸‡æ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç¼–è¾‘æŒ‡å¯¼ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶å¼•å…¥ç²¾ç»†æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—æ¥æ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œä»¥æ”¯æŒæŒ‡ä»¤æ¨ç†ã€‚ä¸ºäº†å‡è½»è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œä½¿ç²¾ç»†çº¿ç´¢å’ŒMLLMè¡ç”Ÿç‰¹å¾ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œä¸°å¯Œçš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸ŠæŒç»­ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿ï¼Œå¹¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç°æœ‰å›¾åƒç¼–è¾‘ä¸»è¦å¤„ç†ç®€å•æ˜ç¡®çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>å½“å‰æ•°æ®é›†å¯¹è®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›çš„æ”¯æŒæœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹æ•°æ®é›†Reason50Kï¼ŒåŒ…å«å¤šç§å…³é”®æ¨ç†åœºæ™¯çš„å›¾åƒç¼–è¾‘æ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ReasonBrainæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œæ‰§è¡Œéšå«å‡è®¾æŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>ReasonBrainåŒ…æ‹¬ç²¾ç»†æ¨ç†çº¿ç´¢æå–æ¨¡å—ï¼ˆFRCEï¼‰å’Œè·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œä»¥æé«˜è¯­ä¹‰å¤„ç†èƒ½åŠ›å’Œäº¤äº’æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶åœ¨å¸¸è§„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2507.01908v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Forward-only-Diffusion-Probabilistic-Models"><a href="#Forward-only-Diffusion-Probabilistic-Models" class="headerlink" title="Forward-only Diffusion Probabilistic Models"></a>Forward-only Diffusion Probabilistic Models</h2><p><strong>Authors:Ziwei Luo, Fredrik K. Gustafsson, Jens SjÃ¶lund, Thomas B. SchÃ¶n</strong></p>
<p>This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model, despite its simplicity, achieves state-of-the-art performance on various image restoration tasks. Its general applicability on image-conditioned generation is also demonstrated via qualitative results on image-to-image translation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD">https://github.com/Algolzw/FoD</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åªå‰å‘æ‰©æ•£ï¼ˆFoDï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè€¦åˆçš„å‰å‘-åå‘æ‰©æ•£æ–¹æ¡ˆçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒFoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ç›´æ¥å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„ç”Ÿæˆæ¡†æ¶ã€‚FoDçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸çŠ¶æ€ç›¸å…³çš„éšæœºå¾®åˆ†æ–¹ç¨‹ï¼Œè¯¥æ–¹ç¨‹åœ¨æ¼‚ç§»å’Œæ‰©æ•£å‡½æ•°ä¸­å‡æ¶‰åŠå‡å€¼å›å½’é¡¹ã€‚è¿™ç§å‡å€¼å›å½’å±æ€§ä¿è¯äº†å‘æ¸…æ´æ•°æ®çš„æ”¶æ•›ï¼Œè‡ªç„¶åœ°æ¨¡æ‹Ÿäº†æºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFoDåˆ†æèµ·æ¥æ˜“äºå¤„ç†ï¼Œå¹¶ä¸”ä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†å‡ æ­¥éé©¬å°”å¯å¤«é“¾é‡‡æ ·ã€‚å°½ç®¡FoDæ¨¡å‹ç®€å•ï¼Œä½†å®ƒåœ¨å„ç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„å®šæ€§ç»“æœï¼Œä¹Ÿè¯æ˜äº†å…¶åœ¨å›¾åƒæ¡ä»¶ç”Ÿæˆä¸Šçš„æ™®éé€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD%E3%80%82">https://github.com/Algolzw/FoDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16733v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://algolzw.github.io/fod">https://algolzw.github.io/fod</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»…å‰å‘æ‰©æ•£ï¼ˆFoDï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿä¾èµ–è€¦åˆå‰å‘-åå‘æ‰©æ•£æ–¹æ¡ˆçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒFoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ç›´æ¥å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´é«˜æ•ˆçš„ç”Ÿæˆæ¡†æ¶ã€‚FoDçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ¶‰åŠå‡å€¼å›å½’é¡¹çš„åŸºäºçŠ¶æ€éšæœºå¾®åˆ†æ–¹ç¨‹ï¼Œåœ¨æ¼‚ç§»å’Œæ‰©æ•£å‡½æ•°ä¸­å‡æœ‰ä½“ç°ã€‚è¿™ä¸€å‡å€¼å›å½’å±æ€§ä¿è¯äº†å‘æ¸…æ´æ•°æ®çš„æ”¶æ•›ï¼Œè‡ªç„¶æ¨¡æ‹Ÿäº†æºåˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚æ­¤å¤–ï¼ŒFoDå…·æœ‰åˆ†æå¯è¡Œæ€§ï¼Œä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†å°‘æ•°æ­¥éª¤çš„éé©¬å°”å¯å¤«é“¾é‡‡æ ·ã€‚å°½ç®¡å…¶ç®€æ´æ€§ï¼ŒFoDåœ¨å„é¡¹å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¸”åœ¨å›¾åƒæ¡ä»¶ç”Ÿæˆä¸Šå±•ç°äº†å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚ä»£ç å·²å‘å¸ƒäº <a target="_blank" rel="noopener" href="https://github.com/Algolzw/FoD%E3%80%82">https://github.com/Algolzw/FoDã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºFoDçš„ä»…å‰å‘æ‰©æ•£æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå»ºæ¨¡ã€‚</li>
<li>FoDé€šè¿‡å•ä¸€çš„å‰å‘æ‰©æ•£è¿‡ç¨‹å­¦ä¹ æ•°æ®ç”Ÿæˆï¼Œæ¡†æ¶ç®€æ´é«˜æ•ˆã€‚</li>
<li>FoDçš„æ ¸å¿ƒæ˜¯åŸºäºçŠ¶æ€çš„éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ŒåŒ…å«å‡å€¼å›å½’é¡¹ã€‚</li>
<li>å‡å€¼å›å½’å±æ€§ä¿è¯äº†å‘æ¸…æ´æ•°æ®çš„æ”¶æ•›ï¼Œæ¨¡æ‹Ÿäº†æºåˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„éšæœºæ’å€¼ã€‚</li>
<li>FoDå…·æœ‰åˆ†æå¯è¡Œæ€§ï¼Œä½¿ç”¨ç®€å•çš„éšæœºæµåŒ¹é…ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒFoDå®ç°äº†å°‘æ•°æ­¥éª¤çš„éé©¬å°”å¯å¤«é“¾é‡‡æ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2505.16733v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2505.16733v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SPEED-Scalable-Precise-and-Efficient-Concept-Erasure-for-Diffusion-Models"><a href="#SPEED-Scalable-Precise-and-Efficient-Concept-Erasure-for-Diffusion-Models" class="headerlink" title="SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion   Models"></a>SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion   Models</h2><p><strong>Authors:Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin Hao, Guojun Ma, Fuli Feng</strong></p>
<p>Erasing concepts from large-scale text-to-image (T2I) diffusion models has become increasingly crucial due to the growing concerns over copyright infringement, offensive content, and privacy violations. In scalable applications, fine-tuning-based methods are time-consuming to precisely erase multiple target concepts, while real-time editing-based methods often degrade the generation quality of non-target concepts due to conflicting optimization objectives. To address this dilemma, we introduce SPEED, an efficient concept erasure approach that directly edits model parameters. SPEED searches for a null space, a model editing space where parameter updates do not affect non-target concepts, to achieve scalable and precise erasure. To facilitate accurate null space optimization, we incorporate three complementary strategies: Influence-based Prior Filtering (IPF) to selectively retain the most affected non-target concepts, Directed Prior Augmentation (DPA) to enrich the filtered retain set with semantically consistent variations, and Invariant Equality Constraints (IEC) to preserve key invariants during the T2I generation process. Extensive evaluations across multiple concept erasure tasks demonstrate that SPEED consistently outperforms existing methods in non-target preservation while achieving efficient and high-fidelity concept erasure, successfully erasing 100 concepts within only 5 seconds. Our code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/Ouxiang-Li/SPEED">https://github.com/Ouxiang-Li/SPEED</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­æ¶ˆé™¤æ¦‚å¿µå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå› ä¸ºäººä»¬å¯¹ç‰ˆæƒä¾µçŠ¯ã€å†’çŠ¯æ€§å†…å®¹å’Œéšç§æ³„éœ²çš„æ‹…å¿§æ—¥ç›ŠåŠ å‰§ã€‚åœ¨å¯æ‰©å±•åº”ç”¨ä¸­ï¼ŒåŸºäºå¾®è°ƒçš„æ–¹æ³•åœ¨ç²¾ç¡®æ¶ˆé™¤å¤šä¸ªç›®æ ‡æ¦‚å¿µæ—¶éå¸¸è€—æ—¶ï¼Œè€ŒåŸºäºå®æ—¶ç¼–è¾‘çš„æ–¹æ³•ç”±äºä¼˜åŒ–ç›®æ ‡çš„å†²çªå¾€å¾€ä¼šé™ä½éç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SPEEDï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œå®ƒç›´æ¥ç¼–è¾‘æ¨¡å‹å‚æ•°ã€‚SPEEDæœç´¢ä¸€ä¸ªé›¶ç©ºé—´ï¼Œå³æ¨¡å‹ç¼–è¾‘ç©ºé—´ï¼Œå…¶ä¸­å‚æ•°æ›´æ–°ä¸ä¼šå½±å“éç›®æ ‡æ¦‚å¿µï¼Œä»¥å®ç°å¯æ‰©å±•å’Œç²¾ç¡®çš„æ¶ˆé™¤ã€‚ä¸ºäº†ä¿ƒè¿›å‡†ç¡®çš„é›¶ç©ºé—´ä¼˜åŒ–ï¼Œæˆ‘ä»¬ç»“åˆäº†ä¸‰ç§äº’è¡¥ç­–ç•¥ï¼šåŸºäºå½±å“çš„ä¼˜å…ˆè¿‡æ»¤ï¼ˆIPFï¼‰æ¥é€‰æ‹©ä¿ç•™å—å½±å“æœ€ä¸¥é‡çš„éç›®æ ‡æ¦‚å¿µï¼Œå®šå‘ä¼˜å…ˆå¢å¼ºï¼ˆDPAï¼‰ä»¥ä¸°å¯Œè¿‡æ»¤åçš„ä¿ç•™é›†è¯­ä¹‰ä¸Šä¸€è‡´çš„å˜ä½“ï¼Œä»¥åŠä¸å˜ç­‰å¼çº¦æŸï¼ˆIECï¼‰ä»¥ä¿æŒT2Iç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³é”®ä¸å˜æ€§ã€‚åœ¨å¤šä¸ªæ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSPEEDåœ¨éç›®æ ‡ä¿ç•™æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å®ç°é«˜æ•ˆå’Œé«˜ä¿çœŸåº¦çš„æ¦‚å¿µæ¶ˆé™¤ï¼Œåœ¨ä»…5ç§’å†…æˆåŠŸæ¶ˆé™¤100ä¸ªæ¦‚å¿µã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ouxiang-Li/SPEED%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Ouxiang-Li/SPEEDè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07392v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¦æ¨¡æ–‡æœ¬è½‰åœ–åƒï¼ˆT2Iï¼‰æ“´æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µåˆªé™¤æ—¥ç›Šé‡è¦ï¼Œå› ç‚ºå­˜åœ¨ç‰ˆæ¬Šä¾µçŠ¯ã€å†’çŠ¯æ€§å†…å®¹å’Œé•åéšç§é—®é¢˜ã€‚é¢å¯¹ç²¾å‡†åˆ é™¤å¤šä¸ªç›®æ ‡æ¦‚å¿µæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸€ç§é«˜æ•ˆçš„æ¦‚å¿µåˆ é™¤æ–¹æ³•SPEEDè¢«æå‡ºï¼Œå®ƒç›´æ¥ç·¨è¼¯æ¨¡å‹åƒæ•¸ã€‚SPEEDåœ¨åƒæ•¸æ›´æ–°ä¸æœƒå½±éŸ¿éç›®æ¨™æ¦‚å¿µçš„æ¨¡å‹ç·¨è¼¯ç©ºé–“ä¸­é€²è¡Œæœç´¢ï¼Œå¯¦ç¾å¯æ“´å±•å’Œç²¾ç¢ºçš„åˆªé™¤ã€‚ç»“åˆä¸‰ç§äº’è£œç­–ç•¥ï¼šåŸºäºå½±éŸ¿çš„å…ˆæ“‡æ¿¾é™¤ï¼ˆIPFï¼‰ä¾†é¸æ“‡æ€§ä¿ç•™æœ€å—å½±éŸ¿çš„éç›®æ¨™æ¦‚å¿µï¼Œæœ‰æ–¹å‘æ€§çš„å…ˆæ“‡å¢å¼·ï¼ˆDPAï¼‰ä¾†è±å¯Œéæ¿¾å¾Œçš„ä¿ç•™é›†ï¼Œä»¥åŠä¿æŒT2Iç”Ÿæˆéç¨‹ä¸­çš„é—œéµä¸è®Šæ€§ï¼ˆIECï¼‰ã€‚åœ¨å¤šé‡æ¦‚å¿µåˆªé™¤ä»»å‹™ä¸Šçš„å»£æ³›è©•ä¼°é¡¯ç¤ºï¼ŒSPEEDåœ¨éç›®æ¨™ä¿å­˜æ–¹é¢è¡¨ç¾å„ªæ–¼ç¾æœ‰æ–¹æ³•ï¼ŒåŒæ™‚å¯¦ç¾é«˜æ•ˆå’Œé«˜ä¿çœŸåº¦çš„æ¦‚å¿µåˆªé™¤ï¼Œåƒ…åœ¨5ç§’å…§æˆåŠŸåˆªé™¤100å€‹æ¦‚å¿µã€‚ç›¸é—œä»£ç¢¼å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ouxiang-Li/SPEED%E6%9F%A5%E9%96%B1%E3%80%82">https://github.com/Ouxiang-Li/SPEEDæŸ¥é–±ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¦æ¨¡æ–‡æœ¬è½‰åœ–åƒï¼ˆT2Iï¼‰æ“´æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µåˆªé™¤ç¾åœ¨éå¸¸é‡è¦ï¼Œå› ç‚ºç‰ˆæ¬Šã€å†’çŠ¯æ€§å…§å®¹å’Œéšç§é—®é¢˜æ—¥ç›Šå—åˆ°é—œæ³¨ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•å¦‚ç´°èª¿åŸºç¤æ–¹æ³•å’Œå¯¦æ™‚ç·¨è¼¯åŸºç¤æ–¹æ³•å­˜åœ¨æ™‚é–“æ¶ˆè€—å’Œéç›®æ¨™æ¦‚å¿µè³ªé‡ä¸‹é™å•é¡Œã€‚</li>
<li>SPEEDæ–¹æ³•è¢«æå‡ºä»¥è§£æ±ºé€™å€‹å•é¡Œï¼Œå®ƒé€šè¿‡ç›´æ¥ç·¨è¼¯æ¨¡å‹åƒæ•¸ï¼Œåœ¨ä¸å½±å“éç›®æ¨™æ¦‚å¿µçš„æ¨¡å‹ç·¨è¼¯ç©ºé–“ä¸­é€²è¡Œæœç´¢ã€‚</li>
<li>SPEEDç»“åˆäº†ä¸‰ç¨®ç­–ç•¥ï¼šå½±éŸ¿åŸºç¤å…ˆæ“‡æ“‡æ¿¾é™¤ã€æœ‰æ–¹å‘æ€§çš„å…ˆæ“‡å¢å¼·å’Œä¿æŒä¸è®Šæ€§ç´„æŸã€‚</li>
<li>SPEEDåœ¨å¤šä¸ªæ¦‚å¿µåˆªé™¤ä»»å‹™ä¸Šçš„è¡¨ç¾å„ªæ–¼ç¾æœ‰æ–¹æ³•ï¼Œå¯¦ç¾é«˜æ•ˆå’Œé«˜ä¿çœŸåº¦çš„æ¦‚å¿µåˆªé™¤ã€‚</li>
<li>SPEEDæ–¹æ³•èƒ½å¤ åœ¨çŸ­çŸ­5ç§’å…§åˆªé™¤100å€‹æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2503.07392v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2503.07392v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2503.07392v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2503.07392v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models"><a href="#Calibrated-Multi-Preference-Optimization-for-Aligning-Diffusion-Models" class="headerlink" title="Calibrated Multi-Preference Optimization for Aligning Diffusion Models"></a>Calibrated Multi-Preference Optimization for Aligning Diffusion Models</h2><p><strong>Authors:Kyungmin Lee, Xiaohang Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, Yinxiao Li</strong></p>
<p>Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench. </p>
<blockquote>
<p>å°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸åå¥½ä¼˜åŒ–å¯¹é½å¯¹äºäººç±»æ³¨é‡Šæ•°æ®é›†æ˜¯æœ‰ä»·å€¼çš„ï¼Œä½†æ‰‹åŠ¨æ•°æ®æ”¶é›†çš„æ²‰é‡æˆæœ¬é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä½¿ç”¨å¥–åŠ±æ¨¡å‹æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œç„¶è€Œï¼Œå½“å‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œä¿¡æ¯ï¼Œå› ä¸ºå®ƒä»¬åªè€ƒè™‘æˆå¯¹åå¥½åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç¼ºä¹åœ¨å¤šç§åå¥½åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†å¥–åŠ±ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ¡å‡†åå¥½ä¼˜åŒ–ï¼ˆCaPOï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„é€šç”¨åå¥½ï¼Œæ— éœ€äººç±»æ³¨é‡Šæ•°æ®å³å¯å¯¹é½T2Iæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå¥–åŠ±æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬çš„é¢„æœŸèƒœç‡æ¥è¿‘ä¼¼é€šç”¨åå¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡ä»å¸•ç´¯æ‰˜å‰æ²¿é€‰æ‹©é…å¯¹ï¼Œæœ‰æ•ˆåœ°ç®¡ç†å¤šåå¥½åˆ†å¸ƒã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å›å½’æŸå¤±å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŒ¹é…æ‰€é€‰é…å¯¹ä¹‹é—´æ ¡å‡†å¥–åŠ±çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å•ä¸€å¥–åŠ±è¿˜æ˜¯å¤šå¥–åŠ±è®¾ç½®ä¸‹ï¼Œç»è¿‡GenEvalå’ŒT2I-Compbenchè¯„ä¼°ï¼ŒCaPOå§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02588v3">PDF</a> CVPR 2025, Project page: <a target="_blank" rel="noopener" href="https://kyungmnlee.github.io/capo.github.io/">https://kyungmnlee.github.io/capo.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®çš„æ–¹æ³•ï¼Œå³é€šè¿‡èåˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„ä¸€èˆ¬åå¥½ï¼Œå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹è¿›è¡Œæ ¡å‡†ä¼˜åŒ–ï¼ˆCaPOï¼‰ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåŒ…æ‹¬å¥–åŠ±æ ¡å‡†æ–¹æ³•ï¼Œç”¨äºè®¡ç®—é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬çš„é¢„æœŸèƒœç‡ï¼Œä»è€Œè¿‘ä¼¼ä¸€èˆ¬åå¥½ï¼›åŒæ—¶æå‡ºåŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•ï¼Œæœ‰æ•ˆç®¡ç†å¤šåå¥½åˆ†å¸ƒï¼›æœ€åé€šè¿‡å›å½’æŸå¤±å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼ŒåŒ¹é…é€‰å®šé…å¯¹çš„æ ¡å‡†å¥–åŠ±å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCaPOåœ¨å•å¥–åŠ±å’Œå¤šå¥–åŠ±è®¾ç½®ä¸‹å‡ä¼˜äºå…ˆå‰çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå¹¶åœ¨T2IåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸åå¥½ä¼˜åŒ–å¯¹äºæé«˜æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œäººå·¥æ•°æ®æ”¶é›†çš„æ˜‚è´µæˆæœ¬é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆæ˜¯ä¸€ä¸ªå¯è¡Œçš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œä¿¡æ¯ã€ä»…è€ƒè™‘æˆå¯¹åå¥½åˆ†å¸ƒç­‰ã€‚</li>
<li>æå‡ºäº†æ ¡å‡†åå¥½ä¼˜åŒ–ï¼ˆCaPOï¼‰æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿèåˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„ä¸€èˆ¬åå¥½ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ã€‚</li>
<li>CaPOçš„æ ¸å¿ƒåŒ…æ‹¬å¥–åŠ±æ ¡å‡†æ–¹æ³•ã€åŸºäºå‰æ²¿çš„é…å¯¹é€‰æ‹©æ–¹æ³•ä»¥åŠé€šè¿‡å›å½’æŸå¤±å¾®è°ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCaPOåœ¨å•å¥–åŠ±å’Œå¤šå¥–åŠ±è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸å…ˆå‰çš„ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼ŒCaPOå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2502.02588v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Unforgettable-Lessons-from-Forgettable-Images-Intra-Class-Memorability-Matters-in-Computer-Vision"><a href="#Unforgettable-Lessons-from-Forgettable-Images-Intra-Class-Memorability-Matters-in-Computer-Vision" class="headerlink" title="Unforgettable Lessons from Forgettable Images: Intra-Class Memorability   Matters in Computer Vision"></a>Unforgettable Lessons from Forgettable Images: Intra-Class Memorability   Matters in Computer Vision</h2><p><strong>Authors:Jie Jing, Yongjian Huang, Serena J. -W. Wang, Shuangpeng Han, Lucia Schiatti, Yen-Ling Kuo, Qing Lin, Mengmi Zhang</strong></p>
<p>We introduce intra-class memorability, where certain images within the same class are more memorable than others despite shared category characteristics. To investigate what features make one object instance more memorable than others, we design and conduct human behavior experiments, where participants are shown a series of images, and they must identify when the current image matches the image presented a few steps back in the sequence. To quantify memorability, we propose the Intra-Class Memorability score (ICMscore), a novel metric that incorporates the temporal intervals between repeated image presentations into its calculation. Furthermore, we curate the Intra-Class Memorability Dataset (ICMD), comprising over 5,000 images across ten object classes with their ICMscores derived from 2,000 participantsâ€™ responses. Subsequently, we demonstrate the usefulness of ICMD by training AI models on this dataset for various downstream tasks: memorability prediction, image recognition, continual learning, and memorability-controlled image editing. Surprisingly, high-ICMscore images impair AI performance in image recognition and continual learning tasks, while low-ICMscore images improve outcomes in these tasks. Additionally, we fine-tune a state-of-the-art image diffusion model on ICMD image pairs with and without masked semantic objects. The diffusion model can successfully manipulate image elements to enhance or reduce memorability. Our contributions open new pathways in understanding intra-class memorability by scrutinizing fine-grained visual features behind the most and least memorable images and laying the groundwork for real-world applications in computer vision. We will release all code, data, and models publicly. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ç±»å†…è®°å¿†æ€§çš„æ¦‚å¿µï¼Œå³åŒä¸€ç±»åˆ«ä¸­çš„æŸäº›å›¾åƒæ¯”å…¶ä»–å›¾åƒæ›´å®¹æ˜“è®°ä½ï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰å…±äº«çš„åˆ†ç±»ç‰¹å¾ã€‚ä¸ºäº†ç ”ç©¶ä»€ä¹ˆç‰¹å¾ä½¿å¾—ä¸€ä¸ªå¯¹è±¡å®ä¾‹æ¯”å…¶ä»–å®ä¾‹æ›´å®¹æ˜“è®°ä½ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è¿›è¡Œäº†äººç±»è¡Œä¸ºå®éªŒï¼Œå®éªŒå‚ä¸è€…ä¼šè§‚çœ‹ä¸€ç³»åˆ—å›¾åƒï¼Œå¹¶å¿…é¡»åˆ¤æ–­å½“å‰å›¾åƒæ˜¯å¦ä¸åºåˆ—ä¸­å‡ æ­¥ä¹‹å‰çš„å›¾åƒç›¸åŒ¹é…ã€‚ä¸ºäº†é‡åŒ–è®°å¿†æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç±»å†…è®°å¿†æ€§å¾—åˆ†ï¼ˆICMscoreï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡å°†é‡å¤å‘ˆç°å›¾åƒä¹‹é—´çš„æ—¶é—´é—´éš”çº³å…¥è®¡ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´ç†äº†ç±»å†…è®°å¿†æ€§æ•°æ®é›†ï¼ˆICMDï¼‰ï¼Œæ¶µç›–åå¤§å¯¹è±¡ç±»åˆ«çš„è¶…è¿‡5000å¼ å›¾åƒï¼Œå…¶ICMscoreæ¥è‡ª2000åå‚ä¸è€…çš„å›ç­”ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒAIæ¨¡å‹æ¥å±•ç¤ºICMDçš„å®ç”¨æ€§ï¼Œç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼šè®°å¿†æ€§é¢„æµ‹ã€å›¾åƒè¯†åˆ«ã€æŒç»­å­¦ä¹ å’Œè®°å¿†æ€§æ§åˆ¶å›¾åƒå¤„ç†ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé«˜ICMå¾—åˆ†çš„å›¾åƒä¼šåœ¨å›¾åƒè¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸­æŸå®³AIæ€§èƒ½ï¼Œè€Œä½ICMå¾—åˆ†çš„å›¾åƒåˆ™ä¼šæ”¹å–„è¿™äº›ä»»åŠ¡çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¤„ç†å¸¦æœ‰å’Œä¸å¸¦æ©ç è¯­ä¹‰å¯¹è±¡çš„ICMDå›¾åƒå¯¹ã€‚æ‰©æ•£æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ“ä½œå›¾åƒå…ƒç´ ä»¥å¢å¼ºæˆ–å‡å°‘è®°å¿†æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä»”ç»†ç ”ç©¶æœ€éš¾å¿˜å’Œæœ€å®¹æ˜“å¿˜è®°çš„å›¾åƒèƒŒåçš„ç²¾ç»†è§†è§‰ç‰¹å¾ï¼Œä¸ºç†è§£ç±»å†…è®°å¿†æ€§å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶ä¸ºè®¡ç®—æœºè§†è§‰çš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20761v5">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æ¢è®¨äº†åŒä¸€ç±»åˆ«å†…ä¸åŒå›¾åƒçš„è®°å¿†æ€§å·®å¼‚ï¼Œå¹¶å®šä¹‰äº†â€œç±»å†…è®°å¿†æ€§â€çš„æ¦‚å¿µã€‚ä¸ºäº†ç¡®å®šä½¿æŸä¸€å¯¹è±¡å®ä¾‹æ¯”å…¶ä»–å¯¹è±¡æ›´æ˜“äºè®°å¿†çš„ç‰¹å¾ï¼Œè¿›è¡Œäº†äººç±»è¡Œä¸ºå®éªŒã€‚ä¸ºæ­¤æå‡ºé‡åŒ–è®°å¿†æ€§çš„æ–°æŒ‡æ ‡â€”â€”ç±»å†…è®°å¿†æ€§å¾—åˆ†ï¼ˆICMscoreï¼‰ï¼Œå¹¶å»ºç«‹äº†åŒ…å«è¶…è¿‡äº”åƒå¼ å›¾ç‰‡çš„ç±»å†…è®°å¿†æ€§æ•°æ®é›†ï¼ˆICMDï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜ICMscoreå›¾åƒä¼šé™ä½AIåœ¨å›¾åƒè¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè€Œä½ICMscoreå›¾åƒåˆ™èƒ½æé«˜ç»“æœã€‚æ­¤å¤–ï¼Œå¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹å¯ç”¨äºæé«˜æˆ–é™ä½å›¾åƒè®°å¿†æ€§ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£æœ€éš¾å¿˜å’Œæœ€æ˜“å¿˜çš„å›¾åƒèƒŒåçš„ç»†å¾®è§†è§‰ç‰¹å¾ä»¥åŠè®¡ç®—æœºè§†è§‰çš„æ½œåœ¨åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†â€œç±»å†…è®°å¿†æ€§â€æ¦‚å¿µï¼ŒåŒä¸€ç±»åˆ«çš„å›¾åƒä¹‹é—´ä¹Ÿå­˜åœ¨è®°å¿†æ€§çš„å·®å¼‚ã€‚</li>
<li>é€šè¿‡äººç±»è¡Œä¸ºå®éªŒæ¢ç©¶äº†å“ªäº›ç‰¹å¾ä½¿æŸä¸€å¯¹è±¡å®ä¾‹æ¯”å…¶ä»–å¯¹è±¡æ›´æ˜“äºè®°å¿†ã€‚</li>
<li>æå‡ºäº†é‡åŒ–è®°å¿†æ€§çš„æ–°æŒ‡æ ‡â€”â€”ç±»å†…è®°å¿†æ€§å¾—åˆ†ï¼ˆICMscoreï¼‰ï¼Œå¹¶ç»“åˆæ—¶åºé—´éš”è¿›è¡Œè®¡ç®—ã€‚</li>
<li>å»ºç«‹äº†ç±»å†…è®°å¿†æ€§æ•°æ®é›†ï¼ˆICMDï¼‰ï¼ŒåŒ…å«è¶…è¿‡äº”åƒå¼ å›¾ç‰‡å’Œç›¸åº”çš„ICMscoreå¾—åˆ†ã€‚</li>
<li>é«˜ICMscoreå›¾åƒåœ¨AIçš„è¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸Šé€ æˆæ€§èƒ½æŒ‘æˆ˜ï¼Œè€Œä½ICMscoreå›¾åƒæœ‰åŠ©äºæé«˜æ€§èƒ½ã€‚</li>
<li>å¯¹å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜æˆ–é™ä½å›¾åƒçš„ç±»å†…è®°å¿†æ€§æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.20761v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.20761v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.20761v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.20761v5/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Self-Guidance-Boosting-Flow-and-Diffusion-Generation-on-Their-Own"><a href="#Self-Guidance-Boosting-Flow-and-Diffusion-Generation-on-Their-Own" class="headerlink" title="Self-Guidance: Boosting Flow and Diffusion Generation on Their Own"></a>Self-Guidance: Boosting Flow and Diffusion Generation on Their Own</h2><p><strong>Authors:Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, Guo-Jun Qi</strong></p>
<p>Proper guidance strategies are essential to achieve high-quality generation results without retraining diffusion and flow-based text-to-image models. Existing guidance either requires specific training or strong inductive biases of diffusion model networks, which potentially limits their ability and application scope. Motivated by the observation that artifact outliers can be detected by a significant decline in the density from a noisier to a cleaner noise level, we propose Self-Guidance (SG), which can significantly improve the quality of the generated image by suppressing the generation of low-quality samples. The biggest difference from existing guidance is that SG only relies on the sampling score function of the original diffusion or flow model at different noise levels, with no need for any tricky and expensive guidance-specific training. This makes SG highly flexible to be used in a plug-and-play manner by any diffusion or flow models. We also introduce an efficient variant of SG, named SG-prev, which reuses the output from the immediately previous diffusion step to avoid additional forward passes of the diffusion network.We conduct extensive experiments on text-to-image and text-to-video generation with different architectures, including UNet and transformer models. With open-sourced diffusion models such as Stable Diffusion 3.5 and FLUX, SG exceeds existing algorithms on multiple metrics, including both FID and Human Preference Score. SG-prev also achieves strong results over both the baseline and the SG, with 50 percent more efficiency. Moreover, we find that SG and SG-prev both have a surprisingly positive effect on the generation of physiologically correct human body structures such as hands, faces, and arms, showing their ability to eliminate human body artifacts with minimal efforts. We have released our code at <a target="_blank" rel="noopener" href="https://github.com/maple-research-lab/Self-Guidance">https://github.com/maple-research-lab/Self-Guidance</a>. </p>
<blockquote>
<p>é€‚å½“çš„æŒ‡å¯¼ç­–ç•¥å¯¹äºåœ¨ä¸é‡æ–°è®­ç»ƒæ‰©æ•£å’ŒåŸºäºæµçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡çš„ç”Ÿæˆç»“æœè‡³å…³é‡è¦ã€‚ç°æœ‰çš„æŒ‡å¯¼æ–¹æ³•è¦ä¹ˆéœ€è¦ç‰¹å®šçš„è®­ç»ƒï¼Œè¦ä¹ˆéœ€è¦æ‰©æ•£æ¨¡å‹ç½‘ç»œçš„å¼ºçƒˆå½’çº³åè§ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶èƒ½åŠ›å’Œåº”ç”¨èŒƒå›´ã€‚æˆ‘ä»¬å—åˆ°è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œå³å¯ä»¥é€šè¿‡ä»è¾ƒå˜ˆæ‚åˆ°è¾ƒå¹²å‡€å™ªå£°æ°´å¹³çš„å¯†åº¦æ˜¾ç€ä¸‹é™æ¥æ£€æµ‹å¼‚å¸¸å€¼ï¼Œå› æ­¤æå‡ºäº†è‡ªå¼•å¯¼ï¼ˆSGï¼‰ï¼Œå¯ä»¥é€šè¿‡æŠ‘åˆ¶ä½è´¨é‡æ ·æœ¬çš„ç”Ÿæˆæ¥æ˜¾ç€æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚ä¸ç°æœ‰æŒ‡å¯¼æ–¹æ³•æœ€å¤§çš„ä¸åŒæ˜¯ï¼ŒSGä»…ä¾èµ–äºä¸åŒå™ªå£°çº§åˆ«çš„åŸå§‹æ‰©æ•£æˆ–æµæ¨¡å‹çš„é‡‡æ ·åˆ†æ•°å‡½æ•°ï¼Œæ— éœ€ä»»ä½•å¤æ‚ä¸”æ˜‚è´µçš„ç‰¹å®šæŒ‡å¯¼è®­ç»ƒã€‚è¿™ä½¿å¾—SGé«˜åº¦çµæ´»ï¼Œå¯ä»¥ä»¥å³æ’å³ç”¨æ–¹å¼ç”¨äºä»»ä½•æ‰©æ•£æˆ–æµæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†SGçš„ä¸€ä¸ªé«˜æ•ˆå˜ä½“ï¼Œåä¸ºSG-prevï¼Œå®ƒé‡ç”¨å‰ä¸€ä¸ªæ‰©æ•£æ­¥éª¤çš„è¾“å‡ºï¼Œä»¥é¿å…æ‰©æ•£ç½‘ç»œçš„å‰å‘ä¼ é€’ã€‚æˆ‘ä»¬åœ¨ä¸åŒæ¶æ„çš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬UNetå’Œtransformeræ¨¡å‹ã€‚ä½¿ç”¨å¼€æºæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusion 3.5å’ŒFLUXï¼‰ï¼ŒSGåœ¨å¤šä¸ªæŒ‡æ ‡ï¼ˆåŒ…æ‹¬FIDå’Œäººç±»åå¥½å¾—åˆ†ï¼‰ä¸Šè¶…è¿‡äº†ç°æœ‰ç®—æ³•ã€‚SG-prevåœ¨åŸºå‡†æµ‹è¯•å’ŒSGæ–¹é¢éƒ½å–å¾—äº†å¼ºå¤§ç»“æœï¼Œå¹¶ä¸”æ•ˆç‡æé«˜äº†50%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°SGå’ŒSG-prevåœ¨å¯¹ç”Ÿç†ä¸Šæ­£ç¡®çš„äººç±»èº«ä½“ç»“æ„ï¼ˆå¦‚æ‰‹ã€è„¸å’Œæ‰‹è‡‚ï¼‰çš„ç”Ÿæˆæ–¹é¢äº§ç”Ÿäº†ä»¤äººæƒŠè®¶çš„ç§¯æå½±å“ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬ä»¥æœ€å°çš„åŠªåŠ›æ¶ˆé™¤äººä½“ä¼ªå½±çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/maple-research-lab/Self-Guidance%E3%80%82">https://github.com/maple-research-lab/Self-Guidanceã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05827v5">PDF</a> 16 pages, 13 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSelf-Guidanceï¼ˆSGï¼‰çš„æ–°ç­–ç•¥ï¼Œç”¨äºæé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹çš„ç”Ÿæˆå›¾åƒè´¨é‡ã€‚SGç­–ç•¥é€šè¿‡æŠ‘åˆ¶ä½è´¨é‡æ ·æœ¬çš„ç”Ÿæˆï¼Œæ”¹è¿›äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚å…¶æœ€å¤§ç‰¹ç‚¹æ˜¯ä¸éœ€è¦å¤æ‚ä¸”æ˜‚è´µçš„ç‰¹å®šæŒ‡å¯¼è®­ç»ƒï¼Œä»…ä¾èµ–äºä¸åŒå™ªå£°æ°´å¹³çš„æ‰©æ•£æˆ–æµæ¨¡å‹çš„é‡‡æ ·åˆ†æ•°å‡½æ•°ï¼Œä½¿å…¶å…·æœ‰é«˜åº¦çµæ´»æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§æ‰©æ•£å’Œæµæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†SGçš„å˜ä½“SG-prevï¼Œå®ƒåˆ©ç”¨å‰ä¸€ä¸ªæ‰©æ•£æ­¥éª¤çš„è¾“å‡ºï¼Œé¿å…é¢å¤–çš„ç½‘ç»œå‰å‘ä¼ é€’ã€‚å®éªŒè¯æ˜ï¼ŒSGåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰ç®—æ³•ï¼ŒåŒ…æ‹¬FIDå’Œäººç±»åå¥½å¾—åˆ†ã€‚SG-prevåœ¨åŸºçº¿æµ‹è¯•å’Œæ ‡å‡†SGä¸­éƒ½å–å¾—äº†å¼ºåŠ²ç»“æœï¼Œæ•ˆç‡æé«˜äº†50%ã€‚SGå’ŒSG-prevåœ¨ç”Ÿæˆç”Ÿç†æ­£ç¡®çš„äººä½“ç»“æ„ï¼ˆå¦‚æ‰‹ã€è„¸ã€æ‰‹è‡‚ç­‰ï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶æ¶ˆé™¤äººä½“ä¼ªå½±çš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Self-Guidanceç­–ç•¥é€šè¿‡æŠ‘åˆ¶ä½è´¨é‡æ ·æœ¬ç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥ç­–ç•¥ä»…ä¾èµ–äºä¸åŒå™ªå£°æ°´å¹³çš„åŸå§‹æ‰©æ•£æˆ–æµæ¨¡å‹çš„é‡‡æ ·åˆ†æ•°å‡½æ•°ï¼Œæ— éœ€ç‰¹å®šçš„æŒ‡å¯¼è®­ç»ƒï¼Œå…·æœ‰é«˜åº¦çµæ´»æ€§ã€‚</li>
<li>SGçš„å˜ä½“SG-prevé€šè¿‡åˆ©ç”¨å‰ä¸€ä¸ªæ‰©æ•£æ­¥éª¤çš„è¾“å‡ºï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSGåœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå®éªŒä¸­è¶…è¶Šäº†ç°æœ‰ç®—æ³•ã€‚</li>
<li>SGå’ŒSG-prevåœ¨ç”Ÿæˆç”Ÿç†æ­£ç¡®çš„äººä½“ç»“æ„æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
<li>Self-Guidanceç­–ç•¥æœ‰æœ›ä¸ºæ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹çš„åº”ç”¨å¼€è¾Ÿæ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.05827v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DynamicControl-Adaptive-Condition-Selection-for-Improved-Text-to-Image-Generation"><a href="#DynamicControl-Adaptive-Condition-Selection-for-Improved-Text-to-Image-Generation" class="headerlink" title="DynamicControl: Adaptive Condition Selection for Improved Text-to-Image   Generation"></a>DynamicControl: Adaptive Condition Selection for Improved Text-to-Image   Generation</h2><p><strong>Authors:Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Yong Liu, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>To enhance the controllability of text-to-image diffusion models, current ControlNet-like models have explored various control signals to dictate image attributes. However, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. This underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. To address this issue, we propose a novel framework, DynamicControl, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. Our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. This controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. Then, we integrate a Multimodal Large Language Model (MLLM) to build an efficient condition evaluator. This evaluator optimizes the ordering of conditions based on the double-cycle controllerâ€™s score ranking. Our method jointly optimizes MLLMs and diffusion models, utilizing MLLMsâ€™ reasoning capabilities to facilitate multi-condition text-to-image (T2I) tasks. The final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate ControlNet, thereby enhancing control over generated images. Through both quantitative and qualitative comparisons, DynamicControl demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls. </p>
<blockquote>
<p>ä¸ºäº†å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§æ€§ï¼Œå½“å‰çš„ControlNetç±»æ¨¡å‹å·²ç»æ¢ç´¢äº†å„ç§æ§åˆ¶ä¿¡å·æ¥æŒ‡ç¤ºå›¾åƒå±æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆå¤„ç†æ¡ä»¶æ•ˆç‡ä½ä¸‹ï¼Œè¦ä¹ˆä½¿ç”¨å›ºå®šæ•°é‡çš„æ¡ä»¶ï¼Œè¿™å¹¶æ²¡æœ‰å®Œå…¨è§£å†³å¤šä¸ªæ¡ä»¶çš„å¤æ‚æ€§åŠå…¶æ½œåœ¨å†²çªã€‚è¿™å¼ºè°ƒäº†å¯¹åˆ›æ–°æ–¹æ³•çš„éœ€æ±‚ï¼Œéœ€è¦æœ‰æ•ˆç®¡ç†å¤šä¸ªæ¡ä»¶ä»¥è¿›è¡Œæ›´å¯é å’Œè¯¦ç»†çš„å›¾åƒåˆæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DynamicControlï¼Œå®ƒæ”¯æŒå„ç§æ§åˆ¶ä¿¡å·çš„åŠ¨æ€ç»„åˆï¼Œå…è®¸çµæ´»é€‰æ‹©ä¸åŒæ•°é‡å’Œç±»å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å§‹äºåŒé‡å¾ªç¯æ§åˆ¶å™¨ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ï¼Œå¯¹æ‰€æœ‰è¾“å…¥æ¡ä»¶ç”Ÿæˆåˆå§‹çœŸå®åˆ†æ•°æ’åºã€‚è¯¥æ§åˆ¶å™¨è¯„ä¼°æå–çš„æ¡ä»¶ä¸è¾“å…¥æ¡ä»¶ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠä¸æºå›¾åƒçš„åƒç´ çº§ç›¸ä¼¼æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬é›†æˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥æ„å»ºæœ‰æ•ˆçš„æ¡ä»¶è¯„ä¼°å™¨ã€‚è¯¥è¯„ä¼°å™¨æ ¹æ®åŒé‡å¾ªç¯æ§åˆ¶å™¨çš„åˆ†æ•°æ’åä¼˜åŒ–æ¡ä»¶çš„é¡ºåºã€‚æˆ‘ä»¬çš„æ–¹æ³•è”åˆä¼˜åŒ–äº†MLLMså’Œæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨MLLMsçš„æ¨ç†èƒ½åŠ›ä¿ƒè¿›å¤šæ¡ä»¶æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡ã€‚æœ€ç»ˆçš„æ’åºæ¡ä»¶è¢«è¾“å…¥åˆ°å¹¶è¡Œå¤šæ§åˆ¶é€‚é…å™¨ä¸­ï¼Œè¯¥é€‚é…å™¨ä»åŠ¨æ€è§†è§‰æ¡ä»¶ä¸­å­¦ä¹ ç‰¹å¾æ˜ å°„å¹¶å°†å…¶é›†æˆä»¥è°ƒåˆ¶ControlNetï¼Œä»è€Œå¢å¼ºå¯¹ç”Ÿæˆå›¾åƒçš„æ§åˆ¶ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§æ¯”è¾ƒï¼ŒDynamicControlåœ¨å¯æ§æ€§ã€ç”Ÿæˆè´¨é‡å’Œå„ç§æ¡ä»¶æ§åˆ¶çš„ç»„åˆèƒ½åŠ›æ–¹é¢å‡è¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03255v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDynamicControlçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§æ€§ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§æ§åˆ¶ä¿¡å·åŠ¨æ€ç»„åˆï¼Œå¯é€‚åº”æ€§åœ°é€‰æ‹©ä¸åŒæ•°é‡å’Œç±»å‹çš„æ¡ä»¶ã€‚é€šè¿‡åŒé‡å¾ªç¯æ§åˆ¶å™¨ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¹¶è¡Œå¤šæ§åˆ¶é€‚é…å™¨ç­‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ç”Ÿæˆå›¾åƒçš„æ›´ç²¾ç»†æ§åˆ¶ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒDynamicControlåœ¨å¯æ§æ€§ã€ç”Ÿæˆè´¨é‡å’Œç»„åˆæ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DynamicControlæ¡†æ¶æ—¨åœ¨æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ§æ€§ï¼Œé€šè¿‡åŠ¨æ€ç»„åˆå¤šç§æ§åˆ¶ä¿¡å·å®ç°æ›´å¯é å’Œè¯¦ç»†çš„å›¾åƒåˆæˆã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒé‡å¾ªç¯æ§åˆ¶å™¨ç”Ÿæˆåˆå§‹çœŸå®åˆ†æ•°æ’åºï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹å¯¹æå–çš„æ¡ä»¶å’Œè¾“å…¥æ¡ä»¶è¿›è¡Œç›¸ä¼¼æ€§è¯„ä¼°ã€‚</li>
<li>é›†æˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»¥æ„å»ºé«˜æ•ˆçš„æ¡ä»¶è¯„ä¼°å™¨ï¼Œä¼˜åŒ–åŸºäºåŒé‡å¾ªç¯æ§åˆ¶å™¨å¾—åˆ†æ’åºçš„æ¡ä»¶æ’åºã€‚</li>
<li>DynamicControlè”åˆä¼˜åŒ–MLLMså’Œæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨MLLMsçš„æ¨ç†èƒ½åŠ›ä¿ƒè¿›å¤šæ¡ä»¶æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡ã€‚</li>
<li>æœ€åçš„æ’åºæ¡ä»¶è¢«è¾“å…¥åˆ°å¹¶è¡Œå¤šæ§åˆ¶é€‚é…å™¨ä¸­ï¼Œè¯¥é€‚é…å™¨ä»åŠ¨æ€è§†è§‰æ¡ä»¶å­¦ä¹ ç‰¹å¾æ˜ å°„å¹¶å°†å…¶æ•´åˆåˆ°ControlNetä¸­ï¼Œä»è€Œæé«˜å¯¹ç”Ÿæˆå›¾åƒçš„æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>DynamicControlåœ¨å®šé‡å’Œå®šæ€§æ¯”è¾ƒä¸­å‡è¡¨ç°å‡ºåœ¨å¯æ§æ€§ã€ç”Ÿæˆè´¨é‡å’Œç»„åˆæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºè§£å†³ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šæ¡ä»¶æ—¶çš„å±€é™æ€§æä¾›äº†ä¸€ç§æ–°é¢–ã€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.03255v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.03255v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2412.03255v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance"><a href="#Rare-to-Frequent-Unlocking-Compositional-Generation-Power-of-Diffusion-Models-on-Rare-Concepts-with-LLM-Guidance" class="headerlink" title="Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance"></a>Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion   Models on Rare Concepts with LLM Guidance</h2><p><strong>Authors:Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho</strong></p>
<p>State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at <a target="_blank" rel="noopener" href="https://github.com/krafton-ai/Rare-to-Frequent">https://github.com/krafton-ai/Rare-to-Frequent</a>. </p>
<blockquote>
<p>å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆï¼ˆä¾‹å¦‚å…·æœ‰ä¸å¯»å¸¸å±æ€§çš„ç‰©ä½“ï¼‰æ—¶ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡å¯¼ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨è¿™ç§ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬ä»å®è¯å’Œç†è®ºåˆ†æå¼€å§‹ï¼Œè¯æ˜åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œå¯ä»¥äº§ç”Ÿæ›´ç²¾ç¡®çš„æ¦‚å¿µç»„åˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•R2Fï¼Œå®ƒåˆ©ç”¨LLMä¸­ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡æ‰©æ•£æ¨æ–­ï¼Œè§„åˆ’å’Œæ‰§è¡Œä»ç½•è§åˆ°é¢‘ç¹çš„æ•´ä½“æ¦‚å¿µæŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶ä¸”å¯ä»¥ä¸åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•æ— ç¼é›†æˆã€‚åœ¨åŒ…æ‹¬æˆ‘ä»¬æ–°æå‡ºçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†RareBenchåœ¨å†…çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR2Fåœ¨å„ç§åŒ…å«ç½•è§æ¦‚å¿µç»„åˆçš„æç¤ºä¸‹ï¼Œæ˜¾è‘—è¶…è¶Šäº†SD3.0å’ŒFLUXæ¨¡å‹ï¼Œåœ¨T2Iå¯¹é½æ–¹é¢çš„æ€§èƒ½æé«˜äº†é«˜è¾¾28.1%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/krafton-ai/Rare-to-Frequent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/krafton-ai/Rare-to-Frequentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22376v3">PDF</a> ICLR 2025 (spotlight)</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å±•ç¤ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹åœ¨ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å®è¯å’Œç†è®ºåˆ†æï¼Œè¡¨æ˜åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¿›è¡Œæ¦‚å¿µç»„åˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•R2Fï¼Œåˆ©ç”¨LLMä¸­çš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†ï¼Œé€šè¿‡æ‰©æ•£æ¨ç†ï¼Œå®ç°ç½•è§åˆ°é¢‘ç¹çš„æ¦‚å¿µæŒ‡å¯¼ã€‚è¯¥æ–¹æ³•çµæ´»é€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶å¯æ— ç¼é›†æˆåˆ°åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•ä¸­ã€‚åœ¨åŒ…æ‹¬æ–°æå‡ºçš„ç½•è§æ¦‚å¿µåŸºå‡†æµ‹è¯•é›†RareBenchåœ¨å†…çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR2Fåœ¨T2Iå¯¹é½æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬SD3.0å’ŒFLUXï¼Œæé«˜äº†é«˜è¾¾28.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç½•è§æ¦‚å¿µç»„åˆæ—¶å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼æœ‰åŠ©äºæé«˜æ‰©æ•£æ¨¡å‹åœ¨ç½•è§æ¦‚å¿µä¸Šçš„ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æš´éœ²ä¸ç›®æ ‡ç½•è§æ¦‚å¿µç›¸å…³çš„é¢‘ç¹æ¦‚å¿µï¼Œèƒ½æ›´å‡†ç¡®è¿›è¡Œæ¦‚å¿µç»„åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•R2Fï¼Œåˆ©ç”¨LLMçš„ä¸°å¯Œè¯­ä¹‰çŸ¥è¯†ï¼Œå®ç°ç½•è§åˆ°é¢‘ç¹çš„æ¦‚å¿µæŒ‡å¯¼ã€‚</li>
<li>R2Fæ–¹æ³•çµæ´»é€‚ç”¨äºå„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’ŒLLMï¼Œå¹¶å¯é›†æˆåˆ°åŒºåŸŸå¼•å¯¼æ‰©æ•£æ–¹æ³•ä¸­ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR2Fæ˜¾è‘—æé«˜äº†T2Iå¯¹é½æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–°æå‡ºçš„ç½•è§æ¦‚å¿µåŸºå‡†æµ‹è¯•é›†RareBenchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.22376v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.22376v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.22376v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.22376v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.22376v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Diffusion-Curriculum-Synthetic-to-Real-Data-Curriculum-via-Image-Guided-Diffusion"><a href="#Diffusion-Curriculum-Synthetic-to-Real-Data-Curriculum-via-Image-Guided-Diffusion" class="headerlink" title="Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided   Diffusion"></a>Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided   Diffusion</h2><p><strong>Authors:Yijun Liang, Shweta Bhardwaj, Tianyi Zhou</strong></p>
<p>Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic imagesâ€™ proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel â€œDiffusion Curriculum (DisCL)â€. DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base modelâ€™s tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy. </p>
<blockquote>
<p>åœ¨å®è·µä¸­ï¼Œä½è´¨é‡æˆ–ç¨€ç¼ºæ•°æ®ä¸ºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ç»å…¸çš„æ•°æ®å¢å¼ºæ— æ³•æä¾›éå¸¸ä¸åŒçš„æ–°æ•°æ®ï¼Œä½†æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬å¼•å¯¼æç¤ºç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ï¼Œä¸ºæ„å»ºè‡ªæˆ‘è¿›åŒ–çš„AIæ‰“å¼€äº†æ–°é—¨ã€‚ç„¶è€Œï¼Œä»…ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ— æ³•æ§åˆ¶åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒçš„æ¥è¿‘ç¨‹åº¦ï¼Œä»è€Œå¯¼è‡´å¯¹æ¨¡å‹æ€§èƒ½æœ‰å®³çš„åˆ†å¸ƒå¤–æ•°æ®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å›¾åƒæŒ‡å¯¼ï¼Œä»¥å®ç°åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„é¢‘è°±æ’å€¼ã€‚é€šè¿‡æ›´å¼ºçš„å›¾åƒæŒ‡å¯¼ï¼Œç”Ÿæˆçš„å›¾åƒä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Œä½†éš¾ä»¥å­¦ä¹ ã€‚è€Œè¾ƒå¼±çš„å›¾åƒæŒ‡å¯¼ä¼šä½¿åˆæˆå›¾åƒæ›´å®¹æ˜“è¢«æ¨¡å‹æ¥å—ï¼Œä½†ä¸åŸå§‹æ•°æ®çš„åˆ†å¸ƒå·®è·æ›´å¤§ã€‚ç”Ÿæˆçš„å…¨æ•°æ®é¢‘è°±ä½¿æˆ‘ä»¬èƒ½å¤Ÿå»ºç«‹ä¸€ç§æ–°çš„â€œæ‰©æ•£è¯¾ç¨‹ï¼ˆDisCLï¼‰â€ã€‚DisCLé’ˆå¯¹æ¯ä¸ªè®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆçš„å›¾åƒæŒ‡å¯¼çº§åˆ«ï¼šå®ƒè¯†åˆ«å¹¶ä¸“æ³¨äºæ¨¡å‹çš„å›°éš¾æ ·æœ¬ï¼Œå¹¶è¯„ä¼°åˆæˆå›¾åƒçš„æœ€æœ‰æ•ˆæŒ‡å¯¼çº§åˆ«ï¼Œä»¥æé«˜å›°éš¾æ•°æ®çš„å­¦ä¹ æ•ˆæœã€‚æˆ‘ä»¬å°†DisCLåº”ç”¨äºä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼šé•¿å°¾ï¼ˆLTï¼‰åˆ†ç±»å’Œå­¦ä¹ ä½è´¨é‡æ•°æ®ã€‚å®ƒä¸“æ³¨äºé«˜è´¨é‡ä½†æŒ‡å¯¼è¾ƒå°‘çš„å›¾åƒï¼Œä»¥å­¦ä¹ å…¸å‹ç‰¹å¾ï¼Œä½œä¸ºå­¦ä¹ å¤šæ ·æ€§æˆ–è´¨é‡å¯èƒ½è¾ƒå¼±çš„è¾ƒé«˜æŒ‡å¯¼å›¾åƒçš„é¢„çƒ­ã€‚åœ¨iWildCamæ•°æ®é›†ä¸Šåº”ç”¨DisCLçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨OODå’ŒIDå®å‡†ç¡®ç‡ä¸Šåˆ†åˆ«æé«˜äº†2.7%å’Œ2.1%ã€‚åœ¨ImageNet-LTä¸Šï¼ŒDisCLå°†åŸºç¡€æ¨¡å‹çš„å°¾ç±»å‡†ç¡®ç‡ä»4.4%æé«˜åˆ°23.64%ï¼Œå¹¶å¯¼è‡´æ‰€æœ‰ç±»åˆ«å‡†ç¡®ç‡æé«˜4.02%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13674v4">PDF</a> Accepted in ICCV2025. 22 pages, including references and appendix.   Code is available at <a target="_blank" rel="noopener" href="http://github.com/tianyi-lab/DisCL">http://github.com/tianyi-lab/DisCL</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¸­çš„ä½è´¨é‡æˆ–ç¨€ç¼ºæ•°æ®é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆæŠ€æœ¯ã€‚ä¸ºè§£å†³æ–‡æœ¬å¼•å¯¼ç”Ÿæˆå›¾åƒæ—¶äº§ç”Ÿçš„åˆ†å¸ƒå¤–æ•°æ®é—®é¢˜ï¼Œå¼•å…¥äº†å›¾åƒæŒ‡å¯¼ç­–ç•¥ã€‚å¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ‰©æ•£è¯¾ç¨‹ï¼ˆDisCLï¼‰ï¼Œæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆä¸­çš„å›¾åƒæŒ‡å¯¼æ°´å¹³ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡é•¿æœŸåˆ†ç±»å’Œä½è´¨é‡æ•°æ®å­¦ä¹ ä»»åŠ¡éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å¯¹äºè§£å†³æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡åˆ†ç±»å’Œä½è´¨é‡æ•°æ®é›†æ–¹é¢å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½è´¨é‡æˆ–ç¨€ç¼ºæ•°æ®å¯¹è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æŒ‡å¯¼ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚ä½†çº¯æ–‡æœ¬æŒ‡å¯¼å­˜åœ¨ç”Ÿæˆå›¾åƒä¸åŸå›¾åƒè·ç¦»ä¸å¯æ§çš„é—®é¢˜ï¼Œå¯èƒ½äº§ç”Ÿä¸åˆ©äºæ¨¡å‹æ€§èƒ½çš„åˆ†å¤–æ•°æ®ã€‚</li>
<li>å›¾åƒæŒ‡å¯¼ç­–ç•¥ç”¨äºè§£å†³ä¸Šè¿°æ–‡æœ¬å¼•å¯¼çš„é—®é¢˜ï¼Œä»¥å®ç°åˆæˆä¸çœŸå®å›¾åƒä¹‹é—´çš„ä¸åŒæ’å€¼æ°´å¹³ã€‚ </li>
<li>å¼ºå›¾åƒæŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒç›¸ä¼¼äºè®­ç»ƒæ•°æ®ä½†éš¾ä»¥å­¦ä¹ ï¼›å¼±å›¾åƒæŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒå­¦ä¹ è¾ƒå®¹æ˜“ä½†ä¸å®é™…æ•°æ®åˆ†å¸ƒå·®è·è¾ƒå¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œæ‰©æ•£è¯¾ç¨‹ï¼ˆDisCLï¼‰â€çš„æ–°æ–¹æ³•ï¼Œæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´å›¾åƒåˆæˆçš„å›¾åƒæŒ‡å¯¼æ°´å¹³ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å›°éš¾æ ·æœ¬çš„å­¦ä¹ æ•ˆæœã€‚ </li>
<li>åœ¨é•¿æœŸåˆ†ç±»å’Œä½è´¨é‡æ•°æ®å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œåº”ç”¨DisCLå¯æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.13674v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.13674v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.13674v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.13674v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models"><a href="#Leveraging-Model-Guidance-to-Extract-Training-Data-from-Personalized-Diffusion-Models" class="headerlink" title="Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models"></a>Leveraging Model Guidance to Extract Training Data from Personalized   Diffusion Models</h2><p><strong>Authors:Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu</strong></p>
<p>Diffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: â€œCan training data be extracted from these fine-tuned DMs shared online?â€ A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the modelâ€™s learned distribution â€“ from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases. The code is available <a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract">https://github.com/Nicholas0228/FineXtract</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å¾®è°ƒä¸­ï¼Œé¢„è®­ç»ƒçš„DMè¢«å¾®è°ƒä»¥æ•æ‰ç‰¹å®šçš„é£æ ¼æˆ–å¯¹è±¡ã€‚è®¸å¤šäººåœ¨çº¿ä¸Šä¼ è¿™äº›ä¸ªæ€§åŒ–çš„æ£€æŸ¥ç‚¹ï¼Œä¿ƒè¿›äº†å¦‚Civitaiå’ŒHuggingFaceç­‰ç¤¾åŒºçš„å‘å±•ã€‚ç„¶è€Œï¼Œåœ¨å‘å¸ƒå¾®è°ƒæ£€æŸ¥ç‚¹æ—¶ï¼Œæ¨¡å‹æ‰€æœ‰è€…å¯èƒ½ä¼šå¿½ç•¥æ•°æ®æ³„éœ²çš„é£é™©ã€‚æ­¤å¤–ï¼Œå½“æœªç»æˆæƒçš„æ•°æ®ç”¨äºå¾®è°ƒæ—¶ï¼Œå°±ä¼šå‡ºç°ç‰ˆæƒè¿è§„çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šâ€œå¯ä»¥ä»åœ¨çº¿å…±äº«çš„è¿™äº›å¾®è°ƒè¿‡çš„æ‰©æ•£æ¨¡å‹ä¸­æå–è®­ç»ƒæ•°æ®å—ï¼Ÿâ€æˆåŠŸçš„æå–ä¸ä»…ä¼šå¸¦æ¥æ•°æ®æ³„éœ²çš„å¨èƒï¼Œè€Œä¸”ä¹Ÿä¼šæä¾›ç‰ˆæƒä¾µæƒçš„åˆ‡å®è¯æ®ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FineXtractï¼Œä¸€ä¸ªç”¨äºæå–å¾®è°ƒæ•°æ®çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¾®è°ƒè¿‘ä¼¼ä¸ºæ¨¡å‹å­¦ä¹ åˆ†å¸ƒçš„ä¸€ä¸ªé€æ¸å˜åŒ–â€”â€”ä»åŸå§‹çš„é¢„è®­ç»ƒDMå‘å¾®è°ƒæ•°æ®è½¬å˜ã€‚é€šè¿‡å¯¹å¾®è°ƒå‰åçš„æ¨¡å‹è¿›è¡Œå¤–æ¨ï¼Œæˆ‘ä»¬å¼•å¯¼ç”Ÿæˆå‘å¾®è°ƒæ•°æ®åˆ†å¸ƒä¸­çš„é«˜æ¦‚ç‡åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨èšç±»ç®—æ³•ä»è¿™äº›ä½¿ç”¨å¤–æ¨æŒ‡å¯¼ç”Ÿæˆçš„å›¾åƒä¸­æå–æœ€å¯èƒ½çš„å›¾åƒã€‚åœ¨ä½¿ç”¨WikiArtã€DreamBoothå’Œåœ¨çº¿å‘å¸ƒçš„ç°å®ä¸–ç•Œæ£€æŸ¥ç‚¹ç­‰æ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„DMä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Nicholas0228/FineXtract">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03039v3">PDF</a> Accepted at the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒä¸­å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå°æ ·æœ¬å›¾åƒé›†å¾®è°ƒä»¥æ•æ‰ç‰¹å®šé£æ ¼æˆ–å¯¹è±¡ã€‚ä½†æ¨¡å‹å…¬å¼€æ—¶å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²é£é™©åŠç‰ˆæƒä¾µçŠ¯é—®é¢˜ã€‚æœ¬æ–‡æå‡ºâ€œFineXtractâ€æ¡†æ¶ï¼Œæ—¨åœ¨ä»åœ¨çº¿å…±äº«çš„å¾®è°ƒåçš„DMsä¸­æå–å¾®è°ƒæ•°æ®ã€‚é€šè¿‡æ¨¡æ‹Ÿå¾®è°ƒè¿‡ç¨‹ï¼Œä»é¢„è®­ç»ƒæ¨¡å‹å‘å¾®è°ƒæ•°æ®é›†å­¦ä¹ åˆ†å¸ƒçš„é€æ¸è½¬å˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¼•å¯¼ç”Ÿæˆå›¾åƒèšé›†äºå¾®è°ƒæ•°æ®åˆ†å¸ƒçš„é«˜æ¦‚ç‡åŒºåŸŸï¼Œå¹¶åˆ©ç”¨èšç±»ç®—æ³•æå–æœ€å¯èƒ½çš„å›¾åƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨WikiArtã€DreamBoothåŠåœ¨çº¿çœŸå®æ•°æ®é›†ä¸Šæœ‰æ•ˆï¼Œå¤šæ•°æƒ…å†µä¸‹å¯æå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒä¸­å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å…¬å¼€æ—¶å­˜åœ¨æ•°æ®æ³„éœ²é£é™©åŠç‰ˆæƒä¾µçŠ¯é—®é¢˜ã€‚</li>
<li>FineXtractæ¡†æ¶æ—¨åœ¨ä»åœ¨çº¿å…±äº«çš„å¾®è°ƒåçš„DMsä¸­æå–å¾®è°ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå¾®è°ƒè¿‡ç¨‹ï¼ŒFineXtractåˆ©ç”¨æ¨¡å‹å­¦ä¹ åˆ†å¸ƒçš„é€æ¸è½¬å˜æ¥å¼•å¯¼ç”Ÿæˆå›¾åƒã€‚</li>
<li>åˆ©ç”¨èšç±»ç®—æ³•æå–æœ€å¯èƒ½çš„å›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šæœ‰æ•ˆï¼Œå¯æå–çº¦20%çš„å¾®è°ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.03039v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.03039v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.03039v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.03039v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_Diffusion Models/2410.03039v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_åŒ»å­¦å›¾åƒ/2509.21531v1/page_2_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Orochi Versatile Biomedical Image Processor
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-30\./crop_NeRF/2509.15548v3/page_0_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  MS-GS Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
