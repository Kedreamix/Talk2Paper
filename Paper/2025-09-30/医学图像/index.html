<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Orochi Versatile Biomedical Image Processor">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-253247c48e5f4fe4758606f516fd9b41')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-30-æ›´æ–°"><a href="#2025-09-30-æ›´æ–°" class="headerlink" title="2025-09-30 æ›´æ–°"></a>2025-09-30 æ›´æ–°</h1><h2 id="Orochi-Versatile-Biomedical-Image-Processor"><a href="#Orochi-Versatile-Biomedical-Image-Processor" class="headerlink" title="Orochi: Versatile Biomedical Image Processor"></a>Orochi: Versatile Biomedical Image Processor</h2><p><strong>Authors:Gaole Dai, Chenghao Zhou, Yu Zhou, Rongyu Zhang, Yuan Zhang, Chengkai Hou, Tiejun Huang, Jianxu Chen, Shanghang Zhang</strong></p>
<p>Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches&#x2F;volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mambaâ€™s linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²æˆä¸ºåŠ é€Ÿç”Ÿå‘½ç§‘å­¦é¢†åŸŸç ”ç©¶çš„å…³é”®å·¥å…·ä¹‹ä¸€ï¼Œå…¶åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒçš„åº•å±‚å¤„ç†ï¼ˆä¾‹å¦‚æ³¨å†Œã€èåˆã€ä¿®å¤ã€è¶…åˆ†è¾¨ç‡ï¼‰æ–¹é¢çš„åº”ç”¨å°¤ä¸ºå…³é”®ã€‚ImageJï¼ˆFijiï¼‰å’Œnapariç­‰å¹³å°ä¸ºå„ç§æ¨¡å‹å®šåˆ¶æ’ä»¶çš„å¼€å‘æä¾›äº†æ”¯æŒã€‚ç„¶è€Œï¼Œè¿™äº›æ’ä»¶é€šå¸¸ä»…é™äºç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†ï¼Œå¯¹äºç”Ÿç‰©å­¦å®¶æ¥è¯´å®ç”¨æ€§è¾ƒä½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Orochiï¼Œè¿™æ˜¯ä¸€æ¬¾é¢å‘åº”ç”¨ã€é«˜æ•ˆä¸”é€šç”¨çš„å›¾åƒå¤„ç†å·¥å…·ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ã€‚Orochié‡‡ç”¨æˆ‘ä»¬çš„éšæœºå¤šå°ºåº¦é‡‡æ ·ç­–ç•¥ï¼Œå¯¹æ¥è‡ªè¶…è¿‡100é¡¹å…¬å¼€ç ”ç©¶çš„åŸå§‹æ•°æ®æå–çš„è¡¥ä¸&#x2F;ä½“ç§¯è¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä»»åŠ¡ç›¸å…³è”åˆåµŒå…¥é¢„è®­ç»ƒï¼ˆTJPï¼‰ï¼Œå®ƒé‡‡ç”¨ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ç›¸å…³é€€åŒ–è¿›è¡Œè‡ªç›‘ç£ï¼Œè€Œä¸æ˜¯ä¾èµ–é®æŒ¡å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ï¼Œåè€…åœ¨æ³¨å†Œç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†ä¿è¯è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬åˆ©ç”¨Mambaçš„çº¿æ€§è®¡ç®—å¤æ‚æ€§ï¼Œæ„å»ºäº†å¤šå¤´å±‚æ¬¡Mambaã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸‰å±‚å¾®è°ƒæ¡†æ¶ï¼ˆå…¨é¢ã€æ­£å¸¸å’Œè½»é‡çº§ï¼‰ï¼Œå¹¶è¯æ˜Orochiå³ä½¿ä½¿ç”¨è½»é‡çº§å‚æ•°é«˜æ•ˆé€‰é¡¹ï¼Œä¹Ÿèƒ½è¾¾åˆ°æˆ–è¶…è¶Šå½“å‰å…ˆè¿›çš„ä¸“ä¸šæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬ç ”ç©¶ä¸ºå…¨é›†æˆå·¥ä½œæµç¨‹çš„å‘å±•åšå‡ºè´¡çŒ®ï¼Œä»è€Œå‡è½»ç”Ÿç‰©å­¦å®¶åœ¨é€‰æ‹©ä¼—å¤šæ¨¡å‹æ–¹é¢çš„ç¹é‡ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22583v1">PDF</a> accepted by 39th Conference on Neural Information Processing Systems   (NeurIPS 2025) as spotlight paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ åœ¨ç”Ÿå‘½ç§‘å­¦é¢†åŸŸä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†Orochiå›¾åƒå¤„ç†å·¥å…·ã€‚Orochié€šè¿‡é‡‡ç”¨å¤šç§ç­–ç•¥å¦‚éšæœºå¤šå°ºåº¦é‡‡æ ·å’Œé¢å‘ä»»åŠ¡è”åˆåµŒå…¥é¢„è®­ç»ƒï¼Œæé«˜äº†å¯¹å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œå®ƒé‡‡ç”¨é«˜æ•ˆçš„è®¡ç®—ç­–ç•¥ä»¥å¤šå±‚æ¬¡çš„å¾®è°ƒæ¡†æ¶å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œæ—¨åœ¨ä¸ºç”Ÿç‰©å­¦å®¶æä¾›ä¸€ä¸ªé›†æˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç”Ÿå‘½ç§‘å­¦ç ”ç©¶ä¸­çš„åŠ é€Ÿä½œç”¨æ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒå¤„ç†æ–¹é¢ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„å›¾åƒå¤„ç†å·¥å…·å¦‚ImageJå’Œnapariè™½ç„¶å…è®¸å®šåˆ¶æ’ä»¶ï¼Œä½†å®ƒä»¬åœ¨ç‰¹å®šä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å±€é™æ€§é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚</li>
<li>Orochiæ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒå¤„ç†çš„ç»¼åˆæ€§å·¥å…·ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œç­–ç•¥ä¼˜åŒ–æé«˜äº†å¯¹å„ç§ä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>Orochié‡‡ç”¨éšæœºå¤šå°ºåº¦é‡‡æ ·ç­–ç•¥ï¼ŒåŸºäºè¶…è¿‡100é¡¹å…¬å¼€ç ”ç©¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Task-related Joint-embedding Pre-Trainingï¼ˆTJPï¼‰æ–¹æ³•ç”¨äºè‡ªæˆ‘ç›‘ç£ï¼Œä¸ä¾èµ–Mask Image Modellingï¼ˆMIMï¼‰ï¼Œåè€…åœ¨æŸäº›ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>Orochiåˆ©ç”¨é«˜æ•ˆçš„è®¡ç®—ç­–ç•¥å®ç°çº¿æ€§è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶é€šè¿‡å¤šå±‚æ¬¡å¾®è°ƒæ¡†æ¶å®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-733044402011a05e6895782afbfafa5c" align="middle">
<img src="https://picx.zhimg.com/v2-339c6dc606ffd065248d309940a9535d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e42746d7b7d8424b39c5194b2c27c8ca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="U-MAN-U-Net-with-Multi-scale-Adaptive-KAN-Network-for-Medical-Image-Segmentation"><a href="#U-MAN-U-Net-with-Multi-scale-Adaptive-KAN-Network-for-Medical-Image-Segmentation" class="headerlink" title="U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image   Segmentation"></a>U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image   Segmentation</h2><p><strong>Authors:Bohan Huang, Qianyun Bao, Haoyuan Ma</strong></p>
<p>Medical image segmentation faces significant challenges in preserving fine-grained details and precise boundaries due to complex anatomical structures and pathological regions. These challenges primarily stem from two key limitations of conventional U-Net architectures: (1) their simple skip connections ignore the encoder-decoder semantic gap between various features, and (2) they lack the capability for multi-scale feature extraction in deep layers. To address these challenges, we propose the U-Net with Multi-scale Adaptive KAN (U-MAN), a novel architecture that enhances the emerging Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN). Our PAGF module replaces the simple skip connection, using attention to fuse features from the encoder and decoder. The MAN module enables the network to adaptively process features at multiple scales, improving its ability to segment objects of various sizes. Experiments on three public datasets (BUSI, GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods, particularly in defining accurate boundaries and preserving fine details. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¿ç•™ç²¾ç»†ç²’åº¦å’Œç²¾ç¡®è¾¹ç•Œæ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸæ‰€å¯¼è‡´çš„ã€‚è¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºä¼ ç»ŸU-Netæ¶æ„çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰å…¶ç®€å•çš„è·³è·ƒè¿æ¥å¿½ç•¥äº†å„ç§ç‰¹å¾ä¹‹é—´çš„ç¼–ç å™¨-è§£ç å™¨è¯­ä¹‰é¸¿æ²Ÿï¼›ï¼ˆ2ï¼‰å®ƒä»¬åœ¨æ·±å±‚ä¸­ç¼ºä¹å¤šå°ºåº¦ç‰¹å¾æå–çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰å¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆU-MANï¼‰çš„U-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªä¸“ç”¨æ¨¡å—å¢å¼ºäº†æ–°å…´çš„Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼šæ¸è¿›å¼æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾èåˆï¼ˆPAGFï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆMANï¼‰ã€‚æˆ‘ä»¬çš„PAGFæ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›èåˆç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ï¼Œæ›¿æ¢äº†ç®€å•çš„è·³è·ƒè¿æ¥ã€‚MANæ¨¡å—ä½¿ç½‘ç»œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œæé«˜äº†åˆ†å‰²å„ç§å°ºå¯¸ç‰©ä½“çš„èƒ½åŠ›ã€‚åœ¨BUSIã€GLASå’ŒCVCä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒU-MANä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å®šä¹‰ç²¾ç¡®è¾¹ç•Œå’Œä¿ç•™ç»†èŠ‚æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22444v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºKolmogorov-Arnoldç½‘ç»œçš„æ”¹è¿›æ¨¡å‹U-Net Multi-scale Adaptive KANï¼ˆU-MANï¼‰ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç²¾ç»†ç²’åº¦ç»†èŠ‚ä¿ç•™å’Œç²¾ç¡®è¾¹ç•Œä¿æŒé—®é¢˜ã€‚é€šè¿‡å¼•å…¥Progressive Attention-Guided Feature Fusionï¼ˆPAGFï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆMANï¼‰ä¸¤ä¸ªæ¨¡å—ï¼Œè§£å†³äº†ä¼ ç»ŸU-Netæ¶æ„çš„è¯­ä¹‰å·®è·å’Œå¤šå°ºåº¦ç‰¹å¾æå–ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-MANåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾¹ç•Œå‡†ç¡®æ€§å’Œç»†èŠ‚ä¿ç•™æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ä¿ç•™ç²¾ç»†ç²’åº¦å’Œç²¾ç¡®è¾¹ç•Œçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸU-Netæ¶æ„å­˜åœ¨è¯­ä¹‰å·®è·å’Œå¤šå°ºåº¦ç‰¹å¾æå–çš„é—®é¢˜ã€‚</li>
<li>U-MANæ¨¡å‹ç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä¸ä¸¤ä¸ªæ¨¡å—ï¼šProgressive Attention-Guided Feature Fusionï¼ˆPAGFï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”KANï¼ˆMANï¼‰ã€‚</li>
<li>PAGFæ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èåˆç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ï¼Œæ›¿ä»£äº†ç®€å•çš„è·³è·ƒè¿æ¥ã€‚</li>
<li>MANæ¨¡å—ä½¿ç½‘ç»œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†å¤šå°ºåº¦ç‰¹å¾ï¼Œæé«˜äº†åˆ†å‰²ä¸åŒå¤§å°ç‰©ä½“çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜U-MANåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d403ef419eb41a10e809984c08f49106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62d30f204ca3b223b4f67ad3751ebab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1751a0ea8ca65175b753b04d4a0078c8" align="middle">
<img src="https://picx.zhimg.com/v2-fef045fa5db6b771cedd5fdd31343bdb" align="middle">
<img src="https://pic1.zhimg.com/v2-0a1b504a91c38e837a0cd625847d7719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16274eb83bce5a0a14def04b21b29ccc" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Integrating-Background-Knowledge-in-Medical-Semantic-Segmentation-with-Logic-Tensor-Networks"><a href="#Integrating-Background-Knowledge-in-Medical-Semantic-Segmentation-with-Logic-Tensor-Networks" class="headerlink" title="Integrating Background Knowledge in Medical Semantic Segmentation with   Logic Tensor Networks"></a>Integrating Background Knowledge in Medical Semantic Segmentation with   Logic Tensor Networks</h2><p><strong>Authors:Luca Bergamin, Giovanna Maria Dimitri, Fabio Aiolli</strong></p>
<p>Semantic segmentation is a fundamental task in medical image analysis, aiding medical decision-making by helping radiologists distinguish objects in an image. Research in this field has been driven by deep learning applications, which have the potential to scale these systems even in the presence of noise and artifacts. However, these systems are not yet perfected. We argue that performance can be improved by incorporating common medical knowledge into the segmentation modelâ€™s loss function. To this end, we introduce Logic Tensor Networks (LTNs) to encode medical background knowledge using first-order logic (FOL) rules. The encoded rules span from constraints on the shape of the produced segmentation, to relationships between different segmented areas. We apply LTNs in an end-to-end framework with a SwinUNETR for semantic segmentation. We evaluate our method on the task of segmenting the hippocampus in brain MRI scans. Our experiments show that LTNs improve the baseline segmentation performance, especially when training data is scarce. Despite being in its preliminary stages, we argue that neurosymbolic methods are general enough to be adapted and applied to other medical semantic segmentation tasks. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œé€šè¿‡å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”ŸåŒºåˆ†å›¾åƒä¸­çš„å¯¹è±¡æ¥è¾…åŠ©åŒ»ç–—å†³ç­–ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸»è¦å—åˆ°æ·±åº¦å­¦ä¹ åº”ç”¨çš„å½±å“ï¼Œå³ä½¿å­˜åœ¨å™ªå£°å’Œä¼ªåƒï¼Œæ·±åº¦å­¦ä¹ ä¹Ÿæœ‰æ½œåŠ›æ¨åŠ¨è¿™äº›ç³»ç»Ÿçš„è§„æ¨¡åŒ–åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿå°šæœªå®Œå–„ã€‚æˆ‘ä»¬è®¤ä¸ºå¯ä»¥é€šè¿‡å°†é€šç”¨åŒ»å­¦çŸ¥è¯†çº³å…¥åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°æ¥æé«˜æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€»è¾‘å¼ é‡ç½‘ç»œï¼ˆLTNsï¼‰ï¼Œä½¿ç”¨ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰è§„åˆ™ç¼–ç åŒ»å­¦èƒŒæ™¯çŸ¥è¯†ã€‚ç¼–ç çš„è§„åˆ™ä»å¯¹ç”Ÿæˆåˆ†å‰²å½¢çŠ¶çš„çº¦æŸæ‰©å±•åˆ°ä¸åŒåˆ†å‰²åŒºåŸŸä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬åœ¨ç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­åº”ç”¨äº†LTNsä¸SwinUNETRè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬å¯¹è„‘MRIæ‰«æä¸­æµ·é©¬ä½“çš„åˆ†å‰²ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒLTNsæé«˜äº†åŸºçº¿åˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚å°½ç®¡ä»å¤„äºåˆæ­¥é˜¶æ®µï¼Œæˆ‘ä»¬è®¤ä¸ºç¥ç»ç¬¦å·æ–¹æ³•è¶³å¤Ÿé€šç”¨ï¼Œå¯ä»¥é€‚åº”å¹¶åº”ç”¨äºå…¶ä»–åŒ»å­¦è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22399v1">PDF</a> Accepted at TAIM@IJCNN 2025</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯åŒ»å­¦å†³ç­–æ”¯æŒä¸­çš„åŸºç¡€ä»»åŠ¡ä¹‹ä¸€ï¼Œé€šè¿‡å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”ŸåŒºåˆ†å›¾åƒä¸­çš„å¯¹è±¡æ¥è¾…åŠ©å†³ç­–ã€‚æœ¬ç ”ç©¶å¼•å…¥é€»è¾‘å¼ é‡ç½‘ç»œï¼ˆLTNsï¼‰æ¥ç¼–ç åŒ»å­¦èƒŒæ™¯çŸ¥è¯†ï¼Œå¹¶å°†å…¶åº”ç”¨äºåŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒLTNsèƒ½æé«˜åŸºçº¿åˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­æ˜¯ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œæœ‰åŠ©äºæ”¾å°„ç§‘åŒ»ç”ŸåŒºåˆ†å›¾åƒä¸­çš„å¯¹è±¡ï¼Œä»è€Œæ”¯æŒåŒ»å­¦å†³ç­–ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„åº”ç”¨æ¨åŠ¨äº†ç ”ç©¶å‘å±•ï¼Œå¹¶å…·å¤‡åœ¨å­˜åœ¨å™ªå£°å’Œä¼ªå½±çš„æƒ…å†µä¸‹æ‰©å±•ç³»ç»Ÿçš„æ½œåŠ›ã€‚</li>
<li>åœ¨åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸­èå…¥åŒ»å­¦å¸¸è¯†èƒ½æé«˜æ€§èƒ½ã€‚</li>
<li>é€»è¾‘å¼ é‡ç½‘ç»œï¼ˆLTNsï¼‰è¢«å¼•å…¥ä»¥ä½¿ç”¨ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰è§„åˆ™ç¼–ç åŒ»å­¦èƒŒæ™¯çŸ¥è¯†ã€‚</li>
<li>LTNså¯åº”ç”¨äºç«¯å¯¹ç«¯æ¡†æ¶ä¸­çš„SwinUNETRè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>åœ¨å¤§è„‘MRIæ‰«æä¸­åˆ†å‰²æµ·é©¬çš„å®éªŒä¸­ï¼ŒLTNsæ”¹å–„äº†åŸºçº¿åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a86de74f1fa89c1820314644703bf813" align="middle">
<img src="https://picx.zhimg.com/v2-9d0574104b852cc259be49b3e0ad647e" align="middle">
<img src="https://picx.zhimg.com/v2-856424e6488d21f0caab6f8f8591163e" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks"><a href="#Beyond-Classification-Accuracy-Neural-MedBench-and-the-Need-for-Deeper-Reasoning-Benchmarks" class="headerlink" title="Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks"></a>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper   Reasoning Benchmarks</h2><p><strong>Authors:Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang, Yuanyuan Peng, Huan Gao, Mingkun Xu, Shangyang Li</strong></p>
<p>Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at <a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a> as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ ‡å‡†åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œç„¶è€Œå…¶çœŸæ­£çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚ç°æœ‰æ•°æ®é›†ä¸»è¦å¼ºè°ƒåˆ†ç±»å‡†ç¡®æ€§ï¼Œè¿™å¯¼è‡´äº†ä¸€ç§è¯„ä¼°é”™è§‰ï¼Œå³æ¨¡å‹è™½ç„¶è¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†åœ¨é«˜é£é™©è¯Šæ–­æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†Neural-MedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘è€Œæ¨ç†å¯†é›†å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºæ¢ç´¢ç¥ç»å­¦ä¸´åºŠæ¨ç†çš„æé™ã€‚Neural-MedBenché›†æˆäº†å¤šåºåˆ—MRIæ‰«æã€ç»“æ„åŒ–ç”µå­ç—…å†å’Œä¸´åºŠç¬”è®°ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡å®¶æ—ï¼šé‰´åˆ«è¯Šæ–­ã€ç—…ç¶è¯†åˆ«å’Œç†ç”±ç”Ÿæˆã€‚ä¸ºäº†ç¡®ä¿å¯é çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ··åˆè¯„åˆ†ç®¡é“ï¼Œç»“åˆäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„åˆ†è€…ã€ä¸´åºŠåŒ»ç”ŸéªŒè¯å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMsçš„ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬GPT-4oã€Claude-4å’ŒMedGemmaï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸å¸¸è§„æ•°æ®é›†ç›¸æ¯”ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œæ¨ç†å¤±è´¥è€Œä¸æ˜¯æ„ŸçŸ¥é”™è¯¯ä¸»å¯¼äº†æ¨¡å‹çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†åŒå‘è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ï¼šé¢å‘å¹¿åº¦çš„å¤§å‹æ•°æ®é›†ç”¨äºç»Ÿè®¡æ³›åŒ–ï¼Œä»¥åŠé¢å‘æ·±åº¦çš„ç´§å‡‘åŸºå‡†æµ‹è¯•ï¼Œå¦‚Neural-MedBenchï¼Œç”¨äºæ¨ç†ä¿çœŸåº¦ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://neuromedbench.github.io/">https://neuromedbench.github.io/</a>ä¸Šå‘å¸ƒäº†Neural-MedBenchï¼Œä½œä¸ºä¸€ä¸ªå¼€æ”¾å’Œå¯æ‰©å±•çš„è¯Šæ–­æµ‹è¯•å¹³å°ï¼Œä»¥æŒ‡å¯¼æœªæ¥åŸºå‡†æµ‹è¯•çš„æ‰©å±•ï¼Œå¹¶èƒ½å¤Ÿå®ç°ä¸¥æ ¼è€Œç»æµçš„ä¸´åºŠå¯ä¿¡äººå·¥æ™ºèƒ½è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22258v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç¥ç»å­¦é¢†åŸŸä¸­çš„å¤šæ¨¡æ€ä¸´åºŠæ¨ç†èƒ½åŠ›è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å¹³å°Neural-MedBenchã€‚è¯¥å¹³å°é›†æˆäº†MRIæ‰«æã€ç»“æ„åŒ–ç”µå­ç—…å†å’Œä¸´åºŠç¬”è®°ç­‰æ ¸å¿ƒä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯Šæ–­é‰´åˆ«ã€ç—…ç¶è¯†åˆ«å’Œç†ç”±ç”Ÿæˆç­‰æ ¸å¿ƒä»»åŠ¡å®¶æ—ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰çš„æ•°æ®é›†ä¸»è¦ä¾§é‡äºåˆ†ç±»å‡†ç¡®æ€§ï¼Œæ— æ³•çœŸæ­£è¯„ä¼°æ¨¡å‹çš„è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€ä¸ªæ··åˆè¯„åˆ†ç®¡é“æ¥ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ï¼Œå¹¶å¯¹å½“å‰å…ˆè¿›çš„VLMsè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æ–‡ç« å¼ºè°ƒéœ€è¦Two-Axisè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ç”¨äºç»Ÿè®¡æ³›åŒ–çš„å¤§æ•°æ®é›†å’Œç”¨äºæ¨ç†å¿ å®åº¦çš„ç´§å‡‘åŸºå‡†æµ‹è¯•å¹³å°ã€‚æœ€åï¼Œä½œè€…å‘å¸ƒäº†Neural-MedBenchä½œä¸ºå¼€æ”¾å’Œå¯æ‰©å±•çš„è¯Šæ–­æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨åˆ†ç±»å‡†ç¡®æ€§ï¼Œæ— æ³•çœŸå®è¯„ä¼°æ¨¡å‹çš„è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Neural-MedBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹ç¥ç»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€ä¸´åºŠæ¨ç†åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>Neural-MedBenché›†æˆäº†MRIæ‰«æã€ç»“æ„åŒ–ç”µå­ç—…å†å’Œä¸´åºŠç¬”è®°ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>è¯„ä¼°æ–¹æ³•ç»“åˆäº†LLMè¯„åˆ†å™¨ã€ä¸´åºŠåŒ»ç”ŸéªŒè¯å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ•°æ®é›†ç›¸æ¯”ï¼Œå½“å‰å…ˆè¿›çš„VLMsåœ¨Neural-MedBenchä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>é”™è¯¯åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹å¤±è´¥çš„ä¸»è¦åŸå› æ˜¯æ¨ç†å¤±è´¥è€Œéæ„ŸçŸ¥é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-101dbbc008b15e0ecb7719ddd919518f" align="middle">
<img src="https://picx.zhimg.com/v2-00760c0a6937eb06ae90262c0d12787d" align="middle">
<img src="https://picx.zhimg.com/v2-8d732e65fb684b9a920a28b17486960d" align="middle">
<img src="https://picx.zhimg.com/v2-d3a477b11590f393fd02ed9bd0908035" align="middle">
<img src="https://picx.zhimg.com/v2-3d22a37cba0c3039329a148b6243458a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ee1e583a2088419b495f2100c61e9ea" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FoodSEM-Large-Language-Model-Specialized-in-Food-Named-Entity-Linking"><a href="#FoodSEM-Large-Language-Model-Specialized-in-Food-Named-Entity-Linking" class="headerlink" title="FoodSEM: Large Language Model Specialized in Food Named-Entity Linking"></a>FoodSEM: Large Language Model Specialized in Food Named-Entity Linking</h2><p><strong>Authors:Ana Gjorgjevikj, Matej Martinc, Gjorgjina Cenikj, SaÅ¡o DÅ¾eroski, Barbara KorouÅ¡iÄ‡ Seljak, Tome Eftimov</strong></p>
<p>This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source large language model (LLM) for named-entity linking (NEL) to food-related ontologies. To the best of our knowledge, food NEL is a task that cannot be accurately solved by state-of-the-art general-purpose (large) language models or custom domain-specific models&#x2F;systems. Through an instruction-response (IR) scenario, FoodSEM links food-related entities mentioned in a text to several ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM model achieves state-of-the-art performance compared to related models&#x2F;systems, with F1 scores even reaching 98% on some ontologies and datasets. The presented comparative analyses against zero-shot, one-shot, and few-shot LLM prompting baselines further highlight FoodSEMâ€™s superior performance over its non-fine-tuned version. By making FoodSEM and its related resources publicly available, the main contributions of this article include (1) publishing a food-annotated corpora into an IR format suitable for LLM fine-tuning&#x2F;evaluation, (2) publishing a robust model to advance the semantic understanding of text in the food domain, and (3) providing a strong baseline on food NEL for future benchmarking. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†FoodSEMï¼Œè¿™æ˜¯ä¸€ç§æœ€æ–°å¾®è°ƒè¿‡çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå‘½åå®ä½“é“¾æ¥ï¼ˆNELï¼‰åˆ°ä¸é£Ÿå“ç›¸å…³çš„æœ¬ä½“ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œé£Ÿå“NELæ˜¯ä¸€é¡¹ä»»åŠ¡ï¼Œæ— æ³•ç”±æœ€æ–°é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æˆ–å®šåˆ¶çš„é¢†åŸŸç‰¹å®šæ¨¡å‹&#x2F;ç³»ç»Ÿå‡†ç¡®è§£å†³ã€‚é€šè¿‡æŒ‡ä»¤-å“åº”ï¼ˆIRï¼‰åœºæ™¯ï¼ŒFoodSEMå°†æ–‡æœ¬ä¸­æåˆ°çš„ä¸é£Ÿå“ç›¸å…³çš„å®ä½“é“¾æ¥åˆ°å¤šä¸ªæœ¬ä½“ï¼ŒåŒ…æ‹¬FoodOnã€SNOMED-CTå’Œæ±‰è¨åˆ†ç±»æ³•ã€‚FoodSEMæ¨¡å‹ä¸ç›¸å…³çš„æ¨¡å‹&#x2F;ç³»ç»Ÿç›¸æ¯”ï¼Œè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨æŸäº›æœ¬ä½“å’Œæ•°æ®é›†ä¸Šçš„F1åˆ†æ•°ç”šè‡³è¾¾åˆ°98%ã€‚ä¸é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºåŸºçº¿ç›¸æ¯”çš„åˆ†æè¿›ä¸€æ­¥çªå‡ºäº†FoodSEMåœ¨éå¾®è°ƒç‰ˆæœ¬ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚é€šè¿‡å°†FoodSEMåŠå…¶ç›¸å…³èµ„æºå…¬å¼€å¯ç”¨ï¼Œæœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å°†é£Ÿå“æ³¨é‡Šè¯­æ–™åº“å‘å¸ƒä¸ºé€‚åˆå¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒ&#x2F;è¯„ä¼°çš„IRæ ¼å¼ï¼Œï¼ˆ2ï¼‰å‘å¸ƒä¸€ä¸ªç¨³å¥çš„æ¨¡å‹ï¼Œä»¥æ¨è¿›é£Ÿå“é¢†åŸŸçš„æ–‡æœ¬è¯­ä¹‰ç†è§£ï¼Œï¼ˆ3ï¼‰ä¸ºæœªæ¥çš„åŸºå‡†æµ‹è¯•æä¾›é£Ÿå“NELçš„å¼ºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22125v1">PDF</a> To appear in the Proceedings of the 28th International Conference on   Discovery Science (DS 2025)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†FoodSEMï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é£Ÿå“ç›¸å…³æœ¬ä½“å‘½åå®ä½“é“¾æ¥ä»»åŠ¡çš„å…ˆè¿›å¾®è°ƒå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚FoodSEMåœ¨é£Ÿå“é¢†åŸŸçš„æ–‡æœ¬è¯­ä¹‰ç†è§£ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰æ¨¡å‹&#x2F;ç³»ç»Ÿç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ä»¥ä¾›æœªæ¥è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FoodSEMæ˜¯ä¸€ä¸ªé’ˆå¯¹é£Ÿå“ç›¸å…³æœ¬ä½“å‘½åå®ä½“é“¾æ¥ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>FoodSEMèƒ½å¤Ÿé“¾æ¥æ–‡æœ¬ä¸­çš„é£Ÿå“ç›¸å…³å®ä½“åˆ°å¤šä¸ªæœ¬ä½“ï¼ŒåŒ…æ‹¬FoodOnã€SNOMED-CTå’ŒHansardåˆ†ç±»æ³•ã€‚</li>
<li>FoodSEMå®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œä¸ç›¸å…³é¢†åŸŸæ¨¡å‹&#x2F;ç³»ç»Ÿç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼ŒF1åˆ†æ•°åœ¨æŸäº›æœ¬ä½“å’Œæ•°æ®é›†ä¸Šç”šè‡³è¾¾åˆ°98%ã€‚</li>
<li>FoodSEMé€šè¿‡æŒ‡ä»¤å“åº”åœºæ™¯è¿›è¡Œå·¥ä½œã€‚</li>
<li>FoodSEMåŠå…¶ç›¸å…³èµ„æºå·²å…¬å¼€å‘å¸ƒï¼Œä¸ºé£Ÿå“é¢†åŸŸçš„æ–‡æœ¬è¯­ä¹‰ç†è§£åšå‡ºäº†ä¸»è¦è´¡çŒ®ã€‚</li>
<li>FoodSEMçš„å‘å¸ƒæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºå‡†ï¼Œä¾›æœªæ¥åœ¨é£Ÿå“å‘½åå®ä½“é“¾æ¥ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5e6c28920b031814fb129b4e8dc3abf" align="middle">
<img src="https://picx.zhimg.com/v2-21bc9e197f564cb1eb686eedc3f5c2ad" align="middle">
<img src="https://picx.zhimg.com/v2-503ffa1f0488c2533b94aac7d65cee61" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unveiling-Obscured-Accretion-in-the-Local-Universe"><a href="#Unveiling-Obscured-Accretion-in-the-Local-Universe" class="headerlink" title="Unveiling Obscured Accretion in the Local Universe"></a>Unveiling Obscured Accretion in the Local Universe</h2><p><strong>Authors:Indrani Pal, Stefano Marchesi, Ross Silver, Marco Ajello, Vittoria Gianolli, NÃºria Torres-AlbÃ , Isaiah Cox, Xiurui Zhao, Dhrubojyoti Sengupta, Anuvab Banerjee, Kouser Imam, Andrealuna Pizzetti</strong></p>
<p>Heavily obscured Active Galactic Nuclei (AGN), especially Compton-thick sources with line-of-sight column density ($N_{\rm H,los}$) $&gt;$ 10$^{24}$ cm$^{-2}$, are critical to understanding supermassive black hole (SMBH) growth and the origin of the Cosmic X-ray Background (CXB). However, their observed fraction remains significantly below model predictions, due to strong absorption bias, even in the hard X-ray (i.e., above 10 keV) band. We analyze a sample of 26 nearby ($z &lt; 0.1$) AGN from the Swift-BAT 150-month catalog, selected via mid-IR to X-ray diagnostics and observed with NuSTAR and soft X-ray telescopes (Xmm-Newton, Chandra, or Swift-xrt). Using self-consistent torus models (MyTorus, Borus02, and UXCLUMPY), we aim to constrain $N_{\rm H,los}$, the average torus column density, and other geometrical parameters of the obscuring medium. A comparative analysis among the three torus models showed that while estimates of $N_{\rm{H,los}}$ were generally in agreement, Borus02 tended to classify a slightly larger number of sources as Compton-thick AGN (CT-AGN). Building on this comparison, we benchmark two prediction schemes â€“ a mid-IR&#x2F;X-ray relation and a machine-learning model â€“ against our broadband best-fit $N_{\rm H,los}$ measurements to assess which approach more effectively bridges the gap between predicted and measured obscuration, finding that while the former works effectively in the heavily obscured region (log$\rm{N_H} \gtrsim$ 23.5 $\rm{cm^{-2}}$), the latter provides improved accuracy, particularly for Compton-thin to moderately thick regimes (log$\rm{N_H} \lesssim$ 23.5 $\rm{cm^{-2}}$). </p>
<blockquote>
<p>è¢«ä¸¥é‡é®è”½çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆActive Galactic Nucleiï¼Œç®€ç§°AGNï¼‰ï¼Œç‰¹åˆ«æ˜¯è§†çº¿æŸ±å¯†åº¦ï¼ˆ$N_{\rm H,los}$ï¼‰å¤§äº10$^{24}$ cm$^{-2}$çš„åº·æ™®é¡¿åšæºï¼Œå¯¹äºç†è§£è¶…å¤§è´¨é‡é»‘æ´ï¼ˆSMBHï¼‰çš„å¢é•¿å’Œå®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰çš„èµ·æºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå…¶å¼ºçƒˆçš„å¸æ”¶åè§ï¼Œå³ä½¿åœ¨ç¡¬Xå°„çº¿ï¼ˆå³é«˜äº10 keVï¼‰æ³¢æ®µï¼Œå®ƒä»¬çš„è§‚æµ‹æ¯”ä¾‹ä»æ˜¾è‘—ä½äºæ¨¡å‹é¢„æµ‹ã€‚æˆ‘ä»¬åˆ†æäº†æ¥è‡ªSwift-BAT 150ä¸ªæœˆç›®å½•çš„26ä¸ªè¿‘è·ç¦»ï¼ˆ$z &lt; 0.1$ï¼‰çš„AGNæ ·æœ¬ï¼Œé€šè¿‡ä¸­çº¢å¤–åˆ°Xå°„çº¿çš„è¯Šæ–­æ–¹æ³•é€‰å‡ºæ¥ï¼Œå¹¶ç”¨NuSTARå’Œè½¯Xå°„çº¿æœ›è¿œé•œï¼ˆXmm-Newtonã€Chandraæˆ–Swift-xrtï¼‰è¿›è¡Œè§‚æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªæ´½çš„ç¯æ¨¡å‹ï¼ˆMyTorusã€Borus02å’ŒUXCLUMPYï¼‰æ¥é™åˆ¶$N_{\rm H,los}$ã€å¹³å‡ç¯æŸ±å¯†åº¦ä»¥åŠé®è”½ä»‹è´¨çš„å…¶ä»–å‡ ä½•å‚æ•°ã€‚å¯¹ä¸‰ä¸ªç¯æ¨¡å‹çš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œè™½ç„¶å¯¹$N_{\rm{H,los}}$çš„ä¼°è®¡å¤§ä½“ä¸Šæ˜¯ä¸€è‡´çš„ï¼Œä½†Borus02æ›´å€¾å‘äºå°†æ›´å¤šçš„æºåˆ†ç±»ä¸ºåº·æ™®é¡¿åšAGNï¼ˆCT-AGNï¼‰ã€‚åŸºäºè¿™ä¸€æ¯”è¾ƒï¼Œæˆ‘ä»¬å°†ä¸¤ç§é¢„æµ‹æ–¹æ¡ˆâ€”â€”ä¸­çº¢å¤–&#x2F;Xå°„çº¿å…³ç³»å’Œæœºå™¨å­¦ä¹ æ¨¡å‹â€”â€”ä¸æˆ‘ä»¬çš„å®½å¸¦æœ€ä½³æ‹Ÿåˆ$N_{\rm H,los}$æµ‹é‡å€¼è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°å“ªç§æ–¹æ³•æ›´æœ‰æ•ˆåœ°å¼¥åˆäº†é¢„æµ‹å’Œæµ‹é‡é®è”½ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå‘ç°å‰è€…åœ¨é‡åº¦é®è”½åŒºåŸŸï¼ˆlog$\rm{N_H} \geq 23.5 cm^{-2}$ï¼‰ä¸­æœ‰æ•ˆï¼Œè€Œåè€…åˆ™æä¾›äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åº·æ™®é¡¿è–„åˆ°ä¸­ç­‰åšåº¦èŒƒå›´ï¼ˆlog$\rm{N_H} \leq 23.5 cm^{-2}$ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21532v1">PDF</a> 16 pages, 13 Figures, submitted to the journal</p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ç ”ç©¶äº†è¢«ä¸¥é‡é®è”½çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆActive Galactic Nucleiï¼Œç®€ç§°AGNï¼‰ï¼Œç‰¹åˆ«æ˜¯çº¿è§†å‘æŸ±å¯†åº¦ï¼ˆ$N_&#123;\rm H,los&#125;$ï¼‰å¤§äº10$^&#123;24&#125;$ cm$^&#123;-2&#125;$çš„åº·æ™®é¡¿åšæºã€‚æ–‡ç« åˆ†æäº†æ¥è‡ªSwift-BAT 150ä¸ªæœˆçš„26ä¸ªé™„è¿‘ï¼ˆ$z &lt; 0.1$ï¼‰çš„AGNæ ·æœ¬ï¼Œé€šè¿‡çº¢å¤–è‡³Xå°„çº¿çš„è¯Šæ–­æ–¹æ³•è¿›è¡Œç­›é€‰ï¼Œå¹¶ç”¨NuSTARå’Œè½¯Xå°„çº¿æœ›è¿œé•œï¼ˆå¦‚Xmm-Newtonã€Chandraæˆ–Swift-xrtï¼‰è§‚æµ‹ã€‚ä½¿ç”¨ä¸€è‡´çš„ç¯æ¨¡å‹ï¼ˆMyTorusã€Borus02å’ŒUXCLUMPYï¼‰ï¼Œæ–‡ç« æ—¨åœ¨é™åˆ¶$N_&#123;\rm H,los&#125;$ã€å¹³å‡ç¯æŸ±å¯†åº¦ä»¥åŠé®è”½ä»‹è´¨çš„å…¶å®ƒå‡ ä½•å‚æ•°ã€‚æ¯”è¾ƒä¸‰ä¸ªç¯æ¨¡å‹çš„ä¼°è®¡å€¼ï¼Œå‘ç°Borus02æ›´åå‘äºå°†æ›´å¤šçš„æºåˆ†ç±»ä¸ºåº·æ™®é¡¿åšAGNï¼ˆCT-AGNï¼‰ã€‚æ–‡ç« é€šè¿‡å¯¹æ¯”ä¸¤ä¸ªé¢„æµ‹æ–¹æ¡ˆï¼ˆçº¢å¤–/Xå°„çº¿å…³ç³»å’Œæœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰ä¸å®½å¸¦æœ€ä½³æ‹Ÿåˆçš„$N_&#123;\rm H,los&#125;$æµ‹é‡å€¼ï¼Œè¯„ä¼°å“ªç§æ–¹æ³•æ›´æœ‰æ•ˆåœ°å¼¥åˆäº†é¢„æµ‹å’Œæµ‹é‡é®è”½ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå‘ç°çº¢å¤–/Xå°„çº¿å…³ç³»åœ¨é«˜åº¦é®è”½åŒºåŸŸæœ‰æ•ˆï¼Œè€Œæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åº·æ™®é¡¿è–„è‡³ä¸­ç­‰åšåº¦åŒºåŸŸæä¾›æ›´å‡†ç¡®çš„é¢„æµ‹ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡é®è”½çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼Œç‰¹åˆ«æ˜¯åº·æ™®é¡¿åšæºï¼Œå¯¹ç†è§£è¶…å¤§è´¨é‡é»‘æ´å¢é•¿å’Œå®‡å®™Xå°„çº¿èƒŒæ™¯èµ·æºè‡³å…³é‡è¦ã€‚</li>
<li>æ¥è‡ªSwift-BAT 150ä¸ªæœˆçš„26ä¸ªé™„è¿‘AGNæ ·æœ¬è¢«ç”¨äºåˆ†æã€‚</li>
<li>ä½¿ç”¨è‡ªæˆ‘ä¸€è‡´çš„ç¯æ¨¡å‹æ¥é™åˆ¶çº¿è§†å‘æŸ±å¯†åº¦å’Œé®è”½ä»‹è´¨çš„å‡ ä½•å‚æ•°ã€‚</li>
<li>Borus02æ¨¡å‹æ›´å€¾å‘äºå°†æ›´å¤šæºåˆ†ç±»ä¸ºåº·æ™®é¡¿åšAGNã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒï¼Œå‘ç°æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é¢„æµ‹åº·æ™®é¡¿è–„è‡³ä¸­ç­‰åšåº¦åŒºåŸŸçš„é®è”½ç¨‹åº¦æ—¶æ›´ä¸ºå‡†ç¡®ã€‚</li>
<li>ä¸­çº¢å¤–&#x2F;Xå°„çº¿å…³ç³»åœ¨é«˜åº¦é®è”½åŒºåŸŸæ˜¯æœ‰æ•ˆçš„é¢„æµ‹å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-267f4b00a263e47d6dd447d939c6eeb4" align="middle">
<img src="https://picx.zhimg.com/v2-b69f4482fc92c6861a4c900325b88c07" align="middle">
<img src="https://picx.zhimg.com/v2-46a610aa88dd0ae4f1a6aad226c60648" align="middle">
<img src="https://picx.zhimg.com/v2-259346cfc2fe3fd33944d9a0d368e572" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Patch-Based-Diffusion-for-Data-Efficient-Radiologist-Preferred-MRI-Reconstruction"><a href="#Patch-Based-Diffusion-for-Data-Efficient-Radiologist-Preferred-MRI-Reconstruction" class="headerlink" title="Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI   Reconstruction"></a>Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI   Reconstruction</h2><p><strong>Authors:Rohan Sanda, Asad Aali, Andrew Johnston, Eduardo Reis, Jonathan Singh, Gordon Wetzstein, Sara Fridovich-Keil</strong></p>
<p>Magnetic resonance imaging (MRI) requires long acquisition times, raising costs, reducing accessibility, and making scans more susceptible to motion artifacts. Diffusion probabilistic models that learn data-driven priors can potentially assist in reducing acquisition time. However, they typically require large training datasets that can be prohibitively expensive to collect. Patch-based diffusion models have shown promise in learning effective data-driven priors over small real-valued datasets, but have not yet demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x undersampled MRI reconstruction on the FastMRI brain dataset. We show that PaDIS-MRI models trained on small datasets of as few as 25 k-space images outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE), pixel-level uncertainty, cross-contrast generalization, and robustness to severe k-space undersampling. In a blinded study with three radiologists, PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex reconstruction with wavelet sparsity. These findings highlight the potential of patch-based diffusion priors for high-fidelity MRI reconstruction in data-scarce clinical settings where diagnostic confidence matters. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰éœ€è¦è¾ƒé•¿çš„é‡‡é›†æ—¶é—´ï¼Œè¿™å¢åŠ äº†æˆæœ¬ï¼Œé™ä½äº†å¯åŠæ€§ï¼Œå¹¶ä½¿æ‰«ææ›´å®¹æ˜“å—åˆ°è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚å­¦ä¹ æ•°æ®é©±åŠ¨å…ˆéªŒçš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹æœ‰åŠ©äºå‡å°‘é‡‡é›†æ—¶é—´ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®çš„æ”¶é›†æˆæœ¬å¯èƒ½éå¸¸é«˜ã€‚åŸºäºè¡¥ä¸çš„æ‰©æ•£æ¨¡å‹åœ¨å°å‹å®æ•°æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºå­¦ä¹ æœ‰æ•ˆæ•°æ®é©±åŠ¨å…ˆéªŒçš„æ½œåŠ›ï¼Œä½†å°šæœªåœ¨MRIä¸­æ˜¾ç¤ºå‡ºä¸´åºŠä»·å€¼ã€‚æˆ‘ä»¬å°†åŸºäºè¡¥ä¸çš„æ‰©æ•£åæ¼”æ±‚è§£å™¨ï¼ˆPaDISï¼‰æ‰©å±•åˆ°å¤æ•°ã€å¤šçº¿åœˆMRIé‡å»ºï¼Œå¹¶å°†å…¶ä¸FastMRI-EDMæœ€å…ˆè¿›çš„å…¨å›¾åƒæ‰©æ•£åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œç”¨äºFastMRIè„‘éƒ¨æ•°æ®é›†çš„7å€æ¬ é‡‡æ ·MRIé‡å»ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨ä»…2.5ä¸‡kç©ºé—´å›¾åƒçš„å°æ•°æ®é›†è®­ç»ƒçš„PaDIS-MRIæ¨¡å‹åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆPSNRã€SSIMã€NRMSEï¼‰ã€åƒç´ çº§ä¸ç¡®å®šæ€§ã€è·¨å¯¹æ¯”åº¦æ³›åŒ–å’Œä¸¥é‡kç©ºé—´æ¬ é‡‡æ ·é²æ£’æ€§æ–¹é¢ä¼˜äºFastMRI-EDMã€‚åœ¨ä¸‰åæ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„ç›²æ³•ç ”ç©¶ä¸­ï¼Œä¸åŸºçº¿ï¼ˆiï¼‰FastMRI-EDMå’Œï¼ˆiiï¼‰ç»å…¸å‡¸é‡æ„å°æ³¢ç¨€ç–æ€§ç›¸æ¯”ï¼ŒPaDIS-MRIé‡å»ºåœ¨91.7%çš„æƒ…å†µä¸‹è¢«é€‰æ‹©ä¸ºè¯Šæ–­ä¸Šæ›´ä¼˜è¶Šã€‚è¿™äº›å‘ç°çªå‡ºäº†åŸºäºè¡¥ä¸çš„æ‰©æ•£å…ˆéªŒåœ¨æ•°æ®ç¨€ç¼ºçš„ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œé«˜ä¿çœŸMRIé‡å»ºçš„æ½œåŠ›ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œè¯Šæ–­ä¿¡å¿ƒè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21531v1">PDF</a> Code is available at: <a target="_blank" rel="noopener" href="https://github.com/voilalab/PaDIS-MRI">https://github.com/voilalab/PaDIS-MRI</a></p>
<p><strong>Summary</strong><br>     åŸºäºPatchçš„æ‰©æ•£æ¨¡å‹ï¼ˆPaDIS-MRIï¼‰åœ¨å¤æ•°ã€å¤šçº¿åœˆMRIé‡å»ºä¸­å±•ç°å‡ºä¼˜åŠ¿ï¼Œç›¸è¾ƒäºFastMRI-EDMç­‰å…ˆè¿›å…¨å›¾æ‰©æ•£åŸºçº¿æ–¹æ³•ï¼Œå…¶åœ¨FastMRIè„‘éƒ¨æ•°æ®é›†ä¸Šçš„7å€æ¬ é‡‡æ ·MRIé‡å»ºæ•ˆæœæ›´ä¼˜ã€‚PaDIS-MRIåœ¨å°å‹æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä»…éœ€å°‘é‡kç©ºé—´å›¾åƒï¼Œä¾¿èƒ½äºå›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆPSNRã€SSIMã€NRMSEï¼‰ã€åƒç´ çº§ä¸ç¡®å®šæ€§ã€è·¨å¯¹æ¯”åº¦æ³›åŒ–ä»¥åŠæŠ—ä¸¥é‡kç©ºé—´æ¬ é‡‡æ ·ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸‰ä½æ”¾å°„ç§‘åŒ»ç”Ÿå‚ä¸çš„åŒç›²ç ”ç©¶ä¸­ï¼ŒPaDIS-MRIé‡å»ºåœ¨91.7%çš„æƒ…å†µä¸‹è¢«é€‰ä¸ºè¯Šæ–­ä¸Šæ›´ä¼˜ã€‚è¿™è¡¨æ˜åœ¨æ•°æ®ç¨€ç¼ºçš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œå¯¹äºéœ€è¦æé«˜è¯Šæ–­ä¿¡å¿ƒçš„åœºåˆï¼ŒåŸºäºPatchçš„æ‰©æ•£å…ˆéªŒå…·æœ‰æ½œåœ¨çš„é«˜ä¿çœŸMRIé‡å»ºä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹èƒ½å‡å°‘MRIçš„é‡‡é›†æ—¶é—´ï¼Œæé«˜å›¾åƒè´¨é‡å¹¶é™ä½è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚</li>
<li>åŸºäºPatchçš„æ‰©æ•£æ¨¡å‹ï¼ˆPaDIS-MRIï¼‰åœ¨å¤æ•°ã€å¤šçº¿åœˆMRIé‡å»ºä¸­æœ‰ä¼˜åŠ¿ã€‚</li>
<li>PaDIS-MRIèƒ½åœ¨å°å‹æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¯¹å›¾åƒè´¨é‡æŒ‡æ ‡è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬PSNRã€SSIMå’ŒNRMSEç­‰ã€‚</li>
<li>PaDIS-MRIå…·æœ‰å‡ºè‰²çš„åƒç´ çº§ä¸ç¡®å®šæ€§å¤„ç†å’Œè·¨å¯¹æ¯”åº¦æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>PaDIS-MRIå³ä½¿åœ¨ä¸¥é‡kç©ºé—´æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚</li>
<li>åŒç›²ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒPaDIS-MRIåœ¨è¯Šæ–­ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¢«é€‰ä¸ºæœ€ä¼˜æ–¹æ¡ˆçš„æ¯”ä¾‹é«˜è¾¾91.7%ã€‚</li>
<li>åŸºäºPatchçš„æ‰©æ•£å…ˆéªŒåœ¨é«˜ä¿çœŸMRIé‡å»ºæ–¹é¢å…·æœ‰æ½œåœ¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd9a1f176fff8d13ba7b84673d943467" align="middle">
<img src="https://picx.zhimg.com/v2-253247c48e5f4fe4758606f516fd9b41" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-LongiMam-model-for-improved-breast-cancer-risk-prediction-using-longitudinal-mammograms"><a href="#The-LongiMam-model-for-improved-breast-cancer-risk-prediction-using-longitudinal-mammograms" class="headerlink" title="The LongiMam model for improved breast cancer risk prediction using   longitudinal mammograms"></a>The LongiMam model for improved breast cancer risk prediction using   longitudinal mammograms</h2><p><strong>Authors:Manel Rakez, Thomas Louis, Julien Guillaumin, Foucauld Chammingâ€™s, Pierre Fillard, Brice Amadeo, Virginie Rondeau</strong></p>
<p>Risk-adapted breast cancer screening requires robust models that leverage longitudinal imaging data. Most current deep learning models use single or limited prior mammograms and lack adaptation for real-world settings marked by imbalanced outcome distribution and heterogeneous follow-up. We developed LongiMam, an end-to-end deep learning model that integrates both current and up to four prior mammograms. LongiMam combines a convolutional and a recurrent neural network to capture spatial and temporal patterns predictive of breast cancer. The model was trained and evaluated using a large, population-based screening dataset with disproportionate case-to-control ratio typical of clinical screening. Across several scenarios that varied in the number and composition of prior exams, LongiMam consistently improved prediction when prior mammograms were included. The addition of prior and current visits outperformed single-visit models, while priors alone performed less well, highlighting the importance of combining historical and recent information. Subgroup analyses confirmed the modelâ€™s efficacy across key risk groups, including women with dense breasts and those aged 55 years or older. Moreover, the model performed best in women with observed changes in mammographic density over time. These findings demonstrate that longitudinal modeling enhances breast cancer prediction and support the use of repeated mammograms to refine risk stratification in screening programs. LongiMam is publicly available as open-source software. </p>
<blockquote>
<p>é£é™©é€‚åº”å‹ä¹³è…ºç™Œç­›æŸ¥éœ€è¦å¼ºå¤§çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯åˆ©ç”¨çºµå‘æˆåƒæ•°æ®ã€‚å½“å‰å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹ä»…ä½¿ç”¨å•æ¬¡æˆ–æœ‰é™çš„å…ˆå‰ä¹³è…ºXå…‰æ£€æŸ¥ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹ç»“æœåˆ†å¸ƒä¸å‡è¡¡å’Œéšè®¿å¼‚è´¨æ€§çš„çœŸå®ä¸–ç•Œç¯å¢ƒçš„é€‚åº”æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†LongiMamï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥æ•´åˆå½“å‰ä»¥åŠæœ€å¤šå››ä¸ªå…ˆå‰çš„ä¹³è…ºXå…‰æ£€æŸ¥ã€‚LongiMamç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼Œä»¥æ•è·é¢„æµ‹ä¹³è…ºç™Œçš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å¤§è§„æ¨¡åŸºäºäººç¾¤ç­›æŸ¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå…·æœ‰å…¸å‹ä¸´åºŠç­›æŸ¥ä¸­çš„ä¸å¹³è¡¡ç—…ä¾‹å¯¹ç…§æ¯”ä¾‹ã€‚åœ¨å…ˆå‰æ£€æŸ¥çš„æ•°é‡å’Œç»„æˆä¸åŒçš„å‡ ä¸ªåœºæ™¯ä¸­ï¼ŒåŒ…å«å…ˆå‰ä¹³è…ºXå…‰æ£€æŸ¥æ—¶ï¼ŒLongiMamå§‹ç»ˆæé«˜äº†é¢„æµ‹èƒ½åŠ›ã€‚å½“å‰å’Œå…ˆå‰å°±è¯Šçš„æ·»åŠ è¡¨ç°ä¼˜äºå•æ¬¡å°±è¯Šæ¨¡å‹ï¼Œè€Œä»…ä½¿ç”¨å…ˆå‰çš„ä¿¡æ¯è¡¨ç°è¾ƒå·®ï¼Œè¿™çªå‡ºäº†ç»“åˆå†å²å’Œæœ€æ–°ä¿¡æ¯çš„é‡è¦æ€§ã€‚äºšç»„åˆ†æè¯å®äº†è¯¥æ¨¡å‹åœ¨å…³é”®é£é™©ç¾¤ä½“ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¹³è…ºç»„ç»‡å¯†åº¦è¾ƒå¤§çš„å¥³æ€§å’Œå¹´é¾„55å²åŠä»¥ä¸Šçš„å¥³æ€§ã€‚æ­¤å¤–ï¼Œåœ¨è§‚å¯Ÿåˆ°ä¹³è…ºXå…‰æ£€æŸ¥å¯†åº¦éšæ—¶é—´å˜åŒ–çš„å¥³æ€§ä¸­ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½æœ€ä½³ã€‚è¿™äº›å‘ç°è¡¨æ˜çºµå‘å»ºæ¨¡å¢å¼ºäº†ä¹³è…ºç™Œé¢„æµ‹ï¼Œå¹¶æ”¯æŒä½¿ç”¨å¤šæ¬¡ä¹³è…ºXå…‰æ£€æŸ¥æ¥æ”¹è¿›ç­›æŸ¥ç¨‹åºä¸­çš„é£é™©åˆ†å±‚ã€‚LongiMamä½œä¸ºå¼€æºè½¯ä»¶å¯ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21383v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLongiMamçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å½“å‰å’Œæœ€å¤šå››ä¸ªå…ˆå‰çš„ä¹³è…ºXçº¿æ‘„å½±å›¾åƒï¼Œç”¨äºé£é™©é€‚åº”çš„ä¹³è…ºç™Œç­›æŸ¥ã€‚å®ƒé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œæ•æ‰ä¹³è…ºç™Œçš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ã€‚æ¨¡å‹ä½¿ç”¨å¤§å‹åŸºäºäººç¾¤çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå±•ç°å‡ºä¼˜è‰¯çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ç»“åˆäº†å…ˆå‰çš„æ£€æŸ¥ä¿¡æ¯åï¼Œé¢„æµ‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚è¯¥æ¨¡å‹å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LongiMamæ¨¡å‹ç»“åˆäº†çºµå‘æˆåƒæ•°æ®ï¼Œç”¨äºé£é™©é€‚åº”çš„ä¹³è…ºç™Œç­›æŸ¥ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼Œä»¥æ•æ‰ä¹³è…ºç™Œçš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ã€‚</li>
<li>LongiMamåœ¨åŒ…å«å…ˆå‰ä¹³è…ºXçº¿æ‘„å½±å›¾åƒæ—¶é¢„æµ‹æ€§èƒ½æå‡ã€‚</li>
<li>ç›¸è¾ƒäºä»…ä½¿ç”¨å•æ¬¡è®¿é—®ä¿¡æ¯çš„æ¨¡å‹ï¼Œç»“åˆå†å²å’Œæœ€æ–°ä¿¡æ¯çš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>å­ç»„åˆ†æè¯å®è¯¥æ¨¡å‹åœ¨å…³é”®é£é™©ç¾¤ä½“ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>LongiMamæ¨¡å‹åœ¨è§‚å¯Ÿåˆ°ä¹³è…ºå¯†åº¦éšæ—¶é—´å˜åŒ–çš„å¥³æ€§ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>çºµå‘å»ºæ¨¡æœ‰åŠ©äºæé«˜ä¹³è…ºç™Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf05fe218bde7839cdf138e58aa962b2" align="middle">
<img src="https://picx.zhimg.com/v2-9b20d00f42c7a48b9266723a204be284" align="middle">
<img src="https://picx.zhimg.com/v2-99417985954437ae93a70f3dd723192c" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tracing-the-Origins-of-Hot-Halo-Gas-in-Milky-Way-Type-Galaxies-with-SMUGGLE"><a href="#Tracing-the-Origins-of-Hot-Halo-Gas-in-Milky-Way-Type-Galaxies-with-SMUGGLE" class="headerlink" title="Tracing the Origins of Hot Halo Gas in Milky Way-Type Galaxies with   SMUGGLE"></a>Tracing the Origins of Hot Halo Gas in Milky Way-Type Galaxies with   SMUGGLE</h2><p><strong>Authors:Zhijie Zhang, Xiaoxia Zhang, Hui Li, Taotao Fang, Yang Luo, Federico Marinacci, Laura V. Sales, Paul Torrey, Mark Vogelsberger, Qingzheng Yu, Feng Yuan</strong></p>
<p>Current galaxy formation models predict the existence of X-ray-emitting gaseous halos around Milky Way (MW)-type galaxies. To investigate properties of this coronal gas in MW-like galaxies, we analyze a suite of high-resolution simulations based on the SMUGGLE framework and compare the results with X-ray observations of both the MW and external galaxies. We find that for subgrid models incorporating any form of stellar feedback, e.g., early feedback (including stellar winds and radiation) and&#x2F;or supernova (SN) explosions, the total 0.5-2 keV luminosity is consistent within uncertainties with X-ray observations of the MW and with scaling relations derived for external disk galaxies. However, all models exhibit an X-ray surface brightness profile that declines too steeply beyond $\sim5$ kpc, underpredicting the extended emission seen in recent eROSITA stacking results. Across all subgrid prescriptions, the simulated surface brightness and emission measure fall below MW observations by at least 1-2 orders of magnitude, with the most severe discrepancy occurring in the no-feedback model. Our results suggest that (i) stellar feedback primarily shapes the innermost hot atmosphere (central $\sim5$ kpc), with comparable contributions from early feedback and SNe to the resulting X-ray luminosity; (ii) additional mechanisms such as gravitational heating, active galactic nuclei feedback, and&#x2F;or Compton effects of GeV cosmic ray are necessary to generate the extended, volume-filling hot gaseous halo of MW-mass galaxies; (iii) the origins of hot corona in MW-like galaxies are partially distinct from those of the warm ($\sim10^5$ K) gas, by combining our previous finding that the SMUGGLE model successfully reproduces the kinematics and spatial distribution of MW O VI absorbers. </p>
<blockquote>
<p>å½“å‰æ˜Ÿç³»å½¢æˆæ¨¡å‹é¢„æµ‹åœ¨ç±»ä¼¼é“¶æ²³ç³»ï¼ˆMWï¼‰çš„æ˜Ÿç³»å‘¨å›´å­˜åœ¨å‘å°„Xå°„çº¿çš„æ°”ä½“æ™•ã€‚ä¸ºäº†ç ”ç©¶MWå‹æ˜Ÿç³»ä¸­è¿™ç§å†•çŠ¶æ°”ä½“çš„å±æ€§ï¼Œæˆ‘ä»¬åŸºäºSMUGGLEæ¡†æ¶åˆ†æäº†ä¸€å¥—é«˜åˆ†è¾¨ç‡æ¨¡æ‹Ÿï¼Œå¹¶å°†ç»“æœä¸MWå’Œå¤–éƒ¨æ˜Ÿç³»çš„Xå°„çº¿è§‚æµ‹ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºä»»ä½•å½¢å¼çš„æ’æ˜Ÿåé¦ˆï¼ˆå¦‚æ—©æœŸåé¦ˆï¼ˆåŒ…æ‹¬æ’æ˜Ÿé£å’Œè¾å°„ï¼‰å’Œ&#x2F;æˆ–è¶…æ–°æ˜Ÿï¼ˆSNï¼‰çˆ†ç‚¸ï¼‰çš„äºšç½‘æ ¼æ¨¡å‹ï¼Œå…¶æ€»0.5-2åƒç”µå­ä¼çš„äº®åº¦åœ¨ä¸ç¡®å®šæ€§ä¸MWçš„Xå°„çº¿è§‚æµ‹ç»“æœå’Œå¤–éƒ¨æ˜Ÿç³»çš„ç¼©æ”¾å…³ç³»æ´¾ç”Ÿç»“æœä¹‹é—´æ˜¯ä¸€è‡´çš„ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹çš„Xå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒæ›²çº¿åœ¨è·ç¦»çº¦5åƒç§’å·®è·ä¹‹å¤–ä¸‹é™å¾—å¤ªé™¡å³­ï¼Œä½äºæœ€è¿‘çš„eROSITAå †å ç»“æœæ‰€è§‚å¯Ÿåˆ°çš„æ‰©å±•å‘å°„ã€‚åœ¨æ‰€æœ‰äºšç½‘æ ¼è§„å®šä¸­ï¼Œæ¨¡æ‹Ÿçš„è¡¨é¢äº®åº¦å’Œå‘å°„é‡è‡³å°‘ä½äºMWè§‚æµ‹å€¼ä¸€ä¸ªåˆ°ä¸¤ä¸ªæ•°é‡çº§ï¼Œæ— åé¦ˆæ¨¡å‹ä¸­å‡ºç°æœ€ä¸¥é‡çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆiï¼‰æ’æ˜Ÿåé¦ˆä¸»è¦å¡‘é€ äº†æœ€å†…éƒ¨çš„çƒ­å¤§æ°”ï¼ˆä¸­å¿ƒçº¦5åƒç§’å·®è·ï¼‰ï¼Œæ—©æœŸåé¦ˆå’Œè¶…æ–°æ˜Ÿå¯¹æœ€ç»ˆçš„Xå°„çº¿äº®åº¦æœ‰ç›¸å½“çš„è´¡çŒ®ï¼›ï¼ˆiiï¼‰ä¸ºäº†äº§ç”Ÿç±»ä¼¼é“¶æ²³ç³»è´¨é‡çš„æ‰©å±•ã€ä½“ç§¯å¡«å……çƒ­æ°”ä½“æ™•ï¼Œéœ€è¦é¢å¤–çš„æœºåˆ¶ï¼Œå¦‚å¼•åŠ›åŠ çƒ­ã€æ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆå’Œ&#x2F;æˆ–GeVå®‡å®™å°„çº¿çš„åº·æ™®é¡¿æ•ˆåº”ï¼›ï¼ˆiiiï¼‰ç»“åˆæˆ‘ä»¬ä¹‹å‰å‘ç°SMUGGLEæ¨¡å‹æˆåŠŸå†ç°äº†MW O VIå¸æ”¶ä½“çš„è¿åŠ¨å­¦å’Œç©ºé—´åˆ†å¸ƒï¼ŒMWå‹æ˜Ÿç³»ä¸­çƒ­å†•çš„èµ·æºä¸çº¦10ä¸‡åº¦ï¼ˆæš–ï¼‰æ°”ä½“çš„èµ·æºéƒ¨åˆ†ä¸åŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21576v2">PDF</a> 13 pages, 7 figures, 1 table;</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†åŸºäºSMUGGLEæ¡†æ¶çš„é«˜åˆ†è¾¨ç‡æ¨¡æ‹Ÿä¸‹ï¼Œç±»ä¼¼äºé“¶æ²³ç³»ï¼ˆMWï¼‰çš„æ˜Ÿç³»ä¸­çš„Xå°„çº¿å‘å°„æ°”ä½“ç‰¹æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè€ƒè™‘æ’æ˜Ÿåé¦ˆï¼ˆå¦‚æ—©æœŸåé¦ˆå’Œ&#x2F;æˆ–è¶…æ–°æ˜Ÿçˆ†å‘ï¼‰çš„æ¨¡å‹ä¸é“¶æ²³ç³»å’Œå¤–æ˜Ÿç³»çš„Xå°„çº¿è§‚æµ‹ç»“æœç›¸ç¬¦ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨çº¦5kpcä»¥å¤–çš„åŒºåŸŸXå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒä¸‹é™å¾—å¤ªå¿«ï¼Œä½äºæœ€è¿‘çš„eROSITAå åŠ ç»“æœæ‰€è§‚å¯Ÿåˆ°çš„æ‰©å±•å‘å°„ã€‚è¿™è¡¨æ˜ä»…ä¾é æ’æ˜Ÿåé¦ˆæ— æ³•å®Œå…¨è§£é‡Šè§‚å¯Ÿåˆ°çš„ç°è±¡ï¼Œå¯èƒ½éœ€è¦å…¶ä»–æœºåˆ¶å¦‚å¼•åŠ›åŠ çƒ­ã€æ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆå’Œ&#x2F;æˆ–ç›¸å¯¹è®ºå®‡å®™å°„çº¿æ•ˆåº”ç­‰ç”Ÿæˆå»¶å±•æ€§ã€ä½“ç§¯å¡«å……çƒ­æ°”ä½“æ™•ã€‚å› æ­¤ï¼Œé“¶æ²³ç³»ç±»ä¼¼æ˜Ÿç³»çš„çƒ­å†•çš„èµ·æºå¯èƒ½ä¸è¾ƒæ¸©æš–ï¼ˆçº¦10ä¸‡åº¦ï¼‰æ°”ä½“çš„èµ·æºéƒ¨åˆ†ä¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºSMUGGLEæ¡†æ¶çš„é«˜åˆ†è¾¨ç‡æ¨¡æ‹Ÿæ˜¾ç¤ºï¼Œå¯¹äºè€ƒè™‘æ’æ˜Ÿåé¦ˆçš„æ¨¡å‹ï¼Œå…¶ä¸é“¶æ²³ç³»åŠå¤–æ˜Ÿç³»çš„Xå°„çº¿è§‚æµ‹ç»“æœä¸€è‡´ã€‚</li>
<li>æ‰€æœ‰çš„æ¨¡æ‹Ÿæ¨¡å‹åœ¨çº¦5kpcä»¥å¤–çš„åŒºåŸŸå±•ç°å‡ºXå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒä¸‹é™è¿‡å¿«çš„ç°è±¡ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœä¸æœ€è¿‘çš„eROSITAå åŠ ç»“æœç›¸æ¯”ï¼Œæ¨¡æ‹Ÿçš„è¡¨é¢äº®åº¦å’Œå‘å°„åº¦é‡ä½äºå®é™…è§‚æµ‹ç»“æœï¼Œå·®è·è¾¾åˆ°è‡³å°‘1-2ä¸ªæ•°é‡çº§ã€‚</li>
<li>æ— åé¦ˆçš„æ¨¡å‹å‡ºç°äº†æœ€ä¸¥é‡çš„å·®å¼‚ã€‚</li>
<li>æ’æ˜Ÿåé¦ˆä¸»è¦å¡‘é€ äº†æœ€å†…éƒ¨çš„çƒ­å¤§æ°”ï¼ˆä¸­å¿ƒçº¦5kpcï¼‰ï¼Œæ—©æœŸåé¦ˆå’Œè¶…æ–°æ˜Ÿå¯¹Xå°„çº¿å…‰åº¦æœ‰ç›¸å½“çš„è´¡çŒ®ã€‚</li>
<li>ç”Ÿæˆå»¶å±•æ€§ã€ä½“ç§¯å¡«å……çƒ­æ°”ä½“æ™•å¯èƒ½éœ€è¦é¢å¤–çš„æœºåˆ¶ï¼Œå¦‚å¼•åŠ›åŠ çƒ­ã€æ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆå’Œç›¸å¯¹è®ºå®‡å®™å°„çº¿æ•ˆåº”ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55dbd1452407e9eaa71f7b94bfaad314" align="middle">
<img src="https://picx.zhimg.com/v2-4e2081e006b2a2058ac503c47379f210" align="middle">
<img src="https://picx.zhimg.com/v2-ffbade027e9d408ba6cc065796b24eb0" align="middle">
<img src="https://picx.zhimg.com/v2-1cc52376dde3891fb44f0c04e6b50fcc" align="middle">
<img src="https://picx.zhimg.com/v2-d4488fdaeadd6c5310dc295b473c2beb" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Intercept-Cancer-Cancer-Pre-Screening-with-Large-Scale-Healthcare-Foundation-Models"><a href="#Intercept-Cancer-Cancer-Pre-Screening-with-Large-Scale-Healthcare-Foundation-Models" class="headerlink" title="Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare   Foundation Models"></a>Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare   Foundation Models</h2><p><strong>Authors:Liwen Sun, Hao-Ren Yao, Gary Gao, Ophir Frieder, Chenyan Xiong</strong></p>
<p>Cancer screening, leading to early detection, saves lives. Unfortunately, existing screening techniques require expensive and intrusive medical procedures, not globally available, resulting in too many lost would-be-saved lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation Models, a cancer pre-screening methodology that identifies high-risk patients for further screening solely based on their historical medical records. With millions of electronic healthcare records (EHR), we establish the scaling law of EHR foundation models pretrained on medical code sequences, pretrain compute-optimal foundation models of up to 2.4 billion parameters, and finetune them on clinician-curated cancer risk prediction cohorts. In our retrospective evaluation comprising of thirty thousand patients, CATCH-FM achieves strong efficacy, with 50% sensitivity in predicting first cancer risks at 99% specificity cutoff, and outperforming feature-based tree models and both general and medical LLMs by up to 20% AUPRC. Despite significant demographic, healthcare system, and EHR coding differences, CATCH-FM achieves state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot leaderboard, outperforming EHR foundation models pretrained using on-site patient data. Our analysis demonstrates the robustness of CATCH-FM in various patient distributions, the benefits of operating in the ICD code space, and its ability to capture non-trivial cancer risk factors. Our code will be open-sourced. </p>
<blockquote>
<p>ç™Œç—‡ç­›æŸ¥èƒ½å¤Ÿå¯¼è‡´æ—©æœŸå‘ç°ï¼Œä»è€Œæ‹¯æ•‘ç”Ÿå‘½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç­›æŸ¥æŠ€æœ¯éœ€è¦æ˜‚è´µä¸”ä¾µå…¥æ€§çš„åŒ»ç–—ç¨‹åºï¼Œå¹¶éå…¨çƒé€šç”¨ï¼Œå¯¼è‡´è®¸å¤šæœ¬å¯æŒ½æ•‘çš„ç”Ÿå‘½ä¸§å¤±ã€‚æˆ‘ä»¬æå‡ºCATCH-FMï¼ˆCATCH Cancer early with Healthcare Foundation Modelsï¼Œå³CATCHæ—©æœŸç™Œç—‡ä¸åŒ»ç–—åŸºç¡€æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç™Œç—‡é¢„ç­›æŸ¥æ–¹æ³•ï¼Œä»…åŸºäºæ‚£è€…çš„å†å²åŒ»ç–—è®°å½•æ¥ç¡®å®šé«˜å±æ‚£è€…ï¼Œä»¥ä¾›è¿›ä¸€æ­¥ç­›æŸ¥ã€‚æˆ‘ä»¬åˆ©ç”¨æ•°ç™¾ä¸‡ä»½ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ï¼Œå»ºç«‹åŸºäºåŒ»ç–—ç¼–ç åºåˆ—çš„EHRåŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•æ€§æ³•åˆ™ï¼Œé¢„è®­ç»ƒäº†ä¼˜åŒ–è®¡ç®—çš„æœ€å¤šé«˜è¾¾2.4äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡åŒ»ç”Ÿç²¾å¿ƒç­–åˆ’çš„ç™Œç—‡é£é™©é¢„æµ‹é˜Ÿåˆ—è¿›è¡Œå¾®è°ƒã€‚åœ¨æˆ‘ä»¬çš„æ¶µç›–ä¸‰ä¸‡åæ‚£è€…çš„å›é¡¾æ€§è¯„ä¼°ä¸­ï¼ŒCATCH-FMè¡¨ç°å‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§ï¼Œåœ¨99%çš„ç‰¹å¼‚æ€§æˆªæ–­ç‚¹é¢„æµ‹é¦–æ¬¡ç™Œç—‡é£é™©çš„æ•æ„Ÿæ€§è¾¾åˆ°50%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºåŸºäºç‰¹å¾æ ‘æ¨¡å‹å’Œä¸€èˆ¬ä¸åŒ»ç–—LLMæ¨¡å‹ï¼Œå…¶æé«˜äº†é«˜è¾¾20%çš„AUPRCã€‚å°½ç®¡å­˜åœ¨æ˜¾è‘—çš„äººå£ç»Ÿè®¡ç‰¹å¾ã€åŒ»ç–—ä¿å¥ç³»ç»Ÿå’ŒEHRç¼–ç å·®å¼‚ï¼ŒCATCH-FMä»åœ¨EHRSHOTçš„å°‘æ ·æœ¬é¢†å¯¼è€…åå•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„èƒ°è…ºç™Œé£é™©é¢„æµ‹æ°´å¹³ï¼Œå¹¶è¶…è¶Šäº†ä½¿ç”¨ç°åœºæ‚£è€…æ•°æ®é¢„è®­ç»ƒçš„EHRåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æå±•ç¤ºäº†CATCH-FMåœ¨å„ç§æ‚£è€…åˆ†å¸ƒä¸­çš„ç¨³å¥æ€§ï¼Œä»¥åŠåœ¨ICDä»£ç ç©ºé—´ä¸­æ“ä½œçš„å¥½å¤„ä»¥åŠå…¶æ•æ‰éé‡è¦ç™Œç—‡é£é™©å› ç´ çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†å¼€æºå…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00209v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå†å²åŒ»ç–—è®°å½•çš„ç™Œç—‡é¢„ç­›æŸ¥æ–¹æ³•CATCH-FMï¼Œèƒ½å¤Ÿä»…é€šè¿‡æ‚£è€…çš„å†å²åŒ»ç–—è®°å½•è¯†åˆ«å‡ºé«˜é£é™©æ‚£è€…ï¼Œä¸ºè¿›ä¸€æ­¥çš„ç­›æŸ¥æä¾›ä¾æ®ï¼Œä»è€Œæœ‰åŠ©äºç™Œç—‡çš„æ—©æœŸå‘ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰è¿›è¡Œè§„æ¨¡åŒ–å»ºæ¨¡ï¼Œè®­ç»ƒå‡ºå‚æ•°è¾¾æ•°åäº¿çš„ä¼˜åŒ–åŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨ä¸´åºŠåŒ»ç”Ÿç­–åˆ’çš„ç™Œç—‡é£é™©é¢„æµ‹é˜Ÿåˆ—ä¸­è¿›è¡Œå¾®è°ƒã€‚åœ¨åŒ…å«ä¸‰ä¸‡åæ‚£è€…çš„å›é¡¾æ€§è¯„ä¼°ä¸­ï¼ŒCATCH-FMè¡¨ç°å‡ºå¼ºå¤§çš„æ•ˆèƒ½ï¼Œåœ¨é¢„æµ‹é¦–æ¬¡ç™Œç—‡é£é™©æ–¹é¢å…·æœ‰è¾ƒé«˜çš„çµæ•åº¦å’Œç‰¹å¼‚æ€§ï¼Œä¸”ç›¸è¾ƒäºåŸºäºç‰¹å¾çš„æ ‘æ¨¡å‹å’Œä¸€èˆ¬åŠåŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„AUPRCå€¼ã€‚å°½ç®¡å­˜åœ¨äººå£ç»Ÿè®¡å­¦ã€åŒ»ç–—ç³»ç»Ÿå’ŒEHRç¼–ç å·®å¼‚ï¼ŒCATCH-FMåœ¨EHRSHOTå°‘æ ·æœ¬æ’è¡Œæ¦œä¸Šä»å®ç°äº†æœ€å…ˆè¿›çš„èƒ°è…ºç™Œé£é™©é¢„æµ‹æ€§èƒ½ã€‚åˆ†æè¡¨æ˜ï¼ŒCATCH-FMåœ¨å„ç§æ‚£è€…åˆ†å¸ƒä¸­è¡¨ç°ç¨³å¥ï¼Œåœ¨ICDä»£ç ç©ºé—´ä¸­æ“ä½œå…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶èƒ½æ•æ‰éå¹³å‡¡çš„ç™Œç—‡é£é™©å› ç´ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CATCH-FMæ˜¯ä¸€ç§åŸºäºå†å²åŒ»ç–—è®°å½•çš„ç™Œç—‡é¢„ç­›æŸ¥æ–¹æ³•ï¼Œå¯è¯†åˆ«å‡ºé«˜é£é™©æ‚£è€…ã€‚</li>
<li>é€šè¿‡ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰å»ºç«‹è§„æ¨¡åŒ–æ¨¡å‹ï¼Œè®­ç»ƒå‡ºå¤§å‹åŸºç¡€æ¨¡å‹ã€‚</li>
<li>CATCH-FMåœ¨é¢„æµ‹é¦–æ¬¡ç™Œç—‡é£é™©æ–¹é¢è¡¨ç°å‡ºé«˜çµæ•åº¦å’Œç‰¹å¼‚æ€§ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒCATCH-FMå…·æœ‰æ›´é«˜çš„AUPRCå€¼ã€‚</li>
<li>CATCH-FMåœ¨ä¸åŒäººå£ç»Ÿè®¡å­¦ã€åŒ»ç–—ç³»ç»Ÿå’ŒEHRç¼–ç èƒŒæ™¯ä¸‹è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>CATCH-FMåœ¨èƒ°è…ºç™Œé£é™©é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>CATCH-FMèƒ½å¤Ÿæ•æ‰éå¹³å‡¡çš„ç™Œç—‡é£é™©å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb6a67a1c1ce63c43b73469445dc6528" align="middle">
<img src="https://picx.zhimg.com/v2-1ff56c7f7bcbae0023e86492e773fe28" align="middle">
<img src="https://picx.zhimg.com/v2-af3ae5e0748f75600133aaee9d6dc014" align="middle">
<img src="https://picx.zhimg.com/v2-a7901f4c6589304f3e8bf9dfc3240416" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PathGene-Benchmarking-Driver-Gene-Mutations-and-Exon-Prediction-Using-Multicenter-Lung-Cancer-Histopathology-Image-Dataset"><a href="#PathGene-Benchmarking-Driver-Gene-Mutations-and-Exon-Prediction-Using-Multicenter-Lung-Cancer-Histopathology-Image-Dataset" class="headerlink" title="PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using   Multicenter Lung Cancer Histopathology Image Dataset"></a>PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using   Multicenter Lung Cancer Histopathology Image Dataset</h2><p><strong>Authors:Liangrui Pan, Qingchun Liang, Shen Zhao, Songqing Fan, Shaoliang Peng</strong></p>
<p>Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/panliangrui/NIPS2025/">https://github.com/panliangrui/NIPS2025/</a>. </p>
<blockquote>
<p>å‡†ç¡®åœ°é¢„æµ‹è‚ºç™Œä¸­çš„åŸºå› çªå˜ã€çªå˜äºšå‹åŠå…¶å¤–æ˜¾å­å¯¹äºä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’å’Œé¢„åè¯„ä¼°è‡³å…³é‡è¦ã€‚é¢å¯¹åŒ»ç–—èµ„æºçš„åœ°åŸŸå·®å¼‚å’Œé«˜æ˜‚çš„åŸºå› ç»„æ£€æµ‹æˆæœ¬ï¼Œåˆ©ç”¨äººå·¥æ™ºèƒ½ä»å¸¸è§„ç—…ç†å›¾åƒæ¨æ–­è¿™äº›çªå˜å’Œå¤–æ˜¾å­å˜å¼‚å¯ä»¥æå¤§åœ°ä¿ƒè¿›ç²¾å‡†æ²»ç–—ã€‚è™½ç„¶ä¸€äº›æ—©æœŸçš„ç ”ç©¶å·²ç»è¡¨æ˜æ·±åº¦å­¦ä¹ å¯ä»¥åŠ é€Ÿä»è‚ºç™Œç—…ç†åˆ‡ç‰‡ä¸­é¢„æµ‹å…³é”®åŸºå› çªå˜ï¼Œä½†å…¶æ€§èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œè¿„ä»Šä¸ºæ­¢ä¸»è¦å±€é™äºæ—©æœŸç­›æŸ¥ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ±‡é›†äº†PathGeneï¼Œå®ƒåŒ…æ‹¬æ¥è‡ªä¸­å—å¤§å­¦é™„å±ç¬¬äºŒæ¹˜é›…åŒ»é™¢çš„1576åæ‚£è€…çš„ç—…ç†å›¾åƒå’Œä¸‹ä¸€ä»£æµ‹åºæŠ¥å‘Šï¼Œä»¥åŠ448åTCGA-LUADæ‚£è€…ã€‚è¿™ä¸ªå¤šä¸­å¿ƒæ•°æ®é›†å°†å…¨å¹»ç¯ç‰‡å›¾åƒä¸é©±åŠ¨åŸºå› çªå˜çŠ¶æ€ã€çªå˜äºšå‹ã€å¤–æ˜¾å­å’Œè‚¿ç˜¤çªå˜è´Ÿè·ï¼ˆTMBï¼‰çŠ¶æ€ç›¸å…³è”ï¼Œæ—¨åœ¨åˆ©ç”¨ç—…ç†å›¾åƒé¢„æµ‹çªå˜ã€äºšå‹ã€å¤–æ˜¾å­ä½ç½®å’ŒTMBï¼Œç”¨äºæ—©æœŸåŸºå› ç­›æŸ¥ï¼Œå¹¶æ¨åŠ¨ç²¾å‡†è‚¿ç˜¤å­¦çš„å‘å±•ã€‚ä¸ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒPathGeneæä¾›äº†ä¸ç—…ç†å›¾åƒç›¸å…³çš„åˆ†å­æ°´å¹³ä¿¡æ¯ï¼Œæœ‰åŠ©äºå¼€å‘ç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨PathGeneä¸Šå¯¹çªå˜ã€äºšå‹ã€å¤–æ˜¾å­å’ŒTMBé¢„æµ‹ä»»åŠ¡è¿›è¡Œäº†11ç§å¤šå®ä¾‹å­¦ä¹ æ–¹æ³•åŸºå‡†æµ‹è¯•ã€‚è¿™äº›å®éªŒæ–¹æ³•ä¸ºè‚ºç™Œæ‚£è€…çš„æ—©æœŸåŸºå› ç­›æŸ¥æä¾›äº†æœ‰ä»·å€¼çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿå¿«é€Ÿä¸ºæ‚£è€…åˆ¶å®šä¸ªæ€§åŒ–çš„ç²¾å‡†é¶å‘æ²»ç–—æ–¹æ¡ˆã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/panliangrui/NIPS2025/">https://github.com/panliangrui/NIPS2025/</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00096v2">PDF</a> This submission is being withdrawn because we identified issues in   the analysis that may affect the results. A corrected version will be   submitted in the future. The manuscript is withdrawn as it requires   substantial revision. An improved version will be submitted in the future</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨äººå·¥æ™ºèƒ½ä»å¸¸è§„ç—…ç†å›¾åƒæ¨æ–­è‚ºç™ŒåŸºå› çªå˜å’ŒåŸºå› å¤–æ˜¾å­å˜å¼‚çš„é‡è¦æ€§ï¼Œä»¥è§£å†³åŒ»ç–—èµ„æºåˆ†é…ä¸å‡å’ŒåŸºå› ç»„æ£€æµ‹æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºåŒ…å«å¤šä¸­å¿ƒæ•°æ®çš„PathGeneæ•°æ®é›†ï¼Œç»“åˆæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°å¯¹è‚ºç™ŒåŸºå› çªå˜ã€äºšå‹ã€åŸºå› å¤–æ˜¾å­ä½ç½®å’Œè‚¿ç˜¤çªå˜è´Ÿè·çš„é¢„æµ‹ï¼Œä»¥æ¨è¿›ç²¾å‡†è‚¿ç˜¤å­¦çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®é¢„æµ‹è‚ºç™ŒåŸºå› çªå˜ã€äºšå‹åŠå¤–æ˜¾å­å¯¹ä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’å’Œé¢„åè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>åˆ©ç”¨äººå·¥æ™ºèƒ½ä»å¸¸è§„ç—…ç†å›¾åƒæ¨æ–­åŸºå› çªå˜æœ‰åŠ©äºä¿ƒè¿›ç²¾å‡†æ²»ç–—ã€‚</li>
<li>PathGeneæ•°æ®é›†ç»“åˆå¤šä¸­å¿ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç—…ç†å›¾åƒå’ŒåŸºå› æµ‹åºæŠ¥å‘Šï¼Œæ—¨åœ¨é¢„æµ‹è‚ºç™Œçš„å¤šç§é—ä¼ ç‰¹å¾ã€‚</li>
<li>PathGeneæ•°æ®é›†æä¾›åˆ†å­æ°´å¹³ä¿¡æ¯ï¼Œæœ‰åŠ©äºå¼€å‘ç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>å¤šç§å®ä¾‹å­¦ä¹ æ–¹æ³•è¢«åº”ç”¨äºPathGeneæ•°æ®é›†ä¸Šè¿›è¡Œçªå˜ã€äºšå‹ã€å¤–æ˜¾å­å’Œè‚¿ç˜¤çªå˜è´Ÿè·çš„é¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>è¿™äº›æ–¹æ³•å¯¹äºæ—©æœŸè‚ºç™Œæ‚£è€…çš„é—ä¼ ç­›æŸ¥å…·æœ‰å®è´µä»·å€¼ã€‚</li>
<li>è¿™äº›æŠ€æœ¯æœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿå¿«é€Ÿä¸ºè‚ºç™Œæ‚£è€…åˆ¶å®šä¸ªæ€§åŒ–çš„ç²¾å‡†é¶å‘æ²»ç–—æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-734cfb88bb2412f902e06b8676000b96" align="middle">
<img src="https://picx.zhimg.com/v2-21cc0e311dbeef35b13504671441616b" align="middle">
<img src="https://picx.zhimg.com/v2-69cf28ddf94dc8704b9a93985dca4d0b" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning"><a href="#cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning" class="headerlink" title="cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning"></a>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning</h2><p><strong>Authors:Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich</strong></p>
<p>Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ï¼Œä½¿å¾—åˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„3Dæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚ä½¿ç”¨å„ç§ä¼ æ„Ÿå™¨æˆ–ç”¨æˆ·æä¾›çš„æ•°æ®ä½œä¸ºCADé‡å»ºçš„è¾“å…¥ï¼Œå¯ä»¥ä½¿è®¾è®¡åº”ç”¨ç¨‹åºçš„è®¿é—®æ›´åŠ æ™®åŠã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•ä¸€è¾“å…¥æ¨¡å¼ï¼Œå¦‚ç‚¹äº‘ã€å›¾åƒæˆ–æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡å¼ã€‚å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒèŒƒå¼çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåœ¨å¤§é‡ç¨‹åºç”Ÿæˆæ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶åé‡‡ç”¨åœ¨çº¿åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒï¼Œè¿™äº›åé¦ˆæ˜¯é€šè¿‡ç¨‹åºè·å¾—çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡æ¢ç´¢RLå¾®è°ƒLLMç”¨äºCADä»»åŠ¡çš„ç ”ç©¶å›¢é˜Ÿï¼Œè¡¨æ˜åœ¨çº¿RLç®—æ³•ï¼ˆå¦‚é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼‰ä¼˜äºç¦»çº¿æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„SFTæ¨¡å‹åœ¨æ‰€æœ‰ä¸‰ç§è¾“å…¥æ¨¡å¼ä¸Šå‡ä¼˜äºç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç»è¿‡RLå¾®è°ƒåï¼Œâ€œcadrilleâ€åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22914v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œç ”ç©¶æå‡ºä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œå¯åŒæ—¶å¤„ç†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬ä¸‰ç§è¾“å…¥æ¨¡æ€ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒã€‚åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰ç§è¾“å…¥æ¨¡æ€ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ã€‚ç»è¿‡RLå¾®è°ƒåï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šåˆ›é€ äº†æ–°çš„æŠ€æœ¯æ°´å‡†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸šä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ï¼Œèƒ½å¤Ÿåˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„3Dæ¨¡å‹ã€‚</li>
<li>ç°æœ‰çš„CADé‡å»ºæ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€çš„è¾“å…¥æ¨¡æ€ï¼Œå¦‚ç‚¹äº‘ã€å›¾åƒæˆ–æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡æ€ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†ç”¨äºCADä»»åŠ¡çš„LLMçš„RLå¾®è°ƒï¼Œå¹¶å‘ç°åœ¨çº¿RLç®—æ³•å¦‚Group Relative Preference Optimizationï¼ˆGRPOï¼‰ä¼˜äºç¦»çº¿æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹æ€§èƒ½ä¼˜äºç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ï¼ŒåŒæ—¶å¤„ç†æ‰€æœ‰ä¸‰ç§è¾“å…¥æ¨¡æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-650f4be72424061e2be7048baab424fd" align="middle">
<img src="https://picx.zhimg.com/v2-2d0e2f739fc2eeae3602a881f16b83b0" align="middle">
<img src="https://picx.zhimg.com/v2-547dc86ba4c2c54237a277985093fcc6" align="middle">
<img src="https://picx.zhimg.com/v2-26eaafa4b55ab3e8c2172b7fc164265f" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging"><a href="#Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging" class="headerlink" title="Towards Scalable Language-Image Pre-training for 3D Medical Imaging"></a>Towards Scalable Language-Image Pre-training for 3D Medical Imaging</h2><p><strong>Authors:Chenhui Zhao, Yiwei Lyu, Asadur Chowdury, Edward Harake, Akhil Kondepudi, Akshay Rao, Xinhai Hou, Honglak Lee, Todd Hollon</strong></p>
<p>The scalability of current language-image pre-training for 3D medical imaging, such as CT and MRI, is constrained by the need for radiologists to manually curate raw clinical studies. In this work, we pioneer pre-training directly on uncurated studies, which both aligns more closely with the radiologistâ€™s workflow and provides a natural path to scalability. However, the unique structure of such data presents new challenges for existing model architectures, which were originally designed for 2D slices or single 3D scans. To address this, we introduce a novel hierarchical attention mechanism inspired by the intrinsic hierarchy of radiology data: slice, scan, and study. We denote our framework as Hierarchical attention for Language-Image Pre-training (HLIP). Trained on 220K studies with 3.13 million scans for brain MRI and 240K studies with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +10.5% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +8.3% and +1.7% macro AUC on head CT benchmarks CQ500 and RSNA, respectively. HLIP also exhibits strong generalizability on existing 3D medical language-image pre-training benchmarks, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is a scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zch0414/hlip">https://github.com/Zch0414/hlip</a>. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹å¦‚CTå’ŒMRIç­‰3DåŒ»å­¦å½±åƒçš„è¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„å¯æ‰©å±•æ€§å—é™äºæ”¾å°„ç§‘åŒ»ç”Ÿéœ€è¦æ‰‹åŠ¨æ•´ç†åŸå§‹ä¸´åºŠç ”ç©¶çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç‡å…ˆå¯¹æœªæ•´ç†çš„ç ”ç©¶è¿›è¡Œç›´æ¥é¢„è®­ç»ƒï¼Œè¿™æ›´ç¬¦åˆæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ï¼Œå¹¶ä¸ºå®ç°å¯æ‰©å±•æ€§æä¾›äº†è‡ªç„¶è·¯å¾„ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ•°æ®çš„ç‹¬ç‰¹ç»“æ„ç»™ç°æœ‰æ¨¡å‹æ¶æ„å¸¦æ¥äº†æ–°æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹æœ€åˆæ˜¯ä¸º2Dåˆ‡ç‰‡æˆ–å•ä¸ª3Dæ‰«æè€Œè®¾è®¡çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å—æ”¾å°„æ•°æ®å†…åœ¨å±‚æ¬¡ç»“æ„çš„å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹å±‚æ¬¡æ³¨æ„æœºåˆ¶ï¼šåˆ‡ç‰‡ã€æ‰«æå’Œç ”ç©¶ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¡†æ¶ç§°ä¸ºç”¨äºè¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„åˆ†å±‚æ³¨æ„åŠ›ï¼ˆHLIPï¼‰ã€‚åœ¨24ä¸‡é¡¹ç ”ç©¶ã€æ¶‰åŠåŒ…å«144ä¸‡æ‰«æçš„å¤§è„‘MRIä¸Šä»¥åŠæ¶‰åŠè¶…è¿‡æ•°ç™¾ä¸‡åˆ‡ç‰‡çš„å¤§å‹çœŸå®å¤´éƒ¨CTæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒHLIPå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æå‡ºçš„å…¬å¼€å¯ç”¨çš„å¤§è„‘MRIåŸºå‡†æµ‹è¯•Pub-Brain-5ä¸Šæé«˜äº†+10.5ï¼…çš„å¹³è¡¡å‡†ç¡®åº¦ï¼›åœ¨å¤´éƒ¨CTåŸºå‡†æµ‹è¯•CQ500å’ŒRSNAä¸Šåˆ†åˆ«æé«˜äº†+8.3ï¼…å’Œ+1.7ï¼…çš„å®AUCã€‚HLIPåœ¨ç°æœ‰çš„ä¸‰ç»´åŒ»å­¦è¯­è¨€å›¾åƒé¢„è®­ç»ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨CT-RATEä¸Šé¢„è®­ç»ƒååœ¨Rad-ChestCTåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†+4.3ï¼…çš„å®AUCã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HLIPå¯¹æœªæ•´ç†çš„ä¸´åºŠæ•°æ®é›†è¿›è¡Œç›´æ¥é¢„è®­ç»ƒæ˜¯åŒ»å­¦å½±åƒé¢†åŸŸè¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„å¯æ‰©å±•å’Œæœ‰æ•ˆæ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zch0414/hlip%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Zch0414/hlipè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“å‰è¯­è¨€å›¾åƒåœ¨åŒ»å­¦é¢†åŸŸçš„é¢„è®­ç»ƒå—é™äºæ”¾å°„ç§‘åŒ»ç”Ÿæ‰‹åŠ¨ç­›é€‰åŸå§‹ä¸´åºŠç ”ç©¶çš„éœ€è¦ã€‚æœ¬ç ”ç©¶ç‡å…ˆå°è¯•ç›´æ¥åœ¨æœªç­›é€‰çš„ç ”ç©¶ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ›´ç¬¦åˆæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ï¼Œå¹¶ä¸ºæ‰©å¤§è§„æ¨¡æä¾›äº†è‡ªç„¶è·¯å¾„ã€‚é’ˆå¯¹æ­¤ç±»æ•°æ®çš„ç‹¬ç‰¹ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å±‚æ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå€Ÿé‰´æ”¾å°„æ•°æ®çš„å†…åœ¨å±‚æ¬¡ç»“æ„ï¼šåˆ‡ç‰‡ã€æ‰«æå’Œç ”ç©¶ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºç”¨äºè¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„å±‚æ¬¡æ³¨æ„åŠ›æ¡†æ¶ï¼ˆHLIPï¼‰ã€‚åœ¨å…¬å¼€å¯ç”¨çš„è„‘MRIåŸºå‡†æµ‹è¯•Pub-Brain-5ä¸Šï¼ŒHLIPå–å¾—äº†é¢†å…ˆæ°´å¹³ï¼Œå¦‚å¹³è¡¡å‡†ç¡®ç‡æé«˜10.5%ï¼›åœ¨å¤´éƒ¨CTåŸºå‡†æµ‹è¯•CQ500å’ŒRSNAä¸Šï¼Œå®è§‚AUCåˆ†åˆ«æé«˜8.3%å’Œ1.7%ã€‚HLIPåœ¨ç°æœ‰çš„åŒ»å­¦ä¸‰ç»´è¯­è¨€å›¾åƒé¢„è®­ç»ƒåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤æ–¹æ³•çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰åŒ»å­¦å›¾åƒé¢„è®­ç»ƒå—é™äºæ‰‹åŠ¨ç­›é€‰ä¸´åºŠç ”ç©¶çš„ç¹çè¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡å°è¯•åœ¨æœªç­›é€‰çš„ç ”ç©¶æ•°æ®ä¸Šç›´æ¥è¿›è¡Œé¢„è®­ç»ƒï¼Œæ›´è´´è¿‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹å¹¶å®ç°è§„æ¨¡åŒ–ã€‚</li>
<li>ä¸ºè§£å†³æ•°æ®ç‹¬ç‰¹ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¼•å…¥äº†æ–°é¢–çš„å±‚æ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬åˆ‡ç‰‡ã€æ‰«æå’Œç ”ç©¶ä¸‰ä¸ªå±‚æ¬¡ã€‚</li>
<li>HLIPæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚Pub-Brain-5ä¸Šçš„å¹³è¡¡å‡†ç¡®ç‡æé«˜10.5%ã€‚</li>
<li>HLIPå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§åŒ»å­¦ä¸‰ç»´è¯­è¨€å›¾åƒé¢„è®­ç»ƒåœºæ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a46f6305646d393794ca00c089a85a24" align="middle">
<img src="https://picx.zhimg.com/v2-bc035046dd104898d7c32accd27cd561" align="middle">
<img src="https://picx.zhimg.com/v2-dbe7e9418b1d9120b1dcf783090e90a1" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DescriptorMedSAM-Language-Image-Fusion-with-Multi-Aspect-Text-Guidance-for-Medical-Image-Segmentation"><a href="#DescriptorMedSAM-Language-Image-Fusion-with-Multi-Aspect-Text-Guidance-for-Medical-Image-Segmentation" class="headerlink" title="DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance   for Medical Image Segmentation"></a>DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance   for Medical Image Segmentation</h2><p><strong>Authors:Wenjie Zhang, Liming Luo, Mengnan He, Jiarui Hai, Jiancheng Ye</strong></p>
<p>Accurate organ segmentation is essential for clinical tasks such as radiotherapy planning and disease monitoring. Recent foundation models like MedSAM achieve strong results using point or bounding-box prompts but still require manual interaction. We propose DescriptorMedSAM, a lightweight extension of MedSAM that incorporates structured text prompts, ranging from simple organ names to combined shape and location descriptors to enable click-free segmentation. DescriptorMedSAM employs a CLIP text encoder to convert radiology-style descriptors into dense embeddings, which are fused with visual tokens via a cross-attention block and a multi-scale feature extractor. We designed four descriptor types: Name (N), Name + Shape (NS), Name + Location (NL), and Name + Shape + Location (NSL), and evaluated them on the FLARE 2022 dataset under zero-shot and few-shot settings, where organs unseen during training must be segmented with minimal additional data. NSL prompts achieved the highest performance, with a Dice score of 0.9405 under full supervision, a 76.31% zero-shot retention ratio, and a 97.02% retention ratio after fine-tuning with only 50 labeled slices per unseen organ. Adding shape and location cues consistently improved segmentation accuracy, especially for small or morphologically complex structures. We demonstrate that structured language prompts can effectively replace spatial interactions, delivering strong zero-shot performance and rapid few-shot adaptation. By quantifying the role of descriptor, this work lays the groundwork for scalable, prompt-aware segmentation models that generalize across diverse anatomical targets with minimal annotation effort. </p>
<blockquote>
<p>ç²¾ç¡®å™¨å®˜åˆ†å‰²å¯¹äºä¸´åºŠä»»åŠ¡ï¼ˆå¦‚æ”¾ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹ï¼‰è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„MedSAMç­‰åŸºç¡€æ¨¡å‹é€šè¿‡ä½¿ç”¨ç‚¹æˆ–è¾¹ç•Œæ¡†æç¤ºå–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œä½†ä»éœ€è¦æ‰‹åŠ¨äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†DescriptorMedSAMï¼Œå®ƒæ˜¯MedSAMçš„è½»é‡çº§æ‰©å±•ï¼Œç»“åˆäº†ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºï¼Œä»ç®€å•çš„å™¨å®˜åç§°åˆ°ç»“åˆå½¢çŠ¶å’Œä½ç½®æè¿°ç¬¦ï¼Œä»¥å®ç°æ— éœ€ç‚¹å‡»çš„åˆ†å‰²ã€‚DescriptorMedSAMé‡‡ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†æ”¾å°„å­¦é£æ ¼çš„æè¿°ç¬¦è½¬æ¢ä¸ºå¯†é›†åµŒå…¥ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›å—å’Œå¤šå°ºåº¦ç‰¹å¾æå–å™¨ä¸è§†è§‰ä»¤ç‰Œèåˆã€‚æˆ‘ä»¬è®¾è®¡äº†å››ç§æè¿°ç¬¦ç±»å‹ï¼šåç§°ï¼ˆNï¼‰ã€åç§°+å½¢çŠ¶ï¼ˆNSï¼‰ã€åç§°+ä½ç½®ï¼ˆNLï¼‰å’Œåç§°+å½¢çŠ¶+ä½ç½®ï¼ˆNSLï¼‰ï¼Œå¹¶åœ¨FLARE 2022æ•°æ®é›†ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼Œå…¶ä¸­åœ¨è®­ç»ƒæœŸé—´æœªè§çš„å™¨å®˜å¿…é¡»åœ¨æ²¡æœ‰é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†å‰²ã€‚NSLæç¤ºåœ¨å…¨ç›‘ç£ä¸‹è·å¾—äº†æœ€é«˜æ€§èƒ½ï¼ŒDiceåˆ†æ•°ä¸º0.9405ï¼Œé›¶æ ·æœ¬ä¿ç•™ç‡ä¸º76.31%ï¼Œä»…åœ¨çœ‹ä¸è§çš„å™¨å®˜ä¸Šæœ‰50ä¸ªæ ‡ç­¾åˆ‡ç‰‡åè¿›è¡Œå¾®è°ƒæ—¶ä¿ç•™ç‡ä¸º97.02%ã€‚æ·»åŠ å½¢çŠ¶å’Œä½ç½®çº¿ç´¢å¯ä»¥æŒç»­æ”¹å–„åˆ†å‰²ç²¾åº¦ï¼Œå°¤å…¶æ˜¯å¯¹äºå°å‹æˆ–å½¢æ€å¤æ‚çš„ç»“æ„ã€‚æˆ‘ä»¬è¯æ˜äº†ç»“æ„åŒ–è¯­è¨€æç¤ºå¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£ç©ºé—´äº¤äº’ï¼Œæä¾›å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶å¿«é€Ÿé€‚åº”å°‘æ•°æ ·æœ¬ã€‚é€šè¿‡å¯¹æè¿°ç¬¦çš„ä½œç”¨è¿›è¡Œé‡åŒ–ï¼Œè¿™é¡¹å·¥ä½œå¥ å®šäº†å¯æ‰©å±•ã€æç¤ºæ„ŸçŸ¥åˆ†å‰²æ¨¡å‹çš„åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨å¤šç§è§£å‰–ç›®æ ‡ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Œå¹¶å‡å°‘æ³¨é‡Šå·¥ä½œé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13806v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DescriptorMedSAMæ¨¡å‹ï¼Œå®ƒæ˜¯MedSAMçš„è½»é‡çº§æ‰©å±•ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç»“æ„åŒ–æ–‡æœ¬æç¤ºï¼Œç»“åˆç‚¹æˆ–è¾¹ç•Œæ¡†æç¤ºï¼Œå®ç°äº†æ— éœ€ç‚¹å‡»çš„åˆ†å‰²ã€‚é€šè¿‡èåˆæ”¾å°„å­¦æè¿°ç¬¦å·ä¸è§†è§‰æ ‡è®°ï¼ŒDescriptorMedSAMåœ¨FLARE 2022æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ç»“åˆå½¢çŠ¶å’Œä½ç½®æç¤ºæ—¶è¡¨ç°æ›´ä½³ã€‚æ­¤æ¨¡å‹ä¸ºå¯æ‰©å±•ã€æç¤ºæ„ŸçŸ¥çš„åˆ†å‰²æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œèƒ½å¤Ÿè·¨å¤šç§è§£å‰–ç›®æ ‡è¿›è¡Œæ³›åŒ–ï¼Œå¹¶å‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DescriptorMedSAMæ˜¯MedSAMçš„è½»é‡çº§æ‰©å±•ï¼Œç”¨äºå®ç°æ— éœ€ç‚¹å‡»çš„å™¨å®˜åˆ†å‰²ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç»“æ„åŒ–æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬å™¨å®˜åç§°ã€å½¢çŠ¶å’Œä½ç½®æè¿°ç­‰ã€‚</li>
<li>DescriptorMedSAMåœ¨FLARE 2022æ•°æ®é›†ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå¾ˆé«˜çš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆå½¢çŠ¶å’Œä½ç½®æç¤ºæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æˆ–å½¢æ€å¤æ‚çš„ç»“æ„ä¸Šã€‚</li>
<li>è¯¥æ¨¡å‹å¯æœ‰æ•ˆæ›¿ä»£ç©ºé—´äº¤äº’ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½å’Œå¿«é€Ÿçš„å°‘æ ·æœ¬é€‚åº”ã€‚</li>
<li>DescripterMedSAMä¸ºå¯æ‰©å±•ã€æç¤ºæ„ŸçŸ¥çš„åˆ†å‰²æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒçš„è§£å‰–ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4253c5d9fa17f9132478dce4fa5834bd" align="middle">
<img src="https://picx.zhimg.com/v2-d35ec510ec06e223c0ce830aa1e7106c" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition"><a href="#LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition" class="headerlink" title="LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?"></a>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?</h2><p><strong>Authors:Bangyan Li, Wenxuan Huang, Zhenkun Gao, Yeqiang Wang, Yunhang Shen, Jingzhong Lin, Ling You, Yuxiang Shen, Shaohui Lin, Wanli Ouyang, Yuling Sun</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, we found that MLLMs cannot process effectively from fine-grained medical image data in the traditional Visual Question Answering (VQA) pipeline, as they do not exploit the captured features and available medical knowledge fully, results in MLLMs usually performing poorly in zero-shot medical disease recognition. Fortunately, this limitation does not indicate that MLLMs are fundamentally incapable of addressing fine-grained recognition tasks. From a feature representation perspective, MLLMs demonstrate considerable potential for tackling such challenging problems. Thus, to address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition via utilizing the existing MLLM features. Specifically, we design an end-to-end training strategy, termed Decoding-Side Feature Alignment Training (DFAT) to take advantage of the characteristics of the MLLM decoder architecture and incorporate modality-specific tokens tailored for different modalities. Additionally, we introduce a Domain Knowledge Anchoring Module (DKAM) to exploit the intrinsic medical knowledge of large models, which mitigates the category semantic gap in image-text alignment. Extensive experiments demonstrate that our LLaVA-RadZ significantly outperforms traditional MLLMs in zero-shot disease recognition, achieving the comparable performance to the well-established and highly-optimized CLIP-based approaches. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°MLLMsåœ¨ä¼ ç»Ÿçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç®¡é“ä¸­æ— æ³•æœ‰æ•ˆåœ°å¤„ç†ç²¾ç»†çš„åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å……åˆ†åˆ©ç”¨æ•è·çš„ç‰¹å¾å’Œå¯ç”¨çš„åŒ»å­¦çŸ¥è¯†ï¼Œå¯¼è‡´MLLMsé€šå¸¸åœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­è¡¨ç°ä¸ä½³ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§å¹¶ä¸æ„å‘³ç€MLLMsä»æ ¹æœ¬ä¸Šæ— æ³•è§£å†³ç²¾ç»†è¯†åˆ«ä»»åŠ¡ã€‚ä»ç‰¹å¾è¡¨ç¤ºçš„è§’åº¦æ¥çœ‹ï¼ŒMLLMsåœ¨è§£å†³æ­¤ç±»æŒ‘æˆ˜æ€§é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-RadZæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç°æœ‰MLLMç‰¹å¾è¿›è¡Œé›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«çš„ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºè§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰ï¼Œä»¥åˆ©ç”¨MLLMè§£ç å™¨çš„ç‰¹ç‚¹ï¼Œå¹¶èå…¥é’ˆå¯¹ä¸åŒæ¨¡æ€çš„ç‰¹å®šæ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œä»¥åˆ©ç”¨å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ï¼Œè¿™å‡è½»äº†å›¾åƒæ–‡æœ¬å¯¹é½ä¸­çš„ç±»åˆ«è¯­ä¹‰å·®è·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LLaVA-RadZåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸMLLMsï¼Œå…¶æ€§èƒ½ä¸ç»è¿‡è‰¯å¥½éªŒè¯å’Œä¼˜åŒ–è¿‡çš„CLIPæ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07487v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦åŒ»å­¦å›¾åƒæ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆåœ°è¿›è¡Œè§†è§‰ç†è§£å’Œæ¨ç†ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†LLaVA-RadZæ¡†æ¶ï¼Œåˆ©ç”¨ç°æœ‰MLLMç‰¹æ€§è¿›è¡Œé›¶æ ·æœ¬åŒ»å­¦ç–¾ç—…è¯†åˆ«ã€‚é€šè¿‡è®¾è®¡è§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒç­–ç•¥ï¼ˆDFATï¼‰å’Œé¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«çš„æ€§èƒ½ï¼Œè¾¾åˆ°ä¸ç»è¿‡è‰¯å¥½ä¼˜åŒ–å’Œé«˜åº¦ä¼˜åŒ–çš„CLIPæ–¹æ³•ç›¸å½“çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LLaVA-RadZæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶æé«˜äº†é›¶æ ·æœ¬åŒ»å­¦ç–¾ç—…è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>LLaVA-RadZåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œé€šè¿‡è®¾è®¡è§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒç­–ç•¥ï¼ˆDFATï¼‰ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ä»¥åˆ©ç”¨å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ã€‚</li>
<li>LLaVA-RadZåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç»è¿‡é«˜åº¦ä¼˜åŒ–çš„CLIPæ–¹æ³•ç›¸å½“ã€‚</li>
<li>è¯¥æ¡†æ¶ç®€å•æœ‰æ•ˆï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒé¢†åŸŸçš„ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b9d97277ee0cb9a02e0f15ba3fd937e" align="middle">
<img src="https://picx.zhimg.com/v2-e2baac8642f1e902fcef43c6530079de" align="middle">
<img src="https://picx.zhimg.com/v2-39ed1522903d035d09885671c22c8a6b" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain"><a href="#Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain" class="headerlink" title="Rethinking domain generalization in medical image segmentation: One   image as one domain"></a>Rethinking domain generalization in medical image segmentation: One   image as one domain</h2><p><strong>Authors:Jin Hong, Bo Liu, Qiankun Zuo, Guoli Long, Siyue Li, Yudong Zhang, Shuihua Wang, Junxin Chen</strong></p>
<p>Domain shifts in medical image segmentation, particularly when data comes from different centers, pose significant challenges. Intra-center variability, such as differences in scanner models or imaging protocols, can cause domain shifts as large as, or even larger than, those between centers. To address this, we propose the â€œone image as one domainâ€ (OIOD) hypothesis, which treats each image as a unique domain, enabling flexible and robust domain generalization. Based on this hypothesis, we develop a unified disentanglement-based domain generalization (UniDDG) framework, which simultaneously handles both multi-source and single-source domain generalization without requiring explicit domain labels. This approach simplifies training with a fixed architecture, independent of the number of source domains, reducing complexity and enhancing scalability. We decouple each input image into content representation and style code, then exchange and combine these within the batch for segmentation, reconstruction, and further disentanglement. By maintaining distinct style codes for each image, our model ensures thorough decoupling of content representations and style codes, improving domain invariance of the content representations. Additionally, we enhance generalization with expansion mask attention (EMA) for boundary preservation and style augmentation (SA) to simulate diverse image styles, improving robustness to domain shifts. Extensive experiments show that our method achieves Dice scores of 84.43% and 88.91% for multi-source to single-center and single-center generalization in optic disc and optic cup segmentation, respectively, and 86.96% and 88.56% for prostate segmentation, outperforming current state-of-the-art domain generalization methods, offering superior performance and adaptability across clinical settings. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ¼‚ç§»ï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®æ¥è‡ªä¸åŒä¸­å¿ƒæ—¶ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æ¥è‡ªåŒä¸€ä¸­å¿ƒçš„å†…éƒ¨å·®å¼‚ï¼ˆå¦‚æ‰«æä»ªå‹å·æˆ–æˆåƒåè®®çš„ä¸åŒï¼‰å¯èƒ½äº§ç”Ÿä¸ä¸åŒä¸­å¿ƒä¹‹é—´çš„å·®å¼‚ä¸€æ ·å¤§çš„é¢†åŸŸæ¼‚ç§»ï¼Œç”šè‡³æ›´å¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¸€å›¾ä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œå°†æ¯ä¸€å¼ å›¾åƒè§†ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„é¢†åŸŸï¼Œä»¥å®ç°çµæ´»å’Œç¨³å¥çš„é¢†åŸŸæ³›åŒ–ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºåˆ†ç¦»çš„é¢†åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–ï¼Œæ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†ä½¿ç”¨å›ºå®šæ¶æ„çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ— è®ºæºé¢†åŸŸæ•°é‡å¦‚ä½•ï¼Œéƒ½é™ä½äº†å¤æ‚æ€§å¹¶å¢å¼ºäº†å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å°†æ¯ä¸ªè¾“å…¥å›¾åƒè§£è€¦ä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œç„¶ååœ¨æ‰¹æ¬¡å†…äº¤æ¢å¹¶ç»“åˆè¿™äº›ä»£ç è¿›è¡Œåˆ†å‰²ã€é‡å»ºå’Œè¿›ä¸€æ­¥çš„è§£è€¦ã€‚é€šè¿‡ä¿æŒæ¯ä¸ªå›¾åƒç‹¬ç‰¹çš„é£æ ¼ä»£ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç¡®ä¿äº†å†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„å½»åº•è§£è€¦ï¼Œæé«˜äº†å†…å®¹è¡¨ç¤ºçš„é¢†åŸŸä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰è¿›è¡Œè¾¹ç•Œä¿ç•™å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æ¥æ¨¡æ‹Ÿå¤šç§å›¾åƒé£æ ¼ï¼Œæé«˜æ¨¡å‹å¯¹é¢†åŸŸæ¼‚ç§»çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†ç›˜å’Œè§†æ¯åˆ†å‰²çš„å¤šæºåˆ°å•ä¸­å¿ƒä»¥åŠå•ä¸­å¿ƒæ³›åŒ–ä¸­åˆ†åˆ«å®ç°äº†84.43%å’Œ88.91%çš„Diceå¾—åˆ†ï¼Œå‰åˆ—è…ºåˆ†å‰²ä¸­åˆ†åˆ«å®ç°äº†86.96%å’Œ88.56%çš„Diceå¾—åˆ†ã€‚è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨å„ç§ä¸´åºŠç¯å¢ƒä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04741v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è·¨ä¸­å¿ƒæ•°æ®å¼•èµ·çš„é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæå‡ºâ€œä¸€å¹…å›¾åƒä½œä¸ºä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾å’ŒåŸºäºç»Ÿä¸€è§£çº ç¼ çš„é¢†åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾å³å¯åŒæ—¶å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚é€šè¿‡å°†è¾“å…¥å›¾åƒè§£è€¦ä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œå¹¶åœ¨æ‰¹æ¬¡å†…è¿›è¡Œäº¤æ¢å’Œç»„åˆï¼Œä»¥å®ç°åˆ†å‰²ã€é‡å»ºå’Œè¿›ä¸€æ­¥çš„è§£çº ç¼ ã€‚é€šè¿‡ä¸ºæ¯ä¸ªå›¾åƒç»´æŠ¤ç‹¬ç‰¹çš„é£æ ¼ä»£ç ï¼Œç¡®ä¿äº†å†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„å®Œå…¨è§£è€¦ï¼Œæé«˜äº†å†…å®¹è¡¨ç¤ºçš„é¢†åŸŸä¸å˜æ€§ã€‚åœ¨å…‰å­¦åœ†ç›˜ã€å…‰å­¦æ¯çŠ¶ç‰©çš„åˆ†å‰²ä»¥åŠå‰åˆ—è…ºåˆ†å‰²çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­çš„ä¸åŒè®¾ç½®æä¾›äº†ä¼˜è¶Šçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è·¨ä¸­å¿ƒå’Œè·¨æ¨¡å‹æ•°æ®å¯¼è‡´é¢†åŸŸæ¼‚ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºâ€œä¸€å¹…å›¾åƒä½œä¸ºä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œå°†æ¯å¹…å›¾åƒè§†ä¸ºç‹¬ç‰¹é¢†åŸŸï¼Œå®ç°çµæ´»å’Œç¨³å¥çš„é¢†åŸŸæ³›åŒ–ã€‚</li>
<li>å‘å±•åŸºäºç»Ÿä¸€è§£çº ç¼ çš„é¢†åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ï¼Œæ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾å³å¯å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡è§£è€¦è¾“å…¥å›¾åƒä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œç®€åŒ–è®­ç»ƒå¹¶å¢å¼ºé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»´æŠ¤æ¯ä¸ªå›¾åƒç‹¬ç‰¹çš„é£æ ¼ä»£ç ï¼Œç¡®ä¿å†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„å®Œå…¨è§£è€¦ã€‚</li>
<li>ä½¿ç”¨æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰è¿›ä¸€æ­¥æé«˜é¢†åŸŸæ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0f39c2811ff967806d069ac8dcd677f" align="middle">
<img src="https://picx.zhimg.com/v2-0be162de5308a3707d2df0d800671c20" align="middle">
<img src="https://picx.zhimg.com/v2-fcfdad5bab14a564f241c68c162f58c8" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings"><a href="#CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings" class="headerlink" title="CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings"></a>CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings</h2><p><strong>Authors:Fuyi Yang, Jiazuo Mu, Yanshun Zhang, Mingqian Zhang, Junxiong Zhang, Yongjian Luo, Lan Xu, Jingyi Yu, Yujiao Shi, Yingliang Zhang</strong></p>
<p>We introduce CADSpotting, an effective method for panoptic symbol spotting in large-scale architectural CAD drawings. Existing approaches often struggle with symbol diversity, scale variations, and overlapping elements in CAD designs, and typically rely on additional features (e.g., primitive types or graphical layers) to improve performance. CADSpotting overcomes these challenges by representing primitives through densely sampled points with only coordinate attributes, using a unified 3D point cloud model for robust feature learning. To enable accurate segmentation in large drawings, we further propose a novel Sliding Window Aggregation (SWA) technique that combines weighted voting and Non-Maximum Suppression (NMS). Moreover, we introduce LS-CAD, a new large-scale dataset comprising 45 finely annotated floorplans, each covering approximately 1,000 $m^2$, significantly larger than prior benchmarks. LS-CAD will be publicly released to support future research. Experiments on FloorPlanCAD and LS-CAD demonstrate that CADSpotting significantly outperforms existing methods. We also showcase its practical value by enabling automated parametric 3D interior reconstruction directly from raw CAD inputs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CADSpottingï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§å‹å»ºç­‘CADå›¾çº¸ä¸­è¿›è¡Œå…¨æ™¯ç¬¦å·è¯†åˆ«çš„é«˜æ•ˆæ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é¢ä¸´ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’ŒCADè®¾è®¡ä¸­çš„é‡å å…ƒç´ ç­‰æŒ‘æˆ˜ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–é¢å¤–çš„ç‰¹å¾ï¼ˆä¾‹å¦‚åŸºæœ¬ç±»å‹æˆ–å›¾å½¢å±‚ï¼‰æ¥æé«˜æ€§èƒ½ã€‚CADSpottingé€šè¿‡ä»…å…·æœ‰åæ ‡å±æ€§çš„å¯†é›†é‡‡æ ·ç‚¹æ¥è¡¨ç¤ºåŸºæœ¬å…ƒç´ ï¼Œä½¿ç”¨ç»Ÿä¸€çš„3Dç‚¹äº‘æ¨¡å‹è¿›è¡Œç¨³å¥çš„ç‰¹å¾å­¦ä¹ ï¼Œä»è€Œå…‹æœäº†è¿™äº›æŒ‘æˆ˜ã€‚ä¸ºäº†åœ¨å¤§å›¾ä¸­å®ç°ç²¾ç¡®åˆ†å‰²ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°å‹çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†åŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†LS-CADï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«45ä¸ªç²¾ç»†æ ‡æ³¨çš„å¹³é¢å›¾ï¼Œæ¯ä¸ªå¹³é¢å›¾è¦†ç›–çº¦1000å¹³æ–¹ç±³ï¼Œæ˜¾è‘—å¤§äºå…ˆå‰çš„åŸºå‡†æµ‹è¯•ã€‚LS-CADå°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚åœ¨FloorPlanCADå’ŒLS-CADä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCADSpottingæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ç›´æ¥ä»åŸå§‹CADè¾“å…¥è¿›è¡Œè‡ªåŠ¨åŒ–çš„å‚æ•°åŒ–3Då®¤å†…é‡å»ºï¼Œå±•ç¤ºäº†å…¶å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07377v4">PDF</a> 16pages, 14 figures, Project web-page:   <a target="_blank" rel="noopener" href="https://dgeneai.github.io/cadspotting-pages/">https://dgeneai.github.io/cadspotting-pages/</a></p>
<p><strong>Summary</strong></p>
<p>CADSpottingæ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³å¤§è§„æ¨¡å»ºç­‘CADç»˜å›¾ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¸¸é¢ä¸´ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å ç­‰æŒ‘æˆ˜ï¼Œå¹¶é€šå¸¸ä¾èµ–é¢å¤–ç‰¹å¾ï¼ˆå¦‚åŸå§‹ç±»å‹æˆ–å›¾å½¢å±‚ï¼‰æ¥æé«˜æ€§èƒ½ã€‚CADSpottingé€šè¿‡å¯†é›†é‡‡æ ·ç‚¹å¹¶ç”¨ç»Ÿä¸€çš„ä¸‰ç»´ç‚¹äº‘æ¨¡å‹è¿›è¡Œç¨³å¥ç‰¹å¾å­¦ä¹ æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæå‡ºäº†æ–°é¢–çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰å®ç°å‡†ç¡®çš„å¤§å›¾åˆ†å‰²ã€‚å¹¶å¼•å…¥äº†æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†LS-CADï¼ŒåŒ…å«45ä¸ªç²¾ç»†æ ‡æ³¨çš„å¹³é¢å›¾ï¼Œæ¯ä¸ªå¹³é¢å›¾çº¦è¦†ç›–1000å¹³æ–¹ç±³ï¼Œæ˜¾è‘—è¶…è¿‡å…ˆå‰åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADSpottingæ˜¯ä¸€ç§æœ‰æ•ˆçš„å…¨æ™¯ç¬¦å·è¯†åˆ«æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å»ºç­‘CADç»˜å›¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œå…ƒç´ é‡å çš„æŒ‘æˆ˜ã€‚</li>
<li>CADSpottingé€šè¿‡å¯†é›†é‡‡æ ·ç‚¹å’Œç»Ÿä¸€çš„ä¸‰ç»´ç‚¹äº‘æ¨¡å‹è¿›è¡Œç¨³å¥ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>æå‡ºäº†æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰å®ç°å‡†ç¡®çš„å¤§å›¾åˆ†å‰²ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†LS-CADï¼Œç”¨äºæ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
<li>CADSpottingåœ¨FloorPlanCADå’ŒLS-CADä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e7feb8fec2d5db6191ae11656a0f7e7" align="middle">
<img src="https://picx.zhimg.com/v2-b335a619d2b83f061a65963c7df315ee" align="middle">
<img src="https://picx.zhimg.com/v2-410cb1eac9a4a041438009edeeaaf882" align="middle">
<img src="https://picx.zhimg.com/v2-5fd94cd81f054467db247046d024279f" align="middle">
<img src="https://picx.zhimg.com/v2-3f3e65e71d53692c48144c67d4ef0ab4" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Development-and-Validation-of-a-Large-Language-Model-for-Generating-Fully-Structured-Radiology-Reports"><a href="#Development-and-Validation-of-a-Large-Language-Model-for-Generating-Fully-Structured-Radiology-Reports" class="headerlink" title="Development and Validation of a Large Language Model for Generating   Fully-Structured Radiology Reports"></a>Development and Validation of a Large Language Model for Generating   Fully-Structured Radiology Reports</h2><p><strong>Authors:Chuang Niu, Md Sayed Tanveer, Md Zabirul Islam, Parisa Kaviani, Qing Lyu, Mannudeep K. Kalra, Christopher T. Whitlow, Ge Wang</strong></p>
<p>Current LLMs for creating fully-structured reports face the challenges of formatting errors, content hallucinations, and privacy leakage issues when uploading data to external servers.We aim to develop an open-source, accurate LLM for creating fully-structured and standardized LCS reports from varying free-text reports across institutions and demonstrate its utility in automatic statistical analysis and individual lung nodule retrieval. With IRB approvals, our retrospective study included 5,442 de-identified LDCT LCS radiology reports from two institutions. We constructed two evaluation datasets by labeling 500 pairs of free-text and fully-structured radiology reports and one large-scale consecutive dataset from January 2021 to December 2023. Two radiologists created a standardized template for recording 27 lung nodule features on LCS. We designed a dynamic-template-constrained decoding method to enhance existing LLMs for creating fully-structured reports from free-text radiology reports. Using consecutive structured reports, we automated descriptive statistical analyses and a nodule retrieval prototype. Our best LLM for creating fully-structured reports achieved high performance on cross-institutional datasets with an F1 score of about 97%, with neither formatting errors nor content hallucinations. Our method consistently improved the best open-source LLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically derived statistical distributions were consistent with prior findings regarding attenuation, location, size, stability, and Lung-RADS. The retrieval system with structured reports allowed flexible nodule-level search and complex statistical analysis. Our developed software is publicly available for local deployment and further research. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ›å»ºå…¨ç»“æ„åŒ–æŠ¥å‘Šæ—¶é¢ä¸´ç€æ ¼å¼é”™è¯¯ã€å†…å®¹å¹»è§‰å’Œéšç§æ³„éœ²çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å°†æ•°æ®ä¸Šä¼ åˆ°å¤–éƒ¨æœåŠ¡å™¨æ—¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§å¼€æºã€ç²¾ç¡®çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºä»ä¸åŒæœºæ„çš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šåˆ›å»ºå…¨ç»“æ„åŒ–ã€æ ‡å‡†åŒ–çš„LCSæŠ¥å‘Šï¼Œå¹¶å±•ç¤ºå…¶åœ¨è‡ªåŠ¨ç»Ÿè®¡åˆ†æå’Œä¸ªäººè‚ºç»“èŠ‚æ£€ç´¢ä¸­çš„å®ç”¨æ€§ã€‚ç»è¿‡ä¼¦ç†å®¡æŸ¥å§”å‘˜ä¼šæ‰¹å‡†ï¼Œæˆ‘ä»¬çš„å›é¡¾æ€§ç ”ç©¶çº³å…¥äº†æ¥è‡ªä¸¤ä¸ªæœºæ„çš„5442ä»½å»æ ‡è¯†åŒ–çš„LDCT LCSæ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬é€šè¿‡æ ‡è®°500å¯¹è‡ªç”±æ–‡æœ¬å’Œå…¨ç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šæ„å»ºäº†ä¸¤ä¸ªè¯„ä¼°æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªä»2021å¹´1æœˆè‡³2023å¹´12æœˆçš„å¤§è§„æ¨¡è¿ç»­æ•°æ®é›†ã€‚ä¸¤ä½æ”¾å°„ç§‘åŒ»ç”Ÿä¸ºLCSä¸Šè®°å½•27ä¸ªè‚ºç»“èŠ‚ç‰¹å¾åˆ›å»ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–æ¨¡æ¿ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŠ¨æ€æ¨¡æ¿çº¦æŸè§£ç æ–¹æ³•ï¼Œä»¥å¢å¼ºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä»è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­ç”Ÿæˆå…¨ç»“æ„åŒ–æŠ¥å‘Šçš„èƒ½åŠ›ã€‚ä½¿ç”¨è¿ç»­çš„ç»“æ„åŒ–æŠ¥å‘Šï¼Œæˆ‘ä»¬è‡ªåŠ¨åŒ–äº†æè¿°æ€§ç»Ÿè®¡åˆ†æå’Œä¸€ä¸ªç»“èŠ‚æ£€ç´¢åŸå‹ã€‚æˆ‘ä»¬åˆ›å»ºå…¨ç»“æ„åŒ–æŠ¥å‘Šçš„æœ€ä½³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨æœºæ„æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾çº¦97%çš„F1åˆ†æ•°ï¼Œæ²¡æœ‰å‡ºç°æ ¼å¼é”™è¯¯æˆ–å†…å®¹å¹»è§‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ–­æ”¹è¿›äº†æœ€ä½³å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜äº†æœ€å¤šè¾¾10.42%ï¼Œå¹¶ä¸”æ¯”GPT-4oæé«˜äº†17.19%ã€‚è‡ªåŠ¨å¾—å‡ºçš„ç»Ÿè®¡åˆ†å¸ƒä¸å…³äºè¡°å‡ã€ä½ç½®ã€å¤§å°ã€ç¨³å®šæ€§å’ŒLung-RADSçš„å…ˆå‰å‘ç°ä¸€è‡´ã€‚ä½¿ç”¨ç»“æ„åŒ–æŠ¥å‘Šçš„æ£€ç´¢ç³»ç»Ÿå¯ä»¥è¿›è¡Œçµæ´»çš„ç»“èŠ‚çº§æœç´¢å’Œå¤æ‚çš„ç»Ÿè®¡åˆ†æã€‚æˆ‘ä»¬å¼€å‘çš„è½¯ä»¶å¯å…¬å¼€ç”¨äºæœ¬åœ°éƒ¨ç½²å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18319v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ›å»ºç»“æ„åŒ–æŠ¥å‘Šæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ ¼å¼é”™è¯¯ã€å†…å®¹å¹»è§‰å’Œéšç§æ³„éœ²é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¼€æºã€å‡†ç¡®çš„LLMï¼Œèƒ½å¤Ÿä»ä¸åŒæœºæ„çš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šåˆ›å»ºæ ‡å‡†åŒ–ç»“æ„åŒ–è‚ºç™Œç­›æŸ¥ï¼ˆLCSï¼‰æŠ¥å‘Šï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è‡ªåŠ¨ç»Ÿè®¡åˆ†æåŠä¸ªæ€§åŒ–è‚ºç»“èŠ‚æ£€ç´¢ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å›é¡¾æ€§ç ”ç©¶ï¼ŒéªŒè¯äº†æ¨¡å‹çš„é«˜æ€§èƒ½ï¼Œå¹¶å…¬å¼€äº†å¼€å‘çš„è½¯ä»¶ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰LLMåœ¨åˆ›å»ºç»“æ„åŒ–æŠ¥å‘Šæ—¶å­˜åœ¨æ ¼å¼é”™è¯¯ã€å†…å®¹å¹»è§‰å’Œéšç§æ³„éœ²çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¼€æºçš„LLMï¼Œèƒ½å¤Ÿåˆ›å»ºæ ‡å‡†åŒ–çš„ç»“æ„åŒ–LCSæŠ¥å‘Šã€‚</li>
<li>è¯¥LLMå¯ä»¥ä»ä¸åŒæœºæ„çš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šä¸­ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šã€‚</li>
<li>é€šè¿‡å›é¡¾æ€§ç ”ç©¶éªŒè¯äº†LLMçš„é«˜æ€§èƒ½ã€‚</li>
<li>è¯¥LLMåœ¨è‡ªåŠ¨ç»Ÿè®¡åˆ†æå’Œè‚ºç»“èŠ‚æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºå®ç”¨æ€§ã€‚</li>
<li>æœ€ä½³LLMçš„F1åˆ†æ•°è¾¾åˆ°çº¦97%ï¼Œä¸”åœ¨è·¨æœºæ„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c64c7ca814bc8a94f28f9900944c157" align="middle">
<img src="https://picx.zhimg.com/v2-c789a0822870a960da413403e152a511" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Unified-Deep-Learning-Framework-for-Motion-Correction-in-Medical-Imaging"><a href="#A-Unified-Deep-Learning-Framework-for-Motion-Correction-in-Medical-Imaging" class="headerlink" title="A Unified Deep Learning Framework for Motion Correction in Medical   Imaging"></a>A Unified Deep Learning Framework for Motion Correction in Medical   Imaging</h2><p><strong>Authors:Jian Wang, Razieh Faghihpirayesh, Danny Joca, Polina Golland, Ali Gholipour</strong></p>
<p>Deep learning has shown significant value in image registration, however, current techniques are either limited by the type and range of motion they can handle, or require iterative inference and&#x2F;or retraining for new imaging data. To address these limitations, we introduce UniMo, a Unified Motion Correction framework that leverages deep neural networks to correct diverse motion in medical imaging. UniMo employs an alternating optimization scheme for a unified loss function to train an integrated model of 1) an equivariant neural network for global rigid motion correction and 2) an encoder-decoder network for local deformations. It features a geometric deformation augmenter that 1) enhances the robustness of global correction by addressing local deformations from non-rigid motion or geometric distortions, and 2) generates augmented data to improve training. UniMo is a hybrid model that uses both image intensities and shapes to achieve robust performance amid appearance variations, and therefore generalizes to multiple imaging modalities without retraining. We trained and tested UniMo to track motion in fetal magnetic resonance imaging, a challenging application due to 1) both large rigid and non-rigid motion, and 2) wide variations in image appearance. We then evaluated the trained model, without retraining, on MedMNIST, lung CT, and BraTS datasets. Results show that UniMo surpassed existing motion correction methods in accuracy, and notably enabled one-time training on a single modality while maintaining high stability and adaptability across unseen datasets. By offering a unified solution to motion correction, UniMo marks a significant advance in medical imaging, especially in applications with combined bulk and local motion. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/IntelligentImaging/UNIMO">https://github.com/IntelligentImaging/UNIMO</a> </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒé…å‡†ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä»·å€¼ï¼Œç„¶è€Œï¼Œå½“å‰çš„æŠ€æœ¯è¦ä¹ˆå—é™äºå®ƒä»¬èƒ½å¤Ÿå¤„ç†çš„ç±»å‹å’ŒèŒƒå›´çš„è¿åŠ¨ï¼Œè¦ä¹ˆéœ€è¦å¯¹æ–°çš„æˆåƒæ•°æ®è¿›è¡Œè¿­ä»£æ¨ç†å’Œï¼ˆæˆ–ï¼‰é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniMoï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ ¡æ­£åŒ»å­¦å½±åƒä¸­å„ç§è¿åŠ¨çš„ç»Ÿä¸€è¿åŠ¨æ ¡æ­£æ¡†æ¶ã€‚UniMoé‡‡ç”¨äº¤æ›¿ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¸ºç»Ÿä¸€çš„æŸå¤±å‡½æ•°è®­ç»ƒä¸€ä¸ªé›†æˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬1ï¼‰ç”¨äºå…¨å±€åˆšä½“è¿åŠ¨æ ¡æ­£çš„ç­‰ä»·ç¥ç»ç½‘ç»œå’Œ2ï¼‰ç”¨äºå±€éƒ¨å˜å½¢çš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œã€‚å®ƒåŒ…å«ä¸€ä¸ªå‡ ä½•å˜å½¢å¢å¼ºå™¨ï¼Œè¯¥å¢å¼ºå™¨1ï¼‰é€šè¿‡è§£å†³éåˆšä½“è¿åŠ¨æˆ–å‡ ä½•å¤±çœŸå¼•èµ·çš„å±€éƒ¨å˜å½¢ï¼Œå¢å¼ºäº†å…¨å±€æ ¡æ­£çš„ç¨³å¥æ€§ï¼Œ2ï¼‰ç”Ÿæˆå¢å¼ºæ•°æ®ä»¥æ”¹è¿›è®­ç»ƒã€‚UniMoæ˜¯ä¸€ä¸ªæ··åˆæ¨¡å‹ï¼Œå®ƒä½¿ç”¨å›¾åƒå¼ºåº¦å’Œå½¢çŠ¶æ¥å®ç°ç¨³å¥æ€§èƒ½ï¼Œå› æ­¤èƒ½å¤Ÿåœ¨å¤–è§‚å˜åŒ–ä¸­ä¿æŒé€šç”¨æ€§ï¼Œå¹¶æ¨å¹¿åº”ç”¨äºå¤šç§æˆåƒæ¨¡å¼è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬è®­ç»ƒå’Œæµ‹è¯•äº†UniMoä»¥è·Ÿè¸ªèƒå„¿ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åº”ç”¨ï¼Œå› ä¸ºæ—¢å­˜åœ¨å¤§é‡çš„åˆšæ€§å’Œéåˆšæ€§è¿åŠ¨ï¼Œè€Œä¸”å›¾åƒå¤–è§‚ä¹Ÿå­˜åœ¨å¾ˆå¤§çš„å˜åŒ–ã€‚ç„¶åæˆ‘ä»¬åœ¨MedMNISTã€è‚ºéƒ¨CTå’ŒBraTSæ•°æ®é›†ä¸Šè¯„ä¼°äº†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒï¼‰ã€‚ç»“æœè¡¨æ˜ï¼ŒUniMoåœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è¿åŠ¨æ ¡æ­£æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ä¸€æ¨¡æ€çš„ä¸€æ¬¡æ€§è®­ç»ƒä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨è·¨æœªè§æ•°æ®é›†æ—¶ä¿æŒäº†é«˜åº¦çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡ä¸ºè¿åŠ¨æ ¡æ­£æä¾›ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆï¼ŒUniMoåœ¨åŒ»å­¦å½±åƒä¸­å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æ•´ä½“å’Œå±€éƒ¨è¿åŠ¨çš„ç»„åˆåº”ç”¨ä¸­ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/IntelligentImaging/UNIMO">https://github.com/IntelligentImaging/UNIMO</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14204v3">PDF</a> 10 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒé…å‡†ä¸­å±•ç°å‡ºæ˜¾è‘—ä»·å€¼ï¼Œä½†å½“å‰æŠ€æœ¯å­˜åœ¨å¤„ç†ç±»å‹å’ŒèŒƒå›´å—é™çš„é—®é¢˜ï¼Œæˆ–æ˜¯éœ€è¦é’ˆå¯¹æ–°çš„æˆåƒæ•°æ®è¿›è¡Œè¿­ä»£æ¨æ–­å’Œå†è®­ç»ƒã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniMoâ€”â€”ä¸€ä¸ªç»Ÿä¸€è¿åŠ¨æ ¡æ­£æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ ¡æ­£åŒ»å­¦æˆåƒä¸­çš„å¤šæ ·è¿åŠ¨ã€‚UniMoé‡‡ç”¨äº¤æ›¿ä¼˜åŒ–æ–¹æ¡ˆæ¥ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æŸå¤±å‡½æ•°è®­ç»ƒä¸€ä¸ªé›†æˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬ç”¨äºå…¨å±€åˆšä½“è¿åŠ¨æ ¡æ­£çš„ç­‰ä»·ç¥ç»ç½‘ç»œå’Œç”¨äºå±€éƒ¨å˜å½¢çš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œã€‚å®ƒæ‹¥æœ‰ä¸€ä¸ªå‡ ä½•å˜å½¢å¢å¼ºå™¨ï¼Œæ—¨åœ¨é€šè¿‡è§£å†³éåˆšæ€§è¿åŠ¨æˆ–å‡ ä½•å¤±çœŸå¼•èµ·çš„å±€éƒ¨å˜å½¢é—®é¢˜ï¼Œå¢å¼ºå…¨å±€æ ¡æ­£çš„ç¨³å¥æ€§ï¼Œå¹¶ç”Ÿæˆæ‰©å……æ•°æ®ä»¥æ”¹è¿›è®­ç»ƒã€‚UniMoæ˜¯ä¸€ä¸ªæ··åˆæ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨å›¾åƒå¼ºåº¦å’Œå½¢çŠ¶æ¥å®ç°ç¨³å¥æ€§èƒ½ï¼Œå› æ­¤åœ¨å¤–è§‚å˜åŒ–ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°å¤šç§æˆåƒæ¨¡å¼è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬å¯¹èƒå„¿ç£å…±æŒ¯æˆåƒè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„åº”ç”¨è¿›è¡Œäº†è®­ç»ƒå’Œæµ‹è¯•ï¼Œè¯¥åº”ç”¨å­˜åœ¨å¤§é‡åˆšæ€§å’Œéåˆšæ€§è¿åŠ¨ä»¥åŠå¹¿æ³›çš„å›¾åƒå¤–è§‚å˜åŒ–ã€‚ç„¶åæˆ‘ä»¬åœ¨MedMNISTã€è‚ºéƒ¨CTå’ŒBraTSæ•°æ®é›†ä¸Šè¯„ä¼°äº†è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒUniMoåœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è¿åŠ¨æ ¡æ­£æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨å•ä¸€æ¨¡æ€ä¸Šä¸€æ¬¡è®­ç»ƒåï¼Œç»´æŒé«˜ç¨³å®šæ€§å’Œé€‚åº”æ€§å¤„ç†æœªè§æ•°æ®é›†ã€‚UniMoçš„ç»Ÿä¸€è¿åŠ¨æ ¡æ­£è§£å†³æ–¹æ¡ˆæ ‡å¿—ç€åŒ»å­¦æˆåƒé¢†åŸŸçš„é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨æ•´ä½“å’Œå±€éƒ¨è¿åŠ¨çš„åº”ç”¨ä¸­ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/IntelligentImaging/UNIMO">https://github.com/IntelligentImaging/UNIMO</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>UniMoæ˜¯ä¸€ä¸ªç»Ÿä¸€è¿åŠ¨æ ¡æ­£æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦æˆåƒä¸­çš„å¤šæ ·è¿åŠ¨æ ¡æ­£ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå…¨å±€å’Œå±€éƒ¨è¿åŠ¨æ ¡æ­£ã€‚</li>
<li>UniMoé‡‡ç”¨äº¤æ›¿ä¼˜åŒ–æ–¹æ¡ˆå’Œå‡ ä½•å˜å½¢å¢å¼ºå™¨æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>UniMoèƒ½å¤Ÿå¤„ç†å¤šç§æˆåƒæ¨¡å¼ï¼Œè€Œæ— éœ€é’ˆå¯¹æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èƒå„¿ç£å…±æŒ¯æˆåƒåº”ç”¨ä¸­ï¼ŒUniMoè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>UniMoåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„è¿åŠ¨æ ¡æ­£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02153e6c62f2693153abeed437b89032" align="middle">
<img src="https://picx.zhimg.com/v2-a59f17dbf43c9d18722f0f94b6e363c3" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Img2CAD-Reverse-Engineering-3D-CAD-Models-from-Images-through-VLM-Assisted-Conditional-Factorization"><a href="#Img2CAD-Reverse-Engineering-3D-CAD-Models-from-Images-through-VLM-Assisted-Conditional-Factorization" class="headerlink" title="Img2CAD: Reverse Engineering 3D CAD Models from Images through   VLM-Assisted Conditional Factorization"></a>Img2CAD: Reverse Engineering 3D CAD Models from Images through   VLM-Assisted Conditional Factorization</h2><p><strong>Authors:Yang You, Mikaela Angelina Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang, Yi Du, Hansheng Chen, Francis Engelmann, Suya You, Leonidas Guibas</strong></p>
<p>Reverse engineering 3D computer-aided design (CAD) models from images is an important task for many downstream applications including interactive editing, manufacturing, architecture, robotics, etc. The difficulty of the task lies in vast representational disparities between the CAD output and the image input. CAD models are precise, programmatic constructs that involve sequential operations combining discrete command structure with continuous attributes, making it challenging to learn and optimize in an end-to-end fashion. Concurrently, input images introduce inherent challenges such as photometric variability and sensor noise, complicating the reverse engineering process. In this work, we introduce a novel approach that conditionally factorizes the task into two sub-problems. First, we leverage vision-language foundation models (VLMs), a finetuned Llama3.2, to predict the global discrete base structure with semantic information. Second, we propose TrAssembler that, conditioned on the discrete structure with semantics, predicts the continuous attribute values. To support the training of our TrAssembler, we further constructed an annotated CAD dataset of common objects from ShapeNet. Putting all together, our approach and data demonstrate significant first steps towards CAD-ifying images in the wild. Code and data can be found in <a target="_blank" rel="noopener" href="https://github.com/qq456cvb/Img2CAD">https://github.com/qq456cvb/Img2CAD</a>. </p>
<blockquote>
<p>ä»å›¾åƒä¸­é€†å‘å·¥ç¨‹3Dè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹æ˜¯è®¸å¤šä¸‹æ¸¸åº”ç”¨çš„é‡è¦ä»»åŠ¡ï¼ŒåŒ…æ‹¬äº¤äº’å¼ç¼–è¾‘ã€åˆ¶é€ ã€å»ºç­‘ã€æœºå™¨äººç­‰ã€‚è¯¥ä»»åŠ¡çš„éš¾åº¦åœ¨äºCADè¾“å‡ºä¸å›¾åƒè¾“å…¥ä¹‹é—´å­˜åœ¨çš„å·¨å¤§ä»£è¡¨æ€§å·®å¼‚ã€‚CADæ¨¡å‹æ˜¯ç²¾ç¡®çš„ç¨‹åºæ„å»ºï¼Œæ¶‰åŠå°†ç¦»æ•£å‘½ä»¤ç»“æ„ä¸è¿ç»­å±æ€§ç›¸ç»“åˆè¿›è¡Œé¡ºåºæ“ä½œï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ å’Œä¼˜åŒ–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼Œè¾“å…¥å›¾åƒå¸¦æ¥äº†å›ºæœ‰çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å…‰åº¦å˜åŒ–å’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œä½¿é€†å‘å·¥ç¨‹è¿‡ç¨‹å¤æ‚åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ¡ä»¶åœ°å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œç»è¿‡å¾®è°ƒåçš„Llama3.2æ¥é¢„æµ‹å¸¦æœ‰è¯­ä¹‰ä¿¡æ¯çš„å…¨å±€ç¦»æ•£åŸºç¡€ç»“æ„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†TrAssemblerï¼Œå®ƒåœ¨ç»™å®šå¸¦æœ‰è¯­ä¹‰çš„ç¦»æ•£ç»“æ„æ¡ä»¶ä¸‹ï¼Œé¢„æµ‹è¿ç»­å±æ€§å€¼ã€‚ä¸ºäº†æ”¯æŒTrAssemblerçš„è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†æ¥è‡ªShapeNetçš„å¸¸è§å¯¹è±¡æ³¨é‡ŠCADæ•°æ®é›†ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å’Œæ•°æ®ä¸ºåœ¨é‡å¤–å®ç°å›¾åƒçš„CADåŒ–è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/Img2CAD%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qq456cvb/Img2CADä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01437v2">PDF</a> Accepted to SIGGRAPH Asia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»å›¾åƒä¸­é€†å‘å·¥ç¨‹3Dè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªå­é—®é¢˜ã€‚é¦–å…ˆï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰é¢„æµ‹å¸¦æœ‰è¯­ä¹‰ä¿¡æ¯çš„å…¨å±€ç¦»æ•£åŸºç¡€ç»“æ„ï¼›å…¶æ¬¡ï¼Œæå‡ºTrAssemblerï¼Œæ ¹æ®ç¦»æ•£ç»“æ„å’Œè¯­ä¹‰é¢„æµ‹è¿ç»­å±æ€§å€¼ã€‚ä¸ºæ”¯æŒTrAssemblerçš„è®­ç»ƒï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªæ¥è‡ªShapeNetçš„å¸¸è§ç‰©ä½“CADæ•°æ®é›†ã€‚æ­¤æ–¹æ³•å®ç°äº†ä»è‡ªç„¶å›¾åƒåˆ°CADæ¨¡å‹çš„è½¬åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä»å›¾åƒä¸­é€†å‘å·¥ç¨‹3D CADæ¨¡å‹çš„é‡è¦æ€§åŠå…¶åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>ä»»åŠ¡éš¾åº¦ä½“ç°åœ¨CADè¾“å‡ºä¸å›¾åƒè¾“å…¥ä¹‹é—´çš„å·¨å¤§ä»£è¡¨æ€§å·®å¼‚ã€‚</li>
<li>CADæ¨¡å‹æ¶‰åŠç¦»æ•£å‘½ä»¤ç»“æ„ä¸è¿ç»­å±æ€§çš„ç»„åˆï¼Œä½¿å¾—ç«¯åˆ°ç«¯çš„ä¼˜åŒ–å­¦ä¹ æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>å›¾åƒè¾“å…¥å­˜åœ¨å›ºæœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚å…‰åº¦å˜åŒ–å’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œå¢åŠ äº†é€†å‘å·¥ç¨‹çš„å¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼šé¢„æµ‹å…¨å±€ç¦»æ•£åŸºç¡€ç»“æ„å¹¶å¸¦æœ‰è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥åŠæ ¹æ®ç¦»æ•£ç»“æ„å’Œè¯­ä¹‰é¢„æµ‹è¿ç»­å±æ€§å€¼ã€‚</li>
<li>ä¸ºäº†æ”¯æŒTrAssemblerçš„è®­ç»ƒï¼Œæ„å»ºäº†ä¸€ä¸ªæ¥è‡ªShapeNetçš„å¸¸è§ç‰©ä½“CADæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ced71f893eaa9966287bd5764cc195b0" align="middle">
<img src="https://picx.zhimg.com/v2-f46a420f9e0de510be43e3aeb95d9352" align="middle">
<img src="https://picx.zhimg.com/v2-4a54366731628c710d8e17139ee5448b" align="middle">
<img src="https://picx.zhimg.com/v2-9ed17da6f7f7048b5ed9574e24d6977e" align="middle">
<img src="https://picx.zhimg.com/v2-df15abd101ad8102d41535db5e938bcf" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-35b3070b82351a29fedfee6d7197dc78" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  Dynamic Experts Search Enhancing Reasoning in Mixture-of-Experts LLMs   at Test Time
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-30/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f9bc1e8f4c4121cfb4d35e717c0e5808.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-30  RefAM Attention Magnets for Zero-Shot Referral Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
