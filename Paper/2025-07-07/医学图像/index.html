<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-07  MedDiff-FT Data-Efficient Diffusion Model Fine-tuning with Structural   Guidance for Controllable Medical Image Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-10e1d75aa748a0df493c4413d017c878.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-07-æ›´æ–°"><a href="#2025-07-07-æ›´æ–°" class="headerlink" title="2025-07-07 æ›´æ–°"></a>2025-07-07 æ›´æ–°</h1><h2 id="MedDiff-FT-Data-Efficient-Diffusion-Model-Fine-tuning-with-Structural-Guidance-for-Controllable-Medical-Image-Synthesis"><a href="#MedDiff-FT-Data-Efficient-Diffusion-Model-Fine-tuning-with-Structural-Guidance-for-Controllable-Medical-Image-Synthesis" class="headerlink" title="MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural   Guidance for Controllable Medical Image Synthesis"></a>MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural   Guidance for Controllable Medical Image Synthesis</h2><p><strong>Authors:Jianhao Xie, Ziang Zhang, Zhenyu Weng, Yuesheng Zhu, Guibo Luo</strong></p>
<p>Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data.While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FTâ€™s synthetic image-mask pairs improve SOTA methodâ€™s segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/JianhaoXie1/MedDiff-FT">https://github.com/JianhaoXie1/MedDiff-FT</a>. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„è¿›å±•å¸¸å¸¸å—åˆ°é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæä¾›äº†ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºå®ƒä»¬ä¾èµ–äºå¤§è§„æ¨¡åŒ»å­¦æ•°æ®é›†å’Œå¯¹æ›´é«˜å›¾åƒè´¨é‡çš„éœ€æ±‚ï¼Œå®ƒä»¬åœ¨åŒ»å­¦æˆåƒä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedDiff-FTï¼Œè¿™æ˜¯ä¸€ç§å¯æ§çš„åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¾®è°ƒæ‰©æ•£åŸºç¡€æ¨¡å‹ä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼äº§ç”Ÿå…·æœ‰ç»“æ„ä¾èµ–æ€§å’Œé¢†åŸŸç‰¹å¼‚æ€§çš„åŒ»å­¦å›¾åƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€è‡ªé€‚åº”å¼•å¯¼æ©è†œæ–½åŠ ç©ºé—´çº¦æŸä»¥ç¡®ä¿è§£å‰–ç»“æ„è¿è´¯çš„åˆæˆï¼Œè€Œè½»é‡çº§éšæœºæ©è†œç”Ÿæˆå™¨é€šè¿‡åˆ†å±‚éšæœºæ³¨å…¥å¢å¼ºå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨è´¨é‡è¯„ä¼°åè®®ä½¿ç”¨ç‰¹å¾ç©ºé—´åº¦é‡è¿‡æ»¤æ¬¡ä¼˜è¾“å‡ºï¼Œéšåé€šè¿‡æ©è†œè…èš€æ¥æé«˜ä¿çœŸåº¦ã€‚åœ¨äº”ä¸ªåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒMedDiff-FTçš„åˆæˆå›¾åƒ-æ©è†œå¯¹æé«˜äº†æœ€å…ˆè¿›æ–¹æ³•çš„åˆ†å‰²æ€§èƒ½ï¼ŒDiceå¾—åˆ†å¹³å‡æé«˜1%ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°å¹³è¡¡äº†ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºåŒ»å­¦æ•°æ®å¢å¼ºæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianhaoXie1/MedDiff-FT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JianhaoXie1/MedDiff-FTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00377v1">PDF</a> 11 pages,3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæ·±åº¦å­¦ä¹ ç®—æ³•è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯æ§çš„åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•MedDiff-FTã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒæ‰©æ•£åŸºç¡€æ¨¡å‹ï¼Œä»¥æ•°æ®é«˜æ•ˆçš„æ–¹å¼ç”Ÿæˆå…·æœ‰ç»“æ„ä¾èµ–æ€§å’Œé¢†åŸŸç‰¹å¼‚æ€§çš„åŒ»å­¦å›¾åƒã€‚é€šè¿‡åŠ¨æ€è‡ªé€‚åº”å¼•å¯¼æ©è†œå’Œè½»é‡çº§éšæœºæ©è†œç”Ÿæˆå™¨ï¼Œç¡®ä¿åˆæˆå›¾åƒçš„è§£å‰–ç»“æ„ä¸€è‡´æ€§å¹¶å¢å¼ºå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°åè®®è¿‡æ»¤ä¸ä½³è¾“å‡ºï¼Œå¹¶é€šè¿‡æ©è†œè…èš€æé«˜ä¿çœŸåº¦ã€‚åœ¨äº”ä¸ªåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMedDiff-FTç”Ÿæˆçš„åˆæˆå›¾åƒ-æ©è†œå¯¹æé«˜äº†æœ€ä½³æ–¹æ³•çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†æé«˜1%ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ï¼Œä¸ºåŒ»å­¦æ•°æ®å¢å¼ºæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedDiff-FTæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®ä¸è¶³é—®é¢˜çš„å¯æ§åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¾®è°ƒæ‰©æ•£åŸºç¡€æ¨¡å‹ï¼ŒMedDiff-FTèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç»“æ„ä¾èµ–æ€§å’Œé¢†åŸŸç‰¹å¼‚æ€§çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>åŠ¨æ€è‡ªé€‚åº”å¼•å¯¼æ©è†œå’Œè½»é‡çº§éšæœºæ©è†œç”Ÿæˆå™¨ç”¨äºç¡®ä¿åˆæˆå›¾åƒçš„è§£å‰–ç»“æ„ä¸€è‡´æ€§å¹¶å¢å¼ºå¤šæ ·æ€§ã€‚</li>
<li>è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°åè®®ç”¨äºè¿‡æ»¤ä¸ä½³çš„è¾“å‡ºå›¾åƒï¼Œå¹¶é€šè¿‡æ©è†œè…èš€æé«˜å›¾åƒçš„ä¿çœŸåº¦ã€‚</li>
<li>åœ¨äº”ä¸ªåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMedDiff-FTæé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†æé«˜1%ã€‚</li>
<li>MedDiff-FTåœ¨ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea3627047543544245e9e86e67658c43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15874363c090b480350e8231d0f5f150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fb44d8f2857a2a4a35feec444b97fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f1794d9e1249520cefe8fe4b66bc8c5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Î¼-2-Tokenizer-Differentiable-Multi-Scale-Multi-Modal-Tokenizer-for-Radiology-Report-Generation"><a href="#Î¼-2-Tokenizer-Differentiable-Multi-Scale-Multi-Modal-Tokenizer-for-Radiology-Report-Generation" class="headerlink" title="$Î¼^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for   Radiology Report Generation"></a>$Î¼^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for   Radiology Report Generation</h2><p><strong>Authors:Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang</strong></p>
<p>Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale $\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited data for RRG tasks. At the same time, for prompt engineering, we introduce a five-stage, LLM-driven pipeline that converts routine CT reports into paired visual-question-answer triples and citation-linked reasoning narratives, creating a scalable, high-quality supervisory corpus for explainable multimodal radiology LLM. All code, datasets, and models will be publicly available in our official repository. <a target="_blank" rel="noopener" href="https://github.com/Siyou-Li/u2Tokenizer">https://github.com/Siyou-Li/u2Tokenizer</a> </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨ä»ä¸´åºŠå½±åƒï¼ˆå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ï¼‰ä¸­äº§ç”Ÿè¯¦ç»†çš„æ–‡æœ¬æŠ¥å‘Šï¼Œä»¥æé«˜è¯Šæ–­å’Œæä¾›ç®¡ç†å»ºè®®çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚RRGé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯åœ¨èµ„æºçº¦æŸä¸‹ä»æˆåƒæ•°æ®ä¸­æå–ç›¸å…³ä¿¡æ¯æ‰€å›ºæœ‰çš„å¤æ‚æ€§ï¼›äºŒæ˜¯å®¢è§‚è¯„ä¼°æ¨¡å‹ç”ŸæˆæŠ¥å‘Šä¸ä¸“å®¶ç¼–å†™æŠ¥å‘Šä¹‹é—´çš„å·®å¼‚çš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Î¼^2LLMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºRRGä»»åŠ¡çš„å¤šå°ºåº¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ–°å‹Î¼^2åˆ†è¯å™¨ä½œä¸ºä¸­é—´å±‚ï¼Œé›†æˆäº†å¤šå°ºåº¦è§†è§‰åˆ†è¯å™¨å’Œæ–‡æœ¬åˆ†è¯å™¨çš„å¤šæ¨¡æ€ç‰¹å¾ï¼Œç„¶åé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æé«˜æŠ¥å‘Šç”Ÿæˆè´¨é‡ï¼Œç”±GREEN-RedLlamaæŒ‡å¯¼ã€‚åœ¨å››ä¸ªå¤§å‹CTå›¾åƒæŠ¥å‘ŠåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†æˆ‘ä»¬åœ¨æœ‰é™æ•°æ®ä¸Šå¾®è°ƒÎ¼^2LLMçš„æ½œåŠ›å¯¹äºRRGä»»åŠ¡ã€‚åŒæ—¶ï¼Œå¯¹äºæç¤ºå·¥ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†äº”é˜¶æ®µLLMé©±åŠ¨æµç¨‹ï¼Œå°†å¸¸è§„CTæŠ¥å‘Šè½¬æ¢ä¸ºæˆå¯¹çš„è§†è§‰é—®ç­”å¯¹å’Œå¼•æ–‡å…³è”æ¨ç†å™è¿°ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€é«˜è´¨é‡çš„è§£é‡Šæ€§å¤šæ¨¡æ€æ”¾å°„å­¦LLMç›‘ç£è¯­æ–™åº“ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹éƒ½å°†åœ¨æˆ‘ä»¬çš„å®˜æ–¹ä»“åº“ä¸­å…¬å¼€å¯ç”¨ã€‚[<a target="_blank" rel="noopener" href="https://github.com/Siyou-Li/u2Tokenizer]">https://github.com/Siyou-Li/u2Tokenizer]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00316v2">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–ç”ŸæˆæŠ¥å‘Šçš„ç›®æ ‡ã€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠÎ¼Â²LLMæ¨¡å‹çš„ä¼˜åŠ¿å’Œç‰¹ç‚¹ã€‚æå‡ºä¸€ç§æ–°å‹çš„å¤šå°ºåº¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Î¼Â²LLMç”¨äºè§£å†³æŒ‘æˆ˜ï¼Œå®ç°äº†é«˜è´¨é‡çš„æŠ¥å‘Šç”Ÿæˆï¼Œå¹¶å±•ç¤ºäº†åœ¨å››ä¸ªå¤§å‹CTå›¾åƒæŠ¥å‘Šæ•°æ®é›†ä¸Šçš„å‡ºè‰²æ€§èƒ½ã€‚åŒæ—¶ä»‹ç»äº†å…¶åº”ç”¨çš„æç¤ºå·¥ç¨‹ç®¡é“å’Œåœ¨çº¿èµ„æºçš„å…±äº«è®¡åˆ’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç”ŸæˆåŒ»å­¦å›¾åƒæŠ¥å‘Šå¯æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>Î¼Â²LLMæ¨¡å‹è§£å†³äº†ä»åŒ»å­¦å›¾åƒæ•°æ®ä¸­æå–ç›¸å…³ä¿¡æ¯çš„å¤æ‚æ€§å’Œæ¨¡å‹ç”ŸæˆæŠ¥å‘Šä¸ä¸“å®¶ä¹¦å†™æŠ¥å‘Šä¹‹é—´çš„å·®å¼‚è¯„ä¼°éš¾é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Î¼Â²LLMç”¨äºæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>Î¼Â²Tokenizerä½œä¸ºä¸­é—´å±‚ï¼Œé›†æˆäº†å¤šæ¨¡æ€ç‰¹å¾ï¼Œæé«˜äº†æŠ¥å‘Šç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒÎ¼Â²LLMåœ¨å››ä¸ªå¤§å‹CTå›¾åƒæŠ¥å‘Šæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§äº”é˜¶æ®µçš„æç¤ºå·¥ç¨‹ç®¡é“ï¼Œç”¨äºå°†å¸¸è§„CTæŠ¥å‘Šè½¬æ¢ä¸ºè§†è§‰é—®ç­”å¯¹å’Œå¼•æ–‡é“¾æ¥æ¨ç†å™è¿°ï¼Œä¸ºå¯è§£é‡Šçš„å¤šæ¨¡æ€æ”¾å°„å­¦å¤§å‹è¯­è¨€æ¨¡å‹åˆ›å»ºé«˜è´¨é‡ç›‘ç£è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3899df383507908585173368082b22a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78afe9150b962cd682af288d4048e2f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd848b09e676cec6218e3b9796ab6e48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8078944a8b0c9913296647a635ecb7d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6a2d3cd1de53dde0a7c4ca2aec9906.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PPFL-RDSN-Privacy-Preserving-Federated-Learning-based-Residual-Dense-Spatial-Networks-for-Encrypted-Lossy-Image-Reconstruction"><a href="#PPFL-RDSN-Privacy-Preserving-Federated-Learning-based-Residual-Dense-Spatial-Networks-for-Encrypted-Lossy-Image-Reconstruction" class="headerlink" title="PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense   Spatial Networks for Encrypted Lossy Image Reconstruction"></a>PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense   Spatial Networks for Encrypted Lossy Image Reconstruction</h2><p><strong>Authors:Peilin He, James Joshi</strong></p>
<p>Reconstructing high-quality images from low-resolution inputs using Residual Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in collaborative scenarios where centralized training poses significant privacy risks, including data leakage and inference attacks, as well as high computational costs. We propose a novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image reconstruction. PPFL-RDSN integrates Federated Learning (FL), local differential privacy, and robust model watermarking techniques, ensuring data remains secure on local devices, safeguarding sensitive information, and maintaining model authenticity without revealing underlying data. Empirical evaluations show that PPFL-RDSN achieves comparable performance to the state-of-the-art centralized methods while reducing computational burdens, and effectively mitigates security and privacy vulnerabilities, making it a practical solution for secure and privacy-preserving collaborative computer vision applications. </p>
<blockquote>
<p>åˆ©ç”¨æ®‹å·®å¯†é›†ç©ºé—´ç½‘ç»œï¼ˆRDSNsï¼‰ä»ä½åˆ†è¾¨ç‡è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒæ˜¯éå¸¸å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨åä½œåœºæ™¯ä¸­ï¼Œé›†ä¸­è®­ç»ƒå¸¦æ¥äº†å·¨å¤§çš„éšç§é£é™©ï¼ŒåŒ…æ‹¬æ•°æ®æ³„éœ²å’Œæ¨ç†æ”»å‡»ï¼Œä»¥åŠé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºéšç§ä¿æŠ¤çš„è”é‚¦å­¦ä¹ RDSNï¼ˆPPFL-RDSNï¼‰æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæœ‰æŸå›¾åƒé‡å»ºã€‚PPFL-RDSNèåˆäº†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ã€æœ¬åœ°å·®åˆ†éšç§å’Œç¨³å¥çš„æ¨¡å‹æ°´å°æŠ€æœ¯ï¼Œç¡®ä¿æ•°æ®åœ¨æœ¬åœ°è®¾å¤‡ä¸Šä¿æŒå®‰å…¨ï¼Œä¿æŠ¤æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶åœ¨ä¸æš´éœ²åº•å±‚æ•°æ®çš„æƒ…å†µä¸‹ä¿æŒæ¨¡å‹çš„çœŸå®æ€§ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPPFL-RDSNåœ¨è¾¾åˆ°æœ€æ–°é›†ä¸­æ–¹æ³•ç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œå‡è½»äº†è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†å®‰å…¨å’Œéšç§æ¼æ´ï¼Œä½¿å…¶æˆä¸ºå®‰å…¨å’Œéšç§ä¿æŠ¤åä½œè®¡ç®—æœºè§†è§‰åº”ç”¨çš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00230v1">PDF</a> This paper is under review; do not distribute</p>
<p><strong>Summary</strong></p>
<p>åŸºäºResidual Dense Spatial Networksï¼ˆRDSNsï¼‰ä»ä½åˆ†è¾¨ç‡è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒè‡³å…³é‡è¦ä¸”å……æ»¡æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆä½œåœºæ™¯ä¸­ï¼Œé›†ä¸­å¼è®­ç»ƒå­˜åœ¨æ•°æ®æ³„éœ²ã€æ¨ç†æ”»å‡»ç­‰éšç§é£é™©å’Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ é©±åŠ¨çš„RDSNï¼ˆPPFL-RDSNï¼‰æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹æœ‰æŸå›¾åƒé‡å»ºè®¾è®¡ã€‚PPFL-RDSNç»“åˆäº†è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ã€æœ¬åœ°å·®åˆ†éšç§å’Œç¨³å¥çš„æ¨¡å‹æ°´å°æŠ€æœ¯ï¼Œç¡®ä¿æ•°æ®åœ¨æœ¬åœ°è®¾å¤‡ä¸Šä¿æŒå®‰å…¨ï¼Œä¿æŠ¤æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶åœ¨ä¸æš´éœ²åº•å±‚æ•°æ®çš„æƒ…å†µä¸‹ä¿æŒæ¨¡å‹çœŸå®æ€§ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒPPFL-RDSNåœ¨å®ç°ä¸æœ€æ–°é›†ä¸­æ–¹æ³•ç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œå‡è½»äº†è®¡ç®—è´Ÿæ‹…ï¼Œæœ‰æ•ˆç¼“è§£äº†å®‰å…¨å’Œéšç§æ¼æ´ï¼Œä½¿å…¶æˆä¸ºå®‰å…¨å’Œéšç§ä¿æŠ¤ååŒè®¡ç®—æœºè§†è§‰åº”ç”¨çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PPFL-RDSNæ¡†æ¶ç»“åˆäº†Residual Dense Spatial Networksï¼ˆRDSNsï¼‰è¿›è¡Œä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºã€‚</li>
<li>åœ¨åˆä½œåœºæ™¯ä¸­ï¼Œé›†ä¸­å¼è®­ç»ƒå­˜åœ¨æ•°æ®æ³„éœ²å’Œæ¨ç†æ”»å‡»ç­‰éšç§é£é™©ã€‚</li>
<li>PPFL-RDSNé‡‡ç”¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä»¥å¤„ç†è¿™äº›éšç§é—®é¢˜ã€‚</li>
<li>æœ¬åœ°å·®åˆ†éšç§å’Œæ¨¡å‹æ°´å°æŠ€æœ¯è¢«é›†æˆåˆ°PPFL-RDSNä¸­ï¼Œä»¥å¢å¼ºæ•°æ®å®‰å…¨æ€§ã€‚</li>
<li>PPFL-RDSNå®ç°äº†ä¸æœ€æ–°é›†ä¸­æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>PPFL-RDSNèƒ½å‡è½»è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶æœ‰æ•ˆç¼“è§£å®‰å…¨å’Œéšç§æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2dced0b664e13aa9ab08d88068016deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5512f720a1f813e70ce76bd4a85d9e4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aca8623786552bbe61d2aa87d5fba031.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multimodal-Multi-Disease-Medical-Imaging-Foundation-Model-MerMED-FM"><a href="#Multimodal-Multi-Disease-Medical-Imaging-Foundation-Model-MerMED-FM" class="headerlink" title="Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)"></a>Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)</h2><p><strong>Authors:Yang Zhou, Chrystie Wan Ning Quek, Jun Zhou, Yan Wang, Yang Bai, Yuhe Ke, Jie Yao, Laura Gutierrez, Zhen Ling Teo, Darren Shu Jeng Ting, Brian T. Soetikno, Christopher S. Nielsen, Tobias Elze, Zengxiang Li, Linh Le Dinh, Lionel Tim-Ee Cheng, Tran Nguyen Tuan Anh, Chee Leong Cheng, Tien Yin Wong, Nan Liu, Iain Beehuat Tan, Tony Kiat Hon Lim, Rick Siow Mong Goh, Yong Liu, Daniel Shu Wei Ting</strong></p>
<p>Current artificial intelligence models for medical imaging are predominantly single modality and single disease. Attempts to create multimodal and multi-disease models have resulted in inconsistent clinical accuracy. Furthermore, training these models typically requires large, labour-intensive, well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal, multi-specialty foundation model trained using self-supervised learning and a memory module. MerMED-FM was trained on 3.3 million medical images from over ten specialties and seven modalities, including computed tomography (CT), chest X-rays (CXR), ultrasound (US), pathology patches, color fundus photography (CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was evaluated across multiple diseases and compared against existing foundational models. Strong performance was achieved across all modalities, with AUROCs of 0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894 (CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable, versatile, cross-specialty foundation model that enables robust medical imaging interpretation across diverse medical disciplines. </p>
<blockquote>
<p>å½“å‰ç”¨äºåŒ»å­¦æˆåƒçš„äººå·¥æ™ºèƒ½æ¨¡å‹ä¸»è¦æ˜¯å•æ¨¡æ€å’Œå•ç—…ç§ã€‚å°è¯•åˆ›å»ºå¤šæ¨¡æ€å’Œå¤šç—…ç§æ¨¡å‹çš„ç»“æœåœ¨ä¸´åºŠå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¸ä¸€ã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡ã€åŠ³åŠ¨å¯†é›†ã€æ ‡æ³¨è‰¯å¥½çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼€å‘äº†MerMED-FMï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å…ˆè¿›çš„ã€å¤šæ¨¡æ€ã€å¤šä¸“ä¸šçš„åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ å’Œå†…å­˜æ¨¡å—è¿›è¡Œè®­ç»ƒã€‚MerMED-FMåœ¨è¶…è¿‡åä¸ªä¸“ä¸šå’Œä¸ƒç§æ¨¡æ€çš„330ä¸‡åŒ»å­¦å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰ã€è¶…å£°æ³¢ï¼ˆUSï¼‰ã€ç—…ç†åˆ‡ç‰‡ã€å½©è‰²çœ¼åº•æ‘„å½±ï¼ˆCFPï¼‰ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å’Œçš®è‚¤ç§‘å›¾åƒã€‚MerMED-FMåœ¨å¤šç§ç–¾ç—…ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨æ‰€æœ‰æ¨¡æ€ä¸‹å‡å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå…¶ä¸­OCTçš„AUROCä¸º0.988ï¼›ç—…ç†å­¦çš„AUROCä¸º0.982ï¼›è¶…å£°æ³¢çš„AUROCä¸º0.951ï¼›CTçš„AUROCä¸º0.943ï¼›çš®è‚¤ç—…çš„AUROCä¸º0.931ï¼›CFPçš„AUROCä¸º0.894ï¼›CXRçš„AUROCä¸º0.858ã€‚MerMED-FMå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯æˆä¸ºä¸€ä¸ªé«˜åº¦é€‚åº”ã€é€šç”¨ã€è·¨ä¸“ä¸šçš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„åŒ»å­¦é¢†åŸŸå®ç°ç¨³å¥çš„åŒ»å­¦æˆåƒè§£è¯»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00185v1">PDF</a> 42 pages, 3 composite figures, 4 tables</p>
<p><strong>Summary</strong></p>
<pre><code> æœ€æ–°çš„åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½æ¨¡å‹MerMED-FMï¼Œé›†å¤šæ¨¡æ€ã€å¤šä¸“ä¸šäºä¸€èº«ï¼Œé‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ä¸è®°å¿†æ¨¡å—è®­ç»ƒã€‚æ¨¡å‹åœ¨å¤šç§ç–¾ç—…ä¸å¤šç§å½±åƒæ–¹å¼ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæœ‰æœ›æˆä¸ºä¸€ä¸ªé«˜åº¦é€‚åº”ã€é€šç”¨æ€§å¼ºã€è·¨ä¸“ä¸šçš„åŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½æ¨¡å‹ä¸»è¦ä¸ºå•æ¨¡æ€å’Œå•ç–¾ç—…æ¨¡å‹ï¼Œåˆ›å»ºå¤šæ¨¡æ€å’Œå¤šç–¾ç—…æ¨¡å‹çš„å°è¯•å¯¼è‡´ä¸´åºŠå‡†ç¡®æ€§ä¸ä¸€è‡´ã€‚</li>
<li>MerMED-FMæ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤šæ¨¡æ€ã€å¤šä¸“ä¸šåŸºç¡€æ¨¡å‹ï¼Œç»è¿‡è‡ªç›‘ç£å­¦ä¹ å’Œè®°å¿†æ¨¡å—çš„è®­ç»ƒã€‚</li>
<li>MerMED-FMåœ¨è¶…è¿‡åä¸ªä¸“ä¸šå’Œä¸ƒç§æ¨¡æ€çš„330ä¸‡åŒ»å­¦å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰ã€è¶…å£°ï¼ˆUSï¼‰ã€ç—…ç†è´´ç‰‡ã€å½©è‰²çœ¼åº•æ‘„å½±ï¼ˆCFPï¼‰ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å’Œçš®è‚¤ç§‘å›¾åƒã€‚</li>
<li>MerMED-FMåœ¨å¤šç§ç–¾ç—…ä¸Šçš„è¡¨ç°ç»è¿‡è¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰åŸºç¡€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨æ‰€æœ‰æ¨¡æ€ä¸­éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>MerMED-FMçš„AUROCå€¼åœ¨ä¸åŒæ¨¡æ€ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¦‚åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ä¸Šä¸º0.988ï¼Œç—…ç†å­¦ä¸Šä¸º0.982ç­‰ã€‚</li>
<li>MerMED-FMå…·æœ‰æˆä¸ºé«˜åº¦é€‚åº”ã€é€šç”¨æ€§å¼ºã€è·¨ä¸“ä¸šçš„åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°å¯¹ä¸åŒåŒ»å­¦é¢†åŸŸçš„ç¨³å¥åŒ»å­¦æˆåƒè§£è¯»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d39307d4e542d456ab11babc865a0728.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mono-Modalizing-Extremely-Heterogeneous-Multi-Modal-Medical-Image-Registration"><a href="#Mono-Modalizing-Extremely-Heterogeneous-Multi-Modal-Medical-Image-Registration" class="headerlink" title="Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration"></a>Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image   Registration</h2><p><strong>Authors:Kyobin Choo, Hyunkyung Han, Jinyeong Kim, Chanyong Yoon, Seong Jae Hwang</strong></p>
<p>In clinical practice, imaging modalities with functional characteristics, such as positron emission tomography (PET) and fractional anisotropy (FA), are often aligned with a structural reference (e.g., MRI, CT) for accurate interpretation or group analysis, necessitating multi-modal deformable image registration (DIR). However, due to the extreme heterogeneity of these modalities compared to standard structural scans, conventional unsupervised DIR methods struggle to learn reliable spatial mappings and often distort images. We find that the similarity metrics guiding these models fail to capture alignment between highly disparate modalities. To address this, we propose M2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal DIR models using only mono-modal similarity while preserving the established architectural paradigm for seamless integration into existing models. We also introduce GradCyCon, a regularizer that leverages M2M-Regâ€™s cyclic training scheme to promote diffeomorphism. Furthermore, our framework naturally extends to a semi-supervised setting, integrating pre-aligned and unaligned pairs only, without requiring ground-truth transformations or segmentation masks. Experiments on the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) dataset demonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for PET-MRI and FA-MRI registration, highlighting its effectiveness in handling highly heterogeneous multi-modal DIR. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MICV-yonsei/M2M-Reg">https://github.com/MICV-yonsei/M2M-Reg</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå…·æœ‰åŠŸèƒ½ç‰¹å¾çš„æˆåƒæ–¹å¼ï¼Œå¦‚æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å’Œåˆ†æ•°å¼‚æ€§ï¼ˆFAï¼‰ï¼Œé€šå¸¸ä¸ç»“æ„å‚è€ƒï¼ˆä¾‹å¦‚MRIã€CTï¼‰å¯¹å‡†ï¼Œä»¥è¿›è¡Œå‡†ç¡®çš„è§£é‡Šæˆ–ç¾¤ç»„åˆ†æï¼Œè¿™éœ€è¦å¤šæ¨¡å¼å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æˆåƒæ–¹å¼ä¸æ ‡å‡†ç»“æ„æ‰«æç›¸æ¯”å…·æœ‰æå¤§çš„å¼‚è´¨æ€§ï¼Œä¼ ç»Ÿæ— ç›‘ç£çš„DIRæ–¹æ³•åœ¨å­¦ä¹ å¯é çš„ç©ºé—´æ˜ å°„æ—¶é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”ç»å¸¸å¯¼è‡´å›¾åƒå¤±çœŸã€‚æˆ‘ä»¬å‘ç°ï¼Œå¼•å¯¼è¿™äº›æ¨¡å‹çš„ç›¸ä¼¼åº¦åº¦é‡æ— æ³•æ•è·é«˜åº¦ä¸åŒæ¨¡å¼ä¹‹é—´çš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2M-Regï¼ˆå¤šå¯¹å•æ³¨å†Œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ä»…å•æ¨¡æ€ç›¸ä¼¼åº¦è®­ç»ƒå¤šæ¨¡å¼DIRæ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼ŒåŒæ—¶ä¿ç•™äº†æ— ç¼é›†æˆç°æœ‰æ¨¡å‹çš„æ—¢å®šæ¶æ„èŒƒå¼ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†GradCyConï¼Œä¸€ç§åˆ©ç”¨M2M-Regçš„å¾ªç¯è®­ç»ƒæ–¹æ¡ˆæ¥ä¿ƒè¿›å¾®åˆ†åŒèƒšçš„æ­£åˆ™åŒ–å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è‡ªç„¶åœ°æ‰©å±•åˆ°åŠç›‘ç£è®¾ç½®ï¼Œä»…æ•´åˆé¢„å…ˆå¯¹é½å’Œæœªå¯¹é½çš„å¯¹ï¼Œæ— éœ€åœ°é¢çœŸå®å˜æ¢æˆ–åˆ†å‰²æ©è†œã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®ï¼ˆADNIï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM2M-Regåœ¨PET-MRIå’ŒFA-MRIé…å‡†æ–¹é¢çš„DSCå¾—åˆ†é«˜äºå…ˆå‰æ–¹æ³•é«˜è¾¾2å€ï¼Œçªæ˜¾å…¶åœ¨å¤„ç†é«˜åº¦å¼‚è´¨å¤šæ¨¡å¼DIRæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MICV-yonsei/M2M-Reg">https://github.com/MICV-yonsei/M2M-Reg</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15596v2">PDF</a> 11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing   and Computer Assisted Intervention (MICCAI) 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­åŠŸèƒ½ç‰¹æ€§æˆåƒæ¨¡å¼ï¼ˆå¦‚æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å’Œåˆ†æ•°å„å‘å¼‚æ€§ï¼ˆFAï¼‰ï¼‰ä¸ç»“æ„å‚è€ƒï¼ˆå¦‚MRIã€CTï¼‰çš„é…å‡†é—®é¢˜ï¼ŒM2M-Regæ¡†æ¶åˆ©ç”¨å•æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œå¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†ï¼Œæé«˜äº†é«˜åº¦ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½èƒ½åŠ›ï¼Œé™ä½å›¾åƒå¤±çœŸã€‚å¼•å…¥GradCyConæ­£åˆ™åŒ–å™¨ä¿ƒè¿›å¾®åˆ†åŒèƒšã€‚è¯¥æ¡†æ¶å¯æ‰©å±•åˆ°åŠç›‘ç£è®¾ç½®ï¼Œä»…é›†æˆé¢„å¯¹é½å’Œéå¯¹é½å¯¹ï¼Œæ— éœ€åœ°é¢çœŸå®å˜æ¢æˆ–åˆ†å‰²æ©è†œã€‚åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒM2M-Regçš„DSCå¾—åˆ†è¾ƒä»¥å‰çš„æ–¹æ³•é«˜å‡ºä¸¤å€ï¼Œè¯æ˜å…¶åœ¨å¤„ç†é«˜åº¦å¼‚è´¨çš„å¤šæ¨¡æ€å¯å˜å½¢å›¾åƒé…å‡†ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒçš„åŠŸèƒ½ç‰¹æ€§æˆåƒæ¨¡å¼ï¼ˆPETã€FAç­‰ï¼‰ä¸ç»“æ„å‚è€ƒå›¾åƒï¼ˆMRIã€CTï¼‰çš„é…å‡†åœ¨ä¸´åºŠå®è·µä¸­éå¸¸é‡è¦ã€‚</li>
<li>ç”±äºæˆåƒæ¨¡æ€çš„æç«¯å¼‚è´¨æ€§ï¼Œå¸¸è§„çš„æ— ç›‘ç£å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰æ–¹æ³•éš¾ä»¥å­¦ä¹ å¯é çš„ç©ºé—´æ˜ å°„ï¼Œå¹¶å¯èƒ½å¯¼è‡´å›¾åƒå¤±çœŸã€‚</li>
<li>M2M-Regæ¡†æ¶é€šè¿‡åˆ©ç”¨å•æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œå¤šæ¨¡æ€DIRè®­ç»ƒï¼Œè§£å†³äº†é«˜åº¦ä¸åŒæ¨¡æ€ä¹‹é—´çš„é…å‡†é—®é¢˜ã€‚</li>
<li>M2M-Regæ¡†æ¶å¼•å…¥çš„GradCyConæ­£åˆ™åŒ–å™¨æœ‰åŠ©äºä¿ƒè¿›å¾®åˆ†åŒèƒšã€‚</li>
<li>è¯¥æ¡†æ¶å¯è‡ªç„¶åœ°æ‰©å±•åˆ°åŠç›‘ç£è®¾ç½®ï¼Œèƒ½å¤Ÿé›†æˆé¢„å¯¹é½å’Œéå¯¹é½çš„å›¾åƒå¯¹ï¼Œæ— éœ€é¢å¤–çš„åœ°é¢çœŸå®å˜æ¢æˆ–åˆ†å‰²æ©è†œã€‚</li>
<li>åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM2M-Regåœ¨PET-MRIå’ŒFA-MRIé…å‡†æ–¹é¢çš„è¡¨ç°ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œè¾¾åˆ°é«˜è¾¾ä¸¤å€çš„DSCå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-520d4662ed5d00f5d44512832f98826a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8296e9f799675ac04f7d4c81a878fc8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10e1d75aa748a0df493c4413d017c878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0b34f5bc1c3054a0e52fbd63a48dfd2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unleashing-Diffusion-and-State-Space-Models-for-Medical-Image-Segmentation"><a href="#Unleashing-Diffusion-and-State-Space-Models-for-Medical-Image-Segmentation" class="headerlink" title="Unleashing Diffusion and State Space Models for Medical Image   Segmentation"></a>Unleashing Diffusion and State Space Models for Medical Image   Segmentation</h2><p><strong>Authors:Rong Wu, Ziqi Chen, Liming Zhong, Heng Li, Hai Shu</strong></p>
<p>Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the modelâ€™s robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Rows21/k-Means_Mask_Mamba">https://github.com/Rows21/k-Means_Mask_Mamba</a>. </p>
<blockquote>
<p>ç°æœ‰ä»…åœ¨å•ä¸€åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ï¼Œåœ¨é‡åˆ°æœªè§è¿‡çš„å™¨å®˜æˆ–è‚¿ç˜¤æ—¶ï¼Œå…¶ç¨³å¥æ€§å¸¸å¸¸ä¸è¶³ã€‚å¼€å‘ä¸€ç§èƒ½å¤Ÿè¯†åˆ«è®­ç»ƒæœŸé—´æœªå‡ºç°çš„ç½•è§æˆ–æ–°å‹è‚¿ç˜¤ç±»åˆ«çš„ç¨³å¥æ¨¡å‹ï¼Œå¯¹äºæ¨åŠ¨åŒ»å­¦æˆåƒåº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†DSMï¼Œä¸€ä¸ªåˆ©ç”¨æ‰©æ•£çŠ¶æ€å’Œç©ºé—´æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ä»¥å¤–çš„æœªè§è‚¿ç˜¤ç±»åˆ«è¿›è¡Œåˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚DSMåˆ©ç”¨ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢ï¼Œåœ¨ä¿®æ”¹è¿‡çš„æ³¨æ„åŠ›è§£ç å™¨å†…è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜åˆ†ç±»å‡†ç¡®åº¦ã€‚èµ·åˆï¼Œæ¨¡å‹é‡‡ç”¨å¯¹è±¡æ„ŸçŸ¥ç‰¹å¾åˆ†ç»„ç­–ç•¥å­¦ä¹ å™¨å®˜æŸ¥è¯¢ï¼Œä»¥æ•è·å™¨å®˜å±‚é¢çš„è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒé€šè¿‡ä¸“æ³¨äºåŸºäºæ‰©æ•£çš„è§†è§‰æç¤ºæ¥ä¼˜åŒ–è‚¿ç˜¤æŸ¥è¯¢ï¼Œä»è€Œå®ç°å…ˆå‰æœªè§è‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†æ‰©æ•£å¼•å¯¼ç‰¹å¾èåˆï¼Œä»¥æ”¹å–„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡æ•´åˆCLIPæ–‡æœ¬åµŒå…¥ï¼ŒDSMæ•è·ç±»åˆ«æ•æ„Ÿç±»ï¼Œæ”¹å–„è¯­è¨€è½¬ç§»çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å„ç§åœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDSMåœ¨å„ç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rows21/k-Means_Mask_Mamba">https://github.com/Rows21/k-Means_Mask_Mamba</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12747v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶DSMï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ä»¥å¤–çš„æœªçŸ¥è‚¿ç˜¤ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚DSMé€šè¿‡ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢æé«˜åˆ†ç±»ç²¾åº¦ï¼Œå¹¶é¦–æ¬¡ä½¿ç”¨å¯¹è±¡æ„ŸçŸ¥ç‰¹å¾åˆ†ç»„ç­–ç•¥å­¦ä¹ å™¨å®˜æŸ¥è¯¢ï¼Œç„¶åé€šè¿‡åŸºäºæ‰©æ•£çš„è§†è§‰æç¤ºç»†åŒ–è‚¿ç˜¤æŸ¥è¯¢ï¼Œå®ç°å¯¹æœªè§è‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾èåˆå’ŒCLIPæ–‡æœ¬åµŒå…¥çš„é›†æˆï¼ŒDSMæé«˜äº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒåœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDSMåœ¨å¤šç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSMæ˜¯ä¸€ä¸ªæ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†è®­ç»ƒæ•°æ®ä»¥å¤–çš„æœªçŸ¥è‚¿ç˜¤ç±»åˆ«ã€‚</li>
<li>DSMåˆ©ç”¨æ‰©æ•£å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œé€šè¿‡ä¸¤ç»„å¯¹è±¡æŸ¥è¯¢æé«˜åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>DSMé¦–æ¬¡ä½¿ç”¨å¯¹è±¡æ„ŸçŸ¥ç‰¹å¾åˆ†ç»„ç­–ç•¥å­¦ä¹ å™¨å®˜æŸ¥è¯¢ã€‚</li>
<li>é€šè¿‡åŸºäºæ‰©æ•£çš„è§†è§‰æç¤ºï¼ŒDSMèƒ½å¤Ÿç²¾ç¡®åˆ†å‰²æœªè§è‚¿ç˜¤ã€‚</li>
<li>DSMé€šè¿‡æ‰©æ•£å¼•å¯¼çš„ç‰¹å¾èåˆæé«˜äº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç»“åˆCLIPæ–‡æœ¬åµŒå…¥ï¼ŒDSMå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒåœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58c4abafed1909011e96d31e1b4d3b21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c912093ebef788a044b64ad94b26a55f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45db4878913bc44f18407f29c4f1eab1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0185a1f99ab8e14d7559da14ef88b44.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Narrative-Review-on-Large-AI-Models-in-Lung-Cancer-Screening-Diagnosis-and-Treatment-Planning"><a href="#A-Narrative-Review-on-Large-AI-Models-in-Lung-Cancer-Screening-Diagnosis-and-Treatment-Planning" class="headerlink" title="A Narrative Review on Large AI Models in Lung Cancer Screening,   Diagnosis, and Treatment Planning"></a>A Narrative Review on Large AI Models in Lung Cancer Screening,   Diagnosis, and Treatment Planning</h2><p><strong>Authors:Jiachen Zhong, Yiting Wang, Di Zhu, Ziwei Wang</strong></p>
<p>Lung cancer remains one of the most prevalent and fatal diseases worldwide, demanding accurate and timely diagnosis and treatment. Recent advancements in large AI models have significantly enhanced medical image understanding and clinical decision-making. This review systematically surveys the state-of-the-art in applying large AI models to lung cancer screening, diagnosis, prognosis, and treatment. We categorize existing models into modality-specific encoders, encoder-decoder frameworks, and joint encoder architectures, highlighting key examples such as CLIP, BLIP, Flamingo, BioViL-T, and GLoRIA. We further examine their performance in multimodal learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR. Applications span pulmonary nodule detection, gene mutation prediction, multi-omics integration, and personalized treatment planning, with emerging evidence of clinical deployment and validation. Finally, we discuss current limitations in generalizability, interpretability, and regulatory compliance, proposing future directions for building scalable, explainable, and clinically integrated AI systems. Our review underscores the transformative potential of large AI models to personalize and optimize lung cancer care. </p>
<blockquote>
<p>è‚ºç™Œä»ç„¶æ˜¯å…¨çƒæœ€å¸¸è§å’Œè‡´å‘½çš„ç–¾ç—…ä¹‹ä¸€ï¼Œéœ€è¦å‡†ç¡®åŠæ—¶çš„è¯Šæ–­å’Œæ²»ç–—ã€‚æœ€è¿‘äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¤§å‹è¿›å±•å·²ç»æ˜¾è‘—æé«˜äº†å¯¹åŒ»å­¦å›¾åƒçš„ç†è§£å’Œä¸´åºŠå†³ç­–ã€‚è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿåœ°ä»‹ç»äº†å°†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åº”ç”¨äºè‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å°†ç°æœ‰æ¨¡å‹åˆ†ç±»ä¸ºç‰¹å®šæ¨¡æ€ç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†CLIPã€BLIPã€Flamingoã€BioViL-Tå’ŒGLoRIAç­‰å…³é”®ç¤ºä¾‹ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†å®ƒä»¬åœ¨LIDC-IDRIã€NLSTå’ŒMIMIC-CXRç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚åº”ç”¨åŒ…æ‹¬è‚ºç»“èŠ‚æ£€æµ‹ã€åŸºå› çªå˜é¢„æµ‹ã€å¤šç»„å­¦æ•´åˆå’Œä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’ï¼Œæœ‰ä¸´åºŠéƒ¨ç½²å’ŒéªŒè¯çš„åˆæ­¥è¯æ®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†ç›®å‰åœ¨æ³›åŒ–æ€§ã€è§£é‡Šæ€§å’Œåˆè§„æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ„å»ºå¯æ‰©å±•ã€å¯è§£é‡Šå’Œä¸´åºŠæ•´åˆçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬çš„ç»¼è¿°å¼ºè°ƒäº†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä¸ªæ€§åŒ–ä¼˜åŒ–è‚ºç™Œæ²»ç–—æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07236v2">PDF</a> This request is based on the fact that one of the co-authors is a PhD   student whose advisor has informed her that she was not authorized to   publicly release this work without his prior approval. Unfortunately, this   approval was not obtained, and as such, the submission was made without   proper institutional and supervisory consent</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡ç»¼è¿°äº†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—çš„æœ€æ–°åº”ç”¨è¿›å±•ï¼ŒåŒ…æ‹¬æ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†CLIPã€BLIPã€Flamingoã€BioViL-Tå’ŒGLoRIAç­‰å…³é”®æ¨¡å‹ï¼Œå¹¶åœ¨LIDC-IDRIã€NLSTå’ŒMIMIC-CXRç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†å®ƒä»¬åœ¨å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è‚ºç™ŒæŠ¤ç†ä¸­çš„æ½œåŠ›åœ¨äºä¸ªæ€§åŒ–è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šï¼Œä½†ä¹Ÿå­˜åœ¨é€šç”¨æ€§ã€å¯è§£é‡Šæ€§å’Œç›‘ç®¡åˆè§„æ€§çš„å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‚ºç™Œä»ç„¶æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„ä¸»è¦ç–¾ç—…ï¼Œéœ€è¦å‡†ç¡®åŠæ—¶çš„è¯Šæ–­å’Œæ²»ç–—ã€‚</li>
<li>å¤§å‹AIæ¨¡å‹åœ¨åŒ»ç–—å›¾åƒç†è§£å’Œä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢çš„åº”ç”¨å·²ç»æ˜¾è‘—æé«˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¯åˆ†ä¸ºæ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨è‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>åº”ç”¨é¢†åŸŸåŒ…æ‹¬è‚ºç»“èŠ‚æ£€æµ‹ã€åŸºå› çªå˜é¢„æµ‹ã€å¤šç»„å­¦æ•´åˆå’Œä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’åˆ¶å®šã€‚</li>
<li>å¤§å‹AIæ¨¡å‹åœ¨ä¸´åºŠéƒ¨ç½²å’ŒéªŒè¯æ–¹é¢å·²æœ‰åˆæ­¥è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cbb9f2bc2de98920a6a0b87b4aa855c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84a2f1f149fa4206dd7c997cbccf77b0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Radiology-with-Zero-Shot-Multi-Task-Capability"><a href="#RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Radiology-with-Zero-Shot-Multi-Task-Capability" class="headerlink" title="RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability"></a>RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability</h2><p><strong>Authors:Jonggwon Park, Soobum Kim, Byungmu Yoon, Kyoyun Choi</strong></p>
<p>Recent advancements in multi-modal models have significantly improved vision-language (VL) alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning and offer limited interpretability through attention probability visualizations. To address these challenges, we introduce RadZero, a novel framework for VL alignment in radiology with zero-shot multi-task capability. A key component of our approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity), which aligns text embeddings with local image features for interpretable, fine-grained VL reasoning. RadZero leverages large language models to extract concise semantic sentences from radiology reports and employs multi-positive contrastive training to effectively capture relationships between images and multiple relevant textual descriptions. It uses a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, VL-CABS enables zero-shot inference with similarity probability for classification, and pixel-level VL similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, VL similarity map analysis highlights the potential of VL-CABS for improving explainability in VL alignment. Additionally, qualitative evaluation demonstrates RadZeroâ€™s capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å½±åƒå­¦é¢†åŸŸï¼Œå¤šæ¨¡æ€æ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†è§†è§‰è¯­è¨€ï¼ˆVLï¼‰çš„å¯¹é½èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤æ‚çš„æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä¸”é€šè¿‡æ³¨æ„åŠ›æ¦‚ç‡å¯è§†åŒ–æä¾›çš„è§£é‡Šæ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RadZeroï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›çš„åŒ»å­¦å½±åƒå­¦è§†è§‰è¯­è¨€å¯¹é½æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®ç»„ä»¶æ˜¯VL-CABSï¼ˆåŸºäºç›¸ä¼¼æ€§çš„è§†è§‰è¯­è¨€äº¤å‰æ³¨æ„åŠ›ï¼‰ï¼Œå®ƒå°†æ–‡æœ¬åµŒå…¥ä¸å±€éƒ¨å›¾åƒç‰¹å¾å¯¹é½ï¼Œä»¥å®ç°å¯è§£é‡Šã€ç»†ç²’åº¦çš„è§†è§‰è¯­è¨€æ¨ç†ã€‚RadZeroåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–ç®€æ´è¯­ä¹‰å¥å­ï¼Œå¹¶é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”è®­ç»ƒæœ‰æ•ˆåœ°æ•è·å›¾åƒä¸å¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä»¥åŠé¢å¤–çš„å¯è®­ç»ƒTransformerå±‚ï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒVL-CABSå®ç°äº†é›¶æ ·æœ¬æ¨ç†ï¼Œå…·æœ‰ç›¸ä¼¼æ€§æ¦‚ç‡åˆ†ç±»ã€åƒç´ çº§è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§åœ°å›¾ç”¨äºå®šä½å’Œåˆ†å‰²ã€‚åœ¨å…¬å…±èƒ¸éƒ¨Xå…‰å½±åƒåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½å’Œåˆ†å‰²æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè§†è§‰è¯­è¨€ç›¸ä¼¼æ€§åœ°å›¾åˆ†æçªå‡ºäº†VL-CABSåœ¨æé«˜è§†è§‰è¯­è¨€å¯¹é½è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚å®šæ€§è¯„ä¼°è¿˜è¯æ˜äº†RadZeroåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨åŒ»å­¦å½±åƒä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07416v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RadZeroæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€æ¨¡å‹æé«˜äº†æ”¾å°„å­¦ä¸­çš„è§†è§‰è¯­è¨€å¯¹é½æ•ˆæœã€‚é€šè¿‡å¼•å…¥VL-CABSï¼ˆåŸºäºç›¸ä¼¼æ€§çš„è§†è§‰è¯­è¨€äº¤å‰æ³¨æ„åŠ›ï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯ï¼ŒRadZeroè§£å†³äº†ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæé«˜äº†æŠ¥å‘Šçš„åˆ©ç”¨ç‡å’Œè§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½ã€åˆ†å‰²ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadZeroæ˜¯ä¸€ä¸ªç”¨äºæ”¾å°„å­¦è§†è§‰è¯­è¨€å¯¹é½çš„æ–°æ¡†æ¶ï¼Œå…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>RadZeroé€šè¿‡å¼•å…¥VL-CABSæŠ€æœ¯ï¼Œå®ç°äº†æ–‡æœ¬åµŒå…¥ä¸å±€éƒ¨å›¾åƒç‰¹å¾çš„å¯¹é½ï¼Œä¸ºå¯è§£é‡Šçš„ç²¾ç»†è§†è§‰è¯­è¨€æ¨ç†æä¾›äº†åŸºç¡€ã€‚</li>
<li>RadZeroåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–ç®€æ´è¯­ä¹‰å¥å­ã€‚</li>
<li>é€šè¿‡å¤šé˜³æ€§å¯¹æ¯”è®­ç»ƒï¼ŒRadZeroèƒ½æœ‰æ•ˆæ•æ‰å›¾åƒä¸å¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>RadZeroä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼Œå¹¶æ·»åŠ å¯è®­ç»ƒçš„Transformerå±‚ï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚</li>
<li>RadZeroçš„é›¶æ ·æœ¬æ¨æ–­èƒ½åŠ›é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥å®ç°åˆ†ç±»ã€å®šä½ä¸åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e19c2d0e82a42a5511f34a47b2bad19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fbd884ca91812336cd1c0b213114e11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56279a06232c1454f40f83d18a16ac78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1bc904495e8ecfabb424ca7894a3823.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd52106d3145dd918efdc3f367693157.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedSegNet10-A-Publicly-Accessible-Network-Repository-for-Split-Federated-Medical-Image-Segmentation"><a href="#MedSegNet10-A-Publicly-Accessible-Network-Repository-for-Split-Federated-Medical-Image-Segmentation" class="headerlink" title="MedSegNet10: A Publicly Accessible Network Repository for Split   Federated Medical Image Segmentation"></a>MedSegNet10: A Publicly Accessible Network Repository for Split   Federated Medical Image Segmentation</h2><p><strong>Authors:Chamani Shiranthika, Zahra Hafezi Kafshgari, Hadi Hadizadeh, Parvaneh Saeedi</strong></p>
<p>Machine Learning (ML) and Deep Learning (DL) have shown significant promise in healthcare, particularly in medical image segmentation, which is crucial for accurate disease diagnosis and treatment planning. Despite their potential, challenges such as data privacy concerns, limited annotated data, and inadequate training data persist. Decentralized learning approaches such as federated learning (FL), split learning (SL), and split federated learning (SplitFed&#x2F;SFL) address these issues effectively. This paper introduces â€œMedSegNet10,â€ a publicly accessible repository designed for medical image segmentation using split-federated learning. MedSegNet10 provides a collection of pre-trained neural network architectures optimized for various medical image types, including microscopic images of human blastocysts, dermatoscopic images of skin lesions, and endoscopic images of lesions, polyps, and ulcers, with applications extending beyond these examples. By leveraging SplitFedâ€™s benefits, MedSegNet10 allows collaborative training on privately stored, horizontally split data, ensuring privacy and integrity. This repository supports researchers, practitioners, trainees, and data scientists, aiming to advance medical image segmentation while maintaining patient data privacy. The repository is available at: <a target="_blank" rel="noopener" href="https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX">https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX</a> (password upon request to the authors). </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ï¼Œè¿™å¯¹äºå‡†ç¡®çš„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚å°½ç®¡å­˜åœ¨æ½œåŠ›ï¼Œä½†æ•°æ®éšç§æ‹…å¿§ã€ç¼ºä¹æ³¨é‡Šæ•°æ®å’Œè®­ç»ƒæ•°æ®ä¸è¶³ç­‰æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ã€åˆ†å‰²å­¦ä¹ ï¼ˆSLï¼‰å’Œåˆ†å‰²è”é‚¦å­¦ä¹ ï¼ˆSplitFed&#x2F;SFLï¼‰ç­‰åˆ†å¸ƒå¼å­¦ä¹ æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†â€œMedSegNet10â€ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¬å¼€å¯è®¿é—®çš„å­˜å‚¨åº“ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œé‡‡ç”¨åˆ†å‰²è”é‚¦å­¦ä¹ çš„æ–¹æ³•ã€‚MedSegNet10æä¾›äº†ä¸€ç³»åˆ—é’ˆå¯¹å„ç§åŒ»å­¦å›¾åƒç±»å‹è¿›è¡Œä¼˜åŒ–çš„é¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬äººç±»å›Šèƒšçš„æ˜¾å¾®é•œå›¾åƒã€çš®è‚¤ç—…å˜çš„çš®è‚¤ç§‘å†…é•œå›¾åƒã€ç—…å˜ã€æ¯è‚‰å’Œæºƒç–¡çš„å†…çª¥é•œå›¾åƒç­‰ï¼Œåº”ç”¨èŒƒå›´ä¸é™äºè¿™äº›ç¤ºä¾‹ã€‚é€šè¿‡åˆ©ç”¨SplitFedçš„ä¼˜åŠ¿ï¼ŒMedSegNet10å¯ä»¥åœ¨ç§å¯†å­˜å‚¨çš„æ°´å¹³åˆ†å‰²æ•°æ®ä¸Šè¿›è¡Œåä½œè®­ç»ƒï¼Œç¡®ä¿éšç§å’Œå®Œæ•´æ€§ã€‚è¯¥å­˜å‚¨åº“æ”¯æŒç ”ç©¶äººå‘˜ã€å®è·µè€…ã€åŸ¹è®­äººå‘˜å’Œæ•°æ®åˆ†æå¸ˆï¼Œæ—¨åœ¨æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‘å±•ï¼ŒåŒæ—¶ä¿æŒæ‚£è€…æ•°æ®çš„éšç§ã€‚è¯¥å­˜å‚¨åº“ä½äºï¼š<a target="_blank" rel="noopener" href="https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX%EF%BC%88%E9%9C%80%E5%90%91%E4%BD%9C%E8%80%85%E6%B3%A8%E5%A4%87%E5%AF%86%E7%A0%81%EF%BC%89%E3%80%82">https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuXï¼ˆéœ€å‘ä½œè€…ç”³è¯·å¯†ç ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20830v2">PDF</a> 20 pages, 14 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨è¯¥é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ•°æ®éšç§ã€æ ‡æ³¨æ•°æ®æœ‰é™å’Œè®­ç»ƒæ•°æ®ä¸è¶³ç­‰é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚æœ¬æ–‡ä»‹ç»äº†MedSegNet10ï¼Œä¸€ä¸ªé‡‡ç”¨åˆ†è£‚è”åˆå­¦ä¹ ï¼ˆSplitFedï¼‰çš„åŒ»å­¦å›¾åƒåˆ†å‰²å…¬å¼€å­˜å‚¨åº“ã€‚è¯¥å­˜å‚¨åº“æä¾›é’ˆå¯¹å¤šç§åŒ»å­¦å›¾åƒç±»å‹çš„é¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬äººç±»å›Šèƒšæ˜¾å¾®é•œå›¾åƒã€çš®è‚¤ç—…å˜çš„çš®è‚¤ç§‘å†…é•œå›¾åƒä»¥åŠç—…å˜ã€æ¯è‚‰å’Œæºƒç–¡çš„å†…çª¥é•œå›¾åƒç­‰ã€‚å®ƒæ—¨åœ¨æ¨è¿›åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç ”ç©¶ï¼ŒåŒæ—¶ä¿æŒæ‚£è€…æ•°æ®éšç§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>æ•°æ®éšç§ã€æ ‡æ³¨æ•°æ®æœ‰é™å’Œè®­ç»ƒæ•°æ®ä¸è¶³æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ†è£‚è”åˆå­¦ä¹ ï¼ˆSplitFedï¼‰æ–¹æ³•å¯ä»¥æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>MedSegNet10æ˜¯ä¸€ä¸ªé‡‡ç”¨SplitFedçš„åŒ»å­¦å›¾åƒåˆ†å‰²å…¬å¼€å­˜å‚¨åº“ã€‚</li>
<li>MedSegNet10æä¾›é’ˆå¯¹å¤šç§åŒ»å­¦å›¾åƒç±»å‹çš„é¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c37827af1fd1850f4b45b3f43716760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335fb33cb3bcc7d85be030200f4a3b28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04c061d5a64118ae0c46aa47d8047830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f808a2906dd76870ee76b438f5b7bae3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4055e0fcbceafb1b2846933597ebb12e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction"><a href="#VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction" class="headerlink" title="VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction"></a>VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction</h2><p><strong>Authors:Zizhi Chen, Minghao Han, Xukun Zhang, Shuwei Ma, Tao Liu, Xing Wei, Lihua Zhang</strong></p>
<p>Multimodal learning combining pathology images and genomic sequences enhances cancer survival analysis but faces clinical implementation barriers due to limited access to genomic sequencing in under-resourced regions. To enable survival prediction using only whole-slide images (WSI), we propose the Visual-Genomic Answering-Guided Transformer (VGAT), a framework integrating Visual Question Answering (VQA) techniques for genomic modality reconstruction. By adapting VQAâ€™s text feature extraction approach, we derive stable genomic representations that circumvent dimensionality challenges in raw genomic data. Simultaneously, a cluster-based visual prompt module selectively enhances discriminative WSI patches, addressing noise from unfiltered image regions. Evaluated across five TCGA datasets, VGAT outperforms existing WSI-only methods, demonstrating the viability of genomic-informed inference without sequencing. This approach bridges multimodal research and clinical feasibility in resource-constrained settings. The code link is <a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT">https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT</a>. </p>
<blockquote>
<p>å°†ç—…ç†å›¾åƒä¸åŸºå› ç»„åºåˆ—ç›¸ç»“åˆçš„å¤šæ¨¡æ€å­¦ä¹ èƒ½æé«˜ç™Œç—‡ç”Ÿå­˜åˆ†æèƒ½åŠ›ï¼Œä½†ç”±äºèµ„æºåŒ®ä¹åœ°åŒºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®æ€§ï¼Œå…¶åœ¨ä¸´åºŠå®æ–½ä¸­é¢ä¸´éšœç¢ã€‚ä¸ºäº†ä»…ä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWhole Slide Images, WSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸºå› ç»„é—®ç­”å¼•å¯¼è½¬æ¢å™¨ï¼ˆVisual-Genomic Answering-Guided Transformerï¼ŒVGATï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†è§†è§‰é—®ç­”ï¼ˆVisual Question Answering, VQAï¼‰æŠ€æœ¯ï¼Œç”¨äºåŸºå› ç»„æ¨¡æ€é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œé¿å…äº†åŸå§‹åŸºå› ç»„æ•°æ®çš„ç»´åº¦æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—æœ‰é€‰æ‹©åœ°å¢å¼ºäº†åˆ¤åˆ«æ€§çš„WSIæ–‘å—ï¼Œè§£å†³äº†æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒVGATåœ¨ä»…ä½¿ç”¨WSIçš„æ–¹æ³•ä¸­è¡¨ç°çªå‡ºï¼Œè¯æ˜äº†åœ¨æ²¡æœ‰æµ‹åºçš„æƒ…å†µä¸‹è¿›è¡ŒåŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šæ¨¡æ€ç ”ç©¶å’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„ä¸´åºŠå¯è¡Œæ€§ä¹‹é—´æ­å»ºäº†æ¡¥æ¢ã€‚ä»£ç é“¾æ¥æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT">ä»£ç é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19367v3">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>ç»“åˆç—…ç†å›¾åƒå’ŒåŸºå› ç»„åºåˆ—çš„å¤šæ¨¡æ€å­¦ä¹ å¢å¼ºäº†ç™Œç—‡ç”Ÿå­˜åˆ†æï¼Œä½†ç”±äºèµ„æºåŒ®ä¹åœ°åŒºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®ï¼Œé¢ä¸´ä¸´åºŠå®æ–½éšœç¢ã€‚ä¸ºä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºVisual-Genomic Answering-Guided Transformerï¼ˆVGATï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯ç”¨äºåŸºå› ç»„æ¨¡æ€é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬å¯¼å‡ºç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œå¯é¿å…åŸå§‹åŸºå› ç»„æ•°æ®çš„ç»´åº¦æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—é€‰æ‹©æ€§åœ°å¢å¼ºé‰´åˆ«æ€§çš„WSIæ–‘å—ï¼Œè§£å†³æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVGATåœ¨ä»…ä½¿ç”¨WSIçš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ•ˆæœï¼Œè¯æ˜äº†æ— æµ‹åºçš„åŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚è¯¥æ–¹æ³•åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°äº†å¤šæ¨¡æ€ç ”ç©¶ä¸ä¸´åºŠå¯è¡Œæ€§çš„ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ æ•´åˆç—…ç†å›¾åƒå’ŒåŸºå› ç»„åºåˆ—å¢å¼ºäº†ç™Œç—‡ç”Ÿå­˜åˆ†æçš„æ•ˆæœã€‚</li>
<li>VGATæ¡†æ¶é€šè¿‡æ•´åˆVQAæŠ€æœ¯ç”¨äºåŸºå› ç»„æ¨¡æ€é‡å»ºï¼Œå…‹æœäº†ä¸´åºŠå®æ–½ä¸­çš„éšœç¢ã€‚</li>
<li>VGATåˆ©ç”¨ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºæ¥è§£å†³åŸå§‹åŸºå› ç»„æ•°æ®çš„ç»´åº¦æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—å¢å¼ºäº†é‰´åˆ«æ€§çš„WSIæ–‘å—ï¼Œé™ä½äº†æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°å¹²æ‰°ã€‚</li>
<li>VGATåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†æ— æµ‹åºçš„åŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚</li>
<li>VGATæ–¹æ³•ä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„ä¸´åºŠå®æ–½æä¾›äº†å¯èƒ½çš„å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-001074ec0ecb723a079223b5a43dbba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832917e08e6d9470f2d6d8137c783037.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09a3d4d5585b895dd0efdb66b1e0879f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e504c5c98b17cc8d9e22464901c320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7ecea5958615700f6cb7868676a6405.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f8164d1d306337ce7ca27561231d59.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis"><a href="#Efficient-Self-Supervised-Adaptation-for-Medical-Image-Analysis" class="headerlink" title="Efficient Self-Supervised Adaptation for Medical Image Analysis"></a>Efficient Self-Supervised Adaptation for Medical Image Analysis</h2><p><strong>Authors:Moein Sorkhei, Emir Konuk, Jingyu Guo, Chanjuan Meng, Christos Matsoukas, Kevin Smith</strong></p>
<p>Self-supervised adaptation (SSA) improves foundation model transfer to medical domains but is computationally prohibitive. Although parameter efficient fine-tuning methods such as LoRA have been explored for supervised adaptation, their effectiveness for SSA remains unknown. In this work, we introduce efficient self-supervised adaptation (ESSA), a framework that applies parameter-efficient fine-tuning techniques to SSA with the aim of reducing computational cost and improving adaptation performance. Among the methods tested, Attention Projection Layer Adaptation (APLA) sets a new state-of-the-art, consistently surpassing full-parameter SSA and supervised fine-tuning across diverse medical tasks, while reducing GPU memory by up to 40.1% and increasing training throughput by 25.2%, all while maintaining inference efficiency. </p>
<blockquote>
<p>è‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆSSAï¼‰æé«˜äº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»èƒ½åŠ›ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚è™½ç„¶LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å·²è¢«æ¢ç´¢ç”¨äºç›‘ç£é€‚åº”ï¼Œä½†å…¶åœ¨SSAä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ•ˆè‡ªæˆ‘ç›‘ç£é€‚åº”ï¼ˆESSAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åº”ç”¨äºSSAï¼Œæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§èƒ½ã€‚åœ¨æµ‹è¯•çš„æ–¹æ³•ä¸­ï¼Œæ³¨æ„åŠ›æŠ•å½±å±‚é€‚åº”ï¼ˆAPLAï¼‰è¡¨ç°å“è¶Šï¼ŒæŒç»­è¶…è¶Šå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒï¼Œåœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒæ—¶é™ä½GPUå†…å­˜ä½¿ç”¨ç‡é«˜è¾¾40.1%ï¼Œæé«˜è®­ç»ƒæ•ˆç‡25.2%ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18873v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SSAï¼ˆè‡ªç›‘ç£é€‚åº”ï¼‰åœ¨åŸºç¡€æ¨¡å‹å‘åŒ»å­¦é¢†åŸŸè¿ç§»æ—¶è™½ç„¶èƒ½æé«˜æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„è‡ªç›‘ç£é€‚åº”æ¡†æ¶ï¼ˆESSAï¼‰ï¼Œé‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ä»¥é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜é€‚åº”æ€§èƒ½ã€‚å…¶ä¸­ï¼ŒAttention Projection Layer Adaptationï¼ˆAPLAï¼‰æ–¹æ³•è¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒåœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼ŒåŒæ—¶å‡å°‘äº†GPUå†…å­˜ä½¿ç”¨å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSAåœ¨æé«˜åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„è¿ç§»æ€§èƒ½æ—¶é¢ä¸´è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>ESSAæ¡†æ¶è¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ä»¥è§£å†³SSAçš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>APLAæ–¹æ³•åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¨å‚æ•°SSAå’Œç›‘ç£å¾®è°ƒã€‚</li>
<li>APLAèƒ½å‡å°‘GPUå†…å­˜ä½¿ç”¨ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ESSAæ¡†æ¶åœ¨ç»´æŒæ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œæé«˜äº†é€‚åº”æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºè‡ªç›‘ç£é€‚åº”æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0a5d817d39128587af9ad233eb61599c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-380606255f1db1f238e4fd5f66b6f0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6052497ba5251734b558ad775cd93c78.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RLCAD-Reinforcement-Learning-Training-Gym-for-Revolution-Involved-CAD-Command-Sequence-Generation"><a href="#RLCAD-Reinforcement-Learning-Training-Gym-for-Revolution-Involved-CAD-Command-Sequence-Generation" class="headerlink" title="RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD   Command Sequence Generation"></a>RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD   Command Sequence Generation</h2><p><strong>Authors:Xiaolong Yin, Xingyu Lu, Jiahang Shen, Jingzhe Ni, Hailong Li, Ruofeng Tong, Min Tang, Peng Du</strong></p>
<p>A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å‘½ä»¤åºåˆ—æ˜¯ä¸‰ç»´CADç³»ç»Ÿä¸­å…¸å‹çš„å‚æ•°åŒ–è®¾è®¡èŒƒä¾‹ï¼Œå…¶ä¸­æ¨¡å‹æ˜¯é€šè¿‡å åŠ äºŒç»´è‰å›¾å¹¶æ‰§è¡ŒæŒ¤å‹ã€æ—‹è½¬å’Œå¸ƒå°”è¿ç®—ç­‰æ“ä½œæ¥æ„å»ºçš„ã€‚å°½ç®¡å­¦æœ¯ç•Œå¯¹å‘½ä»¤åºåˆ—çš„è‡ªåŠ¨ç”Ÿæˆè¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†ç°æœ‰æ–¹æ³•å’Œæ•°æ®é›†ä»…æ”¯æŒäºŒç»´è‰å›¾ã€æŒ¤å‹å’Œå¸ƒå°”è¿ç®—ç­‰æ“ä½œã€‚è¿™ä¸€å±€é™æ€§ä½¿å¾—è¡¨ç¤ºæ›´å¤æ‚çš„å‡ ä½•å½¢çŠ¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºCADå‡ ä½•å¼•æ“çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒç¯å¢ƒï¼ˆgymï¼‰ã€‚ç»™å®šè¾“å…¥è¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰å‡ ä½•ä½“ï¼ŒRLç®—æ³•ä¸­çš„ç­–ç•¥ç½‘ç»œä¼šç”Ÿæˆä¸€ä¸ªåŠ¨ä½œã€‚è¯¥åŠ¨ä½œä¸å…ˆå‰ç”Ÿæˆçš„åŠ¨ä½œä¸€èµ·åœ¨gymä¸­è¿›è¡Œå¤„ç†ï¼Œä»¥äº§ç”Ÿç›¸åº”çš„CADå‡ ä½•ä½“ï¼Œç„¶ååé¦ˆåˆ°ç­–ç•¥ç½‘ç»œã€‚å¥–åŠ±ç”±gymå†…ç”Ÿæˆç›®æ ‡å‡ ä½•ä½“ä¹‹é—´çš„å·®å¼‚æ¥ç¡®å®šï¼Œç”¨äºæ›´æ–°RLç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒè‰å›¾ã€å¸ƒå°”å’ŒæŒ¤å‹æ“ä½œä»¥å¤–çš„æ“ä½œï¼ŒåŒ…æ‹¬æ—‹è½¬æ“ä½œã€‚ä½¿ç”¨è¿™ä¸ªè®­ç»ƒç¯å¢ƒï¼Œæˆ‘ä»¬åœ¨ä»B-Repå‡ ä½•ä½“ç”Ÿæˆå‘½ä»¤åºåˆ—æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18549v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„CADå‡ ä½•å¼•æ“è®­ç»ƒç¯å¢ƒï¼Œèƒ½å¤Ÿç”Ÿæˆå‘½ä»¤åºåˆ—ä»¥æ„å»ºå¤æ‚çš„CADæ¨¡å‹ã€‚é€šè¿‡è¾“å…¥è¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰å‡ ä½•ä½“ï¼Œç­–ç•¥ç½‘ç»œç”ŸæˆåŠ¨ä½œï¼Œå¹¶é€šè¿‡å¤„ç†è¿™äº›åŠ¨ä½œç”Ÿæˆç›¸åº”çš„CADå‡ ä½•ä½“ã€‚å¥–åŠ±æ ¹æ®ç”Ÿæˆçš„CADå‡ ä½•ä½“ä¸ç›®æ ‡å‡ ä½•ä½“ä¹‹é—´çš„å·®å¼‚æ¥ç¡®å®šï¼Œç”¨äºæ›´æ–°RLç½‘ç»œã€‚æ­¤æ–¹æ³•æ”¯æŒåŒ…æ‹¬æ—‹è½¬æ“ä½œåœ¨å†…çš„å¤šç§æ“ä½œï¼Œå®ç°äº†ä»B-Repå‡ ä½•ä½“ç”Ÿæˆå‘½ä»¤åºåˆ—çš„æœ€ä¼˜è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ„å»ºCADè®­ç»ƒç¯å¢ƒï¼Œä»¥ç”Ÿæˆå‘½ä»¤åºåˆ—ã€‚</li>
<li>è¾“å…¥ä¸ºè¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰å‡ ä½•ä½“ï¼Œè¾“å‡ºä¸ºç›¸åº”çš„CADå‡ ä½•ä½“ã€‚</li>
<li>ç­–ç•¥ç½‘ç»œåœ¨è®­ç»ƒç¯å¢ƒä¸­ç”ŸæˆåŠ¨ä½œï¼Œå¹¶é€šè¿‡å¤„ç†è¿™äº›åŠ¨ä½œç”ŸæˆCADæ¨¡å‹ã€‚</li>
<li>å¥–åŠ±åŸºäºç”Ÿæˆçš„CADæ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ç¡®å®šï¼Œç”¨äºæ›´æ–°å¼ºåŒ–å­¦ä¹ ç½‘ç»œã€‚</li>
<li>æ”¯æŒå¤šç§æ“ä½œï¼ŒåŒ…æ‹¬æ—‹è½¬æ“ä½œï¼Œè¿™æ‰©å¤§äº†ç°æœ‰æ–¹æ³•å’Œæ•°æ®é›†çš„æ”¯æŒèŒƒå›´ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä»B-Repå‡ ä½•ä½“ç”Ÿæˆå‘½ä»¤åºåˆ—çš„æœ€ä¼˜è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21d73e3e127fdf5adbdb3dcf74b9dfed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a96fcbc51b60a88ed4371dbf4a0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5ef60e1f0b6ce06966de93a94295369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfa6bca3ff5a38089449ff3dcde01f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-188dc001b8c4fa7c9cad691df13d0fa2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Disentangled-and-Interpretable-Multimodal-Attention-Fusion-for-Cancer-Survival-Prediction"><a href="#Disentangled-and-Interpretable-Multimodal-Attention-Fusion-for-Cancer-Survival-Prediction" class="headerlink" title="Disentangled and Interpretable Multimodal Attention Fusion for Cancer   Survival Prediction"></a>Disentangled and Interpretable Multimodal Attention Fusion for Cancer   Survival Prediction</h2><p><strong>Authors:Aniek Eijpe, Soufyan Lakbir, Melis Erdal Cesur, Sara P. Oliveira, Sanne Abeln, Wilson Silva</strong></p>
<p>To improve the prediction of cancer survival using whole-slide images and transcriptomics data, it is crucial to capture both modality-shared and modality-specific information. However, multimodal frameworks often entangle these representations, limiting interpretability and potentially suppressing discriminative features. To address this, we propose Disentangled and Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that separates the intra- and inter-modal interactions within an attention-based fusion mechanism to learn distinct modality-specific and modality-shared representations. We introduce a loss based on Distance Correlation to promote disentanglement between these representations and integrate Shapley additive explanations to assess their relative contributions to survival prediction. We evaluate DIMAF on four public cancer survival datasets, achieving a relative average improvement of 1.85% in performance and 23.7% in disentanglement compared to current state-of-the-art multimodal models. Beyond improved performance, our interpretable framework enables a deeper exploration of the underlying interactions between and within modalities in cancer biology. </p>
<blockquote>
<p>ä¸ºäº†æé«˜ä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒå’Œè½¬å½•ç»„æ•°æ®å¯¹ç™Œç—‡ç”Ÿå­˜ç‡çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ•è·æ—¢é€‚ç”¨äºå¤šç§æ¨¡å¼åˆé€‚ç”¨äºç‰¹å®šæ¨¡å¼çš„ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤šæ¨¡å¼æ¡†æ¶é€šå¸¸ä¼šå°†è¿™äº›è¡¨ç¤ºçº ç¼ åœ¨ä¸€èµ·ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§å¹¶å¯èƒ½æŠ‘åˆ¶è¾¨åˆ«ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£ç¼ ä¸”å¯è§£é‡Šçš„å¤šæ¨¡å¼æ³¨æ„åŠ›èåˆâ€ï¼ˆDIMAFï¼‰æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„èåˆæœºåˆ¶æ¥åˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œä»¥å­¦ä¹ ç‹¬ç‰¹çš„ç‰¹å®šæ¨¡æ€å’Œå…±äº«æ¨¡æ€çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºè·ç¦»ç›¸å…³çš„æŸå¤±æ¥ä¿ƒè¿›è¿™äº›è¡¨ç¤ºä¹‹é—´çš„è§£ç¼ ï¼Œå¹¶æ•´åˆæ²™æ™®åˆ©åŠ æ³•è§£é‡Šæ¥è¯„ä¼°å®ƒä»¬å¯¹ç”Ÿå­˜é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šè¯„ä¼°äº†DIMAFï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡å¼æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½å¹³å‡æé«˜äº†1.85%ï¼Œè§£ç¼ ç¨‹åº¦æé«˜äº†23.7%ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼Œæˆ‘ä»¬çš„å¯è§£é‡Šæ¡†æ¶è¿˜ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ›´æ·±å…¥åœ°æ¢ç´¢ç™Œç—‡ç”Ÿç‰©å­¦ä¸­æ¨¡å¼ä¹‹é—´åŠå…¶å†…éƒ¨çš„æ½œåœ¨ç›¸äº’ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16069v2">PDF</a> 11 pages, 1 figure, 3 tables. Preprint submitted and accepted to   MICCAI 2025. This preprint has not undergone peer review or any   post-submission improvements or corrections</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨æ»‘å›¾åƒå’Œè½¬å½•ç»„å­¦æ•°æ®çš„ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æ”¹è¿›ä¸­ï¼Œå…³é”®åœ¨äºæ•æ‰æ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šçš„ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„å¤šæ¨¡æ€æ³¨æ„åŠ›èåˆæ¡†æ¶ï¼ˆDIMAFï¼‰ï¼Œå®ƒé€šè¿‡æ³¨æ„åŠ›èåˆæœºåˆ¶åˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œå­¦ä¹ ä¸åŒçš„æ¨¡æ€ç‰¹å®šå’Œæ¨¡æ€å…±äº«è¡¨ç¤ºã€‚é‡‡ç”¨åŸºäºè·ç¦»ç›¸å…³çš„æŸå¤±æ¥ä¿ƒè¿›è¿™äº›è¡¨ç¤ºä¹‹é—´çš„è§£è€¦ï¼Œå¹¶åˆ©ç”¨æ²™æ™®åˆ©åŠ æ³•è§£é‡Šæ³•è¯„ä¼°å…¶å¯¹ç”Ÿå­˜é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šè¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†å¹³å‡1.85%ï¼Œè§£è€¦ç¨‹åº¦æé«˜äº†23.7%ã€‚é™¤äº†æé«˜æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬çš„å¯è§£é‡Šæ¡†æ¶è¿˜æ·±å…¥æ¢ç´¢äº†ç™Œç—‡ç”Ÿç‰©å­¦ä¸­æ¨¡æ€ä¹‹é—´å’Œæ¨¡æ€å†…éƒ¨çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æ”¹è¿›çš„å…³é”®åœ¨äºæ•æ‰å…¨æ»‘å›¾åƒå’Œè½¬å½•ç»„å­¦æ•°æ®çš„æ¨¡æ€å…±äº«ä¸æ¨¡æ€ç‰¹å®šä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDIMAFçš„å¤šæ¨¡æ€æ³¨æ„åŠ›èåˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¡†æ¶åœ¨èåˆä¸åŒä¿¡æ¯æ—¶å­˜åœ¨çš„çº ç¼ é—®é¢˜ã€‚</li>
<li>DIMAFé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åˆ†ç¦»æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ï¼Œå­¦ä¹ ä¸åŒçš„æ¨¡æ€ç‰¹å®šå’Œå…±äº«è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨åŸºäºè·ç¦»ç›¸å…³çš„æŸå¤±å‡½æ•°æ¥ä¿ƒè¿›ä¸åŒè¡¨ç¤ºä¹‹é—´çš„è§£è€¦ã€‚</li>
<li>åˆ©ç”¨æ²™æ™®åˆ©åŠ æ³•è§£é‡Šæ³•è¯„ä¼°ä¸åŒä¿¡æ¯å¯¹ç”Ÿå­˜é¢„æµ‹çš„ç›¸å¯¹è´¡çŒ®ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±ç™Œç—‡ç”Ÿå­˜æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒDIMAFåœ¨æ€§èƒ½å’Œè§£è€¦ç¨‹åº¦æ–¹é¢æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e862a87709debb33896633e2a802a96d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a14c4cf3f2124027adf1e0a255eb0672.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-Segmentation-Framework-for-Thyroid-Nodule-Based-on-High-confidence-Labels-and-High-rationality-Losses"><a href="#Weakly-Supervised-Segmentation-Framework-for-Thyroid-Nodule-Based-on-High-confidence-Labels-and-High-rationality-Losses" class="headerlink" title="Weakly Supervised Segmentation Framework for Thyroid Nodule Based on   High-confidence Labels and High-rationality Losses"></a>Weakly Supervised Segmentation Framework for Thyroid Nodule Based on   High-confidence Labels and High-rationality Losses</h2><p><strong>Authors:Jianning Chi, Zelan Li, Geng Lin, MingYang Sun, Xiaosheng Yu</strong></p>
<p>Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bluehenglee/MLI-MSC">https://github.com/bluehenglee/MLI-MSC</a>. </p>
<blockquote>
<p>å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å¸¦æœ‰ç²—ç•¥æ ‡ç­¾çš„è®­ç»ƒæ•°æ®æœ‰æ•ˆåœ°åœ¨è¶…å£°å›¾åƒä¸­æç»˜ç”²çŠ¶è…ºç»“èŠ‚ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š1ï¼‰éµå¾ªæ‹“æ‰‘å…ˆéªŒçš„ä½ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ï¼Œå¼•å…¥äº†å¤§é‡æ ‡ç­¾å™ªå£°ï¼›2ï¼‰æŸå¤±å‡½æ•°ç¼ºä¹ç†æ€§ï¼Œç”Ÿç¡¬åœ°å°†åˆ†å‰²ç»“æœä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œå¿½è§†äº†å…·æœ‰å¤šæ ·æ€§å’Œå¤æ‚å½¢çŠ¶çš„ç»“èŠ‚çš„åˆ¤åˆ«ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ˜ç¡®äº†å¼±ç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²çš„ç›®æ ‡å’Œå‚è€ƒï¼Œæå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾æ¥è¡¨ç¤ºæ‹“æ‰‘å’Œè§£å‰–ä¿¡æ¯ï¼Œä»¥åŠé«˜ç†æ€§æŸå¤±æ¥æ•æ‰å¤šå±‚æ¬¡çš„åˆ¤åˆ«ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡èåˆå››ç‚¹æ³¨é‡Šçš„å‡ ä½•å˜æ¢å’ŒMedSAMæ¨¡å‹ç»“æœæ¥ç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æ¡†ã€å‰æ™¯å’ŒèƒŒæ™¯æ ‡ç­¾ã€‚æˆ‘ä»¬çš„é«˜ç†æ€§å­¦ä¹ ç­–ç•¥åŒ…æ‹¬ï¼š1ï¼‰å¯¹é½æŸå¤±ï¼Œæµ‹é‡åˆ†å‰²ä¸æ¡†æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä»¥åŠå‰æ™¯æ ‡ç­¾å†…çš„æ‹“æ‰‘è¿ç»­æ€§ï¼ŒæŒ‡å¯¼ç½‘ç»œæ„ŸçŸ¥ç»“èŠ‚ä½ç½®ï¼›2ï¼‰å¯¹æ¯”æŸå¤±ï¼Œä»æ ‡è®°çš„å‰æ™¯åŒºåŸŸä¸­æå–ç‰¹å¾ï¼ŒåŒæ—¶å°†æ ‡è®°çš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸä¸­çš„ç‰¹å¾æ¨å¼€ï¼Œå¼•å¯¼ç½‘ç»œå­¦ä¹ ç»“èŠ‚å’ŒèƒŒæ™¯ç‰¹å¾åˆ†å¸ƒï¼›3ï¼‰åŸå‹å…³è”æŸå¤±ï¼Œæµ‹é‡é€šè¿‡æ¯”è¾ƒç‰¹å¾ä¸å‰æ™¯å’ŒèƒŒæ™¯åŸå‹å¾—å‡ºçš„å…³è”å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œç»†åŒ–ä¸ç¡®å®šåŒºåŸŸä»¥è·å–å‡†ç¡®çš„ç»“èŠ‚è¾¹ç¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨TN3Kå’ŒDDTIæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bluehenglee/MLI-MSC%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bluehenglee/MLI-MSCä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19707v2">PDF</a> 24 pages, 14 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³å¼±ç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²ä¸­ç”²çŠ¶è…ºç»“èŠ‚çš„è¯†åˆ«é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨é«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾å’Œé«˜ç†æ€§çš„æŸå¤±å‡½æ•°ï¼Œæ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆå¤„ç†ç²—æ ‡ç­¾è®­ç»ƒæ•°æ®ï¼Œå¹¶æ•æ‰å¤šçº§åˆ¤åˆ«ç‰¹å¾ã€‚é€šè¿‡èåˆå‡ ä½•å˜æ¢å’Œå››ç‚¹æ³¨é‡Šçš„MedSAMæ¨¡å‹ç»“æœç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æ ‡ç­¾ï¼Œå¹¶è®¾è®¡äº†å¯¹é½æŸå¤±ã€å¯¹æ¯”æŸå¤±å’ŒåŸå‹å…³è”æŸå¤±ï¼Œä»¥æŒ‡å¯¼ç½‘ç»œæ„ŸçŸ¥ç»“èŠ‚ä½ç½®ã€å­¦ä¹ ç»“èŠ‚å’ŒèƒŒæ™¯ç‰¹å¾åˆ†å¸ƒï¼Œå¹¶ä¼˜åŒ–ä¸ç¡®å®šåŒºåŸŸè‡³å‡†ç¡®çš„ç»“èŠ‚è¾¹ç¼˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨TN3Kå’ŒDDTIæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•åˆ©ç”¨ç²—æ ‡ç­¾æ•°æ®åœ¨è¶…å£°å›¾åƒä¸­æœ‰æ•ˆå‹¾å‹’ç”²çŠ¶è…ºç»“èŠ‚ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ä½ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾å¼•å…¥çš„æ ‡ç­¾å™ªå£°é—®é¢˜ä»¥åŠåˆšæ€§å¯¹æ¯”åˆ†å‰²ä¸æ ‡ç­¾çš„æŸå¤±å‡½æ•°å¿½ç•¥ç»“èŠ‚å¤æ‚å½¢çŠ¶ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä½¿ç”¨é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ä»£è¡¨æ‹“æ‰‘å’Œè§£å‰–ä¿¡æ¯ï¼Œèåˆå‡ ä½•å˜æ¢å’Œç‰¹å®šæ³¨é‡Šç”Ÿæˆé«˜ç½®ä¿¡åº¦æ ‡ç­¾ã€‚</li>
<li>è®¾è®¡é«˜ç†æ€§æŸå¤±ç­–ç•¥ï¼ŒåŒ…æ‹¬å¯¹é½æŸå¤±ä»¥æŒ‡å¯¼ç½‘ç»œæ„ŸçŸ¥ç»“èŠ‚ä½ç½®ï¼Œå¯¹æ¯”æŸå¤±ä½¿ç½‘ç»œå­¦ä¹ ç»“èŠ‚å’ŒèƒŒæ™¯ç‰¹å¾åˆ†å¸ƒï¼Œä»¥åŠåŸå‹å…³è”æŸå¤±ä»¥ä¼˜åŒ–ä¸ç¡®å®šåŒºåŸŸè‡³å‡†ç¡®è¾¹ç¼˜ã€‚</li>
<li>æ–¹æ³•åœ¨TN3Kå’ŒDDTIæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æä¾›å®éªŒä»£ç é“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5728544dd0681618843db59e35274849.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4efbeb5e3f62724409021dff99148ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62442d0a3d71e14e828e2d9bc31e34bf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ConceptAttention-Diffusion-Transformers-Learn-Highly-Interpretable-Features"><a href="#ConceptAttention-Diffusion-Transformers-Learn-Highly-Interpretable-Features" class="headerlink" title="ConceptAttention: Diffusion Transformers Learn Highly Interpretable   Features"></a>ConceptAttention: Diffusion Transformers Learn Highly Interpretable   Features</h2><p><strong>Authors:Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, Duen Horng Chau</strong></p>
<p>Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention maps. ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 15 other zero-shot interpretability methods on the ImageNet-Segmentation dataset. ConceptAttention works for popular image models and even seamlessly generalizes to video generation. Our work contributes the first evidence that the representations of multi-modal DiTs are highly transferable to vision tasks like segmentation. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ‰©æ•£transformerï¼ˆDiTï¼‰çš„ä¸°å¯Œè¡¨ç¤ºæ˜¯å¦å…·æœ‰å¢å¼ºå…¶å¯è§£é‡Šæ€§çš„ç‹¬ç‰¹å±æ€§ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºConceptAttentionçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„è¡¨ç°åŠ›æ¥ç”Ÿæˆé«˜è´¨é‡çš„æ˜¾è‘—æ€§åœ°å›¾ï¼Œç²¾ç¡®åœ°åœ¨å›¾åƒä¸­å®šä½æ–‡æœ¬æ¦‚å¿µã€‚ConceptAttentionä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå®ƒå¯ä»¥é‡æ–°åˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„å‚æ•°æ¥ç”Ÿæˆé«˜åº¦ä¸Šä¸‹æ–‡åŒ–çš„æ¦‚å¿µåµŒå…¥ï¼Œå¹¶åšå‡ºé‡å¤§å‘ç°ï¼šåœ¨DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´ä¸­è¿›è¡Œçº¿æ€§æŠ•å½±ä¼šäº§ç”Ÿæ¯”å¸¸ç”¨çš„äº¤å‰æ³¨æ„åŠ›å›¾æ›´æ¸…æ™°çš„æ˜¾è‘—æ€§åœ°å›¾ã€‚ConceptAttentionç”šè‡³åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ImageNet-Segmentationæ•°æ®é›†ä¸Šè¶…è¶Šäº†å…¶ä»–15ç§é›¶æ ·æœ¬å¯è§£é‡Šæ€§æ–¹æ³•ã€‚ConceptAttentioné€‚ç”¨äºæµè¡Œçš„å›¾åƒæ¨¡å‹ï¼Œç”šè‡³å¯ä»¥æ— ç¼åœ°æ¨å¹¿åˆ°è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡è¯æ˜å¤šæ¨¡æ€DiTçš„è¡¨ç¤ºé«˜åº¦é€‚ç”¨äºåˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04320v2">PDF</a> Oral Presentation at ICML 2025, Best Paper Award at CVPR Workshop on   Visual Concepts</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºConceptAttentionçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„è¡¨è¾¾èƒ½åŠ›å¼ºçš„æ³¨æ„åŠ›å±‚ç”Ÿæˆé«˜è´¨é‡çš„æ˜¾è‘—æ€§åœ°å›¾ï¼Œç²¾ç¡®å®šä½å›¾åƒä¸­çš„æ–‡æœ¬æ¦‚å¿µã€‚é€šè¿‡æ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½åœ¨DiTæ³¨æ„åŠ›å±‚ç”Ÿæˆé«˜åº¦è¯­å¢ƒåŒ–çš„æ¦‚å¿µåµŒå…¥ï¼Œä¸»è¦å‘ç°å¯¹DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´è¿›è¡Œçº¿æ€§æŠ•å½±å¯ä»¥äº§ç”Ÿæ›´æ¸…æ™°æ˜¾è‘—æ€§åœ°å›¾ã€‚ConceptAttentionåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ImageNet-Segmentationæ•°æ®é›†ä¸Šè¶…è¶Šäº†å…¶ä»–15ç§é›¶æ ·æœ¬å¯è§£é‡Šæ€§æ–¹æ³•ã€‚ConceptAttentioné€‚ç”¨äºæµè¡Œçš„å›¾åƒæ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿæ— ç¼æ¨å¹¿åˆ°è§†é¢‘ç”Ÿæˆä¸­ã€‚æœ¬æ–‡é¦–æ¬¡è¯æ˜äº†å¤šæ¨¡æ€DiTçš„è¡¨å¾é«˜åº¦é€‚ç”¨äºåˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ConceptAttentionåˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„ä¸°å¯Œè¡¨ç¤ºç”Ÿæˆç²¾ç¡®çš„å®šä½æ–‡æœ¬æ¦‚å¿µçš„æ˜¾è‘—æ€§åœ°å›¾ã€‚</li>
<li>ConceptAttentioné€šè¿‡æ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½ä½¿ç”¨DiTå‚æ•°äº§ç”Ÿé«˜åº¦è¯­å¢ƒåŒ–çš„æ¦‚å¿µåµŒå…¥ã€‚</li>
<li>å¯¹DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´è¿›è¡Œçº¿æ€§æŠ•å½±å¯ä»¥äº§ç”Ÿæ›´æ¸…æ™°çš„æ˜¾è‘—æ€§åœ°å›¾ã€‚</li>
<li>ConceptAttentionåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚</li>
<li>ConceptAttentioné€‚ç”¨äºå¤šç§å›¾åƒæ¨¡å‹ï¼Œå¹¶å¯ä»¥æ¨å¹¿åˆ°è§†é¢‘ç”Ÿæˆä¸­ã€‚</li>
<li>å¤šæ¨¡æ€DiTçš„è¡¨å¾é«˜åº¦é€‚ç”¨äºåˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aea825a06e5a89357e1012239b9b4704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bc27d786d5bc22520a0a44efec30ede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da3c6c4afd18ea34db3226b858d190ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-add5500a3fbec4b4f21e166cf4dd9d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23a9261de88ccd77add882051c22adad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c966e3a5a962463e832f298398c2c1fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588ab4e0a8363c43f0238cfe1b6ebbe2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CAD-Editor-A-Locate-then-Infill-Framework-with-Automated-Training-Data-Synthesis-for-Text-Based-CAD-Editing"><a href="#CAD-Editor-A-Locate-then-Infill-Framework-with-Automated-Training-Data-Synthesis-for-Text-Based-CAD-Editing" class="headerlink" title="CAD-Editor: A Locate-then-Infill Framework with Automated Training Data   Synthesis for Text-Based CAD Editing"></a>CAD-Editor: A Locate-then-Infill Framework with Automated Training Data   Synthesis for Text-Based CAD Editing</h2><p><strong>Authors:Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian</strong></p>
<p>Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively. The code is available at \url {<a target="_blank" rel="noopener" href="https://github.com/microsoft/CAD-Editor%7D">https://github.com/microsoft/CAD-Editor}</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å„è¡Œå„ä¸šä¸­éƒ½æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘èƒ½å¤Ÿè‡ªåŠ¨æ ¹æ®æ–‡æœ¬æŒ‡ä»¤ä¿®æ”¹CADæ¨¡å‹ï¼Œè¿™ä¸€æŠ€æœ¯æ½œåŠ›å·¨å¤§ï¼Œä½†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å˜ä½“ç”Ÿæˆæˆ–åŸºäºæ–‡æœ¬çš„CADç”Ÿæˆä¸Šï¼Œå®ƒä»¬è¦ä¹ˆä¸æ”¯æŒåŸºäºæ–‡æœ¬çš„æ§åˆ¶ï¼Œè¦ä¹ˆå¿½è§†äº†ç°æœ‰çš„CADæ¨¡å‹ä½œä¸ºçº¦æŸã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘é¦–ä¸ªæ¡†æ¶â€”â€”CAD-Editorã€‚ä¸ºè§£å†³è®­ç»ƒæ‰€éœ€çš„ä¸‰å…ƒç»„æ•°æ®å¯¹åº”ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ã€‚è¯¥ç®¡é“åˆ©ç”¨è®¾è®¡å˜ä½“æ¨¡å‹ç”ŸæˆåŸå§‹å’Œç¼–è¾‘åçš„CADæ¨¡å‹å¯¹ï¼Œå¹¶åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å°†å…¶å·®å¼‚æ€»ç»“ä¸ºç¼–è¾‘æŒ‡ä»¤ã€‚ä¸ºè§£å†³åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘çš„å¤åˆæ€§è´¨ï¼Œæˆ‘ä»¬æå‡ºäº†å…ˆå®šä½åå¡«å……æ¡†æ¶ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæœ‰é’ˆå¯¹æ€§çš„å­ä»»åŠ¡ï¼šå®šä½éœ€è¦ä¿®æ”¹çš„åŒºåŸŸï¼Œå¹¶ç”¨é€‚å½“çš„ç¼–è¾‘å¡«å……è¿™äº›åŒºåŸŸã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¿™ä¸¤ä¸ªå­ä»»åŠ¡çš„åç›¾ï¼Œåˆ©ç”¨å…¶åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’ŒCADçŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-Editoråœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/CAD-Editor%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/microsoft/CAD-Editorä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03997v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„ç¼–è¾‘æŠ€æœ¯ï¼Œä»‹ç»äº†æ–‡æœ¬åŸºç¡€ä¸Šçš„CADç¼–è¾‘çš„é‡è¦æ€§å’Œç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚æå‡ºäº†CAD-Editoræ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“è§£å†³è®­ç»ƒæ•°æ®å¯¹åº”é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡Œç¼–è¾‘æŒ‡ä»¤çš„æ€»ç»“ã€‚æ¡†æ¶é‡‡ç”¨å®šä½åå¡«å……çš„ç­–ç•¥è§£å†³åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘çš„å¤åˆæ€§è´¨é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºä¸¤ä¸ªå­ä»»åŠ¡çš„æ”¯æŸ±ã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-Editoråœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç¼–è¾‘çš„é‡è¦æ€§åŠå…¶ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>ä»‹ç»CAD-Editoræ¡†æ¶ç”¨äºæ–‡æœ¬åŸºç¡€çš„CADç¼–è¾‘ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“è§£å†³è®­ç»ƒæ•°æ®å¯¹åº”é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ€»ç»“ç¼–è¾‘æŒ‡ä»¤ã€‚</li>
<li>CAD-Editoré‡‡ç”¨å®šä½åå¡«å……ç­–ç•¥è§£å†³åŸºäºæ–‡æœ¬çš„CADç¼–è¾‘çš„å¤åˆæ€§è´¨é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºå­ä»»åŠ¡çš„æ”¯æŸ±ï¼Œåˆ©ç”¨å…¶è‡ªç„¶è¯­è¨€ç†è§£å’ŒCADçŸ¥è¯†ã€‚</li>
<li>CAD-Editoråœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å–å¾—äº†å“è¶Šçš„å®éªŒæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75b046b42a495c4bec3d41b1498d6c8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6006cc1a109b40aa49d82a2f32279863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18949362867e0b79df127b7aca420eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9676f22f9299d35782a407a3efbd5db9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdfc03f91f9948c67cb75bbab01d06c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c257e00d3d562e9166ff0ba037c5b7f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention"><a href="#Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention" class="headerlink" title="Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention"></a>Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</h2><p><strong>Authors:Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han</strong></p>
<p>Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡ç­¾çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œé¥æ„Ÿå›¾åƒå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾å’Œé«˜çš„ç±»é—´ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å¤šå°ºåº¦ä¸ç¡®å®šæ€§åŠäº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›ï¼ˆMUCAï¼‰æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚å…·ä½“è€Œè¨€ï¼ŒMUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œçº¦æŸç½‘ç»œä¸åŒå±‚ç‰¹å¾å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®ƒæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œçš„äº’è¡¥ç‰¹å¾ï¼Œå¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚è¯¥è®¾è®¡æœ‰æ•ˆåœ°ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼ˆWAå’ŒSAï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼å¯¹è±¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10736v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£å­¦ä¹ ä¸ºè§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡æ³¨è´Ÿæ‹…æä¾›äº†æœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åŠç›‘ç£Multi-Scale Uncertainty and Cross-Teacher-Student Attentionï¼ˆMUCAï¼‰æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°åŠç›‘ç£æ–¹æ³•ï¼Œå°¤å…¶åœ¨é«˜ç›¸ä¼¼åº¦ç‰©ä½“åŒºåˆ†ä¸Šè¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ æ˜¯è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åŠ³åŠ¨å¯†é›†å‹åƒç´ çº§æ ‡æ³¨è´Ÿæ‹…çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŠç›‘ç£MUCAæ¨¡å‹ï¼Œé€‚ç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>MUCAæ¨¡å‹é€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MUCAæ¨¡å‹åˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>MUCAæ¨¡å‹ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼Œè¿›ä¸€æ­¥æå‡äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†MUCAæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac237e5f98558cdf40e112c011a60874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c5331caa60f7e8d315d9de686cebf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7c03d48be5bd01f423539faf724de64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc037fe583e31dd05abb88309799578b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34995eae5323ebc1f8e4f519cd1ddb64.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT"><a href="#Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT" class="headerlink" title="Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT"></a>Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT</h2><p><strong>Authors:Zi Li, Ying Chen, Zeli Chen, Yanzhou Su, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Yunhai Bai, Zhinlin Zheng, Le Lu, Yirui Wang, Jia Ge, Xianghua Ye, Senxiang Yan, Dakai Jin</strong></p>
<p>In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues necessitates that radiation oncologists manually delineate the tumors, often relying on diagnostic MRI for guidance. % In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor segmentation (SATs) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of features to semantic asymmetries. % Extensive experiments demonstrate that the proposed SATs achieves the leading NPC GTV segmentation performance in both internal and external testing, \emph{e.g.}, with at least 2% absolute Dice score improvement and 12% average distance error reduction when compared to other state-of-the-art methods in the external testing. </p>
<blockquote>
<p>åœ¨é¼»å’½ç™Œï¼ˆNPCï¼‰çš„æ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œä¸´åºŠåŒ»ç”Ÿé€šå¸¸ä½¿ç”¨éå¯¹æ¯”è®¡åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ¥ç²¾ç¡®æç»˜å¤§ä½“è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¾å°„å‰‚é‡ä¼ é€’ã€‚ç„¶è€Œï¼Œè‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¿«ä½¿æ”¾ç–—ç§‘åŒ»ç”Ÿæ‰‹åŠ¨æç»˜è‚¿ç˜¤ï¼Œé€šå¸¸ä¾èµ–è¯Šæ–­æ€§ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œå¼•å¯¼ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨éå¯¹æ¯”è®¡åˆ’CTå›¾åƒä¸Šåˆ†å‰²é¼»å’½ç™Œçš„å¤§ä½“è‚¿ç˜¤ï¼Œé¿å…äº†å°†MRIæˆ–MRIè¡ç”Ÿçš„è‚¿ç˜¤æ©è†œä¸è§„åˆ’CTè¿›è¡Œé…å‡†æ—¶å¯èƒ½å‡ºç°çš„æ½œåœ¨é…å‡†è¯¯å·®ã€‚ä¸ºäº†è§£å†³è§„åˆ’CTä¸­è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»“æ„ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†3Dè¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¤ä¸ºå¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰å…¸å‹çš„åŒä¾§å¯¹ç§°æ€§ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–åŸå§‹å’Œç¿»è½¬åŒºåŸŸçš„éè‚¿ç˜¤éƒ¨ä½çš„ä½“ç´ é—´è·ç¦»æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶é¼“åŠ±åŸå§‹å’Œå¸¦æœ‰è‚¿ç˜¤çš„ç¿»è½¬åŒºåŸŸä¹‹é—´çš„è·ç¦»æœ€å¤§åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹è¯­ä¹‰ä¸å¯¹ç§°æ€§çš„ç‰¹å¾æ•æ„Ÿæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SATsåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ï¼Œä¾‹å¦‚ä¸å¤–éƒ¨æµ‹è¯•ä¸­çš„å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè‡³å°‘æé«˜äº†2ï¼…çš„ç»å¯¹Diceå¾—åˆ†å’Œé™ä½äº†12ï¼…çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18290v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹é¼»å’½ç™Œæ”¾å°„æ²»ç–—çš„å½±åƒç ”ç©¶æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ã€‚æ­¤æ–¹æ³•èƒ½ç›´æ¥åœ¨éå¯¹æ¯”åº¦è§„åˆ’çš„è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒä¸­åˆ’åˆ†é¼»å’½ç™Œè‚¿ç˜¤ä½“ç§¯ï¼Œä»è€Œé¿å…MRIæˆ–MRIè¡ç”Ÿçš„è‚¿ç˜¤æ©è†œä¸è§„åˆ’CTå›¾åƒå¯¹é½æ—¶å¯èƒ½å‡ºç°çš„æ³¨å†Œè¯¯å·®ã€‚é€šè¿‡å¼•å…¥ä¸‰ç»´è¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•æ¥è§£å†³è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»“æ„ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„é¼»å’½ç™ŒGTVåˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¼»å’½ç™Œæ”¾å°„æ²»ç–—é€šå¸¸ä½¿ç”¨éå¯¹æ¯”åº¦è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«ææ¥æç»˜è‚¿ç˜¤ä½“ç§¯ã€‚</li>
<li>ç”±äºè‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦ä½ï¼Œé€šå¸¸éœ€è¦æ”¾å°„è‚¿ç˜¤å­¦å®¶æ‰‹åŠ¨æç»˜è‚¿ç˜¤ï¼Œå¹¶ç»å¸¸ä¾èµ–è¯Šæ–­MRIè¿›è¡Œå¼•å¯¼ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥å¯¹éå¯¹æ¯”åº¦è§„åˆ’CTå›¾åƒä¸­çš„é¼»å’½ç™Œè‚¿ç˜¤è¿›è¡Œåˆ†å‰²çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥ä¸‰ç»´è¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ¥è§£å†³è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»“æ„ä¹‹é—´çš„å¯¹æ¯”åº¦é—®é¢˜ã€‚</li>
<li>SATsæ–¹æ³•åˆ©ç”¨å¥åº·é¼»å’½åŒºåŸŸçš„åŒä¾§å¯¹ç§°æ€§ç‰¹å¾ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡æœ€å°åŒ–åŸå§‹å’Œç¿»è½¬åŒºåŸŸçš„ä½“ç´ è·ç¦»æ¥å¢å¼ºç‰¹å¾å¯¹è¯­ä¹‰ä¸å¯¹ç§°çš„æ•æ„Ÿæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d886c414ddad89946e4dd15634c7637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ec48313fe6dfe79a08c7af7994c88df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-529c18e271b61d703b3713ce277c75eb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Genetic-algorithm-as-a-tool-for-detection-setup-optimisation-SiFi-CC-case-study"><a href="#Genetic-algorithm-as-a-tool-for-detection-setup-optimisation-SiFi-CC-case-study" class="headerlink" title="Genetic algorithm as a tool for detection setup optimisation: SiFi-CC   case study"></a>Genetic algorithm as a tool for detection setup optimisation: SiFi-CC   case study</h2><p><strong>Authors:Jonas Kasper, Awal Awal, Ronja Hetzel, Magdalena KoÅ‚odziej, Katarzyna Rusiecka, Achim Stahl, Ming-Liang Wong, Aleksandra WroÅ„ska</strong></p>
<p>Objective: Proton therapy is a precision-focused cancer treatment where accurate proton beam range monitoring is critical to ensure effective dose delivery. This can be achieved by prompt gamma detection with a Compton camera like the SiFi-CC. This study aims to show the feasibility of optimising the geometry of SiFi-CC Compton camera for verification of dose distribution via prompt gamma detection using a genetic algorithm (GA). Approach: The SiFi-CC key geometric parameters for optimisation with the GA are the source-to-scatterer and scatterer-to-absorber distances, and the module thicknesses. The optimisation process was conducted with a software framework based on the Geant4 toolkit, which included detailed and realistic modelling of gamma interactions, detector response, and further steps such as event selection and image reconstruction. The performance of each individual configuration was evaluated using a fitness function incorporating factors related to gamma detection efficiency and image resolution. Results: The GA-optimised SiFi-CC configuration demonstrated the capability to detect a 5 mm proton beam range shift with a 2 mm resolution using 5e8 protons. The best-performing geometry, with 16 fibre layers in the scatterer, 36 layers in the absorber, source-to-scatterer distance 150 mm and scatterer-to-absorber distance 120 mm, has an imaging sensitivity of 5.58(1)e-5. Significance: This study demonstrates that the SiFi-CC setup, optimised through a GA, can reliably detect clinically relevant proton beam range shifts, improving real-time range verification accuracy in proton therapy. The presented implementation of a GA is a systematic and feasible way of searching for a SiFi-CC geometry that shows the best performance. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè´¨å­ç–—æ³•æ˜¯ä¸€ç§ç²¾å‡†ç™Œç—‡æ²»ç–—æ–¹æ³•ï¼Œå…¶ä¸­å‡†ç¡®çš„è´¨å­æŸèŒƒå›´ç›‘æµ‹å¯¹äºç¡®ä¿æœ‰æ•ˆå‰‚é‡ä¼ é€’è‡³å…³é‡è¦ã€‚è¿™å¯ä»¥é€šè¿‡ä½¿ç”¨SiFi-CCç­‰åº·æ™®é¡¿ç›¸æœºè¿›è¡Œå³æ—¶ä¼½é©¬æ£€æµ‹æ¥å®ç°ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å±•ç¤ºä½¿ç”¨é—ä¼ ç®—æ³•ï¼ˆGAï¼‰ä¼˜åŒ–SiFi-CCåº·æ™®é¡¿ç›¸æœºå‡ ä½•ç»“æ„ï¼Œé€šè¿‡å³æ—¶ä¼½é©¬æ£€æµ‹éªŒè¯å‰‚é‡åˆ†å¸ƒçš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šä½¿ç”¨é—ä¼ ç®—æ³•è¿›è¡Œä¼˜åŒ–çš„SiFi-CCå…³é”®å‡ ä½•å‚æ•°åŒ…æ‹¬æºåˆ°æ•£å°„å™¨å’Œæ•£å°„å™¨åˆ°å¸æ”¶å™¨çš„è·ç¦»ä»¥åŠæ¨¡å—åšåº¦ã€‚ä¼˜åŒ–è¿‡ç¨‹æ˜¯åœ¨åŸºäºGeant4å·¥å…·åŒ…çš„è½¯ä»¶æ¡†æ¶ä¸­è¿›è¡Œçš„ï¼Œå…¶ä¸­åŒ…æ‹¬ä¼½é©¬ç›¸äº’ä½œç”¨ã€æ¢æµ‹å™¨å“åº”çš„è¯¦ç»†å’ŒçœŸå®å»ºæ¨¡ï¼Œä»¥åŠäº‹ä»¶é€‰æ‹©å’Œå›¾åƒé‡å»ºç­‰è¿›ä¸€æ­¥æ­¥éª¤ã€‚ä½¿ç”¨çº³å…¥ä¸ä¼½é©¬æ£€æµ‹æ•ˆç‡å’Œå›¾åƒåˆ†è¾¨ç‡ç›¸å…³å› ç´ çš„é€‚åº”åº¦å‡½æ•°æ¥è¯„ä¼°æ¯ç§é…ç½®çš„æ€§èƒ½ã€‚</p>
<p>ç»“æœï¼šç»è¿‡é—ä¼ ç®—æ³•ä¼˜åŒ–çš„SiFi-CCé…ç½®èƒ½å¤Ÿä½¿ç”¨5e8ä¸ªè´¨å­ä»¥2mmçš„åˆ†è¾¨ç‡æ£€æµ‹å‡º5mmçš„è´¨å­æŸèŒƒå›´åç§»ã€‚è¡¨ç°æœ€ä½³çš„å‡ ä½•ç»“æ„å…·æœ‰æ•£å°„å™¨ä¸­çš„16å±‚çº¤ç»´ã€å¸æ”¶å™¨ä¸­çš„36å±‚ã€æºåˆ°æ•£å°„å™¨çš„è·ç¦»ä¸º150mmä»¥åŠæ•£å°„å™¨åˆ°å¸æ”¶å™¨çš„è·ç¦»ä¸º120mmï¼Œå…¶æˆåƒçµæ•åº¦ä¸º5.58ï¼ˆ1ï¼‰e-5ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18239v2">PDF</a> 10 figures, 3 tables</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨é—ä¼ ç®—æ³•ï¼ˆGAï¼‰ä¼˜åŒ–SiFi-CCåº·æ™®é¡¿ç›¸æœºå‡ ä½•ç»“æ„ï¼Œé€šè¿‡å³æ—¶ä¼½é©¬æ£€æµ‹éªŒè¯å‰‚é‡åˆ†å¸ƒã€‚ä¼˜åŒ–å‚æ•°åŒ…æ‹¬æºæ•£å°„å™¨åŠæ•£å°„å™¨è‡³å¸æ”¶å™¨çš„è·ç¦»å’Œæ¨¡å—åšåº¦ã€‚ç»Geant4å·¥å…·åŒ…è¿›è¡Œè¯¦å°½æ¨¡æ‹Ÿï¼Œç»“æœæ˜¾ç¤ºä¼˜åŒ–åçš„SiFi-CCå¯åœ¨ä½¿ç”¨5e8è´¨å­æ—¶æ£€æµ‹åˆ°5æ¯«ç±³è´¨å­æŸèŒƒå›´åç§»ï¼Œåˆ†è¾¨ç‡è¾¾2æ¯«ç±³ã€‚æœ€ä½³å‡ ä½•ç»“æ„å…·æœ‰16å±‚æ•£å°„å™¨å’Œ36å±‚å¸æ”¶å™¨ï¼Œæºæ•£å°„å™¨è·ç¦»ä¸º150æ¯«ç±³ï¼Œæ•£å°„å™¨è‡³å¸æ”¶å™¨è·ç¦»ä¸º120æ¯«ç±³ï¼Œæˆåƒçµæ•åº¦ä¸º5.58ï¼ˆ1ï¼‰e-5ã€‚è¯¥ç ”ç©¶è¯å®ï¼Œé€šè¿‡é—ä¼ ç®—æ³•ä¼˜åŒ–çš„SiFi-CCå¯å‡†ç¡®æ£€æµ‹ä¸´åºŠç›¸å…³çš„è´¨å­æŸèŒƒå›´åç§»ï¼Œæé«˜è´¨å­ç–—æ³•ä¸­çš„å®æ—¶èŒƒå›´éªŒè¯ç²¾åº¦ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨é—ä¼ ç®—æ³•ä¼˜åŒ–SiFi-CCåº·æ™®é¡¿ç›¸æœºçš„å‡ ä½•ç»“æ„ã€‚</li>
<li>ä¼˜åŒ–å‚æ•°åŒ…æ‹¬æºè‡³æ•£å°„å™¨ã€æ•£å°„å™¨è‡³å¸æ”¶å™¨çš„è·ç¦»ä»¥åŠæ¨¡å—åšåº¦ã€‚</li>
<li>åˆ©ç”¨åŸºäºGeant4å·¥å…·åŒ…çš„è½¯ä»¶æ¡†æ¶è¿›è¡Œè¯¦ç»†ä¸”ç°å®çš„ä¼½é©¬ç›¸äº’ä½œç”¨ã€æ¢æµ‹å™¨å“åº”ç­‰æ¨¡æ‹Ÿã€‚</li>
<li>é—ä¼ ç®—æ³•ä¼˜åŒ–çš„SiFi-CCé…ç½®å¯æ£€æµ‹åˆ°5æ¯«ç±³çš„è´¨å­æŸèŒƒå›´åç§»ï¼Œåˆ†è¾¨ç‡è¾¾åˆ°2æ¯«ç±³ã€‚</li>
<li>æœ€ä½³è¡¨ç°çš„å‡ ä½•ç»“æ„å‚æ•°å…·ä½“ä¸ºï¼šæ•£å°„å™¨æœ‰16å±‚ï¼Œå¸æ”¶å™¨æœ‰36å±‚ï¼Œæºæ•£å°„å™¨è·ç¦»ä¸º150æ¯«ç±³ï¼Œæ•£å°„å™¨è‡³å¸æ”¶å™¨è·ç¦»ä¸º120æ¯«ç±³ã€‚</li>
<li>è¯¥ç ”ç©¶éªŒè¯äº†ä¼˜åŒ–åçš„SiFi-CCåœ¨è´¨å­ç–—æ³•ä¸­å®æ—¶èŒƒå›´éªŒè¯çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-950052dda3712792343d882006403c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0226f42d779c98c627d22a3734284560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4928d250d925d1eaa55035b353fc874c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d76e09a66b8d3af02b22c4f877f985c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b23dd83964481dd5d0e9559c6bc0e97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3de26586dd3869df66e34d9667dca697.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Segment-as-You-Wish-â€“-Free-Form-Language-Based-Segmentation-for-Medical-Images"><a href="#Segment-as-You-Wish-â€“-Free-Form-Language-Based-Segmentation-for-Medical-Images" class="headerlink" title="Segment as You Wish â€“ Free-Form Language-Based Segmentation for Medical   Images"></a>Segment as You Wish â€“ Free-Form Language-Based Segmentation for Medical   Images</h2><p><strong>Authors:Longchao Da, Rui Wang, Xiaojian Xu, Parminder Bhatia, Taha Kass-Hout, Hua Wei, Cao Xiao</strong></p>
<p>Medical imaging is crucial for diagnosing a patientâ€™s health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the modelâ€™s superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒå¯¹äºè¯Šæ–­æ‚£è€…çš„å¥åº·çŠ¶å†µè‡³å…³é‡è¦ï¼Œè€Œå‡†ç¡®åœ°å¯¹è¿™äº›å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œå¯¹äºéš”ç¦»æ„Ÿå…´è¶£åŒºåŸŸä»¥ç¡®ä¿ç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾¹ç•Œæ¡†æˆ–åŸºäºç‚¹çš„æç¤ºï¼Œå°½ç®¡ä¸´åºŠåŒ»ç”Ÿç»å¸¸ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä»–ä»¬çš„è§‚å¯Ÿå’ŒæŒ‡ç¤ºï¼Œä½†å¾ˆå°‘æœ‰æ–¹æ³•æ¢ç´¢æ–‡æœ¬ç›¸å…³çš„æç¤ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åŸºäºRAGçš„è‡ªç”±å½¢å¼æ–‡æœ¬æç¤ºç”Ÿæˆå™¨ï¼Œå®ƒåˆ©ç”¨é¢†åŸŸè¯­æ–™åº“ç”Ÿæˆå¤šæ ·ä¸”ç°å®çš„æè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†FLanSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§è‡ªç”±å½¢å¼çš„æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬ä¸“ä¸šè§£å‰–ä¿¡æ¯æŸ¥è¯¢ã€è§£å‰–æ— å…³çš„ä½ç½®é©±åŠ¨æŸ¥è¯¢å’Œè§£å‰–æ— å…³çš„å¤§å°é©±åŠ¨æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜èå…¥äº†ä¸€ä¸ªå¯¹ç§°æ„ŸçŸ¥è§„èŒƒåŒ–æ¨¡å—ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ‰«ææ–¹å‘ä¸‹å®ç°ä¸€è‡´ã€å‡†ç¡®çš„åˆ†å‰²ï¼Œå¹¶å‡å°‘å™¨å®˜è§£å‰–ä½ç½®ä¸å…¶åœ¨æ‰«æä¸­å‡ºç°çš„æ··æ·†ã€‚FLanSæ˜¯åœ¨æ¥è‡ª7ä¸ªå…¬å…±æ•°æ®é›†çš„è¶…è¿‡10ä¸‡å¼ åŒ»å­¦å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç»¼åˆå®éªŒè¯æ˜äº†è¯¥æ¨¡å‹å‡ºè‰²çš„è¯­è¨€ç†è§£å’Œåˆ†å‰²ç²¾åº¦ï¼Œä»¥åŠå¯¹ä¸¤è€…å…³ç³»çš„æ·±å…¥ç†è§£ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šéƒ½è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12831v2">PDF</a> 19 pages, 9 as main content. The paper was accepted to KDD2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè¯Šæ–­æ‚£è€…å¥åº·çŠ¶å†µè‡³å…³é‡è¦ã€‚ä¸ºå®ç°æ›´ç²¾ç¡®çš„åˆ†å‰²ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºRAGçš„è‡ªç”±å½¢å¼æ–‡æœ¬æç¤ºç”Ÿæˆå™¨ï¼Œå¹¶ç»“åˆFLanSæ–°å‹åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œå¯å¤„ç†å¤šç§è‡ªç”±å½¢å¼æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬ä¸“ä¸šè§£å‰–å­¦ä¿¡æ¯æŸ¥è¯¢ã€éè§£å‰–å­¦ä½ç½®é©±åŠ¨æŸ¥è¯¢å’Œéè§£å‰–å­¦å°ºå¯¸é©±åŠ¨æŸ¥è¯¢ã€‚è¯¥æ¨¡å‹è¿˜é…å¤‡äº†ä¸€ä¸ªå¯¹ç§°æ„ŸçŸ¥è§„èŒƒåŒ–æ¨¡å—ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ‰«ææ–¹å‘ä¸Šè·å¾—ä¸€è‡´ä¸”ç²¾ç¡®çš„åˆ†å‰²ï¼Œå¹¶å‡å°‘å™¨å®˜è§£å‰–ä½ç½®ä¸å…¶åœ¨æ‰«æä¸­å‡ºç°çš„æ··æ·†ã€‚FLanSåœ¨è¶…è¿‡10ä¸‡å¼ åŒ»å­¦å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®éªŒè¯æ˜å…¶åœ¨è¯­è¨€ç†è§£å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä»¥åŠå¯¹ä¸¤è€…å…³ç³»çš„æ·±åˆ»ç†è§£ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºç²¾ç¡®è¯Šæ–­æ‚£è€…å¥åº·çŠ¶å†µè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è¾¹ç•Œæ¡†æˆ–ç‚¹åŸºæç¤ºï¼Œè¾ƒå°‘æ¢ç´¢æ–‡æœ¬ç›¸å…³æç¤ºã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºRAGçš„è‡ªç”±å½¢å¼æ–‡æœ¬æç¤ºç”Ÿæˆå™¨ï¼Œä»¥ç”Ÿæˆå¤šæ ·ä¸”ç°å®çš„æè¿°ã€‚</li>
<li>å¼•å…¥çš„FLanSæ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§è‡ªç”±å½¢å¼æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬ä¸“ä¸šè§£å‰–å­¦ä¿¡æ¯æŸ¥è¯¢ç­‰ã€‚</li>
<li>FLanSæ¨¡å‹é…å¤‡å¯¹ç§°æ„ŸçŸ¥è§„èŒƒåŒ–æ¨¡å—ï¼Œä»¥ç¡®ä¿ä¸€è‡´çš„åˆ†å‰²ç²¾åº¦å¹¶å‡å°‘è§£å‰–ä½ç½®æ··æ·†ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®éªŒè¯æ˜å…¶è¯­è¨€ç†è§£å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e84e412c2a4ec555828d1acd7dc8cc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cf4d7da24abd02f42f7b50ba0f6e048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54379e4511f2c0ece0e18bf01492bb73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7adc827d1c1742c756515be0a4c5f5b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8431660ac872d7bc2df9e39af78c4a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdf076d32ac88a3301b3476e7d596951.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-913e399ca3527dd47d6feb49ff889d86.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-08  TTRL Test-Time Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-cce2ff1466f9e978a0ab1daaf67a66a3.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-07  REMOR Automated Peer Review Generation with LLM Reasoning and   Multi-Objective Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
