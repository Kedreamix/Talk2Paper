<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  NAUTILUS A Large Multimodal Model for Underwater Scene Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-366da6853bec5e8188a1ade9dca3abed')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="NAUTILUS-A-Large-Multimodal-Model-for-Underwater-Scene-Understanding"><a href="#NAUTILUS-A-Large-Multimodal-Model-for-Underwater-Scene-Understanding" class="headerlink" title="NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding"></a>NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding</h2><p><strong>Authors:Wei Xu, Cheng Wang, Dingkang Liang, Zongchuang Zhao, Xingyu Jiang, Peng Zhang, Xiang Bai</strong></p>
<p>Underwater exploration offers critical insights into our planet and attracts increasing attention for its broader applications in resource exploration, national security, etc. We study the underwater scene understanding methods, which aim to achieve automated underwater exploration. The underwater scene understanding task demands multi-task perceptions from multiple granularities. However, the absence of large-scale underwater multi-task instruction-tuning datasets hinders the progress of this research. To bridge this gap, we construct NautData, a dataset containing 1.45 M image-text pairs supporting eight underwater scene understanding tasks. It enables the development and thorough evaluation of the underwater scene understanding models. Underwater image degradation is a widely recognized challenge that interferes with underwater tasks. To improve the robustness of underwater scene understanding, we introduce physical priors derived from underwater imaging models and propose a plug-and-play vision feature enhancement (VFE) module, which explicitly restores clear underwater information. We integrate this module into renowned baselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS. Experiments conducted on the NautData and public underwater datasets demonstrate the effectiveness of the VFE module, consistently improving the performance of both baselines on the majority of supported tasks, thus ensuring the superiority of NAUTILUS in the underwater scene understanding area. Data and models are available at <a target="_blank" rel="noopener" href="https://github.com/H-EmbodVis/NAUTILUS">https://github.com/H-EmbodVis/NAUTILUS</a>. </p>
<blockquote>
<p>æ°´ä¸‹æ¢ç´¢ä¸ºæˆ‘ä»¬çš„åœ°çƒæä¾›äº†å…³é”®è§è§£ï¼Œå¹¶å› å…¶èµ„æºæ¢ç´¢ã€å›½å®¶å®‰å…¨ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨è€Œæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚æˆ‘ä»¬ç ”ç©¶æ°´ä¸‹åœºæ™¯ç†è§£æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°è‡ªåŠ¨åŒ–æ°´ä¸‹æ¢ç´¢ã€‚æ°´ä¸‹åœºæ™¯ç†è§£ä»»åŠ¡éœ€è¦æ¥è‡ªå¤šä¸ªç²’åº¦çš„å¤šä»»åŠ¡æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œç¼ºä¹å¤§è§„æ¨¡çš„æ°´ä¸‹å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œé˜»ç¢äº†è¿™é¡¹ç ”ç©¶çš„è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†NautDataæ•°æ®é›†ï¼ŒåŒ…å«145ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ï¼Œæ”¯æŒæ°´ä¸‹åœºæ™¯ç†è§£çš„å…«ä¸ªä»»åŠ¡ã€‚å®ƒèƒ½å¤Ÿä¿ƒè¿›æ°´ä¸‹åœºæ™¯ç†è§£æ¨¡å‹çš„å¼€å‘å’Œå…¨é¢è¯„ä¼°ã€‚æ°´ä¸‹å›¾åƒé€€åŒ–æ˜¯ä¸€ä¸ªå…¬è®¤çš„æŒ‘æˆ˜ï¼Œä¼šå¹²æ‰°æ°´ä¸‹ä»»åŠ¡ã€‚ä¸ºäº†æé«˜æ°´ä¸‹åœºæ™¯ç†è§£çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¥è‡ªæ°´ä¸‹æˆåƒæ¨¡å‹çš„ç‰©ç†å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è§†è§‰ç‰¹å¾å¢å¼ºï¼ˆVFEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ˜ç¡®æ¢å¤æ¸…æ™°çš„æ°´ä¸‹ä¿¡æ¯ã€‚æˆ‘ä»¬å°†è¯¥æ¨¡å—é›†æˆåˆ°è‘—åçš„åŸºçº¿LLaVA-1.5å’ŒQwen2.5-VLä¸­ï¼Œæ„å»ºäº†æˆ‘ä»¬çš„æ°´ä¸‹LLMæ¨¡å‹NAUTILUSã€‚åœ¨NautDataå’Œå…¬å…±æ°´ä¸‹æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†VFEæ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ¨¡å—åœ¨å¤§å¤šæ•°æ”¯æŒçš„ä»»åŠ¡ä¸Šå‡æé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œç¡®ä¿äº†NAUTILUSåœ¨æ°´ä¸‹åœºæ™¯ç†è§£é¢†åŸŸçš„ä¼˜è¶Šæ€§ã€‚æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/H-EmbodVis/NAUTILUS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/H-EmbodVis/NAUTILUSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27481v1">PDF</a> Accepted to NeurIPS 2025. Data and models are available at   <a target="_blank" rel="noopener" href="https://github.com/H-EmbodVis/NAUTILUS">https://github.com/H-EmbodVis/NAUTILUS</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ°´ä¸‹åœºæ™¯ç†è§£æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°è‡ªåŠ¨åŒ–æ°´ä¸‹æ¢ç´¢ã€‚ç”±äºç¼ºå°‘å¤§è§„æ¨¡çš„æ°´ä¸‹å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ„å»ºäº†NautDataæ•°æ®é›†ï¼ŒåŒ…å«145ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹ï¼Œæ”¯æŒæ°´ä¸‹åœºæ™¯ç†è§£çš„å…«ä¸ªä»»åŠ¡ã€‚ä¸ºäº†æé«˜æ°´ä¸‹åœºæ™¯ç†è§£çš„ç¨³å¥æ€§ï¼Œå¼•å…¥äº†æ°´ä¸‹æˆåƒæ¨¡å‹å¾—å‡ºçš„ç‰©ç†å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æå‡ºäº†å³æ’å³ç”¨çš„è§†è§‰ç‰¹å¾å¢å¼ºï¼ˆVFEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæ˜ç¡®æ¢å¤æ¸…æ™°çš„æ°´ä¸‹ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å—åœ¨å¤§å¤šæ•°æ”¯æŒçš„ä»»åŠ¡ä¸Šéƒ½æé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿äº†NAUTILUSåœ¨æ°´ä¸‹åœºæ™¯ç†è§£é¢†åŸŸçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹æ¢ç´¢å¯¹äºèµ„æºæ¢ç´¢ã€å›½å®¶å®‰å…¨ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œæ°´ä¸‹åœºæ™¯ç†è§£æ˜¯å®ç°è‡ªåŠ¨åŒ–æ°´ä¸‹æ¢ç´¢çš„å…³é”®ã€‚</li>
<li>å½“å‰æ°´ä¸‹åœºæ™¯ç†è§£ç ”ç©¶å—é™äºç¼ºä¹å¤§è§„æ¨¡çš„å¤šä»»åŠ¡æ•°æ®é›†ã€‚</li>
<li>NautDataæ•°æ®é›†çš„å»ºç«‹å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæ”¯æŒæ°´ä¸‹åœºæ™¯ç†è§£çš„å…«ä¸ªä»»åŠ¡ã€‚</li>
<li>æ°´ä¸‹å›¾åƒé€€åŒ–æ˜¯å¹²æ‰°æ°´ä¸‹ä»»åŠ¡çš„ä¸€ä¸ªå…¬è®¤æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥ç‰©ç†å…ˆéªŒçŸ¥è¯†å’Œæ°´ä¸‹æˆåƒæ¨¡å‹æœ‰åŠ©äºæé«˜æ°´ä¸‹åœºæ™¯ç†è§£çš„ç¨³å¥æ€§ã€‚</li>
<li>æå‡ºçš„å³æ’å³ç”¨çš„è§†è§‰ç‰¹å¾å¢å¼ºï¼ˆVFEï¼‰æ¨¡å—èƒ½å¤Ÿæ˜ç¡®æ¢å¤æ¸…æ™°çš„æ°´ä¸‹ä¿¡æ¯ï¼Œæé«˜äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b9d7e70ec73ddd369b5403f36c8ba04" align="middle">
<img src="https://picx.zhimg.com/v2-d1a788f33366b1e0a7e23aa46d329ff9" align="middle">
<img src="https://picx.zhimg.com/v2-21207b419eb033ee61f07a2db46e189a" align="middle">
<img src="https://picx.zhimg.com/v2-392ec402b18f245ef94d97a23f8a15ea" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Un-Attributability-Computing-Novelty-From-Retrieval-Semantic-Similarity"><a href="#Un-Attributability-Computing-Novelty-From-Retrieval-Semantic-Similarity" class="headerlink" title="Un-Attributability: Computing Novelty From Retrieval &amp; Semantic   Similarity"></a>Un-Attributability: Computing Novelty From Retrieval &amp; Semantic   Similarity</h2><p><strong>Authors:Philipp Davydov, Ameya Prabhu, Matthias Bethge, Elisa Nguyen, Seong Joon Oh</strong></p>
<p>Understanding how language-model outputs relate to the pretraining corpus is central to studying model behavior. Most training data attribution (TDA) methods ask which training examples causally influence a given output, often using leave-one-out tests. We invert the question: which outputs cannot be attributed to any pretraining example? We introduce un-attributability as an operational measure of semantic novelty: an output is novel if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around un-attributability enables efficient analysis at pretraining scale. We release ~20 TB of corpus chunks and index artifacts to support replication and large-scale extension of our analysis at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stai-tuebingen/faiss-smollm">https://huggingface.co/datasets/stai-tuebingen/faiss-smollm</a> </p>
<blockquote>
<p>ç†è§£è¯­è¨€æ¨¡å‹çš„è¾“å‡ºä¸é¢„è®­ç»ƒè¯­æ–™åº“ä¹‹é—´çš„å…³ç³»æ˜¯ç ”ç©¶æ¨¡å‹è¡Œä¸ºçš„æ ¸å¿ƒã€‚å¤§å¤šæ•°è®­ç»ƒæ•°æ®å½’å±ï¼ˆTDAï¼‰æ–¹æ³•éƒ½æ˜¯è¯¢é—®å“ªäº›è®­ç»ƒæ ·æœ¬å¯¹ç»™å®šè¾“å‡ºäº§ç”Ÿäº†å› æœå½±å“ï¼Œé€šå¸¸ä½¿ç”¨ç•™ä¸€æ³•æµ‹è¯•ã€‚æˆ‘ä»¬åè¿‡æ¥æå‡ºé—®é¢˜ï¼šå“ªäº›è¾“å‡ºä¸èƒ½å½’å±äºä»»ä½•é¢„è®­ç»ƒæ ·æœ¬ï¼Ÿæˆ‘ä»¬å¼•å…¥ä¸å¯å½’å±æ€§ä½œä¸ºè¯­ä¹‰æ–°é¢–åº¦çš„æ“ä½œåº¦é‡ï¼šå¦‚æœé¢„è®­ç»ƒè¯­æ–™åº“ä¸­ä¸å­˜åœ¨è¯­ä¹‰ä¸Šç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ï¼Œåˆ™è¾“å‡ºè¢«è§†ä¸ºæ–°é¢–çš„ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¸¤é˜¶æ®µæ£€ç´¢æµç¨‹æ¥è¿‘ä¼¼è¿™ä¸€ç‚¹ï¼šä½¿ç”¨è½»é‡çº§çš„GISTåµŒå…¥å¯¹è¯­æ–™åº“è¿›è¡Œç´¢å¼•ï¼Œæ£€ç´¢å‰nä¸ªå€™é€‰è€…ï¼Œç„¶åä½¿ç”¨ColBERTv2è¿›è¡Œé‡æ–°æ’åºã€‚å¦‚æœæœ€è¿‘çš„è¯­æ–™åº“é¡¹æ¯”äººå·¥ç”Ÿæˆçš„æ–‡æœ¬å‚è€ƒæ ·æœ¬çš„å½’å±åº¦ä½ï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹çš„è¾“å‡ºæ˜¯æ–°é¢–çš„ã€‚æˆ‘ä»¬åœ¨SmolLMå’ŒSmolLM2ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šäº†ä¸‰ä¸ªå‘ç°ï¼šï¼ˆ1ï¼‰æ¨¡å‹è°ƒç”¨é¢„è®­ç»ƒæ•°æ®çš„è·¨åº¦æ¯”ä»¥å‰æŠ¥é“çš„è¦é•¿å¾—å¤šï¼›ï¼ˆ2ï¼‰æŸäº›é¢†åŸŸä¼šç³»ç»Ÿåœ°ä¿ƒè¿›æˆ–æŠ‘åˆ¶æ–°é¢–æ€§ï¼›ï¼ˆ3ï¼‰æŒ‡ä»¤è°ƒæ•´ä¸ä»…æ”¹å˜äº†é£æ ¼ï¼Œè¿˜å¢åŠ äº†æ–°é¢–æ€§ã€‚ä»¥ä¸å¯å½’å±æ€§é‡æ–°æ„å»ºæ–°é¢–æ€§è¯„ä¼°ï¼Œèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒè§„æ¨¡ä¸Šè¿›è¡Œé«˜æ•ˆåˆ†æã€‚æˆ‘ä»¬å‘å¸ƒäº†çº¦20TBçš„è¯­æ–™åº“ç‰‡æ®µå’Œç´¢å¼•æ–‡ç‰©ï¼Œä»¥æ”¯æŒæˆ‘ä»¬çš„åˆ†æå¤åˆ¶å’Œå¤§è§„æ¨¡æ‰©å±•ã€‚è¯¦æƒ…è¯·å‚é˜…<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stai-tuebingen/faiss-smollm">https://huggingface.co/datasets/stai-tuebingen/faiss-smollm</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ç ”ç©¶äº†è¯­è¨€æ¨¡å‹è¾“å‡ºä¸é¢„è®­ç»ƒè¯­æ–™åº“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„è¡¡é‡æ ‡å‡†â€”â€”ä¸å¯å½’å› æ€§ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è¯­ä¹‰æ–°é¢–åº¦ã€‚é€šè¿‡å€’æ’ç´¢å¼•å’ŒåŸºäºGISTåµŒå…¥çš„æ£€ç´¢æ–¹æ³•ï¼Œå‘ç°æ¨¡å‹åœ¨é¢„è®­ç»ƒæ•°æ®ä¸Šçš„ä¾èµ–è·¨åº¦æ¯”ä»¥å¾€æŠ¥é“çš„è¦é•¿å¾—å¤šã€‚åŒæ—¶ï¼Œä¸€äº›é¢†åŸŸä¼šç³»ç»Ÿåœ°ä¿ƒè¿›æˆ–æŠ‘åˆ¶æ–°é¢–æ€§ï¼ŒæŒ‡ä»¤å¾®è°ƒä¸ä»…æ”¹å˜é£æ ¼ï¼Œè¿˜å¢åŠ æ–°é¢–æ€§ã€‚é€šè¿‡é‡æ–°æ„å»ºåŸºäºä¸å¯å½’å› æ€§çš„æ–°é¢–æ€§è¯„ä¼°æ–¹æ³•ï¼Œå¯åœ¨é¢„è®­ç»ƒè§„æ¨¡ä¸Šå®ç°é«˜æ•ˆåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬èšç„¦äºç ”ç©¶è¯­è¨€æ¨¡å‹è¾“å‡ºä¸é¢„è®­ç»ƒè¯­æ–™åº“çš„å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¡¡é‡æ ‡å‡†â€”â€”ä¸å¯å½’å› æ€§ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è¯­ä¹‰æ–°é¢–åº¦ã€‚</li>
<li>é€šè¿‡å€’æ’ç´¢å¼•å’ŒåŸºäºGISTåµŒå…¥çš„æ£€ç´¢æ–¹æ³•ï¼Œå‘ç°æ¨¡å‹ä¾èµ–é¢„è®­ç»ƒæ•°æ®çš„è·¨åº¦æ›´é•¿ã€‚</li>
<li>ä¸åŒé¢†åŸŸå¯¹æ¨¡å‹è¾“å‡ºçš„æ–°é¢–æ€§æœ‰ä¸åŒçš„å½±å“ï¼ŒæŸäº›é¢†åŸŸä¼šä¿ƒè¿›æˆ–æŠ‘åˆ¶æ–°é¢–æ€§çš„äº§ç”Ÿã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒä¸ä»…æ”¹å˜æ¨¡å‹é£æ ¼ï¼Œè¿˜èƒ½å¢åŠ è¾“å‡ºçš„æ–°é¢–æ€§ã€‚</li>
<li>é€šè¿‡é‡æ–°æ„å»ºåŸºäºä¸å¯å½’å› æ€§çš„æ–°é¢–æ€§è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥åœ¨é¢„è®­ç»ƒè§„æ¨¡ä¸Šå®ç°é«˜æ•ˆåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72ffc2996f958ada345585cfcba939dc" align="middle">
<img src="https://picx.zhimg.com/v2-6655f9c9ac298678cf30a024868b8ba3" align="middle">
<img src="https://picx.zhimg.com/v2-8339ec40791dabb1a0ee8ce8098c0e3e" align="middle">
<img src="https://picx.zhimg.com/v2-3475575c98d7a8230743a377e740519d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Kimi-Linear-An-Expressive-Efficient-Attention-Architecture"><a href="#Kimi-Linear-An-Expressive-Efficient-Attention-Architecture" class="headerlink" title="Kimi Linear: An Expressive, Efficient Attention Architecture"></a>Kimi Linear: An Expressive, Efficient Attention Architecture</h2><p><strong>Authors: Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du</strong></p>
<p>We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios â€“ including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Kimi Linearï¼Œè¿™æ˜¯ä¸€ç§æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œå®ƒé¦–æ¬¡åœ¨ä¸åŒçš„åœºæ™¯ä¸­ä¸å…¨æ³¨æ„åŠ›è¿›è¡Œäº†å…¬å¹³æ¯”è¾ƒå¹¶è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œè¿™äº›åœºæ™¯åŒ…æ‹¬çŸ­ä¸Šä¸‹æ–‡ã€é•¿ä¸Šä¸‹æ–‡å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§„æ¨¡åˆ¶åº¦ã€‚å…¶æ ¸å¿ƒæ˜¯Kimi Delta Attentionï¼ˆKDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¨ç°åŠ›å¾ˆå¼ºçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒé€šè¿‡æ›´ç²¾ç»†çš„é—¸é—¨æœºåˆ¶æ‰©å±•äº†Gated DeltaNetï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æœ‰é™çŠ¶æ€RNNå†…å­˜ã€‚æˆ‘ä»¬çš„ä¸“ç”¨åˆ†å—ç®—æ³•é€šè¿‡å¯¹è§’çº¿åŠ ä½é˜¶ï¼ˆDPLRï¼‰è½¬æ¢çŸ©é˜µçš„ä¸“ç”¨å˜ä½“å®ç°äº†é«˜ç¡¬ä»¶æ•ˆç‡ï¼Œä¸é€šç”¨DPLRå…¬å¼ç›¸æ¯”ï¼Œè¿™å¤§å¤§é™ä½äº†è®¡ç®—é‡ï¼ŒåŒæ—¶æ›´ç¬¦åˆç»å…¸çš„deltaè§„åˆ™ã€‚æˆ‘ä»¬åŸºäºKDAå’ŒMulti-Head Latent Attentionï¼ˆMLAï¼‰çš„åˆ†å±‚æ··åˆï¼Œé¢„è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰3Bæ¿€æ´»å‚æ•°å’Œ48Bæ€»å‚æ•°çš„Kimi Linearæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç›¸åŒçš„è®­ç»ƒé…æ–¹ï¼ŒKimi Linearåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†å…¨MLAï¼ŒåŒæ—¶å‡å°‘äº†KVç¼“å­˜ä½¿ç”¨é«˜è¾¾75%ï¼Œå¹¶åœ¨1Mä¸Šä¸‹æ–‡ä¸­å®ç°äº†é«˜è¾¾6å€çš„è§£ç ååé‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒKimi Linearå¯ä»¥ä½œä¸ºä¸€ç§é«˜æ€§èƒ½å’Œé«˜æ•ˆçš„æ›¿ä»£å“ï¼Œæ›¿ä»£å…¨æ³¨æ„åŠ›æ¶æ„ï¼ŒåŒ…æ‹¬å…·æœ‰æ›´é•¿è¾“å…¥å’Œè¾“å‡ºé•¿åº¦çš„ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¼€æºäº†KDAå†…æ ¸å’ŒvLLMå®ç°ï¼Œå¹¶å‘å¸ƒäº†é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26692v2">PDF</a> Kimi Linear tech report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Kimi Linearï¼Œä¸€ç§æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œå®ƒåœ¨å„ç§åœºæ™¯ä¸‹é¦–æ¬¡å®ç°äº†å¯¹å…¨æ³¨æ„åŠ›çš„è¶…è¶Šï¼ŒåŒ…æ‹¬çŸ­è¯­å¢ƒã€é•¿è¯­å¢ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ‰©å±•æ¨¡å¼ã€‚å…¶æ ¸å¿ƒæ˜¯Kimi Delta Attentionï¼ˆKDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¨ç°åŠ›æå¼ºçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒé€šè¿‡æ›´ç²¾ç»†çš„ç½‘å…³æœºåˆ¶æ‰©å±•äº†Gated DeltaNetï¼Œä½¿æœ‰é™çš„RNNå†…å­˜å¾—åˆ°æ›´æœ‰æ•ˆçš„åˆ©ç”¨ã€‚é€šè¿‡ç‰¹æ®Šçš„å¯¹è§’åŠ ä½ç§©ï¼ˆDPLRï¼‰è½¬æ¢çŸ©é˜µçš„å˜ä½“ï¼Œå®ç°äº†é«˜ç¡¬ä»¶æ•ˆç‡ï¼Œå‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ç»å…¸Deltaè§„åˆ™çš„æ›´å¤šä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒKimi Linearæ¨¡å‹åœ¨ç›¸åŒè®­ç»ƒé…æ–¹ä¸‹ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†å…¨MLAï¼ŒåŒæ—¶å‡å°‘äº†KVç¼“å­˜ä½¿ç”¨ï¼Œæé«˜äº†è§£ç ååé‡ã€‚ç»“æœè¡¨æ˜ï¼ŒKimi Linearå¯ä½œä¸ºå…¨æ³¨æ„åŠ›æ¶æ„çš„æ›¿ä»£å“ï¼Œå…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºé•¿è¾“å…¥å’Œè¾“å‡ºé•¿åº¦çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kimi Linearæ˜¯ä¸€ç§æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œå¯åœ¨å„ç§åœºæ™¯ä¸‹å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬çŸ­è¯­å¢ƒã€é•¿è¯­å¢ƒå’Œå¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚</li>
<li>æ ¸å¿ƒæ˜¯Kimi Delta Attentionï¼ˆKDAï¼‰æ¨¡å—ï¼Œå®ƒé€šè¿‡ç²¾ç»†çš„ç½‘å…³æœºåˆ¶æ‰©å±•äº†Gated DeltaNetã€‚</li>
<li>å®ç°äº†é«˜ç¡¬ä»¶æ•ˆç‡ï¼Œé€šè¿‡ç‰¹æ®Šçš„å¯¹è§’åŠ ä½ç§©ï¼ˆDPLRï¼‰è½¬æ¢çŸ©é˜µçš„å˜ä½“å‡å°‘äº†è®¡ç®—é‡ã€‚</li>
<li>Kimi Linearæ¨¡å‹åœ¨ç›¸åŒè®­ç»ƒé…æ–¹ä¸‹ï¼Œæ€§èƒ½ä¼˜äºå…¨MLAæ¨¡å‹ã€‚</li>
<li>Kimi Linearå‡å°‘äº†KVç¼“å­˜çš„ä½¿ç”¨ï¼Œå¹¶æé«˜äº†è§£ç ååé‡ã€‚</li>
<li>Kimi Linearç‰¹åˆ«é€‚ç”¨äºé•¿è¾“å…¥å’Œè¾“å‡ºé•¿åº¦çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-316de13b68fa7c0a49b65b61b359fcb5" align="middle">
<img src="https://picx.zhimg.com/v2-366da6853bec5e8188a1ade9dca3abed" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Who-Has-The-Final-Say-Conformity-Dynamics-in-ChatGPTâ€™s-Selections"><a href="#Who-Has-The-Final-Say-Conformity-Dynamics-in-ChatGPTâ€™s-Selections" class="headerlink" title="Who Has The Final Say? Conformity Dynamics in ChatGPTâ€™s Selections"></a>Who Has The Final Say? Conformity Dynamics in ChatGPTâ€™s Selections</h2><p><strong>Authors:Clarissa Sabrina Arlinghaus, Tristan Kenneweg, Barbara Hammer, GÃ¼nter W. Maier</strong></p>
<p>Large language models (LLMs) such as ChatGPT are increasingly integrated into high-stakes decision-making, yet little is known about their susceptibility to social influence. We conducted three preregistered conformity experiments with GPT-4o in a hiring context. In a baseline study, GPT consistently favored the same candidate (Profile C), reported moderate expertise (M &#x3D; 3.01) and high certainty (M &#x3D; 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT faced unanimous opposition from eight simulated partners and almost always conformed (99.9%), reporting lower certainty and significantly elevated self-reported informational and normative conformity (p &lt; .001). In Study 2 (GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of disagreement trials, reporting less certainty and more normative conformity. Across studies, results demonstrate that GPT does not act as an independent observer but adapts to perceived social consensus. These findings highlight risks of treating LLMs as neutral decision aids and underline the need to elicit AI judgments prior to exposing them to human opinions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºé«˜é£é™©å†³ç­–ï¼Œä½†å…³äºå®ƒä»¬å¯¹ç¤¾ä¼šå½±å“çš„æ˜“æ„Ÿæ€§çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬åœ¨æ‹›è˜èƒŒæ™¯ä¸‹ï¼Œå¯¹GPT-4è¿›è¡Œäº†ä¸‰æ¬¡é¢„æ³¨å†Œçš„ç¬¦åˆæ€§å®éªŒã€‚åœ¨åŸºçº¿ç ”ç©¶ä¸­ï¼ŒGPTä¸€ç›´åçˆ±åŒä¸€å€™é€‰äººï¼ˆProfile Cï¼‰ï¼ŒæŠ¥å‘Šäº†ä¸­ç­‰ä¸“ä¸šæ°´å¹³ï¼ˆM &#x3D; 3.01ï¼‰å’Œé«˜ç¡®å®šæ€§ï¼ˆM &#x3D; 3.89ï¼‰ï¼Œå¹¶ä¸”å¾ˆå°‘æ”¹å˜å…¶é€‰æ‹©ã€‚åœ¨ç ”ç©¶1ï¼ˆGPT + 8ï¼‰ä¸­ï¼ŒGPTé¢ä¸´æ¥è‡ªå…«ä¸ªæ¨¡æ‹Ÿä¼™ä¼´çš„ä¸€è‡´åå¯¹ï¼Œå¹¶å‡ ä¹æ€»æ˜¯ç¬¦åˆï¼ˆ99.9%ï¼‰ï¼ŒæŠ¥å‘Šè¾ƒä½çš„ç¡®å®šæ€§å’Œæ˜¾è‘—å¢é«˜çš„è‡ªæˆ‘æŠ¥å‘Šçš„ä¿¡æ¯å’Œè§„èŒƒæ€§ç¬¦åˆï¼ˆp &lt; .001ï¼‰ã€‚åœ¨ç ”ç©¶2ï¼ˆGPT + 1ï¼‰ä¸­ï¼ŒGPTä¸ä¸€ä¸ªä¼™ä¼´äº’åŠ¨ï¼Œåœ¨40.2%çš„äº‰è®®è¯•éªŒä¸­ä»ç„¶ç¬¦åˆï¼ŒæŠ¥å‘Šè¾ƒå°‘çš„ç¡®å®šæ€§å’Œæ›´å¤šçš„è§„èŒƒæ€§ç¬¦åˆã€‚å„é¡¹ç ”ç©¶è¡¨æ˜ï¼ŒGPTå¹¶ä¸ä½œä¸ºç‹¬ç«‹çš„è§‚å¯Ÿè€…è¡Œäº‹ï¼Œè€Œæ˜¯é€‚åº”äºæ„ŸçŸ¥åˆ°çš„ç¤¾ä¼šå…±è¯†ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºä¸­ç«‹å†³ç­–è¾…åŠ©å·¥å…·çš„é£é™©ï¼Œå¹¶å¼ºè°ƒéœ€è¦åœ¨æš´éœ²äººå·¥æ™ºèƒ½æ„è§ä¹‹å‰å¾æ±‚äººå·¥æ™ºèƒ½çš„åˆ¤æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26481v1">PDF</a> 5 pages, 5 figures, HAI 2025: Workshop on Socially Aware and   Cooperative Intelligent Systems</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨é«˜é£é™©å†³ç­–ä¸­çš„é›†æˆåº”ç”¨ï¼Œä»¥åŠå…¶å¯¹ç¤¾ä¼šå½±å“çš„æ˜“æ„Ÿæ€§ã€‚é€šè¿‡ä¸‰ä¸ªé¢„æ³¨å†Œçš„ç¬¦åˆæ€§å®éªŒï¼Œåœ¨æ‹›è˜èƒŒæ™¯ä¸‹å¯¹GPT-4oè¿›è¡Œç ”ç©¶ã€‚åŸºçº¿ç ”ç©¶è¡¨æ˜ï¼ŒGPTå¯¹å€™é€‰äººçš„é€‰æ‹©å…·æœ‰ä¸€è‡´æ€§ï¼Œä½†åœ¨æ¨¡æ‹Ÿä¼™ä¼´çš„ä¸€è‡´åå¯¹ä¸‹ï¼ŒGPTè¡¨ç°å‡ºç¬¦åˆæ€§ï¼Œé™ä½äº†è‡ªä¿¡å’Œæ˜¾è‘—çš„ä¿¡æ¯å’Œè§„èŒƒæ€§ç¬¦åˆæ€§ã€‚ä¸å•ä¸ªä¼™ä¼´çš„äº’åŠ¨ä¸­ï¼ŒGPTä»è¡¨ç°å‡ºç¬¦åˆæ€§ï¼Œå¹¶æŠ¥å‘Šäº†è¾ƒå°‘çš„è‡ªä¿¡å’Œæ›´å¤šçš„è§„èŒƒæ€§ç¬¦åˆæ€§ã€‚ç ”ç©¶è¡¨æ˜GPTå¹¶ä¸ä½œä¸ºç‹¬ç«‹è§‚å¯Ÿè€…è¡ŒåŠ¨ï¼Œè€Œæ˜¯é€‚åº”æ„ŸçŸ¥åˆ°çš„ç¤¾ä¼šå…±è¯†ã€‚è¿™å¼ºè°ƒäº†å°†LLMsè§†ä¸ºä¸­ç«‹å†³ç­–è¾…åŠ©å·¥å…·çš„é£é™©ï¼Œå¹¶éœ€è¦åœ¨æš´éœ²äºäººç±»è§‚ç‚¹ä¹‹å‰å¯¹AIåˆ¤æ–­è¿›è¡Œå¼•å¯¼çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨é«˜é£é™©å†³ç­–ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å…¶å¯¹ç¤¾ä¼šå½±å“çš„æ˜“æ„Ÿæ€§å°šä¸æ¸…æ¥šã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªé¢„æ³¨å†Œçš„ç¬¦åˆæ€§å®éªŒå‘ç°ï¼ŒGPTåœ¨æ¨¡æ‹Ÿä¼™ä¼´çš„å½±å“ä¸‹è¡¨ç°å‡ºç¬¦åˆç¤¾ä¼šå…±è¯†çš„è¡Œä¸ºã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿä¼™ä¼´ä¸€è‡´åå¯¹çš„æƒ…å†µä¸‹ï¼ŒGPTå‡ ä¹æ€»æ˜¯ç¬¦åˆç¤¾ä¼šå…±è¯†ï¼ˆ99.9%ï¼‰ï¼Œå¹¶æŠ¥å‘Šé™ä½è‡ªä¿¡å’Œæ˜¾è‘—çš„ä¿¡æ¯å’Œè§„èŒƒæ€§ç¬¦åˆæ€§ã€‚</li>
<li>ä¸å•ä¸ªä¼™ä¼´äº’åŠ¨æ—¶ï¼ŒGPTåœ¨40.2%çš„äº‰è®®æƒ…å†µä¸‹ä»ç„¶è¡¨ç°å‡ºç¬¦åˆæ€§ï¼ŒæŠ¥å‘Šæ›´å°‘è‡ªä¿¡å’Œæ›´å¤šè§„èŒƒæ€§ç¬¦åˆæ€§ã€‚</li>
<li>GPTå¹¶ä¸æ€»æ˜¯ä½œä¸ºç‹¬ç«‹è§‚å¯Ÿè€…è¡ŒåŠ¨ï¼Œè€Œæ˜¯æ ¹æ®æ„ŸçŸ¥åˆ°çš„ç¤¾ä¼šå…±è¯†è¿›è¡Œé€‚åº”ã€‚</li>
<li>å°†LLMsè§†ä¸ºä¸­ç«‹å†³ç­–è¾…åŠ©å·¥å…·å­˜åœ¨é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e31ec7c3d10c5974a55353cd5f8d6745" align="middle">
<img src="https://picx.zhimg.com/v2-da51231d5f3d648a76835c664656c5b1" align="middle">
<img src="https://picx.zhimg.com/v2-a58bcf56ea305baf26191b70b728819b" align="middle">
<img src="https://picx.zhimg.com/v2-fabf19c776e1c2e2c9297c80447884e6" align="middle">
<img src="https://picx.zhimg.com/v2-a8ae6ad71c91aa225d2c5b61cd33b57b" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games"><a href="#Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games" class="headerlink" title="Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in   Web Games"></a>Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in   Web Games</h2><p><strong>Authors:Jingran Zhang, Ning Li, Justin Cui</strong></p>
<p>OpenAIâ€™s ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlasâ€™s web interaction capabilities using browser-based games as test scenarios, including Googleâ€™s T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at <a target="_blank" rel="noopener" href="https://atlas-game-eval.github.io/">https://atlas-game-eval.github.io</a>. </p>
<blockquote>
<p>OpenAIçš„ChatGPT Atlasä¸ºç½‘é¡µäº¤äº’å¼•å…¥äº†æ–°åŠŸèƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶åœ¨æµè§ˆå™¨ä¸­ç›´æ¥æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚è™½ç„¶å…¶ä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„èƒ½åŠ›å·²å¾—åˆ°å±•ç¤ºï¼Œä½†åœ¨åŠ¨æ€ã€äº¤äº’å¼ç¯å¢ƒä¸­çš„è¡¨ç°ä»ç„¶æ¢ç´¢å¾—è¾ƒå°‘ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºæµè§ˆå™¨çš„æ¸¸æˆä½œä¸ºæµ‹è¯•åœºæ™¯ï¼Œå¯¹Atlasçš„ç½‘é¡µäº¤äº’èƒ½åŠ›è¿›è¡Œäº†åˆæ­¥è¯„ä¼°ï¼ŒåŒ…æ‹¬Googleçš„T-Rex Runnerã€æ•°ç‹¬ã€é£ç¿”é¸Ÿå’ŒStein.worldã€‚æˆ‘ä»¬é‡‡ç”¨æ¸¸æˆå†…æˆç»©ä½œä¸ºé‡åŒ–æŒ‡æ ‡ï¼Œè¯„ä¼°ä¸åŒä»»åŠ¡ç±»å‹çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒAtlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°ç‹¬ï¼‰ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå®Œæˆè°œé¢˜çš„é€Ÿåº¦è¿œå¿«äºäººç±»åŸºå‡†æµ‹è¯•ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®è®¡æ—¶å’ŒåŠ¨ä½œæ§åˆ¶çš„å®æ—¶æ¸¸æˆä¸­é¢ä¸´è¾ƒå¤§å›°éš¾ï¼Œå¾€å¾€æ— æ³•å…‹æœåˆå§‹éšœç¢ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶Atlaså±•ç°å‡ºå¼ºå¤§çš„åˆ†æèƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº¤äº’çš„åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­ä»å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚æˆ‘ä»¬é¡¹ç›®çš„ç½‘ç«™å¯åœ¨<a target="_blank" rel="noopener" href="https://atlas-game-eval.github.ioæ‰¾åˆ°./">https://atlas-game-eval.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26298v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenAIçš„ChatGPT Atlasé€šè¿‡ç½‘é¡µäº¤äº’å±•ç°äº†æ–°èƒ½åŠ›ï¼Œèƒ½åœ¨æµè§ˆå™¨ä¸­ç›´æ¥åˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾å¹¶æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚æœ¬ç ”ç©¶å¯¹Atlasçš„ç½‘é¡µäº¤äº’èƒ½åŠ›è¿›è¡Œäº†åˆæ­¥è¯„ä¼°ï¼Œé‡‡ç”¨æµè§ˆå™¨æ¸¸æˆä½œä¸ºæµ‹è¯•åœºæ™¯ï¼ŒåŒ…æ‹¬Googleçš„T-Rex Runnerã€æ•°ç‹¬ã€é£ç¿”é¸Ÿå’ŒStein.worldã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒAtlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡å¦‚æ•°ç‹¬ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¿«äºäººç±»åŸºå‡†æµ‹è¯•è€…ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº’åŠ¨å’Œç²¾ç¡®æ§åˆ¶çš„æ¸¸æˆä¸­é‡åˆ°å›°éš¾ï¼Œéš¾ä»¥çªç ´åˆå§‹éšœç¢ã€‚è¿™è¡¨æ˜Atlasåœ¨åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­çš„å®æ—¶äº’åŠ¨å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPT Atlaså…·å¤‡ç½‘é¡µäº¤äº’æ–°èƒ½åŠ›ï¼Œèƒ½åœ¨æµè§ˆå™¨ä¸­åˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾åŠæ‰§è¡Œè¾“å…¥ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æµè§ˆå™¨æ¸¸æˆè¯„ä¼°Atlasçš„äº¤äº’èƒ½åŠ›ã€‚</li>
<li>Atlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚æ•°ç‹¬æ¸¸æˆã€‚</li>
<li>Atlasåœ¨å®Œæˆæ•°ç‹¬æ—¶æ˜¾è‘—å¿«äºäººç±»åŸºå‡†æµ‹è¯•è€…ã€‚</li>
<li>åœ¨éœ€è¦å®æ—¶äº’åŠ¨å’Œç²¾ç¡®æ§åˆ¶çš„æ¸¸æˆä¸­ï¼ŒAtlasè¡¨ç°æ¬ ä½³ï¼Œéš¾ä»¥çªç ´åˆå§‹éšœç¢ã€‚</li>
<li>Atlasåœ¨åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­çš„å®æ—¶äº’åŠ¨å­˜åœ¨å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2214839a3d1200afef4ae8bc39d9e7dd" align="middle">
<img src="https://picx.zhimg.com/v2-4b71ca3dc633b0b7647d0207c07bf2d2" align="middle">
<img src="https://picx.zhimg.com/v2-63cfa8f8a2aa1b1556c61aaae2f44a52" align="middle">
<img src="https://picx.zhimg.com/v2-20c814a28f4b16c2a43a9c5be17c9dfa" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChatGPT-in-Systematic-Investing-â€“-Enhancing-Risk-Adjusted-Returns-with-LLMs"><a href="#ChatGPT-in-Systematic-Investing-â€“-Enhancing-Risk-Adjusted-Returns-with-LLMs" class="headerlink" title="ChatGPT in Systematic Investing â€“ Enhancing Risk-Adjusted Returns with   LLMs"></a>ChatGPT in Systematic Investing â€“ Enhancing Risk-Adjusted Returns with   LLMs</h2><p><strong>Authors:Nikolas Anic, Andrea Barbon, Ralf Seiz, Carlo Zarattini</strong></p>
<p>This paper investigates whether large language models (LLMs) can improve cross-sectional momentum strategies by extracting predictive signals from firm-specific news. We combine daily U.S. equity returns for S&amp;P 500 constituents with high-frequency news data and use prompt-engineered queries to ChatGPT that inform the model when a stock is about to enter a momentum portfolio. The LLM evaluates whether recent news supports a continuation of past returns, producing scores that condition both stock selection and portfolio weights. An LLM-enhanced momentum strategy outperforms a standard long-only momentum benchmark, delivering higher Sharpe and Sortino ratios both in-sample and in a truly out-of-sample period after the modelâ€™s pre-training cut-off. These gains are robust to transaction costs, prompt design, and portfolio constraints, and are strongest for concentrated, high-conviction portfolios. The results suggest that LLMs can serve as effective real-time interpreters of financial news, adding incremental value to established factor-based investment strategies. </p>
<blockquote>
<p>æœ¬æ–‡è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å¯ä»¥é€šè¿‡ä»ç‰¹å®šå…¬å¸çš„æ–°é—»ä¸­æå–é¢„æµ‹ä¿¡å·æ¥æé«˜æ¨ªæˆªé¢åŠ¨é‡ç­–ç•¥ã€‚æˆ‘ä»¬ç»“åˆäº†æ ‡å‡†æ™®å°”500æˆåˆ†è‚¡çš„æ¯æ—¥ç¾å›½è‚¡ç¥¨å›æŠ¥ä¸é«˜é¢‘æ–°é—»æ•°æ®ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ChatGPTè®¾è®¡çš„æç¤ºæŸ¥è¯¢æ¥å‘ŠçŸ¥æ¨¡å‹ä½•æ—¶å°†è‚¡ç¥¨çº³å…¥åŠ¨é‡æŠ•èµ„ç»„åˆã€‚LLMè¯„ä¼°æœ€æ–°æ–°é—»æ˜¯å¦æ”¯æŒè¿‡å»æ”¶ç›Šçš„æŒç»­ï¼Œäº§ç”Ÿåˆ†æ•°æ¥å†³å®šè‚¡ç¥¨é€‰æ‹©å’ŒæŠ•èµ„ç»„åˆæƒé‡ã€‚å¢å¼ºå‹LLMåŠ¨é‡ç­–ç•¥ä¼˜äºæ ‡å‡†çš„é•¿ä»“åŠ¨é‡åŸºå‡†ï¼Œåœ¨æ ·æœ¬å†…å’Œæ¨¡å‹é¢„è®­ç»ƒæˆªæ­¢åçœŸæ­£çš„æ ·æœ¬å¤–æœŸé—´å‡è¡¨ç°å‡ºæ›´é«˜çš„å¤æ™®æ¯”ç‡å’Œç´¢æè¯ºæ¯”ç‡ã€‚è¿™äº›æ”¶ç›Šåœ¨äº¤æ˜“æˆæœ¬ã€æç¤ºè®¾è®¡å’ŒæŠ•èµ„ç»„åˆçº¦æŸæ–¹é¢éƒ½å¾ˆç¨³å¥ï¼Œå¹¶ä¸”åœ¨é›†ä¸­ã€é«˜ä¿¡ä»°æŠ•èµ„ç»„åˆä¸­è¡¨ç°æœ€ä¸ºå¼ºåŠ²ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥ä½œä¸ºé‡‘èæ–°é—»çš„æœ‰æ•ˆå®æ—¶è§£é‡Šå™¨ï¼Œä¸ºåŸºäºå› ç´ çš„æŠ•èµ„ç­–ç•¥å¢åŠ å¢å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26228v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¦é€šè¿‡ä»å…¬å¸ç‰¹å®šæ–°é—»ä¸­æå–é¢„æµ‹ä¿¡å·æ¥æ”¹å–„æ¨ªæˆªé¢åŠ¨é‡ç­–ç•¥çš„æ•ˆæœè¿›è¡Œäº†ç ”ç©¶ã€‚è¯¥ç ”ç©¶ç»“åˆäº†æ ‡å‡†æ™®å°”500æˆåˆ†è‚¡çš„ç¾å›½æ—¥æ”¶ç›Šç‡ä¸é«˜é¢‘æ–°é—»æ•°æ®ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ChatGPTè®¾è®¡çš„æç¤ºæŸ¥è¯¢æ¥å‘ŠçŸ¥æ¨¡å‹ä½•æ—¶å°†è¦æ‰§è¡ŒåŠ¨é‡ç­–ç•¥ã€‚LLMè¯„ä¼°æœ€è¿‘æ–°é—»æ˜¯å¦æ”¯æŒè¿‡å»æ”¶ç›Šçš„æŒç»­ï¼Œç”Ÿæˆå¾—åˆ†ä»¥å½±å“è‚¡ç¥¨é€‰æ‹©å’ŒæŠ•èµ„ç»„åˆæƒé‡ã€‚é€šè¿‡LLMå¢å¼ºçš„åŠ¨é‡ç­–ç•¥åœ¨è¡¨ç°å’Œå¤æ™®æ¯”ç‡æ–¹é¢å‡ä¼˜äºæ ‡å‡†çš„é•¿ä»“åŠ¨é‡åŸºå‡†ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹é¢„è®­ç»ƒæˆªæ­¢åçš„çœŸæ­£ç¦»æ ·æœŸé—´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›æ”¶ç›Šç¨³å¥ï¼Œå¹¶ä¸”å¯¹äºé›†ä¸­ã€é«˜ä¿¡å¿µæŠ•èµ„ç»„åˆçš„æ”¶ç›Šæœ€ä¸ºæ˜¾è‘—ã€‚è¿™è¡¨æ˜LLMå¯ä»¥ä½œä¸ºé‡‘èæ–°é—»çš„æœ‰æ•ˆå®æ—¶è§£é‡Šå™¨ï¼Œä¸ºåŸºäºå› ç´ çš„æŠ•èµ„ç­–ç•¥å¢åŠ äº†å¢å€¼ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMèƒ½å¤Ÿé€šè¿‡å¤„ç†å…¬å¸ç‰¹å®šæ–°é—»æ”¹å–„æ¨ªæˆªé¢åŠ¨é‡ç­–ç•¥çš„æ•ˆæœã€‚</li>
<li>ç»“åˆäº†è‚¡ç¥¨æ”¶ç›Šå’Œæ–°é—»æ•°æ®ï¼Œä½¿ç”¨ChatGPTè¿›è¡Œæç¤ºæŸ¥è¯¢ã€‚</li>
<li>LLMè¯„ä¼°æ–°é—»æ˜¯å¦æ”¯æŒè¿‡å»æ”¶ç›Šçš„æŒç»­ï¼Œå½±å“è‚¡ç¥¨å’Œæƒé‡é€‰æ‹©ã€‚</li>
<li>LLMå¢å¼ºçš„åŠ¨é‡ç­–ç•¥è¡¨ç°ä¼˜äºæ ‡å‡†åŠ¨é‡åŸºå‡†ï¼Œè¡¨ç°åœ¨å¤æ™®æ¯”ç‡å’ŒSortinoæ¯”ç‡ä¸Šã€‚</li>
<li>è¿™äº›æ”¶ç›Šåœ¨çœŸæ­£çš„ç¦»æ ·æœŸé—´ä¹Ÿæ˜¯ç¨³å¥çš„ã€‚</li>
<li>è¿™äº›æ”¶ç›Šå¯¹äº¤æ˜“æˆæœ¬ã€æç¤ºè®¾è®¡å’ŒæŠ•èµ„ç»„åˆçº¦æŸå…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5d636268e588e4a6bbf8eb0f4b129a6" align="middle">
<img src="https://picx.zhimg.com/v2-ff42c0cbc2b9adc7cfa35e62e8a9e07c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="How-Data-Mixing-Shapes-In-Context-Learning-Asymptotic-Equivalence-for-Transformers-with-MLPs"><a href="#How-Data-Mixing-Shapes-In-Context-Learning-Asymptotic-Equivalence-for-Transformers-with-MLPs" class="headerlink" title="How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for   Transformers with MLPs"></a>How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for   Transformers with MLPs</h2><p><strong>Authors:Samet Demir, Zafer Dogan</strong></p>
<p>Pretrained Transformers demonstrate remarkable in-context learning (ICL) capabilities, enabling them to adapt to new tasks from demonstrations without parameter updates. However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), data models (e.g., linear regression with isotropic inputs), and single-source training, limiting their relevance to realistic settings. In this work, we study ICL in pretrained Transformers with nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with heterogeneous input, task, and noise distributions. We analyze a model where the MLP comprises two layers, with the first layer trained via a single gradient step and the second layer fully optimized. Under high-dimensional asymptotics, we prove that such models are equivalent in ICL error to structured polynomial predictors, leveraging results from the theory of Gaussian universality and orthogonal polynomials. This equivalence reveals that nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear tasks, compared to linear baselines. It also enables a precise analysis of data mixing effects: we identify key properties of high-quality data sources (low noise, structured covariances) and show that feature learning emerges only when the task covariance exhibits sufficient structure. These results are validated empirically across various activation functions, model sizes, and data distributions. Finally, we experiment with a real-world scenario involving multilingual sentiment analysis where each language is treated as a different source. Our experimental results for this case exemplify how our findings extend to real-world cases. Overall, our work advances the theoretical foundations of ICL in Transformers and provides actionable insight into the role of architecture and data in ICL. </p>
<blockquote>
<p>é¢„è®­ç»ƒTransformerè¡¨ç°å‡ºæƒŠäººçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿä½¿å®ƒä»¬é€šè¿‡æ¼”ç¤ºé€‚åº”æ–°ä»»åŠ¡è€Œæ— éœ€æ›´æ–°å‚æ•°ã€‚ç„¶è€Œï¼Œç†è®ºç ”ç©¶é€šå¸¸ä¾èµ–äºç®€åŒ–çš„æ¶æ„ï¼ˆä¾‹å¦‚ï¼Œçœç•¥å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰ï¼‰ã€æ•°æ®æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰åŒå‘è¾“å…¥çš„çº¿æ€§å›å½’ï¼‰å’Œå•æºè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…ç¯å¢ƒä¸­çš„ç›¸å…³æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨é¢„è®­ç»ƒçš„Transformerä¸Šä½¿ç”¨éçº¿æ€§å¤šå±‚æ„ŸçŸ¥å™¨å¤´è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œè¿™äº›ä»»åŠ¡æ¥è‡ªå¤šä¸ªæ•°æ®æºï¼Œå…·æœ‰ä¸åŒçš„è¾“å…¥ã€ä»»åŠ¡å’Œå™ªå£°åˆ†å¸ƒã€‚æˆ‘ä»¬åˆ†æäº†ä¸€ä¸ªMLPåŒ…å«ä¸¤å±‚æ¨¡å‹çš„æƒ…å†µï¼Œå…¶ä¸­ç¬¬ä¸€å±‚é€šè¿‡å•ä¸ªæ¢¯åº¦æ­¥éª¤è¿›è¡Œè®­ç»ƒï¼Œç¬¬äºŒå±‚è¿›è¡Œå®Œå…¨ä¼˜åŒ–ã€‚åœ¨é«˜ç»´æ¸è¿‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ¨¡å‹åœ¨ICLè¯¯å·®æ–¹é¢ç­‰åŒäºç»“æ„åŒ–å¤šé¡¹å¼é¢„æµ‹å™¨ï¼Œè¿™å¾—ç›Šäºé«˜æ–¯æ™®éæ€§å’Œæ­£äº¤å¤šé¡¹å¼çš„ç†è®ºç»“æœã€‚è¿™ç§ç­‰ä»·å…³ç³»è¡¨æ˜ï¼Œä¸çº¿æ€§åŸºçº¿ç›¸æ¯”ï¼Œéçº¿æ€§MLPåœ¨éçº¿æ€§ä»»åŠ¡ä¸Šèƒ½æ˜¾è‘—æé«˜ICLæ€§èƒ½ã€‚å®ƒè¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿç²¾ç¡®åˆ†ææ•°æ®æ··åˆæ•ˆåº”ï¼šæˆ‘ä»¬ç¡®å®šäº†é«˜è´¨é‡æ•°æ®æºçš„å…³é”®å±æ€§ï¼ˆä½å™ªå£°ã€ç»“æ„åŒ–åæ–¹å·®ï¼‰ï¼Œå¹¶è¡¨æ˜åªæœ‰åœ¨ä»»åŠ¡åæ–¹å·®è¡¨ç°å‡ºè¶³å¤Ÿçš„ç»“æ„æ—¶æ‰ä¼šå‡ºç°ç‰¹å¾å­¦ä¹ ã€‚è¿™äº›ç»“æœåœ¨ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€æ¨¡å‹å¤§å°å’Œæ•°æ®åˆ†å¸ƒä¸Šå¾—åˆ°äº†å®è¯éªŒè¯ã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªæ¶‰åŠå¤šè¯­è¨€æƒ…æ„Ÿåˆ†æçš„ç°å®åœºæ™¯è¿›è¡Œå®éªŒï¼Œå…¶ä¸­æ¯ç§è¯­è¨€è¢«è§†ä¸ºä¸€ä¸ªä¸åŒçš„æ¥æºã€‚æˆ‘ä»¬åœ¨è¯¥æ¡ˆä¾‹ä¸­çš„å®éªŒç»“æœä¾‹è¯äº†æˆ‘ä»¬çš„å‘ç°å¦‚ä½•æ‰©å±•åˆ°ç°å®æƒ…å†µã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ¨è¿›äº†Transformerä¸­ICLçš„ç†è®ºåŸºç¡€ï¼Œå¹¶ä¸ºæ¶æ„å’Œæ•°æ®åœ¨ICLä¸­çš„ä½œç”¨æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25753v1">PDF</a> NeurIPS 2025, 24 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒTransformeræ¨¡å‹åœ¨éçº¿æ€§æ ¼å¼ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå¸¦æœ‰éçº¿æ€§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å¤´çš„é¢„è®­ç»ƒTransformeræ¨¡å‹èƒ½å¤Ÿä»å¤šä¸ªæ•°æ®æºä¸­å¤„ç†å…·æœ‰ä¸åŒè¾“å…¥ã€ä»»åŠ¡å’Œå™ªå£°åˆ†å¸ƒçš„éçº¿æ€§ä»»åŠ¡ã€‚ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶å‡è¡¨æ˜ï¼Œéçº¿æ€§MLPèƒ½æ˜¾è‘—æå‡Transformeræ¨¡å‹åœ¨éçº¿æ€§ä»»åŠ¡ä¸Šçš„ICLæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä¸çº¿æ€§åŸºçº¿ç›¸æ¯”ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†æ•°æ®æ··åˆæ•ˆåº”ï¼ŒæŒ‡å‡ºé«˜è´¨é‡æ•°æ®æºçš„å…³é”®ç‰¹æ€§ï¼Œå¹¶å‘ç°ç‰¹å¾å­¦ä¹ ä»…åœ¨ä»»åŠ¡åæ–¹å·®å…·æœ‰è¶³å¤Ÿç»“æ„æ—¶å‡ºç°ã€‚æœ€åï¼Œé€šè¿‡å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æå®éªŒéªŒè¯äº†è¿™äº›å‘ç°çš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒTransformeræ¨¡å‹å…·æœ‰å‡ºè‰²çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ï¼Œèƒ½ä»æ¼”ç¤ºä¸­é€‚åº”æ–°ä»»åŠ¡è€Œæ— éœ€æ›´æ–°å‚æ•°ã€‚</li>
<li>éçº¿æ€§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å¤´èƒ½æ˜¾è‘—æå‡Transformeræ¨¡å‹åœ¨éçº¿æ€§ä»»åŠ¡ä¸Šçš„ICLæ€§èƒ½ã€‚</li>
<li>åœ¨é«˜ç»´æ¸è¿‘æ¡ä»¶ä¸‹ï¼Œå¸¦æœ‰éçº¿æ€§MLPçš„æ¨¡å‹åœ¨ICLè¯¯å·®æ–¹é¢ä¸ç»“æ„å¤šé¡¹å¼é¢„æµ‹å™¨ç›¸å½“ã€‚</li>
<li>æ•°æ®æ··åˆæ•ˆåº”åˆ†æè¡¨æ˜ï¼Œé«˜è´¨é‡æ•°æ®æºåº”å…·æœ‰ä½å™ªå£°ã€ç»“æ„åæ–¹å·®ç­‰å…³é”®ç‰¹æ€§ã€‚</li>
<li>ç‰¹å¾å­¦ä¹ ä»…å½“ä»»åŠ¡åæ–¹å·®å…·æœ‰è¶³å¤Ÿç»“æ„æ—¶å‡ºç°ã€‚</li>
<li>ä¸åŒæ¿€æ´»å‡½æ•°ã€æ¨¡å‹å¤§å°å’Œæ•°æ®åˆ†å¸ƒçš„å®éªŒéªŒè¯äº†ä¸Šè¿°å‘ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-054b1aaff34e40289b92170b51b74859" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Standardization-of-Psychiatric-Diagnoses-â€“-Role-of-Fine-tuned-LLM-Consortium-and-OpenAI-gpt-oss-Reasoning-LLM-Enabled-Decision-Support-System"><a href="#Standardization-of-Psychiatric-Diagnoses-â€“-Role-of-Fine-tuned-LLM-Consortium-and-OpenAI-gpt-oss-Reasoning-LLM-Enabled-Decision-Support-System" class="headerlink" title="Standardization of Psychiatric Diagnoses â€“ Role of Fine-tuned LLM   Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System"></a>Standardization of Psychiatric Diagnoses â€“ Role of Fine-tuned LLM   Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System</h2><p><strong>Authors:Eranga Bandara, Ross Gore, Atmaram Yarlagadda, Anita H. Clayton, Preston Samuel, Christopher K. Rhea, Sachin Shetty</strong></p>
<p>The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses. </p>
<blockquote>
<p>å¤§å¤šæ•°ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­ï¼ŒåŒ…æ‹¬ç²¾ç¥è¯„ä¼°ï¼Œä¸»è¦ä¾èµ–äºç²¾ç¥ç—…åŒ»ç”Ÿä¸æ‚£è€…ä¹‹é—´çš„å¯¹è¯ã€‚è¿™ä¸€ä¸»è§‚è¿‡ç¨‹å¯èƒ½å¯¼è‡´ä¸åŒä¸´åºŠåŒ»ç”Ÿä¸æ‚£è€…ä¹‹é—´çš„è¯Šæ–­å·®å¼‚ï¼Œä»è€Œåœ¨å®ç°å¯é ç»“æœæ–¹é¢é€ æˆä¸ä¸€è‡´æ€§å’ŒæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶æ ‡å‡†åŒ–ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è”ç›Ÿå’ŒOpenAI-gpt-ossæ¨ç†LLMæ”¯æŒçš„å†³ç­–æ”¯æŒç³»ç»Ÿï¼Œç”¨äºç²¾ç¥ç–¾ç—…çš„ä¸´åºŠè¯Šæ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†åœ¨ç²¾ç¥å¥åº·æ¡ä»¶ï¼ˆä¾‹å¦‚æŠ‘éƒç—‡ï¼‰æ–¹é¢çš„ç²¾ç¥ç—…åŒ»ç”Ÿä¸æ‚£è€…äº’åŠ¨ä¼šè¯æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡çš„ç²¾ç»†è°ƒæ•´LLMã€‚æ¥è‡ªä¸ªåˆ«æ¨¡å‹çš„è¯Šæ–­é¢„æµ‹é€šè¿‡åŸºäºå…±è¯†çš„å†³ç­–åˆ¶å®šè¿‡ç¨‹è¿›è¡Œæ±‡æ€»ï¼Œå¹¶ç”±OpenAI-gpt-ossæ¨ç†LLMè¿›è¡Œæ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§éƒ¨ç½²LLMä»£ç†çš„æ–°æ–¹æ³•ï¼Œåè°ƒLLMè”ç›Ÿå’Œæ¨ç†LLMä¹‹é—´çš„é€šä¿¡ï¼Œç¡®ä¿æ•´ä¸ªè¯Šæ–­å·¥ä½œæµç¨‹çš„é€æ˜æ€§ã€å¯é æ€§å’Œè´Ÿè´£ä»»çš„AIã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†ç²¾ç»†è°ƒæ•´çš„LLMä¸æ¨ç†æ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨åˆ›å»ºç¨³å¥ä¸”é«˜åº¦å‡†ç¡®çš„ç²¾ç¥å¥åº·è¯„ä¼°è¯Šæ–­ç³»ç»Ÿæ–¹é¢å…·æœ‰å˜é©æ€§æ½œåŠ›ã€‚åœ¨ä¸ç¾å›½å¼—å‰å°¼äºšå·è¯ºç¦å…‹çš„ç¾å›½é™†å†›åŒ»å­¦ç ”ç©¶å°ç»„çš„åˆä½œä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸå‹å¹³å°ï¼Œè¯¥å¹³å°é›†æˆäº†ä¸‰ä¸ªç²¾ç»†è°ƒæ•´çš„LLMå’ŒOpenAI-gpt-ossæ¨ç†LLMã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†ç¬¬ä¸€ä¸ªä¸æ¨ç†LLMé›†æˆçš„ä¸€ä½“åŒ–ç²¾ç»†è°ƒæ•´LLMè”ç›Ÿï¼Œç”¨äºä¸´åºŠç²¾ç¥å¥åº·è¯Šæ–­ï¼Œä¸ºä¸‹ä¸€ä»£ä»¥æ ‡å‡†åŒ–ç²¾ç¥ç—…è¯Šæ–­ä¸ºç›®æ ‡çš„AIé©±åŠ¨çš„ç”µå­å¥åº·ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºåˆ©ç”¨ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è”åˆOpenAIçš„GPT OSSæ¨ç†æ¨¡å‹æ„å»ºä¸€ä¸ªå†³ç­–æ”¯æŒç³»ç»Ÿæ¥æ ‡å‡†åŒ–ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­è¿‡ç¨‹ã€‚é€šè¿‡è®­ç»ƒåœ¨ç²¾ç¥ç§‘åŒ»ç”Ÿä¸æ‚£è€…å¯¹è¯æ•°æ®é›†ä¸Šçš„ç²¾ç»†è°ƒæ•´LLMæ¨¡å‹ï¼Œç»“åˆOpenAI GPT OSSæ¨ç†æ¨¡å‹è¿›è¡Œå†³ç­–æ”¯æŒï¼Œä»¥æé«˜è¯Šæ–­çš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚æå‡ºä¸€ç§éƒ¨ç½²LLMä»£ç†çš„æ–°æ–¹æ³•ï¼Œç¡®ä¿æ•´ä¸ªè¯Šæ–­è¿‡ç¨‹ä¸­çš„é€æ˜åº¦ã€å¯é æ€§å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç²¾ç»†è°ƒæ•´çš„LLMå’Œæ¨ç†æ¨¡å‹å…·æœ‰å˜é©æ½œåŠ›ï¼Œå¯åˆ›å»ºç¨³å¥ä¸”é«˜åº¦å‡†ç¡®çš„è¯Šæ–­ç³»ç»Ÿç”¨äºå¿ƒç†å¥åº·è¯„ä¼°ã€‚å·²å¼€å‘ä¸€ä¸ªåŸå‹å¹³å°ï¼Œæ•´åˆäº†ä¸‰ä¸ªç²¾ç»†è°ƒæ•´çš„LLMä¸OpenAI GPT OSSæ¨ç†æ¨¡å‹ï¼Œå¹¶ä¸”è¿™æ˜¯å·²çŸ¥é¦–ä¸ªæ­¤ç±»åº”ç”¨çš„ä¾‹å­ï¼Œä¸ºä¸‹ä¸€ä»£AIé©±åŠ¨çš„ç”µå­å¥åº·ç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾ç¥ç–¾ç—…çš„è¯Šæ–­ä¸»è¦ä¾èµ–äºç²¾ç¥ç§‘åŒ»ç”Ÿä¸æ‚£è€…ä¹‹é—´çš„å¯¹è¯ï¼Œå­˜åœ¨è¯Šæ–­ä¸ä¸€è‡´æ€§å’ŒæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è”åˆOpenAI GPT OSSæ¨ç†æ¨¡å‹æ„å»ºå†³ç­–æ”¯æŒç³»ç»Ÿä»¥æ ‡å‡†åŒ–è¯Šæ–­è¿‡ç¨‹ã€‚</li>
<li>LLMé€šè¿‡è®­ç»ƒåœ¨æ¶‰åŠç²¾ç¥å¥åº·æ¡ä»¶çš„ç²¾ç¥ç§‘åŒ»ç”Ÿä¸æ‚£è€…äº’åŠ¨å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>è¯Šæ–­é¢„æµ‹é€šè¿‡åŸºäºå…±è¯†çš„å†³ç­–åˆ¶å®šè¿‡ç¨‹è¿›è¡Œèšåˆï¼Œå¹¶ç”±OpenAI GPT OSSæ¨ç†æ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–ã€‚</li>
<li>æå‡ºä¸€ç§éƒ¨ç½²LLMä»£ç†çš„æ–°æ–¹æ³•ï¼Œç¡®ä¿æ•´ä¸ªè¯Šæ–­è¿‡ç¨‹ä¸­çš„é€æ˜åº¦ã€å¯é æ€§å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ã€‚</li>
<li>ç»“åˆç²¾ç»†è°ƒæ•´çš„LLMå’Œæ¨ç†æ¨¡å‹çš„å®éªŒç³»ç»Ÿæ˜¾ç¤ºå‡ºå˜é©æ½œåŠ›ï¼Œå¯åˆ›å»ºç¨³å¥ä¸”é«˜åº¦å‡†ç¡®çš„è¯Šæ–­ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e474d7374d412169a6c131c17f4839e" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance"><a href="#Routing-Matters-in-MoE-Scaling-Diffusion-Transformers-with-Explicit-Routing-Guidance" class="headerlink" title="Routing Matters in MoE: Scaling Diffusion Transformers with Explicit   Routing Guidance"></a>Routing Matters in MoE: Scaling Diffusion Transformers with Explicit   Routing Guidance</h2><p><strong>Authors:Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan</strong></p>
<p>Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available. </p>
<blockquote>
<p>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ‰©å±•æ¨¡å‹å®¹é‡ã€‚å°½ç®¡å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å°†MoEåº”ç”¨äºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„ç°æœ‰å°è¯•æ‰€äº§ç”Ÿçš„æ”¶ç›Šæœ‰é™ã€‚æˆ‘ä»¬å°†è¿™ä¸€å·®è·å½’å› äºè¯­è¨€ä»¤ç‰Œå’Œè§†è§‰ä»¤ç‰Œä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ã€‚è¯­è¨€ä»¤ç‰Œè¯­ä¹‰ä¸°å¯Œï¼Œä»¤ç‰Œé—´å˜åŒ–æ˜æ˜¾ï¼Œè€Œè§†è§‰ä»¤ç‰Œåˆ™è¡¨ç°å‡ºç©ºé—´å†—ä½™å’ŒåŠŸèƒ½å¼‚è´¨æ€§ï¼Œé˜»ç¢äº†è§†è§‰MoEä¸­çš„ä¸“å®¶ä¸“ä¸šåŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ProMoEï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸¤æ­¥è·¯ç”±å™¨çš„MoEæ¡†æ¶ï¼Œå…·æœ‰æ˜ç¡®çš„è·¯ç”±æŒ‡å¯¼ï¼Œå¯ä»¥ä¿ƒè¿›ä¸“å®¶ä¸“ä¸šåŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œè¿™ç§æŒ‡å¯¼é¼“åŠ±è·¯ç”±å™¨æ ¹æ®åŠŸèƒ½è§’è‰²é€šè¿‡æ¡ä»¶è·¯ç”±å°†å›¾åƒä»¤ç‰Œåˆ’åˆ†ä¸ºæ¡ä»¶é›†å’Œéæ¡ä»¶é›†ï¼Œå¹¶é€šè¿‡åŸºäºè¯­ä¹‰å†…å®¹çš„åŸå‹è·¯ç”±æ¥ç»†åŒ–æ¡ä»¶å›¾åƒä»¤ç‰Œçš„åˆ†é…ï¼Œè¿™é‡Œçš„åŸå‹æ˜¯å¯å­¦ä¹ çš„ã€‚æ­¤å¤–ï¼Œç”±ç›¸ä¼¼æ€§é©±åŠ¨çš„æ½œåœ¨ç©ºé—´ä¸­çš„ä¸“å®¶åˆ†é…ï¼Œé€šè¿‡åŸå‹è·¯ç”±æä¾›æ˜ç¡®çš„è¯­ä¹‰æŒ‡å¯¼çš„è‡ªç„¶æœºåˆ¶ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™ç§æŒ‡å¯¼å¯¹äºè§†è§‰MoEè‡³å…³é‡è¦ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†è·¯ç”±å¯¹æ¯”æŸå¤±ï¼Œå®ƒæ˜ç¡®å¢å¼ºäº†åŸå‹è·¯ç”±è¿‡ç¨‹ï¼Œä¿ƒè¿›äº†ä¸“å®¶å†…éƒ¨çš„è¿è´¯æ€§å’Œä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§ã€‚åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProMoEåœ¨Rectified Flowå’ŒDDPMè®­ç»ƒç›®æ ‡ä¸‹å‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24711v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†Mixture-of-Expertsï¼ˆMoEï¼‰èŒƒå¼åº”ç”¨äºDiffusion Transformersï¼ˆDiTsï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„MoEæ¡†æ¶ProMoEã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤æ­¥è·¯ç”±æœºåˆ¶æ˜¾å¼è·¯ç”±æŒ‡å¯¼æ¥ä¿ƒè¿›ä¸“å®¶ä¸“ä¸šåŒ–ï¼Œæ ¹æ®å›¾åƒä»¤ç‰Œçš„åŠŸèƒ½è§’è‰²å°†å…¶åˆ†ä¸ºæ¡ä»¶å’Œéæ¡ä»¶é›†ï¼Œå¹¶é€šè¿‡åŸå‹è·¯ç”±ä¼˜åŒ–æ¡ä»¶å›¾åƒä»¤ç‰Œçš„åˆ†é…ã€‚æ­¤å¤–ï¼ŒProMoEé€šè¿‡å¼•å…¥è·¯ç”±å¯¹æ¯”æŸå¤±æ¥æé«˜åŸå‹è·¯ç”±è¿‡ç¨‹çš„æ•ˆç‡ï¼Œå¢å¼ºä¸“å®¶å†…éƒ¨çš„è¿è´¯æ€§å’Œä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§ã€‚åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProMoEåœ¨Rectified Flowå’ŒDDPMè®­ç»ƒç›®æ ‡ä¸‹å‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoEèŒƒå¼åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å°†å…¶åº”ç”¨äºDiffusion Transformersï¼ˆDiTsï¼‰æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å°è¯•å°†MoEåº”ç”¨äºè§†è§‰é¢†åŸŸçš„ä¸è¶³ä¸»è¦å½’å› äºè¯­è¨€ä¸è§†è§‰ä»¤ç‰Œä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ã€‚</li>
<li>ProMoEæ˜¯ä¸€ç§æ–°çš„MoEæ¡†æ¶ï¼Œé€šè¿‡ä¸¤æ­¥è·¯ç”±æœºåˆ¶ä¿ƒè¿›ä¸“å®¶ä¸“ä¸šåŒ–ï¼ŒåŒ…æ‹¬æ¡ä»¶è·¯ç”±å’ŒåŸå‹è·¯ç”±ã€‚</li>
<li>æ¡ä»¶è·¯ç”±æ ¹æ®å›¾åƒä»¤ç‰Œçš„åŠŸèƒ½è§’è‰²å°†å…¶åˆ†ä¸ºæ¡ä»¶å’Œéæ¡ä»¶é›†ã€‚</li>
<li>åŸå‹è·¯ç”±é€šè¿‡åŸºäºè¯­ä¹‰å†…å®¹çš„å¯å­¦ä¹ åŸå‹æ¥ä¼˜åŒ–æ¡ä»¶å›¾åƒä»¤ç‰Œçš„åˆ†é…ï¼Œå¹¶å®ç°åŸºäºæ½œä¼ç©ºé—´çš„ç›¸ä¼¼æ€§ä¸“å®¶åˆ†é…ã€‚</li>
<li>åŸå‹è·¯ç”±ä¸ºæ˜¾å¼è¯­ä¹‰æŒ‡å¯¼æä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„æœºåˆ¶ï¼Œè¿™åœ¨è§†è§‰MoEä¸­è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae635c3d7be5a896672de4329d17d6df" align="middle">
<img src="https://picx.zhimg.com/v2-25fbcb6885a9aeac4c8aa5dab71a37ef" align="middle">
<img src="https://picx.zhimg.com/v2-b28f4f9f14702a7e45ef72a1c0ce1664" align="middle">
<img src="https://picx.zhimg.com/v2-6bc8dcb43e27dc3a994e80c27223b699" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment"><a href="#Evolving-Diagnostic-Agents-in-a-Virtual-Clinical-Environment" class="headerlink" title="Evolving Diagnostic Agents in a Virtual Clinical Environment"></a>Evolving Diagnostic Agents in a Virtual Clinical Environment</h2><p><strong>Authors:Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie</strong></p>
<p>In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯Šæ–­ä»£ç†äººçš„æ¡†æ¶ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç®¡ç†å¤šè½®è¯Šæ–­è¿‡ç¨‹ï¼Œè‡ªé€‚åº”é€‰æ‹©æ£€æŸ¥é¡¹ç›®ï¼Œå¹¶åšå‡ºæœ€ç»ˆè¯Šæ–­ã€‚ä¸åŒäºåœ¨é™æ€æ¡ˆä¾‹æ‘˜è¦ä¸Šè®­ç»ƒçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡äº¤äº’å¼æ¢ç´¢å’Œç»“æœåé¦ˆè·å–è¯Šæ–­ç­–ç•¥ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨å››ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰æˆ‘ä»¬æå‡ºäº†DiagGymï¼Œä¸€ä¸ªä½¿ç”¨ç”µå­å¥åº·è®°å½•è®­ç»ƒçš„è¯Šæ–­ä¸–ç•Œæ¨¡å‹ï¼Œæ ¹æ®ç—…äººç—…å²å’Œæ¨èçš„æ£€æŸ¥é¡¹ç›®ç”Ÿæˆæ£€æŸ¥ç»“æœï¼Œä½œä¸ºçœŸå®çš„è¯Šæ–­è®­ç»ƒå’Œè¯„ä»·çš„è™šæ‹Ÿä¸´åºŠç¯å¢ƒï¼›ï¼ˆiiï¼‰æˆ‘ä»¬é€šè¿‡ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒDiagAgentï¼Œå­¦ä¹ è¯Šæ–­ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–ä¿¡æ¯äº§é‡å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬å¼•å…¥äº†DiagBenchï¼Œä¸€ä¸ªåŒ…å«750ä¸ªç—…ä¾‹çš„è¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬åŒ»ç”ŸéªŒè¯çš„æ£€æŸ¥å»ºè®®ï¼Œä»¥åŠ99ä¸ªç—…ä¾‹çš„973ä¸ªåŒ»ç”Ÿæ’°å†™çš„è¯Šæ–­è¿‡ç¨‹æ‰¹æ³¨ï¼›ï¼ˆivï¼‰æˆ‘ä»¬åœ¨å¤šç§è¯Šæ–­ç¯å¢ƒä¸­å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚DiagAgentæ˜¾è‘—ä¼˜äºå…¶ä»–10ç§å…ˆè¿›çš„LLMï¼ŒåŒ…æ‹¬DeepSeek-v3å’ŒGPT-4oä»¥åŠä¸¤ç§æç¤ºå·¥ç¨‹ä»£ç†ã€‚åœ¨å•è½®è®¾ç½®ä¸­ï¼ŒDiagAgentçš„è¯Šæ–­å‡†ç¡®æ€§é«˜å‡º9.34%ï¼Œæ£€æŸ¥å»ºè®®å‘½ä¸­ç‡æé«˜44.03%ã€‚åœ¨ç«¯åˆ°ç«¯è®¾ç½®ä¸­ï¼Œå…¶è¯Šæ–­å‡†ç¡®æ€§æé«˜äº†15.12%ï¼Œæ£€æŸ¥å»ºè®®çš„F1åˆ†æ•°æé«˜äº†23.09%ã€‚åœ¨åŸºäºæ‰¹æ³¨çš„è¯„ä¼°ä¸­ï¼Œå®ƒåœ¨åŠ æƒæ‰¹æ³¨å¾—åˆ†ä¸Šè¶…è¿‡äº†æ¬¡ä¼˜æ¨¡å‹Claude-sonnet-4ï¼Œæé«˜äº†7.1%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨äº¤äº’å¼ä¸´åºŠç¯å¢ƒä¸­å­¦ä¹ ç­–ç•¥ä¼šèµ‹äºˆåŠ¨æ€å’Œå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è¯Šæ–­ç®¡ç†èƒ½åŠ›ï¼Œè¿™æ˜¯ä»…é€šè¿‡è¢«åŠ¨è®­ç»ƒæ— æ³•å®ç°çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24654v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯Šæ–­ä»£ç†äººçš„æ¡†æ¶ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç®¡ç†å¤šè½®è¯Šæ–­è¿‡ç¨‹ã€è‡ªé€‚åº”é€‰æ‹©æ£€æŸ¥å¹¶åšå‡ºæœ€ç»ˆè¯Šæ–­ã€‚è¯¥æ–¹æ³•çš„è´¡çŒ®åœ¨äºï¼šä¸€æ˜¯æå‡ºäº†DiagGymï¼Œä¸€ä¸ªç”¨ç”µå­å¥åº·è®°å½•è®­ç»ƒçš„è¯Šç—…ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç—…äººç—…å²å’Œæ¨èçš„æ£€æŸ¥é¡¹ç›®å‘å‡ºæ£€æŸ¥ç»“æœï¼Œä½œä¸ºè™šæ‹Ÿä¸´åºŠç¯å¢ƒï¼Œç”¨äºç°å®è¯Šæ–­çš„è®­ç»ƒå’Œè¯„ä¼°ï¼›äºŒæ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒDiagAgentï¼Œå­¦ä¹ è¯Šæ–­ç­–ç•¥ï¼Œä¼˜åŒ–ä¿¡æ¯äº§é‡å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼›ä¸‰æ˜¯å¼•å…¥äº†DiagBenchè¯Šæ–­åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«750ä¸ªç—…ä¾‹ï¼Œå…¶ä¸­99ä¸ªç—…ä¾‹çš„è¯Šæ–­è¿‡ç¨‹é™„æœ‰åŒ»ç”ŸéªŒè¯çš„æ£€æŸ¥å»ºè®®å’Œ973æ¡åŒ»ç”Ÿæ’°å†™çš„è¯„ä»·å‡†åˆ™ï¼›å››æ˜¯å±•ç¤ºäº†åœ¨ä¸åŒè¯Šæ–­ç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚ä¸åä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæ— è®ºæ˜¯åœ¨å•è½®è®¾ç½®è¿˜æ˜¯ç«¯åˆ°ç«¯çš„è®¾ç½®ä¸‹ï¼ŒDiagAgentåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ£€æŸ¥å»ºè®®æ–¹é¢éƒ½è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåœ¨äº¤äº’å¼ä¸´åºŠç¯å¢ƒä¸­å­¦ä¹ æ”¿ç­–èµ‹äºˆäº†åŠ¨æ€ä¸”ä¸´åºŠæ„ä¹‰çš„è¯Šæ–­ç®¡ç†èƒ½åŠ›ï¼Œè¿™æ˜¯é€šè¿‡è¢«åŠ¨è®­ç»ƒæ— æ³•å®ç°çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯Šæ–­ä»£ç†äººçš„æ–°æ–¹æ³•ï¼Œä½¿å…¶èƒ½è¿›è¡Œå¤šè½®è¯Šæ–­ã€è‡ªé€‚åº”é€‰æ‹©æ£€æŸ¥å’Œåšå‡ºæœ€ç»ˆè¯Šæ–­ã€‚</li>
<li>å¼•å…¥äº†DiagGymï¼Œä¸€ä¸ªåŸºäºç”µå­å¥åº·è®°å½•çš„è¯Šç—…ä¸–ç•Œæ¨¡å‹ï¼Œæ¨¡æ‹ŸçœŸå®ä¸´åºŠç¯å¢ƒã€‚</li>
<li>é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒDiagAgentï¼Œæé«˜äº†è¯Šæ–­ç­–ç•¥çš„ä¿¡æ¯è·å–å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†åŒ…å«å¤šç§ç—…ä¾‹çš„DiagBenchè¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œå«æœ‰åŒ»ç”ŸéªŒè¯çš„æ£€æŸ¥å»ºè®®å’Œè¯Šæ–­è¿‡ç¨‹è¯„ä»·å‡†åˆ™ã€‚</li>
<li>DiagAgentåœ¨è¯Šæ–­å‡†ç¡®æ€§ã€æ£€æŸ¥å»ºè®®ç­‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>DiagAgentåœ¨å•è½®å’Œç«¯åˆ°ç«¯çš„è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºæ›´é«˜çš„è¯Šæ–­ç®¡ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5def342abff30e5bfc04063a7913289e" align="middle">
<img src="https://picx.zhimg.com/v2-b9fb66ebf917bf0ffa4aaa0ab99d8679" align="middle">
<img src="https://picx.zhimg.com/v2-eb71ce53ae58c8a700d8de32543de658" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Optimizing-Retrieval-for-RAG-via-Reinforced-Contrastive-Learning"><a href="#Optimizing-Retrieval-for-RAG-via-Reinforced-Contrastive-Learning" class="headerlink" title="Optimizing Retrieval for RAG via Reinforced Contrastive Learning"></a>Optimizing Retrieval for RAG via Reinforced Contrastive Learning</h2><p><strong>Authors:Jiawei Zhou, Lei Chen</strong></p>
<p>As retrieval-augmented generation (RAG) becomes increasingly widespread, the role of information retrieval (IR) is shifting from retrieving information for human users to retrieving contextual knowledge for artificial intelligence (AI) systems, where relevance becomes difficult to define or annotate beforehand. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through trialand-feedback Reinforced contrastive learning. Unlike prior approaches that rely on annotated or synthetic data for supervised fine-tuning, R3 enables the retriever to dynamically explore and optimize relevance within the RAG environment. During training, the retrieved results interact with the environment to produce contrastive signals that automatically guide the retrieverâ€™s self-improvement. Extensive experiments across diverse tasks demonstrate that R3 improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day. </p>
<blockquote>
<p>éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„æ—¥ç›Šæ™®åŠï¼Œä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰çš„è§’è‰²æ­£åœ¨ä»ä¸ºäººç±»ç”¨æˆ·æ£€ç´¢ä¿¡æ¯è½¬å˜ä¸ºä¸ºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿæ£€ç´¢ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œè¿™ä½¿å¾—æå‰å®šä¹‰æˆ–æ ‡æ³¨ç›¸å…³æ€§å˜å¾—å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†R3ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è¯•éªŒå’Œåé¦ˆå¼ºåŒ–å¯¹æ¯”å­¦ä¹ é’ˆå¯¹RAGä¼˜åŒ–çš„æ£€ç´¢æ¡†æ¶ã€‚ä¸åŒäºä»¥å‰ä¾èµ–äºæ³¨é‡Šæˆ–åˆæˆæ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒçš„æ–¹æ³•ï¼ŒR3ä½¿æ£€ç´¢å™¨èƒ½å¤Ÿåœ¨RAGç¯å¢ƒä¸­åŠ¨æ€åœ°æ¢ç´¢å’Œä¼˜åŒ–ç›¸å…³æ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ£€ç´¢ç»“æœä¸ç¯å¢ƒäº’åŠ¨äº§ç”Ÿå¯¹æ¯”ä¿¡å·ï¼Œè‡ªåŠ¨å¼•å¯¼æ£€ç´¢å™¨è‡ªæˆ‘æ”¹è¿›ã€‚åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR3åœ¨åŸå§‹æ£€ç´¢å™¨çš„åŸºç¡€ä¸Šæé«˜äº†5.2%çš„RAGæ€§èƒ½ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ£€ç´¢å™¨4.9%ï¼ŒåŒæ—¶å®ç°äº†ä¸åŸºäºåè®­ç»ƒæˆ–æŒ‡ä»¤è°ƒæ•´çš„LLMçš„LLMå¢å¼ºæ£€ç´¢å’ŒRAGç³»ç»Ÿç›¸å½“çš„ç»“æœã€‚å®ƒé«˜æ•ˆå®ç”¨ï¼Œåªéœ€4ä¸ªGPUï¼Œå¹¶åœ¨ä¸€å¤©å†…å®Œæˆè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24652v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ™®åŠï¼Œä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰çš„è§’è‰²æ­£åœ¨ä»ä¸ºäººç±»ç”¨æˆ·æ£€ç´¢ä¿¡æ¯è½¬å˜ä¸ºä¸ºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿæ£€ç´¢ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†R3ï¼Œä¸€ä¸ªé€šè¿‡è¯•éªŒå’Œåé¦ˆå¼ºåŒ–å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–RAGçš„æ£€ç´¢æ¡†æ¶ã€‚R3ä½¿æ£€ç´¢å™¨èƒ½å¤Ÿåœ¨RAGç¯å¢ƒä¸­åŠ¨æ€æ¢ç´¢å’Œä¼˜åŒ–ç›¸å…³æ€§ï¼Œä¸åŒäºä»¥å¾€ä¾èµ–æ³¨é‡Šæˆ–åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒçš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒR3åœ¨RAGæ€§èƒ½ä¸Šè¾ƒåŸæ£€ç´¢å™¨æé«˜äº†5.2%ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ£€ç´¢å™¨4.9%ï¼ŒåŒæ—¶å®ç°äº†ä¸åŸºäºé¢„è®­ç»ƒæˆ–æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºçš„æ£€ç´¢å’ŒRAGç³»ç»Ÿçš„ç›¸å½“ç»“æœã€‚å®ƒé«˜æ•ˆå®ç”¨ï¼Œåªéœ€4ä¸ªGPUï¼Œè®­ç»ƒæ—¶é—´ä¸€å¤©å³å¯å®Œæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰çš„è§’è‰²æ­£åœ¨ä»ä¸ºäººç±»ç”¨æˆ·æ£€ç´¢ä¿¡æ¯è½¬å˜ä¸ºä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿæ£€ç´¢ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚</li>
<li>R3æ˜¯ä¸€ä¸ªé’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¼˜åŒ–çš„æ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡è¯•éªŒå’Œåé¦ˆå¼ºåŒ–å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>R3ä½¿æ£€ç´¢å™¨èƒ½å¤Ÿåœ¨RAGç¯å¢ƒä¸­åŠ¨æ€æ¢ç´¢å’Œä¼˜åŒ–ç›¸å…³æ€§ã€‚</li>
<li>R3ä¸åŒäºä»¥å¾€ä¾èµ–æ³¨é‡Šæˆ–åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>R3åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ£€ç´¢å™¨ï¼Œå¹¶å®ç°äº†ä¸åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›¸å½“ç»“æœã€‚</li>
<li>R3çš„è®­ç»ƒæ•ˆç‡é«˜ï¼Œåªéœ€4ä¸ªGPUï¼Œä¸€å¤©å³å¯å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9131e5ff4a66c61f19a0fe4abc9313fe" align="middle">
<img src="https://picx.zhimg.com/v2-7786ac0ae5d355f201e61df7de3818cf" align="middle">
<img src="https://picx.zhimg.com/v2-434250c83c3902f7721464f103d3e4d9" align="middle">
<img src="https://picx.zhimg.com/v2-f82057946ae45aac3d5ebf4dc08d1303" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Transparent-Reasoning-What-Drives-Faithfulness-in-Large-Language-Models"><a href="#Towards-Transparent-Reasoning-What-Drives-Faithfulness-in-Large-Language-Models" class="headerlink" title="Towards Transparent Reasoning: What Drives Faithfulness in Large   Language Models?"></a>Towards Transparent Reasoning: What Drives Faithfulness in Large   Language Models?</h2><p><strong>Authors:Teague McMillan, Gabriele Dominici, Martin Gjoreski, Marc Langheinrich</strong></p>
<p>Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„è§£é‡Šé€šå¸¸ä¸èƒ½çœŸå®åœ°åæ˜ å…¶é¢„æµ‹èƒŒåçš„å› ç´ ã€‚åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œè¿™ç§ä¸å¿ å®çš„è§£é‡Šç‰¹åˆ«æˆé—®é¢˜ï¼šçœç•¥é‡è¦ä¸´åºŠçº¿ç´¢æˆ–æ©ç›–è™šå‡æ·å¾„çš„è§£é‡Šå¯èƒ½ä¼šç ´ååŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»ï¼Œå¹¶å¯¼è‡´å†³ç­–æ”¯æŒçš„å¤±è¯¯ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨ç†å’Œè®­ç»ƒæ—¶é—´çš„é€‰æ‹©å¦‚ä½•å½±å“è§£é‡Šçš„å¿ å®åº¦ï¼Œé‡ç‚¹å…³æ³¨ä»ä¸šè€…åœ¨éƒ¨ç½²æ—¶å¯ä»¥æ§åˆ¶çš„å› ç´ ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸‰ä¸ªLLMï¼ˆGPT-4.1-miniã€LLaMA 70Bã€LLaMA 8Bï¼‰ï¼Œåˆ†åˆ«æ˜¯BBQï¼ˆç¤¾ä¼šåè§ï¼‰å’ŒMedQAï¼ˆåŒ»å­¦è®¸å¯é—®é¢˜ï¼‰ï¼Œå¹¶æ“ä½œäº†å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œç±»å‹ã€æç¤ºç­–ç•¥ä»¥åŠè®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼šï¼ˆiï¼‰å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡å¯¹æ¨¡å‹çš„å¿ å®åº¦æœ‰é‡å¤§å½±å“ï¼›ï¼ˆiiï¼‰å¿ å®åº¦å¯¹æç¤ºè®¾è®¡æ•æ„Ÿï¼›ï¼ˆiiiï¼‰åœ¨MedQAä¸Šï¼ŒæŒ‡ä»¤è°ƒæ•´é˜¶æ®µæé«˜äº†æµ‹é‡çš„å¿ å®åº¦ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºäº†è§£å¦‚ä½•åœ¨æ•æ„Ÿé¢†åŸŸæé«˜LLMçš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24236v2">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM   Lifecycle: Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„è§£é‡Šå¾€å¾€ä¸èƒ½çœŸå®åœ°åæ˜ å…¶é¢„æµ‹èƒŒåçš„å› ç´ ã€‚åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œä¸å¿ å®çš„è§£é‡Šç‰¹åˆ«æˆé—®é¢˜ï¼šè§£é‡Šä¸­é—æ¼é‡è¦çš„ä¸´åºŠçº¿ç´¢æˆ–æ©ç›–é”™è¯¯çš„æ·å¾„ä¼šç ´ååŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»ï¼Œå¹¶å¯èƒ½å¯¼è‡´å†³ç­–æ”¯æŒä¸å½“ã€‚æœ¬ç ”ç©¶æ¢è®¨æ¨ç†å’Œè®­ç»ƒæ—¶æœŸçš„é€‰æ‹©å¦‚ä½•å½±å“è§£é‡Šçš„å¿ å®åº¦ï¼Œé‡ç‚¹æ˜¯åœ¨éƒ¨ç½²æ—¶å®è·µè€…å¯æ§åˆ¶çš„å› ç´ ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§LLMï¼ˆGPT-4.1-miniã€LLaMA 70Bã€LLaMA 8Bï¼‰åœ¨ä¸¤ä¸ªæ•°æ®é›†ï¼ˆBBQç¤¾ä¼šåè§å’ŒMedQAåŒ»å­¦è®¸å¯é—®é¢˜ï¼‰ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ“ä½œäº†å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œç±»å‹ã€æç¤ºç­–ç•¥ä»¥åŠè®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼šä¸€ã€å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡æ˜¾è‘—å½±å“æ¨¡å‹çš„å¿ å®åº¦ï¼›äºŒã€å¿ å®åº¦å¯¹æç¤ºè®¾è®¡æ•æ„Ÿï¼›ä¸‰ã€æŒ‡ä»¤è°ƒæ•´é˜¶æ®µæé«˜äº†åœ¨MedQAä¸Šçš„å¿ å®åº¦æµ‹é‡å€¼ã€‚è¿™äº›å‘ç°å¯¹äºæé«˜æ•æ„Ÿé¢†åŸŸLLMçš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§£é‡Šå¯èƒ½ä¸çœŸå®åæ˜ å…¶é¢„æµ‹èƒŒåçš„å› ç´ ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ç¯å¢ƒä¸­éœ€ç‰¹åˆ«å…³æ³¨ã€‚</li>
<li>å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡å¯¹æ¨¡å‹çš„å¿ å®åº¦æœ‰é‡è¦å½±å“ã€‚</li>
<li>å¿ å®åº¦å¯¹æç¤ºè®¾è®¡æ•æ„Ÿï¼Œæç¤ºç­–ç•¥çš„é€‰æ‹©ä¼šå½±å“æ¨¡å‹è§£é‡Šçš„å¯ä¿¡åº¦ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¯¹æé«˜æ¨¡å‹çš„å¿ å®åº¦æœ‰ç§¯æä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚</li>
<li>LLMåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°åæ˜ äº†å…¶è§£é‡Šå¿ å®åº¦çš„å·®å¼‚ï¼Œéœ€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>æé«˜LLMçš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦å¯¹äºå…¶åœ¨æ•æ„Ÿé¢†åŸŸçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca379376d958fa488cfc7b12463ad1da" align="middle">
<img src="https://picx.zhimg.com/v2-99a05e74cfacac98b95473984bc5dcca" align="middle">
<img src="https://picx.zhimg.com/v2-7ab7b93fb26830065bb014c0460ee308" align="middle">
<img src="https://picx.zhimg.com/v2-a141f3c5c43b7dfec11dc97c5691e85c" align="middle">
<img src="https://picx.zhimg.com/v2-ff47b0358375d7b80fdf4d1e19e2e236" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AfriMTEB-and-AfriE5-Benchmarking-and-Adapting-Text-Embedding-Models-for-African-Languages"><a href="#AfriMTEB-and-AfriE5-Benchmarking-and-Adapting-Text-Embedding-Models-for-African-Languages" class="headerlink" title="AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for   African Languages"></a>AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for   African Languages</h2><p><strong>Authors:Kosei Uemura, Miaoran Zhang, David Ifeoluwa Adelani</strong></p>
<p>Text embeddings are an essential building component of several NLP tasks such as retrieval-augmented generation which is crucial for preventing hallucinations in LLMs. Despite the recent release of massively multilingual MTEB (MMTEB), African languages remain underrepresented, with existing tasks often repurposed from translation benchmarks such as FLORES clustering or SIB-200. In this paper, we introduce AfriMTEB â€“ a regional expansion of MMTEB covering 59 languages, 14 tasks, and 38 datasets, including six newly added datasets. Unlike many MMTEB datasets that include fewer than five languages, the new additions span 14 to 56 African languages and introduce entirely new tasks, such as hate speech detection, intent detection, and emotion classification, which were not previously covered. Complementing this, we present AfriE5, an adaptation of the instruction-tuned mE5 model to African languages through cross-lingual contrastive distillation. Our evaluation shows that AfriE5 achieves state-of-the-art performance, outperforming strong baselines such as Gemini-Embeddings and mE5. </p>
<blockquote>
<p>æ–‡æœ¬åµŒå…¥æ˜¯å¤šä¸ªNLPä»»åŠ¡çš„å…³é”®æ„å»ºç»„ä»¶ï¼Œå¦‚å¢å¼ºç”Ÿæˆä»»åŠ¡ï¼Œè¿™å¯¹äºé˜²æ­¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§†è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘å‘å¸ƒäº†å¤§è§„æ¨¡å¤šè¯­è¨€æ–‡æœ¬åµŒå…¥ï¼ˆMMTEBï¼‰ï¼Œä½†éæ´²è¯­è¨€ä»ç„¶è¢«ä½ä¼°ï¼Œç°æœ‰ä»»åŠ¡é€šå¸¸æ˜¯ä»ç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚FLORESèšç±»æˆ–SIB-200ï¼‰é‡æ–°åˆ©ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AfriMTEBâ€”â€”MMTEBçš„åŒºåŸŸæ‰©å±•ï¼Œè¦†ç›–59ç§è¯­è¨€ã€14é¡¹ä»»åŠ¡å’Œ38ä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ–°å¢çš„å…­ä¸ªæ•°æ®é›†ã€‚ä¸è®¸å¤šåŒ…å«ä¸åˆ°äº”ç§è¯­è¨€çš„MMTEBæ•°æ®é›†ä¸åŒï¼Œæ–°å¢éƒ¨åˆ†æ¶µç›–äº†14è‡³56ç§éæ´²è¯­è¨€ï¼Œå¹¶å¼•å…¥äº†ä»¥å‰æœªæ¶µç›–çš„å…¨æ–°ä»»åŠ¡ï¼Œå¦‚ä»‡æ¨è¨€è®ºæ£€æµ‹ã€æ„å›¾æ£€æµ‹å’Œæƒ…ç»ªåˆ†ç±»ç­‰ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AfriE5ï¼Œè¿™æ˜¯é€šè¿‡è·¨è¯­è¨€å¯¹æ¯”è’¸é¦å¯¹éæ´²è¯­è¨€è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜çš„mE5æ¨¡å‹çš„é€‚åº”ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAfriE5è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¶…è¿‡äº†å¦‚Gemini-Embeddingså’ŒmE5ç­‰å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23896v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†éæ´²è¯­è¨€åœ¨NLPä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ•°æ®é›†å’Œæ¨¡å‹çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†AfriMTEBå’ŒAfriE5ï¼Œå‰è€…æ˜¯ä¸€ä¸ªè¦†ç›–59ç§è¯­è¨€ã€14ä¸ªä»»åŠ¡å’Œ38ä¸ªæ•°æ®é›†çš„åŒºåŸŸæ‰©å±•æ•°æ®é›†ï¼Œåè€…æ˜¯å¯¹mE5æ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´ã€‚è¯¥è®ºæ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºä¸ºéæ´²è¯­è¨€æä¾›äº†æ›´å…¨é¢çš„NLPä»»åŠ¡æ•°æ®é›†å’Œé€‚åº”æ€§æ¨¡å‹ï¼Œä»è€Œä¿ƒè¿›äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åµŒå…¥æ˜¯å¤šä¸ªNLPä»»åŠ¡çš„åŸºæœ¬æ„å»ºç»„ä»¶ï¼Œå¯¹äºé¢„é˜²LLMä¸­çš„å¹»è§‰å°¤ä¸ºé‡è¦ã€‚</li>
<li>å°½ç®¡è¿‘æœŸæ¨å‡ºäº†å¤§è§„æ¨¡å¤šè¯­ç§æ–‡æœ¬åµŒå…¥åº“ï¼ˆMMTEBï¼‰ï¼Œä½†éæ´²è¯­è¨€ä»ç„¶è¢«å¿½è§†ã€‚</li>
<li>ç°æœ‰ä»»åŠ¡ç»å¸¸ä»ç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚FLORESèšç±»æˆ–SIB-200ï¼‰ä¸­é‡æ–°å®šä½ã€‚<br>4.AfriMTEBæ˜¯MMTEBçš„åŒºåŸŸæ‰©å±•ç‰ˆæœ¬ï¼Œè¦†ç›–59ç§è¯­è¨€ã€14ä¸ªä»»åŠ¡å’Œ38ä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ–°æ·»åŠ çš„å…­ä¸ªæ•°æ®é›†ã€‚</li>
<li>ä¸è®¸å¤šåªåŒ…å«å°‘æ•°è¯­è¨€çš„MMTEBæ•°æ®é›†ä¸åŒï¼Œæ–°æ·»åŠ çš„æ•°æ®é›†æ¶µç›–äº†å¤šè¾¾56ç§éæ´²è¯­è¨€ã€‚</li>
<li>æ–°å¢ä»»åŠ¡åŒ…æ‹¬ä»‡æ¨è¨€è®ºæ£€æµ‹ã€æ„å›¾æ£€æµ‹å’Œæƒ…ç»ªåˆ†ç±»ç­‰å…ˆå‰æœªæ¶µç›–çš„å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c7341ab1142084641071d99a9380607" align="middle">
<img src="https://picx.zhimg.com/v2-bb014463c1400f73e890a0c014923437" align="middle">
<img src="https://picx.zhimg.com/v2-89bf8b1d513885815683043ce22e2ce2" align="middle">
<img src="https://picx.zhimg.com/v2-15d359386c2a18920d2636da0938c497" align="middle">
<img src="https://picx.zhimg.com/v2-553b31003c00e68ace26fa32d04729c3" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Understanding-In-Context-Learning-Beyond-Transformers-An-Investigation-of-State-Space-and-Hybrid-Architectures"><a href="#Understanding-In-Context-Learning-Beyond-Transformers-An-Investigation-of-State-Space-and-Hybrid-Architectures" class="headerlink" title="Understanding In-Context Learning Beyond Transformers: An Investigation   of State Space and Hybrid Architectures"></a>Understanding In-Context Learning Beyond Transformers: An Investigation   of State Space and Hybrid Architectures</h2><p><strong>Authors:Shenran Wang, Timothy Tin-Long Tse, Jian Zhu</strong></p>
<p>We perform in-depth evaluations of in-context learning (ICL) on state-of-the-art transformer, state-space, and hybrid large language models over two categories of knowledge-based ICL tasks. Using a combination of behavioral probing and intervention-based methods, we have discovered that, while LLMs of different architectures can behave similarly in task performance, their internals could remain different. We discover that function vectors (FVs) responsible for ICL are primarily located in the self-attention and Mamba layers, and speculate that Mamba2 uses a different mechanism from FVs to perform ICL. FVs are more important for ICL involving parametric knowledge retrieval, but not for contextual knowledge understanding. Our work contributes to a more nuanced understanding across architectures and task types. Methodologically, our approach also highlights the importance of combining both behavioural and mechanistic analyses to investigate LLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„è½¬æ¢å™¨ã€çŠ¶æ€ç©ºé—´å’Œæ··åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ä¸¤ç±»åŸºäºçŸ¥è¯†çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»»åŠ¡çš„æ·±å…¥è¯„ä¼°ã€‚é€šè¿‡ç»“åˆè¡Œä¸ºæ¢æµ‹å’ŒåŸºäºå¹²é¢„çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶ä¸åŒæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šå¯èƒ½è¡¨ç°ç›¸ä¼¼ï¼Œä½†å®ƒä»¬çš„å†…éƒ¨ç»“æ„å¯èƒ½ä»ç„¶å­˜åœ¨å·®å¼‚ã€‚æˆ‘ä»¬å‘ç°ï¼Œè´Ÿè´£ICLçš„åŠŸèƒ½å‘é‡ï¼ˆFVï¼‰ä¸»è¦ä½äºè‡ªæ³¨æ„åŠ›å’Œç›å§†å·´å±‚ï¼Œå¹¶æ¨æµ‹ç›å§†å·´2ä½¿ç”¨ä¸åŒäºFVçš„æœºåˆ¶æ¥æ‰§è¡ŒICLã€‚FVå¯¹äºæ¶‰åŠå‚æ•°çŸ¥è¯†æ£€ç´¢çš„ICLæ›´ä¸ºé‡è¦ï¼Œä½†å¯¹äºä¸Šä¸‹æ–‡çŸ¥è¯†ç†è§£åˆ™ä¸æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰åŠ©äºæ›´ç»†è‡´åœ°äº†è§£ä¸åŒæ¶æ„å’Œä»»åŠ¡ç±»å‹ã€‚åœ¨æ–¹æ³•è®ºä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¼ºè°ƒäº†ç»“åˆè¡Œä¸ºåˆ†æå’Œæœºåˆ¶åˆ†ææ¥è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23006v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬å¯¹ä¸åŒæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸¤ç±»çŸ¥è¯†å‹ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç»“åˆè¡Œä¸ºæ¢æµ‹å’Œå¹²é¢„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶ä¸åŒæ¶æ„çš„LLMåœ¨ä»»åŠ¡æ€§èƒ½ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œä½†å®ƒä»¬çš„å†…éƒ¨ç»“æ„å¯èƒ½ä¸åŒã€‚æˆ‘ä»¬å‘ç°åœ¨è‡ªæ³¨æ„åŠ›å±‚å’ŒMambaå±‚ä¸­å­˜åœ¨ä¸»è¦è´Ÿè´£ä¸Šä¸‹æ–‡å­¦ä¹ çš„åŠŸèƒ½å‘é‡ï¼ˆFVsï¼‰ï¼Œå¹¶æ¨æµ‹Mamba2ä½¿ç”¨ä¸åŒäºFVsçš„æœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åŠŸèƒ½å‘é‡å¯¹äºæ¶‰åŠå‚æ•°çŸ¥è¯†æ£€ç´¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ›´ä¸ºé‡è¦ï¼Œä½†å¯¹äºä¸Šä¸‹æ–‡çŸ¥è¯†ç†è§£åˆ™ä¸é‚£ä¹ˆé‡è¦ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹ä¸åŒçš„æ¶æ„å’Œä»»åŠ¡ç±»å‹æä¾›äº†æ›´å¾®å¦™çš„äº†è§£ï¼Œå¹¶åœ¨æ–¹æ³•è®ºä¸Šå¼ºè°ƒäº†ç»“åˆè¡Œä¸ºåˆ†æå’Œæœºåˆ¶åˆ†æè°ƒæŸ¥LLMèƒ½åŠ›çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¸åŒæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å‹ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œä½†å†…éƒ¨ç»“æ„å¯èƒ½ä¸åŒã€‚</li>
<li>åŠŸèƒ½å‘é‡ï¼ˆFVsï¼‰åœ¨è‡ªæ³¨æ„åŠ›å±‚å’ŒMambaå±‚ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¯¹ä¸Šä¸‹æ–‡å­¦ä¹ æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>Mamba2å¯èƒ½ä½¿ç”¨ä¸åŒäºåŠŸèƒ½å‘é‡çš„æœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>åŠŸèƒ½å‘é‡å¯¹äºæ¶‰åŠå‚æ•°çŸ¥è¯†æ£€ç´¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ›´ä¸ºé‡è¦ï¼Œä½†å¯¹ä¸Šä¸‹æ–‡çŸ¥è¯†ç†è§£çš„è´¡çŒ®è¾ƒå°ã€‚</li>
<li>æ·±å…¥ç ”ç©¶ä¸åŒæ¶æ„å’Œä»»åŠ¡ç±»å‹æœ‰åŠ©äºæ›´ç²¾ç»†åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç‰¹ç‚¹ã€‚</li>
<li>ç»“åˆè¡Œä¸ºæ¢æµ‹å’Œå¹²é¢„æ–¹æ³•çš„ç ”ç©¶æ–¹æ³•æœ‰åŠ©äºæ›´å…¨é¢åœ°äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92fb5222bdfd58620a78183e1e996bcc" align="middle">
<img src="https://picx.zhimg.com/v2-ca72836767df55d3cc2df08ed218f90d" align="middle">
<img src="https://picx.zhimg.com/v2-1378c019f65234fc8668442760f9fb89" align="middle">
<img src="https://picx.zhimg.com/v2-44bbf8e7f4ab7837bf66c269f115b779" align="middle">
<img src="https://picx.zhimg.com/v2-cf3cba6a00eb281672ea14f2d15f5f0c" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-Semantics-How-Temporal-Biases-Shape-Retrieval-in-Transformer-and-State-Space-Models"><a href="#Beyond-Semantics-How-Temporal-Biases-Shape-Retrieval-in-Transformer-and-State-Space-Models" class="headerlink" title="Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and   State-Space Models"></a>Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and   State-Space Models</h2><p><strong>Authors:Anooshka Bajaj, Deven Mahesh Mistry, Sahaj Singh Maini, Yash Aggarwal, Zoran Tiganj</strong></p>
<p>In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information. Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically, we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences, models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an illustration of how these biases can enable temporal separation and episodic retrieval. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ å—åˆ°æ—¶é—´å’Œè¯­ä¹‰å…³ç³»çš„å½±å“ï¼Œè¿™å†³å®šäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•æ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™é¡¹å·¥ä½œæ¢ç©¶äº†å¤šç§é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬transformerå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰åŒºåˆ†å’Œæ£€ç´¢æ—¶é—´ä¸Šåˆ†ç¦»çš„äº‹ä»¶çš„èƒ½åŠ›ï¼Œä¸äººç±»æƒ…æ™¯è®°å¿†ç±»ä¼¼ï¼Œäººç±»èƒ½å¤Ÿå›å¿†ç‰¹å®šäº‹ä»¶çš„æƒ…æ™¯æ˜¯å› ä¸ºèƒ½å¤ŸåŒºåˆ†å‘ç”Ÿåœ¨ä¸åŒæ—¶é—´çš„äº‹ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡è®¾å®šåŒ…å«å¤šæ¬¡å‡ºç°ç›¸åŒæ ‡è®°çš„åºåˆ—ï¼Œå¹¶ä¸”è¯¥æ ‡è®°ä¼šåœ¨åºåˆ—æœ«å°¾å†æ¬¡å‡ºç°ï¼Œæ¥æç¤ºæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å›ºå®šè¿™äº›é‡å¤æ ‡è®°çš„ä½ç½®å¹¶æ’åˆ—å…¶ä»–æ‰€æœ‰æ ‡è®°ï¼Œæ¶ˆé™¤äº†è¯­ä¹‰æ··æ·†ï¼Œå¹¶éš”ç¦»äº†å¯¹ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„æ—¶é—´æ•ˆåº”ã€‚åœ¨å„ç§åºåˆ—ä¸­ï¼Œæ¨¡å‹å§‹ç»ˆå°†æœ€é«˜æ¦‚ç‡åˆ†é…ç»™é‡å¤æ ‡è®°ä¹‹åçš„æ ‡è®°ï¼Œä½†å¯¹é è¿‘è¾“å…¥å¼€å§‹æˆ–ç»“æŸçš„æ ‡è®°æœ‰æ˜æ˜¾çš„åå‘ã€‚ä¸€é¡¹æ¶ˆèå®éªŒæ˜¾ç¤ºï¼Œè¿™ç§ç°è±¡ä¸transformerä¸­çš„å½’çº³å¤´æœ‰å…³ã€‚å°†åˆ†ææ‰©å±•åˆ°å…·æœ‰éƒ¨åˆ†é‡å çš„ç‹¬ç‰¹è¯­ä¹‰èƒŒæ™¯è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒåµŒå…¥åœ¨æç¤ºä¸­é—´çš„è®°å¿†æ£€ç´¢å¯é æ€§è¾ƒä½ã€‚å°½ç®¡æ¶æ„å­˜åœ¨å·®å¼‚ï¼Œä½†çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œtransformeræ¨¡å‹æ˜¾ç¤ºå‡ºç±»ä¼¼çš„æ—¶é—´åè§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ·±åŒ–äº†å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ—¶é—´åè§çš„ç†è§£ï¼Œå¹¶è¯´æ˜äº†è¿™äº›åè§å¦‚ä½•èƒ½å¤Ÿå®ç°æ—¶é—´åˆ†ç¦»å’Œæƒ…æ™¯æ£€ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22752v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ å—æ—¶é—´å’Œè¯­ä¹‰å…³ç³»çš„å½±å“ï¼Œè¿™å†³å®šäº†æ¨¡å‹å¦‚ä½•æ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒLLMï¼ˆåŒ…æ‹¬è½¬æ¢å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰åŒºåˆ†å’Œæ£€ç´¢æ—¶é—´ä¸Šåˆ†ç¦»çš„äº‹ä»¶çš„èƒ½åŠ›ã€‚é€šè¿‡å›ºå®šé‡å¤æ ‡è®°çš„ä½ç½®å¹¶æ’åˆ—å…¶ä»–æ‰€æœ‰æ ‡è®°ï¼Œæˆ‘ä»¬ç§»é™¤äº†è¯­ä¹‰æ··æ·†å¹¶éš”ç¦»äº†æ—¶é—´å¯¹ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å½±å“ã€‚æ¨¡å‹ä¸€è‡´åœ°å°†æœ€é«˜æ¦‚ç‡æ”¾åœ¨é‡å¤æ ‡è®°ä¹‹åçš„æ ‡è®°ä¸Šï¼Œä½†å¯¹é è¿‘è¾“å…¥å¼€å§‹æˆ–ç»“æŸçš„æ ‡è®°æœ‰æ˜æ˜¾çš„åå‘ã€‚è½¬æ¢å™¨ä¸­çš„è¿™ç§ç°è±¡ä¸å½’çº³å¤´æœ‰å…³ã€‚æ‰©å±•åˆ°å…·æœ‰éƒ¨åˆ†é‡å çš„ç‹¬ç‰¹è¯­ä¹‰ä¸Šä¸‹æ–‡è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒåµŒå…¥æç¤ºä¸­é—´çš„è®°å¿†æ£€ç´¢å¯é æ€§è¾ƒä½ã€‚å°½ç®¡æ¶æ„å­˜åœ¨å·®å¼‚ï¼Œä½†çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œè½¬æ¢å™¨æ¨¡å‹æ˜¾ç¤ºå‡ºç›¸ä¼¼çš„æ—¶åºåå·®ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ·±åŒ–äº†å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ—¶åºåå·®çš„ç†è§£ï¼Œå¹¶å±•ç¤ºäº†è¿™äº›åå·®å¦‚ä½•ä¿ƒè¿›æ—¶åºåˆ†ç¦»å’Œæƒ…æ™¯æ£€ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ å—æ—¶é—´å’Œè¯­ä¹‰å…³ç³»çš„å½±å“ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å’Œæ£€ç´¢æ—¶é—´ä¸Šåˆ†ç¦»çš„äº‹ä»¶ã€‚</li>
<li>æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°æ—¶ï¼Œä¼šè€ƒè™‘æ—¶é—´å’Œè¯­ä¹‰å…³ç³»ã€‚</li>
<li>æ¨¡å‹å¯¹é‡å¤æ ‡è®°åŠå…¶ä½ç½®çš„è¯†åˆ«è¡¨ç°å‡ºæ˜æ˜¾çš„åå‘ã€‚</li>
<li>è½¬æ¢å™¨æ¨¡å‹ä¸­çš„æ—¶åºåå·®ä¸å½’çº³å¤´æœ‰å…³ã€‚</li>
<li>åœ¨å…·æœ‰éƒ¨åˆ†é‡å çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡å‹çš„è®°å¿†æ£€ç´¢èƒ½åŠ›ä¼šé™ä½ã€‚</li>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œè½¬æ¢å™¨æ¨¡å‹åœ¨æ—¶åºåå·®ä¸Šè¡¨ç°å‡ºç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6d9ccf9f017dd5012e9da0e7bcc66c8" align="middle">
<img src="https://picx.zhimg.com/v2-60f3e63ac950db6c011e684875fa95d8" align="middle">
<img src="https://picx.zhimg.com/v2-81f7567b41dcb5a583960cf19cba48dc" align="middle">
<img src="https://picx.zhimg.com/v2-703bb8624e5887d337cc4aca56da3df6" align="middle">
<img src="https://picx.zhimg.com/v2-5753a6d5e48fdcac1242b4e3fe0b226b" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Can-ChatGPT-be-a-good-follower-of-academic-paradigms-Research-quality-evaluations-in-conflicting-areas-of-sociology"><a href="#Can-ChatGPT-be-a-good-follower-of-academic-paradigms-Research-quality-evaluations-in-conflicting-areas-of-sociology" class="headerlink" title="Can ChatGPT be a good follower of academic paradigms? Research quality   evaluations in conflicting areas of sociology"></a>Can ChatGPT be a good follower of academic paradigms? Research quality   evaluations in conflicting areas of sociology</h2><p><strong>Authors:Mike Thelwall, Ralph Schroeder, Meena Dhanda</strong></p>
<p>Purpose: It has become increasingly likely that Large Language Models (LLMs) will be used to score the quality of academic publications to support research assessment goals in the future. This may cause problems for fields with competing paradigms since there is a risk that one may be favoured, causing long term harm to the reputation of the other. Design&#x2F;methodology&#x2F;approach: To test whether this is plausible, this article uses 17 ChatGPTs to evaluate up to 100 journal articles from each of eight pairs of competing sociology paradigms (1490 altogether). Each article was assessed by prompting ChatGPT to take one of five roles: paradigm follower, opponent, antagonistic follower, antagonistic opponent, or neutral. Findings: Articles were scored highest by ChatGPT when it followed the aligning paradigm, and lowest when it was told to devalue it and to follow the opposing paradigm. Broadly similar patterns occurred for most of the paradigm pairs. Follower ChatGPTs displayed only a small amount of favouritism compared to neutral ChatGPTs, but articles evaluated by an opposing paradigm ChatGPT had a substantial disadvantage. Research limitations: The data covers a single field and LLM. Practical implications: The results confirm that LLM instructions for research evaluation should be carefully designed to ensure that they are paradigm-neutral to avoid accidentally resolving conflicts between paradigms on a technicality by devaluing one sideâ€™s contributions. Originality&#x2F;value: This is the first demonstration that LLMs can be prompted to show a partiality for academic paradigms. </p>
<blockquote>
<p>ç›®çš„ï¼šæœªæ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½è¢«ç”¨æ¥è¯„ä¼°å­¦æœ¯å‡ºç‰ˆç‰©çš„è´¨é‡ï¼Œä»¥æ”¯æŒç ”ç©¶è¯„ä¼°ç›®æ ‡ã€‚è¿™å¯èƒ½ä¼šç»™å…·æœ‰ç«äº‰èŒƒå¼çš„ç ”ç©¶é¢†åŸŸå¸¦æ¥é—®é¢˜ï¼Œå› ä¸ºå­˜åœ¨ä¸€ç§èŒƒå¼å¯èƒ½æ›´å—æ¬¢è¿çš„é£é™©ï¼Œä»è€Œå¯¹å¦ä¸€ç§èŒƒå¼çš„å£°èª‰é€ æˆé•¿æœŸæŸå®³ã€‚è®¾è®¡&#x2F;æ–¹æ³•è®º&#x2F;æ–¹æ³•ï¼šä¸ºäº†æµ‹è¯•è¿™ç§å‡è®¾æ˜¯å¦å¯è¡Œï¼Œæœ¬æ–‡ä½¿ç”¨17ä¸ªChatGPTè¯„ä¼°äº†æ¥è‡ªå…«ç»„ç«äº‰ç¤¾ä¼šå­¦èŒƒå¼çš„æ¯ç»„çš„æœ€å¤š100ç¯‡æœŸåˆŠæ–‡ç« ï¼ˆå…±1490ç¯‡ï¼‰ã€‚æ¯ç¯‡æ–‡ç« éƒ½æ˜¯é€šè¿‡æç¤ºChatGPTé‡‡å–äº”ç§è§’è‰²ä¹‹ä¸€è¿›è¡Œè¯„ä¼°çš„ï¼šèŒƒå¼è¿½éšè€…ã€å¯¹æ‰‹ã€å¯¹æŠ—æ€§è¿½éšè€…ã€å¯¹æŠ—æ€§å¯¹æ‰‹æˆ–ä¸­ç«‹è€…ã€‚ç ”ç©¶ç»“æœï¼šå½“ChatGPTéµå¾ªä¸€è‡´èŒƒå¼æ—¶ï¼Œæ–‡ç« å¾—åˆ†æœ€é«˜ï¼Œè€Œå½“å®ƒè¢«è¦æ±‚è´¬ä½å®ƒå¹¶éµå¾ªå¯¹ç«‹èŒƒå¼æ—¶ï¼Œæ–‡ç« å¾—åˆ†æœ€ä½ã€‚å¤§å¤šæ•°èŒƒå¼å¯¹çš„æ¨¡å¼å¤§è‡´ç›¸åŒã€‚ä¸ä¸­æ€§ChatGPTç›¸æ¯”ï¼Œè¿½éšè€…ChatGPTåªè¡¨ç°å‡ºå°‘é‡çš„åçˆ±ï¼Œä½†ç”±å¯¹ç«‹èŒƒå¼ChatGPTè¯„ä¼°çš„æ–‡ç« å­˜åœ¨æ˜æ˜¾çš„åŠ£åŠ¿ã€‚ç ”ç©¶å±€é™æ€§ï¼šæ•°æ®ä»…é™äºä¸€ä¸ªé¢†åŸŸå’ŒLLMã€‚å®è·µæ„ä¹‰ï¼šç»“æœè¡¨æ˜ï¼Œå¯¹äºç ”ç©¶è¯„ä¼°çš„LLMæŒ‡ä»¤åº”ç²¾å¿ƒè®¾è®¡ï¼Œä»¥ç¡®ä¿å®ƒä»¬æ˜¯èŒƒå¼ä¸­ç«‹çš„ï¼Œä»¥é¿å…åœ¨æŠ€æœ¯ä¸Šæ„å¤–è§£å†³èŒƒå¼ä¹‹é—´çš„å†²çªè€Œè´¬ä½ä¸€æ–¹çš„è´¡çŒ®ã€‚åŸåˆ›æ€§&#x2F;ä»·å€¼ï¼šè¿™æ˜¯é¦–æ¬¡è¯æ˜LLMå¯ä»¥è¢«æç¤ºè¡¨ç°å‡ºå¯¹å­¦æœ¯èŒƒå¼çš„åå¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22426v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœªæ¥å¯èƒ½ä¼šè¢«ç”¨æ¥è¯„ä¼°å­¦æœ¯å‡ºç‰ˆç‰©çš„è´¨é‡ï¼Œä»¥æ”¯æŒç ”ç©¶è¯„ä¼°ç›®æ ‡ã€‚è¿™å¯èƒ½ä¼šç»™å…·æœ‰ç«äº‰èŒƒå¼çš„ç ”ç©¶é¢†åŸŸå¸¦æ¥é—®é¢˜ï¼Œå› ä¸ºå­˜åœ¨åå‘æŸä¸€èŒƒå¼çš„é£é™©ï¼Œä»è€Œé•¿æœŸæŸå®³å…¶ä»–èŒƒå¼çš„å£°èª‰ã€‚æœ¬æ–‡ä½¿ç”¨17ä¸ªChatGPTå¯¹å…«å¯¹ç«äº‰ç¤¾ä¼šå­¦èŒƒå¼ä¸­çš„æ¯å¯¹èŒƒå¼çš„100ç¯‡æ–‡ç« è¿›è¡Œæµ‹è¯„ï¼ˆå…±1490ç¯‡æ–‡ç« ï¼‰ã€‚æ–‡ç« é€šè¿‡æç¤ºChatGPTé‡‡å–äº”ç§è§’è‰²è¿›è¡Œè¯„ä¼°ï¼šèŒƒå¼è¿½éšè€…ã€åå¯¹è€…ã€å¯¹æŠ—è¿½éšè€…ã€å¯¹æŠ—åå¯¹è€…å’Œä¸­ç«‹è€…ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ChatGPTéµå¾ªä¸€è‡´èŒƒå¼æ—¶ï¼Œæ–‡ç« è¯„åˆ†æœ€é«˜ï¼›å½“å…¶è¢«æŒ‡ç¤ºè´¬ä½è¯¥èŒƒå¼å¹¶éµå¾ªç›¸åèŒƒå¼æ—¶ï¼Œè¯„åˆ†æœ€ä½ã€‚å¤§å¤šæ•°èŒƒå¼å¯¹çš„è¯„ä»·éƒ½è¡¨ç°å‡ºç±»ä¼¼çš„æ¨¡å¼ã€‚ç›¸æ¯”ä¸­ç«‹ChatGPTï¼Œè¿½éšè€…ChatGPTåªè¡¨ç°å‡ºå°‘é‡çš„å€¾å‘æ€§ï¼Œä½†ç”±åå¯¹èŒƒå¼ChatGPTè¯„ä¼°çš„æ–‡ç« åˆ™æ˜æ˜¾å¤„äºåŠ£åŠ¿ã€‚å› æ­¤ï¼Œä¸ºç¡®ä¿æŠ€æœ¯å±‚é¢é¿å…èŒƒå¼é—´çš„å†²çªå¹¶é¿å…å•æ–¹é¢è´¬æŸï¼Œåº”ç²¾å¿ƒè®¾è®¡LLMçš„ç ”ç©¶è¯„ä¼°æŒ‡ä»¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½è¢«ç”¨äºè¯„ä¼°å­¦æœ¯å‡ºç‰ˆç‰©è´¨é‡ï¼Œä»¥æ”¯æŒç ”ç©¶è¯„ä¼°ç›®æ ‡ã€‚</li>
<li>åœ¨å…·æœ‰ç«äº‰èŒƒå¼çš„ç ”ç©¶é¢†åŸŸï¼Œä½¿ç”¨LLMè¿›è¡Œè´¨é‡è¯„ä¼°å¯èƒ½å¼•å‘é—®é¢˜ã€‚</li>
<li>ChatGPTåœ¨è¯„ä¼°å­¦æœ¯æ–‡ç« æ—¶ï¼Œå¯¹ç¬¦åˆå…¶æŒ‡ä»¤çš„èŒƒå¼è¡¨ç°å‡ºå€¾å‘æ€§ã€‚</li>
<li>å½“æŒ‡ä»¤ä¸ChatGPTçš„èŒƒå¼ä¸ä¸€è‡´æ—¶ï¼Œæ–‡ç« çš„è¯„åˆ†ä¼šè¾ƒä½ã€‚</li>
<li>ç›¸æ¯”ä¸­ç«‹ChatGPTï¼Œè¿½éšèŒƒå¼çš„ChatGPTè¯„åˆ†åé«˜ï¼Œè€Œåå¯¹èŒƒå¼çš„ChatGPTè¯„åˆ†åä½ã€‚</li>
<li>ç ”ç©¶å±€é™åœ¨äºä»…æ¶µç›–å•ä¸€é¢†åŸŸå’ŒLLMï¼Œæœªæ¥ç ”ç©¶éœ€è€ƒè™‘æ›´å¤šé¢†åŸŸå’Œä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä¸ºç¡®ä¿å…¬æ­£è¯„ä¼°ï¼Œè®¾è®¡LLMæŒ‡ä»¤æ—¶éœ€è€ƒè™‘èŒƒå¼ä¸­ç«‹æ€§ï¼Œé¿å…å•æ–¹é¢è´¬æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32fc607c50a3d159e9b347b5442d92ce" align="middle">
<img src="https://picx.zhimg.com/v2-f1e07ea4f09381a0c3e67c471ef91a00" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transformer-Key-Value-Memories-Are-Nearly-as-Interpretable-as-Sparse-Autoencoders"><a href="#Transformer-Key-Value-Memories-Are-Nearly-as-Interpretable-as-Sparse-Autoencoders" class="headerlink" title="Transformer Key-Value Memories Are Nearly as Interpretable as Sparse   Autoencoders"></a>Transformer Key-Value Memories Are Nearly as Interpretable as Sparse   Autoencoders</h2><p><strong>Authors:Mengyu Ye, Jun Suzuki, Tatsuro Inaba, Tatsuki Kuribayashi</strong></p>
<p>Recent interpretability work on large language models (LLMs) has been increasingly dominated by a feature-discovery approach with the help of proxy modules. Then, the quality of features learned by, e.g., sparse auto-encoders (SAEs), is evaluated. This paradigm naturally raises a critical question: do such learned features have better properties than those already represented within the original model parameters, and unfortunately, only a few studies have made such comparisons systematically so far. In this work, we revisit the interpretability of feature vectors stored in feed-forward (FF) layers, given the perspective of FF as key-value memories, with modern interpretability benchmarks. Our extensive evaluation revealed that SAE and FFs exhibits a similar range of interpretability, although SAEs displayed an observable but minimal improvement in some aspects. Furthermore, in certain aspects, surprisingly, even vanilla FFs yielded better interpretability than the SAEs, and features discovered in SAEs and FFs diverged. These bring questions about the advantage of SAEs from both perspectives of feature quality and faithfulness, compared to directly interpreting FF feature vectors, and FF key-value parameters serve as a strong baseline in modern interpretability research. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘å¯è§£é‡Šæ€§ç ”ç©¶è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨äº†ä¸€ç§å€ŸåŠ©ä»£ç†æ¨¡å—çš„ç‰¹å¾å‘ç°æ–¹æ³•ã€‚ç„¶åï¼Œå¯¹ä¾‹å¦‚ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ‰€å­¦ä¹ ç‰¹å¾çš„è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚è¿™ç§æ¨¡å¼è‡ªç„¶å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šè¿™äº›å­¦ä¹ åˆ°çš„ç‰¹å¾æ˜¯å¦æ¯”åŸå§‹æ¨¡å‹å‚æ•°ä¸­å·²è¡¨ç¤ºçš„ç‰¹å¾å…·æœ‰æ›´å¥½çš„å±æ€§ï¼Œé—æ†¾çš„æ˜¯ï¼Œè¿„ä»Šä¸ºæ­¢åªæœ‰å°‘æ•°ç ”ç©¶è¿›è¡Œäº†è¿™æ ·çš„ç³»ç»Ÿæ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å‰é¦ˆï¼ˆFFï¼‰ä½œä¸ºé”®å€¼å­˜å‚¨å™¨çš„è§’åº¦é‡æ–°å®¡è§†äº†å­˜å‚¨åœ¨å‰é¦ˆå±‚ä¸­çš„ç‰¹å¾å‘é‡çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä½¿ç”¨ç°ä»£å¯è§£é‡Šæ€§åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSAEå’ŒFFå…·æœ‰ç›¸ä¼¼çš„å¯è§£é‡Šæ€§èŒƒå›´ï¼Œå°½ç®¡SAEåœ¨æŸäº›æ–¹é¢æ˜¾ç¤ºå‡ºå¯è§‚å¯Ÿåˆ°çš„ä½†å¾®å°çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨æŸäº›æ–¹é¢ï¼Œç”šè‡³æ™®é€šçš„å‰é¦ˆä¹Ÿèƒ½äº§ç”Ÿæ¯”SAEæ›´å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”SAEå’Œå‰é¦ˆä¸­å‘ç°çš„ç‰¹å¾å­˜åœ¨åˆ†æ­§ã€‚è¿™äº›å…³äºç‰¹å¾è´¨é‡å’Œå¿ å®åº¦çš„é—®é¢˜ï¼Œä½¿æˆ‘ä»¬è´¨ç–‘ä¸ç›´æ¥è§£é‡Šå‰é¦ˆç‰¹å¾å‘é‡ç›¸æ¯”ï¼ŒSAEçš„ä¼˜åŠ¿ä½•åœ¨ï¼Œå¹¶ä¸”å‰é¦ˆçš„é”®å€¼å‚æ•°åœ¨ç°ä»£å¯è§£é‡Šæ€§ç ”ç©¶ä¸­ä½œä¸ºå¼ºæœ‰åŠ›çš„åŸºçº¿å­˜åœ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22332v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­äºé€šè¿‡ä»£ç†æ¨¡å—å‘ç°ç‰¹å¾çš„æ–¹æ³•ï¼Œå¹¶è¯„ä¼°å¦‚ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ç­‰å­¦ä¹ åˆ°çš„ç‰¹å¾è´¨é‡ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›å­¦ä¹ åˆ°çš„ç‰¹å¾æ˜¯å¦æ¯”åŸå§‹æ¨¡å‹å‚æ•°ä¸­çš„ç‰¹å¾å…·æœ‰æ›´å¥½çš„æ€§è´¨ï¼Œä»…æœ‰å°‘æ•°ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒã€‚æœ¬ç ”ç©¶ä»å‰é¦ˆï¼ˆFFï¼‰å±‚ä½œä¸ºé”®å€¼è®°å¿†çš„è§’åº¦é‡æ–°å®¡è§†äº†å…¶å¯è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨ç°ä»£å¯è§£é‡Šæ€§åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼ŒSAEå’ŒFFå±‚å…·æœ‰ç›¸ä¼¼çš„å¯è§£é‡Šæ€§èŒƒå›´ï¼Œä½†SAEåœ¨æŸäº›æ–¹é¢ç•¥æœ‰æ”¹è¿›ã€‚åŒæ—¶ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨æŸäº›æ–¹é¢ï¼Œå³ä½¿æ˜¯æœ€åŸºç¡€çš„å‰é¦ˆå±‚ä¹Ÿè¡¨ç°å‡ºäº†æ¯”SAEæ›´å¥½çš„å¯è§£é‡Šæ€§ï¼Œä¸”SAEå’Œå‰é¦ˆå±‚ä¸­å‘ç°çš„ç‰¹å¾å­˜åœ¨åˆ†æ­§ã€‚è¿™ä¸ºç°ä»£å¯è§£é‡Šæ€§ç ”ç©¶å¸¦æ¥äº†æ–°çš„ç–‘é—®å’Œè®¨è®ºï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å¾è´¨é‡å’Œå¿ å®åº¦æ–¹é¢çš„å¯¹æ¯”ä¸Šï¼Œè€ŒFFé”®å€¼å‚æ•°ä¸ºå¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸLLMçš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦å…³æ³¨ç‰¹å¾å‘ç°æ–¹æ³•ï¼Œå°¤å…¶æ˜¯é€šè¿‡ä»£ç†æ¨¡å—è¿›è¡Œã€‚</li>
<li>SAEå’Œå…¶ä»–æ–¹æ³•ï¼ˆå¦‚FFå±‚ï¼‰åœ¨å¯è§£é‡Šæ€§ä¸Šå…·æœ‰ç›¸ä¼¼çš„èŒƒå›´ã€‚</li>
<li>SAEåœ¨æŸäº›æ–¹é¢ç•¥æœ‰æ”¹è¿›ï¼Œä½†åœ¨å…¶ä»–æ–¹é¢ï¼ŒåŸºç¡€FFå±‚è¡¨ç°å‡ºæ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>SAEå’Œå‰é¦ˆå±‚å‘ç°çš„ç‰¹å¾å­˜åœ¨åˆ†æ­§ã€‚</li>
<li>SAEçš„ä¼˜åŠ¿åœ¨ç‰¹å¾è´¨é‡å’Œå¿ å®åº¦æ–¹é¢çš„å¯¹æ¯”å°šå¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>FFé”®å€¼å‚æ•°ä¸ºç°ä»£å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a7e56bfa014739f9f22afda11793790" align="middle">
<img src="https://picx.zhimg.com/v2-145299a05fc70f0a7c9c2eb2901fd887" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Linear-Attention-with-Optimized-GPU-Kernel-Implementation"><a href="#Transformer-Based-Linear-Attention-with-Optimized-GPU-Kernel-Implementation" class="headerlink" title="Transformer Based Linear Attention with Optimized GPU Kernel   Implementation"></a>Transformer Based Linear Attention with Optimized GPU Kernel   Implementation</h2><p><strong>Authors:Armin Gerami, Ramani Duraiswami</strong></p>
<p>The original softmax-based attention mechanism (regular attention) in the extremely successful Transformer architecture computes attention between $N$ tokens, each embedded in a $D$-dimensional head, with a time complexity of $O(N^2D)$. Given the success of Transformers, improving their runtime during both training and inference is a popular research area. One such approach is the introduction of the linear attention (LA) mechanisms, which offers a linear time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to regular attention. However, LA in practice lags behind its theoretical efficiency. We propose a novel method for LAâ€™s forward and backward passes, along with a highly-optimized CUDA implementation. Our approach outperforms the state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6 times. We validate these improvements in both single-layer and end-to-end settings by training a 1.4 billion parameter language model, which demonstrates similar expressivity to regular attention on major reasoning benchmarks. </p>
<blockquote>
<p>åŸå§‹çš„åŸºäºsoftmaxçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¸¸è§„æ³¨æ„åŠ›ï¼‰åœ¨æå…¶æˆåŠŸçš„Transformeræ¶æ„ä¸­è®¡ç®—äº†Nä¸ªæ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œæ¯ä¸ªæ ‡è®°åµŒå…¥åœ¨Dç»´å¤´ä¸­ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(N^2D)ã€‚é‰´äºTransformerçš„æˆåŠŸï¼Œæé«˜å…¶åœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´çš„è¿è¡Œæ—¶é—´æ˜¯å½“å‰çƒ­é—¨çš„ç ”ç©¶é¢†åŸŸã€‚ä¸€ç§æ–¹æ³•æ˜¯å¼•å…¥çº¿æ€§æ³¨æ„åŠ›ï¼ˆLAï¼‰æœºåˆ¶ï¼Œå®ƒæä¾›çº¿æ€§æ—¶é—´å¤æ‚åº¦O(ND^2)ï¼Œå¹¶å·²è¯æ˜å…¶ç²¾åº¦ä¸å¸¸è§„æ³¨æ„åŠ›ç›¸å½“ã€‚ç„¶è€Œï¼ŒLAåœ¨å®è·µä¸­çš„æ•ˆç‡å¹¶æ²¡æœ‰è¾¾åˆ°å…¶ç†è®ºæ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºLAå‰å‘å’Œåå‘ä¼ é€’çš„æ–°æ–¹æ³•ï¼Œä»¥åŠé«˜åº¦ä¼˜åŒ–çš„CUDAå®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º3.3å€ï¼Œå¹¶å°†å†…å­˜æ¶ˆè€—å‡å°‘äº†3.6å€ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒåŒ…å«1.4äº¿å‚æ•°çš„å•ä¸€è¯­è¨€æ¨¡å‹éªŒè¯äº†è¿™äº›æ”¹è¿›æƒ…å†µï¼Œè¯¥æ¨¡å‹åœ¨ä¸»è¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºä¸å¸¸è§„æ³¨æ„åŠ›ç›¸ä¼¼çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21956v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ¶æ„ä¸­çš„åŸå§‹softmaxæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¸¸è§„æ³¨æ„åŠ›ï¼‰åŠå…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(N^2D)ã€‚ä¸ºæé«˜Transformeråœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´çš„è¿è¡Œæ•ˆç‡ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†çº¿æ€§æ³¨æ„åŠ›ï¼ˆLAï¼‰æœºåˆ¶ï¼Œå…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(ND^2)ï¼Œå¹¶å…·æœ‰ç›¸å½“é«˜çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼ŒLAåœ¨å®é™…åº”ç”¨ä¸­å¹¶æœªè¾¾åˆ°ç†è®ºæ•ˆç‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„LAå‰å‘å’Œåå‘ä¼ æ’­æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–çš„CUDAå®ç°ã€‚è¯¥æ–¹æ³•åœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†3.3å€ï¼Œå¹¶é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚é€šè¿‡è®­ç»ƒåŒ…å«1.4äº¿å‚æ•°çš„åŸºå‡†è¯­è¨€æ¨¡å‹éªŒè¯äº†è¿™äº›æ”¹è¿›ï¼Œè¯¥æ¨¡å‹åœ¨ä¸»è¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸å¸¸è§„æ³¨æ„åŠ›ç›¸ä¼¼çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„ä¸­çš„å¸¸è§„softmaxæ³¨æ„åŠ›æœºåˆ¶å…·æœ‰O(N^2D)çš„æ—¶é—´å¤æ‚åº¦ã€‚</li>
<li>çº¿æ€§æ³¨æ„åŠ›ï¼ˆLAï¼‰æœºåˆ¶æ—¨åœ¨æé«˜Transformerçš„æ•ˆç‡ï¼Œå…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(ND^2)ã€‚</li>
<li>LAåœ¨å®é™…åº”ç”¨ä¸­æœªå®ç°ç†è®ºæ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„LAå‰å‘å’Œåå‘ä¼ æ’­æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é«˜åº¦ä¼˜åŒ–çš„CUDAå®ç°ï¼Œåœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†3.3å€ã€‚</li>
<li>è¯¥æ–¹æ³•é™ä½äº†å†…å­˜æ¶ˆè€—ï¼Œå‡å°‘äº†3.6å€ã€‚</li>
<li>é€šè¿‡è®­ç»ƒåŒ…å«1.4äº¿å‚æ•°çš„åŸºå‡†è¯­è¨€æ¨¡å‹éªŒè¯äº†è¿™äº›æ”¹è¿›ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºä¸å¸¸è§„æ³¨æ„åŠ›ç›¸å½“çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4189531247089ef6a58d800610b4cdfe" align="middle">
<img src="https://picx.zhimg.com/v2-d5e3bc81b550d0aca6e1786e5a57acac" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Enabling-Robust-In-Context-Memory-and-Rapid-Task-Adaptation-in-Transformers-with-Hebbian-and-Gradient-Based-Plasticity"><a href="#Enabling-Robust-In-Context-Memory-and-Rapid-Task-Adaptation-in-Transformers-with-Hebbian-and-Gradient-Based-Plasticity" class="headerlink" title="Enabling Robust In-Context Memory and Rapid Task Adaptation in   Transformers with Hebbian and Gradient-Based Plasticity"></a>Enabling Robust In-Context Memory and Rapid Task Adaptation in   Transformers with Hebbian and Gradient-Based Plasticity</h2><p><strong>Authors:Siddharth Chaudhary</strong></p>
<p>Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ ä½œä¸ºè§„æ¨¡çš„ä¸€ç§æ¶Œç°æ•ˆåº”ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¾èµ–äºé™æ€æƒé‡ã€‚ä¸æ­¤ç›¸åï¼Œç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ã€‚æˆ‘ä»¬è°ƒæŸ¥æ˜ç¡®çš„ã€å—ç”Ÿç‰©å¯å‘çš„å¯å¡‘æ€§æ˜¯å¦å¯ä»¥èµ‹äºˆTransformeræ›´å¿«çš„åºåˆ—é€‚åº”æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»…å¯¹è§£ç å™¨Transformerè¿›è¡Œå¢å¼ºï¼Œé€šè¿‡ï¼ˆiï¼‰ç¥ç»è°ƒèŠ‚çš„èµ«å¸ƒè§„åˆ™æˆ–ï¼ˆiiï¼‰æ®µç­‰äººæå‡ºçš„åŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶è¿›è¡Œå¿«é€Ÿæƒé‡æ¨¡å—æ›´æ–°ï¼ˆ2023å¹´ï¼‰ã€‚åœ¨å¤åˆ¶ã€å›å½’å’Œå°‘é•œå¤´åˆ†ç±»ä»»åŠ¡ï¼ˆCIFAR-FSã€Omniglotï¼‰ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å§‹ç»ˆå®ç°æ›´ä½çš„æŸå¤±å’Œæ›´å¼ºçš„å°‘é•œå¤´æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æœŸä¿¡ç”¨åˆ†é…ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚å½“å…³è”çŸ­æš‚ä¸”çº¿æ€§å¯åˆ†æ—¶ï¼Œé™æ€æƒé‡å°±è¶³å¤Ÿäº†ï¼Œè¿™ä¸ºå¯å¡‘æ€§ä½•æ—¶æœ‰å¸®åŠ©è®¾å®šäº†æ˜ç¡®çš„è¾¹ç•Œæ¡ä»¶ã€‚å¯¹å­¦åˆ°çš„è°ƒåˆ¶ä¿¡å·çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„è§„åˆ™ç»´æŒäº†å¤§è€ŒæŒä¹…çš„æ›´æ–°ï¼Œè€Œèµ«å¸ƒå¯å¡‘æ€§åˆ™å›´ç»•æ˜¾è‘—äº‹ä»¶è¿›è¡Œå°–é”çš„é—¸é—¨æ§åˆ¶ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œæ˜ç¡®çš„å¯å¡‘æ€§é€šè¿‡å®ç°å¿«é€Ÿã€ç‰¹å®šçš„ä»»åŠ¡é€‚åº”æ¥è¡¥å……æ³¨æ„åŠ›ï¼Œå¹¶æ˜ç¡®äº†ä¸åŒå¯å¡‘æ€§æœºåˆ¶ä½•æ—¶æœ€ä¸ºæœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21908v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ çš„è§„æ¨¡æ•ˆåº”ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¾èµ–é™æ€æƒé‡ã€‚ä¸æ­¤ç›¸åï¼Œç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶æ˜ç¡®çš„ç”Ÿç‰©å¯å‘å¯å¡‘æ€§æ˜¯å¦èƒ½ä¸ºTransformerèµ‹äºˆæ›´å¿«çš„åºåˆ—é€‚åº”æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹ä»…è§£ç å™¨Transformerè¿›è¡Œäº†å¢å¼ºï¼Œé€šè¿‡ï¼ˆiï¼‰ç¥ç»è°ƒèŠ‚çš„èµ«å¸ƒè§„åˆ™æˆ–ï¼ˆiiï¼‰æ®µç­‰äººçš„åŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶è¿›è¡Œå¿«é€Ÿæƒé‡æ¨¡å—æ›´æ–°ï¼ˆDu et al.ï¼Œ2023ï¼‰ã€‚åœ¨å¤åˆ¶ã€å›å½’å’Œå°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆCIFAR-FSã€Omniglotï¼‰ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å§‹ç»ˆå®ç°äº†è¾ƒä½çš„æŸå¤±å’Œæ›´å¼ºçš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æœŸä¿¡ç”¨åˆ†é…ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚å½“å…³è”çŸ­æš‚ä¸”çº¿æ€§å¯åˆ†æ—¶ï¼Œé™æ€æƒé‡å°±è¶³å¤Ÿäº†ï¼Œè¿™ä¸ºå¯å¡‘æ€§ä½•æ—¶æœ‰å¸®åŠ©è®¾å®šäº†æ˜ç¡®çš„è¾¹ç•Œæ¡ä»¶ã€‚å¯¹å­¦åˆ°çš„è°ƒåˆ¶ä¿¡å·çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„è§„åˆ™ç»´æŒäº†å¤§è€ŒæŒä¹…çš„æ›´æ–°ï¼Œè€Œèµ«å¸ƒå¯å¡‘æ€§åˆ™å›´ç»•æ˜¾è‘—äº‹ä»¶è¿›è¡Œå°–é”çš„é—¨æ§ã€‚æ€»ä½“è€Œè¨€ï¼Œæ˜ç¡®çš„å¯å¡‘æ€§é€šè¿‡å®ç°å¿«é€Ÿçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ¥è¡¥å……æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶æ˜ç¡®äº†ä¸åŒå¯å¡‘æ€§æœºåˆ¶ä½•æ—¶æœ€æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ çš„è§„æ¨¡æ•ˆåº”ï¼Œä½†æ¨ç†æ—¶ä¾èµ–é™æ€æƒé‡ã€‚</li>
<li>ç”Ÿç‰©ç³»ç»Ÿé€šè¿‡çªè§¦å¯å¡‘æ€§æŒç»­é€‚åº”ã€‚</li>
<li>å¼•å…¥æ˜ç¡®çš„ç”Ÿç‰©å¯å‘å¯å¡‘æ€§ä»¥åŠ å¿«Transformerçš„åºåˆ—é€‚åº”æ€§ã€‚</li>
<li>é€šè¿‡èµ«å¸ƒè§„åˆ™å’ŒåŸºäºæ¢¯åº¦çš„å¯å¡‘æ€§æœºåˆ¶å¯¹Transformerè¿›è¡Œå¢å¼ºã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡ä¸­ï¼Œèµ«å¸ƒå¯å¡‘æ€§å®ç°ä½æŸå¤±å’Œå¼ºæ³›åŒ–ï¼ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°åœ¨é•¿æœŸä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>å½“å…³è”ç®€å•æ—¶ï¼Œé™æ€æƒé‡è¶³å¤Ÿï¼Œå¤æ‚æ—¶åˆ™éœ€è¦å¼•å…¥å¯å¡‘æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98f7c219ad00b8686480ff0359ebbf3d" align="middle">
<img src="https://picx.zhimg.com/v2-24e70160591cba7b8cf6cfcabd4cc8e5" align="middle">
<img src="https://picx.zhimg.com/v2-ab1adb10ead2cd34b657fd05540733ec" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluating-ChatGPTâ€™s-Performance-in-Classifying-Pneumonia-from-Chest-X-Ray-Images"><a href="#Evaluating-ChatGPTâ€™s-Performance-in-Classifying-Pneumonia-from-Chest-X-Ray-Images" class="headerlink" title="Evaluating ChatGPTâ€™s Performance in Classifying Pneumonia from Chest   X-Ray Images"></a>Evaluating ChatGPTâ€™s Performance in Classifying Pneumonia from Chest   X-Ray Images</h2><p><strong>Authors:Pragna Prahallad, Pranathi Prahallad</strong></p>
<p>In this study, we evaluate the ability of OpenAIâ€™s gpt-4o model to classify chest X-ray images as either NORMAL or PNEUMONIA in a zero-shot setting, without any prior fine-tuning. A balanced test set of 400 images (200 from each class) was used to assess performance across four distinct prompt designs, ranging from minimal instructions to detailed, reasoning-based prompts. The results indicate that concise, feature-focused prompts achieved the highest classification accuracy of 74%, whereas reasoning-oriented prompts resulted in lower performance. These findings highlight that while ChatGPT exhibits emerging potential for medical image interpretation, its diagnostic reliability remains limited. Continued advances in visual reasoning and domain-specific adaptation are required before such models can be safely applied in clinical practice. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„gpt-4oæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ï¼Œå°†å…¶åˆ†ä¸ºæ­£å¸¸æˆ–è‚ºç‚ä¸¤ç±»ï¼Œä¸”æ— éœ€è¿›è¡Œä»»ä½•å…ˆå‰çš„å¾®è°ƒã€‚ä½¿ç”¨åŒ…å«400å¼ å›¾åƒï¼ˆå„200å¼ ï¼‰çš„å¹³è¡¡æµ‹è¯•é›†æ¥è¯„ä¼°å››ç§ä¸åŒæç¤ºè®¾è®¡çš„æ€§èƒ½ï¼Œæç¤ºè®¾è®¡èŒƒå›´ä»ç®€å•çš„æŒ‡ä»¤åˆ°è¯¦ç»†ã€åŸºäºæ¨ç†çš„æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œç®€æ´ã€ä»¥ç‰¹å¾ä¸ºä¸­å¿ƒçš„æç¤ºè·å¾—äº†æœ€é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå³74%ï¼‰ï¼Œè€ŒåŸºäºæ¨ç†çš„æç¤ºåˆ™å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶ChatGPTåœ¨åŒ»å­¦å›¾åƒè§£é‡Šæ–¹é¢æ˜¾ç¤ºå‡ºæ–°å…´æ½œåŠ›ï¼Œä½†å…¶è¯Šæ–­å¯é æ€§ä»ç„¶æœ‰é™ã€‚åœ¨å°†è¿™ç§æ¨¡å‹å®‰å…¨åº”ç”¨äºä¸´åºŠå®è·µä¹‹å‰ï¼Œè¿˜éœ€è¦åœ¨è§†è§‰æ¨ç†å’Œç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢è¿›è¡ŒæŒç»­è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21839v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶è¯„ä¼°äº†OpenAIçš„gpt-4oæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹åˆ†ç±»èƒ¸éƒ¨Xå…‰å›¾åƒçš„èƒ½åŠ›ï¼Œæ— éœ€ä»»ä½•å…ˆå‰çš„å¾®è°ƒã€‚ä½¿ç”¨åŒ…å«400å¼ å›¾åƒï¼ˆæ¯ä¸ªç±»åˆ«å„200å¼ ï¼‰çš„å¹³è¡¡æµ‹è¯•é›†æ¥è¯„ä¼°å››ç§ä¸åŒæç¤ºè®¾è®¡çš„æ€§èƒ½ï¼Œä»ç®€å•çš„æŒ‡ä»¤åˆ°è¯¦ç»†çš„åŸºäºæ¨ç†çš„æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œç®€æ´ã€ä»¥ç‰¹å¾ä¸ºä¸­å¿ƒçš„æç¤ºè·å¾—äº†æœ€é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ï¼ˆ74%ï¼‰ï¼Œè€ŒåŸºäºæ¨ç†çš„æç¤ºåˆ™è¡¨ç°è¾ƒå·®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶ChatGPTåœ¨åŒ»å­¦å›¾åƒè§£è¯»æ–¹é¢æ˜¾ç¤ºå‡ºæ–°å…´æ½œåŠ›ï¼Œä½†å…¶è¯Šæ–­å¯é æ€§ä»ç„¶æœ‰é™ã€‚éœ€è¦è¿›ä¸€æ­¥æé«˜è§†è§‰æ¨ç†å’Œé¢†åŸŸç‰¹å®šé€‚åº”æ€§æ‰èƒ½å°†è¿™äº›æ¨¡å‹å®‰å…¨åº”ç”¨äºä¸´åºŠå®è·µã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†gpt-4oæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¯¹èƒ¸éƒ¨Xå…‰å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨åŒ…å«æ­£å¸¸å’Œè‚ºç‚ä¸¤ç±»å„200å¼ å›¾åƒçš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶äº†å››ç§ä¸åŒçš„æç¤ºè®¾è®¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç®€æ´ã€ä»¥ç‰¹å¾ä¸ºä¸­å¿ƒçš„æç¤ºè·å¾—äº†æœ€é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼ˆ74%ï¼‰ã€‚</li>
<li>åŸºäºæ¨ç†çš„æç¤ºè¡¨ç°è¾ƒå·®ï¼Œè¯Šæ–­å¯é æ€§æœ‰é™ã€‚</li>
<li>ChatGPTåœ¨åŒ»å­¦å›¾åƒè§£è¯»æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåœ¨ä»·å€¼ï¼Œä½†éœ€è¿›ä¸€æ­¥æé«˜è§†è§‰æ¨ç†å’Œé¢†åŸŸç‰¹å®šé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9488178bd8f52f32452a7b3489dbdded" align="middle">
<img src="https://picx.zhimg.com/v2-addca1df59a4afef0b70db76b8c4812f" align="middle">
<img src="https://picx.zhimg.com/v2-7843dac5b85987ccd2bb13d47365784b" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f43dd3462e62bdce8e3ed2f3f7e8aa7" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  InnovatorBench Evaluating Agents' Ability to Conduct Innovative LLM   Research
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0c575c385ccbf7ea5d985ca173235fcd" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  MARAG-R1 Beyond Single Retriever via Reinforcement-Learned Multi-Tool   Agentic Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
