<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Navigated hepatic tumor resection using intraoperative ultrasound   imaging">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-92e5e711b5c7ab70b00de81301a94f57~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304869&auth_key=1762304869-0-0-7a1abb02d01d77cd84e3da91b5b1a863&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Navigated-hepatic-tumor-resection-using-intraoperative-ultrasound-imaging"><a href="#Navigated-hepatic-tumor-resection-using-intraoperative-ultrasound-imaging" class="headerlink" title="Navigated hepatic tumor resection using intraoperative ultrasound   imaging"></a>Navigated hepatic tumor resection using intraoperative ultrasound   imaging</h2><p><strong>Authors:Karin Olthof, Theo Ruers, Tiziano Natali, Lisanne Venix, Jasper Smit, Anne den Hartor, Niels Kok, Matteo Fusaglia, Koert Kuhlmann</strong></p>
<p>Purpose: This proof-of-concept study evaluates feasibility and accuracy of an ultrasound-based navigation system for open liver surgery. Unlike most conventional systems that rely on registration to preoperative imaging, the proposed system provides navigation-guided resection using 3D models generated from intraoperative ultrasound.   Methods: A pilot study was conducted in 25 patients undergoing resection of liver metastases. The first five cases served to optimize the workflow. Intraoperatively, an electromagnetic sensor compensated for organ motion, after which an ultrasound volume was acquired. Vasculature was segmented automatically and tumors semi-automatically using region-growing (n&#x3D;15) or a deep learning algorithm (n&#x3D;5). The resulting 3D model was visualized alongside tracked surgical instruments. Accuracy was assessed by comparing the distance between surgical clips and tumors in the navigation software with the same distance on a postoperative CT of the resected specimen.   Results: Navigation was successfully established in all 20 patients. However, four cases were excluded from accuracy assessment due to intraoperative sensor detachment (n&#x3D;3) or incorrect data recording (n&#x3D;1). The complete navigation workflow was operational within 5-10 minutes. In 16 evaluable patients, 78 clip-to-tumor distances were analyzed. The median navigation accuracy was 3.2 mm [IQR: 2.8-4.8 mm], and an R0 resection was achieved in 15&#x2F;16 (93.8%) patients and one patient had an R1 vascular resection.   Conclusion: Navigation based solely on intra-operative ultrasound is feasible and accurate for liver surgery. This registration-free approach paves the way for simpler and more accurate image guidance systems. </p>
<blockquote>
<p>ç›®çš„ï¼šæœ¬æ¦‚å¿µéªŒè¯ç ”ç©¶æ—¨åœ¨è¯„ä¼°åŸºäºè¶…å£°çš„å¯¼èˆªç³»ç»Ÿåœ¨å¼€æ”¾è‚è„æ‰‹æœ¯ä¸­çš„å¯è¡Œæ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„å¤§å¤šç³»ç»Ÿä¾èµ–äºæœ¯å‰æˆåƒæ³¨å†Œä¸åŒï¼Œæ‰€æè®®çš„ç³»ç»Ÿä½¿ç”¨ä»æœ¯ä¸­è¶…å£°ç”Ÿæˆçš„3Dæ¨¡å‹æä¾›å¯¼èˆªå¼•å¯¼åˆ‡é™¤ã€‚æ–¹æ³•ï¼šåœ¨25ä¾‹æ¥å—è‚è½¬ç§»åˆ‡é™¤çš„æ‚£è€…ä¸­è¿›è¡Œäº†è¯•ç‚¹ç ”ç©¶ã€‚å‰äº”ä¸ªç—…ä¾‹ç”¨äºä¼˜åŒ–å·¥ä½œæµç¨‹ã€‚æœ¯ä¸­ï¼Œç”µç£ä¼ æ„Ÿå™¨å¯¹å™¨å®˜è¿åŠ¨è¿›è¡Œäº†è¡¥å¿ï¼Œä¹‹åè·å–äº†è¶…å£°ä½“ç§¯ã€‚è¡€ç®¡è‡ªåŠ¨åˆ†å‰²ï¼Œè‚¿ç˜¤åŠè‡ªåŠ¨ä½¿ç”¨åŒºåŸŸå¢é•¿ï¼ˆn&#x3D;15ï¼‰æˆ–æ·±åº¦å­¦ä¹ ç®—æ³•ï¼ˆn&#x3D;5ï¼‰ã€‚å°†å¾—åˆ°çš„3Dæ¨¡å‹ä¸è¿½è¸ªçš„æ‰‹æœ¯å™¨æ¢°è¿›è¡Œå¯è§†åŒ–ã€‚é€šè¿‡æ¯”è¾ƒå¯¼èˆªè½¯ä»¶ä¸­æ‰‹æœ¯å¤¹ä¸è‚¿ç˜¤ä¹‹é—´çš„è·ç¦»ä¸åˆ‡é™¤æ ‡æœ¬æœ¯åCTä¸Šçš„ç›¸åŒè·ç¦»ï¼Œæ¥è¯„ä¼°å‡†ç¡®æ€§ã€‚ç»“æœï¼šæ‰€æœ‰20åæ‚£è€…æˆåŠŸå»ºç«‹å¯¼èˆªã€‚ç„¶è€Œï¼Œç”±äºæœ¯ä¸­ä¼ æ„Ÿå™¨è„±è½ï¼ˆn&#x3D;3ï¼‰æˆ–æ•°æ®è®°å½•é”™è¯¯ï¼ˆn&#x3D;1ï¼‰ï¼Œå››ä¾‹è¢«æ’é™¤åœ¨å‡†ç¡®æ€§è¯„ä¼°ä¹‹å¤–ã€‚å®Œæ•´çš„å¯¼èˆªå·¥ä½œæµç¨‹åœ¨5-10åˆ†é’Ÿå†…è¿è¡Œã€‚åœ¨16ä¾‹å¯è¯„ä¼°çš„æ‚£è€…ä¸­ï¼Œåˆ†æäº†78ä¸ªå¤¹ç‰‡è‡³è‚¿ç˜¤çš„è·ç¦»ã€‚ä¸­ä½æ•°å¯¼èˆªç²¾åº¦ä¸º3.2æ¯«ç±³[IQRï¼š2.8-4.8æ¯«ç±³]ï¼Œåœ¨15&#x2F;16ï¼ˆ93.8%ï¼‰ä¾‹æ‚£è€…ä¸­å®ç°R0åˆ‡é™¤ï¼Œä¸€ä¾‹æ‚£è€…å‡ºç°R1è¡€ç®¡åˆ‡é™¤ã€‚ç»“è®ºï¼šä»…åŸºäºæœ¯ä¸­è¶…å£°çš„å¯¼èˆªå¯¹äºè‚è„æ‰‹æœ¯æ˜¯å¯è¡Œä¸”å‡†ç¡®çš„ã€‚è¿™ç§æ— éœ€æ³¨å†Œçš„æ–¹æ³•ä¸ºæ›´ç®€å•å’Œæ›´å‡†ç¡®çš„å›¾åƒå¼•å¯¼ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27596v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ˜¯ä¸€é¡¹å…³äºè¶…å£°å¯¼èˆªç³»ç»Ÿåœ¨å¼€æ”¾è‚è„æ‰‹æœ¯ä¸­çš„å¯è¡Œæ€§åŠå‡†ç¡®æ€§çš„æ¦‚å¿µéªŒè¯ç ”ç©¶ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¯¼èˆªå¼•å¯¼åˆ‡é™¤ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨æœ¯ä¸­è¶…å£°ç”Ÿæˆçš„3Dæ¨¡å‹è¿›è¡Œå¯¼èˆªï¼Œè€Œéä¾èµ–æœ¯å‰å½±åƒæ³¨å†Œã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿçš„å¯¼èˆªç²¾åº¦è¾ƒé«˜ï¼Œä¸”æ“ä½œç®€ä¾¿å¿«æ·ï¼Œä¸ºè‚è„æ‰‹æœ¯çš„å›¾åƒå¼•å¯¼ç³»ç»Ÿæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è¯„ä¼°äº†ä¸€ç§åŸºäºè¶…å£°çš„å¯¼èˆªç³»ç»Ÿåœ¨å¼€æ”¾è‚è„æ‰‹æœ¯ä¸­çš„åº”ç”¨ï¼Œè¯¥ç³»ç»Ÿæ— éœ€æ³¨å†Œåˆ°æœ¯å‰å½±åƒï¼Œè€Œæ˜¯åˆ©ç”¨æœ¯ä¸­è¶…å£°ç”Ÿæˆçš„3Dæ¨¡å‹è¿›è¡Œå¯¼èˆªã€‚</li>
<li>åœ¨ä¸€é¡¹é’ˆå¯¹è‚è„è½¬ç§»ç™Œåˆ‡é™¤çš„è¯•ç‚¹ç ”ç©¶ä¸­ï¼Œå¯¹25åæ‚£è€…è¿›è¡Œäº†æ‰‹æœ¯æ“ä½œï¼Œå…¶ä¸­å‰äº”ä¸ªç—…ä¾‹ç”¨äºä¼˜åŒ–å·¥ä½œæµç¨‹ã€‚</li>
<li>æœ¯ä¸­é‡‡ç”¨ç”µç£ä¼ æ„Ÿå™¨è¡¥å¿å™¨å®˜è¿åŠ¨ï¼Œå¹¶è·å–è¶…å£°ä½“ç§¯æ•°æ®ã€‚è¡€ç®¡è‡ªåŠ¨åˆ†å‰²å’Œè‚¿ç˜¤åŠè‡ªåŠ¨åˆ†å‰²é‡‡ç”¨åŒºåŸŸå¢é•¿æˆ–æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚</li>
<li>å¯¼èˆªåœ¨20åæ‚£è€…ä¸­æˆåŠŸå»ºç«‹ï¼Œå…¶ä¸­4ä¾‹å› æœ¯ä¸­ä¼ æ„Ÿå™¨è„±è½æˆ–æ•°æ®è®°å½•é”™è¯¯è€Œæœªè¢«çº³å…¥ç²¾åº¦è¯„ä¼°ã€‚</li>
<li>åœ¨å¯è¯„ä»·çš„16åæ‚£è€…ä¸­ï¼Œåˆ†æçš„æ•°æ®æ˜¾ç¤ºå¯¼èˆªç²¾åº¦ä¸­ä½æ•°ä¸º3.2æ¯«ç±³ï¼Œä¸”R0åˆ‡é™¤ç‡é«˜ï¼Œè¾¾åˆ°äº†93.8%ã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„æ“ä½œå…¨ç¨‹åœ¨5-10åˆ†é’Ÿå†…å®Œæˆï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„å®ç”¨æ€§å’Œæ“ä½œä¾¿æ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c9d2dd54d9e4bc11c3082cfdd1d9f592~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304404&auth_key=1762304404-0-0-a0a412c4e2d03ce59c2c8c1c831d4c99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c7bb65eb4649bb1ea0bb28a1b0139fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304411&auth_key=1762304411-0-0-b49218b12c7c6c46c418f2d7c542f50d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75c41f72bf00b577f2ec2d41741c8b6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304418&auth_key=1762304418-0-0-73bff5bce7a65cef48d51b9d75db71c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3876001769212c2b9b4cb95c3605a826~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304425&auth_key=1762304425-0-0-2ec8f06141ff8168931194b1fb715b3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3adf6c28adacae536f716815f2e9687f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304432&auth_key=1762304432-0-0-ec7f49ea3428f09a87b54ad75afe1dec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b236b7892a1d98c1380bccb371e70c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304438&auth_key=1762304438-0-0-f46ee93f44a0ec3d4ec38781485af620&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Context-Gated-Cross-Modal-Perception-with-Visual-Mamba-for-PET-CT-Lung-Tumor-Segmentation"><a href="#Context-Gated-Cross-Modal-Perception-with-Visual-Mamba-for-PET-CT-Lung-Tumor-Segmentation" class="headerlink" title="Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung   Tumor Segmentation"></a>Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung   Tumor Segmentation</h2><p><strong>Authors:Elena Mulero AyllÃ³n, Linlin Shen, Pierangelo Veltri, Fabrizia Gelardi, Arturo Chiti, Paolo Soda, Matteo Tortora</strong></p>
<p>Accurate lung tumor segmentation is vital for improving diagnosis and treatment planning, and effectively combining anatomical and functional information from PET and CT remains a major challenge. In this study, we propose vMambaX, a lightweight multimodal framework integrating PET and CT scan images through a Context-Gated Cross-Modal Perception Module (CGM). Built on the Visual Mamba architecture, vMambaX adaptively enhances inter-modality feature interaction, emphasizing informative regions while suppressing noise. Evaluated on the PCLT20K dataset, the model outperforms baseline models while maintaining lower computational complexity. These results highlight the effectiveness of adaptive cross-modal gating for multimodal tumor segmentation and demonstrate the potential of vMambaX as an efficient and scalable framework for advanced lung cancer analysis. The code is available at <a target="_blank" rel="noopener" href="https://github.com/arco-group/vMambaX">https://github.com/arco-group/vMambaX</a>. </p>
<blockquote>
<p>ç²¾ç¡®è¿›è¡Œè‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²å¯¹äºæé«˜è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ï¼Œå¦‚ä½•å°†PETå’ŒCTæ‰«æçš„è§£å‰–å­¦å’ŒåŠŸèƒ½ä¿¡æ¯è¿›è¡Œæœ‰æ•ˆç»“åˆä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†vMambaXï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥æ¨¡å—ï¼ˆCGMï¼‰èåˆäº†PETå’ŒCTæ‰«æå›¾åƒã€‚åŸºäºVisual Mambaæ¶æ„ï¼ŒvMambaXè‡ªé€‚åº”åœ°å¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾äº¤äº’èƒ½åŠ›ï¼Œå¼ºè°ƒä¿¡æ¯åŒºåŸŸçš„åŒæ—¶æŠ‘åˆ¶å™ªå£°ã€‚åœ¨PCLT20Kæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ã€‚è¿™äº›ç»“æœçªå‡ºäº†è‡ªé€‚åº”è·¨æ¨¡æ€é—¨æ§åœ¨å¤šæ¨¡æ€è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†vMambaXä½œä¸ºä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¡†æ¶åœ¨é«˜çº§è‚ºç™Œåˆ†æä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/arco-group/vMambaX%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/arco-group/vMambaXè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å¤šæ¨¡æ€æ¡†æ¶vMambaXï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥æ¨¡å—ï¼ˆCGMï¼‰èåˆPETå’ŒCTæ‰«æå›¾åƒã€‚vMambaXåŸºäºVisual Mambaæ¶æ„æ„å»ºï¼Œè‡ªé€‚åº”å¢å¼ºè·¨æ¨¡æ€ç‰¹å¾äº¤äº’ï¼Œå¼ºè°ƒä¿¡æ¯åŒºåŸŸåŒæ—¶æŠ‘åˆ¶å™ªå£°ã€‚åœ¨PCLT20Kæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶è®¡ç®—å¤æ‚åº¦è¾ƒä½ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†è‡ªé€‚åº”è·¨æ¨¡æ€é—¨æ§åœ¨å¤šæ¨¡æ€è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜äº†vMambaXä½œä¸ºå…ˆè¿›è‚ºç™Œåˆ†æçš„é«˜æ•ˆå¯æ‹“å±•æ¡†æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>vMambaXæ˜¯ä¸€ä¸ªåŸºäºVisual Mambaæ¶æ„çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºèåˆPETå’ŒCTæ‰«æå›¾åƒã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥æ¨¡å—ï¼ˆCGMï¼‰ï¼ŒvMambaXèƒ½è‡ªé€‚åº”å¢å¼ºè·¨æ¨¡æ€ç‰¹å¾äº¤äº’ã€‚</li>
<li>vMambaXå¼ºè°ƒä¿¡æ¯åŒºåŸŸï¼ŒåŒæ—¶æŠ‘åˆ¶å™ªå£°ï¼Œä»¥æé«˜è‚ºè‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨PCLT20Kæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒvMambaXæ¨¡å‹æ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>vMambaXè®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œå…·æœ‰é«˜æ•ˆæ€§ã€‚</li>
<li>è‡ªé€‚åº”è·¨æ¨¡æ€é—¨æ§æŠ€æœ¯åœ¨å¤šæ¨¡æ€è‚¿ç˜¤åˆ†å‰²ä¸­å±•ç°æœ‰æ•ˆæ€§ã€‚</li>
<li>vMambaXå…·æœ‰æ½œåŠ›æˆä¸ºè‚ºç™Œåˆ†æçš„é«˜æ•ˆä¸”å¯æ‹“å±•æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63b168e5b45bdefe5feaaed83edd753f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304446&auth_key=1762304446-0-0-647da1e21619275eef090d697aff4493&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad6f3f0d5241d0b35f307475fd558b2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304453&auth_key=1762304453-0-0-a72ceb577a8aed8b7a75a240847921b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aec3da78eeffa110233a02709dd1ec4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304460&auth_key=1762304460-0-0-876eded33e4bade1fab4aeb7c068ac3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-441d56500db700a12a189121ba761318~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304467&auth_key=1762304467-0-0-97cf2074ed5572981a6f1ce2f07d402a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoMViT-An-Efficient-Vision-Backbone-for-Supervised-Classification-in-Medical-Imaging"><a href="#CoMViT-An-Efficient-Vision-Backbone-for-Supervised-Classification-in-Medical-Imaging" class="headerlink" title="CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging"></a>CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging</h2><p><strong>Authors:Aon Safdar, Mohamed Saadeldin</strong></p>
<p>Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒVision Transformersï¼ˆViTsï¼‰åœ¨åŒ»å­¦æˆåƒæ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬è®¡ç®—é‡å¤§ä¸”å®¹æ˜“åœ¨å°æ•°æ®é›†ä¸Šè¿‡åº¦æ‹Ÿåˆï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoMViTï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘ä¸”å¯æ¨å¹¿çš„ä¸“ä¸ºèµ„æºå—é™åŒ»å­¦å›¾åƒåˆ†æä¼˜åŒ–çš„Vision Transformeræ¶æ„ã€‚CoMViTé›†æˆäº†å·ç§¯åˆ†è¯å™¨ã€å¯¹è§’çº¿æ©ç ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾å’ŒåŸºäºæ± åŒ–çš„åºåˆ—èšåˆï¼Œä»¥æé«˜æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶æ„ä¼˜åŒ–ï¼ŒCoMViTåœ¨åäºŒä¸ªMedMNISTæ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ä»…æœ‰~450ä¸‡ä¸ªå‚æ•°çš„å°å‹è®¾è®¡ã€‚å®ƒåŒ¹é…æˆ–è¶…è¶Šäº†æ›´æ·±çš„CNNå’ŒViTå˜ä½“ï¼Œå®ç°äº†é«˜è¾¾5-20å€çš„å‚æ•°ç¼©å‡ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚å®šæ€§Grad-CAMåˆ†æè¡¨æ˜ï¼Œå°½ç®¡CoMViTå°ºå¯¸ç´§å‡‘ï¼Œä½†å®ƒå§‹ç»ˆå…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚è¿™äº›ç»“æœçªå‡ºäº†åŸåˆ™æ€§ViTé‡æ–°è®¾è®¡çš„æ½œåŠ›ï¼Œä¸ºä½èµ„æºåŒ»å­¦æˆåƒç¯å¢ƒä¸­å¼€å‘é«˜æ•ˆå’Œå¯è§£é‡Šæ¨¡å‹æä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27442v1">PDF</a> Preprint (submitted manuscript). Accepted at the MICCAI 2025 MIRASOL   Workshop; to appear in the Springer proceedings volume. This is the   pre-review version (not the Version of Record). DOI will be added after   publication. [Optional: 8 pages, 4 figures, 4 tables.]</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„ViTæ¨¡å‹å­˜åœ¨è®¡ç®—é‡å¤§å’Œåœ¨å°å‹æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç´§å‡‘ä¸”é€šç”¨çš„Vision Transformeræ¶æ„â€”â€”CoMViTï¼Œé€‚ç”¨äºèµ„æºå—é™çš„åŒ»ç–—å›¾åƒåˆ†æã€‚CoMViTé€šè¿‡æ•´åˆå·ç§¯æ ‡è®°å™¨ã€å¯¹è§’æ©ç ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾å’ŒåŸºäºæ± çš„åºåˆ—èšåˆç­‰æŠ€æœ¯ï¼Œè¿›è¡Œç³»ç»Ÿæ¶æ„ä¼˜åŒ–ï¼Œå®ç°äº†åœ¨åäºŒä¸ªMedMNISTæ•°æ®é›†ä¸Šçš„ç¨³å¥æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒä»…æœ‰~4.5Må‚æ•°ã€‚å®ƒåŒ¹é…æˆ–ä¼˜äºæ›´æ·±çš„CNNå’ŒViTå˜ä½“ï¼Œå®ç°äº†é«˜è¾¾5-20å€çš„å‚æ•°å‡å°‘ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚å®šæ€§Grad-CAMåˆ†æè¡¨æ˜ï¼ŒCoMViTè™½ç„¶å°ºå¯¸ç´§å‡‘ï¼Œä½†å§‹ç»ˆå…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoMViTæ˜¯ä¸€ä¸ªé’ˆå¯¹èµ„æºå—é™åŒ»ç–—å›¾åƒåˆ†æä¼˜åŒ–çš„ç´§å‡‘ä¸”é€šç”¨çš„Vision Transformeræ¶æ„ã€‚</li>
<li>CoMViTé€šè¿‡æ•´åˆå¤šé¡¹æŠ€æœ¯ï¼ˆå·ç§¯æ ‡è®°å™¨ã€å¯¹è§’æ©ç ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾å’ŒåŸºäºæ± çš„åºåˆ—èšåˆï¼‰æå‡æ€§èƒ½å¹¶å¢å¼ºé€šç”¨æ€§ã€‚</li>
<li>CoMViTåœ¨åäºŒä¸ªMedMNISTæ•°æ®é›†ä¸Šè¡¨ç°ç¨³å¥ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>CoMViTå…·æœ‰è½»é‡çº§è®¾è®¡ï¼Œä»…æœ‰~4.5Må‚æ•°ã€‚</li>
<li>CoMViTä¸æ›´æ·±çš„CNNå’ŒViTå˜ä½“ç›¸æ¯”ï¼Œå®ç°äº†å‚æ•°å‡å°‘ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚</li>
<li>å®šé‡Grad-CAMåˆ†æè¯å®CoMViTå…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-14b105cab70abc4c7b41d4dc4fe205f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304474&auth_key=1762304474-0-0-57ddfbf3c15ab8ba8d3f7b8e81d24da5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da8d596ea94a9dea47e80ec836fd15e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304481&auth_key=1762304481-0-0-5258424e77fe11861b29214b29ca57c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ef9be54cd2e370db89885b8e56b3671~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304562&auth_key=1762304562-0-0-d0bdcc20a96d14055e9da380aca4ee9c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fa47173a434053af3c83d341781e38f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304569&auth_key=1762304569-0-0-6f82dbd21cee9d68a65011d3dd4df22c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46fafd6b76660509c00ab243a022bb60~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304575&auth_key=1762304575-0-0-521e7d62ffcb57e5c89488d2df2cc9d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3ec09ef0f276aee12a1f9752b87eeaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304582&auth_key=1762304582-0-0-949b8ca0c5a27ae864c894f4a5f2e65f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Who-Does-Your-Algorithm-Fail-Investigating-Age-and-Ethnic-Bias-in-the-MAMA-MIA-Dataset"><a href="#Who-Does-Your-Algorithm-Fail-Investigating-Age-and-Ethnic-Bias-in-the-MAMA-MIA-Dataset" class="headerlink" title="Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the   MAMA-MIA Dataset"></a>Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the   MAMA-MIA Dataset</h2><p><strong>Authors:Aditya Parikh, Sneha Das, Aasa Feragen</strong></p>
<p>Deep learning models aim to improve diagnostic workflows, but fairness evaluation remains underexplored beyond classification, e.g., in image segmentation. Unaddressed segmentation bias can lead to disparities in the quality of care for certain populations, potentially compounded across clinical decision points and amplified through iterative model development. Here, we audit the fairness of the automated segmentation labels provided in the breast cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation quality across age, ethnicity, and data source. Our analysis reveals an intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source. We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems. Finally, we show how aggregating data from multiple data sources influences site-specific ethnic biases, underscoring the necessity of investigating data at a granular level. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¨åœ¨æ”¹å–„è¯Šæ–­æµç¨‹ï¼Œä½†åœ¨å›¾åƒåˆ†å‰²ç­‰åˆ†ç±»ä¹‹å¤–ï¼Œå…¬å¹³æ€§è¯„ä»·ä»è¢«å¿½è§†ã€‚æœªè§£å†³çš„åˆ†å‰²åè§å¯èƒ½å¯¼è‡´ç‰¹å®šäººç¾¤æŠ¤ç†è´¨é‡çš„ä¸å¹³ç­‰ï¼Œè¿™ç§ä¸å¹³ç­‰å¯èƒ½åœ¨ä¸´åºŠå†³ç­–ç‚¹åŠ å‰§ï¼Œå¹¶åœ¨æ¨¡å‹è¿­ä»£å¼€å‘ä¸­æ”¾å¤§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹ä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†MAMA-MIAæä¾›çš„è‡ªåŠ¨åˆ†å‰²æ ‡ç­¾çš„å…¬å¹³æ€§è¿›è¡Œäº†å®¡è®¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒå¹´é¾„æ®µã€ç§æ—å’Œæ•°æ®æ¥æºçš„è‡ªåŠ¨åˆ†å‰²è´¨é‡ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å¯¹å¹´è½»æ‚£è€…çš„å†…åœ¨å¹´é¾„ç›¸å…³åè§ï¼Œå³ä½¿åœ¨æ§åˆ¶è¯¸å¦‚æ•°æ®æ¥æºç­‰æ··æ‚å› ç´ åï¼Œè¿™ç§åè§ä¾ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§åè§å¯èƒ½ä¸ç”Ÿç†å› ç´ ï¼ˆå·²çŸ¥å¯¹æ”¾å°„åŒ»å¸ˆå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿéƒ½æ˜¯å·²çŸ¥çš„æŒ‘æˆ˜ï¼‰æœ‰å…³ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»å¤šä¸ªæ•°æ®æºèšåˆæ•°æ®å¦‚ä½•å½±å“ç‰¹å®šåœ°ç‚¹çš„ç§æ—åè§ï¼Œè¿™å¼ºè°ƒäº†ä»å¾®è§‚å±‚é¢è°ƒæŸ¥æ•°æ®çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27421v1">PDF</a> Medical Imaging Meets EurIPS (NeurIPS-endorsed workshop) - MedEurIPS</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¯Šæ–­å·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶å…¬å¹³æ€§è¯„ä¼°åœ¨åˆ†ç±»ä»¥å¤–é¢†åŸŸä»ç„¶è¢«å¿½è§†ã€‚æœªè§£å†³çš„åˆ†å‰²åè§å¯èƒ½å¯¼è‡´æŸäº›ç¾¤ä½“åœ¨æŠ¤ç†è´¨é‡ä¸Šçš„å·®è·ï¼Œè¿™äº›å·®è·å¯èƒ½ä¼šåœ¨å†³ç­–ç‚¹ç´¯ç§¯æ”¾å¤§å¹¶åœ¨æ¨¡å‹è¿­ä»£å¼€å‘ä¸­è¿›ä¸€æ­¥å¤æ‚åŒ–ã€‚æœ¬ç ”ç©¶å®¡è®¡äº†ä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†MAMA-MIAä¸­æä¾›çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ ‡ç­¾çš„å…¬å¹³æ€§ï¼Œè¯„ä¼°äº†å…¶åœ¨å¹´é¾„ã€ç§æ—å’Œæ•°æ®æºæ–¹é¢çš„è‡ªåŠ¨åŒ–åˆ†å‰²è´¨é‡ã€‚åˆ†ææ˜¾ç¤ºå­˜åœ¨é’ˆå¯¹å¹´è½»æ‚£è€…çš„å¹´é¾„ç›¸å…³åè§ï¼Œå³ä½¿åœ¨æ§åˆ¶æ•°æ®æºç­‰æ··æ·†å› ç´ åï¼Œè¿™ç§åè§ä¾ç„¶å­˜åœ¨ã€‚å‡è®¾è¿™ç§åè§å¯èƒ½ä¸ç”Ÿç†å› ç´ ç›¸å…³ï¼Œè¿™æ˜¯æ”¾å°„åŒ»å¸ˆå’Œè‡ªåŠ¨ç³»ç»Ÿéƒ½é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šæ•°æ®æºèšåˆæ•°æ®å¯¹ç‰¹å®šç«™ç‚¹ç§æ—åè§çš„å½±å“ï¼Œå¼ºè°ƒäº†è¿›è¡Œç²’åº¦çº§æ•°æ®è°ƒæŸ¥çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­å·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨è™½å¹¿æ³›ï¼Œä½†åœ¨é™¤åˆ†ç±»ä»¥å¤–çš„é¢†åŸŸï¼Œå¦‚å›¾åƒåˆ†å‰²ä¸­çš„å…¬å¹³æ€§è¯„ä¼°ä»ç„¶è¢«å¿½è§†ã€‚</li>
<li>æœªè§£å†³çš„åˆ†å‰²åè§å¯èƒ½å¯¼è‡´ä¸åŒç¾¤ä½“åœ¨æ¥å—åŒ»ç–—æœåŠ¡æ—¶å­˜åœ¨è´¨é‡å·®è·ã€‚</li>
<li>é’ˆå¯¹ä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†MAMA-MIAçš„è‡ªåŠ¨åŒ–åˆ†å‰²æ ‡ç­¾è¿›è¡Œäº†å…¬å¹³æ€§å®¡è®¡ã€‚</li>
<li>è¯„ä¼°äº†è‡ªåŠ¨åŒ–åˆ†å‰²è´¨é‡åœ¨å¹´é¾„ã€ç§æ—å’Œæ•°æ®æºæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºå­˜åœ¨é’ˆå¯¹å¹´è½»æ‚£è€…çš„å¹´é¾„ç›¸å…³åè§ï¼Œå³ä½¿æ§åˆ¶å…¶ä»–å› ç´ åä¾ç„¶å­˜åœ¨ã€‚</li>
<li>å‡è®¾è¿™ç§åè§å¯èƒ½ä¸ç”Ÿç†å› ç´ ç›¸å…³ï¼Œè¿™æ˜¯å¯¹æ”¾å°„åŒ»å¸ˆå’Œè‡ªåŠ¨ç³»ç»Ÿçš„å…±åŒæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-57bcc8f7a51c371651063d60276c2e3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304589&auth_key=1762304589-0-0-c119a7d85837fb27f8058ab19b0bc0b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b588655f8e92d6833b7276d5386651b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304597&auth_key=1762304597-0-0-94da43f41a3696defd08eb1acf327c2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MeisenMeister-A-Simple-Two-Stage-Pipeline-for-Breast-Cancer-Classification-on-MRI"><a href="#MeisenMeister-A-Simple-Two-Stage-Pipeline-for-Breast-Cancer-Classification-on-MRI" class="headerlink" title="MeisenMeister: A Simple Two Stage Pipeline for Breast Cancer   Classification on MRI"></a>MeisenMeister: A Simple Two Stage Pipeline for Breast Cancer   Classification on MRI</h2><p><strong>Authors:Benjamin Hamm, Yannick Kirchhoff, Maximilian Rokuss, Klaus Maier-Hein</strong></p>
<p>The ODELIA Breast MRI Challenge 2025 addresses a critical issue in breast cancer screening: improving early detection through more efficient and accurate interpretation of breast MRI scans. Even though methods for general-purpose whole-body lesion segmentation as well as multi-time-point analysis exist, breast cancer detection remains highly challenging, largely due to the limited availability of high-quality segmentation labels. Therefore, developing robust classification-based approaches is crucial for the future of early breast cancer detection, particularly in applications such as large-scale screening. In this write-up, we provide a comprehensive overview of our approach to the challenge. We begin by detailing the underlying concept and foundational assumptions that guided our work. We then describe the iterative development process, highlighting the key stages of experimentation, evaluation, and refinement that shaped the evolution of our solution. Finally, we present the reasoning and evidence that informed the design choices behind our final submission, with a focus on performance, robustness, and clinical relevance. We release our full implementation publicly at <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/MeisenMeister">https://github.com/MIC-DKFZ/MeisenMeister</a> </p>
<blockquote>
<p>ODELIAä¹³è…ºç™ŒMRIæŒ‘æˆ˜èµ›2025è§£å†³äº†ä¹³è…ºç™Œç­›æŸ¥ä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šé€šè¿‡æ›´æœ‰æ•ˆç‡ä¸”å‡†ç¡®çš„ä¹³è…ºMRIæ‰«æè§£è¯»æ¥æ”¹å–„æ—©æœŸæ£€æµ‹ã€‚å°½ç®¡å­˜åœ¨ç”¨äºå…¨èº«é€šç”¨ç—…å˜åˆ†å‰²ä»¥åŠå¤šç‚¹æ—¶é—´åˆ†æçš„æ–¹æ³•ï¼Œä½†æ£€æµ‹ä¹³è…ºç™Œä»ç„¶æ˜¯ä¸€é¡¹å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºé«˜è´¨é‡åˆ†å‰²æ ‡ç­¾çš„å¯ç”¨æ€§æœ‰é™ã€‚å› æ­¤ï¼Œå¼€å‘ç¨³å¥çš„åˆ†ç±»æ–¹æ³•å¯¹äºæœªæ¥æ—©æœŸä¹³è…ºç™Œæ£€æµ‹è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ç­›æŸ¥ç­‰åº”ç”¨ä¸­ã€‚åœ¨æ­¤æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å…¨é¢æ¦‚è¿°äº†æˆ‘ä»¬åº”å¯¹æŒ‘æˆ˜çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»æŒ‡å¯¼æˆ‘ä»¬å·¥ä½œçš„åŸºæœ¬æ¦‚å¿µå’ŒåŸºæœ¬å‡è®¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æè¿°äº†è¿­ä»£å¼€å‘è¿‡ç¨‹ï¼Œçªå‡ºæ˜¾ç¤ºå®éªŒã€è¯„ä¼°å’Œä¼˜åŒ–çš„å…³é”®é˜¶æ®µï¼Œè¿™äº›é˜¶æ®µå¡‘é€ äº†æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆçš„æ¼”å˜ã€‚æœ€åï¼Œæˆ‘ä»¬ç»™å‡ºäº†è®¾è®¡æœ€ç»ˆæäº¤æ—¶çš„æ¨ç†å’Œè¯æ®ï¼Œé‡ç‚¹è€ƒè™‘æ€§èƒ½ã€ç¨³å¥æ€§å’Œä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/MeisenMeister%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0%E3%80%82">https://github.com/MIC-DKFZ/MeisenMeisterä¸Šå…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„å®Œæ•´å®ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27326v1">PDF</a> Winning Solution of the MICCAI 2025 ODELIA Breast MRI Classification   Challenge</p>
<p><strong>Summary</strong></p>
<p>MRIå½±åƒçš„è‡ªåŠ¨åŒ–åˆ†ææŠ€æœ¯æ˜¯æ”¹å–„ä¹³è…ºç™Œæ—©æœŸæ£€æµ‹çš„å…³é”®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡ç­›æŸ¥ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†åœ¨ODELIAä¹³è…ºç™ŒMRIæŒ‘æˆ˜èµ›ä¸Šçš„æ–¹æ³•ã€‚ä»‹ç»çš„æ–¹æ³•ç€çœ¼äºé«˜æ€§èƒ½å’Œé²æ£’æ€§ï¼Œå…¬å¼€åˆ†äº«äº†å…·ä½“å®æ–½æ–¹æ¡ˆå’Œå®éªŒç»“æœã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™æ ·çš„æŠ€æœ¯è¿›æ­¥æœ‰æœ›æå‡MRIå›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæé«˜ä¹³è…ºç™Œçš„æ—©æœŸè¯Šæ–­æ°´å¹³ã€‚æ›´å…¨é¢çš„è§£è¯»ä¸åº”ç”¨æƒ…å†µè¯·è®¿é—®å…¬å¼€ç½‘å€ <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/MeisenMeister">https://github.com/MIC-DKFZ/MeisenMeister</a> äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ODELIA Breast MRIæŒ‘æˆ˜æ—¨åœ¨é€šè¿‡æ”¹è¿›MRIæ‰«æçš„è§£è¯»æ¥æå‡ä¹³è…ºç™Œçš„æ—©æœŸæ£€æµ‹æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</li>
<li>ç›®å‰ä¹³è…ºç™Œæ£€æµ‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯é«˜è´¨é‡åˆ†å‰²æ ‡ç­¾çš„æœ‰é™å¯ç”¨æ€§ã€‚</li>
<li>å¼€å‘ç¨³å¥çš„åˆ†ç±»æ–¹æ³•å¯¹äºæœªæ¥æ—©æœŸä¹³è…ºç™Œæ£€æµ‹è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ç­›æŸ¥åº”ç”¨ä¸­ã€‚</li>
<li>æ–‡ç« è¯¦ç»†ä»‹ç»äº†ç ”ç©¶å›¢é˜Ÿçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºç¡€å‡è®¾ã€è¿­ä»£å¼€å‘è¿‡ç¨‹ã€å®éªŒã€è¯„ä¼°å’Œè§£å†³æ–¹æ¡ˆçš„æ”¹è¿›ç­‰å…³é”®é˜¶æ®µã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé‡è§†æ€§èƒ½ã€é²æ£’æ€§å’Œä¸´åºŠæ„ä¹‰ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºè®¾è®¡é€‰æ‹©çš„æ ¸å¿ƒä¾æ®ã€‚</li>
<li>è¯¥ç ”ç©¶å›¢é˜Ÿå…¬å¼€åˆ†äº«äº†å…¶å®Œæ•´å®æ–½æ–¹æ¡ˆå’Œå®éªŒç»“æœï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶è€…å¯ä»¥å€Ÿé‰´å’Œæ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9995aaddccc5e1effd67830013c60e2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304604&auth_key=1762304604-0-0-e247bb28e23bffda9598c0eac74f403e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b60df746b3f7ef82df86eec0d6169de~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304611&auth_key=1762304611-0-0-61f1e38ab7eea8caaa8a806e9454e8c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-528cf1e7238c6932bad9aa0c8d1b5b20~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304618&auth_key=1762304618-0-0-cd16b9820f8bfe72cb7ce62c18eb3d79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d663f31e31436b231bd71753e8d051eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304625&auth_key=1762304625-0-0-212540dde23ce183f31c5595cc1d27ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-224139fddd423d24757ed859e3b2cac3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304631&auth_key=1762304631-0-0-cdaaa1307354bf91ca99367f9e0de836&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Versatile-and-Efficient-Medical-Image-Super-Resolution-Via-Frequency-Gated-Mamba"><a href="#Versatile-and-Efficient-Medical-Image-Super-Resolution-Via-Frequency-Gated-Mamba" class="headerlink" title="Versatile and Efficient Medical Image Super-Resolution Via   Frequency-Gated Mamba"></a>Versatile and Efficient Medical Image Super-Resolution Via   Frequency-Gated Mamba</h2><p><strong>Authors:Wenfeng Huang, Xiangyun Liao, Wei Cao, Wenjing Jia, Weixin Si</strong></p>
<p>Medical image super-resolution (SR) is essential for enhancing diagnostic accuracy while reducing acquisition cost and scanning time. However, modeling both long-range anatomical structures and fine-grained frequency details with low computational overhead remains challenging. We propose FGMamba, a novel frequency-aware gated state-space model that unifies global dependency modeling and fine-detail enhancement into a lightweight architecture. Our method introduces two key innovations: a Gated Attention-enhanced State-Space Module (GASM) that integrates efficient state-space modeling with dual-branch spatial and channel attention, and a Pyramid Frequency Fusion Module (PFFM) that captures high-frequency details across multiple resolutions via FFT-guided fusion. Extensive evaluations across five medical imaging modalities (Ultrasound, OCT, MRI, CT, and Endoscopic) demonstrate that FGMamba achieves superior PSNR&#x2F;SSIM while maintaining a compact parameter footprint ($&lt;$0.75M), outperforming CNN-based and Transformer-based SOTAs. Our results validate the effectiveness of frequency-aware state-space modeling for scalable and accurate medical image enhancement. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†é‡‡é›†æˆæœ¬å’Œæ‰«ææ—¶é—´ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä½è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå¯¹è¿œç¨‹è§£å‰–ç»“æ„å’Œç²¾ç»†é¢‘ç‡ç»†èŠ‚è¿›è¡Œå»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†FGMambaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é¢‘ç‡æ„ŸçŸ¥é—¨æ§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå®ƒå°†å…¨å±€ä¾èµ–å»ºæ¨¡å’Œç»†èŠ‚å¢å¼ºé›†æˆåˆ°ä¸€ä¸ªè½»é‡çº§æ¶æ„ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šä¸€ä¸ªé—¨æ§æ³¨æ„åŠ›å¢å¼ºçŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆGASMï¼‰ï¼Œå®ƒå°†é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´å»ºæ¨¡ä¸åŒåˆ†æ”¯ç©ºé—´å’Œé€šé“æ³¨æ„åŠ›ç›¸ç»“åˆï¼›ä¸€ä¸ªé‡‘å­—å¡”é¢‘ç‡èåˆæ¨¡å—ï¼ˆPFFMï¼‰ï¼Œå®ƒé€šè¿‡FFTå¼•å¯¼èåˆæ•è·å¤šä¸ªåˆ†è¾¨ç‡ä¸‹çš„é«˜é¢‘ç»†èŠ‚ã€‚åœ¨äº”ç§åŒ»å­¦å½±åƒæ¨¡æ€ï¼ˆè¶…å£°ã€OCTã€MRIã€CTå’Œå†…çª¥é•œï¼‰çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFGMambaåœ¨ä¿æŒç´§å‡‘çš„å‚æ•°å ç”¨ç©ºé—´ï¼ˆå°äº0.75Mï¼‰çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜è¶Šçš„PSNR&#x2F;SSIMæ€§èƒ½ï¼Œè¶…è¶Šäº†åŸºäºCNNå’ŒåŸºäºTransformerçš„SOTAsã€‚æˆ‘ä»¬çš„ç»“æœéªŒè¯äº†é¢‘ç‡æ„ŸçŸ¥çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¯æ‰©å±•å’Œç²¾ç¡®åŒ»å­¦å½±åƒå¢å¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27296v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯å¯¹äºæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€é™ä½é‡‡é›†æˆæœ¬å’Œæ‰«ææ—¶é—´å…·æœ‰é‡è¦æ„ä¹‰ã€‚æå‡ºä¸€ç§æ–°å‹é¢‘ç‡æ„ŸçŸ¥é—¨æ§çŠ¶æ€ç©ºé—´æ¨¡å‹FGMambaï¼Œè¯¥æ¨¡å‹å°†å…¨å±€ä¾èµ–æ€§å»ºæ¨¡å’Œç²¾ç»†ç»†èŠ‚å¢å¼ºé›†æˆåˆ°ä¸€ä¸ªè½»é‡çº§æ¶æ„ä¸­ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šé—¨æ§æ³¨æ„åŠ›å¢å¼ºçŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆGASMï¼‰å’Œé‡‘å­—å¡”é¢‘ç‡èåˆæ¨¡å—ï¼ˆPFFMï¼‰ã€‚å‰è€…ç»“åˆäº†é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´å»ºæ¨¡å’ŒåŒåˆ†æ”¯ç©ºé—´å’Œé€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼›åè€…é€šè¿‡FFTå¼•å¯¼èåˆæ•è·å¤šåˆ†è¾¨ç‡ä¸‹çš„é«˜é¢‘ç»†èŠ‚ã€‚åœ¨äº”ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFGMambaåœ¨ä¿æŒç´§å‡‘çš„å‚æ•°è¶³è¿¹çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜å¼‚çš„PSNR&#x2F;SSIMæ€§èƒ½ï¼Œä¼˜äºåŸºäºCNNå’ŒTransformerçš„SOTAsã€‚ç»“æœéªŒè¯äº†é¢‘ç‡æ„ŸçŸ¥çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¯ä¼¸ç¼©å’Œå‡†ç¡®çš„åŒ»å­¦å›¾åƒå¢å¼ºä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯å¯¹äºæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€é™ä½æˆæœ¬å’Œç¼©çŸ­æ‰«ææ—¶é—´è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é¢‘ç‡æ„ŸçŸ¥é—¨æ§çŠ¶æ€ç©ºé—´æ¨¡å‹FGMambaï¼Œè¯¥æ¨¡å‹èåˆäº†å…¨å±€ä¾èµ–å»ºæ¨¡å’Œç²¾ç»†ç»†èŠ‚å¢å¼ºã€‚</li>
<li>FGMambaåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šGASMå’ŒPFFMã€‚</li>
<li>GASMç»“åˆäº†çŠ¶æ€ç©ºé—´å»ºæ¨¡å’Œæ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>PFFMé€šè¿‡FFTå¼•å¯¼èåˆæ•è·å¤šåˆ†è¾¨ç‡ä¸‹çš„é«˜é¢‘ç»†èŠ‚ï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>FGMambaåœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰çš„CNNå’ŒTransformeræ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-84e394dbe3684a5adf0ae1a28e033342~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304639&auth_key=1762304639-0-0-77607835e9d894cd4d187644d5b3a4d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30469f826e8a8506e3e36e3ee98b77c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304647&auth_key=1762304647-0-0-4a6a9bba9dbb5600ac8a464a2b9c027b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24c384b91c088f055b3ffcbd912439c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304654&auth_key=1762304654-0-0-47cf390381bd733d2f76dcb5764f6548&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21735be59ac2018572e49f2dbcafb09d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304661&auth_key=1762304661-0-0-ceda136836ebc7f1ec9cd8c8dc1c8104&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb58666338f25ff7b693e6e2870a7b23~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304668&auth_key=1762304668-0-0-40c584a61e5123301f891e9657619ac5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8c163d7b2e3b1404a120170b5aaf82a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304675&auth_key=1762304675-0-0-041a70090c253aca6c6d45e474ae9951&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="T3-Test-Time-Model-Merging-in-VLMs-for-Zero-Shot-Medical-Imaging-Analysis"><a href="#T3-Test-Time-Model-Merging-in-VLMs-for-Zero-Shot-Medical-Imaging-Analysis" class="headerlink" title="T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging   Analysis"></a>T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging   Analysis</h2><p><strong>Authors:Raza Imam, Hu Wang, Dwarikanath Mahapatra, Mohammad Yaqub</strong></p>
<p>In medical imaging, vision-language models face a critical duality: pretrained networks offer broad robustness but lack subtle, modality-specific characteristics, while fine-tuned expert models achieve high in-distribution accuracy yet falter under modality shift. Existing model-merging techniques, designed for natural-image benchmarks, are simple and efficient but fail to deliver consistent gains across diverse medical modalities; their static interpolation limits reliability in varied clinical tasks. To address this, we introduce Test-Time Task adaptive merging (T^3), a backpropagation-free framework that computes per-sample interpolation coefficients via the Jensen-Shannon divergence between the two modelsâ€™ output distributions. T^3 dynamically preserves local precision when models agree and defers to generalist robustness under drift. To overcome the inference costs of sample-wise merging, we further propose a batch-wise extension, T^3_B, that computes a merging coefficient across a batch of samples, dramatically reducing computational bottleneck. Recognizing the lack of a standardized medical-merging benchmark, we present a rigorous cross-evaluation protocol spanning in-domain, base-to-novel, and corruptions across four modalities. Empirically, T^3 sets new state-of-the-art in Top-1 accuracy and error reduction, outperforming strong baselines while maintaining efficiency, paving the way for adaptive MVLM deployment in clinical settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Razaimam45/TCube">https://github.com/Razaimam45/TCube</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´ç€ä¸€ä¸ªå…³é”®çš„ä¸¤éš¾é—®é¢˜ï¼šé¢„è®­ç»ƒç½‘ç»œæä¾›äº†å¹¿æ³›çš„ç¨³å¥æ€§ï¼Œä½†ç¼ºä¹ç»†å¾®çš„ã€ç‰¹å®šäºæ¨¡æ€çš„ç‰¹å¾ï¼Œè€Œç²¾ç»†è°ƒæ•´çš„ä¸“ä¸šæ¨¡å‹åœ¨å†…éƒ¨åˆ†å¸ƒä¸Šè¾¾åˆ°äº†é«˜å‡†ç¡®æ€§ï¼Œä½†åœ¨æ¨¡æ€è½¬å˜æ—¶å´è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨ä¸ºè‡ªç„¶å›¾åƒåŸºå‡†æµ‹è¯•è®¾è®¡ï¼Œç®€å•é«˜æ•ˆï¼Œä½†åœ¨ä¸åŒçš„åŒ»å­¦æ¨¡æ€ä¸Šæœªèƒ½å®ç°ä¸€è‡´çš„å¢ç›Šï¼›ä»–ä»¬çš„é™æ€æ’å€¼é™åˆ¶äº†å…¶åœ¨å„ç§ä¸´åºŠä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶é—´ä»»åŠ¡è‡ªé€‚åº”åˆå¹¶ï¼ˆT^3ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€åå‘ä¼ æ’­çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡è®¡ç®—ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„Jensen-Shannonæ•£åº¦æ¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ’å€¼ç³»æ•°ã€‚T^3åœ¨æ¨¡å‹ä¸€è‡´æ—¶åŠ¨æ€ä¿ç•™å±€éƒ¨ç²¾åº¦ï¼Œå¹¶åœ¨æ¼‚ç§»æ—¶è½¬å‘é€šç”¨ç¨³å¥æ€§ã€‚ä¸ºäº†å…‹æœæ ·æœ¬çº§åˆå¹¶çš„æ¨ç†æˆæœ¬ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ‰¹é‡æ‰©å±•T^3_Bï¼Œå®ƒå¯ä»¥åœ¨ä¸€æ‰¹æ ·æœ¬ä¸Šè®¡ç®—åˆå¹¶ç³»æ•°ï¼Œä»è€Œå¤§å¤§é™ä½è®¡ç®—ç“¶é¢ˆã€‚è®¤è¯†åˆ°ç¼ºä¹æ ‡å‡†åŒ–çš„åŒ»å­¦åˆå¹¶åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¥æ ¼çš„è·¨è¯„ä»·åè®®ï¼Œè¯¥åè®®æ¶µç›–äº†å››ä¸ªæ¨¡æ€çš„åŸŸå†…ã€ä»åŸºç¡€åˆ°æ–°é¢–ä»¥åŠè…è´¥æƒ…å†µã€‚ç»éªŒä¸Šï¼ŒT^3åœ¨Top-1å‡†ç¡®ç‡å’Œè¯¯å·®å‡å°‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆç‡ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­è‡ªé€‚åº”MVLMéƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Razaimam45/TCube%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/Razaimam45/TCubeæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27265v1">PDF</a> Main: 11 pages, Supplementary: 9 pages 10 tables, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé¢å‘åŒ»å­¦æˆåƒçš„Test-Time Taskè‡ªé€‚åº”èåˆï¼ˆT^3ï¼‰æ¡†æ¶ï¼Œè§£å†³äº†é¢„è®­ç»ƒç½‘ç»œå’Œç²¾ç»†è°ƒæ•´ä¸“å®¶æ¨¡å‹ä¹‹é—´çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¡ç®—ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„Jensen-Shannonæ•£åº¦æ¥åŠ¨æ€è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„èåˆç³»æ•°ï¼Œæ—¢ä¿ç•™äº†å±€éƒ¨ç²¾åº¦åˆåœ¨æ¨¡å‹æ¼‚ç§»æ—¶ä¿æŒç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½æ ·æœ¬çº§èåˆçš„è®¡ç®—æˆæœ¬ï¼Œè¿˜æå‡ºäº†æ‰¹é‡æ‰©å±•çš„T^3_Bæ–¹æ³•ã€‚ç»è¿‡ä¸¥æ ¼çš„è·¨åŸŸã€è·¨æ¨¡æ€è¯„ä¼°åè®®éªŒè¯ï¼ŒT^3åœ¨å››ç§æ¨¡æ€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„Top-1å‡†ç¡®ç‡å’Œè¯¯å·®é™ä½ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­å¤šæ¨¡æ€åŒ»å­¦æˆåƒçš„é€‚åº”æ€§éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T^3æ¡†æ¶è§£å†³äº†é¢„è®­ç»ƒç½‘ç»œå’Œä¸“å®¶æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„å±€é™æ€§ã€‚</li>
<li>T^3é€šè¿‡è®¡ç®—æ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„Jensen-Shannonæ•£åº¦æ¥åŠ¨æ€è®¡ç®—èåˆç³»æ•°ã€‚</li>
<li>T^3èƒ½åœ¨æ¨¡å‹ä¸€è‡´æ—¶ä¿ç•™å±€éƒ¨ç²¾åº¦ï¼Œå¹¶åœ¨æ¨¡å‹æ¼‚ç§»æ—¶ä¿æŒç¨³å¥æ€§ã€‚</li>
<li>T^3_Bä½œä¸ºæ‰¹é‡æ‰©å±•æ–¹æ³•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„åŒ»å­¦èåˆåŸºå‡†æµ‹è¯•ï¼Œéœ€è¦å»ºç«‹ä¸¥æ ¼çš„è¯„ä¼°åè®®ã€‚</li>
<li>T^3åœ¨å››ç§æ¨¡æ€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸŸå†…ã€åŸºç¡€åˆ°æ–°é¢–å’Œè…è´¥æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4ead56f5187f24de61a695b0b16c34e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304682&auth_key=1762304682-0-0-f92293b8d1b99a6a111cd7fc7d106bdc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-313bbfe20c7428b69d47e8f910a0d4a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304690&auth_key=1762304690-0-0-7be649d1b39feb774f84923474fde8c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d4cf4f91ec16a2fd0f7ed9a51c309bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304697&auth_key=1762304697-0-0-719e7c34f7674e48d441b24dc7d854d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4eaa64996fe4b3754862a31093681582~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304704&auth_key=1762304704-0-0-f9bbb6304f464df3486d33889c9466fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be21cbc90978aa4247233a885da4f371~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304710&auth_key=1762304710-0-0-5011bec303db7ef6a38bfe570d6daf86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4345ea354ff55ac865bd8e794f6d911f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304717&auth_key=1762304717-0-0-034e98fbab439f60cd9914f421631785&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fusion-of-Heterogeneous-Pathology-Foundation-Models-for-Whole-Slide-Image-Analysis"><a href="#Fusion-of-Heterogeneous-Pathology-Foundation-Models-for-Whole-Slide-Image-Analysis" class="headerlink" title="Fusion of Heterogeneous Pathology Foundation Models for Whole Slide   Image Analysis"></a>Fusion of Heterogeneous Pathology Foundation Models for Whole Slide   Image Analysis</h2><p><strong>Authors:Zhidong Yang, Xiuhui Shi, Wei Ba, Zhigang Song, Haijing Luan, Taiyuan Hu, Senlin Lin, Jiguang Wang, Shaohua Kevin Zhou, Rui Yan</strong></p>
<p>Whole slide image (WSI) analysis has emerged as an increasingly essential technique in computational pathology. Recent advances in the pathological foundation models (FMs) have demonstrated significant advantages in deriving meaningful patch-level or slide-level feature representations from WSIs. However, current pathological FMs have exhibited substantial heterogeneity caused by diverse private training datasets and different network architectures. This heterogeneity introduces performance variability when we utilize the extracted features from different FMs in the downstream tasks. To fully explore the advantage of multiple FMs effectively, in this work, we propose a novel framework for the fusion of heterogeneous pathological FMs, called FuseCPath, yielding a model with a superior ensemble performance. The main contributions of our framework can be summarized as follows: (i) To guarantee the representativeness of the training patches, we propose a multi-view clustering-based method to filter out the discriminative patches via multiple FMsâ€™ embeddings. (ii) To effectively fuse the heterogeneous patch-level FMs, we devise a cluster-level re-embedding strategy to online capture patch-level local features. (iii) To effectively fuse the heterogeneous slide-level FMs, we devise a collaborative distillation strategy to explore the connections between slide-level FMs. Extensive experiments conducted on lung cancer, bladder cancer, and colorectal cancer datasets from The Cancer Genome Atlas (TCGA) have demonstrated that the proposed FuseCPath achieves state-of-the-art performance across multiple tasks on these public datasets. </p>
<blockquote>
<p>å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWhole Slide Image, WSIï¼‰åˆ†æåœ¨è®¡ç®—ç—…ç†å­¦ä¸­å·²æˆä¸ºè¶Šæ¥è¶Šé‡è¦çš„æŠ€æœ¯ã€‚è¿‘æœŸç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„è¿›å±•åœ¨ä»WSIsä¸­æå–æœ‰æ„ä¹‰çš„æ–‘å—çº§åˆ«æˆ–å¹»ç¯ç‰‡çº§åˆ«çš„ç‰¹å¾è¡¨ç¤ºæ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç—…ç†FMsç”±äºå„ç§ç§äººè®­ç»ƒæ•°æ®é›†å’Œç½‘ç»œæ¶æ„çš„ä¸åŒè€Œè¡¨ç°å‡ºç›¸å½“å¤§çš„å¼‚è´¨æ€§ã€‚è¿™ç§å¼‚è´¨æ€§åœ¨æˆ‘ä»¬å°†ä¸åŒFMä¸­æå–çš„ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¼•å…¥äº†æ€§èƒ½å˜åŒ–ã€‚ä¸ºäº†å……åˆ†æ¢ç´¢å¤šä¸ªFMsçš„ä¼˜åŠ¿ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFuseCPathçš„æ–°å‹å¼‚è´¨ç—…ç†FMèåˆæ¡†æ¶ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªå…·æœ‰å“è¶Šé›†æˆæ€§èƒ½çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šï¼ˆiï¼‰ä¸ºäº†ä¿è¯è®­ç»ƒæ–‘å—çš„ä»£è¡¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†å›¾èšç±»çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªFMsçš„åµŒå…¥æ¥è¿‡æ»¤å‡ºé‰´åˆ«æ€§æ–‘å—ã€‚ï¼ˆiiï¼‰ä¸ºäº†æœ‰æ•ˆåœ°èåˆå¼‚è´¨æ–‘å—çº§åˆ«çš„FMsï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é›†ç¾¤çº§åˆ«çš„é‡æ–°åµŒå…¥ç­–ç•¥ï¼Œä»¥åœ¨çº¿æ•è·æ–‘å—çº§åˆ«çš„å±€éƒ¨ç‰¹å¾ã€‚ï¼ˆiiiï¼‰ä¸ºäº†æœ‰æ•ˆåœ°èåˆå¼‚è´¨å¹»ç¯ç‰‡çº§åˆ«çš„FMsï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åä½œè’¸é¦ç­–ç•¥ï¼Œä»¥æ¢ç´¢å¹»ç¯ç‰‡çº§åˆ«FMsä¹‹é—´çš„è”ç³»ã€‚åœ¨ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰çš„è‚ºç™Œã€è†€èƒ±ç™Œå’Œç»“è‚ ç™Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FuseCPathåœ¨è¿™äº›å…¬å¼€æ•°æ®é›†ä¸Šçš„å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27237v1">PDF</a> 22 pages, 9 figures</p>
<p><strong>Summary</strong><br>     å…¨åˆ‡ç‰‡å›¾åƒåˆ†æåœ¨è®¡ç®—ç—…ç†å­¦ä¸­æ˜¯è‡³å…³é‡è¦çš„æŠ€æœ¯ï¼Œè€Œæœ€æ–°çš„ç—…ç†åŸºç¡€æ¨¡å‹å±•ç°å‡ºä»å…¨åˆ‡ç‰‡å›¾åƒä¸­æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†æˆ–æ•´ä½“ç‰¹å¾è¡¨ç¤ºçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç”±äºå„ç§ç§æœ‰è®­ç»ƒæ•°æ®é›†å’Œç½‘ç»œæ¶æ„çš„å·®å¼‚ï¼Œç—…ç†åŸºç¡€æ¨¡å‹çš„å¼‚è´¨æ€§å¯¼è‡´äº†æ€§èƒ½å˜åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFuseCPathçš„å¼‚è´¨ç—…ç†åŸºç¡€æ¨¡å‹èåˆæ¡†æ¶ï¼Œé€šè¿‡èåˆå¤šä¸ªæ¨¡å‹å®ç°å“è¶Šçš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åˆ‡ç‰‡å›¾åƒåˆ†æåœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç—…ç†åŸºç¡€æ¨¡å‹èƒ½å¤Ÿä»å…¨åˆ‡ç‰‡å›¾åƒä¸­æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†æˆ–æ•´ä½“ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ç—…ç†åŸºç¡€æ¨¡å‹å­˜åœ¨å¼‚è´¨æ€§ï¼Œè¿™ä¼šå½±å“å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFuseCPathçš„å¼‚è´¨ç—…ç†åŸºç¡€æ¨¡å‹èåˆæ¡†æ¶ã€‚</li>
<li>FuseCPathæ¡†æ¶é€šè¿‡å¤šè§†è§’èšç±»æ–¹æ³•ç­›é€‰é‰´åˆ«æ€§è¡¥ä¸ï¼Œä¿è¯è®­ç»ƒè¡¥ä¸çš„ä»£è¡¨æ€§ã€‚</li>
<li>FuseCPathæ¡†æ¶é‡‡ç”¨é›†ç¾¤çº§åˆ«çš„é‡æ–°åµŒå…¥ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°èåˆäº†å¼‚è´¨çš„éƒ¨åˆ†çº§æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åœ¨è‚ºç™Œã€è†€èƒ±ç™Œå’Œç»“è‚ ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜ï¼ŒFuseCPathæ¡†æ¶å®ç°äº†å¤šä¸ªä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d26d0e7868944040480828daa6cb3875~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304725&auth_key=1762304725-0-0-ec6e11bb86ad6870df51f7b4583ef598&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce724fecdc16b3832678a9a643dc80c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304732&auth_key=1762304732-0-0-1c30571f45914da9cd38aefb18180a70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ZEBRA-Towards-Zero-Shot-Cross-Subject-Generalization-for-Universal-Brain-Visual-Decoding"><a href="#ZEBRA-Towards-Zero-Shot-Cross-Subject-Generalization-for-Universal-Brain-Visual-Decoding" class="headerlink" title="ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal   Brain Visual Decoding"></a>ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal   Brain Visual Decoding</h2><p><strong>Authors:Haonan Wang, Jingyu Lu, Hongrui Li, Xiaomeng Li</strong></p>
<p>Recent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/ZEBRA">https://github.com/xmed-lab/ZEBRA</a>. </p>
<blockquote>
<p>è¿‘æœŸç¥ç»è§£ç æŠ€æœ¯çš„è¿›å±•å·²ç»èƒ½å¤Ÿä»è„‘æ´»åŠ¨ä¸­é‡å»ºè§†è§‰ä½“éªŒï¼Œä½¿fMRI-to-imageé‡å»ºåœ¨ç¥ç»ç§‘å­¦å’Œè®¡ç®—æœºè§†è§‰ä¹‹é—´æ¶èµ·äº†ä¸€åº§å……æ»¡å¸Œæœ›çš„æ¡¥æ¢ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç‰¹å®šä¸»ä½“çš„æ¨¡å‹æˆ–éœ€è¦è¿›è¡Œä¸»ä½“ç‰¹å®šçš„å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ZEBRAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€ç‰¹å®šä¸»ä½“é€‚åº”çš„é›¶æ ·æœ¬è„‘è§†è§‰è§£ç æ¡†æ¶ã€‚ZEBRAå»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªå…³é”®è§è§£ä¹‹ä¸Šï¼Œå³fMRIè¡¨ç¤ºå¯ä»¥åˆ†è§£ä¸ºä¸ä¸»ä½“ç›¸å…³çš„ç»„ä»¶å’Œä¸è¯­ä¹‰ç›¸å…³çš„ç»„ä»¶ã€‚é€šè¿‡åˆ©ç”¨å¯¹æŠ—è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾å¼åœ°è§£å¼€è¿™äº›ç»„ä»¶ï¼Œä»¥éš”ç¦»ä¸ä¸»ä½“æ— å…³ã€ä¸è¯­ä¹‰ç‰¹å®šçš„è¡¨ç¤ºã€‚è¿™ç§è§£å¼€å…è®¸ZEBRAåœ¨æ²¡æœ‰ä»»ä½•é¢å¤–çš„fMRIæ•°æ®æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æœªè§è¿‡çš„ä¸»ä½“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒZEBRAæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£è¡¨ç€æœç€é€šç”¨ç¥ç»è§£ç çš„ä¸€ä¸ªå¯æ‰©å±•å’Œå®ç”¨çš„æ­¥éª¤ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/ZEBRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmed-lab/ZEBRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27128v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç¥ç»è§£ç æŠ€æœ¯çš„è¿›å±•ä½¿å¾—ä»è„‘æ´»åŠ¨é‡å»ºè§†è§‰ä½“éªŒæˆä¸ºå¯èƒ½ï¼Œå°†fMRI-to-imageé‡å»ºæŠ€æœ¯ä½œä¸ºè¿æ¥ç¥ç»ç§‘å­¦ä¸è®¡ç®—æœºè§†è§‰ä¹‹é—´çš„ä¸€åº§æœ‰å‰é€”çš„æ¡¥æ¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç‰¹å®šä¸»ä½“çš„æ¨¡å‹æˆ–éœ€è¦ä¸»ä½“ç‰¹å®šçš„å¾®è°ƒï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ— éœ€ä¸»ä½“ç‰¹å®šé€‚åº”çš„é›¶å°„å‡»è„‘è§†è§‰è§£ç æ¡†æ¶ZEBRAã€‚ZEBRAå»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªå…³é”®è§è§£ä¸Šï¼šfMRIè¡¨ç¤ºå¯ä»¥åˆ†è§£ä¸ºä¸ä¸»ä½“ç›¸å…³çš„æˆåˆ†å’Œè¯­ä¹‰ç›¸å…³çš„æˆåˆ†ã€‚é€šè¿‡åˆ©ç”¨å¯¹æŠ—è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾å¼åœ°è§£å¼€è¿™äº›æˆåˆ†ï¼Œä»¥éš”ç¦»ä¸»ä½“ä¸å˜ã€è¯­ä¹‰ç‰¹å®šçš„è¡¨ç¤ºã€‚è¿™ç§è§£å¼€å…è®¸ZEBRAåœ¨æ²¡æœ‰ä»»ä½•é¢å¤–çš„fMRIæ•°æ®æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æœªè§è¿‡çš„ä¸»ä½“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒZEBRAæ˜¾è‘—ä¼˜äºé›¶å°„å‡»åŸºçº¿ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£è¡¨ç€æœç€é€šç”¨ç¥ç»è§£ç çš„å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»è§£ç æŠ€æœ¯èƒ½å¤Ÿä»è„‘æ´»åŠ¨ä¸­é‡å»ºè§†è§‰ä½“éªŒï¼Œå½¢æˆç¥ç»ç§‘å­¦ä¸è®¡ç®—æœºè§†è§‰ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–ç‰¹å®šä¸»ä½“çš„æ¨¡å‹æˆ–å¾®è°ƒï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œå®ç”¨æ€§ã€‚</li>
<li>ZEBRAæ¡†æ¶æ˜¯ä¸€ç§é›¶å°„å‡»è„‘è§†è§‰è§£ç æ–¹æ³•ï¼Œæ— éœ€ä¸»ä½“ç‰¹å®šé€‚åº”ã€‚</li>
<li>ZEBRAåˆ©ç”¨å¯¹æŠ—è®­ç»ƒè§£å¼€fMRIè¡¨ç¤ºçš„ç»„ä»¶ï¼Œå®ç°ä¸»ä½“ä¸å˜ã€è¯­ä¹‰ç‰¹å®šçš„è¡¨ç¤ºã€‚</li>
<li>ZEBRAåœ¨æœªè§è¿‡çš„ä¸»ä½“ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–æ•°æ®å’Œé‡æ–°è®­ç»ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ZEBRAæ€§èƒ½æ˜¾è‘—ä¼˜äºé›¶å°„å‡»åŸºçº¿ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¥è¿‘å®Œå…¨å¾®è°ƒæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-be551d31f410503ef962d17cf39ea6c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304740&auth_key=1762304740-0-0-6ada0e4e4a2315889a29293e2f680673&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75f4ad59990b81e49183ffd0caf244fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304747&auth_key=1762304747-0-0-1f3fa4d13323cdd3666e50a5534bc4ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f8fd70a3fbe61e83df94948d66fcdc2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304755&auth_key=1762304755-0-0-c44d303a94500a375aa4d5f75ec664a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ba4b71367532f79ddc4ef0aa70501d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304762&auth_key=1762304762-0-0-7aaca9933d75e795ef4044a3b47638b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UP2D-Uncertainty-aware-Progressive-Pseudo-label-Denoising-for-Source-Free-Domain-Adaptive-Medical-Image-Segmentation"><a href="#UP2D-Uncertainty-aware-Progressive-Pseudo-label-Denoising-for-Source-Free-Domain-Adaptive-Medical-Image-Segmentation" class="headerlink" title="UP2D: Uncertainty-aware Progressive Pseudo-label Denoising for   Source-Free Domain Adaptive Medical Image Segmentation"></a>UP2D: Uncertainty-aware Progressive Pseudo-label Denoising for   Source-Free Domain Adaptive Medical Image Segmentation</h2><p><strong>Authors:Quang-Khai Bui-Tran, Thanh-Huy Nguyen, Manh D. Ho, Thinh B. Lam, Vi Vu, Hoang-Thien Nguyen, Phat Huynh, Ulas Bagci</strong></p>
<p>Medical image segmentation models face severe performance drops under domain shifts, especially when data sharing constraints prevent access to source images. We present a novel Uncertainty-aware Progressive Pseudo-label Denoising (UP2D) framework for source-free domain adaptation (SFDA), designed to mitigate noisy pseudo-labels and class imbalance during adaptation. UP2D integrates three key components: (i) a Refined Prototype Filtering module that suppresses uninformative regions and constructs reliable class prototypes to denoise pseudo-labels, (ii) an Uncertainty-Guided EMA (UG-EMA) strategy that selectively updates the teacher model based on spatially weighted boundary uncertainty, and (iii) a quantile-based entropy minimization scheme that focuses learning on ambiguous regions while avoiding overconfidence on easy pixels. This single-stage student-teacher framework progressively improves pseudo-label quality and reduces confirmation bias. Extensive experiments on three challenging retinal fundus benchmarks demonstrate that UP2D achieves state-of-the-art performance across both standard and open-domain settings, outperforming prior UDA and SFDA approaches while maintaining superior boundary precision. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨é¢†åŸŸè¿ç§»ä¸‹é¢ä¸´æ€§èƒ½ä¸¥é‡ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å…±äº«å—é™æ— æ³•è®¿é—®æºå›¾åƒæ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€æºæ•°æ®çš„åŸŸé€‚åº”ï¼ˆSFDAï¼‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¸è¿›ä¼ªæ ‡ç­¾å»å™ªï¼ˆUP2Dï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å‡è½»ä¼ªæ ‡ç­¾å™ªå£°å’Œç±»ä¸å¹³è¡¡é—®é¢˜ã€‚UP2Dé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆiï¼‰ç²¾ç»†åŒ–åŸå‹è¿‡æ»¤æ¨¡å—ï¼Œè¯¥æ¨¡å—æŠ‘åˆ¶äº†éä¿¡æ¯åŒºåŸŸå¹¶æ„å»ºäº†å¯é çš„ç±»åŸå‹ä»¥å»å™ªä¼ªæ ‡ç­¾ï¼›ï¼ˆiiï¼‰åŸºäºä¸ç¡®å®šæ€§çš„EMAï¼ˆUG-EMAï¼‰ç­–ç•¥ï¼Œæ ¹æ®ç©ºé—´åŠ æƒè¾¹ç•Œä¸ç¡®å®šæ€§é€‰æ‹©æ€§åœ°æ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼›ï¼ˆiiiï¼‰åŸºäºåˆ†ä½æ•°çš„ç†µæœ€å°åŒ–æ–¹æ¡ˆï¼Œä¸“æ³¨äºå­¦ä¹ æ¨¡ç³ŠåŒºåŸŸï¼ŒåŒæ—¶é¿å…å¯¹ç®€å•åƒç´ è¿‡äºè‡ªä¿¡ã€‚è¿™ä¸ªå•ä¸€é˜¶æ®µçš„å­¦ç”Ÿ-æ•™å¸ˆæ¡†æ¶é€æ­¥æé«˜äº†ä¼ªæ ‡ç­¾è´¨é‡å¹¶å‡å°‘äº†ç¡®è®¤åè§ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†ç½‘è†œåŸºé‡‘æ ‡å‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUP2Dåœ¨æ ‡å‡†å’Œå¼€æ”¾é¢†åŸŸè®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¼˜äºå…ˆå‰çš„UDAå’ŒSFDAæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è¶Šçš„è¾¹ç•Œç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26826v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨è·¨åŸŸæ—¶æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å…±äº«å—é™æ— æ³•è·å–æºå›¾åƒæ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¸è¿›ä¼ªæ ‡ç­¾å»å™ªï¼ˆUP2Dï¼‰æ¡†æ¶ï¼Œç”¨äºæ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰ï¼Œæ—¨åœ¨å‡è½»ä¼ªæ ‡ç­¾å™ªå£°å’Œç±»ä¸å¹³è¡¡é—®é¢˜ã€‚UP2Dé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬ç²¾ç»†åŒ–åŸå‹è¿‡æ»¤æ¨¡å—ã€ä¸ç¡®å®šæ€§å¼•å¯¼EMAç­–ç•¥å’ŒåŸºäºåˆ†ä½æ•°ç†µæœ€å°åŒ–æ–¹æ¡ˆã€‚è¯¥å•é˜¶æ®µå­¦ç”Ÿ-æ•™å¸ˆæ¡†æ¶é€æ­¥æé«˜äº†ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå‡å°‘äº†ç¡®è®¤åè§ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†ç½‘è†œåŸºé‡‘å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUP2Dåœ¨æ ‡å‡†åŸŸå’Œå¼€æ”¾åŸŸè®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¼˜äºå…ˆå‰çš„åŸŸè‡ªé€‚åº”å’ŒSFDAæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è¾¹ç•Œç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨è·¨åŸŸæ—¶é¢ä¸´æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å…±äº«å—é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>UP2Dæ¡†æ¶æ—¨åœ¨è§£å†³æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰ä¸­çš„ä¼ªæ ‡ç­¾å™ªå£°å’Œç±»ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>UP2Dé›†æˆäº†ç²¾ç»†åŒ–åŸå‹è¿‡æ»¤æ¨¡å—ã€ä¸ç¡®å®šæ€§å¼•å¯¼EMAç­–ç•¥å’ŒåŸºäºåˆ†ä½æ•°ç†µæœ€å°åŒ–æ–¹æ¡ˆä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>å•é˜¶æ®µå­¦ç”Ÿ-æ•™å¸ˆæ¡†æ¶é€æ­¥æé«˜ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå‡å°‘ç¡®è®¤åè§ã€‚</li>
<li>UP2Dåœ¨è§†ç½‘è†œåŸºé‡‘å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>UP2Dä¼˜äºå…ˆå‰çš„åŸŸè‡ªé€‚åº”å’ŒSFDAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4fb7b7848894cd490963d4ecd01625e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304770&auth_key=1762304770-0-0-6e628bfdf2f8eee6806c9e44b9329366&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MORE-Multi-Organ-Medical-Image-REconstruction-Dataset"><a href="#MORE-Multi-Organ-Medical-Image-REconstruction-Dataset" class="headerlink" title="MORE: Multi-Organ Medical Image REconstruction Dataset"></a>MORE: Multi-Organ Medical Image REconstruction Dataset</h2><p><strong>Authors:Shaokai Wu, Yapan Guo, Yanbiao Ji, Jing Tong, Yuxiang Lu, Mei Li, Suizhi Huang, Yue Ding, Hongtao Lu</strong></p>
<p>CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page <a target="_blank" rel="noopener" href="https://more-med.github.io/">https://more-med.github.io/</a> </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æé‡å»ºä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†ç”¨äºè¯Šæ–­å’Œæ²»ç–—çš„å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸ä»…é™äºç‰¹å®šçš„è§£å‰–éƒ¨ä½å’Œæ•°æ®é›†ï¼Œé˜»ç¢äº†å…¶åœ¨æœªè§è¿‡çš„è§£å‰–éƒ¨ä½å’Œç—…å˜ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå™¨å®˜åŒ»å­¦å›¾åƒé‡å»ºï¼ˆMOREï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«9ç§ä¸åŒè§£å‰–éƒ¨ä½çš„è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒï¼Œæ¶‰åŠ15ç§ç—…å˜ç±»å‹ã€‚è¯¥æ•°æ®é›†æœ‰ä¸¤ä¸ªä¸»è¦ç›®çš„ï¼šï¼ˆ1ï¼‰èƒ½å¤Ÿåœ¨å¹¿æ³›ã€å¼‚è´¨çš„æ•°æ®ä¸Šè¿›è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç¨³å¥è®­ç»ƒï¼›ï¼ˆ2ï¼‰å¯¹è®¡ç®—æœºæ–­å±‚æ‰«æé‡å»ºçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¿™ä¸ªæŒ‘æˆ˜æ¡ä»¶ä¸‹è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰ç»¼åˆæ•°æ®é›†æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰åŸºäºä¼˜åŒ–çš„æ–¹æ³•ä¸ºæœªè§è¿‡çš„è§£å‰–éƒ¨ä½æä¾›äº†æ›´å¼ºçš„ç¨³å¥æ€§ã€‚MOREæ•°æ®é›†å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ <a target="_blank" rel="noopener" href="https://more-med.github.io/">https://more-med.github.io/</a> ä¸‹å…è´¹è®¿é—®ï¼Œéµå¾ªCC-BY-NC 4.0åè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26759v1">PDF</a> Accepted to ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é‡å»ºçš„å¤šå™¨å®˜åŒ»å­¦å›¾åƒé‡å»ºï¼ˆMOREï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«9ç§ä¸åŒè§£å‰–ç»“æ„å’Œ15ç§ç—…å˜ç±»å‹çš„CTæ‰«æï¼Œæ—¨åœ¨ä¿ƒè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¹¿æ³›ã€å¼‚è´¨æ•°æ®ä¸Šçš„ç¨³å¥è®­ç»ƒï¼Œå¹¶è¯„ä¼°æ¨¡å‹å¯¹æœªè§è§£å‰–ç»“æ„å’Œç—…å˜çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œå»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æ€»ç»“æ¥è¯´ï¼Œå…¨é¢æ•°æ®é›†æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºä¼˜åŒ–çš„æ–¹æ³•åˆ™æä¾›äº†å¯¹æœªè§è§£å‰–ç»“æ„çš„å¢å¼ºç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤šå™¨å®˜åŒ»å­¦å›¾åƒé‡å»ºï¼ˆMOREï¼‰æ•°æ®é›†ï¼ŒåŒ…å«9ç§ä¸åŒè§£å‰–ç»“æ„å’Œ15ç§ç—…å˜ç±»å‹çš„CTæ‰«æã€‚</li>
<li>MOREæ•°æ®é›†æ—¨åœ¨ä¿ƒè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¹¿æ³›ã€å¼‚è´¨æ•°æ®ä¸Šçš„è®­ç»ƒï¼Œå¹¶è¯„ä¼°æ¨¡å‹å¯¹æœªè§è§£å‰–ç»“æ„å’Œç—…å˜çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨MOREæ•°æ®é›†å»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å…¨é¢æ•°æ®é›†æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸºäºä¼˜åŒ–çš„æ–¹æ³•æä¾›äº†å¯¹æœªè§è§£å‰–ç»“æ„çš„å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>MOREæ•°æ®é›†å¯é€šè¿‡CC-BY-NC 4.0åè®®å…è´¹è®¿é—®ã€‚</li>
<li>è¯¥é¡¹ç›®é¡µé¢ä¸ºâ€œ<a target="_blank" rel="noopener" href="https://more-med.github.ioâ€./">https://more-med.github.io/â€ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-644d118ac44b4ffe43422d92e4361dd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304778&auth_key=1762304778-0-0-158355829a8b8554b166ffcf152b48a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-010ef4c00e890c037df5d5cb05648ceb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304785&auth_key=1762304785-0-0-64be5b81cfe7183b8f76f3b56a366655&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-60e645c0e85acfcff94a7f3103c3aa60~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304792&auth_key=1762304792-0-0-6f5898ee902e8e9e9384a2b65830d2e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e92fe5e811a1646ad20d44514523287d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304798&auth_key=1762304798-0-0-693b5f17eb1992a9773e06a301d9291d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea01727e936edd399e9f3a0b8c866995~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304805&auth_key=1762304805-0-0-5ce846dd26e17f9db42f792626882758&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b31d9e9163db90294db9f19d5327ed1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304811&auth_key=1762304811-0-0-d6e8ed64bca8c3d336e5816bf86c2b10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b17390efaf3a95fa6fe65025a2d4b11~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304819&auth_key=1762304819-0-0-ffc8d36d46fd00d4aa012fe795f2231a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Simultaneous-optimization-of-non-coplanar-beam-orientations-and-cumulative-EQD2-distribution-for-high-dose-reirradiation-of-locoregionally-recurrent-non-small-cell-lung-cancer"><a href="#Simultaneous-optimization-of-non-coplanar-beam-orientations-and-cumulative-EQD2-distribution-for-high-dose-reirradiation-of-locoregionally-recurrent-non-small-cell-lung-cancer" class="headerlink" title="Simultaneous optimization of non-coplanar beam orientations and   cumulative EQD2 distribution for high-dose reirradiation of locoregionally   recurrent non-small cell lung cancer"></a>Simultaneous optimization of non-coplanar beam orientations and   cumulative EQD2 distribution for high-dose reirradiation of locoregionally   recurrent non-small cell lung cancer</h2><p><strong>Authors:Nathan Torelli, Jonas Willmann, Katja Daehler, Madalyne Day, Nicolaus Andratschke, Jan Unkelbach</strong></p>
<p>Background and Purpose: Reirradiation for non-small cell lung cancer (NSCLC) is commonly delivered using coplanar techniques. In this study, we developed a beam orientation optimization algorithm for reirradiation planning to investigate whether the selection of favorable non-coplanar beam orientations may limit cumulative doses to critical organs-at-risk (OARs) and thus improve the therapeutic window.   Materials and Methods: Fifteen cases of challenging high-dose reirradiation for locoregionally recurrent NSCLC were included in this in-silico study. For each patient, the dose distribution from the previous treatment was first mapped to the reirradiation planning CT using rigid dose registration, and subsequently converted to equivalent dose in 2 Gy fractions (EQD2). A 2-arc non-coplanar reirradiation plan, combining dynamic gantry and couch rotation, was then generated using an EQD2-based direct aperture optimization algorithm, which allows for the simultaneous optimization of the dynamic gantry-couch path and the cumulative EQD2 distribution. Non-coplanar reirradiation plans were benchmarked against 2-arc coplanar VMAT plans, which mimic state-of-the-art practice for reirradiation of NSCLC.   Results: Non-coplanar reirradiation plans could reduce the maximum cumulative EQD2 to critical OARs such as bronchial tree, esophagus, thoracic wall and trachea by at least 5 Gy2 for 6 out of 15 patients compared to coplanar reirradiation plans. At the same time, target coverage and lung EQD2 metrics were comparable for both methods.   Conclusions: The automated selection of favorable non-coplanar beam orientations may reduce the maximum cumulative EQD2 to critical OARs in challenging thoracic reirradiation cases. This allows to explore either better OAR sparing or dose-escalation in future clinical studies. </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®çš„ï¼šéå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„å†ç…§å°„é€šå¸¸ä½¿ç”¨å…±é¢æŠ€æœ¯è¿›è¡Œæ²»ç–—ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§æŸæµæ–¹å‘ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå†ç…§å°„æ²»ç–—è®¡åˆ’ï¼Œä»¥ç ”ç©¶é€‰æ‹©åˆé€‚çš„éå…±é¢æŸæµæ–¹å‘æ˜¯å¦å¯èƒ½å‡å°‘å…³é”®å±é™©å™¨å®˜çš„ç´¯ç§¯å‰‚é‡ï¼Œä»è€Œæé«˜æ²»ç–—çª—å£ã€‚ææ–™ä¸æ–¹æ³•ï¼šæœ¬ç ”ç©¶é‡‡ç”¨è™šæ‹Ÿæ¨¡æ‹Ÿæ–¹å¼å¯¹15ä¾‹å±€éƒ¨åŒºåŸŸå¤å‘NSCLCçš„é«˜å‰‚é‡å†ç…§å°„æ²»ç–—è¿›è¡Œç ”ç©¶ã€‚å¯¹äºæ¯ä½æ‚£è€…ï¼Œé¦–å…ˆä½¿ç”¨åˆšå‰‚é‡æ³¨å†Œå°†å‰æ¬¡æ²»ç–—çš„å‰‚é‡åˆ†å¸ƒæ˜ å°„åˆ°å†ç…§å°„è®¡åˆ’CTä¸Šï¼Œç„¶åè½¬æ¢ä¸ºç­‰æ•ˆå‰‚é‡ï¼ˆåœ¨2 Gyå‰‚é‡ä¸­ï¼‰ã€‚ç„¶åé‡‡ç”¨åŸºäºç­‰æ•ˆå‰‚é‡çš„ç›´æ¥å­”å¾„ä¼˜åŒ–ç®—æ³•ç”Ÿæˆä¸€ä¸ªç”±åŠ¨æ€æœºæ¶å’ŒåºŠæ¿æ—‹è½¬ç›¸ç»“åˆçš„ä¸¤å¼§éå…±é¢å†ç…§å°„è®¡åˆ’ï¼Œè¯¥ç®—æ³•å…è®¸åŒæ—¶ä¼˜åŒ–åŠ¨æ€æœºæ¶-åºŠæ¿è·¯å¾„å’Œç´¯ç§¯ç­‰æ•ˆå‰‚é‡åˆ†å¸ƒã€‚éå…±é¢å†ç…§å°„è®¡åˆ’ä¸ä¸¤å¼§å…±é¢VMATè®¡åˆ’è¿›è¡Œäº†æ¯”è¾ƒï¼Œåè€…æ¨¡æ‹Ÿäº†NSCLCå†ç…§å°„çš„ç°è¡Œå®è·µã€‚ç»“æœï¼šå¯¹äºè¿™åäº”ä¾‹æ‚£è€…ä¸­çš„å…­åæ‚£è€…ï¼Œä¸ä¸¤å¼§å…±é¢å†ç…§å°„è®¡åˆ’ç›¸æ¯”ï¼Œéå…±é¢å†ç…§å°„è®¡åˆ’å¯ä»¥å‡å°‘å¯¹æ”¯æ°”ç®¡æ ‘ã€é£Ÿé“ã€èƒ¸å£å’Œæ°”ç®¡ç­‰å…³é”®å±é™©å™¨å®˜çš„ç´¯ç§¯ç­‰æ•ˆå‰‚é‡è‡³å°‘å‡å°‘5 GyÂ²ã€‚åŒæ—¶ï¼Œä¸¤ç§æ–¹æ³•çš„ç›®æ ‡è¦†ç›–èŒƒå›´å’Œè‚ºç­‰æ•ˆå‰‚é‡æŒ‡æ ‡ç›¸å½“ã€‚ç»“è®ºï¼šè‡ªåŠ¨é€‰æ‹©æœ‰åˆ©çš„éå…±é¢æŸæµæ–¹å‘å¯ä»¥å‡å°‘å…·æœ‰æŒ‘æˆ˜æ€§çš„èƒ¸éƒ¨å†ç…§å°„ç—…ä¾‹ä¸­å…³é”®å±é™©å™¨å®˜çš„ç´¯ç§¯ç­‰æ•ˆå‰‚é‡æœ€å¤§å€¼ã€‚è¿™ä¸ºæœªæ¥çš„ä¸´åºŠç ”ç©¶æä¾›äº†æ›´å¥½çš„å™¨å®˜ä¿æŠ¤æˆ–å‰‚é‡é€’å¢çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºéå…±é¢æŠ€æœ¯è¿›è¡Œä¼˜åŒ–åçš„æ”¾å°„æ²»ç–—è®¡åˆ’å¯¹éå°ç»†èƒè‚ºç™Œæ‚£è€…è€Œè¨€å¯ä¼˜åŒ–æ²»ç–—æ•ˆæœã€‚è¯¥è®¡åˆ’é‡‡ç”¨éå…±é¢å¼§è¿›è¡Œæ”¾å°„æŸæ“ä½œä»¥å®ç°å¯¹æ•æ„Ÿå™¨å®˜çš„ç´¯ç§¯å‰‚é‡æœ€å°åŒ–ã€‚è¯¥æ–¹æ³•å¯åœ¨ä¸å½±å“ç›®æ ‡è¦†ç›–ç‡å’Œè‚ºEQD2æŒ‡æ ‡çš„å‰æä¸‹å®ç°ç›®çš„ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶è¡¨æ˜å¯¹éå°ç»†èƒè‚ºç™Œæ‚£è€…çš„é‡æ–°æ”¾ç–—è¿›è¡Œå…‰æŸæ–¹ä½è§’ä¼˜åŒ–å¯æé«˜æ²»ç–—æ•ˆæœå’Œä¸´åºŠå¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–ç®—æ³•èƒ½å¤Ÿå®ç°æ²»ç–—ç›®æ ‡çš„è‰¯å¥½è¦†ç›–ä»¥åŠå®ç°å™¨å®˜çš„å‰‚é‡ç´¯ç§¯é™ä½çš„ç›®æ ‡ï¼Œä»è€Œå‡å°‘æ•æ„Ÿå™¨å®˜å¦‚æ”¯æ°”ç®¡æ ‘ã€é£Ÿç®¡ã€èƒ¸å£å’Œæ°”ç®¡ç­‰å™¨å®˜å¯¹é«˜å‰‚é‡è¾å°„çš„æš´éœ²é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>å…³äºåŒ»å­¦å›¾åƒä¸­é‡æ”¾ç–—æ–¹æ¡ˆçš„ä¼˜åŒ–ç ”ç©¶å…³é”®æ´è§å¦‚ä¸‹ï¼š</p>
<ol>
<li>é‡æ”¾ç–—ä¸­å¸¸ç”¨å…±é¢æŠ€æœ¯æ¥æ²»ç–—éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ã€‚ä½†å¯¹äºå¤æ‚ç—…ä¾‹ï¼Œå¯èƒ½éœ€è¦æ›´å…ˆè¿›çš„è®¡åˆ’æ–¹æ¡ˆä»¥æé«˜æ²»ç–—æ•ˆæœã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨éå…±é¢å¼§æŠ€æœ¯ä¼˜åŒ–é‡æ”¾ç–—è®¡åˆ’ï¼Œæ—¨åœ¨å‡å°‘æ•æ„Ÿå™¨å®˜å¯¹é«˜å‰‚é‡è¾å°„çš„ç´¯ç§¯æš´éœ²é£é™©ã€‚è¿™æœ‰åŠ©äºæé«˜æ²»ç–—æ•ˆæœå¹¶å‡å°‘å¹¶å‘ç—‡é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶æŒ‡å‡ºéå…±é¢è®¡åˆ’å¯é’ˆå¯¹æ”¯æ°”ç®¡æ ‘ã€é£Ÿç®¡ç­‰å…³é”®å™¨å®˜é™ä½æœ€å¤§ç´¯ç§¯ç­‰æ•ˆå‰‚é‡ï¼ˆEQD2ï¼‰ã€‚è¿™ä¸ºæœªæ¥ä¸´åºŠç ”ç©¶ä¸­æ›´å¥½çš„å™¨å®˜ä¿æŠ¤æˆ–å‰‚é‡æå‡æä¾›äº†å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e787fffdb625daa2b826b1a65d8a211f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304826&auth_key=1762304826-0-0-3caa8d249f2ba32e1f34f9c68ad73707&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MV-MLM-Bridging-Multi-View-Mammography-and-Language-for-Breast-Cancer-Diagnosis-and-Risk-Prediction"><a href="#MV-MLM-Bridging-Multi-View-Mammography-and-Language-for-Breast-Cancer-Diagnosis-and-Risk-Prediction" class="headerlink" title="MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer   Diagnosis and Risk Prediction"></a>MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer   Diagnosis and Risk Prediction</h2><p><strong>Authors:Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba</strong></p>
<p>Large annotated datasets are essential for training robust Computer-Aided Diagnosis (CAD) models for breast cancer detection or risk prediction. However, acquiring such datasets with fine-detailed annotation is both costly and time-consuming. Vision-Language Models (VLMs), such as CLIP, which are pre-trained on large image-text pairs, offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks. This paper introduces a novel Multi-View Mammography and Language Model for breast cancer classification and risk prediction, trained on a dataset of paired mammogram images and synthetic radiology reports. Our MV-MLM leverages multi-view supervision to learn rich representations from extensive radiology data by employing cross-modal self-supervision across image-text pairs. This includes multiple views and the corresponding pseudo-radiology reports. We propose a novel joint visual-textual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristics(calcification, mass) and utilize these patterns to understand mammography images and predict cancer risk. We evaluated our method on both private and publicly available datasets, demonstrating that the proposed model achieves state-of-the-art performance in three classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. Furthermore, the model exhibits strong data efficiency, outperforming existing fully supervised or VLM baselines while trained on synthetic text reports and without the need for actual radiology reports. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ³¨é‡Šæ•°æ®é›†å¯¹äºè®­ç»ƒç”¨äºä¹³è…ºç™Œæ£€æµ‹æˆ–é£é™©é¢„æµ‹çš„ç¨³å¥è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰æ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè·å–å…·æœ‰ç²¾ç»†è¯¦ç»†æ³¨é‡Šçš„æ­¤ç±»æ•°æ®é›†æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä¸ºè§£å†³åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæ•°æ®æ•ˆç‡é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºä¹³è…ºç™Œåˆ†ç±»å’Œé£é™©é¢„æµ‹çš„æ–°å‹å¤šè§†è§’ä¹³è…ºXå…‰æ‘„å½±å’Œè¯­è¨€æ¨¡å‹ï¼ˆMV-MLMï¼‰ã€‚è¯¥æ¨¡å‹åœ¨é…å¯¹çš„ä¹³è…ºXå…‰å›¾åƒå’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„MV-MLMåˆ©ç”¨å¤šè§†å›¾ç›‘ç£ï¼Œé€šè¿‡è·¨å›¾åƒæ–‡æœ¬å¯¹çš„è·¨æ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ä»å¹¿æ³›çš„æ”¾å°„å­¦æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„è¡¨ç¤ºã€‚è¿™åŒ…æ‹¬å¤šä¸ªè§†å›¾å’Œç›¸åº”çš„ä¼ªæ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è”åˆè§†è§‰æ–‡æœ¬å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜ä¸åŒæ•°æ®ç±»å‹å’Œä»»åŠ¡ä¸Šçš„é€šç”¨æ€§å’Œå‡†ç¡®æ€§ï¼Œä»¥åŒºåˆ†ä¹³è…ºç»„ç»‡æˆ–ç™Œç—‡ç‰¹å¾ï¼ˆé’™åŒ–ã€è‚¿å—ï¼‰ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ¨¡å¼æ¥ç†è§£ä¹³è…ºXå…‰å›¾åƒå¹¶é¢„æµ‹ç™Œç—‡é£é™©ã€‚æˆ‘ä»¬åœ¨ç§æœ‰å’Œå…¬å¼€å¯ç”¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰é¡¹åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼šï¼ˆ1ï¼‰æ¶æ€§åˆ†ç±»ï¼Œï¼ˆ2ï¼‰äºšå‹åˆ†ç±»ï¼Œï¼ˆ3ï¼‰åŸºäºå›¾åƒçš„ç™Œç—‡é£é™©é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ•°æ®æ•ˆç‡ï¼Œåœ¨åˆæˆæ–‡æœ¬æŠ¥å‘Šä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ— éœ€å®é™…æ”¾å°„å­¦æŠ¥å‘Šå³å¯è¶…è¶Šç°æœ‰çš„å®Œå…¨ç›‘ç£æˆ–VLMåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26151v1">PDF</a> Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)   Workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤šè§†è§’ä¹³è…ºæ‘„å½±ä¸è¯­è¨€æ¨¡å‹ï¼ˆMV-MLMï¼‰è¿›è¡Œä¹³è…ºç™Œåˆ†ç±»ä¸é£é™©é¢„æµ‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ï¼Œåœ¨é…å¯¹ä¹³è…ºXå…‰å›¾åƒå’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡å¤šè§†è§’ç›‘ç£å­¦ä¹ å’Œè·¨æ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ï¼ŒMV-MLMèƒ½å¤Ÿä»ä¸°å¯Œçš„æ”¾å°„å­¦æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶åœ¨ä¸åŒç±»å‹çš„ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå®ç°ä¼˜å¼‚çš„æ¨å¹¿å’Œå‡†ç¡®æ€§èƒ½ã€‚åœ¨ç§æœ‰å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¶æ€§åˆ†ç±»ã€äºšå‹åˆ†ç±»å’ŒåŸºäºå›¾åƒçš„ç™Œç—‡é£é™©é¢„æµ‹ä¸‰é¡¹ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨ä½¿ç”¨åˆæˆæ–‡æœ¬æŠ¥å‘Šæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ•ˆç‡ï¼Œæ— éœ€å®é™…æ”¾å°„å­¦æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ³¨é‡Šæ•°æ®é›†å¯¹äºè®­ç»ƒè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰æ¨¡å‹è¿›è¡Œä¹³è…ºç™Œæ£€æµ‹æˆ–é£é™©é¢„æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>è·å–ç²¾ç»†è¯¦ç»†æ³¨é‡Šçš„æ•°æ®é›†æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚</li>
<li>è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPèƒ½æé«˜åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„ç¨³å¥æ€§å’Œæ•°æ®æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’ä¹³è…ºæ‘„å½±ä¸è¯­è¨€æ¨¡å‹ï¼ˆMV-MLMï¼‰è¿›è¡Œä¹³è…ºç™Œåˆ†ç±»ä¸é£é™©é¢„æµ‹ã€‚</li>
<li>MV-MLMåˆ©ç”¨å¤šè§†è§’ç›‘ç£å­¦ä¹ ä»ä¸°å¯Œçš„æ”¾å°„å­¦æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºã€‚</li>
<li>MV-MLMåœ¨å¤šç§åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬æ¶æ€§åˆ†ç±»ã€äºšå‹åˆ†ç±»å’ŒåŸºäºå›¾åƒçš„ç™Œç—‡é£é™©é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bd8861949d6101b905544bf9c7eabc4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304834&auth_key=1762304834-0-0-402154990774d945dedf4c578251a769&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8218ac635f5991310f35fe100a4965a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304841&auth_key=1762304841-0-0-4abef9b141fd718ccadf99a85f426341&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c4719c02473929605d6efa5c2f48b27~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304848&auth_key=1762304848-0-0-bc964ad70f22d627eea3d8b77cbeb960&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3d16fd9873afe75c385b3991ee5c342~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304854&auth_key=1762304854-0-0-b1a6357a7c6b4b4382326514c2226f40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FlexICL-A-Flexible-Visual-In-context-Learning-Framework-for-Elbow-and-Wrist-Ultrasound-Segmentation"><a href="#FlexICL-A-Flexible-Visual-In-context-Learning-Framework-for-Elbow-and-Wrist-Ultrasound-Segmentation" class="headerlink" title="FlexICL: A Flexible Visual In-context Learning Framework for Elbow and   Wrist Ultrasound Segmentation"></a>FlexICL: A Flexible Visual In-context Learning Framework for Elbow and   Wrist Ultrasound Segmentation</h2><p><strong>Authors:Yuyue Zhou, Jessica Knight, Shrimanti Ghosh, Banafshe Felfeliyan, Jacob L. Jaremko, Abhilash R. Hareendranathan</strong></p>
<p>Elbow and wrist fractures are the most common fractures in pediatric populations. Automatic segmentation of musculoskeletal structures in ultrasound (US) can improve diagnostic accuracy and treatment planning. Fractures appear as cortical defects but require expert interpretation. Deep learning (DL) can provide real-time feedback and highlight key structures, helping lightly trained users perform exams more confidently. However, pixel-wise expert annotations for training remain time-consuming and costly. To address this challenge, we propose FlexICL, a novel and flexible in-context learning (ICL) framework for segmenting bony regions in US images. We apply it to an intra-video segmentation setting, where experts annotate only a small subset of frames, and the model segments unseen frames. We systematically investigate various image concatenation techniques and training strategies for visual ICL and introduce novel concatenation methods that significantly enhance model performance with limited labeled data. By integrating multiple augmentation strategies, FlexICL achieves robust segmentation performance across four wrist and elbow US datasets while requiring only 5% of the training images. It outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and conventional segmentation models like U-Net and TransUNet by 1-27% Dice coefficient on 1,252 US sweeps. These initial results highlight the potential of FlexICL as an efficient and scalable solution for US image segmentation well suited for medical imaging use cases where labeled data is scarce. </p>
<blockquote>
<p>è‚˜å…³èŠ‚å’Œè…•éƒ¨éª¨æŠ˜æ˜¯å„¿ç«¥ä¸­æœ€å¸¸è§çš„éª¨æŠ˜ç±»å‹ã€‚è¶…å£°ï¼ˆUSï¼‰ä¸­éª¨éª¼è‚Œè‚‰ç»“æ„çš„è‡ªåŠ¨åˆ†å‰²å¯ä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ²»ç–—è®¡åˆ’ã€‚éª¨æŠ˜è¡¨ç°ä¸ºçš®è´¨ç¼ºé™·ï¼Œä½†éœ€è¦è¿›è¡Œä¸“å®¶è§£è¯»ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å¯ä»¥æä¾›å®æ—¶åé¦ˆå¹¶çªå‡ºå…³é”®ç»“æ„ï¼Œå¸®åŠ©è½»åº¦è®­ç»ƒçš„ç”¨æˆ·æ›´æœ‰ä¿¡å¿ƒåœ°è¿›è¡Œæ£€æŸ¥ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„é€åƒç´ ä¸“å®¶æ³¨é‡Šä»ç„¶è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FlexICLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¶…å£°å›¾åƒåˆ†å‰²éª¨éª¼åŒºåŸŸçš„æ–°å‹çµæ´»ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºè§†é¢‘å†…åˆ†å‰²è®¾ç½®ï¼Œå…¶ä¸­ä¸“å®¶åªæ³¨é‡Šä¸€å°éƒ¨åˆ†å¸§ï¼Œæ¨¡å‹åˆ™åˆ†å‰²æœªè§è¿‡çš„å¸§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å„ç§å›¾åƒæ‹¼æ¥æŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥ï¼Œç”¨äºè§†è§‰ICLï¼Œå¹¶å¼•å…¥äº†æ–°å‹æ‹¼æ¥æ–¹æ³•ï¼Œåœ¨æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹æ˜¾ç€æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡æ•´åˆå¤šç§å¢å¼ºç­–ç•¥ï¼ŒFlexICLåœ¨å››ä¸ªæ‰‹è…•å’Œè‚˜å…³èŠ‚è¶…å£°æ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦ä½¿ç”¨5%çš„è®­ç»ƒå›¾åƒã€‚å®ƒåœ¨1252æ¬¡è¶…å£°æ‰«æä¸­ï¼Œç›¸è¾ƒäºå…ˆè¿›çš„è§†è§‰ICLæ¨¡å‹ï¼ˆå¦‚Painterã€MAE-VQGANï¼‰å’Œä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ï¼ˆå¦‚U-Netå’ŒTransUNetï¼‰ï¼ŒDiceç³»æ•°æé«˜äº†1-27%ã€‚è¿™äº›åˆæ­¥ç»“æœçªæ˜¾äº†FlexICLä½œä¸ºè¶…å£°å›¾åƒåˆ†å‰²çš„é«˜æ•ˆå¯ä¼¸ç¼©è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œå°¤å…¶é€‚åˆåŒ»å­¦å½±åƒç”¨ä¾‹ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlexICLçš„æ–°å‹çµæ´»ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨è¶…å£°å›¾åƒä¸­åˆ†å‰²éª¨éª¼åŒºåŸŸã€‚è¯¥æ¡†æ¶åº”ç”¨äºè§†é¢‘å†…åˆ†å‰²åœºæ™¯ï¼Œä¸“å®¶åªéœ€å¯¹ä¸€å°éƒ¨åˆ†å¸§è¿›è¡Œæ ‡æ³¨ï¼Œæ¨¡å‹å³å¯å¯¹æœªè§çš„å¸§è¿›è¡Œåˆ†å‰²ã€‚é€šè¿‡æ¢ç´¢å„ç§å›¾åƒæ‹¼æ¥æŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¼•å…¥æ–°çš„æ‹¼æ¥æ–¹æ³•ï¼ŒFlexICLåœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹å®ç°äº†å‡ºè‰²çš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨å››ä¸ªæ‰‹è…•å’Œè‚˜éƒ¨è¶…å£°æ•°æ®é›†ä¸Šï¼Œä»…éœ€ä½¿ç”¨5%çš„è®­ç»ƒå›¾åƒï¼Œè¯¥æ¡†æ¶ä¾¿å®ç°äº†ç¨³å¥çš„åˆ†å‰²æ•ˆæœï¼Œä¼˜äºå…¶ä»–å…ˆè¿›è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å‹å’Œä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexICLæ˜¯ä¸€ç§æ–°å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨è¶…å£°å›¾åƒä¸­åˆ†å‰²éª¨éª¼åŒºåŸŸã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºè§†é¢‘å†…åˆ†å‰²åœºæ™¯ï¼Œä¸“å®¶åªéœ€æ ‡æ³¨ä¸€å°éƒ¨åˆ†å¸§ã€‚</li>
<li>FlexICLé€šè¿‡æ¢ç´¢å¤šç§å›¾åƒæ‹¼æ¥æŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥çš„æ–°å‹æ‹¼æ¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„æ¨¡å‹è¡¨ç°ã€‚</li>
<li>FlexICLå®ç°äº†ç¨³å¥çš„åˆ†å‰²æ€§èƒ½ï¼Œé€‚ç”¨äºæ‰‹è…•å’Œè‚˜éƒ¨è¶…å£°æ•°æ®çš„åˆ†å‰²ã€‚</li>
<li>è¯¥æ¡†æ¶ä¼˜äºå…¶ä»–å…ˆè¿›çš„è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å‹å’Œä¼ ç»Ÿåˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-36e4395efb772e3c00ac5bc724dd853a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304862&auth_key=1762304862-0-0-5f0e431a0418896c45c7a1a9e7f4b211&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92e5e711b5c7ab70b00de81301a94f57~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304869&auth_key=1762304869-0-0-7a1abb02d01d77cd84e3da91b5b1a863&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2166044adb8228e02551b4683a96850d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304875&auth_key=1762304875-0-0-74e99ac8a1ba32469491bc1566bedfe7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a813a12c785e26872b925291a143e19~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304882&auth_key=1762304882-0-0-787ca031275f3679f5845c58f05a820b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56f9561dffbc7e23215c95787314cdc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304889&auth_key=1762304889-0-0-ce5ad07a9bf6462e09ddf8fd4f723ac4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Fine-tuning-Segment-Anything-for-Real-Time-Tumor-Tracking-in-Cine-MRI"><a href="#Fine-tuning-Segment-Anything-for-Real-Time-Tumor-Tracking-in-Cine-MRI" class="headerlink" title="Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI"></a>Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI</h2><p><strong>Authors:Valentin Boussot, CÃ©dric HÃ©mon, Jean-Claude Nunes, Jean-Louis Dillenseger</strong></p>
<p>In this work, we address the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences of the thoracic and abdominal regions under strong data scarcity constraints. Two complementary strategies were explored: (i) unsupervised registration with the IMPACT similarity metric and (ii) foundation model-based segmentation leveraging SAM 2.1 and its recent variants through prompt-based interaction. Due to the one-second runtime constraint, the SAM-based method was ultimately selected. The final configuration used SAM2.1 b+ with mask-based prompts from the first annotated slice, fine-tuned solely on the small labeled subset from TrackRAD2025. Training was configured to minimize overfitting, using 1024x1024 patches (batch size 1), standard augmentations, and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve generalization while adapting to annotator-specific styles. Training lasted 300 epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently applied across all anatomical sites and MRI field strengths. Test-time augmentation was considered but ultimately discarded due to negligible performance gains. The final model was selected based on the highest Dice Similarity Coefficient achieved on the validation set after fine-tuning. On the hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall in the TrackRAD2025 challenge. These results highlight the strong potential of foundation models for accurate and real-time tumor tracking in MRI-guided radiotherapy. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹TrackRAD2025æŒ‘æˆ˜ï¼Œåœ¨å¼ºçƒˆçš„æ•°æ®ç¨€ç¼ºé™åˆ¶ä¸‹ï¼Œå®ç°äº†èƒ¸éƒ¨å’Œè…¹éƒ¨MRIåºåˆ—ä¸­çš„å®æ—¶è‚¿ç˜¤è·Ÿè¸ªã€‚æ¢ç´¢äº†ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šï¼ˆiï¼‰ä½¿ç”¨IMPACTç›¸ä¼¼åº¦æŒ‡æ ‡çš„æ— ç›‘ç£æ³¨å†Œï¼›ï¼ˆiiï¼‰åŸºäºæç¤ºäº¤äº’çš„SAM 2.1åŠå…¶æœ€æ–°å˜ç§çš„åŸºç¡€æ¨¡å‹åˆ†å‰²ã€‚ç”±äºå­˜åœ¨ä¸€ç§’é’Ÿçš„è¿è¡Œæ—¶é—´é™åˆ¶ï¼Œæœ€ç»ˆé€‰æ‹©äº†åŸºäºSAMçš„æ–¹æ³•ã€‚æœ€ç»ˆé…ç½®ä½¿ç”¨äº†SAM 2.1 b+ï¼Œå¹¶åŸºäºæ¥è‡ªç¬¬ä¸€ä¸ªæ ‡æ³¨åˆ‡ç‰‡çš„æ©æ¨¡æç¤ºï¼Œä»…å¯¹TrackRAD2025ä¸­çš„å°éƒ¨åˆ†æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚è®­ç»ƒé…ç½®æ—¨åœ¨æœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆï¼Œä½¿ç”¨1024x1024çš„è¡¥ä¸ï¼ˆæ‰¹æ¬¡å¤§å°ä¸º1ï¼‰ã€æ ‡å‡†å¢å¼ºå’Œå¹³è¡¡çš„Dice + IoUæŸå¤±ã€‚æ‰€æœ‰æ¨¡å—ï¼ˆæç¤ºç¼–ç å™¨ã€è§£ç å™¨ã€Hieraéª¨å¹²ç½‘ï¼‰åº”ç”¨ä½ç»Ÿä¸€å­¦ä¹ ç‡ï¼ˆ0.0001ï¼‰ï¼Œä»¥åœ¨é€‚åº”æ³¨é‡Šå™¨ç‰¹å®šé£æ ¼çš„åŒæ—¶ä¿æŒæ³›åŒ–èƒ½åŠ›ã€‚è®­ç»ƒæŒç»­äº†300ä¸ªå‘¨æœŸï¼ˆåœ¨RTX A6000ã€48GBä¸Šçº¦12å°æ—¶ï¼‰ã€‚æ‰€æœ‰è§£å‰–éƒ¨ä½å’ŒMRIåœºå¼ºå‡å§‹ç»ˆåº”ç”¨ç›¸åŒçš„æ¨ç†ç­–ç•¥ã€‚è€ƒè™‘äº†æµ‹è¯•æ—¶é—´å¢å¼ºï¼Œä½†æœ€ç»ˆå› å‡ ä¹å¯ä»¥å¿½ç•¥çš„æ€§èƒ½æå‡è€Œé­åˆ°æ”¾å¼ƒã€‚æœ€ç»ˆæ¨¡å‹çš„é€‰æ‹©åŸºäºå¾®è°ƒååœ¨éªŒè¯é›†ä¸Šå®ç°çš„æœ€é«˜Diceç›¸ä¼¼ç³»æ•°ã€‚åœ¨éšè—æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†0.8794çš„Diceå¾—åˆ†ï¼Œåœ¨TrackRAD2025æŒ‘æˆ˜ä¸­æ€»ä½“æ’åç¬¬6ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨MRIå¼•å¯¼çš„æ”¾å°„æ²»ç–—ä¸­è¿›è¡Œå‡†ç¡®å®æ—¶è‚¿ç˜¤è·Ÿè¸ªçš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25990v1">PDF</a> Paper for the Trackrad2025 challenge, Team BreizhTrack</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é’ˆå¯¹TrackRAD2025æŒ‘æˆ˜ï¼Œç ”ç©¶åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹å®æ—¶è¿½è¸ªèƒ¸è…¹éƒ¨MRIåºåˆ—ä¸­çš„è‚¿ç˜¤ã€‚é‡‡ç”¨ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šä¸€æ˜¯ä½¿ç”¨IMPACTç›¸ä¼¼æ€§åº¦é‡çš„æ— ç›‘ç£æ³¨å†Œï¼ŒäºŒæ˜¯åŸºäºSAM 2.1åŠå…¶å˜ä½“é€šè¿‡æç¤ºäº¤äº’çš„åŸºç¡€æ¨¡å‹åˆ†å‰²æ³•ã€‚æœ€ç»ˆé€‰ç”¨SAM-basedæ–¹æ³•ï¼Œåœ¨TrackRAD2025çš„å°æ ‡æ³¨å­é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†é«˜æ•ˆçš„è‚¿ç˜¤è¿½è¸ªã€‚æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°è¾ƒé«˜çš„Diceç›¸ä¼¼ç³»æ•°ï¼Œåœ¨éšè—æµ‹è¯•é›†ä¸Šæ’åç¬¬å…­ã€‚ç»“æœå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨MRIå¼•å¯¼çš„æ”¾å°„æ²»ç–—ä¸­è¿›è¡Œå‡†ç¡®å®æ—¶è‚¿ç˜¤è¿½è¸ªçš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹TrackRAD2025æŒ‘æˆ˜ï¼Œè§£å†³å®æ—¶è‚¿ç˜¤è¿½è¸ªé—®é¢˜ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç§ç­–ç•¥ï¼šæ— ç›‘ç£æ³¨å†Œå’ŒåŸºäºåŸºç¡€æ¨¡å‹çš„åˆ†å‰²æ³•ã€‚</li>
<li>æœ€ç»ˆé€‰ç”¨SAM-basedæ–¹æ³•ï¼Œæ»¡è¶³ä¸€ç§’é’Ÿè¿è¡Œæ—¶é—´é™åˆ¶ã€‚</li>
<li>ä½¿ç”¨SAM2.1 b+ä¸åŸºäºæ©è†œçš„æç¤ºï¼Œä»é¦–ä¸ªæ³¨é‡Šåˆ‡ç‰‡å¼€å§‹å¾®è°ƒã€‚</li>
<li>è®­ç»ƒé…ç½®æ—¨åœ¨æœ€å°åŒ–è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨1024x1024çš„è¡¥ä¸å’Œå¹³è¡¡Dice + IoUæŸå¤±ã€‚</li>
<li>åº”ç”¨ä½ç»Ÿä¸€å­¦ä¹ ç‡ä»¥ä¿ç•™æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é€‚åº”æ³¨é‡Šå™¨ç‰¹å®šé£æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-60ff729c542977c6811b5cf6af07f3bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304896&auth_key=1762304896-0-0-bdb05b9f3465e193bfba1d1640e1d2f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-322fe317f6233f2060ab67a085da6328~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304903&auth_key=1762304903-0-0-7229393ae5698c41aa433cfd8701aa61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e597d8fb9b3e420b002b9270d7f85a8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304909&auth_key=1762304909-0-0-a3845183686e02cfb714922c64ca52ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b5c6fdcb4bec837f18148c9fbdbc53d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304916&auth_key=1762304916-0-0-cda9f03456b460b6d811082579960654&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b193fcb21e60ab5b658e18a6af21c8cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304922&auth_key=1762304922-0-0-daf827f110426aebd876e02d015c481d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer"><a href="#Brain-IT-Image-Reconstruction-from-fMRI-via-Brain-Interaction-Transformer" class="headerlink" title="Brain-IT: Image Reconstruction from fMRI via Brain-Interaction   Transformer"></a>Brain-IT: Image Reconstruction from fMRI via Brain-Interaction   Transformer</h2><p><strong>Authors:Roman Beliy, Amit Zalcher, Jonathan Kogman, Navve Wasserman, Michal Irani</strong></p>
<p>Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present â€œBrain-ITâ€, a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters &amp; subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BITâ€™s design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings. </p>
<blockquote>
<p>é€šè¿‡ä»fMRIè„‘è®°å½•ä¸­é‡å»ºäººä»¬çœ‹åˆ°çš„å›¾åƒï¼Œä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ— åˆ›åœ°è§‚å¯Ÿäººè„‘çš„çª—å£ã€‚å°½ç®¡æœ€è¿‘ç”±æ‰©æ•£æ¨¡å‹å–å¾—çš„è¿›å±•ï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹å®é™…çœ‹åˆ°çš„å›¾åƒçš„å¿ å®åº¦ã€‚æˆ‘ä»¬æå‡ºâ€œBrain-ITâ€ï¼Œè¿™æ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡Brain Interaction Transformer (BIT)è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½¿åŠŸèƒ½ç›¸ä¼¼çš„è„‘ä½“ç´ ç°‡ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„äº¤äº’ã€‚è¿™äº›åŠŸèƒ½ç°‡æ˜¯æ‰€æœ‰ä¸»é¢˜æ‰€å…±æœ‰çš„ï¼Œå¯ä½œä¸ºæ•´åˆè„‘å†…å’Œè·¨è„‘ä¿¡æ¯çš„æ„å»ºå—ã€‚æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½è¢«æ‰€æœ‰é›†ç¾¤å’Œä¸»é¢˜æ‰€å…±äº«ï¼Œå…è®¸åœ¨æœ‰é™çš„æ•°æ®é‡ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚ä¸ºäº†å¼•å¯¼å›¾åƒé‡å»ºï¼ŒBITé¢„æµ‹ä¸¤ä¸ªäº’è¡¥çš„å±€éƒ¨æ–‘å—çº§å›¾åƒç‰¹å¾ï¼š(i)é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹èµ°å‘å›¾åƒçš„æ­£ç¡®è¯­ä¹‰å†…å®¹ï¼›(ii)ä½çº§ç»“æ„ç‰¹å¾ï¼Œæœ‰åŠ©äºä»¥æ­£ç¡®çš„ç²—ç•¥å¸ƒå±€åˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹ã€‚BITçš„è®¾è®¡å®ç°äº†ä»è„‘ä½“ç´ ç°‡åˆ°å±€éƒ¨å›¾åƒç‰¹å¾çš„ç›´æ¥ä¿¡æ¯æµã€‚é€šè¿‡è¿™äº›åŸç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»fMRIé‡å»ºå›¾åƒï¼Œå¿ å®é‡å»ºæ‰€çœ‹åˆ°çš„å›¾åƒï¼Œæ— è®ºåœ¨è§†è§‰ä¸Šè¿˜æ˜¯é€šè¿‡æ ‡å‡†å®¢è§‚æŒ‡æ ‡ä¸Šï¼Œéƒ½è¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è€Œä¸”ï¼Œä»…ä½¿ç”¨æ–°ä¸»é¢˜çš„1å°æ—¶fMRIæ•°æ®ï¼Œæˆ‘ä»¬å°±èƒ½è¾¾åˆ°ä¸å½“å‰åœ¨å®Œæ•´çš„40å°æ—¶è®°å½•ä¸Šè®­ç»ƒçš„æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25976v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é€šè¿‡fMRIè„‘è®°å½•é‡å»ºäººç±»æ‰€çœ‹åˆ°çš„å›¾åƒï¼Œæä¾›äº†ä¸€ä¸ªéä¾µå…¥å¼çš„ç ”ç©¶äººç±»å¤§è„‘çš„æ–¹å¼ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€ä¸èƒ½çœŸå®åœ°é‡å»ºå®é™…çœ‹åˆ°çš„å›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºäº†â€œBrain-ITâ€ï¼Œè¿™æ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡Brain Interaction Transformerï¼ˆBITï¼‰è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œä½¿åŠŸèƒ½ç›¸ä¼¼çš„è„‘ä½“ç´ ç°‡ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„äº¤äº’ã€‚è¿™äº›åŠŸèƒ½é›†ç¾¤å¯¹æ‰€æœ‰å—è¯•è€…éƒ½æ˜¯é€šç”¨çš„ï¼Œå¯ä½œä¸ºæ•´åˆè„‘å†…å’Œè·¨è„‘ä¿¡æ¯çš„æ„å»ºæ¨¡å—ã€‚æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½è¢«æ‰€æœ‰é›†ç¾¤å’Œå—è¯•è€…å…±äº«ï¼Œä½¿å¾—åœ¨æœ‰é™çš„æ•°æ®é‡ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚ä¸ºäº†æŒ‡å¯¼å›¾åƒé‡å»ºï¼ŒBITé¢„æµ‹äº†ä¸¤ä¸ªäº’è¡¥çš„å±€éƒ¨æ–‘å—çº§å›¾åƒç‰¹å¾ï¼šä¸€æ˜¯é«˜å±‚æ¬¡çš„è¯­ä¹‰ç‰¹å¾ï¼Œå®ƒå¼•å¯¼æ‰©æ•£æ¨¡å‹èµ°å‘å›¾åƒçš„æ­£ç¡®è¯­ä¹‰å†…å®¹ï¼›äºŒæ˜¯ä½å±‚æ¬¡çš„ç»“æ„ç‰¹å¾ï¼Œå®ƒæœ‰åŠ©äºåˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹ï¼Œå½¢æˆå›¾åƒçš„æ­£ç¡®ç²—ç•¥å¸ƒå±€ã€‚Brain-ITçš„è®¾è®¡å®ç°äº†ä»è„‘ä½“ç´ ç°‡åˆ°å±€éƒ¨å›¾åƒç‰¹å¾çš„ç›´æ¥ä¿¡æ¯æµã€‚é€šè¿‡è¿™ä¸€åŸç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé‡å»ºå¿ å®äºæ‰€çœ‹åˆ°çš„å›¾åƒçš„fMRIå›¾åƒï¼Œæ— è®ºåœ¨è§†è§‰ä¸Šè¿˜æ˜¯é€šè¿‡æ ‡å‡†å®¢è§‚æŒ‡æ ‡ï¼Œéƒ½è¶…è¶Šäº†å½“å‰çš„æœ€ä¼˜æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä»…ä½¿ç”¨æ–°å—è¯•è€…1å°æ—¶çš„fMRIæ•°æ®ï¼Œæˆ‘ä»¬å°±èƒ½è¾¾åˆ°ä¸å½“å‰æ–¹æ³•ç›¸å½“çš„ç»“æœï¼Œè€Œå½“å‰æ–¹æ³•éœ€è¦åœ¨å®Œæ•´çš„40å°æ—¶è®°å½•ä¸Šè¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡fMRIè®°å½•é‡å»ºå›¾åƒæä¾›äº†éä¾µå…¥å¼ç ”ç©¶å¤§è„‘çš„æ–¹å¼ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨é‡å»ºçœŸå®å›¾åƒæ—¶å­˜åœ¨ä¸å¿ å®çš„é—®é¢˜ã€‚</li>
<li>Brain-ITé€šè¿‡Brain Interaction Transformerï¼ˆBITï¼‰è§£å†³æ­¤æŒ‘æˆ˜ï¼Œå…è®¸åŠŸèƒ½ç›¸ä¼¼çš„è„‘ä½“ç´ ç°‡é—´æœ‰æ•ˆäº¤äº’ã€‚</li>
<li>åŠŸèƒ½é›†ç¾¤å¯¹æ‰€æœ‰å—è¯•è€…é€šç”¨ï¼Œä½œä¸ºæ•´åˆä¿¡æ¯çš„æ„å»ºæ¨¡å—ã€‚</li>
<li>BITé¢„æµ‹é«˜å±‚æ¬¡çš„è¯­ä¹‰ç‰¹å¾å’Œä½å±‚æ¬¡çš„ç»“æ„ç‰¹å¾æ¥æŒ‡å¯¼å›¾åƒé‡å»ºã€‚</li>
<li>Brain-ITè®¾è®¡å®ç°ç›´æ¥ä»è„‘ä½“ç´ ç°‡åˆ°å±€éƒ¨å›¾åƒç‰¹å¾çš„ä¿¡æ¯æµã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13b7cc12ebde04eb485527a4886bfd03~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304930&auth_key=1762304930-0-0-342b3b4dec13b6eea60fbe726ba65bd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-909f2b9ee503254bbd129faed8ecfd62~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304937&auth_key=1762304937-0-0-845f1d8c108897adc0b782ed97076415&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1bf4b28e30475423d031a2d5c8f509a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304944&auth_key=1762304944-0-0-5880fd737e42ec850adff60c122f4327&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aab469fe8b22b3fbd904d4ea42e2efe1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304951&auth_key=1762304951-0-0-3bf6766f9bbfb08dc171fe59ba09ee00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-170d28db83d2268228853cb51607e4b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304958&auth_key=1762304958-0-0-519eb8f26fd425a2369e03c154f443b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Physics-Guided-Conditional-Diffusion-Networks-for-Microwave-Image-Reconstruction"><a href="#Physics-Guided-Conditional-Diffusion-Networks-for-Microwave-Image-Reconstruction" class="headerlink" title="Physics-Guided Conditional Diffusion Networks for Microwave Image   Reconstruction"></a>Physics-Guided Conditional Diffusion Networks for Microwave Image   Reconstruction</h2><p><strong>Authors:Shirin Chehelgami, Joe LoVetri, Vahab Khoshdel</strong></p>
<p>A conditional latent-diffusion based framework for solving the electromagnetic inverse scattering problem associated with microwave imaging is introduced. This generative machine-learning model explicitly mirrors the non-uniqueness of the ill-posed inverse problem. Unlike existing inverse solvers utilizing deterministic machine learning techniques that produce a single reconstruction, the proposed latent-diffusion model generates multiple plausible permittivity maps conditioned on measured scattered-field data, thereby generating several potential instances in the range-space of the non-unique inverse mapping. A forward electromagnetic solver is integrated into the reconstruction pipeline as a physics-based evaluation mechanism. The space of candidate reconstructions form a distribution of possibilities consistent with the conditioning data and the member of this space yielding the lowest scattered-field data discrepancy between the predicted and measured scattered fields is reported as the final solution. Synthetic and experimental labeled datasets are used for training and evaluation of the model. An innovative labeled synthetic dataset is created that exemplifies a varied set of scattering features. Training of the model using this new dataset produces high quality permittivity reconstructions achieving improved generalization with excellent fidelity to shape recognition. The results highlight the potential of hybrid generative physics frameworks as a promising direction for robust, data-driven microwave imaging. </p>
<blockquote>
<p>ä»‹ç»äº†ä¸€ä¸ªåŸºäºæ¡ä»¶æ½œæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸å¾®æ³¢æˆåƒç›¸å…³çš„ç”µç£é€†æ•£å°„é—®é¢˜ã€‚è¿™ä¸€ç”Ÿæˆæœºå™¨å­¦ä¹ æ¨¡å‹æ˜ç¡®åæ˜ äº†ä¸é€‚å®šé€†é—®é¢˜çš„éå”¯ä¸€æ€§ã€‚ä¸åŒäºç°æœ‰çš„åˆ©ç”¨ç¡®å®šæ€§æœºå™¨å­¦ä¹ æŠ€æœ¯äº§ç”Ÿå•ä¸€é‡å»ºçš„é€†æ±‚è§£å™¨ï¼Œæ‰€æå‡ºæ½œæ‰©æ•£æ¨¡å‹æ ¹æ®æµ‹é‡æ•£å°„åœºæ•°æ®ç”Ÿæˆå¤šä¸ªåˆç†çš„ä»‹ç”µå¸¸æ•°å›¾ï¼Œä»è€Œç”Ÿæˆéå”¯ä¸€é€†æ˜ å°„èŒƒå›´ç©ºé—´ä¸­çš„å¤šä¸ªæ½œåœ¨å®ä¾‹ã€‚å°†æ­£å‘ç”µç£æ±‚è§£å™¨é›†æˆåˆ°é‡å»ºæµç¨‹ä¸­ä½œä¸ºåŸºäºç‰©ç†çš„è¯„ä¼°æœºåˆ¶ã€‚å€™é€‰é‡å»ºçš„ç©ºé—´å½¢æˆä¸æ¡ä»¶æ•°æ®ä¸€è‡´çš„å¯èƒ½æ€§åˆ†å¸ƒï¼Œå¹¶æŠ¥å‘Šäº†ä½¿é¢„æµ‹å’Œæµ‹é‡æ•£å°„åœºä¹‹é—´çš„æ•£å°„åœºæ•°æ®å·®å¼‚æœ€å°çš„ç©ºé—´æˆå‘˜ä¸ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆã€‚åˆæˆå’Œå®éªŒæ ‡è®°æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚åˆ›å»ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ ‡è®°åˆæˆæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å±•ç¤ºäº†å„ç§æ•£å°„ç‰¹å¾ã€‚ä½¿ç”¨è¯¥æ–°æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œäº§ç”Ÿäº†é«˜è´¨é‡çš„ä»‹ç”µå¸¸æ•°é‡å»ºï¼Œå®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å¹¶å¯¹å½¢çŠ¶è¯†åˆ«å…·æœ‰é«˜åº¦çš„ä¿çœŸåº¦ã€‚ç»“æœçªå‡ºäº†æ··åˆç”Ÿæˆç‰©ç†æ¡†æ¶ä½œä¸ºç¨³å¥çš„æ•°æ®é©±åŠ¨å¾®æ³¢æˆåƒçš„æœ‰å‰é€”æ–¹å‘æ‰€å…·å¤‡çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25729v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¡ä»¶æ½œåœ¨æ‰©æ•£çš„è§£å†³å¾®æ³¢æˆåƒä¸­çš„ç”µç£é€†æ•£å°„é—®é¢˜çš„æ¡†æ¶ã€‚è¯¥ç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹æ˜ç¡®åæ˜ äº†ä¸é€‚å®šé€†é—®é¢˜çš„éå”¯ä¸€æ€§ã€‚ä¸ä¼ ç»Ÿçš„åˆ©ç”¨ç¡®å®šæ€§æœºå™¨å­¦ä¹ æŠ€æœ¯æ±‚è§£é€†é—®é¢˜çš„é‡å»ºæ–¹æ³•ä¸åŒï¼Œæ‰€æå‡ºçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åŸºäºæµ‹é‡çš„æ•£å°„åœºæ•°æ®ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„ä»‹ç”µå¸¸æ•°å›¾ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªæ½œåœ¨çš„æ˜ å°„èŒƒå›´ç©ºé—´çš„å®ä¾‹ã€‚ç»“åˆæ­£å‘ç”µç£æ±‚è§£å™¨ä½œä¸ºåŸºäºç‰©ç†çš„è¯„ä¼°æœºåˆ¶ï¼Œå°†å€™é€‰é‡å»ºç‰©çš„ç©ºé—´å½¢æˆä¸æ¡ä»¶æ•°æ®ä¸€è‡´çš„å¯èƒ½æ€§åˆ†å¸ƒï¼Œå¹¶æŠ¥å‘Šåœ¨é¢„æµ‹å’Œæµ‹é‡æ•£å°„åœºä¹‹é—´å…·æœ‰æœ€ä½æ•£å°„åœºæ•°æ®å·®å¼‚çš„ç©ºé—´æˆå‘˜ä½œä¸ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆã€‚ä½¿ç”¨åˆæˆå’Œå®éªŒæ ‡è®°æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ä½¿ç”¨åˆ›æ–°æ ‡è®°çš„åˆæˆæ•°æ®é›†åˆ›å»ºäº†ä¸€ä¸ªå±•ç¤ºå„ç§æ•£å°„ç‰¹å¾çš„æ ·æœ¬é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒæ¨¡å‹äº§ç”Ÿäº†é«˜è´¨é‡çš„ä»‹ç”µå¸¸æ•°é‡å»ºï¼Œå®ç°äº†è‰¯å¥½çš„å½¢çŠ¶è¯†åˆ«æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœçªå‡ºäº†æ··åˆç”Ÿæˆç‰©ç†æ¡†æ¶åœ¨ç¨³å¥çš„æ•°æ®é©±åŠ¨å¾®æ³¢æˆåƒä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºæ¡ä»¶æ½œåœ¨æ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸å¾®æ³¢æˆåƒç›¸å…³çš„ç”µç£é€†æ•£å°„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¯ä¸€ä¸ªç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†é€†é—®é¢˜çš„éå”¯ä¸€æ€§ã€‚</li>
<li>ä¸å…¶ä»–ç¡®å®šæ€§æœºå™¨å­¦ä¹ é€†æ±‚è§£å™¨ä¸åŒï¼Œè¯¥æ¨¡å‹èƒ½ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„ä»‹ç”µå¸¸æ•°å›¾ã€‚</li>
<li>é›†æˆæ­£å‘ç”µç£æ±‚è§£å™¨ä½œä¸ºåŸºäºç‰©ç†çš„è¯„ä¼°æœºåˆ¶ã€‚</li>
<li>å€™é€‰é‡å»ºç‰©çš„ç©ºé—´å½¢æˆä¸€ä¸ªä¸æ¡ä»¶æ•°æ®ä¸€è‡´çš„å¯èƒ½æ€§åˆ†å¸ƒã€‚</li>
<li>ä½¿ç”¨åˆæˆå’Œå®éªŒæ ‡è®°æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5de655a4b49b6afc53d22203eb682d51~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304966&auth_key=1762304966-0-0-f405deb6ce417cf6534991116865823b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f68d0833ca3a3275e8166024e06297a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304973&auth_key=1762304973-0-0-370b17789ce7994ac9da30c9457a5369&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-607cb771bc4bebb65d052acc73f1f454~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304980&auth_key=1762304980-0-0-0dd74e1db10cf9904f8452df684057c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e8bad648032041fcf5effb599dff979~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304987&auth_key=1762304987-0-0-5943f51a484ecff2c6c241758335409e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography"><a href="#Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography" class="headerlink" title="Comparative Study of UNet-based Architectures for Liver Tumor   Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography"></a>Comparative Study of UNet-based Architectures for Liver Tumor   Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography</h2><p><strong>Authors:Doan-Van-Anh Ly, Thi-Thu-Hien Pham, Thanh-Hai Le</strong></p>
<p>Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The modelâ€™s superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the regionâ€™s most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice. </p>
<blockquote>
<p>åœ¨å¤šç§é˜¶æ®µçš„å¢å¼ºå‹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰ä¸­ï¼Œè‚è„ç»“æ„çš„åˆ†å‰²å¯¹è‚è„ç–¾ç—…çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ï¼ˆåŒ…æ‹¬è‚¿ç˜¤æ£€æµ‹ï¼‰èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹çš„UNetæ‰©å±•åˆ°å…·æœ‰å„ç§éª¨å¹²ç½‘ç»œçš„UNet3+ã€‚æˆ‘ä»¬è¯„ä¼°äº†ResNetã€åŸºäºTransformerå’ŒState-spaceï¼ˆMambaï¼‰çš„éª¨å¹²ç½‘ç»œï¼Œæ‰€æœ‰ç½‘ç»œéƒ½ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡ç°ä»£æ¶æ„æœ‰æ‰€è¿›æ­¥ï¼ŒåŸºäºResNetçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºè¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ï¼Œæˆ‘ä»¬åœ¨éª¨å¹²ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶è§‚å¯Ÿåˆ°åŠ å…¥å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰ä¼šå–å¾—æœ€ä½³æ€§èƒ½ã€‚å¸¦æœ‰CBAMæ¨¡å—çš„ResNetUNet3+ä¸ä»…ä»¥Diceç³»æ•°0.755å’ŒIoU 0.662çš„æœ€ä½³é‡å åº¦æŒ‡æ ‡äº§å‡ºç»“æœï¼Œè€Œä¸”å®ç°äº†æœ€ç²¾ç¡®çš„è¾¹ç•Œæç»˜ï¼Œä»¥æœ€ä½çš„HD95è·ç¦»77.911ä¸ºè¯æ®ã€‚è¯¥æ¨¡å‹çš„ä¼˜è¶Šæ€§è¿˜ä½“ç°åœ¨å…¶æ€»ä½“å‡†ç¡®ç‡0.925å’Œç‰¹å¼‚æ€§0.926ä¸Šï¼Œå±•ç¤ºå‡ºå…¶å‡†ç¡®è¯†åˆ«ç—…å˜ç»„ç»‡å’Œå¥åº·ç»„ç»‡çš„ç¨³å¥èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯è§£é‡Šæ€§ï¼Œé‡‡ç”¨äº†Grad-CAMå¯è§†åŒ–æ¥çªå‡ºæ˜¾ç¤ºå¯¹é¢„æµ‹æœ€å…·æœ‰å½±å“åŠ›çš„åŒºåŸŸï¼Œä»è€Œæ·±å…¥äº†è§£å…¶å†³ç­–è¿‡ç¨‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»å…¸çš„ResNetæ¶æ„ä¸ç°ä»£æ³¨æ„åŠ›æ¨¡å—ç›¸ç»“åˆæ—¶ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä»å…·æœ‰é«˜åº¦çš„ç«äº‰åŠ›ï¼Œä¸ºä¸´åºŠå®è·µä¸­çš„è‚è„è‚¿ç˜¤æ£€æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25522v1">PDF</a> 27 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹UNetåˆ°UNet3+ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§backboneç½‘ç»œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç°ä»£æ¶æ„æœ‰æ‰€è¿›æ­¥ï¼Œä½†åŸºäºResNetçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ¨¡å‹ã€‚å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶åï¼Œç‰¹åˆ«æ˜¯åŠ å…¥å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰çš„ResNetUNet3+è¡¨ç°æœ€ä½³ï¼Œä¸ä»…é‡å åº¦é‡æŒ‡æ ‡ï¼ˆDiceåˆ†æ•°ä¸º0.755ï¼ŒIoUä¸º0.662ï¼‰æœ€ä½³ï¼Œè€Œä¸”è¾¹ç•Œåˆ’å®šæœ€ç²¾ç¡®ï¼ˆHD95è·ç¦»ä¸º77.911ï¼‰ï¼Œæ€»ä½“å‡†ç¡®åº¦ï¼ˆ0.925ï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆ0.926ï¼‰é¢†å…ˆã€‚Grad-CAMå¯è§†åŒ–æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œçªå‡ºäº†é¢„æµ‹ä¸­æœ€å…·å½±å“åŠ›çš„åŒºåŸŸï¼Œä¸ºå†³ç­–è¿‡ç¨‹æä¾›äº†è§è§£ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»å…¸ResNetæ¶æ„ä¸ç°ä»£æ³¨æ„åŠ›æ¨¡å—çš„ç»“åˆåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä»å…·æœ‰ç«äº‰åŠ›ï¼Œä¸ºä¸´åºŠå®è·µä¸­è‚è„è‚¿ç˜¤æ£€æµ‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>UNetæ¶æ„åœ¨è‚è„è‚¿ç˜¤åˆ†å‰²ä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>åœ¨å¤šç§backboneç½‘ç»œä¸­ï¼ŒResNetæ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æé«˜äº†åˆ†å‰²è´¨é‡ï¼Œå°¤å…¶æ˜¯CBAMæ¨¡å—çš„åº”ç”¨ã€‚</li>
<li>ResNetUNet3+ä¸CBAMçš„ç»“åˆåœ¨é‡å åº¦é‡ã€è¾¹ç•Œåˆ’å®šã€æ€»ä½“å‡†ç¡®åº¦å’Œç‰¹å¼‚æ€§æ–¹é¢å‡è¡¨ç°æœ€ä½³ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–æŠ€æœ¯å¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>ç»å…¸ResNetæ¶æ„ä¸ç°ä»£æ¨¡å—çš„èåˆåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä»å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-61181777eace2dd23fc7039458f6199e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304994&auth_key=1762304994-0-0-4e0c899f7553a8a58ebc1093c63a6810&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2add1e86abf51994ef6852dfabfeefdc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305002&auth_key=1762305002-0-0-6db9e0ba478a1898aaef2cc4209c57c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Aligning-What-You-Separate-Denoised-Patch-Mixing-for-Source-Free-Domain-Adaptation-in-Medical-Image-Segmentation"><a href="#Aligning-What-You-Separate-Denoised-Patch-Mixing-for-Source-Free-Domain-Adaptation-in-Medical-Image-Segmentation" class="headerlink" title="Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain   Adaptation in Medical Image Segmentation"></a>Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain   Adaptation in Medical Image Segmentation</h2><p><strong>Authors:Quang-Khai Bui-Tran, Thanh-Huy Nguyen, Hoang-Thien Nguyen, Ba-Thinh Lam, Nguyen Lan Vi Vu, Phat K. Huynh, Ulas Bagci, Min Xu</strong></p>
<p>Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for medical image segmentation under privacy constraints, yet current approaches often ignore sample difficulty and struggle with noisy supervision under domain shift. We present a new SFDA framework that leverages Hard Sample Selection and Denoised Patch Mixing to progressively align target distributions. First, unlabeled images are partitioned into reliable and unreliable subsets through entropy-similarity analysis, allowing adaptation to start from easy samples and gradually incorporate harder ones. Next, pseudo-labels are refined via Monte Carlo-based denoising masks, which suppress unreliable pixels and stabilize training. Finally, intra- and inter-domain objectives mix patches between subsets, transferring reliable semantics while mitigating noise. Experiments on benchmark datasets show consistent gains over prior SFDA and UDA methods, delivering more accurate boundary delineation and achieving state-of-the-art Dice and ASSD scores. Our study highlights the importance of progressive adaptation and denoised supervision for robust segmentation under domain shift. </p>
<blockquote>
<p>æºåŸŸè‡ªç”±é€‚åº”ï¼ˆSFDAï¼‰ä½œä¸ºéšç§çº¦æŸä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸€ç§å¸å¼•äººçš„è§£å†³æ–¹æ¡ˆæ­£å´­éœ²å¤´è§’ï¼Œç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ ·æœ¬çš„éš¾åº¦ï¼Œå¹¶åœ¨åŸŸè½¬ç§»æ—¶é¢ä¸´å™ªå£°ç›‘ç£çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SFDAæ¡†æ¶ï¼Œåˆ©ç”¨ç¡¬æ ·æœ¬é€‰æ‹©å’Œå»å™ªè¡¥ä¸æ··åˆæ¥é€æ­¥å¯¹é½ç›®æ ‡åˆ†å¸ƒã€‚é¦–å…ˆï¼Œé€šè¿‡ç†µç›¸ä¼¼æ€§åˆ†æå°†æ— æ ‡ç­¾å›¾åƒåˆ’åˆ†ä¸ºå¯é å’Œä¸å¯é çš„å­é›†ï¼Œä½¿é€‚åº”ä»ç®€å•çš„æ ·æœ¬å¼€å§‹ï¼Œå¹¶é€æ­¥ç»“åˆæ›´å›°éš¾çš„æ ·æœ¬ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡åŸºäºè’™ç‰¹å¡æ´›çš„å»å™ªæ©è†œå¯¹ä¼ªæ ‡ç­¾è¿›è¡Œç²¾ç‚¼ï¼Œè¿™å¯ä»¥æŠ‘åˆ¶ä¸å¯é çš„åƒç´ å¹¶ç¨³å®šè®­ç»ƒã€‚æœ€åï¼ŒåŸŸå†…å’ŒåŸŸé—´çš„ç›®æ ‡åœ¨å­é›†ä¹‹é—´æ··åˆè¡¥ä¸ï¼Œåœ¨ä¼ é€’å¯é è¯­ä¹‰çš„åŒæ—¶å‡è½»å™ªå£°ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å…ˆå‰çš„SFDAå’ŒUDAæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¸€è‡´çš„å¢ç›Šï¼Œå®ç°äº†æ›´å‡†ç¡®çš„è¾¹ç•Œæç»˜ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„Diceå’ŒASSDåˆ†æ•°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é€æ­¥é€‚åº”å’Œå»å™ªç›‘ç£åœ¨åŸŸè½¬ç§»ä¸‹è¿›è¡Œç¨³å¥åˆ†å‰²çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25227v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰æ–¹æ³•åœ¨å¤„ç†éšç§çº¦æŸä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥æ ·æœ¬éš¾åº¦å’Œè·¨åŸŸå™ªå£°ç›‘ç£çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SFDAæ¡†æ¶ï¼Œç»“åˆç¡¬æ ·æœ¬é€‰æ‹©å’Œå»å™ªè¡¥ä¸æ··åˆï¼Œé€æ­¥å¯¹é½ç›®æ ‡åˆ†å¸ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ï¿½ï¿½actåˆ†æå°†æœªæ ‡è®°å›¾åƒåˆ†ä¸ºå¯é å’Œä¸å¯é å­é›†ï¼Œä»å®¹æ˜“æ ·æœ¬å¼€å§‹é€‚åº”å¹¶é€æ¸çº³å…¥å›°éš¾æ ·æœ¬ã€‚é€šè¿‡åŸºäºè’™ç‰¹å¡æ´›çš„å»å™ªæ©è†œä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼ŒæŠ‘åˆ¶ä¸å¯é åƒç´ å¹¶ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åŸŸå†…å’ŒåŸŸé—´çš„ç›®æ ‡æ–‘å—æ··åˆåœ¨ä¸€èµ·ï¼Œåœ¨è½¬ç§»å¯é è¯­ä¹‰çš„åŒæ—¶å‡è½»å™ªå£°å½±å“ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…ˆå‰SFDAå’ŒUDAæ–¹æ³•çš„åŸºç¡€ä¸Šå®ç°äº†ä¸€è‡´çš„å¢ç›Šï¼Œå–å¾—äº†æ›´ç²¾ç¡®çš„è¾¹ç•Œè½®å»“ï¼Œå¹¶è·å¾—äº†é¢†å…ˆçš„Diceå’ŒASSDå¾—åˆ†ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æ¸è¿›é€‚åº”å’Œå»å™ªç›‘ç£åœ¨è·¨åŸŸç¨³å¥åˆ†å‰²ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰åœ¨å¤„ç†éšç§çº¦æŸä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å½“å‰SFDAæ–¹æ³•å¿½ç•¥æ ·æœ¬éš¾åº¦å’Œè·¨åŸŸå™ªå£°ç›‘ç£ã€‚</li>
<li>æå‡ºæ–°çš„SFDAæ¡†æ¶ç»“åˆç¡¬æ ·æœ¬é€‰æ‹©å’Œå»å™ªè¡¥ä¸æ··åˆã€‚</li>
<li>é€šè¿‡ç†µåˆ†æå°†æœªæ ‡è®°å›¾åƒåˆ†ä¸ºå¯é å’Œä¸å¯é å­é›†ï¼Œå®ç°æ¸è¿›é€‚åº”ã€‚</li>
<li>ä½¿ç”¨åŸºäºè’™ç‰¹å¡æ´›çš„å»å™ªæ©è†œä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”çš„ä¸€è‡´å¢ç›Šï¼Œå¹¶è·å¾—äº†æ›´é«˜çš„Diceå’ŒASSDå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b0a16e31de4518a61c49aa65f175a2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305009&auth_key=1762305009-0-0-b2013de090583c4380c3e842bd640ed2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b535a70faaaec849246b9bf430162ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305017&auth_key=1762305017-0-0-52c55ece02c43c3df4f33ddaab85bffa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-919fd9ac5c0c759f0768a1da6e361b31~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305024&auth_key=1762305024-0-0-49d2eff871a15f468d30baadd5a22380&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformers-in-Medicine-Improving-Vision-Language-Alignment-for-Medical-Image-Captioning"><a href="#Transformers-in-Medicine-Improving-Vision-Language-Alignment-for-Medical-Image-Captioning" class="headerlink" title="Transformers in Medicine: Improving Vision-Language Alignment for   Medical Image Captioning"></a>Transformers in Medicine: Improving Vision-Language Alignment for   Medical Image Captioning</h2><p><strong>Authors:Yogesh Thakku Suresh, Vishwajeet Shivaji Hogale, Luca-Alexandru Zamfira, Anandavardhana Hegde</strong></p>
<p>We present a transformer-based multimodal framework for generating clinically relevant captions for MRI scans. Our system combines a DEiT-Small vision transformer as an image encoder, MediCareBERT for caption embedding, and a custom LSTM-based decoder. The architecture is designed to semantically align image and textual embeddings, using hybrid cosine-MSE loss and contrastive inference via vector similarity. We benchmark our method on the MultiCaRe dataset, comparing performance on filtered brain-only MRIs versus general MRI images against state-of-the-art medical image captioning methods including BLIP, R2GenGPT, and recent transformer-based approaches. Results show that focusing on domain-specific data improves caption accuracy and semantic alignment. Our work proposes a scalable, interpretable solution for automated medical image reporting. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºä¸ºMRIæ‰«æç”Ÿæˆä¸´åºŠç›¸å…³çš„æè¿°ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç»“åˆäº†DEiT-Smallè§†è§‰transformerä½œä¸ºå›¾åƒç¼–ç å™¨ã€MediCareBERTç”¨äºæè¿°åµŒå…¥ï¼Œä»¥åŠåŸºäºLSTMçš„è‡ªå®šä¹‰è§£ç å™¨ã€‚è¯¥æ¶æ„æ—¨åœ¨ä½¿ç”¨æ··åˆä½™å¼¦-MSEæŸå¤±å’Œé€šè¿‡å‘é‡ç›¸ä¼¼æ€§çš„å¯¹æ¯”æ¨æ–­ï¼Œè¯­ä¹‰å¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ã€‚æˆ‘ä»¬åœ¨MultiCaReæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¯”è¾ƒäº†è¿‡æ»¤åçš„ä»…å¤§è„‘MRIä¸é€šç”¨MRIå›¾åƒçš„ç»©æ•ˆï¼Œå¹¶ä¸æœ€å‰æ²¿çš„åŒ»ç–—å›¾åƒæè¿°æ–¹æ³•ï¼ŒåŒ…æ‹¬BLIPã€R2GenGPTå’Œæœ€è¿‘çš„åŸºäºtransformerçš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œä¸“æ³¨äºé¢†åŸŸç‰¹å®šæ•°æ®æé«˜äº†æè¿°çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§å¯æ‰©å±•ã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨åŒ–åŒ»ç–—å›¾åƒæŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25164v2">PDF</a> This work is to appear in the Proceedings of MICAD 2025, the 6th   International Conference on Medical Imaging and Computer-Aided Diagnosis</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæè¿°ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨transformeræ¨¡å‹ç»“åˆå›¾åƒç¼–ç å™¨ã€æ–‡æœ¬åµŒå…¥å™¨å’Œè§£ç å™¨ï¼Œå®ç°è¯­ä¹‰å¯¹é½ã€‚åœ¨MultiCaReæ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½ä¼˜äºå…¶ä»–åŒ»ç–—å›¾åƒæè¿°ç”Ÿæˆæ–¹æ³•ã€‚ä¸“æ³¨äºé¢†åŸŸç‰¹å®šæ•°æ®ï¼Œæé«˜æè¿°å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½æ€§ï¼Œæå‡ºå¯è§£é‡Šçš„è‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæŠ¥å‘Šæ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†åŸºäºtransformerçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸MRIæ‰«æç›¸å…³çš„ä¸´åºŠæè¿°ã€‚</li>
<li>ç³»ç»ŸåŒ…å«å›¾åƒç¼–ç å™¨ï¼ˆä½¿ç”¨DEiT-Smallè§†è§‰è½¬æ¢å™¨ï¼‰ã€æ–‡æœ¬åµŒå…¥å™¨ï¼ˆä½¿ç”¨MediCareBERTï¼‰å’ŒåŸºäºLSTMçš„è§£ç å™¨ã€‚</li>
<li>ç³»ç»Ÿè®¾è®¡ç”¨äºè¯­ä¹‰å¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ï¼Œé‡‡ç”¨æ··åˆçš„cosine-MSEæŸå¤±å’ŒåŸºäºå‘é‡ç›¸ä¼¼æ€§çš„å¯¹æ¯”æ¨æ–­ã€‚</li>
<li>åœ¨MultiCaReæ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œå¯¹æ¯”äº†è¿‡æ»¤åçš„è„‘éƒ¨MRIä¸ä¸€èˆ¬MRIå›¾åƒä¸Šçš„è¡¨ç°ï¼Œä»¥åŠä¸å…¶ä»–å…ˆè¿›çš„åŒ»ç–—å›¾åƒæè¿°ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“æ³¨äºé¢†åŸŸç‰¹å®šæ•°æ®èƒ½æé«˜æè¿°å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½æ€§ã€‚</li>
<li>è¯¥æ–¹æ¡ˆä¸ºè‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæŠ¥å‘Šæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b87e1457f1777c3c8f7ee2e1d8b7cf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305031&auth_key=1762305031-0-0-4c196fdbe5f72f196568108b10419682&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75e564d2eafca948c91e3b401679bdac~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305059&auth_key=1762305059-0-0-3e9cb7ff79f8bcf5a8924add38ab680b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b011ea5eaf8607186e0d759d2b3158a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762305066&auth_key=1762305066-0-0-4db8b8e502e8d206a02523e975dc83d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-91f52eeab94e5400651a411296db98a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306748&auth_key=1762306748-0-0-c7334f6ff1cb11b7e0091fb88d442d94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV   Networks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-09f9281634df8b2feca498a1dcc8bd99~resize:0:q75.jpg?source=1f5c5e47&expiration=1762469446&auth_key=1762469446-0-0-6e69de8f4dda490504eb12797913e768&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Who Made This? Fake Detection and Source Attribution with Diffusion   Features
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
