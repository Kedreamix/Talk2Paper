<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  EgoExo-Con Exploring View-Invariant Video Temporal Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-79e6e73f8c70116953a199dc9cd38815')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="EgoExo-Con-Exploring-View-Invariant-Video-Temporal-Understanding"><a href="#EgoExo-Con-Exploring-View-Invariant-Video-Temporal-Understanding" class="headerlink" title="EgoExo-Con: Exploring View-Invariant Video Temporal Understanding"></a>EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</h2><p><strong>Authors:Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao</strong></p>
<p>Can Video-LLMs achieve consistent temporal understanding when videos capture the same event from different viewpoints? To study this, we introduce EgoExo-Con (Consistency), a benchmark of comprehensively synchronized egocentric and exocentric video pairs with human-refined queries in natural language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal Verification and Temporal Grounding. It evaluates not only correctness but consistency across viewpoints. Our analysis reveals two critical limitations of existing Video-LLMs: (1) models often fail to maintain consistency, with results far worse than their single-view performances. (2) When naively finetuned with synchronized videos of both viewpoints, the models show improved consistency but often underperform those trained on a single view. For improvements, we propose View-GRPO, a novel reinforcement learning framework that effectively strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints. Our method demonstrates its superiority over naive SFT and GRPO, especially for improving cross-view consistency. All resources will be made publicly available. </p>
<blockquote>
<p>å½“è§†é¢‘ä»ä¸åŒè§†è§’æ•æ‰åŒä¸€äº‹ä»¶æ—¶ï¼Œè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰èƒ½å¦å®ç°ä¸€è‡´çš„æ—¶é—´ç†è§£ï¼Ÿä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoExo-Conï¼ˆä¸€è‡´æ€§ï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å…¨é¢åŒæ­¥çš„ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†é¢‘å¯¹ä»¥åŠäººç±»ç”¨è‡ªç„¶è¯­è¨€ç²¾ç»†æŸ¥è¯¢çš„åŸºå‡†æµ‹è¯•ã€‚EgoExo-Conå¼ºè°ƒä¸¤ç§æ—¶é—´ç†è§£ä»»åŠ¡ï¼šæ—¶é—´éªŒè¯å’Œæ—¶é—´åŸºç¡€ã€‚å®ƒä¸ä»…è¯„ä¼°æ­£ç¡®æ€§ï¼Œè¿˜è¯„ä¼°ä¸åŒè§†è§’ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ç°æœ‰Video-LLMsçš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ¨¡å‹å¾€å¾€ä¸èƒ½ä¿æŒä¸€è‡´æ€§ï¼Œå…¶å¤šè§†è§’çš„ç»“æœæ¯”å•ä¸€è§†è§’çš„ç»“æœè¦å·®å¾—å¤šã€‚ï¼ˆ2ï¼‰å½“ç”¨ä¸¤ä¸ªè§†è§’çš„åŒæ­¥è§†é¢‘è¿›è¡Œç®€å•å¾®è°ƒæ—¶ï¼Œæ¨¡å‹çš„ä¸€è‡´æ€§æœ‰æ‰€æé«˜ï¼Œä½†åœ¨å•ä¸€è§†è§’ä¸Šçš„è®­ç»ƒå¾€å¾€è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†View-GRPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°åŠ å¼ºç‰¹å®šè§†è§’çš„æ—¶é—´æ¨ç†ï¼ŒåŒæ—¶é¼“åŠ±ä¸åŒè§†è§’ä¹‹é—´çš„ä¸€è‡´ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å¼€æµ‹è¯•é›†ä¸Šä¼˜äºç®€å•çš„å¾®è°ƒæ–¹æ³•å’ŒGRPOæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚æ‰€æœ‰èµ„æºéƒ½å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26113v1">PDF</a> project page:   \url{<a target="_blank" rel="noopener" href="https://minjoong507.github.io/projects/EgoExo-Con/%7D">https://minjoong507.github.io/projects/EgoExo-Con/}</a></p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç†è§£é¢†åŸŸçš„ä¸€ä¸ªç ”ç©¶å¼•å…¥äº†EgoExo-Conï¼ˆä¸€è‡´æ€§ï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«å…¨é¢åŒæ­¥çš„ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†é¢‘å¯¹ï¼Œä»¥åŠäººç±»ä»¥è‡ªç„¶è¯­è¨€ç²¾ä¿®çš„æŸ¥è¯¢ã€‚EgoExo-Conå¼ºè°ƒä¸¤ç§æ—¶é—´ç†è§£ä»»åŠ¡ï¼šæ—¶é—´éªŒè¯å’Œæ—¶é—´å®šä½ã€‚å®ƒä¸ä»…è¯„ä¼°æ­£ç¡®æ€§ï¼Œè¿˜è¯„ä¼°ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚åˆ†ææ˜¾ç¤ºç°æœ‰è§†é¢‘ç†è§£å¤§æ¨¡å‹å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯éš¾ä»¥ä¿æŒä¸€è‡´æ€§ï¼Œå•è§†è§’è¡¨ç°è¾ƒå¥½ä½†åœ¨å¤šè§†è§’ä¸‹ç»“æœè¾ƒå·®ï¼›äºŒæ˜¯ç®€å•å¾®è°ƒæ— æ³•æ”¹å–„è·¨è§†è§’ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†View-GRPOè¿™ä¸€æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆå¼ºåŒ–ç‰¹å®šè§†è§’çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶é¼“åŠ±è·¨è§†è§’çš„ä¸€è‡´æ€§ç†è§£ã€‚æ­¤æ¡†æ¶ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥EgoExo-ConåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«åŒæ­¥çš„ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§†é¢‘å¯¹ï¼Œç”¨äºç ”ç©¶è§†é¢‘ç†è§£æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¼ºè°ƒæ—¶é—´éªŒè¯å’Œæ—¶é—´å®šä½ä¸¤ç§æ—¶é—´ç†è§£ä»»åŠ¡ï¼ŒåŒæ—¶è¯„ä¼°æ¨¡å‹çš„æ­£ç¡®æ€§å’Œä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>åˆ†æå‘ç°ç°æœ‰è§†é¢‘ç†è§£å¤§æ¨¡å‹åœ¨ç»´æŒè·¨è§†è§’ä¸€è‡´æ€§ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>ç®€å•å¾®è°ƒæ¨¡å‹å¹¶ä¸èƒ½æ”¹å–„è·¨è§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†View-GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆå¼ºåŒ–ç‰¹å®šè§†è§’çš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é¼“åŠ±è·¨è§†è§’çš„ä¸€è‡´æ€§ç†è§£ã€‚</li>
<li>View-GRPOæ¡†æ¶ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87331238a761cb4cc8f3c343821f8434" align="middle">
<img src="https://picx.zhimg.com/v2-f25e6495ff1726fa3c382a6cf4b0440c" align="middle">
<img src="https://picx.zhimg.com/v2-9c783264b71c48ac5e9008a38fe7e533" align="middle">
<img src="https://picx.zhimg.com/v2-12f184a215496584e09314140181d91e" align="middle">
<img src="https://picx.zhimg.com/v2-cc5806587a095f2527e8a82cce4eee9d" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Temporal-Understanding-in-Video-LLMs-through-Stacked-Temporal-Attention-in-Vision-Encoders"><a href="#Enhancing-Temporal-Understanding-in-Video-LLMs-through-Stacked-Temporal-Attention-in-Vision-Encoders" class="headerlink" title="Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal   Attention in Vision Encoders"></a>Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal   Attention in Vision Encoders</h2><p><strong>Authors:Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz</strong></p>
<p>Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: <a target="_blank" rel="noopener" href="https://alirasekh.github.io/STAVEQ2/">https://alirasekh.github.io/STAVEQ2/</a>. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç†è§£è§†é¢‘ä¸­çš„å¤æ‚æ—¶é—´åŠ¨æ€ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰æ¶æ„åœ¨æ—¶é—´ç†è§£æ–¹é¢å­˜åœ¨å…³é”®å±€é™ï¼Œéš¾ä»¥åº”å¯¹è¦æ±‚è¯¦ç»†äº†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Video-LLMæ¶æ„ï¼Œè¯¥æ¶æ„ç›´æ¥åœ¨è§†è§‰ç¼–ç å™¨å†…å¼•å…¥å †å çš„æ—¶é—´æ³¨æ„åŠ›æ¨¡å—ã€‚è¿™ç§è®¾è®¡åœ¨è§†è§‰ç¼–ç å™¨ä¸­èå…¥äº†æ—¶é—´æ³¨æ„åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åŠ¨ä½œçš„è¿›å±•ä»¥åŠå¸§ä¹‹é—´çš„å…³ç³»ï¼Œç„¶åå°†è§†è§‰ä»¤ç‰Œä¼ é€’ç»™LLMã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè¯†åˆ«æ–¹é¢è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬VITATECSã€MVBenchå’ŒVideo-MMEåœ¨å†…çš„åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†é«˜è¾¾+5.5%ã€‚é€šè¿‡å¢å¼ºè§†è§‰ç¼–ç å™¨çš„æ—¶é—´ç»“æ„ï¼Œæˆ‘ä»¬è§£å†³äº†è§†é¢‘ç†è§£æ–¹é¢çš„ä¸€ä¸ªå…³é”®ç©ºç™½é¢†åŸŸã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://alirasekh.github.io/STAVEQ2/%E6%89%BE%E5%88%B0%E3%80%82">https://alirasekh.github.io/STAVEQ2/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26027v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç†è§£è§†é¢‘çš„å¤æ‚æ—¶é—´åŠ¨æ€æ–¹é¢ä»å­˜åœ¨å·¨å¤§æŒ‘æˆ˜ã€‚å½“å‰è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰æ¶æ„åœ¨ç†è§£æ—¶é—´æ–¹é¢å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œéš¾ä»¥å®Œæˆéœ€è¦è¯¦ç»†ç†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Video-LLMæ¶æ„ï¼Œè¯¥æ¶æ„åœ¨è§†è§‰ç¼–ç å™¨å†…ç›´æ¥å¼•å…¥å †å çš„æ—¶é—´æ³¨æ„æ¨¡å—ã€‚é€šè¿‡åŠ å…¥æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰åŠ¨ä½œçš„è¿›å±•å’Œå¸§ä¹‹é—´çš„å…³ç³»ï¼Œç„¶åå°†è§†è§‰ä»¤ç‰Œä¼ é€’ç»™LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„æ—¶é—´æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè¯†åˆ«æ–¹é¢ã€‚æ”¹è¿›åçš„æ¨¡å‹åœ¨VITATECSã€MVBenchå’ŒVideo-MMEç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æé«˜äº†é«˜è¾¾+5.5%ã€‚é€šè¿‡å¢å¼ºè§†è§‰ç¼–ç å™¨çš„æ—¶é—´ç»“æ„ï¼Œè§£å†³äº†Video-LLMåœ¨è§†é¢‘ç†è§£æ–¹é¢çš„å…³é”®å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰Video-LLMåœ¨ç†è§£è§†é¢‘å¤æ‚æ—¶é—´åŠ¨æ€æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰Video-LLMæ¶æ„åœ¨ç†è§£æ—¶é—´æ–¹é¢å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œéš¾ä»¥å®Œæˆéœ€è¦è¯¦ç»†ç†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ã€‚</li>
<li>æ–°æå‡ºçš„Video-LLMæ¶æ„å¼•å…¥äº†å †å çš„æ—¶é—´æ³¨æ„æ¨¡å—ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚</li>
<li>åŠ å…¥æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºæ¨¡å‹æ•æ‰åŠ¨ä½œçš„è¿›å±•å’Œå¸§ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ–°æ¶æ„åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„æ—¶é—´æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè¯†åˆ«æ–¹é¢ã€‚</li>
<li>æ”¹è¿›åçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54954adcf1d865c6db6a8b48d99d9a21" align="middle">
<img src="https://picx.zhimg.com/v2-6bed9685735c5e98c30c502ba94b2b0b" align="middle">
<img src="https://picx.zhimg.com/v2-238e8a1f521ec1762fde04570bb90fcc" align="middle">
<img src="https://picx.zhimg.com/v2-9f7848e647f38bdc44bb6d3b2bbf5af9" align="middle">
<img src="https://picx.zhimg.com/v2-4e002f1a73b8eb9ae0751d5e689d85c2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InfiniPot-V-Memory-Constrained-KV-Cache-Compression-for-Streaming-Video-Understanding"><a href="#InfiniPot-V-Memory-Constrained-KV-Cache-Compression-for-Streaming-Video-Understanding" class="headerlink" title="InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding"></a>InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding</h2><p><strong>Authors:Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang</strong></p>
<p>Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time-quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy-even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants. </p>
<blockquote>
<p>ç°ä»£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿå¯¹é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘è¿›è¡Œæ¨ç†ï¼Œç„¶è€Œå…¶é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¼šéšæ—¶é—´çº¿æ€§å¢é•¿ï¼Œå¾ˆå¿«è¶…è¿‡æ‰‹æœºã€ARçœ¼é•œå’Œè¾¹ç¼˜æœºå™¨äººçš„å›ºå®šå†…å­˜ã€‚ä¹‹å‰çš„å‹ç¼©æ–¹æ¡ˆè¦ä¹ˆå‡è®¾æ•´ä¸ªè§†é¢‘å’Œç”¨æˆ·æŸ¥è¯¢éƒ½å¯ä»¥ç¦»çº¿ä½¿ç”¨ï¼Œè¦ä¹ˆå¿…é¡»å…ˆå»ºç«‹å®Œæ•´çš„ç¼“å­˜ï¼Œå› æ­¤å†…å­˜ä»ç„¶ä¼šéšç€æµé•¿åº¦çš„å¢åŠ è€Œæ‰©å±•ã€‚InfiniPot-Væ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒã€ä¸æŸ¥è¯¢æ— å…³çš„æ¡†æ¶ï¼Œå®ƒä¸ºæµå¼è§†é¢‘ç†è§£å¼ºåˆ¶æ‰§è¡Œä¸€ä¸ªç¡¬æ€§çš„ã€ä¸é•¿åº¦æ— å…³çš„å†…å­˜ä¸Šé™ã€‚åœ¨è§†é¢‘ç¼–ç è¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šç›‘æ§ç¼“å­˜ï¼Œä¸€æ—¦è¾¾åˆ°ç”¨æˆ·è®¾å®šçš„é˜ˆå€¼ï¼Œå°±ä¼šè¿è¡Œä¸€ä¸ªè½»é‡çº§çš„å‹ç¼©è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹ï¼ˆiï¼‰é€šè¿‡æ—¶é—´è½´å†—ä½™ï¼ˆTaRï¼‰æŒ‡æ ‡åˆ é™¤æ—¶é—´ä¸Šçš„å†—ä½™æ ‡è®°ï¼Œï¼ˆiiï¼‰é€šè¿‡å€¼èŒƒæ•°ï¼ˆVaNï¼‰æ’åä¿ç•™è¯­ä¹‰ä¸Šé‡è¦çš„æ ‡è®°ã€‚åœ¨å››ä¸ªå¼€æºçš„MLLMå’Œå››ä¸ªé•¿è§†é¢‘åŠæµå¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInfiniPot-Vå°†å³°å€¼GPUå†…å­˜å‡å°‘äº†é«˜è¾¾94%ï¼Œä¿æŒå®æ—¶ç”Ÿæˆï¼Œå¹¶åœ¨å¤šè½®å¯¹è¯ä¸­åŒ¹é…æˆ–è¶…è¶Šäº†å…¨ç¼“å­˜çš„å‡†ç¡®ç‡ã€‚é€šè¿‡æ¶ˆé™¤æ— éœ€é‡æ–°è®­ç»ƒæˆ–æŸ¥è¯¢çŸ¥è¯†çš„KVç¼“å­˜ç“¶é¢ˆï¼ŒInfiniPot-Vç¼©å°äº†è®¾å¤‡æµåª’ä½“è§†é¢‘åŠ©ç†ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15745v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘æ—¶ï¼Œå…¶é”®å€¼ç¼“å­˜ä¼šéšç€æ—¶é—´çº¿æ€§å¢é•¿ï¼Œè¿…é€Ÿè¶…è¿‡æ‰‹æœºã€ARçœ¼é•œå’Œè¾¹ç¼˜æœºå™¨äººçš„å›ºå®šå†…å­˜ã€‚ä»¥å¾€å‹ç¼©æ–¹æ¡ˆè¦ä¹ˆå‡è®¾æ•´ä¸ªè§†é¢‘å’Œç”¨æˆ·æŸ¥è¯¢éƒ½å¯ä»¥ç¦»çº¿è®¿é—®ï¼Œè¦ä¹ˆå¿…é¡»å…ˆå»ºç«‹å®Œæ•´çš„ç¼“å­˜ï¼Œå› æ­¤å†…å­˜ä»ç„¶éšæµé•¿åº¦è€Œå¢é•¿ã€‚InfiniPot-Væ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒã€æ— éœ€æŸ¥è¯¢çŸ¥è¯†çš„æ¡†æ¶ï¼Œå®ƒä¸ºæµå¼è§†é¢‘ç†è§£å¼ºåˆ¶æ‰§è¡Œå›ºå®šçš„å†…å­˜é™åˆ¶ã€‚åœ¨è§†é¢‘ç¼–ç è¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šç›‘æ§ç¼“å­˜ï¼Œåœ¨ç”¨æˆ·è®¾å®šçš„é˜ˆå€¼è¢«è¾¾åˆ°æ—¶ï¼Œè¿è¡Œä¸€ä¸ªè½»é‡çº§çš„å‹ç¼©è¿‡ç¨‹ï¼Œé€šè¿‡æ—¶é—´è½´å†—ä½™ï¼ˆTaRï¼‰æŒ‡æ ‡å»é™¤æ—¶é—´å†—ä½™çš„ä»¤ç‰Œï¼Œå¹¶é€šè¿‡å€¼èŒƒæ•°ï¼ˆVaNï¼‰æ’åä¿ç•™è¯­ä¹‰é‡è¦çš„ä»¤ç‰Œã€‚åœ¨å››ä¸ªå¼€æºå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œå››ä¸ªé•¿è§†é¢‘åŠæµåª’ä½“åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInfiniPot-Vå°†å³°å€¼GPUå†…å­˜æœ€å¤šå‡å°‘94%ï¼Œä¿æŒå®æ—¶ç”Ÿæˆé€Ÿåº¦ï¼Œç”šè‡³åœ¨å¤šè½®å¯¹è¯ä¸­è¾¾åˆ°æˆ–è¶…è¿‡äº†å…¨ç¼“å­˜çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è§£å†³é”®å€¼ç¼“å­˜ç“¶é¢ˆè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–äº†è§£æŸ¥è¯¢çŸ¥è¯†ï¼ŒInfiniPot-Vå¡«è¡¥äº†åœ¨çº¿æµå¼è§†é¢‘åŠ©ç†çš„ç©ºç™½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´å†…å­˜æŒ‘æˆ˜ã€‚</li>
<li>ä»¥å¾€çš„å‹ç¼©æ–¹æ¡ˆä¸èƒ½é€‚åº”æµå¼è§†é¢‘çš„å†…å­˜éœ€æ±‚ã€‚</li>
<li>InfiniPot-Væ˜¯é¦–ä¸ªé’ˆå¯¹æµå¼è§†é¢‘ç†è§£çš„è®­ç»ƒå…è´¹ã€æŸ¥è¯¢æ— å…³æ¡†æ¶ã€‚</li>
<li>InfiniPot-Våœ¨è§†é¢‘ç¼–ç è¿‡ç¨‹ä¸­å®æ—¶ç›‘æ§ç¼“å­˜å¹¶å‹ç¼©æ•°æ®ã€‚</li>
<li>é€šè¿‡æ—¶é—´è½´å†—ä½™å’Œå€¼èŒƒæ•°æ’åæœºåˆ¶ï¼ŒInfiniPot-Vå»é™¤å†—ä½™å¹¶ä¿æŒè¯­ä¹‰é‡è¦æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒInfiniPot-Væ˜¾è‘—å‡å°‘GPUå†…å­˜ä½¿ç”¨å¹¶æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fc873a7a060c7f4d2f1d070dc3146bb" align="middle">
<img src="https://picx.zhimg.com/v2-2897b9df3fdd66cbfe9a3f3fa1fcbc36" align="middle">
<img src="https://picx.zhimg.com/v2-a523379647309722b6db89b1d1f52e94" align="middle">
<img src="https://picx.zhimg.com/v2-e7a581e6e9a0539bcec8c6fdcd644c0e" align="middle">
<img src="https://picx.zhimg.com/v2-12694dce76a3d04be88bd1b922f689a3" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoExplorer-Think-With-Videos-For-Agentic-Long-Video-Understanding"><a href="#VideoExplorer-Think-With-Videos-For-Agentic-Long-Video-Understanding" class="headerlink" title="VideoExplorer: Think With Videos For Agentic Long-Video Understanding"></a>VideoExplorer: Think With Videos For Agentic Long-Video Understanding</h2><p><strong>Authors:Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, Zhicheng Dou</strong></p>
<p>Long-video understanding~(LVU) is a challenging problem in computer vision. Existing methods either downsample frames for single-pass reasoning, sacrificing fine-grained details, or depend on textual reasoning over task-agnostic representations, hindering task-specific perception and exploration. In this paper, we propose VideoExplorer, a framework grounded in the principle of &#96;&#96;thinking with videoâ€™â€™, which naturally intertwines planning, temporal grounding, and scalable perception into a coherent reasoning process. Rather than reasoning over a static context, VideoExplorer iteratively formulates sub-questions, locates relevant moments, and performs task-oriented, temporally scalable video understanding until reaching the final answer, enabling faithful, efficient, and interpretable reasoning. To address the lack of LVU training resources, we construct a long-video reasoning dataset using difficulty-adaptive sampling to ensure high-quality trajectories on complex tasks. Building on this dataset, we design a two-stage training pipeline: supervised trajectory initialization followed by trajectory-level preference optimization, encouraging adaptive temporal grounding and iterative information integration guided by downstream rewards. Extensive evaluations on popular long-video understanding and reasoning benchmarks demonstrate VideoExplorerâ€™s significant advantage over existing baselines, highlighting its robustness, adaptability, and efficiency. Our code is made publicly available in this repository(<a target="_blank" rel="noopener" href="https://github.com/yhy-2000/VideoDeepResearch">https://github.com/yhy-2000/VideoDeepResearch</a>). </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªéš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå¯¹å¸§è¿›è¡Œé™é‡‡æ ·ä»¥è¿›è¡Œå•æ¬¡æ¨ç†ï¼Œç‰ºç‰²äº†ç²¾ç»†ç»†èŠ‚ï¼Œè¦ä¹ˆä¾èµ–äºä»»åŠ¡æ— å…³è¡¨ç¤ºä¸Šçš„æ–‡æœ¬æ¨ç†ï¼Œé˜»ç¢äº†ç‰¹å®šä»»åŠ¡çš„æ„ŸçŸ¥å’Œæ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VideoExploreræ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºâ€œç”¨è§†é¢‘æ€è€ƒâ€çš„åŸåˆ™ï¼Œå°†è§„åˆ’ã€æ—¶é—´å®šä½å’Œå¯æ‰©å±•æ„ŸçŸ¥è‡ªç„¶åœ°èåˆåˆ°ä¸€ä¸ªè¿è´¯çš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚VideoExplorerä¸æ˜¯å¯¹é™æ€ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ï¼Œè€Œæ˜¯è¿­ä»£åœ°åˆ¶å®šå­é—®é¢˜ï¼Œå®šä½ç›¸å…³æ—¶åˆ»ï¼Œå¹¶è¿›è¡Œé¢å‘ä»»åŠ¡çš„ã€å¯æ‰©å±•çš„è§†é¢‘ç†è§£ï¼Œç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå®ç°å¿ å®ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„æ¨ç†ã€‚ä¸ºäº†è§£å†³LVUè®­ç»ƒèµ„æºçš„ç¼ºä¹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé•¿è§†é¢‘æ¨ç†æ•°æ®é›†ï¼Œé‡‡ç”¨éš¾åº¦è‡ªé€‚åº”é‡‡æ ·ï¼Œä»¥ç¡®ä¿å¤æ‚ä»»åŠ¡çš„é«˜è´¨é‡è½¨è¿¹ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šç›‘ç£è½¨è¿¹åˆå§‹åŒ–ï¼Œç„¶åæ˜¯è½¨è¿¹çº§åˆ«çš„åå¥½ä¼˜åŒ–ï¼Œé¼“åŠ±è‡ªé€‚åº”æ—¶é—´å®šä½å’Œè¿­ä»£ä¿¡æ¯æ•´åˆï¼Œç”±ä¸‹æ¸¸å¥–åŠ±å¼•å¯¼ã€‚åœ¨æµè¡Œé•¿è§†é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒVideoExplorerç›¸å¯¹äºç°æœ‰åŸºçº¿å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œçªæ˜¾äº†å…¶ç¨³å¥æ€§ã€é€‚åº”æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨æœ¬ä»“åº“å…¬å¼€å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/yhy-2000/VideoDeepResearch%EF%BC%89%E3%80%82">https://github.com/yhy-2000/VideoDeepResearchï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10821v6">PDF</a> </p>
<p><strong>Summary</strong><br>é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆé™ä½å¸§åˆ†è¾¨ç‡è¿›è¡Œå•æ¬¡æ¨ç†ï¼Œç‰ºç‰²äº†ç»†èŠ‚ï¼Œè¦ä¹ˆä¾èµ–äºä»»åŠ¡æ— å…³è¡¨ç¤ºä¸Šçš„æ–‡æœ¬æ¨ç†ï¼Œé˜»ç¢äº†ç‰¹å®šä»»åŠ¡çš„æ„ŸçŸ¥å’Œæ¢ç´¢ã€‚æœ¬æ–‡æå‡ºVideoExploreræ¡†æ¶ï¼Œä»¥â€œç”¨è§†é¢‘æ€è€ƒâ€çš„åŸåˆ™ä¸ºåŸºç¡€ï¼Œè‡ªç„¶åœ°å°†è§„åˆ’ã€æ—¶é—´å®šä½å’Œå¯ä¼¸ç¼©æ„ŸçŸ¥èå…¥ä¸€ä¸ªåˆç†çš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚VideoExploreré€šè¿‡è¿­ä»£åœ°æå‡ºå­é—®é¢˜ã€å®šä½ç›¸å…³æ—¶åˆ»ï¼Œå¹¶æ‰§è¡Œé¢å‘ä»»åŠ¡çš„ã€å¯ä¼¸ç¼©çš„è§†é¢‘ç†è§£ï¼Œç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå®ç°å¿ å®ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„æ¨ç†ã€‚ä¸ºåº”å¯¹LVUè®­ç»ƒèµ„æºçš„ç¼ºä¹ï¼Œæˆ‘ä»¬åˆ©ç”¨éš¾åº¦è‡ªé€‚åº”é‡‡æ ·æ„å»ºäº†ä¸€ä¸ªé•¿è§†é¢‘æ¨ç†æ•°æ®é›†ï¼Œç¡®ä¿åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„é«˜è´¨é‡è½¨è¿¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒç®¡é“ï¼šç›‘ç£è½¨è¿¹åˆå§‹åŒ–ï¼Œéšåæ˜¯è½¨è¿¹å±‚é¢çš„åå¥½ä¼˜åŒ–ï¼Œé¼“åŠ±è‡ªé€‚åº”æ—¶é—´å®šä½ä»¥åŠè¿­ä»£ä¿¡æ¯æ•´åˆç”±ä¸‹æ¸¸å¥–åŠ±å¼•å¯¼ã€‚åœ¨æµè¡Œé•¿è§†é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVideoExplorerç›¸æ¯”ç°æœ‰åŸºçº¿å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½“ç°äº†å…¶ç¨³å¥æ€§ã€é€‚åº”æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€äº <a target="_blank" rel="noopener" href="https://github.com/yhy-2000/VideoDeepResearch%E3%80%82">https://github.com/yhy-2000/VideoDeepResearchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šé™ä½å¸§åˆ†è¾¨ç‡å¯¼è‡´ç»†èŠ‚ä¸¢å¤±ï¼Œä¾èµ–æ–‡æœ¬æ¨ç†å½±å“ä»»åŠ¡ç‰¹å®šæ„ŸçŸ¥å’Œæ¢ç´¢ã€‚</li>
<li>VideoExploreræ¡†æ¶æå‡ºâ€œç”¨è§†é¢‘æ€è€ƒâ€åŸåˆ™ï¼Œç»“åˆè§„åˆ’ã€æ—¶é—´å®šä½å’Œå¯ä¼¸ç¼©æ„ŸçŸ¥ã€‚</li>
<li>VideoExploreré€šè¿‡è¿­ä»£å­é—®é¢˜ã€å®šä½ç›¸å…³æ—¶åˆ»ï¼Œå®ç°å¿ å®ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„æ¨ç†ã€‚</li>
<li>ç¼ºä¹LVUè®­ç»ƒèµ„æºï¼Œæ„å»ºäº†ä¸€ä¸ªé•¿è§†é¢‘æ¨ç†æ•°æ®é›†ä»¥åº”å¯¹æ­¤é—®é¢˜ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“è®¾è®¡ï¼šç›‘ç£è½¨è¿¹åˆå§‹åŒ–åï¼Œè¿›è¡Œè½¨è¿¹å±‚é¢çš„åå¥½ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b31468bd5159a731ce376126022c81d6" align="middle">
<img src="https://picx.zhimg.com/v2-b41e55244a19e89f085020aef2fe1e7d" align="middle">
<img src="https://picx.zhimg.com/v2-a1354efd99dd20d14ec19fa2317db009" align="middle">
<img src="https://picx.zhimg.com/v2-8254d77d416f5d1febadae624aad3065" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VidText-Towards-Comprehensive-Evaluation-for-Video-Text-Understanding"><a href="#VidText-Towards-Comprehensive-Evaluation-for-Video-Text-Understanding" class="headerlink" title="VidText: Towards Comprehensive Evaluation for Video Text Understanding"></a>VidText: Towards Comprehensive Evaluation for Video Text Understanding</h2><p><strong>Authors:Zhoufaran Yang, Yan Shu, Jing Wang, Zhifei Yang, Yan Zhang, Yu Li, Keyang Lu, Gangyan Zeng, Shaohui Liu, Yu Zhou, Nicu Sebe</strong></p>
<p>Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments. </p>
<blockquote>
<p>è§†é¢‘ä¸­çš„åµŒå…¥æ–‡æœ¬æºå¸¦ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™å¯¹äºæ•´ä½“è§†é¢‘ç†è§£å’Œå±€éƒ¨äººç±»åŠ¨ä½œçš„ç²¾ç»†æ¨ç†éƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œä¸“é—¨é’ˆå¯¹OCRçš„åŸºå‡†æµ‹è¯•ä»…é™äºé™æ€å›¾åƒï¼Œé™åˆ¶äº†å…¶æ•æ‰æ–‡æœ¬å’ŒåŠ¨æ€è§†è§‰ä¸Šä¸‹æ–‡ä¹‹é—´äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VidTextï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºè§†é¢‘æ–‡æœ¬ç†è§£è¿›è¡Œå…¨é¢å’Œæ·±å…¥è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ã€‚VidTextæä¾›ä»¥ä¸‹å…³é”®åŠŸèƒ½ï¼š1ï¼‰å®ƒæ¶µç›–å¹¿æ³›çš„çœŸå®åœºæ™¯ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€å†…å®¹ï¼Œæ¶µç›–è§†é¢‘æ–‡æœ¬è‡ªç„¶å‡ºç°çš„å„ç§è®¾ç½®ã€‚2ï¼‰å®ƒå¼•å…¥äº†åˆ†å±‚è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è§†é¢‘çº§åˆ«ã€å‰ªè¾‘çº§åˆ«å’Œå®ä¾‹çº§åˆ«çš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿè¯„ä¼°å…¨å±€æ€»ç»“å’Œå±€éƒ¨æ£€ç´¢èƒ½åŠ›ã€‚3ï¼‰è¯¥åŸºå‡†æµ‹è¯•è¿˜å¼•å…¥äº†ä¸€ç³»åˆ—é…å¯¹æ„ŸçŸ¥æ¨ç†ä»»åŠ¡ï¼Œä»è§†è§‰æ–‡æœ¬æ„ŸçŸ¥åˆ°æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„è·¨æ¨¡æ€æ¨ç†ã€‚å¯¹18ä¸ªæœ€æ–°å…ˆè¿›çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šéƒ½é‡åˆ°äº†å›°éš¾ï¼Œæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚è¿›ä¸€æ­¥çš„åˆ†æçªå‡ºäº†æ¨¡å‹å†…åœ¨å› ç´ çš„å½±å“ï¼Œå¦‚è¾“å…¥åˆ†è¾¨ç‡å’ŒOCRèƒ½åŠ›ï¼Œä»¥åŠå¤–éƒ¨å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬ä½¿ç”¨è¾…åŠ©ä¿¡æ¯å’Œé“¾å¼æ€ç»´æ¨ç†ç­–ç•¥ã€‚æˆ‘ä»¬å¸Œæœ›VidTextèƒ½å¤Ÿå¡«è¡¥å½“å‰è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œå¹¶æˆä¸ºæœªæ¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22810v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è§†é¢‘ä¸­çš„æ–‡å­—ä¿¡æ¯æºå¸¦ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¹äºæ•´ä½“è§†é¢‘ç†è§£å’Œå±€éƒ¨äººç±»è¡Œä¸ºçš„ç²¾ç»†æ¨ç†éƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•å¤§å¤šå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œä¸“é—¨é’ˆå¯¹OCRçš„åŸºå‡†æµ‹è¯•ä»…é™äºé™æ€å›¾åƒï¼Œæ— æ³•æ•æ‰æ–‡æœ¬ä¸åŠ¨æ€è§†è§‰ä¸Šä¸‹æ–‡ä¹‹é—´çš„äº¤äº’ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†VidTextï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå…¨é¢æ·±å…¥åœ°è¯„ä¼°è§†é¢‘æ–‡æœ¬ç†è§£è€Œè®¾è®¡çš„æ–°åŸºå‡†æµ‹è¯•ã€‚å®ƒè¦†ç›–å¤šç§ç°å®åœºæ™¯å¹¶æ”¯æŒå¤šè¯­è¨€å†…å®¹ï¼Œå¼•å…¥åˆ†å±‚è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è§†é¢‘çº§ã€å‰ªè¾‘çº§å’Œå®ä¾‹çº§ä»»åŠ¡ï¼Œä»¥åŠä¸€ç³»åˆ—é…å¯¹æ„ŸçŸ¥æ¨ç†ä»»åŠ¡ï¼Œä»è§†è§‰æ–‡æœ¬æ„ŸçŸ¥åˆ°æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„è·¨æ¨¡æ€æ¨ç†ã€‚å¯¹18ç§æœ€æ–°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä»æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘ä¸­çš„æ–‡å­—ä¿¡æ¯å¯¹æ•´ä½“è§†é¢‘ç†è§£å’Œå±€éƒ¨äººç±»è¡Œä¸ºæ¨ç†è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•å¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼ŒOCRåŸºå‡†æµ‹è¯•å—é™äºé™æ€å›¾åƒã€‚</li>
<li>VidTextå¡«è¡¥äº†è§†é¢‘æ–‡æœ¬ç†è§£çš„åŸºå‡†æµ‹è¯•ç©ºç™½ï¼Œè¦†ç›–å¤šç§ç°å®åœºæ™¯å¹¶æ”¯æŒå¤šè¯­è¨€ã€‚</li>
<li>VidTextå¼•å…¥åˆ†å±‚è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è§†é¢‘çº§ã€å‰ªè¾‘çº§å’Œå®ä¾‹çº§ä»»åŠ¡ã€‚</li>
<li>VidTextæä¾›é…å¯¹æ„ŸçŸ¥æ¨ç†ä»»åŠ¡ï¼Œæ¶‰åŠè§†è§‰æ–‡æœ¬æ„ŸçŸ¥å’Œè·¨æ¨¡æ€æ¨ç†ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨VidTextåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21868b729258ce64ee0e072c77276aa4" align="middle">
<img src="https://picx.zhimg.com/v2-ade1c776541239caf86290c47a115e9d" align="middle">
<img src="https://picx.zhimg.com/v2-fa67526f971f8e26a4f1184758a55d40" align="middle">
<img src="https://picx.zhimg.com/v2-d4260b422c63b8ece7888b548329deec" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding"><a href="#Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding" class="headerlink" title="Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding"></a>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding</h2><p><strong>Authors:Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</strong></p>
<p>Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Unlike previous video agents that rely on predefined workflows applied uniformly across different queries, our approach emphasizes the autonomous and adaptive nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code has been released at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery">https://github.com/microsoft/DeepVideoDiscovery</a>. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶åºå¤§çš„æ—¶ç©ºå¤æ‚æ€§å’Œåœ¨è¿™ç§æ‰©å±•è¯­å¢ƒä¸‹è¿›è¡Œé—®ç­”çš„éš¾åº¦ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘åˆ†æèƒ½åŠ›å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†ä¿¡æ¯å¯†é›†çš„ä¸€å°æ—¶é•¿è§†é¢‘æ—¶ï¼Œå®ƒä»¬ä»ç„¶è¡¨ç°å‡ºä¸€äº›å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦è§†é¢‘å‘ç°ï¼ˆDVDï¼‰ä»£ç†ï¼Œé‡‡ç”¨åˆ†æ®µè§†é¢‘å‰ªè¾‘çš„ä»£ç†æœç´¢ç­–ç•¥ã€‚ä¸åŒäºä»¥å‰ä¾èµ–äºä¸ºä¸åŒæŸ¥è¯¢ç»Ÿä¸€åº”ç”¨é¢„å®šå·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒä»£ç†çš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡åœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šæä¾›ä¸€ç³»åˆ—ä»¥æœç´¢ä¸ºä¸­å¿ƒçš„å·¥å…·ï¼Œæˆ‘ä»¬çš„DVDä»£ç†åˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›æ¥è§„åˆ’å…¶å½“å‰è§‚å¯ŸçŠ¶æ€ï¼Œæ ¹æ®æ”¶é›†çš„ä¿¡æ¯æˆ˜ç•¥æ€§åœ°é€‰æ‹©å·¥å…·æ¥åè°ƒé€‚åº”ä¸åŒæŸ¥è¯¢çš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„DVDä»£ç†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šè¾¾åˆ°äº†74.2%çš„å‡†ç¡®ç‡ï¼Œè¿™å¤§å¤§è¶…è¿‡äº†æ‰€æœ‰å…ˆå‰çš„å·¥ä½œï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨å­—å¹•çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æé«˜åˆ°7.%ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery">https://github.com/microsoft/DeepVideoDiscovery</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18079v4">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>é•¿è§†é¢‘ç†è§£é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚çš„æ—¶ç©ºä¿¡æ¯å’Œé•¿æ—¶é—´ä¸Šä¸‹æ–‡çš„é—®é¢˜å›ç­”ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶èƒ½å¤„ç†è§†é¢‘åˆ†æå’Œå¤„ç†é•¿æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨å¤„ç†ä¿¡æ¯å¯†é›†çš„å°æ—¶çº§è§†é¢‘æ—¶ä»æœ‰å±€é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦è§†é¢‘å‘ç°ï¼ˆDVDï¼‰ä»£ç†ï¼Œé‡‡ç”¨ä»£ç†æœç´¢ç­–ç•¥å¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œåˆ†å‰²ã€‚ä¸ä¾èµ–é¢„è®¾å·¥ä½œæµå¯¹ä¸åŒæŸ¥è¯¢è¿›è¡Œç»Ÿä¸€å¤„ç†çš„ä¼ ç»Ÿè§†é¢‘ä»£ç†ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒäº†ä»£ç†çš„è‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚DVDä»£ç†åœ¨å¤šç»´è§†é¢‘æ•°æ®åº“ä¸Šæä¾›ä¸€ç³»åˆ—æœç´¢ä¸­å¿ƒå·¥å…·ï¼Œåˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›è§„åˆ’å½“å‰è§‚æµ‹çŠ¶æ€ï¼Œå¹¶æ ¹æ®æ‰€è·å¾—çš„ä¿¡æ¯ä¸ºä¸åŒæŸ¥è¯¢åè°ƒè‡ªé€‚åº”å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒé•¿æ—¶é—´è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œå…¨é¢è¯„ä¼°è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†æƒŠäººçš„é«˜åº¦ï¼Œä¸ä½¿ç”¨å­—å¹•çš„æƒ…å†µä¸‹å‡†ç¡®ç‡ä¸º74.2%ï¼Œè¶…è¶Šæ‰€æœ‰å…ˆå‰çš„å·¥ä½œï¼›ä½¿ç”¨å­—å¹•åè¿›ä¸€æ­¥æé«˜è‡³76.0%ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery%E3%80%82">https://github.com/microsoft/DeepVideoDiscoveryã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é•¿è§†é¢‘ç†è§£å­˜åœ¨å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦å› ä¸ºæ—¶ç©ºå¤æ‚æ€§ä»¥åŠé•¿ä¸Šä¸‹æ–‡ä¸­çš„é—®ç­”éš¾åº¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä¿¡æ¯å¯†é›†çš„é•¿è§†é¢‘æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„æ·±åº¦è§†é¢‘å‘ç°ï¼ˆDVDï¼‰ä»£ç†é‡‡ç”¨ä»£ç†æœç´¢ç­–ç•¥åˆ†å‰²è§†é¢‘ç‰‡æ®µï¼Œå¼ºè°ƒè‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>DVDä»£ç†åœ¨å¤šç»´è§†é¢‘æ•°æ®åº“ä¸Šæä¾›æœç´¢ä¸­å¿ƒå·¥å…·ï¼Œåˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›è¿›è¡Œè§„åˆ’å’Œé€‰æ‹©ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€é«˜æ€§èƒ½æ ‡å‡†ï¼Œå‡†ç¡®ç‡ä¸º74.2%ï¼ˆä¸ä½¿ç”¨å­—å¹•ï¼‰å’Œ76.0%ï¼ˆä½¿ç”¨å­—å¹•ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-796f68dbba55594e233ef07afa279018" align="middle">
<img src="https://picx.zhimg.com/v2-953a2e0735dc1523c356980ffc004edb" align="middle">
<img src="https://picx.zhimg.com/v2-c0e8498103aa5bc89e894bf7d2bb4679" align="middle">
<img src="https://picx.zhimg.com/v2-3dad245277e45299ac6e5439e602471c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RTV-Bench-Benchmarking-MLLM-Continuous-Perception-Understanding-and-Reasoning-through-Real-Time-Video"><a href="#RTV-Bench-Benchmarking-MLLM-Continuous-Perception-Understanding-and-Reasoning-through-Real-Time-Video" class="headerlink" title="RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and   Reasoning through Real-Time Video"></a>RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and   Reasoning through Real-Time Video</h2><p><strong>Authors:Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Ying Ma, Xuming Hu</strong></p>
<p>Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: <a target="_blank" rel="noopener" href="https://github.com/LJungang/RTV-Bench">https://github.com/LJungang/RTV-Bench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¶Šæ¥è¶Šå‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°å®ƒä»¬åœ¨åŠ¨æ€ã€ç°å®ç¯å¢ƒä¸­è¿ç»­æ‰§è¡Œè¿™äº›ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RTV-Benchï¼Œè¿™æ˜¯é’ˆå¯¹MLLMå®æ—¶è§†é¢‘åˆ†æçš„é«˜ç²¾åº¦åŸºå‡†æµ‹è¯•ã€‚RTV-Benché‡‡ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼šï¼ˆ1ï¼‰å¤šæ—¶é—´æˆ³é—®ç­”ï¼ˆMTQAï¼‰ï¼Œç­”æ¡ˆéšåœºæ™¯å˜åŒ–è€Œæ¼”å˜ï¼›ï¼ˆ2ï¼‰åˆ†å±‚é—®é¢˜ç»“æ„ï¼Œç»“åˆåŸºæœ¬å’Œé«˜çº§æŸ¥è¯¢ï¼›ï¼ˆ3ï¼‰å¤šç»´è¯„ä¼°ï¼Œè¯„ä¼°è¿ç»­æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†çš„èƒ½åŠ›ã€‚RTV-BenchåŒ…å«552ä¸ªå¤šæ ·åŒ–è§†é¢‘ï¼ˆ167.2å°æ—¶ï¼‰å’Œ4631ç»„é«˜è´¨é‡é—®ç­”å¯¹ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰æ¨¡å‹ï¼ˆGPT-4oã€Gemini 2.0ï¼‰ã€å¼€æºç¦»çº¿æ¨¡å‹ï¼ˆQwen2.5-VLã€VideoLLaMA3ï¼‰å’Œå¼€æºå®æ—¶æ¨¡å‹ï¼ˆVITA-1.5ã€InternLM-XComposer2.5-OmniLiveï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹å¤§å¤šä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºé¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œæ›´å¤§çš„æ¨¡å‹è§„æ¨¡æˆ–æ›´é«˜çš„å¸§é‡‡æ ·ç‡å¹¶ä¸ä¼šæ˜¾è‘—æé«˜RTV-Benchçš„æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¯¼è‡´è½»å¾®ä¸‹é™ã€‚è¿™å¼ºè°ƒäº†åœ¨è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—ä¼˜åŒ–æ–¹é¢éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¶æ„ï¼Œä»¥æ¨åŠ¨ä½¿ç”¨MLLMsçš„å®æ—¶è§†é¢‘åˆ†æçš„å‘å±•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å·¥å…·åŒ…å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/LJungang/RTV-Bench">https://github.com/LJungang/RTV-Bench</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02064v3">PDF</a> Accepted by NeurIPS 2025 Datasets and Benchmarks Track;</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¶Šæ¥è¶Šå‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ ‡å‡†è¯„ä¼°æ— æ³•å……åˆ†è¡¡é‡å…¶åœ¨åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„è¿ç»­ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºRTV-Benchï¼Œä¸€ä¸ªé’ˆå¯¹MLLMå®æ—¶è§†é¢‘åˆ†æçš„ç²¾ç»†åŸºå‡†æµ‹è¯•ã€‚RTV-Benché‡‡ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼š1ï¼‰å¤šæ—¶é—´æˆ³é—®ç­”ï¼ˆMTQAï¼‰ï¼Œç­”æ¡ˆéšåœºæ™¯å˜åŒ–è€Œæ¼”å˜ï¼›2ï¼‰åˆ†å±‚é—®é¢˜ç»“æ„ï¼Œç»“åˆåŸºæœ¬å’Œé«˜çº§æŸ¥è¯¢ï¼›3ï¼‰å¤šç»´åº¦è¯„ä¼°ï¼Œè¯„ä¼°è¿ç»­æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†çš„èƒ½åŠ›ã€‚RTV-BenchåŒ…å«552ä¸ªå¤šæ ·åŒ–è§†é¢‘ï¼ˆ167.2å°æ—¶ï¼‰å’Œ4631ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰ï¼ˆGPT-4oã€Gemini 2.0ï¼‰ã€å¼€æºç¦»çº¿ï¼ˆQwen2.5-VLã€VideoLLaMA3ï¼‰å’Œå¼€æºå®æ—¶æ¨¡å‹ï¼ˆVITA-1.5ã€InternLM-XComposer2.5-OmniLiveï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹å¤§å¤šä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºé¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚åˆ†æè¿˜æ˜¾ç¤ºï¼Œæ¨¡å‹è§„æ¨¡å¢å¤§æˆ–å¸§é‡‡æ ·ç‡æé«˜å¹¶ä¸æ˜¾è‘—å¢å¼ºRTV-Benchçš„æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¯¼è‡´è½»å¾®ä¸‹é™ã€‚è¿™å¼ºè°ƒäº†åœ¨è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—ä¼˜åŒ–æ–¹é¢éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¶æ„ï¼Œä»¥æ¨åŠ¨MLLMåœ¨å®æ—¶è§†é¢‘åˆ†ææ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å·¥å…·åŒ…å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/LJungang/RTV-Bench">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€ç°å®ç¯å¢ƒä¸­çš„è¿ç»­ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›çš„è¯„ä¼°å­˜åœ¨å·®è·ã€‚</li>
<li>RTV-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®æ—¶è§†é¢‘åˆ†ææ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>RTV-Benché‡‡ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼šå¤šæ—¶é—´æˆ³é—®ç­”ã€åˆ†å±‚é—®é¢˜ç»“æ„å’Œå¤šç»´åº¦è¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¼€æºå®æ—¶æ¨¡å‹è¡¨ç°ä¼˜äºç¦»çº¿æ¨¡å‹ï¼Œä½†ä»è½åäºä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å¢å¤§æˆ–å¸§é‡‡æ ·ç‡æé«˜å¯¹RTV-Benchæ€§èƒ½çš„å½±å“å¹¶ä¸æ˜¾è‘—ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¶æ„æ¥ä¼˜åŒ–è§†é¢‘æµå¤„ç†å’Œé•¿åºåˆ—å¤„ç†ï¼Œä»¥æ¨åŠ¨å®æ—¶è§†é¢‘åˆ†æçš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9614c3b55acb7944783fe11cce93e707" align="middle">
<img src="https://picx.zhimg.com/v2-5e8e4c798605363ed765cce779a5b054" align="middle">
<img src="https://picx.zhimg.com/v2-dea6fe57be8c8e303d422ed6b4962c42" align="middle">
<img src="https://picx.zhimg.com/v2-79e6e73f8c70116953a199dc9cd38815" align="middle">
<img src="https://picx.zhimg.com/v2-40456f1f993ee6fb9d4d24d9ce49250a" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Fuxiao Liu, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Vision-Language Models (VLMs) have achieved strong results in video understanding, yet a key question remains: do they truly comprehend visual content or only learn shallow correlations between vision and language? Real visual understanding, especially of physics and common sense, is essential for AI systems that interact with the physical world. Current evaluations mostly use real-world videos similar to training data, so high benchmark scores may not reflect real reasoning ability. To address this, we propose negative-control tests using videos that depict physically impossible or logically inconsistent events. We introduce VideoHallu, a synthetic dataset of physics- and commonsense-violating scenes generated with Veo2, Sora, and Kling. It includes expert-annotated question-answer pairs across four categories of violations. Tests of leading VLMs (Qwen-2.5-VL, Video-R1, VideoChat-R1) show that, despite strong results on benchmarks such as MVBench and MMVU, they often miss these violations, exposing gaps in visual reasoning. Reinforcement learning fine-tuning on VideoHallu improves recognition of such violations without reducing standard benchmark performance. Our data is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å…³é”®é—®é¢˜ä»ç„¶å­˜åœ¨ï¼šå®ƒä»¬æ˜¯å¦çœŸçš„ç†è§£è§†è§‰å†…å®¹ï¼Œè¿˜æ˜¯åªå­¦ä¹ è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„æµ…å±‚å…³è”ï¼ŸçœŸæ­£çš„è§†è§‰ç†è§£ï¼Œå°¤å…¶æ˜¯å¯¹ç‰©ç†å’Œå¸¸è¯†çš„ç†è§£ï¼Œå¯¹äºä¸ç‰©ç†ä¸–ç•Œäº¤äº’çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚å½“å‰çš„è¯„ä¼°ä¸»è¦ä½¿ç”¨ä¸ç°å®ä¸–ç•Œè§†é¢‘ç›¸ä¼¼çš„è®­ç»ƒæ•°æ®ï¼Œå› æ­¤é«˜åŸºå‡†æµ‹è¯•åˆ†æ•°å¯èƒ½å¹¶ä¸èƒ½åæ˜ çœŸå®çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æç»˜ç‰©ç†ä¸Šä¸å¯èƒ½æˆ–é€»è¾‘ä¸Šä¸ä¸€è‡´äº‹ä»¶çš„è´Ÿé¢æ§åˆ¶æµ‹è¯•ã€‚æˆ‘ä»¬å¼•å…¥äº†VideoHalluï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡Veo2ã€Soraå’ŒKlingç”Ÿæˆçš„è¿åç‰©ç†å’Œå¸¸è¯†çš„åˆæˆæ•°æ®é›†ã€‚å®ƒåŒ…å«å››ä¸ªç±»åˆ«è¿è§„çš„ä¸“å®¶æ³¨é‡Šé—®ç­”å¯¹ã€‚å¯¹é¢†å…ˆçš„VLMsï¼ˆQwen-2.5-VLã€Video-R1ã€VideoChat-R1ï¼‰çš„æµ‹è¯•è¡¨æ˜ï¼Œå°½ç®¡å®ƒä»¬åœ¨MVBenchå’ŒMMVUç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¿½ç•¥è¿™äº›è¿è§„è¡Œä¸ºï¼Œæš´éœ²å‡ºè§†è§‰æ¨ç†ä¸Šçš„å·®è·ã€‚åœ¨VideoHalluä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œå¯ä»¥æé«˜å¯¹è¿™äº›è¿è§„è¡Œä¸ºçš„è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶ä¸é™ä½æ ‡å‡†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zli12321/VideoHallu.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºæ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£äº†è§†è§‰å†…å®¹ï¼Œè¿˜æ˜¯ä»…ä»…å­¦ä¹ äº†è§†è§‰å’Œè¯­è¨€çš„æµ…å±‚å…³è”ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½œè€…æå‡ºäº†ä½¿ç”¨æç»˜ç‰©ç†ä¸Šä¸å¯èƒ½æˆ–é€»è¾‘ä¸Šä¸ä¸€è‡´äº‹ä»¶çš„è´Ÿé¢æ§åˆ¶æµ‹è¯•ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªåä¸ºVideoHalluçš„åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¿åç‰©ç†å’Œå¸¸è¯†çš„åœºæ™¯ï¼Œå¹¶å¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨MVBenchå’ŒMMVUç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œé¢†å…ˆçš„VLMsåœ¨è¯†åˆ«è¿™äº›è¿è§„äº‹ä»¶æ—¶ä»å­˜åœ¨ç¼ºé™·ã€‚é€šè¿‡VideoHalluè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œå¯æé«˜å¯¹è¿™äº›è¿è§„äº‹ä»¶çš„è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶ä¸é™ä½æ ‡å‡†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨æ˜¯å¦çœŸæ­£ç†è§£è§†è§‰å†…å®¹çš„ç–‘é—®ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼çš„ç°å®ä¸–ç•Œè§†é¢‘ï¼Œå› æ­¤é«˜åŸºå‡†æµ‹è¯•åˆ†æ•°å¯èƒ½æ— æ³•åæ˜ çœŸæ­£çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä½¿ç”¨æç»˜ç‰©ç†ä¸Šä¸å¯èƒ½æˆ–é€»è¾‘ä¸Šä¸ä¸€è‡´äº‹ä»¶çš„è´Ÿé¢æ§åˆ¶æµ‹è¯•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªåä¸ºVideoHalluçš„åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«è¿åç‰©ç†å’Œå¸¸è¯†çš„åœºæ™¯ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>é¢†å…ˆçš„VLMsåœ¨è¯†åˆ«è¿™äº›è¿è§„äº‹ä»¶æ—¶å­˜åœ¨ç¼ºé™·ï¼Œæš´éœ²äº†è§†è§‰æ¨ç†çš„å·®è·ã€‚</li>
<li>é€šè¿‡VideoHalluè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒå¯ä»¥æé«˜æ¨¡å‹å¯¹è¿è§„äº‹ä»¶çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d287477b9dd29857c43d2940e034547c" align="middle">
<img src="https://picx.zhimg.com/v2-74a6bdd863b47b685cc088c1cffbbffd" align="middle">
<img src="https://picx.zhimg.com/v2-c412c59aebcf95aa9110394870c97e96" align="middle">
<img src="https://picx.zhimg.com/v2-c6351cc98e36ab792557987faa85d7ff" align="middle">
<img src="https://picx.zhimg.com/v2-0edb1c9a1db28c5b91245ac408212507" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-755ac230a8575ea201695e23cbd4ac24" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Vision Transformer for Robust Occluded Person Reidentification in   Complex Surveillance Scenes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-225a662404a8c32f6e7e723321d5bdb4" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Enabling Fast and Accurate Neutral Atom Readout through Image Denoising
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
