<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  MARAG-R1 Beyond Single Retriever via Reinforcement-Learned Multi-Tool   Agentic Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-0c575c385ccbf7ea5d985ca173235fcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278518&auth_key=1762278518-0-0-13a46b56186c27282a6046c220770ca9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="MARAG-R1-Beyond-Single-Retriever-via-Reinforcement-Learned-Multi-Tool-Agentic-Retrieval"><a href="#MARAG-R1-Beyond-Single-Retriever-via-Reinforcement-Learned-Multi-Tool-Agentic-Retrieval" class="headerlink" title="MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool   Agentic Retrieval"></a>MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool   Agentic Retrieval</h2><p><strong>Authors:Qi Luo, Xiaonan Li, Yuxin Wang, Tingshuo Fan, Yuan Li, Xinchi Chen, Xipeng Qiu</strong></p>
<p>Large Language Models (LLMs) excel at reasoning and generation but are inherently limited by static pretraining data, resulting in factual inaccuracies and weak adaptability to new information. Retrieval-Augmented Generation (RAG) addresses this issue by grounding LLMs in external knowledge; However, the effectiveness of RAG critically depends on whether the model can adequately access relevant information. Existing RAG systems rely on a single retriever with fixed top-k selection, restricting access to a narrow and static subset of the corpus. As a result, this single-retriever paradigm has become the primary bottleneck for comprehensive external information acquisition, especially in tasks requiring corpus-level reasoning. To overcome this limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG framework that enables LLMs to dynamically coordinate multiple retrieval mechanisms for broader and more precise information access. MARAG-R1 equips the model with four retrieval tools â€“ semantic search, keyword search, filtering, and aggregation â€“ and learns both how and when to use them through a two-stage training process: supervised fine-tuning followed by reinforcement learning. This design allows the model to interleave reasoning and retrieval, progressively gathering sufficient evidence for corpus-level synthesis. Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that MARAG-R1 substantially outperforms strong baselines and achieves new state-of-the-art results in corpus-level reasoning tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿æ¨ç†å’Œç”Ÿæˆï¼Œä½†å—åˆ°é™æ€é¢„è®­ç»ƒæ•°æ®æœ¬èº«çš„å›ºæœ‰é™åˆ¶ï¼Œå¯¼è‡´äº‹å®ä¸å‡†ç¡®å’Œå¯¹æ–°ä¿¡æ¯çš„é€‚åº”èƒ½åŠ›è¾ƒå¼±ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä½¿LLMä»¥å¤–éƒ¨çŸ¥è¯†ä¸ºåŸºç¡€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›ç„¶è€Œï¼ŒRAGçš„æœ‰æ•ˆæ€§å…³é”®åœ¨äºæ¨¡å‹æ˜¯å¦èƒ½å……åˆ†è®¿é—®ç›¸å…³ä¿¡æ¯ã€‚ç°æœ‰çš„RAGç³»ç»Ÿä¾èµ–äºå•ä¸ªæ£€ç´¢å™¨ï¼Œå¹¶ä½¿ç”¨å›ºå®šçš„å‰kä¸ªé€‰é¡¹é€‰æ‹©ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹è®¿é—®è¯­æ–™åº“çš„ç‹­çª„ä¸”é™æ€çš„å­é›†èŒƒå›´ã€‚å› æ­¤ï¼Œå•ä¸€æ£€ç´¢å™¨æ¨¡å¼å·²æˆä¸ºå…¨é¢è·å–å¤–éƒ¨ä¿¡æ¯çš„ä¸»è¦ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¯­æ–™åº“çº§åˆ«æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MARAG-R1ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¤šå·¥å…·RAGæ¡†æ¶ï¼Œå®ƒä½¿LLMèƒ½å¤ŸåŠ¨æ€åè°ƒå¤šç§æ£€ç´¢æœºåˆ¶ï¼Œä»¥è·å–æ›´å¹¿æ³›å’Œæ›´ç²¾ç¡®çš„ä¿¡æ¯ã€‚MARAG-R1ä¸ºæ¨¡å‹é…å¤‡äº†å››ç§æ£€ç´¢å·¥å…·ï¼ŒåŒ…æ‹¬è¯­ä¹‰æœç´¢ã€å…³é”®è¯æœç´¢ã€è¿‡æ»¤å’Œèšåˆï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼šé¦–å…ˆæ˜¯ç›‘ç£å¾®è°ƒï¼Œç„¶åæ˜¯å¼ºåŒ–å­¦ä¹ ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨æ¨ç†å’Œæ£€ç´¢ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œé€æ­¥æ”¶é›†è¶³å¤Ÿçš„è¯æ®è¿›è¡Œè¯­æ–™åº“çº§åˆ«çš„ç»¼åˆã€‚åœ¨å…¨çƒé—®ç­”ï¼ˆGlobalQAï¼‰ã€HotpotQAå’Œ2WikiMultiHopQAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARAG-R1æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨è¯­æ–™åº“çº§åˆ«çš„æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27569v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºé™æ€é¢„è®­ç»ƒæ•°æ®çš„å›ºæœ‰å±€é™æ€§ï¼Œå­˜åœ¨äº‹å®æ€§ä¸å‡†ç¡®å’Œé€‚åº”æ–°ä¿¡æ¯èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä½¿LLMä»¥å¤–éƒ¨çŸ¥è¯†ä¸ºåŸºç¡€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼ŒRAGçš„æœ‰æ•ˆæ€§å–å†³äºæ¨¡å‹è·å–ç›¸å…³ä¿¡æ¯çš„å……åˆ†æ€§ã€‚ç°æœ‰çš„RAGç³»ç»Ÿä¾èµ–äºå•ä¸€æ£€ç´¢å™¨ï¼Œä¸”åªé€‰æ‹©å›ºå®šçš„å‰kä¸ªç»“æœï¼Œè¿™é™åˆ¶äº†ä»è¯­æ–™åº“ä¸­è·å–ä¿¡æ¯çš„èŒƒå›´ã€‚å› æ­¤ï¼Œå•ä¸€æ£€ç´¢å™¨å·²æˆä¸ºå…¨é¢è·å–å¤–éƒ¨ä¿¡æ¯çš„ä¸»è¦ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¯­æ–™åº“çº§æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MARAG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„å¤šå·¥å…·RAGæ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€åè°ƒå¤šç§æ£€ç´¢æœºåˆ¶ï¼Œä»¥è·å–æ›´å¹¿æ³›ã€æ›´ç²¾ç¡®çš„ä¿¡æ¯ã€‚MARAG-R1é…å¤‡äº†å››ç§æ£€ç´¢å·¥å…·â€”â€”è¯­ä¹‰æœç´¢ã€å…³é”®è¯æœç´¢ã€è¿‡æ»¤å’Œèšåˆï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨è¿™äº›å·¥å…·ï¼šç›‘ç£å¾®è°ƒåè·Ÿç€å¼ºåŒ–å­¦ä¹ ã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨æ¨ç†å’Œæ£€ç´¢ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œé€æ­¥æ”¶é›†è¶³å¤Ÿçš„è¯æ®è¿›è¡Œè¯­æ–™åº“çº§ç»¼åˆã€‚åœ¨GlobalQAã€HotpotQAå’Œ2WikiMultiHopQAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARAG-R1æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶åœ¨è¯­æ–™åº“çº§æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å—é™äºé™æ€é¢„è®­ç»ƒæ•°æ®ï¼Œå­˜åœ¨äº‹å®æ€§ä¸å‡†ç¡®å’Œé€‚åº”æ–°ä¿¡æ¯èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä½¿LLMä»¥å¤–éƒ¨çŸ¥è¯†ä¸ºåŸºç¡€æ¥è§£å†³LLMçš„å±€é™æ€§ã€‚</li>
<li>ç°æœ‰RAGç³»ç»Ÿä¾èµ–å•ä¸€æ£€ç´¢å™¨ï¼Œé™åˆ¶äº†ä»è¯­æ–™åº“ä¸­è·å–ä¿¡æ¯çš„èŒƒå›´ï¼Œæˆä¸ºå…¨é¢è·å–å¤–éƒ¨ä¿¡æ¯çš„ä¸»è¦ç“¶é¢ˆã€‚</li>
<li>MARAG-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„å¤šå·¥å…·RAGæ¡†æ¶ï¼ŒåŠ¨æ€åè°ƒå¤šç§æ£€ç´¢æœºåˆ¶ï¼Œä»¥è·å–æ›´å¹¿æ³›ã€æ›´ç²¾ç¡®çš„ä¿¡æ¯ã€‚</li>
<li>MARAG-R1é…å¤‡äº†è¯­ä¹‰æœç´¢ã€å…³é”®è¯æœç´¢ã€è¿‡æ»¤å’Œèšåˆå››ç§æ£€ç´¢å·¥å…·ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å­¦ä¹ ä½¿ç”¨è¿™äº›å·¥å…·ã€‚</li>
<li>MARAG-R1å…è®¸æ¨¡å‹åœ¨æ¨ç†å’Œæ£€ç´¢ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œé€æ­¥æ”¶é›†è¯æ®è¿›è¡Œè¯­æ–™åº“çº§ç»¼åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4e50b42e2b3cb72c9b9128fa4321f5fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278290&auth_key=1762278290-0-0-503d93d1931e3ba82d43d9e09216a591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db0504a8982f2e66173c0fe35e2ca04b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278297&auth_key=1762278297-0-0-c0ebcb59a183e43a5bd469fcc1bab9a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c10c8d9cc57eccaf0b1a6a962e30fe7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278304&auth_key=1762278304-0-0-9b1bdba05c75b24ead778e4bd1a419f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Interact-RAG-Reason-and-Interact-with-the-Corpus-Beyond-Black-Box-Retrieval"><a href="#Interact-RAG-Reason-and-Interact-with-the-Corpus-Beyond-Black-Box-Retrieval" class="headerlink" title="Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box   Retrieval"></a>Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box   Retrieval</h2><p><strong>Authors:Yulong Hui, Chao Chen, Zhihang Fu, Yihao Liu, Jieping Ye, Huanchen Zhang</strong></p>
<p>Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agentsâ€™ actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡èå…¥å¤–éƒ¨ä¿¡æ¯æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰æµè¡Œçš„RAGæ–¹æ³•å—åˆ°ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šå®ƒä»¬å°†æ£€ç´¢è¿‡ç¨‹è§†ä¸ºé»‘ç®±æŸ¥è¯¢æ“ä½œã€‚è¿™é™åˆ¶äº†ä»£ç†çš„æ“ä½œä»…é™äºæŸ¥è¯¢å‘å¸ƒï¼Œé˜»ç¢äº†å…¶å¤„ç†å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Interact-RAGï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„èŒƒå¼ï¼Œå®ƒå°†LLMä»£ç†ä»è¢«åŠ¨çš„æŸ¥è¯¢å‘å¸ƒè€…æå‡ä¸ºæ£€ç´¢è¿‡ç¨‹çš„ä¸»åŠ¨æ“çºµè€…ã€‚æˆ‘ä»¬é€šè¿‡è¯­æ–™åº“äº¤äº’å¼•æ“ç ´è§£äº†é»‘ç®±ï¼Œä¸ºä»£ç†æä¾›äº†ä¸€ç»„åŠ¨ä½œåŸè¯­ï¼Œä»¥å®ç°ä¿¡æ¯æ£€ç´¢çš„ç²¾ç»†æ§åˆ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºä»£ç†åœ¨æ•´ä¸ªRAGç®¡é“ä¸Šçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªå¢å¼ºæ¨ç†çš„å·¥ä½œæµï¼Œå®ƒæ”¯æŒé›¶æ‰§è¡Œåˆæˆäº¤äº’è½¨è¿¹ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›åˆæˆæ•°æ®é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒä¸€ä¸ªå®Œå…¨è‡ªä¸»ç«¯åˆ°ç«¯çš„ä»£ç†ï¼Œéšåé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¾®è°ƒã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInteract-RAGæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ¨ç†äº¤äº’ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27566v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶è®ºæ–‡ï¼Œä¸»è¦ä»‹ç»äº†Interact-RAGè¿™ä¸€æ–°èŒƒå¼ã€‚è¯¥èŒƒå¼å°†LLMä»£ç†ä»è¢«åŠ¨æŸ¥è¯¢å‘å¸ƒè€…æå‡ä¸ºæ£€ç´¢è¿‡ç¨‹çš„ä¸»åŠ¨æ“çºµè€…ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•å°†æ£€ç´¢è¿‡ç¨‹è§†ä¸ºé»‘ç›’æŸ¥è¯¢æ“ä½œçš„å±€é™æ€§ã€‚é€šè¿‡å¼•å…¥Corpus Interaction Engineï¼Œä¸ºä»£ç†æä¾›äº†ä¸€å¥—ç²¾ç»†æ§åˆ¶ä¿¡æ¯æ£€ç´¢çš„åŠ¨ä½œåŸè¯­ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ä¸ªå¢å¼ºå‹æ¨ç†å·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿåœ¨é›¶èµ·ç‚¹ä¸Šæ‰§è¡Œå¹¶åˆæˆäº¤äº’è½¨è¿¹ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒï¼ŒInteract-RAGåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æ¨ç†äº¤äº’ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Interact-RAGå°†LLMä»£ç†ä»è¢«åŠ¨æŸ¥è¯¢å‘å¸ƒè€…è½¬å˜ä¸ºç§¯ææ“çºµæ£€ç´¢è¿‡ç¨‹çš„æ–°èŒƒå¼ï¼Œè§£å†³äº†ç°æœ‰RAGæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥Corpus Interaction Engineï¼Œä¸ºä»£ç†æä¾›ç²¾ç»†æ§åˆ¶ä¿¡æ¯æ£€ç´¢çš„åŠ¨ä½œåŸè¯­ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå¢å¼ºå‹æ¨ç†å·¥ä½œæµç¨‹ï¼Œæ”¯æŒé›¶èµ·ç‚¹æ‰§è¡Œå’Œäº¤äº’è½¨è¿¹çš„åˆæˆã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒï¼ŒInteract-RAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Interact-RAGæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æ¨ç†äº¤äº’ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶æˆæœå¯¹äºæå‡LLMçš„ä¿¡æ¯æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºLLMåœ¨æœªæ¥æ›´å¹¿æ³›çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a8ef127f3f90f81f1d2bd3d0e9dc84ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278312&auth_key=1762278312-0-0-2d6550e6614f2674863c5be6aa24766e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7089c7b45dff37c43f3f0b14c91c636~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278319&auth_key=1762278319-0-0-cdacc08408bd5da6382a494455891be9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e423c36c4ac94684555d0a3932219d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278326&auth_key=1762278326-0-0-d45a896b39dec0a5de24f3650cb8e3e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80277586c1d42a835f68b7446e6314f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278333&auth_key=1762278333-0-0-7e2bc5c898c7402e1d8a441e19e59676&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mechanics-of-Learned-Reasoning-1-TempoBench-A-Benchmark-for-Interpretable-Deconstruction-of-Reasoning-System-Performance"><a href="#Mechanics-of-Learned-Reasoning-1-TempoBench-A-Benchmark-for-Interpretable-Deconstruction-of-Reasoning-System-Performance" class="headerlink" title="Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for   Interpretable Deconstruction of Reasoning System Performance"></a>Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for   Interpretable Deconstruction of Reasoning System Performance</h2><p><strong>Authors:Nikolaus Holzer, William Fishell, Baishakhi Ray, Mark Santolucito</strong></p>
<p>Large Language Models (LLMs) are increasingly excelling and outpacing human performance on many tasks. However, to improve LLM reasoning, researchers either rely on ad-hoc generated datasets or formal mathematical proof systems such as the Lean proof assistant. Whilst ad-hoc generated methods can capture the decision chains of real-world reasoning processes, they may encode some inadvertent bias in the space of reasoning they cover; they also cannot be formally verified. On the other hand, systems like Lean can guarantee verifiability, but are not well-suited to capture the nature of agentic decision chain-based tasks. This creates a gap both in performance for functions such as business agents or code assistants, and in the usefulness of LLM reasoning benchmarks, whereby these fall short in reasoning structure or real-world alignment. We introduce TempoBench, the first formally grounded and verifiable diagnostic benchmark that parametrizes difficulty to systematically analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks to break down reasoning ability. First, temporal trace evaluation (TTE) tests the ability of an LLM to understand and simulate the execution of a given multi-step reasoning system. Subsequently, temporal causal evaluation (TCE) tests an LLMâ€™s ability to perform multi-step causal reasoning and to distill cause-and-effect relations from complex systems. We find that models score 65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art LLMs clearly understand the TCE task but perform poorly as system complexity increases. Our code is available at our \href{<a target="_blank" rel="noopener" href="https://github.com/nik-hz/tempobench%7D%7BGitHub">https://github.com/nik-hz/tempobench}{GitHub</a> repository}. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¶Šæ¥è¶Šè¶…è¶Šäººç±»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸ºäº†æé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬è¦ä¹ˆä¾èµ–äºä¸“é—¨ç”Ÿæˆçš„æ•°æ®é›†ï¼Œè¦ä¹ˆä¾èµ–äºè¯¸å¦‚Leanè¯æ˜åŠ©æ‰‹ä¹‹ç±»çš„å½¢å¼åŒ–æ•°å­¦è¯æ˜ç³»ç»Ÿã€‚è™½ç„¶ä¸“é—¨ç”Ÿæˆçš„æ–¹æ³•å¯ä»¥æ•æ‰ç°å®ä¸–ç•Œä¸­æ¨ç†è¿‡ç¨‹çš„å†³ç­–é“¾ï¼Œä½†å®ƒä»¬å¯èƒ½åœ¨æ‰€è¦†ç›–çš„æ¨ç†ç©ºé—´ä¸­ç¼–ç äº†ä¸€äº›æ— æ„è¯†çš„åè§ï¼›åŒæ—¶å®ƒä»¬ä¹Ÿæ— æ³•è¿›è¡Œå½¢å¼åŒ–éªŒè¯ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒLeanè¿™æ ·çš„ç³»ç»Ÿå¯ä»¥ä¿è¯å¯éªŒè¯æ€§ï¼Œä½†ä¸é€‚åˆæ•æ‰åŸºäºä»£ç†çš„å†³ç­–é“¾ä»»åŠ¡çš„æ€§è´¨ã€‚è¿™é€ æˆäº†ä¸šåŠ¡ä»£ç†æˆ–ä»£ç åŠ©æ‰‹ç­‰åŠŸèƒ½çš„æ€§èƒ½å·®è·ï¼Œä»¥åŠLLMæ¨ç†åŸºå‡†æµ‹è¯•çš„å®ç”¨æ€§ä¸è¶³ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•åœ¨æ¨ç†ç»“æ„æˆ–ç°å®ä¸–ç•Œå¯¹é½æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚æˆ‘ä»¬å¼•å…¥äº†TempoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ­£å¼ä¾æ®å’Œå¯éªŒè¯çš„è¯Šæ–­åŸºå‡†ï¼Œå¯ä»¥é€šè¿‡å‚æ•°åŒ–éš¾åº¦æ¥ç³»ç»Ÿåœ°åˆ†æLLMså¦‚ä½•è¿›è¡Œæ¨ç†ã€‚TempoBenchä½¿ç”¨ä¸¤ä¸ªè¯„ä¼°åŸºå‡†æ¥åˆ†è§£æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæ—¶é—´è½¨è¿¹è¯„ä¼°ï¼ˆTTEï¼‰æµ‹è¯•LLMç†è§£å’Œæ¨¡æ‹Ÿç»™å®šå¤šæ­¥éª¤æ¨ç†ç³»ç»Ÿçš„æ‰§è¡Œèƒ½åŠ›ã€‚éšåï¼Œæ—¶é—´å› æœè¯„ä¼°ï¼ˆTCEï¼‰æµ‹è¯•LLMæ‰§è¡Œå¤šæ­¥éª¤å› æœæ¨ç†çš„èƒ½åŠ›ï¼Œå¹¶ä»å¤æ‚ç³»ç»Ÿä¸­æç‚¼å› æœå…³ç³»ã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨TCE-normalä¸Šçš„å¾—åˆ†ä¸º65.6%ï¼Œåœ¨TCE-hardä¸Šçš„å¾—åˆ†ä¸º7.5%ã€‚è¿™è¡¨æ˜æœ€å…ˆè¿›çš„LLMsæ˜æ˜¾ç†è§£TCEä»»åŠ¡ï¼Œä½†éšç€ç³»ç»Ÿå¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶è¡¨ç°è¾ƒå·®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨\href{<a target="_blank" rel="noopener" href="https://github.com/nik-hz/tempobench%7D%7BGitHub%E4%BB%93%E5%BA%93%7D%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nik-hz/tempobench}{GitHubä»“åº“}ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27544v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¶…è¶Šäº†äººç±»çš„è¡¨ç°ï¼Œä½†ä¸ºäº†æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬ä¾èµ–äºç‰¹å®šæƒ…å¢ƒç”Ÿæˆçš„æ•°æ®é›†æˆ–å½¢å¼åŒ–æ•°å­¦è¯æ˜ç³»ç»Ÿï¼Œå¦‚Leanè¯æ˜åŠ©æ‰‹ã€‚ç‰¹å®šæƒ…å¢ƒç”Ÿæˆçš„æ–¹æ³•èƒ½å¤Ÿæ•æ‰ç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ï¼Œä½†å¯èƒ½å¼•å…¥ä¸€äº›æ„å¤–çš„åè§ï¼›è€Œå½¢å¼åŒ–ç³»ç»Ÿè™½ç„¶å¯ä»¥éªŒè¯ï¼Œä½†ä¸æ“…é•¿æ•æ‰åŸºäºä»£ç†çš„å†³ç­–é“¾ä»»åŠ¡ã€‚å› æ­¤ï¼Œå­˜åœ¨ä¸šåŠ¡ä»£ç†æˆ–ä»£ç åŠ©æ‰‹ç­‰åŠŸèƒ½çš„æ€§èƒ½å·®è·ï¼Œä»¥åŠLLMæ¨ç†åŸºå‡†æµ‹è¯•çš„å®ç”¨æ€§å’Œç»“æ„åˆç†æ€§ä¸Šçš„ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†TempoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå½¢å¼åŒ–ä¸”å¯éªŒè¯çš„è¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥å‚æ•°åŒ–éš¾åº¦ï¼Œä»¥ç³»ç»Ÿåœ°åˆ†æLLMçš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªè¯„ä¼°åŸºå‡†ï¼Œå³æ—¶é—´è½¨è¿¹è¯„ä¼°ï¼ˆTTEï¼‰å’Œæ—¶é—´å› æœè¯„ä¼°ï¼ˆTCEï¼‰ï¼Œæ¥æµ‹è¯•LLMçš„æ¨ç†èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨TCEæ­£å¸¸ä»»åŠ¡ä¸Šçš„å¾—åˆ†ä¸º65.6%ï¼Œåœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„å¾—åˆ†ä¸º7.5%ï¼Œè¡¨æ˜éšç€ç³»ç»Ÿå¤æ‚æ€§çš„å¢åŠ ï¼ŒLLMçš„ç†è§£èƒ½åŠ›æœ‰æ‰€ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¶…è¶Šäººç±»çš„èƒ½åŠ›ï¼Œä½†åœ¨æ¨ç†æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>ç°æœ‰æé«˜LLMæ¨ç†èƒ½åŠ›çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ç‰¹å®šæƒ…å¢ƒç”Ÿæˆçš„æ•°æ®é›†å’Œå½¢å¼åŒ–æ•°å­¦è¯æ˜ç³»ç»Ÿï¼Œä½†å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>å­˜åœ¨ä¸šåŠ¡ä»£ç†æˆ–ä»£ç åŠ©æ‰‹ç­‰åŠŸèƒ½çš„æ€§èƒ½å·®è·ï¼Œä»¥åŠLLMæ¨ç†åŸºå‡†æµ‹è¯•çš„åˆç†æ€§é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†TempoBenchä½œä¸ºé¦–ä¸ªå½¢å¼åŒ–ä¸”å¯éªŒè¯çš„è¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œä»¥ç³»ç»Ÿåœ°åˆ†æLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TempoBenchåŒ…æ‹¬ä¸¤ä¸ªè¯„ä¼°åŸºå‡†ï¼šæ—¶é—´è½¨è¿¹è¯„ä¼°ï¼ˆTTEï¼‰å’Œæ—¶é—´å› æœè¯„ä¼°ï¼ˆTCEï¼‰ã€‚</li>
<li>LLMåœ¨TCEæ­£å¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„å¾—åˆ†è¾ƒä½ï¼Œè¡¨æ˜å…¶å¤„ç†å¤æ‚ç³»ç»Ÿæ—¶çš„ç†è§£èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a0107c10b0ba8be67056c2114b1d11bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278340&auth_key=1762278340-0-0-beddf6a845709a432e8a3444d56a2cc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4db07757aa5ac0e948abfc069d185434~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278347&auth_key=1762278347-0-0-1e127b3e821ef3aac1df48ef3c8988c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ccc161c7103567b18d5164cb43b9a1e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278353&auth_key=1762278353-0-0-16bafb8c220e9adbff186c52f3a4752d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86d7ae926573ba54a1e280b8383c8f2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278360&auth_key=1762278360-0-0-a8df3dcbeefa5620399bc66620443a57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db22ab71129855bbba23564165ff76ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278367&auth_key=1762278367-0-0-fb8e47c1b3de30057e65ceca2b4d45d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffuse-Thinking-Exploring-Diffusion-Language-Models-as-Efficient-Thought-Proposers-for-Reasoning"><a href="#Diffuse-Thinking-Exploring-Diffusion-Language-Models-as-Efficient-Thought-Proposers-for-Reasoning" class="headerlink" title="Diffuse Thinking: Exploring Diffusion Language Models as Efficient   Thought Proposers for Reasoning"></a>Diffuse Thinking: Exploring Diffusion Language Models as Efficient   Thought Proposers for Reasoning</h2><p><strong>Authors:Chenyang Shao, Sijian Ren, Fengli Xu, Yong Li</strong></p>
<p>In recent years, large language models (LLMs) have witnessed remarkable advancements, with the test-time scaling law consistently enhancing the reasoning capabilities. Through systematic evaluation and exploration of a diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to generate deliberate reasoning steps, thereby substantially enhancing reasoning accuracy. However, LLMsâ€™ autoregressive generation paradigm results in reasoning performance scaling sub-optimally with test-time computation, often requiring excessive computational overhead to propose thoughts while yielding only marginal performance gains. In contrast, diffusion language models (DLMs) can efficiently produce diverse samples through parallel denoising in a single forward pass, inspiring us to leverage them for proposing intermediate thoughts, thereby alleviating the computational burden associated with autoregressive generation while maintaining quality. In this work, we propose an efficient collaborative reasoning framework, leveraging DLMs to generate candidate thoughts and LLMs to evaluate their quality. Experiments across diverse benchmarks demonstrate that our framework achieves strong performance in complex reasoning tasks, offering a promising direction for future research. Our code is open-source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Diffuse-Thinking-EC60">https://anonymous.4open.science/r/Diffuse-Thinking-EC60</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œæµ‹è¯•æ—¶çš„è§„æ¨¡å®šå¾‹æŒç»­æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸€ç³»åˆ—ä¸­é—´æ€æƒ³çš„ç³»ç»Ÿè¯„ä¼°å’Œæ¢ç´¢ï¼ŒLLMå±•ç°å‡ºäº§ç”Ÿæœ‰æ„è¯†çš„æ¨ç†æ­¥éª¤çš„æ½œåŠ›ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼ŒLLMçš„è‡ªå›å½’ç”Ÿæˆæ¨¡å¼å¯¼è‡´æ¨ç†æ€§èƒ½åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—ä¸Šæ— æ³•ä»¥æœ€ä¼˜çš„æ–¹å¼æ‰©å±•ï¼Œé€šå¸¸éœ€è¦è¿‡å¤šçš„è®¡ç®—å¼€é”€æ¥æå‡ºæ€æƒ³ï¼Œè€Œè·å¾—çš„æ€§èƒ½æå‡åªæœ‰è½»å¾®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰å¯ä»¥åœ¨å•æ¬¡å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­é€šè¿‡å¹¶è¡Œå»å™ªæœ‰æ•ˆåœ°ç”Ÿæˆå„ç§æ ·æœ¬ï¼Œè¿™å¯å‘æˆ‘ä»¬åˆ©ç”¨å®ƒä»¬æ¥æå‡ºä¸­é—´æ€æƒ³ï¼Œä»è€Œåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å‡è½»ä¸è‡ªå›å½’ç”Ÿæˆç›¸å…³çš„è®¡ç®—è´Ÿæ‹…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„åä½œæ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨DLMç”Ÿæˆå€™é€‰æ€æƒ³ï¼Œå¹¶åˆ©ç”¨LLMè¯„ä¼°å…¶è´¨é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Diffuse-Thinking-EC60">https://anonymous.4open.science/r/Diffuse-Thinking-EC60</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27469v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿‘å¹´æ¥çš„æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å®šå¾‹å¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsçš„è‡ªå›å½’ç”Ÿæˆæ¨¡å¼å¯¼è‡´æ¨ç†æ€§èƒ½åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—å¹¶ä¸ç†æƒ³ï¼Œå¸¸å¸¸éœ€è¦è¿‡å¤šçš„è®¡ç®—å¼€é”€æ¥æå‡ºæ€è€ƒï¼Œä¸”ä»…è·å¾—è½»å¾®çš„æ€§èƒ½æå‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’ä¸­çš„å¹¶è¡Œå»å™ªèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå¤šæ ·åŒ–æ ·æœ¬ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨DLMsç”Ÿæˆå€™é€‰æ€è€ƒã€LLMsè¯„ä¼°å…¶è´¨é‡çš„åä½œæ¨ç†æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å®šå¾‹å¢å¼ºäº†è¿™ä¸€èƒ½åŠ›ã€‚</li>
<li>LLMsçš„è‡ªå›å½’ç”Ÿæˆæ¨¡å¼åœ¨æµ‹è¯•æ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦è¿‡å¤šçš„è®¡ç®—èµ„æºæ¥äº§ç”Ÿæ€è€ƒã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰å¯ä»¥é€šè¿‡å¹¶è¡Œå»å™ªé«˜æ•ˆç”Ÿæˆå¤šæ ·åŒ–æ ·æœ¬ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä½œæ¨ç†æ¡†æ¶ï¼Œç»“åˆDLMså’ŒLLMsçš„ä¼˜åŠ¿ï¼ŒDLMsè´Ÿè´£ç”Ÿæˆå€™é€‰æ€è€ƒï¼ŒLLMsè´Ÿè´£è¯„ä¼°è´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶çš„ä»£ç å·²å¼€æºï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºæé«˜è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c68f978aeb8be21af2583aed3d02260b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278375&auth_key=1762278375-0-0-d2bc7cdc20f289852fd73a540fd1b0cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b17804a9589fbe68209c4ff210ed373~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278381&auth_key=1762278381-0-0-30541633d8afb7116694f0f0a6d30f97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-911f8e7fd32a216645140fe8d6b3d890~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278388&auth_key=1762278388-0-0-1f4de6e64ce6d3393c95cfd9c85867bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VCORE-Variance-Controlled-Optimization-based-Reweighting-for-Chain-of-Thought-Supervision"><a href="#VCORE-Variance-Controlled-Optimization-based-Reweighting-for-Chain-of-Thought-Supervision" class="headerlink" title="VCORE: Variance-Controlled Optimization-based Reweighting for   Chain-of-Thought Supervision"></a>VCORE: Variance-Controlled Optimization-based Reweighting for   Chain-of-Thought Supervision</h2><p><strong>Authors:Xuan Gong, Senmiao Wang, Hanbo Huang, Ruoyu Sun, Shiyu Liang</strong></p>
<p>Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \textbf{V}ariance-\textbf{C}ontrolled \textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at <a target="_blank" rel="noopener" href="https://github.com/coder-gx/VCORE">https://github.com/coder-gx/VCORE</a>. </p>
<blockquote>
<p>å¯¹é•¿é“¾æ€ç»´è½¨è¿¹çš„ç²¾ç»†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæ ‡å‡†äº¤å‰ç†µæŸå¤±å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ ‡è®°ï¼Œå¿½ç•¥äº†å®ƒä»¬åœ¨æ¨ç†è½¨è¿¹ä¸­çš„ä¸åŒè´¡çŒ®ã€‚è¿™ç§ç»Ÿä¸€çš„å¤„ç†æ–¹å¼å¯¼è‡´ç›‘ç£åˆ†é…ä¸å½“å’Œæ³›åŒ–èƒ½åŠ›å¼±ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿ç¯‡æ¨ç†ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ–¹å·®æ§åˆ¶ä¼˜åŒ–çš„åŠ æƒï¼ˆVCOREï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶ï¼Œå°†æ€ç»´è½¨è¿¹çš„ç›‘ç£é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨ä¼˜åŒ–ç†è®ºçš„è§’åº¦ï¼ŒVCOREèƒ½å¤ŸåŸåˆ™æ€§å’Œè‡ªé€‚åº”åœ°åˆ†é…æ ‡è®°é—´çš„ç›‘ç£ï¼Œä»è€Œæ›´ç´§å¯†åœ°å°†è®­ç»ƒç›®æ ‡ä¸ç¨³å¥æ¨ç†æ³›åŒ–çš„ç›®æ ‡å¯¹é½ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒVCOREä¸€è‡´åœ°ä¼˜äºç°æœ‰çš„æ ‡è®°åŠ æƒæ–¹æ³•ã€‚åœ¨é¢†åŸŸå†…å¤–ç¯å¢ƒä¸‹ï¼ŒVCOREåœ¨æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸Šéƒ½å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œé€‚ç”¨äºQwen3ç³»åˆ—ï¼ˆ4Bã€8Bã€32Bï¼‰å’ŒLLaMA-3.1-8B-Instructæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜VCOREä¸ºåç»­å¼ºåŒ–å­¦ä¹ æä¾›äº†æ›´æœ‰æ•ˆçš„åˆå§‹åŒ–ï¼Œä¸ºæå‡LLMçš„æ¨ç†èƒ½åŠ›å¥ å®šäº†åšå®åŸºç¡€ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/coder-gx/VCORE%E4%B8%8A%E3%80%82">https://github.com/coder-gx/VCOREä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27462v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç²¾ç»†ç›‘ç£è°ƒæ•´ï¼ˆSFTï¼‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰è½¨è¿¹ä¸Šçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œæ ‡å‡†äº¤å‰ç†µæŸå¤±å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ ‡è®°ï¼Œå¿½ç•¥äº†å®ƒä»¬åœ¨æ¨ç†è½¨è¿¹ä¸­çš„ä¸åŒè´¡çŒ®ï¼Œå¯¼è‡´ç›‘ç£åˆ†é…ä¸å½“å’Œæ³›åŒ–èƒ½åŠ›å¼±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºä¼˜åŒ–ç†è®ºçš„æ–¹å·®æ§åˆ¶ä¼˜åŒ–é‡åŠ æƒï¼ˆVCOREï¼‰æ¡†æ¶ï¼Œå°†CoTç›‘ç£é‡æ–°å®šä¹‰ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–ç†è®ºè§†è§’ï¼ŒVCOREå®ç°äº†å¯¹æ ‡è®°çš„çµæ´»ä¸”æœ‰é’ˆå¯¹æ€§çš„ç›‘ç£åˆ†é…ï¼Œæ›´å¥½åœ°ç¬¦åˆäº†æ¨ç†æ³›åŒ–çš„ç›®æ ‡ã€‚ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒVCOREåœ¨å¤šç§æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºä¸åŒé¢†åŸŸå’Œæ¨¡å‹è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVCOREè¿˜ä¸ºåç»­çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†æ›´æœ‰æ•ˆçš„åˆå§‹åŒ–ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VCOREæ¡†æ¶è§£å†³äº†æ ‡å‡†äº¤å‰ç†µæŸå¤±åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç›‘ç£åˆ†é…é—®é¢˜ã€‚</li>
<li>VCOREé€šè¿‡å¯¹æ¨ç†è½¨è¿¹ä¸­çš„ä¸åŒæ ‡è®°è´¡çŒ®è¿›è¡Œå·®å¼‚åŒ–å¤„ç†ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>VCOREæ¡†æ¶é‡‡ç”¨ä¼˜åŒ–ç†è®ºè§†è§’è¿›è¡Œç²¾ç»†åŒ–ç›‘ç£è°ƒæ•´ã€‚</li>
<li>VCOREæ¡†æ¶åœ¨å¤šç§æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é€‚ç”¨äºä¸åŒè§„æ¨¡å’Œé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>VCOREä½œä¸ºæœ‰æ•ˆçš„åˆå§‹åŒ–æ–¹æ³•ä¸ºåç»­å¼ºåŒ–å­¦ä¹ æ‰“ä¸‹åŸºç¡€ã€‚</li>
<li>VCOREæ¡†æ¶æœ‰åŠ©äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-423510d63c3026b2400e5749b786329e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278395&auth_key=1762278395-0-0-1f956174e9ac3d809f994412c01cf79a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d9e60b7e26755470de760090ca2c1f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278403&auth_key=1762278403-0-0-09df0ca7774691c47e4dfb28b32435cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GeoFM-Enhancing-Geometric-Reasoning-of-MLLMs-via-Synthetic-Data-Generation-through-Formal-Language"><a href="#GeoFM-Enhancing-Geometric-Reasoning-of-MLLMs-via-Synthetic-Data-Generation-through-Formal-Language" class="headerlink" title="GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data   Generation through Formal Language"></a>GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data   Generation through Formal Language</h2><p><strong>Authors:Yuhao Zhang, Dingxin Hu, Tinghao Yu, Hao Liu, Yiting Liu</strong></p>
<p>Multi-modal Large Language Models (MLLMs) have gained significant attention in both academia and industry for their capabilities in handling multi-modal tasks. However, these models face challenges in mathematical geometric reasoning due to the scarcity of high-quality geometric data. To address this issue, synthetic geometric data has become an essential strategy. Current methods for generating synthetic geometric data involve rephrasing or expanding existing problems and utilizing predefined rules and templates to create geometric images and problems. However, these approaches often produce data that lacks diversity or is prone to noise. Additionally, the geometric images synthesized by existing methods tend to exhibit limited variation and deviate significantly from authentic geometric diagrams. To overcome these limitations, we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses formal languages to explore combinations of conditions within metric space, generating high-fidelity geometric problems that differ from the originals while ensuring correctness through a symbolic engine. Experimental results show that our synthetic data significantly outperforms existing methods. The model trained with our data surpass the proprietary GPT-4o model by 18.7% on geometry problem-solving tasks in MathVista and by 16.5% on GeoQA. Additionally, it exceeds the performance of a leading open-source model by 5.7% on MathVista and by 2.7% on GeoQA. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å­¦æœ¯å’Œä¸šç•Œä¸­éƒ½å—åˆ°äº†æå¤§çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ•°å­¦å‡ ä½•æ¨ç†æ–¹é¢é¢ä¸´ç€æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºé«˜è´¨é‡å‡ ä½•æ•°æ®çš„ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆæˆå‡ ä½•æ•°æ®å·²ç»æˆä¸ºä¸€ç§é‡è¦çš„ç­–ç•¥ã€‚å½“å‰ç”Ÿæˆåˆæˆå‡ ä½•æ•°æ®çš„æ–¹æ³•åŒ…æ‹¬é‡æ–°è¡¨è¿°æˆ–æ‰©å±•ç°æœ‰é—®é¢˜ï¼Œä»¥åŠåˆ©ç”¨é¢„å…ˆå®šä¹‰çš„è§„åˆ™å’Œæ¨¡æ¿æ¥åˆ›å»ºå‡ ä½•å›¾åƒå’Œé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸äº§ç”Ÿçš„æ•°æ®ç¼ºä¹å¤šæ ·æ€§æˆ–å®¹æ˜“å¼•å…¥å™ªå£°ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åˆæˆçš„å‡ ä½•å›¾åƒå¾€å¾€å˜åŒ–æœ‰é™ï¼Œä¸çœŸå®çš„å‡ ä½•å›¾è¡¨å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GeoFMï¼Œä¸€ç§åˆæˆå‡ ä½•æ•°æ®çš„æ–°æ–¹æ³•ã€‚GeoFMä½¿ç”¨å½¢å¼è¯­è¨€æ¥æ¢ç´¢åº¦é‡ç©ºé—´å†…çš„æ¡ä»¶ç»„åˆï¼Œç”Ÿæˆä¸åŸå§‹é—®é¢˜ä¸åŒçš„é«˜ä¿çœŸå‡ ä½•é—®é¢˜ï¼Œå¹¶é€šè¿‡ç¬¦å·å¼•æ“ç¡®ä¿æ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®åœ¨å¤šä¸ªæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä½¿ç”¨æˆ‘ä»¬æ•°æ®çš„æ¨¡å‹åœ¨MathVistaçš„å‡ ä½•é—®é¢˜è§£å†³ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¸“æœ‰GPT-4oæ¨¡å‹ï¼Œæé«˜äº†18.7%ï¼Œåœ¨GeoQAä¸Šæé«˜äº†16.5%ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è¶…è¿‡äº†ä¸€ä¸ªé¢†å…ˆçš„å¼€æºæ¨¡å‹åœ¨MathVistaä¸Šçš„æ€§èƒ½æé«˜äº†5.7%ï¼Œåœ¨GeoQAä¸Šçš„æ€§èƒ½æé«˜äº†2.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨æ•°å­¦å‡ ä½•æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé«˜è´¨é‡å‡ ä½•æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆæˆå‡ ä½•æ•°æ®å·²æˆä¸ºä¸€ç§é‡è¦ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„æ•°æ®ç¼ºä¹å¤šæ ·æ€§ä¸”æ˜“äº§ç”Ÿå™ªå£°ã€‚ä¸ºæ­¤ï¼Œæå‡ºGeoFMè¿™ä¸€æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å½¢å¼è¯­è¨€æ¢ç´¢åº¦é‡ç©ºé—´å†…çš„æ¡ä»¶ç»„åˆï¼Œç”Ÿæˆé«˜ä¿çœŸå‡ ä½•é—®é¢˜ï¼ŒåŒæ—¶ç¡®ä¿æ­£ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeoFMåˆæˆçš„æ•°æ®æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½¿ç”¨æ­¤æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨MathVistaå’ŒGeoQAçš„å‡ ä½•é—®é¢˜è§£ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†GPT-4oæ¨¡å‹ï¼Œå¹¶é¢†å…ˆå¼€æºæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶å—åˆ°å…³æ³¨ï¼Œä½†åœ¨æ•°å­¦å‡ ä½•æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åˆæˆå‡ ä½•æ•°æ®çš„æ–¹æ³•ç¼ºä¹å¤šæ ·æ€§å’Œæ˜“å—å™ªå£°å½±å“ã€‚</li>
<li>GeoFMæ˜¯ä¸€ç§åˆ©ç”¨å½¢å¼è¯­è¨€ç”Ÿæˆé«˜ä¿çœŸå‡ ä½•æ•°æ®çš„æ–°æ–¹æ³•ï¼Œèƒ½æ¢ç´¢åº¦é‡ç©ºé—´å†…çš„æ¡ä»¶ç»„åˆã€‚</li>
<li>GeoFMç”Ÿæˆçš„æ•°æ®æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å‡ ä½•é—®é¢˜è§£ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨GeoFMæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°è¶…è¿‡GPT-4oæ¨¡å‹å’Œå…¶ä»–é¢†å…ˆæ¨¡å‹ã€‚</li>
<li>GeoFMç¡®ä¿äº†ç”Ÿæˆé—®é¢˜çš„æ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-88f60666b8df4db48c45eecbc79ef072~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278410&auth_key=1762278410-0-0-0caafe988ac1636b7663fc20d7eed223&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a5113a9cd4033844e9513fff04e7e8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278417&auth_key=1762278417-0-0-5c510751c11936fcf7ae51436537f6be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf464299d785ea3857c1f8b4d5f296e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278424&auth_key=1762278424-0-0-8ab27909b4eab14d9bb715ac3f18c9ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-921c6a5505519a454bbe0521c952d02d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278431&auth_key=1762278431-0-0-5163e6ccdcb85c55e804fed37d199eeb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DeepCompress-A-Dual-Reward-Strategy-for-Dynamically-Exploring-and-Compressing-Reasoning-Chains"><a href="#DeepCompress-A-Dual-Reward-Strategy-for-Dynamically-Exploring-and-Compressing-Reasoning-Chains" class="headerlink" title="DeepCompress: A Dual Reward Strategy for Dynamically Exploring and   Compressing Reasoning Chains"></a>DeepCompress: A Dual Reward Strategy for Dynamically Exploring and   Compressing Reasoning Chains</h2><p><strong>Authors:Tian Liang, Wenxiang Jiao, Zhiwei He, Jiahao Xu, Haitao Mi, Dong Yu</strong></p>
<p>Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like <code>overthinking&#39;&#39; simple problems and </code>underthinkingâ€™â€™ complex ones. While existing methods that use supervised fine-tuning<del>(SFT) or reinforcement learning</del>(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as <code>Simple&#39;&#39; or </code>Hardâ€™â€™ in real-time based on the modelâ€™s evolving capability. It encourages shorter, more efficient reasoning for <code>Simple&#39;&#39; problems while promoting longer, more exploratory thought chains for </code>Hardâ€™â€™ problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è™½ç„¶å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨è®¤çŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¦‚â€œè¿‡åº¦æ€è€ƒâ€ç®€å•é—®é¢˜ä»¥åŠâ€œç¼ºä¹æ·±åº¦æ€è€ƒâ€å¤æ‚é—®é¢˜ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–åŸºäºæ ‡è®°é•¿åº¦çš„å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šåœ¨å‡†ç¡®æ€§æ–¹é¢æœ‰æ‰€æŸå¤±ã€‚æœ¬æ–‡ä»‹ç»äº†\textbf{DeepCompress}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶æé«˜LRMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬è´¨ç–‘ä¸€è´¯å€¾å‘äºè¾ƒçŸ­æ¨ç†è·¯å¾„çš„æµè¡Œæ–¹æ³•ï¼Œå¹¶è¡¨æ˜å¯¹äºéš¾é¢˜ï¼Œè¾ƒé•¿çš„ç­”æ¡ˆå¯ä»¥åŒ…å«æ›´å¹¿æ³›çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚DeepCompressé‡‡ç”¨è‡ªé€‚åº”é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®æ¨¡å‹çš„ä¸æ–­è¿›åŒ–èƒ½åŠ›å®æ—¶å°†é—®é¢˜åˆ†ç±»ä¸ºâ€œç®€å•â€æˆ–â€œå›°éš¾â€ã€‚å®ƒé¼“åŠ±å¯¹â€œç®€å•â€é—®é¢˜è¿›è¡Œç®€çŸ­ã€é«˜æ•ˆçš„æ¨ç†ï¼ŒåŒæ—¶ä¿ƒè¿›å¯¹â€œå›°éš¾â€é—®é¢˜è¿›è¡Œæ›´é•¿æ—¶é—´ã€æ›´æ¢ç´¢æ€§çš„æ€è€ƒé“¾ã€‚è¿™ç§åŒé‡å¥–åŠ±ç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åœ°è°ƒæ•´å…¶æ€ç»´é“¾ï¼ˆCoTï¼‰çš„é•¿åº¦ï¼Œå¯¹å·²ç»ç†Ÿç»ƒæŒæ¡çš„é—®é¢˜è¿›è¡Œæ¨ç†å‹ç¼©ï¼Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜è¿›è¡Œæ¨ç†æ‰©å±•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepCompresså§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ ‡è®°æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27419v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è™½ç„¶å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨è®¤çŸ¥æ•ˆç‡é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒç®€å•é—®é¢˜è€Œå¿½è§†å¤æ‚é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é…åˆä»¤ç‰Œé•¿åº¦å¥–åŠ±å¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†å¾€å¾€ä¼šé™ä½å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºDeepCompressæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æé«˜LRMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚DeepCompressé‡‡ç”¨è‡ªé€‚åº”é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®æ¨¡å‹çš„ä¸æ–­è¿›åŒ–èƒ½åŠ›å®æ—¶å°†é—®é¢˜åˆ†ç±»ä¸ºâ€œç®€å•â€æˆ–â€œå›°éš¾â€ã€‚å¯¹äºç®€å•é—®é¢˜ï¼Œå®ƒé¼“åŠ±ç®€çŸ­é«˜æ•ˆçš„æ¨ç†ï¼›å¯¹äºå¤æ‚é—®é¢˜ï¼Œå®ƒä¿ƒè¿›æ›´é•¿çš„æ¢ç´¢æ€§æ€ç»´é“¾ã€‚è¿™ç§åŒé‡å¥–åŠ±ç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è°ƒæ•´å…¶æ€ç»´é“¾é•¿åº¦ï¼Œå‹ç¼©å¯¹æŒæ¡è‰¯å¥½çš„é—®é¢˜çš„æ¨ç†ï¼Œå¹¶æ‰©å±•å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepCompressåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸ŠæŒç»­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†é«˜å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„ä»¤ç‰Œæ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é¢ä¸´è®¤çŸ¥æ•ˆç‡é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒç®€å•é—®é¢˜å’Œå¿½è§†å¤æ‚é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æé«˜æ•ˆç‡æ—¶å¯èƒ½ä¼šé™ä½å‡†ç¡®æ€§ã€‚</li>
<li>DeepCompressæ¡†æ¶è¢«å¼•å…¥ï¼Œæ—¨åœ¨åŒæ—¶æé«˜LRMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>DeepCompressé‡‡ç”¨è‡ªé€‚åº”é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®é—®é¢˜çš„éš¾æ˜“ç¨‹åº¦å®æ—¶è°ƒæ•´æ¨ç†é•¿åº¦ã€‚</li>
<li>å¯¹äºç®€å•é—®é¢˜ï¼ŒDeepCompressé¼“åŠ±ç®€çŸ­é«˜æ•ˆçš„æ¨ç†ï¼›å¯¹äºå¤æ‚é—®é¢˜ï¼Œå®ƒä¿ƒè¿›æ›´é•¿çš„æ¢ç´¢æ€§æ€ç»´é“¾ã€‚</li>
<li>DeepCompressé‡‡ç”¨åŒé‡å¥–åŠ±ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è°ƒæ•´æ€ç»´é“¾é•¿åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6d995a1e5aacfe3dcaf2d2d7f28d6ed2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278438&auth_key=1762278438-0-0-8f2ef89668ad1a1f1e3ee32542d9e982&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f92a083f1c4d8ffa3774d2ec86746c11~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278446&auth_key=1762278446-0-0-3566322458d4bbd962ad735cacd8a7e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0770ec310409285f60c9df84de4a4ff9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278453&auth_key=1762278453-0-0-04fbf416e4162a34d9dca1a59fed958c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MedCalc-Eval-and-MedCalc-Env-Advancing-Medical-Calculation-Capabilities-of-Large-Language-Models"><a href="#MedCalc-Eval-and-MedCalc-Env-Advancing-Medical-Calculation-Capabilities-of-Large-Language-Models" class="headerlink" title="MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities   of Large Language Models"></a>MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities   of Large Language Models</h2><p><strong>Authors:Kangkun Mao, Jinru Ding, Jiayuan Chen, Mouxiao Bian, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu</strong></p>
<p>As large language models (LLMs) enter the medical domain, most benchmarks evaluate them on question answering or descriptive reasoning, overlooking quantitative reasoning critical to clinical decision-making. Existing datasets like MedCalc-Bench cover few calculation tasks and fail to reflect real-world computational scenarios.   We introduce MedCalc-Eval, the largest benchmark for assessing LLMsâ€™ medical calculation abilities, comprising 700+ tasks across two types: equation-based (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale). These tasks span diverse specialties including internal medicine, surgery, pediatrics, and cardiology, offering a broader and more challenging evaluation setting.   To improve performance, we further develop MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, enabling multi-step clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this environment achieves state-of-the-art results on MedCalc-Eval, with notable gains in numerical sensitivity, formula selection, and reasoning robustness. Remaining challenges include unit conversion, multi-condition logic, and contextual understanding.   Code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/maokangkun/MedCalc-Eval">https://github.com/maokangkun/MedCalc-Eval</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å…¥åŒ»å­¦é¢†åŸŸï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•éƒ½åœ¨é—®ç­”æˆ–æè¿°æ€§æ¨ç†æ–¹é¢å¯¹å®ƒä»¬è¿›è¡Œè¯„ä¼°ï¼Œå¿½è§†äº†ä¸´åºŠå†³ç­–è‡³å…³é‡è¦çš„å®šé‡æ¨ç†ã€‚ç°æœ‰æ•°æ®é›†å¦‚MedCalc-Benchæ¶µç›–çš„è®¡ç®—ä»»åŠ¡è¾ƒå°‘ï¼Œæ— æ³•åæ˜ ç°å®ä¸–ç•Œçš„è®¡ç®—åœºæ™¯ã€‚æˆ‘ä»¬æ¨å‡ºMedCalc-Evalï¼Œè¿™æ˜¯è¯„ä¼°LLMåŒ»ç–—è®¡ç®—èƒ½åŠ›çš„å¤§å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«700å¤šä¸ªä»»åŠ¡ï¼Œåˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºæ–¹ç¨‹çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚Cockcroft-Gaultã€BMIã€BSAï¼‰å’ŒåŸºäºè§„åˆ™è¯„åˆ†ç³»ç»Ÿçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚Apgarã€Glasgowæ˜è¿·é‡è¡¨ï¼‰ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–å†…ç§‘ã€å¤–ç§‘ã€å„¿ç§‘å’Œå¿ƒè„ç—…å­¦ç­‰å¤šä¸ªä¸“ä¸šï¼Œæä¾›æ›´å¹¿æ³›å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°ç¯å¢ƒã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†åŸºäºInternBootcampæ¡†æ¶çš„MedCalc-Envå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œæ”¯æŒå¤šæ­¥éª¤çš„ä¸´åºŠæ¨ç†å’Œè§„åˆ’ã€‚åœ¨æ­¤ç¯å¢ƒä¸­å¯¹Qwen2.5-32Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨MedCalc-Evalä¸Šè¾¾åˆ°æœ€æ–°ç»“æœï¼Œåœ¨æ•°å­—æ•æ„Ÿåº¦ã€å…¬å¼é€‰æ‹©å’Œæ¨ç†ç¨³å¥æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å‰©ä½™çš„æŒ‘æˆ˜åŒ…æ‹¬å•ä½è½¬æ¢ã€å¤šæ¡ä»¶é€»è¾‘å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/maokangkun/MedCalc-Eval%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/maokangkun/MedCalc-Evalè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27267v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å…¥åŒ»ç–—é¢†åŸŸï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¤§å¤šä¾§é‡äºé—®ç­”æˆ–æè¿°æ€§æ¨ç†ï¼Œå¿½ç•¥äº†ä¸´åºŠå†³ç­–è‡³å…³é‡è¦çš„å®šé‡æ¨ç†ã€‚æœ¬æ–‡ä»‹ç»äº†MedCalc-Evalï¼Œè¿™æ˜¯è¯„ä¼°LLMåŒ»ç–—è®¡ç®—èƒ½åŠ›çš„æ–°åŸºå‡†ï¼ŒåŒ…å«700å¤šä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ–¹ç¨‹å’ŒåŸºäºè§„åˆ™çš„è®¡ç®—ç³»ç»Ÿï¼Œåæ˜ çœŸå®ä¸–ç•Œçš„è®¡ç®—åœºæ™¯ã€‚åŒæ—¶ï¼Œå¼€å‘äº†MedCalc-Envç¯å¢ƒï¼Œç”¨äºæé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å§‹è¿›å…¥åŒ»ç–—é¢†åŸŸï¼Œä½†è¯„ä¼°åŸºå‡†æœªå……åˆ†æ¶µç›–åŒ»ç–—è®¡ç®—èƒ½åŠ›ã€‚</li>
<li>MedCalc-Evalæ˜¯è¯„ä¼°LLMåŒ»ç–—è®¡ç®—èƒ½åŠ›çš„æœ€æ–°åŸºå‡†ï¼ŒåŒ…å«700å¤šä¸ªä»»åŠ¡ï¼Œæ¶µç›–æ–¹ç¨‹å’ŒåŸºäºè§„åˆ™çš„è®¡ç®—ç³»ç»Ÿã€‚</li>
<li>MedCalc-Evalä»»åŠ¡æ¶µç›–å†…éƒ¨åŒ»å­¦ã€å¤–ç§‘ã€å„¿ç§‘å’Œå¿ƒè„ç—…å­¦ç­‰å¤šä¸ªä¸“ä¸šé¢†åŸŸã€‚</li>
<li>MedCalc-Envç¯å¢ƒçš„å¼€å‘æœ‰åŠ©äºæé«˜LLMçš„æ€§èƒ½ï¼Œå®ç°äº†åœ¨MedCalc-Evalä¸Šçš„æœ€ä½³ç»“æœã€‚</li>
<li>è¯¥ç¯å¢ƒèƒ½æé«˜æ¨¡å‹çš„æ•°å€¼æ•æ„Ÿåº¦ã€å…¬å¼é€‰æ‹©å’Œæ¨ç†ç¨³å¥æ€§ã€‚</li>
<li>ç›®å‰ä»é¢ä¸´å•ä½è½¬æ¢ã€å¤šæ¡ä»¶é€»è¾‘å’Œä¸Šä¸‹æ–‡ç†è§£ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6697168838718e4425dcf3fe1599214~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278460&auth_key=1762278460-0-0-d235e3af7736c40a67a3ca93746e36fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a39938812216996fa12559dad111c580~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278468&auth_key=1762278468-0-0-d9f181163b40353f41a1b7f37ad4e4d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b03026d90a157a45bb6599a4f61e0116~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278475&auth_key=1762278475-0-0-0107babba901bb7b63348f3cd86be118&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GUI-Rise-Structured-Reasoning-and-History-Summarization-for-GUI-Navigation"><a href="#GUI-Rise-Structured-Reasoning-and-History-Summarization-for-GUI-Navigation" class="headerlink" title="GUI-Rise: Structured Reasoning and History Summarization for GUI   Navigation"></a>GUI-Rise: Structured Reasoning and History Summarization for GUI   Navigation</h2><p><strong>Authors:Tao Liu, Chongyu Wang, Rongjie Li, Yingchen Yu, Xuming He, Bai Song</strong></p>
<p>While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our frameworkâ€™s ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at <a target="_blank" rel="noopener" href="https://leon022.github.io/GUI-Rise">https://leon022.github.io/GUI-Rise</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½ç„¶åœ¨GUIå¯¼èˆªä»£ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰æ–¹æ³•åœ¨è·¨åŸŸæ³›åŒ–å’Œæœ‰æ•ˆåˆ©ç”¨å†å²ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºæ¨ç†çš„æ¡†æ¶ï¼Œç³»ç»Ÿåœ°èåˆäº†ç»“æ„åŒ–æ¨ç†ã€åŠ¨ä½œé¢„æµ‹å’Œå†å²æ‘˜è¦ã€‚ç»“æ„åŒ–æ¨ç†ç»„ä»¶ç”Ÿæˆè¿è´¯çš„â€œæ€ç»´é“¾â€åˆ†æï¼Œç»“åˆè¿›åº¦ä¼°è®¡å’Œå†³ç­–æ¨ç†ï¼Œä¸ºå³æ—¶åŠ¨ä½œé¢„æµ‹å’Œæœªæ¥æ­¥éª¤çš„ç®€æ´å†å²æ‘˜è¦æä¾›ä¿¡æ¯ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªGUIä»£ç†â€”â€”GUI-Riseï¼Œé€šè¿‡ä¼ªæ ‡ç­¾è½¨è¿¹çš„ç›‘ç£å¾®è°ƒï¼Œå¹¶é‡‡ç”¨å¸¦æœ‰é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç‰¹æ®Šå¥–åŠ±ï¼ŒåŒ…æ‹¬ä¸€ä¸ªäº†è§£å†å²çš„ç›®æ ‡ï¼Œç›´æ¥å°†æ‘˜è¦è´¨é‡ä¸éšåçš„è¡ŒåŠ¨è¡¨ç°è”ç³»èµ·æ¥ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„è®­ç»ƒæ•°æ®æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–åœºæ™¯ä¸‹çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚è¿™äº›å‘ç°éªŒè¯äº†æˆ‘ä»¬æ¡†æ¶åœ¨å¤šç§GUIå¯¼èˆªä»»åŠ¡ä¸­ä¿æŒç¨³å¥æ¨ç†å’Œæ³›åŒ–çš„èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://leon022.github.io/GUI-Rise%E8%8E%B7%E5%8F%96%E3%80%82">https://leon022.github.io/GUI-Riseè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27210v1">PDF</a> Published in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æ¢è®¨äº†ä¸€ç§æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è·¨åŸŸæ³›åŒ–å’Œæœ‰æ•ˆå†å²åˆ©ç”¨èƒ½åŠ›çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†ç»“æ„åŒ–æ¨ç†ã€åŠ¨ä½œé¢„æµ‹å’Œå†å²æ‘˜è¦ç­‰åŠŸèƒ½ï¼Œé€šè¿‡ç”Ÿæˆè¿è´¯çš„æ€è€ƒé“¾åˆ†æï¼Œç»“åˆè¿›åº¦ä¼°è®¡å’Œå†³ç­–æ¨ç†ï¼Œä¸ºå³æ—¶åŠ¨ä½œé¢„æµ‹å’Œæœªæ¥æ­¥éª¤çš„ç®€æ´å†å²æ‘˜è¦æä¾›ä¿¡æ¯ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæ–‡ç« è®­ç»ƒäº†ä¸€ä¸ªGUIä»£ç†â€œGUI-Riseâ€ï¼Œåœ¨ä¼ªæ ‡ç­¾è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¹¶é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç‰¹æ®Šå¥–åŠ±ï¼ŒåŒ…æ‹¬å†å²æ„ŸçŸ¥ç›®æ ‡ï¼Œç›´æ¥å°†æ‘˜è¦è´¨é‡ä¸åç»­è¡ŒåŠ¨è¡¨ç°è”ç³»èµ·æ¥ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç›¸åŒè®­ç»ƒæ•°æ®æ¡ä»¶ä¸‹ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶åœ¨è·¨åŸŸåœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚è¿™éªŒè¯äº†æ¡†æ¶åœ¨å¤šæ ·åŒ–GUIå¯¼èˆªä»»åŠ¡ä¸­ä¿æŒç¨³å¥æ¨ç†å’Œæ³›åŒ–çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨GUIå¯¼èˆªä»£ç†æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨è·¨åŸŸæ³›åŒ–å’Œæœ‰æ•ˆå†å²åˆ©ç”¨æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶é›†æˆäº†ç»“æ„åŒ–æ¨ç†ã€åŠ¨ä½œé¢„æµ‹å’Œå†å²æ‘˜è¦ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå†å²ä¿¡æ¯åˆ©ç”¨æ•ˆæœã€‚</li>
<li>é€šè¿‡ç”Ÿæˆè¿è´¯çš„æ€è€ƒé“¾åˆ†æï¼Œè¯¥æ¡†æ¶ç»“åˆè¿›åº¦ä¼°è®¡å’Œå†³ç­–æ¨ç†ï¼Œä¸ºå³æ—¶åŠ¨ä½œé¢„æµ‹å’Œæœªæ¥æ­¥éª¤çš„ç®€æ´å†å²æ‘˜è¦æä¾›ä¿¡æ¯ã€‚</li>
<li>è®­ç»ƒäº†åä¸ºâ€œGUI-Riseâ€çš„GUIä»£ç†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¼ªæ ‡ç­¾è½¨è¿¹ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•è¢«é‡‡ç”¨ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨ç‰¹æ®Šå¥–åŠ±æœºåˆ¶ï¼ŒåŒ…æ‹¬å†å²æ„ŸçŸ¥ç›®æ ‡ï¼Œå°†æ‘˜è¦è´¨é‡ä¸åç»­è¡ŒåŠ¨è¡¨ç°ç›´æ¥å…³è”ã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶æ“…é•¿å¤„ç†è·¨åŸŸåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-818fe141ec6ea707e9000cc322ac4e18~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278482&auth_key=1762278482-0-0-e9eaf8dff32db1531944a393d392e5ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9a8a62d5bba5227906b2eb4464a8132~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278490&auth_key=1762278490-0-0-3f1d209cf397ea7e8f7a1b7503b73ada&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd0af1676f2e8c14c208de040a81c75b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278497&auth_key=1762278497-0-0-49d6509e0f8fb749ca80137157bfa969&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning"><a href="#Limits-of-Generalization-in-RLVR-Two-Case-Studies-in-Mathematical-Reasoning" class="headerlink" title="Limits of Generalization in RLVR: Two Case Studies in Mathematical   Reasoning"></a>Limits of Generalization in RLVR: Two Case Studies in Mathematical   Reasoning</h2><p><strong>Authors:Md Tanvirul Alam, Nidhi Rastogi</strong></p>
<p>Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at <a target="_blank" rel="noopener" href="https://github.com/xashru/rlvr-seq-generalization">https://github.com/xashru/rlvr-seq-generalization</a>. </p>
<blockquote>
<p>æ•°å­¦æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ä»…éœ€è¦æ­£ç¡®ç­”æ¡ˆï¼Œè¿˜éœ€è¦å¿ å®çš„æ¨ç†è¿‡ç¨‹ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºæ­¤ç±»èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å…¶ä¿ƒè¿›çœŸæ­£æ¨ç†çš„èƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸¤ä¸ªå…·æœ‰å¯éªŒè¯è§£å†³æ–¹æ¡ˆçš„ç»„åˆé—®é¢˜ä¸Šè°ƒæŸ¥RLVRæ¥ç ”ç©¶å…¶æ•ˆæœï¼Œè¿™ä¸¤ä¸ªé—®é¢˜åˆ†åˆ«æ˜¯â€œæ´»åŠ¨è°ƒåº¦â€å’Œâ€œæœ€é•¿é€’å¢å­åºåˆ—â€ï¼Œæˆ‘ä»¬ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„å…·æœ‰ç‹¬ç‰¹æœ€ä¼˜è§£çš„æ•°æ®é›†è¿›è¡Œç ”ç©¶ã€‚åœ¨å¤šç§å¥–åŠ±è®¾è®¡ä¸‹ï¼Œæˆ‘ä»¬å‘ç°RLVRè™½ç„¶æé«˜äº†è¯„ä¼°æŒ‡æ ‡ï¼Œä½†å¾€å¾€æ˜¯é€šè¿‡å¯¹è¡¨é¢å¯å‘å¼ç­–ç•¥çš„å¼ºåŒ–è€Œéè·å–æ–°çš„æ¨ç†ç­–ç•¥æ¥å®ç°çš„ã€‚è¿™äº›å‘ç°çªå‡ºäº†RLVRæ³›åŒ–çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†åœ¨è¡¡é‡çœŸæ­£æ•°å­¦æ¨ç†ä¸æ·å¾„åˆ©ç”¨ä¹‹é—´è§£å¼€è”ç³»å’Œæä¾›å¿ å®è¿›æ­¥åº¦é‡æ–¹é¢ï¼ŒåŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/xashru/rlvr-seq-generalization%E3%80%82">https://github.com/xashru/rlvr-seq-generalizationã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°å­¦æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ä»…éœ€è¦æ­£ç¡®ç­”æ¡ˆï¼Œè¿˜éœ€è¦å¿ å®çš„æ¨ç†è¿‡ç¨‹ã€‚å¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•è¢«æå‡ºç”¨äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ˜¯å¦èƒ½ä¿ƒè¿›çœŸæ­£çš„æ¨ç†å°šä¸æ¸…æ¥šã€‚ç ”ç©¶è€…åœ¨ä¸¤ä¸ªå…·æœ‰å¯éªŒè¯è§£å†³æ–¹æ¡ˆçš„ç»„åˆé—®é¢˜ä¸Šå¯¹RLVRè¿›è¡Œäº†è°ƒæŸ¥ï¼šæ´»åŠ¨è°ƒåº¦å’Œæœ€é•¿é€’å¢å­åºåˆ—ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„å…·æœ‰ç‹¬ç‰¹æœ€ä¼˜è§£çš„æ•°æ®é›†è¿›è¡Œäº†å®éªŒã€‚åœ¨å¤šç§å¥–åŠ±è®¾è®¡ä¸‹ï¼Œç ”ç©¶å‘ç°RLVRè™½ç„¶æé«˜äº†è¯„ä¼°æŒ‡æ ‡ï¼Œä½†å¾€å¾€æ˜¯é€šè¿‡å¼ºåŒ–è¡¨å±‚å¯å‘å¼ç­–ç•¥è€Œéè·å¾—æ–°çš„æ¨ç†ç­–ç•¥æ¥æ”¹å–„æ€§èƒ½ã€‚è¿™å‡¸æ˜¾äº†RLVRæ³›åŒ–çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒéœ€è¦åŸºå‡†æµ‹è¯•æ¥åŒºåˆ†çœŸæ­£çš„æ•°å­¦æ¨ç†å’Œæ·å¾„åˆ©ç”¨ï¼Œä»¥åŠæä¾›å¯é çš„è¿›æ­¥è¡¡é‡æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­¦æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œéœ€è¦ä¸ä»…æ­£ç¡®ç­”æ¡ˆï¼Œè¿˜éœ€å¿ å®æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•è¢«ç”¨äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RLVRåœ¨è§£å†³ç»„åˆé—®é¢˜ä¸Šè¿›è¡Œäº†è°ƒæŸ¥ï¼Œå¦‚æ´»åŠ¨è°ƒåº¦å’Œæœ€é•¿é€’å¢å­åºåˆ—ã€‚</li>
<li>ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„å…·æœ‰ç‹¬ç‰¹æœ€ä¼˜è§£çš„æ•°æ®é›†è¿›è¡Œå®éªŒã€‚</li>
<li>RLVRæé«˜äº†è¯„ä¼°æŒ‡æ ‡ï¼Œä½†å¾€å¾€æ˜¯é€šè¿‡å¼ºåŒ–è¡¨å±‚å¯å‘å¼ç­–ç•¥æ¥æ”¹å–„æ€§èƒ½ï¼Œè€Œéè·å–æ–°çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>RLVRæ³›åŒ–å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>éœ€è¦åŸºå‡†æµ‹è¯•æ¥åŒºåˆ†çœŸæ­£çš„æ•°å­¦æ¨ç†å’Œæ·å¾„åˆ©ç”¨ï¼Œå¹¶æä¾›å¯é çš„è¿›æ­¥è¡¡é‡æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-523e879dc44094516284c3c68377cb39~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278504&auth_key=1762278504-0-0-7da3e539c7e6837433324451dcab3bcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a110e8a3b359becfc15945aca5789fd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278511&auth_key=1762278511-0-0-e53b6912aa057f4a45cd455f2b246f59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c575c385ccbf7ea5d985ca173235fcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278518&auth_key=1762278518-0-0-13a46b56186c27282a6046c220770ca9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86993f5c0cab86b21848fe7d6ac1239e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278524&auth_key=1762278524-0-0-b88074966f333569112d618c01693130&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Emu3-5-Native-Multimodal-Models-are-World-Learners"><a href="#Emu3-5-Native-Multimodal-Models-are-World-Learners" class="headerlink" title="Emu3.5: Native Multimodal Models are World Learners"></a>Emu3.5: Native Multimodal Models are World Learners</h2><p><strong>Authors:Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang</strong></p>
<p>We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at <a target="_blank" rel="noopener" href="https://github.com/baaivision/Emu3.5">https://github.com/baaivision/Emu3.5</a> to support community research. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Emu3.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒå¤©ç”Ÿå°±èƒ½é¢„æµ‹è§†è§‰å’Œè¯­è¨€çš„ä¸‹ä¸€çŠ¶æ€ã€‚Emu3.5é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡10ä¸‡äº¿æ ‡è®°çš„è§†å¬è¯­è¨€äº¤ç»‡æ•°æ®è¯­æ–™åº“ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒï¼Œä»¥ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡æ¥å®ç°ã€‚è¿™äº›æ•°æ®ä¸»è¦æ¥æºäºäº’è”ç½‘è§†é¢‘çš„è¿ç»­å¸§å’Œå­—å¹•ã€‚è¯¥æ¨¡å‹è‡ªç„¶åœ°æ¥å—äº¤ç»‡çš„è§†å¬è¯­è¨€è¾“å…¥ï¼Œå¹¶äº§ç”Ÿäº¤ç»‡çš„è§†å¬è¯­è¨€è¾“å‡ºã€‚ä¸ºäº†å¢å¼ºå¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒEmu3.5è¿˜é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†åè®­ç»ƒã€‚ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ç¦»æ•£æ‰©æ•£é€‚åº”ï¼ˆDiDAï¼‰æ–¹æ³•ï¼Œå®ƒå°†é€ä¸ªæ ‡è®°çš„è§£ç è½¬æ¢ä¸ºåŒå‘å¹¶è¡Œé¢„æµ‹ï¼Œåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å°†æ¯å¼ å›¾åƒçš„æ¨ç†é€Ÿåº¦æé«˜äº†å¤§çº¦20å€ã€‚Emu3.5è¡¨ç°å‡ºå¼ºå¤§çš„æœ¬åœŸå¤šæ¨¡æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿æœŸè§†ç•Œè§†å¬è¯­è¨€ç”Ÿæˆã€ä»»æ„å›¾åƒç”Ÿæˆï¼ˆX2Iï¼‰å’Œä¸°å¯Œçš„æ–‡æœ¬å›¾åƒç”Ÿæˆã€‚å®ƒè¿˜è¡¨ç°å‡ºé€šç”¨çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å„ç§åœºæ™¯å’Œä»»åŠ¡ä¸­è¿›è¡Œæ—¶ç©ºä¸€è‡´çš„ä¸–ç•Œæ¢ç´¢å’Œå¼€æ”¾ä¸–ç•Œæ“ä½œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒEmu3.5åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸Gemini 2.5 Flash Imageï¼ˆNano Bananaï¼‰ç›¸å½“ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—äº¤ç»‡ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„ç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/baaivision/Emu3.5%E5%BC%80%E6%BA%90%E4%BA%86Emu3.5%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E7%A4%BE%E5%8C%BA%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/baaivision/Emu3.5å¼€æºäº†Emu3.5ï¼Œä»¥æ”¯æŒç¤¾åŒºç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26583v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://emu.world/">https://emu.world</a></p>
<p><strong>Summary</strong></p>
<p>Emu3.5æ˜¯ä¸€æ¬¾å¤§è§„æ¨¡çš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨è§†è§‰å’Œè¯­è¨€ä¹‹é—´è¿›è¡Œé¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚è¯¥æ¨¡å‹é€šè¿‡ç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒï¼Œä»¥ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡ï¼Œå¤„ç†è¶…è¿‡åäº¿ä»¤ç‰Œçš„è§†è§‰è¯­è¨€äº¤ç»‡æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œä»¥æé«˜å¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºæé«˜æ¨ç†æ•ˆç‡ï¼Œæå‡ºäº†ç¦»æ•£æ‰©æ•£é€‚åº”ï¼ˆDiDAï¼‰æ–¹æ³•ï¼Œå°†ä»¤ç‰Œé€è§£ç è½¬æ¢ä¸ºåŒå‘å¹¶è¡Œé¢„æµ‹ï¼Œæé«˜æ¯å¼ å›¾åƒçš„æ¨ç†é€Ÿåº¦çº¦20å€ä¸”ä¸å½±å“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é•¿è§†ç•Œè§†è§‰è¯­è¨€ç”Ÿæˆã€ä»»æ„åˆ°å›¾åƒç”Ÿæˆå’Œå¤æ‚æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å…¶åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸Gemini 2.5 Flash Imageï¼ˆNano Bananaï¼‰ç›¸å½“ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—äº¤ç»‡ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¨¡å‹å·²å¼€æºï¼Œæ”¯æŒç¤¾åŒºç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Emu3.5æ˜¯ä¸€æ¬¾å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé¢„æµ‹è§†è§‰å’Œè¯­è¨€çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚</li>
<li>Emu3.5é€šè¿‡ç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œæé«˜å¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç¦»æ•£æ‰©æ•£é€‚åº”ï¼ˆDiDAï¼‰æ–¹æ³•æé«˜äº†æ¨ç†æ•ˆç‡ï¼ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦çº¦20å€ã€‚</li>
<li>Emu3.5å…·æœ‰å¼ºå¤§çš„è§†è§‰è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿è§†ç•Œè§†è§‰è¯­è¨€ç”Ÿæˆã€ä»»æ„åˆ°å›¾åƒç”Ÿæˆå’Œå¤æ‚æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>Emu3.5åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œä¸Gemini 2.5 Flash Imageç›¸å½“ã€‚</li>
<li>Emu3.5å…·æœ‰é€šç”¨ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„åœºæ™¯å’Œä»»åŠ¡ä¸­è¿›è¡Œæ—¶ç©ºä¸€è‡´çš„æ¢ç´¢å’Œæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c03ce856cb8e97c4456a02e55a2c879c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278531&auth_key=1762278531-0-0-375104d0cafbe6606a54daaf58aedf84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98775ff74c3ff4610d2c4d2d9707637c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278540&auth_key=1762278540-0-0-c5896c2feebb2c8a07f4980cff3939cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c794d1ef01aff260d10d986cf417850c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278547&auth_key=1762278547-0-0-505150d35e931b8c361cad8355620afe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5c118c934a9ed21ee890b31616d49f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278553&auth_key=1762278553-0-0-6a18310b23fb861a457e03f9179899b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf0b6901c877870229937c88071984b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278560&auth_key=1762278560-0-0-772b11ec5f991980ec22e46db7fe57ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games"><a href="#Can-Agent-Conquer-Web-Exploring-the-Frontiers-of-ChatGPT-Atlas-Agent-in-Web-Games" class="headerlink" title="Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in   Web Games"></a>Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in   Web Games</h2><p><strong>Authors:Jingran Zhang, Ning Li, Justin Cui</strong></p>
<p>OpenAIâ€™s ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlasâ€™s web interaction capabilities using browser-based games as test scenarios, including Googleâ€™s T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at <a target="_blank" rel="noopener" href="https://atlas-game-eval.github.io/">https://atlas-game-eval.github.io</a>. </p>
<blockquote>
<p>OpenAIçš„ChatGPT Atlaså¼•å…¥äº†ç½‘é¡µäº¤äº’çš„æ–°åŠŸèƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶åœ¨æµè§ˆå™¨å†…ç›´æ¥æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚è™½ç„¶å…¶åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›å·²ç»å¾—åˆ°è¯æ˜ï¼Œä½†åœ¨åŠ¨æ€ã€äº¤äº’å¼ç¯å¢ƒä¸­çš„è¡¨ç°ä»ç„¶ç ”ç©¶è¾ƒå°‘ã€‚æœ¬ç ”ç©¶é€šè¿‡åŸºäºæµè§ˆå™¨çš„æ¸¸æˆæµ‹è¯•åœºæ™¯å¯¹Atlasçš„ç½‘é¡µäº¤äº’èƒ½åŠ›è¿›è¡Œäº†åˆæ­¥è¯„ä¼°ï¼ŒåŒ…æ‹¬Googleçš„T-Rex Runnerã€æ•°ç‹¬ã€é£ç¿”é¸Ÿå’ŒStein.worldã€‚æˆ‘ä»¬é‡‡ç”¨æ¸¸æˆå†…ç»©æ•ˆå¾—åˆ†ä½œä¸ºå®šé‡æŒ‡æ ‡ï¼Œè¯„ä¼°ä¸åŒä»»åŠ¡ç±»å‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒAtlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°ç‹¬ï¼‰ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå®Œæˆè°œé¢˜çš„é€Ÿåº¦æ˜æ˜¾å¿«äºäººç±»åŸºå‡†çº¿ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®è®¡æ—¶å’ŒåŠ¨ä½œæ§åˆ¶çš„å®æ—¶æ¸¸æˆä¸­é‡åˆ°å›°éš¾ï¼Œå¾€å¾€æ— æ³•å…‹æœåˆå§‹éšœç¢ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶Atlasè¡¨ç°å‡ºäº†å¼ºå¤§çš„åˆ†æèƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº¤äº’çš„åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­ä»å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚æˆ‘ä»¬é¡¹ç›®çš„ç½‘ç«™å¯åœ¨ <a target="_blank" rel="noopener" href="https://atlas-game-eval.github.io/">https://atlas-game-eval.github.io</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26298v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenAIçš„ChatGPT Atlasé€šè¿‡ç½‘é¡µäº¤äº’èƒ½åŠ›å®ç°äº†åœ¨æµè§ˆå™¨å†…çš„ç›´æ¥åˆ†æä¸æ“ä½œã€‚æœ¬ç ”ç©¶é€šè¿‡æµè§ˆå™¨æ¸¸æˆæµ‹è¯•å…¶æ€§èƒ½ï¼Œå‘ç°Atlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦å®æ—¶äº’åŠ¨å’Œç²¾ç¡®æ§åˆ¶çš„æ¸¸æˆä¸­è¡¨ç°æ¬ ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPT Atlaså…·å¤‡ç½‘é¡µäº¤äº’èƒ½åŠ›ï¼Œå¯ç›´æ¥åœ¨æµè§ˆå™¨å†…åˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾å¹¶æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚</li>
<li>é€šè¿‡æµè§ˆå™¨æ¸¸æˆæµ‹è¯•ï¼Œå‘ç°Atlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡å¦‚Sudokuæ¸¸æˆä¸­è¡¨ç°ä¼˜ç§€ï¼Œå®Œæˆè°œé¢˜çš„é€Ÿåº¦æ˜¾è‘—å¿«äºäººç±»åŸºå‡†æµ‹è¯•è€…ã€‚</li>
<li>åœ¨éœ€è¦å®æ—¶äº’åŠ¨å’Œç²¾ç¡®æ§åˆ¶çš„æ¸¸æˆï¼ˆå¦‚T-Rex Runnerã€Flappy Birdå’ŒStein.worldï¼‰ä¸­ï¼ŒAtlasè¡¨ç°æŒ£æ‰ï¼Œå¾€å¾€æ— æ³•å…‹æœåˆå§‹éšœç¢ã€‚</li>
<li>Atlasåœ¨åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­çš„å®æ—¶äº¤äº’å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>Atlasçš„åˆ†æå¤„ç†èƒ½åŠ›å¾—åˆ°å±•ç°ï¼Œä½†åœ¨åº”å¯¹å¤æ‚ã€å¤šå˜çš„å®æ—¶äº’åŠ¨ç¯å¢ƒæ—¶ä»éœ€æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£Atlasçš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œæ”¹è¿›æ–¹å‘æä¾›å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2214839a3d1200afef4ae8bc39d9e7dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278567&auth_key=1762278567-0-0-8d5a2b6bb712d1f6975f9a83013a2803&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b71ca3dc633b0b7647d0207c07bf2d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278574&auth_key=1762278574-0-0-3619c899c8aa1b73496235b47cbc6264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63cfa8f8a2aa1b1556c61aaae2f44a52~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278581&auth_key=1762278581-0-0-0f439898873639ba1a8ba12442ed31cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20c814a28f4b16c2a43a9c5be17c9dfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278588&auth_key=1762278588-0-0-6e05cd35d291b6df5faf137b9dd1f7b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-Not-Step-Into-the-Same-River-Twice-Learning-to-Reason-from-Trial-and-Error"><a href="#Do-Not-Step-Into-the-Same-River-Twice-Learning-to-Reason-from-Trial-and-Error" class="headerlink" title="Do Not Step Into the Same River Twice: Learning to Reason from Trial and   Error"></a>Do Not Step Into the Same River Twice: Learning to Reason from Trial and   Error</h2><p><strong>Authors:Chenming Tang, Hsiu-Yuan Huang, Weijie Liu, Saiyong Yang, Yunfang Wu</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of large language models (LLMs) recently. However, existing RLVR approaches merely train LLMs based on their own generated responses and are constrained by the initial capability of LLMs, thus prone to exploration stagnation, in which LLMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems but requires external guidance from experts which suffers from limited availability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach hinting LLMs with their previously self-generated incorrect answers and problem of overlong responses, which does not require any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 6.38 in Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the problem of exploration stagnation and enhances both exploitation and exploration during training. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘æå¤§åœ°æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLVRæ–¹æ³•ä»…ä»…åŸºäºLLMè‡ªèº«ç”Ÿæˆçš„å“åº”è¿›è¡Œè®­ç»ƒï¼Œå¹¶å—åˆ°LLMåˆå§‹èƒ½åŠ›çš„é™åˆ¶ï¼Œå› æ­¤å®¹æ˜“å‡ºç°æ¢ç´¢åœæ»ï¼Œå³LLMæ— æ³•è§£å†³æ›´å¤šçš„è®­ç»ƒé—®é¢˜ï¼Œæ— æ³•ä»è®­ç»ƒæ•°æ®ä¸­è¿›ä¸€æ­¥å­¦ä¹ ã€‚ä¸€äº›å·¥ä½œè¯•å›¾é€šè¿‡åˆ©ç”¨ç¦»çº¿ç­–ç•¥è§£å†³æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†éœ€è¦ä¸“å®¶çš„å¤–éƒ¨æŒ‡å¯¼ï¼Œè€Œä¸“å®¶çš„æŒ‡å¯¼å¾€å¾€æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LTEï¼ˆä»è¯•é”™ä¸­å­¦ä¹ æ¨ç†ï¼‰ï¼Œä¸€ç§é€šè¿‡æš—ç¤ºLLMä¹‹å‰è‡ªæˆ‘ç”Ÿæˆçš„é”™è¯¯ç­”æ¡ˆä»¥åŠè¿‡é•¿å“åº”çš„é—®é¢˜çš„æ–¹æ³•ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨ä¸“å®¶æŒ‡å¯¼ã€‚å®éªŒéªŒè¯äº†LTEçš„æœ‰æ•ˆæ€§ï¼Œåœ¨Qwen3-4B-Baseçš„å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLTEåœ¨Pass@1ä¸Šä¼˜äºæ­£å¸¸ç»„çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰6.38ï¼Œåœ¨Pass@kä¸Šä¼˜äº9.00ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®ï¼ŒLTEæˆåŠŸåœ°ç¼“è§£äº†æ¢ç´¢åœæ»çš„é—®é¢˜ï¼Œå¹¶åœ¨è®­ç»ƒä¸­å¢å¼ºäº†å¼€å‘å’Œæ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26109v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘æå¤§åœ°æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLVRæ–¹æ³•ä»…åŸºäºLLMè‡ªèº«ç”Ÿæˆçš„å“åº”è¿›è¡Œè®­ç»ƒï¼Œå¹¶å—åˆ°LLMåˆå§‹èƒ½åŠ›çš„é™åˆ¶ï¼Œå®¹æ˜“å‡ºç°æ¢ç´¢åœæ»é—®é¢˜ï¼Œå³LLMæ— æ³•è§£å†³æ›´å¤šçš„è®­ç»ƒé—®é¢˜ï¼Œæ— æ³•ä»è®­ç»ƒæ•°æ®ä¸­è¿›ä¸€æ­¥å­¦ä¹ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¸€äº›ç ”ç©¶å°è¯•åˆ©ç”¨éç­–ç•¥è§£å†³æ–¹æ¡ˆï¼Œä½†éœ€è¦ä¸“å®¶å¤–éƒ¨æŒ‡å¯¼ï¼Œå­˜åœ¨æŒ‡å¯¼æœ‰é™çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†LTEï¼ˆä»è¯•é”™ä¸­å­¦ä¹ æ¨ç†ï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æç¤ºLLMä¹‹å‰è‡ªæˆ‘ç”Ÿæˆçš„é”™è¯¯ç­”æ¡ˆå’Œè¿‡é•¿å“åº”çš„é—®é¢˜ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨ä¸“å®¶æŒ‡å¯¼ã€‚å®éªŒéªŒè¯äº†LTEçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸å¯¹äºå¸¸è§„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒLTEåœ¨Pass@1å’ŒPass@kä¸Šçš„å¹³å‡è¡¨ç°åˆ†åˆ«æé«˜äº†6.38å’Œ9.00ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒLTEæˆåŠŸç¼“è§£äº†æ¢ç´¢åœæ»é—®é¢˜ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æé«˜äº†å¼€å‘å’Œæ¢ç´¢èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰RLVRæ–¹æ³•å—LLMåˆå§‹èƒ½åŠ›é™åˆ¶ï¼Œæ˜“é™·å…¥æ¢ç´¢åœæ»ã€‚</li>
<li>éç­–ç•¥è§£å†³æ–¹æ¡ˆéœ€ä¸“å®¶å¤–éƒ¨æŒ‡å¯¼ï¼Œä½†æŒ‡å¯¼èµ„æºæœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºLTEæ–¹æ³•ï¼ŒåŸºäºLLMè‡ªæˆ‘ç”Ÿæˆçš„é”™è¯¯ç­”æ¡ˆè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LTEåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè¶…è¶Šå¸¸è§„æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>LTEæˆåŠŸç¼“è§£æ¢ç´¢åœæ»é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-715d2245c56e585e6d513072a247bc19~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278596&auth_key=1762278596-0-0-2c95b20ee2d561dc34cf0d1b603a587f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d5e099db1232faf55468b384c2e39bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278603&auth_key=1762278603-0-0-5cecc8bf4b9fa497399975990a436578&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2194110874ecfb27a354eebc0355094a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278610&auth_key=1762278610-0-0-b6916f97bc798baceab2b7bd24be4395&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d51e66943d283f5f9bb8832107044de~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278616&auth_key=1762278616-0-0-ed218fe9d6850b2e16d422ff0417e7fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4104a5bc8c08e39d35ede82b16a68f55~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278623&auth_key=1762278623-0-0-4603f74d7fde2a6f1721f44803b26352&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb7fa8e4a77e8e83b8e309c2ee988a7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278629&auth_key=1762278629-0-0-25d5777e52fdcbece4ced2fbf579a26c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84e5aeeb72404d7a4f0fd226f30268f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278636&auth_key=1762278636-0-0-3ec84f0740795fbb8523085e63a86bd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AutoSurvey2-Empowering-Researchers-with-Next-Level-Automated-Literature-Surveys"><a href="#AutoSurvey2-Empowering-Researchers-with-Next-Level-Automated-Literature-Surveys" class="headerlink" title="AutoSurvey2: Empowering Researchers with Next Level Automated Literature   Surveys"></a>AutoSurvey2: Empowering Researchers with Next Level Automated Literature   Surveys</h2><p><strong>Authors:Siyi Wu, Chiaxin Liang, Ziqian Bi, Leyi Zhao, Tianyang Wang, Junhao Song, Yichao Zhang, Keyu Chen, Xinyuan Song</strong></p>
<p>The rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at <a target="_blank" rel="noopener" href="https://github.com/annihi1ation/auto_research">https://github.com/annihi1ation/auto_research</a>. </p>
<blockquote>
<p>éšç€ç ”ç©¶æ–‡çŒ®çš„å¿«é€Ÿå¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ’°å†™å…¨é¢ä¸”æœ€æ–°çš„ç»¼è¿°è®ºæ–‡å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†autosurvey2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µçš„ç®¡é“ï¼Œé€šè¿‡å¢å¼ºæ£€ç´¢çš„ç»¼åˆæ–¹æ³•å’Œç»“æ„åŒ–è¯„ä¼°æ¥è‡ªåŠ¨åŒ–ç»¼è¿°ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¹¶è¡Œæ®µè½ç”Ÿæˆã€è¿­ä»£ç»†åŒ–å’Œå®æ—¶æ£€ç´¢æœ€æ–°å‡ºç‰ˆç‰©ï¼Œä»¥ç¡®ä¿ä¸»é¢˜å®Œæ•´æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚è´¨é‡è¯„ä¼°é‡‡ç”¨å¤šLLMè¯„ä¼°æ¡†æ¶ï¼Œæ ¹æ®ä¸“å®¶è¯„å®¡æ ‡å‡†è¡¡é‡è¦†ç›–ç‡ã€ç»“æ„å’Œç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œautosurvey2æŒç»­ä¼˜äºç°æœ‰çš„åŸºäºæ£€ç´¢å’Œè‡ªåŠ¨åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨ç»“æ„è¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§æ–¹é¢å¾—åˆ†æ›´é«˜ï¼ŒåŒæ—¶ä¿æŒé«˜å¼•ç”¨ä¿çœŸåº¦ã€‚é€šè¿‡å°†æ£€ç´¢ã€æ¨ç†å’Œè‡ªåŠ¨è¯„ä¼°ç»“åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œautosurvey2ä¸ºç”Ÿæˆé•¿æ ¼å¼çš„å­¦æœ¯ç»¼è¿°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯å¤åˆ¶çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥è‡ªåŠ¨åŒ–å­¦æœ¯å†™ä½œçš„ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚æ‰€æœ‰ä»£ç å’Œèµ„æºéƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/annihi1ation/auto_research%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/annihi1ation/auto_researchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26012v2">PDF</a> TKDD 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†åä¸ºautosurvey2çš„å¤šé˜¶æ®µç®¡é“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¢å¼ºåˆæˆå’Œç»“æ„åŒ–è¯„ä¼°è‡ªåŠ¨åŒ–ç”Ÿæˆè°ƒæŸ¥æŠ¥å‘Šã€‚ç³»ç»Ÿé›†æˆäº†å¹¶è¡Œæ®µè½ç”Ÿæˆã€è¿­ä»£ä¼˜åŒ–å’Œæœ€æ–°æ–‡çŒ®çš„å®æ—¶æ£€ç´¢ï¼Œç¡®ä¿ä¸»é¢˜å®Œæ•´æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚è´¨é‡è¯„ä¼°é‡‡ç”¨å¤šè¯­è¨€æ¨¡å‹è¯„ä¼°æ¡†æ¶ï¼Œæ ¹æ®ä¸“å®¶è¯„å®¡æ ‡å‡†è¡¡é‡è¦†ç›–èŒƒå›´ã€ç»“æ„å’Œç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œautosurvey2åœ¨ç»“æ„è¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„åŸºäºæ£€ç´¢å’Œè‡ªåŠ¨åŒ–çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦çš„å¼•æ–‡ä¿çœŸåº¦ã€‚é€šè¿‡å°†æ£€ç´¢ã€æ¨ç†å’Œè‡ªåŠ¨è¯„ä¼°ç»“åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œautosurvey2ä¸ºç”Ÿæˆé•¿ç¯‡æ–‡ç« æä¾›äº†å¯æ‰©å±•å’Œå¯å¤åˆ¶çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–å­¦æœ¯å†™ä½œçš„æœªæ¥ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>autosurvey2æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–ç”Ÿæˆè°ƒæŸ¥æŠ¥å‘Šã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æ£€ç´¢å¢å¼ºåˆæˆå’Œç»“æ„åŒ–è¯„ä¼°ç¡®ä¿æŠ¥å‘Šçš„ä¸»é¢˜å®Œæ•´æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>ç³»ç»Ÿä¸­é›†æˆäº†å¹¶è¡Œæ®µè½ç”Ÿæˆã€è¿­ä»£ä¼˜åŒ–å’Œæœ€æ–°æ–‡çŒ®çš„å®æ—¶æ£€ç´¢ã€‚</li>
<li>é‡‡ç”¨å¤šè¯­è¨€æ¨¡å‹è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡æŠ¥å‘Šçš„è´¨é‡ï¼ŒåŒ…æ‹¬è¦†ç›–èŒƒå›´ã€ç»“æ„å’Œç›¸å…³æ€§ã€‚</li>
<li>autosurvey2åœ¨ç»“æ„è¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>autosurvey2ä¿æŒé«˜å¼•æ–‡ä¿çœŸåº¦ï¼Œä¸ºé•¿ç¯‡æ–‡ç« çš„è‡ªåŠ¨ç”Ÿæˆæä¾›äº†å¯æ‰©å±•å’Œå¯å¤åˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-53cb54d5a3812aa7e38f14595b04cd80~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278644&auth_key=1762278644-0-0-45b11859c13149762d5a3de2634f70bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-024767e1851d67e59f37ea5538a369bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278651&auth_key=1762278651-0-0-86e9b8e54355bd857e3d25f43a3a1d69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Approximating-Human-Preferences-Using-a-Multi-Judge-Learned-System"><a href="#Approximating-Human-Preferences-Using-a-Multi-Judge-Learned-System" class="headerlink" title="Approximating Human Preferences Using a Multi-Judge Learned System"></a>Approximating Human Preferences Using a Multi-Judge Learned System</h2><p><strong>Authors:EitÃ¡n Sprejer, Fernando Avalos, Augusto Bernardi, Jose Pedro Brito de Azevedo Faustino, Jacob Haimes, Narmeen Fatimah Oozeer</strong></p>
<p>Aligning LLM-based judges with human preferences is a significant challenge, as they are difficult to calibrate and often suffer from rubric sensitivity, bias, and instability. Overcoming this challenge advances key applications, such as creating reliable reward models for Reinforcement Learning from Human Feedback (RLHF) and building effective routing systems that select the best-suited model for a given user query. In this work, we propose a framework for modeling diverse, persona-based preferences by learning to aggregate outputs from multiple rubric-conditioned judges. We investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and LLM-judges biases. Our primary contributions include a persona-based method for synthesizing preference labels at scale and two distinct implementations of our aggregator: Generalized Additive Model (GAM) and a Multi-Layer Perceptron (MLP). </p>
<blockquote>
<p>å°†åŸºäºLLMçš„è£åˆ¤ä¸äººç±»åå¥½å¯¹é½æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥æ ¡å‡†ï¼Œä¸”ç»å¸¸å—åˆ°è¯„åˆ†æ ‡å‡†æ•æ„Ÿæ€§ã€åè§å’Œä¸ç¨³å®šæ€§çš„å½±å“ã€‚å…‹æœè¿™ä¸€æŒ‘æˆ˜æœ‰åŠ©äºæ¨åŠ¨å…³é”®åº”ç”¨çš„å‘å±•ï¼Œä¾‹å¦‚å»ºç«‹å¯é çš„å¥–åŠ±æ¨¡å‹ç”¨äºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œä»¥åŠæ„å»ºæœ‰æ•ˆçš„è·¯ç”±ç³»ç»Ÿæ¥ä¸ºç»™å®šç”¨æˆ·æŸ¥è¯¢é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šè¿‡å­¦ä¹ èšåˆå¤šä¸ªè¯„åˆ†æ ‡å‡†æ¡ä»¶ä¸‹çš„è£åˆ¤è¾“å‡ºæ¥å»ºæ¨¡å¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–åå¥½çš„æ¡†æ¶ã€‚æˆ‘ä»¬ç ”ç©¶äº†è¯¥æ–¹æ³•ç›¸å¯¹äºç®€å•åŸºå‡†çº¿çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡å…³äºäººç±»å’ŒLLMè£åˆ¤åè§çš„æ¡ˆä¾‹ç ”ç©¶æ¥è¯„ä¼°å…¶ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸€ç§åŸºäºä¸ªæ€§åŒ–çš„å¤§è§„æ¨¡åå¥½æ ‡ç­¾åˆæˆæ–¹æ³•ï¼Œä»¥åŠæˆ‘ä»¬èšåˆå™¨çš„ä¸¤ç§ä¸åŒå®ç°ï¼šå¹¿ä¹‰å¯åŠ æ¨¡å‹ï¼ˆGAMï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25884v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLMåœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ¡å‡†å›°éš¾ã€å¯¹è¯„ä»·æ ‡å‡†çš„æ•æ„Ÿæ€§ã€åè§å’Œä¸ç¨³å®šç­‰é—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæ¨è¿›å…³é”®åº”ç”¨å¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„å¯é å¥–åŠ±æ¨¡å‹åŠé’ˆå¯¹ç‰¹å®šç”¨æˆ·æŸ¥è¯¢é€‰æ‹©æœ€é€‚åˆæ¨¡å‹çš„æœ‰æ•ˆè·¯ç”±ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºä¸€ç§é€šè¿‡å»ºæ¨¡å¤šç§ä¸ªæ€§åŒ–åå¥½å¹¶å­¦ä¹ æ•´åˆå¤šä¸ªåŸºäºè¯„ä»·æ ‡å‡†çš„æ³•å®˜è¾“å‡ºç»“æœçš„æ¡†æ¶ã€‚ç ”ç©¶æ­¤æ–¹æ³•çš„æ€§èƒ½å¹¶è¯„ä¼°å…¶åœ¨åº”å¯¹äººä¸ºåè§å’ŒLLMæ³•å®˜åè§æ–¹é¢çš„ç¨³å¥æ€§ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬å¤§è§„æ¨¡åˆæˆåå¥½æ ‡ç­¾çš„ä¸ªæ€§åŒ–æ–¹æ³•å’Œä¸¤ç§ç‹¬ç‰¹çš„èšåˆå™¨å®ç°ï¼šå¹¿ä¹‰å¯åŠ æ¨¡å‹ï¼ˆGAMï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸äººç±»åå¥½å¯¹é½å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ¡å‡†å›°éš¾ã€è¯„ä»·æ ‡å‡†æ•æ„Ÿæ€§ã€åè§å’Œä¸ç¨³å®šç­‰é—®é¢˜ã€‚</li>
<li>é€šè¿‡å»ºæ¨¡å¤šç§ä¸ªæ€§åŒ–åå¥½ä»¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œå¹¶æ¨è¿›å…³é”®åº”ç”¨å¦‚å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹å’Œè·¯ç”±ç³»ç»Ÿã€‚</li>
<li>æå‡ºä¸€ç§æ•´åˆå¤šä¸ªåŸºäºè¯„ä»·æ ‡å‡†çš„æ³•å®˜è¾“å‡ºç»“æœçš„æ¡†æ¶ã€‚</li>
<li>ç ”ç©¶äº†æ¡†æ¶çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯„ä¼°äº†å…¶ç¨³å¥æ€§ã€‚</li>
<li>ä¸»è¦çš„è´¡çŒ®åŒ…æ‹¬å¤§è§„æ¨¡åˆæˆåå¥½æ ‡ç­¾çš„ä¸ªæ€§åŒ–æ–¹æ³•ã€‚</li>
<li>æä¾›äº†ä¸¤ç§ç‹¬ç‰¹çš„èšåˆå™¨å®ç°ï¼šå¹¿ä¹‰å¯åŠ æ¨¡å‹ï¼ˆGAMï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3ec952d219a98d8c6b7a0bf7cac1b7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278658&auth_key=1762278658-0-0-835edf4414cedd09a4bd3e5dbd958b57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df09101e7e9962fdcd6cb3688f83a69d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278665&auth_key=1762278665-0-0-044ac43e4e97a2bb10887345d5c910b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-080c204eb50448500d2a190686d69501~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278672&auth_key=1762278672-0-0-839cec38c99ebf5270df9d46bd4fe939&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f3cf9f76068193f24380cb8c4e2428d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278678&auth_key=1762278678-0-0-e38ed582b26475f269f8bb14616f4e77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a003e0b901cf8d0cd3f057c94c2b0c75~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278685&auth_key=1762278685-0-0-9299d5c0b83561f4561c371d861623ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks"><a href="#Multimodal-Spatial-Reasoning-in-the-Large-Model-Era-A-Survey-and-Benchmarks" class="headerlink" title="Multimodal Spatial Reasoning in the Large Model Era: A Survey and   Benchmarks"></a>Multimodal Spatial Reasoning in the Large Model Era: A Survey and   Benchmarks</h2><p><strong>Authors:Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu</strong></p>
<p>Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at <a target="_blank" rel="noopener" href="https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning">https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning</a>. </p>
<blockquote>
<p>äººç±»æ‹¥æœ‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿï¼ˆå¦‚è§†è§‰å’Œå¬è§‰ï¼‰ç†è§£ç©ºé—´ã€‚å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹é€šè¿‡å­¦ä¹ å’Œæ„ŸçŸ¥æ¨ç†æ¥æ‰©å±•è¿™äº›èƒ½åŠ›ï¼Œåœ¨å¤šç§ç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›æ¨¡å‹çš„ç³»ç»Ÿæ€§å›é¡¾å’Œå…¬å¼€çš„åŸºå‡†æµ‹è¯•ä»ç„¶æœ‰é™ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å…¨é¢å›é¡¾äº†ä½¿ç”¨å¤§å‹æ¨¡å‹çš„å¤šæ¨¡æ€ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œå¯¹è¿‘æœŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›å±•è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä»‹ç»äº†ç”¨äºè¯„ä¼°çš„å…¬å¼€åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬é¦–å…ˆæ¦‚è¿°ä¸€èˆ¬çš„ç©ºé—´æ¨ç†ï¼Œé‡ç‚¹å…³æ³¨åè®­ç»ƒæŠ€æœ¯ã€è§£é‡Šæ€§å’Œæ¶æ„ã€‚é™¤äº†ç»å…¸çš„äºŒç»´ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†ç©ºé—´å…³ç³»æ¨ç†ã€åœºæ™¯å’Œå¸ƒå±€ç†è§£ï¼Œä»¥åŠè§†è§‰é—®ç­”å’Œä¸‰ç»´ç©ºé—´å®šä½ã€‚æˆ‘ä»¬è¿˜å›é¡¾äº†åµŒå…¥å¼AIçš„è¿›å±•ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€å¯¼èˆªå’ŒåŠ¨ä½œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†æ–°å…´çš„æ¨¡æ€ï¼Œå¦‚éŸ³é¢‘å’Œè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ï¼Œè¿™äº›é€šè¿‡æ–°ä¼ æ„Ÿå™¨ä¸ºæ–°å‹ç©ºé—´ç†è§£åšå‡ºäº†è´¡çŒ®ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç¯‡ç»¼è¿°ä¸ºä¸æ–­å‘å±•çš„å¤šæ¨¡æ€ç©ºé—´æ¨ç†é¢†åŸŸå¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œå¹¶æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚æœ‰å…³æ­¤ç»¼è¿°çš„æ›´æ–°ä¿¡æ¯ã€ä»£ç å’Œå…¬å¼€åŸºå‡†æµ‹è¯•çš„å®æ–½ï¼Œå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhengxuJosh/Awesome-Spatial-Reasoningæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25760v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šäººç±»æ‹¥æœ‰é€šè¿‡å¤šæ¨¡å¼è§‚å¯Ÿï¼ˆå¦‚è§†è§‰å’Œå¬è§‰ï¼‰ç†è§£ç©ºé—´çš„èƒ½åŠ›ã€‚å¤§å‹å¤šæ¨¡å¼æ¨ç†æ¨¡å‹é€šè¿‡å­¦ä¹ å’Œæ„ŸçŸ¥æ‰©å±•è¿™äº›èƒ½åŠ›ï¼Œåœ¨å„ç§ç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†ä½¿ç”¨å¤§å‹æ¨¡å‹çš„å¤šæ¨¡å¼ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œåˆ†ç±»äº†è¿‘æœŸåœ¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ–¹é¢çš„è¿›å±•ï¼Œå¹¶ä»‹ç»äº†å…¬å¼€è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« æ¦‚è¿°äº†ä¸€èˆ¬ç©ºé—´æ¨ç†ï¼Œé‡ç‚¹å…³æ³¨äº†è®­ç»ƒåæŠ€æœ¯ã€è§£é‡Šæ€§å’Œæ¶æ„ã€‚é™¤äº†ä¼ ç»Ÿçš„äºŒç»´ä»»åŠ¡å¤–ï¼Œè¿˜æ¢è®¨äº†ç©ºé—´å…³ç³»æ¨ç†ã€åœºæ™¯å’Œå¸ƒå±€ç†è§£ä»¥åŠè§†è§‰é—®ç­”å’Œä¸‰ç»´ç©ºé—´å®šä½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å›é¡¾äº†åŒ…æ‹¬è§†è§‰è¯­è¨€å¯¼èˆªå’ŒåŠ¨ä½œæ¨¡å‹åœ¨å†…çš„åµŒå…¥å¼äººå·¥æ™ºèƒ½çš„è¿›å±•ï¼Œå¹¶è€ƒè™‘äº†éŸ³é¢‘å’Œç¬¬ä¸€äººç§°è§†é¢‘ç­‰æ–°å…´æ¨¡å¼ï¼Œè¿™äº›æ–°å…´æ¨¡å¼é€šè¿‡æ–°å‹ä¼ æ„Ÿå™¨ä¸ºç©ºé—´ç†è§£åšå‡ºäº†è´¡çŒ®ã€‚æœ¬æ–‡å»ºç«‹äº†åšå®çš„ç†è®ºåŸºç¡€ï¼Œä¸ºä¸æ–­å‘å±•çš„å¤šæ¨¡å¼ç©ºé—´æ¨ç†é¢†åŸŸæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººç±»å…·å¤‡é€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿï¼ˆå¦‚è§†è§‰å’Œå¬è§‰ï¼‰è¿›è¡Œç©ºé—´æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹é€šè¿‡å­¦ä¹ å’Œæ„ŸçŸ¥æ‰©å±•è¿™äº›èƒ½åŠ›ï¼Œåœ¨å„ç§ç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰å¯¹å¤šæ¨¡æ€ç©ºé—´æ¨ç†æ¨¡å‹çš„ç³»ç»Ÿæ€§è¯„ä»·åŸºå‡†ç›¸å¯¹æœ‰é™ã€‚</li>
<li>æ–‡ç« å…¨é¢å›é¡¾äº†å¤šæ¨¡æ€ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œæ¶‰åŠå¤§å‹æ¨¡å‹çš„åº”ç”¨å’Œå‘å±•ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†ç©ºé—´æ¨ç†çš„å…³é”®é¢†åŸŸï¼ŒåŒ…æ‹¬ç©ºé—´å…³ç³»æ¨ç†ã€åœºæ™¯å’Œå¸ƒå±€ç†è§£ç­‰ã€‚</li>
<li>æ–‡ç« æ¶µç›–äº†é™¤ä¼ ç»ŸäºŒç»´ä»»åŠ¡å¤–çš„æ›´å¹¿æ³›çš„ç©ºé—´ç†è§£æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9eba52050da28ec99d4d27b0599c2fe8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278692&auth_key=1762278692-0-0-3b19c468dabaaf1cd2e430796ec01a24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f280167a9f50ab4181493617700940da~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278700&auth_key=1762278700-0-0-90a84249a31ca06e99851996589110d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6782120a165d1ff9547dfbab35468045~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278707&auth_key=1762278707-0-0-069b96dd994c06137fc792a98badb705&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33528a375d6e3df548567109a1f16d61~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278714&auth_key=1762278714-0-0-926ca16e8a60c6c8955f586eaea95179&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03ebec4b77be15aa17b6fa22b1f75a0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278722&auth_key=1762278722-0-0-88b5692335261b2ca85a8b4be118b180&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Scaling-Latent-Reasoning-via-Looped-Language-Models"><a href="#Scaling-Latent-Reasoning-via-Looped-Language-Models" class="headerlink" title="Scaling Latent Reasoning via Looped Language Models"></a>Scaling Latent Reasoning via Looped Language Models</h2><p><strong>Authors:Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, Yoshua Bengio, Jason Eshraghian</strong></p>
<p>Modern LLMs are trained to â€œthinkâ€ primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model is available here: <a target="_blank" rel="noopener" href="http://ouro-llm.github.io/">http://ouro-llm.github.io</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦é€šè¿‡æ˜ç¡®çš„æ–‡æœ¬ç”Ÿæˆæ¥è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œä¾‹å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œè¿™ç§æ€ç»´å°†æ¨ç†æ¨è¿Ÿåˆ°è®­ç»ƒä¹‹åï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºå¹¶å¼€æºäº†åä¸ºOuroçš„é€’å½’Ouroborosç³»åˆ—é¢„è®­ç»ƒå¾ªç¯è¯­è¨€æ¨¡å‹ï¼ˆLoopLMï¼‰ã€‚å®ƒé€šè¿‡ï¼ˆiï¼‰æ½œåœ¨ç©ºé—´ä¸­çš„è¿­ä»£è®¡ç®—ï¼Œï¼ˆiiï¼‰ç”¨äºå­¦ä¹ æ·±åº¦åˆ†é…çš„ç†µæ­£åˆ™åŒ–ç›®æ ‡ï¼Œä»¥åŠï¼ˆiiiï¼‰æ‰©å±•åˆ°7.7ä¸‡äº¿ä¸ªä»¤ç‰Œï¼Œå°†æ¨ç†èå…¥åˆ°é¢„è®­ç»ƒé˜¶æ®µã€‚Ouro 1.4Bå’Œ2.6Bæ¨¡å‹äº«æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ä¸é«˜è¾¾12Bçš„é¡¶å°–å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“æœç›¸åŒ¹é…ã€‚é€šè¿‡å—æ§å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§ä¼˜åŠ¿å¹¶éæ¥è‡ªå¢åŠ çš„çŸ¥è¯†å®¹é‡ï¼Œè€Œæ˜¯æ¥è‡ªå“è¶Šçš„çŸ¥è¯†æ“ä½œèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒLoopLMäº§ç”Ÿçš„æ¨ç†ç—•è¿¹ä¸æœ€ç»ˆè¾“å‡ºæ›´åŠ ä¸€è‡´ï¼Œè€Œä¸æ˜¯æ˜ç¡®çš„CoTã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç»“æœèƒ½å¤Ÿå±•ç¤ºLoopLMä½œä¸ºæ¨ç†æ—¶ä»£æ–°å‹æ‰©å±•æ–¹å‘çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="http://ouro-llm.github.io/">è¿™é‡Œè®¿é—®</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25741v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦é€šè¿‡æ˜ç¡®çš„æ–‡æœ¬ç”Ÿæˆæ¥è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œä¾‹å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œè¿™ç§æ–¹å¼å°†æ¨ç†æ¨è¿Ÿåˆ°è®­ç»ƒä¹‹åï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºå¹¶å¼€æºäº†åä¸ºOuroçš„é¢„è®­ç»ƒå¾ªç¯è¯­è¨€æ¨¡å‹ï¼ˆLoopLMï¼‰ï¼Œå®ƒé€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µèå…¥æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰æ½œåœ¨ç©ºé—´ä¸­çš„è¿­ä»£è®¡ç®—ï¼Œï¼ˆiiï¼‰ç”¨äºæ·±åº¦åˆ†é…çš„ç†µæ­£åˆ™åŒ–ç›®æ ‡ï¼Œä»¥åŠï¼ˆiiiï¼‰æ‰©å±•åˆ°7.7ä¸‡äº¿ä»¤ç‰Œã€‚Ouro 1.4Bå’Œ2.6Bæ¨¡å‹çš„æ€§èƒ½ä¼˜è¶Šï¼Œåœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ä¸å¤šè¾¾12Bçš„SOTA LLMç›¸åŒ¹é…ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§ä¼˜åŠ¿å¹¶éæ¥è‡ªå¢åŠ çš„çŸ¥è¯†å®¹é‡ï¼Œè€Œæ˜¯æ¥è‡ªå“è¶Šçš„çŸ¥è¯†æ“ä½œèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒLoopLMäº§ç”Ÿçš„æ¨ç†è½¨è¿¹ä¸æœ€ç»ˆè¾“å‡ºæ›´åŠ ä¸€è‡´ï¼Œä¸åŒäºæ˜ç¡®çš„CoTã€‚æˆ‘ä»¬å¸Œæœ›ç»“æœå±•ç¤ºLoopLMä½œä¸ºæ¨ç†æ—¶ä»£æ–°å‹æ‰©å±•æ–¹å‘çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£LLMä¸»è¦é€šè¿‡æ–‡æœ¬ç”Ÿæˆè¿›è¡Œâ€œæ€è€ƒâ€ï¼Œå°†æ¨ç†æ¨è¿Ÿåˆ°è®­ç»ƒåã€‚</li>
<li>Ouroæ˜¯é¢„è®­ç»ƒå¾ªç¯è¯­è¨€æ¨¡å‹ï¼ˆLoopLMï¼‰ï¼Œå°†æ¨ç†èå…¥é¢„è®­ç»ƒé˜¶æ®µã€‚</li>
<li>LoopLMé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„è¿­ä»£è®¡ç®—ã€ç†µæ­£åˆ™åŒ–ç›®æ ‡å’Œæ‰©å±•åˆ°å¤§é‡ä»¤ç‰Œæ¥æå‡æ€§èƒ½ã€‚</li>
<li>Ouro 1.4Bå’Œ2.6Bæ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œä¸SOTA LLMç›¸å½“ã€‚</li>
<li>æ§åˆ¶å®éªŒè¡¨æ˜ä¼˜åŠ¿æ¥è‡ªçŸ¥è¯†æ“ä½œèƒ½åŠ›çš„æå‡ï¼Œè€Œéå¢åŠ çš„çŸ¥è¯†å®¹é‡ã€‚</li>
<li>LoopLMäº§ç”Ÿçš„æ¨ç†è½¨è¿¹ä¸æœ€ç»ˆè¾“å‡ºæ›´åŠ ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-64794accde57d92aaa154a735db5160f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278729&auth_key=1762278729-0-0-b87e33685ea46f71b309e3758b987b7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3437ee51960dbb38ae704fc4290ea9cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278736&auth_key=1762278736-0-0-d541a103a4d05c7ce4ee368e8de35b9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fa31cc73268dc91c6cb0fdcbddd3910~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278743&auth_key=1762278743-0-0-ab89244634029f0a7a16a13fa744f222&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-Aware-GRPO-using-Process-Mining"><a href="#Reasoning-Aware-GRPO-using-Process-Mining" class="headerlink" title="Reasoning-Aware GRPO using Process Mining"></a>Reasoning-Aware GRPO using Process Mining</h2><p><strong>Authors:Taekhyun Park, Yongjae Lee, Hyerim Bae</strong></p>
<p>Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer&#x2F;format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy modelâ€™s reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models. </p>
<blockquote>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åæœŸè®­ç»ƒå¯¹äºå®ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­çš„å¤šæ­¥æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¥–åŠ±æœºåˆ¶å¤§å¤šä»¥ç»“æœä¸ºä¸­å¿ƒã€‚æˆ‘ä»¬æå‡ºäº†PM4GRPOï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æ¨ç†æ„è¯†çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå®ƒé™¤äº†æ ‡å‡†ç­”æ¡ˆ&#x2F;æ ¼å¼å¥–åŠ±å¤–ï¼Œè¿˜å¢åŠ äº†å¯¹æ¨ç†è¿‡ç¨‹çš„ä¿¡å·ã€‚ä¸ºæ­¤ï¼Œé‡‡ç”¨è¿‡ç¨‹æŒ–æ˜æŠ€æœ¯æ¥è®¡ç®—æ ‡é‡ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¯¥å¥–åŠ±è¡¡é‡ç­–ç•¥æ¨¡å‹çš„æ¨ç†ä¸é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„å¥‘åˆç¨‹åº¦ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒPM4GRPOåœ¨åŸºäºGRPOçš„åæœŸè®­ç»ƒæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œåˆ©ç”¨è¿‡ç¨‹æŒ–æ˜è¿›è¡Œå…·æœ‰æ¨ç†æ„è¯†çš„GRPOå¯æœ‰æ•ˆæé«˜ç­–ç•¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25065v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¤šæ­¥æ¨ç†è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¥–åŠ±æœºåˆ¶å¤šä»¥ç»“æœä¸ºä¸­å¿ƒã€‚æˆ‘ä»¬æå‡ºPM4GRPOï¼Œä¸€ç§æ¨ç†æ„ŸçŸ¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå®ƒä½¿ç”¨è¿‡ç¨‹æŒ–æ˜æŠ€æœ¯æ¥è®¡ç®—æ”¿ç­–æ¨¡å‹æ¨ç†ä¸æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒçš„ä¸€è‡´æ€§ç¨‹åº¦çš„æ ‡é‡åˆè§„å¥–åŠ±ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒPM4GRPOåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„GRPOåè®­ç»ƒçš„æ–¹æ³•è®ºã€‚è¿™å‡¸æ˜¾äº†åˆ©ç”¨è¿‡ç¨‹æŒ–æ˜å®ç°æ¨ç†æ„ŸçŸ¥çš„GRPOèƒ½æœ‰æ•ˆæå‡ç­–ç•¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯¹äºå¤§å‹æ¨ç†æ¨¡å‹çš„å¤šæ­¥æ¨ç†è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¥–åŠ±æœºåˆ¶å¤šä»¥ç»“æœä¸ºä¸­å¿ƒï¼Œéœ€è¦ä¸€ç§æ¨ç†æ„ŸçŸ¥çš„å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>PM4GRPOæ˜¯ä¸€ç§æ–°çš„æ¨ç†æ„ŸçŸ¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>PM4GRPOåˆ©ç”¨è¿‡ç¨‹æŒ–æ˜æŠ€æœ¯è®¡ç®—æ”¿ç­–æ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„åˆè§„æ€§å¥–åŠ±ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜PM4GRPOæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨è¿‡ç¨‹æŒ–æ˜å®ç°æ¨ç†æ„ŸçŸ¥çš„ç­–ç•¥ä¼˜åŒ–èƒ½æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3bd597ee339aecf672573f619019541d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278750&auth_key=1762278750-0-0-12f810284a6440eb4945af96ca24f068&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c57dd642986e7fe9d1d946eb0d946da1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278758&auth_key=1762278758-0-0-73924844b2b0631b162800b38ac36a38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37e0fd16016ba604859fbcaf4b5dd504~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278764&auth_key=1762278764-0-0-25fdf5a91f55ad31c6ce6bd2f8904a14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision"><a href="#Repurposing-Synthetic-Data-for-Fine-grained-Search-Agent-Supervision" class="headerlink" title="Repurposing Synthetic Data for Fine-grained Search Agent Supervision"></a>Repurposing Synthetic Data for Fine-grained Search Agent Supervision</h2><p><strong>Authors:Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang</strong></p>
<p>LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative â€œnear-missâ€ samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agentâ€™s reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these â€œnear-missesâ€. Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœç´¢ä»£ç†è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥è§£å†³å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæµè¡Œçš„è®­ç»ƒæ–¹æ³•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¼šä¸¢å¼ƒè¿™äº›ä¸°å¯Œçš„å®ä½“ä¿¡æ¯ï¼Œè½¬è€Œä¾èµ–ç¨€ç–çš„ç»“æœå¯¼å‘å¥–åŠ±ã€‚è¿™ä¸€å…³é”®å±€é™ä½¿å¾—å®ƒä»¬æ— æ³•åŒºåˆ†å…·æœ‰å®è´¨æ€§æ­£ç¡®æ¨ç†ä½†æœ€ç»ˆç­”æ¡ˆæœ‰ç¼ºé™·çš„â€œè¿‘ä¼¼æˆåŠŸâ€æ ·æœ¬å’Œå®Œå…¨å¤±è´¥çš„æ ·æœ¬ï¼Œä»è€Œä¸¢å¼ƒäº†æœ‰ä»·å€¼çš„å­¦ä¹ ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¼ƒçš„å®ä½“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„ç»éªŒåˆ†æè¡¨æ˜ï¼Œä»£ç†æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«çš„çœŸå®å®ä½“æ•°é‡ä¸æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆæ­£ç›¸å…³å…³ç³»ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®ä½“æ„ŸçŸ¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆE-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ¶å®šäº†ä¸€ä¸ªå¯†é›†çš„å®ä½“æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚E-GRPOä¸ºä¸æ­£ç¡®çš„æ ·æœ¬åˆ†é…éƒ¨åˆ†å¥–åŠ±ï¼Œæ¯”ä¾‹ä¸å®ƒä»¬çš„å®ä½“åŒ¹é…ç‡ç›¸ç¬¦ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»è¿™äº›â€œè¿‘ä¼¼æˆåŠŸâ€çš„æ ·æœ¬ä¸­å­¦ä¹ ã€‚åœ¨å¤šç§é—®ç­”ï¼ˆQAï¼‰å’Œæ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒE-GRPOå§‹ç»ˆä¸”æ˜¾è‘—ä¼˜äºGRPOåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒE-GRPOä¸ä»…å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”è¿˜è¯±å¯¼äº†æ›´æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ï¼Œéœ€è¦æ›´å°‘çš„å·¥å…·è°ƒç”¨ï¼Œæ˜¾ç¤ºäº†ä¸€ç§æ›´æœ‰æ•ˆå’Œæ ·æœ¬æ•ˆç‡æ›´é«˜çš„æœç´¢ä»£ç†å¯¹é½æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24694v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœç´¢ä»£ç†åœ¨å®ä½“ä¸ºä¸­å¿ƒçš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒä»¥è§£å†³å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ™®éé‡‡ç”¨çš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–GRPOï¼‰å¿½è§†äº†ä¸°å¯Œçš„å®ä½“ä¿¡æ¯ï¼Œä¾èµ–ç¨€ç–çš„ç»“æœå¯¼å‘å¥–åŠ±ï¼Œå¯¼è‡´æ— æ³•åŒºåˆ†ä¿¡æ¯ä¸°å¯Œçš„â€œè¿‘é”™æ ·æœ¬â€ï¼ˆå…·æœ‰æ­£ç¡®æ¨ç†ä½†ç­”æ¡ˆæœ‰ç¼ºé™·çš„æ ·æœ¬ï¼‰å’Œå®Œå…¨å¤±è´¥çš„æ ·æœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¼ƒçš„å®ä½“ä¿¡æ¯ï¼Œå‘ç°ä»£ç†æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«çš„çœŸå®å®ä½“æ•°é‡ä¸æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆæ­£ç›¸å…³å…³ç³»ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å®ä½“æ„ŸçŸ¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆE-GRPOï¼‰çš„æ–°æ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªå¯†é›†çš„å®ä½“æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚E-GRPOä¸ºé”™è¯¯æ ·æœ¬åˆ†é…éƒ¨åˆ†å¥–åŠ±ï¼Œå¥–åŠ±é‡ä¸å®ä½“åŒ¹é…ç‡æˆæ­£æ¯”ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»è¿™äº›â€œè¿‘é”™æ ·æœ¬â€ä¸­å­¦ä¹ ã€‚åœ¨é—®ç­”å’Œæ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒE-GRPOåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºGRPOã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼ŒE-GRPOä¸ä»…å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¿˜äº§ç”Ÿäº†æ›´é«˜æ•ˆçš„æ¨ç†ç­–ç•¥ï¼Œéœ€è¦è¾ƒå°‘çš„å·¥å…·è°ƒç”¨æ¬¡æ•°ã€‚å®ƒä¸ºæˆ‘ä»¬æä¾›äº†æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMæœç´¢ä»£ç†é€šè¿‡å®ä½“ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ•°æ®è§£å†³å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚</li>
<li>å½“å‰è®­ç»ƒæ–¹æ³•å¿½ç•¥å®ä½“ä¿¡æ¯å¹¶ä¾èµ–ç¨€ç–å¥–åŠ±ç³»ç»Ÿï¼Œéš¾ä»¥åŒºåˆ†é«˜è´¨é‡çš„é”™è¯¯æ ·æœ¬ä¸å¤±è´¥æ ·æœ¬ã€‚ </li>
<li>å®ä½“æ„ŸçŸ¥å¥–åŠ±å‡½æ•°è¢«å¼•å…¥ä»¥è§£å†³è¯¥é—®é¢˜ã€‚æ–°æå‡ºçš„E-GRPOæ¡†æ¶èƒ½æœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸é”™è¯¯æ ·æœ¬ï¼Œå¹¶ä¸ºé”™è¯¯æ ·æœ¬åˆ†é…éƒ¨åˆ†å¥–åŠ±ã€‚ </li>
<li>E-GRPOæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå‡å°‘äº†å·¥å…·è°ƒç”¨çš„éœ€æ±‚ã€‚è¿™è¡¨æ˜å…¶åœ¨æé«˜æœç´¢ä»£ç†çš„å¯¹é½æ–¹é¢å–å¾—äº†è¿›å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-da91b856e6db107e1fe66fd758ac4695~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278772&auth_key=1762278772-0-0-dbd8780c25ba26029d439b3fc222e225&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b38ca71d4db190b124ae3bfa2722315~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278779&auth_key=1762278779-0-0-3efcf697dfe5c6b2f98f5970e60a551c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d49e85504dbfb88a45b87206a6d5022~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278786&auth_key=1762278786-0-0-5d65c80bf077b7b5080a2c4671995afd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OpenReward-Learning-to-Reward-Long-form-Agentic-Tasks-via-Reinforcement-Learning"><a href="#OpenReward-Learning-to-Reward-Long-form-Agentic-Tasks-via-Reinforcement-Learning" class="headerlink" title="OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning"></a>OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement   Learning</h2><p><strong>Authors:Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren</strong></p>
<p>Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the modelâ€™s internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ï¼Œå·²æˆä¸ºè®­ç»ƒå’Œæ¨ç†ä¸­äººç±»è¯„ä¼°çš„å¯æ‰©å±•ä»£ç†ã€‚ç„¶è€Œï¼Œç°æœ‰RMsåœ¨çŸ¥è¯†å¯†é›†å‹å’Œé•¿æ ¼å¼ä»»åŠ¡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™äº›ä»»åŠ¡çš„æ­£ç¡®æ€§è¯„ä¼°éœ€è¦è¶…å‡ºæ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„åŸºç¡€ã€‚è¿™ä¸€å±€é™æ€§ä½¿ä»–ä»¬éš¾ä»¥å¯é åœ°åŒºåˆ†ç»†å¾®çš„è´¨é‡å·®å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤–éƒ¨è¯æ®çš„æƒ…å†µä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OpenRMï¼Œè¿™æ˜¯ä¸€ä¸ªå·¥å…·å¢å¼ºå‹é•¿æ ¼å¼å¥–åŠ±æ¨¡å‹ï¼Œå®ƒé€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥æ”¶é›†ç›¸å…³è¯æ®ï¼Œä»è€Œç³»ç»Ÿåœ°åˆ¤æ–­å¼€æ”¾å¼å“åº”ã€‚æˆ‘ä»¬ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨è¶…è¿‡27000ä¸ªé€šè¿‡å¯æ§æ•°æ®åˆæˆæ¡†æ¶åˆæˆçš„é…å¯¹ç¤ºä¾‹ä¸Šè®­ç»ƒOpenRMã€‚è®­ç»ƒç›®æ ‡è”åˆç›‘ç£ä¸­é—´å·¥å…·ä½¿ç”¨å’Œæœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ï¼Œæ¿€åŠ±æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„åŸºäºè¯æ®çš„åˆ¤æ–­ç­–ç•¥ã€‚åœ¨ä¸‰ä¸ªæ–°æ”¶é›†çš„æ•°æ®é›†å’Œä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOpenRMæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚ä½œä¸ºè¿›ä¸€æ­¥çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†OpenRMé›†æˆåˆ°æ¨ç†æ—¶é—´çš„å“åº”é€‰æ‹©å’Œè®­ç»ƒæ—¶é—´çš„æ•°æ®é€‰æ‹©ä¸­ã€‚è¿™åœ¨ä¸‹æ¸¸LLMå¯¹é½ä»»åŠ¡ä¸­äº§ç”Ÿäº†æŒç»­çš„æ”¶ç›Šï¼Œçªå‡ºäº†å·¥å…·å¢å¼ºå¥–åŠ±æ¨¡å‹åœ¨å¯é é•¿æ ¼å¼è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24636v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç¯‡å…³äºå¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰çš„è®ºæ–‡æŒ‡å‡ºï¼ŒRMsåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨ä¸­å·²æˆä¸ºå…³é”®ï¼Œä½œä¸ºäººç±»è¯„ä¼°çš„å¯æ‰©å±•ä»£ç†ï¼Œç”¨äºè®­ç»ƒå’Œæ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰RMsåœ¨çŸ¥è¯†å¯†é›†å‹å’Œé•¿æ ¼å¼ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦è¶…è¶Šæ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„è¯„ä¼°æ­£ç¡®æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†OpenRMï¼Œä¸€ä¸ªé€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥ç³»ç»Ÿåœ°è¯„ä¼°å¼€æ”¾å¼å“åº”çš„å·¥å…·å¢å¼ºå‹é•¿æ ¼å¼å¥–åŠ±æ¨¡å‹ã€‚é€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨è¶…è¿‡2.7ä¸‡ä¸ªåˆæˆå¯¹ç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›ç¤ºä¾‹é€šè¿‡ä¸€ä¸ªå¯æ§çš„æ•°æ®åˆæˆæ¡†æ¶ç”Ÿæˆã€‚åŸ¹è®­ç›®æ ‡è”åˆç›‘ç£ä¸­é—´å·¥å…·ä½¿ç”¨å’Œæœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ï¼Œæ¿€åŠ±å¥–åŠ±æ¨¡å‹å­¦ä¹ æœ‰æ•ˆçš„åŸºäºè¯æ®çš„åˆ¤æ–­ç­–ç•¥ã€‚åœ¨ä¸‰ä¸ªæ–°æ”¶é›†çš„æ•°æ®é›†å’Œä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOpenRMæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚ä½œä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶æ–¹å‘ï¼Œè®ºæ–‡å°†OpenRMé›†æˆåˆ°æ¨ç†æ—¶é—´å“åº”é€‰æ‹©å’Œè®­ç»ƒæ—¶é—´æ•°æ®é€‰æ‹©ä¸­ï¼Œä¸ºä¸‹æ¸¸LLMå¯¹é½ä»»åŠ¡å¸¦æ¥äº†ä¸€è‡´çš„æ”¶ç›Šï¼Œçªæ˜¾äº†å·¥å…·å¢å¼ºå¥–åŠ±æ¨¡å‹åœ¨å¯é é•¿æ ¼å¼è¯„ä¼°æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½œä¸ºäººç±»è¯„ä¼°çš„ä»£ç†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ã€‚</li>
<li>ç°æœ‰RMsåœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹å’Œé•¿æ ¼å¼ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å¯é åœ°åŒºåˆ†å¾®å¦™çš„å“è´¨å·®å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤–éƒ¨è¯æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>OpenRMæ˜¯ä¸€ä¸ªå·¥å…·å¢å¼ºå‹é•¿æ ¼å¼å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥ç³»ç»Ÿåœ°è¯„ä¼°å¼€æ”¾å¼å“åº”ã€‚</li>
<li>OpenRMé€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè”åˆç›‘ç£ä¸­é—´å·¥å…·ä½¿ç”¨å’Œæœ€ç»ˆç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>OpenRMæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>OpenRMè¢«é›†æˆåˆ°æ¨ç†æ—¶é—´å“åº”é€‰æ‹©å’Œè®­ç»ƒæ—¶é—´æ•°æ®é€‰æ‹©ä¸­ï¼Œæé«˜äº†ä¸‹æ¸¸LLMå¯¹é½ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å·¥å…·å¢å¼ºå¥–åŠ±æ¨¡å‹åœ¨å¯é çš„é•¿æ ¼å¼è¯„ä¼°æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5424f25d4e77b1d2f35b14fabafab1b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278793&auth_key=1762278793-0-0-a921e04e31c1e92f786fd2e3f1cca414&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b0a7688471cd22ba9a049b4f46f1c32b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278800&auth_key=1762278800-0-0-735549041906cfd1634b4b8246660148&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8003d6b47eb7325e5343407f77f0ae6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278807&auth_key=1762278807-0-0-52210e48ee603f7555658a67e03f8e66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1fa83c05917e6e710d41880d7732bb06~resize:0:q75.jpg?source=1f5c5e47&expiration=1762278814&auth_key=1762278814-0-0-6f49f886aff12d45436a57a6273373ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-366da6853bec5e8188a1ade9dca3abed~resize:0:q75.jpg?source=1f5c5e47&expiration=1762280933&auth_key=1762280933-0-0-41bf84b108d9ee610b531cb0f29ab4e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  NAUTILUS A Large Multimodal Model for Underwater Scene Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4f8226f0f48b4aadf7c84fc351e6cdd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310336&auth_key=1762310336-0-0-2b070882100d87c2463e117caf5b099c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  OmniMotion-X Versatile Multimodal Whole-Body Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
